<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.986478">
Structured Learning for Taxonomy Induction with Belief Propagation
</title>
<author confidence="0.950394">
Mohit Bansal David Burkett Gerard de Melo Dan Klein
</author>
<affiliation confidence="0.93375">
TTI Chicago Twitter Inc. Tsinghua University UC Berkeley
</affiliation>
<email confidence="0.99518">
mbansal@ttic.edu dburkett@twitter.com gdm@demelo.org klein@cs.berkeley.edu
</email>
<sectionHeader confidence="0.99497" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970291666667">
We present a structured learning approach
to inducing hypernym taxonomies using a
probabilistic graphical model formulation.
Our model incorporates heterogeneous re-
lational evidence about both hypernymy
and siblinghood, captured by semantic
features based on patterns and statistics
from Web n-grams and Wikipedia ab-
stracts. For efficient inference over tax-
onomy structures, we use loopy belief
propagation along with a directed span-
ning tree algorithm for the core hyper-
nymy factor. To train the system, we ex-
tract sub-structures of WordNet and dis-
criminatively learn to reproduce them, us-
ing adaptive subgradient stochastic opti-
mization. On the task of reproducing
sub-hierarchies of WordNet, our approach
achieves a 51% error reduction over a
chance baseline, including a 15% error re-
duction due to the non-hypernym-factored
sibling features. On a comparison setup,
we find up to 29% relative error reduction
over previous work on ancestor F1.
</bodyText>
<sectionHeader confidence="0.99812" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999677923076923">
Many tasks in natural language understanding,
such as question answering, information extrac-
tion, and textual entailment, benefit from lexical
semantic information in the form of types and hy-
pernyms. A recent example is IBM’s Jeopardy!
system Watson (Ferrucci et al., 2010), which used
type information to restrict the set of answer can-
didates. Information of this sort is present in term
taxonomies (e.g., Figure 1), ontologies, and the-
sauri. However, currently available taxonomies
such as WordNet are incomplete in coverage (Pen-
nacchiotti and Pantel, 2006; Hovy et al., 2009),
unavailable in many domains and languages, and
</bodyText>
<figure confidence="0.815212">
vertebrate
mammal reptile
</figure>
<figureCaption confidence="0.998028">
Figure 1: An excerpt of WordNet’s vertebrates taxonomy.
</figureCaption>
<bodyText confidence="0.999950793103448">
time-intensive to create or extend manually. There
has thus been considerable interest in building lex-
ical taxonomies automatically.
In this work, we focus on the task of taking col-
lections of terms as input and predicting a com-
plete taxonomy structure over them as output. Our
model takes a loglinear form and is represented
using a factor graph that includes both 1st-order
scoring factors on directed hypernymy edges (a
parent and child in the taxonomy) and 2nd-order
scoring factors on sibling edge pairs (pairs of hy-
pernym edges with a shared parent), as well as in-
corporating a global (directed spanning tree) struc-
tural constraint. Inference for both learning and
decoding uses structured loopy belief propagation
(BP), incorporating standard spanning tree algo-
rithms (Chu and Liu, 1965; Edmonds, 1967; Tutte,
1984). The belief propagation approach allows us
to efficiently and effectively incorporate hetero-
geneous relational evidence via hypernymy and
siblinghood (e.g., coordination) cues, which we
capture by semantic features based on simple sur-
face patterns and statistics from Web n-grams and
Wikipedia abstracts. We train our model to max-
imize the likelihood of existing example ontolo-
gies using stochastic optimization, automatically
learning the most useful relational patterns for full
taxonomy induction.
As an example of the relational patterns that our
</bodyText>
<figure confidence="0.96401675">
placental
cow rodent
squirrel rat
metatherian
marsupial
kangaroo
diapsid
snake crocodilian
anapsid
chelonian
turtle
1041
</figure>
<note confidence="0.9430595">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1041–1051,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998372309090909">
system learns, suppose we are interested in build-
ing a taxonomy for types of mammals (see Fig-
ure 1). Frequent attestation of hypernymy patterns
like rat is a rodent in large corpora is a strong sig-
nal of the link rodent —* rat. Moreover, sibling
or coordination cues like either rats or squirrels
suggest that rat is a sibling of squirrel and adds
evidence for the links rodent —* rat and rodent
—* squirrel. Our supervised model captures ex-
actly these types of intuitions by automatically dis-
covering such heterogeneous relational patterns as
features (and learning their weights) on edges and
on sibling edge pairs, respectively.
There have been several previous studies on
taxonomy induction. e.g., the incremental tax-
onomy induction system of Snow et al. (2006),
the longest path approach of Kozareva and Hovy
(2010), and the maximum spanning tree (MST)
approach of Navigli et al. (2011) (see Section 4 for
a more detailed overview). The main contribution
of this work is that we present the first discrimina-
tively trained, structured probabilistic model over
the full space of taxonomy trees, using a struc-
tured inference procedure through both the learn-
ing and decoding phases. Our model is also the
first to directly learn relational patterns as part of
the process of training an end-to-end taxonomic
induction system, rather than using patterns that
were hand-selected or learned via pairwise clas-
sifiers on manually annotated co-occurrence pat-
terns. Finally, it is the first end-to-end (i.e., non-
incremental) system to include sibling (e.g., coor-
dination) patterns at all.
We test our approach in two ways. First, on
the task of recreating fragments of WordNet, we
achieve a 51% error reduction on ancestor-based
F1 over a chance baseline, including a 15% error
reduction due to the non-hypernym-factored sib-
ling features. Second, we also compare to the re-
sults of Kozareva and Hovy (2010) by predicting
the large animal subtree of WordNet. Here, we
get up to 29% relative error reduction on ancestor-
based F1. We note that our approach falls at a
different point in the space of performance trade-
offs from past work – by producing complete,
highly articulated trees, we naturally see a more
even balance between precision and recall, while
past work generally focused on precision.1 To
1While different applications will value precision and
recall differently, and past work was often intentionally
precision-focused, it is certainly the case that an ideal solu-
tion would maximize both.
avoid presumption of a single optimal tradeoff, we
also present results for precision-based decoding,
where we trade off recall for precision.
</bodyText>
<sectionHeader confidence="0.896376" genericHeader="method">
2 Structured Taxonomy Induction
</sectionHeader>
<bodyText confidence="0.993092647058823">
Given an input term set x = {x1, x2, ... , xn},
we wish to compute the conditional distribution
over taxonomy trees y. This distribution P(yjx)
is represented using the graphical model formu-
lation shown in Figure 2. A taxonomy tree y is
composed of a set of indicator random variables
yij (circles in Figure 2), where yij = ON means
that xi is the parent of xj in the taxonomy tree
(i.e. there exists a directed edge from xi to xj).
One such variable exists for each pair (i, j) with
0 &lt; i &lt; n, 1 &lt; j &lt; n, and i =�j.2
In a factor graph formulation, a set of factors
(squares and rectangles in Figure 2) determines the
probability of each possible variable assignment.
Each factor F has an associated scoring function
OF, with the probability of a total assignment de-
termined by the product of all these scores:
</bodyText>
<equation confidence="0.9952735">
P (yjx) °C � OF(y) (1)
F
</equation>
<subsectionHeader confidence="0.989216">
2.1 Factor Types
</subsectionHeader>
<bodyText confidence="0.998974384615385">
In the models we present here, there are three
types of factors: EDGE factors that score individ-
ual edges in the taxonomy tree, SIBLING factors
that score pairs of edges with a shared parent, and
a global TREE factor that imposes the structural
constraint that y form a legal taxonomy tree.
EDGE Factors. For each edge variable yij in
the model, there is a corresponding factor Eij
(small blue squares in Figure 2) that depends only
on yij. We score each edge by extracting a set
of features f(xi, xj) and weighting them by the
(learned) weight vector w. So, the factor scoring
function is:
</bodyText>
<equation confidence="0.999818">
OEij(yij) = � exp(w · f(xi, xj)) yij = ON
exp(0) = 1 yij = OFF
</equation>
<bodyText confidence="0.9922166">
SIBLING Factors. Our second model also in-
cludes factors that permit 2nd-order features look-
ing at terms that are siblings in the taxonomy tree.
For each triple (i, j, k) with i =� j, i =� k, and
j &lt; k,3 we have a factor Sijk (green rectangles in
</bodyText>
<footnote confidence="0.96308825">
2We assume a special dummy root symbol xo.
3The ordering of the siblings xj and xk doesn’t mat-
ter here, so having separate factors for (i, j, k) and (i, k, j)
would be redundant.
</footnote>
<page confidence="0.606295">
1042
</page>
<figure confidence="0.99476006060606">
(a) Edge Features Only
(b) Full Model
T
Y01 Y02 Yon
Eol
E02
Eon
3112
3/1n
E12
Eln
Y21 Y2n
E21
E2n
Yn1 Yn2
Ent
Ent
T
3/01 Y02 3/on
Y21 Y2n
Yn1 Yn2
Ent
Eol
E21 E2n
3112 3hn
Ent
Eoz
E12
Eon
Eln
S12n
S21n
Sn12
</figure>
<figureCaption confidence="0.997871">
Figure 2: Factor graph representation of our model, both without (a) and with (b) SIBLING factors.
</figureCaption>
<bodyText confidence="0.89950825">
Figure 2b) that depends on yij and yik, and thus
can be used to encode features that should be ac-
tive whenever xj and xk share the same parent, xi.
The scoring function is similar to the one above:
</bodyText>
<equation confidence="0.97407025">
�
exp(w · f(xi, xj, xk)) yij = yik = ON
φSijk(yij, yik) =
1 otherwise
</equation>
<bodyText confidence="0.982169642857143">
TREE Factor. Of course, not all variable as-
signments y form legal taxonomy trees (i.e., di-
rected spanning trees). For example, the assign-
ment ∀i, j, yij = ON might get a high score, but
would not be a valid output of the model. Thus,
we need to impose a structural constraint to ensure
that such illegal variable assignments are assigned
0 probability by the model. We encode this in our
factor graph setting using a single global factor T
(shown as a large red square in Figure 2) with the
following scoring function:
Note that by substituting our model’s factor scor-
ing functions into Equation 1, we get:
�
</bodyText>
<equation confidence="0.987408666666667">
exp(w · f(y)) y is a tree
P(y|x) a
0 otherwise
</equation>
<bodyText confidence="0.9998805">
Thus, our model has the form of a standard loglin-
ear model with feature function f.
</bodyText>
<subsectionHeader confidence="0.989089">
2.2 Inference via Belief Propagation
</subsectionHeader>
<bodyText confidence="0.999920225806451">
With the model defined, there are two main in-
ference tasks we wish to accomplish: computing
expected feature counts and selecting a particular
taxonomy tree for a given set of input terms (de-
coding). As an initial step to each of these pro-
cedures, we wish to compute the marginal prob-
abilities of particular edges (and pairs of edges)
being on. In a factor graph, the natural infer-
ence procedure for computing marginals is belief
propagation. Note that finding taxonomy trees is
a structurally identical problem to directed span-
ning trees (and thereby non-projective dependency
parsing), for which belief propagation has previ-
ously been worked out in depth (Smith and Eisner,
2008). Therefore, we will only briefly sketch the
procedure here.
Belief propagation is a general-purpose infer-
ence method that computes marginals via directed
messages passed from variables to adjacent fac-
tors (and vice versa) in the factor graph. These
messages take the form of (possibly unnormal-
ized) distributions over values of the variable. The
two types of messages (variable to factor or fac-
tor to variable) have mutually recursive defini-
tions. The message from a factor F to an adjacent
variable V involves a sum over all possible val-
ues of every other variable that F touches. While
the EDGE and SIBLING factors are simple enough
to compute this sum by brute force, performing
the sum naively for computing messages from the
TREE factor would take exponential time. How-
</bodyText>
<equation confidence="0.907025">
OT (y) =
Model. For a given global assignment y, let
�
1 y forms a legal taxonomy tree
0 otherwise
E
f(y) =
i,j
yij=ON
Ef(xi, xj) + f(xi, xj, xk)
i,j,k
yij=yik=ON
1043
</equation>
<bodyText confidence="0.9999758">
ever, due to the structure of that particular factor,
all of its outgoing messages can be computed si-
multaneously in O(n3) time via an efficient adap-
tation of Kirchhoff’s Matrix Tree Theorem (MTT)
(Tutte, 1984) which computes partition functions
and marginals for directed spanning trees.
Once message passing is completed, marginal
beliefs are computed by merely multiplying to-
gether all the messages received by a particular
variable or factor.
</bodyText>
<subsectionHeader confidence="0.810084">
2.2.1 Loopy Belief Propagation
</subsectionHeader>
<bodyText confidence="0.999996962962963">
Looking closely at Figure 2a, one can observe
that the factor graph for the first version of our
model, containing only EDGE and TREE factors,
is acyclic. In this special case, belief propagation
is exact: after one round of message passing, the
beliefs computed (as discussed in Section 2.2) will
be the true marginal probabilities under the cur-
rent model. However, in the full model, shown
in Figure 2b, the SIBLING factors introduce cy-
cles into the factor graph, and now the messages
being passed around often depend on each other
and so they will change as they are recomputed.
The process of iteratively recomputing messages
based on earlier messages is known as loopy belief
propagation. This procedure only finds approx-
imate marginal beliefs, and is not actually guar-
anteed to converge, but in practice can be quite
effective for finding workable marginals in mod-
els for which exact inference is intractable, as is
the case here. All else equal, the more rounds
of message passing that are performed, the closer
the computed marginal beliefs will be to the true
marginals, though in practice, there are usually di-
minishing returns after the first few iterations. In
our experiments, we used a fairly conservative up-
per bound of 20 iterations, but in most cases, the
messages converged much earlier than that.
</bodyText>
<subsectionHeader confidence="0.997075">
2.3 Training
</subsectionHeader>
<bodyText confidence="0.999561555555555">
We used gradient-based maximum likelihood
training to learn the model parameters w. Since
our model has a loglinear form, the derivative
of w with respect to the likelihood objective is
computed by just taking the gold feature vec-
tor and subtracting the vector of expected feature
counts. For computing expected counts, we run
belief propagation until completion and then, for
each factor in the model, we simply read off the
marginal probability of that factor being active (as
computed in Section 2.2), and accumulate a par-
tial count for each feature that is fired by that fac-
tor. This method of computing the gradient can be
incorporated into any gradient-based optimizer in
order to learn the weights w. In our experiments
we used AdaGrad (Duchi et al., 2011), an adaptive
subgradient variant of standard stochastic gradient
ascent for online learning.
</bodyText>
<subsectionHeader confidence="0.995405">
2.4 Decoding
</subsectionHeader>
<bodyText confidence="0.95121172">
Finally, once the model parameters have been
learned, we want to use the model to find taxon-
omy trees for particular sets of input terms. Note
that if we limit our scores to be edge-factored,
then finding the highest scoring taxonomy tree
becomes an instance of the MST problem (also
known as the maximum arborescence problem
for the directed case), which can be solved effi-
ciently in O(n2) quadratic time (Tarjan, 1977) us-
ing the greedy, recursive Chu-Liu-Edmonds algo-
rithm (Chu and Liu, 1965; Edmonds, 1967).4
Since the MST problem can be solved effi-
ciently, the main challenge becomes finding a way
to ensure that our scores are edge-factored. In the
first version of our model, we could simply set the
score of each edge to be w·f(xZ, xj), and the MST
recovered in this way would indeed be the high-
est scoring tree: arg max,P(y|x). However, this
straightforward approach doesn’t apply to the full
model which also uses sibling features. Hence, at
decoding time, we instead start out by once more
using belief propagation to find marginal beliefs,
and then set the score of each edge to be its belief
bYij (ON) 5
odds ratio: bYij (OFF).
</bodyText>
<sectionHeader confidence="0.996262" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.998494842105263">
While spanning trees are familiar from non-
projective dependency parsing, features based on
the linear order of the words or on lexical identi-
4See Georgiadis (2003) for a detailed algorithmic proof,
and McDonald et al. (2005) for an illustrative example. Also,
we constrain the Chu-Liu-Edmonds MST algorithm to out-
put only single-root MSTs, where the (dummy) root has ex-
actly one child (Koo et al., 2007), because multi-root span-
ning ‘forests’ are not applicable to our task.
Also, note that we currently assume one node per term. We
are following the task description from previous work where
the goal is to create a taxonomy for a specific domain (e.g.,
animals). Within a specific domain, terms typically just have
a single sense. However, our algorithms could certainly be
adapted to the case of multiple term senses (by treating the
different senses as unique nodes in the tree) in future work.
5The MST that is found using these edge scores is actually
the minimum Bayes risk tree (Goodman, 1996) for an edge
accuracy loss function (Smith and Eisner, 2008).
</bodyText>
<page confidence="0.43282">
1044
</page>
<bodyText confidence="0.999978833333333">
ties or syntactic word classes, which are primary
drivers for dependency parsing, are mostly unin-
formative for taxonomy induction. Instead, induc-
ing taxonomies requires world knowledge to cap-
ture the semantic relations between various unseen
terms. For this, we use semantic cues to hyper-
nymy and siblinghood via features on simple sur-
face patterns and statistics in large text corpora.
We fire features on both the edge and the sibling
factors. We first describe all the edge features
in detail (Section 3.1 and Section 3.2), and then
briefly describe the sibling features (Section 3.3),
which are quite similar to the edge ones.
For each edge factor Eij, which represents the
potential parent-child term pair (xi, xj), we add
the surface and semantic features discussed below.
Note that since edges are directed, we have sepa-
rate features for the factors Eij versus Eji.
</bodyText>
<subsectionHeader confidence="0.998716">
3.1 Surface Features
</subsectionHeader>
<bodyText confidence="0.975334666666667">
Capitalization: Checks which of xi and xj are
capitalized, with one feature for each value of the
tuple (isCap(xi), isCap(xj)). The intuition is that
leaves of a taxonomy are often proper names and
hence capitalized, e.g., (bison, American bison).
Therefore, the feature for (true, false) (i.e., parent
capitalized but not the child) gets a substantially
negative weight.
Ends with: Checks if xj ends with xi, or not. This
captures pairs such as (fish, bony fish) in our data.
Contains: Checks if xj contains xi, or not. This
captures pairs such as (bird, bird ofprey).
Suffix match: Checks whether the k-length suf-
fixes of xi and xj match, or not, for k =
1,2,...,7.
LCS: We compute the longest common substring
of xi and xj, and create indicator features for
rounded-off and binned values of |LCS|/((|xi |+
|xj|)/2).
Length difference: We compute the signed length
difference between xj and xi, and create indica-
tor features for rounded-off and binned values of
(|xj |− |xi|)/((|xi |+ |xj|)/2). Yang and Callan
(2009) use a similar feature.
</bodyText>
<subsectionHeader confidence="0.970997">
3.2 Semantic Features
3.2.1 Web n-gram Features
</subsectionHeader>
<bodyText confidence="0.999733375">
Patterns and counts: Hypernymy for a term pair
(P=xi, C=xj) is often signaled by the presence
of surface patterns like C is a P, P such as C
in large text corpora, an observation going back
to Hearst (1992). For each potential parent-child
edge (P=xi, C=xj), we mine the top k strings
(based on count) in which both xi and xj occur
(we use k=200). We collect patterns in both direc-
tions, which allows us to judge the correct direc-
tion of an edge (e.g., C is a P is a positive signal
for hypernymy whereas P is a C is a negative sig-
nal).6 Next, for each pattern in this top-k list, we
compute its normalized pattern count c, and fire
an indicator feature on the tuple (pattern, t), for
all thresholds t (in a fixed set) s.t. c &gt; t. Our
supervised model then automatically learns which
patterns are good indicators of hypernymy.
Pattern order: We add features on the order (di-
rection) in which the pair (xi, xj) found a pattern
(in its top-k list) – indicator features for boolean
values of the four cases: P ... C, C ... P, neither
direction, and both directions. Ritter et al. (2009)
used the ‘both’ case of this feature.
Individual counts: We also compute the indi-
vidual Web-scale term counts cxi and cxj, and
add a comparison feature (cxi&gt;cxj), plus features
on values of the signed count difference (|cxi |−
|cxj|)/((|cxi |+ |cxj|)/2), after rounding off, and
binning at multiple granularities. The intuition is
that this feature could learn whether the relative
popularity of the terms signals their hypernymy di-
rection.
</bodyText>
<subsectionHeader confidence="0.538416">
3.2.2 Wikipedia Abstract Features
</subsectionHeader>
<bodyText confidence="0.999799529411765">
The Web n-grams corpus has broad coverage but
is limited to up to 5-grams, so it may not contain
pattern-based evidence for various longer multi-
word terms and pairs. Therefore, we supplement
it with a full-sentence resource, namely Wikipedia
abstracts, which are concise descriptions (hence
useful to signal hypernymy) of a large variety of
world entities.
Presence and distance: For each potential edge
(xi, xj), we mine patterns from all abstracts in
which the two terms co-occur in either order, al-
lowing a maximum term distance of 20 (because
beyond that, co-occurrence may not imply a rela-
tion). We add a presence feature based on whether
the process above found at least one pattern for
that term pair, or not. We also fire features on
the value of the minimum distance dmin at which
</bodyText>
<footnote confidence="0.911353">
6We also allow patterns with surrounding words, e.g., the
C is a P and C , P of.
</footnote>
<page confidence="0.63966">
1045
</page>
<bodyText confidence="0.9951055">
the two terms were found in some abstract (plus
thresholded versions).
Patterns: For each term pair, we take the top-k&apos;
patterns (based on count) of length up to l from
its full list of patterns, and add an indicator feature
on each pattern string (without the counts). We use
k&apos;=5, l=10. Similar to the Web n-grams case, we
also fire Wikipedia-based pattern order features.
</bodyText>
<subsectionHeader confidence="0.999721">
3.3 Sibling Features
</subsectionHeader>
<bodyText confidence="0.999986611111111">
We also incorporate similar features on sibling
factors. For each sibling factor Sijk which rep-
resents the potential parent-children term triple
(xi, xj, xk), we consider the potential sibling term
pair (xj, xk). Siblinghood for this pair would be
indicated by the presence of surface patterns such
as either C1 or C2, C1 is similar to C2 in large cor-
pora. Hence, we fire Web n-gram pattern features
and Wikipedia presence, distance, and pattern fea-
tures, similar to those described above, on each
potential sibling term pair.7 The main difference
here from the edge factors is that the sibling fac-
tors are symmetric (in the sense that Sijk is redun-
dant to Sikj) and hence the patterns are undirected.
Therefore, for each term pair, we first symmetrize
the collected Web n-grams and Wikipedia patterns
by accumulating the counts of symmetric patterns
like rats or squirrels and squirrels or rats.8
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.985619671052632">
In our work, we assume a known term set and
do not address the problem of extracting related
terms from text. However, a great deal of past
work has considered automating this process, typ-
ically taking one of two major approaches. The
clustering-based approach (Lin, 1998; Lin and
Pantel, 2002; Davidov and Rappoport, 2006; Ya-
mada et al., 2009) discovers relations based on the
assumption that similar concepts appear in sim-
7One can also add features on the full triple (xi, xj, xk)
but most such features will be sparse.
8All the patterns and counts for our Web and Wikipedia
edge and sibling features described above are extracted after
stemming the words in the terms, the n-grams, and the ab-
stracts (using the Porter stemmer). Also, we threshold the
features (to prune away the sparse ones) by considering only
those that fire for at least t trees in the training data (t = 4 in
our experiments).
Note that one could also add various complementary types of
useful features presented by previous work, e.g., bootstrap-
ping using syntactic heuristics (Phillips and Riloff, 2002),
dependency patterns (Snow et al., 2006), doubly anchored
patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web
definition classifiers (Navigli et al., 2011).
ilar contexts (Harris, 1954). The pattern-based
approach uses special lexico-syntactic patterns to
extract pairwise relation lists (Phillips and Riloff,
2002; Girju et al., 2003; Pantel and Pennacchiotti,
2006; Suchanek et al., 2007; Ritter et al., 2009;
Hovy et al., 2009; Baroni et al., 2010; Ponzetto
and Strube, 2011) and semantic classes or class-
instance pairs (Riloff and Shepherd, 1997; Katz
and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005;
Talukdar et al., 2008).
We focus on the second step of taxonomy induc-
tion, namely the structured organization of terms
into a complete and coherent tree-like hierarchy.9
Early work on this task assumes a starting par-
tial taxonomy and inserts missing terms into it.
Widdows (2003) place unknown words into a re-
gion with the most semantically-similar neigh-
bors. Snow et al. (2006) add novel terms by greed-
ily maximizing the conditional probability of a set
of relational evidence given a taxonomy. Yang and
Callan (2009) incrementally cluster terms based
on a pairwise semantic distance. Lao et al. (2012)
extend a knowledge base using a random walk
model to learn binary relational inference rules.
However, the task of inducing full taxonomies
without assuming a substantial initial partial tax-
onomy is relatively less well studied. There is
some prior work on the related task of hierarchical
clustering, or grouping together of semantically
related words (Cimiano et al., 2005; Cimiano and
Staab, 2005; Poon and Domingos, 2010; Fountain
and Lapata, 2012). The task we focus on, though,
is the discovery of direct taxonomic relationships
(e.g., hypernymy) between words.
We know of two closely-related previous sys-
tems, Kozareva and Hovy (2010) and Navigli et
al. (2011), that build full taxonomies from scratch.
Both of these systems use a process that starts
by finding basic level terms (leaves of the fi-
nal taxonomy tree, typically) and then using re-
lational patterns (hand-selected ones in the case of
Kozareva and Hovy (2010), and ones learned sep-
arately by a pairwise classifier on manually anno-
tated co-occurrence patterns for Navigli and Ve-
lardi (2010), Navigli et al. (2011)) to find interme-
diate terms and all the attested hypernymy links
between them.10 To prune down the resulting tax-
9Determining the set of input terms is orthogonal to our
work, and our method can be used in conjunction with vari-
ous term extraction approaches described above.
10Unlike our system, which assumes a complete set of
terms and only attempts to induce the taxonomic structure,
</bodyText>
<page confidence="0.564448">
1046
</page>
<bodyText confidence="0.999947136363636">
onomy graph, Kozareva and Hovy (2010) use a
procedure that iteratively retains the longest paths
between root and leaf terms, removing conflicting
graph edges as they go. The end result is acyclic,
though not necessarily a tree; Navigli et al. (2011)
instead use the longest path intuition to weight
edges in the graph and then find the highest weight
taxonomic tree using a standard MST algorithm.
Our work differs from the two systems above
in that ours is the first discriminatively trained,
structured probabilistic model over the full space
of taxonomy trees that uses structured inference
via spanning tree algorithms (MST and MTT)
through both the learning and decoding phases.
Our model also automatically learns relational pat-
terns as a part of the taxonomic training phase, in-
stead of relying on hand-picked rules or pairwise
classifiers on manually annotated co-occurrence
patterns, and it is the first end-to-end (i.e., non-
incremental) system to include heterogeneous re-
lational information via sibling (e.g., coordina-
tion) patterns.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997016">
5.1 Data and Experimental Regime
</subsectionHeader>
<bodyText confidence="0.999132815384616">
We considered two distinct experimental setups,
one that illustrates the general performance of
our model by reproducing various medium-sized
WordNet domains, and another that facilitates
comparison to previous work by reproducing the
much larger animal subtree provided by Kozareva
and Hovy (2010).
General setup: In order to test the accuracy
of structured prediction on medium-sized full-
domain taxonomies, we extracted from WordNet
3.0 all bottomed-out full subtrees which had a
tree-height of 3 (i.e., 4 nodes from root to leaf),
and contained (10, 50] terms.11 This gives us
761 non-overlapping trees, which we partition into
both these systems include term discovery in the taxonomy
building process.
11Subtrees that had a smaller or larger tree height were dis-
carded in order to avoid overlap between the training and test
divisions. This makes it a much stricter setting than other
tasks such as parsing, which usually has repeated sentences,
clauses and phrases between training and test sets.
To project WordNet synsets to terms, we used the first (most
frequent) term in each synset. A few WordNet synsets have
multiple parents so we only keep the first of each such pair of
overlapping trees. We also discard a few trees with duplicate
terms because this is mostly due to the projection of different
synsets to the same term, and theoretically makes the tree a
graph.
70/15/15% (533/114/114 trees) train/dev/test sets.
Comparison setup: We also compare our method
(as closely as possible) with related previous work
by testing on the much larger animal subtree made
available by Kozareva and Hovy (2010), who cre-
ated this dataset by selecting a set of ‘harvested’
terms and retrieving all the WordNet hypernyms
between each input term and the root (i.e., an-
imal), resulting in —700 terms and —4,300 is-a
ancestor-child links.12 Our training set for this an-
imal test case was generated from WordNet us-
ing the following process: First, we strictly re-
move the full animal subtree from WordNet in or-
der to avoid any possible overlap with the test data.
Next, we create random 25-sized trees by picking
random nodes as singleton trees, and repeatedly
adding child edges from WordNet to the tree. This
process gives us a total of —1600 training trees.13
Feature sources: The n-gram semantic features
are extracted from the Google n-grams corpus
(Brants and Franz, 2006), a large collection of
English n-grams (for n = 1 to 5) and their fre-
quencies computed from almost 1 trillion tokens
(95 billion sentences) of Web text. The Wikipedia
abstracts are obtained via the publicly available
dump, which contains almost —4.1 million ar-
ticles.14 Preprocessing includes standard XML
parsing and tokenization. Efficient collection of
feature statistics is important because these must
be extracted for millions of query pairs (for each
potential edge and sibling pair in each term set).
For this, we use a hash-trie on term pairs (sim-
ilar to that of Bansal and Klein (2011)), and scan
once through the n-gram (or abstract) set, skipping
many n-grams (or abstracts) based on fast checks
of missing unigrams, exceeding length, suffix mis-
matches, etc.
</bodyText>
<subsectionHeader confidence="0.993214">
5.2 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.863462625">
Ancestor F1: Measures the precision, recall, and
F1 = 2PR/(P + R) of correctly predicted ances-
12This is somewhat different from our general setup where
we work with any given set of terms; they start with a large
set of leaves which have substantial Web-based relational
information based on their selected, hand-picked patterns.
Their data is available at http://www.isi.edu/˜kozareva/
downloads.html.
</bodyText>
<footnote confidence="0.5245514">
13We tried this training regimen as different from that of
the general setup (which contains only bottomed-out sub-
trees), so as to match the animal test tree, which is of depth
12 and has intermediate nodes from higher up in WordNet.
14We used the 20130102 dump.
</footnote>
<table confidence="0.9884918">
1047
System P R F1
Edges-Only Model
Baseline 5.9 8.3 6.9
Surface Features 17.5 41.3 24.6
Semantic Features 37.0 49.1 42.2
Surface+Semantic 41.1 54.4 46.8
Edges + Siblings Model
Surface+Semantic 53.1 56.6 54.8
Surface+Semantic (Test) 48.0 55.2 51.4
</table>
<tableCaption confidence="0.999471">
Table 1: Main results on our general setup. On the devel-
</tableCaption>
<bodyText confidence="0.549358666666667">
opment set, we present incremental results on the edges-only
model where we start with the chance baseline, then use sur-
face features only, semantic features only, and both. Finally,
we add sibling factors and features to get results for the full,
edges+siblings model with all features, and also report the
final test result for this setting.
</bodyText>
<equation confidence="0.772107">
tors, i.e., pairwise is-a relations:
|isagold n isapredicted |_ |isagold n isapredicted|
P = |isapredicted  |R |isagold|
</equation>
<subsectionHeader confidence="0.882584">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999901896551724">
Table 1 shows our main results for ancestor-based
evaluation on the general setup. We present a de-
velopment set ablation study where we start with
the edges-only model (Figure 2a) and its random
tree baseline (which chooses any arbitrary span-
ning tree for the term set). Next, we show results
on the edges-only model with surface features
(Section 3.1), semantic features (Section 3.2), and
both. We see that both surface and semantic fea-
tures make substantial contributions, and they also
stack. Finally, we add the sibling factors and fea-
tures (Figure 2b, Section 3.3), which further im-
proves the results significantly (8% absolute and
15% relative error reduction over the edges-only
results on the ancestor F1 metric). The last row
shows the final test set results for the full model
with all features.
Table 2 shows our results for comparison to
the larger animal dataset of Kozareva and Hovy
(2010).15 In the table, ‘Kozareva2010’ refers
to Kozareva and Hovy (2010) and ‘Navigli2011’
refers to Navigli et al. (2011).16 For appropri-
15These results are for the 1st order model due to the scale
of the animal taxonomy (-700 terms). For scaling the 2nd
order sibling model, one can use approximations, e.g., prun-
ing the set of sibling factors based on 1st order link marginals,
or a hierarchical coarse-to-fine approach based on taxonomy
induction on subtrees, or a greedy approach of adding a few
sibling factors at a time. This is future work.
</bodyText>
<footnote confidence="0.6570635">
16The Kozareva and Hovy (2010) ancestor results are ob-
tained by using the output files provided on their webpage.
</footnote>
<table confidence="0.999933285714286">
System P R F1
Previous Work
Kozareva2010 98.6 36.2 52.9
Navigli2011** 97.0** 43.7** 60.3**
This Paper
Fixed Prediction 84.2 55.1 66.6
Free Prediction 79.3 49.0 60.6
</table>
<tableCaption confidence="0.998921">
Table 2: Comparison results on the animal dataset of
</tableCaption>
<bodyText confidence="0.974151930232558">
Kozareva and Hovy (2010). Here, ‘Kozareva2010’ refers to
Kozareva and Hovy (2010) and ‘Navigli2011’ refers to Nav-
igli et al. (2011). For appropriate comparison to each previ-
ous work, we show our results both for the ‘Fixed Prediction’
setup, which assumes the true root and leaves, and for the
‘Free Prediction’ setup, which doesn’t assume any prior in-
formation. The ** results of Navigli et al. (2011) represent a
different ground-truth data condition, making them incompa-
rable to our results; see Section 5.3 for details.
ate comparison to each previous work, we show
results for two different setups. The first setup
‘Fixed Prediction’ assumes that the model knows
the true root and leaves of the taxonomy to provide
for a somewhat fairer comparison to Kozareva and
Hovy (2010). We get substantial improvements
on ancestor-based recall and F1 (a 29% relative
error reduction). The second setup ‘Free Predic-
tion’ assumes no prior knowledge and predicts the
full tree (similar to the general setup case). On
this setup, we do compare as closely as possible
to Navigli et al. (2011) and see a small gain in F1,
but regardless, we should note that their results are
incomparable (denoted by ** in Table 2) because
they have a different ground-truth data condition:
their definition and hypernym extraction phase in-
volves using the Google define keyword, which
often returns WordNet glosses itself.
We note that previous work achieves higher an-
cestor precision, while our approach achieves a
more even balance between precision and recall.
Of course, precision and recall should both ide-
ally be high, even if some applications weigh one
over the other. This is why our tuning optimized
for F1, which represents a neutral combination
for comparison, but other Fα metrics could also
be optimized. In this direction, we also tried an
experiment on precision-based decoding (for the
‘Free Prediction’ scenario), where we discard any
edges with score (i.e., the belief odds ratio de-
scribed in Section 2.4) less than a certain thresh-
old. This allowed us to achieve high values of pre-
cision (e.g., 90.8%) at still high enough F1 values
(e.g., 61.7%).
</bodyText>
<table confidence="0.986359307692308">
1048
Hypernymy features
C and other P &gt; P &gt; C
C,Pof CisaP
C , a P P , including C
C or other P P ( C
C : a P C , american P
C - like P C , the P
Siblinghood features
C1 and C2 C1, C2 (
C1 or C2 of C1 and / or C2
, C1 , C2 and either C1 or C2
the C1 / C2 &lt;s&gt; C1 and C2 &lt;/s&gt;
</table>
<tableCaption confidence="0.9933855">
Table 3: Examples of high-weighted hypernymy and sibling-
hood features learned during development.
</tableCaption>
<figure confidence="0.770425333333333">
butterfly
copper hairstreak admiral
American copper Strymon melinus white admiral
</figure>
<figureCaption confidence="0.928889">
Figure 3: Excerpt from the predicted butterfly tree. The terms
</figureCaption>
<equation confidence="0.259436">
1
</equation>
<bodyText confidence="0.935043">
attached erroneously according to WordNet are marked in red
and italicized.
</bodyText>
<sectionHeader confidence="0.995259" genericHeader="method">
6 Analysis
</sectionHeader>
<bodyText confidence="0.999872291666667">
Table 3 shows some of the hypernymy and sibling-
hood features given highest weight by our model
(in general-setup development experiments). The
training process not only rediscovers most of the
standard Hearst-style hypernymy patterns (e.g., C
and other P, C is a P), but also finds various
novel, intuitive patterns. For example, the pattern
C, american P is prominent because it captures
pairs like Lemmon, american actor and Bryon,
american politician, etc. Another pattern &gt; P &gt;
C captures webpage navigation breadcrumb trails
(representing category hierarchies). Similarly, the
algorithm also discovers useful siblinghood fea-
tures, e.g., either C1 or C2, C1 and/or C2, etc.
Finally, we look at some specific output errors
to give as concrete a sense as possible of some sys-
tem confusions, though of course any hand-chosen
examples must be taken as illustrative. In Figure
3, we attach white admiral to admiral, whereas
the gold standard makes these two terms siblings.
In reality, however, white admirals are indeed a
species of admirals, so WordNet’s ground truth
turns out to be incomplete. Another such example
is that we place logistic assessment in the evalu-
</bodyText>
<figure confidence="0.952480333333333">
bottle
flask wine bottle jeroboam
vacuum flask thermos Erlenmeyer flask
</figure>
<figureCaption confidence="0.980361666666667">
Figure 4: Excerpt from the predicted bottle tree. The terms
attached erroneously according to WordNet are marked in red
and italicized.
</figureCaption>
<bodyText confidence="0.999504428571429">
ation subtree of judgment, but WordNet makes it
a direct child of judgment. However, other dictio-
naries do consider logistic assessments to be eval-
uations. Hence, this illustrates that there may be
more than one right answer, and that the low re-
sults on this task should only be interpreted as
such. In Figure 4, our algorithm did not recog-
nize that thermos is a hyponym of vacuum flask,
and that jeroboam is a kind of wine bottle. Here,
our Web n-grams dataset (which only contains fre-
quent n-grams) and Wikipedia abstracts do not
suffice and we would need to add richer Web data
for such world knowledge to be reflected in the
features.
</bodyText>
<sectionHeader confidence="0.99844" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999922941176471">
Our approach to taxonomy induction allows het-
erogeneous information sources to be combined
and balanced in an error-driven way. Direct indi-
cators of hypernymy, such as Hearst-style context
patterns, are the core feature for the model and are
discovered automatically via discriminative train-
ing. However, other indicators, such as coordina-
tion cues, can indicate that two words might be
siblings, independently of what their shared par-
ent might be. Adding second-order factors to our
model allows these two kinds of evidence to be
weighed and balanced in a discriminative, struc-
tured probabilistic framework. Empirically, we
see substantial gains (in ancestor F1) from sibling
features, and also over comparable previous work.
We also present results on the precision and recall
trade-offs inherent in this task.
</bodyText>
<sectionHeader confidence="0.997714" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999736833333333">
We would like to thank the anonymous review-
ers for their insightful comments. This work
was supported by BBN under DARPA contract
HR0011-12-C-0014, 973 Program China Grants
2011CBA00300, 2011CBA00301, and NSFC
Grants 61033001, 61361136003.
</bodyText>
<page confidence="0.796818">
1049
</page>
<sectionHeader confidence="0.995233" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999175458064516">
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of ACL.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based seman-
tic model based on properties and types. Cognitive
Science, 34(2):222–254.
Thorsten Brants and Alex Franz. 2006. The Google
Web 1T 5-gram corpus version 1.1. LDC2006T13.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the
shortest arborescence of a directed graph. Science
Sinica, 14(1396-1400):270.
Philipp Cimiano and Steffen Staab. 2005. Learning
concept hierarchies from text with a guided agglom-
erative clustering algorithm. In Proceedings of the
ICML 2005 Workshop on Learning and Extending
Lexical Ontologies with Machine Learning Meth-
ods.
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning concept hierarchies from text cor-
pora using formal concept analysis. Journal of Arti-
ficial Intelligence Research, 24(1):305–339.
Dmitry Davidov and Ari Rappoport. 2006. Effi-
cient unsupervised discovery of word categories us-
ing symmetric patterns and high frequency words.
In Proceedings of COLING-ACL.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71:233–240.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the Web:
An experimental study. Artificial Intelligence,
165(1):91–134.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, Nico Schlaefer, and Chris Welty. 2010.
Building watson: An overview of the DeepQA
project. AI magazine, 31(3):59–79.
Trevor Fountain and Mirella Lapata. 2012. Taxonomy
induction using hierarchical random graphs. In Pro-
ceedings of NAACL.
Leonidas Georgiadis. 2003. Arborescence optimiza-
tion problems solvable by edmonds algorithm. The-
oretical Computer Science, 301(1):427–437.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proceed-
ings of NAACL.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of ACL.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff.
2009. Toward completeness in concept extraction
and classification. In Proceedings of EMNLP.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
In Proceedings of the Workshop on NLP for Ques-
tion Answering (EACL 2003).
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured prediction mod-
els via the matrix-tree theorem. In Proceedings of
EMNLP-CoNLL.
Zornitsa Kozareva and Eduard Hovy. 2010. A
semi-supervised method to learn and construct tax-
onomies using the Web. In Proceedings of EMNLP.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W. Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP.
Dekang Lin and Patrick Pantel. 2002. Concept discov-
ery from text. In Proceedings of COLING.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexical
taxonomies from scratch. In Proceedings of IJCAI.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of COLING-ACL.
1050
Marius Pas¸ca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of CIKM.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In Proceedings of
COLING-ACL.
William Phillips and Ellen Riloff. 2002. Exploiting
strong syntactic heuristics and co-training to learn
semantic lexicons. In Proceedings of EMNLP.
Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively
built knowledge repository. Artificial Intelligence,
175(9):1737–1756.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings
of ACL.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-
based approach for building semantic lexicons. In
Proceedings of EMNLP.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of AAAI Spring Sympo-
sium on Learning by Reading and Learning to Read.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of WWW.
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pas¸ca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proceedings of EMNLP.
Robert E. Tarjan. 1977. Finding optimum branchings.
Networks, 7:25–35.
William T. Tutte. 1984. Graph theory. Addison-
Wesley.
Dominic Widdows. 2003. Unsupervised methods
for developing taxonomies by combining syntactic
and statistical information. In Proceedings of HLT-
NAACL.
Ichiro Yamada, Kentaro Torisawa, Jun’ichi Kazama,
Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-
cis Bond, and Asuka Sumida. 2009. Hypernym dis-
covery based on distributional similarity and hierar-
chical structures. In Proceedings of EMNLP.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In
Proceedings of ACL-IJCNLP.
</reference>
<page confidence="0.773124">
1051
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823426">
<title confidence="0.999701">Structured Learning for Taxonomy Induction with Belief Propagation</title>
<author confidence="0.999936">Mohit Bansal David Burkett Gerard de_Melo Dan Klein</author>
<affiliation confidence="0.995586">TTI Chicago Twitter Inc. Tsinghua University UC</affiliation>
<email confidence="0.992237">mbansal@ttic.edudburkett@twitter.comgdm@demelo.orgklein@cs.berkeley.edu</email>
<abstract confidence="0.99296268">We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation. Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood, captured by semantic features based on patterns and statistics Web and Wikipedia abstracts. For efficient inference over taxonomy structures, we use loopy belief propagation along with a directed spanning tree algorithm for the core hypernymy factor. To train the system, we extract sub-structures of WordNet and discriminatively learn to reproduce them, using adaptive subgradient stochastic optimization. On the task of reproducing sub-hierarchies of WordNet, our approach achieves a 51% error reduction over a chance baseline, including a 15% error reduction due to the non-hypernym-factored sibling features. On a comparison setup, we find up to 29% relative error reduction over previous work on ancestor F1.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Web-scale features for full-scale parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="29669" citStr="Bansal and Klein (2011)" startWordPosition="4919" endWordPosition="4922">us (Brants and Franz, 2006), a large collection of English n-grams (for n = 1 to 5) and their frequencies computed from almost 1 trillion tokens (95 billion sentences) of Web text. The Wikipedia abstracts are obtained via the publicly available dump, which contains almost —4.1 million articles.14 Preprocessing includes standard XML parsing and tokenization. Efficient collection of feature statistics is important because these must be extracted for millions of query pairs (for each potential edge and sibling pair in each term set). For this, we use a hash-trie on term pairs (similar to that of Bansal and Klein (2011)), and scan once through the n-gram (or abstract) set, skipping many n-grams (or abstracts) based on fast checks of missing unigrams, exceeding length, suffix mismatches, etc. 5.2 Evaluation Metric Ancestor F1: Measures the precision, recall, and F1 = 2PR/(P + R) of correctly predicted ances12This is somewhat different from our general setup where we work with any given set of terms; they start with a large set of leaves which have substantial Web-based relational information based on their selected, hand-picked patterns. Their data is available at http://www.isi.edu/˜kozareva/ downloads.html.</context>
</contexts>
<marker>Bansal, Klein, 2011</marker>
<rawString>Mohit Bansal and Dan Klein. 2011. Web-scale features for full-scale parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Brian Murphy</author>
<author>Eduard Barbu</author>
<author>Massimo Poesio</author>
</authors>
<title>Strudel: A corpus-based semantic model based on properties and types.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="23371" citStr="Baroni et al., 2010" startWordPosition="3904" endWordPosition="3907"> also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probabili</context>
</contexts>
<marker>Baroni, Murphy, Barbu, Poesio, 2010</marker>
<rawString>Marco Baroni, Brian Murphy, Eduard Barbu, and Massimo Poesio. 2010. Strudel: A corpus-based semantic model based on properties and types. Cognitive Science, 34(2):222–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>The Google Web 1T 5-gram corpus version 1.1.</title>
<date>2006</date>
<pages>2006--13</pages>
<contexts>
<context position="29073" citStr="Brants and Franz, 2006" startWordPosition="4821" endWordPosition="4824">he root (i.e., animal), resulting in —700 terms and —4,300 is-a ancestor-child links.12 Our training set for this animal test case was generated from WordNet using the following process: First, we strictly remove the full animal subtree from WordNet in order to avoid any possible overlap with the test data. Next, we create random 25-sized trees by picking random nodes as singleton trees, and repeatedly adding child edges from WordNet to the tree. This process gives us a total of —1600 training trees.13 Feature sources: The n-gram semantic features are extracted from the Google n-grams corpus (Brants and Franz, 2006), a large collection of English n-grams (for n = 1 to 5) and their frequencies computed from almost 1 trillion tokens (95 billion sentences) of Web text. The Wikipedia abstracts are obtained via the publicly available dump, which contains almost —4.1 million articles.14 Preprocessing includes standard XML parsing and tokenization. Efficient collection of feature statistics is important because these must be extracted for millions of query pairs (for each potential edge and sibling pair in each term set). For this, we use a hash-trie on term pairs (similar to that of Bansal and Klein (2011)), a</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. The Google Web 1T 5-gram corpus version 1.1. LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoeng-Jin Chu</author>
<author>Tseng-Hong Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Science Sinica,</journal>
<pages>14--1396</pages>
<contexts>
<context position="2724" citStr="Chu and Liu, 1965" startWordPosition="405" endWordPosition="408">g collections of terms as input and predicting a complete taxonomy structure over them as output. Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the taxonomy) and 2nd-order scoring factors on sibling edge pairs (pairs of hypernym edges with a shared parent), as well as incorporating a global (directed spanning tree) structural constraint. Inference for both learning and decoding uses structured loopy belief propagation (BP), incorporating standard spanning tree algorithms (Chu and Liu, 1965; Edmonds, 1967; Tutte, 1984). The belief propagation approach allows us to efficiently and effectively incorporate heterogeneous relational evidence via hypernymy and siblinghood (e.g., coordination) cues, which we capture by semantic features based on simple surface patterns and statistics from Web n-grams and Wikipedia abstracts. We train our model to maximize the likelihood of existing example ontologies using stochastic optimization, automatically learning the most useful relational patterns for full taxonomy induction. As an example of the relational patterns that our placental cow roden</context>
<context position="14377" citStr="Chu and Liu, 1965" startWordPosition="2385" endWordPosition="2388">chi et al., 2011), an adaptive subgradient variant of standard stochastic gradient ascent for online learning. 2.4 Decoding Finally, once the model parameters have been learned, we want to use the model to find taxonomy trees for particular sets of input terms. Note that if we limit our scores to be edge-factored, then finding the highest scoring taxonomy tree becomes an instance of the MST problem (also known as the maximum arborescence problem for the directed case), which can be solved efficiently in O(n2) quadratic time (Tarjan, 1977) using the greedy, recursive Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967).4 Since the MST problem can be solved efficiently, the main challenge becomes finding a way to ensure that our scores are edge-factored. In the first version of our model, we could simply set the score of each edge to be w·f(xZ, xj), and the MST recovered in this way would indeed be the highest scoring tree: arg max,P(y|x). However, this straightforward approach doesn’t apply to the full model which also uses sibling features. Hence, at decoding time, we instead start out by once more using belief propagation to find marginal beliefs, and then set the score of each edge to be </context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14(1396-1400):270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Steffen Staab</author>
</authors>
<title>Learning concept hierarchies from text with a guided agglomerative clustering algorithm.</title>
<date>2005</date>
<booktitle>In Proceedings of the ICML 2005 Workshop on Learning and Extending Lexical Ontologies with Machine Learning Methods.</booktitle>
<contexts>
<context position="24531" citStr="Cimiano and Staab, 2005" startWordPosition="4090" endWordPosition="4093">dd novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually anno</context>
</contexts>
<marker>Cimiano, Staab, 2005</marker>
<rawString>Philipp Cimiano and Steffen Staab. 2005. Learning concept hierarchies from text with a guided agglomerative clustering algorithm. In Proceedings of the ICML 2005 Workshop on Learning and Extending Lexical Ontologies with Machine Learning Methods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Andreas Hotho</author>
<author>Steffen Staab</author>
</authors>
<title>Learning concept hierarchies from text corpora using formal concept analysis.</title>
<date>2005</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="24506" citStr="Cimiano et al., 2005" startWordPosition="4086" endWordPosition="4089">. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise cl</context>
</contexts>
<marker>Cimiano, Hotho, Staab, 2005</marker>
<rawString>Philipp Cimiano, Andreas Hotho, and Steffen Staab. 2005. Learning concept hierarchies from text corpora using formal concept analysis. Journal of Artificial Intelligence Research, 24(1):305–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words. In</title>
<date>2006</date>
<booktitle>Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="22154" citStr="Davidov and Rappoport, 2006" startWordPosition="3706" endWordPosition="3709">ymmetric (in the sense that Sijk is redundant to Sikj) and hence the patterns are undirected. Therefore, for each term pair, we first symmetrize the collected Web n-grams and Wikipedia patterns by accumulating the counts of symmetric patterns like rats or squirrels and squirrels or rats.8 4 Related Work In our work, we assume a known term set and do not address the problem of extracting related terms from text. However, a great deal of past work has considered automating this process, typically taking one of two major approaches. The clustering-based approach (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Yamada et al., 2009) discovers relations based on the assumption that similar concepts appear in sim7One can also add features on the full triple (xi, xj, xk) but most such features will be sparse. 8All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could a</context>
</contexts>
<marker>Davidov, Rappoport, 2006</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2006. Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="13777" citStr="Duchi et al., 2011" startWordPosition="2287" endWordPosition="2290">th respect to the likelihood objective is computed by just taking the gold feature vector and subtracting the vector of expected feature counts. For computing expected counts, we run belief propagation until completion and then, for each factor in the model, we simply read off the marginal probability of that factor being active (as computed in Section 2.2), and accumulate a partial count for each feature that is fired by that factor. This method of computing the gradient can be incorporated into any gradient-based optimizer in order to learn the weights w. In our experiments we used AdaGrad (Duchi et al., 2011), an adaptive subgradient variant of standard stochastic gradient ascent for online learning. 2.4 Decoding Finally, once the model parameters have been learned, we want to use the model to find taxonomy trees for particular sets of input terms. Note that if we limit our scores to be edge-factored, then finding the highest scoring taxonomy tree becomes an instance of the MST problem (also known as the maximum arborescence problem for the directed case), which can be solved efficiently in O(n2) quadratic time (Tarjan, 1977) using the greedy, recursive Chu-Liu-Edmonds algorithm (Chu and Liu, 1965</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Edmonds</author>
</authors>
<title>Optimum branchings.</title>
<date>1967</date>
<journal>Journal of Research of the National Bureau of Standards B,</journal>
<pages>71--233</pages>
<contexts>
<context position="2739" citStr="Edmonds, 1967" startWordPosition="409" endWordPosition="410">rms as input and predicting a complete taxonomy structure over them as output. Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the taxonomy) and 2nd-order scoring factors on sibling edge pairs (pairs of hypernym edges with a shared parent), as well as incorporating a global (directed spanning tree) structural constraint. Inference for both learning and decoding uses structured loopy belief propagation (BP), incorporating standard spanning tree algorithms (Chu and Liu, 1965; Edmonds, 1967; Tutte, 1984). The belief propagation approach allows us to efficiently and effectively incorporate heterogeneous relational evidence via hypernymy and siblinghood (e.g., coordination) cues, which we capture by semantic features based on simple surface patterns and statistics from Web n-grams and Wikipedia abstracts. We train our model to maximize the likelihood of existing example ontologies using stochastic optimization, automatically learning the most useful relational patterns for full taxonomy induction. As an example of the relational patterns that our placental cow rodent squirrel rat </context>
<context position="14393" citStr="Edmonds, 1967" startWordPosition="2389" endWordPosition="2390">an adaptive subgradient variant of standard stochastic gradient ascent for online learning. 2.4 Decoding Finally, once the model parameters have been learned, we want to use the model to find taxonomy trees for particular sets of input terms. Note that if we limit our scores to be edge-factored, then finding the highest scoring taxonomy tree becomes an instance of the MST problem (also known as the maximum arborescence problem for the directed case), which can be solved efficiently in O(n2) quadratic time (Tarjan, 1977) using the greedy, recursive Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967).4 Since the MST problem can be solved efficiently, the main challenge becomes finding a way to ensure that our scores are edge-factored. In the first version of our model, we could simply set the score of each edge to be w·f(xZ, xj), and the MST recovered in this way would indeed be the highest scoring tree: arg max,P(y|x). However, this straightforward approach doesn’t apply to the full model which also uses sibling features. Hence, at decoding time, we instead start out by once more using belief propagation to find marginal beliefs, and then set the score of each edge to be its belief bYij </context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>Jack Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards B, 71:233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the Web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="23526" citStr="Etzioni et al., 2005" startWordPosition="3929" endWordPosition="3932">2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the Web: An experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Eric Brown</author>
<author>Jennifer Chu-Carroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya A Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
</authors>
<title>Eric Nyberg,</title>
<date>2010</date>
<journal>AI magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<location>John Prager, Nico</location>
<contexts>
<context position="1495" citStr="Ferrucci et al., 2010" startWordPosition="212" endWordPosition="215">t stochastic optimization. On the task of reproducing sub-hierarchies of WordNet, our approach achieves a 51% error reduction over a chance baseline, including a 15% error reduction due to the non-hypernym-factored sibling features. On a comparison setup, we find up to 29% relative error reduction over previous work on ancestor F1. 1 Introduction Many tasks in natural language understanding, such as question answering, information extraction, and textual entailment, benefit from lexical semantic information in the form of types and hypernyms. A recent example is IBM’s Jeopardy! system Watson (Ferrucci et al., 2010), which used type information to restrict the set of answer candidates. Information of this sort is present in term taxonomies (e.g., Figure 1), ontologies, and thesauri. However, currently available taxonomies such as WordNet are incomplete in coverage (Pennacchiotti and Pantel, 2006; Hovy et al., 2009), unavailable in many domains and languages, and vertebrate mammal reptile Figure 1: An excerpt of WordNet’s vertebrates taxonomy. time-intensive to create or extend manually. There has thus been considerable interest in building lexical taxonomies automatically. In this work, we focus on the t</context>
</contexts>
<marker>Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, 2010</marker>
<rawString>David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welty. 2010. Building watson: An overview of the DeepQA project. AI magazine, 31(3):59–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Fountain</author>
<author>Mirella Lapata</author>
</authors>
<title>Taxonomy induction using hierarchical random graphs.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="24584" citStr="Fountain and Lapata, 2012" startWordPosition="4098" endWordPosition="4101">nal probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patterns for Navigli and Velardi </context>
</contexts>
<marker>Fountain, Lapata, 2012</marker>
<rawString>Trevor Fountain and Mirella Lapata. 2012. Taxonomy induction using hierarchical random graphs. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonidas Georgiadis</author>
</authors>
<title>Arborescence optimization problems solvable by edmonds algorithm.</title>
<date>2003</date>
<journal>Theoretical Computer Science,</journal>
<volume>301</volume>
<issue>1</issue>
<contexts>
<context position="15198" citStr="Georgiadis (2003)" startWordPosition="2529" endWordPosition="2530"> set the score of each edge to be w·f(xZ, xj), and the MST recovered in this way would indeed be the highest scoring tree: arg max,P(y|x). However, this straightforward approach doesn’t apply to the full model which also uses sibling features. Hence, at decoding time, we instead start out by once more using belief propagation to find marginal beliefs, and then set the score of each edge to be its belief bYij (ON) 5 odds ratio: bYij (OFF). 3 Features While spanning trees are familiar from nonprojective dependency parsing, features based on the linear order of the words or on lexical identi4See Georgiadis (2003) for a detailed algorithmic proof, and McDonald et al. (2005) for an illustrative example. Also, we constrain the Chu-Liu-Edmonds MST algorithm to output only single-root MSTs, where the (dummy) root has exactly one child (Koo et al., 2007), because multi-root spanning ‘forests’ are not applicable to our task. Also, note that we currently assume one node per term. We are following the task description from previous work where the goal is to create a taxonomy for a specific domain (e.g., animals). Within a specific domain, terms typically just have a single sense. However, our algorithms could </context>
</contexts>
<marker>Georgiadis, 2003</marker>
<rawString>Leonidas Georgiadis. 2003. Arborescence optimization problems solvable by edmonds algorithm. Theoretical Computer Science, 301(1):427–437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of part-whole relations.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="23255" citStr="Girju et al., 2003" startWordPosition="3884" endWordPosition="3887">ring only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automatic discovery of part-whole relations. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="16036" citStr="Goodman, 1996" startWordPosition="2670" endWordPosition="2671">oo et al., 2007), because multi-root spanning ‘forests’ are not applicable to our task. Also, note that we currently assume one node per term. We are following the task description from previous work where the goal is to create a taxonomy for a specific domain (e.g., animals). Within a specific domain, terms typically just have a single sense. However, our algorithms could certainly be adapted to the case of multiple term senses (by treating the different senses as unique nodes in the tree) in future work. 5The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). 1044 ties or syntactic word classes, which are primary drivers for dependency parsing, are mostly uninformative for taxonomy induction. Instead, inducing taxonomies requires world knowledge to capture the semantic relations between various unseen terms. For this, we use semantic cues to hypernymy and siblinghood via features on simple surface patterns and statistics in large text corpora. We fire features on both the edge and the sibling factors. We first describe all the edge features in detail (Section 3.1 and Section 3.2), and th</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing algorithms and metrics. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="23106" citStr="Harris, 1954" startWordPosition="3866" endWordPosition="3867">the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early wo</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="18294" citStr="Hearst (1992)" startWordPosition="3045" endWordPosition="3046">the longest common substring of xi and xj, and create indicator features for rounded-off and binned values of |LCS|/((|xi |+ |xj|)/2). Length difference: We compute the signed length difference between xj and xi, and create indicator features for rounded-off and binned values of (|xj |− |xi|)/((|xi |+ |xj|)/2). Yang and Callan (2009) use a similar feature. 3.2 Semantic Features 3.2.1 Web n-gram Features Patterns and counts: Hypernymy for a term pair (P=xi, C=xj) is often signaled by the presence of surface patterns like C is a P, P such as C in large text corpora, an observation going back to Hearst (1992). For each potential parent-child edge (P=xi, C=xj), we mine the top k strings (based on count) in which both xi and xj occur (we use k=200). We collect patterns in both directions, which allows us to judge the correct direction of an edge (e.g., C is a P is a positive signal for hypernymy whereas P is a C is a negative signal).6 Next, for each pattern in this top-k list, we compute its normalized pattern count c, and fire an indicator feature on the tuple (pattern, t), for all thresholds t (in a fixed set) s.t. c &gt; t. Our supervised model then automatically learns which patterns are good indi</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
</authors>
<title>Toward completeness in concept extraction and classification.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1800" citStr="Hovy et al., 2009" startWordPosition="260" endWordPosition="263">ous work on ancestor F1. 1 Introduction Many tasks in natural language understanding, such as question answering, information extraction, and textual entailment, benefit from lexical semantic information in the form of types and hypernyms. A recent example is IBM’s Jeopardy! system Watson (Ferrucci et al., 2010), which used type information to restrict the set of answer candidates. Information of this sort is present in term taxonomies (e.g., Figure 1), ontologies, and thesauri. However, currently available taxonomies such as WordNet are incomplete in coverage (Pennacchiotti and Pantel, 2006; Hovy et al., 2009), unavailable in many domains and languages, and vertebrate mammal reptile Figure 1: An excerpt of WordNet’s vertebrates taxonomy. time-intensive to create or extend manually. There has thus been considerable interest in building lexical taxonomies automatically. In this work, we focus on the task of taking collections of terms as input and predicting a complete taxonomy structure over them as output. Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the taxonomy) and 2nd-orde</context>
<context position="23021" citStr="Hovy et al., 2009" startWordPosition="3852" endWordPosition="3855">pedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structu</context>
</contexts>
<marker>Hovy, Kozareva, Riloff, 2009</marker>
<rawString>Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff. 2009. Toward completeness in concept extraction and classification. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
</authors>
<title>Selectively using relations to improve precision in question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on NLP for Question Answering (EACL</booktitle>
<contexts>
<context position="23490" citStr="Katz and Lin, 2003" startWordPosition="3923" endWordPosition="3926"> heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pair</context>
</contexts>
<marker>Katz, Lin, 2003</marker>
<rawString>Boris Katz and Jimmy Lin. 2003. Selectively using relations to improve precision in question answering. In Proceedings of the Workshop on NLP for Question Answering (EACL 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Amir Globerson</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Structured prediction models via the matrix-tree theorem.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="15438" citStr="Koo et al., 2007" startWordPosition="2567" endWordPosition="2570">es. Hence, at decoding time, we instead start out by once more using belief propagation to find marginal beliefs, and then set the score of each edge to be its belief bYij (ON) 5 odds ratio: bYij (OFF). 3 Features While spanning trees are familiar from nonprojective dependency parsing, features based on the linear order of the words or on lexical identi4See Georgiadis (2003) for a detailed algorithmic proof, and McDonald et al. (2005) for an illustrative example. Also, we constrain the Chu-Liu-Edmonds MST algorithm to output only single-root MSTs, where the (dummy) root has exactly one child (Koo et al., 2007), because multi-root spanning ‘forests’ are not applicable to our task. Also, note that we currently assume one node per term. We are following the task description from previous work where the goal is to create a taxonomy for a specific domain (e.g., animals). Within a specific domain, terms typically just have a single sense. However, our algorithms could certainly be adapted to the case of multiple term senses (by treating the different senses as unique nodes in the tree) in future work. 5The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) f</context>
</contexts>
<marker>Koo, Globerson, Carreras, Collins, 2007</marker>
<rawString>Terry Koo, Amir Globerson, Xavier Carreras, and Michael Collins. 2007. Structured prediction models via the matrix-tree theorem. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard Hovy</author>
</authors>
<title>A semi-supervised method to learn and construct taxonomies using the Web. In</title>
<date>2010</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4444" citStr="Kozareva and Hovy (2010)" startWordPosition="664" endWordPosition="667">gnal of the link rodent —* rat. Moreover, sibling or coordination cues like either rats or squirrels suggest that rat is a sibling of squirrel and adds evidence for the links rodent —* rat and rodent —* squirrel. Our supervised model captures exactly these types of intuitions by automatically discovering such heterogeneous relational patterns as features (and learning their weights) on edges and on sibling edge pairs, respectively. There have been several previous studies on taxonomy induction. e.g., the incremental taxonomy induction system of Snow et al. (2006), the longest path approach of Kozareva and Hovy (2010), and the maximum spanning tree (MST) approach of Navigli et al. (2011) (see Section 4 for a more detailed overview). The main contribution of this work is that we present the first discriminatively trained, structured probabilistic model over the full space of taxonomy trees, using a structured inference procedure through both the learning and decoding phases. Our model is also the first to directly learn relational patterns as part of the process of training an end-to-end taxonomic induction system, rather than using patterns that were hand-selected or learned via pairwise classifiers on man</context>
<context position="24773" citStr="Kozareva and Hovy (2010)" startWordPosition="4127" endWordPosition="4130">e base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patterns for Navigli and Velardi (2010), Navigli et al. (2011)) to find intermediate terms and all the attested hypernymy links between them.10 To prune down the resulting tax9Determining the set of input terms is orthogon</context>
<context position="26996" citStr="Kozareva and Hovy (2010)" startWordPosition="4478" endWordPosition="4481"> training phase, instead of relying on hand-picked rules or pairwise classifiers on manually annotated co-occurrence patterns, and it is the first end-to-end (i.e., nonincremental) system to include heterogeneous relational information via sibling (e.g., coordination) patterns. 5 Experiments 5.1 Data and Experimental Regime We considered two distinct experimental setups, one that illustrates the general performance of our model by reproducing various medium-sized WordNet domains, and another that facilitates comparison to previous work by reproducing the much larger animal subtree provided by Kozareva and Hovy (2010). General setup: In order to test the accuracy of structured prediction on medium-sized fulldomain taxonomies, we extracted from WordNet 3.0 all bottomed-out full subtrees which had a tree-height of 3 (i.e., 4 nodes from root to leaf), and contained (10, 50] terms.11 This gives us 761 non-overlapping trees, which we partition into both these systems include term discovery in the taxonomy building process. 11Subtrees that had a smaller or larger tree height were discarded in order to avoid overlap between the training and test divisions. This makes it a much stricter setting than other tasks su</context>
<context position="28313" citStr="Kozareva and Hovy (2010)" startWordPosition="4693" endWordPosition="4696"> test sets. To project WordNet synsets to terms, we used the first (most frequent) term in each synset. A few WordNet synsets have multiple parents so we only keep the first of each such pair of overlapping trees. We also discard a few trees with duplicate terms because this is mostly due to the projection of different synsets to the same term, and theoretically makes the tree a graph. 70/15/15% (533/114/114 trees) train/dev/test sets. Comparison setup: We also compare our method (as closely as possible) with related previous work by testing on the much larger animal subtree made available by Kozareva and Hovy (2010), who created this dataset by selecting a set of ‘harvested’ terms and retrieving all the WordNet hypernyms between each input term and the root (i.e., animal), resulting in —700 terms and —4,300 is-a ancestor-child links.12 Our training set for this animal test case was generated from WordNet using the following process: First, we strictly remove the full animal subtree from WordNet in order to avoid any possible overlap with the test data. Next, we create random 25-sized trees by picking random nodes as singleton trees, and repeatedly adding child edges from WordNet to the tree. This process</context>
<context position="32217" citStr="Kozareva and Hovy (2010)" startWordPosition="5329" endWordPosition="5332">show results on the edges-only model with surface features (Section 3.1), semantic features (Section 3.2), and both. We see that both surface and semantic features make substantial contributions, and they also stack. Finally, we add the sibling factors and features (Figure 2b, Section 3.3), which further improves the results significantly (8% absolute and 15% relative error reduction over the edges-only results on the ancestor F1 metric). The last row shows the final test set results for the full model with all features. Table 2 shows our results for comparison to the larger animal dataset of Kozareva and Hovy (2010).15 In the table, ‘Kozareva2010’ refers to Kozareva and Hovy (2010) and ‘Navigli2011’ refers to Navigli et al. (2011).16 For appropri15These results are for the 1st order model due to the scale of the animal taxonomy (-700 terms). For scaling the 2nd order sibling model, one can use approximations, e.g., pruning the set of sibling factors based on 1st order link marginals, or a hierarchical coarse-to-fine approach based on taxonomy induction on subtrees, or a greedy approach of adding a few sibling factors at a time. This is future work. 16The Kozareva and Hovy (2010) ancestor results are obta</context>
<context position="33873" citStr="Kozareva and Hovy (2010)" startWordPosition="5602" endWordPosition="5605">son to each previous work, we show our results both for the ‘Fixed Prediction’ setup, which assumes the true root and leaves, and for the ‘Free Prediction’ setup, which doesn’t assume any prior information. The ** results of Navigli et al. (2011) represent a different ground-truth data condition, making them incomparable to our results; see Section 5.3 for details. ate comparison to each previous work, we show results for two different setups. The first setup ‘Fixed Prediction’ assumes that the model knows the true root and leaves of the taxonomy to provide for a somewhat fairer comparison to Kozareva and Hovy (2010). We get substantial improvements on ancestor-based recall and F1 (a 29% relative error reduction). The second setup ‘Free Prediction’ assumes no prior knowledge and predicts the full tree (similar to the general setup case). On this setup, we do compare as closely as possible to Navigli et al. (2011) and see a small gain in F1, but regardless, we should note that their results are incomparable (denoted by ** in Table 2) because they have a different ground-truth data condition: their definition and hypernym extraction phase involves using the Google define keyword, which often returns WordNet</context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard Hovy. 2010. A semi-supervised method to learn and construct taxonomies using the Web. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="23001" citStr="Kozareva et al., 2008" startWordPosition="3848" endWordPosition="3851">ts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Amarnag Subramanya</author>
<author>Fernando Pereira</author>
<author>William W Cohen</author>
</authors>
<title>Reading the web with learned syntactic-semantic inference rules.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="24131" citStr="Lao et al. (2012)" startWordPosition="4028" endWordPosition="4031">et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related </context>
</contexts>
<marker>Lao, Subramanya, Pereira, Cohen, 2012</marker>
<rawString>Ni Lao, Amarnag Subramanya, Fernando Pereira, and William W. Cohen. 2012. Reading the web with learned syntactic-semantic inference rules. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Concept discovery from text.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING. Dekang Lin.</booktitle>
<contexts>
<context position="22125" citStr="Lin and Pantel, 2002" startWordPosition="3702" endWordPosition="3705"> sibling factors are symmetric (in the sense that Sijk is redundant to Sikj) and hence the patterns are undirected. Therefore, for each term pair, we first symmetrize the collected Web n-grams and Wikipedia patterns by accumulating the counts of symmetric patterns like rats or squirrels and squirrels or rats.8 4 Related Work In our work, we assume a known term set and do not address the problem of extracting related terms from text. However, a great deal of past work has considered automating this process, typically taking one of two major approaches. The clustering-based approach (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Yamada et al., 2009) discovers relations based on the assumption that similar concepts appear in sim7One can also add features on the full triple (xi, xj, xk) but most such features will be sparse. 8All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experi</context>
</contexts>
<marker>Lin, Pantel, 2002</marker>
<rawString>Dekang Lin and Patrick Pantel. 2002. Concept discovery from text. In Proceedings of COLING. Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Learning word-class lattices for definition and hypernym extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="25190" citStr="Navigli and Velardi (2010)" startWordPosition="4196" endWordPosition="4200">in and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patterns for Navigli and Velardi (2010), Navigli et al. (2011)) to find intermediate terms and all the attested hypernymy links between them.10 To prune down the resulting tax9Determining the set of input terms is orthogonal to our work, and our method can be used in conjunction with various term extraction approaches described above. 10Unlike our system, which assumes a complete set of terms and only attempts to induce the taxonomic structure, 1046 onomy graph, Kozareva and Hovy (2010) use a procedure that iteratively retains the longest paths between root and leaf terms, removing conflicting graph edges as they go. The end result</context>
</contexts>
<marker>Navigli, Velardi, 2010</marker>
<rawString>Roberto Navigli and Paola Velardi. 2010. Learning word-class lattices for definition and hypernym extraction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
<author>Stefano Faralli</author>
</authors>
<title>A graph-based algorithm for inducing lexical taxonomies from scratch.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="4515" citStr="Navigli et al. (2011)" startWordPosition="676" endWordPosition="679">e either rats or squirrels suggest that rat is a sibling of squirrel and adds evidence for the links rodent —* rat and rodent —* squirrel. Our supervised model captures exactly these types of intuitions by automatically discovering such heterogeneous relational patterns as features (and learning their weights) on edges and on sibling edge pairs, respectively. There have been several previous studies on taxonomy induction. e.g., the incremental taxonomy induction system of Snow et al. (2006), the longest path approach of Kozareva and Hovy (2010), and the maximum spanning tree (MST) approach of Navigli et al. (2011) (see Section 4 for a more detailed overview). The main contribution of this work is that we present the first discriminatively trained, structured probabilistic model over the full space of taxonomy trees, using a structured inference procedure through both the learning and decoding phases. Our model is also the first to directly learn relational patterns as part of the process of training an end-to-end taxonomic induction system, rather than using patterns that were hand-selected or learned via pairwise classifiers on manually annotated co-occurrence patterns. Finally, it is the first end-to</context>
<context position="23076" citStr="Navigli et al., 2011" startWordPosition="3860" endWordPosition="3863">extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent </context>
<context position="24799" citStr="Navigli et al. (2011)" startWordPosition="4132" endWordPosition="4135">del to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patterns for Navigli and Velardi (2010), Navigli et al. (2011)) to find intermediate terms and all the attested hypernymy links between them.10 To prune down the resulting tax9Determining the set of input terms is orthogonal to our work, and our me</context>
<context position="32334" citStr="Navigli et al. (2011)" startWordPosition="5347" endWordPosition="5350">see that both surface and semantic features make substantial contributions, and they also stack. Finally, we add the sibling factors and features (Figure 2b, Section 3.3), which further improves the results significantly (8% absolute and 15% relative error reduction over the edges-only results on the ancestor F1 metric). The last row shows the final test set results for the full model with all features. Table 2 shows our results for comparison to the larger animal dataset of Kozareva and Hovy (2010).15 In the table, ‘Kozareva2010’ refers to Kozareva and Hovy (2010) and ‘Navigli2011’ refers to Navigli et al. (2011).16 For appropri15These results are for the 1st order model due to the scale of the animal taxonomy (-700 terms). For scaling the 2nd order sibling model, one can use approximations, e.g., pruning the set of sibling factors based on 1st order link marginals, or a hierarchical coarse-to-fine approach based on taxonomy induction on subtrees, or a greedy approach of adding a few sibling factors at a time. This is future work. 16The Kozareva and Hovy (2010) ancestor results are obtained by using the output files provided on their webpage. System P R F1 Previous Work Kozareva2010 98.6 36.2 52.9 Nav</context>
<context position="34175" citStr="Navigli et al. (2011)" startWordPosition="5652" endWordPosition="5655">them incomparable to our results; see Section 5.3 for details. ate comparison to each previous work, we show results for two different setups. The first setup ‘Fixed Prediction’ assumes that the model knows the true root and leaves of the taxonomy to provide for a somewhat fairer comparison to Kozareva and Hovy (2010). We get substantial improvements on ancestor-based recall and F1 (a 29% relative error reduction). The second setup ‘Free Prediction’ assumes no prior knowledge and predicts the full tree (similar to the general setup case). On this setup, we do compare as closely as possible to Navigli et al. (2011) and see a small gain in F1, but regardless, we should note that their results are incomparable (denoted by ** in Table 2) because they have a different ground-truth data condition: their definition and hypernym extraction phase involves using the Google define keyword, which often returns WordNet glosses itself. We note that previous work achieves higher ancestor precision, while our approach achieves a more even balance between precision and recall. Of course, precision and recall should both ideally be high, even if some applications weigh one over the other. This is why our tuning optimize</context>
</contexts>
<marker>Navigli, Velardi, Faralli, 2011</marker>
<rawString>Roberto Navigli, Paola Velardi, and Stefano Faralli. 2011. A graph-based algorithm for inducing lexical taxonomies from scratch. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="23287" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="3888" endWordPosition="3891"> fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors.</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
</authors>
<title>Acquisition of categorized named entities for web search.</title>
<date>2004</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<marker>Pas¸ca, 2004</marker>
<rawString>Marius Pas¸ca. 2004. Acquisition of categorized named entities for web search. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Patrick Pantel</author>
</authors>
<title>Ontologizing semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="1780" citStr="Pennacchiotti and Pantel, 2006" startWordPosition="255" endWordPosition="259">ative error reduction over previous work on ancestor F1. 1 Introduction Many tasks in natural language understanding, such as question answering, information extraction, and textual entailment, benefit from lexical semantic information in the form of types and hypernyms. A recent example is IBM’s Jeopardy! system Watson (Ferrucci et al., 2010), which used type information to restrict the set of answer candidates. Information of this sort is present in term taxonomies (e.g., Figure 1), ontologies, and thesauri. However, currently available taxonomies such as WordNet are incomplete in coverage (Pennacchiotti and Pantel, 2006; Hovy et al., 2009), unavailable in many domains and languages, and vertebrate mammal reptile Figure 1: An excerpt of WordNet’s vertebrates taxonomy. time-intensive to create or extend manually. There has thus been considerable interest in building lexical taxonomies automatically. In this work, we focus on the task of taking collections of terms as input and predicting a complete taxonomy structure over them as output. Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the ta</context>
</contexts>
<marker>Pennacchiotti, Pantel, 2006</marker>
<rawString>Marco Pennacchiotti and Patrick Pantel. 2006. Ontologizing semantic relations. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Phillips</author>
<author>Ellen Riloff</author>
</authors>
<title>Exploiting strong syntactic heuristics and co-training to learn semantic lexicons.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="22911" citStr="Phillips and Riloff, 2002" startWordPosition="3835" endWordPosition="3838"> the full triple (xi, xj, xk) but most such features will be sparse. 8All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzio</context>
</contexts>
<marker>Phillips, Riloff, 2002</marker>
<rawString>William Phillips and Ellen Riloff. 2002. Exploiting strong syntactic heuristics and co-training to learn semantic lexicons. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Taxonomy induction based on a collaboratively built knowledge repository.</title>
<date>2011</date>
<journal>Artificial Intelligence,</journal>
<volume>175</volume>
<issue>9</issue>
<contexts>
<context position="23399" citStr="Ponzetto and Strube, 2011" startWordPosition="3908" endWordPosition="3911">plementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational ev</context>
</contexts>
<marker>Ponzetto, Strube, 2011</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2011. Taxonomy induction based on a collaboratively built knowledge repository. Artificial Intelligence, 175(9):1737–1756.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised ontology induction from text.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="24556" citStr="Poon and Domingos, 2010" startWordPosition="4094" endWordPosition="4097">y maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patte</context>
</contexts>
<marker>Poon, Domingos, 2010</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2010. Unsupervised ontology induction from text. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Jessica Shepherd</author>
</authors>
<title>A corpusbased approach for building semantic lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="23470" citStr="Riloff and Shepherd, 1997" startWordPosition="3919" endWordPosition="3922">otstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster t</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>Ellen Riloff and Jessica Shepherd. 1997. A corpusbased approach for building semantic lexicons. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>What is this, anyway: Automatic hypernym discovery.</title>
<date>2009</date>
<booktitle>In Proceedings of AAAI Spring Symposium on Learning by Reading and Learning to Read.</booktitle>
<contexts>
<context position="19171" citStr="Ritter et al. (2009)" startWordPosition="3207" endWordPosition="3210">s a positive signal for hypernymy whereas P is a C is a negative signal).6 Next, for each pattern in this top-k list, we compute its normalized pattern count c, and fire an indicator feature on the tuple (pattern, t), for all thresholds t (in a fixed set) s.t. c &gt; t. Our supervised model then automatically learns which patterns are good indicators of hypernymy. Pattern order: We add features on the order (direction) in which the pair (xi, xj) found a pattern (in its top-k list) – indicator features for boolean values of the four cases: P ... C, C ... P, neither direction, and both directions. Ritter et al. (2009) used the ‘both’ case of this feature. Individual counts: We also compute the individual Web-scale term counts cxi and cxj, and add a comparison feature (cxi&gt;cxj), plus features on values of the signed count difference (|cxi |− |cxj|)/((|cxi |+ |cxj|)/2), after rounding off, and binning at multiple granularities. The intuition is that this feature could learn whether the relative popularity of the terms signals their hypernymy direction. 3.2.2 Wikipedia Abstract Features The Web n-grams corpus has broad coverage but is limited to up to 5-grams, so it may not contain pattern-based evidence for </context>
<context position="23331" citStr="Ritter et al., 2009" startWordPosition="3896" endWordPosition="3899">in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greed</context>
</contexts>
<marker>Ritter, Soderland, Etzioni, 2009</marker>
<rawString>Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009. What is this, anyway: Automatic hypernym discovery. In Proceedings of AAAI Spring Symposium on Learning by Reading and Learning to Read.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="10264" citStr="Smith and Eisner, 2008" startWordPosition="1701" endWordPosition="1704"> we wish to accomplish: computing expected feature counts and selecting a particular taxonomy tree for a given set of input terms (decoding). As an initial step to each of these procedures, we wish to compute the marginal probabilities of particular edges (and pairs of edges) being on. In a factor graph, the natural inference procedure for computing marginals is belief propagation. Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). Therefore, we will only briefly sketch the procedure here. Belief propagation is a general-purpose inference method that computes marginals via directed messages passed from variables to adjacent factors (and vice versa) in the factor graph. These messages take the form of (possibly unnormalized) distributions over values of the variable. The two types of messages (variable to factor or factor to variable) have mutually recursive definitions. The message from a factor F to an adjacent variable V involves a sum over all possible values of every other variable that F touches. While the EDGE an</context>
<context position="16096" citStr="Smith and Eisner, 2008" startWordPosition="2678" endWordPosition="2681">sts’ are not applicable to our task. Also, note that we currently assume one node per term. We are following the task description from previous work where the goal is to create a taxonomy for a specific domain (e.g., animals). Within a specific domain, terms typically just have a single sense. However, our algorithms could certainly be adapted to the case of multiple term senses (by treating the different senses as unique nodes in the tree) in future work. 5The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). 1044 ties or syntactic word classes, which are primary drivers for dependency parsing, are mostly uninformative for taxonomy induction. Instead, inducing taxonomies requires world knowledge to capture the semantic relations between various unseen terms. For this, we use semantic cues to hypernymy and siblinghood via features on simple surface patterns and statistics in large text corpora. We fire features on both the edge and the sibling factors. We first describe all the edge features in detail (Section 3.1 and Section 3.2), and then briefly describe the sibling features (Section 3.3), whic</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David A. Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="4389" citStr="Snow et al. (2006)" startWordPosition="655" endWordPosition="658">e rat is a rodent in large corpora is a strong signal of the link rodent —* rat. Moreover, sibling or coordination cues like either rats or squirrels suggest that rat is a sibling of squirrel and adds evidence for the links rodent —* rat and rodent —* squirrel. Our supervised model captures exactly these types of intuitions by automatically discovering such heterogeneous relational patterns as features (and learning their weights) on edges and on sibling edge pairs, respectively. There have been several previous studies on taxonomy induction. e.g., the incremental taxonomy induction system of Snow et al. (2006), the longest path approach of Kozareva and Hovy (2010), and the maximum spanning tree (MST) approach of Navigli et al. (2011) (see Section 4 for a more detailed overview). The main contribution of this work is that we present the first discriminatively trained, structured probabilistic model over the full space of taxonomy trees, using a structured inference procedure through both the learning and decoding phases. Our model is also the first to directly learn relational patterns as part of the process of training an end-to-end taxonomic induction system, rather than using patterns that were h</context>
<context position="22952" citStr="Snow et al., 2006" startWordPosition="3841" endWordPosition="3844">res will be sparse. 8All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). </context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="23310" citStr="Suchanek et al., 2007" startWordPosition="3892" endWordPosition="3895">e training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Joseph Reisinger</author>
<author>Marius Pas¸ca</author>
<author>Deepak Ravichandran</author>
<author>Rahul Bhagat</author>
<author>Fernando Pereira</author>
</authors>
<title>Weakly-supervised acquisition of labeled class instances using graph random walks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Talukdar, Reisinger, Pas¸ca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>Partha Pratim Talukdar, Joseph Reisinger, Marius Pas¸ca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira. 2008. Weakly-supervised acquisition of labeled class instances using graph random walks. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Tarjan</author>
</authors>
<title>Finding optimum branchings.</title>
<date>1977</date>
<journal>Networks,</journal>
<pages>7--25</pages>
<contexts>
<context position="14304" citStr="Tarjan, 1977" startWordPosition="2375" endWordPosition="2376"> order to learn the weights w. In our experiments we used AdaGrad (Duchi et al., 2011), an adaptive subgradient variant of standard stochastic gradient ascent for online learning. 2.4 Decoding Finally, once the model parameters have been learned, we want to use the model to find taxonomy trees for particular sets of input terms. Note that if we limit our scores to be edge-factored, then finding the highest scoring taxonomy tree becomes an instance of the MST problem (also known as the maximum arborescence problem for the directed case), which can be solved efficiently in O(n2) quadratic time (Tarjan, 1977) using the greedy, recursive Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967).4 Since the MST problem can be solved efficiently, the main challenge becomes finding a way to ensure that our scores are edge-factored. In the first version of our model, we could simply set the score of each edge to be w·f(xZ, xj), and the MST recovered in this way would indeed be the highest scoring tree: arg max,P(y|x). However, this straightforward approach doesn’t apply to the full model which also uses sibling features. Hence, at decoding time, we instead start out by once more using belief propaga</context>
</contexts>
<marker>Tarjan, 1977</marker>
<rawString>Robert E. Tarjan. 1977. Finding optimum branchings. Networks, 7:25–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William T Tutte</author>
</authors>
<title>Graph theory.</title>
<date>1984</date>
<publisher>AddisonWesley.</publisher>
<contexts>
<context position="2753" citStr="Tutte, 1984" startWordPosition="411" endWordPosition="412">d predicting a complete taxonomy structure over them as output. Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the taxonomy) and 2nd-order scoring factors on sibling edge pairs (pairs of hypernym edges with a shared parent), as well as incorporating a global (directed spanning tree) structural constraint. Inference for both learning and decoding uses structured loopy belief propagation (BP), incorporating standard spanning tree algorithms (Chu and Liu, 1965; Edmonds, 1967; Tutte, 1984). The belief propagation approach allows us to efficiently and effectively incorporate heterogeneous relational evidence via hypernymy and siblinghood (e.g., coordination) cues, which we capture by semantic features based on simple surface patterns and statistics from Web n-grams and Wikipedia abstracts. We train our model to maximize the likelihood of existing example ontologies using stochastic optimization, automatically learning the most useful relational patterns for full taxonomy induction. As an example of the relational patterns that our placental cow rodent squirrel rat metatherian ma</context>
<context position="11417" citStr="Tutte, 1984" startWordPosition="1901" endWordPosition="1902">s of every other variable that F touches. While the EDGE and SIBLING factors are simple enough to compute this sum by brute force, performing the sum naively for computing messages from the TREE factor would take exponential time. HowOT (y) = Model. For a given global assignment y, let � 1 y forms a legal taxonomy tree 0 otherwise E f(y) = i,j yij=ON Ef(xi, xj) + f(xi, xj, xk) i,j,k yij=yik=ON 1043 ever, due to the structure of that particular factor, all of its outgoing messages can be computed simultaneously in O(n3) time via an efficient adaptation of Kirchhoff’s Matrix Tree Theorem (MTT) (Tutte, 1984) which computes partition functions and marginals for directed spanning trees. Once message passing is completed, marginal beliefs are computed by merely multiplying together all the messages received by a particular variable or factor. 2.2.1 Loopy Belief Propagation Looking closely at Figure 2a, one can observe that the factor graph for the first version of our model, containing only EDGE and TREE factors, is acyclic. In this special case, belief propagation is exact: after one round of message passing, the beliefs computed (as discussed in Section 2.2) will be the true marginal probabilities</context>
</contexts>
<marker>Tutte, 1984</marker>
<rawString>William T. Tutte. 1984. Graph theory. AddisonWesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Unsupervised methods for developing taxonomies by combining syntactic and statistical information.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="23807" citStr="Widdows (2003)" startWordPosition="3977" endWordPosition="3978"> relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task o</context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>Dominic Widdows. 2003. Unsupervised methods for developing taxonomies by combining syntactic and statistical information. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ichiro Yamada</author>
<author>Kentaro Torisawa</author>
<author>Jun’ichi Kazama</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
<author>Stijn De Saeger</author>
<author>Francis Bond</author>
<author>Asuka Sumida</author>
</authors>
<title>Hypernym discovery based on distributional similarity and hierarchical structures.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Yamada, Torisawa, Kazama, Kuroda, Murata, De Saeger, Bond, Sumida, 2009</marker>
<rawString>Ichiro Yamada, Kentaro Torisawa, Jun’ichi Kazama, Kow Kuroda, Masaki Murata, Stijn De Saeger, Francis Bond, and Asuka Sumida. 2009. Hypernym discovery based on distributional similarity and hierarchical structures. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yang</author>
<author>Jamie Callan</author>
</authors>
<title>A metric-based framework for automatic taxonomy induction.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="18016" citStr="Yang and Callan (2009)" startWordPosition="2993" endWordPosition="2996">nds with xi, or not. This captures pairs such as (fish, bony fish) in our data. Contains: Checks if xj contains xi, or not. This captures pairs such as (bird, bird ofprey). Suffix match: Checks whether the k-length suffixes of xi and xj match, or not, for k = 1,2,...,7. LCS: We compute the longest common substring of xi and xj, and create indicator features for rounded-off and binned values of |LCS|/((|xi |+ |xj|)/2). Length difference: We compute the signed length difference between xj and xi, and create indicator features for rounded-off and binned values of (|xj |− |xi|)/((|xi |+ |xj|)/2). Yang and Callan (2009) use a similar feature. 3.2 Semantic Features 3.2.1 Web n-gram Features Patterns and counts: Hypernymy for a term pair (P=xi, C=xj) is often signaled by the presence of surface patterns like C is a P, P such as C in large text corpora, an observation going back to Hearst (1992). For each potential parent-child edge (P=xi, C=xj), we mine the top k strings (based on count) in which both xi and xj occur (we use k=200). We collect patterns in both directions, which allows us to judge the correct direction of an edge (e.g., C is a P is a positive signal for hypernymy whereas P is a C is a negative </context>
<context position="24046" citStr="Yang and Callan (2009)" startWordPosition="4015" endWordPosition="4018">classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct tax</context>
</contexts>
<marker>Yang, Callan, 2009</marker>
<rawString>Hui Yang and Jamie Callan. 2009. A metric-based framework for automatic taxonomy induction. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>