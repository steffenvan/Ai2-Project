<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008572">
<title confidence="0.987434">
V3: Unsupervised Aspect Based Sentiment Analysis
for SemEval-2015 Task 12
</title>
<author confidence="0.734135">
Aitor Garcia-Pablos, Montse Cuadros
</author>
<address confidence="0.830437666666667">
Vicomtech-IK4 research center
Mikeletegi 57
San Sebastian, Spain
</address>
<email confidence="0.999218">
{agarciap,mcuadros}@vicomtech.org
</email>
<author confidence="0.903895">
German Rigau
</author>
<affiliation confidence="0.8261655">
IXA Group
Euskal Herriko Unibertsitatea,
</affiliation>
<address confidence="0.976871">
San Sebastian, Spain
</address>
<email confidence="0.998801">
german.rigau@ehu.es
</email>
<sectionHeader confidence="0.998591" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999381454545454">
This paper presents our participation in
SemEval-2015 task 12 (Aspect Based Senti-
ment Analysis). We participated employing
only unsupervised or weakly-supervised ap-
proaches. Our attempt is based on requiring
the minimum annotated or hand-crafted con-
tent, and avoids training a model using the
provided training set. We use a continuous
word representations (Word2Vec) to leverage
in-domain semantic similarities of words for
many of the involved subtasks.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998104944444445">
The continuous growing of textual content on the In-
ternet has motivated an important research on find-
ing automatic ways of processing and exploiting this
valuable source of information. That is one of the
reasons why sentiment analysis has become a very
active research field during the last decade (Pang and
Lee, 2008; Liu et al., 2012; Zhang and Liu, 2014).
Sentiment analysis aims to detect and classify the
polarity of sentiments expressed in a text. The gran-
ularity of this classification goes from the overall
polarity of full documents to paragraphs, sentences
or, as in Aspect Based Sentiment Analysis (ABSA),
the sentiment about precise aspects being opinion-
ated (Hu and Liu, 2004) (Popescu and Etzioni, 2005)
(Wu et al., 2009) (Zhang et al. , 2010).
In this paper we describe our participation in
SemEval-2015 task 121 (Pontiki et al., 2015), which
is about ABSA. We have participated in all subtasks
</bodyText>
<footnote confidence="0.930566">
1http://alt.qcri.org/semeval2015/task12/
</footnote>
<bodyText confidence="0.999183">
employing unsupervised or weakly supervised ap-
proaches.
The rest of the paper is structured as follows. Sec-
tion 2 introduces the SemEval-2015 task 12 compe-
tition and provided datasets, and a brief introduction
about how we have approached the different slots.
Sections 3, 4 and 5 describe more in detail the em-
ployed techniques. Section 6 shows the results of
the evaluation, and finally section 7 summarizes the
conclusions.
</bodyText>
<sectionHeader confidence="0.904652" genericHeader="method">
2 Our approach
</sectionHeader>
<bodyText confidence="0.999951636363637">
SemEval2015 task 12 was about ABSA. The task
was divided into 3 slots. Slot 1 was about classify-
ing review sentences into entity-attribute pairs, be-
ing the entity a main aspect of the reviewed item
(e.g. food, drinks, location) and the attribute a par-
ticular facet of that aspect (e.g. food-quality, food-
price, etc.). Slot 1 runs on two domains, restaurants
and laptops. Slot 2 was about detecting explicit men-
tions to aspect-terms that are being reviewed (e.g.
service in ”The service was attentive”). Slot 2 runs
only on restaurants domain. Slot 3 was about detect-
ing the polarity/sentiment for the given gold entity-
attribute pairs in sentences (see slot 1). Slot 3 was
meant for restaurants and laptops domain, plus an
additional hidden domain (i.e. revealed in the last
moment and with no training data available) which
resulted to be about hotels.
Two training datasets were provided. The first
dataset contains 254 annotated reviews about restau-
rants (a total of 1315 sentences). The second dataset
contains 277 annotated reviews about laptops (a to-
tal of 1,739 sentences). The annotation consists of
</bodyText>
<page confidence="0.972172">
714
</page>
<bodyText confidence="0.948761695652174">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 714–718,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
quintuples of aspect-term, entity-attribute, polarity,
and starting and ending position of the aspect-term.
When there is no explicit aspect-term mentioned
”null” is employed to fill the gap. Only the restau-
rants dataset contains aspect-term annotation.
Our aim is to apply only unsupervised or mini-
mally supervised techniques. We have applied dif-
ferent unsupervised approaches to the different slot
tasks avoiding the use of the provided datasets to
train a supervised system. We have used them only
to evaluate and tune the performance of the em-
ployed techniques. For some of the employed tech-
niques we have also used big unlabeled datasets.
In particular, for the domain of restaurants we
have employed a subset of 100k restaurant reviews
from Yelp dataset2. We name this corpus as Yelp-
restaurants. For laptops domain we have used a sub-
set about 100k reviews from a big dataset of Amazon
electronic device reviews 3 (retaining only the ones
that contain the word laptop). We name this corpus
as Amazon-laptops.
</bodyText>
<sectionHeader confidence="0.947367" genericHeader="method">
3 Aspect term extraction
</sectionHeader>
<bodyText confidence="0.999642142857143">
SemEval2015 Task 12 slot 2 was about detect-
ing mentions to explicit aspect terms, but only for
restaurant domain (i.e. other slots run for restaurants
and laptop domains).
For aspect term extraction our aim is to bootstrap
a list of candidate domain aspect terms and use it to
annotate the reviews of the same domain. We have
implemented a system inspired in the method de-
scribed at (Liu et al., 2014). In this work the authors
employ what they call a graph co-ranking approach.
They model aspect-terms (AT) and opinion-words
(OW) as graph nodes, and then they generate three
different sub-graphs defining different types of rela-
tions (what they call semantic-relations and opinion-
relations) between the nodes. Finally they rank the
nodes using a combined random walk on the three
sub-graphs to obtain a list of reliable aspect-term
candidates. Due to space limitations we cannot ex-
plain all the details here. Please, refer to the original
article for more in detail explanation.
Based on some of these ideas we have imple-
</bodyText>
<footnote confidence="0.970228">
2http://www.yelp.com/dataset_challenge
3http://snap.stanford.edu/data/
web-Amazon.html
</footnote>
<bodyText confidence="0.999272666666667">
mented a system that aims to rank aspect-terms
modeling them as a graph. From our datasets
(i.e. Yelp-restaurants and Amazon-laptops) we have
taken nouns as aspect term candidates, and adjec-
tives as opinion word candidates, filtering out those
words that appear less than 5 times. These are the
nodes to build our graph. Then we have computed
our own definition of semantic relations and opinion
relations to build sub-graphs as follows:
</bodyText>
<listItem confidence="0.999870666666667">
• Opinion relations (AT-OW edges): we have
computed how many times each AT has some
syntactical dependency relation with each OW,
from a certain set of dependency relations (i.e.
direct object, adjectival modifier, attribute of a
copulative verb). The result of this count is
used as the weight of the edges between AT and
OW nodes.
• Semantic relations (AT-AT and OW-OW
</listItem>
<bodyText confidence="0.951394391304348">
edges): we have computed a continuous
word representation of the datasets employing
Word2Vec4 (Mikolov et al., 2013) (with the
following parameters: skip-grams, vector size
of 200, context window of 5, hierarchical
softmax). Then we have used the cosine
similarity between word vectors as the weight
of the semantic relation edges.
Once we have built the graph with the different
type of nodes and different type of weighted edges,
we execute a PageRank (Brin and Page, 1998) (al-
pha parameter set to 0.15) to score and rank the
nodes. With the obtained score we generate an or-
dered list of aspect terms. We have done this only for
restaurants since it was the only domain requested
in the task 12 slot 2. Example of some of the higher
scored words for restaurant domain are: food, ser-
vice, place, restaurant, portion, atmosphere, experi-
ence, dish, meal, burger.
The obtained aspect term list is then cropped to
retain only the top N ranked words, and this cropped
word list is used to annotate the given sentences per-
forming a simple lemma matching.
</bodyText>
<footnote confidence="0.999893">
4We have employed the implementation in Apache Spark
MLlib library https://spark.apache.org/mllib/
</footnote>
<page confidence="0.990747">
715
</page>
<subsectionHeader confidence="0.997355">
3.1 Multiword handling
</subsectionHeader>
<bodyText confidence="0.966344666666667">
Handling multiword terms is important in an ABSA
system (e.g. it is not the same to detect just mem-
ory than flash memory and/or RAM memory, etc.).
Multiword terms affect also to some opinion expres-
sions like top notch. Finally, multiword terms arise
from usual collocations of single terms, so they vary
between domains.
In order to bootstrap a list of candidate multiword
terms for each given domain, we have employed
again our own Yelp-restaurants and Amazon-laptops
datasets. We have computed Log-Likelihood Ratio
(LLR) of n-grams (with n¡=3) to detect the more
salient word collocations keeping the top K candi-
dates (i.e. the ones with higher confidence of being
a true multiword).
Examples of obtained multiwords for restaurants:
happy hour, onion ring, ice cream, spring roll, live
music.
Examples of obtained multiwords for laptops:
tech support, power supply, customer service, op-
erating system, battery life.
We have used this list in a pre-processing step
to merge individual words into a single token when
they match a multiword in the list.
</bodyText>
<sectionHeader confidence="0.999246" genericHeader="method">
4 Entity-attribute detection
</sectionHeader>
<bodyText confidence="0.9997787">
The definition of entity-attribute detection in slot 1
states the difference between entities (coarse grained
aspects that are being reviewed, e.g. food, drinks)
and attributes (particular facet that is being actually
reviewed, e.g. price, quality). This subtask runs
both for restaurant and laptop domain. Due to the
big amount of possible combinations and the con-
sequent overlapping of some of them, this subtask
becomes very difficult for an unsupervised system.
In order to employ a weakly-supervised approach
we have faced this subtask defining some represen-
tative seed words for each possible entity (e.g. food:
chicken, salad, rice) and attribute (e.g. price: expen-
sive, cheap). Then we reused the Word2Vec model
for each domain to compute the similarity between
sentence words and the seed words. When the ac-
cumulated similarity with some entity and attribute
seed words is salient enough, we annotate the sen-
tence with that entity-attribute pair. If the similarity
is low, or is equally distributed among a every can-
</bodyText>
<table confidence="0.954469428571429">
Word Polarity Score Polarity label
delicious 0.424 positive
tasty 0.439 positive
inexpensive 0.341 positive
slow -0.182 negative
arrogant -0.254 negative
mediocre -0.051 negative
</table>
<tableCaption confidence="0.989002">
Table 1: Examples of polarity values obtained from the
restaurants polarity lexicon.
</tableCaption>
<bodyText confidence="0.8873">
didate entity, we leave the sentence unlabeled.
</bodyText>
<sectionHeader confidence="0.980342" genericHeader="method">
5 Polarity detection
</sectionHeader>
<bodyText confidence="0.997237176470588">
For polarity detection we have developed a polarity
lexicon reusing the generated Word2Vec model for
each domain. The intuition we have followed is that
a polarity word in a domain should be more ”simi-
lar” to a set of ”very positive” words than to a set of
”very negative” words, and vice versa.
We have employed the in-domain generated
Word2Vec models because the polarity of words
may vary between domains and we want to capture
the polarity for each particular domain.
Let POS be a domain-independent positive word
(e.g. excellent) and NEG a domain-independent
negative word (e.g. horrible). Let W be the set of
words we want to know the polarity. Let sim be
the similarity between words (computed using the
Word2Vec model for the domain). Then for each
w ∈ W we calculate its polarity using (1).
</bodyText>
<equation confidence="0.9353145">
polarity(w) = sim(w, POS) − sim(w, NEG)
(1)
</equation>
<bodyText confidence="0.999103571428572">
We obtain polarity(w) &gt; 0 if the word is more sim-
ilar to POS than to NEG and vice versa. This gives
us a continuous value from very positive to very neg-
ative, but we have simplified it to a binary labeling:
”positive” for any word w with polarity(w) &gt;= 0
and ”negative” if polarity(w) &lt; 0.
In the table 1 we can see some examples of words,
their punctuation in the positive-negative axis, and
the assigned polarity label.
With these sentiment lexicons for each of the do-
mains we have performed the annotation of the sen-
tences. We have faced the annotation as a simple po-
larity count process. For each sentence we counted
the polarity of the words regarding our in-domain
</bodyText>
<page confidence="0.995273">
716
</page>
<table confidence="0.9994604">
Slot 2 systems Restaurants F-score
Baseline 0.48
V3 (ours) 0.45
Best 0.70
Average 0.52
</table>
<tableCaption confidence="0.999739">
Table 2: Results on the restaurant reviews for slot 2.
</tableCaption>
<bodyText confidence="0.999681">
lexicons and labeled the provided gold quintuples
with the most frequent polarity. We have taken into
account the negation words (e.g. not) present in the
sentence in order to reverse the polarity of the words
within a certain window (one token before and two
tokens after the current word).
</bodyText>
<sectionHeader confidence="0.995021" genericHeader="evaluation">
6 Experiments and results
</sectionHeader>
<bodyText confidence="0.999955766666666">
We have participated in SemEval-2015 Task 12 slot
1 (entity-attribute detection), slot 2 (aspect-term de-
tection) and slot 3 (polarity detection). In general the
task definition is more challenging than in SemEval-
2014 ABSA competition5 as the average results of
all participants indicate. The participation number
is also lower and varies between of subtasks and
domains (15 participants for restaurants slot 1, 9
for laptops slot 1, 21 for restaurants slot 2, and an
average of 14 for slot 3 in the three available do-
mains). As far as we know, we are the only team
that has faced the competition using unsupervised
approaches. As expected, the supervised systems
obtain better results in general than our unsupervised
one.
Slot 2 (detecting explicit aspect terms) was only
available for restaurants. After performing the steps
described in section 3, we employed the top 500
bootstrapped terms to annotate the provided set of
reviews using a simple lemma matching. The re-
sults are shown in table 2, together with the official
results of the supervised baseline, the best perform-
ing system, and the average of all participants.
Slot 1 (detecting entity-attribute pairs in sen-
tences) was available both for restaurants and lap-
tops. We employed the described manual bag of
words plus Word2Vec approach. The results are
quite modest as it can be appreciated in table 3.
Slot 3 (polarity annotation) was available both for
restaurants and laptops, plus and additional hidden
</bodyText>
<footnote confidence="0.965356">
5http://alt.qcri.org/semeval2014/task4/
</footnote>
<table confidence="0.9991856">
Slot 1 systems Restaur. F-score Laptops F-score
Baseline 0.51 0.46
V3 (ours) 0.41 0.25
Best 0.62 0.50
Average 0.53 0.45
</table>
<tableCaption confidence="0.9055575">
Table 3: Results on the restaurant and laptops reviews for
entity-attribute detection (SemEval-2015 task 12 slot 1).
</tableCaption>
<table confidence="0.9996992">
Slot 3 accuracy Restaurants Laptops Hotels
Baseline 0.635 0.699 0.716
V3 (ours) 0.694 0.683 0.710
Best 0.786 0.793 0.805
Average 0.713 0.713 0.712
</table>
<tableCaption confidence="0.9982325">
Table 4: Results on the restaurant, laptops and hotels for
slot 3.
</tableCaption>
<bodyText confidence="0.999605166666667">
domain. This hidden domain, which was about ho-
tels, was revealed in the last moment and no training
data was provided. For this hidden domain we had
no time to develop its own sentiment lexicon so we
employed the one from restaurants domain. The re-
sults for all domains are shown in table 4.
</bodyText>
<sectionHeader confidence="0.99934" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9995044375">
In this paper we have described our participa-
tion in SemEval-2015 task 12 (ABSA). We have
approached all subtasks from an unsupervised or
weakly-supervised point of view. To our opinion
this year the tasks were more challenging than in the
previous SemEval ABSA edition. We have explored
different ways of approaching the challenges with-
out requiring a manually labeled train set. We have
made an intensive use of continuous word represen-
tations (e.g. Word2Vec) to exploit semantic simi-
larities between words and despite the low results
we have found some promising ideas. In the future
we will explore how to improve the developed sys-
tems and how to combine with other unsupervised
or semi-supervised techniques to achieve competi-
tive results.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.578008">
This work has been partially funded by SKaTer6
(TIN2012-38584-C06-02), NewsReader7 (ICT-
316404) and Vicomtech-IK4.
</reference>
<footnote confidence="0.9989595">
6http://nlp.lsi.upc.edu/skater/
7http://www.newsreader-project.eu
</footnote>
<page confidence="0.995497">
717
</page>
<sectionHeader confidence="0.998304" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9983565">
Brin, Sergey and Page, Lawrence 1998. The anatomy
of a large-scale hypertextual Web search engine Com-
puter networks and ISDN systems
Hu, Minqing and Liu, Bing 2004. Mining opinion fea-
tures in customer reviews AAAI
Bo Pang and Lillian Lee 2008. Opinion mining and sen-
timent analysis Foundations and trends in information
retrieval,
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean 2013. Efficient Estimation of Word Represen-
tations in Vector Space Proceedings of Workshop at
ICLR
Liu, Kang and Xu, Liheng and Zhao, Jun 2014. Extract-
ing Opinion Targets and Opinion Words from Online
Reviews with Graph Co-ranking Proceedings of the
52nd Annual Meeting of the Association for Computa-
tional Linguistics
Bing Liu 2012. Sentiment analysis and opinion mining
Synthesis Lectures on Human Language Technologies
Maria Pontiki, Dimitrios Galanis, Haris Papageogiou,
Suresh Manandhar, and Ion Androutsopoulos 2015.
SemEval-2015 Task 12: Aspect Based Sentiment
Analysis Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval 2015), Denver,
Colorado
Popescu, AM and Etzioni, Oren 2005. Extracting prod-
uct features and opinions from reviews Natural lan-
guage processing and text mining
Wu, Yuanbin and Zhang, Qi and Huang, Xuanjing and
Wu, Lide 2009. Phrase dependency parsing for opin-
ion mining Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 3
Zhang, L and Liu, Bing and Lim, SH and O’Brien-Strain,
E 2010. Extracting and ranking product features in
opinion documents Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics
Zhang, Lei and Liu, Bing 2014. Aspect and Entity Ex-
traction for Opinion Mining Data Mining and Knowl-
edge Discovery for Big Data
</reference>
<page confidence="0.994307">
718
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.066972">
<note confidence="0.88876525">V3: Unsupervised Aspect Based Sentiment for SemEval-2015 Task 12 Aitor Garcia-Pablos, Montse Vicomtech-IK4 research</note>
<author confidence="0.430437">Mikeletegi</author>
<affiliation confidence="0.817374">San Sebastian,</affiliation>
<address confidence="0.469796">German</address>
<affiliation confidence="0.723329">IXA Euskal Herriko San Sebastian,</affiliation>
<email confidence="0.995392">german.rigau@ehu.es</email>
<abstract confidence="0.999423166666667">This paper presents our participation in SemEval-2015 task 12 (Aspect Based Sentiment Analysis). We participated employing only unsupervised or weakly-supervised approaches. Our attempt is based on requiring the minimum annotated or hand-crafted content, and avoids training a model using the provided training set. We use a continuous word representations (Word2Vec) to leverage in-domain semantic similarities of words for many of the involved subtasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This work has been partially funded by</title>
<booktitle>SKaTer6 (TIN2012-38584-C06-02), NewsReader7 (ICT316404) and Vicomtech-IK4.</booktitle>
<marker></marker>
<rawString>This work has been partially funded by SKaTer6 (TIN2012-38584-C06-02), NewsReader7 (ICT316404) and Vicomtech-IK4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual Web search engine Computer networks and ISDN systems</title>
<date>1998</date>
<contexts>
<context position="6903" citStr="Brin and Page, 1998" startWordPosition="1080" endWordPosition="1083">f a copulative verb). The result of this count is used as the weight of the edges between AT and OW nodes. • Semantic relations (AT-AT and OW-OW edges): we have computed a continuous word representation of the datasets employing Word2Vec4 (Mikolov et al., 2013) (with the following parameters: skip-grams, vector size of 200, context window of 5, hierarchical softmax). Then we have used the cosine similarity between word vectors as the weight of the semantic relation edges. Once we have built the graph with the different type of nodes and different type of weighted edges, we execute a PageRank (Brin and Page, 1998) (alpha parameter set to 0.15) to score and rank the nodes. With the obtained score we generate an ordered list of aspect terms. We have done this only for restaurants since it was the only domain requested in the task 12 slot 2. Example of some of the higher scored words for restaurant domain are: food, service, place, restaurant, portion, atmosphere, experience, dish, meal, burger. The obtained aspect term list is then cropped to retain only the top N ranked words, and this cropped word list is used to annotate the given sentences performing a simple lemma matching. 4We have employed the imp</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Brin, Sergey and Page, Lawrence 1998. The anatomy of a large-scale hypertextual Web search engine Computer networks and ISDN systems</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinion features in customer reviews</title>
<date>2004</date>
<publisher>AAAI</publisher>
<contexts>
<context position="1474" citStr="Hu and Liu, 2004" startWordPosition="212" endWordPosition="215">n important research on finding automatic ways of processing and exploiting this valuable source of information. That is one of the reasons why sentiment analysis has become a very active research field during the last decade (Pang and Lee, 2008; Liu et al., 2012; Zhang and Liu, 2014). Sentiment analysis aims to detect and classify the polarity of sentiments expressed in a text. The granularity of this classification goes from the overall polarity of full documents to paragraphs, sentences or, as in Aspect Based Sentiment Analysis (ABSA), the sentiment about precise aspects being opinionated (Hu and Liu, 2004) (Popescu and Etzioni, 2005) (Wu et al., 2009) (Zhang et al. , 2010). In this paper we describe our participation in SemEval-2015 task 121 (Pontiki et al., 2015), which is about ABSA. We have participated in all subtasks 1http://alt.qcri.org/semeval2015/task12/ employing unsupervised or weakly supervised approaches. The rest of the paper is structured as follows. Section 2 introduces the SemEval-2015 task 12 competition and provided datasets, and a brief introduction about how we have approached the different slots. Sections 3, 4 and 5 describe more in detail the employed techniques. Section 6</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Hu, Minqing and Liu, Bing 2004. Mining opinion features in customer reviews AAAI</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis Foundations and trends in information retrieval,</title>
<date>2008</date>
<contexts>
<context position="1102" citStr="Pang and Lee, 2008" startWordPosition="152" endWordPosition="155">s. Our attempt is based on requiring the minimum annotated or hand-crafted content, and avoids training a model using the provided training set. We use a continuous word representations (Word2Vec) to leverage in-domain semantic similarities of words for many of the involved subtasks. 1 Introduction The continuous growing of textual content on the Internet has motivated an important research on finding automatic ways of processing and exploiting this valuable source of information. That is one of the reasons why sentiment analysis has become a very active research field during the last decade (Pang and Lee, 2008; Liu et al., 2012; Zhang and Liu, 2014). Sentiment analysis aims to detect and classify the polarity of sentiments expressed in a text. The granularity of this classification goes from the overall polarity of full documents to paragraphs, sentences or, as in Aspect Based Sentiment Analysis (ABSA), the sentiment about precise aspects being opinionated (Hu and Liu, 2004) (Popescu and Etzioni, 2005) (Wu et al., 2009) (Zhang et al. , 2010). In this paper we describe our participation in SemEval-2015 task 121 (Pontiki et al., 2015), which is about ABSA. We have participated in all subtasks 1http:/</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee 2008. Opinion mining and sentiment analysis Foundations and trends in information retrieval,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<date>2013</date>
<booktitle>Efficient Estimation of Word Representations in Vector Space Proceedings of Workshop at ICLR</booktitle>
<contexts>
<context position="6544" citStr="Mikolov et al., 2013" startWordPosition="1021" endWordPosition="1024">build our graph. Then we have computed our own definition of semantic relations and opinion relations to build sub-graphs as follows: • Opinion relations (AT-OW edges): we have computed how many times each AT has some syntactical dependency relation with each OW, from a certain set of dependency relations (i.e. direct object, adjectival modifier, attribute of a copulative verb). The result of this count is used as the weight of the edges between AT and OW nodes. • Semantic relations (AT-AT and OW-OW edges): we have computed a continuous word representation of the datasets employing Word2Vec4 (Mikolov et al., 2013) (with the following parameters: skip-grams, vector size of 200, context window of 5, hierarchical softmax). Then we have used the cosine similarity between word vectors as the weight of the semantic relation edges. Once we have built the graph with the different type of nodes and different type of weighted edges, we execute a PageRank (Brin and Page, 1998) (alpha parameter set to 0.15) to score and rank the nodes. With the obtained score we generate an ordered list of aspect terms. We have done this only for restaurants since it was the only domain requested in the task 12 slot 2. Example of </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean 2013. Efficient Estimation of Word Representations in Vector Space Proceedings of Workshop at ICLR</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kang Liu</author>
<author>Liheng Xu</author>
<author>Zhao</author>
</authors>
<title>Extracting Opinion Targets and Opinion Words from Online Reviews with Graph Co-ranking</title>
<date>2014</date>
<booktitle>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</booktitle>
<contexts>
<context position="4911" citStr="Liu et al., 2014" startWordPosition="768" endWordPosition="771"> have used a subset about 100k reviews from a big dataset of Amazon electronic device reviews 3 (retaining only the ones that contain the word laptop). We name this corpus as Amazon-laptops. 3 Aspect term extraction SemEval2015 Task 12 slot 2 was about detecting mentions to explicit aspect terms, but only for restaurant domain (i.e. other slots run for restaurants and laptop domains). For aspect term extraction our aim is to bootstrap a list of candidate domain aspect terms and use it to annotate the reviews of the same domain. We have implemented a system inspired in the method described at (Liu et al., 2014). In this work the authors employ what they call a graph co-ranking approach. They model aspect-terms (AT) and opinion-words (OW) as graph nodes, and then they generate three different sub-graphs defining different types of relations (what they call semantic-relations and opinionrelations) between the nodes. Finally they rank the nodes using a combined random walk on the three sub-graphs to obtain a list of reliable aspect-term candidates. Due to space limitations we cannot explain all the details here. Please, refer to the original article for more in detail explanation. Based on some of thes</context>
</contexts>
<marker>Liu, Xu, Zhao, 2014</marker>
<rawString>Liu, Kang and Xu, Liheng and Zhao, Jun 2014. Extracting Opinion Targets and Opinion Words from Online Reviews with Graph Co-ranking Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining Synthesis Lectures on Human Language Technologies</title>
<date>2012</date>
<marker>Liu, 2012</marker>
<rawString>Bing Liu 2012. Sentiment analysis and opinion mining Synthesis Lectures on Human Language Technologies</rawString>
</citation>
<citation valid="false">
<authors>
<author>Maria Pontiki</author>
</authors>
<title>Dimitrios Galanis, Haris Papageogiou, Suresh Manandhar, and Ion Androutsopoulos 2015. SemEval-2015 Task 12: Aspect Based Sentiment Analysis</title>
<booktitle>Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, Colorado</location>
<marker>Pontiki, </marker>
<rawString>Maria Pontiki, Dimitrios Galanis, Haris Papageogiou, Suresh Manandhar, and Ion Androutsopoulos 2015. SemEval-2015 Task 12: Aspect Based Sentiment Analysis Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, Colorado</rawString>
</citation>
<citation valid="true">
<authors>
<author>AM Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews Natural language processing and text mining</title>
<date>2005</date>
<contexts>
<context position="1502" citStr="Popescu and Etzioni, 2005" startWordPosition="216" endWordPosition="219">h on finding automatic ways of processing and exploiting this valuable source of information. That is one of the reasons why sentiment analysis has become a very active research field during the last decade (Pang and Lee, 2008; Liu et al., 2012; Zhang and Liu, 2014). Sentiment analysis aims to detect and classify the polarity of sentiments expressed in a text. The granularity of this classification goes from the overall polarity of full documents to paragraphs, sentences or, as in Aspect Based Sentiment Analysis (ABSA), the sentiment about precise aspects being opinionated (Hu and Liu, 2004) (Popescu and Etzioni, 2005) (Wu et al., 2009) (Zhang et al. , 2010). In this paper we describe our participation in SemEval-2015 task 121 (Pontiki et al., 2015), which is about ABSA. We have participated in all subtasks 1http://alt.qcri.org/semeval2015/task12/ employing unsupervised or weakly supervised approaches. The rest of the paper is structured as follows. Section 2 introduces the SemEval-2015 task 12 competition and provided datasets, and a brief introduction about how we have approached the different slots. Sections 3, 4 and 5 describe more in detail the employed techniques. Section 6 shows the results of the ev</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Popescu, AM and Etzioni, Oren 2005. Extracting product features and opinions from reviews Natural language processing and text mining</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanbin Wu</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
<author>Lide Wu</author>
</authors>
<title>Phrase dependency parsing for opinion mining</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<contexts>
<context position="1520" citStr="Wu et al., 2009" startWordPosition="220" endWordPosition="223">of processing and exploiting this valuable source of information. That is one of the reasons why sentiment analysis has become a very active research field during the last decade (Pang and Lee, 2008; Liu et al., 2012; Zhang and Liu, 2014). Sentiment analysis aims to detect and classify the polarity of sentiments expressed in a text. The granularity of this classification goes from the overall polarity of full documents to paragraphs, sentences or, as in Aspect Based Sentiment Analysis (ABSA), the sentiment about precise aspects being opinionated (Hu and Liu, 2004) (Popescu and Etzioni, 2005) (Wu et al., 2009) (Zhang et al. , 2010). In this paper we describe our participation in SemEval-2015 task 121 (Pontiki et al., 2015), which is about ABSA. We have participated in all subtasks 1http://alt.qcri.org/semeval2015/task12/ employing unsupervised or weakly supervised approaches. The rest of the paper is structured as follows. Section 2 introduces the SemEval-2015 task 12 competition and provided datasets, and a brief introduction about how we have approached the different slots. Sections 3, 4 and 5 describe more in detail the employed techniques. Section 6 shows the results of the evaluation, and fina</context>
</contexts>
<marker>Wu, Zhang, Huang, Wu, 2009</marker>
<rawString>Wu, Yuanbin and Zhang, Qi and Huang, Xuanjing and Wu, Lide 2009. Phrase dependency parsing for opinion mining Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhang</author>
<author>Bing Liu</author>
<author>SH Lim</author>
<author>E O’Brien-Strain</author>
</authors>
<title>Extracting and ranking product features in opinion documents</title>
<date>2010</date>
<booktitle>Proceedings of the 23rd International Conference on Computational Linguistics</booktitle>
<marker>Zhang, Liu, Lim, O’Brien-Strain, 2010</marker>
<rawString>Zhang, L and Liu, Bing and Lim, SH and O’Brien-Strain, E 2010. Extracting and ranking product features in opinion documents Proceedings of the 23rd International Conference on Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Zhang</author>
<author>Bing Liu</author>
</authors>
<title>Aspect and Entity Extraction for Opinion Mining Data Mining and Knowledge Discovery for Big Data</title>
<date>2014</date>
<contexts>
<context position="1142" citStr="Zhang and Liu, 2014" startWordPosition="160" endWordPosition="163">he minimum annotated or hand-crafted content, and avoids training a model using the provided training set. We use a continuous word representations (Word2Vec) to leverage in-domain semantic similarities of words for many of the involved subtasks. 1 Introduction The continuous growing of textual content on the Internet has motivated an important research on finding automatic ways of processing and exploiting this valuable source of information. That is one of the reasons why sentiment analysis has become a very active research field during the last decade (Pang and Lee, 2008; Liu et al., 2012; Zhang and Liu, 2014). Sentiment analysis aims to detect and classify the polarity of sentiments expressed in a text. The granularity of this classification goes from the overall polarity of full documents to paragraphs, sentences or, as in Aspect Based Sentiment Analysis (ABSA), the sentiment about precise aspects being opinionated (Hu and Liu, 2004) (Popescu and Etzioni, 2005) (Wu et al., 2009) (Zhang et al. , 2010). In this paper we describe our participation in SemEval-2015 task 121 (Pontiki et al., 2015), which is about ABSA. We have participated in all subtasks 1http://alt.qcri.org/semeval2015/task12/ employ</context>
</contexts>
<marker>Zhang, Liu, 2014</marker>
<rawString>Zhang, Lei and Liu, Bing 2014. Aspect and Entity Extraction for Opinion Mining Data Mining and Knowledge Discovery for Big Data</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>