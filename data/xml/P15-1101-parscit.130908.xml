<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.978942">
Sentence-level Emotion Classification with Label and Context Dependence
</title>
<author confidence="0.95224">
Shoushan Li†‡, Lei Huang†, Rong Wang†, Guodong Zhou†*
†Natural Language Processing Lab, Soochow University, China
</author>
<affiliation confidence="0.967777">
‡ Collaborative Innovation Center of Novel Software Technology and Industrialization
</affiliation>
<email confidence="0.836028">
{shoushan.li, lei.huang2013, wangrong2022}@gmail.com,
gdzhou@suda.edu.cn
</email>
<sectionHeader confidence="0.977238" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996551875">
Predicting emotion categories, such as anger,
joy, and anxiety, expressed by a sentence is
challenging due to its inherent multi-label
classification difficulty and data sparseness.
In this paper, we address above two chal-
lenges by incorporating the label dependence
among the emotion labels and the context de-
pendence among the contextual instances into
a factor graph model. Specifically, we recast
sentence-level emotion classification as a fac-
tor graph inferring problem in which the label
and context dependence are modeled as vari-
ous factor functions. Empirical evaluation
demonstrates the great potential and effective-
ness of our proposed approach to sentence-
level emotion classification.
</bodyText>
<sectionHeader confidence="0.992497" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999734647058824">
Predicting emotion categories, such as anger, joy,
and anxiety, expressed by a piece of text encom-
passes a variety of applications, such as online
chatting (Galik et al., 2012), news classification
(Liu et al., 2013) and stock marketing (Bollen et
al., 2011). Over the past decade, there has been a
substantial body of research on emotion classifi-
cation, where a considerable amount of work has
focused on document-level emotion classification.
Recently, the research community has become
increasingly aware of the need on sentence-level
emotion classification due to its wide potential ap-
plications, e.g. the massively growing importance
of analyzing short text in social media (Ki-
ritchenko et al., 2014; Wen and Wan, 2014). In
general, sentence-level emotion classification ex-
hibits two challenges.
</bodyText>
<note confidence="0.96466">
* Corresponding author
</note>
<figureCaption confidence="0.97818">
Figure 1: An example of a paragraph and the
</figureCaption>
<bodyText confidence="0.924543307692308">
sentences therein with their emotion categories
from the corpus collected by Quan and Ren
(2009)
On one hand, like document-level emotion
classification, sentence-level emotion classifica-
tion is naturally a multi-label classification prob-
lem. That is, each sentence might involve more
than one emotion category. For example, as
shown in Figure 1, in one paragraph, two sen-
tences, i.e., S1 and S3, have two and three emotion
categories respectively. Automatically classifying
instances with multiple possible categories is
......
</bodyText>
<note confidence="0.900524913043478">
脸Air&amp;quot;&lt;/S1&gt; &lt;S2&gt;
现在我终于如愿以偿。&lt;/S2&gt; &lt;S3&gt;感受着小手
的温度,享受着这份她对我的依恋,生怕动
一下,会让她的小手离我而去。&lt;/S3&gt;
(English:
&lt;S1&gt; The girls fall to sleep, so I make my way
noiselessly onto the bed, wishing I could get a
chance to give a kiss to Yan, suddenly she turn
over to me and her little soft hand fall onto my
face.&lt;/S1&gt; &lt;S2&gt;Praise the Lord, that is all I
want.&lt;/S2&gt; &lt;S3&gt;Feeling the warm of her hand
and the attachment she hold to me, I couldn’t af-
ford to move even a little, fearing I may lost her
hand.&lt;/S3&gt;) )
Sentence-level Emotion Classification
➢Input: S1, S2, S3
➢Output: S1 : joy, love
S2: joy
S3: joy, love, anxiety
1045
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1045–1053,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.99980924">
sometimes much more difficult than classifying
instances with a single label.
On the other hand, unlike document-level emo-
tion classification, sentence-level emotion classi-
fication is prone to the data sparseness problem
because a sentence normally contains much less
content. Given the short text of a sentence, it is
often difficult to predict its emotion due to the
limited information therein. For example, in S2,
only one phrase “如愿W偿(that is all I want)” ex-
presses the joy emotion. Once this phrase fails to
appear in the training data, it will be hard for the
classifier to give a correct prediction according to
the limited content in this sentence.
In this paper, we address above two challenges
in sentence-level emotion classification by mod-
eling both the label and context dependence. Here,
the label dependence indicates that multiple emo-
tion labels of an instance are highly correlated to
each other. For instance, the two positive emo-
tions, joy and love, are more likely to appear at the
same time than the two counterpart emotions, joy
and hate. The context dependence indicates that
two neighboring sentences or two sentences in the
same paragraph (or document) might share the
same emotion categories. For instance, in Figure
1, S1, S2, and S3, from the same paragraph, all
share the emotion category joy.
Specifically, we propose a factor graph, namely
Dependence Factor Graph (DFG), to model the la-
bel and context dependence in sentence-level
emotion classification. In our DFG approach, both
the label and context dependence are modeled as
various factor functions and the learning task aims
to maximize the joint probability of all these fac-
tor functions. Empirical evaluation demonstrates
the effectiveness of our DFG approach to captur-
ing the inherent label and context dependence. To
the best of our knowledge, this work is the first
attempt to incorporate both the label and context
dependence of sentence-level emotion classifica-
tion into a unified framework.
The remainder of this paper is organized as fol-
lows. Section 2 overviews related work on emo-
tion analysis. Section 3 presents our observations
on label and context dependence in the corpus.
Section 4 proposes our DFG approach to sen-
tence-level emotion classification. Section 5 eval-
uates the proposed approach. Finally, Section 6
gives the conclusion and future work.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999939659090909">
Over the last decade, there has been an explosion
of work exploring various aspects of emotion
analysis, such as emotion resource creation
(Wiebe et al., 2005; Quan and Ren, 2009; Xu et
al., 2010), writer’s emotion vs. reader’s emotion
analysis (Lin et al., 2008; Liu et al., 2013), emo-
tion cause event analysis (Chen et al., 2010), doc-
ument-level emotion classification (Alm et al.,
2005; Li et al., 2014) and sentence-level or short
text-level emotion classification (Tokushisa et al.,
2008; Bhowmick et al., 2009; Xu et al., 2012).
This work focuses on sentence-level emotion clas-
sification.
Among the studies on sentence-level emotion
classification, Tokushisa et al. (2008) propose a
data-oriented method for inferring the emotion of
an utterance sentence in a dialog system. They
leverage a huge collection of emotion-provoking
event instances from the Web to deal with the data
sparseness problem in sentence-level emotion
classification. Bhowmick et al. (2009) and
Bhowmick et al. (2010) apply KNN-based classi-
fication algorithms to classify news sentences into
multiple reader emotion categories. Although the
multi-label classification difficulty has been no-
ticed in their study, the label dependence is not
exploited. More recently, Xu et al. (2012) pro-
poses a coarse-to-fine strategy for sentence-level
emotion classification. They deal with the data
sparseness problem by incorporating the transfer
probabilities from the neighboring sentences to
refine the emotion categories. To some extent, this
can be seen a specific kind of context information.
However, they ignore the label dependence by di-
rectly applying Binary Relevance to overcome the
multi-label classification difficulty.
Unlike all above studies, this paper emphasizes
the importance of the label dependence and ex-
ploits it in sentence-level emotion classification
via a factor graph model. Moreover, besides the
label dependence, our factor graph-based ap-
proach incorporates the context dependence in a
unified framework to further improve the perfor-
mance of sentence-level emotion classification.
</bodyText>
<sectionHeader confidence="0.993945" genericHeader="method">
3 Observations
</sectionHeader>
<bodyText confidence="0.999598">
To better illustrate our motivation of modeling the
label and context dependence, we systematically
investigate both dependence phenomena in our
evaluation corpus.
</bodyText>
<page confidence="0.702527">
1046
</page>
<figureCaption confidence="0.9654695">
Figure 2: Probability distribution of most and least frequently-occurred pairs of emotion categories,
with left four most frequently-occurred and right four least frequently-occurred, among all 28 pairs
</figureCaption>
<figure confidence="0.991233466666667">
0.2
0.183
0.005 0.003 0.002 0.0003
0.078 0.077
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
0.094
</figure>
<bodyText confidence="0.98991025">
The corpus contains 100 documents, randomly
selected from Quan and Ren (2009). There are to-
tally 2751 sentences and each of them is manually
annotated with one or more emotion labels.
</bodyText>
<tableCaption confidence="0.999084">
Table 1: The numbers of the sentences in each
</tableCaption>
<table confidence="0.948424333333333">
emotion category
Emotion #Sentence Emotion #Sentence
joy 691 anxiety 567
hate 532 surprise 180
love 1025 anger 287
sorrow 611 expect 603
</table>
<tableCaption confidence="0.9942345">
Table 2: The numbers of the sentences
grouped by the emotion labels they contain
</tableCaption>
<table confidence="0.995597714285714">
#Sentence
No Label 180
One Label 1096
Two Labels 1081
Three Labels 346
Four or more labels 48
ALL 2751
</table>
<bodyText confidence="0.986610305555555">
Table 1 shows the sentence distribution of the
eight emotion categories. Obviously, the distribu-
tion is a bit imbalanced. While about to one quar-
ter of sentences express the emotion category love,
only —6% and —10% express surprise and anger
respectively, with the remaining 5 emotion cate-
gories distributed rather evenly from —20% to
—25%. Table 2 shows the numbers of the sen-
tences grouped by the emotion labels they contain.
From this table, we can see that more than half
sentences have two or more emotion labels. This
indicates the popularity of the multi-label issue in
sentence-level emotion classification.
To investigate the phenomenon of label de-
pendence, we first assume that denotes
an input domain of instances and Y= 11, 12,...,1.1
be a finite domain of possible emotion labels.
Each instance is associated with a subset of Y and
this subset is described as an in-dimensional vec-
tor y = {y&apos;, yz,..., ym} where y`=1 only if in-
stance x has label l;. and otherwise. Then,
we can calculate the probability that an instance
takes both emotion labels l; and , denoted as
p(l;,lj) . Figure 2 shows the probability distribu-
tion of most and least frequently-occurred pairs of
emotion categories, with left four most fre-
quently-occurred and right four least frequently-
occurred, among all 28 pairs. From this figure, we
can see that some pairs, e.g., joy and love, are
much more likely to be taken by one sentence than
some other pairs, e.g. joy and anger.
Finally, we investigate the phenomenon of the
context dependence by calculating the probabili-
ties that two instances xk and x, have at least one
identical emotion label, i.e., p(y, in
different settings.
</bodyText>
<figure confidence="0.909736846153846">
1047
Sentence-1
Sentence-2
DFG model
f (
g(
)
)
h( Y, )
h( , y )
f (
)
g( Y, )
</figure>
<figureCaption confidence="0.588126">
Figure 4: An example of DFG when two instances are involved: sentence-1 with the label vector [1, 0,
1] and sentence-2 with the label vector [1, 1, 0]
Note: each multi-label instance is transformed into three pseudo samples, represented as .
</figureCaption>
<bodyText confidence="0.875192166666667">
f o represents a factor function for modeling textual features. represents a factor function for mod-
eling the label dependence between two pseudo samples. represents a factor function for modeling
the context dependence between two instances in the same context.
Figure 3: Probabilities that two instances have
an identical emotion label in different settings
Figure 3 shows the probabilities that two in-
stances have at least one identical emotion label in
different settings, where neighbor, paragraph,
document and random mean two neighboring in-
stances, two instances from the same paragraph,
two instances from the same document, and two
instances from a random selection, respectively.
From this figure, we can see that two instances
from the same context are much more likely to
take an identical emotion label than two random
instances.
From above statistics, we come to two basic ob-
servations:
</bodyText>
<listItem confidence="0.7166455">
1) Label dependency: One sentence is more
likely to take some pair of emotion labels, e.g.,
</listItem>
<bodyText confidence="0.9848915">
hate and angry than some other pair of emo-
tion labels, e.g., hate and happy.
2) Context dependency: Two instances from the
same context are more likely to share the same
emotion label than those from a random selec-
tion.
</bodyText>
<sectionHeader confidence="0.975949" genericHeader="method">
4 Dependence Factor Graph Model
</sectionHeader>
<bodyText confidence="0.999720666666667">
In this section, we propose a dependence factor
graph (DFG) model for learning emotion labels of
sentences with both label and context dependence.
</bodyText>
<subsectionHeader confidence="0.9850585">
4.1 Preliminary
Factor Graph
</subsectionHeader>
<bodyText confidence="0.999988571428571">
A factor graph consists of two layers of nodes, i.e.,
variable nodes and factor nodes, with links be-
tween them. The joint distribution over the whole
set of variables can be factorized as a product of
all factors. Figure 4 gives an example of our de-
pendence factor graph (DFG) when two instances,
i.e., sentence-1 and sentence-2 are involved.
</bodyText>
<subsectionHeader confidence="0.678355">
Binary Relevance
</subsectionHeader>
<bodyText confidence="0.998651">
A popular solution to multi-label classification is
called binary relevance which constructs a binary
classifier for each label, resulting a set of inde-
</bodyText>
<figure confidence="0.97242175">
0.8 0.68 0.69
0.6 0.5
0.4
0.2
0
0.22
Neighbor Paragraph Document Random
1048
</figure>
<bodyText confidence="0.996702777777778">
pendent binary classification problems (Tsouma-
kas and Katakis, 2007; Tsoumakas et al., 2009).
In our approach, binary relevance is utilized as a
preliminary step so that each original instance is
transformed into K pseudo samples, where K is
the number of categories. For example, in Figure
4, , , and represent the three pseudo
samples, generated from the same original in-
stance sentence-1.
</bodyText>
<subsectionHeader confidence="0.989748">
4.2 Model Definition
</subsectionHeader>
<bodyText confidence="0.998396733333333">
Formally, let G = (V, E, X) represent an instance
network, where V denotes a set of sentence in-
stances. is a set of relationships be-
tween sentences. Two kinds of relationship exist
in our instance network: One represents the label
dependence between each two pseudo instances
generated from the same original instance, while
the other represents the context dependence when
the two instances are from the same context, e.g.,
the same paragraph. is the textual feature vec-
tor associated with a sentence.
We model the above network with a factor
graph and our objective is to infer the emotion cat-
egories of instances by learning the following
joint distribution:
</bodyText>
<equation confidence="0.965143777777778">
P Y G
 
j �

f k k ( k \ k \ k \ (1)
Xi ,yi Yi ,G Yi h Yi ,H Yik
k i
f( Xk �)k _ 1 yr k �)k
i,Yi ) Z1 e`Y akj(D(xi), Jl ) �
</equation>
<bodyText confidence="0.991169">
where three kinds of factor functions are used.
</bodyText>
<listItem confidence="0.4735705">
1) Textual feature factor function: f l X k ° yk )
denotes the traditional textual feature factor
</listItem>
<bodyText confidence="0.940419894736842">
functions associated with each text . The
textual feature factor function is instantiated as
follows:
(3)
Where/3„, is the weight of the function, rep-
resenting the influence degree of the two in-
stancesyk andy; .
3) Context dependence factor function:
h(yk,H(yk)) denotes the additional context
dependence relationship among the instances,
whereH(yk) is the set of the instances con-
nected to
H(yk) andyk are the labels of
the pseudo instances from the same context but
generated from different original instances.
The context dependence factor function is in-
ample, we can write the gradient of eachak
with
regard to the objective function:
</bodyText>
<equation confidence="0.781530285714286">
�L ( )
,yik  
kj
 
(x,
(Y
G)
(x,
(D
yik) —EP
kj
|
(D
yik) (6)
</equation>
<figure confidence="0.798206225">
a
o
aakj=E
function
, yk) under the distri

ij
bution
Pkj Y Ggiven by the estimated model. Figure 5
illustrates the detailed algorithm for learning the
parameter . Note that LBP denotes the Loopy
(2)
Where
xis , yk
is a feature function and x
represents a textual feature, i.e., a word feature
in this study.
2) Label dependence factor function:
g(yk, G(yk )) denotes the additional label de-
pendence relationship among the pseudo in-
stances, where G(yk) is the label set of the
instances connected to yk . G(yk) andyk are
labels of the pseudo instances generated from
the same original instance. The label depend-
ence factor function is instan


tiated as follows:
stan
.
tiated as follows:
(4)
Where8i�k is the weight of the function, repre-
senting the influence degree of the two in-
stan
ces and .
4.3 Model Learning
Learning the DFG model is to estimate the best
parameter configurationto
maximize the log-likelihood objective function
</figure>
<equation confidence="0.948944333333333">
L(9) =1ogPA (Y
, i.e.,
d*=argmaxL(H) (5)
</equation>
<bodyText confidence="0.8168196">
In this study, we employ the gradient decent
method to optimize the objective function. For ex-
Where E C(D (x,, yk)] is the expectation of feature
function (D (x,, yk) given the data distribution.
Ea
</bodyText>
<equation confidence="0.861007">
YG
x


is the expectation of feature
1049
</equation>
<bodyText confidence="0.9977795">
Belief Propagation (LBP) algorithm which is ap-
plied to approximately infer the marginal distribu-
tion in a factor graph (Frey and MacKay, 1998).
A similar gradient can be derived for the other pa-
rameters.
Ren, 2009). In our experiments, we use 80 docu-
ments as the training data and the remaining 20
documents as the test data.
</bodyText>
<figure confidence="0.904628">
Features
Input: Learning rate17
Output: Estimated parametersa
Initializea0
Repeat
1) Calculate using LBP
2) Calculate using LBP
3) Calculate the gradient of according to
Eq. (6)
4) Update parameter with the learning
raterl
Until Convergence
</figure>
<figureCaption confidence="0.999543">
Figure 5: The learning algorithm for DGP model
</figureCaption>
<subsectionHeader confidence="0.982917">
4.4 Model Prediction
</subsectionHeader>
<bodyText confidence="0.999482">
With the learned parameter configuration , the
prediction task is to find a which optimizes
the objective function, i.e.,
</bodyText>
<equation confidence="0.922697">
Y&apos;- =argmaxP(Y&amp;quot; Y°,G,B) (7)
</equation>
<bodyText confidence="0.999426375">
Where are the labels of the instances in the
testing data.
Again, we utilize LBP to calculate the marginal
probability of each instance and
predict the label with the largest marginal proba-
bility. As all instances in the test data are con-
cerned, above prediction is performed in an itera-
tion process until the results converge.
</bodyText>
<sectionHeader confidence="0.996297" genericHeader="method">
5 Experimentation
</sectionHeader>
<bodyText confidence="0.999913">
We have systematically evaluated our DFG ap-
proach to sentence-level emotion classification.
</bodyText>
<subsectionHeader confidence="0.8678905">
5.1 Experimental Setting
Corpus
</subsectionHeader>
<bodyText confidence="0.999945">
The corpus contains 100 documents (2751 sen-
tences) from the Ren-CECps corpus (Quan and
Each instance is treated as a bag-of-words and
transformed into a binary vector encoding the
presence or absence of word unigrams.
</bodyText>
<subsectionHeader confidence="0.74519">
Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.998884666666667">
In our study, we employ three evaluation metrics
to measure the performances of different ap-
proaches to sentence-level emotion classification.
These metrics have been popularly used in some
multi-label classification problems (Godbole and
Sarawagi, 2004; Schapire and Singer, 2000).
</bodyText>
<listItem confidence="0.9080052">
1) Hamming loss: It evaluates how many times
an instance-label pair is misclassified consid-
ering the predicted set of labels and the
ground truth set of labels, i.e.,
(8)
</listItem>
<bodyText confidence="0.987858666666667">
where q is the number of all test instances and
m is the number of all emotion labels. is
the estimated label while is the true label.
</bodyText>
<listItem confidence="0.8477259">
2) Accuracy: It gives an average degree of the
similarity between the predicted and the
ground truth label sets of all test examples, i.e.,
Accuracy

3) F1-measure: It is the harmonic mean between
precision and recall. It can be calculated from
true positives, true negatives, false positive
and false negatives based on the predictions
and the corresponding actual values, i.e.,
</listItem>
<equation confidence="0.93324325">
1 q

1

</equation>
<bodyText confidence="0.99976125">
Note that smaller Hamming loss corresponds to
better classification quality, while larger accuracy
and F-measure corresponds to better classifica-
tion quality.
</bodyText>
<equation confidence="0.888031428571428">
&apos;
yi  yi
&apos;
F1
(10)

i
q
yi
yi
&apos;
yi  yi
1 q

</equation>
<page confidence="0.167288">
&apos;
</page>
<figure confidence="0.934965608695652">
(9)
i
1
q
yi U yi
1050
0.7
0.6
0.5
0.4
0.3
0.2
0.1
Baseline LebelD(Wang et al.,2014) DFG-label(Our approach)
0.477 0.461
Hloss Accuracy F1
0.242
0.378
0.391
0.634
0.261
0.269
0.379
</figure>
<figureCaption confidence="0.99201025">
Figure 6: Performance comparison of different approaches to sentence-level emotion classification
with the label dependence only
Figure 7: Performance comparison of different approaches to sentence-level emotion classification
with the context dependence only
</figureCaption>
<figure confidence="0.96387252">
0.6
0.5
0.4
0.3
0.2
0.477
Hloss Accuracy F1
Baseline Tansfer(Xu et al.,2012) DFG-context(Neighbor)
DFG-context(Paragraph) DFG-context(Document)
0.472
0.416
0.45
0.569
0.378
0.382
0.443
0.407
0.295
0.261
0.264
0.292
0.275
0.215
5.2 Experimental Results with Label De-
pendence
</figure>
<bodyText confidence="0.867646605263158">
In this section, we compare following approaches
which only consider the label dependence among
pseudo instances:
➢ Baseline: As a baseline, this approach applies
a maximum entropy (ME) classifier with only
textual features, ignoring both the label and
context dependence.
➢ LabelD: As the state-of-the-art approach to
handling multi-label classification, this ap-
proach incorporates label dependence, as de-
scribed in (Wang et al., 2014). Specifically,
this approach first utilizes a Bayesian network
to infer the relationship among the labels and
then employ them in the classifier.
➢ DFG-label: Our DFG approach with the label
dependence.
Figure 6 compares the performance of different
approaches to sentence-level emotion classifica-
tion with the label dependence. From this figure,
we can see that our DFG approach improves the
baseline approach with an impressive improve-
ment in all three kinds of evaluation metrics, i.e.,
23.5% reduction in Hloss, 25.6% increase in Ac-
curacy, and 11.8% increase in F1. This result ver-
ifies the effectiveness of incorporating the label
dependence in sentence-level emotion classifica-
tion. Compared to the state-of-the-art LabelD ap-
proach, our DFG approach is much superior. Sig-
nificant test show that our DFG approach signifi-
cantly outperforms both the baseline approach and
LabelD (p-value&lt;0.01). One reason that LabelD
performs worse than our approach is possibly due
to their separating learning on textual features and
label relationships. Also, different from ours, their
approach could not capture the information be-
tween two conflict emotion labels, such as “happy”
and “sad” (they are not possibly appearing to-
gether).
</bodyText>
<sectionHeader confidence="0.358314" genericHeader="method">
5.3 Experimental Results with Context De-
pendence
</sectionHeader>
<bodyText confidence="0.999902666666667">
In this section, we compare following approaches
which only consider the context dependence
among pseudo instances:
</bodyText>
<page confidence="0.469644">
1051
</page>
<bodyText confidence="0.982958238095238">
➢ Baseline: same as the one in Section 5.2,
which applies a maximum entropy (ME) clas-
sifier with only textual features, ignoring both
the label and context dependence.
➢ Transfer: As the state-of-the-art approach to
incorporating contextual information in sen-
tence-level emotion classification (Xu et al.,
2012), this approach utilizes the label transfor-
mation probability to refine the classification
results.
➢ DFG-label (Neighbor): Our DFG approach
with the context dependence only. Specifically,
the neighboring instances are considered as
context.
➢ DFG-label (Paragraph): Our DFG approach
with the context dependence only. Specifically,
the instances in the same paragraph are consid-
ered as context.
➢ DFG-label (Document): Our DFG approach
with the context dependence only. Specifically,
the instances in the same document are consid-
ered as context.
Figure 7 compares the performance of different
approaches to sentence-level emotion classifica-
tion with the context dependence only. From this
figure, we can see that our DFG approach consist-
ently improves the state-of-the-art in all three
kinds of evaluation metrics, i.e., 6.1% reduction in
Hloss, 6.5% increase in Accuracy, and 3.1% in-
crease in F1 when the neighboring instances are
considered as context. Among the three kinds of
context, the neighboring setting performs best.
We also find that using the whole document as the
context is not helpful and it performs even worse
than the baseline approach. Compared to the state-
of-the-art Transfer approach, our DFG approach
with the neighboring context dependence is much
superior. Significant test show that our DFG ap-
proach with the neighboring context dependence
significantly outperforms the baseline approach
and the state-of-the-art LabelD approach (p-
value&lt;0.01).
</bodyText>
<subsectionHeader confidence="0.8536155">
5.4 Experimental Results with Both Label
and Context Dependence
</subsectionHeader>
<bodyText confidence="0.999780833333333">
Table 3 shows the performance of our DFG ap-
proach with both label and context dependence,
denoted as DGF-both. From this table, we can see
that using both label and context dependence fur-
ther improves the performance.
Figure 8 shows the performance of our DGF-
both approach when different sizes of training
data are used to train the model. From this figure,
we can see that incorporating both the label and
context dependence consistently improves the
performance with a large margin, irrespective of
the amount of training data available.
</bodyText>
<tableCaption confidence="0.9510715">
Table 3: Performance of our DFG approach
with both label and context dependence
</tableCaption>
<table confidence="0.9993076">
Hloss Accuracy F1
Baseline 0.447 0.378 0.261
DFG-label 0.254 0.621 0.372
DFG-context 0.416 0.443 0.292
DFG-both 0.242 0.634 0.379
</table>
<figureCaption confidence="0.92889">
Figure 8: Performance of our DGF-both ap-
</figureCaption>
<bodyText confidence="0.6285275">
proach when different sizes of training data are
used
</bodyText>
<sectionHeader confidence="0.992882" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999969857142857">
In this paper, we propose a novel approach to sen-
tence-level emotion classification by incorporat-
ing both the label dependence among the emotion
labels and the context dependence among the con-
textual instances into a factor graph, where the la-
bel and context dependence is modeled as various
factor functions. Empirical evaluation shows that
</bodyText>
<figure confidence="0.984192375">
Hloss
20% 40% 60% 80%
0.6
0.5
0.4
0.3
0.2
Accuracy
20% 40% 60% 80%
0.7
0.6
0.5
0.4
0.3
0.2
Baseline DFG-both
F1
20% 40% 60% 80%
0.4
0.35
0.3
0.25
0.2
1052
</figure>
<bodyText confidence="0.996269857142857">
our DFG approach performs significantly better
than the state-of-the-art.
In the future work, we would like to explore bet-
ter ways of modeling the label and context de-
pendence and apply our DFG approach in more
applications, e.g. micro-blogging emotion classi-
fication.
</bodyText>
<sectionHeader confidence="0.997138" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995192">
This research work has been partially supported
by three NSFC grants, No.61273320,
No.61375073, No.61331011, and Collaborative
Innovation Center of Novel Software Technology
and Industrialization.
</bodyText>
<sectionHeader confidence="0.994241" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999904105882353">
Alm C., D. Roth and R. Sproat. 2005. Emotions from
Text: Machine Learning for Text-based Emotion
Prediction. In Proceedings of EMNLP-05, pp.579-
586.
Bhowmick P., A. Basu, P. Mitra, and A. Prasad. 2009.
Multi-label Text Classification Approach for Sen-
tence Level News Emotion Analysis. Pattern
Recognition and Machine Intelligence. Lecture
Notes in Computer Science, Volume 5909, pp 261-
266.
Bhowmick P., A. Basu, P. Mitra, and A. Prasad. 2010.
Sentence Level News Emotion Analysis in Fuzzy
Multi-label Classification Framework. Research in
Computing Science. Special Issue: Natural Lan-
guage Processing and its Applications, pp.143-154.
Bollen J., H. Mao, and X.-J. Zeng. 2011. Twitter Mood
Predicts the Stock Market. Journal of Computa-
tional Science, 2(1):1–8, 2011.
Chen Y., S. Lee, S. Li and C. Huang. 2010. Emotion
Cause Detection with Linguistic Constructions. In
Proceedings of COLING-10, pp.179-187.
Frey B. and D. MacKay. 1998. A Revolution: Belief
Propagation in Graphs with Cycles. In Proceedings
of NIPS-98, pp.479–485.
Galik M. and S. Rank. 2012. Modelling Emotional Tra-
jectories of Individuals in an Online Chat. In Pro-
ceedings of Springer-Verlag Berlin Heidelberg-12,
pp.96-105.
Godbole S. and S. Sarawagi. 2004. Discriminative
Methods for Multi-labeled Classification. In Ad-
vances in knowledge discovery and data mining. pp.
22-30.
Kiritchenko S., X. Zhu, and S. Mohammad. 2014. Sen-
timent Analysis of Short Informal Texts. Journal of
Artificial Intelligence Research, 50(2014), pp.723-
762.
Li C., H. Wu, and Q. Jin. 2014. Emotion Classification
of Chinese Miroblog Text via Fusion of BoW and
eVector Feature Representations. In Proceedings of
NLP&amp;CC-14, pp.217-228.
Lin K., C. Yang, and H. Chen. 2008. Emotion Classi-
fication of Online News Articles from the Reader’s
Perspective. In Proceedings of the International
Conference on Web Intelligence and Intelligent
Agent Technology-08, pp.220-226.
Liu H., S. Li, G. Zhou, C. Huang, and P. Li. 2013. Joint
Modeling of News Reader’s and Comment Writer’s
Emotions. In Proceedings of ACL-13, short paper,
pp.511-515.
Quan C. and F. Ren. 2009. Construction of a Blog
Emotion Corpus for Chinese Emotional Expression
Analysis. In Proceedings of EMNLP-09, pp.1446-
1454.
Schapire R. E and Y. Singer. 2000. A Boosting-based
System for Text Categorization. Machine learning,
pp. 135-168
TOKUHISA R., K. Inui, and Y. Matsumoto. 2008.
Emotion Classification Using Massive Examples
Extracted from the Web. In Proceedings of COL-
ING-2008, pp.881-888.
Tsoumakas G. and I. Katakis. 2007. Multi-label Clas-
sification: An Overview. In Proceedings of Interna-
tional Journal of Data Warehousing and Mining,
3(3), pp.1-13.
Tsoumakas G., I. Katakis, and I. Vlahavas. 2009. Min-
ing Multi-label Data. Data Mining and Knowledge
Discovery Handbook, pages 1–19.
Wen S. and X. Wan. 2014. Emotion Classification in
Microblog Texts Using Class Sequential Rules. In
Proceedings of AAAI-14, 187-193.
Wang S., J. Wang, Z. Wang, and Q. Ji. 2014. Enhanc-
ing Multi-label Classification by Modeling Depend-
encies among Labels. Pattern Recognition. Vol. 47.
Issue 10: 3405-3413, 2014.
Wiebe J., T. Wilson, and C. Cardie. 2005. Annotating
Expressions of Opinions and Emotions in Language.
Language Resources and Evaluation, 39, 65-210.
Xu G., X. Meng and H. Wang. 2010. Build Chinese
Emotion Lexicons Using A Graph-based Algorithm
and Multiple Resources. In Proceedings of COL-
ING-10, pp.1209-1217.
Xu J., R. Xu, Q. Lu, and X. Wang. 2012. Coarse-to-
fine Sentence-level Emotion Classification based on
the Intra-sentence Features and Sentential Context.
In Proceedings of CIKM-12, poster, pp.2455-2458.
</reference>
<page confidence="0.550131">
1053
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.165177">
<title confidence="0.616836">Sentence-level Emotion Classification with Label and Context Dependence Language Processing Lab, Soochow University, China Innovation Center of Novel Software Technology and Industrialization</title>
<email confidence="0.59623">shoushan.li@gmail.com,gdzhou@suda.edu.cn</email>
<email confidence="0.59623">lei.huang2013@gmail.com,gdzhou@suda.edu.cn</email>
<email confidence="0.59623">wangrong2022@gmail.com,gdzhou@suda.edu.cn</email>
<abstract confidence="0.997977470588235">emotion categories, such as and expressed by a sentence is challenging due to its inherent multi-label classification difficulty and data sparseness. In this paper, we address above two challenges by incorporating the label dependence among the emotion labels and the context dependence among the contextual instances into a factor graph model. Specifically, we recast sentence-level emotion classification as a factor graph inferring problem in which the label and context dependence are modeled as various factor functions. Empirical evaluation demonstrates the great potential and effectiveness of our proposed approach to sentencelevel emotion classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Alm</author>
<author>D Roth</author>
<author>R Sproat</author>
</authors>
<title>Emotions from Text: Machine Learning for Text-based Emotion Prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP-05,</booktitle>
<pages>579--586</pages>
<contexts>
<context position="6086" citStr="Alm et al., 2005" startWordPosition="933" endWordPosition="936"> context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a huge collection of emotion-provoking event instances from the Web to deal with the data sparseness problem in sentence-level emotion classification. Bhowmick et al. (2009) and Bhowmick et al.</context>
</contexts>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>Alm C., D. Roth and R. Sproat. 2005. Emotions from Text: Machine Learning for Text-based Emotion Prediction. In Proceedings of EMNLP-05, pp.579-586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bhowmick</author>
<author>A Basu</author>
<author>P Mitra</author>
<author>A Prasad</author>
</authors>
<title>Multi-label Text Classification Approach for Sentence Level News Emotion Analysis. Pattern Recognition and Machine Intelligence.</title>
<date>2009</date>
<journal>Lecture Notes in Computer Science,</journal>
<volume>5909</volume>
<pages>261--266</pages>
<contexts>
<context position="6213" citStr="Bhowmick et al., 2009" startWordPosition="952" endWordPosition="955"> evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a huge collection of emotion-provoking event instances from the Web to deal with the data sparseness problem in sentence-level emotion classification. Bhowmick et al. (2009) and Bhowmick et al. (2010) apply KNN-based classification algorithms to classify news sentences into multiple reader emotion categories. Although </context>
</contexts>
<marker>Bhowmick, Basu, Mitra, Prasad, 2009</marker>
<rawString>Bhowmick P., A. Basu, P. Mitra, and A. Prasad. 2009. Multi-label Text Classification Approach for Sentence Level News Emotion Analysis. Pattern Recognition and Machine Intelligence. Lecture Notes in Computer Science, Volume 5909, pp 261-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bhowmick</author>
<author>A Basu</author>
<author>P Mitra</author>
<author>A Prasad</author>
</authors>
<title>Sentence Level News Emotion Analysis in Fuzzy Multi-label Classification Framework. Research in Computing Science. Special Issue: Natural Language Processing and its Applications,</title>
<date>2010</date>
<pages>143--154</pages>
<contexts>
<context position="6693" citStr="Bhowmick et al. (2010)" startWordPosition="1023" endWordPosition="1026">lm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a huge collection of emotion-provoking event instances from the Web to deal with the data sparseness problem in sentence-level emotion classification. Bhowmick et al. (2009) and Bhowmick et al. (2010) apply KNN-based classification algorithms to classify news sentences into multiple reader emotion categories. Although the multi-label classification difficulty has been noticed in their study, the label dependence is not exploited. More recently, Xu et al. (2012) proposes a coarse-to-fine strategy for sentence-level emotion classification. They deal with the data sparseness problem by incorporating the transfer probabilities from the neighboring sentences to refine the emotion categories. To some extent, this can be seen a specific kind of context information. However, they ignore the label </context>
</contexts>
<marker>Bhowmick, Basu, Mitra, Prasad, 2010</marker>
<rawString>Bhowmick P., A. Basu, P. Mitra, and A. Prasad. 2010. Sentence Level News Emotion Analysis in Fuzzy Multi-label Classification Framework. Research in Computing Science. Special Issue: Natural Language Processing and its Applications, pp.143-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bollen</author>
<author>H Mao</author>
<author>X-J Zeng</author>
</authors>
<title>Twitter Mood Predicts the Stock Market.</title>
<date>2011</date>
<journal>Journal of Computational Science,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1322" citStr="Bollen et al., 2011" startWordPosition="180" endWordPosition="183"> into a factor graph model. Specifically, we recast sentence-level emotion classification as a factor graph inferring problem in which the label and context dependence are modeled as various factor functions. Empirical evaluation demonstrates the great potential and effectiveness of our proposed approach to sentencelevel emotion classification. 1 Introduction Predicting emotion categories, such as anger, joy, and anxiety, expressed by a piece of text encompasses a variety of applications, such as online chatting (Galik et al., 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). Over the past decade, there has been a substantial body of research on emotion classification, where a considerable amount of work has focused on document-level emotion classification. Recently, the research community has become increasingly aware of the need on sentence-level emotion classification due to its wide potential applications, e.g. the massively growing importance of analyzing short text in social media (Kiritchenko et al., 2014; Wen and Wan, 2014). In general, sentence-level emotion classification exhibits two challenges. * Corresponding author Figure 1: An example of a paragrap</context>
</contexts>
<marker>Bollen, Mao, Zeng, 2011</marker>
<rawString>Bollen J., H. Mao, and X.-J. Zeng. 2011. Twitter Mood Predicts the Stock Market. Journal of Computational Science, 2(1):1–8, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chen</author>
<author>S Lee</author>
<author>S Li</author>
<author>C Huang</author>
</authors>
<title>Emotion Cause Detection with Linguistic Constructions.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING-10,</booktitle>
<pages>179--187</pages>
<contexts>
<context position="6029" citStr="Chen et al., 2010" startWordPosition="925" endWordPosition="928"> analysis. Section 3 presents our observations on label and context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a huge collection of emotion-provoking event instances from the Web to deal with the data sparseness problem in sentence-level emotion c</context>
</contexts>
<marker>Chen, Lee, Li, Huang, 2010</marker>
<rawString>Chen Y., S. Lee, S. Li and C. Huang. 2010. Emotion Cause Detection with Linguistic Constructions. In Proceedings of COLING-10, pp.179-187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Frey</author>
<author>D MacKay</author>
</authors>
<title>A Revolution: Belief Propagation in Graphs with Cycles.</title>
<date>1998</date>
<booktitle>In Proceedings of NIPS-98,</booktitle>
<pages>479--485</pages>
<contexts>
<context position="16240" citStr="Frey and MacKay, 1998" startWordPosition="2622" endWordPosition="2625">nting the influence degree of the two instan ces and . 4.3 Model Learning Learning the DFG model is to estimate the best parameter configurationto maximize the log-likelihood objective function L(9) =1ogPA (Y , i.e., d*=argmaxL(H) (5) In this study, we employ the gradient decent method to optimize the objective function. For exWhere E C(D (x,, yk)] is the expectation of feature function (D (x,, yk) given the data distribution. Ea YG x   is the expectation of feature 1049 Belief Propagation (LBP) algorithm which is applied to approximately infer the marginal distribution in a factor graph (Frey and MacKay, 1998). A similar gradient can be derived for the other parameters. Ren, 2009). In our experiments, we use 80 documents as the training data and the remaining 20 documents as the test data. Features Input: Learning rate17 Output: Estimated parametersa Initializea0 Repeat 1) Calculate using LBP 2) Calculate using LBP 3) Calculate the gradient of according to Eq. (6) 4) Update parameter with the learning raterl Until Convergence Figure 5: The learning algorithm for DGP model 4.4 Model Prediction With the learned parameter configuration , the prediction task is to find a which optimizes the objective f</context>
</contexts>
<marker>Frey, MacKay, 1998</marker>
<rawString>Frey B. and D. MacKay. 1998. A Revolution: Belief Propagation in Graphs with Cycles. In Proceedings of NIPS-98, pp.479–485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galik</author>
<author>S Rank</author>
</authors>
<title>Modelling Emotional Trajectories of Individuals in an Online Chat.</title>
<date>2012</date>
<booktitle>In Proceedings of Springer-Verlag Berlin Heidelberg-12,</booktitle>
<pages>96--105</pages>
<marker>Galik, Rank, 2012</marker>
<rawString>Galik M. and S. Rank. 2012. Modelling Emotional Trajectories of Individuals in an Online Chat. In Proceedings of Springer-Verlag Berlin Heidelberg-12, pp.96-105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Godbole</author>
<author>S Sarawagi</author>
</authors>
<title>Discriminative Methods for Multi-labeled Classification.</title>
<date>2004</date>
<booktitle>In Advances in knowledge discovery and data mining.</booktitle>
<pages>22--30</pages>
<contexts>
<context position="17843" citStr="Godbole and Sarawagi, 2004" startWordPosition="2871" endWordPosition="2874">rimentation We have systematically evaluated our DFG approach to sentence-level emotion classification. 5.1 Experimental Setting Corpus The corpus contains 100 documents (2751 sentences) from the Ren-CECps corpus (Quan and Each instance is treated as a bag-of-words and transformed into a binary vector encoding the presence or absence of word unigrams. Evaluation Metrics In our study, we employ three evaluation metrics to measure the performances of different approaches to sentence-level emotion classification. These metrics have been popularly used in some multi-label classification problems (Godbole and Sarawagi, 2004; Schapire and Singer, 2000). 1) Hamming loss: It evaluates how many times an instance-label pair is misclassified considering the predicted set of labels and the ground truth set of labels, i.e., (8) where q is the number of all test instances and m is the number of all emotion labels. is the estimated label while is the true label. 2) Accuracy: It gives an average degree of the similarity between the predicted and the ground truth label sets of all test examples, i.e., Accuracy  3) F1-measure: It is the harmonic mean between precision and recall. It can be calculated from true positives, tr</context>
</contexts>
<marker>Godbole, Sarawagi, 2004</marker>
<rawString>Godbole S. and S. Sarawagi. 2004. Discriminative Methods for Multi-labeled Classification. In Advances in knowledge discovery and data mining. pp. 22-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kiritchenko</author>
<author>X Zhu</author>
<author>S Mohammad</author>
</authors>
<title>Sentiment Analysis of Short Informal Texts.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>50</volume>
<issue>2014</issue>
<pages>723--762</pages>
<contexts>
<context position="1768" citStr="Kiritchenko et al., 2014" startWordPosition="246" endWordPosition="250">iece of text encompasses a variety of applications, such as online chatting (Galik et al., 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). Over the past decade, there has been a substantial body of research on emotion classification, where a considerable amount of work has focused on document-level emotion classification. Recently, the research community has become increasingly aware of the need on sentence-level emotion classification due to its wide potential applications, e.g. the massively growing importance of analyzing short text in social media (Kiritchenko et al., 2014; Wen and Wan, 2014). In general, sentence-level emotion classification exhibits two challenges. * Corresponding author Figure 1: An example of a paragraph and the sentences therein with their emotion categories from the corpus collected by Quan and Ren (2009) On one hand, like document-level emotion classification, sentence-level emotion classification is naturally a multi-label classification problem. That is, each sentence might involve more than one emotion category. For example, as shown in Figure 1, in one paragraph, two sentences, i.e., S1 and S3, have two and three emotion categories r</context>
</contexts>
<marker>Kiritchenko, Zhu, Mohammad, 2014</marker>
<rawString>Kiritchenko S., X. Zhu, and S. Mohammad. 2014. Sentiment Analysis of Short Informal Texts. Journal of Artificial Intelligence Research, 50(2014), pp.723-762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Li</author>
<author>H Wu</author>
<author>Q Jin</author>
</authors>
<title>Emotion Classification of Chinese Miroblog Text via Fusion of BoW and eVector Feature Representations.</title>
<date>2014</date>
<booktitle>In Proceedings of NLP&amp;CC-14,</booktitle>
<pages>217--228</pages>
<contexts>
<context position="6104" citStr="Li et al., 2014" startWordPosition="937" endWordPosition="940">e in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a huge collection of emotion-provoking event instances from the Web to deal with the data sparseness problem in sentence-level emotion classification. Bhowmick et al. (2009) and Bhowmick et al. (2010) apply KNN-</context>
</contexts>
<marker>Li, Wu, Jin, 2014</marker>
<rawString>Li C., H. Wu, and Q. Jin. 2014. Emotion Classification of Chinese Miroblog Text via Fusion of BoW and eVector Feature Representations. In Proceedings of NLP&amp;CC-14, pp.217-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lin</author>
<author>C Yang</author>
<author>H Chen</author>
</authors>
<title>Emotion Classification of Online News Articles from the Reader’s Perspective.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Web Intelligence and Intelligent Agent Technology-08,</booktitle>
<pages>220--226</pages>
<contexts>
<context position="5960" citStr="Lin et al., 2008" startWordPosition="912" endWordPosition="915">s organized as follows. Section 2 overviews related work on emotion analysis. Section 3 presents our observations on label and context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a huge collection of emotion-provoking event instances from the Web</context>
</contexts>
<marker>Lin, Yang, Chen, 2008</marker>
<rawString>Lin K., C. Yang, and H. Chen. 2008. Emotion Classification of Online News Articles from the Reader’s Perspective. In Proceedings of the International Conference on Web Intelligence and Intelligent Agent Technology-08, pp.220-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Liu</author>
<author>S Li</author>
<author>G Zhou</author>
<author>C Huang</author>
<author>P Li</author>
</authors>
<title>Joint Modeling of News Reader’s and Comment Writer’s Emotions.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL-13,</booktitle>
<pages>511--515</pages>
<contexts>
<context position="1280" citStr="Liu et al., 2013" startWordPosition="173" endWordPosition="176">pendence among the contextual instances into a factor graph model. Specifically, we recast sentence-level emotion classification as a factor graph inferring problem in which the label and context dependence are modeled as various factor functions. Empirical evaluation demonstrates the great potential and effectiveness of our proposed approach to sentencelevel emotion classification. 1 Introduction Predicting emotion categories, such as anger, joy, and anxiety, expressed by a piece of text encompasses a variety of applications, such as online chatting (Galik et al., 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). Over the past decade, there has been a substantial body of research on emotion classification, where a considerable amount of work has focused on document-level emotion classification. Recently, the research community has become increasingly aware of the need on sentence-level emotion classification due to its wide potential applications, e.g. the massively growing importance of analyzing short text in social media (Kiritchenko et al., 2014; Wen and Wan, 2014). In general, sentence-level emotion classification exhibits two challenges. * Corresponding</context>
<context position="5979" citStr="Liu et al., 2013" startWordPosition="916" endWordPosition="919">lows. Section 2 overviews related work on emotion analysis. Section 3 presents our observations on label and context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a huge collection of emotion-provoking event instances from the Web to deal with the d</context>
</contexts>
<marker>Liu, Li, Zhou, Huang, Li, 2013</marker>
<rawString>Liu H., S. Li, G. Zhou, C. Huang, and P. Li. 2013. Joint Modeling of News Reader’s and Comment Writer’s Emotions. In Proceedings of ACL-13, short paper, pp.511-515.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quan</author>
<author>F Ren</author>
</authors>
<title>Construction of a Blog Emotion Corpus for Chinese Emotional Expression Analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP-09,</booktitle>
<pages>1446--1454</pages>
<contexts>
<context position="2028" citStr="Quan and Ren (2009)" startWordPosition="287" endWordPosition="290">fication, where a considerable amount of work has focused on document-level emotion classification. Recently, the research community has become increasingly aware of the need on sentence-level emotion classification due to its wide potential applications, e.g. the massively growing importance of analyzing short text in social media (Kiritchenko et al., 2014; Wen and Wan, 2014). In general, sentence-level emotion classification exhibits two challenges. * Corresponding author Figure 1: An example of a paragraph and the sentences therein with their emotion categories from the corpus collected by Quan and Ren (2009) On one hand, like document-level emotion classification, sentence-level emotion classification is naturally a multi-label classification problem. That is, each sentence might involve more than one emotion category. For example, as shown in Figure 1, in one paragraph, two sentences, i.e., S1 and S3, have two and three emotion categories respectively. Automatically classifying instances with multiple possible categories is ...... 脸Air&amp;quot;&lt;/S1&gt; &lt;S2&gt; 现在我终于如愿以偿。&lt;/S2&gt; &lt;S3&gt;感受着小手 的温度,享受着这份她对我的依恋,生怕动 一下,会让她的小手离我而去。&lt;/S3&gt; (English: &lt;S1&gt; The girls fall to sleep, so I make my way noiselessly onto the bed, wi</context>
<context position="5876" citStr="Quan and Ren, 2009" startWordPosition="898" endWordPosition="901">e-level emotion classification into a unified framework. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion analysis. Section 3 presents our observations on label and context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog syste</context>
<context position="8338" citStr="Quan and Ren (2009)" startWordPosition="1259" endWordPosition="1262">e performance of sentence-level emotion classification. 3 Observations To better illustrate our motivation of modeling the label and context dependence, we systematically investigate both dependence phenomena in our evaluation corpus. 1046 Figure 2: Probability distribution of most and least frequently-occurred pairs of emotion categories, with left four most frequently-occurred and right four least frequently-occurred, among all 28 pairs 0.2 0.183 0.005 0.003 0.002 0.0003 0.078 0.077 0.18 0.16 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0 0.094 The corpus contains 100 documents, randomly selected from Quan and Ren (2009). There are totally 2751 sentences and each of them is manually annotated with one or more emotion labels. Table 1: The numbers of the sentences in each emotion category Emotion #Sentence Emotion #Sentence joy 691 anxiety 567 hate 532 surprise 180 love 1025 anger 287 sorrow 611 expect 603 Table 2: The numbers of the sentences grouped by the emotion labels they contain #Sentence No Label 180 One Label 1096 Two Labels 1081 Three Labels 346 Four or more labels 48 ALL 2751 Table 1 shows the sentence distribution of the eight emotion categories. Obviously, the distribution is a bit imbalanced. Whil</context>
</contexts>
<marker>Quan, Ren, 2009</marker>
<rawString>Quan C. and F. Ren. 2009. Construction of a Blog Emotion Corpus for Chinese Emotional Expression Analysis. In Proceedings of EMNLP-09, pp.1446-1454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>A Boosting-based System for Text Categorization. Machine learning,</title>
<date>2000</date>
<pages>135--168</pages>
<contexts>
<context position="17871" citStr="Schapire and Singer, 2000" startWordPosition="2875" endWordPosition="2878">ically evaluated our DFG approach to sentence-level emotion classification. 5.1 Experimental Setting Corpus The corpus contains 100 documents (2751 sentences) from the Ren-CECps corpus (Quan and Each instance is treated as a bag-of-words and transformed into a binary vector encoding the presence or absence of word unigrams. Evaluation Metrics In our study, we employ three evaluation metrics to measure the performances of different approaches to sentence-level emotion classification. These metrics have been popularly used in some multi-label classification problems (Godbole and Sarawagi, 2004; Schapire and Singer, 2000). 1) Hamming loss: It evaluates how many times an instance-label pair is misclassified considering the predicted set of labels and the ground truth set of labels, i.e., (8) where q is the number of all test instances and m is the number of all emotion labels. is the estimated label while is the true label. 2) Accuracy: It gives an average degree of the similarity between the predicted and the ground truth label sets of all test examples, i.e., Accuracy  3) F1-measure: It is the harmonic mean between precision and recall. It can be calculated from true positives, true negatives, false positive</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Schapire R. E and Y. Singer. 2000. A Boosting-based System for Text Categorization. Machine learning, pp. 135-168</rawString>
</citation>
<citation valid="true">
<authors>
<author>R TOKUHISA</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Emotion Classification Using Massive Examples Extracted from the Web. In</title>
<date>2008</date>
<booktitle>Proceedings of COLING-2008,</booktitle>
<pages>881--888</pages>
<marker>TOKUHISA, Inui, Matsumoto, 2008</marker>
<rawString>TOKUHISA R., K. Inui, and Y. Matsumoto. 2008. Emotion Classification Using Massive Examples Extracted from the Web. In Proceedings of COLING-2008, pp.881-888.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tsoumakas</author>
<author>I Katakis</author>
</authors>
<title>Multi-label Classification: An Overview.</title>
<date>2007</date>
<booktitle>In Proceedings of International Journal of Data Warehousing and Mining,</booktitle>
<volume>3</volume>
<issue>3</issue>
<pages>1--13</pages>
<contexts>
<context position="12875" citStr="Tsoumakas and Katakis, 2007" startWordPosition="2023" endWordPosition="2027">o layers of nodes, i.e., variable nodes and factor nodes, with links between them. The joint distribution over the whole set of variables can be factorized as a product of all factors. Figure 4 gives an example of our dependence factor graph (DFG) when two instances, i.e., sentence-1 and sentence-2 are involved. Binary Relevance A popular solution to multi-label classification is called binary relevance which constructs a binary classifier for each label, resulting a set of inde0.8 0.68 0.69 0.6 0.5 0.4 0.2 0 0.22 Neighbor Paragraph Document Random 1048 pendent binary classification problems (Tsoumakas and Katakis, 2007; Tsoumakas et al., 2009). In our approach, binary relevance is utilized as a preliminary step so that each original instance is transformed into K pseudo samples, where K is the number of categories. For example, in Figure 4, , , and represent the three pseudo samples, generated from the same original instance sentence-1. 4.2 Model Definition Formally, let G = (V, E, X) represent an instance network, where V denotes a set of sentence instances. is a set of relationships between sentences. Two kinds of relationship exist in our instance network: One represents the label dependence between each</context>
</contexts>
<marker>Tsoumakas, Katakis, 2007</marker>
<rawString>Tsoumakas G. and I. Katakis. 2007. Multi-label Classification: An Overview. In Proceedings of International Journal of Data Warehousing and Mining, 3(3), pp.1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tsoumakas</author>
<author>I Katakis</author>
<author>I Vlahavas</author>
</authors>
<title>Mining Multi-label Data. Data Mining and Knowledge Discovery Handbook,</title>
<date>2009</date>
<pages>1--19</pages>
<contexts>
<context position="12900" citStr="Tsoumakas et al., 2009" startWordPosition="2028" endWordPosition="2031">able nodes and factor nodes, with links between them. The joint distribution over the whole set of variables can be factorized as a product of all factors. Figure 4 gives an example of our dependence factor graph (DFG) when two instances, i.e., sentence-1 and sentence-2 are involved. Binary Relevance A popular solution to multi-label classification is called binary relevance which constructs a binary classifier for each label, resulting a set of inde0.8 0.68 0.69 0.6 0.5 0.4 0.2 0 0.22 Neighbor Paragraph Document Random 1048 pendent binary classification problems (Tsoumakas and Katakis, 2007; Tsoumakas et al., 2009). In our approach, binary relevance is utilized as a preliminary step so that each original instance is transformed into K pseudo samples, where K is the number of categories. For example, in Figure 4, , , and represent the three pseudo samples, generated from the same original instance sentence-1. 4.2 Model Definition Formally, let G = (V, E, X) represent an instance network, where V denotes a set of sentence instances. is a set of relationships between sentences. Two kinds of relationship exist in our instance network: One represents the label dependence between each two pseudo instances gen</context>
</contexts>
<marker>Tsoumakas, Katakis, Vlahavas, 2009</marker>
<rawString>Tsoumakas G., I. Katakis, and I. Vlahavas. 2009. Mining Multi-label Data. Data Mining and Knowledge Discovery Handbook, pages 1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wen</author>
<author>X Wan</author>
</authors>
<title>Emotion Classification in Microblog Texts Using Class Sequential Rules.</title>
<date>2014</date>
<booktitle>In Proceedings of AAAI-14,</booktitle>
<pages>187--193</pages>
<contexts>
<context position="1788" citStr="Wen and Wan, 2014" startWordPosition="251" endWordPosition="254"> variety of applications, such as online chatting (Galik et al., 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). Over the past decade, there has been a substantial body of research on emotion classification, where a considerable amount of work has focused on document-level emotion classification. Recently, the research community has become increasingly aware of the need on sentence-level emotion classification due to its wide potential applications, e.g. the massively growing importance of analyzing short text in social media (Kiritchenko et al., 2014; Wen and Wan, 2014). In general, sentence-level emotion classification exhibits two challenges. * Corresponding author Figure 1: An example of a paragraph and the sentences therein with their emotion categories from the corpus collected by Quan and Ren (2009) On one hand, like document-level emotion classification, sentence-level emotion classification is naturally a multi-label classification problem. That is, each sentence might involve more than one emotion category. For example, as shown in Figure 1, in one paragraph, two sentences, i.e., S1 and S3, have two and three emotion categories respectively. Automat</context>
</contexts>
<marker>Wen, Wan, 2014</marker>
<rawString>Wen S. and X. Wan. 2014. Emotion Classification in Microblog Texts Using Class Sequential Rules. In Proceedings of AAAI-14, 187-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
<author>J Wang</author>
<author>Z Wang</author>
<author>Q Ji</author>
</authors>
<title>Enhancing Multi-label Classification by Modeling Dependencies among Labels. Pattern Recognition.</title>
<date>2014</date>
<volume>47</volume>
<pages>3405--3413</pages>
<contexts>
<context position="19931" citStr="Wang et al., 2014" startWordPosition="3204" endWordPosition="3207">ghbor) DFG-context(Paragraph) DFG-context(Document) 0.472 0.416 0.45 0.569 0.378 0.382 0.443 0.407 0.295 0.261 0.264 0.292 0.275 0.215 5.2 Experimental Results with Label Dependence In this section, we compare following approaches which only consider the label dependence among pseudo instances: ➢ Baseline: As a baseline, this approach applies a maximum entropy (ME) classifier with only textual features, ignoring both the label and context dependence. ➢ LabelD: As the state-of-the-art approach to handling multi-label classification, this approach incorporates label dependence, as described in (Wang et al., 2014). Specifically, this approach first utilizes a Bayesian network to infer the relationship among the labels and then employ them in the classifier. ➢ DFG-label: Our DFG approach with the label dependence. Figure 6 compares the performance of different approaches to sentence-level emotion classification with the label dependence. From this figure, we can see that our DFG approach improves the baseline approach with an impressive improvement in all three kinds of evaluation metrics, i.e., 23.5% reduction in Hloss, 25.6% increase in Accuracy, and 11.8% increase in F1. This result verifies the effe</context>
</contexts>
<marker>Wang, Wang, Wang, Ji, 2014</marker>
<rawString>Wang S., J. Wang, Z. Wang, and Q. Ji. 2014. Enhancing Multi-label Classification by Modeling Dependencies among Labels. Pattern Recognition. Vol. 47. Issue 10: 3405-3413, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<date>2005</date>
<booktitle>Annotating Expressions of Opinions and Emotions in Language. Language Resources and Evaluation,</booktitle>
<volume>39</volume>
<pages>65--210</pages>
<contexts>
<context position="5856" citStr="Wiebe et al., 2005" startWordPosition="894" endWordPosition="897">ependence of sentence-level emotion classification into a unified framework. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion analysis. Section 3 presents our observations on label and context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance senten</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Wiebe J., T. Wilson, and C. Cardie. 2005. Annotating Expressions of Opinions and Emotions in Language. Language Resources and Evaluation, 39, 65-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Xu</author>
<author>X Meng</author>
<author>H Wang</author>
</authors>
<title>Build Chinese Emotion Lexicons Using A Graph-based Algorithm and Multiple Resources.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING-10,</booktitle>
<pages>1209--1217</pages>
<contexts>
<context position="5894" citStr="Xu et al., 2010" startWordPosition="902" endWordPosition="905">sification into a unified framework. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion analysis. Section 3 presents our observations on label and context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a</context>
</contexts>
<marker>Xu, Meng, Wang, 2010</marker>
<rawString>Xu G., X. Meng and H. Wang. 2010. Build Chinese Emotion Lexicons Using A Graph-based Algorithm and Multiple Resources. In Proceedings of COLING-10, pp.1209-1217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>R Xu</author>
<author>Q Lu</author>
<author>X Wang</author>
</authors>
<title>Coarse-tofine Sentence-level Emotion Classification based on the Intra-sentence Features and Sentential Context.</title>
<date>2012</date>
<booktitle>In Proceedings of CIKM-12,</booktitle>
<pages>2455--2458</pages>
<contexts>
<context position="6231" citStr="Xu et al., 2012" startWordPosition="956" endWordPosition="959"> approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a huge collection of emotion-provoking event instances from the Web to deal with the data sparseness problem in sentence-level emotion classification. Bhowmick et al. (2009) and Bhowmick et al. (2010) apply KNN-based classification algorithms to classify news sentences into multiple reader emotion categories. Although the multi-label cl</context>
<context position="21640" citStr="Xu et al., 2012" startWordPosition="3462" endWordPosition="3465">not capture the information between two conflict emotion labels, such as “happy” and “sad” (they are not possibly appearing together). 5.3 Experimental Results with Context Dependence In this section, we compare following approaches which only consider the context dependence among pseudo instances: 1051 ➢ Baseline: same as the one in Section 5.2, which applies a maximum entropy (ME) classifier with only textual features, ignoring both the label and context dependence. ➢ Transfer: As the state-of-the-art approach to incorporating contextual information in sentence-level emotion classification (Xu et al., 2012), this approach utilizes the label transformation probability to refine the classification results. ➢ DFG-label (Neighbor): Our DFG approach with the context dependence only. Specifically, the neighboring instances are considered as context. ➢ DFG-label (Paragraph): Our DFG approach with the context dependence only. Specifically, the instances in the same paragraph are considered as context. ➢ DFG-label (Document): Our DFG approach with the context dependence only. Specifically, the instances in the same document are considered as context. Figure 7 compares the performance of different approac</context>
</contexts>
<marker>Xu, Xu, Lu, Wang, 2012</marker>
<rawString>Xu J., R. Xu, Q. Lu, and X. Wang. 2012. Coarse-tofine Sentence-level Emotion Classification based on the Intra-sentence Features and Sentential Context. In Proceedings of CIKM-12, poster, pp.2455-2458.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>