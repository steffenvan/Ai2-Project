<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000287">
<title confidence="0.9991435">
Manifold Learning for the Semi-Supervised Induction
of FrameNet Predicates: An Empirical Investigation
</title>
<author confidence="0.829631">
Danilo Croce and Daniele Previtali
</author>
<affiliation confidence="0.797230666666667">
{croce,previtali}@info.uniroma2.it
Department of Computer Science, Systems and Production
University of Roma, Tor Vergata
</affiliation>
<sectionHeader confidence="0.978809" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99962684">
This work focuses on the empirical inves-
tigation of distributional models for the
automatic acquisition of frame inspired
predicate words. While several seman-
tic spaces, both word-based and syntax-
based, are employed, the impact of ge-
ometric representation based on dimen-
sionality reduction techniques is inves-
tigated. Data statistics are accordingly
studied along two orthogonal perspectives:
Latent Semantic Analysis exploits global
properties while Locality Preserving Pro-
jection emphasizes the role of local reg-
ularities. This latter is employed by em-
bedding prior FrameNet-derived knowl-
edge in the corresponding non-euclidean
transformation. The empirical investiga-
tion here reported sheds some light on the
role played by these spaces as complex
kernels for supervised (i.e. Support Vector
Machine) algorithms: their use configures,
as a novel way to semi-supervised lexical
learning, a highly appealing research di-
rection for knowledge rich scenarios like
FrameNet-based semantic parsing.
</bodyText>
<sectionHeader confidence="0.999107" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999526056603774">
Automatic Semantic Role Labeling (SRL) is a
natural language processing (NLP) technique that
maps sentences to semantic representations and
identifies the semantic roles conveyed by senten-
tial constituents (Gildea and Jurafsky, 2002). Sev-
eral NLP applications have exploited this kind of
semantic representation ranging from Information
Extraction (Surdeanu et al., 2003; Moschitti et al.,
2003)) to Question Answering (Shen and Lapata,
2007), Paraphrase Identification (Pado and Erk,
2005), and the modeling of Textual Entailment re-
lations (Tatu and Moldovan, 2005). Large scale
annotated resources have been used by Seman-
tic Role Labeling methods: they are commonly
developed using a supervised learning paradigm
where a classifier learns to predict role labels
based on features extracted from annotated train-
ing data. One prominent resource has been de-
veloped under the Berkeley FrameNet project as
a semantic lexicon for the core vocabulary of En-
glish, according to the so-called frame seman-
tic model (Fillmore, 1985). Here, a frame is a
conceptual structure modeling a prototypical sit-
uation, evoked in texts through the occurrence of
its lexical units (LU) that linguistically expresses
the situation of the frame. Lexical units of the
same frame share semantic arguments. For ex-
ample, the frame KILLING has lexical units such
as assassin, assassinate, blood-bath, fatal, mur-
derer, kill or suicide that share semantic arguments
such as KILLER, INSTRUMENT, CAUSE, VICTIM.
The current FrameNet release contains about 700
frames and 10,000 LUs. A corpus of 150,000 an-
notated examples sentences, from the British Na-
tional Corpus (BNC), is also part of FrameNet.
Despite the size of this resource, it is un-
der development and hence incomplete: several
frames are not represented by evoking words and
the number of annotated sentences is unbalanced
across frames. It is one of the main reason for the
performance drop of supervised SRL systems in
out-of-domain scenarios (Baker et al., 2007) (Jo-
hansson and Nugues, 2008). The limited cover-
age of FrameNet corpus is even more noticeable
for the LUs dictionary: it only contains 10,000
lexical units, far less than the 210,000 entries in
WordNet 3.0. For example, the lexical unit crown,
according to the annotations, evokes the ACCOU-
TREMENT frame. It refers to a particular sense:
according to WordNet, it is “an ornamental jew-
eled headdress signifying sovereignty”. Accord-
ing to the same lexical resource, this LU has 12
lexical senses and the first one (i.e. “The Crown
</bodyText>
<page confidence="0.995072">
7
</page>
<note confidence="0.979332">
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 7–16,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999941867924529">
(or the reigning monarch) as the symbol of the
power and authority of a monarchy”) could evoke
other frames, like LEADERSHIP. In (Pennacchiotti
et al., 2008) and (De Cao et al., 2008), the prob-
lem of LU automatic induction has been treated
in a semi-supervised fashion. First, LUs are mod-
eled by exploiting the distributional analysis of an
unannotated corpus and the lexical information of
WordNet. These representations were used in or-
der to find out frames potentially evoked by novel
words in order to extend the FrameNet dictionary
limiting the effort of manual annotations.
In this work the distributional model of LUs
is further developed. As in (Pennacchiotti et al.,
2008), several word spaces (Pado and Lapata,
2007) are investigated in order to find the most
suitable representation of the properties which
characterize a frame. Two dimensionality reduc-
tion techniques are applied here in this context.
Latent Semantic Analysis (Landauer and Dumais,
1997) uses the Singular Value Decomposition to
find the best subspace approximation of the orig-
inal word space, in the sense of minimizing the
global reconstruction error projecting data along
the directions of maximal variance. Locality Pre-
serving Projection (He and Niyogi, 2003) is a
linear approximation of the nonlinear Laplacian
Eigenmap algorithm: its locality preserving prop-
erties allows to add a set of constraints forcing
LUs that belong to the same frame to be near in
the resulting space after the transformation. LSA
performs a global analysis of a corpus capturing
relations between LUs and removing the noise in-
troduced by spurious directions. However it risks
to ignore lexical senses poorly represented into the
corpus. In (De Cao et al., 2008) external knowl-
edge about LUs is provided by their lexical senses
from a lexical resource (e.g WordNet). In this
work, prior knowledge about the target problem is
directly embedded into the space through the LPP
transformation, by exploiting locality constraints.
Then a Support Vector Machine is employed to
provide a robust acquisition of lexical units com-
bining global information provided by LSA and
the local information provided by LPP into a com-
plex kernel function.
In Section 2 related work is presented. In Sec-
tions 3 the investigated distributional model of
LUs is presented as well as the dimensionality re-
duction techniques. Then, in Section 4 the exper-
imental investigation and comparative evaluations
are reported. Finally, in Section 5 we draw final
conclusions and outline future work.
</bodyText>
<sectionHeader confidence="0.999344" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999927744680851">
As defined in (Pennacchiotti et al., 2008), LU in-
duction is the task of assigning a generic lexical
unit not yet present in the FrameNet database (the
so-called unknown LU) to the correct frame(s).
The number of possible classes (i.e. frames) and
the multiple assignment problem make it a chal-
lenging task. LU induction has been integrated
at SemEval-2007 as part of the Frame Seman-
tic Structure Extraction shared task (Baker et al.,
2007), where systems are requested to assign the
correct frame to a given LU, even when the LU is
not yet present in FrameNet. Several approaches
show low coverage (Johansson and Nugues, 2007)
or low accuracy, like (Burchardt et al., 2005). This
task is presented in (Pennacchiotti et al., 2008) and
(De Cao et al., 2008), where two different mod-
els which combine distributional and paradigmatic
(i.e. lexical) information have been discussed. The
distributional model is used to select a list of frame
suggested by the corpus’ evidences and then the
plausible lexical senses of the unknown LU are
used to re-rank proposed frames.
In order to exploit prior information provided
by the frame theory, the idea underlying is that se-
mantic knowledge can be embedded from exter-
nal sources (i.e the FrameNet database) into the
distributional model of unannotated corpora. In
(Basu et al., 2006) a limited prior knowledge is ex-
ploited in several clustering tasks, in term of pair-
wise constraints (i.e., pairs of instances labeled
as belonging to same or different clusters). Sev-
eral existing algorithms enhance clustering qual-
ity by applying supervision in the form of con-
straints. These algorithms typically utilize the
pairwise constraints to either modify the clustering
objective function or to learn the clustering distor-
tion measure. The approach discussed in (Basu et
al., 2006) employs Hidden Markov Random Fields
(HMRFs) as a probabilistic generative model for
semi-supervised clustering, providing a principled
framework for incorporating constraint-based su-
pervision into prototype-based clustering.
Another possible approach is to directly embed
the prior-knowledge into data representations. The
main idea is to employ effective and efficient algo-
rithms for constructing nonlinear low-dimensional
manifolds from sample data points embedded
</bodyText>
<page confidence="0.995749">
8
</page>
<bodyText confidence="0.999790142857143">
in high-dimensional spaces. Several algorithms
are defined, including Isometric feature mapping
(ISOMAP) (Tenenbaum et al., 2000), Locally Lin-
ear Embedding (LLE) (Roweis and Saul, 2000),
Local Tangent Space alignment (LTSA) (Zhang
and Zha, 2004) and Locality Preserving Projec-
tion (LPP) (He and Niyogi, 2003) and they have
been successfully applied in several computer vi-
sion and pattern recognition problems. In (Yang
et al., 2006) it is demonstrated that basic nonlinear
dimensionality reduction algorithms, such as LLE,
ISOMAP, and LTSA, can be modified by taking
into account prior information on exact mapping
of certain data points. The sensitivity analysis
of these algorithms shows that prior information
improves stability of the solution. In (Goldberg
and Elhadad, 2009), a strategy to incorporate lexi-
cal features into classification models is proposed.
Another possible approach is the strategy pursued
in recent works on deep learning techniques to
NLP tasks. In (Collobert and Weston, 2008) a
unified architecture for NLP that learns features
relevant to the tasks at hand given very limited
prior knowledge is presented. It embodies the
idea that a multitask learning architecture coupled
with semi-supervised learning can be effectively
applied even to complex linguistic tasks such as
Semantic Role Labeling. In particular, (Collobert
and Weston, 2008) proposes an embedding of lex-
ical information using Wikipedia as source, and
exploits the resulting language model for the mul-
titask learning process. The extensive use of unla-
beled texts allows to achieve a significant level of
lexical generalization in order to better capitalize
on the smaller annotated data sets.
</bodyText>
<sectionHeader confidence="0.897577" genericHeader="method">
3 Geometrical Embeddings as models of
</sectionHeader>
<subsectionHeader confidence="0.831044">
Frame Semantics
</subsectionHeader>
<bodyText confidence="0.99998903125">
The aim of this distributional approach is to model
frames in semantic spaces where words are repre-
sented from the distributional analysis of their co-
occurrences over a corpus. Semantic spaces are
widely used in NLP for representing the meaning
of words or other lexical entities. They have been
successfully applied in several tasks, such as in-
formation retrieval (Salton et al., 1975) and har-
vesting thesauri (Lin, 1998). The fundamental in-
tuition is that the meaning of a word can be de-
scribed by the set of textual contexts in which it
appears (Distributional Hypothesis as described in
(Harris, 1964)), and that words with similar vec-
tors are semantically related. Contexts are words
appearing together with a LU: such a space mod-
els a generic notion of semantic relatedness, i.e.
two LUs spatially close in the space are likely to
be either in paradigmatic or syntagmatic relation
as in (Sahlgren, 2006). Here, LUs delimit sub-
spaces modeling the prototypical semantic of the
corresponding evoked frames and novel LUs can
be induced by exploiting their projections.
Since a semantic space supports the language
in use from the corpus statistics in an unsuper-
vised fashion, vectors representing LUs can be
characterized by different distributions. For exam-
ple, LUs of the frame KILLING, such as blood-
bath, crucify or fratricide, are statistically infe-
rior in a corpus if compared to a wide-spanning
term as kill. Moreover other ambiguous LUs, as
liquidate or terminate, could appear in sentences
evoking different frames. These problems of data-
sparseness and distribution noise can be over-
come by applying space transformation techniques
augmenting the space expressiveness in model-
ing frame semantics. Semantic space models very
elegantly map words in vector spaces (there are
as many dimensions as words in the dictionary)
and LUs collections into distributions of data-
points. Every distribution implicitly expresses two
orthogonal facets: global properties, as the occur-
rence scores computed for terms across the entire
collection (irrespectively from their word senses
or evoking situation) and local regularities, for ex-
ample the existence of subsets of terms that tend to
be used every time a frame manifests. These also
tend to be closer in the space and should be closer
in the transformed space too. Another important
aspect that a transformation could account is exter-
nal semantic information. In the new space, prior
knowledge can be exploited to gather a more regu-
lar LUs representation and a clearer separation be-
tween subspaces representing different frame se-
mantics.
In the following sections the investigated dis-
tributional model of LUs will be discussed. As
many criteria can be adopted to define a LU con-
text, one of the goals of this investigation is to find
a co-occurrence model that better captures the no-
tion of frames, as described in Section 3.1. Then,
two dimensionality reduction techniques, exploit-
ing semantic space distributions to improve frames
representation, are discussed. In Section 3.2 the
role of global properties of data statistics will be
</bodyText>
<page confidence="0.989605">
9
</page>
<bodyText confidence="0.999987">
investigated through the Latent Semantic Analy-
sis while in Section 3.3 the Locality Preserving
Projection algorithm will be discussed in order to
combine prior knowledge about frames with local
regularities of LUs obtained from text.
</bodyText>
<subsectionHeader confidence="0.999784">
3.1 Choosing the space
</subsectionHeader>
<bodyText confidence="0.99986109375">
Different types of context define spaces with dif-
ferent semantic properties. Such spaces model a
generic notion of semantic relatedness. Two LUs
close in the space are likely to be related by some
type of generic semantic relation, either paradig-
matic (e.g. synonymy, hyperonymy, antonymy)
or syntagmatic (e.g. meronymy, conceptual and
phrasal association), as observed in (Sahlgren,
2006). The target of this work is the construc-
tion of a space able to capture the properties which
characterize a frame, assuming those LUs in the
same frame tend to be either co-occurring or sub-
stitutional words (e.g. murder/kill). Two tradi-
tional word-based co-occurrence models capture
the above property:
Word-based space: Contexts are words, as
lemmas, appearing in a n-window of the LU.
The window width n is a parameter that allows
the space to capture different aspects of a frame:
higher values risk to introduce noise, since a frame
could not cover an entire sentence, while lower
values lead to sparse representations.
Syntax-based space: Contexts words are en-
riched through information about syntactic rela-
tions (e.g. X-VSubj-killer where X is the LU), as
described in (Pado and Lapata, 2007). Two LUs
close in the space are likely to be in a paradig-
matic relation, i.e. to be close in an IS-A hierarchy
(Budanitsky and Hirst, 2006; Lin, 1998). Indeed,
as contexts are syntactic relations, targets with the
same part of speech are much closer than targets
of different types.
</bodyText>
<subsectionHeader confidence="0.999915">
3.2 Latent Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.985303658536585">
Latent Semantic Analysis (LSA) is an algorithm
presented in (Furnas et al., 1988) afterwards dif-
fused by Landauer (Landauer and Dumais, 1997):
it can be seen as a variant of the Principal Compo-
nent Analysis idea. LSA aims to find the best sub-
space approximation to the original word space,
in the sense of minimizing the global reconstruc-
tion error projecting data along the directions of
maximal variance. It captures term (semantic)
dependencies by applying a matrix decomposi-
tion process called Singular Value Decomposition
(SVD). The original term-by-term matrix M is
transformed into the product of three new matri-
ces: U, 5, and V so that M = U5VT . Matrix
M is approximated by Ml = Ul5lVT
l in which
only the first l columns of U and V are used, and
only the first l greatest singular values are consid-
ered. This approximation supplies a way to project
term vectors into the l-dimensional space using
Yterms = Ul51/2
l . Notice that the SVD process
accounts for the eigenvectors of the entire original
distribution (matrix M). LSA is thus an example
of a decomposition process strongly dependent on
a global property. The original statistical informa-
tion about M is captured by the new l-dimensional
space which preserves the global structure while
removing low-variant dimensions, i.e. distribu-
tion noise. These newly derived features may be
thought of as artificial concepts, each one repre-
senting an emerging meaning component as a lin-
ear combination of many different words (i.e. con-
texts). Such contextual usages can be used instead
of the words to represent texts. This technique has
two main advantages. First, the overall computa-
tional cost of the model is reduced, as similarities
are computed on a space with much fewer dimen-
sions. Secondly, it allows to capture second-order
relations among LUs, thus improving the quality
of the similarity measure.
</bodyText>
<subsectionHeader confidence="0.812606">
3.3 The Locality Preserving Projection
Method
</subsectionHeader>
<bodyText confidence="0.999068421052632">
An alternative to LSA, much tighter to local prop-
erties of data, is the Locality Preserving Projection
(LPP), a linear approximation of the non-linear
Laplacian Eigenmap algorithm introduced in (He
and Niyogi, 2003). LPP is a linear dimensional-
ity reduction method whose goal is, given a set of
LUs x1, x2, .., xm in Rn, to find a transformation
matrix A that maps these m points into a set of
points y1, y2,.., ym in Rk (k « n). LPP achieves
this result through a cascade of processing steps
described hereafter.
Construction of an Adjacency graph. Let G
denote a graph with m nodes. Nodes i and j have
got a weighted connection if vectors xi and xj are
close, according to an arbitrary measure of simi-
larity. There are many ways to build an adjacency
graph. The cosine graph with cosine weighting
scheme is explored: given two vectors xi and xj,
the weight wij between them is set by
</bodyText>
<equation confidence="0.995547">
wij = max{0, cos(xi, xj) − τ
|cos(xi, xj) − τ |� cos(xi, xj)� (1)
</equation>
<page confidence="0.920228">
10
</page>
<bodyText confidence="0.999528416666667">
where a cosine threshold T is necessary. The ad-
jacency graph can be represented by using a sym-
metric mxm adjacency matrix, named W, whose
element Wij contains the weight between nodes i
and j. The method of constructing an adjacency
graph outlined above is correct if the data actually
lie on a low dimensional manifold. Once such an
adjacency graph is obtained, LPP will try to opti-
mally preserve it in choosing projections.
Solve an Eigenmap problem. Compute the
eigenvectors and eigenvalues for the generalized
eigenvector problem:
</bodyText>
<equation confidence="0.515133">
XLXTa = AXDXTa
</equation>
<bodyText confidence="0.999353939393939">
where X is a n x m matrix whose columns are the
original m vectors in Rn, D is a diagonal m x m
matrix whose entries are column (or row) sums of
W, Dii = Ej Wij and L = D − W is the Lapla-
cian matrix. The solution of this problem is the
set of eigenvectors a0, a1, .., an−1, ordered accord-
ing to their eigenvalues A0 &lt; a1 &lt; .. &lt; An−1.
LPP projection matrix A is obtained by selecting
the k eigenvectors corresponding to the k smallest
eigenvalues: therefore it is a n x k matrix whose
columns are the selected n-dimensional k eigen-
vectors. Final projection of original vectors into
Rk can be linearly performed by Y = ATX. This
transformation provides a valid kernel that can be
efficiently embedded into a classifier.
Embedding predicate knowledge through
LPPs. While LSA finds a projection, according to
the global properties of the space, LPP tries to pre-
serve the local structures of the data. LPP exploits
the adjacency graph in order to represent neigh-
borhood information. It computes a transforma-
tion matrix which maps data points into a lower di-
mensional subspace. As the construction of an ad-
jacency graph G can be based on any principle, its
definition could account on some external infor-
mation reflecting prior knowledge available about
the task.
In this work, prior knowledge about LUs is em-
bedded by exploiting their membership to frame
dictionaries, thus removing from the graph all con-
nections between LUs xi and xj that do not evoke
the same prototypical situation. More formally
Equation 1 can be rewritten more formally as:
</bodyText>
<equation confidence="0.996061">
wij = max{0,  |cos (os
is xj) − τ |· cos(xi, xj) · δ(i, j)�
</equation>
<bodyText confidence="0.796228">
where
</bodyText>
<equation confidence="0.804908">
� 1 iff 3F s.t. LUi E F ∧ LUj E F
δ(i, j) = 0 otherwise
</equation>
<bodyText confidence="0.999957444444445">
so the resulting manifold keeps close all LUs
evoking the same frame. Since the number of con-
nections could introduce too many constraints to
the Eigenmap problem, a threshold is introduced
to avoid the space collapse: for each LU, only
the most-similar c connections are selected. The
adoption of the proper a priori knowledge about
the target task can be thus seen as a promising re-
search direction.
</bodyText>
<sectionHeader confidence="0.992837" genericHeader="method">
4 Empirical Analysis
</sectionHeader>
<bodyText confidence="0.999977756756756">
In this section the empirical evaluation of distribu-
tional models applied to the task of inducing LUs
is presented. Different spaces obtained through
the dimensionality reduction techniques imply dif-
ferent kernel functions used to independently train
different SVMs. Our aim is to investigate the im-
pact of these kernels in capturing both the frames
and LUs’ properties, as well as the effectiveness
of their possible combination.
The problem of LUs’ induction is here treated
as a multi-classification problem, where each LU
is considered as a positive or negative instance of a
frame. We use Support Vector Machines (SVMs),
(Joachims, 1999) a maximum-margin classifier
that realizes a linear discriminative model. In case
of not linearly separable examples, convolution
functions O(·) can be used in order to transform
the initial feature space into another one, where a
hyperplane that separates the data with the widest
margin can be found. Here new similarity mea-
sures, the kernel functions, can be defined through
the dot-product K(oi, oj) = (O(oi) · O(oj)) over
the new representation. In this way, kernel func-
tions KLSA and KLPP can be induced through
the dimensionality reduction techniques OLSA and
OLPP respectively, as described in sections 3.2
and 3.3. Kernel methods are advantageous be-
cause the combination of of kernel functions can
be integrated into the SVM as they are still kernels.
Consequently, the kernel combination aKLSA +
QKLPP linearly combines the global properties
captured by LSA and the locality constraints im-
posed by the LPP transformation. Here, parame-
ters a and Q weight the combination of the two
kernels. The evoking frame for a novel LU is
the one whose corresponding SVM has the high-
est (possibly negative) margin, according to a one-
</bodyText>
<page confidence="0.998819">
11
</page>
<table confidence="0.99798675">
train tune test overall
max 107 35 34 176
avg 28 8 8 44
total 2466 722 723 3911
</table>
<tableCaption confidence="0.997612">
Table 1: Number of LU examples for each data set
</tableCaption>
<bodyText confidence="0.971970357142857">
from the 100 frames
vs-all scheme. In order to evaluate the quality of
the presented models, accuracy is measured as the
percentage of LUs that are correctly re-assigned to
their original (gold-standard) frame. As the sys-
tem can suggest more than one frame, different
accuracy levels can be obtained. A LU is cor-
rectly assigned if its correct frame (according to
FrameNet) belongs to the set of the best b pro-
posals by the system (i.e. the first b scores from
the underlying SVMs). Assigning different val-
ues to b, we obtained different levels of accuracy
as the percentage of LUs that is correctly assigned
among the first b proposals, as shown in Table 3.
</bodyText>
<subsectionHeader confidence="0.974538">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9996605">
The adopted gold standard is a subset of the
FrameNet database and it consists of the most 100
represented frames in term of annotated examples
and LUs. As the number of example is extremely
unbalanced across frames1, the LUs dictionary of
each selected frame contains at least 10 LUs. It is
a reasonable amount of information for the SVMs
training and it is still a representative data set, be-
ing composed of 3,911 LUs, i.e. the 55% of the
entire dictionary2 of 7,230 evoking words. All
word spaces are derived from the British National
Corpus (BNC), which is underlying FrameNet and
consisting of about 100 million words for English.
Each selected frame is represented into the BNC
by at least 362 annotated sentences, as the lack
of a reasonable number of examples hardly pro-
duces a good distributional model of LUs. Each
frame’s list of LUs is split into train (60%), tuning
(20%) and test set (20%) and LUs having Part-of-
speech different from verb, noun or adjective are
removed. In Table 1 the number of LUs for each
set, as well as the maximum and the average num-
ber per frame, are summarized.
Four different approaches for the Word Space
</bodyText>
<footnote confidence="0.796802833333333">
1For example the SELF MOTION frame counts 6,248 ex-
amples while 119 frames are represented by less than 10 ex-
amples
2The entire database contains 10,228 LUs and the number
of evoking word is 7,230, without taking in account multiple
frame assignments.
</footnote>
<bodyText confidence="0.999930692307692">
construction are used. The first two correspond to
a Word-Based space, the last to a Syntax-Based,
as described in section 3.1:
Window-n (Wn): contextual features correspond
to the set of the 20,000 most frequent lemmatized
words in the BNC. The association measure be-
tween LUs and contexts is the Point-wise Mu-
tual Information (PMI). Valid contexts for LUs are
fixed to a n-window. Hereafter two window width
values will be investigated: Window5 (W5) and
Window10 (W10).
Sentence (Sent): contextual features are the same
above, but the valid contexts are extended to the
entire sentence length.
SyntaxBased (SyntB): contextual features have
been computed according to the “dependency-
based” vector space discussed3 in (Pado and La-
pata, 2007). Observable contexts here are made of
syntactically-typed co-occurrences within depen-
dency graphs built from the entire set of BNC sen-
tences. The most frequent 20,000 basic features,
i.e. (syntactic relation,lemma) pairs, have been
employed as contextual features corresponding to
PMI scores. Syntactic relations are extracted using
the Minipar parser.
Word space models thus focus on the LUs of the
selected 100 frames and the dimensionality have
been reduced by applying LSA and LPP at a new
size of l = 100. Any prior knowledge informa-
tion is provided to the tuning and test sets during
the LPP transformation: the construction of the
reduced feature space takes in account only LUs
from the train set while remaining predicates are
represented through the LPP linear projection. In
these experiments the cosine threshold T and the
maximum number of constraints c are estimated
over the tuning set and the best parametrizations
are shown in Table 2. The adopted implementa-
tion of SVM is SVM-Light-TK 4.
</bodyText>
<subsectionHeader confidence="0.689742">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999907">
In these experiments the impact of the lexical
knowledge gathered by different word-spaces is
evaluated over the LU induction task. Moreover,
the improvements achieved through LSA and LPP
is measured. SVM classifiers are trained over the
semantic spaces produced through the dimension-
</bodyText>
<footnote confidence="0.9899416">
3The Minimal context provided by the De-
pendency Vectors tool is used. It is available at
http://www.nlpado.de/∼sebastian/dv.html
4SVM-Light-TK is available at the url
http://disi.unitn.it/∼moschitt/Tree-Kernel.htm
</footnote>
<page confidence="0.991181">
12
</page>
<table confidence="0.999559333333333">
1.0/0.0 .9/.1 .8/.2 .7/.3 .6/.4 α/β .4/.6 .3/.7 .2/.8 .1/.9 0.0/1.0 τ c
.5/.5
W5 0.668 0.669 0.672 0.673 0.669 0.662 0.649 0.632 0.612 0.570 0.033 0.55 5
W10 0.615 0.619 0.618 0.612 0.604 0.597 0.580 0.575 0.565 0.528 0.048 0.65 3
Sent 0.557 0.567 0.580 0.584 0.574 0.564 0.561 0.545 0.523 0.496 0.048 0.80 5
SyntB 0.654 0.664 0.662 0.652 0.651 0.647 0.649 0.634 0.627 0.592 0.056 0.40 3
</table>
<tableCaption confidence="0.797125">
Table 2: Accuracy at different combination weights of kernel αKLSA + QKLPP (specific baseline is
0.043)
</tableCaption>
<table confidence="0.999853111111111">
b-1 b-2 b-3 b-4 b-5 b-6 b-7 b-8 b-9 b-10 α/β
W5orig 0,563 0,685 0,733 0,770 0,801 0,835 0,841 0,854 0,868 0,879 -
W10orig 0,510 0,634 0,707 0,776 0,810 0,830 0,841 0,857 0,865 0,875 -
Sentorig 0,479 0,618 0,680 0,734 0,764 0,793 0,813 0,837 0,845 0,852 -
SyntBorig 0,585 0,741 0,803 0,840 0,866 0,874 0,886 0,903 0,907 0,913 -
W5LSA+LPP 0.673 0.781 0.831 0.865 0.881 0.891 0.906 0.912 0.926 0.938 0.7/0.3
W 10LSA+LPP 0.619 0.739 0.786 0.818 0.849 0.865 0.878 0.888 0.901 0.909 0.9/0.1
SentLSA+LPP 0.584 0.705 0.766 0.798 0.825 0.835 0.848 0.864 0.876 0.889 0.7/0.3
SyntBLSA+LPP 0.664 0.791 0.840 0.864 0.878 0.893 0.901 0.903 0.907 0.911 0.9/0.1
</table>
<tableCaption confidence="0.9558285">
Table 3: Accuracy of original word-space models (orig) and semantic space models (LSA+LPP) on
best-k proposed frames
</tableCaption>
<bodyText confidence="0.98990384375">
ality reduction transformations. Representations
of both semantic spaces are linearly combined as
αKLSA + QKLPP, where kernel weights α and
Q are estimated over the tuning set. Both ker-
nels are used even without a combination: a ra-
tio α = 1.0/Q = 0.0 denotes the LSA kernel
alone, while α = 0.0/Q = 1.0 the LPP kernel. Ta-
ble 2 shows best results, obtained through a RBF
kernel. The Window5 model achieves the high-
est accuracy, i.e. 67% of correct classification,
where a baseline of 4.3% is estimated assigning
LUs to the most likely frame in the training set (i.e.
the one containing the highest number of LUs).
Wider windows achieve lower classification accu-
racy confirming that most of lexical information
tied to a frame is near the LU. The Syntactic-based
word space does not outperform the accuracy of a
word-based space. The combination of both ker-
nels has always provided the best outcome and the
LSA space seems to be more accurate and expres-
sive respect to the LPP one, as shown in Figure
1. In particular LPP alone is extremely unstable,
suggesting that constraints imposed by the prior
knowledge are orthogonal with respect to the cor-
pus statistics.
Further experiments are carried out using the
original co-occurrence space models, to assess im-
provements due to LSA and LPP kernel. In the
latter investigation linear kernel achieved best re-
sults as confirmed in (Bengio et al., 2005), where
the sensitivity to the curse of dimensionality of
a large class of modern learning algorithms (e.g.
</bodyText>
<figureCaption confidence="0.991538">
Figure 1: Accuracy at different combination
weights of kernel αKLSA + QKLPP
</figureCaption>
<bodyText confidence="0.989853466666667">
SVM) based on local kernels (e.g. RBF) is ar-
gued. As shown in Table 3, the performance drop
of original (orig) models against the best kernel
combination of LSA and LPP are significant,
i.e. - 10%, showing how the latent semantic
spaces better capture properties of frames, avoid-
ing data-sparseness, dimensionality problem and
low-regularities of data-distribution.
Moreover, Table 3 shows how the accuracy level
largely increases when more than one frame is
considered: at a level b = 3, i.e. the novel
LU is correctly classified if one of the original
frames is comprised in the list (of three frames)
proposed by the system, accuracy is 0.84 (i.e the
SyntaxBased model), while at b = 10 accuracy is
</bodyText>
<figure confidence="0.9984275">
0,70
0,65
0,60
0,55
0,50
0,45
0,40
αLSA / βLPP weights
Window5
Window10
Sentence
SyntaxBased
</figure>
<page confidence="0.991464">
13
</page>
<table confidence="0.783945411764706">
frame 1 frame 2 frame 3 Correct frames
FOOD FLUIDIC MOTION CONTAINERS CAUSE HARM
SOUNDS MAKE NOISE COMMUNICATION NOISE BODY MOVEMENT
LEADERSHIP ACCOUTREMENTS PLACING ACCOUTREMENTS
OBSERVABLE BODYPARTS
EDUCATION TEACHING BUILDINGS LOCALE BY USE EDUCATION TEACHING
LOCALE BY USE
AGGREGATE
HOSTILE ENCOUNTER IMPACT COMMITMENT COMMITMENT
TEXT KILLING EMOTION DIRECTED TEXT
LU (# WN3,�3)
boil.v (5)
clap.v (7)
crown.n (12)
school.n (7)
threat.n (4)
tragedy.n (2)
</table>
<tableCaption confidence="0.840375">
Table 4: Proposed 3 frames for each LU (ordered by SVM scores) and correct frames provided by the
FrameNet dictionary. In parenthesis the number of different WordNet lexical senses for each LU.
</tableCaption>
<bodyText confidence="0.999898772727273">
nearly 0.94 (i.e Window5). It is high enough to
support tasks such as the semi-automatic creation
of new FrameNets. An error analysis indicates that
many misclassifications are induced by a lack in
the frame annotations, especially those concern-
ing polysemic LUs5. Table 4 reports the analysis
of a LU subset where the first 3 frames proposed
for each evoking word are shown, ranked by the
margin of the SMVs. The last column contains the
frames evoked by LUs, according to the FrameNet
dictionary, and the frame names in bold suggest
their correct classification. Some LUs, like threat
(characterized by 4 lexical senses) seem to be mis-
classified: in this case the FrameNet annotation
regards a specific sense that evokes the COMMIT-
MENT frame (e.g. “There was a real threat that
she might have to resign”) without taking in ac-
count other senses like WordNet’s “menace, threat
(something that is a source of danger)” that could
evoke the HOSTILE ENCOUNTER frame. In other
cases proposed frames seem to enrich the LUs dic-
tionary, like BUILDINGS, here evoked by school.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99915">
The core purpose of this was to present an em-
pirical investigation of the impact of different dis-
tributional models on the lexical unit induction
task. The employed word-spaces, based on dif-
ferent co-occurrence models (either context and
syntax-driven), are used as vector models of the
LU semantics. On these spaces, two dimensional-
ity reduction techniques have been applied. Latent
Semantic Analysis (LSA) exploits global proper-
ties of data distributions and results in a global
model for lexical semantics. On the other hand,
the Locality Preserving Projection (LPP) method,
that exploits regularities in the neighborhood of
</bodyText>
<footnote confidence="0.6972415">
5According to WordNet, in our dataset an average of 3.6
lexical senses for each LU is estimated.
</footnote>
<bodyText confidence="0.99992128125">
each lexical predicate, is also employed in a semi-
supervised manner: local constraints expressing
prior knowledge on frames are defined in the ad-
jacency graph. The resulting embedding is there-
fore expected to determine a new space where re-
gions for LU of a given frame can be more eas-
ily discovered. Experiments have been run using
the resulting spaces for task dependent kernels in
a SVM learning setting. The application of the
FrameNet KB on the 100 best represented frames
showed that a combined use of the global and lo-
cal models made available by LSA and LPP, re-
spectively, achieves the best results, as the 67.3%
of LUs recovers the same frames of the annotated
dictionary. This is a significant improvement with
respect to previous results achieved by the pure
distributional model reported in (Pennacchiotti et
al., 2008).
Future work is required to increase the level
of constraints made available from the semi-
supervised setting of LPP: syntactic informa-
tion, as well as role-related evidence, can be
both accommodated by the adjacency constraints
imposed for LPP. This constitutes a significant
area of research towards a comprehensive semi-
supervised model of frame semantics, entirely
based on manifold learning methods, of which this
study on LSA and LPP is just a starting point.
Acknowledgement We want to acknowledge
Prof. Roberto Basili because this work would not
exist without his ideas, inspiration and invaluable
support.
</bodyText>
<sectionHeader confidence="0.998841" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990824333333333">
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval-2007 task 19: Frame semantic struc-
ture extraction. In Proceedings of SemEval-2007,
</reference>
<page confidence="0.993663">
14
</page>
<reference confidence="0.998361844660194">
pages 99–104, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Sugato Basu, Mikhail Bilenko, Arindam Banerjee,
and Raymond Mooney. 2006. Probabilistic semi-
supervised clustering with constraints. In Semi-
Supervised Learning, pages 73–102. MIT Press.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le
Roux. 2005. The curse of dimensionality for lo-
cal kernel machines. Technical report, Departement
d’Informatique et Recherche Operationnelle.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of semantic dis-
tance. Computational Linguistics, 32(1):13–47.
Aljoscha Burchardt, Katrin Erk, and Anette Frank.
2005. A WordNet Detour to FrameNet. In
Sprachtechnologie, mobile Kommunikation und lin-
guistische Resourcen, volume 8 of Computer Stud-
ies in Language and Speech. Peter Lang, Frank-
furt/Main.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In In Pro-
ceedings of ICML ’08, pages 160–167, New York,
NY, USA. ACM.
Diego De Cao, Danilo Croce, Marco Pennacchiotti,
and Roberto Basili. 2008. Combining word sense
and usage for modeling frame semantics. In In Pro-
ceedings of STEP 2008, Venice, Italy.
Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 4(2):222–
254.
G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-
dauer, R. A. Harshman, L. A. Streeter, and K. E.
Lochbaum. 1988. Information retrieval using a sin-
gular value decomposition model of latent semantic
structure. In Proc. of SIGIR ’88, New York, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Yoav Goldberg and Michael Elhadad. 2009. On the
role of lexical features in sequence labeling. In In
Proceedings of EMNLP ’09, pages 1142–1151, Sin-
gapore.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics, New York. Oxford University
Press.
Xiaofei He and Partha Niyogi. 2003. Locality preserv-
ing projections. In Proceedings of NIPS03, Vancou-
ver, Canada.
T. Joachims. 1999. Making large-Scale SVM Learning
Practical. MIT Press, Cambridge, MA.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Proceed-
ings of the Workshop on Building Frame-semantic
Resources for Scandinavian and Baltic Languages,
at NODALIDA, Tartu, Estonia, May 24.
Richard Johansson and Pierre Nugues. 2008. The
effect of syntactic representation on semantic role
labeling. In Proceedings of COLING, Manchester,
UK, August 18-22.
Tom Landauer and Sue Dumais. 1997. A solution to
plato’s problem: The latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar word. In Proceedings of COLING-ACL,
Montreal, Canada.
Alessandro Moschitti, Paul Morarescu, and Sanda M.
Harabagiu. 2003. Open domain information ex-
traction via automatic semantic labeling. In FLAIRS
Conference, pages 397–401.
Sebastian Pado and Katrin Erk. 2005. To cause or
not to cause: Cross-lingual semantic matching for
paraphrase modelling. In Proceedings of the Cross-
Language Knowledge Induction Workshop, Cluj-
Napoca, Romania.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of framenet lexical units. In Proceedings
of The Empirical Methods in Natural Language Pro-
cessing (EMNLP 2008) Waikiki, Honolulu, Hawaii.
S.T. Roweis and L.K. Saul. 2000. Nonlinear dimen-
sionality reduction by locally linear embedding. Sci-
ence, 290(5500):2323–2326.
Magnus Sahlgren. 2006. The Word-Space Model.
Ph.D. thesis, Stockholm University.
G. Salton, A. Wong, and C. Yang. 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM, 18:613 ¨Ai620.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceed-
ings of EMNLP-CoNLL, pages 12–21, Prague.
Mihai Surdeanu, , Mihai Surdeanu, A Harabagiu, John
Williams, and Paul Aarseth. 2003. Using predicate-
argument structures for information extraction. In
In Proceedings ofACL 2003.
Marta Tatu and Dan I. Moldovan. 2005. A seman-
tic approach to recognizing textual entailment. In
HLT/EMNLP.
</reference>
<page confidence="0.961459">
15
</page>
<reference confidence="0.999722692307692">
J. B. Tenenbaum, V. Silva, and J. C. Langford. 2000.
A Global Geometric Framework for Nonlinear Di-
mensionality Reduction. Science, 290(5500):2319–
2323.
Xin Yang, Haoying Fu, Hongyuan Zha, and Jesse Bar-
low. 2006. Semi-supervised nonlinear dimension-
ality reduction. In 23rd International Conference
on Machine learning, pages 1065–1072, New York,
NY, USA. ACM Press.
Zhenyue Zhang and Hongyuan Zha. 2004. Princi-
pal manifolds and nonlinear dimensionality reduc-
tion via tangent space alignment. SIAM J. Scientific
Computing, 26(1):313–338.
</reference>
<page confidence="0.998697">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.714558">
<title confidence="0.996248">Manifold Learning for the Semi-Supervised of FrameNet Predicates: An Empirical Investigation</title>
<author confidence="0.957371">Croce</author>
<affiliation confidence="0.992109">Department of Computer Science, Systems and</affiliation>
<address confidence="0.772123">of Roma, Vergata</address>
<abstract confidence="0.998616653846154">This work focuses on the empirical investigation of distributional models for the automatic acquisition of frame inspired predicate words. While several semantic spaces, both word-based and syntaxbased, are employed, the impact of geometric representation based on dimensionality reduction techniques is investigated. Data statistics are accordingly studied along two orthogonal perspectives: Latent Semantic Analysis exploits global properties while Locality Preserving Projection emphasizes the role of local regularities. This latter is employed by embedding prior FrameNet-derived knowledge in the corresponding non-euclidean transformation. The empirical investigation here reported sheds some light on the role played by these spaces as complex kernels for supervised (i.e. Support Vector Machine) algorithms: their use configures, as a novel way to semi-supervised lexical learning, a highly appealing research direction for knowledge rich scenarios like FrameNet-based semantic parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Michael Ellsworth</author>
<author>Katrin Erk</author>
</authors>
<title>Semeval-2007 task 19: Frame semantic structure extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007,</booktitle>
<pages>99--104</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3271" citStr="Baker et al., 2007" startWordPosition="477" endWordPosition="480">atal, murderer, kill or suicide that share semantic arguments such as KILLER, INSTRUMENT, CAUSE, VICTIM. The current FrameNet release contains about 700 frames and 10,000 LUs. A corpus of 150,000 annotated examples sentences, from the British National Corpus (BNC), is also part of FrameNet. Despite the size of this resource, it is under development and hence incomplete: several frames are not represented by evoking words and the number of annotated sentences is unbalanced across frames. It is one of the main reason for the performance drop of supervised SRL systems in out-of-domain scenarios (Baker et al., 2007) (Johansson and Nugues, 2008). The limited coverage of FrameNet corpus is even more noticeable for the LUs dictionary: it only contains 10,000 lexical units, far less than the 210,000 entries in WordNet 3.0. For example, the lexical unit crown, according to the annotations, evokes the ACCOUTREMENT frame. It refers to a particular sense: according to WordNet, it is “an ornamental jeweled headdress signifying sovereignty”. According to the same lexical resource, this LU has 12 lexical senses and the first one (i.e. “The Crown 7 Proceedings of the 2010 Workshop on GEometrical Models of Natural La</context>
<context position="6971" citStr="Baker et al., 2007" startWordPosition="1071" endWordPosition="1074">Then, in Section 4 the experimental investigation and comparative evaluations are reported. Finally, in Section 5 we draw final conclusions and outline future work. 2 Related Work As defined in (Pennacchiotti et al., 2008), LU induction is the task of assigning a generic lexical unit not yet present in the FrameNet database (the so-called unknown LU) to the correct frame(s). The number of possible classes (i.e. frames) and the multiple assignment problem make it a challenging task. LU induction has been integrated at SemEval-2007 as part of the Frame Semantic Structure Extraction shared task (Baker et al., 2007), where systems are requested to assign the correct frame to a given LU, even when the LU is not yet present in FrameNet. Several approaches show low coverage (Johansson and Nugues, 2007) or low accuracy, like (Burchardt et al., 2005). This task is presented in (Pennacchiotti et al., 2008) and (De Cao et al., 2008), where two different models which combine distributional and paradigmatic (i.e. lexical) information have been discussed. The distributional model is used to select a list of frame suggested by the corpus’ evidences and then the plausible lexical senses of the unknown LU are used to</context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>Collin Baker, Michael Ellsworth, and Katrin Erk. 2007. Semeval-2007 task 19: Frame semantic structure extraction. In Proceedings of SemEval-2007, pages 99–104, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sugato Basu</author>
<author>Mikhail Bilenko</author>
<author>Arindam Banerjee</author>
<author>Raymond Mooney</author>
</authors>
<title>Probabilistic semisupervised clustering with constraints.</title>
<date>2006</date>
<booktitle>In SemiSupervised Learning,</booktitle>
<pages>73--102</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7854" citStr="Basu et al., 2006" startWordPosition="1216" endWordPosition="1219">nnacchiotti et al., 2008) and (De Cao et al., 2008), where two different models which combine distributional and paradigmatic (i.e. lexical) information have been discussed. The distributional model is used to select a list of frame suggested by the corpus’ evidences and then the plausible lexical senses of the unknown LU are used to re-rank proposed frames. In order to exploit prior information provided by the frame theory, the idea underlying is that semantic knowledge can be embedded from external sources (i.e the FrameNet database) into the distributional model of unannotated corpora. In (Basu et al., 2006) a limited prior knowledge is exploited in several clustering tasks, in term of pairwise constraints (i.e., pairs of instances labeled as belonging to same or different clusters). Several existing algorithms enhance clustering quality by applying supervision in the form of constraints. These algorithms typically utilize the pairwise constraints to either modify the clustering objective function or to learn the clustering distortion measure. The approach discussed in (Basu et al., 2006) employs Hidden Markov Random Fields (HMRFs) as a probabilistic generative model for semi-supervised clusterin</context>
</contexts>
<marker>Basu, Bilenko, Banerjee, Mooney, 2006</marker>
<rawString>Sugato Basu, Mikhail Bilenko, Arindam Banerjee, and Raymond Mooney. 2006. Probabilistic semisupervised clustering with constraints. In SemiSupervised Learning, pages 73–102. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Olivier Delalleau</author>
<author>Nicolas Le Roux</author>
</authors>
<title>The curse of dimensionality for local kernel machines.</title>
<date>2005</date>
<tech>Technical report, Departement d’Informatique et Recherche Operationnelle.</tech>
<marker>Bengio, Delalleau, Le Roux, 2005</marker>
<rawString>Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. 2005. The curse of dimensionality for local kernel machines. Technical report, Departement d’Informatique et Recherche Operationnelle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of semantic distance.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="15225" citStr="Budanitsky and Hirst, 2006" startWordPosition="2362" endWordPosition="2365">ed space: Contexts are words, as lemmas, appearing in a n-window of the LU. The window width n is a parameter that allows the space to capture different aspects of a frame: higher values risk to introduce noise, since a frame could not cover an entire sentence, while lower values lead to sparse representations. Syntax-based space: Contexts words are enriched through information about syntactic relations (e.g. X-VSubj-killer where X is the LU), as described in (Pado and Lapata, 2007). Two LUs close in the space are likely to be in a paradigmatic relation, i.e. to be close in an IS-A hierarchy (Budanitsky and Hirst, 2006; Lin, 1998). Indeed, as contexts are syntactic relations, targets with the same part of speech are much closer than targets of different types. 3.2 Latent Semantic Analysis Latent Semantic Analysis (LSA) is an algorithm presented in (Furnas et al., 1988) afterwards diffused by Landauer (Landauer and Dumais, 1997): it can be seen as a variant of the Principal Component Analysis idea. LSA aims to find the best subspace approximation to the original word space, in the sense of minimizing the global reconstruction error projecting data along the directions of maximal variance. It captures term (s</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of semantic distance. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
</authors>
<title>A WordNet Detour to FrameNet.</title>
<date>2005</date>
<booktitle>In Sprachtechnologie, mobile Kommunikation und linguistische Resourcen,</booktitle>
<volume>8</volume>
<contexts>
<context position="7205" citStr="Burchardt et al., 2005" startWordPosition="1111" endWordPosition="1114">uction is the task of assigning a generic lexical unit not yet present in the FrameNet database (the so-called unknown LU) to the correct frame(s). The number of possible classes (i.e. frames) and the multiple assignment problem make it a challenging task. LU induction has been integrated at SemEval-2007 as part of the Frame Semantic Structure Extraction shared task (Baker et al., 2007), where systems are requested to assign the correct frame to a given LU, even when the LU is not yet present in FrameNet. Several approaches show low coverage (Johansson and Nugues, 2007) or low accuracy, like (Burchardt et al., 2005). This task is presented in (Pennacchiotti et al., 2008) and (De Cao et al., 2008), where two different models which combine distributional and paradigmatic (i.e. lexical) information have been discussed. The distributional model is used to select a list of frame suggested by the corpus’ evidences and then the plausible lexical senses of the unknown LU are used to re-rank proposed frames. In order to exploit prior information provided by the frame theory, the idea underlying is that semantic knowledge can be embedded from external sources (i.e the FrameNet database) into the distributional mod</context>
</contexts>
<marker>Burchardt, Erk, Frank, 2005</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, and Anette Frank. 2005. A WordNet Detour to FrameNet. In Sprachtechnologie, mobile Kommunikation und linguistische Resourcen, volume 8 of Computer Studies in Language and Speech. Peter Lang, Frankfurt/Main.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In In Proceedings of ICML ’08,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9818" citStr="Collobert and Weston, 2008" startWordPosition="1501" endWordPosition="1504">nd pattern recognition problems. In (Yang et al., 2006) it is demonstrated that basic nonlinear dimensionality reduction algorithms, such as LLE, ISOMAP, and LTSA, can be modified by taking into account prior information on exact mapping of certain data points. The sensitivity analysis of these algorithms shows that prior information improves stability of the solution. In (Goldberg and Elhadad, 2009), a strategy to incorporate lexical features into classification models is proposed. Another possible approach is the strategy pursued in recent works on deep learning techniques to NLP tasks. In (Collobert and Weston, 2008) a unified architecture for NLP that learns features relevant to the tasks at hand given very limited prior knowledge is presented. It embodies the idea that a multitask learning architecture coupled with semi-supervised learning can be effectively applied even to complex linguistic tasks such as Semantic Role Labeling. In particular, (Collobert and Weston, 2008) proposes an embedding of lexical information using Wikipedia as source, and exploits the resulting language model for the multitask learning process. The extensive use of unlabeled texts allows to achieve a significant level of lexica</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In In Proceedings of ICML ’08, pages 160–167, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego De Cao</author>
<author>Danilo Croce</author>
<author>Marco Pennacchiotti</author>
<author>Roberto Basili</author>
</authors>
<title>Combining word sense and usage for modeling frame semantics. In</title>
<date>2008</date>
<booktitle>In Proceedings of STEP 2008,</booktitle>
<location>Venice, Italy.</location>
<marker>De Cao, Croce, Pennacchiotti, Basili, 2008</marker>
<rawString>Diego De Cao, Danilo Croce, Marco Pennacchiotti, and Roberto Basili. 2008. Combining word sense and usage for modeling frame semantics. In In Proceedings of STEP 2008, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frames and the semantics of understanding.</title>
<date>1985</date>
<journal>Quaderni di Semantica,</journal>
<volume>4</volume>
<issue>2</issue>
<pages>254</pages>
<contexts>
<context position="2301" citStr="Fillmore, 1985" startWordPosition="324" endWordPosition="325">n Answering (Shen and Lapata, 2007), Paraphrase Identification (Pado and Erk, 2005), and the modeling of Textual Entailment relations (Tatu and Moldovan, 2005). Large scale annotated resources have been used by Semantic Role Labeling methods: they are commonly developed using a supervised learning paradigm where a classifier learns to predict role labels based on features extracted from annotated training data. One prominent resource has been developed under the Berkeley FrameNet project as a semantic lexicon for the core vocabulary of English, according to the so-called frame semantic model (Fillmore, 1985). Here, a frame is a conceptual structure modeling a prototypical situation, evoked in texts through the occurrence of its lexical units (LU) that linguistically expresses the situation of the frame. Lexical units of the same frame share semantic arguments. For example, the frame KILLING has lexical units such as assassin, assassinate, blood-bath, fatal, murderer, kill or suicide that share semantic arguments such as KILLER, INSTRUMENT, CAUSE, VICTIM. The current FrameNet release contains about 700 frames and 10,000 LUs. A corpus of 150,000 annotated examples sentences, from the British Nation</context>
</contexts>
<marker>Fillmore, 1985</marker>
<rawString>Charles J. Fillmore. 1985. Frames and the semantics of understanding. Quaderni di Semantica, 4(2):222– 254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Furnas</author>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>R A Harshman</author>
<author>L A Streeter</author>
<author>K E Lochbaum</author>
</authors>
<title>Information retrieval using a singular value decomposition model of latent semantic structure.</title>
<date>1988</date>
<booktitle>In Proc. of SIGIR ’88,</booktitle>
<location>New York, USA.</location>
<contexts>
<context position="15480" citStr="Furnas et al., 1988" startWordPosition="2402" endWordPosition="2405"> while lower values lead to sparse representations. Syntax-based space: Contexts words are enriched through information about syntactic relations (e.g. X-VSubj-killer where X is the LU), as described in (Pado and Lapata, 2007). Two LUs close in the space are likely to be in a paradigmatic relation, i.e. to be close in an IS-A hierarchy (Budanitsky and Hirst, 2006; Lin, 1998). Indeed, as contexts are syntactic relations, targets with the same part of speech are much closer than targets of different types. 3.2 Latent Semantic Analysis Latent Semantic Analysis (LSA) is an algorithm presented in (Furnas et al., 1988) afterwards diffused by Landauer (Landauer and Dumais, 1997): it can be seen as a variant of the Principal Component Analysis idea. LSA aims to find the best subspace approximation to the original word space, in the sense of minimizing the global reconstruction error projecting data along the directions of maximal variance. It captures term (semantic) dependencies by applying a matrix decomposition process called Singular Value Decomposition (SVD). The original term-by-term matrix M is transformed into the product of three new matrices: U, 5, and V so that M = U5VT . Matrix M is approximated b</context>
</contexts>
<marker>Furnas, Deerwester, Dumais, Landauer, Harshman, Streeter, Lochbaum, 1988</marker>
<rawString>G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Landauer, R. A. Harshman, L. A. Streeter, and K. E. Lochbaum. 1988. Information retrieval using a singular value decomposition model of latent semantic structure. In Proc. of SIGIR ’88, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1512" citStr="Gildea and Jurafsky, 2002" startWordPosition="202" endWordPosition="205">on-euclidean transformation. The empirical investigation here reported sheds some light on the role played by these spaces as complex kernels for supervised (i.e. Support Vector Machine) algorithms: their use configures, as a novel way to semi-supervised lexical learning, a highly appealing research direction for knowledge rich scenarios like FrameNet-based semantic parsing. 1 Introduction Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Several NLP applications have exploited this kind of semantic representation ranging from Information Extraction (Surdeanu et al., 2003; Moschitti et al., 2003)) to Question Answering (Shen and Lapata, 2007), Paraphrase Identification (Pado and Erk, 2005), and the modeling of Textual Entailment relations (Tatu and Moldovan, 2005). Large scale annotated resources have been used by Semantic Role Labeling methods: they are commonly developed using a supervised learning paradigm where a classifier learns to predict role labels based on features extracted from annotated training data. One promine</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>On the role of lexical features in sequence labeling. In</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP ’09,</booktitle>
<pages>1142--1151</pages>
<contexts>
<context position="9594" citStr="Goldberg and Elhadad, 2009" startWordPosition="1467" endWordPosition="1470">ng (LLE) (Roweis and Saul, 2000), Local Tangent Space alignment (LTSA) (Zhang and Zha, 2004) and Locality Preserving Projection (LPP) (He and Niyogi, 2003) and they have been successfully applied in several computer vision and pattern recognition problems. In (Yang et al., 2006) it is demonstrated that basic nonlinear dimensionality reduction algorithms, such as LLE, ISOMAP, and LTSA, can be modified by taking into account prior information on exact mapping of certain data points. The sensitivity analysis of these algorithms shows that prior information improves stability of the solution. In (Goldberg and Elhadad, 2009), a strategy to incorporate lexical features into classification models is proposed. Another possible approach is the strategy pursued in recent works on deep learning techniques to NLP tasks. In (Collobert and Weston, 2008) a unified architecture for NLP that learns features relevant to the tasks at hand given very limited prior knowledge is presented. It embodies the idea that a multitask learning architecture coupled with semi-supervised learning can be effectively applied even to complex linguistic tasks such as Semantic Role Labeling. In particular, (Collobert and Weston, 2008) proposes a</context>
</contexts>
<marker>Goldberg, Elhadad, 2009</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2009. On the role of lexical features in sequence labeling. In In Proceedings of EMNLP ’09, pages 1142–1151, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1964</date>
<booktitle>The Philosophy of Linguistics,</booktitle>
<editor>In Jerrold J. Katz and Jerry A. Fodor, editors,</editor>
<publisher>University Press.</publisher>
<location>New York. Oxford</location>
<contexts>
<context position="11160" citStr="Harris, 1964" startWordPosition="1716" endWordPosition="1717">mantics The aim of this distributional approach is to model frames in semantic spaces where words are represented from the distributional analysis of their cooccurrences over a corpus. Semantic spaces are widely used in NLP for representing the meaning of words or other lexical entities. They have been successfully applied in several tasks, such as information retrieval (Salton et al., 1975) and harvesting thesauri (Lin, 1998). The fundamental intuition is that the meaning of a word can be described by the set of textual contexts in which it appears (Distributional Hypothesis as described in (Harris, 1964)), and that words with similar vectors are semantically related. Contexts are words appearing together with a LU: such a space models a generic notion of semantic relatedness, i.e. two LUs spatially close in the space are likely to be either in paradigmatic or syntagmatic relation as in (Sahlgren, 2006). Here, LUs delimit subspaces modeling the prototypical semantic of the corresponding evoked frames and novel LUs can be induced by exploiting their projections. Since a semantic space supports the language in use from the corpus statistics in an unsupervised fashion, vectors representing LUs ca</context>
</contexts>
<marker>Harris, 1964</marker>
<rawString>Zellig Harris. 1964. Distributional structure. In Jerrold J. Katz and Jerry A. Fodor, editors, The Philosophy of Linguistics, New York. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei He</author>
<author>Partha Niyogi</author>
</authors>
<title>Locality preserving projections.</title>
<date>2003</date>
<booktitle>In Proceedings of NIPS03,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5233" citStr="He and Niyogi, 2003" startWordPosition="788" endWordPosition="791">Us is further developed. As in (Pennacchiotti et al., 2008), several word spaces (Pado and Lapata, 2007) are investigated in order to find the most suitable representation of the properties which characterize a frame. Two dimensionality reduction techniques are applied here in this context. Latent Semantic Analysis (Landauer and Dumais, 1997) uses the Singular Value Decomposition to find the best subspace approximation of the original word space, in the sense of minimizing the global reconstruction error projecting data along the directions of maximal variance. Locality Preserving Projection (He and Niyogi, 2003) is a linear approximation of the nonlinear Laplacian Eigenmap algorithm: its locality preserving properties allows to add a set of constraints forcing LUs that belong to the same frame to be near in the resulting space after the transformation. LSA performs a global analysis of a corpus capturing relations between LUs and removing the noise introduced by spurious directions. However it risks to ignore lexical senses poorly represented into the corpus. In (De Cao et al., 2008) external knowledge about LUs is provided by their lexical senses from a lexical resource (e.g WordNet). In this work, </context>
<context position="9122" citStr="He and Niyogi, 2003" startWordPosition="1396" endWordPosition="1399">ating constraint-based supervision into prototype-based clustering. Another possible approach is to directly embed the prior-knowledge into data representations. The main idea is to employ effective and efficient algorithms for constructing nonlinear low-dimensional manifolds from sample data points embedded 8 in high-dimensional spaces. Several algorithms are defined, including Isometric feature mapping (ISOMAP) (Tenenbaum et al., 2000), Locally Linear Embedding (LLE) (Roweis and Saul, 2000), Local Tangent Space alignment (LTSA) (Zhang and Zha, 2004) and Locality Preserving Projection (LPP) (He and Niyogi, 2003) and they have been successfully applied in several computer vision and pattern recognition problems. In (Yang et al., 2006) it is demonstrated that basic nonlinear dimensionality reduction algorithms, such as LLE, ISOMAP, and LTSA, can be modified by taking into account prior information on exact mapping of certain data points. The sensitivity analysis of these algorithms shows that prior information improves stability of the solution. In (Goldberg and Elhadad, 2009), a strategy to incorporate lexical features into classification models is proposed. Another possible approach is the strategy p</context>
<context position="17527" citStr="He and Niyogi, 2003" startWordPosition="2738" endWordPosition="2741">ntextual usages can be used instead of the words to represent texts. This technique has two main advantages. First, the overall computational cost of the model is reduced, as similarities are computed on a space with much fewer dimensions. Secondly, it allows to capture second-order relations among LUs, thus improving the quality of the similarity measure. 3.3 The Locality Preserving Projection Method An alternative to LSA, much tighter to local properties of data, is the Locality Preserving Projection (LPP), a linear approximation of the non-linear Laplacian Eigenmap algorithm introduced in (He and Niyogi, 2003). LPP is a linear dimensionality reduction method whose goal is, given a set of LUs x1, x2, .., xm in Rn, to find a transformation matrix A that maps these m points into a set of points y1, y2,.., ym in Rk (k « n). LPP achieves this result through a cascade of processing steps described hereafter. Construction of an Adjacency graph. Let G denote a graph with m nodes. Nodes i and j have got a weighted connection if vectors xi and xj are close, according to an arbitrary measure of similarity. There are many ways to build an adjacency graph. The cosine graph with cosine weighting scheme is explor</context>
</contexts>
<marker>He, Niyogi, 2003</marker>
<rawString>Xiaofei He and Partha Niyogi. 2003. Locality preserving projections. In Proceedings of NIPS03, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-Scale SVM Learning Practical.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="21545" citStr="Joachims, 1999" startWordPosition="3453" endWordPosition="3454">al evaluation of distributional models applied to the task of inducing LUs is presented. Different spaces obtained through the dimensionality reduction techniques imply different kernel functions used to independently train different SVMs. Our aim is to investigate the impact of these kernels in capturing both the frames and LUs’ properties, as well as the effectiveness of their possible combination. The problem of LUs’ induction is here treated as a multi-classification problem, where each LU is considered as a positive or negative instance of a frame. We use Support Vector Machines (SVMs), (Joachims, 1999) a maximum-margin classifier that realizes a linear discriminative model. In case of not linearly separable examples, convolution functions O(·) can be used in order to transform the initial feature space into another one, where a hyperplane that separates the data with the widest margin can be found. Here new similarity measures, the kernel functions, can be defined through the dot-product K(oi, oj) = (O(oi) · O(oj)) over the new representation. In this way, kernel functions KLSA and KLPP can be induced through the dimensionality reduction techniques OLSA and OLPP respectively, as described i</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-Scale SVM Learning Practical. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Using WordNet to extend FrameNet coverage.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Building Frame-semantic Resources for Scandinavian and Baltic Languages, at NODALIDA,</booktitle>
<location>Tartu, Estonia,</location>
<contexts>
<context position="7158" citStr="Johansson and Nugues, 2007" startWordPosition="1103" endWordPosition="1106"> As defined in (Pennacchiotti et al., 2008), LU induction is the task of assigning a generic lexical unit not yet present in the FrameNet database (the so-called unknown LU) to the correct frame(s). The number of possible classes (i.e. frames) and the multiple assignment problem make it a challenging task. LU induction has been integrated at SemEval-2007 as part of the Frame Semantic Structure Extraction shared task (Baker et al., 2007), where systems are requested to assign the correct frame to a given LU, even when the LU is not yet present in FrameNet. Several approaches show low coverage (Johansson and Nugues, 2007) or low accuracy, like (Burchardt et al., 2005). This task is presented in (Pennacchiotti et al., 2008) and (De Cao et al., 2008), where two different models which combine distributional and paradigmatic (i.e. lexical) information have been discussed. The distributional model is used to select a list of frame suggested by the corpus’ evidences and then the plausible lexical senses of the unknown LU are used to re-rank proposed frames. In order to exploit prior information provided by the frame theory, the idea underlying is that semantic knowledge can be embedded from external sources (i.e the</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Using WordNet to extend FrameNet coverage. In Proceedings of the Workshop on Building Frame-semantic Resources for Scandinavian and Baltic Languages, at NODALIDA, Tartu, Estonia, May 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>The effect of syntactic representation on semantic role labeling.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>18--22</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="3300" citStr="Johansson and Nugues, 2008" startWordPosition="481" endWordPosition="485">or suicide that share semantic arguments such as KILLER, INSTRUMENT, CAUSE, VICTIM. The current FrameNet release contains about 700 frames and 10,000 LUs. A corpus of 150,000 annotated examples sentences, from the British National Corpus (BNC), is also part of FrameNet. Despite the size of this resource, it is under development and hence incomplete: several frames are not represented by evoking words and the number of annotated sentences is unbalanced across frames. It is one of the main reason for the performance drop of supervised SRL systems in out-of-domain scenarios (Baker et al., 2007) (Johansson and Nugues, 2008). The limited coverage of FrameNet corpus is even more noticeable for the LUs dictionary: it only contains 10,000 lexical units, far less than the 210,000 entries in WordNet 3.0. For example, the lexical unit crown, according to the annotations, evokes the ACCOUTREMENT frame. It refers to a particular sense: according to WordNet, it is “an ornamental jeweled headdress signifying sovereignty”. According to the same lexical resource, this LU has 12 lexical senses and the first one (i.e. “The Crown 7 Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, p</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. The effect of syntactic representation on semantic role labeling. In Proceedings of COLING, Manchester, UK, August 18-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Landauer</author>
<author>Sue Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<contexts>
<context position="4957" citStr="Landauer and Dumais, 1997" startWordPosition="746" endWordPosition="749"> unannotated corpus and the lexical information of WordNet. These representations were used in order to find out frames potentially evoked by novel words in order to extend the FrameNet dictionary limiting the effort of manual annotations. In this work the distributional model of LUs is further developed. As in (Pennacchiotti et al., 2008), several word spaces (Pado and Lapata, 2007) are investigated in order to find the most suitable representation of the properties which characterize a frame. Two dimensionality reduction techniques are applied here in this context. Latent Semantic Analysis (Landauer and Dumais, 1997) uses the Singular Value Decomposition to find the best subspace approximation of the original word space, in the sense of minimizing the global reconstruction error projecting data along the directions of maximal variance. Locality Preserving Projection (He and Niyogi, 2003) is a linear approximation of the nonlinear Laplacian Eigenmap algorithm: its locality preserving properties allows to add a set of constraints forcing LUs that belong to the same frame to be near in the resulting space after the transformation. LSA performs a global analysis of a corpus capturing relations between LUs and</context>
<context position="15540" citStr="Landauer and Dumais, 1997" startWordPosition="2411" endWordPosition="2414">ntax-based space: Contexts words are enriched through information about syntactic relations (e.g. X-VSubj-killer where X is the LU), as described in (Pado and Lapata, 2007). Two LUs close in the space are likely to be in a paradigmatic relation, i.e. to be close in an IS-A hierarchy (Budanitsky and Hirst, 2006; Lin, 1998). Indeed, as contexts are syntactic relations, targets with the same part of speech are much closer than targets of different types. 3.2 Latent Semantic Analysis Latent Semantic Analysis (LSA) is an algorithm presented in (Furnas et al., 1988) afterwards diffused by Landauer (Landauer and Dumais, 1997): it can be seen as a variant of the Principal Component Analysis idea. LSA aims to find the best subspace approximation to the original word space, in the sense of minimizing the global reconstruction error projecting data along the directions of maximal variance. It captures term (semantic) dependencies by applying a matrix decomposition process called Singular Value Decomposition (SVD). The original term-by-term matrix M is transformed into the product of three new matrices: U, 5, and V so that M = U5VT . Matrix M is approximated by Ml = Ul5lVT l in which only the first l columns of U and V</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Tom Landauer and Sue Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar word.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="10977" citStr="Lin, 1998" startWordPosition="1684" endWordPosition="1685">xts allows to achieve a significant level of lexical generalization in order to better capitalize on the smaller annotated data sets. 3 Geometrical Embeddings as models of Frame Semantics The aim of this distributional approach is to model frames in semantic spaces where words are represented from the distributional analysis of their cooccurrences over a corpus. Semantic spaces are widely used in NLP for representing the meaning of words or other lexical entities. They have been successfully applied in several tasks, such as information retrieval (Salton et al., 1975) and harvesting thesauri (Lin, 1998). The fundamental intuition is that the meaning of a word can be described by the set of textual contexts in which it appears (Distributional Hypothesis as described in (Harris, 1964)), and that words with similar vectors are semantically related. Contexts are words appearing together with a LU: such a space models a generic notion of semantic relatedness, i.e. two LUs spatially close in the space are likely to be either in paradigmatic or syntagmatic relation as in (Sahlgren, 2006). Here, LUs delimit subspaces modeling the prototypical semantic of the corresponding evoked frames and novel LUs</context>
<context position="15237" citStr="Lin, 1998" startWordPosition="2366" endWordPosition="2367">, as lemmas, appearing in a n-window of the LU. The window width n is a parameter that allows the space to capture different aspects of a frame: higher values risk to introduce noise, since a frame could not cover an entire sentence, while lower values lead to sparse representations. Syntax-based space: Contexts words are enriched through information about syntactic relations (e.g. X-VSubj-killer where X is the LU), as described in (Pado and Lapata, 2007). Two LUs close in the space are likely to be in a paradigmatic relation, i.e. to be close in an IS-A hierarchy (Budanitsky and Hirst, 2006; Lin, 1998). Indeed, as contexts are syntactic relations, targets with the same part of speech are much closer than targets of different types. 3.2 Latent Semantic Analysis Latent Semantic Analysis (LSA) is an algorithm presented in (Furnas et al., 1988) afterwards diffused by Landauer (Landauer and Dumais, 1997): it can be seen as a variant of the Principal Component Analysis idea. LSA aims to find the best subspace approximation to the original word space, in the sense of minimizing the global reconstruction error projecting data along the directions of maximal variance. It captures term (semantic) dep</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar word. In Proceedings of COLING-ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Paul Morarescu</author>
<author>Sanda M Harabagiu</author>
</authors>
<title>Open domain information extraction via automatic semantic labeling.</title>
<date>2003</date>
<booktitle>In FLAIRS Conference,</booktitle>
<pages>397--401</pages>
<contexts>
<context position="1674" citStr="Moschitti et al., 2003" startWordPosition="225" endWordPosition="228">pport Vector Machine) algorithms: their use configures, as a novel way to semi-supervised lexical learning, a highly appealing research direction for knowledge rich scenarios like FrameNet-based semantic parsing. 1 Introduction Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Several NLP applications have exploited this kind of semantic representation ranging from Information Extraction (Surdeanu et al., 2003; Moschitti et al., 2003)) to Question Answering (Shen and Lapata, 2007), Paraphrase Identification (Pado and Erk, 2005), and the modeling of Textual Entailment relations (Tatu and Moldovan, 2005). Large scale annotated resources have been used by Semantic Role Labeling methods: they are commonly developed using a supervised learning paradigm where a classifier learns to predict role labels based on features extracted from annotated training data. One prominent resource has been developed under the Berkeley FrameNet project as a semantic lexicon for the core vocabulary of English, according to the so-called frame sema</context>
</contexts>
<marker>Moschitti, Morarescu, Harabagiu, 2003</marker>
<rawString>Alessandro Moschitti, Paul Morarescu, and Sanda M. Harabagiu. 2003. Open domain information extraction via automatic semantic labeling. In FLAIRS Conference, pages 397–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Katrin Erk</author>
</authors>
<title>To cause or not to cause: Cross-lingual semantic matching for paraphrase modelling.</title>
<date>2005</date>
<booktitle>In Proceedings of the CrossLanguage Knowledge Induction Workshop,</booktitle>
<location>ClujNapoca, Romania.</location>
<contexts>
<context position="1769" citStr="Pado and Erk, 2005" startWordPosition="238" endWordPosition="241">earning, a highly appealing research direction for knowledge rich scenarios like FrameNet-based semantic parsing. 1 Introduction Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Several NLP applications have exploited this kind of semantic representation ranging from Information Extraction (Surdeanu et al., 2003; Moschitti et al., 2003)) to Question Answering (Shen and Lapata, 2007), Paraphrase Identification (Pado and Erk, 2005), and the modeling of Textual Entailment relations (Tatu and Moldovan, 2005). Large scale annotated resources have been used by Semantic Role Labeling methods: they are commonly developed using a supervised learning paradigm where a classifier learns to predict role labels based on features extracted from annotated training data. One prominent resource has been developed under the Berkeley FrameNet project as a semantic lexicon for the core vocabulary of English, according to the so-called frame semantic model (Fillmore, 1985). Here, a frame is a conceptual structure modeling a prototypical si</context>
</contexts>
<marker>Pado, Erk, 2005</marker>
<rawString>Sebastian Pado and Katrin Erk. 2005. To cause or not to cause: Cross-lingual semantic matching for paraphrase modelling. In Proceedings of the CrossLanguage Knowledge Induction Workshop, ClujNapoca, Romania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4717" citStr="Pado and Lapata, 2007" startWordPosition="711" endWordPosition="714">mes, like LEADERSHIP. In (Pennacchiotti et al., 2008) and (De Cao et al., 2008), the problem of LU automatic induction has been treated in a semi-supervised fashion. First, LUs are modeled by exploiting the distributional analysis of an unannotated corpus and the lexical information of WordNet. These representations were used in order to find out frames potentially evoked by novel words in order to extend the FrameNet dictionary limiting the effort of manual annotations. In this work the distributional model of LUs is further developed. As in (Pennacchiotti et al., 2008), several word spaces (Pado and Lapata, 2007) are investigated in order to find the most suitable representation of the properties which characterize a frame. Two dimensionality reduction techniques are applied here in this context. Latent Semantic Analysis (Landauer and Dumais, 1997) uses the Singular Value Decomposition to find the best subspace approximation of the original word space, in the sense of minimizing the global reconstruction error projecting data along the directions of maximal variance. Locality Preserving Projection (He and Niyogi, 2003) is a linear approximation of the nonlinear Laplacian Eigenmap algorithm: its locali</context>
<context position="15086" citStr="Pado and Lapata, 2007" startWordPosition="2335" endWordPosition="2338">urring or substitutional words (e.g. murder/kill). Two traditional word-based co-occurrence models capture the above property: Word-based space: Contexts are words, as lemmas, appearing in a n-window of the LU. The window width n is a parameter that allows the space to capture different aspects of a frame: higher values risk to introduce noise, since a frame could not cover an entire sentence, while lower values lead to sparse representations. Syntax-based space: Contexts words are enriched through information about syntactic relations (e.g. X-VSubj-killer where X is the LU), as described in (Pado and Lapata, 2007). Two LUs close in the space are likely to be in a paradigmatic relation, i.e. to be close in an IS-A hierarchy (Budanitsky and Hirst, 2006; Lin, 1998). Indeed, as contexts are syntactic relations, targets with the same part of speech are much closer than targets of different types. 3.2 Latent Semantic Analysis Latent Semantic Analysis (LSA) is an algorithm presented in (Furnas et al., 1988) afterwards diffused by Landauer (Landauer and Dumais, 1997): it can be seen as a variant of the Principal Component Analysis idea. LSA aims to find the best subspace approximation to the original word spac</context>
<context position="25626" citStr="Pado and Lapata, 2007" startWordPosition="4141" endWordPosition="4145">ion 3.1: Window-n (Wn): contextual features correspond to the set of the 20,000 most frequent lemmatized words in the BNC. The association measure between LUs and contexts is the Point-wise Mutual Information (PMI). Valid contexts for LUs are fixed to a n-window. Hereafter two window width values will be investigated: Window5 (W5) and Window10 (W10). Sentence (Sent): contextual features are the same above, but the valid contexts are extended to the entire sentence length. SyntaxBased (SyntB): contextual features have been computed according to the “dependencybased” vector space discussed3 in (Pado and Lapata, 2007). Observable contexts here are made of syntactically-typed co-occurrences within dependency graphs built from the entire set of BNC sentences. The most frequent 20,000 basic features, i.e. (syntactic relation,lemma) pairs, have been employed as contextual features corresponding to PMI scores. Syntactic relations are extracted using the Minipar parser. Word space models thus focus on the LUs of the selected 100 frames and the dimensionality have been reduced by applying LSA and LPP at a new size of l = 100. Any prior knowledge information is provided to the tuning and test sets during the LPP t</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Diego De Cao</author>
<author>Roberto Basili</author>
<author>Danilo Croce</author>
<author>Michael Roth</author>
</authors>
<title>Automatic induction of framenet lexical units.</title>
<date>2008</date>
<booktitle>In Proceedings of The Empirical Methods in Natural Language Processing (EMNLP 2008) Waikiki,</booktitle>
<location>Honolulu, Hawaii.</location>
<marker>Pennacchiotti, De Cao, Basili, Croce, Roth, 2008</marker>
<rawString>Marco Pennacchiotti, Diego De Cao, Roberto Basili, Danilo Croce, and Michael Roth. 2008. Automatic induction of framenet lexical units. In Proceedings of The Empirical Methods in Natural Language Processing (EMNLP 2008) Waikiki, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Roweis</author>
<author>L K Saul</author>
</authors>
<title>Nonlinear dimensionality reduction by locally linear embedding.</title>
<date>2000</date>
<journal>Science,</journal>
<volume>290</volume>
<issue>5500</issue>
<contexts>
<context position="8999" citStr="Roweis and Saul, 2000" startWordPosition="1377" endWordPosition="1380">lds (HMRFs) as a probabilistic generative model for semi-supervised clustering, providing a principled framework for incorporating constraint-based supervision into prototype-based clustering. Another possible approach is to directly embed the prior-knowledge into data representations. The main idea is to employ effective and efficient algorithms for constructing nonlinear low-dimensional manifolds from sample data points embedded 8 in high-dimensional spaces. Several algorithms are defined, including Isometric feature mapping (ISOMAP) (Tenenbaum et al., 2000), Locally Linear Embedding (LLE) (Roweis and Saul, 2000), Local Tangent Space alignment (LTSA) (Zhang and Zha, 2004) and Locality Preserving Projection (LPP) (He and Niyogi, 2003) and they have been successfully applied in several computer vision and pattern recognition problems. In (Yang et al., 2006) it is demonstrated that basic nonlinear dimensionality reduction algorithms, such as LLE, ISOMAP, and LTSA, can be modified by taking into account prior information on exact mapping of certain data points. The sensitivity analysis of these algorithms shows that prior information improves stability of the solution. In (Goldberg and Elhadad, 2009), a s</context>
</contexts>
<marker>Roweis, Saul, 2000</marker>
<rawString>S.T. Roweis and L.K. Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Stockholm University.</institution>
<contexts>
<context position="11464" citStr="Sahlgren, 2006" startWordPosition="1767" endWordPosition="1768">n successfully applied in several tasks, such as information retrieval (Salton et al., 1975) and harvesting thesauri (Lin, 1998). The fundamental intuition is that the meaning of a word can be described by the set of textual contexts in which it appears (Distributional Hypothesis as described in (Harris, 1964)), and that words with similar vectors are semantically related. Contexts are words appearing together with a LU: such a space models a generic notion of semantic relatedness, i.e. two LUs spatially close in the space are likely to be either in paradigmatic or syntagmatic relation as in (Sahlgren, 2006). Here, LUs delimit subspaces modeling the prototypical semantic of the corresponding evoked frames and novel LUs can be induced by exploiting their projections. Since a semantic space supports the language in use from the corpus statistics in an unsupervised fashion, vectors representing LUs can be characterized by different distributions. For example, LUs of the frame KILLING, such as bloodbath, crucify or fratricide, are statistically inferior in a corpus if compared to a wide-spanning term as kill. Moreover other ambiguous LUs, as liquidate or terminate, could appear in sentences evoking d</context>
<context position="14287" citStr="Sahlgren, 2006" startWordPosition="2207" endWordPosition="2208"> Semantic Analysis while in Section 3.3 the Locality Preserving Projection algorithm will be discussed in order to combine prior knowledge about frames with local regularities of LUs obtained from text. 3.1 Choosing the space Different types of context define spaces with different semantic properties. Such spaces model a generic notion of semantic relatedness. Two LUs close in the space are likely to be related by some type of generic semantic relation, either paradigmatic (e.g. synonymy, hyperonymy, antonymy) or syntagmatic (e.g. meronymy, conceptual and phrasal association), as observed in (Sahlgren, 2006). The target of this work is the construction of a space able to capture the properties which characterize a frame, assuming those LUs in the same frame tend to be either co-occurring or substitutional words (e.g. murder/kill). Two traditional word-based co-occurrence models capture the above property: Word-based space: Contexts are words, as lemmas, appearing in a n-window of the LU. The window width n is a parameter that allows the space to capture different aspects of a frame: higher values risk to introduce noise, since a frame could not cover an entire sentence, while lower values lead to</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model. Ph.D. thesis, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<pages>18--613</pages>
<contexts>
<context position="10941" citStr="Salton et al., 1975" startWordPosition="1676" endWordPosition="1679">ing process. The extensive use of unlabeled texts allows to achieve a significant level of lexical generalization in order to better capitalize on the smaller annotated data sets. 3 Geometrical Embeddings as models of Frame Semantics The aim of this distributional approach is to model frames in semantic spaces where words are represented from the distributional analysis of their cooccurrences over a corpus. Semantic spaces are widely used in NLP for representing the meaning of words or other lexical entities. They have been successfully applied in several tasks, such as information retrieval (Salton et al., 1975) and harvesting thesauri (Lin, 1998). The fundamental intuition is that the meaning of a word can be described by the set of textual contexts in which it appears (Distributional Hypothesis as described in (Harris, 1964)), and that words with similar vectors are semantically related. Contexts are words appearing together with a LU: such a space models a generic notion of semantic relatedness, i.e. two LUs spatially close in the space are likely to be either in paradigmatic or syntagmatic relation as in (Sahlgren, 2006). Here, LUs delimit subspaces modeling the prototypical semantic of the corre</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18:613 ¨Ai620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Mirella Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>12--21</pages>
<location>Prague.</location>
<contexts>
<context position="1721" citStr="Shen and Lapata, 2007" startWordPosition="232" endWordPosition="235">igures, as a novel way to semi-supervised lexical learning, a highly appealing research direction for knowledge rich scenarios like FrameNet-based semantic parsing. 1 Introduction Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Several NLP applications have exploited this kind of semantic representation ranging from Information Extraction (Surdeanu et al., 2003; Moschitti et al., 2003)) to Question Answering (Shen and Lapata, 2007), Paraphrase Identification (Pado and Erk, 2005), and the modeling of Textual Entailment relations (Tatu and Moldovan, 2005). Large scale annotated resources have been used by Semantic Role Labeling methods: they are commonly developed using a supervised learning paradigm where a classifier learns to predict role labels based on features extracted from annotated training data. One prominent resource has been developed under the Berkeley FrameNet project as a semantic lexicon for the core vocabulary of English, according to the so-called frame semantic model (Fillmore, 1985). Here, a frame is a</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>Dan Shen and Mirella Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of EMNLP-CoNLL, pages 12–21, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>A Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicateargument structures for information extraction. In</title>
<date>2003</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="1649" citStr="Surdeanu et al., 2003" startWordPosition="221" endWordPosition="224">for supervised (i.e. Support Vector Machine) algorithms: their use configures, as a novel way to semi-supervised lexical learning, a highly appealing research direction for knowledge rich scenarios like FrameNet-based semantic parsing. 1 Introduction Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Several NLP applications have exploited this kind of semantic representation ranging from Information Extraction (Surdeanu et al., 2003; Moschitti et al., 2003)) to Question Answering (Shen and Lapata, 2007), Paraphrase Identification (Pado and Erk, 2005), and the modeling of Textual Entailment relations (Tatu and Moldovan, 2005). Large scale annotated resources have been used by Semantic Role Labeling methods: they are commonly developed using a supervised learning paradigm where a classifier learns to predict role labels based on features extracted from annotated training data. One prominent resource has been developed under the Berkeley FrameNet project as a semantic lexicon for the core vocabulary of English, according to</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, , Mihai Surdeanu, A Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicateargument structures for information extraction. In In Proceedings ofACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Dan I Moldovan</author>
</authors>
<title>A semantic approach to recognizing textual entailment.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="1845" citStr="Tatu and Moldovan, 2005" startWordPosition="250" endWordPosition="253">rios like FrameNet-based semantic parsing. 1 Introduction Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Several NLP applications have exploited this kind of semantic representation ranging from Information Extraction (Surdeanu et al., 2003; Moschitti et al., 2003)) to Question Answering (Shen and Lapata, 2007), Paraphrase Identification (Pado and Erk, 2005), and the modeling of Textual Entailment relations (Tatu and Moldovan, 2005). Large scale annotated resources have been used by Semantic Role Labeling methods: they are commonly developed using a supervised learning paradigm where a classifier learns to predict role labels based on features extracted from annotated training data. One prominent resource has been developed under the Berkeley FrameNet project as a semantic lexicon for the core vocabulary of English, according to the so-called frame semantic model (Fillmore, 1985). Here, a frame is a conceptual structure modeling a prototypical situation, evoked in texts through the occurrence of its lexical units (LU) th</context>
</contexts>
<marker>Tatu, Moldovan, 2005</marker>
<rawString>Marta Tatu and Dan I. Moldovan. 2005. A semantic approach to recognizing textual entailment. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Tenenbaum</author>
<author>V Silva</author>
<author>J C Langford</author>
</authors>
<title>A Global Geometric Framework for Nonlinear Dimensionality Reduction.</title>
<date>2000</date>
<journal>Science,</journal>
<volume>290</volume>
<issue>5500</issue>
<pages>2323</pages>
<contexts>
<context position="8943" citStr="Tenenbaum et al., 2000" startWordPosition="1368" endWordPosition="1371">d in (Basu et al., 2006) employs Hidden Markov Random Fields (HMRFs) as a probabilistic generative model for semi-supervised clustering, providing a principled framework for incorporating constraint-based supervision into prototype-based clustering. Another possible approach is to directly embed the prior-knowledge into data representations. The main idea is to employ effective and efficient algorithms for constructing nonlinear low-dimensional manifolds from sample data points embedded 8 in high-dimensional spaces. Several algorithms are defined, including Isometric feature mapping (ISOMAP) (Tenenbaum et al., 2000), Locally Linear Embedding (LLE) (Roweis and Saul, 2000), Local Tangent Space alignment (LTSA) (Zhang and Zha, 2004) and Locality Preserving Projection (LPP) (He and Niyogi, 2003) and they have been successfully applied in several computer vision and pattern recognition problems. In (Yang et al., 2006) it is demonstrated that basic nonlinear dimensionality reduction algorithms, such as LLE, ISOMAP, and LTSA, can be modified by taking into account prior information on exact mapping of certain data points. The sensitivity analysis of these algorithms shows that prior information improves stabili</context>
</contexts>
<marker>Tenenbaum, Silva, Langford, 2000</marker>
<rawString>J. B. Tenenbaum, V. Silva, and J. C. Langford. 2000. A Global Geometric Framework for Nonlinear Dimensionality Reduction. Science, 290(5500):2319– 2323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Yang</author>
<author>Haoying Fu</author>
<author>Hongyuan Zha</author>
<author>Jesse Barlow</author>
</authors>
<title>Semi-supervised nonlinear dimensionality reduction.</title>
<date>2006</date>
<booktitle>In 23rd International Conference on Machine learning,</booktitle>
<pages>1065--1072</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9246" citStr="Yang et al., 2006" startWordPosition="1416" endWordPosition="1419">nowledge into data representations. The main idea is to employ effective and efficient algorithms for constructing nonlinear low-dimensional manifolds from sample data points embedded 8 in high-dimensional spaces. Several algorithms are defined, including Isometric feature mapping (ISOMAP) (Tenenbaum et al., 2000), Locally Linear Embedding (LLE) (Roweis and Saul, 2000), Local Tangent Space alignment (LTSA) (Zhang and Zha, 2004) and Locality Preserving Projection (LPP) (He and Niyogi, 2003) and they have been successfully applied in several computer vision and pattern recognition problems. In (Yang et al., 2006) it is demonstrated that basic nonlinear dimensionality reduction algorithms, such as LLE, ISOMAP, and LTSA, can be modified by taking into account prior information on exact mapping of certain data points. The sensitivity analysis of these algorithms shows that prior information improves stability of the solution. In (Goldberg and Elhadad, 2009), a strategy to incorporate lexical features into classification models is proposed. Another possible approach is the strategy pursued in recent works on deep learning techniques to NLP tasks. In (Collobert and Weston, 2008) a unified architecture for </context>
</contexts>
<marker>Yang, Fu, Zha, Barlow, 2006</marker>
<rawString>Xin Yang, Haoying Fu, Hongyuan Zha, and Jesse Barlow. 2006. Semi-supervised nonlinear dimensionality reduction. In 23rd International Conference on Machine learning, pages 1065–1072, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenyue Zhang</author>
<author>Hongyuan Zha</author>
</authors>
<title>Principal manifolds and nonlinear dimensionality reduction via tangent space alignment.</title>
<date>2004</date>
<journal>SIAM J. Scientific Computing,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="9059" citStr="Zhang and Zha, 2004" startWordPosition="1386" endWordPosition="1389">vised clustering, providing a principled framework for incorporating constraint-based supervision into prototype-based clustering. Another possible approach is to directly embed the prior-knowledge into data representations. The main idea is to employ effective and efficient algorithms for constructing nonlinear low-dimensional manifolds from sample data points embedded 8 in high-dimensional spaces. Several algorithms are defined, including Isometric feature mapping (ISOMAP) (Tenenbaum et al., 2000), Locally Linear Embedding (LLE) (Roweis and Saul, 2000), Local Tangent Space alignment (LTSA) (Zhang and Zha, 2004) and Locality Preserving Projection (LPP) (He and Niyogi, 2003) and they have been successfully applied in several computer vision and pattern recognition problems. In (Yang et al., 2006) it is demonstrated that basic nonlinear dimensionality reduction algorithms, such as LLE, ISOMAP, and LTSA, can be modified by taking into account prior information on exact mapping of certain data points. The sensitivity analysis of these algorithms shows that prior information improves stability of the solution. In (Goldberg and Elhadad, 2009), a strategy to incorporate lexical features into classification </context>
</contexts>
<marker>Zhang, Zha, 2004</marker>
<rawString>Zhenyue Zhang and Hongyuan Zha. 2004. Principal manifolds and nonlinear dimensionality reduction via tangent space alignment. SIAM J. Scientific Computing, 26(1):313–338.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>