<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.086552">
<figure confidence="0.964290214285714">
Source Language Text
Transformation Pr(f1J  |e1I)
J
f1
Global Search: Lexicon Model
maximize Pr( e1
I) Pr(f1J  |e1 I )
over eI
1
I )
Pr( e1
Alignment Model
Language Model
Transformation
</figure>
<affiliation confidence="0.766323">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 156-163.
Target Language Text
Association for Computational Linguistics.
</affiliation>
<note confidence="0.7470014">
?
Friday
about
how
wie sieht es am Freitag aus ?
</note>
<bodyText confidence="0.999895588235294">
whose probability is below this value multiplied
with a threshold (lower than one) will not be re-
garded for further expansion. Histogram prun-
ing means that all but the M best hypotheses
are pruned for a fixed M.
For finding the most likely partial hypotheses,
first all hypotheses with the same set of covered
source sentence positions are compared. Af-
ter threshold and histogram pruning have been
applied, we also compare all hypotheses with
the same number of covered source sentence
positions and apply both pruning types again.
Those hypotheses that survive the pruning are
called the active hypotheses.
The word graph structure and the results pre-
sented here can easily be transferred to other
search algorithms, such as A* search.
</bodyText>
<sectionHeader confidence="0.97838" genericHeader="abstract">
3 Word Graphs
</sectionHeader>
<subsectionHeader confidence="0.644254">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999935458333333">
It is widely accepted in the community that a
significant improvement in translation quality
will come from more sophisticated translation
and language models. For example, a language
model that goes beyond m-gram dependencies
could be used, but this would be difficult to in-
tegrate into the search process. As a step to-
wards the solution of this problem, we determine
not only the single best sentence hypothesis, but
also other complete sentences that the search al-
gorithm found but that were judged worse. We
can then apply rescoring with a refined model
to those hypotheses. One efficient way to store
the different alternatives is a word graph.
Word graphs have been successfully applied
in speech recognition, for the search process
(Ortmanns et al., 1997) and as an interface to
other systems (Oerder and Ney, 1993). (Knight
and Hatzivassiloglou, 1995) and (Langkilde and
Knight, 1998) propose the use of word graphs
for natural language generation. In this paper,
we are going to present a concept for the gen-
eration of word graphs in a machine translation
system.
</bodyText>
<subsectionHeader confidence="0.999642">
3.2 Bookkeeping
</subsectionHeader>
<bodyText confidence="0.9995616">
During search, we keep a bookkeeping tree. It
is not necessary to keep all the information that
we need for the expansion of hypotheses during
search in this structure, thus we store only the
following:
</bodyText>
<listItem confidence="0.996384">
• the produced target word e,
• the covered source sentence position j,
• a backpointer to the preceding bookkeeping
entry.
</listItem>
<bodyText confidence="0.999859357142857">
After the search has finished, i.e. when all
source sentence positions have been translated,
we trace back the best sentence in the bookkeep-
ing tree.
To generate the N best hypotheses after
search, it is not sufficient to simply trace back
the complete hypotheses with the highest prob-
abilities in the bookkeeping, because those hy-
potheses have been recombined. Thus, many hy-
potheses with a high probability have not been
stored.
To overcome this problem, we enhance the
bookkeeping concept and generate a word graph
as described in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.998884">
3.3 Word Graph Structure
</subsectionHeader>
<bodyText confidence="0.99997525">
If we want to generate a word graph, we have to
store both alternatives in the bookkeeping when
two hypotheses are recombined. Thus, an entry
in the bookkeeping structure may have several
backpointers to different preceding entries. The
bookkeeping structure is no longer a tree but a
network where the source is the bookkeeping en-
try with zero covered source sentence positions
and the sink is a node accounting for complete
hypotheses (see Figure 3). This leads us to the
concept of word graph nodes and edges contain-
ing the following information:
</bodyText>
<listItem confidence="0.9896182">
• node
— the last covered source sentence posi-
tion j,
• edge
— the target word e,
</listItem>
<bodyText confidence="0.98244484375">
— the probabilities according to the dif-
ferent models: the language model and
the translation submodels,
— the backpointer to the preceding book-
keeping entry.
After the pruning in beam search, all hypothe-
ses that are no longer active do not have to
be kept in the bookkeeping structure. Thus,
we can perform garbage collection and re-
move all those bookkeeping entries that cannot
be reached from the backpointers of the active
hypotheses. This reduces the size of the book-
keeping structure significantly.
An example of a word graph can be seen in
Figure 3. To keep the presentation simple, we
chose an example without reordering of sentence
positions. The words on the edges are the pro-
duced target words, and the bitvectors in the
nodes show the covered source sentence posi-
tions. If an edge is labeled with two words, this
means that the first English word has no equiva-
lence in the source sentence, like &apos;just&apos; and &apos;have&apos;
in Figure 3. The reference translation &apos;what did
you say ?&apos; is contained in the graph, but it has
a slightly lower probability than the sentence
&apos;what do you say ?&apos;, which is then chosen by
the single best search.
The recombination of hypotheses can be seen
in the nodes with two or more incoming edges:
those hypotheses have been recombined, be-
cause they were indistinguishable by translation
and language model state.
</bodyText>
<sectionHeader confidence="0.85414" genericHeader="categories and subject descriptors">
4 Pruning and Rescoring
</sectionHeader>
<subsectionHeader confidence="0.9827">
4.1 Word Graph Pruning
</subsectionHeader>
<bodyText confidence="0.999988866666667">
To study the effect of the word graph size on the
translation quality, we produce a conservatively
large word graph. Then we apply word graph
pruning with a threshold t &lt; 1 and study the
change of graph error rate (see Section 5). The
pruning is based on the beam search concept
also used in the single best search: we determine
the probability of the best sentence hypothesis
in the word graph. All hypotheses in the graph
which probability is lower than this maximum
probability multiplied with the pruning thresh-
old are discarded.
If the pruning threshold t is zero, the word
graph is not pruned at all, and if t = 1, we retain
only the sentence with maximum probability.
</bodyText>
<subsectionHeader confidence="0.996171">
4.2 Word Graph Rescoring
</subsectionHeader>
<bodyText confidence="0.99998">
In single best search, a standard trigram lan-
guage model is used. Search with a bigram lan-
guage model is much faster, but it yields a lower
translation quality. Therefore, we apply a two-
pass approach as it was widely used in speech
recognition in the past (Ortmanns et al., 1997).
This method combines both advantages in the
following way: a word graph is constructed using
a bigram language model and is then rescored
with a trigram language model. The rescoring
algorithm is based on dynamic programming; a
description can be found in (Ortmanns et al.,
1997).
The results of the comparison of the one-pass
and the two-pass search are given in Section 5.
</bodyText>
<subsectionHeader confidence="0.999865">
4.3 Extraction of N-best Lists
</subsectionHeader>
<bodyText confidence="0.99992712">
We use A* search for finding the N best sen-
tences in a word graph: starting in the root of
the graph, we successively expand the sentence
hypotheses. The probability of the partial hy-
pothesis is obtained by multiplying the proba-
bilities of the edges expanded for this sentence.
As rest cost estimation, we use the probabilities
determined in a backward pass as follows: for
each node in the graph, we calculate the proba-
bility of a best path from this node to the goal
node, i.e. the highest probability for completing
a partial hypothesis. This rest cost estimation
is perfect because it takes the exact probability
as heuristic, i.e. the probability of the partial
hypothesis multiplied with the rest cost estima-
tion yields the actual probability of the com-
plete hypothesis. Thus, the N best hypothesis
are extracted from the graph without additional
overhead of finding sentences with a lower prob-
ability.
Of course, the hypotheses must not be recom-
bined during this search. We have to keep every
partial hypothesis in the priority queue in order
to determine the N best sentences. Otherwise,
we might lose one of them by recombination.
</bodyText>
<figure confidence="0.996950254901961">
cardinality of coverage vector
are
what
say
you
111100
110000
you
111000
said
say
.
111110
have
did
110000
111110
you
111000 say 111100
?
said
100000
do
110000
.
111110
111111
000000
110000
you
you
111000
say
said
?
?
111110
do
are
how
111100
you
just said
?
111100
0 1 2 3 4 5
100000
110000
110000
111000
have said
</figure>
<tableCaption confidence="0.986328">
Table 1: Algorithm for the Extraction of an N-best List from a Word Graph
</tableCaption>
<table confidence="0.998096222222222">
input: word graph G
initialization: set Ar = {goal node of G} , -Arm ew = 0
while Ar is non-empty do
for each node n in ./V. do
for each incoming edge (s, n) do
costs(s) = costs(n)+ edgecosts((s,n))
insert s into Arnew
Ar — Arn e w , Arm ew — 0
perform A* search with costs(n) as rest cost estimation
</table>
<tableCaption confidence="0.986175">
Table 2: Training and Test Corpus Statistics
</tableCaption>
<table confidence="0.999738777777778">
German English
Training Sentences 58 073
Words 519 523 549 921
Words without Punctuation Marks 418 979 453 632
Vocabulary Size 7 911 4 648
Singletons 3 453 1 699
Test Sentences 251
Words 2 627 2 866
Bigram/Trigram Perplexity - 39.3/30.7
</table>
<listItem confidence="0.855553">
• GER (graph error rate):
</listItem>
<bodyText confidence="0.999966666666667">
The graph error rate is computed by deter-
mining that sentence in the word graph that
has the minimum Levenstein distance to a
given reference. Thus, it is a lower bound
for the word error rate and gives a measure-
ment of what can be achieved by rescoring
with more complex models.
The calculation of the graph error rate
is performed by a dynamic programming
based algorithm. Its space complexity is the
number of graph nodes times the length of
the reference translation.
</bodyText>
<subsectionHeader confidence="0.999882">
5.2 Effect of Word Graph Pruning
</subsectionHeader>
<bodyText confidence="0.999883571428572">
In our experiments, we varied the word graph
pruning threshold in order to obtain word
graphs of different densities, i.e. different num-
bers of hypotheses. The word graph density is
computed as the total number of word graph
edges divided by the number of reference sen-
tence words — analogously to the word graph
density in speech recognition.
The effect of pruning on the graph error rate
is shown in Table 3. The value of the pruning
threshold is given as the negative logarithm of
the probability. Thus, t = 0 refers to pruning
everything but the best hypothesis.
Figure 4 shows the change in graph error rate
in relation to the average graph density. We see
that for graph densities up to 200, the graph er-
ror rate significantly changes if the graph is en-
larged. The saturation point of the GER lies at
13% and is reached for an average graph density
about 1000 which relates to a pruning threshold
of 20.
</bodyText>
<figure confidence="0.987529866666667">
40
35
0 100 200 300 400 500 600 700 800 900 1000
average graph density
30
25
o
�
20
15
10
graph error rate [%]
�
5
0
</figure>
<tableCaption confidence="0.994876">
Table 5: Error Rates [%] and CPU Time for One-Pass and Two-Pass Search
</tableCaption>
<table confidence="0.999446333333333">
Integrated Bigram Search +
Trigram Search Trigram Rescoring
Error Rates WER 41.41 42.70
mWER 36.46 37.26
SSER 36.90 35.54
CPU Time sec./sentence 10.75 2.77+0.19
</table>
<sectionHeader confidence="0.992937" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993833333333">
We have presented a concept for constructing
word graphs for statistical machine translation
by extending the single best search algorithm.
Experiments have shown that the graph error
rate significantly decreases for rising word graph
densities. The quality of the hypotheses con-
tained in a word graph is better than of those in
an N-best list. This indicates that word graph
rescoring can yield a significant gain in transla-
tion quality. For the future, we plan the applica-
tion of refined translation and language models
for rescoring on word graphs.
</bodyText>
<sectionHeader confidence="0.998249" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996945404761905">
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation:
Parameter estimation. Computational Linguis-
tics, 19(2):263-311.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings
of the Conference of the Association for Compu-
tational Linguistics, pages 252-260, Cambridge,
MA, June.
Irene Langkilde and Kevin Knight. 1998. The prac-
tical value of n-grams in generation. In Proceed-
ings of the International Natural Language Gener-
ation Workshop, pages 248-255, Ontario, Canada,
August.
Franz J. Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for sta-
tistical machine translation. In ACL-EACL-2001:
39th Annual Meeting of the Association for Com-
putational Linguistics - joint with EACL 2001:
Proceedings of the Workshop on Data-Driven Ma-
chine Translation, pages 55-62, Toulouse, France,
July.
Martin Oerder and Hermann Ney. 1993. Word
graphs: An efficient interface between continous
speech recognition and language understanding.
In IEEE International Conference on Acoustics,
Speech and Signal Processing, volume 2, pages
119-122, Minneapolis, MN, April.
Stefan Ortmanns, Hermann Ney, and Xavier Aubert.
1997. A word graph algorithm for large vocab-
ulary continuous speech recognition. Computer,
Speech and Language, 11(1):43-72, January.
Christoph Tillmann and Hermann Ney. 2000. Word
re-ordering and DP-based search in statistical ma-
chine translation. In COLING &apos;00: The 18th Int.
Conf. on Computational Linguistics, pages 850-
856, Saarbrucken, Germany, July.
Christoph Tillmann and Hermann Ney. 2002. Word
re-ordering and DP beam search for statistical
machine translation. to appear in Computational
Linguistics.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.974384375">Source Language Text Transformation | J Global Search: Lexicon Model  |I Alignment Model Language Model Transformation</title>
<note confidence="0.9248395">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 156-163.</note>
<title confidence="0.807067">Target Language Text</title>
<abstract confidence="0.990820361386139">Association for Computational Linguistics. ? Friday about how wie sieht es am Freitag aus ? whose probability is below this value multiplied with a threshold (lower than one) will not be regarded for further expansion. Histogram prunmeans that all but the hypotheses are pruned for a fixed M. For finding the most likely partial hypotheses, all hypotheses with the same covered source sentence positions are compared. After threshold and histogram pruning have been applied, we also compare all hypotheses with same covered source sentence positions and apply both pruning types again. Those hypotheses that survive the pruning are the The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search. 3 Word Graphs 3.1 Motivation It is widely accepted in the community that a significant improvement in translation quality will come from more sophisticated translation and language models. For example, a language model that goes beyond m-gram dependencies could be used, but this would be difficult to integrate into the search process. As a step towards the solution of this problem, we determine not only the single best sentence hypothesis, but also other complete sentences that the search algorithm found but that were judged worse. We can then apply rescoring with a refined model to those hypotheses. One efficient way to store the different alternatives is a word graph. Word graphs have been successfully applied in speech recognition, for the search process (Ortmanns et al., 1997) and as an interface to other systems (Oerder and Ney, 1993). (Knight and Hatzivassiloglou, 1995) and (Langkilde and Knight, 1998) propose the use of word graphs for natural language generation. In this paper, we are going to present a concept for the generation of word graphs in a machine translation system. 3.2 Bookkeeping During search, we keep a bookkeeping tree. It is not necessary to keep all the information that we need for the expansion of hypotheses during search in this structure, thus we store only the following: • the produced target word e, the covered source sentence position • a backpointer to the preceding bookkeeping entry. After the search has finished, i.e. when all source sentence positions have been translated, we trace back the best sentence in the bookkeeping tree. generate the hypotheses after search, it is not sufficient to simply trace back the complete hypotheses with the highest probabilities in the bookkeeping, because those hypotheses have been recombined. Thus, many hypotheses with a high probability have not been stored. To overcome this problem, we enhance the bookkeeping concept and generate a word graph as described in Section 3.3. 3.3 Word Graph Structure If we want to generate a word graph, we have to store both alternatives in the bookkeeping when two hypotheses are recombined. Thus, an entry in the bookkeeping structure may have several backpointers to different preceding entries. The bookkeeping structure is no longer a tree but a network where the source is the bookkeeping entry with zero covered source sentence positions and the sink is a node accounting for complete hypotheses (see Figure 3). This leads us to the concept of word graph nodes and edges containing the following information: • node — the last covered source sentence posi- • edge — the target word e, — the probabilities according to the different models: the language model and the translation submodels, — the backpointer to the preceding bookkeeping entry. After the pruning in beam search, all hypotheses that are no longer active do not have to be kept in the bookkeeping structure. Thus, can perform collection remove all those bookkeeping entries that cannot be reached from the backpointers of the active hypotheses. This reduces the size of the bookkeeping structure significantly. An example of a word graph can be seen in Figure 3. To keep the presentation simple, we chose an example without reordering of sentence positions. The words on the edges are the produced target words, and the bitvectors in the nodes show the covered source sentence positions. If an edge is labeled with two words, this means that the first English word has no equivalence in the source sentence, like &apos;just&apos; and &apos;have&apos; in Figure 3. The reference translation &apos;what did you say ?&apos; is contained in the graph, but it has a slightly lower probability than the sentence &apos;what do you say ?&apos;, which is then chosen by the single best search. The recombination of hypotheses can be seen in the nodes with two or more incoming edges: those hypotheses have been recombined, because they were indistinguishable by translation and language model state. 4 Pruning and Rescoring 4.1 Word Graph Pruning To study the effect of the word graph size on the translation quality, we produce a conservatively large word graph. Then we apply word graph with a threshold &lt; and study the change of graph error rate (see Section 5). The pruning is based on the beam search concept also used in the single best search: we determine the probability of the best sentence hypothesis in the word graph. All hypotheses in the graph which probability is lower than this maximum probability multiplied with the pruning threshold are discarded. the pruning threshold zero, the word is not pruned at all, and if = we retain only the sentence with maximum probability. 4.2 Word Graph Rescoring In single best search, a standard trigram language model is used. Search with a bigram language model is much faster, but it yields a lower translation quality. Therefore, we apply a twopass approach as it was widely used in speech recognition in the past (Ortmanns et al., 1997). This method combines both advantages in the following way: a word graph is constructed using a bigram language model and is then rescored with a trigram language model. The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997). The results of the comparison of the one-pass and the two-pass search are given in Section 5. 4.3 Extraction of N-best Lists use A* search for finding the sentences in a word graph: starting in the root of the graph, we successively expand the sentence hypotheses. The probability of the partial hypothesis is obtained by multiplying the probabilities of the edges expanded for this sentence. As rest cost estimation, we use the probabilities determined in a backward pass as follows: for each node in the graph, we calculate the probability of a best path from this node to the goal node, i.e. the highest probability for completing a partial hypothesis. This rest cost estimation is perfect because it takes the exact probability as heuristic, i.e. the probability of the partial hypothesis multiplied with the rest cost estimation yields the actual probability of the comhypothesis. Thus, the hypothesis are extracted from the graph without additional overhead of finding sentences with a lower probability. Of course, the hypotheses must not be recombined during this search. We have to keep every partial hypothesis in the priority queue in order determine the sentences. Otherwise, we might lose one of them by recombination. cardinality of coverage vector are what say you 111100 110000 you 111000 said say . 111110 have did 110000 111110 you ? said 100000 do 110000 .</abstract>
<address confidence="0.7249315">111110 111111 000000 110000</address>
<abstract confidence="0.985146133333333">you you 111000 say said ? ? 111110 do are how 111100 you just said ?</abstract>
<address confidence="0.837436333333333">111100 0 1 2 3 4 5 100000 110000 110000 111000</address>
<abstract confidence="0.906454266666666">have said Table 1: Algorithm for the Extraction of an N-best List from a Word Graph input: word graph G set node of G} , ew while non-empty do for each n in do for each edge n) edgecosts((s,n)) e ew A* search with rest cost estimation Table 2: Training and Test Corpus Statistics German English Training Sentences 58 073 Words 519 523 549 921 Words without Punctuation Marks 418 979 453 632 Vocabulary Size 7 911 4 648 Singletons 3 453 1 699 Test Sentences 251 Words 2 627 2 866 Bigram/Trigram Perplexity - 39.3/30.7 • GER (graph error rate): The graph error rate is computed by determining that sentence in the word graph that has the minimum Levenstein distance to a given reference. Thus, it is a lower bound for the word error rate and gives a measurement of what can be achieved by rescoring with more complex models. The calculation of the graph error rate is performed by a dynamic programming based algorithm. Its space complexity is the number of graph nodes times the length of the reference translation. 5.2 Effect of Word Graph Pruning In our experiments, we varied the word graph pruning threshold in order to obtain word of different densities, i.e. different numbers of hypotheses. The word graph density is computed as the total number of word graph edges divided by the number of reference sentence words — analogously to the word graph density in speech recognition. The effect of pruning on the graph error rate is shown in Table 3. The value of the pruning threshold is given as the negative logarithm of probability. Thus, = refers to pruning everything but the best hypothesis. Figure 4 shows the change in graph error rate in relation to the average graph density. We see that for graph densities up to 200, the graph error rate significantly changes if the graph is enlarged. The saturation point of the GER lies at 13% and is reached for an average graph density about 1000 which relates to a pruning threshold of 20. 40 35 0 100 200 300 400 500 600 700 800 900 1000 average graph density 30 25 o � 20 15 10 graph error rate [%] � 5 0 Table 5: Error Rates [%] and CPU Time for One-Pass and Two-Pass Search Trigram Search Bigram Search Trigram Rescoring Error Rates WER 41.41 42.70 mWER 36.46 37.26 SSER 36.90 35.54 Time 10.75 2.77+0.19 6 Conclusion We have presented a concept for constructing word graphs for statistical machine translation by extending the single best search algorithm. Experiments have shown that the graph error rate significantly decreases for rising word graph densities. The quality of the hypotheses contained in a word graph is better than of those in an N-best list. This indicates that word graph rescoring can yield a significant gain in translation quality. For the future, we plan the application of refined translation and language models for rescoring on word graphs.</abstract>
<note confidence="0.538641458333333">References Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: estimation. Linguis- Kevin Knight and Vasileios Hatzivassiloglou. 1995. many-paths generation. In of the Conference of the Association for Compu- Linguistics, 252-260, Cambridge, MA, June. Irene Langkilde and Kevin Knight. 1998. The pracvalue of n-grams in generation. In Proceedings of the International Natural Language Gener- Workshop, 248-255, Ontario, Canada, August. Franz J. Och, Nicola Ueffing, and Hermann Ney. 2001. An efficient A* search algorithm for stamachine translation. In 39th Annual Meeting of the Association for Computational Linguistics joint with EACL 2001: Proceedings of the Workshop on Data-Driven Ma- Translation, 55-62, Toulouse, France, July. Martin Oerder and Hermann Ney. 1993. Word</note>
<abstract confidence="0.877893333333333">graphs: An efficient interface between continous speech recognition and language understanding. International Conference on Acoustics, and Signal Processing, 2, pages 119-122, Minneapolis, MN, April. Stefan Ortmanns, Hermann Ney, and Xavier Aubert. 1997. A word graph algorithm for large vocabcontinuous speech recognition. and Language, January. Christoph Tillmann and Hermann Ney. 2000. Word re-ordering and DP-based search in statistical matranslation. In &apos;00: The 18th Int.</abstract>
<note confidence="0.949468166666667">on Computational Linguistics, 850- 856, Saarbrucken, Germany, July. Christoph Tillmann and Hermann Ney. 2002. Word re-ordering and DP beam search for statistical translation. appear in Computational Linguistics.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Two-level, many-paths generation.</title>
<date>1995</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics,</booktitle>
<pages>252--260</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="2051" citStr="Knight and Hatzivassiloglou, 1995" startWordPosition="328" endWordPosition="331">s could be used, but this would be difficult to integrate into the search process. As a step towards the solution of this problem, we determine not only the single best sentence hypothesis, but also other complete sentences that the search algorithm found but that were judged worse. We can then apply rescoring with a refined model to those hypotheses. One efficient way to store the different alternatives is a word graph. Word graphs have been successfully applied in speech recognition, for the search process (Ortmanns et al., 1997) and as an interface to other systems (Oerder and Ney, 1993). (Knight and Hatzivassiloglou, 1995) and (Langkilde and Knight, 1998) propose the use of word graphs for natural language generation. In this paper, we are going to present a concept for the generation of word graphs in a machine translation system. 3.2 Bookkeeping During search, we keep a bookkeeping tree. It is not necessary to keep all the information that we need for the expansion of hypotheses during search in this structure, thus we store only the following: • the produced target word e, • the covered source sentence position j, • a backpointer to the preceding bookkeeping entry. After the search has finished, i.e. when al</context>
</contexts>
<marker>Knight, Hatzivassiloglou, 1995</marker>
<rawString>Kevin Knight and Vasileios Hatzivassiloglou. 1995. Two-level, many-paths generation. In Proceedings of the Conference of the Association for Computational Linguistics, pages 252-260, Cambridge, MA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>The practical value of n-grams in generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Natural Language Generation Workshop,</booktitle>
<pages>248--255</pages>
<location>Ontario, Canada,</location>
<contexts>
<context position="2084" citStr="Langkilde and Knight, 1998" startWordPosition="333" endWordPosition="336">cult to integrate into the search process. As a step towards the solution of this problem, we determine not only the single best sentence hypothesis, but also other complete sentences that the search algorithm found but that were judged worse. We can then apply rescoring with a refined model to those hypotheses. One efficient way to store the different alternatives is a word graph. Word graphs have been successfully applied in speech recognition, for the search process (Ortmanns et al., 1997) and as an interface to other systems (Oerder and Ney, 1993). (Knight and Hatzivassiloglou, 1995) and (Langkilde and Knight, 1998) propose the use of word graphs for natural language generation. In this paper, we are going to present a concept for the generation of word graphs in a machine translation system. 3.2 Bookkeeping During search, we keep a bookkeeping tree. It is not necessary to keep all the information that we need for the expansion of hypotheses during search in this structure, thus we store only the following: • the produced target word e, • the covered source sentence position j, • a backpointer to the preceding bookkeeping entry. After the search has finished, i.e. when all source sentence positions have </context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. The practical value of n-grams in generation. In Proceedings of the International Natural Language Generation Workshop, pages 248-255, Ontario, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>An efficient A* search algorithm for statistical machine translation.</title>
<date>2001</date>
<booktitle>In ACL-EACL-2001: 39th Annual Meeting of the Association for Computational Linguistics - joint with EACL 2001: Proceedings of the Workshop on Data-Driven Machine Translation,</booktitle>
<pages>55--62</pages>
<location>Toulouse, France,</location>
<marker>Och, Ueffing, Ney, 2001</marker>
<rawString>Franz J. Och, Nicola Ueffing, and Hermann Ney. 2001. An efficient A* search algorithm for statistical machine translation. In ACL-EACL-2001: 39th Annual Meeting of the Association for Computational Linguistics - joint with EACL 2001: Proceedings of the Workshop on Data-Driven Machine Translation, pages 55-62, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Oerder</author>
<author>Hermann Ney</author>
</authors>
<title>Word graphs: An efficient interface between continous speech recognition and language understanding.</title>
<date>1993</date>
<booktitle>In IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<volume>2</volume>
<pages>119--122</pages>
<location>Minneapolis, MN,</location>
<contexts>
<context position="2014" citStr="Oerder and Ney, 1993" startWordPosition="324" endWordPosition="327">eyond m-gram dependencies could be used, but this would be difficult to integrate into the search process. As a step towards the solution of this problem, we determine not only the single best sentence hypothesis, but also other complete sentences that the search algorithm found but that were judged worse. We can then apply rescoring with a refined model to those hypotheses. One efficient way to store the different alternatives is a word graph. Word graphs have been successfully applied in speech recognition, for the search process (Ortmanns et al., 1997) and as an interface to other systems (Oerder and Ney, 1993). (Knight and Hatzivassiloglou, 1995) and (Langkilde and Knight, 1998) propose the use of word graphs for natural language generation. In this paper, we are going to present a concept for the generation of word graphs in a machine translation system. 3.2 Bookkeeping During search, we keep a bookkeeping tree. It is not necessary to keep all the information that we need for the expansion of hypotheses during search in this structure, thus we store only the following: • the produced target word e, • the covered source sentence position j, • a backpointer to the preceding bookkeeping entry. After </context>
</contexts>
<marker>Oerder, Ney, 1993</marker>
<rawString>Martin Oerder and Hermann Ney. 1993. Word graphs: An efficient interface between continous speech recognition and language understanding. In IEEE International Conference on Acoustics, Speech and Signal Processing, volume 2, pages 119-122, Minneapolis, MN, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Ortmanns</author>
<author>Hermann Ney</author>
<author>Xavier Aubert</author>
</authors>
<title>A word graph algorithm for large vocabulary continuous speech recognition.</title>
<date>1997</date>
<journal>Computer, Speech and Language,</journal>
<pages>11--1</pages>
<contexts>
<context position="1954" citStr="Ortmanns et al., 1997" startWordPosition="313" endWordPosition="316">nd language models. For example, a language model that goes beyond m-gram dependencies could be used, but this would be difficult to integrate into the search process. As a step towards the solution of this problem, we determine not only the single best sentence hypothesis, but also other complete sentences that the search algorithm found but that were judged worse. We can then apply rescoring with a refined model to those hypotheses. One efficient way to store the different alternatives is a word graph. Word graphs have been successfully applied in speech recognition, for the search process (Ortmanns et al., 1997) and as an interface to other systems (Oerder and Ney, 1993). (Knight and Hatzivassiloglou, 1995) and (Langkilde and Knight, 1998) propose the use of word graphs for natural language generation. In this paper, we are going to present a concept for the generation of word graphs in a machine translation system. 3.2 Bookkeeping During search, we keep a bookkeeping tree. It is not necessary to keep all the information that we need for the expansion of hypotheses during search in this structure, thus we store only the following: • the produced target word e, • the covered source sentence position j</context>
<context position="6182" citStr="Ortmanns et al., 1997" startWordPosition="1043" endWordPosition="1046">sentence hypothesis in the word graph. All hypotheses in the graph which probability is lower than this maximum probability multiplied with the pruning threshold are discarded. If the pruning threshold t is zero, the word graph is not pruned at all, and if t = 1, we retain only the sentence with maximum probability. 4.2 Word Graph Rescoring In single best search, a standard trigram language model is used. Search with a bigram language model is much faster, but it yields a lower translation quality. Therefore, we apply a twopass approach as it was widely used in speech recognition in the past (Ortmanns et al., 1997). This method combines both advantages in the following way: a word graph is constructed using a bigram language model and is then rescored with a trigram language model. The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997). The results of the comparison of the one-pass and the two-pass search are given in Section 5. 4.3 Extraction of N-best Lists We use A* search for finding the N best sentences in a word graph: starting in the root of the graph, we successively expand the sentence hypotheses. The probability of the partial hypothesis </context>
</contexts>
<marker>Ortmanns, Ney, Aubert, 1997</marker>
<rawString>Stefan Ortmanns, Hermann Ney, and Xavier Aubert. 1997. A word graph algorithm for large vocabulary continuous speech recognition. Computer, Speech and Language, 11(1):43-72, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word re-ordering and DP-based search in statistical machine translation.</title>
<date>2000</date>
<booktitle>In COLING &apos;00: The 18th Int. Conf. on Computational Linguistics,</booktitle>
<pages>850--856</pages>
<location>Saarbrucken, Germany,</location>
<marker>Tillmann, Ney, 2000</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2000. Word re-ordering and DP-based search in statistical machine translation. In COLING &apos;00: The 18th Int. Conf. on Computational Linguistics, pages 850-856, Saarbrucken, Germany, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word re-ordering and DP beam search for statistical machine translation.</title>
<date>2002</date>
<note>to appear in Computational Linguistics.</note>
<marker>Tillmann, Ney, 2002</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2002. Word re-ordering and DP beam search for statistical machine translation. to appear in Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>