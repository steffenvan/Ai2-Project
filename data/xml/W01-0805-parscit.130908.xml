<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.999544">
A Meta-Algorithm for the Generation of Referring Expressions
</title>
<author confidence="0.988003">
Emiel Krahmer, Sebastiaan van Erk and Andr´e Verleg
</author>
<affiliation confidence="0.7921795">
TU/e, Eindhoven University of Technology,
The Netherlands
</affiliation>
<email confidence="0.999545">
email: E.J.Krahmer@tue.nl
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888071428572">
This paper describes a new approach
to the generation of referring expres-
sions. We propose to formalize a scene
as a labeled directed graph and describe
content selection as a subgraph con-
struction problem. Cost functions are
used to guide the search process and
to give preference to some solutions
over others. The resulting graph al-
gorithm can be seen as a meta-algorithm
in the sense that defining cost functions
in different ways allows us to mimic —
and even improve— a number of well-
known algorithms.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99947507936508">
The generation of referring expressions is one
of the most common tasks in natural language
generation, and has been addressed by many re-
searchers in the past two decades (including Ap-
pelt 1985, Dale 1992, Reiter 1990, Dale &amp; Had-
dock 1991, Dale &amp; Reiter 1995, Horacek 1997,
Stone &amp; Webber 1998, Krahmer &amp; Theune 1999
and van Deemter 2000). As a result, there are
many different algorithms for the generation of
referring expressions, each with its own object-
ives: some aim at producing the shortest possible
description, others focus on efficiency or realistic
output. The degree of detail in which the various
algorithms are described differs considerably, and
as a result it is often difficult to compare the vari-
ous proposals. In addition, most of the algorithms
are primarily concerned with the generation of de-
scriptions only using properties of the target ob-
ject. Consequently, the problem of generating re-
lational descriptions (i.e., descriptions which in-
corporate references to other objects to single out
the target object) has not received the attention it
deserves.
In this paper, we describe a general, graph-
theoretic approach to the generation of referring
expressions. We propose to formalize a scene
(i.e., a domain of objects and their properties and
relations) as a labeled directed graph and describe
the content selection problem —which proper-
ties and relations to include in a description for
an object?— as a subgraph construction problem.
The graph perspective has three main advantages.
The first one is that there are many attractive al-
gorithms for dealing with graph structures. In
this paper, we describe a branch and bound al-
gorithm for finding the relevant subgraphs, where
we use cost functions to guide the search pro-
cess. Arguably, the proposed algorithm is a meta-
algorithm, in the sense that by defining the cost
function in different ways, we can mimic various
well-known algorithms for the generation of re-
ferring expressions. A second advantage of the
graph-theoretical framework is that it does not run
into problems with relational descriptions, due to
the fact that properties and relations are formal-
ized in the same way, namely as edges in a graph.
The third advantage is that the combined usage
of graphs and cost-functions paves the way for
a natural integration of traditional rule-based ap-
proaches to generation with more recent statist-
ical approaches (e.g., Langkilde &amp; Knight 1998,
Malouf 2000) in a single algorithm.
The outline of this paper is as follows. In sec-
tion 2, we describe how scenes can be described as
labeled directed graphs and show how content se-
lection can be formalized as a subgraph construc-
tion problem. Section 3 contains a sketch of the
branch and bound algorithm, which is illustrated
with a worked example. In section 4 it is argued
that by defining cost functions in different ways,
we can mimic various well-known algorithms for
the generation of referring expressions. We end
with some concluding remarks in section 5.
</bodyText>
<sectionHeader confidence="0.984848" genericHeader="introduction">
2 Graphs
</sectionHeader>
<bodyText confidence="0.3154">
Consider the following scene:
</bodyText>
<figureCaption confidence="0.999508">
Figure 1: An example scene
</figureCaption>
<bodyText confidence="0.995035476190476">
In this scene, as in any other scene, we see a
finite set of entities with properties and
relations . In this particular scene, the set
is the set of entities,
dog, chihuahua, doghouse, small, large, white,
brown is the set of properties and next to,
left of, right of, contain, in is the set of relations.
A scene can be represented in various ways. One
common representation is to build a database,
listing the properties of each element of :
Here we take a different approach and represent a
scene as a labeled directed graph. Let
be the set of labels (with and disjoint, i.e.,
). Then, a labeled directed graph
, where is the set of vertices
(or nodes) and is the set of labeled
directed arcs (or edges). The scene given in Fig-
ure 1 can be represented by the graph in Figure 2.
Keep in mind that the labels are only added to
ease reference to nodes. Notice also that proper-
ties (such as being a dog) are always modelled as
loops, i.e., edges which start and end in the same
node, while relations may (but need not) have dif-
ferent start and end nodes.
Now the content determination problem for re-
ferring expressions can be formulated as a graph
construction task. In order to decide which in-
formation to include in a referring expression for
an object , we construct a connected dir-
ected labeled graph over the set of labels and
an arbitrary set of nodes, but including . This
graph can be understood as the “meaning repres-
entation” from which a referring expression can
be generated by a linguistic realizer. Informally,
we say that a graph refers to a given entity iff
the graph can be “placed over” the scene graph
in such a way that the node being referred to is
“placed over” the given entity and each edge can
be “placed over” an edge labeled with the same la-
bel. Furthermore, a graph is distinguishing iff it
refers to exactly one node in the scene graph.
Consider the three graphs in Figure 3. Here and
elsewhere circled nodes stand for the intended ref-
erent. Graph (i) refers to all nodes of the graph
in Figure 2 (every object in the scene is next to
some other object), graph (ii) can refer to both
and , and graph (iii) is distinguishing in that it
can only refer to . Notice that the three graphs
might be realized as something next to something
else, a chihuahua and the dog in the doghouse re-
spectively. In this paper, we will concentrate on
the generation of distinguishing graphs.
Formally, the notion that a graph
can be “placed over” another graph
corresponds to the notion of a sub-
graph isomorphism. can be “placed over” iff
there exists a subgraph of such
that is isomorphic to . is isomorphic to
iff there exists a bijection such that
for all nodes and all
In words: the bijective function maps all the
nodes in to corresponding nodes in , in such
a way that any edge with label between nodes
</bodyText>
<figure confidence="0.994931166666667">
...
: dog( ), brown( ), ..., in ( , )
...
...
...
: doghouse ( ), white ( ), ..., right of ( , )
</figure>
<figureCaption confidence="0.843634">
Figure 2: A graph representation of Figure 1.
</figureCaption>
<figure confidence="0.999636833333333">
next_to
doghouse
doghouse
chihuahua
chihuahua
dog
brown
small
next_to
left_of
small
dog
brown
d1
d2
right_of
next_to
left_of
contains in
next_to
next_to
right_of
next_to
left_of
d3
d4
large
right_of
large
white
white
next_to
(i)
chihuahua
(ii)
(iii)
</figure>
<figureCaption confidence="0.9916945">
Figure 3: Some graphs for referring expressions,
with circles around the intended referent.
</figureCaption>
<bodyText confidence="0.999081611111111">
and in is matched by an edge with the same
label between the counterparts of and , i.e.,
and respectively. When is isomorphic
to some subgraph of by an isomorphism , we
write .
Given a graph and a node in , and a graph
and a node in , we define that the pair
refers to the pair iff is connected and
and . Furthermore,
uniquely refers to (i.e., is distin-
guishing) iff refers to and there is
no node in different from such that
refers to . The problem considered in this
paper can now be formalized as follows: given a
graph and a node in , find a pair such
that uniquely refers to .
Consider, for instance, the task of finding a pair
which uniquely refers to the node labeled
in Figure 2. It is easily seen that there are a
number of such pairs, three of which are depic-
ted in Figure 4. We would like to have a mechan-
ism which allows us to give certain solutions pref-
erence over other solutions. For this purpose we
shall use cost-functions. In general, a cost func-
tion is a function which assigns to each sub-
graph of a scene graph a positive number. As we
shall see, by defining cost functions in different
ways, we can mimic various algorithms for the
generation of referring expressions known from
the literature.
A note on problem complexity The basic de-
cision problem for subgraph isomorphism (i.e.,
testing whether a graph is isomorphic to a sub-
graph of ) is known to be NP complete (see
e.g., Garey &amp; Johnson 1979). Here we are in-
terested in connected , but unfortunately that
</bodyText>
<figure confidence="0.484974666666667">
dog doghouse
in
in
</figure>
<figureCaption confidence="0.9579075">
Figure 4: Three distinguishing node-graph pairs
referring to in Figure 2.
</figureCaption>
<bodyText confidence="0.999475190476191">
restriction does not reduce the theoretical com-
plexity. However, as soon as we define an up-
per bound on the number of edges in a distin-
guishing graph, the problem loses its intractability
and becomes solvable in polynomial time.
Such a restriction is rather harmless for our cur-
rent purposes, as it would only prohibit the gen-
eration of distinguishing descriptions with more
than properties, for an arbitrary large . In
general, there are various classes of graphs for
which the subgraph isomorphism problem can be
solved much more efficiently, without postulating
upper bounds. For instance, if and are planar
graphs the problem can be solved in time linear in
the number of nodes of (Eppstein 1999). Ba-
sically, a planar graph is one which can be drawn
on a plane in such a way that there are no cross-
ing edges (thus, for instance, the graph in Figure
2 is planar). It is worth investigating to what ex-
tent planar graphs suffice for the generation of re-
ferring expressions.
</bodyText>
<sectionHeader confidence="0.980586" genericHeader="method">
3 Outline of the algorithm
</sectionHeader>
<bodyText confidence="0.99036185">
In this section we give a high-level sketch of
the algorithm. The algorithm (called make-
ReferringExpression) consists of two main
components, a subgraph construction algorithm
(called findGraph) and a subgraph isomorphism
testing algorithm (called matchGraphs). We
assume that a scene graph is given.
The algorithm systematically tries all relevant
subgraphs of the scene graph by starting with
the subgraph containing only the node (the
target object) and expanding it recursively by
trying to add edges from which are adjacent to
the subgraph constructed so far. In this way
we know that the results will be a connected sub-
graph. We refer to this set of adjacent edges as the
neighbors in (notation: .neighbors( )).
The algorithm returns the cheapest distinguishing
subgraph which refers to , if such a distin-
guishing graph exists, otherwise it returns the
empty graph .
</bodyText>
<subsectionHeader confidence="0.999012">
3.1 Cost functions
</subsectionHeader>
<bodyText confidence="0.9824336875">
We use cost functions to guide the search process
and to give preference to some solutions over oth-
ers. If is a subgraph of , then
the costs of , notation , are given by sum-
ming over the costs associated with the nodes and
edges of H. Formally:
We require the cost function to be monotonic.
That is, adding an edge to a (non-empty) graph can
never result in a cheaper graph. Formally:
edges:
cost cost
This assumption helps reducing the search space
substantially, since extensions of subgraphs with
a cost greater than the best subgraph found so far
can safely be ignored. The costs of the empty, un-
defined graph are infinite, i.e. .
</bodyText>
<subsectionHeader confidence="0.997862">
3.2 Worked example
</subsectionHeader>
<bodyText confidence="0.9759152">
We now illustrate the algorithm with an example.
Suppose the scene graph is as given in Figure
2, and that we want to generate a referring expres-
sion for object in this graph. Let us assume
for the sake of illustration that the cost function
is defined in such a way that adding a node or an
edge always costs 1 point. Thus: for each
and for each : .
Here and elsewhere, we use the following notation. Let
be a graph and an edge, then is the graph
</bodyText>
<figure confidence="0.910019">
node1 node2 .
dog
doghouse
dog doghouse
in
brown
small
in
large
white
makeReferringExpression( )
</figure>
<equation confidence="0.921418">
bestGraph := ;
:= ;
return findGraph(, bestGraph, );
findGraph( , bestGraph, )
</equation>
<bodyText confidence="0.89276325">
if (bestGraph.cost .cost) then return bestGraph fi;
distractors := nodes matchGraphs
if (distractors = ) then return fi;
for each edge .neighbors( ) do
findGraph( , bestGraph, );
if .cost bestGraph.cost then bestGraph := fi;
rof;
return bestGraph;
</bodyText>
<figureCaption confidence="0.9744255">
Figure 5: Sketch of the main function (makeReferringExpression) and the subgraph construction func-
tion (findGraph).
</figureCaption>
<equation confidence="0.942736714285714">
;
H =
(i)
H =
in
H =
(iii)
</equation>
<figureCaption confidence="0.982705">
Figure 6: Three values for in the generation
</figureCaption>
<bodyText confidence="0.995344815789474">
process for .
(In the next section we describe a number of more
interesting cost functions and discuss the impact
these have on the output of the algorithm.) We
call the function makeReferringExpression (given
in Figure 5) with as parameter. In this function
the variable bestGraph (for the best solution found
so far) is initialized as the empty graph and the
variable (for the distinguishing subgraph un-
der construction) is initialized as the graph con-
taining only node ((i) in Figure 6). Then the
function findGraph (see also Figure 5) is called,
with parameters , bestGraph and . In this
function, first it is checked whether the costs of
(the graph under construction) are higher than
the costs of the bestGraph found so far. If that is
the case, it is not worth extending since, due
to the monotonicity constraint, it will never end
up being cheaper than the current bestGraph. The
initial value of bestGraph is the empty, undefined
graph, and since its costs are astronomically high,
we continue. Then the set of distractors (the ob-
jects from which the intended referent should be
distinguished, Dale &amp; Reiter 1995) is calculated.
In terms of the graph perspective this is the set of
nodes in the scene graph (other then the target
node ) to which the graph refers. It is easily
seen that the initial value of , i.e., (i) in Figure
6, refers to every node in . Hence, as one would
expect, the initial set of distractors is nodes
. Next we check whether the current set of
distractors is empty. If so, we have managed to
find a distinguishing graph, which is subsequently
stored in the variable bestGraph. In this first iter-
ation, this is obviously not the case and we con-
tinue, recursively trying to extend by adding
adjacent (neighboring) edges until either a distin-
guishing graph has been constructed (all distract-
</bodyText>
<equation confidence="0.941323272727273">
(ii)
brown
left_of
chihuahua
matchGraphs( , , , )
if .edges( , ) .edges( , ) then return false fi;
matching :=
;
:=.neighbors( );
return matchHelper(matching, ,);
matchHelper(matching, , )
</equation>
<bodyText confidence="0.9051039">
if matching then return true fi;
if then return false fi;
choose a fresh, unmatched from ;
might be matched to ;
for each do
if is a valid extension of the mapping
then if matchHelper(matching , ,) then return true fi;
fi;
rof;
return false;
</bodyText>
<figureCaption confidence="0.999233">
Figure 7: Sketch of the function testing for subgraph isomorphism (matchGraphs).
</figureCaption>
<bodyText confidence="0.99981475">
ors are ruled out) or the costs of exceed the costs
of the bestGraph found so far. While bestGraph
is still the empty set (i.e., no distinguishing graph
has been found yet), the algorithm continues un-
til is a distinguishing graph. Which is the first
distinguishing graph to be found (if one or more
exist) depends on the order in which the adjacent
edges are tried. Suppose for the sake of argument
that the first distinguishing graph to be found is (ii)
in Figure 6. This graph is returned and stored in
bestGraph. The costs associated with this graph
are 5 points (two nodes and three edges). At this
stage in the generation process only graphs with
lower costs are worth investigating, which yields a
drastic reduction of the search space. In fact, there
are only a few distinguishing graphs which cost
less. After a number of iterations the algorithm
will find the cheapest solution (given this particu-
lar, simple definition of the cost function), which
is (iii) in Figure 6.
</bodyText>
<subsectionHeader confidence="0.969549">
3.3 Subgraph Isomorphism testing
</subsectionHeader>
<bodyText confidence="0.995138095238095">
Figure 7 contains a sketch of the part of the al-
gorithm which tests for subgraph isomorphism,
matchGraphs. This function is called each time
the distractor set is calculated. It tests whether the
pair can refer to , or put differently,
it checks whether there exists an isomorphism
such that with . The function
matchGraphs first determines whether the looping
edges starting from node (i.e., the properties of
) match those of . If not (e.g., is a dog and
is a doghouse), we can immediately discard the
matching. Otherwise we start with the matching
, and expand it recursively. Each recur-
sion step afresh and as yet unmatched node from
is selected which is adjacent to one of the nodes
in the current matching. For each we calculate
the set of possible nodes in to which can
be matched. This set consist of all the nodes in
which have the same looping edges as and the
same edges to and from other nodes in the domain
of the current matching function :
</bodyText>
<construct confidence="0.3082686">
nodes
edges edges
neighbors Dom
edges edges
edges edges
</construct>
<bodyText confidence="0.996640222222222">
The matching can now be extended with ,
for . The algorithm then branches over all
these possibilities. Once a mapping has been
found which has exactly as much elements as
has nodes, we have found a subgraph isomorph-
ism. If there are still unmatched nodes in or
if all possible extensions with a node have been
checked and no matching could be found, the test
for subgraph isomorphism has failed.
</bodyText>
<subsectionHeader confidence="0.994971">
3.4 A note on the implementation
</subsectionHeader>
<bodyText confidence="0.999972428571428">
The basic algorithm outlined in Figures 5 and 7
has been implemented in Java. Various optimiz-
ations increase the efficiency of the algorithm, as
certain calculations need not be repeated each iter-
ation (e.g., the set .neighbors( )). In addition,
the user has the possibility of specifying the cost
function in a way which he or she sees fit.
</bodyText>
<sectionHeader confidence="0.996728" genericHeader="method">
4 Search methods and cost functions
</sectionHeader>
<bodyText confidence="0.9999715">
Arguably, the algorithm outlined above is a meta-
algorithm, since by formulating the cost func-
tion in certain ways we can simulate various al-
gorithms known from the generation literature.
</bodyText>
<subsectionHeader confidence="0.981839">
4.1 Full (relational) Brevity Algorithm
</subsectionHeader>
<bodyText confidence="0.999977722222222">
The algorithm described in the previous section
can be seen as a generalization of Dale’s (1992)
Full Brevity algorithm, in the sense that there
is a guarantee that the algorithm will output the
shortest possible description, if one exists. It is
also an extension of the Full Brevity algorithm,
since it allows for relational descriptions, as does
the Dale &amp; Haddock (1991) algorithm. The latter
algorithm has a problem with infinite recursions;
in principle their algorithm could output descrip-
tions like “the dog in the doghouse which con-
tains a dog which is in a doghouse which ...etc.”
Dale &amp; Haddock propose to solve this problem
by stipulating that a property or relation may only
be included once. In the graph-based model de-
scribed above the possibility of such infinite re-
cursions does not arise, since a particular edge is
either present in a graph or not.
</bodyText>
<footnote confidence="0.78777175">
Notice incidentally that Dale’s (1992) Greedy Heuristic
algorithm can also be cast in the graph framework, by sort-
ing edges on their descriptive power (measured as a count
of the number of occurrences of this particular edge in the
scene graph). The algorithm then adds the most discrimin-
ating edge first (or the cheapest, if there are various equally
distinguishing edges) and repeats this process until a distin-
guishing graph is found.
</footnote>
<subsectionHeader confidence="0.966645">
4.2 Incremental Algorithm
</subsectionHeader>
<bodyText confidence="0.999567">
Dale &amp; Reiter’s (1995) Incremental Algorithm,
generally considered the state of the art in this
field, has the following characteristic properties.
</bodyText>
<listItem confidence="0.701908384615385">
(1) It defines a list of preferred attributes, list-
ing the attributes which human speakers prefer
for a certain domain. For example, when dis-
cussing domestic animals, speakers usually first
describe the “type” of animal (dog, cat), before
absolute properties such as “color” are used. If
that still is not sufficient to produce a distin-
guishing description, relative properties such as
“size” can be included. Thus, the list of preferred
attributes for this particular domain could be
type, color, size. The Incremental Algorithm
now simply iterates through this list, adding a
property if it rules out any distractors not pre-
</listItem>
<bodyText confidence="0.982629230769231">
viously ruled out. (2) The algorithm always in-
cludes the “type” attribute, even if it is not distin-
guishing. And (3) the algorithm allows subsump-
tion hierarchies on certain attributes (most notably
for the “type” attribute) stating things like a fox ter-
rier is a dog, and a dog is an animal. In such a hier-
archy we can specify what the basic level value is
(in this case it is dog). Dale &amp; Reiter claim that
there is a general preference for basic level values,
and hence their algorithm includes the basic level
value of an attribute, unless values subsumed by
the basic level value rule out more distractors.
These properties can be incorporated in the
graph framework in the following way. (1) The
list of preferred attributes can easily be modelled
using the cost function. All “type” edges should
be cheaper than all other edges (in fact, they could
be for free), and moreover, the edges correspond-
ing to absolute properties should cost less than
those corresponding to relative ones. This gives
us exactly the effect of having preferred attributes.
(2) It also implies that the “type” of an object is
always included if it is in any way distinguishing.
That by itself does not guarantee that type is al-
ways is included. The most principled and effi-
cient way to achieve that would be to reformu-
late the findGraph algorithm in such a way that
the “type” loop is always included. (Given such a
minor modification, the algorithm described in the
previous section would output (iii) from Figure 3
instead of (iii) from Figure 6 when applied to .)
Such a general modification might be undesirable
from an empirical point of view however, since
in various domains it is very common to not in-
clude type information, for instance when the do-
main contains only objects of the same type (see
van der Sluis &amp; Krahmer 2001). (3) The subsump-
tion hierarchy can be modelled in the same way
as preferred attributes are: for a given attribute,
the basic level value should have the lowest costs
and the values farthest away from the basic level
value should have the highest costs. This implies
that adding an edge labeled dog is cheaper than
adding an edge labeled chihuahua, unless more
(or more expensive) edges are needed to build
a distinguishing graph including dog than are re-
quired for the graph including chihuahua. Assum-
ing that the scene representation is well-defined,
the algorithm never outputs a graph which con-
tains both dog and chihuahua, since there will al-
ways be a cheaper distinguishing graph omitting
one of the two edges.
So, we can recast the Incremental Algorithm
quite easily in terms of graphs. Note that the
original Incremental Algorithm only operates on
properties, looped edges in graph terminology. It
is worth stressing that when all edges in the scene
graph are of the looping variety, testing for sub-
graph isomorphism becomes trivial and we re-
gain polynomial complexity. However, the above
graph-theoretical formalization of the Incremental
Algorithm does not fully exploit the possibilities
offered by the graph framework and the use of cost
functions. First, from the graph-theoretical per-
spective the generation of relational descriptions
poses no problems whatsoever, while the incre-
mental generation of relational descriptions is by
no means trivial (see e.g., Theune 2000, Krahmer
&amp; Theune 1999). In fact, while it could be argued
to some extent that incremental selection of prop-
erties is psychologically plausible, this somehow
seems less plausible for incremental generation of
relational extensions. Notice that the use of a
As Dale &amp; Reiter (1995:248) point out, redundant prop-
erties are not uncommon. That is: in certain situations people
may describe an object as “the white bird” even though
the simpler “the bird” would have been sufficient (cf. Pech-
mann 1989, see also Krahmer &amp; Theune 1999 for discus-
sion). However, a similar argument seems somewhat far-
fetched when applied to relations. It is unlikely that someone
would describe an object as “the dog next to the tree in front
of the garage” in a situation where “the dog in front of the
garage” would suffice.
cost function to simulate subsumption hierarch-
ies for properties carries over directly to relations;
for instance, the costs of adding a edge labeled
next to should be less than those of adding one
labeled left of or right of. Hence, next to will be pre-
ferred, unless using left of or right of has more dis-
criminative power. Another advantage of the way
the graph-based algorithm models the list of pre-
ferred attributes is that more fine-grained distinc-
tions can be made than can be done in the Incre-
mental Algorithm. In particular, we are not forced
to say that values of the attribute “type” are always
preferred over values of the attribute “color”. In-
stead we have the freedom to assign edges labeled
with a common type value (e.g., dog) a lower cost
than edges labeled with uncommon colors (such
as Vandyke-brown), while at the same time edges
labeled with obscure type values, such as polish
owczarek nizinny sheepdog, can be given a higher
cost than edges labeled with common colors such
as brown.
</bodyText>
<subsectionHeader confidence="0.990155">
4.3 Stochastic cost functions
</subsectionHeader>
<bodyText confidence="0.99320004">
One of the important open questions in natural
language generation is how the common, rule-
based approaches to generation can be combined
with recent insights from statistical NLP (see e.g.,
Langkilde &amp; Knight 1998, Malouf 2000 for par-
tial answers). Indeed, when looking at the Incre-
mental Algorithm, for instance, it is not directly
obvious how statistical information can be integ-
rated in the algorithm. Arguably, this is differ-
ent when we have cost functions. One can easily
imagine deriving a stochastic cost function from a
sufficiently large corpus and using it in the graph-
theoretical framework (the result looks like but is
not quite a Markov Model). As a first approxima-
tion, we could define the costs of adding an edge
in terms of the probability that oc-
curs in a distinguishing description (estimated by
counting occurrences):
log
Thus, properties which occur frequently are
cheap, properties which are relatively rare are
expensive. In this way, we would probably derive
that dog is indeed less expensive than Vandyke
brown and that brown is less expensive than polish
owczarek nizinny sheepdog.
</bodyText>
<sectionHeader confidence="0.975268" genericHeader="conclusions">
5 Concluding remarks
</sectionHeader>
<bodyText confidence="0.997019706666667">
In this paper, we have presented a general graph-
theoretical approach to content-determination for
referring expressions. The basic algorithm has
clear computational properties: it is NP com-
plete, but there exist various modifications (a
ban on non-looping edges, planar graphs, upper
bound to the number of edges in a distinguish-
ing graph) which make the algorithm polynomial.
The algorithm is fully implemented. The graph
perspective has a number of attractive proper-
ties. The generation of relational descriptions is
straightforward; the problems which plague some
other algorithms for the generation of relational
descriptions do not arise. The use of cost func-
tions allows us to model different search meth-
ods, each restricting the search space in its own
way. By defining cost functions in different ways,
we can model and extend various well-known al-
gorithms from the literature such as the Full Brev-
ity Algorithm and the Incremental Algorithm. In
addition, the use of cost functions paves the way
for integrating statistical information directly in
the generation process.
Various important ingredients of other genera-
tion algorithms can be captured in the algorithm
proposed here as well. For instance, Horacek
(1997) points out that an algorithm should not col-
lect a set of properties which cannot be realized
given the constraints of the grammar. This prob-
lem can be solved, following Horacek’s sugges-
tion, by slightly modifying the algorithm in such
a way that for each potential edge it is immedi-
ately investigated whether it can expressed by the
realizer. Van Deemter’s (2000) proposal to gener-
ate (distributional) distinguishing plural descrip-
A final advantage of the graph model certainly deserves
further investigation is the following. We can look at a graph
such as that in Figure 2 as a Kripke model. The advantage
of this way of looking at it, is that we can use tools from
modal logic to reason about these structures. For example,
we can reformulate the problem of determining the content
of a distinguishing description in terms of hybrid logic (see
e.g., Blackburn 2000) as follows:
A
In words: when we want to refer to node, we are looking for
that distinguishing formula which is true of (“at”) but not
of any different from. One advantage of this perspective
is that logical properties which are usually considered prob-
lematic from a generation perspective (such as not having a
certain property), fit in very well with the logical perspective.
tions (such as the dogs) can also be modelled quite
easily. Van Deemter’s algorithm takes as input a
set of objects, which in our case, translates into a
set of nodes from the scene graph. The algorithm
should be reformulated in such a way that it tries to
generate a subgraph which can refer to each of the
nodes in the set, but not to any of the nodes in the
scene graph outside this set. Krahmer &amp; Theune
(1999) present an extension of the Incremental Al-
gorithm which takes context into account. They
argue that an object which has been mentioned in
the recent context is somehow salient, and hence
can be referred to using fewer properties. This
is modelled by assigning salience weights to ob-
jects (basically using a version of Centering The-
ory (Grosz et al. 1995) augmented with a recency
effect), and by defining the set of distractors as
the set of objects with a salience weight higher or
equal than that of the target object. In terms of the
graph-theoretical framework, one can easily ima-
gine assigning salience weights to the nodes in the
scene graph, and restricting the distractor set es-
sentially as Krahmer &amp; Theune do. In this way,
distinguishing graphs for salient objects will gen-
erally be smaller than those of non-salient objects.
</bodyText>
<sectionHeader confidence="0.994408" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999924">
Thanks are due to Alexander Koller, Kees van
Deemter, Paul Piwek, Mari¨et Theune and two an-
onymous referees for discussions and comments
on an earlier version of this paper.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999699135593221">
Appelt, D. (1985), Planning English Referring Expres-
sions, Artificial Intelligence 26:1-33.
Blackburn, P. (2000), Representation, Reasoning, and
Relational Structure: A Hybrid Logic Manifesto,
Logic Journal of the IGPL 8(3):339-365.
Dale, R. (1992), Generating Referring Expressions:
Constructing Descriptions in a Domain of Objects
and Processes, MIT Press, Cambridge, Massachu-
setts.
Dale, R. &amp; N. Haddock (1991), Generating Refer-
ring Expressions Involving Relations, Proceedings
ofEACL, Berlin, 161-166.
Dale, R. &amp; E. Reiter (1995), Computational Interpret-
ations of the Gricean Maxims in the Generation of
Referring Expressions, Cognitive Science 18: 233-
263.
van Deemter, K. (2000), Generating Vague Descrip-
tions, Proceedings INLG, Mitzpe Ramon.
Eppstein, D. (1999), Subgraph Isomorphism in Planar
Graphs and Related Problems, J. Graph Algorithms
and Applications 3(3):1-27.
Garey, M. &amp; D. Johnson (1979), Computers and
Intractability: A Guide to the Theory of NP-
Completeness, W.H. Freeman.
Grosz, B., A. Joshi &amp; S. Weinstein (1995), Centering:
A Framework for Modeling the Local Coherence
of Discourse, Computational Linguistics 21(2):203-
225.
Horacek, H. (1997), An Algorithm for Generating Ref-
erential Descriptions with Flexible Interfaces, Pro-
ceedings of the 35th ACL/EACL, Madrid, 206-213.
Krahmer, E. &amp; M. Theune (1999), Efficient Generation
of Descriptions in Context, Proceedings of Work-
shop on Generation of Nominals, R. Kibble and K.
van Deemter (eds.), Utrecht, The Netherlands.
Langkilde, I. &amp; K. Knight (1998), The Practical Value
of -Grams in Generation, Proceedings INLG,
Niagara-on-the-lake, Ontario, 248-255.
Malouf, R., (2000), The Order of Prenominal Adject-
ives in Natural Language Generation, Proceedings
of the 38th ACL , Hong Kong.
Pechmann, T. (1989), Incremental Speech Produc-
tion and Referential Overspecification, Linguistics
27:98–110.
Reiter, E. (1990), The Computational Complexity of
Avoiding Conversational Implicatures, Proceedings
of the 28th ACL , 97-104.
van der Sluis, I. &amp; E. Krahmer (2001), Generating
Referring Expressions in a Multimodal Context:
An Empirically Motivated Approach, Proceedings
CLIN, W. Daelemans et al. (eds), Rodopi, Amster-
dam/Atlanta.
Stone, M. &amp; B. Webber (1998), Textual Economy
Through Close Coupling of Syntax and Semantics,
Proceedings INLG, Niagara-on-the-lake, Ontario,
178-187.
Theune, M. (2000), From Data to Speech: Language
Generation in Context, Ph.D. dissertation, Eind-
hoven University of Technology.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.276312">
<title confidence="0.99952">A Meta-Algorithm for the Generation of Referring Expressions</title>
<author confidence="0.625966">Sebastiaan van_Erk Verleg Krahmer</author>
<affiliation confidence="0.336156">TU/e, Eindhoven University of</affiliation>
<address confidence="0.408404">The Netherlands</address>
<abstract confidence="0.998062866666667">This paper describes a new approach to the generation of referring expres- We propose to formalize a as a labeled directed graph and describe selection a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Appelt</author>
</authors>
<title>Planning English Referring Expressions,</title>
<date>1985</date>
<journal>Artificial Intelligence</journal>
<pages>26--1</pages>
<contexts>
<context position="922" citStr="Appelt 1985" startWordPosition="145" endWordPosition="147"> scene as a labeled directed graph and describe content selection as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms. 1 Introduction The generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including Appelt 1985, Dale 1992, Reiter 1990, Dale &amp; Haddock 1991, Dale &amp; Reiter 1995, Horacek 1997, Stone &amp; Webber 1998, Krahmer &amp; Theune 1999 and van Deemter 2000). As a result, there are many different algorithms for the generation of referring expressions, each with its own objectives: some aim at producing the shortest possible description, others focus on efficiency or realistic output. The degree of detail in which the various algorithms are described differs considerably, and as a result it is often difficult to compare the various proposals. In addition, most of the algorithms are primarily concerned wit</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Appelt, D. (1985), Planning English Referring Expressions, Artificial Intelligence 26:1-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blackburn</author>
</authors>
<title>Representation, Reasoning, and Relational Structure: A Hybrid Logic Manifesto,</title>
<date>2000</date>
<journal>Logic Journal of the IGPL</journal>
<pages>8--3</pages>
<contexts>
<context position="28135" citStr="Blackburn 2000" startWordPosition="4816" endWordPosition="4817">h potential edge it is immediately investigated whether it can expressed by the realizer. Van Deemter’s (2000) proposal to generate (distributional) distinguishing plural descripA final advantage of the graph model certainly deserves further investigation is the following. We can look at a graph such as that in Figure 2 as a Kripke model. The advantage of this way of looking at it, is that we can use tools from modal logic to reason about these structures. For example, we can reformulate the problem of determining the content of a distinguishing description in terms of hybrid logic (see e.g., Blackburn 2000) as follows: A In words: when we want to refer to node, we are looking for that distinguishing formula which is true of (“at”) but not of any different from. One advantage of this perspective is that logical properties which are usually considered problematic from a generation perspective (such as not having a certain property), fit in very well with the logical perspective. tions (such as the dogs) can also be modelled quite easily. Van Deemter’s algorithm takes as input a set of objects, which in our case, translates into a set of nodes from the scene graph. The algorithm should be reformula</context>
</contexts>
<marker>Blackburn, 2000</marker>
<rawString>Blackburn, P. (2000), Representation, Reasoning, and Relational Structure: A Hybrid Logic Manifesto, Logic Journal of the IGPL 8(3):339-365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
</authors>
<title>Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes,</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="933" citStr="Dale 1992" startWordPosition="148" endWordPosition="149">abeled directed graph and describe content selection as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms. 1 Introduction The generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including Appelt 1985, Dale 1992, Reiter 1990, Dale &amp; Haddock 1991, Dale &amp; Reiter 1995, Horacek 1997, Stone &amp; Webber 1998, Krahmer &amp; Theune 1999 and van Deemter 2000). As a result, there are many different algorithms for the generation of referring expressions, each with its own objectives: some aim at producing the shortest possible description, others focus on efficiency or realistic output. The degree of detail in which the various algorithms are described differs considerably, and as a result it is often difficult to compare the various proposals. In addition, most of the algorithms are primarily concerned with the gener</context>
</contexts>
<marker>Dale, 1992</marker>
<rawString>Dale, R. (1992), Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes, MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>N Haddock</author>
</authors>
<title>Generating Referring Expressions Involving Relations,</title>
<date>1991</date>
<booktitle>Proceedings ofEACL,</booktitle>
<pages>161--166</pages>
<location>Berlin,</location>
<contexts>
<context position="967" citStr="Dale &amp; Haddock 1991" startWordPosition="152" endWordPosition="156">d describe content selection as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms. 1 Introduction The generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including Appelt 1985, Dale 1992, Reiter 1990, Dale &amp; Haddock 1991, Dale &amp; Reiter 1995, Horacek 1997, Stone &amp; Webber 1998, Krahmer &amp; Theune 1999 and van Deemter 2000). As a result, there are many different algorithms for the generation of referring expressions, each with its own objectives: some aim at producing the shortest possible description, others focus on efficiency or realistic output. The degree of detail in which the various algorithms are described differs considerably, and as a result it is often difficult to compare the various proposals. In addition, most of the algorithms are primarily concerned with the generation of descriptions only using p</context>
<context position="18043" citStr="Dale &amp; Haddock (1991)" startWordPosition="3142" endWordPosition="3145"> and cost functions Arguably, the algorithm outlined above is a metaalgorithm, since by formulating the cost function in certain ways we can simulate various algorithms known from the generation literature. 4.1 Full (relational) Brevity Algorithm The algorithm described in the previous section can be seen as a generalization of Dale’s (1992) Full Brevity algorithm, in the sense that there is a guarantee that the algorithm will output the shortest possible description, if one exists. It is also an extension of the Full Brevity algorithm, since it allows for relational descriptions, as does the Dale &amp; Haddock (1991) algorithm. The latter algorithm has a problem with infinite recursions; in principle their algorithm could output descriptions like “the dog in the doghouse which contains a dog which is in a doghouse which ...etc.” Dale &amp; Haddock propose to solve this problem by stipulating that a property or relation may only be included once. In the graph-based model described above the possibility of such infinite recursions does not arise, since a particular edge is either present in a graph or not. Notice incidentally that Dale’s (1992) Greedy Heuristic algorithm can also be cast in the graph framework,</context>
</contexts>
<marker>Dale, Haddock, 1991</marker>
<rawString>Dale, R. &amp; N. Haddock (1991), Generating Referring Expressions Involving Relations, Proceedings ofEACL, Berlin, 161-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>E Reiter</author>
</authors>
<date>1995</date>
<journal>Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions, Cognitive Science</journal>
<volume>18</volume>
<pages>233--263</pages>
<contexts>
<context position="987" citStr="Dale &amp; Reiter 1995" startWordPosition="157" endWordPosition="160">lection as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms. 1 Introduction The generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including Appelt 1985, Dale 1992, Reiter 1990, Dale &amp; Haddock 1991, Dale &amp; Reiter 1995, Horacek 1997, Stone &amp; Webber 1998, Krahmer &amp; Theune 1999 and van Deemter 2000). As a result, there are many different algorithms for the generation of referring expressions, each with its own objectives: some aim at producing the shortest possible description, others focus on efficiency or realistic output. The degree of detail in which the various algorithms are described differs considerably, and as a result it is often difficult to compare the various proposals. In addition, most of the algorithms are primarily concerned with the generation of descriptions only using properties of the tar</context>
<context position="13397" citStr="Dale &amp; Reiter 1995" startWordPosition="2331" endWordPosition="2334">ction findGraph (see also Figure 5) is called, with parameters , bestGraph and . In this function, first it is checked whether the costs of (the graph under construction) are higher than the costs of the bestGraph found so far. If that is the case, it is not worth extending since, due to the monotonicity constraint, it will never end up being cheaper than the current bestGraph. The initial value of bestGraph is the empty, undefined graph, and since its costs are astronomically high, we continue. Then the set of distractors (the objects from which the intended referent should be distinguished, Dale &amp; Reiter 1995) is calculated. In terms of the graph perspective this is the set of nodes in the scene graph (other then the target node ) to which the graph refers. It is easily seen that the initial value of , i.e., (i) in Figure 6, refers to every node in . Hence, as one would expect, the initial set of distractors is nodes . Next we check whether the current set of distractors is empty. If so, we have managed to find a distinguishing graph, which is subsequently stored in the variable bestGraph. In this first iteration, this is obviously not the case and we continue, recursively trying to extend by addin</context>
<context position="23355" citStr="Dale &amp; Reiter (1995" startWordPosition="4025" endWordPosition="4028">al Algorithm does not fully exploit the possibilities offered by the graph framework and the use of cost functions. First, from the graph-theoretical perspective the generation of relational descriptions poses no problems whatsoever, while the incremental generation of relational descriptions is by no means trivial (see e.g., Theune 2000, Krahmer &amp; Theune 1999). In fact, while it could be argued to some extent that incremental selection of properties is psychologically plausible, this somehow seems less plausible for incremental generation of relational extensions. Notice that the use of a As Dale &amp; Reiter (1995:248) point out, redundant properties are not uncommon. That is: in certain situations people may describe an object as “the white bird” even though the simpler “the bird” would have been sufficient (cf. Pechmann 1989, see also Krahmer &amp; Theune 1999 for discussion). However, a similar argument seems somewhat farfetched when applied to relations. It is unlikely that someone would describe an object as “the dog next to the tree in front of the garage” in a situation where “the dog in front of the garage” would suffice. cost function to simulate subsumption hierarchies for properties carries over</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Dale, R. &amp; E. Reiter (1995), Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions, Cognitive Science 18: 233-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
</authors>
<title>Generating Vague Descriptions,</title>
<date>2000</date>
<booktitle>Proceedings INLG, Mitzpe</booktitle>
<location>Ramon.</location>
<marker>van Deemter, 2000</marker>
<rawString>van Deemter, K. (2000), Generating Vague Descriptions, Proceedings INLG, Mitzpe Ramon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Eppstein</author>
</authors>
<title>Subgraph Isomorphism in Planar Graphs and Related Problems,</title>
<date>1999</date>
<journal>J. Graph Algorithms and Applications</journal>
<pages>3--3</pages>
<contexts>
<context position="9367" citStr="Eppstein 1999" startWordPosition="1636" endWordPosition="1637">an upper bound on the number of edges in a distinguishing graph, the problem loses its intractability and becomes solvable in polynomial time. Such a restriction is rather harmless for our current purposes, as it would only prohibit the generation of distinguishing descriptions with more than properties, for an arbitrary large . In general, there are various classes of graphs for which the subgraph isomorphism problem can be solved much more efficiently, without postulating upper bounds. For instance, if and are planar graphs the problem can be solved in time linear in the number of nodes of (Eppstein 1999). Basically, a planar graph is one which can be drawn on a plane in such a way that there are no crossing edges (thus, for instance, the graph in Figure 2 is planar). It is worth investigating to what extent planar graphs suffice for the generation of referring expressions. 3 Outline of the algorithm In this section we give a high-level sketch of the algorithm. The algorithm (called makeReferringExpression) consists of two main components, a subgraph construction algorithm (called findGraph) and a subgraph isomorphism testing algorithm (called matchGraphs). We assume that a scene graph is give</context>
</contexts>
<marker>Eppstein, 1999</marker>
<rawString>Eppstein, D. (1999), Subgraph Isomorphism in Planar Graphs and Related Problems, J. Graph Algorithms and Applications 3(3):1-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Garey</author>
<author>D Johnson</author>
</authors>
<title>Computers and Intractability: A Guide to the Theory of NPCompleteness,</title>
<date>1979</date>
<publisher>W.H. Freeman.</publisher>
<contexts>
<context position="8511" citStr="Garey &amp; Johnson 1979" startWordPosition="1491" endWordPosition="1494"> to have a mechanism which allows us to give certain solutions preference over other solutions. For this purpose we shall use cost-functions. In general, a cost function is a function which assigns to each subgraph of a scene graph a positive number. As we shall see, by defining cost functions in different ways, we can mimic various algorithms for the generation of referring expressions known from the literature. A note on problem complexity The basic decision problem for subgraph isomorphism (i.e., testing whether a graph is isomorphic to a subgraph of ) is known to be NP complete (see e.g., Garey &amp; Johnson 1979). Here we are interested in connected , but unfortunately that dog doghouse in in Figure 4: Three distinguishing node-graph pairs referring to in Figure 2. restriction does not reduce the theoretical complexity. However, as soon as we define an upper bound on the number of edges in a distinguishing graph, the problem loses its intractability and becomes solvable in polynomial time. Such a restriction is rather harmless for our current purposes, as it would only prohibit the generation of distinguishing descriptions with more than properties, for an arbitrary large . In general, there are vario</context>
</contexts>
<marker>Garey, Johnson, 1979</marker>
<rawString>Garey, M. &amp; D. Johnson (1979), Computers and Intractability: A Guide to the Theory of NPCompleteness, W.H. Freeman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>A Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: A Framework for Modeling the Local Coherence of Discourse,</title>
<date>1995</date>
<journal>Computational Linguistics</journal>
<pages>21--2</pages>
<contexts>
<context position="29282" citStr="Grosz et al. 1995" startWordPosition="5018" endWordPosition="5021"> a set of nodes from the scene graph. The algorithm should be reformulated in such a way that it tries to generate a subgraph which can refer to each of the nodes in the set, but not to any of the nodes in the scene graph outside this set. Krahmer &amp; Theune (1999) present an extension of the Incremental Algorithm which takes context into account. They argue that an object which has been mentioned in the recent context is somehow salient, and hence can be referred to using fewer properties. This is modelled by assigning salience weights to objects (basically using a version of Centering Theory (Grosz et al. 1995) augmented with a recency effect), and by defining the set of distractors as the set of objects with a salience weight higher or equal than that of the target object. In terms of the graph-theoretical framework, one can easily imagine assigning salience weights to the nodes in the scene graph, and restricting the distractor set essentially as Krahmer &amp; Theune do. In this way, distinguishing graphs for salient objects will generally be smaller than those of non-salient objects. Acknowledgements Thanks are due to Alexander Koller, Kees van Deemter, Paul Piwek, Mari¨et Theune and two anonymous re</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Grosz, B., A. Joshi &amp; S. Weinstein (1995), Centering: A Framework for Modeling the Local Coherence of Discourse, Computational Linguistics 21(2):203-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Horacek</author>
</authors>
<title>An Algorithm for Generating Referential Descriptions with Flexible Interfaces,</title>
<date>1997</date>
<booktitle>Proceedings of the 35th ACL/EACL,</booktitle>
<pages>206--213</pages>
<location>Madrid,</location>
<contexts>
<context position="1001" citStr="Horacek 1997" startWordPosition="161" endWordPosition="162">h construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms. 1 Introduction The generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including Appelt 1985, Dale 1992, Reiter 1990, Dale &amp; Haddock 1991, Dale &amp; Reiter 1995, Horacek 1997, Stone &amp; Webber 1998, Krahmer &amp; Theune 1999 and van Deemter 2000). As a result, there are many different algorithms for the generation of referring expressions, each with its own objectives: some aim at producing the shortest possible description, others focus on efficiency or realistic output. The degree of detail in which the various algorithms are described differs considerably, and as a result it is often difficult to compare the various proposals. In addition, most of the algorithms are primarily concerned with the generation of descriptions only using properties of the target object. Co</context>
<context position="27266" citStr="Horacek (1997)" startWordPosition="4669" endWordPosition="4670">ional descriptions do not arise. The use of cost functions allows us to model different search methods, each restricting the search space in its own way. By defining cost functions in different ways, we can model and extend various well-known algorithms from the literature such as the Full Brevity Algorithm and the Incremental Algorithm. In addition, the use of cost functions paves the way for integrating statistical information directly in the generation process. Various important ingredients of other generation algorithms can be captured in the algorithm proposed here as well. For instance, Horacek (1997) points out that an algorithm should not collect a set of properties which cannot be realized given the constraints of the grammar. This problem can be solved, following Horacek’s suggestion, by slightly modifying the algorithm in such a way that for each potential edge it is immediately investigated whether it can expressed by the realizer. Van Deemter’s (2000) proposal to generate (distributional) distinguishing plural descripA final advantage of the graph model certainly deserves further investigation is the following. We can look at a graph such as that in Figure 2 as a Kripke model. The a</context>
</contexts>
<marker>Horacek, 1997</marker>
<rawString>Horacek, H. (1997), An Algorithm for Generating Referential Descriptions with Flexible Interfaces, Proceedings of the 35th ACL/EACL, Madrid, 206-213.</rawString>
</citation>
<citation valid="true">
<title>Efficient Generation of Descriptions in Context,</title>
<date>1999</date>
<booktitle>Proceedings of Workshop on Generation of Nominals,</booktitle>
<editor>Krahmer, E. &amp; M. Theune</editor>
<location>Utrecht, The Netherlands.</location>
<contexts>
<context position="28927" citStr="(1999)" startWordPosition="4961" endWordPosition="4961">ctive is that logical properties which are usually considered problematic from a generation perspective (such as not having a certain property), fit in very well with the logical perspective. tions (such as the dogs) can also be modelled quite easily. Van Deemter’s algorithm takes as input a set of objects, which in our case, translates into a set of nodes from the scene graph. The algorithm should be reformulated in such a way that it tries to generate a subgraph which can refer to each of the nodes in the set, but not to any of the nodes in the scene graph outside this set. Krahmer &amp; Theune (1999) present an extension of the Incremental Algorithm which takes context into account. They argue that an object which has been mentioned in the recent context is somehow salient, and hence can be referred to using fewer properties. This is modelled by assigning salience weights to objects (basically using a version of Centering Theory (Grosz et al. 1995) augmented with a recency effect), and by defining the set of distractors as the set of objects with a salience weight higher or equal than that of the target object. In terms of the graph-theoretical framework, one can easily imagine assigning </context>
</contexts>
<marker>1999</marker>
<rawString>Krahmer, E. &amp; M. Theune (1999), Efficient Generation of Descriptions in Context, Proceedings of Workshop on Generation of Nominals, R. Kibble and K. van Deemter (eds.), Utrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<date>1998</date>
<booktitle>The Practical Value of -Grams in Generation, Proceedings INLG,</booktitle>
<pages>248--255</pages>
<location>Niagara-on-the-lake, Ontario,</location>
<contexts>
<context position="3155" citStr="Langkilde &amp; Knight 1998" startWordPosition="506" endWordPosition="509">m, in the sense that by defining the cost function in different ways, we can mimic various well-known algorithms for the generation of referring expressions. A second advantage of the graph-theoretical framework is that it does not run into problems with relational descriptions, due to the fact that properties and relations are formalized in the same way, namely as edges in a graph. The third advantage is that the combined usage of graphs and cost-functions paves the way for a natural integration of traditional rule-based approaches to generation with more recent statistical approaches (e.g., Langkilde &amp; Knight 1998, Malouf 2000) in a single algorithm. The outline of this paper is as follows. In section 2, we describe how scenes can be described as labeled directed graphs and show how content selection can be formalized as a subgraph construction problem. Section 3 contains a sketch of the branch and bound algorithm, which is illustrated with a worked example. In section 4 it is argued that by defining cost functions in different ways, we can mimic various well-known algorithms for the generation of referring expressions. We end with some concluding remarks in section 5. 2 Graphs Consider the following s</context>
<context position="25126" citStr="Langkilde &amp; Knight 1998" startWordPosition="4329" endWordPosition="4332">s of the attribute “color”. Instead we have the freedom to assign edges labeled with a common type value (e.g., dog) a lower cost than edges labeled with uncommon colors (such as Vandyke-brown), while at the same time edges labeled with obscure type values, such as polish owczarek nizinny sheepdog, can be given a higher cost than edges labeled with common colors such as brown. 4.3 Stochastic cost functions One of the important open questions in natural language generation is how the common, rulebased approaches to generation can be combined with recent insights from statistical NLP (see e.g., Langkilde &amp; Knight 1998, Malouf 2000 for partial answers). Indeed, when looking at the Incremental Algorithm, for instance, it is not directly obvious how statistical information can be integrated in the algorithm. Arguably, this is different when we have cost functions. One can easily imagine deriving a stochastic cost function from a sufficiently large corpus and using it in the graphtheoretical framework (the result looks like but is not quite a Markov Model). As a first approximation, we could define the costs of adding an edge in terms of the probability that occurs in a distinguishing description (estimated by</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde, I. &amp; K. Knight (1998), The Practical Value of -Grams in Generation, Proceedings INLG, Niagara-on-the-lake, Ontario, 248-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>The Order of Prenominal Adjectives in Natural Language Generation,</title>
<date>2000</date>
<booktitle>Proceedings of the 38th ACL , Hong Kong.</booktitle>
<contexts>
<context position="3169" citStr="Malouf 2000" startWordPosition="510" endWordPosition="511">efining the cost function in different ways, we can mimic various well-known algorithms for the generation of referring expressions. A second advantage of the graph-theoretical framework is that it does not run into problems with relational descriptions, due to the fact that properties and relations are formalized in the same way, namely as edges in a graph. The third advantage is that the combined usage of graphs and cost-functions paves the way for a natural integration of traditional rule-based approaches to generation with more recent statistical approaches (e.g., Langkilde &amp; Knight 1998, Malouf 2000) in a single algorithm. The outline of this paper is as follows. In section 2, we describe how scenes can be described as labeled directed graphs and show how content selection can be formalized as a subgraph construction problem. Section 3 contains a sketch of the branch and bound algorithm, which is illustrated with a worked example. In section 4 it is argued that by defining cost functions in different ways, we can mimic various well-known algorithms for the generation of referring expressions. We end with some concluding remarks in section 5. 2 Graphs Consider the following scene: Figure 1</context>
<context position="25139" citStr="Malouf 2000" startWordPosition="4333" endWordPosition="4334">”. Instead we have the freedom to assign edges labeled with a common type value (e.g., dog) a lower cost than edges labeled with uncommon colors (such as Vandyke-brown), while at the same time edges labeled with obscure type values, such as polish owczarek nizinny sheepdog, can be given a higher cost than edges labeled with common colors such as brown. 4.3 Stochastic cost functions One of the important open questions in natural language generation is how the common, rulebased approaches to generation can be combined with recent insights from statistical NLP (see e.g., Langkilde &amp; Knight 1998, Malouf 2000 for partial answers). Indeed, when looking at the Incremental Algorithm, for instance, it is not directly obvious how statistical information can be integrated in the algorithm. Arguably, this is different when we have cost functions. One can easily imagine deriving a stochastic cost function from a sufficiently large corpus and using it in the graphtheoretical framework (the result looks like but is not quite a Markov Model). As a first approximation, we could define the costs of adding an edge in terms of the probability that occurs in a distinguishing description (estimated by counting occ</context>
</contexts>
<marker>Malouf, 2000</marker>
<rawString>Malouf, R., (2000), The Order of Prenominal Adjectives in Natural Language Generation, Proceedings of the 38th ACL , Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pechmann</author>
</authors>
<title>Incremental Speech Production and Referential Overspecification, Linguistics 27:98–110.</title>
<date>1989</date>
<contexts>
<context position="23572" citStr="Pechmann 1989" startWordPosition="4062" endWordPosition="4064">hatsoever, while the incremental generation of relational descriptions is by no means trivial (see e.g., Theune 2000, Krahmer &amp; Theune 1999). In fact, while it could be argued to some extent that incremental selection of properties is psychologically plausible, this somehow seems less plausible for incremental generation of relational extensions. Notice that the use of a As Dale &amp; Reiter (1995:248) point out, redundant properties are not uncommon. That is: in certain situations people may describe an object as “the white bird” even though the simpler “the bird” would have been sufficient (cf. Pechmann 1989, see also Krahmer &amp; Theune 1999 for discussion). However, a similar argument seems somewhat farfetched when applied to relations. It is unlikely that someone would describe an object as “the dog next to the tree in front of the garage” in a situation where “the dog in front of the garage” would suffice. cost function to simulate subsumption hierarchies for properties carries over directly to relations; for instance, the costs of adding a edge labeled next to should be less than those of adding one labeled left of or right of. Hence, next to will be preferred, unless using left of or right of </context>
</contexts>
<marker>Pechmann, 1989</marker>
<rawString>Pechmann, T. (1989), Incremental Speech Production and Referential Overspecification, Linguistics 27:98–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
</authors>
<title>The Computational Complexity of Avoiding Conversational Implicatures,</title>
<date>1990</date>
<booktitle>Proceedings of the 28th ACL ,</booktitle>
<pages>97--104</pages>
<contexts>
<context position="946" citStr="Reiter 1990" startWordPosition="150" endWordPosition="151">cted graph and describe content selection as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms. 1 Introduction The generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including Appelt 1985, Dale 1992, Reiter 1990, Dale &amp; Haddock 1991, Dale &amp; Reiter 1995, Horacek 1997, Stone &amp; Webber 1998, Krahmer &amp; Theune 1999 and van Deemter 2000). As a result, there are many different algorithms for the generation of referring expressions, each with its own objectives: some aim at producing the shortest possible description, others focus on efficiency or realistic output. The degree of detail in which the various algorithms are described differs considerably, and as a result it is often difficult to compare the various proposals. In addition, most of the algorithms are primarily concerned with the generation of desc</context>
</contexts>
<marker>Reiter, 1990</marker>
<rawString>Reiter, E. (1990), The Computational Complexity of Avoiding Conversational Implicatures, Proceedings of the 28th ACL , 97-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I van der Sluis</author>
<author>E Krahmer</author>
</authors>
<title>Generating Referring Expressions in a Multimodal Context: An Empirically Motivated Approach,</title>
<date>2001</date>
<booktitle>Proceedings CLIN, W. Daelemans et</booktitle>
<editor>al. (eds),</editor>
<location>Rodopi, Amsterdam/Atlanta.</location>
<marker>van der Sluis, Krahmer, 2001</marker>
<rawString>van der Sluis, I. &amp; E. Krahmer (2001), Generating Referring Expressions in a Multimodal Context: An Empirically Motivated Approach, Proceedings CLIN, W. Daelemans et al. (eds), Rodopi, Amsterdam/Atlanta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
<author>B Webber</author>
</authors>
<title>Textual Economy Through Close Coupling of Syntax and Semantics,</title>
<date>1998</date>
<booktitle>Proceedings INLG,</booktitle>
<pages>178--187</pages>
<location>Niagara-on-the-lake, Ontario,</location>
<contexts>
<context position="1022" citStr="Stone &amp; Webber 1998" startWordPosition="163" endWordPosition="166"> problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms. 1 Introduction The generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including Appelt 1985, Dale 1992, Reiter 1990, Dale &amp; Haddock 1991, Dale &amp; Reiter 1995, Horacek 1997, Stone &amp; Webber 1998, Krahmer &amp; Theune 1999 and van Deemter 2000). As a result, there are many different algorithms for the generation of referring expressions, each with its own objectives: some aim at producing the shortest possible description, others focus on efficiency or realistic output. The degree of detail in which the various algorithms are described differs considerably, and as a result it is often difficult to compare the various proposals. In addition, most of the algorithms are primarily concerned with the generation of descriptions only using properties of the target object. Consequently, the probl</context>
</contexts>
<marker>Stone, Webber, 1998</marker>
<rawString>Stone, M. &amp; B. Webber (1998), Textual Economy Through Close Coupling of Syntax and Semantics, Proceedings INLG, Niagara-on-the-lake, Ontario, 178-187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Theune</author>
</authors>
<title>From Data to Speech: Language Generation</title>
<date>2000</date>
<institution>Eindhoven University of Technology.</institution>
<note>in Context, Ph.D. dissertation,</note>
<contexts>
<context position="23075" citStr="Theune 2000" startWordPosition="3982" endWordPosition="3983">d edges in graph terminology. It is worth stressing that when all edges in the scene graph are of the looping variety, testing for subgraph isomorphism becomes trivial and we regain polynomial complexity. However, the above graph-theoretical formalization of the Incremental Algorithm does not fully exploit the possibilities offered by the graph framework and the use of cost functions. First, from the graph-theoretical perspective the generation of relational descriptions poses no problems whatsoever, while the incremental generation of relational descriptions is by no means trivial (see e.g., Theune 2000, Krahmer &amp; Theune 1999). In fact, while it could be argued to some extent that incremental selection of properties is psychologically plausible, this somehow seems less plausible for incremental generation of relational extensions. Notice that the use of a As Dale &amp; Reiter (1995:248) point out, redundant properties are not uncommon. That is: in certain situations people may describe an object as “the white bird” even though the simpler “the bird” would have been sufficient (cf. Pechmann 1989, see also Krahmer &amp; Theune 1999 for discussion). However, a similar argument seems somewhat farfetched</context>
</contexts>
<marker>Theune, 2000</marker>
<rawString>Theune, M. (2000), From Data to Speech: Language Generation in Context, Ph.D. dissertation, Eindhoven University of Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>