<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.995928">
Word Independent Context Pair Classification Model for Word
Sense Disambiguation
</title>
<author confidence="0.984235">
Cheng Niu, Wei Li, Rohini K. Srihari, and Huifeng Li
</author>
<affiliation confidence="0.947257">
Cymfony Inc.
</affiliation>
<address confidence="0.889447">
600 Essjay Road, Williamsville, NY 14221, USA.
</address>
<email confidence="0.945635">
{cniu, wei, rohini,hli}@cymfony.com
</email>
<sectionHeader confidence="0.997111" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999783125">
Traditionally, word sense disambiguation
(WSD) involves a different context classifi-
cation model for each individual word. This
paper presents a weakly supervised learning
approach to WSD based on learning a word
independent context pair classification
model. Statistical models are not trained for
classifying the word contexts, but for classi-
fying a pair of contexts, i.e. determining if a
pair of contexts of the same ambiguous word
refers to the same or different senses. Using
this approach, annotated corpus of a target
word A can be explored to disambiguate
senses of a different word B. Hence, only a
limited amount of existing annotated corpus
is required in order to disambiguate the entire
vocabulary. In this research, maximum en-
tropy modeling is used to train the word in-
dependent context pair classification model.
Then based on the context pair classification
results, clustering is performed on word men-
tions extracted from a large raw corpus. The
resulting context clusters are mapped onto
the external thesaurus WordNet. This ap-
proach shows great flexibility to efficiently
integrate heterogeneous knowledge sources,
e.g. trigger words and parsing structures.
Based on Senseval-3 Lexical Sample stan-
dards, this approach achieves state-of-the-art
performance in the unsupervised learning
category, and performs comparably with the
supervised Naïve Bayes system.
</bodyText>
<sectionHeader confidence="0.999559" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998076441860466">
Word Sense Disambiguation (WSD) is one of the
central problems in Natural Language Processing.
The difficulty of this task lies in the fact that con-
text features and the corresponding statistical dis-
tribution are different for each individual word.
Traditionally, WSD involves training the context
classification models for each ambiguous word.
(Gale et al. 1992) uses the Naïve Bayes method for
context classification which requires a manually
annotated corpus for each ambiguous word. This
causes a serious Knowledge Bottleneck. The bot-
tleneck is particularly serious when considering the
domain dependency of word senses. To overcome
the Knowledge Bottleneck, unsupervised or weakly
supervised learning approaches have been pro-
posed. These include the bootstrapping approach
(Yarowsky 1995) and the context clustering ap-
proach (Schütze 1998).
The above unsupervised or weakly supervised
learning approaches are less subject to the Knowl-
edge Bottleneck. For example, (Yarowsky 1995)
only requires sense number and a few seeds for
each sense of an ambiguous word (hereafter called
keyword). (Schütze 1998) may only need minimal
annotation to map the resulting context clusters
onto external thesaurus for benchmarking and ap-
plication-related purposes. Both methods are based
on trigger words only.
This paper presents a novel approach based on
learning word-independent context pair classifica-
tion model. This idea may be traced back to
(Schütze 1998) where context clusters based on
generic Euclidean distance are regarded as distinct
word senses. Different from (Schütze 1998), we
observe that generic context clusters may not al-
ways correspond to distinct word senses. There-
fore, we used supervised machine learning to
model the relationships between the context dis-
tinctness and the sense distinctness.
Although supervised machine learning is used
for the context pair classification model, our over-
all system belongs to the weakly supervised cate-
gory because the learned context pair classification
</bodyText>
<page confidence="0.990394">
33
</page>
<note confidence="0.345498">
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 33–39, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999962395348837">
model is independent of the keyword for disam-
biguation. Our system does not need human-
annotated instances for each target ambiguous
word. The weak supervision is performed by using
a limited amount of existing annotated corpus
which does not need to include the target word set.
The insight is that the correlation regularity be-
tween the sense distinction and the context distinc-
tion can be captured at Part-of-Speech category
level, independent of individual words or word
senses. Since context determines the sense of a
word, a reasonable hypothesis is that there is some
mechanism in the human comprehension process
that will decide when two contexts are similar (or
dissimilar) enough to trigger our interpretation of a
word in the contexts as one meaning (or as two
different meanings). We can model this mecha-
nism by capturing the sense distinction regularity
at category level.
In the light of this, a maximum entropy model is
trained to determine if a pair of contexts of the
same keyword refers to the same or different word
senses. The maximum entropy modeling is based
on heterogeneous context features that involve
both trigger words and parsing structures. To en-
sure the resulting model’s independency of indi-
vidual words, the keywords used in training are
different from the keywords used in benchmarking.
For any target keyword, a collection of contexts is
retrieved from a large raw document pool. Context
clustering is performed to derive the optimal con-
text clusters which globally fit the local context
pair classification results. Here statistical annealing
is used for its optimal performance. In benchmark-
ing, a mapping procedure is required to correlate
the context clusters with external ontology senses.
In what follows, Section 2 formulates the maxi-
mum entropy model for context pair classification.
The context clustering algorithm, including the
object function of the clustering and the statistical
annealing-based optimization, is described in Sec-
tion 3. Section 4 presents and discusses bench-
marks, followed by conclusion in Section 5.
</bodyText>
<sectionHeader confidence="0.980231" genericHeader="method">
2 Maximum Entropy Modeling for Con-
text Pair Classification
</sectionHeader>
<bodyText confidence="0.97793222">
Given n mentions of a keyword, we first introduce
the following symbols. Ci refers to the i -th con-
text. Si refers to the sense of the i -th context.
CSi,j refers to the context similarity between the
i -th context and the j -th context, which is a subset
of the predefined context similarity features. fo
refers to the o -th predefined context similarity
feature. So CSi ,j takes the form of Cfo I
In this section, we study the context pair classi-
fication task, i.e. given a pair of contexts Ci and
Cj of the same target word, are they referring to
the same sense? This task is formulated as compar-
ing the following conditional probabilities:
Pr ISi ❑ S j CSi ,j I and Pr ISi ❑ S j CSi ,j I. Unlike
traditional context classification for WSD where
statistical model is trained for each individual
word, our context pair classification model is
trained for each Part-of-speech (POS) category.
The reason for choosing POS as the appropriate
category for learning the context similarity is that
the parsing structures, hence the context represen-
tation, are different for different POS categories.
The training corpora are constructed using the
Senseval-2 English Lexical Sample training cor-
pus. To ensure the resulting model’s independency
of individual words, the target words used for
benchmarking (which will be the ambiguous words
used in Senseval-3 English Lexicon Sample task)
are carefully removed in the corpus construction
process. For each POS category, positive and nega-
tive instances are constructed as follows.
Positive instances are constructed using context
pairs referring to the same sense of a word. Nega-
tive instances are constructed using context pairs
that refer to different senses of a word.
For each POS category, we have constructed
about 36,000 instances, half positive and half nega-
tive. The instances are represented as pairwise con-
text similarities, taking the form of Cfo I
Before presenting the context similarity features
we used, we first introduce the two categories of
the involved context features:
i) Co-occurring trigger words within a prede-
fined window size equal to 50 words to both
sides of the keyword. The trigger words are
learned from a TIPSTER document pool con-
taining ~170 million words of AP and WSJ
news articles. Following (Schütze 1998), L2 is
used to measure the cohesion between the
keyword and a co-occurring word. In our ex-
</bodyText>
<page confidence="0.995082">
34
</page>
<bodyText confidence="0.9996251">
periment, all the words are first sorted based
on its χ2 with the keyword, and then the top
2,000 words are selected as trigger words.
ii) Parsing relationships associated with the
keyword automatically decoded by a broad-
coverage parser, with F-measure (i.e. the pre-
cision-recall combined score) at about 85%
(reference temporarily omitted for the sake of
blind review). The logical dependency rela-
tionships being utilized are listed below.
</bodyText>
<equation confidence="0.9787485">
Noun: subject-of,
object-of,
complement-of,
has-adjective-modifier,
has-noun-modifier,
modifier-of,
possess,
possessed-by,
appositive-of
Verb: has-subject,
</equation>
<bodyText confidence="0.808777642857143">
has-object,
has-complement,
has-adverb-modifier,
has-prepositional-phrase-modifier
Adjective: modifier-of,
has-adverb-modifier
Based on the above context features, the follow-
ing three categories of context similarity features
are defined:
(1) VSM-based (Vector Space Model based)
trigger word similarity: the trigger words
around the keyword are represented as a vec-
tor, and the word i in context j is weighted as
follows:
</bodyText>
<equation confidence="0.9992">
weight i j = tf i j
( , ) ( , ) * log
</equation>
<bodyText confidence="0.984974693877551">
where tf (i, j) is the frequency of word i in
the j-th context; D is the number of docu-
ments in the pool; and df (i) is the number of
documents containing the word i. D and
df (i) are estimated using the document pool
introduced above. The cosine of the angle be-
tween two resulting vectors is used as the
context similarity measure.
(2) LSA-based (Latent Semantic Analysis based)
trigger word similarity: LSA (Deerwester et
al. 1990) is a technique used to uncover the
underlying semantics based on co-occurrence
data. The first step of LSA is to construct
word-vs.-document co-occurrence matrix.
Then singular value decomposition (SVD) is
performed on this co-occurring matrix. The
key idea of LSA is to reduce noise or insig-
nificant association patterns by filtering the
insignificant components uncovered by SVD.
This is done by keeping only the top k singu-
lar values. By using the resulting word-vs.-
document co-occurrence matrix after the fil-
tering, each word can be represented as a vec-
tor in the semantic space.
In our experiment, we constructed the original
word-vs.-document co-occurring matrix as
follows: 100,000 documents from the
TIPSTER corpus were used to construct the
co-occurring matrix. We processed these
documents using our POS tagger, and se-
lected the top n most frequently mentioned
words from each POS category as base
words:
top 20,000 common nouns
top 40,000 proper names
top 10,000 verbs
top 10,000 adjectives
top 2,000 adverbs
In performing SVD, we set k (i.e. the number
of nonzero singular values) as 200, following
the practice reported in (Deerwester et al.
1990) and (Landauer &amp; Dumais, 1997).
Using the LSA scheme described above, each
word is represented as a vector in the seman-
tic space. The co-occurring trigger words are
represented as a vector summation. Then the
cosine of the angle between the two resulting
vector summations is computed, and used as
the context similarity measure.
</bodyText>
<listItem confidence="0.628411">
(3) LSA-based parsing relationship similarity:
each relationship is in the form of Rα(w) .
</listItem>
<bodyText confidence="0.579241">
Using LSA, each word w is represented as a
</bodyText>
<equation confidence="0.44313">
D
df (i)
</equation>
<page confidence="0.9826">
35
</page>
<bodyText confidence="0.990946183098592">
semantic vector V (w .) The similarity between
Rα(w1)and Rα(w2) is represented as the co-
sine of the angle between V (w1 )and V (w2 .)
Two special values are assigned to two excep-
tional cases: (i) when no relationship Rα is
decoded in both contexts; (ii) when the rela-
tionship Rα is decoded only for one context.
In matching parsing relationships in a context
pair, if only exact node match counts, very few
cases can be covered, hence significantly reducing
the effect of the parser in this task. To solve this
problem, LSA is used as a type of synonym expan-
sion in matching. For example, using LSA, the
following word similarity values are generated:
similarity(good, good) 1.00
similarity(good, pretty) 0.79
similarity(good, great) 0.72
Given a context pair of a noun keyword, suppose
the first context involves a relationship has-
adjective-modifier whose value is good, and the
second context involves the same relationship has-
adjective-modifier with the value pretty, then the
system assigns 0.79 as the similarity value for this
relationship pair.
To facilitate the maximum entropy modeling in
the later stage, all the three categories of the result-
ing similarity values are discretized into 10 inte-
gers. Now the pairwise context similarity is
represented as a set of similarity features, e.g.
{VSM-Trigger-Words-Similairty-equal-to-2,
LSA-Trigger-Words-Similarity-equal-to-1,
LSA-Subject-Similarity-equal-to-2}.
In addition to the three categories of basic con-
text similarity features defined above, we also de-
fine induced context similarity features by
combining basic context similarity features using
the logical and operator. With induced features, the
context similarity vector in the previous example is
represented as
{VSM-Trigger-Word-Similairty-equal-to-2,
LSA- Trigger-Word-Similarity-equal-to-1,
LSA-Subject-Similarity-equal-to-2,
[VSM-Similairty-equal-to-2 and
LSA-Trigger-Word-Similarity-equal-to-1],
[VSM-Similairty-equal-to-2 and
LSA-Subject-Similarity-equal-to-2],
.........
[VSM-Trigger-Word-Similairty-equal-to-2
and LSA-Trigger-Word-Similarity-equal-to-1
and LSA-Subject-Similarity-equal-to-2]
}
The induced features provide direct and fine-
grained information, but suffer from less sampling
space. Combining basic features and induced fea-
tures under a smoothing scheme, maximum en-
tropy modeling may achieve optimal performance.
Using the context similarity features defined
above, the training corpora for the context pair
classification model is in the following format:
Instance_0 tag--”positive” {VSM-Trigger-Word-
Similairty-equal-to-2, ...}
Instance_1 tag--”negative” {VSM-Trigger-Word-
Similairty-equal-to-0, ...}
where positive tag denotes a context pair associ-
ated with same sense, and negative tag denotes a
context pair associated with different senses.
The maximum entropy modeling is used to com-
pute the conditional probabilities
Pr (Si = Sj CSi ,j ) and Pr (Si ≠ Sj CSi,j ): once the
context pair CSi ,j is represented as { fα} , the con-
ditional probability is given as
</bodyText>
<equation confidence="0.997458285714286">
1
Pr { } (1)
( )
t f = ∏ wt f
α ,
f f α
∈ { }
</equation>
<bodyText confidence="0.997510571428571">
where t ∈ {Si = Sj , Si ≠ Sj }, Z is the normaliza-
tion factor, wt ,f is the weight associated with tag t
and feature f . Using the training corpora con-
structed above, the weights can be computed based
on Iterative Scaling algorithm (Pietra etc. 1995)
The exponential prior smoothing scheme (Good-
man 2003) is adopted in the training.
</bodyText>
<equation confidence="0.794375">
Z
</equation>
<page confidence="0.958">
36
</page>
<bodyText confidence="0.644532">
applied to search for the global optimal solution.
The optimization algorithm is as follows.
</bodyText>
<figure confidence="0.939524333333333">
1. Set the initial state {K, M }as K =n, and
3 Context Clustering based on Context
Pair Classification Results
</figure>
<bodyText confidence="0.970005611111111">
Given n mentions {Ci }of a keyword, we use the
following context clustering scheme. The discov-
ered context clusters correspond to distinct word
senses.
For any given context pair, the context similarity
features defined in Section 2 are computed. With n
n( n − 1)
mentions of the same keyword, context
2
similarities CSi ,j (i ∈ [1, n ,]j ∈ 1,[i ) )are computed.
Using the context pair classification model, each
pair is associated with two scores
the probabilities of two situations: the pair refers to
the same or different word senses.
Now we introduce the symbol {K, M } which re-
fers to the final context cluster configuration,
where K refers to the number of distinct sense, and
M represents the many-to-one mapping (from con-
</bodyText>
<equation confidence="0.654656833333333">
texts to a sense) such that
M(i )= j, i ∈ [1,n], j ∈ [1, K]. Based on the pairwise
scores { }
sci , j and { }
0 sci , j , WSD is formulated as
1
</equation>
<bodyText confidence="0.8318915">
searching for {K, M }which maximizes the follow-
ing global scores:
</bodyText>
<figure confidence="0.842413285714286">
M(i) =i, i ∈ [1 ,n ;]
2. Select a cluster pair for merging that
maximally increases
( )
,
sca,M})= Isci
3. If no cluster pair can be merged to in-
crease sc a, M }� sc k ©,j ) , output
i 1 ,n
j i
∈ [
∈ [ 1 ,
)
, ]
</figure>
<figureCaption confidence="0.600223333333333">
{K, M } as the intermediate solution;
otherwise, update {K, M } by the merge
and go to step 2.
</figureCaption>
<bodyText confidence="0.99695125">
Using the intermediate solution {K,M }0 of the
greedy algorithm as the initial state, the statistical
annealing is implemented using the following
pseudo-code:
</bodyText>
<equation confidence="0.936707333333333">
Set {K,M }= {K,M 0};
for(R =R0; R &lt;Rfinal;R* =1.01)
{
iterate pre-defined number of times
{
set {K, M }1 = {K, M ;}
</equation>
<bodyText confidence="0.9500945">
update {K, M }1 by randomly changing
cluster number and cluster contents;
</bodyText>
<equation confidence="0.993864184210526">
sc i j ( ( S i S j CS i , j )
1 , = log Pr =
which correspond to
0
, = log
sc i j
( ( S i Sj CSi,j )
Pr =
and
, ]
)
i∈
[ 1 , n
[ 1, i
set
x =
s
, ]
)
otherwise
s c K, M 1
( { } )
c({K,M})
if(x&gt;=1)
{
set {K, M }= {K, M 1}
(sc a, M })= I sc j ) (2)
i 1 ,n
∈ [
j i
∈ [ 1 ,
r0,if M i M j
( ) ( )
=
where ( )
k i j
, = j
l1,
</equation>
<bodyText confidence="0.979351636363636">
Similar clustering scheme has been used success-
fully for the task of co-reference in (Luo etc.
2004), (Zelenko, Aone and Tibbetts, 2004a) and
(Zelenko, Aone and Tibbetts, 2004b).
In this paper, statistical annealing-based optimi-
zation (Neal 1993) is used to search for {K,M }
which maximizes Expression (2).
The optimization process consists of two steps.
First, an intermediate solution {K,M }0 is com-
puted by a greedy algorithm. Then by setting
{K,M }0 as the initial state, statistical annealing is
</bodyText>
<equation confidence="0.795428307692308">
}
else
{
set {K,M }= {K,M 1} with probability
x .
β
}
if sc( { K,M} ) &gt; sc(K, { M 0} )
then set {K, M }0 = {K, M }
}
j∈
}
output {K, M }0 as the optimal state.
</equation>
<page confidence="0.998436">
37
</page>
<sectionHeader confidence="0.98945" genericHeader="method">
4 Benchmarking
</sectionHeader>
<bodyText confidence="0.997578676470588">
Corpus-driven context clusters need to map to a
word sense standard to facilitate performance
benchmark. Using Senseval-3 evaluation stan-
dards, we implemented the following procedure to
map the context clusters:
i) Process TIPSTER corpus and the origi-
nal unlabeled Senseval-3 corpora (in-
cluding the training corpus and the
testing corpus) by our parser, and save
all the parsing results into a repository.
ii) For each keyword, all related contexts in
Senseval-3 corpora and up-to-1,000 re-
lated contexts in TIPSTER corpus are
retrieved from the repository.
iii) All the retrieved contexts are clustered
based on the context clustering algo-
rithm presented in Sect. 2 and 3.
iv) For each keyword sense, three annotated
contexts from Senseval-3 training cor-
pus are used for the sense mapping. The
context cluster is mapped onto the most
frequent word sense associated with the
cluster members. By design, the context
clusters correspond to distinct senses,
therefore, we do not allow multiple con-
text clusters to be mapped onto one
sense. In case multiple clusters corre-
spond to one sense, only the largest
cluster is retained.
v) Each context in the testing corpus is
tagged with the sense to which its con-
text cluster corresponds to.
As mentioned above, Sensval-2 English lexical
sample training corpora is used to train the context
pair classification model. And Sensval-3 English
lexical sample testing corpora is used here for
benchmarking. There are several keyword occur-
ring in both Senseval-2 and Senseval-3 corpora.
The sense tags associated with these keywords are
not used in the context pair classification training
process.
In order to gauge the performance of this new
weakly supervised learning algorithm, we have
also implemented a supervised Naïve Bayes sys-
tem following (Gale et al. 1992). This system is
trained based on the Senseval-3 English Lexical
Sample training corpus. In addition, for the pur-
pose of quantifying the contribution from the pars-
ing structures in WSD, we have run our new
system with two configurations: (i) using only
trigger words; (ii) using both trigger words and
parsing relationships. All the benchmarking is per-
formed using the Senseval-3 English Lexical Sam-
ple testing corpus and standards.
The performance benchmarks for the two sys-
tems in three runs are shown in Table 1, Table 2
and Table 3. When using only trigger words, this
algorithm has 8 percentage degradation from the
supervised Naïve Bayes system (see Table 1 vs.
Table 2). When adding parsing structures, per-
formance degradation is reduced, with about 5 per-
centage drop (see Table 3 vs. Table 2). Comparing
Table 1 with Table 3, we observe about 3% en-
hancement due to the contribution from the parsing
support in WSD. The benchmark of our algorithm
using both trigger words and parsing relationships
is one of the best in unsupervised category of the
Senseval-3 Lexical Sample evaluation.
</bodyText>
<tableCaption confidence="0.998782">
Table 1. New Algorithm Using Only Trigger Words
</tableCaption>
<table confidence="0.999412">
Category Accuracy
Fine grain (%) Coarse grain (%)
Adjective (5) 46.3 60.8
Noun (20) 54.6 62.8
Verb (32) 54.1 64.2
Overall 54.0 63.4
</table>
<tableCaption confidence="0.981343">
Table 2. Supervised Naïve Bayes System
</tableCaption>
<table confidence="0.999537333333333">
Category Accuracy
Fine grain (%) Coarse grain (%)
Adjective (5) 44.7 56.6
Noun (20) 66.3 74.5
Verb (32) 58.6 70.0
Overall 61.6 71.5
</table>
<tableCaption confidence="0.99681">
Table 3. New Algorithm Using Both Trigger Words and
</tableCaption>
<table confidence="0.990287857142857">
Parsing
Category Accuracy
Fine grain (%) Coarse grain (%)
Adjective (5) 49.1 64.8
Noun (20) 57.9 66.6
Verb (32) 55.3 66.3
Overall 56.3 66.4
</table>
<page confidence="0.999182">
38
</page>
<bodyText confidence="0.999974">
It is noted that Naïve Bayes algorithm has many
variation, and its performance has been greatly
enhanced during recent research. Based on Sen-
seval-3 results, the best Naïve Bayse system out-
perform our version (which is implemented based
on Gale et al. 1992) by 8%~10%. So the best su-
pervised WSD systems output-perform our weakly
supervised WSD system by 13%~15% in accuracy.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999978965517241">
We have presented a weakly supervised learning
approach to WSD. Statistical models are not
trained for the contexts of each individual word,
but for context pair classification. This approach
overcomes the knowledge bottleneck that chal-
lenges supervised WSD systems which need la-
beled data for each individual word. It captures the
correlation regularity between the sense distinction
and the context distinction at Part-of-Speech cate-
gory level, independent of individual words and
senses. Hence, it only requires a limited amount of
existing annotated corpus in order to disambiguate
the full target set of ambiguous words, in particu-
lar, the target words that do not appear in the train-
ing corpus.
The weakly supervised learning scheme can
combine trigger words and parsing structures in
supporting WSD. Using Senseval-3 English Lexi-
cal Sample benchmarking, this new approach
reaches one of the best scores in the unsupervised
category of English Lexical Sample evaluation.
This performance is close to the performance for
the supervised Naïve Bayes system.
In the future, we will implement a new scheme
to map context clusters onto WordNet senses by
exploring WordNet glosses and sample sentences.
Based on the new sense mapping scheme, we will
benchmark our system performance using Senseval
English all-words corpora.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99962995">
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K.
Landauer, and R. Harshman. 1990. Indexing by
Latent Semantic Analysis. In Journal of the
American Society of Information Science
Gale, W., K. Church, and D. Yarowsky. 1992. A
Method for Disambiguating Word Senses in a
Large Corpus. Computers and the Humanities,
26.
Goodman, J. 2003. Exponential Priors for Maxi-
mum Entropy Models. In Proceedings of HLT-
NAACL 2004.
Landauer, T. K., &amp; Dumais, S. T. 1997. A solution
to Plato&apos;s problem: The Latent Semantic Analy-
sis theory of the acquisition, induction, and rep-
resentation of knowledge. Psychological
Review, 104, 211-240, 1997.
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla and
S. Roukos. A Mention-Synchronous Corefer-
ence Resolution Algorithm Based on the Bell
Tree. In The Proceedings of ACL 2004.
Neal, R.M. 1993. Probabilistic Inference Using
Markov Chain Monte Carlo Methods. Technical
Report, Univ. of Toronto.
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995.
Inducing Features Of Random Fields. In IEEE
Transactions on Pattern Analysis and Machine
Intelligence.
Schütze, H. 1998. Automatic Word Sense Disam-
biguation. Computational Linguistics, 23.
Yarowsky, D. 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods.
In Proceedings of ACL 1995.
Zelenko, D., C. Aone and J. 2004. Tibbetts.
Coreference Resolution for Information Extrac-
tion. In Proceedings of ACL 2004 Workshop on
Reference Resolution and its Application.
Zelenko, D., C. Aone and J. 2004. Tibbetts. Binary
Integer Programming for Information Extrac-
tion. In Proceedings of ACE 2004 Evaluation
Workshop.
</reference>
<page confidence="0.999518">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.405021">
<title confidence="0.9991365">Word Independent Context Pair Classification Model for Sense Disambiguation</title>
<author confidence="0.574">Cheng Niu</author>
<author confidence="0.574">Wei Li</author>
<author confidence="0.574">Rohini K Srihari</author>
<author confidence="0.574">Huifeng</author>
<affiliation confidence="0.474949">Cymfony</affiliation>
<address confidence="0.989687">600 Essjay Road, Williamsville, NY 14221,</address>
<email confidence="0.996932">cniu@cymfony.com</email>
<email confidence="0.996932">wei@cymfony.com</email>
<email confidence="0.996932">rohini@cymfony.com</email>
<email confidence="0.996932">hli@cymfony.com</email>
<abstract confidence="0.998295303030303">Traditionally, word sense disambiguation (WSD) involves a different context classification model for each individual word. This paper presents a weakly supervised learning approach to WSD based on learning a word independent context pair classification model. Statistical models are not trained for classifying the word contexts, but for classifying a pair of contexts, i.e. determining if a pair of contexts of the same ambiguous word refers to the same or different senses. Using this approach, annotated corpus of a target be explored to disambiguate of a different word Hence, only a limited amount of existing annotated corpus is required in order to disambiguate the entire vocabulary. In this research, maximum entropy modeling is used to train the word independent context pair classification model. Then based on the context pair classification results, clustering is performed on word mentions extracted from a large raw corpus. The resulting context clusters are mapped onto the external thesaurus WordNet. This approach shows great flexibility to efficiently integrate heterogeneous knowledge sources, e.g. trigger words and parsing structures. Based on Senseval-3 Lexical Sample standards, this approach achieves state-of-the-art performance in the unsupervised learning category, and performs comparably with the supervised Naïve Bayes system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>In Journal of the American Society of Information Science</journal>
<contexts>
<context position="9807" citStr="Deerwester et al. 1990" startWordPosition="1523" endWordPosition="1526">l based) trigger word similarity: the trigger words around the keyword are represented as a vector, and the word i in context j is weighted as follows: weight i j = tf i j ( , ) ( , ) * log where tf (i, j) is the frequency of word i in the j-th context; D is the number of documents in the pool; and df (i) is the number of documents containing the word i. D and df (i) are estimated using the document pool introduced above. The cosine of the angle between two resulting vectors is used as the context similarity measure. (2) LSA-based (Latent Semantic Analysis based) trigger word similarity: LSA (Deerwester et al. 1990) is a technique used to uncover the underlying semantics based on co-occurrence data. The first step of LSA is to construct word-vs.-document co-occurrence matrix. Then singular value decomposition (SVD) is performed on this co-occurring matrix. The key idea of LSA is to reduce noise or insignificant association patterns by filtering the insignificant components uncovered by SVD. This is done by keeping only the top k singular values. By using the resulting word-vs.- document co-occurrence matrix after the filtering, each word can be represented as a vector in the semantic space. In our experi</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. 1990. Indexing by Latent Semantic Analysis. In Journal of the American Society of Information Science</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>A Method for Disambiguating Word Senses in a Large Corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26</pages>
<contexts>
<context position="1996" citStr="Gale et al. 1992" startWordPosition="293" endWordPosition="296">trigger words and parsing structures. Based on Senseval-3 Lexical Sample standards, this approach achieves state-of-the-art performance in the unsupervised learning category, and performs comparably with the supervised Naïve Bayes system. 1 Introduction Word Sense Disambiguation (WSD) is one of the central problems in Natural Language Processing. The difficulty of this task lies in the fact that context features and the corresponding statistical distribution are different for each individual word. Traditionally, WSD involves training the context classification models for each ambiguous word. (Gale et al. 1992) uses the Naïve Bayes method for context classification which requires a manually annotated corpus for each ambiguous word. This causes a serious Knowledge Bottleneck. The bottleneck is particularly serious when considering the domain dependency of word senses. To overcome the Knowledge Bottleneck, unsupervised or weakly supervised learning approaches have been proposed. These include the bootstrapping approach (Yarowsky 1995) and the context clustering approach (Schütze 1998). The above unsupervised or weakly supervised learning approaches are less subject to the Knowledge Bottleneck. For exa</context>
<context position="19438" citStr="Gale et al. 1992" startWordPosition="3168" endWordPosition="3171">e to which its context cluster corresponds to. As mentioned above, Sensval-2 English lexical sample training corpora is used to train the context pair classification model. And Sensval-3 English lexical sample testing corpora is used here for benchmarking. There are several keyword occurring in both Senseval-2 and Senseval-3 corpora. The sense tags associated with these keywords are not used in the context pair classification training process. In order to gauge the performance of this new weakly supervised learning algorithm, we have also implemented a supervised Naïve Bayes system following (Gale et al. 1992). This system is trained based on the Senseval-3 English Lexical Sample training corpus. In addition, for the purpose of quantifying the contribution from the parsing structures in WSD, we have run our new system with two configurations: (i) using only trigger words; (ii) using both trigger words and parsing relationships. All the benchmarking is performed using the Senseval-3 English Lexical Sample testing corpus and standards. The performance benchmarks for the two systems in three runs are shown in Table 1, Table 2 and Table 3. When using only trigger words, this algorithm has 8 percentage </context>
<context position="21335" citStr="Gale et al. 1992" startWordPosition="3487" endWordPosition="3490">2. Supervised Naïve Bayes System Category Accuracy Fine grain (%) Coarse grain (%) Adjective (5) 44.7 56.6 Noun (20) 66.3 74.5 Verb (32) 58.6 70.0 Overall 61.6 71.5 Table 3. New Algorithm Using Both Trigger Words and Parsing Category Accuracy Fine grain (%) Coarse grain (%) Adjective (5) 49.1 64.8 Noun (20) 57.9 66.6 Verb (32) 55.3 66.3 Overall 56.3 66.4 38 It is noted that Naïve Bayes algorithm has many variation, and its performance has been greatly enhanced during recent research. Based on Senseval-3 results, the best Naïve Bayse system outperform our version (which is implemented based on Gale et al. 1992) by 8%~10%. So the best supervised WSD systems output-perform our weakly supervised WSD system by 13%~15% in accuracy. 5 Conclusion We have presented a weakly supervised learning approach to WSD. Statistical models are not trained for the contexts of each individual word, but for context pair classification. This approach overcomes the knowledge bottleneck that challenges supervised WSD systems which need labeled data for each individual word. It captures the correlation regularity between the sense distinction and the context distinction at Part-of-Speech category level, independent of indivi</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, W., K. Church, and D. Yarowsky. 1992. A Method for Disambiguating Word Senses in a Large Corpus. Computers and the Humanities, 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Exponential Priors for Maximum Entropy Models.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL</booktitle>
<contexts>
<context position="14799" citStr="Goodman 2003" startWordPosition="2282" endWordPosition="2284">text pair associated with different senses. The maximum entropy modeling is used to compute the conditional probabilities Pr (Si = Sj CSi ,j ) and Pr (Si ≠ Sj CSi,j ): once the context pair CSi ,j is represented as { fα} , the conditional probability is given as 1 Pr { } (1) ( ) t f = ∏ wt f α , f f α ∈ { } where t ∈ {Si = Sj , Si ≠ Sj }, Z is the normalization factor, wt ,f is the weight associated with tag t and feature f . Using the training corpora constructed above, the weights can be computed based on Iterative Scaling algorithm (Pietra etc. 1995) The exponential prior smoothing scheme (Goodman 2003) is adopted in the training. Z 36 applied to search for the global optimal solution. The optimization algorithm is as follows. 1. Set the initial state {K, M }as K =n, and 3 Context Clustering based on Context Pair Classification Results Given n mentions {Ci }of a keyword, we use the following context clustering scheme. The discovered context clusters correspond to distinct word senses. For any given context pair, the context similarity features defined in Section 2 are computed. With n n( n − 1) mentions of the same keyword, context 2 similarities CSi ,j (i ∈ [1, n ,]j ∈ 1,[i ) )are computed.</context>
</contexts>
<marker>Goodman, 2003</marker>
<rawString>Goodman, J. 2003. Exponential Priors for Maximum Entropy Models. In Proceedings of HLTNAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<pages>211--240</pages>
<contexts>
<context position="11000" citStr="Landauer &amp; Dumais, 1997" startWordPosition="1713" endWordPosition="1716">e semantic space. In our experiment, we constructed the original word-vs.-document co-occurring matrix as follows: 100,000 documents from the TIPSTER corpus were used to construct the co-occurring matrix. We processed these documents using our POS tagger, and selected the top n most frequently mentioned words from each POS category as base words: top 20,000 common nouns top 40,000 proper names top 10,000 verbs top 10,000 adjectives top 2,000 adverbs In performing SVD, we set k (i.e. the number of nonzero singular values) as 200, following the practice reported in (Deerwester et al. 1990) and (Landauer &amp; Dumais, 1997). Using the LSA scheme described above, each word is represented as a vector in the semantic space. The co-occurring trigger words are represented as a vector summation. Then the cosine of the angle between the two resulting vector summations is computed, and used as the context similarity measure. (3) LSA-based parsing relationship similarity: each relationship is in the form of Rα(w) . Using LSA, each word w is represented as a D df (i) 35 semantic vector V (w .) The similarity between Rα(w1)and Rα(w2) is represented as the cosine of the angle between V (w1 )and V (w2 .) Two special values a</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, T. K., &amp; Dumais, S. T. 1997. A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104, 211-240, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A Mention-Synchronous Coreference Resolution Algorithm Based on the Bell Tree.</title>
<date>2004</date>
<booktitle>In The Proceedings of ACL</booktitle>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla and S. Roukos. A Mention-Synchronous Coreference Resolution Algorithm Based on the Bell Tree. In The Proceedings of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Neal</author>
</authors>
<title>Probabilistic Inference Using Markov Chain Monte Carlo Methods.</title>
<date>1993</date>
<tech>Technical Report,</tech>
<institution>Univ. of Toronto.</institution>
<contexts>
<context position="17201" citStr="Neal 1993" startWordPosition="2793" endWordPosition="2794">anging cluster number and cluster contents; sc i j ( ( S i S j CS i , j ) 1 , = log Pr = which correspond to 0 , = log sc i j ( ( S i Sj CSi,j ) Pr = and , ] ) i∈ [ 1 , n [ 1, i set x = s , ] ) otherwise s c K, M 1 ( { } ) c({K,M}) if(x&gt;=1) { set {K, M }= {K, M 1} (sc a, M })= I sc j ) (2) i 1 ,n ∈ [ j i ∈ [ 1 , r0,if M i M j ( ) ( ) = where ( ) k i j , = j l1, Similar clustering scheme has been used successfully for the task of co-reference in (Luo etc. 2004), (Zelenko, Aone and Tibbetts, 2004a) and (Zelenko, Aone and Tibbetts, 2004b). In this paper, statistical annealing-based optimization (Neal 1993) is used to search for {K,M } which maximizes Expression (2). The optimization process consists of two steps. First, an intermediate solution {K,M }0 is computed by a greedy algorithm. Then by setting {K,M }0 as the initial state, statistical annealing is } else { set {K,M }= {K,M 1} with probability x . β } if sc( { K,M} ) &gt; sc(K, { M 0} ) then set {K, M }0 = {K, M } } j∈ } output {K, M }0 as the optimal state. 37 4 Benchmarking Corpus-driven context clusters need to map to a word sense standard to facilitate performance benchmark. Using Senseval-3 evaluation standards, we implemented the fol</context>
</contexts>
<marker>Neal, 1993</marker>
<rawString>Neal, R.M. 1993. Probabilistic Inference Using Markov Chain Monte Carlo Methods. Technical Report, Univ. of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D Pietra</author>
<author>V D Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing Features Of Random Fields.</title>
<date>1995</date>
<booktitle>In IEEE Transactions on Pattern Analysis and Machine Intelligence.</booktitle>
<marker>Pietra, Pietra, Lafferty, 1995</marker>
<rawString>Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. Inducing Features Of Random Fields. In IEEE Transactions on Pattern Analysis and Machine Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schütze</author>
</authors>
<title>Automatic Word Sense Disambiguation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<contexts>
<context position="2477" citStr="Schütze 1998" startWordPosition="363" endWordPosition="364"> individual word. Traditionally, WSD involves training the context classification models for each ambiguous word. (Gale et al. 1992) uses the Naïve Bayes method for context classification which requires a manually annotated corpus for each ambiguous word. This causes a serious Knowledge Bottleneck. The bottleneck is particularly serious when considering the domain dependency of word senses. To overcome the Knowledge Bottleneck, unsupervised or weakly supervised learning approaches have been proposed. These include the bootstrapping approach (Yarowsky 1995) and the context clustering approach (Schütze 1998). The above unsupervised or weakly supervised learning approaches are less subject to the Knowledge Bottleneck. For example, (Yarowsky 1995) only requires sense number and a few seeds for each sense of an ambiguous word (hereafter called keyword). (Schütze 1998) may only need minimal annotation to map the resulting context clusters onto external thesaurus for benchmarking and application-related purposes. Both methods are based on trigger words only. This paper presents a novel approach based on learning word-independent context pair classification model. This idea may be traced back to (Schüt</context>
<context position="8222" citStr="Schütze 1998" startWordPosition="1276" endWordPosition="1277">hat refer to different senses of a word. For each POS category, we have constructed about 36,000 instances, half positive and half negative. The instances are represented as pairwise context similarities, taking the form of Cfo I Before presenting the context similarity features we used, we first introduce the two categories of the involved context features: i) Co-occurring trigger words within a predefined window size equal to 50 words to both sides of the keyword. The trigger words are learned from a TIPSTER document pool containing ~170 million words of AP and WSJ news articles. Following (Schütze 1998), L2 is used to measure the cohesion between the keyword and a co-occurring word. In our ex34 periment, all the words are first sorted based on its χ2 with the keyword, and then the top 2,000 words are selected as trigger words. ii) Parsing relationships associated with the keyword automatically decoded by a broadcoverage parser, with F-measure (i.e. the precision-recall combined score) at about 85% (reference temporarily omitted for the sake of blind review). The logical dependency relationships being utilized are listed below. Noun: subject-of, object-of, complement-of, has-adjective-modifie</context>
</contexts>
<marker>Schütze, 1998</marker>
<rawString>Schütze, H. 1998. Automatic Word Sense Disambiguation. Computational Linguistics, 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="2426" citStr="Yarowsky 1995" startWordPosition="355" endWordPosition="356">ding statistical distribution are different for each individual word. Traditionally, WSD involves training the context classification models for each ambiguous word. (Gale et al. 1992) uses the Naïve Bayes method for context classification which requires a manually annotated corpus for each ambiguous word. This causes a serious Knowledge Bottleneck. The bottleneck is particularly serious when considering the domain dependency of word senses. To overcome the Knowledge Bottleneck, unsupervised or weakly supervised learning approaches have been proposed. These include the bootstrapping approach (Yarowsky 1995) and the context clustering approach (Schütze 1998). The above unsupervised or weakly supervised learning approaches are less subject to the Knowledge Bottleneck. For example, (Yarowsky 1995) only requires sense number and a few seeds for each sense of an ambiguous word (hereafter called keyword). (Schütze 1998) may only need minimal annotation to map the resulting context clusters onto external thesaurus for benchmarking and application-related purposes. Both methods are based on trigger words only. This paper presents a novel approach based on learning word-independent context pair classific</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, D. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proceedings of ACL 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>J</author>
</authors>
<title>Tibbetts. Coreference Resolution for Information Extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL 2004 Workshop on Reference Resolution and its Application.</booktitle>
<marker>Zelenko, Aone, J, 2004</marker>
<rawString>Zelenko, D., C. Aone and J. 2004. Tibbetts. Coreference Resolution for Information Extraction. In Proceedings of ACL 2004 Workshop on Reference Resolution and its Application.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>J</author>
</authors>
<title>Tibbetts. Binary Integer Programming for Information Extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACE 2004 Evaluation Workshop.</booktitle>
<marker>Zelenko, Aone, J, 2004</marker>
<rawString>Zelenko, D., C. Aone and J. 2004. Tibbetts. Binary Integer Programming for Information Extraction. In Proceedings of ACE 2004 Evaluation Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>