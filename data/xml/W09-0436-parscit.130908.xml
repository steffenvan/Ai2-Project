<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.792096">
Disambiguating “DE” for Chinese-English Machine Translation
</title>
<author confidence="0.926841">
Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Manning
</author>
<affiliation confidence="0.949182">
Computer Science Department, Stanford University
</affiliation>
<address confidence="0.890872">
Stanford, CA 94305
</address>
<email confidence="0.99604">
pichuan,jurafsky,manning@stanford.edu
</email>
<sectionHeader confidence="0.997355" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993207">
Linking constructions involving n, (DE) are ubiq-
uitous in Chinese, and can be translated into En-
glish in many different ways. This is a major source
of machine translation error, even when syntax-
sensitive translation models are used. This paper
explores how getting more information about the
syntactic, semantic, and discourse context of uses
of n, (DE) can facilitate producing an appropriate
English translation strategy. We describe a finer-
grained classification of n, (DE) constructions in
Chinese NPs, construct a corpus of annotated ex-
amples, and then train a log-linear classifier, which
contains linguistically inspired features. We use the
DE classifier to preprocess MT data by explicitly
labeling n, (DE) constructions, as well as reorder-
ing phrases, and show that our approach provides
significant BLEU point gains on MT02 (+1.24),
MT03 (+0.88) and MT05 (+1.49) on a phrased-
based system. The improvement persists when a hi-
erarchical reordering model is applied.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999744142857143">
Machine translation (MT) from Chinese to En-
glish has been a difficult problem: structural dif-
ferences between Chinese and English, such as
the different orderings of head nouns and rela-
tive clauses, cause BLEU scores to be consis-
tently lower than for other difficult language pairs
like Arabic-English. Many of these structural
differences are related to the ubiquitous Chinese
n, (DE) construction, used for a wide range of
noun modification constructions (both single word
and clausal) and other uses. Part of the solution
to dealing with these ordering issues is hierarchi-
cal decoding, such as the Hiero system (Chiang,
2005), a method motivated by n, (DE) examples
like the one in Figure 1. In this case, the transla-
tion goal is to rotate the noun head and the preced-
ing relative clause around n, (DE), so that we can
translate to “[one of few countries] n, [have diplo-
matic relations with North Korea]”. Hiero can
learn this kind of lexicalized synchronous gram-
mar rule.
But use of hierarchical decoders has not solved
the DE construction translation problem. We ana-
lyzed the errors of three state-of-the-art systems
(the 3 DARPA GALE phase 2 teams’ systems),
and even though all three use some kind of hier-
archical system, we found many remaining errors
related to reordering. One is shown here:
</bodyText>
<figure confidence="0.399463">
�M _ff _tPTIftE n, VP*
</figure>
<figureCaption confidence="0.829149">
local a bad reputation DE middle school
Reference: ‘a local middle school with a bad reputation’
Team 1: ‘a bad reputation of the local secondary school’
Team 2: ‘the local a bad reputation secondary school’
Team 3: ‘a local stigma secondary schools’
</figureCaption>
<bodyText confidence="0.999672266666667">
None of the teams reordered “bad reputation”
and “middle school” around the n, . We argue that
this is because it is not sufficient to have a for-
malism which supports phrasal reordering, but it
is also necessary to have sufficient linguistic mod-
eling that the system knows when and how much
to rearrange.
An alternative way of dealing with structural
differences is to reorder source language sentences
to minimize structural divergence with the target
language, (Xia and McCord, 2004; Collins et al.,
2005; Wang et al., 2007). For example Wang et
al. (2007) introduced a set of rules to decide if
a n, (DE) construction should be reordered or not
before translating to English:
</bodyText>
<listItem confidence="0.981746333333333">
• For DNPs (consisting of“XP+DEG”):
– Reorder if XP is PP or LCP;
– Reorder if XP is a non-pronominal NP
• For CPs (typically formed by “IP+DEC”):
– Reorder to align with the “that+clause”
structure of English.
</listItem>
<bodyText confidence="0.999807727272727">
Although this and previous reordering work has
led to significant improvements, errors still re-
main. Indeed, Wang et al. (2007) found that the
precision of their NP rules is only about 54.6% on
a small human-judged set.
One possible reason the n, (DE) construction re-
mains unsolved is that previous work has paid in-
sufficient attention to the many ways the n, (DE)
construction can be translated and the rich struc-
tural cues to the translation. Wang et al. (2007),
for example, characterized n, (DE) into only two
</bodyText>
<note confidence="0.9378705">
Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 215–223,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997554">
215
</page>
<figure confidence="0.715302">
�� � �� � �� { �� ��Aozhou shi yu Beihan you bangjiao DE shaoshu guojia zhiyi .
Australia is with North Korea have diplomatic relations that few countries one of .
</figure>
<figureCaption confidence="0.7935895">
&apos;Australia is one of the few countries that have diplomatic relations with North Korea.&apos;
Figure 1: An example of the DE construction from (Chiang, 2005)
</figureCaption>
<bodyText confidence="0.999938714285714">
classes. But our investigation shows that there are
many strategies for translating Chinese [A { B]
phrases into English, including the patterns in Ta-
ble 1, only some involving reversal.
Notice that the presence of reordering is only
one part of the rich structure of these examples.
Some reorderings are relative clauses, while others
involve prepositional phrases, but not all preposi-
tional phrase uses involve reorderings. These ex-
amples suggest that capturing finer-grained trans-
lation patterns could help achieve higher accuracy
both in reordering and in lexical choice.
In this work, we propose to use a statistical clas-
sifier trained on various features to predict for a
given Chinese {(DE) construction both whether
it will reorder in English and which construction
it will translate to in English. We suggest that the
necessary classificatory features can be extracted
from Chinese, rather than English. The {(DE)
in Chinese has a unified meaning of ‘noun mod-
ification’, and the choice of reordering and con-
struction realization are mainly a consequence of
facts of English noun modification. Nevertheless,
most of the features that determine the choice of
a felicitous translation are available in the Chi-
nese source. Noun modification realization has
been widely studied in English (e.g., (Rosenbach,
2003)), and many of the important determinative
properties (e.g., topicality, animacy, prototypical-
ity) can be detected working in the source lan-
guage.
We first present some corpus analysis charac-
terizing different DE constructions based on how
they get translated into English (Section 2). We
then train a classifier to label DEs into the 5 dif-
ferent categories that we define (Section 3). The
fine-grained DEs, together with reordering, are
then used as input to a statistical MT system (Sec-
tion 4). We find that classifying DEs into finer-
grained tokens helps MT performance, usually at
least twice as much as just doing phrasal reorder-
ing.
</bodyText>
<sectionHeader confidence="0.988467" genericHeader="introduction">
2 DE classification
</sectionHeader>
<bodyText confidence="0.9995562">
The Chinese character DE serves many different
purposes. According to the Chinese Treebank tag-
ging guidelines (Xia, 2000), the character can be
tagged as DEC, DEG, DEV, SP, DER, or AS. Sim-
ilar to (Wang et al., 2007), we only consider the
majority case when the phrase with {(DE) is a
noun phrase modifier. The DEs in NPs have a
part-of-speech tag of DEC (a complementizer or
a nominalizer) or DEG (a genitive marker or an
associative marker).
</bodyText>
<subsectionHeader confidence="0.991771">
2.1 Class Definition
</subsectionHeader>
<bodyText confidence="0.999559875">
The way we categorize the DEs is based on their
behavior when translated into English. This is im-
plicitly done in the work of Wang et al. (2007)
where they use rules to decide if a certain DE and
the words next to it will need to be reordered. In
this work, we categorize DEs into finer-grained
categories. For a Chinese noun phrase [A { B],
we categorize it into one of these five classes:
</bodyText>
<listItem confidence="0.56892">
1. A B
</listItem>
<bodyText confidence="0.999547142857143">
In this category, A in the Chinese side is trans-
lated as a pre-modifier of B. In most of the
cases A is an adjective form, like Example 1.1
in Table 1 or the possessive adjective exam-
ple in Example 1.2. Compound nouns where
A becomes a pre-modifier of B also fit in this
category (Example 1.3).
</bodyText>
<sectionHeader confidence="0.686402" genericHeader="method">
2. B preposition A
</sectionHeader>
<bodyText confidence="0.991923555555556">
There are several cases that get translated into
the form B preposition A. For example, the of-
genitive in Example 2.1 in Table 1.
Example 2.2 shows cases where the Chinese
A gets translated into a prepositional phrase
that expresses location.
When A becomes a gerund phrase and an ob-
ject of a preposition, it is also categorized in
the B preposition A category (Example 2.3).
</bodyText>
<listItem confidence="0.592059">
3. A ’s B
</listItem>
<bodyText confidence="0.9989392">
In this class, the English translation is an ex-
plicit s-genitive case, as in Example 3.1. This
class occurs much less often but is still in-
teresting because of the difference from the
of-genitive.
</bodyText>
<sectionHeader confidence="0.41297" genericHeader="method">
4. relative clause
</sectionHeader>
<bodyText confidence="0.940316">
We include the obvious relative clause cases
like Example 4.1 where a relative clause is
</bodyText>
<page confidence="0.997485">
216
</page>
<bodyText confidence="0.667065">
introduced by a relative pronoun. We also in-
clude reduced relative clauses like Example
4.2 in this class.
</bodyText>
<listItem confidence="0.744312">
5. A preposition B
</listItem>
<bodyText confidence="0.999829777777778">
This class is another small one. The English
translations that fall into this class usually
have some number, percentage or level word
in the Chinese A.
Some NPs are translated into a hybrid of these cat-
egories, or just don’t fit into one of the five cate-
gories, for instance, involving an adjectival pre-
modifier and a relative clause. In those cases, they
are put into an “other” category.1
</bodyText>
<subsectionHeader confidence="0.998809">
2.2 Data annotation of DE classes
</subsectionHeader>
<bodyText confidence="0.999971882352941">
In order to train a classifier and test its per-
formance, we use the Chinese Treebank 6.0
(LDC2007T36) and the English Chinese Trans-
lation Treebank 1.0 (LDC2007T02). The word
alignment data (LDC2006E93) is also used to
align the English and Chinese words between
LDC2007T36 and LDC2007T02. The overlap-
ping part of the three datasets are a subset of CTB6
files 1 to 325. After preprocessing those three
sets of data, we have 3253 pairs of Chinese sen-
tences and their translations. In those sentences,
we use the gold-standard Chinese tree structure to
get 3412 Chinese DEs in noun phrases that we
want to annotate. Among the 3412 DEs, 530 of
them are in the “other” category and are not used
in the classifier training and evaluation. The statis-
tics of the five classes are:
</bodyText>
<listItem confidence="0.9999048">
1. A B: 693 (24.05%)
2. B preposition A: 1381 (47.92%)
3. A ’s B: 91 (3.16%)
4. relative clause: 669 (23.21%)
5. A preposition B: 48 (1.66%)
</listItem>
<sectionHeader confidence="0.966817" genericHeader="method">
3 Log-linear DE classifier
</sectionHeader>
<bodyText confidence="0.999106">
In order to see how well we can categorize DEs in
noun phrases into one of the five classes, we train a
log-linear classifier to classify each DE according
to features extracted from its surrounding context.
Since we want the training and testing conditions
to match, when we extract features for the classi-
fier, we don’t use gold-standard parses. Instead,
we use a parser trained on CTB6 excluding files
1-325. We then use this parser to parse the 3253
</bodyText>
<footnote confidence="0.941526">
1The “other” category contains many mixed cases that
could be difficult Chinese patterns to translate. We will leave
this for future work.
</footnote>
<table confidence="0.999509">
5-class Acc. (%) 2-class Acc. (%)
baseline - 76.0
DEPOS 54.8 71.0
+A-pattern 67.9 83.7
+POS-ngram 72.1 84.9
+Lexical 74.9 86.5
+SemClass 75.1 86.7
+Topicality 75.4 86.9
</table>
<tableCaption confidence="0.958080666666667">
Table 2: 5-class and 2-class classification accuracy. “base-
line” is the heuristic rules in (Wang et al., 2007). Others are
various features added to the log-linear classifier.
</tableCaption>
<bodyText confidence="0.899676">
Chinese sentences with the DE annotation and ex-
tract parse-related features from there.
</bodyText>
<subsectionHeader confidence="0.999341">
3.1 Experimental setting
</subsectionHeader>
<bodyText confidence="0.999986222222222">
For the classification experiment, we exclude the
“other” class and only use the 2882 examples that
fall into the five pre-defined classes. To evalu-
ate the classification performance and understand
what features are useful, we compute the accuracy
by averaging five 10-fold cross-validations.2
As a baseline, we use the rules introduced in
Wang et al. (2007) to decide if the DEs require re-
ordering or not. However, since their rules only
decide if there is reordering in an NP with DE,
their classification result only has two classes. So,
in order to compare our classifier’s performance
with the rules in Wang et al. (2007), we have to
map our five-class results into two classes. We
mapped our five-class results into two classes. So
we mapped B preposition A and relative clause
into the class “reordered”, and the other three
classes into “not-reordered”.
</bodyText>
<subsectionHeader confidence="0.999761">
3.2 Feature Engineering
</subsectionHeader>
<bodyText confidence="0.819681157894737">
To understand which features are useful for DE
classification, we list our feature engineering steps
and results in Table 2. In Table 2, the 5-class ac-
curacy is defined by:
(number of correctly labeled DEs)
(number of all DEs)
The 2-class accuracy is defined similarly, but it
is evaluated on the 2-class “reordered” and “not-
reordered” after mapping from the 5 classes.
The DEs we are classifying are within an NP;
we refer to them as [A In&apos; B]NP. A includes all
the words in the NP before In&apos;; B includes all the
words in the NP after In&apos;. To illustrate, we will use
the following NP:
[[*04 AYLL )lQ]A In&apos; [RNT 3�&apos; Q]B]NP
2We evaluate the classifier performance using cross-
validations to get the best setting for the classifier. The proof
of efficacy of the DE classifier is MT performance on inde-
pendent data in Section 4.
</bodyText>
<equation confidence="0.892458">
X 100
</equation>
<page confidence="0.96278">
217
</page>
<figure confidence="0.915958944444444">
1. A B
1.1. tt1g-(excellent)/n(DE)/A3E(geographical)/,,+ft-(qualification) → “excellent geographical qualifications”
1.2. Rffl(our)/n(DE)/4-0(financial)/FXLR(risks) → “our financial risks”
1.3. Mfi(trade)/n(DE)/�*&apos;[t(complement) → “trade complement”
2. B preposition A
2.1. RR(investment)/� (environment)/n(DE)/f�Czl-l�(improvement) → “the improvement of the investment environment”
2.2. ARAIA(Chongming county)/A(inside)/n(DE)/* l(organization) → “organizations inside Chongming county”
2.3. —(one)/^(measure word)/A;�,,(observe)/rPQ(China)/�t3j(market)/n(DE)/11&apos;,11&apos;,(small)/gQ(window)
→ “a small window for watching over Chinese markets”
3. A ’s B
3.1. Q*(nation)/n(DE)/94L-�(macro)/ 93E(management) → “the nation ’s macro management”
4. relative clause
4.1. rPQ(China)/TNR&apos;(cannot)/t1?`(produce)/�(and)/R(but)/�(very)/ rXn-(need)/n(DE)/9aa(medicine)
→ “medicine that cannot be produced by China but is urgently needed”
4.2. *_A(foreign business)/&amp;R(invest)/�L�lk(enterprise)/gz(acquire)/n(DE)/k�f5(RMB)/2rA(loan)
→ “the loans in RMB acquired by foreign-invested enterprises”
5. A preposition B
5.1. VQl� T7 (more than 40 million)/Ac7G(US dollar)/n(DE)/1?`aa(product) → more than 40 million US dollars in products
</figure>
<tableCaption confidence="0.984493">
Table 1: Examples for the 5 DE classes
</tableCaption>
<bodyText confidence="0.986151">
to show examples of each feature. The parse struc-
ture of the NP is listed in Figure 2.
</bodyText>
<figure confidence="0.957465777777778">
(NP
(NP (NR Q))
(CP
(IP
(VP
(ADVP (AD J%))
(VP (VA �k))))
(DEC n))
(NP (NN RR) (NNQ))))))
</figure>
<figureCaption confidence="0.999939">
Figure 2: The parse tree of the Chinese NP.
</figureCaption>
<bodyText confidence="0.981471666666667">
A-pattern: Chinese syntactic patterns
appearing before n
Secondly, we want to incorporate the rules in
(Wang et al., 2007) as features in the log-linear
classifier. We added features for certain indicative
patterns in the parse tree (listed in Table 3).
</bodyText>
<listItem confidence="0.617495909090909">
1. A is ADJP:
true if A+DE is a DNP which is in the form of “ADJP+DEG”.
2. A is QP:
true if A+DE is a DNP which is in the form of “QP+DEG”.
3. A is pronoun:
true if A+DE is a DNP which is in the form of “NP+DEG”, and
the NP is a pronoun.
4. A ends with VA:
true if A+DE is a CP which is in the form of “IP+DEC”, and
the IP ends with a VP that’s either just a VA or a VP preceded
by a ADVP.
</listItem>
<tableCaption confidence="0.998355">
Table 3: A-pattern features
</tableCaption>
<bodyText confidence="0.990973225806452">
DEPOS: part-of-speech tag of DE
Since the part-of-speech tag of DE indicates its
syntactic function, it is the first obvious feature
to add. The NP in Figure 2 will have the fea-
ture “DEC”. This basic feature will be referred to
as DEPOS. Note that since we are only classifying
DEs in NPs, ideally the part-of-speech tag of DE
will either be DEC or DEG as described in Section
2. However, since we are using automatic parses
instead of gold-standard ones, the DEPOS feature
might have other values than just DEC and DEG.
From Table 2, we can see that with this simple fea-
ture, the 5-class accuracy is low but at least better
than simply guessing the majority class (47.92%).
The 2-class accuracy is still lower than using the
heuristic rules in (Wang et al., 2007), which is rea-
sonable because their rules encode more informa-
tion than just the POS tags of DEs.
Features 1–3 are inspired by the rules in (Wang
et al., 2007), and the fourth rule is based on the
observation that even though the predicative ad-
jective VA acts as a verb, it actually corresponds to
adjectives in English as described in (Xia, 2000).3
We call these four features A-pattern. Our exam-
ple NP in Figure 2 will have the fourth feature
“A ends with VA” in Table 3, but not the other
three features. In Table 2 we can see that after
adding A-pattern, the 2-class accuracy is already
much higher than the baseline. We attribute this
to the fourth rule and also to the fact that the clas-
sifier can learn weights for each feature.4
</bodyText>
<footnote confidence="0.996524666666667">
3Quote from (Xia, 2000): “VA roughly corresponds to ad-
jectives in English and stative verbs in the literature on Chi-
nese grammar.”
4We also tried extending a rule-based 2-class classifier
with the fourth rule. The accuracy is 83.48%, only slightly
lower than using the same features in a log-linear classifier.
</footnote>
<page confidence="0.979731">
218
</page>
<bodyText confidence="0.965732777777778">
POS-ngram: unigrams and bigrams of POS tags
The POS-ngram feature adds all unigrams and bi-
grams in A and B. Since A and B have different
influences on the choice of DE class, we distin-
guish their ngrams into two sets of features. We
also include the bigram pair across DE which gets
another feature name for itself. The example NP
in Figure 2 will have these features (we use b to
indicate boundaries):
</bodyText>
<listItem confidence="0.9986">
• POS unigrams in A: “NR”, “AD”, “VA”
• POS bigrams in A: “b-NR”, “NR-AD”, “AD-
VA”, “VA-b”
• cross-DE POS bigram: “VA-NN”
• POS unigram in B: “NN”
• POS bigrams in B: “b-NN”, “NN-NN”, “NN-
b”
</listItem>
<bodyText confidence="0.9999055">
The part-of-speech ngram features add 4.24%
accuracy to the 5-class classifier.
</bodyText>
<sectionHeader confidence="0.815368" genericHeader="method">
Lexical: lexical features
</sectionHeader>
<bodyText confidence="0.968711166666667">
In addition to part-of-speech features, we also
tried to use features from the words themselves.
But since using full word identity resulted in a
sparsity issue,5 we take the one-character suffix of
each word and extract suffix unigram and bigram
features from them. The argument for using suf-
fixes is that it often captures the larger category of
the word (Tseng et al., 2005). For example, rp
) (China) and 8) (Korea) share the same suffix
), which means “country”. These suffix ngram
features will result in these features for the NP in
Figure 2:
</bodyText>
<listItem confidence="0.98579475">
• suffix unigrams: “)”, “JRL”, “�”, “n,”,
“_M”, “)”
• suffix bigrams: “b-)”, “)-JRL”, “JRL-)�”,
“)�-n,”, “n,-_M”, “_M-)”, “W-b”
</listItem>
<bodyText confidence="0.999625666666667">
Other than the suffix ngram, we �Ialso add three
other lexical features: first, if the word before DE
is a noun, we add a feature that is the conjunc-
tion of POS and suffix unigram. Secondly, an
“NR only” feature will fire when A only consists of
one or more NRs. Thirdly, we normalize different
forms of “percentage” representation, and add a
feature if they exist. This includes words that start
with “ ff or ends with the percentage sign
“%”. The first two features are inspired by the fact
that a noun and its type can help decide “B prep A”
versus “A B”. Here we use the suffix of the noun
</bodyText>
<footnote confidence="0.979304">
5The accuracy is worse when we tried using the word
identity instead of the suffix.
</footnote>
<bodyText confidence="0.983961382978723">
and the NR (proper noun) tag to help capture its
animacy, which is useful in choosing between the
s-genitive (the boy’s mother) and the of-genitive
(the mother of the boy) in English (Rosenbach,
2003). The third feature is added because many of
the cases in the “A preposition B” class have a per-
centage number in A. We call these sets of features
Lexical. Together they provide 2.73% accuracy im-
provement over the previous setting.
SemClass: semantic class of words
We also use a Chinese thesaurus, CiLin, to look up
the semantic classes of the words in [A n, B] and
use them as features. CiLin is a Chinese thesaurus
published in 1984 (Mei et al., 1984). CiLin is or-
ganized in a conceptual hierarchy with five levels.
We use the level-1 tags which includes 12 cate-
gories.6 This feature fires when a word we look up
has one level-1 tag in CiLin. This kind of feature
is referred to as SemClass in Table 2. For the ex-
ample in Figure 2, two words have a single level-
1 tag: “JRL”(most) has a level-1 tag K7 and “R
_M”(investment) has a level-1 tag H8. “8)” and
“XJ&apos;*)” are not listed in CiLin, and “)�” has
multiple entries. Therefore, the SemClass features
are: (i) before DE: “K”; (ii) after DE: “H”
Topicality: re-occurrence of nouns
The last feature we add is a Topicality feature,
which is also useful for disambiguating s-genitive
and of-genitive. We approximate the feature by
caching the nouns in the previous two sentences,
and fire a topicality feature when the noun appears
in the cache. Take this NP in MT06 as an example:
“W8 -1:j I8 n, Ai2� f Wk
For this NP, all words before DE and after DE
appeared in the previous sentence. Therefore the
topicality features “cache-before-DE” and “cache-
after-DE” both fire.
After all the feature engineering above, the
best accuracy on the 5-class classifier we have
is 75.4%, which maps into a 2-class accuracy of
86.9%. Comparing the 2-class accuracy to the
(Wang et al., 2007) baseline, we have a 10.9%
absolute improvement. The 5-class accuracy and
confusion matrix is listed in Table 4.
“A preposition B” is a small category and is the
most confusing. “A ’s B” also has lower accuracy,
and is mostly confused with “B preposition A”.
</bodyText>
<footnote confidence="0.999978333333333">
6We also tried adding more levels but it did not help.
7K is the category iia (auxiliary) in CiLin.
8H is the category ÙM (activities) in CiLin.
</footnote>
<page confidence="0.5245745">
”
219
</page>
<table confidence="0.999672125">
real — A ’s B AB A prep. B B prep. A rel. clause
A ’s B 168 36 0 110 0
AB 48 2473 73 227 216
A prep. B 0 18 46 23 11
B prep. A 239 691 95 5915 852
rel. clause 0 247 26 630 2266
Total 455 3465 240 6905 3345
Accuracy(%) 36.92 71.37 19.17 85.66 67.74
</table>
<tableCaption confidence="0.999885">
Table 4: The confusion matrix for 5-class DE classification
</tableCaption>
<bodyText confidence="0.9994995">
This could be due to the fact that there are some
cases where the translation is correct both ways,
but also could be because the features we added
have not captured the difference well enough.
</bodyText>
<sectionHeader confidence="0.997119" genericHeader="method">
4 Machine Translation Experiments
</sectionHeader>
<subsectionHeader confidence="0.942765">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.99819940625">
For our MT experiments, we used a re-
implementation of Moses (Koehn et al., 2003), a
state-of-the-art phrase-based system. The align-
ment is done by the Berkeley word aligner (Liang
et al., 2006) and then we symmetrized the word
alignment using the grow-diag heuristic. For fea-
tures, we incorporate Moses’ standard eight fea-
tures as well as the lexicalized reordering model.
Parameter tuning is done with Minimum Error
Rate Training (MERT) (Och, 2003). The tun-
ing set for MERT is the NIST MT06 data set,
which includes 1664 sentences. We evaluate the
result with MT02 (878 sentences), MT03 (919
sentences), and MT05 (1082 sentences).
Our MT training corpus contains 1,560,071 sen-
tence pairs from various parallel corpora from
LDC.9 There are 12,259,997 words on the English
side. Chinese word segmentation is done by the
Stanford Chinese segmenter (Chang et al., 2008).
After segmentation, there are 11,061,792 words
on the Chinese side. We use a 5-gram language
model trained on the Xinhua and AFP sections of
the Gigaword corpus (LDC2007T40) and also the
English side of all the LDC parallel data permissi-
ble under the NIST08 rules. Documents of Giga-
word released during the epochs of MT02, MT03,
MT05, and MT06 were removed.
To run the DE classifier, we also need to parse
the Chinese texts. We use the Stanford Chinese
parser (Levy and Manning, 2003) to parse the Chi-
nese side of the MT training data and the tuning
and test sets.
</bodyText>
<footnote confidence="0.265789333333333">
9LDC2003E07, LDC2003E14, LDC2005E83,
LDC2005T06, LDC2006E26, LDC2006E85, LDC2006E85,
LDC2005T34, and LDC2005T34
</footnote>
<subsectionHeader confidence="0.990778">
4.2 Baseline Experiments
</subsectionHeader>
<bodyText confidence="0.999988857142857">
We have two different settings as baseline exper-
iments. The first is without reordering or DE an-
notation on the Chinese side; we simply align the
parallel texts, extract phrases and tune parameters.
This experiment is referred to as BASELINE. Also,
we reorder the training data, the tuning and the
test sets with the NP rules in (Wang et al., 2007)
and compare our results with this second baseline
(WANG-NP).
The NP reordering preprocessing (WANG-NP)
showed consistent improvement in Table 5 on all
test sets, with BLEU point gains ranging from
0.15 to 0.40. This confirms that having reorder-
ing around DEs in NP helps Chinese-English MT.
</bodyText>
<subsectionHeader confidence="0.99876">
4.3 Experiments with 5-class DE annotation
</subsectionHeader>
<bodyText confidence="0.998094166666667">
We use the best setting of the DE classifier de-
scribed in Section 3 to annotate DEs in NPs in the
MT training data as well as the NIST tuning and
test sets.10 If a DE is in an NP, we use the annota-
tion of {AB, {AsB, {BprepA, {relc, or {AprepB
to replace the original DE character. Once we have
the DEs labeled, we preprocess the Chinese sen-
tences by reordering them.11 Note that not all DEs
in the Chinese data are in NPs, therefore not all
DEs are annotated with the extra labels. Table
6 lists the statistics of the DE classes in the MT
training data.
</bodyText>
<table confidence="0.994667625">
class of {(DE) counts percentage
{AB 112,099 23.55%
{AprepB 2,426 0.51%
{AsB 3,430 0.72%
{BprepA 248,862 52.28%
{relc 95,134 19.99%
{ (unlabeled) 14,056 2.95%
total number of { 476,007 100%
</table>
<tableCaption confidence="0.960057">
Table 6: The number of different DE classes labeled for the
MT training data.
</tableCaption>
<bodyText confidence="0.998831">
After this preprocessing, we restart the whole
MT pipeline – align the preprocessed data, extract
phrases, run MERT and evaluate. This setting is
referred to as DE-Annotated in Table 5.
</bodyText>
<subsectionHeader confidence="0.998698">
4.4 Hierarchical Phrase Reordering Model
</subsectionHeader>
<bodyText confidence="0.999546">
To demonstrate that the technique presented here
is effective even with a hierarchical decoder, we
</bodyText>
<footnote confidence="0.9992638">
10The DE classifier used to annotate the MT experiment
was trained on all the available data described in Section 2.2.
11Reordering is applied on DNP and CP for reasons de-
scribed in Wang et al. (2007). We reorder only when the {
is labeled as {BprepA or {relc.
</footnote>
<page confidence="0.984567">
220
</page>
<table confidence="0.9810015">
BLEU
MT06(tune) MT02 MT03 MT05
BASELINE 32.39 32.51 32.75 31.42
WANG-NP 32.75( +0.36) 32.66( +0.15) 33.15( +0.40) 31.68( +0.26)
DE-Annotated 33.39( +1.00) 33.75( +1.24) 33.63( +0.88) 32.91( +1.49)
BASELINE+Hier 32.96 33.10 32.93 32.23
DE-Annotated+Hier 33.96( +1.00) 34.33( +1.23) 33.88( +0.95) 33.01( +0.77)
Translation Error Rate (TER)
MT06(tune) MT02 MT03 MT05
BASELINE 61.10 63.11 62.09 64.06
WANG-NP 59.78( −1.32) 62.58( −0.53) 61.36(−0.73) 62.35( −1.71)
DE-Annotated 58.21( −2.89) 61.17( −1.94) 60.27(−1.82) 60.78( −3.28)
</table>
<tableCaption confidence="0.999344">
Table 5: MT experiments of different settings on various NIST MT evaluation datasets. We used both the BLEU and TER
metrics for evaluation. All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the
approximate randomization test in (Riezler and Maxwell, 2005)
</tableCaption>
<bodyText confidence="0.999912066666667">
conduct additional experiments with a hierarchi-
cal phrase reordering model introduced by Galley
and Manning (2008). The hierarchical phrase re-
ordering model can handle the key examples of-
ten used to motivated syntax-based systems; there-
fore we think it is valuable to see if the DE an-
notation can still improve on top of that. In Ta-
ble 5, BASELINE+Hier gives consistent BLEU im-
provement over BASELINE. Using DE annotation
on top of the hierarchical phrase reordering mod-
els (DE-Annotated+Hier) provides extra gain over
BASELINE+Hier. This shows the DE annotation can
help a hierarchical system. We think similar im-
provements are likely to occur with other hierar-
chical systems.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="method">
5 Analysis
</sectionHeader>
<subsectionHeader confidence="0.99609">
5.1 Statistics on the Preprocessed Data
</subsectionHeader>
<bodyText confidence="0.999991">
Since our approach DE-Annotated and one of the
baselines (WANG-NP) are both preprocessing Chi-
nese sentences, knowing what percentage of the
sentences are altered will be one useful indicator
of how different the systems are from the baseline.
In our test sets, MT02 has 591 out of 878 sentences
(67.3%) that have DEs under NPs; for MT03 it is
619 out of 919 sentences (67.4%); for MT05 it is
746 out of 1082 sentences (68.9%). This shows
that our preprocessing affects the majority of the
sentences and thus it is not surprising that prepro-
cessing based on the DE construction can make a
significant difference.
</bodyText>
<sectionHeader confidence="0.6619625" genericHeader="method">
5.2 Example: how DE annotation affects
translation
</sectionHeader>
<bodyText confidence="0.999963571428571">
Our approach DE-Annotated reorders the Chinese
sentence, which is similar to the approach pro-
posed by Wang et al. (2007) (WANG-NP). How-
ever, our focus is on the annotation on DEs and
how this can improve translation quality. Table 7
shows an example that contains a DE construction
that translates into a relative clause in English.12
The automatic parse tree of the sentence is listed
in Figure 3. The reordered sentences of WANG-NP
and DE-Annotated appear on the top and bottom in
Figure 4. For this example, both systems decide
to reorder, but DE-Annotated had the extra informa-
tion that this { is a {relc. In Figure 4 we can
see that in WANG-NP, “{” is being translated as
“for”, and the translation afterwards is not gram-
matically correct. On the other hand, the bottom
of Figure 4 shows that with the DE-Annotated pre-
processing, now “{relc” is translated into “which
was” and well connected with the later translation.
This shows that disambiguating { helps in choos-
ing a better English translation.
</bodyText>
<figure confidence="0.944721636363636">
(IP
(NP (NN ��a))
(VP
(ADVP (AD))
(VP (VV *P)P) )
(IP
(VP (VV VIA)
(NP
(QP (CD -)
(CLP (M �)))
(CP
(IP
(VP (VV I)
(NP
(NP (NN I.z)
(CC fp)
(NN 2c) (NN ��))
(ADJP (JJ W))
(NP (NN Mé)))))
(DEC {))
(NP (NN AND (NN Li1C*) (NN *7)))))))
(PU -))
</figure>
<figureCaption confidence="0.999956">
Figure 3: The parse tree of the Chinese sentence in Table 7.
</figureCaption>
<bodyText confidence="0.888334">
12In this example, all four references agreed on the relative
clause translation. Sometimes DE constructions have multi-
ple appropriate translations, which is one of the reasons why
certain classes are more confusable in Table 4.
</bodyText>
<page confidence="0.992022">
221
</page>
<bodyText confidence="0.922411">
Chinese lE - A 0 *P)P) VIA [ iff Iz R �X 5th 9�1 &amp;n1A n [AR i!k* )- A]B °
biagi had assisted in drafting [an employment reform plan]B [that was strongly opposed by the labor
union and the leftists]A .
biagi had helped in drafting [a labor reform proposal]B [that provoked strong protests from labor unions
and the leftists]A .
biagi once helped drafting [an employment reform scheme]B [that was been strongly opposed by the
trade unions and the left - wing]A .
biagi used to assisted to draft [an employment reform plan]B [which is violently opposed by the trade
union and leftest]A .
</bodyText>
<tableCaption confidence="0.9098245">
Table 7: A Chinese example from MT02 that contains a DE construction that translates into a relative clause in English. The
[]A []B is hand-labeled to indicate the approximate translation alignment between the Chinese sentence and English references.
</tableCaption>
<figureCaption confidence="0.9569625">
Figure 4: The top translation is from WANG-NP of the Chinese sentence in Table 7. The bottom one is from DE-Annotated.
In this example, both systems reordered the NP, but DE-Annotated has an annotation on the n.
</figureCaption>
<figure confidence="0.999194208333333">
MI RA * bit 9M — W
Irl A �
biagi had helped draft a reform plan for employment , which was strongly opposed by trade unions and left - wing
biagi had helped draft
It *M
� �
a reform plan for
gtAl
a4k
61* �Ape PJ
Nla �5A
employment
uJrelc �T_Yk fu AM 3J _T
is strongly opposed by trade unions and left - wing activists .
is T_* R
1E 3 J
Wil rXX
NR UC.V
activists
�
Ref 1
Ref 2
Ref 3
Ref 4
</figure>
<sectionHeader confidence="0.975781" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999984266666667">
In this paper, we presented a classification of Chi-
nese n(DE) constructions in NPs according to
how they are translated into English. We applied
this DE classifier to the Chinese sentences of MT
data, and we also reordered the constructions that
required reordering to better match their English
translations. The MT experiments showed our pre-
processing gave significant BLEU and TER score
gains over the baselines. Based on our classifica-
tion and MT experiments, we found that not only
do we have better rules for deciding what to re-
order, but the syntactic, semantic, and discourse
information that we capture in the Chinese sen-
tence allows us to give hints to the MT system
which allows better translations to be chosen.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999277857142857">
The authors would like to thank Michel Galley
and Daniel Cer for useful discussions and techni-
cal help, and Spence Green for his comments on
an earlier draft of the paper. This work is funded
by a Stanford Graduate Fellowship to the first au-
thor and gift funding from Google for the project
“Translating Chinese Correctly”.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999776947368421">
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 224–232, Columbus, Ohio,
June. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings ofACL, pages 263–270, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In ACL ’05: Proceedings ofACL, pages
531–540, Morristown, NJ, USA. Association for
Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of EMNLP, pages 847–855,
Honolulu, Hawaii, October. Association for Compu-
tational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL-HLT.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank?
In Proceedings ofACL, pages 439–446, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL,
pages 104–111, New York City, USA, June. Associ-
ation for Computational Linguistics.
Jia-ju Mei, Yi-Ming Zheng, Yun-Qi Gao, and Hung-
Xiang Yin. 1984. TongYiCi CiLin. Shanghai: the
Commercial Press.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In ACL.
</reference>
<page confidence="0.967532">
222
</page>
<reference confidence="0.998125428571429">
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57–64, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Anette Rosenbach. 2003. Aspects of iconicity and
economy in the choice between the s-genitive and
the of-genitive in English. Topics in English Lin-
guistics, 43:379–412.
Huihsin Tseng, Dan Jurafsky, and Christopher D. Man-
ning. 2005. Morphological features help pos tag-
ging of unknown words across language varieties.
In Proc. of the Fourth SIGHAN Workshop on Chi-
nese Language Processing.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of EMNLP-
CoNLL, pages 737–745, Prague, Czech Republic,
June. Association for Computational Linguistics.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of Coling 2004,
pages 508–514, Geneva, Switzerland, Aug 23–Aug
27. COLING.
Fei Xia. 2000. The part-of-speech tagging guidelines
for the Penn Chinese Treebank (3.0).
</reference>
<page confidence="0.999154">
223
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.727558">
<title confidence="0.9972">Disambiguating “DE” for Chinese-English Machine Translation</title>
<author confidence="0.876667">D</author>
<affiliation confidence="0.918766">Computer Science Department, Stanford</affiliation>
<address confidence="0.888143">Stanford, CA</address>
<email confidence="0.99481">pichuan,jurafsky,manning@stanford.edu</email>
<abstract confidence="0.996626095238095">constructions involving are ubiquitous in Chinese, and can be translated into English in many different ways. This is a major source of machine translation error, even when syntaxsensitive translation models are used. This paper explores how getting more information about the syntactic, semantic, and discourse context of uses can facilitate producing an appropriate English translation strategy. We describe a finerclassification of constructions in Chinese NPs, construct a corpus of annotated examples, and then train a log-linear classifier, which contains linguistically inspired features. We use the DE classifier to preprocess MT data by explicitly constructions, as well as reordering phrases, and show that our approach provides BLEU point gains on MT02 and MT05 on a phrasedbased system. The improvement persists when a hierarchical reordering model is applied.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>224--232</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="22593" citStr="Chang et al., 2008" startWordPosition="3820" endWordPosition="3823">grow-diag heuristic. For features, we incorporate Moses’ standard eight features as well as the lexicalized reordering model. Parameter tuning is done with Minimum Error Rate Training (MERT) (Och, 2003). The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences. We evaluate the result with MT02 (878 sentences), MT03 (919 sentences), and MT05 (1082 sentences). Our MT training corpus contains 1,560,071 sentence pairs from various parallel corpora from LDC.9 There are 12,259,997 words on the English side. Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al., 2008). After segmentation, there are 11,061,792 words on the Chinese side. We use a 5-gram language model trained on the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) and also the English side of all the LDC parallel data permissible under the NIST08 rules. Documents of Gigaword released during the epochs of MT02, MT03, MT05, and MT06 were removed. To run the DE classifier, we also need to parse the Chinese texts. We use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data and the tuning and test sets. 9LDC2003E07, LDC2003E14, LDC2005E</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1850" citStr="Chiang, 2005" startWordPosition="275" endWordPosition="276">tion (MT) from Chinese to English has been a difficult problem: structural differences between Chinese and English, such as the different orderings of head nouns and relative clauses, cause BLEU scores to be consistently lower than for other difficult language pairs like Arabic-English. Many of these structural differences are related to the ubiquitous Chinese n, (DE) construction, used for a wide range of noun modification constructions (both single word and clausal) and other uses. Part of the solution to dealing with these ordering issues is hierarchical decoding, such as the Hiero system (Chiang, 2005), a method motivated by n, (DE) examples like the one in Figure 1. In this case, the translation goal is to rotate the noun head and the preceding relative clause around n, (DE), so that we can translate to “[one of few countries] n, [have diplomatic relations with North Korea]”. Hiero can learn this kind of lexicalized synchronous grammar rule. But use of hierarchical decoders has not solved the DE construction translation problem. We analyzed the errors of three state-of-the-art systems (the 3 DARPA GALE phase 2 teams’ systems), and even though all three use some kind of hierarchical system,</context>
<context position="4699" citStr="Chiang, 2005" startWordPosition="763" endWordPosition="764">d the rich structural cues to the translation. Wang et al. (2007), for example, characterized n, (DE) into only two Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 215–223, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 215 �� � �� � �� { �� ��Aozhou shi yu Beihan you bangjiao DE shaoshu guojia zhiyi . Australia is with North Korea have diplomatic relations that few countries one of . &apos;Australia is one of the few countries that have diplomatic relations with North Korea.&apos; Figure 1: An example of the DE construction from (Chiang, 2005) classes. But our investigation shows that there are many strategies for translating Chinese [A { B] phrases into English, including the patterns in Table 1, only some involving reversal. Notice that the presence of reordering is only one part of the rich structure of these examples. Some reorderings are relative clauses, while others involve prepositional phrases, but not all prepositional phrase uses involve reorderings. These examples suggest that capturing finer-grained translation patterns could help achieve higher accuracy both in reordering and in lexical choice. In this work, we propos</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings ofACL, pages 263–270, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings ofACL,</booktitle>
<pages>531--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In ACL ’05: Proceedings ofACL, pages 531–540, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>847--855</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="26333" citStr="Galley and Manning (2008)" startWordPosition="4432" endWordPosition="4435">ation Error Rate (TER) MT06(tune) MT02 MT03 MT05 BASELINE 61.10 63.11 62.09 64.06 WANG-NP 59.78( −1.32) 62.58( −0.53) 61.36(−0.73) 62.35( −1.71) DE-Annotated 58.21( −2.89) 61.17( −1.94) 60.27(−1.82) 60.78( −3.28) Table 5: MT experiments of different settings on various NIST MT evaluation datasets. We used both the BLEU and TER metrics for evaluation. All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005) conduct additional experiments with a hierarchical phrase reordering model introduced by Galley and Manning (2008). The hierarchical phrase reordering model can handle the key examples often used to motivated syntax-based systems; therefore we think it is valuable to see if the DE annotation can still improve on top of that. In Table 5, BASELINE+Hier gives consistent BLEU improvement over BASELINE. Using DE annotation on top of the hierarchical phrase reordering models (DE-Annotated+Hier) provides extra gain over BASELINE+Hier. This shows the DE annotation can help a hierarchical system. We think similar improvements are likely to occur with other hierarchical systems. 5 Analysis 5.1 Statistics on the Pre</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of EMNLP, pages 847–855, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="21807" citStr="Koehn et al., 2003" startWordPosition="3694" endWordPosition="3697">p. B B prep. A rel. clause A ’s B 168 36 0 110 0 AB 48 2473 73 227 216 A prep. B 0 18 46 23 11 B prep. A 239 691 95 5915 852 rel. clause 0 247 26 630 2266 Total 455 3465 240 6905 3345 Accuracy(%) 36.92 71.37 19.17 85.66 67.74 Table 4: The confusion matrix for 5-class DE classification This could be due to the fact that there are some cases where the translation is correct both ways, but also could be because the features we added have not captured the difference well enough. 4 Machine Translation Experiments 4.1 Experimental Setting For our MT experiments, we used a reimplementation of Moses (Koehn et al., 2003), a state-of-the-art phrase-based system. The alignment is done by the Berkeley word aligner (Liang et al., 2006) and then we symmetrized the word alignment using the grow-diag heuristic. For features, we incorporate Moses’ standard eight features as well as the lexicalized reordering model. Parameter tuning is done with Minimum Error Rate Training (MERT) (Och, 2003). The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences. We evaluate the result with MT02 (878 sentences), MT03 (919 sentences), and MT05 (1082 sentences). Our MT training corpus contains 1,560,071 senten</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher Manning</author>
</authors>
<title>Is it harder to parse Chinese, or the Chinese treebank?</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>439--446</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="23079" citStr="Levy and Manning, 2003" startWordPosition="3904" endWordPosition="3907"> There are 12,259,997 words on the English side. Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al., 2008). After segmentation, there are 11,061,792 words on the Chinese side. We use a 5-gram language model trained on the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) and also the English side of all the LDC parallel data permissible under the NIST08 rules. Documents of Gigaword released during the epochs of MT02, MT03, MT05, and MT06 were removed. To run the DE classifier, we also need to parse the Chinese texts. We use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data and the tuning and test sets. 9LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E85, LDC2006E85, LDC2005T34, and LDC2005T34 4.2 Baseline Experiments We have two different settings as baseline experiments. The first is without reordering or DE annotation on the Chinese side; we simply align the parallel texts, extract phrases and tune parameters. This experiment is referred to as BASELINE. Also, we reorder the training data, the tuning and the test sets with the NP rules in (Wang et al., 2007) and compare our results with this</context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>Roger Levy and Christopher Manning. 2003. Is it harder to parse Chinese, or the Chinese treebank? In Proceedings ofACL, pages 439–446, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="21920" citStr="Liang et al., 2006" startWordPosition="3712" endWordPosition="3715">95 5915 852 rel. clause 0 247 26 630 2266 Total 455 3465 240 6905 3345 Accuracy(%) 36.92 71.37 19.17 85.66 67.74 Table 4: The confusion matrix for 5-class DE classification This could be due to the fact that there are some cases where the translation is correct both ways, but also could be because the features we added have not captured the difference well enough. 4 Machine Translation Experiments 4.1 Experimental Setting For our MT experiments, we used a reimplementation of Moses (Koehn et al., 2003), a state-of-the-art phrase-based system. The alignment is done by the Berkeley word aligner (Liang et al., 2006) and then we symmetrized the word alignment using the grow-diag heuristic. For features, we incorporate Moses’ standard eight features as well as the lexicalized reordering model. Parameter tuning is done with Minimum Error Rate Training (MERT) (Och, 2003). The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences. We evaluate the result with MT02 (878 sentences), MT03 (919 sentences), and MT05 (1082 sentences). Our MT training corpus contains 1,560,071 sentence pairs from various parallel corpora from LDC.9 There are 12,259,997 words on the English side. Chinese word se</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL, pages 104–111, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia-ju Mei</author>
<author>Yi-Ming Zheng</author>
<author>Yun-Qi Gao</author>
<author>HungXiang Yin</author>
</authors>
<date>1984</date>
<booktitle>TongYiCi CiLin. Shanghai: the Commercial</booktitle>
<publisher>Press.</publisher>
<contexts>
<context position="19484" citStr="Mei et al., 1984" startWordPosition="3266" endWordPosition="3269">apture its animacy, which is useful in choosing between the s-genitive (the boy’s mother) and the of-genitive (the mother of the boy) in English (Rosenbach, 2003). The third feature is added because many of the cases in the “A preposition B” class have a percentage number in A. We call these sets of features Lexical. Together they provide 2.73% accuracy improvement over the previous setting. SemClass: semantic class of words We also use a Chinese thesaurus, CiLin, to look up the semantic classes of the words in [A n, B] and use them as features. CiLin is a Chinese thesaurus published in 1984 (Mei et al., 1984). CiLin is organized in a conceptual hierarchy with five levels. We use the level-1 tags which includes 12 categories.6 This feature fires when a word we look up has one level-1 tag in CiLin. This kind of feature is referred to as SemClass in Table 2. For the example in Figure 2, two words have a single level1 tag: “JRL”(most) has a level-1 tag K7 and “R _M”(investment) has a level-1 tag H8. “8)” and “XJ&apos;*)” are not listed in CiLin, and “)�” has multiple entries. Therefore, the SemClass features are: (i) before DE: “K”; (ii) after DE: “H” Topicality: re-occurrence of nouns The last feature we </context>
</contexts>
<marker>Mei, Zheng, Gao, Yin, 1984</marker>
<rawString>Jia-ju Mei, Yi-Ming Zheng, Yun-Qi Gao, and HungXiang Yin. 1984. TongYiCi CiLin. Shanghai: the Commercial Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="22176" citStr="Och, 2003" startWordPosition="3754" endWordPosition="3755">h ways, but also could be because the features we added have not captured the difference well enough. 4 Machine Translation Experiments 4.1 Experimental Setting For our MT experiments, we used a reimplementation of Moses (Koehn et al., 2003), a state-of-the-art phrase-based system. The alignment is done by the Berkeley word aligner (Liang et al., 2006) and then we symmetrized the word alignment using the grow-diag heuristic. For features, we incorporate Moses’ standard eight features as well as the lexicalized reordering model. Parameter tuning is done with Minimum Error Rate Training (MERT) (Och, 2003). The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences. We evaluate the result with MT02 (878 sentences), MT03 (919 sentences), and MT05 (1082 sentences). Our MT training corpus contains 1,560,071 sentence pairs from various parallel corpora from LDC.9 There are 12,259,997 words on the English side. Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al., 2008). After segmentation, there are 11,061,792 words on the Chinese side. We use a 5-gram language model trained on the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) and als</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>57--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="26218" citStr="Riezler and Maxwell, 2005" startWordPosition="4416" endWordPosition="4419">SELINE+Hier 32.96 33.10 32.93 32.23 DE-Annotated+Hier 33.96( +1.00) 34.33( +1.23) 33.88( +0.95) 33.01( +0.77) Translation Error Rate (TER) MT06(tune) MT02 MT03 MT05 BASELINE 61.10 63.11 62.09 64.06 WANG-NP 59.78( −1.32) 62.58( −0.53) 61.36(−0.73) 62.35( −1.71) DE-Annotated 58.21( −2.89) 61.17( −1.94) 60.27(−1.82) 60.78( −3.28) Table 5: MT experiments of different settings on various NIST MT evaluation datasets. We used both the BLEU and TER metrics for evaluation. All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005) conduct additional experiments with a hierarchical phrase reordering model introduced by Galley and Manning (2008). The hierarchical phrase reordering model can handle the key examples often used to motivated syntax-based systems; therefore we think it is valuable to see if the DE annotation can still improve on top of that. In Table 5, BASELINE+Hier gives consistent BLEU improvement over BASELINE. Using DE annotation on top of the hierarchical phrase reordering models (DE-Annotated+Hier) provides extra gain over BASELINE+Hier. This shows the DE annotation can help a hierarchical system. We t</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 57–64, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Rosenbach</author>
</authors>
<title>Aspects of iconicity and economy in the choice between the s-genitive and the of-genitive in English. Topics in English Linguistics,</title>
<date>2003</date>
<pages>43--379</pages>
<contexts>
<context position="6017" citStr="Rosenbach, 2003" startWordPosition="968" endWordPosition="969">struction both whether it will reorder in English and which construction it will translate to in English. We suggest that the necessary classificatory features can be extracted from Chinese, rather than English. The {(DE) in Chinese has a unified meaning of ‘noun modification’, and the choice of reordering and construction realization are mainly a consequence of facts of English noun modification. Nevertheless, most of the features that determine the choice of a felicitous translation are available in the Chinese source. Noun modification realization has been widely studied in English (e.g., (Rosenbach, 2003)), and many of the important determinative properties (e.g., topicality, animacy, prototypicality) can be detected working in the source language. We first present some corpus analysis characterizing different DE constructions based on how they get translated into English (Section 2). We then train a classifier to label DEs into the 5 different categories that we define (Section 3). The fine-grained DEs, together with reordering, are then used as input to a statistical MT system (Section 4). We find that classifying DEs into finergrained tokens helps MT performance, usually at least twice as m</context>
<context position="19029" citStr="Rosenbach, 2003" startWordPosition="3185" endWordPosition="3186"> Thirdly, we normalize different forms of “percentage” representation, and add a feature if they exist. This includes words that start with “ ff or ends with the percentage sign “%”. The first two features are inspired by the fact that a noun and its type can help decide “B prep A” versus “A B”. Here we use the suffix of the noun 5The accuracy is worse when we tried using the word identity instead of the suffix. and the NR (proper noun) tag to help capture its animacy, which is useful in choosing between the s-genitive (the boy’s mother) and the of-genitive (the mother of the boy) in English (Rosenbach, 2003). The third feature is added because many of the cases in the “A preposition B” class have a percentage number in A. We call these sets of features Lexical. Together they provide 2.73% accuracy improvement over the previous setting. SemClass: semantic class of words We also use a Chinese thesaurus, CiLin, to look up the semantic classes of the words in [A n, B] and use them as features. CiLin is a Chinese thesaurus published in 1984 (Mei et al., 1984). CiLin is organized in a conceptual hierarchy with five levels. We use the level-1 tags which includes 12 categories.6 This feature fires when a</context>
</contexts>
<marker>Rosenbach, 2003</marker>
<rawString>Anette Rosenbach. 2003. Aspects of iconicity and economy in the choice between the s-genitive and the of-genitive in English. Topics in English Linguistics, 43:379–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Morphological features help pos tagging of unknown words across language varieties.</title>
<date>2005</date>
<booktitle>In Proc. of the Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="17850" citStr="Tseng et al., 2005" startWordPosition="2971" endWordPosition="2974">n A: “b-NR”, “NR-AD”, “ADVA”, “VA-b” • cross-DE POS bigram: “VA-NN” • POS unigram in B: “NN” • POS bigrams in B: “b-NN”, “NN-NN”, “NNb” The part-of-speech ngram features add 4.24% accuracy to the 5-class classifier. Lexical: lexical features In addition to part-of-speech features, we also tried to use features from the words themselves. But since using full word identity resulted in a sparsity issue,5 we take the one-character suffix of each word and extract suffix unigram and bigram features from them. The argument for using suffixes is that it often captures the larger category of the word (Tseng et al., 2005). For example, rp ) (China) and 8) (Korea) share the same suffix ), which means “country”. These suffix ngram features will result in these features for the NP in Figure 2: • suffix unigrams: “)”, “JRL”, “�”, “n,”, “_M”, “)” • suffix bigrams: “b-)”, “)-JRL”, “JRL-)�”, “)�-n,”, “n,-_M”, “_M-)”, “W-b” Other than the suffix ngram, we �Ialso add three other lexical features: first, if the word before DE is a noun, we add a feature that is the conjunction of POS and suffix unigram. Secondly, an “NR only” feature will fire when A only consists of one or more NRs. Thirdly, we normalize different form</context>
</contexts>
<marker>Tseng, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Dan Jurafsky, and Christopher D. Manning. 2005. Morphological features help pos tagging of unknown words across language varieties. In Proc. of the Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL,</booktitle>
<pages>737--745</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3324" citStr="Wang et al., 2007" startWordPosition="526" endWordPosition="529">2: ‘the local a bad reputation secondary school’ Team 3: ‘a local stigma secondary schools’ None of the teams reordered “bad reputation” and “middle school” around the n, . We argue that this is because it is not sufficient to have a formalism which supports phrasal reordering, but it is also necessary to have sufficient linguistic modeling that the system knows when and how much to rearrange. An alternative way of dealing with structural differences is to reorder source language sentences to minimize structural divergence with the target language, (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007). For example Wang et al. (2007) introduced a set of rules to decide if a n, (DE) construction should be reordered or not before translating to English: • For DNPs (consisting of“XP+DEG”): – Reorder if XP is PP or LCP; – Reorder if XP is a non-pronominal NP • For CPs (typically formed by “IP+DEC”): – Reorder to align with the “that+clause” structure of English. Although this and previous reordering work has led to significant improvements, errors still remain. Indeed, Wang et al. (2007) found that the precision of their NP rules is only about 54.6% on a small human-judged set. One possible rea</context>
<context position="6890" citStr="Wang et al., 2007" startWordPosition="1111" endWordPosition="1114">ated into English (Section 2). We then train a classifier to label DEs into the 5 different categories that we define (Section 3). The fine-grained DEs, together with reordering, are then used as input to a statistical MT system (Section 4). We find that classifying DEs into finergrained tokens helps MT performance, usually at least twice as much as just doing phrasal reordering. 2 DE classification The Chinese character DE serves many different purposes. According to the Chinese Treebank tagging guidelines (Xia, 2000), the character can be tagged as DEC, DEG, DEV, SP, DER, or AS. Similar to (Wang et al., 2007), we only consider the majority case when the phrase with {(DE) is a noun phrase modifier. The DEs in NPs have a part-of-speech tag of DEC (a complementizer or a nominalizer) or DEG (a genitive marker or an associative marker). 2.1 Class Definition The way we categorize the DEs is based on their behavior when translated into English. This is implicitly done in the work of Wang et al. (2007) where they use rules to decide if a certain DE and the words next to it will need to be reordered. In this work, we categorize DEs into finer-grained categories. For a Chinese noun phrase [A { B], we catego</context>
<context position="10911" citStr="Wang et al., 2007" startWordPosition="1825" endWordPosition="1828">ons to match, when we extract features for the classifier, we don’t use gold-standard parses. Instead, we use a parser trained on CTB6 excluding files 1-325. We then use this parser to parse the 3253 1The “other” category contains many mixed cases that could be difficult Chinese patterns to translate. We will leave this for future work. 5-class Acc. (%) 2-class Acc. (%) baseline - 76.0 DEPOS 54.8 71.0 +A-pattern 67.9 83.7 +POS-ngram 72.1 84.9 +Lexical 74.9 86.5 +SemClass 75.1 86.7 +Topicality 75.4 86.9 Table 2: 5-class and 2-class classification accuracy. “baseline” is the heuristic rules in (Wang et al., 2007). Others are various features added to the log-linear classifier. Chinese sentences with the DE annotation and extract parse-related features from there. 3.1 Experimental setting For the classification experiment, we exclude the “other” class and only use the 2882 examples that fall into the five pre-defined classes. To evaluate the classification performance and understand what features are useful, we compute the accuracy by averaging five 10-fold cross-validations.2 As a baseline, we use the rules introduced in Wang et al. (2007) to decide if the DEs require reordering or not. However, since</context>
<context position="14410" citStr="Wang et al., 2007" startWordPosition="2338" endWordPosition="2341">(acquire)/n(DE)/k�f5(RMB)/2rA(loan) → “the loans in RMB acquired by foreign-invested enterprises” 5. A preposition B 5.1. VQl� T7 (more than 40 million)/Ac7G(US dollar)/n(DE)/1?`aa(product) → more than 40 million US dollars in products Table 1: Examples for the 5 DE classes to show examples of each feature. The parse structure of the NP is listed in Figure 2. (NP (NP (NR Q)) (CP (IP (VP (ADVP (AD J%)) (VP (VA �k)))) (DEC n)) (NP (NN RR) (NNQ)))))) Figure 2: The parse tree of the Chinese NP. A-pattern: Chinese syntactic patterns appearing before n Secondly, we want to incorporate the rules in (Wang et al., 2007) as features in the log-linear classifier. We added features for certain indicative patterns in the parse tree (listed in Table 3). 1. A is ADJP: true if A+DE is a DNP which is in the form of “ADJP+DEG”. 2. A is QP: true if A+DE is a DNP which is in the form of “QP+DEG”. 3. A is pronoun: true if A+DE is a DNP which is in the form of “NP+DEG”, and the NP is a pronoun. 4. A ends with VA: true if A+DE is a CP which is in the form of “IP+DEC”, and the IP ends with a VP that’s either just a VA or a VP preceded by a ADVP. Table 3: A-pattern features DEPOS: part-of-speech tag of DE Since the part-of-</context>
<context position="15724" citStr="Wang et al., 2007" startWordPosition="2594" endWordPosition="2597">P in Figure 2 will have the feature “DEC”. This basic feature will be referred to as DEPOS. Note that since we are only classifying DEs in NPs, ideally the part-of-speech tag of DE will either be DEC or DEG as described in Section 2. However, since we are using automatic parses instead of gold-standard ones, the DEPOS feature might have other values than just DEC and DEG. From Table 2, we can see that with this simple feature, the 5-class accuracy is low but at least better than simply guessing the majority class (47.92%). The 2-class accuracy is still lower than using the heuristic rules in (Wang et al., 2007), which is reasonable because their rules encode more information than just the POS tags of DEs. Features 1–3 are inspired by the rules in (Wang et al., 2007), and the fourth rule is based on the observation that even though the predicative adjective VA acts as a verb, it actually corresponds to adjectives in English as described in (Xia, 2000).3 We call these four features A-pattern. Our example NP in Figure 2 will have the fourth feature “A ends with VA” in Table 3, but not the other three features. In Table 2 we can see that after adding A-pattern, the 2-class accuracy is already much highe</context>
<context position="20756" citStr="Wang et al., 2007" startWordPosition="3491" endWordPosition="3494">disambiguating s-genitive and of-genitive. We approximate the feature by caching the nouns in the previous two sentences, and fire a topicality feature when the noun appears in the cache. Take this NP in MT06 as an example: “W8 -1:j I8 n, Ai2� f Wk For this NP, all words before DE and after DE appeared in the previous sentence. Therefore the topicality features “cache-before-DE” and “cacheafter-DE” both fire. After all the feature engineering above, the best accuracy on the 5-class classifier we have is 75.4%, which maps into a 2-class accuracy of 86.9%. Comparing the 2-class accuracy to the (Wang et al., 2007) baseline, we have a 10.9% absolute improvement. The 5-class accuracy and confusion matrix is listed in Table 4. “A preposition B” is a small category and is the most confusing. “A ’s B” also has lower accuracy, and is mostly confused with “B preposition A”. 6We also tried adding more levels but it did not help. 7K is the category iia (auxiliary) in CiLin. 8H is the category ÙM (activities) in CiLin. ” 219 real — A ’s B AB A prep. B B prep. A rel. clause A ’s B 168 36 0 110 0 AB 48 2473 73 227 216 A prep. B 0 18 46 23 11 B prep. A 239 691 95 5915 852 rel. clause 0 247 26 630 2266 Total 455 346</context>
<context position="23645" citStr="Wang et al., 2007" startWordPosition="3995" endWordPosition="3998">the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data and the tuning and test sets. 9LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E85, LDC2006E85, LDC2005T34, and LDC2005T34 4.2 Baseline Experiments We have two different settings as baseline experiments. The first is without reordering or DE annotation on the Chinese side; we simply align the parallel texts, extract phrases and tune parameters. This experiment is referred to as BASELINE. Also, we reorder the training data, the tuning and the test sets with the NP rules in (Wang et al., 2007) and compare our results with this second baseline (WANG-NP). The NP reordering preprocessing (WANG-NP) showed consistent improvement in Table 5 on all test sets, with BLEU point gains ranging from 0.15 to 0.40. This confirms that having reordering around DEs in NP helps Chinese-English MT. 4.3 Experiments with 5-class DE annotation We use the best setting of the DE classifier described in Section 3 to annotate DEs in NPs in the MT training data as well as the NIST tuning and test sets.10 If a DE is in an NP, we use the annotation of {AB, {AsB, {BprepA, {relc, or {AprepB to replace the origina</context>
<context position="25328" citStr="Wang et al. (2007)" startWordPosition="4286" endWordPosition="4289"> 476,007 100% Table 6: The number of different DE classes labeled for the MT training data. After this preprocessing, we restart the whole MT pipeline – align the preprocessed data, extract phrases, run MERT and evaluate. This setting is referred to as DE-Annotated in Table 5. 4.4 Hierarchical Phrase Reordering Model To demonstrate that the technique presented here is effective even with a hierarchical decoder, we 10The DE classifier used to annotate the MT experiment was trained on all the available data described in Section 2.2. 11Reordering is applied on DNP and CP for reasons described in Wang et al. (2007). We reorder only when the { is labeled as {BprepA or {relc. 220 BLEU MT06(tune) MT02 MT03 MT05 BASELINE 32.39 32.51 32.75 31.42 WANG-NP 32.75( +0.36) 32.66( +0.15) 33.15( +0.40) 31.68( +0.26) DE-Annotated 33.39( +1.00) 33.75( +1.24) 33.63( +0.88) 32.91( +1.49) BASELINE+Hier 32.96 33.10 32.93 32.23 DE-Annotated+Hier 33.96( +1.00) 34.33( +1.23) 33.88( +0.95) 33.01( +0.77) Translation Error Rate (TER) MT06(tune) MT02 MT03 MT05 BASELINE 61.10 63.11 62.09 64.06 WANG-NP 59.78( −1.32) 62.58( −0.53) 61.36(−0.73) 62.35( −1.71) DE-Annotated 58.21( −2.89) 61.17( −1.94) 60.27(−1.82) 60.78( −3.28) Table 5</context>
<context position="27731" citStr="Wang et al. (2007)" startWordPosition="4665" endWordPosition="4668">l be one useful indicator of how different the systems are from the baseline. In our test sets, MT02 has 591 out of 878 sentences (67.3%) that have DEs under NPs; for MT03 it is 619 out of 919 sentences (67.4%); for MT05 it is 746 out of 1082 sentences (68.9%). This shows that our preprocessing affects the majority of the sentences and thus it is not surprising that preprocessing based on the DE construction can make a significant difference. 5.2 Example: how DE annotation affects translation Our approach DE-Annotated reorders the Chinese sentence, which is similar to the approach proposed by Wang et al. (2007) (WANG-NP). However, our focus is on the annotation on DEs and how this can improve translation quality. Table 7 shows an example that contains a DE construction that translates into a relative clause in English.12 The automatic parse tree of the sentence is listed in Figure 3. The reordered sentences of WANG-NP and DE-Annotated appear on the top and bottom in Figure 4. For this example, both systems decide to reorder, but DE-Annotated had the extra information that this { is a {relc. In Figure 4 we can see that in WANG-NP, “{” is being translated as “for”, and the translation afterwards is no</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of EMNLPCoNLL, pages 737–745, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>508--514</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="3282" citStr="Xia and McCord, 2004" startWordPosition="518" endWordPosition="521">utation of the local secondary school’ Team 2: ‘the local a bad reputation secondary school’ Team 3: ‘a local stigma secondary schools’ None of the teams reordered “bad reputation” and “middle school” around the n, . We argue that this is because it is not sufficient to have a formalism which supports phrasal reordering, but it is also necessary to have sufficient linguistic modeling that the system knows when and how much to rearrange. An alternative way of dealing with structural differences is to reorder source language sentences to minimize structural divergence with the target language, (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007). For example Wang et al. (2007) introduced a set of rules to decide if a n, (DE) construction should be reordered or not before translating to English: • For DNPs (consisting of“XP+DEG”): – Reorder if XP is PP or LCP; – Reorder if XP is a non-pronominal NP • For CPs (typically formed by “IP+DEC”): – Reorder to align with the “that+clause” structure of English. Although this and previous reordering work has led to significant improvements, errors still remain. Indeed, Wang et al. (2007) found that the precision of their NP rules is only about 54.6% on </context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In Proceedings of Coling 2004, pages 508–514, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>The part-of-speech tagging guidelines for the Penn Chinese Treebank (3.0).</title>
<date>2000</date>
<contexts>
<context position="6796" citStr="Xia, 2000" startWordPosition="1093" endWordPosition="1094">corpus analysis characterizing different DE constructions based on how they get translated into English (Section 2). We then train a classifier to label DEs into the 5 different categories that we define (Section 3). The fine-grained DEs, together with reordering, are then used as input to a statistical MT system (Section 4). We find that classifying DEs into finergrained tokens helps MT performance, usually at least twice as much as just doing phrasal reordering. 2 DE classification The Chinese character DE serves many different purposes. According to the Chinese Treebank tagging guidelines (Xia, 2000), the character can be tagged as DEC, DEG, DEV, SP, DER, or AS. Similar to (Wang et al., 2007), we only consider the majority case when the phrase with {(DE) is a noun phrase modifier. The DEs in NPs have a part-of-speech tag of DEC (a complementizer or a nominalizer) or DEG (a genitive marker or an associative marker). 2.1 Class Definition The way we categorize the DEs is based on their behavior when translated into English. This is implicitly done in the work of Wang et al. (2007) where they use rules to decide if a certain DE and the words next to it will need to be reordered. In this work,</context>
<context position="16070" citStr="Xia, 2000" startWordPosition="2659" endWordPosition="2660">an just DEC and DEG. From Table 2, we can see that with this simple feature, the 5-class accuracy is low but at least better than simply guessing the majority class (47.92%). The 2-class accuracy is still lower than using the heuristic rules in (Wang et al., 2007), which is reasonable because their rules encode more information than just the POS tags of DEs. Features 1–3 are inspired by the rules in (Wang et al., 2007), and the fourth rule is based on the observation that even though the predicative adjective VA acts as a verb, it actually corresponds to adjectives in English as described in (Xia, 2000).3 We call these four features A-pattern. Our example NP in Figure 2 will have the fourth feature “A ends with VA” in Table 3, but not the other three features. In Table 2 we can see that after adding A-pattern, the 2-class accuracy is already much higher than the baseline. We attribute this to the fourth rule and also to the fact that the classifier can learn weights for each feature.4 3Quote from (Xia, 2000): “VA roughly corresponds to adjectives in English and stative verbs in the literature on Chinese grammar.” 4We also tried extending a rule-based 2-class classifier with the fourth rule. </context>
</contexts>
<marker>Xia, 2000</marker>
<rawString>Fei Xia. 2000. The part-of-speech tagging guidelines for the Penn Chinese Treebank (3.0).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>