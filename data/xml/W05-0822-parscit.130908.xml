<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011350">
<title confidence="0.95314">
PORTAGE: A Phrase-based Machine Translation System
</title>
<author confidence="0.800772">
Fatiha Sadat+, Howard Johnson++, Akakpo Agbago+, George Foster+,
</author>
<note confidence="0.417256">
+++ *
</note>
<author confidence="0.967319">
Roland Kuhn, Joel Martinand Aaron Tikuisis
</author>
<affiliation confidence="0.981197">
+ NRC Institute for Information ++ NRC Institute for Information *University of Waterloo
Technology Technology 200 University Avenue W.,
</affiliation>
<address confidence="0.9688555">
101 St-Jean-Bosco Street 1200 Montreal Road Waterloo, Ontario, Canada
Gatineau, QC K1A 0R6, Canada Ottawa, ON K1A 0R6, Canada
</address>
<email confidence="0.995578">
firstname.lastname@cnrc-nrc.gc.ca aptikuis@uwaterloo.ca
</email>
<sectionHeader confidence="0.998563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999897928571429">
This paper describes the participation of
the Portage team at NRC Canada in the
shared task1 of ACL 2005 Workshop on
Building and Using Parallel Texts. We dis-
cuss Portage, a statistical phrase-based
machine translation system, and present
experimental results on the four language
pairs of the shared task. First, we focus on
the French-English task using multiple re-
sources and techniques. Then we describe
our contribution on the Finnish-English,
Spanish-English and German-English lan-
guage pairs using the provided data for the
shared task.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999325214285714">
The rapid growth of the Internet has led to a rapid
growth in the need for information exchange among
different languages. Machine Translation (MT) and
related technologies have become essential to the
information flow between speakers of different lan-
guages on the Internet. Statistical Machine Transla-
tion (SMT), a data-driven approach to producing
translation systems, is becoming a practical solution
to the longstanding goal of cheap natural language
processing.
In this paper, we describe Portage, a statistical
phrase-based machine translation system, which we
evaluated on all different language pairs that were
provided for the shared task. As Portage is a very
</bodyText>
<footnote confidence="0.775117">
1 http://www.statmt.org/wpt05/mt-shared-task/
</footnote>
<bodyText confidence="0.999481384615384">
new system, our main goal in participating in the
workshop was to test it out on different language
pairs, and to establish baseline performance for the
purpose of comparison against other systems and
against future improvements. To do this, we used a
fairly standard configuration for phrase-based SMT,
described in the next section.
Of the language pairs in the shared task, French-
English is particularly interesting to us in light of
Canada’s demographics and policy of official bilin-
gualism. We therefore divided our participation into
two parts: one stream for French-English and an-
other for Finnish-, German-, and Spanish-English.
For the French-English stream, we tested the use of
additional data resources along with hand-coded
rules for translating numbers and dates. For the
other streams, we used only the provided resources
in a purely statistical framework (although we also
investigated several automatic methods of coping
with Finnish morphology).
The remainder of the paper is organized as fol-
lows. Section 2 describes the architecture of the
Portage system, including its hand-coded rules for
French-English. Experimental results for the four
pairs of languages are reported in Section 3. Section
4 concludes and gives pointers to future work.
</bodyText>
<sectionHeader confidence="0.986238" genericHeader="introduction">
2 Portage
</sectionHeader>
<bodyText confidence="0.999897285714286">
Portage operates in three main phases: preprocess-
ing of raw data into tokens, with translation sugges-
tions for some words or phrases generated by rules;
decoding to produce one or more translation hy-
potheses; and error-driven rescoring to choose the
best final hypothesis. (A fourth postprocessing
phase was not needed for the shared task.)
</bodyText>
<page confidence="0.97686">
129
</page>
<note confidence="0.884505">
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 129–132,
Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<subsectionHeader confidence="0.990407">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999981966666667">
Preprocessing is a necessary first step in order to
convert raw texts in both source and target lan-
guages into a format suitable for both model train-
ing and decoding (Foster et al., 2003). For the
supplied Europarl corpora, we relied on the existing
segmentation and tokenization, except for French,
which we manipulated slightly to bring into line
with our existing conventions (e.g., converting l ’
an into l’ an). For the Hansard corpus used to
supplement our French-English resources (de-
scribed in section 3 below), we used our own
alignment based on Moore’s algorithm (Moore,
2002), segmentation, and tokenization procedures.
Languages with rich morphology are often prob-
lematic for statistical machine translation because
the available data lacks instances of all possible
forms of a word to efficiently train a translation sys-
tem. In a language like German, new words can be
formed by compounding (writing two or more
words together without a space or a hyphen in be-
tween). Segmentation is a crucial step in preproc-
essing languages such as German and Finnish texts.
In addition to these simple operations, we also
developed a rule-based component to detect num-
bers and dates in the source text and identify their
translation in the target text. This component was
developed on the Hansard corpus, and applied to the
French-English texts (i.e. Europarl and Hansard), on
the development data in both languages, and on the
test data.
</bodyText>
<subsectionHeader confidence="0.998565">
2.2 Decoding
</subsectionHeader>
<bodyText confidence="0.999985615384615">
Decoding is the central phase in SMT, involving a
search for the hypotheses t that have highest prob-
abilities of being translations of the current source
sentence s according to a model for P(t|s). Our
model for P(t|s) is a log-linear combination of four
main components: one or more trigram language
models, one or more phrase translation models, a
distortion model, and a word-length feature. The
trigram language model is implemented in the
SRILM toolkit (Stolcke, 2002). The phrase-based
translation model is similar to the one described in
(Koehn, 2004), and relies on symmetrized IBM
model 2 word-alignments for phrase pair induction.
The distortion model is also very similar to
Koehn’s, with the exception of a final cost to ac-
count for sentence endings.
To set weights on the components of the log-
linear model, we implemented Och’s algorithm
(Och, 2003). This essentially involves generating,
in an iterative process, a set of nbest translation hy-
potheses that are representative of the entire search
space for a given set of source sentences. Once this
is accomplished, a variant of Powell’s algorithm is
used to find weights that optimize BLEU score
(Papineni et al, 2002) over these hypotheses, com-
pared to reference translations. Unfortunately, our
implementation of this algorithm converged only
very slowly to a satisfactory final nbest list, so we
used two different ad hoc strategies for setting
weights: choosing the best values encountered dur-
ing the iterations of Och’s algorithm (French-
English), and a grid search (all other languages).
To perform the actual translation, we used our
decoder, Canoe, which implements a dynamic-
programming beam search algorithm based on that
of Pharaoh (Koehn, 2004). Canoe is input-output
compatible with Pharaoh , with the exception of a
few extensions such as the ability to decode either
backwards or forwards.
</bodyText>
<subsectionHeader confidence="0.992823">
2.3 Rescoring
</subsectionHeader>
<bodyText confidence="0.999997875">
To improve raw output from Canoe, we used a
rescoring strategy: have Canoe generate a list of
nbest translations rather than just one, then reorder
the list using a model trained with Och’s method to
optimize BLEU score. This is identical to the final
pass of the algorithm described in the previous sec-
tion, except for the use of a more powerful log-
linear model than would have been feasible to use
inside the decoder. In addition to the four basic fea-
tures of the initial model, our rescoring model in-
cluded IBM2 model probabilities in both directions
(i.e., P(s|t) and P(t|s)); and an IBM1-based feature
designed to detect whether any words in one lan-
guage seemed to be left without satisfactory transla-
tions in the other language. This missing-word
feature was also applied in both directions.
</bodyText>
<sectionHeader confidence="0.990767" genericHeader="method">
3 Experiments on the Shared Task
</sectionHeader>
<bodyText confidence="0.993856833333333">
We conducted experiments and evaluations on
Portage using the different language pairs of the
shared task. The training data was provided for the
shared task as follows:
- Training data of 688,031 sentences in
French and English. A similarly sized cor-
</bodyText>
<page confidence="0.985824">
130
</page>
<bodyText confidence="0.979286375">
pus is provided for Finnish, Spanish and
German with matched English translations.
- Development test data of 2,000 sentences in
the four languages.
In addition to the provided data, a set of
6,056,014 sentences extracted from Hansard corpus,
the official record of Canada’s parliamentary de-
bates, was used in both French and English lan-
</bodyText>
<table confidence="0.9994982">
D ecoding Decoding+Rescoring
27.71 29.22
28.71 29.53
26.45 28.21
28.29 28.56
</table>
<tableCaption confidence="0.83099">
Table 1. BLEU scores for the French-English test
sentences
</tableCaption>
<figure confidence="0.9805074">
Method
E
E-H
E-p
E-H-p
</figure>
<figureCaption confidence="0.308393">
guages. This c orpus was used to generate both
</figureCaption>
<bodyText confidence="0.9705985">
language and translation models for use in decoding
and rescoring.
The development test data was split into two
parts: The first part that includes 1,000 sentences in
each language with reference translations into Eng-
lish served in the optimization of weights for both
the decoding and rescoring models. In this study,
number of n-best lists was set to 1,000. The second
part, which includes 1,000 sentences in each lan-
guage with referenc e translations into English, was
used in the evaluation of the performance of the
translation models.
</bodyText>
<subsectionHeader confidence="0.994362">
3.1 Experiments on the French-English Task
</subsectionHeader>
<bodyText confidence="0.996617">
Our goal for this language pair was to conduct ex-
periments on Portage for a comparative study ex-
ploiting and combining different resources and
techniques:
</bodyText>
<listItem confidence="0.961332">
1. Method E is based on the Europarl corpus
as training data,
2. Method E-H is based on both Europarl and
Hansard corpora as training data,
3. Method E-p is based on the Europarl corpus
as training data and parsing numbers and
dates in the preprocessing phase,
4. Method E-H-p is based on both Europarl
and Hansard corpora as training data and
parsing numbers and date in the preprocess-
ing phase.
</listItem>
<bodyText confidence="0.993898096774193">
Results are shown in Table 1 for the French-
English task. The first column of Table 1 indicates
the method, the second column gives results for
decoding with Canoe only, and the third column for
decoding and rescoring with Canoe. For comparison
between the four methods, there was an improve-
ment in terms of BLEU scores when using two lan-
guage models and two translation models generated
from Europarl and Hansard corpora; however, pars-
ing numbers and dates had a negative impact on the
t ranslation models. The b BLEU score for our
est
participation at the French-English task was 29.53.
A noteworthy feature of these results is that the
improvement given by the out-of-domain Hansard
corpus was very slight. Although we suspect that
somewhat better performance could have been
achieved by better weight optimization, this result
clearly underscores the importance of matching
training and test domains. A related point is that our
number and date translation rules actually caused a
performance drop due to the fact that they were op-
timized for typographical conventions prevalent in
Hansard, which are quite different from those used
in Europarl.
Our best result ranked third in the shared
WPT05 French-English task , with a difference of
0.74 in terms of BLEU score from the first ranked
participant, and a difference of 0.67 in terms o
f
BLEU score from the second ranked participant.
</bodyText>
<subsectionHeader confidence="0.999424">
3.2 Experiments on other Pairs of Languages
</subsectionHeader>
<bodyText confidence="0.999942909090909">
The WPT05 workshop provides a good opportunity
to achieve our benchmarking goals with corpora
that provide challenging difficulties. German and
Finnish are languages that make considerable use of
compounding. Finnish, in addition, has a particu-
larly complex morphology that is organized on
principles that are quite different from any in Eng-
lish. This results in much longer word forms each of
which occurs very infrequently.
Our original intent was to propose a number of pos-
sible statistical approaches to analyzing and split-
ting these word forms and improving our results.
Since none of these yielded results as good as the
baseline, we will continue this work until we under-
stand what is really needed. We also care very
much about translating between French and English
in Canada and plan to spend a lot of extra effort on
difficulties that occur in this case. Translation be-
tween Spanish and English is also becoming more
important as a result of increased trade within North
America but also functions as a good counterpoint
for French-English.
</bodyText>
<page confidence="0.993461">
131
</page>
<table confidence="0.9929128">
Language Pair
Decoding+Rescoring
Andreas Stolcke. 2002. SRILM - an Extensible Language
Modeling Toolkit. In ICSLP-2002, 901-904.
Finnish-English
German-English
Spanish English
20.95 Franz Josef Och, Hermann Ney. 2000. Improved Statisti-
23.21 cal Alignment Models. In Proceedings of the 38th An-
29.08 nual Meeting of the Association for Computational
</table>
<tableCaption confidence="0.9753995">
Table 2 BLEU scores for the Finnish-English, Ger-
man-English and Spanish-English test sentences
</tableCaption>
<bodyText confidence="0.999827133333333">
To establish our baseline, the only preprocessing
we did was lowercasing (using the provided tokeni-
zation). Canoe was run without any special settings,
although weights for distortion, word penalty, lan-
guage model, and translation model were optimized
using a grid search, as described above. Rescoring
was also done, and usually resulted in at least an
extra BLEU point.
Our final results are shown in Table 2. Ranks at
the shared WPT05 Finnish-, German-, and Spanish-
English tasks were assigned as second, third and
fourth, with differences of 1.06, 1.87 and 1.56 in
te
rms of BLEU scores, respectively, compared to
the first ranked participant.
</bodyText>
<sectionHeader confidence="0.999533" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99999484">
We have reported on our participation in the shared
task of the ACL 2005 Workshop on Building and
Using Parallel Texts, conducting evaluations of
Portage, our statistical machine translation system,
on all four language pairs. Our best BLEU scores
for the French-, Finnish-, German-, and Spanish-
English at this stage were 29.5, 20.95, 23.21 and
29.08, respectively. In total, eleven teams took part
at the shared task and most of them submitted re-
sults for all pairs of languages. Our results distin-
guished the NRC team at the third, second, third
and fourth ranks with slight differences with the
first ranked participants.
A major goal of this work was to evaluate Port-
age at its first stage of implementation on different
pairs of languages. This evaluation has served to
identify some problems with our system in the areas
of weight optimization and number and date rules.
It has also indicated the limits of using out-of-
domain corpora, and the difficulty of morphologi-
cally complex languages like Finnish.
Current and planned future work includes the
exploitation of comparable corpora for statistical
machine transl ation, greater use of morphological
knowledge, and better features for nbest rescoring.
</bodyText>
<sectionHeader confidence="0.998151" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999490145833334">
Linguistics, Hong Kong, China, October 2000, 440-
447.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, Dragomir Radev. 2004. A Smor-
gasbord of Features for Statistical Machine Transla-
tion. In Proceeding of the HLT/NAACL 2004,
Boston, MA, May 2004.
orge Foster, Simona Gandrabur, Philippe Langlais,
Pierre Plamondon, Graham Russell and Michel Si-
mard. 2003. Statistical Machine Translation: Rap
Development with Limited Resources. In Proceedings
of MT Summit IX 2003, New Orleans, September.
Kevin Knight, Ishwar Chander, Matthew Haines, Va-
sileios Hatzivassiloglou, Eduard Hovy, Masayo Iida,
Steve K. Luk, Richard Whitney, and Kenji Yamada.
1995. Filling Knowledge Gaps in a Broad-Coverage
MT System. In Proceedings of the International Joint
Conference on Artificial Intelligence (IJCAI), 1995.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics ACL, Philadelphia, July 2002,
pp. 311-318.
Moore, Robert. 2002. Fast and Accurate Sentence
Alignment of Bilingual Corpora. In Machine Transla-
tion: From Research to Real Users (Proceedings of the
5th Conference of the Association for Machine Trans-
lation in the Americas, Tiburon, California), Springer-
Verlag, Heidelberg, Germany, pp. 135-244.
Och, F. J. and H. Ney. 2002. Discriminative Training
and Maximum Entropy Models for Statistical Machine
Translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
Philadelphia, pp. 295–302.
Franz Josef Och, 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, Sapporo, July.
Philipp Koehn. 2002. Europarl: A multilingual corpusfor
evaluation of machine translation. Ms., University of
Southern California.
Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder
for Phrase-based Statistical Machine Translation
Models. In Proceedings of the Association for Ma-
chine Translation in the Americas AMTA 2004.
</reference>
<figure confidence="0.911988">
Ge
id
</figure>
<page confidence="0.928933">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.338466">
<title confidence="0.9999">PORTAGE: A Phrase-based Machine Translation System</title>
<author confidence="0.781427666666667">Howard Akakpo George Roland Kuhn</author>
<author confidence="0.781427666666667">Joel Martinand Aaron Tikuisis</author>
<affiliation confidence="0.9399065">Institute for Information Institute for Information of Waterloo Technology Technology 200 University Avenue W.,</affiliation>
<address confidence="0.9966365">101 St-Jean-Bosco Street 1200 Montreal Road Waterloo, Ontario, Canada Gatineau, QC K1A 0R6, Canada Ottawa, ON K1A 0R6, Canada</address>
<email confidence="0.983535">firstname.lastname@cnrc-nrc.gc.caaptikuis@uwaterloo.ca</email>
<abstract confidence="0.998677533333333">This paper describes the participation of the Portage team at NRC Canada in the of ACL 2005 Workshop on Building and Using Parallel Texts. We discuss Portage, a statistical phrase-based machine translation system, and present experimental results on the four language pairs of the shared task. First, we focus on the French-English task using multiple resources and techniques. Then we describe our contribution on the Finnish-English, Spanish-English and German-English language pairs using the provided data for the shared task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hong Kong Linguistics</author>
<author>China</author>
</authors>
<date>2000</date>
<pages>440--447</pages>
<marker>Linguistics, China, 2000</marker>
<rawString>Linguistics, Hong Kong, China, October 2000, 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In Proceeding of the HLT/NAACL 2004,</booktitle>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, Dragomir Radev.</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, Dragomir Radev. 2004. A Smorgasbord of Features for Statistical Machine Translation. In Proceeding of the HLT/NAACL 2004, Boston, MA, May 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>orge Foster</author>
<author>Simona Gandrabur</author>
<author>Philippe Langlais</author>
<author>Pierre Plamondon</author>
<author>Graham Russell</author>
<author>Michel Simard</author>
</authors>
<title>Statistical Machine Translation: Rap Development with Limited Resources.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IX 2003,</booktitle>
<location>New Orleans,</location>
<contexts>
<context position="3748" citStr="Foster et al., 2003" startWordPosition="557" endWordPosition="560">slation suggestions for some words or phrases generated by rules; decoding to produce one or more translation hypotheses; and error-driven rescoring to choose the best final hypothesis. (A fourth postprocessing phase was not needed for the shared task.) 129 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 129–132, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005 2.1 Preprocessing Preprocessing is a necessary first step in order to convert raw texts in both source and target languages into a format suitable for both model training and decoding (Foster et al., 2003). For the supplied Europarl corpora, we relied on the existing segmentation and tokenization, except for French, which we manipulated slightly to bring into line with our existing conventions (e.g., converting l ’ an into l’ an). For the Hansard corpus used to supplement our French-English resources (described in section 3 below), we used our own alignment based on Moore’s algorithm (Moore, 2002), segmentation, and tokenization procedures. Languages with rich morphology are often problematic for statistical machine translation because the available data lacks instances of all possible forms of</context>
</contexts>
<marker>Foster, Gandrabur, Langlais, Plamondon, Russell, Simard, 2003</marker>
<rawString>orge Foster, Simona Gandrabur, Philippe Langlais, Pierre Plamondon, Graham Russell and Michel Simard. 2003. Statistical Machine Translation: Rap Development with Limited Resources. In Proceedings of MT Summit IX 2003, New Orleans, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Ishwar Chander</author>
<author>Matthew Haines</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Eduard Hovy</author>
<author>Masayo Iida</author>
<author>Steve K Luk</author>
<author>Richard Whitney</author>
<author>Kenji Yamada</author>
</authors>
<title>Filling Knowledge Gaps in a Broad-Coverage MT System.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<marker>Knight, Chander, Haines, Hatzivassiloglou, Hovy, Iida, Luk, Whitney, Yamada, 1995</marker>
<rawString>Kevin Knight, Ishwar Chander, Matthew Haines, Vasileios Hatzivassiloglou, Eduard Hovy, Masayo Iida, Steve K. Luk, Richard Whitney, and Kenji Yamada. 1995. Filling Knowledge Gaps in a Broad-Coverage MT System. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia,</location>
<contexts>
<context position="6195" citStr="Papineni et al, 2002" startWordPosition="953" endWordPosition="956">d relies on symmetrized IBM model 2 word-alignments for phrase pair induction. The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. To set weights on the components of the loglinear model, we implemented Och’s algorithm (Och, 2003). This essentially involves generating, in an iterative process, a set of nbest translation hypotheses that are representative of the entire search space for a given set of source sentences. Once this is accomplished, a variant of Powell’s algorithm is used to find weights that optimize BLEU score (Papineni et al, 2002) over these hypotheses, compared to reference translations. Unfortunately, our implementation of this algorithm converged only very slowly to a satisfactory final nbest list, so we used two different ad hoc strategies for setting weights: choosing the best values encountered during the iterations of Och’s algorithm (FrenchEnglish), and a grid search (all other languages). To perform the actual translation, we used our decoder, Canoe, which implements a dynamicprogramming beam search algorithm based on that of Pharaoh (Koehn, 2004). Canoe is input-output compatible with Pharaoh , with the excep</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics ACL, Philadelphia, July 2002, pp. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Moore</author>
</authors>
<title>Fast and Accurate Sentence Alignment of Bilingual Corpora.</title>
<date>2002</date>
<booktitle>In Machine Translation: From Research to Real Users (Proceedings of the 5th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>135--244</pages>
<location>Tiburon, California), SpringerVerlag, Heidelberg, Germany,</location>
<contexts>
<context position="4147" citStr="Moore, 2002" startWordPosition="622" endWordPosition="623">s, 2005 2.1 Preprocessing Preprocessing is a necessary first step in order to convert raw texts in both source and target languages into a format suitable for both model training and decoding (Foster et al., 2003). For the supplied Europarl corpora, we relied on the existing segmentation and tokenization, except for French, which we manipulated slightly to bring into line with our existing conventions (e.g., converting l ’ an into l’ an). For the Hansard corpus used to supplement our French-English resources (described in section 3 below), we used our own alignment based on Moore’s algorithm (Moore, 2002), segmentation, and tokenization procedures. Languages with rich morphology are often problematic for statistical machine translation because the available data lacks instances of all possible forms of a word to efficiently train a translation system. In a language like German, new words can be formed by compounding (writing two or more words together without a space or a hyphen in between). Segmentation is a crucial step in preprocessing languages such as German and Finnish texts. In addition to these simple operations, we also developed a rule-based component to detect numbers and dates in t</context>
</contexts>
<marker>Moore, 2002</marker>
<rawString>Moore, Robert. 2002. Fast and Accurate Sentence Alignment of Bilingual Corpora. In Machine Translation: From Research to Real Users (Proceedings of the 5th Conference of the Association for Machine Translation in the Americas, Tiburon, California), SpringerVerlag, Heidelberg, Germany, pp. 135-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<location>Philadelphia,</location>
<marker>Och, Ney, 2002</marker>
<rawString>Och, F. J. and H. Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, pp. 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sapporo,</location>
<contexts>
<context position="5874" citStr="Och, 2003" startWordPosition="903" endWordPosition="904">ur main components: one or more trigram language models, one or more phrase translation models, a distortion model, and a word-length feature. The trigram language model is implemented in the SRILM toolkit (Stolcke, 2002). The phrase-based translation model is similar to the one described in (Koehn, 2004), and relies on symmetrized IBM model 2 word-alignments for phrase pair induction. The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. To set weights on the components of the loglinear model, we implemented Och’s algorithm (Och, 2003). This essentially involves generating, in an iterative process, a set of nbest translation hypotheses that are representative of the entire search space for a given set of source sentences. Once this is accomplished, a variant of Powell’s algorithm is used to find weights that optimize BLEU score (Papineni et al, 2002) over these hypotheses, compared to reference translations. Unfortunately, our implementation of this algorithm converged only very slowly to a satisfactory final nbest list, so we used two different ad hoc strategies for setting weights: choosing the best values encountered dur</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och, 2003. Minimum Error Rate Training for Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, Sapporo, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A multilingual corpusfor evaluation of machine translation.</title>
<date>2002</date>
<institution>Ms., University of Southern California.</institution>
<marker>Koehn, 2002</marker>
<rawString>Philipp Koehn. 2002. Europarl: A multilingual corpusfor evaluation of machine translation. Ms., University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a Beam Search Decoder for Phrase-based Statistical Machine Translation Models.</title>
<date>2004</date>
<booktitle>In Proceedings of the Association for Machine Translation in the Americas AMTA</booktitle>
<contexts>
<context position="5570" citStr="Koehn, 2004" startWordPosition="852" endWordPosition="853">in both languages, and on the test data. 2.2 Decoding Decoding is the central phase in SMT, involving a search for the hypotheses t that have highest probabilities of being translations of the current source sentence s according to a model for P(t|s). Our model for P(t|s) is a log-linear combination of four main components: one or more trigram language models, one or more phrase translation models, a distortion model, and a word-length feature. The trigram language model is implemented in the SRILM toolkit (Stolcke, 2002). The phrase-based translation model is similar to the one described in (Koehn, 2004), and relies on symmetrized IBM model 2 word-alignments for phrase pair induction. The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. To set weights on the components of the loglinear model, we implemented Och’s algorithm (Och, 2003). This essentially involves generating, in an iterative process, a set of nbest translation hypotheses that are representative of the entire search space for a given set of source sentences. Once this is accomplished, a variant of Powell’s algorithm is used to find weights that optimize BLEU sco</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder for Phrase-based Statistical Machine Translation Models. In Proceedings of the Association for Machine Translation in the Americas AMTA 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>