<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.649738">
Ambiguity Resolution for Machine Translation of Telegraphic Messagesi
</title>
<note confidence="0.9582434">
Young-Suk Lee Clifford Weinstein Stephanie Seneff Dinesh Tummala
Lincoln Laboratory Lincoln Laboratory SLS, LCS Lincoln Laboratory
MIT MIT MIT MIT
Lexington, MA 02173 Lexington, MA 02173 Cambridge, MA 02139 Lexington, MA 02173
USA USA USA USA
</note>
<email confidence="0.864321">
ysl@sst.11.mit.edu cjw@sst.11.mit.edu seneff@lcs.mit.edu tummalasst.11.mit.edu
</email>
<sectionHeader confidence="0.99236" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99932337037037">
Telegraphic messages with numerous instances of omis-
sion pose a new challenge to parsing in that a sen-
tence with omission causes a higher degree of ambigu-
ity than a sentence without omission. Misparsing in-
duced by omissions has a far-reaching consequence in
machine translation. Namely, a misparse of the input
often leads to a translation into the target language
which has incoherent meaning in the given context.
This is more frequently the case if the structures of
the source and target languages are quite different, as
in English and Korean. Thus, the question of how we
parse telegraphic messages accurately and efficiently
becomes a critical issue in machine translation. In this
paper we describe a technical solution for the issue, and
present the performance evaluation of a machine trans-
lation system on telegraphic messages before and after
adopting the proposed solution. The solution lies in
a grammar design in which lexicalized grammar rules
defined in terms of semantic categories and syntactic
rules defined in terms of part-of-speech are utilized to-
gether. The proposed grammar achieves a higher pars-
ing coverage without increasing the amount of ambigu-
ity/misparsing when compared with a purely lexical-
ized semantic grammar, and achieves a lower degree
of ambiguity/misparses without decreasing the pars-
ing coverage when compared with a purely syntactic
grammar.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997556163934426">
Achieving the goal of producing high quality machine transla-
tion output is hindered by lexical and syntactic ambiguity of the
input sentences. Lexical ambiguity may be greatly reduced by
limiting the domain to be translated. However, the same is not
generally true for syntactic ambiguity. In particular, telegraphic
messages, such as military operations reports, pose a new chal-
lenge to parsing in that frequently occurring ellipses in the cor-
pus induce a higher degree of syntactic ambiguity than for text
written in &amp;quot;grammatical&amp;quot; English. Misparsing triggered by the
ambiguity of the input sentence often leads to a mistranslation
in a machine translation system. Therefore, the issue becomes
how to parse telegraphic messages accurately and efficiently to
produce high quality translation output.
In general the syntactic ambiguity of an input text may be
greatly reduced by introducing semantic categories in the gram-
mar to capture the co-occurrence restrictions of the input string.
In addition, ambiguity introduced by omission can be reduced
by lexicalizing grammar rules to delimit the lexical items which
&apos;This work was sponsored by the Defense Advanced Research
Projects Agency. Opinions, interpretations, conclusions, and rec-
ommendations are those of the authors and are not necessarily
endorsed by the United States Air Force.
typically occur in phrases with omission in the given domain. A
drawback of this approach, however, is that the grammar cover-
age is quite low. On the other hand, grammar coverage may be
maximized when we rely on syntactic rules defined in terms of
part-of-speech at the cost of a high degree of ambiguity. Thus,
the goal of maximizing the parsing coverage while minimizing
the ambiguity may be achieved by adequately combining lexi-
calized rules with semantic categories, and non-lexicalized rules
with syntactic categories. The question is how much semantic
and syntactic information is necessary to achieve such a goal.
In this paper we propose that an adequate amount of lex-
ical information to reduce the ambiguity in general originates
from verbs, which provide information on subcategorization, and
prepositions, which are critical for PP-attachment ambiguity res-
olution. For the given domain, lexicalizing domain-specific ex-
pressions which typically occur in phrases with omission is ade-
quate for ambiguity resolution. Our experimental results show
that the mix of syntactic and semantic grammar as proposed
here has advantages over either a syntactic grammar or a lexi-
calized semantic grammar. Compared with a syntactic grammar,
the proposed grammar achieves a much lower degree of ambigu-
ity without decreasing the grammar coverage. Compared with
a lexicalized semantic grammar, the proposed grammar achieves
a higher rate of parsing coverage without increasing the ambi-
guity. Furthermore, the generality introduced by the syntactic
rules facilitates the porting of the system to other domains as
well as enabling the system to handle unknown words efficiently.
This paper is organized as follows. In section 2 we discuss
the motivation for lexicalizing grammar rules with semantic cat-
egories in the context of translating telegraphic messages, and
its drawbacks with respect to parsing coverage. In section 3 we
propose a grammar writing technique which minimizes the ambi-
guity of the input and maximizes the parsing coverage. In section
4 we give our experimental results of the technique on the basis
of two sets of unseen test data. In section 5 we discuss system
engineering issues to accommodate the proposed technique, i.e.,
integration of part-of-speech tagger and the adaptation of the
understanding system. Finally section 6 provides a summary of
the paper.
</bodyText>
<sectionHeader confidence="0.635628" genericHeader="introduction">
2 Translation of Telegraphic Messages
</sectionHeader>
<bodyText confidence="0.9770805">
Telegraphic messages contain many instances of phrases with
omission, cf. (Grishman, 1989), as in (1). This introduces a
greater degree of syntactic ambiguities than for texts without
any omitted element, thereby posing a new challenge to parsing.
</bodyText>
<equation confidence="0.945423">
(1)
</equation>
<bodyText confidence="0.94562775">
TU-95 destroyed 220 nm. An aircraft TU-95 was destroyed
at 220 nautical miles)
Syntactic ambiguity and the resultant misparse induced by
such an omission often leads to a mistranslation in a machine
translation system, such as the one described in (Weinstein et
al., 1996), which is depicted in Figure 1.
The system depicted in Figure 1 has a language understanding
module TINA, (Seneff, 1992), and a language generation module
</bodyText>
<page confidence="0.995314">
120
</page>
<figureCaption confidence="0.9048775">
Figure 1: An Interlingua-Based English-to-Korean Machine
Translation System
</figureCaption>
<bodyText confidence="0.982964285714286">
GENESIS, (Glass, Polifroni and Seneff, 1994), at the core. The
semantic frame is an intermediate meaning representation which
is directly derived from the parse tree and becomes the input to
the generation system. The hierarchical structure of the parse
tree is preserved in the semantic frame, and therefore a misparse
of the input sentence leads to a mistranslation. Suppose that
the sentence (1) is misparsed as an active rather than a passive
sentence due to the omission of the verb was, and that the prepo-
sitional phrase 220 nm is misparsed as the direct object of the
verb destroy. These instances of misunderstanding are reflected
in the semantic frame. Since the semantic frame becomes the
input to the generation system, the generation system produces
the non-sensical Korean translation output, as in (2), as opposed
to the sensible one, as in (3).3
TU-95-ka 220 hayli-lul pakoy-hayssta
TU-95-NOM 220 nautical mile-OBJ destroyed
TU-95-ka 220 hayli-eyse pakoy-toyessta
TU-95-NOM 220 nautical mile-LOC was destroyed
Given that the generation of the semantic frame from the parse
tree, and the generation of the translation output from the se-
mantic frame, are quite straightforward in such a system, and
that the flexibility of the semantic frame representation is well
suited for multilingual machine translation, it would be more de-
sirable to find a way of reducing the ambiguity of the input text
to produce high quality translation output, rather than adjust-
ing the translation process. In the sections below we discuss one
such method in terms of grammar design and some of its side
effects.x
</bodyText>
<subsectionHeader confidence="0.999886">
2.1 Lexicalization of Grammar Rules with
Semantic Categories
</subsectionHeader>
<bodyText confidence="0.9997735">
In the domain of naval operational report messages (MUC-II
messages hereafter),4 (Sundheim, 1989), we find two types of
ellipsis. First, top level categories such as subjects and the copula
verb be are often omitted, as in (4).
</bodyText>
<equation confidence="0.42711">
(4)
</equation>
<bodyText confidence="0.7104178">
Considered hostile act (= This was considered to be a hostile
act).
Second, many function words like prepositions and articles are
omitted. Instances of preposition omission are given in (5), where
z stands for Greenwich Mean Time (GMT).
</bodyText>
<listItem confidence="0.9884485">
(5)
a. Haylor hit by a torpedo and put out of action 8 hours (= for
8 hours)
b. All hostile recon aircraft outbound 1300 z (= at 1300 z)
</listItem>
<bodyText confidence="0.999233388888889">
If we try to parse sentences containing such omissions with the
grammar where the rules are defined in terms of syntactic cat-
egories (i.e. part-of-speech), the syntactic ambiguity multiplies.
3In the examples, NOM stands for the nominative case
marker, OBJ the object case marker, and LOC the locative
postposition.
4MUC-II stands for the Second Message Understanding Con-
ference. MUC-II messages were originally collected and prepared
by NRaD(1989) to support DARPA-sponsored research in mes-
sage understanding.
To accommodate sentences like (5)a-b, the grammar needs to al-
low all instances of noun phrases (NP hereafter) to be ambiguous
between an NP and a prepositional phrase (PP hereafter) where
the preposition is omitted. Allowing an input where the copula
verb be is omitted in the grammar causes the past tense form
of a verb to be interpreted either as the main verb with the ap-
propriate form of be omitted, as in (6)a, or as a reduced relative
clause modifying the preceding noun, as in (6)b.
</bodyText>
<figure confidence="0.59164525">
(6)
Aircraft launched at 1300 z
a. Aircraft were launched at 1300 z
b. Aircraft which were launched at 1300 z
</figure>
<bodyText confidence="0.99817375">
Such instances of ambiguity are usually resolved on the basis
of the semantic information. However, relying on a semantic
module for ambiguity resolution implies that the parser needs
to produce all possible parses of the input text and carry them
along, thereby requiring a more complex understanding process.
One way of reducing the ambiguity at an early stage of pro-
cessing without relying on a semantic module is to incorporate
domain/semantic knowledge into the grammar as follows:
</bodyText>
<listItem confidence="0.9315633">
• Lexicalize grammar rules to delimit the lexical items which
typically occur in phrases with omission;
• Introduce semantic categories to capture the co-occurrence
restrictions of lexical items.
example grammar rules instantiating these ideas are
(7).
b. .headless_PP
at np_distance
at np_bearing
d. temporal_PP
</listItem>
<bodyText confidence="0.995502846153846">
{during after prior to ...} NP
time_expression
f. .gmt
(7)a states that a locative prepositional phrase consists of a
subset of prepositions and a noun phrase. In addition, there is
a subcategory headless_PP which consists of a subset of noun
phrases which typically occur in a locative prepositional phrase
with the preposition omitted. The head nouns which typically
occur in prepositional phrases with the preposition omission are
nautical miles and yard. The rest of the rules can be read in a
similar manner. And it is clear how such lexicalized rules with
the semantic categories reduce the syntactic ambiguity of the
input text.
</bodyText>
<subsectionHeader confidence="0.997707">
2.2 Drawbacks
</subsectionHeader>
<bodyText confidence="0.999741666666667">
Whereas the language processing is very efficient when a system
relies on a lexicalized semantic grammar, there are some draw-
backs as well.
</bodyText>
<listItem confidence="0.9094815">
• Since the grammar is domain and word specific, it is not
easily ported to new constructions and new domains.
• Since the vocabulary items are entered in the grammar as
part of lexicalized grammar rules, if an input sentence con-
</listItem>
<bodyText confidence="0.9675438">
tains words unknown to the grammar, parsing fails.
These drawbacks are reflected in the performance evaluation of
our machine translation system. After the system was developed
on all the training data of the MUC-II corpus (640 sentences, 12
words/sentence average), the system was evaluated on the held-
out test set of 111 sentences (hereafter TEST set). The results
are shown in Table 1. The system was also evaluated on the
data which were collected from an in-house experiment. For this
experiment, the subjects were asked to study a number of MUG.
II sentences, and create about 20 MUC-II-like sentences. These
</bodyText>
<figure confidence="0.999573">
LANGUAGE UNDERSTANDING
PARSE SEMANTIC
TREE FRAME
TINA
LANGUAGE
GENERATION
GENESIS
ENGLISH
TEXT
INPUT
KOREAN
■-•■••• TEXT
OUTPUT
Some
given in
(. .
7)
a locative_PP
{at in near off on ...} NP
heaciless_PP
c. .np_distance
numeric nautical_mile
numeric yard
e. .time_expression
[at] numeric gmt
</figure>
<page confidence="0.981197">
121
</page>
<tableCaption confidence="0.9498985">
Table 1: TEST Data Evaluation Results on the Lexicalized
Semantic Grammar
</tableCaption>
<table confidence="0.9975118">
rota&apos; No. of sentences 281
No. of sentences with no 239/281 (85.1%)
unknown words
No. of parsed sentences 103/239 (43.1%)
No. of misparsed sentences , 15/103 (14.6%)
</table>
<tableCaption confidence="0.884319">
Table 2: TEST&apos; Data Evaluation Results on the Lexicalized
Semantic Grammar
</tableCaption>
<bodyText confidence="0.970995666666667">
NICC-II-like sentences form data set TEST&apos;. The results of the
system evaluation on the data set TEST&apos; are given in Table 2.
Table 1 shows that the grammar coverage for unseen data is
about 35%, excluding the failures due to unknown words. Table 2
indicates that even for sentences constructed to be similar to the
training data, the grammar coverage is about 43%, again exclud-
ing the parsing failures due to unknown words. The misparse5
rate with respect to the total parsed sentences ranges between
8.7% and 14.6%, which is considered to be highly accurate.
</bodyText>
<sectionHeader confidence="0.96464" genericHeader="method">
3 Incorporation of Syntactic Knowledge
</sectionHeader>
<bodyText confidence="0.999712777777778">
Considering the low parsing coverage of a semantic grammar
which relies on domain specific knowledge, and the fact that the
successful parsing of the input sentence is a prerequisite for pro-
ducing translation output, it is critical to improve the parsing
coverage. Such a goal may be achieved by incorporating syn-
tactic rules into the grammar while retaining lexical/semantic
information to minimize the ambiguity of the input text. The
question is: how much semantic and syntactic information is
necessary? We propose a solution, as in (8):
</bodyText>
<listItem confidence="0.912525090909091">
(8)
(a) Rules involving verbs and prepositions need to be lexicalized
to resolve the prepositional phrase attachment ambiguity, cf.
(Brill and Resnik, 1993).
(b) Rules involving verbs need to be lexicalized to prevent mis-
parsing due to an incorrect subcategorization.
(c) Domain specific expressions (e.g. z. nm in the MUC-II cor-
pus) which frequently occur in phrases with omitted elements.
need to be lexicalized.
(d) Otherwise, rely on syntactic rules defined in terms of part-
of-speech.
</listItem>
<bodyText confidence="0.9995618">
In this section, we discuss typical misparses for the syntac-
tic grammar on experiments in the NIUC-II corpus. We then
illustrate how these misparses are corrected by lexicalizing the
grammar rules for verbs, prepositions. and some domain-specific
phrases.
</bodyText>
<subsectionHeader confidence="0.817893">
3.1 Typical Misparses Caused by Syntactic
Grammar
</subsectionHeader>
<bodyText confidence="0.9208535">
The misparses we find in the NIUC-II corpus. when tested on a
syntactic grammar, are largely due to the three factors specified
in (9).
5The term misparse in this paper should be interpreted with
care. A number of the sentences we consider to be misparses are
not -syntactic&amp;quot; misparses, but &amp;quot;semantically anomalous.&amp;quot; Since
we are interested in getting the accurate interpretation in the
given context at the parsing stage, we consider parses which are
semantically anomalous to be misparses.
i. Misparsing due to prepositional phrase attachment
(hereafter PP-attachment) ambiguity
Misparsing due to incorrect verb subcategorizations
in. Misparsing due to the omission of a preposition, e.g.
/4/0 z instead of at 1410 z
Examples of misparses due to an incorrect verb subcatego-
rization and a PP-attachment ambiguity are given in Figure 2
and Figure 3. respectively. An example of a misparse due to
preposition omission is given in Figure 4.
In Figure 2, the verb intercepted incorrectly subcategorizes for a
finite complement clause.
</bodyText>
<figureCaption confidence="0.732946166666667">
In Figure 3, the prepositional phrase with 12 rounds is wrongly
attached to the noun phrase the contact. as opposed to the verb
phrase vp_active, to which it properly belongs.
Figure 4 shows that the prepositional phrase 1410 z with at
omitted is misparsed as a part of the noun phrase expression
hostile raid composition.
</figureCaption>
<subsectionHeader confidence="0.852209">
3.2 Correcting Misparses by Lexicalizing Verbs,
Prepositions, and Domain Specific Phrases
</subsectionHeader>
<bodyText confidence="0.997708230769231">
Providing the accurate subcategorization frame for the verb in-
tercept by lexicalizing the higher level category &apos;vp ensures that
it never takes a finite clause as its complement. leading to the
correct parse, as in Figure 5.
As for PP-attachment ambiguity, lexicalization of verbs and
prepositions helps in identifying the proper attachment site of the
prepositional phrase, cf. (Brill and Resnik, 1993), as illustrated
in Figure 6.
Misparses due to omission are easily corrected by deploying
lexicalized rules for the vocabulary items which occur in phrases
with omitted elements. For the misparse illustrated in Figure 3,
utilizing the lexicalized rules in (10) prevents 1410 z from being
analyzed as part of the subsequent noun phrase. as in Figure 7.
</bodyText>
<equation confidence="0.458303">
(10) a. .time_expression b. .gmt
[at] numeric gmt
</equation>
<sectionHeader confidence="0.9923" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.9995131">
In this section we report two types of experimental results. One
is the parsing results on two sets of unseen data TEST and
TEST&apos; (discussed in Section 2) using the syntactic grammar de-
fined purely in terms of part-of-speech. The other is the parsing
results on the same sets of data using the grammar which com-
bines lexicalized semantic grammar rules and syntactic grammar
rules. The results are compared with respect to the parsing cov-
erage and the misparse rate. These experimental results are also
compared with the parsing results with respect to the lexicalized
semantic grammar discussed in Section 2.
</bodyText>
<subsectionHeader confidence="0.853039">
4.1 Experimental Results on Data Set TEST
</subsectionHeader>
<table confidence="0.908638333333333">
Total No. of sentences 1 111
No. of parsed sentences 84/111 (75.7%)
No. of misparsed sentences 24/84 (29%)
</table>
<tableCaption confidence="0.747203">
Table 3: TEST Data Evaluation Results on the Syntactic
Grammar
</tableCaption>
<table confidence="0.957367666666667">
Total No. of sentences 111
No. Firi:7 --arse sentences 86/111 (77%)
No. of misparsed sentences 9/86 (10%) !
</table>
<tableCaption confidence="0.9167175">
Table 4: TEST Data Evaluation Results on the Mixed
Grammar
</tableCaption>
<bodyText confidence="0.582415333333333">
In terms of parsing coverage, the two grammars perform equally
well (around 76%). In terms of misparse rate, however, the gram-
mar which utilizes only syntactic categories shows a much higher
</bodyText>
<table confidence="0.689303857142857">
Total No. ot sentences 111
66/111 (59.5%)
No, of sentences with no
unknown words
No. of parsed sentences 23/66 (34.8%)
No. of misparsed sentences 2/23 (8.1%)
(9)
</table>
<page confidence="0.866381">
122
</page>
<figureCaption confidence="0.99998">
Figure 2: Misparse due to incorrect verb subcategorization
Figure 3: Misparse due to PP-attachment ambiguity
</figureCaption>
<figure confidence="0.994308245901639">
finite_comp
finite_statement
prep
o_np
PP
det nn_head
sentence
full_parse
statement
predicate
vp_active
when interceptethe range of
prep
the aircraft -.a
d-m.P
nn7head
enterprisewas
adverb vverb
det nn_head
subject
q_np
PP
link_comp
complement
ci-nP
cardinal nn_head
30 nm
cl_np
det nn_head PP
Prep
4-nP
nn_head pp
prep
• q_np
cardinal nn_head
spencer engaged the
prep q_no
cardinal nn_head
at 3000 yam
contact with 12 rounds of 5-inch
sentence
full-Parse
statement
predicate
vp_active
subject
o_np
nn_head vverb
123
sentence
full_parse
fragment
complement
q_np
det possessive adjective nn_head PP
raid composition of
1410 z hostile
cardinal
nn_head
19 aircraft
prep 101-nP
</figure>
<figureCaption confidence="0.999988">
Figure 4: Misparse due to Omission of Preposition
Figure 5: Parse Tree with Correct Verb Subcategorization
</figureCaption>
<figure confidence="0.994045983050848">
liMIC_COMP
complement
Complement_np
quantifiera_distance
cardinal nautical-mi]
prep
q_np
nn_head
det
sentence
full_parse
statement
pre_adjunct subject
temporal_clause q_np
when_clause det nn_head op
statement
participial_ hrase
passive
vp_intercept en
vintereept
interceptedhe
range of the aircraft to enterprisewas 30 nm
when
when
PP
prep
4-nP
nn_head
124
I full_parsentence
1
se
statement
subject predicate
Q-nP vP_engage
nn_head vengage
dir_object
q_np
det nn_head
with
ci-nP
nn_head
with_no
cardinal
1
PP
prep
spencer engaged the
contact with 12 rounds of
at
1
1
4-mP
nn_head
5-inch at
locative_pp
Q-nP
cardinal nn_head
3000 yds
</figure>
<figureCaption confidence="0.9999895">
Figure 6: Parse Tree with Correct PP-attachment
Figure 7: Corrected Parse Tree
</figureCaption>
<figure confidence="0.992839086956522">
sentence
1
full_parse
1
fragment
nn_head
pre_adjunct complement
1 1
time_expression q_np
1
gmt_time adjective
1
numeric_time
n_of
hostile
raid
composition of
cl-nP
cardinal nn_head
19 aircraft
cardinal gmt
1 1
1410
</figure>
<page confidence="0.995758">
125
</page>
<bodyText confidence="0.999920571428571">
rate of misparse (i.e. 29%) than the grammar which utilizes
both syntactic and semantic categories (i.e. 10%). Comparing
the evaluation results on the mixed grammar with those on the
lexicalized semantic grammar discussed in Section 2, the parsing
coverage of the mixed grammar is much higher (77%) than that
of the semantic grammar (59.5%). In terms of misparse rate,
both grammars perform equally well, i.e. around 9%.6
</bodyText>
<note confidence="0.539064">
4.2 Experimental Results on Data Set TEST&apos;
</note>
<table confidence="0.995425">
Total No. of sentences 281
No. of sentences which parse 215/281 (76.5%)
No. of misparsed sentences 60/215 (28%)
</table>
<tableCaption confidence="0.975027">
Table 5: TEST&apos; Data Evaluation Results on Syntactic
</tableCaption>
<table confidence="0.95833775">
Grammar
Total No. of sentences 289
No. of parsed sentences 236/289 (82%)
No. of misparsed sentences 23/236 (10%)
</table>
<tableCaption confidence="0.9393815">
Table 6: TEST&apos; Data Evaluation Results on Mixed Gram-
mar
</tableCaption>
<bodyText confidence="0.999849727272727">
Evaluation results of the two types of grammar on the TEST&apos;
data, given in Table 5 and Table 6, are similar to those of the
two types of grammar on the TEST data discussed above.
To summarize, the grammar which combines syntactic rules
and lexicalized semantic rules fares better than the syntactic
grammar or the semantic grammar. Compared with a lex-
icalized semantic grammar, this grammar achieves a higher
parsing coverage without increasing the amount of ambigu-
ity/misparsing. When compared with a syntactic grammar, this
grammar achieves a lower degree of ambiguity/misparsing with-
out decreasing the parsing rate.
</bodyText>
<sectionHeader confidence="0.930253" genericHeader="method">
5 System Engineering
</sectionHeader>
<bodyText confidence="0.9999695">
An input to the parser driven by a grammar which utilizes both
syntactic and lexicalized semantic rules consists of words (to be
covered by lexicalized semantic rules) and parts-of-speech (to be
covered by syntactic rules). To accommodate the part-of-speech
input to the parser, the input sentence has to be part-of-speech
tagged before parsing. To produce an adequate translation out-
put from the input containing parts-of-speech, there has to be
a mechanism by which parts-of-speech are used for parsing pur-
poses, and the corresponding lexical items are used for the se-
mantic frame representation.
</bodyText>
<subsectionHeader confidence="0.9501295">
5.1 Integration of Rule-Based Part-of-Speech
Tagger
</subsectionHeader>
<bodyText confidence="0.999598117647059">
To accommodate the part-of-speech input to the parser, we have
integrated the rule-based part-of-speech tagger, (Brill, 1992),
(Brill, 1995), as a preprocessor to the language understanding
system TINA, as in Figure 8. An advantage of integrating a
part-of-speech tagger over a lexicon containing part-of-speech in-
formation is that only the former can tag words which are new
to the system, and provides a way of handling unknown words.
While most stochastic taggers require a large amount of train-
ing data to achieve high rates of tagging accuracy, the rule-based
6The parsing coverage of the semantic grammar, i.e. 34.8%,
is after discounting the parsing failure due to words unknown to
the grammar. The reason why we do not give the statistics of the
parsing failure due to unknown words for the syntactic and the
mixed grammar is because the part-of-speech tagging process,
which will be discussed in detail in Section 5, has the effect of
handling unknown words, and therefore the problem does not
arise.
</bodyText>
<sectionHeader confidence="0.530549333333333" genericHeader="method">
ENGLISH RULE-BASED LANGUAGE LANGUAGE KOREAN
TEXT PART-OF-SPEEC UNDERSTANDING GENERATION -Or TEXT
INPUT TAGGER TINA GENESIS OUTPUT
</sectionHeader>
<figureCaption confidence="0.899584">
Figure 8: Integration of the Rule-Based Part-of-Speech Tag-
</figureCaption>
<bodyText confidence="0.98478235483871">
ger as a Preprocessor to the Language Understanding Sys-
tem
tagger achieves performance comparable to or higher than that
of stochastic taggers, even with a training corpus of a modest
size. Given that the size of our training corpus is fairly small
(total 7716 words), a transformation-based tagger is well suited
to our needs.
The transformation-based part-of-speech tagger operates in
two stages. Each word in the tagged training corpus has an
entry in the lexicon consisting of a partially ordered list of tags,
indicating the most likely tag for that word, and all other tags
seen with that word (in no particular order). Every word is first
assigned its most likely tag in isolation. Unknown words are
first assumed to be nouns, and then cues based upon prefixes,
suffixes, infixes, and adjacent word co-occurrences are used to
upgrade the most likely tag. Secondly, after the most likely tag
for each word is assigned, contextual transformations are used to
improve the accuracy.
We have evaluated the tagger performance on the TEST Data
both before and after training on the MUC-II corpus. The re-
sults are given in Table 7. Tagging statistics &apos;before training&apos;
are based on the lexicon and rules acquired from the BROWN
CORPUS and the WALL STREET JOURNAL CORPUS. Tag-
ging statistics &apos;after training&apos; are divided into two categories,
both of which are based on the rules acquired from training data
sets of the MUC-II corpus. The only difference between the two
is that in one case (After Training I) we use a lexicon acquired
from the MUC-II corpus, and in the other case (After Training
II) we use a lexicon acquired from a combination of the BROWN
CORPUS, the WALL STREET JOURNAL CORPUS, and the
MUC-II database.
</bodyText>
<table confidence="0.998008">
Training Status &apos;lagging Accuracy
Before Training 1125/1287 87.4%)
After Training I 1249[1287 97%)
After Training II 1263/1287 98%)
</table>
<tableCaption confidence="0.8668745">
Table 7: Tagger Evaluation on Data Set TEST
Table 7 shows that the tagger achieves a tagging accuracy of
</tableCaption>
<bodyText confidence="0.987070428571429">
up to 98% after training and using the combined lexicon, with
an accuracy for unknown words ranging from 82 to 87%. These
high rates of tagging accuracy are largely due to two factors:
(1) Combination of domain specific contextual rules obtained by
training the MUC-II corpus with general contextual rules ob-
tained by training the WSJ corpus; And (2) Combination of the
MUC-II lexicon with the lexicon for the WSJ corpus.
</bodyText>
<subsectionHeader confidence="0.999064">
5.2 Adaptation of the Understanding System
</subsectionHeader>
<bodyText confidence="0.999995615384616">
The understanding system depicted in Figure 1 derives the se-
mantic frame representation directly from the parse tree. The
terminal symbols (i.e. words in general) in the parse tree are
represented as vocabulary items in the semantic frame. Once we
allow the parser to take part-of-speech as the input, the parts-
of-speech (rather than actual words) will appear as the terminal
symbols in the parse tree, and hence as the vocabulary items
in the semantic frame representation. We adapted the system so
that the part-of-speech tags are used for parsing, but are replaced
with the original words in the final semantic frame. Generation
can then proceed as usual. Figures 9 and (11) illustrate the parse
tree and semantic frame produced by the adapted system for the
input sentence 0819 z unknown contacts replied incorrectly.
</bodyText>
<page confidence="0.997078">
126
</page>
<figureCaption confidence="0.99897">
Figure 9: Parse Tree Based on the Mix of Word and Part-of-Speech Sequence
</figureCaption>
<figure confidence="0.990758375">
sentence
full_parse
1
statement
pre_adjunct
time_expression
gmt_time
numeric_time
cardinal get
subject
1
q_np
adjective nn_head
&apos;
•
I
adv
predicate
vp_reply
vreply adverb_phrase
0819 z unknown contact replied incorrectly
(11)
{c statement
:time_expression
:topic {q nn_head
:name &amp;quot;contact&amp;quot;
:pred {p unknown
:global 1 } }
:subject 1
:pred {p reply_v
:mode &amp;quot;past&amp;quot;
:adverb {p incorrectly
</figure>
<sectionHeader confidence="0.956634" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.9994359">
In this paper we have proposed a technique which maximizes the
parsing coverage and minimizes the misparse rate for machine
translation of telegraphic messages. The key to the technique is
to adequately mix semantic and syntactic rules in the grammar.
We have given experimental results of the proposed grammar,
and compared them with the experimental results of a syntac-
tic grammar and a semantic grammar with respect to parsing
coverage and misparse rate, which are summarized in Table 8
and Table 9. We have also discussed the system adaptation to
accommodate the proposed technique.
</bodyText>
<table confidence="0.99326525">
Grammar Type Parsing Rate Misparse Rate
Semantic Grammar 34.8% 8.7%
Syntactic Grammar 75.7% 29%
Mixed Grammar 77% 10%
</table>
<tableCaption confidence="0.9354025">
Table 8: TEST Data Evaluation Results on the Three Types
of Grammar
</tableCaption>
<table confidence="0.99850925">
Grammar Type Parsing Rate Misparse Rate
Semantic Grammar 43.1% 14.6%
Syntactic Grammar 76.5% 28%
Mixed Grammar 82% 10%
</table>
<tableCaption confidence="0.9540785">
Table 9: TEST&apos; Data Evaluation Results on the Three
Types of Grammar
</tableCaption>
<sectionHeader confidence="0.9965" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996934793103448">
Eric Brill. 1992. A Simple Rule-Based Part of Speech Tagger.
Proceedings of the Third Conference on Applied Natural Lan-
guage Processing, ACL, Trento, Italy.
Eric Brill. 1995. Transformation-Based Error-Driven Learning
and Natural Language Processing: A Case Study in Part-of-
Speech Tagging. Computational Linguistics, 21-4, pages 543-
565.
Eric Brill and Philip Resnik. 1993 A Rule-Based Approach
to Prepositional Phrase Attachment Disambiguation. Techni-
cal report, Department of Computer and Information Science,
University of Pennsylvania.
James Glass, Joseph Polifroni and Stephanie Seneff. 1994. Mul-
tilingual Language Generation Across Multiple Domains. Pre-
sented at the 1994 International Conference on Spoken. Lan-
guage Processing, Yokohama, Japan.
Ralph Grishman. 1989. Analyzing Telegraphic Messages. Pro-
ceedings of Speech and Natural Language Workshop, DARPA.
Stephanie Seneff. 1992. TINA: A Natural Language System for
Spoken Language Applications. Computational Linguistics,
18:1, pages 61-88.
Beth M. Sundheim. Navy Tactical Incident Reporting in a
Highly Constrained Sublanguage: Examples and Analysis.
Technical Document 1477, Naval Ocean Systems Center, San
Diego.
Clifford Weinstein, Dinesh Tummala, Young-Suk Lee, Stephanie
Seneff. 1996. Automatic Engish-to-Korean Text Translation
of Telegraphic Messages in a Limited Domain. To be presented
at the International Conference on Computational Linguistics
&apos;96.
</reference>
<figure confidence="0.989084666666667">
numeric_time
:topic fq gmt
:name &amp;quot;z&amp;quot; 1
:pred {p cardinal
:topic &amp;quot;0819&amp;quot; 1
{p
</figure>
<page confidence="0.884051">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.548753">
<title confidence="0.996083">Resolution for Machine Translation of Telegraphic</title>
<author confidence="0.891587">Young-Suk Lee Clifford Weinstein Stephanie Seneff Dinesh Tummala</author>
<affiliation confidence="0.908743">Lincoln Laboratory Lincoln Laboratory SLS, LCS Lincoln Laboratory MIT MIT MIT MIT</affiliation>
<address confidence="0.999436">Lexington, MA 02173 Lexington, MA 02173 Cambridge, MA 02139 Lexington, MA 02173</address>
<affiliation confidence="0.945895">USA USA USA USA</affiliation>
<email confidence="0.997616">ysl@sst.11.mit.educjw@sst.11.mit.eduseneff@lcs.mit.edutummalasst.11.mit.edu</email>
<abstract confidence="0.989920928571428">Telegraphic messages with numerous instances of omission pose a new challenge to parsing in that a sentence with omission causes a higher degree of ambiguity than a sentence without omission. Misparsing induced by omissions has a far-reaching consequence in machine translation. Namely, a misparse of the input often leads to a translation into the target language which has incoherent meaning in the given context. This is more frequently the case if the structures of the source and target languages are quite different, as in English and Korean. Thus, the question of how we parse telegraphic messages accurately and efficiently becomes a critical issue in machine translation. In this paper we describe a technical solution for the issue, and present the performance evaluation of a machine translation system on telegraphic messages before and after adopting the proposed solution. The solution lies in a grammar design in which lexicalized grammar rules defined in terms of semantic categories and syntactic rules defined in terms of part-of-speech are utilized together. The proposed grammar achieves a higher parsing coverage without increasing the amount of ambiguity/misparsing when compared with a purely lexicalized semantic grammar, and achieves a lower degree of ambiguity/misparses without decreasing the parsing coverage when compared with a purely syntactic grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A Simple Rule-Based Part of Speech Tagger.</title>
<date>1992</date>
<booktitle>Proceedings of the Third Conference on Applied Natural Language Processing, ACL,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="22332" citStr="Brill, 1992" startWordPosition="3494" endWordPosition="3495">es) and parts-of-speech (to be covered by syntactic rules). To accommodate the part-of-speech input to the parser, the input sentence has to be part-of-speech tagged before parsing. To produce an adequate translation output from the input containing parts-of-speech, there has to be a mechanism by which parts-of-speech are used for parsing purposes, and the corresponding lexical items are used for the semantic frame representation. 5.1 Integration of Rule-Based Part-of-Speech Tagger To accommodate the part-of-speech input to the parser, we have integrated the rule-based part-of-speech tagger, (Brill, 1992), (Brill, 1995), as a preprocessor to the language understanding system TINA, as in Figure 8. An advantage of integrating a part-of-speech tagger over a lexicon containing part-of-speech information is that only the former can tag words which are new to the system, and provides a way of handling unknown words. While most stochastic taggers require a large amount of training data to achieve high rates of tagging accuracy, the rule-based 6The parsing coverage of the semantic grammar, i.e. 34.8%, is after discounting the parsing failure due to words unknown to the grammar. The reason why we do no</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Eric Brill. 1992. A Simple Rule-Based Part of Speech Tagger. Proceedings of the Third Conference on Applied Natural Language Processing, ACL, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-ofSpeech Tagging. Computational Linguistics,</title>
<date>1995</date>
<pages>21--4</pages>
<contexts>
<context position="22347" citStr="Brill, 1995" startWordPosition="3496" endWordPosition="3497">f-speech (to be covered by syntactic rules). To accommodate the part-of-speech input to the parser, the input sentence has to be part-of-speech tagged before parsing. To produce an adequate translation output from the input containing parts-of-speech, there has to be a mechanism by which parts-of-speech are used for parsing purposes, and the corresponding lexical items are used for the semantic frame representation. 5.1 Integration of Rule-Based Part-of-Speech Tagger To accommodate the part-of-speech input to the parser, we have integrated the rule-based part-of-speech tagger, (Brill, 1992), (Brill, 1995), as a preprocessor to the language understanding system TINA, as in Figure 8. An advantage of integrating a part-of-speech tagger over a lexicon containing part-of-speech information is that only the former can tag words which are new to the system, and provides a way of handling unknown words. While most stochastic taggers require a large amount of training data to achieve high rates of tagging accuracy, the rule-based 6The parsing coverage of the semantic grammar, i.e. 34.8%, is after discounting the parsing failure due to words unknown to the grammar. The reason why we do not give the stat</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-ofSpeech Tagging. Computational Linguistics, 21-4, pages 543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Philip Resnik</author>
</authors>
<title>A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation.</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="14040" citStr="Brill and Resnik, 1993" startWordPosition="2206" endWordPosition="2209">domain specific knowledge, and the fact that the successful parsing of the input sentence is a prerequisite for producing translation output, it is critical to improve the parsing coverage. Such a goal may be achieved by incorporating syntactic rules into the grammar while retaining lexical/semantic information to minimize the ambiguity of the input text. The question is: how much semantic and syntactic information is necessary? We propose a solution, as in (8): (8) (a) Rules involving verbs and prepositions need to be lexicalized to resolve the prepositional phrase attachment ambiguity, cf. (Brill and Resnik, 1993). (b) Rules involving verbs need to be lexicalized to prevent misparsing due to an incorrect subcategorization. (c) Domain specific expressions (e.g. z. nm in the MUC-II corpus) which frequently occur in phrases with omitted elements. need to be lexicalized. (d) Otherwise, rely on syntactic rules defined in terms of partof-speech. In this section, we discuss typical misparses for the syntactic grammar on experiments in the NIUC-II corpus. We then illustrate how these misparses are corrected by lexicalizing the grammar rules for verbs, prepositions. and some domain-specific phrases. 3.1 Typical</context>
<context position="16527" citStr="Brill and Resnik, 1993" startWordPosition="2594" endWordPosition="2597">at the prepositional phrase 1410 z with at omitted is misparsed as a part of the noun phrase expression hostile raid composition. 3.2 Correcting Misparses by Lexicalizing Verbs, Prepositions, and Domain Specific Phrases Providing the accurate subcategorization frame for the verb intercept by lexicalizing the higher level category &apos;vp ensures that it never takes a finite clause as its complement. leading to the correct parse, as in Figure 5. As for PP-attachment ambiguity, lexicalization of verbs and prepositions helps in identifying the proper attachment site of the prepositional phrase, cf. (Brill and Resnik, 1993), as illustrated in Figure 6. Misparses due to omission are easily corrected by deploying lexicalized rules for the vocabulary items which occur in phrases with omitted elements. For the misparse illustrated in Figure 3, utilizing the lexicalized rules in (10) prevents 1410 z from being analyzed as part of the subsequent noun phrase. as in Figure 7. (10) a. .time_expression b. .gmt [at] numeric gmt 4 Experimental Results In this section we report two types of experimental results. One is the parsing results on two sets of unseen data TEST and TEST&apos; (discussed in Section 2) using the syntactic </context>
</contexts>
<marker>Brill, Resnik, 1993</marker>
<rawString>Eric Brill and Philip Resnik. 1993 A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation. Technical report, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Glass</author>
<author>Joseph Polifroni</author>
<author>Stephanie Seneff</author>
</authors>
<title>Multilingual Language Generation Across Multiple Domains.</title>
<date>1994</date>
<booktitle>Presented at the 1994 International Conference on Spoken. Language Processing,</booktitle>
<location>Yokohama, Japan.</location>
<marker>Glass, Polifroni, Seneff, 1994</marker>
<rawString>James Glass, Joseph Polifroni and Stephanie Seneff. 1994. Multilingual Language Generation Across Multiple Domains. Presented at the 1994 International Conference on Spoken. Language Processing, Yokohama, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>Analyzing Telegraphic Messages.</title>
<date>1989</date>
<booktitle>Proceedings of Speech and Natural Language Workshop,</booktitle>
<location>DARPA.</location>
<contexts>
<context position="5642" citStr="Grishman, 1989" startWordPosition="855" endWordPosition="856">rsing coverage. In section 3 we propose a grammar writing technique which minimizes the ambiguity of the input and maximizes the parsing coverage. In section 4 we give our experimental results of the technique on the basis of two sets of unseen test data. In section 5 we discuss system engineering issues to accommodate the proposed technique, i.e., integration of part-of-speech tagger and the adaptation of the understanding system. Finally section 6 provides a summary of the paper. 2 Translation of Telegraphic Messages Telegraphic messages contain many instances of phrases with omission, cf. (Grishman, 1989), as in (1). This introduces a greater degree of syntactic ambiguities than for texts without any omitted element, thereby posing a new challenge to parsing. (1) TU-95 destroyed 220 nm. An aircraft TU-95 was destroyed at 220 nautical miles) Syntactic ambiguity and the resultant misparse induced by such an omission often leads to a mistranslation in a machine translation system, such as the one described in (Weinstein et al., 1996), which is depicted in Figure 1. The system depicted in Figure 1 has a language understanding module TINA, (Seneff, 1992), and a language generation module 120 Figure</context>
</contexts>
<marker>Grishman, 1989</marker>
<rawString>Ralph Grishman. 1989. Analyzing Telegraphic Messages. Proceedings of Speech and Natural Language Workshop, DARPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Seneff</author>
</authors>
<title>TINA: A Natural Language System for Spoken Language Applications.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<pages>61--88</pages>
<contexts>
<context position="6197" citStr="Seneff, 1992" startWordPosition="945" endWordPosition="946">y instances of phrases with omission, cf. (Grishman, 1989), as in (1). This introduces a greater degree of syntactic ambiguities than for texts without any omitted element, thereby posing a new challenge to parsing. (1) TU-95 destroyed 220 nm. An aircraft TU-95 was destroyed at 220 nautical miles) Syntactic ambiguity and the resultant misparse induced by such an omission often leads to a mistranslation in a machine translation system, such as the one described in (Weinstein et al., 1996), which is depicted in Figure 1. The system depicted in Figure 1 has a language understanding module TINA, (Seneff, 1992), and a language generation module 120 Figure 1: An Interlingua-Based English-to-Korean Machine Translation System GENESIS, (Glass, Polifroni and Seneff, 1994), at the core. The semantic frame is an intermediate meaning representation which is directly derived from the parse tree and becomes the input to the generation system. The hierarchical structure of the parse tree is preserved in the semantic frame, and therefore a misparse of the input sentence leads to a mistranslation. Suppose that the sentence (1) is misparsed as an active rather than a passive sentence due to the omission of the ve</context>
</contexts>
<marker>Seneff, 1992</marker>
<rawString>Stephanie Seneff. 1992. TINA: A Natural Language System for Spoken Language Applications. Computational Linguistics, 18:1, pages 61-88.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Beth M Sundheim</author>
</authors>
<title>Navy Tactical Incident Reporting in a Highly Constrained Sublanguage: Examples and Analysis. Technical Document 1477, Naval Ocean Systems Center,</title>
<location>San Diego.</location>
<marker>Sundheim, </marker>
<rawString>Beth M. Sundheim. Navy Tactical Incident Reporting in a Highly Constrained Sublanguage: Examples and Analysis. Technical Document 1477, Naval Ocean Systems Center, San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clifford Weinstein</author>
<author>Dinesh Tummala</author>
<author>Young-Suk Lee</author>
<author>Stephanie Seneff</author>
</authors>
<title>Automatic Engish-to-Korean Text Translation of Telegraphic Messages in a Limited Domain.</title>
<date>1996</date>
<booktitle>the International Conference on Computational Linguistics &apos;96.</booktitle>
<note>To be presented at</note>
<contexts>
<context position="6076" citStr="Weinstein et al., 1996" startWordPosition="923" endWordPosition="926">g system. Finally section 6 provides a summary of the paper. 2 Translation of Telegraphic Messages Telegraphic messages contain many instances of phrases with omission, cf. (Grishman, 1989), as in (1). This introduces a greater degree of syntactic ambiguities than for texts without any omitted element, thereby posing a new challenge to parsing. (1) TU-95 destroyed 220 nm. An aircraft TU-95 was destroyed at 220 nautical miles) Syntactic ambiguity and the resultant misparse induced by such an omission often leads to a mistranslation in a machine translation system, such as the one described in (Weinstein et al., 1996), which is depicted in Figure 1. The system depicted in Figure 1 has a language understanding module TINA, (Seneff, 1992), and a language generation module 120 Figure 1: An Interlingua-Based English-to-Korean Machine Translation System GENESIS, (Glass, Polifroni and Seneff, 1994), at the core. The semantic frame is an intermediate meaning representation which is directly derived from the parse tree and becomes the input to the generation system. The hierarchical structure of the parse tree is preserved in the semantic frame, and therefore a misparse of the input sentence leads to a mistranslat</context>
</contexts>
<marker>Weinstein, Tummala, Lee, Seneff, 1996</marker>
<rawString>Clifford Weinstein, Dinesh Tummala, Young-Suk Lee, Stephanie Seneff. 1996. Automatic Engish-to-Korean Text Translation of Telegraphic Messages in a Limited Domain. To be presented at the International Conference on Computational Linguistics &apos;96.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>