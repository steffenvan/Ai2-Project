<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003679">
<title confidence="0.978194">
UCB System Description for the WMT 2007 Shared Task
</title>
<author confidence="0.802475">
Preslav Nakov
</author>
<affiliation confidence="0.841045">
EECS, CS division
University of California at Berkeley
</affiliation>
<address confidence="0.787808">
Berkeley, CA 94720
</address>
<email confidence="0.997837">
nakov@cs.berkeley.edu
</email>
<author confidence="0.993226">
Marti Hearst
</author>
<affiliation confidence="0.9978855">
School of Information
University of California at Berkeley
</affiliation>
<address confidence="0.809637">
Berkeley, CA 94720
</address>
<email confidence="0.998748">
hearst@ischool.berkeley.edu
</email>
<sectionHeader confidence="0.995635" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999906615384615">
For the WMT 2007 shared task, the UC
Berkeley team employed three techniques of
interest. First, we used monolingual syntac-
tic paraphrases to provide syntactic variety
to the source training set sentences. Sec-
ond, we trained two language models: a
small in-domain model and a large out-of-
domain model. Finally, we made use of re-
sults from prior research that shows that cog-
nate pairs can improve word alignments. We
contributed runs translating English to Span-
ish, French, and German using various com-
binations of these techniques.
</bodyText>
<sectionHeader confidence="0.998917" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997737904761905">
Modern Statistical Machine Translation (SMT) sys-
tems are trained on aligned sentences of bilingual
corpora, typically from one domain. When tested on
text from that same domain, such systems demon-
strate state-of-the art performance; however, on
out-of-domain text the results can get significantly
worse. For example, on the WMT 2006 Shared
Task evaluation, the French to English translation
BLEU scores dropped from about 30 to about 20 for
nearly all systems, when tested on News Commen-
tary rather than Europarl (Koehn and Monz, 2006).
Therefore, this year the shared task organizers
have provided 1M words of bilingual News Com-
mentary training data in addition to the Europarl
data (about 30M words), thus challenging the par-
ticipants to experiment with domain adaptation.
Below we describe our domain adaptation exper-
iments, trying to achieve better results on the News
Commentary data. In addition to training on both
data sets, we make use of monolingual syntactic
paraphrases of the English side of the data.
</bodyText>
<sectionHeader confidence="0.9619" genericHeader="method">
2 Monolingual Syntactic Paraphrasing
</sectionHeader>
<bodyText confidence="0.9932335">
In many cases, the testing text contains “phrases”
that are equivalent, but syntactically different from
the phrases learned on training, and the potential for
a high-quality translation is missed. We address this
problem by using nearly equivalent syntactic para-
phrases of the original sentences. Each paraphrased
sentence is paired with the foreign translation that is
associated with the original sentence in the training
data. This augmented training corpus can then be
used to train an SMT system. Alternatively, we can
paraphrase the test sentences making them closer to
the target language syntax.
Given an English sentence, we parse it with the
Stanford parser (Klein and Manning, 2003) and then
generate paraphrases using the following syntactic
transformations:
</bodyText>
<listItem confidence="0.933197666666667">
1. [NP NP1 P NP2] ⇒ [NP NP2 NP1].
inequality in income ⇒ income inequality.
2. [NP NP1 of NP2] ⇒ [NP NP2 poss NP1].
inequality of income ⇒ income’s inequality.
3. NPposs ⇒ NP.
income’s inequality ⇒ income inequality.
4. NPposs ⇒ NPPPof.
income’s inequality ⇒ inequality of income.
5. NPNC ⇒ NPposs.
income inequality ⇒ income’s inequality.
6. NPNC ⇒ NPPP.
income inequality ⇒ inequality in incomes.
</listItem>
<page confidence="0.971997">
212
</page>
<bodyText confidence="0.950558117647059">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 212–215,
Prague, June 2007. c�2007 Association for Computational Linguistics
Sharply rising income inequality has raised the stakes of the economic game.
Sharply rising income inequality has raised the economic game ’s stakes .
Sharply rising income inequality has raised the economic game stakes .
Sharply rising inequality of income has raised the stakes of the economic game .
Sharply rising inequality of income has raised the economic game ’s stakes.
Sharply rising inequality of income has raised the economic game stakes.
Sharply rising inequality of incomes has raised the stakes of the economic game .
Sharply rising inequality of incomes has raised the economic game ’s stakes.
Sharply rising inequality of incomes has raised the economic game stakes.
Sharply rising inequality in income has raised the stakes of the economic game .
Sharply rising inequality in income has raised the economic game ’s stakes.
Sharply rising inequality in income has raised the economic game stakes.
Sharply rising inequality in incomes has raised the stakes of the economic game .
Sharply rising inequality in incomes has raised the economic game ’s stakes.
Sharply rising inequality in incomes has raised the economic game stakes.
</bodyText>
<tableCaption confidence="0.987268">
Table 1: Sample sentence and automatically generated paraphrases. Paraphrased NCs are in italics.
</tableCaption>
<figure confidence="0.59051525">
7. remove that where optional
I think that he is right ==&gt;. I think he is right.
8. add that where optional
I think he is right ==&gt;. I think that he is right.
</figure>
<bodyText confidence="0.985489533333333">
where:
N1=“beef”, N2=“import ban lifting”, (b) N1=“beef
import”, N2=“ban lifting”, and (c) N1=“beef import
ban”, N2=“lifting”. For every split, we issue exact
phrase queries to the Google search engine using
the following patterns:
While the first four and the last two transfor-
mations are purely syntactic, (5) and (6) are not.
The algorithm must determine whether a possessive
marker is feasible for (5) and must choose the cor-
rect preposition for (6). In either case, for noun com-
pounds (NCs) of length 3 or more, it also needs to
choose the position to modify, e.g., inquiry’s com-
mittee chairman vs. inquiry committee’s chairman.
In order to ensure accuracy of the paraphrases,
we use statistics gathered from the Web, using a
variation of the approaches presented in Lapata and
Keller (2004) and Nakov and Hearst (2005). We use
patterns to generate possible prepositional or copula
paraphrases in the context of the preceding and the
following word in the sentence, First we split the
NC into two parts N1 and N2 in all possible ways,
e.g., beef import ban lifting would be split as: (a)
&amp;quot;lt N1 poss N2 rt&amp;quot;
&amp;quot;lt N2 prep det Ni rt&amp;quot;
&amp;quot;lt N2 that be det Ni rt&amp;quot;
&amp;quot;lt N2 that be prep det Ni rt&amp;quot;
where: lt is the word preceding N1 in the original
sentence or empty if none, rt is the word following
N2 in the original sentence or empty if none, poss
is a possessive marker (’s or ’), that is that, which
or who, be is is or are, det is a determiner (the, a,
an, or none), prep is one of the 8 prepositions used
by Lauer (1995) for semantic interpretation of NCs:
about, at, for, from, in, of, on, and with, and Ni can
be either N1, or N1 with the number of its last word
changed from singular/plural to plural/singular.
For all splits, we collect the number of page hits
for each instantiation of each pattern, filtering out
the paraphrases whose page hit count is less than 10.
We then calculate the total number of page hits H for
all paraphrases (for all splits and all patterns), and
retain those ones whose page hits count is at least
10% of H. Note that this allows for multiple para-
phrases of an NC. If no paraphrases are retained, we
</bodyText>
<table confidence="0.960028333333333">
poss possessive marker: ’ or ’s;
P preposition;
NPPP NP with internal PP-attachment;
NPPPof NP with internal PP headed by of;
NPposs NP with internal possessive marker;
NPNC NP that is a Noun Compound.
</table>
<page confidence="0.996893">
213
</page>
<bodyText confidence="0.999721545454546">
repeat the above procedure with lt set to the empty
string. If there are still no good paraphrases, we set
the rt to the empty string. If this does not help ei-
ther, we make a final attempt, by setting both lt and
rt to the empty string.
Table 1 shows the paraphrases for a sample sen-
tence. We can see that income inequality is para-
phrased as inequality of income, inequality of in-
comes, inequality in income and inequality in in-
comes; also economic game’s stakes becomes eco-
nomic game stakes and stakes of the economic game.
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999795">
Table 2 shows a summary of our submissions: the
official runs are marked with a *. For our experi-
ments, we used the baseline system, provided by the
organizers, which we modified in different ways, as
described below.
</bodyText>
<subsectionHeader confidence="0.998065">
3.1 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.997769">
All our systems were trained on both corpora.
</bodyText>
<listItem confidence="0.984911545454546">
• Language models. We used two language
models (LM) – a small in-domain one (trained
on News Commentary) and a big out-of-domain
one (trained on Europarl). For example, for EN
--+ ES (from English to Spanish), on the low-
ercased tuning data set, using in-domain LM
only achieved a BLEU of 0.332910, while us-
ing both LMs yielded 0.354927, a significant
effect.
• Cognates. Previous research has found that
using cognates can help get better word align-
</listItem>
<bodyText confidence="0.948962125">
ments (and ultimately better MT results), espe-
cially in case of a small training set. We used
the method described in (Kondrak et al., 2003)
in order to extract cognates from the two data
sets. We then added them as sentence pairs to
the News Commentary corpus before training
the word alignment models1 for ucb3, ucb4 and
ucb5.
</bodyText>
<footnote confidence="0.896272">
1Following (Kondrak et al., 2003), we considered words of
length 4 or more, we required the length ratio to be between
10 and 10
7 7 , and we accepted as potential cognates all pairs for
which the longest common subsequence ratio (LCSR) was 0.58
or more. We repeated 3 times the cognate pairs extracted from
the Europarl, and 4 times the ones from News Commentary.
</footnote>
<listItem confidence="0.88085375">
• Phrases. The ucb5 system uses the Europarl
data in order to learn an additional phrase ta-
ble and an additional lexicalized re-ordering
model.
</listItem>
<subsectionHeader confidence="0.999781">
3.2 Paraphrasing the Training Set
</subsectionHeader>
<bodyText confidence="0.999984870967742">
In two of our experiments (ucb3, ucb4 and ucb5),
we used a paraphrased version of the training News
Commentary data, using all rules (1)-(8). We trained
two separate MT systems: one on the original cor-
pus, and another one on the paraphrased version.
We then used both resulting lexicalized re-ordering
models and a merged phrase table with extra para-
meters: if a phrase appeared in both phrase tables,
it now had 9 instead of 5 parameters (4 from each
table, plus a phrase penalty), and if it was in one
of the phrase tables only, the 4 missing parameters
were filled with 1e-40.
The ucb5 system is also trained on Europarl,
yielding a third lexicalized re-ordering model and
adding 4 new parameters to the phrase table entries.
Unfortunately, longer sentences (up to 100 to-
kens, rather than 40), longer phrases (up to 10 to-
kens, rather than 7), two LMs (rather than just
one), higher-order LMs (order 7, rather than 3),
multiple higher-order lexicalized re-ordering mod-
els (up to 3), etc. all contributed to increased sys-
tem’s complexity, and, as a result, time limitations
prevented us from performing minimum-error-rate
training (MERT) (Och, 2003) for ucb3, ucb4 and
ucb5. Therefore, we used the MERT parameter val-
ues from ucb1 instead, e.g. the first 4 phrase weights
of ucb1 were divided by two, copied twice and used
in ucb3 as the first 8 phrase-table parameters. The
extra 4 parameters of ucb5 came from training a sep-
arate MT system on the Europarl data (scaled ac-
cordingly).
</bodyText>
<subsectionHeader confidence="0.999922">
3.3 Paraphrasing the Test Set
</subsectionHeader>
<bodyText confidence="0.999958666666667">
In some of our experiments (ucb2 and ucb4), given
a test sentence, we generated the single most-likely
paraphrase, which makes it syntactically closer to
Spanish and French. Unlike English, which makes
extensive use of noun compounds, these languages
strongly prefer connecting the nouns with a preposi-
tion (and less often turning a noun into an adjective).
Therefore, we paraphrased all NCs using preposi-
tions, by applying rules (4) and (6). In addition, we
</bodyText>
<page confidence="0.994919">
214
</page>
<table confidence="0.998455636363636">
Languages System LM size Paraphrasing Cognates? Extra phrases MERT
News Europarl train? test? Europarl finished?
EN --� ES ucb1* 3 5 +
ucb2 3 5 + +
ucb3 5 7 + +
ucb4 5 7 + + +
ucb5 5 7 + + +
EN FR ucb3 5 7 + +
ucb4* 5 7 + + +
EN DE ucb1* 5 7 + +
ucb2 5 7 + + +
</table>
<tableCaption confidence="0.982657">
Table 2: Summary of our submissions. All runs are for the News Commentary test data. The official
submissions are marked with a star.
</tableCaption>
<bodyText confidence="0.993515">
applied rule (8), since its Spanish/French equivalent
que (as well as the German daß) is always obliga-
tory. These transformations affected 927 out of the
2007 test sentences. We also used this transformed
data set when translating to German (however, Ger-
man uses NCs as much as English does).
</bodyText>
<subsectionHeader confidence="0.966208">
3.4 Other Non-standard Settings
</subsectionHeader>
<bodyText confidence="0.999989333333333">
Below we discuss some non-standard settings that
differ from the ones suggested by the organizers in
their baseline system. First, following Birch et al.
(2006), who found that higher-order LMs give bet-
ter results2, we used a 5-gram LM for News Com-
mentary, and 7-gram LM for Europarl (as opposed
to 3-gram, as done normally). Second, for all runs
we trained our systems on all sentences of length up
to 100 (rather than 40, as suggested in the baseline
system). Third, we used a maximum phrase length
limit of 10 (rather than 7, as typically done). Fourth,
we used both a lexicalized and distance-based re-
ordering models (as opposed to lexicalized only, as
in the baseline system). Finally, while we did not
use any resources other than the ones provided by
the shared task organizers, we made use of Web fre-
quencies when paraphrasing the training corpus, as
explained above.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9352175">
We have presented various approaches to domain
adaptation and their combinations. Unfortunately,
2They used a 5-gram LM trained on Europarl, but we
pushed the idea further, using a 7-gram LM with a Kneser-Ney
smoothing.
computational complexity and time limitations pre-
vented us from doing proper MERT for the interest-
ing more complex systems. We plan to do a proper
MERT training and to study the impact of the indi-
vidual components in isolation.
Acknowledgements: This work supported in part
by NSF DBI-0317510.
</bodyText>
<sectionHeader confidence="0.99805" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998914">
Alexandra Birch, Chris Callison-Burch, Miles Osborne, and
Philipp Koehn. 2006. Constraining the phrase-based, joint
probability statistical translation model. In Proc. of Work-
shop on Statistical Machine Translation, pages 154–157.
Dan Klein and Christopher Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL ’03.
Philipp Koehn and Christof Monz. 2006. Manual and auto-
matic evaluation of machine translation between european
languages. In Proceedings on the Workshop on Statistical
Machine Translation, pages 102–121.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight. 2003.
Cognates can improve statistical translation models. In Pro-
ceedings of NAACL, pages 46–48.
Mirella Lapata and Frank Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised web-based
models for a range of nlp tasks. In Proceedings of HLT-
NAACL ’04.
Mark Lauer. 1995. Corpus statistics meet the noun compound:
some empirical results. In Proceedings of ACL ’95.
Preslav Nakov and Marti Hearst. 2005. Search engine statistics
beyond the n-gram: Application to noun compound bracket-
ing. In Proceedings of CoNLL ’05.
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL, pages
160–167.
</reference>
<page confidence="0.999146">
215
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.510420">
<title confidence="0.967646">UCB System Description for the WMT 2007 Shared Task</title>
<author confidence="0.676851">Preslav</author>
<affiliation confidence="0.96759">EECS, CS University of California at</affiliation>
<address confidence="0.881729">Berkeley, CA</address>
<email confidence="0.987645">nakov@cs.berkeley.edu</email>
<author confidence="0.996363">Marti</author>
<affiliation confidence="0.998341">School of University of California at</affiliation>
<address confidence="0.892907">Berkeley, CA</address>
<email confidence="0.998789">hearst@ischool.berkeley.edu</email>
<abstract confidence="0.999129285714286">For the WMT 2007 shared task, the UC Berkeley team employed three techniques of interest. First, we used monolingual syntactic paraphrases to provide syntactic variety to the source training set sentences. Second, we trained two language models: a small in-domain model and a large out-ofdomain model. Finally, we made use of results from prior research that shows that cognate pairs can improve word alignments. We contributed runs translating English to Spanish, French, and German using various combinations of these techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Constraining the phrase-based, joint probability statistical translation model.</title>
<date>2006</date>
<booktitle>In Proc. of Workshop on Statistical Machine Translation,</booktitle>
<pages>154--157</pages>
<contexts>
<context position="11972" citStr="Birch et al. (2006)" startWordPosition="2030" endWordPosition="2033">Table 2: Summary of our submissions. All runs are for the News Commentary test data. The official submissions are marked with a star. applied rule (8), since its Spanish/French equivalent que (as well as the German daß) is always obligatory. These transformations affected 927 out of the 2007 test sentences. We also used this transformed data set when translating to German (however, German uses NCs as much as English does). 3.4 Other Non-standard Settings Below we discuss some non-standard settings that differ from the ones suggested by the organizers in their baseline system. First, following Birch et al. (2006), who found that higher-order LMs give better results2, we used a 5-gram LM for News Commentary, and 7-gram LM for Europarl (as opposed to 3-gram, as done normally). Second, for all runs we trained our systems on all sentences of length up to 100 (rather than 40, as suggested in the baseline system). Third, we used a maximum phrase length limit of 10 (rather than 7, as typically done). Fourth, we used both a lexicalized and distance-based reordering models (as opposed to lexicalized only, as in the baseline system). Finally, while we did not use any resources other than the ones provided by th</context>
</contexts>
<marker>Birch, Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Alexandra Birch, Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Constraining the phrase-based, joint probability statistical translation model. In Proc. of Workshop on Statistical Machine Translation, pages 154–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL ’03.</booktitle>
<contexts>
<context position="2584" citStr="Klein and Manning, 2003" startWordPosition="395" endWordPosition="398">, but syntactically different from the phrases learned on training, and the potential for a high-quality translation is missed. We address this problem by using nearly equivalent syntactic paraphrases of the original sentences. Each paraphrased sentence is paired with the foreign translation that is associated with the original sentence in the training data. This augmented training corpus can then be used to train an SMT system. Alternatively, we can paraphrase the test sentences making them closer to the target language syntax. Given an English sentence, we parse it with the Stanford parser (Klein and Manning, 2003) and then generate paraphrases using the following syntactic transformations: 1. [NP NP1 P NP2] ⇒ [NP NP2 NP1]. inequality in income ⇒ income inequality. 2. [NP NP1 of NP2] ⇒ [NP NP2 poss NP1]. inequality of income ⇒ income’s inequality. 3. NPposs ⇒ NP. income’s inequality ⇒ income inequality. 4. NPposs ⇒ NPPPof. income’s inequality ⇒ inequality of income. 5. NPNC ⇒ NPposs. income inequality ⇒ income’s inequality. 6. NPNC ⇒ NPPP. income inequality ⇒ inequality in incomes. 212 Proceedings of the Second Workshop on Statistical Machine Translation, pages 212–215, Prague, June 2007. c�2007 Associa</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<contexts>
<context position="1373" citStr="Koehn and Monz, 2006" startWordPosition="207" endWordPosition="210">h, French, and German using various combinations of these techniques. 1 Introduction Modern Statistical Machine Translation (SMT) systems are trained on aligned sentences of bilingual corpora, typically from one domain. When tested on text from that same domain, such systems demonstrate state-of-the art performance; however, on out-of-domain text the results can get significantly worse. For example, on the WMT 2006 Shared Task evaluation, the French to English translation BLEU scores dropped from about 30 to about 20 for nearly all systems, when tested on News Commentary rather than Europarl (Koehn and Monz, 2006). Therefore, this year the shared task organizers have provided 1M words of bilingual News Commentary training data in addition to the Europarl data (about 30M words), thus challenging the participants to experiment with domain adaptation. Below we describe our domain adaptation experiments, trying to achieve better results on the News Commentary data. In addition to training on both data sets, we make use of monolingual syntactic paraphrases of the English side of the data. 2 Monolingual Syntactic Paraphrasing In many cases, the testing text contains “phrases” that are equivalent, but syntact</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proceedings on the Workshop on Statistical Machine Translation, pages 102–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
<author>Daniel Marcu</author>
<author>Kevin Knight</author>
</authors>
<title>Cognates can improve statistical translation models.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>46--48</pages>
<contexts>
<context position="8382" citStr="Kondrak et al., 2003" startWordPosition="1398" endWordPosition="1401">s were trained on both corpora. • Language models. We used two language models (LM) – a small in-domain one (trained on News Commentary) and a big out-of-domain one (trained on Europarl). For example, for EN --+ ES (from English to Spanish), on the lowercased tuning data set, using in-domain LM only achieved a BLEU of 0.332910, while using both LMs yielded 0.354927, a significant effect. • Cognates. Previous research has found that using cognates can help get better word alignments (and ultimately better MT results), especially in case of a small training set. We used the method described in (Kondrak et al., 2003) in order to extract cognates from the two data sets. We then added them as sentence pairs to the News Commentary corpus before training the word alignment models1 for ucb3, ucb4 and ucb5. 1Following (Kondrak et al., 2003), we considered words of length 4 or more, we required the length ratio to be between 10 and 10 7 7 , and we accepted as potential cognates all pairs for which the longest common subsequence ratio (LCSR) was 0.58 or more. We repeated 3 times the cognate pairs extracted from the Europarl, and 4 times the ones from News Commentary. • Phrases. The ucb5 system uses the Europarl d</context>
</contexts>
<marker>Kondrak, Marcu, Knight, 2003</marker>
<rawString>Grzegorz Kondrak, Daniel Marcu, and Kevin Knight. 2003. Cognates can improve statistical translation models. In Proceedings of NAACL, pages 46–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>The web as a baseline: Evaluating the performance of unsupervised web-based models for a range of nlp tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL ’04.</booktitle>
<contexts>
<context position="5420" citStr="Lapata and Keller (2004)" startWordPosition="853" endWordPosition="856">s to the Google search engine using the following patterns: While the first four and the last two transformations are purely syntactic, (5) and (6) are not. The algorithm must determine whether a possessive marker is feasible for (5) and must choose the correct preposition for (6). In either case, for noun compounds (NCs) of length 3 or more, it also needs to choose the position to modify, e.g., inquiry’s committee chairman vs. inquiry committee’s chairman. In order to ensure accuracy of the paraphrases, we use statistics gathered from the Web, using a variation of the approaches presented in Lapata and Keller (2004) and Nakov and Hearst (2005). We use patterns to generate possible prepositional or copula paraphrases in the context of the preceding and the following word in the sentence, First we split the NC into two parts N1 and N2 in all possible ways, e.g., beef import ban lifting would be split as: (a) &amp;quot;lt N1 poss N2 rt&amp;quot; &amp;quot;lt N2 prep det Ni rt&amp;quot; &amp;quot;lt N2 that be det Ni rt&amp;quot; &amp;quot;lt N2 that be prep det Ni rt&amp;quot; where: lt is the word preceding N1 in the original sentence or empty if none, rt is the word following N2 in the original sentence or empty if none, poss is a possessive marker (’s or ’), that is that, wh</context>
</contexts>
<marker>Lapata, Keller, 2004</marker>
<rawString>Mirella Lapata and Frank Keller. 2004. The web as a baseline: Evaluating the performance of unsupervised web-based models for a range of nlp tasks. In Proceedings of HLTNAACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Corpus statistics meet the noun compound: some empirical results.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL ’95.</booktitle>
<contexts>
<context position="6146" citStr="Lauer (1995)" startWordPosition="1002" endWordPosition="1003">ext of the preceding and the following word in the sentence, First we split the NC into two parts N1 and N2 in all possible ways, e.g., beef import ban lifting would be split as: (a) &amp;quot;lt N1 poss N2 rt&amp;quot; &amp;quot;lt N2 prep det Ni rt&amp;quot; &amp;quot;lt N2 that be det Ni rt&amp;quot; &amp;quot;lt N2 that be prep det Ni rt&amp;quot; where: lt is the word preceding N1 in the original sentence or empty if none, rt is the word following N2 in the original sentence or empty if none, poss is a possessive marker (’s or ’), that is that, which or who, be is is or are, det is a determiner (the, a, an, or none), prep is one of the 8 prepositions used by Lauer (1995) for semantic interpretation of NCs: about, at, for, from, in, of, on, and with, and Ni can be either N1, or N1 with the number of its last word changed from singular/plural to plural/singular. For all splits, we collect the number of page hits for each instantiation of each pattern, filtering out the paraphrases whose page hit count is less than 10. We then calculate the total number of page hits H for all paraphrases (for all splits and all patterns), and retain those ones whose page hits count is at least 10% of H. Note that this allows for multiple paraphrases of an NC. If no paraphrases a</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Corpus statistics meet the noun compound: some empirical results. In Proceedings of ACL ’95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: Application to noun compound bracketing.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL ’05.</booktitle>
<contexts>
<context position="5448" citStr="Nakov and Hearst (2005)" startWordPosition="858" endWordPosition="861"> using the following patterns: While the first four and the last two transformations are purely syntactic, (5) and (6) are not. The algorithm must determine whether a possessive marker is feasible for (5) and must choose the correct preposition for (6). In either case, for noun compounds (NCs) of length 3 or more, it also needs to choose the position to modify, e.g., inquiry’s committee chairman vs. inquiry committee’s chairman. In order to ensure accuracy of the paraphrases, we use statistics gathered from the Web, using a variation of the approaches presented in Lapata and Keller (2004) and Nakov and Hearst (2005). We use patterns to generate possible prepositional or copula paraphrases in the context of the preceding and the following word in the sentence, First we split the NC into two parts N1 and N2 in all possible ways, e.g., beef import ban lifting would be split as: (a) &amp;quot;lt N1 poss N2 rt&amp;quot; &amp;quot;lt N2 prep det Ni rt&amp;quot; &amp;quot;lt N2 that be det Ni rt&amp;quot; &amp;quot;lt N2 that be prep det Ni rt&amp;quot; where: lt is the word preceding N1 in the original sentence or empty if none, rt is the word following N2 in the original sentence or empty if none, poss is a possessive marker (’s or ’), that is that, which or who, be is is or are,</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compound bracketing. In Proceedings of CoNLL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="10264" citStr="Och, 2003" startWordPosition="1721" endWordPosition="1722">sing parameters were filled with 1e-40. The ucb5 system is also trained on Europarl, yielding a third lexicalized re-ordering model and adding 4 new parameters to the phrase table entries. Unfortunately, longer sentences (up to 100 tokens, rather than 40), longer phrases (up to 10 tokens, rather than 7), two LMs (rather than just one), higher-order LMs (order 7, rather than 3), multiple higher-order lexicalized re-ordering models (up to 3), etc. all contributed to increased system’s complexity, and, as a result, time limitations prevented us from performing minimum-error-rate training (MERT) (Och, 2003) for ucb3, ucb4 and ucb5. Therefore, we used the MERT parameter values from ucb1 instead, e.g. the first 4 phrase weights of ucb1 were divided by two, copied twice and used in ucb3 as the first 8 phrase-table parameters. The extra 4 parameters of ucb5 came from training a separate MT system on the Europarl data (scaled accordingly). 3.3 Paraphrasing the Test Set In some of our experiments (ucb2 and ucb4), given a test sentence, we generated the single most-likely paraphrase, which makes it syntactically closer to Spanish and French. Unlike English, which makes extensive use of noun compounds, </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>