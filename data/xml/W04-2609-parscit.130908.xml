<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000173">
<title confidence="0.971008">
Models for the Semantic Classification of Noun Phrases
</title>
<author confidence="0.941327">
Roxana Girju
</author>
<affiliation confidence="0.911881666666667">
Department of Computer Science
Baylor University
Waco, Texas
</affiliation>
<email confidence="0.989767">
girju@cs.baylor.edu
</email>
<author confidence="0.7609235">
Dan Moldovan, Adriana Badulescu,
Marta Tatu, and Daniel Antohe
</author>
<affiliation confidence="0.895849">
Computer Science Department
University of Texas at Dallas
Dallas, Texas
</affiliation>
<email confidence="0.998865">
moldovan@utdallas.edu
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999764">
This paper presents an approach for detecting
semantic relations in noun phrases. A learning
algorithm, called semantic scattering, is used
to automatically label complex nominals, gen-
itives and adjectival noun phrases with the cor-
responding semantic relation.
</bodyText>
<sectionHeader confidence="0.906222" genericHeader="keywords">
1 Problem description
</sectionHeader>
<bodyText confidence="0.999770888888889">
This paper is about the automatic labeling of semantic
relations in noun phrases (NPs).
The semantic relations are the underlying relations be-
tween two concepts expressed by words or phrases. We
distinguish here between semantic relations and semantic
roles. Semantic roles are always between verbs (or nouns
derived from verbs) and other constituents (run quickly,
went to the store, computer maker), whereas semantic
relations can occur between any constituents, for exam-
ple in complex nominals (malaria mosquito (CAUSE)),
genitives (girl’s mouth (PART-WHOLE)), prepositional
phrases attached to nouns (man at the store (LOCATIVE)),
or discourse level (The bus was late. As a result, I missed
my appointment (CAUSE)). Thus, in a sense, semantic
relations are more general than semantic roles and many
semantic role types will appear on our list of semantic
relations.
The following NP level constructions are consid-
ered here (cf. the classifications provided by (Quirk
et al.1985) and (Semmelmeyer and Bolander 1992)):
(1) Compound Nominals consisting of two consecutive
nouns (eg night club - a TEMPORAL relation - indicat-
ing that club functions at night), (2) Adjective Noun con-
structions where the adjectival modifier is derived from a
noun (eg musical clock - a MAKE/PRODUCE relation), (3)
Genitives (eg the door of the car - a PART-WHOLE rela-
tion), and (4) Adjective phrases (cf. (Semmelmeyer and
Bolander 1992)) in which the modifier noun is expressed
by a prepositional phrase which functions as an adjective
(eg toy in the box - a LOCATION relation).
Example: “Saturday’s snowfall topped a one-day record
in Hartford, Connecticut, with the total of 12.5 inches,
the weather service said. The storm claimed its fatal-
ity Thursday, when a car which was driven by a college
student skidded on an interstate overpass in the moun-
tains of Virginia and hit a concrete barrier, police said”.
(www.cnn.com - “Record-setting Northeast snowstorm
winding down”, Sunday, December 7, 2003).
There are several semantic relations at the noun phrase
level: (1) Saturday’s snowfall is a genitive encoding a
TEMPORAL relation, (2) one-day record is a TOPIC noun
compound indicating that record is about one-day snow-
ing - an ellipsis here, (3) record in Hartford is an adjective
phrase in a LOCATION relation, (4) total of 12.5 inches
is an of-genitive that expresses MEASURE, (5) weather
service is a noun compound in a TOPIC relation, (6) car
which was driven by a college student encodes a THEME
semantic role in an adjectival clause, (7) college student is
a compound nominal in a PART-WHOLE/MEMBER-OF re-
lation, (8) interstate overpass is a LOCATION noun com-
pound, (9) mountains of Virginia is an of-genitive show-
ing a PART-WHOLE/PLACE-AREA and LOCATION rela-
tion, (10) concrete barrier is a noun compound encoding
PART-WHOLE/STUFF-OF.
</bodyText>
<subsectionHeader confidence="0.999521">
1.1 List of Semantic Relations
</subsectionHeader>
<bodyText confidence="0.999907731707317">
After many iterations over a period of time we identified a
set of semantic relations that cover a large majority of text
semantics. Table 1 lists these relations, their definitions,
examples, and some references. Most of the time, the
semantic relations are encoded by lexico-syntactic pat-
terns that are highly ambiguous. One pattern can express
a number of semantic relations, its disambiguation be-
ing provided by the context or world knowledge. Often
semantic relations are not disjoint or mutually exclusive,
two or more appearing in the same lexical construct. This
is called semantic blend (Quirk et al.1985). For example,
the expression “Texas city” contains both a LOCATION as
well as a PART-WHOLE relation.
Other researchers have identified other sets of seman-
tic relations (Levi 1979), (Uanderwende 1994), (Sowa
1994), (Baker, Fillmore, and Lowe 1998), (Rosario and
Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and
Charniak 2000), (Gildea and Jurafsky 2002), (Gildea
and Palmer 2002). Our list contains the most frequently
used semantic relations we have observed on a large cor-
pus.
Besides the work on semantic roles, considerable in-
terest has been shown in the automatic interpretation of
complex nominals, and especially of compound nomi-
nals. The focus here is to determine the semantic re-
lations that hold between different concepts within the
same phrase, and to analyze the meaning of these com-
pounds. Several approaches have been proposed for em-
pirical noun-compound interpretation, such as syntactic
analysis based on statistical techniques (Lauer and Dras
1994), (Pustejovsky et al. 1993). Another popular ap-
proach focuses on the interpretation of the underlying se-
mantics. Many researchers that followed this approach
relied mostly on hand-coded rules (Finin 1980), (Uan-
derwende 1994). More recently, (Rosario and Hearst
2001), (Rosario, Hearst, and Fillmore 2002), (Lapata
2002) have proposed automatic methods that analyze and
detect noun compounds relations from text. (Rosario and
Hearst 2001) focused on the medical domain making use
of a lexical ontology and standard machine learning tech-
niques.
</bodyText>
<sectionHeader confidence="0.996944" genericHeader="introduction">
2 Approach
</sectionHeader>
<subsectionHeader confidence="0.999806">
2.1 Basic Approach
</subsectionHeader>
<bodyText confidence="0.999673260869565">
We approach the problem top-down, namely identify
and study first the characteristics or feature vectors of
each noun phrase linguistic pattern, then develop mod-
els for their semantic classification. This is in contrast to
our prior approach ( (Girju, Badulescu, and Moldovan
2003a)) when we studied one relation at a time, and
learned constraints to identify only that relation. We
study the distribution of the semantic relations across dif-
ferent NP patterns and analyze the similarities and dif-
ferences among resulting semantic spaces. We define a
semantic space as the set of semantic relations an NP con-
struction can encode. We aim at uncovering the general
aspects that govern the NP semantics, and thus delineate
the semantic space within clusters of semantic relations.
This process has the advantage of reducing the annotation
effort, a time consuming activity. Instead of manually an-
notating a corpus for each semantic relation, we do it only
for each syntactic pattern and get a clear view of its se-
mantic space. This syntactico-semantic approach allows
us to explore various NP semantic classification models
in a unified way.
This approach stemmed from our desire to answer
questions such as:
</bodyText>
<listItem confidence="0.999512714285714">
1. What influences the semantic interpretation of various
linguistic constructions?
2. Is there only one interpretation system/model that
works best for all types of expressions at all syntactic lev-
els? and
3. What parameters govern the models capable of seman-
tic interpretation of various syntactic constructions?
</listItem>
<subsectionHeader confidence="0.997974">
2.2 Semantic Relations at NP level
</subsectionHeader>
<bodyText confidence="0.999928636363636">
It is well understood and agreed in linguistics that con-
cepts can be represented in many ways using various con-
structions at different syntactic levels. This is in part why
we decided to take the syntactico-semantic approach that
analyzes semantic relations at different syntactic levels
of representation. In this paper we focus only on the be-
havior of semantic relations at NP level. A thorough un-
derstanding of the syntactic and semantic characteristics
of NPs provides valuable insights into defining the most
representative feature vectors that ultimately drive the
discriminating learning models.
</bodyText>
<subsectionHeader confidence="0.705145">
Complex Nominals
</subsectionHeader>
<bodyText confidence="0.999968538461539">
Levi (Levi 1979) defines complex nominals (CNs) as ex-
pressions that have a head noun preceded by one or more
modifying nouns, or by adjectives derived from nouns
(usually called denominal adjectives). Most importantly
for us, each sequence of nouns, or possibly adjectives and
nouns, has a particular meaning as a whole carrying an
implicit semantic relation; for example, “spoon handle”
(PART-WHOLE) or “musical clock” (MAKE/PRODUCE).
CNs have been studied intensively in linguistics,
psycho-linguistics, philosophy, and computational lin-
guistics for a long time. The semantic interpretation
of CNs proves to be very difficult for a number of rea-
sons. (1) Sometimes the meaning changes with the
head (eg “musical clock” MAKE/PRODUCE, “musical cre-
ation” THEME), other times with the modifier (eg “GM
car” MAKE/PRODUCE, “family car” POSSESSION). (2)
CNs’ interpretation is knowledge intensive and can be id-
iosyncratic. For example, in order to interpret correctly
“GM car” we have to know that GM is a car-producing
company. (3) There can be many possible semantic re-
lations between a given pair of word constituents. For
example, “USA city” can be regarded as a LOCATION as
well as a PART-WHOLE relation. (4) Interpretation of CNs
can be highly context-dependent. For example, “apple
juice seat” can be defined as “seat with apple juice on the
table in front of it” (cf. (Downing 1977)).
</bodyText>
<subsectionHeader confidence="0.584344">
Genitives
</subsectionHeader>
<bodyText confidence="0.865732">
The semantic interpretation of genitive constructions
</bodyText>
<table confidence="0.821728410714286">
No. Semantic Definition / Example
Relation
1 POSSESSION an animate entity possesses (owns) another entity; (family estate; the girl has a new car.), (Vanderwende 1994)
2 KINSHIP an animated entity related by blood, marriage, adoption or strong affinity to another animated entity; (Mary’s daughter;
my sister); (Levi 1979)
3 PROPERTY/ characteristic or quality of an entity/event/state; (red rose; The thunderstorm was awful.); (Levi 1979)
ATTRIBUTE-HOLDER
4 AGENT the doer or instigator of the action denoted by the predicate;
(employee protest; parental approval; The king banished the general.); (Baker, Fillmore, and Lowe 1998)
5 TEMPORAL time associated with an event; (5-o’clock tea; winter training; the store opens at 9 am),
includes DURATION (Navigli and Velardi 2003),
6 DEPICTION- an event/action/entity depicting another event/action/entity; (A picture of my niece.),
DEPICTED
7 PART-WHOLE an entity/event/state is part of another entity/event/state (door knob; door of the car),
(MERONYMY) (Levi 1979), (Dolan et al. 1993),
8 HYPERNYMY an entity/event/state is a subclass of another; (daisy flower; Virginia state; large company, such as Microsoft)
(IS-A) (Levi 1979), (Dolan et al. 1993)
9 ENTAIL an event/state is a logical consequence of another; (snoring entails sleeping)
10 CAUSE an event/state makes another event/state to take place; (malaria mosquitoes; to die of hunger; The earthquake
generated a Tsunami), (Levi 1979)
11 MAKE/PRODUCE an animated entity creates or manufactures another entity; (honey bees; nuclear powerplant; GM makes cars) (Levi 1979)
12 INSTRUMENT an entity used in an event/action as instrument; (pump drainage; the hammer broke the box) (Levi 1979)
13 LOCATION/SPACE spatial relation between two entities or between an event and an entity; includes DIRECTION; (field mouse;
street show; I left the keys in the car), (Levi 1979), (Dolan et al. 1993)
14 PURPOSE a state/action intended to result from a another state/event; (migraine drug; wine glass; rescue mission;
He was quiet in order not to disturb her.) (Navigli and Velardi 2003)
15 SOURCE/FROM place where an entity comes from; (olive oil; I got itfrom China) (Levi 1979)
16 TOPIC an object is a topic of another object; (weather report; construction plan; article about terrorism); (Rosario and Hearst 2001)
17 MANNER a way in which an event is performed or takes place; (hard-working immigrants; enjoy immensely; he died of
cancer); (Blaheta and Charniak 2000)
18 MEANS the means by which an event is performed or takes place; (bus service; Igo to school by bus.) (Quirk et al.1985)
19 ACCOMPANIMENT one/more entities accompanying another entity involved in an event; (meeting with friends; She came with us) (Quirk et al.1985)
20 EXPERIENCER an animated entity experiencing a state/feeling; (Mary was in a state ofpanic.); (Sowa 1994)
21 RECIPIENT an animated entity for which an event is performed; (The eggs are for you) ; includes BENEFICIARY; (Sowa 1994)
22 FREQUENCY number of occurrences of an event; (bi-annual meeting; I take the bus every day); (Sowa 1994)
23 INFLUENCE an entity/event that affects other entity/event; (drug-affectedfamilies; The war has an impact on the economy.);
24 ASSOCIATED WITH an entity/event/state that is in an (undefined) relation with another entity/event/state; (Jazz-associated company;)
25 MEASURE an entity expressing quantity of another entity/event; (cup ofsugar;
70-km distance; centennial rite; The jacket cost $60.)
26 SYNONYMY a word/concept that means the same or nearly the same as another word/concept;
(NAME) (Marry is called Minnie); (Sowa 1994)
27 ANTONYMY a word/concept that is the opposite of another word/concept; (empty is the opposite offull); (Sowa 1994)
28 PROBABILITY OF the quality/state of being probable; likelihood
EXISTENCE (There is little chance of rain tonight); (Sowa 1994)
29 POSSIBILITY the state/condition of being possible; (I might go to Opera tonight); (Sowa 1994)
30 CERTAINTY the state/condition of being certain or without doubt; (He definitely left the house this morning);
31 THEME an entity that is changed/involved by the action/event denoted by the predicate;
(music lover; John opened the door.); (Sowa 1994)
32 RESULT the inanimate result of the action/event denoted by the predicate; includes EFFECT and PRODUCT.
(combustion gases; Ifinished the task completely.); (Sowa 1994)
33 STIMULUS stimulus of the action or event denoted by the predicate (We saw [the painting].
I sensed [the eagerness] in him. I can see [that you are feeling great].) (Baker, Fillmore, and Lowe 1998)
34 EXTENT the change of status on a scale (by a percentage or by a value) of some entity;
(The price of oil increased [ten percent]. Oil’s price increased by [ten percent]. ); (Blaheta and Charniak 2000)
35 PREDICATE expresses the property associated with the subject or the object through the verb;
(He feels [sleepy]. They elected him [treasurer]. ) (Blaheta and Charniak 2000)
</table>
<tableCaption confidence="0.8108485">
Table 1: A list of semantic relations at various syntactic levels (including NP level), their definitions, some examples,
and references.
</tableCaption>
<bodyText confidence="0.9957312">
is considered problematic by linguists because they
involve an implicit relation that seems to allow for
a large variety of relational interpretations; for ex-
ample: “John’s car”-POSSESSOR-POSSESSEE, “Mary’s
brother”-KINSHIP, “last year’s exhibition”-TEMPORAL,
“a picture of my nice”-DEPICTION-DEPICTED, and “the
desert’s oasis”-PART-WHOLE/PLACE-AREA. A charac-
teristic of these constructions is that they are very pro-
ductive, as the construction can be given various inter-
pretations depending on the context. One such example
is “Kate’s book” that can mean the book Kate owns, the
book Kate wrote, or the book Kate is very fond of.
Thus, the features that contribute to the semantic in-
terpretation of genitives are: the nouns’ semantic classes,
the type of genitives, discourse and pragmatic informa-
tion.
Adjective Phrases are prepositional phrases attached to
nouns acting as adjectives (cf. (Semmelmeyer and
Bolander 1992)). Prepositions play an important role
both syntactically and semantically. Semantically speak-
ing, prepositional constructions can encode various se-
mantic relations, their interpretations being provided
most of the time by the underlying context. For instance,
the preposition “with” can encode different semantic re-
lations: (1) It was the girl with blue eyes (MERONYMY),
</bodyText>
<listItem confidence="0.838964">
(2) The baby with the red ribbon is cute (POSSESSION),
(3) The woman with triplets received a lot of attention
(KINSHIP).
</listItem>
<bodyText confidence="0.9999711875">
The conclusion for us is that in addition to the nouns se-
mantic classes, the preposition and the context play im-
portant roles here.
In order to focus our research, we will concentrate for
now only on noun - noun or adjective - noun composi-
tional constructions at NP level, ie those whose mean-
ing can be derived from the meaning of the constituent
nouns (“door knob”, “cup of wine”). We don’t consider
metaphorical names (eg, “ladyfinger”), metonymies (eg,
“Vietnam veteran”), proper names (eg, “John Doe”), and
NPs with coordinate structures in which neither noun is
the head (eg, “player-coach”). However, we check if
the constructions are non-compositional (lexicalized) (the
meaning is a matter of convention; e.g., “soap opera”,
“sea lion”), but only for statistical purposes. Fortunately,
some of these can be identified with the help of lexicons.
</bodyText>
<subsectionHeader confidence="0.998695">
2.3 Corpus Analysis at NP level
</subsectionHeader>
<bodyText confidence="0.9994279">
In order to provide a unified approach for the detection of
semantic relations at different NP levels, we analyzed the
syntactic and semantic behavior of these constructions on
a large open-domain corpora of examples. Our intention
is to answer questions like: (1) What are the semantic re-
lations encoded by the NP-level constructions?, (2) What
is their distribution on a large corpus?, (3) Is there a com-
mon subset of semantic relations that can be fully para-
phrased by all types ofNP constructions?, (4) How many
NPs are lexicalized?
</bodyText>
<subsectionHeader confidence="0.741703">
The data
</subsectionHeader>
<bodyText confidence="0.999986307692308">
We have assembled a corpus from two sources: Wall
Street Journal articles from TREC-9, and eXtended
WordNet glosses (XWN) (http://xwn.hlt.utdallas.edu).
We used XWN 2.0 since all its glosses are syntacti-
cally parsed and their words semantically disambiguated
which saved us considerable amount of time. Table 2
shows for each syntactic category the number of ran-
domly selected sentences from each corpus, the num-
ber of instances found in these sentences, and finally the
number of instances that our group managed to annotate
by hand. The annotation of each example consisted of
specifying its feature vector and the most appropriate se-
mantic relation from those listed in Table 1.
</bodyText>
<sectionHeader confidence="0.752264" genericHeader="method">
Inter-annotator Agreement
</sectionHeader>
<bodyText confidence="0.999835754385965">
The annotators, four PhD students in Computational Se-
mantics worked in groups of two, each group focusing
on one half of the corpora to annotate. Noun - noun
(adjective - noun, respectively) sequences of words were
extracted using the Lauer heuristic (Lauer 1995) which
looks for consecutive pairs of nouns that are neither
preceded nor succeeded by a noun after each sentence
was syntactically parsed with Charniak parser (Charniak
2001) (for XWN we used the gold parse trees). More-
over, they were provided with the sentence in which the
pairs occurred along with their corresponding WordNet
senses. Whenever the annotators found an example en-
coding a semantic relation other than those provided or
they didn’t know what interpretation to give, they had
to tag it as “OTHERS”. Besides the type of relation, the
annotators were asked to provide information about the
order of the modifier and the head nouns in the syntac-
tic constructions if applicable. For instance, in “owner
of car”-POSSESSION the possessor owner is followed by
the possessee car, while in “car ofJohn”-POSSESSION/R
the order is reversed. On average, 30% of the training
examples had the nouns in reverse order.
Most of the time, one instance was tagged with one
semantic relation, but there were also situations in which
an example could belong to more than one relation in the
same context. For example, the genitive “city of USA”
was tagged as a PART-WHOLE/PLACE-AREA relation and
as a LOCATION relation. Overall, there were 608 such
cases in the training corpora. Moreover, the annotators
were asked to indicate if the instance was lexicalized or
not. Also, the judges tagged the NP nouns in the training
corpus with their corresponding WordNet senses.
The annotators’ agreement was measured using the
Kappa statistics, one of the most frequently used mea-
sure of inter-annotator agreement for classification tasks:
, where is the proportion of
times the raters agree and is the probability of
agreement by chance. The K coefficient is 1 if there is
a total agreement among the annotators, and 0 if there is
no agreement other than that expected to occur by chance.
Table 3 shows the semantic relations inter-annotator
agreement on both training and test corpora for each NP
construction. For each construction, the corpus was splint
into 80/20 training/testing ratio after agreement.
We computed the K coefficient only for those instances
tagged with one of the 35 semantic relations. For each
pattern, we also computed the number of pairs that were
tagged with OTHERS by both annotators, over the number
of examples classified in this category by at least one of
the judges, averaged by the number of patterns consid-
ered.
The K coefficient shows a fair to good level of agree-
ment for the training and testing data on the set of 35 re-
lations, taking into consideration the task difficulty. This
can be explained by the instructions the annotators re-
ceived prior to annotation and by their expertise in lexical
semantics. There were many heated discussions as well.
</bodyText>
<table confidence="0.993489857142857">
Wall Street Journal eXtended WordNet 2.0
CNs Genitives Adjective Complex Nominals
Phrases
NN AdjN ’s of NN
No. of sentences 7067 5381 50291 27067 14582 51058
No. of instances 5557 500 2990 4185 3502 12412
No. of annotated instances 2315 383 1816 3404 1341 1651
</table>
<tableCaption confidence="0.961622">
Table 2: Corpus statistics.
</tableCaption>
<table confidence="0.9990012">
Kappa Agreement ( 1 - 35 ) OTHERS
Complex Nominals Genitives Adjective
Phrases
NN Adj N ’s of
0.55 0.68 0.66 0.65 0.67 0.69
</table>
<tableCaption confidence="0.7689125">
Table 3: The inter-annotator agreement on the semantic annotation of the NP constructions considered on both training
and test corpora. For the semantic blend examples, the agreement was done on one of the relations only.
</tableCaption>
<subsectionHeader confidence="0.998254">
2.4 Distribution of Semantic Relations
</subsectionHeader>
<bodyText confidence="0.998989516129032">
Even noun phrase constructions are very productive al-
lowing for a large number of possible interpretations, Ta-
ble 4 shows that a relatively small set of 35 semantic re-
lations covers a significant part of the semantic distribu-
tion of these constructions on a large open-domain cor-
pus. Moreover, the distribution of these relations is de-
pendent on the type of NP construction, each type en-
coding a particular subset. For example, in the case of
of-genitives, there were 21 relations found from the total
of 35 relations considered. The most frequently occur-
ring relations were PART-WHOLE, ATTRIBUTE-HOLDER,
POSSESSION, LOCATION, SOURCE, TOPIC, and THEME.
By comparing the subsets of semantic relations in each
column we can notice that these semantic spaces are not
identical, proving our initial intuition that the NP con-
structions cannot be alternative ways of packing the same
information. Table 4 also shows that there is a subset
of semantic relations that can be fully encoded by all
types of NP constructions. The statistics about the lex-
icalized examples are as follows: N-N (30.01%), Adj-N
(0%), s-genitive (0%), of-genitive (0%), adjective phrase
(1%). From the 30.01% lexicalized noun compounds ,
18% were proper names.
This simple analysis leads to the important conclusion
that the NP constructions must be treated separately as
their semantic content is different. This observation is
also partially consistent with other recent work in lin-
guistics and computational linguistics on the grammatical
variation of the English genitives, noun compounds, and
adjective phrases.
We can draw from here the following conclusions:
</bodyText>
<listItem confidence="0.99924925">
1. Not all semantic relations can be encoded by all NP
syntactic constructions.
2. There are semantic relations that have preferences over
particular syntactic constructions.
</listItem>
<subsectionHeader confidence="0.7264165">
2.5 Models
2.5.1 Mathematical formulation
</subsectionHeader>
<bodyText confidence="0.999098756756757">
Given each NP syntactic construction considered, the
goal is to develop a procedure for the automatic label-
ing of the semantic relations they encode. The semantic
relation derives from the lexical, syntactic, semantic and
contextual features of each NP construction.
Semantic classification of syntactic patterns in general
can be formulated as a learning problem, and thus bene-
fit from the theoretical foundation and experience gained
with various learning paradigms. This is a multi-class
classification problem since the output can be one of the
semantic relations in the set. We cast this as a supervised
learning problem where input/ output pairs are available
as training data.
An important first step is to map the characteristics of
each NP construction (usually not numerical) into feature
vectors. Let’s define with the feature vector of an in-
stance and let be the space of all instances; ie .
The multi-class classification is performed by a func-
tion that maps the feature space into a semantic space
,
, where is the set of semantic relations
from Table 1, ie .
Let be the training set of examples or instances
where is the
number of examples each accompanied by its semantic
relation label . The problem is to decide which semantic
relation to assign to a new, unseen example . In or-
der to classify a given set of examples (members of ),
one needs some kind of measure of the similarity (or the
difference) between any two given members of . Most
of the times it is difficult to explicitly define this func-
tion, since can contain features with numerical as well
as non-numerical values.
Note that the features, thus space , vary from an NP
pattern to another and the classification function will be
pattern dependent. The novelty of this learning problem
is the feature space and the nature ofthe discriminating
</bodyText>
<table confidence="0.999907690476191">
No. Semantic Frequency Examples
Relations
CNs Genitives Adjective
Phrases
NN AdjN ’s of
1 POSSESSION 2.91 9.44 14.55 3.89 1.47 “family estate”
2 KINSHIP 0 0 7.94 3.23 0.20 “woman with triplets”
3 ATTRIBUTE-HOLDER 12.38 7.34 8.96 10.77 4.09 “John’s coldness”
4 AGENT 4.21 10.49 9.75 0.98 3.46 “the investigation of the crew”
5 TEMPORAL 0.82 0 11.96 0.53 7.97 “last year’s exhibition”
6 DEPICTION-DEPICTED 0 0 1.49 2.86 0.20 “a picture of my nice”
7 PART-WHOLE 19.68 10.84 27.38 31.70 5.56 “the girl’s mouth”
8 IS-A (HYPERNYMY) 3.95 1.05 0 0.04 1.15 “city ofDallas”
9 ENTAIL 0 0 0 0 0
10 CAUSE 0.08 0 0.23 0.57 1.04 “malaria mosquitoes”
11 MAKE/PRODUCE 1.56 2.09 0.86 0.98 0.63 “shoefactory”
12 INSTRUMENT 0.65 0 0.07 0.16 0.94 “pump drainage”
13 LOCATION/SPACE 7.51 20.28 1.57 3.89 18.04 “university in Texas”
14 PURPOSE 6.69 3.84 0.07 0.20 5.45 “migraine drug”
15 SOURCE 1.69 11.53 2.51 5.85 2.94 “president ofBolivia”
16 TOPIC 4.04 4.54 0.15 4.95 9.13 “museum ofart”
17 MANNER 0.40 0 0 0 0.20 “performance with style”
18 MEANS 0.26 0 0 0 0.10 “bus service”
19 ACCOMPANIMENT 0 0 0 0 0.42 “meeting with friends”
20 EXPERIENCER 0.08 2.09 0.07 0.16 2.30 “victim of lung disease”
21 RECIPIENT 1.04 0 3.54 2.66 2.51 “Josephine’s reward”
22 FREQUENCY 0.04 7.00 0 0 0.52 “bi-annual meeting”
23 INFLUENCE 0.13 0.35 0 0 0.73 “drug-affectedfamilies”
24 ASSOCIATED WITH 1.00 0.35 0.15 0.16 2.51 “John’s lawyer”
25 MEASURE 1.47 0.35 0.07 13.88 1.36 “hundred of dollars”
26 SYNONYMY 0 0 0 0 0
27 ANTONYMY 0 0 0 0 0
28 PROBABILITY 0 0 0 0 0
29 POSSIBILITY 0 0 0 0 0
30 CERTAINTY 0 0 0 0 0
31 THEME 6.51 1.75 3.30 6.26 9.75 “car salesman”
32 RESULT 0.48 0 0.07 0.41 1.36 “combustion gas”
33 STIMULUS 0 0 0 0 0
34 EXTENT 0 0 0 0 0
35 PREDICATE 0.04 0 0 0 0.10
OTHERS 23.19 6.64 5.19 5.77 15.73 “airmail stamp”
Total no. of examples 100 (2302) 100 (286) 100 (1271) 100 (2441) 100 (953)
</table>
<tableCaption confidence="0.98603">
Table 4: The distribution of the semantic relations on the annotated corpus after agreement. The number in parentheses
represent number of examples for a particular pattern.
</tableCaption>
<subsectionHeader confidence="0.941328">
2.5.2 Feature space
</subsectionHeader>
<bodyText confidence="0.983346636363636">
An essential aspect of our approach below is the
word sense disambiguation (WSD) of the content words
(nouns, verbs, adjectives and adverbs). Using a state-
of-the-art open-text WSD system, each word is mapped
into its corresponding WordNet 2.0 sense. When dis-
ambiguating each word, the WSD algorithm takes into
account the surrounding words, and this is one important
way through which context gets to play a role in the se-
mantic classification of NPs.
So far, we have identified and experimented with the
following NP features:
</bodyText>
<listItem confidence="0.998850125">
1. Semantic class of head noun specifies
the WordNet sense (synset) of the head noun and
implicitly points to all its hypernyms. It is extracted
automatically via a word sense disambiguation module.
The NP semantics is influenced heavily by the meaning
of the noun constituents. Example: “car manufacturer”
is a kind of manufacturer that MAKES/PRODUCES cars.
2. Semantic class of modifier noun
</listItem>
<bodyText confidence="0.8631714">
specifies the WordNet synset of the modifier noun. In
case the modifier is a denominal adjective, we take the
synset of the noun from which the adjective is derived.
Example: “musical clock” - MAKE/PRODUCE, and
“electric clock”- INSTRUMENT.
</bodyText>
<subsectionHeader confidence="0.958454">
2.5.3 Learning Models
</subsectionHeader>
<bodyText confidence="0.999545238095238">
Several learning models can be used to provide the dis-
criminating function . So far we have experimented
with three models: (1) semantic scattering, (2) decision
trees, and (3) naive Bayes. The first is described below,
the other two are fairly well known from the machine
learning literature.
Semantic Scattering. This is a new model developed
by us particularly useful for the classification of com-
pound nominals without nominalization. The se-
mantic relation in this case derives from the semantics of
the two noun concepts participating in these constructions
as well as the surrounding context.
Model Formulation. Let us define with
and the sets of semantic class features (ie,
function derived for each syntactic pattern.
WordNet synsets) of the NP modifiers and, respectively
NP heads (ie features 2 and 1). The compound nominal
semantics is distinctly specified by the feature pair ,
written shortly as . Given feature pair , the proba-
bility of a semantic relation r is , de-
fined as the ratio between the number of occurrences of a
relation r in the presence of feature pair over the num-
ber of occurrences of feature pair in the corpus. The
most probable relation is
Since the number of possible noun synsets combina-
tions is large, it is difficult to measure the quantities
and on a training corpus to calculate
. One way of approximating the feature vector
is to perform a semantic generalization, by replacing
the synsets with their most general hypernyms, followed
by a series of specializations for the purpose of eliminat-
ing ambiguities in the training data. There are 9 noun hi-
erarchies, thus only 81 possible combinations at the most
general level. Table 5 shows a row of the probability ma-
trix for . Each entry, for
which there is more than one relation, is scattered into
other subclasses through an iterative process till there is
only one semantic relation per line. This can be achieved
by specializing the feature pair’s semantic classes with
their immediate WordNet hyponyms. The iterative pro-
cess stops when new training data does not bring any im-
provements (see Table 6).
</bodyText>
<subsectionHeader confidence="0.568487">
2.5.4 Overview of the Preliminary Results
</subsectionHeader>
<bodyText confidence="0.999990333333333">
The f-measure results obtained so far are summarized
in Table 7. Overall, these results are very encouraging
given the complexity of the problem.
</bodyText>
<sectionHeader confidence="0.645311" genericHeader="method">
2.5.5 Error Analysis
</sectionHeader>
<bodyText confidence="0.999976736842105">
An important way of improving the performance of a
system is to do a detailed error analysis of the results.
We have analyzed the sources of errors in each case and
found out that most of them are due to (in decreasing or-
der of importance): (1) errors in automatic sense disam-
biguation, (2) missing combinations of features that occur
in testing but not in the training data, (3) levels of special-
ization are too high, (4) errors caused by metonymy, (6)
errors in the modifier-head order, and others. These er-
rors could be substantially decreased with more research
effort.
A further analysis of the data led us to consider a differ-
ent criterion of classification that splits the examples into
nominalizations and non-nominalizations. The reason is
that nominalization noun phrases seem to call for a differ-
ent set of learning features than the non-nominalization
noun phrases, taking advantage of the underlying verb-
argument structure. Details about this approach are pro-
vided in (Girju et al. 2004)).
</bodyText>
<sectionHeader confidence="0.998119" genericHeader="conclusions">
3 Applications
</sectionHeader>
<bodyText confidence="0.999056807692308">
Semantic relations occur with high frequency in open
text, and thus, their discovery is paramount for many ap-
plications. One important application is Question An-
swering. A powerful method of answering more difficult
questions is to associate to each question the semantic re-
lation that reflects the meaning of that question and then
search for that semantic relation over the candidates of
semantically tagged paragraphs. Here is an example.
Q. Where have nuclear incidents occurred? From the
question stem word where, we know the question asks
for a LOCATION which is found in the complex nomi-
nal “Three Mile Island”-LOCATION of the sentence “The
Three Mile Island nuclear incident caused a DOE policy
crisis”, leading to the correct answer “Three Mile Island”.
Q. What did the factory in Howell Michigan make?
The verb make tells us to look for a MAKE/PRODUCE
relation which is found in the complex nominal “car
factory”-MAKE/PRODUCE of the text: “The car factory in
Howell Michigan closed on Dec 22, 1991” which leads to
answer car.
Another important application is building semantically
rich ontologies. Last but not least, the discovery of
text semantic relations can improve syntactic parsing and
even WSD which in turn affects directly the accuracy of
other NLP modules and applications. We consider these
applications for future work.
</bodyText>
<sectionHeader confidence="0.998594" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999768619047619">
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of COLLING/ACL,
Canada.
D. Blaheta and E. Charniak. 2000. Assigning function
tags to parsed text. In Proceedings of the 1st Annual
Meeting of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL), Seattle,
WA.
E. Charniak. 2001. Immediate-head parsing for language
models. In Proceedings ofACL, Toulouse, France.
W. Dolan, L. Vanderwende, and S. Richardson. 1993.
Automatically deriving structured KBs from on-line
dictionaries. In Proceedings of the Pacific Association
for Computational Linguistics Conference.
P. Downing. 1977. On the creation and use of English
compound nouns. Language, 53(4), 810-842.
T. Finin. 1980. The Semantic Interpretation of Com-
pound Nominals. Ph.D dissertation, University of Illi-
nois, Urbana, Illinois.
D. Gildea and D. Jurafsky. 2002. Automatic Labeling of
Semantic Roles. In Computational Linguistics, 28(3).
</reference>
<table confidence="0.964026">
Relation no.
1 2 3 6 7 11 13 15 16 21 25 the rest
0.06103 0.11268 0.00939 0.04225 0.39437 0.01878 0.03286 0.25822 0.04694 0.01878 0.00469 0
</table>
<tableCaption confidence="0.9463835">
Table 5: Sample row from the conditional probability table where the feature pair is entity-entity. The numbers in the
top row identify the semantic relations (as in Table 4).
</tableCaption>
<table confidence="0.9188597">
Level Level 1 Level 2 Level 3 Level 4
Number of modifier 9 52 70 122
features
Number head 9 46 47 47
features
No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734
Number of features 1 152 181 225
with only one relation
Average number of 2.7692 1.291 1.1765 1.144
non-zero relations per line
</table>
<tableCaption confidence="0.710347">
Table 6: Statistics for the semantic class features by level of specialization.
</tableCaption>
<table confidence="0.998876375">
Syntactic Semantic Decision Naive
Pattern Scattering Tree Bayes
Complex NN
Nominals
AdjN
Genitives ’S
Of 39.94% 34.72%
Adjective Phrases
</table>
<tableCaption confidence="0.7839305">
Table 7: F-measure results for the semantic classification of NP patterns obtained with four learning models on a
corpus with an 80/20 training/testing ratio. “NA” means not available.
</tableCaption>
<reference confidence="0.999800759259259">
D. Gildea and M. Palmer. 2002. The Necessity of Parsing
for Predicate Argument Recognition. In Proceedings
of the 40th Meeting of the Association for Computa-
tional Linguistics (ACL 2002).
R. Girju, A. Badulescu, and D. Moldovan. 2003. Learn-
ing Semantic Constraints for the Automatic Discovery
of Part-Whole Relations. In Proceedings of the Human
Language Technology Conference (HLT-03), Canada.
R. Girju, A.M. Giuglea, M. Olteanu, O. Fortu, and D.
Moldovan. 2004. Support Vector Machines Applied to
the Classification of Semantic Relations in Nominal-
ized Noun Phrases. In Proceedings of HLT/NAACL
2004 - Computational Lexical Semantics workshop,
Boston, MA.
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding
Semantic Annotation to the Penn TreeBank. In Pro-
ceedings of the Human Language Technology Confer-
ence (HLT 2002), California.
M. Lapata. 2002. The Disambiguation of Nominalisa-
tions. In Computational Linguistics 28:3, 357-388.
M. Lauer and M. Dras. 1994. A probabilistic model of
compound nouns. In Proceedings of the 7th Australian
Joint Conference on AI.
M. Lauer. 1995. Designing Statistical Language Learn-
ers: Experiments on Compound Nouns. In PhD thesis,
Macquarie University, Sidney.
Judith Levi. 1979. The Syntax and Semantics of Com-
plex Nominals. New York: Academic Press.
R. Navigli and P. Velardi. 2003. Ontology Learning and
Its Application to Automated Terminology Transla-
tion. In IEEE Intelligent Systems.
J. Pustejovsky, S. Bergler, and P. Anick. 1993. Lexical
semantic techniques for corpus analysis. In Computa-
tional Linguistics, 19(2).
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A comprehensive grammar of english language, Long-
man, Harlow.
B. Rosario and M. Hearst. 2001. Classifying the Se-
mantic Relations in Noun Compounds via a Domain-
Specific Lexical Hierarchy. In the Proceedings of the
2001 Conference on Empirical Methods in Natural
Language Processing, (EMNLP 2001), Pittsburgh, PA.
B. Rosario, M. Hearst, and C. Fillmore. 2002. The De-
scent of Hierarchy, and Selection in Relational Seman-
tics. In the Proceedings of the Association for Compu-
tational Linguistics (ACL-02), University of Pennsyl-
vania.
M. Semmelmeyer and D. Bolander. 1992. The New Web-
ster’s Grammar Guide. Lexicon Publications, Inc.
J. F. Sowa. 1994. Conceptual Structures: Information
Processing in Mind and Machine. Addison Wesley.
L. Vanderwende. 1994. Algorithm for automatic in-
terpretation of noun sequences. In Proceedings of
COLING-94, pg. 782-788.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999901">Models for the Semantic Classification of Noun Phrases</title>
<author confidence="0.993169">Roxana</author>
<affiliation confidence="0.997321">Department of Computer</affiliation>
<address confidence="0.5306005">Baylor Waco,</address>
<email confidence="0.999893">girju@cs.baylor.edu</email>
<author confidence="0.998973">Dan Moldovan</author>
<author confidence="0.998973">Adriana</author>
<affiliation confidence="0.8859665">and Computer Science University of Texas at Dallas,</affiliation>
<email confidence="0.999914">moldovan@utdallas.edu</email>
<abstract confidence="0.993375385057471">This paper presents an approach for detecting semantic relations in noun phrases. A learning algorithm, called semantic scattering, is used to automatically label complex nominals, genitives and adjectival noun phrases with the corresponding semantic relation. 1 Problem description This paper is about the automatic labeling of semantic relations in noun phrases (NPs). The semantic relations are the underlying relations between two concepts expressed by words or phrases. We here between relations Semantic roles are always between verbs (or nouns from verbs) and other constituents quickly, to the store, computer whereas semantic relations can occur between any constituents, for examin complex nominals mosquito mouth prepositional attached to nouns at the store discourse level bus was late. As a result, I missed appointment Thus, in a sense, semantic relations are more general than semantic roles and many semantic role types will appear on our list of semantic relations. The following NP level constructions are considered here (cf. the classifications provided by (Quirk et al.1985) and (Semmelmeyer and Bolander 1992)): (1) Compound Nominals consisting of two consecutive (eg club a indicating that club functions at night), (2) Adjective Noun constructions where the adjectival modifier is derived from a (eg clock a (3) (eg door of the car a relation), and (4) Adjective phrases (cf. (Semmelmeyer and Bolander 1992)) in which the modifier noun is expressed by a prepositional phrase which functions as an adjective in the box a snowfall a record Connecticut, with total of 12.5 service The storm claimed its fatal- Thursday, when car which was driven by a college on interstate overpass mounof Virginia hit concrete police said”. (www.cnn.com - “Record-setting Northeast snowstorm winding down”, Sunday, December 7, 2003). There are several semantic relations at the noun phrase (1) snowfall a genitive encoding a (2) record a compound indicating that record is about one-day snow- an ellipsis here, (3) in Hartford an adjective in a (4) of 12.5 inches an of-genitive that expresses (5) a noun compound in a (6) was driven by a college student a role in an adjectival clause, (7) student compound nominal in a re- (8) overpass a com- (9) of Virginia an of-genitive showa rela- (10) barrier a noun compound encoding 1.1 List of Semantic Relations After many iterations over a period of time we identified a set of semantic relations that cover a large majority of text semantics. Table 1 lists these relations, their definitions, examples, and some references. Most of the time, the semantic relations are encoded by lexico-syntactic patterns that are highly ambiguous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This called blend et al.1985). For example, expression contains both a as a Other researchers have identified other sets of semantic relations (Levi 1979), (Uanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsky et al. 1993). Another popular approach focuses on the interpretation of the underlying semantics. Many researchers that followed this approach relied mostly on hand-coded rules (Finin 1980), (Uanderwende 1994). More recently, (Rosario and Hearst 2001), (Rosario, Hearst, and Fillmore 2002), (Lapata 2002) have proposed automatic methods that analyze and detect noun compounds relations from text. (Rosario and Hearst 2001) focused on the medical domain making use of a lexical ontology and standard machine learning techniques. 2 Approach 2.1 Basic Approach We approach the problem top-down, namely identify and study first the characteristics or feature vectors of each noun phrase linguistic pattern, then develop models for their semantic classification. This is in contrast to our prior approach ( (Girju, Badulescu, and Moldovan 2003a)) when we studied one relation at a time, and learned constraints to identify only that relation. We study the distribution of the semantic relations across different NP patterns and analyze the similarities and difamong resulting We define a semantic space as the set of semantic relations an NP construction can encode. We aim at uncovering the general aspects that govern the NP semantics, and thus delineate the semantic space within clusters of semantic relations. This process has the advantage of reducing the annotation effort, a time consuming activity. Instead of manually annotating a corpus for each semantic relation, we do it only for each syntactic pattern and get a clear view of its semantic space. This syntactico-semantic approach allows us to explore various NP semantic classification models in a unified way. This approach stemmed from our desire to answer questions such as: 1. What influences the semantic interpretation of various linguistic constructions? 2. Is there only one interpretation system/model that works best for all types of expressions at all syntactic levels? and 3. What parameters govern the models capable of semantic interpretation of various syntactic constructions? 2.2 Semantic Relations at NP level It is well understood and agreed in linguistics that concepts can be represented in many ways using various constructions at different syntactic levels. This is in part why we decided to take the syntactico-semantic approach that analyzes semantic relations at different syntactic levels of representation. In this paper we focus only on the behavior of semantic relations at NP level. A thorough understanding of the syntactic and semantic characteristics of NPs provides valuable insights into defining the most vectors ultimately drive the discriminating learning models. Complex Nominals Levi (Levi 1979) defines complex nominals (CNs) as expressions that have a head noun preceded by one or more modifying nouns, or by adjectives derived from nouns (usually called denominal adjectives). Most importantly for us, each sequence of nouns, or possibly adjectives and nouns, has a particular meaning as a whole carrying an semantic relation; for example, or CNs have been studied intensively in linguistics, psycho-linguistics, philosophy, and computational linfor a long time. The interpretation of CNs proves to be very difficult for a number of reasons. (1) Sometimes the meaning changes with the (eg creother times with the modifier (eg (2) CNs’ interpretation is knowledge intensive and can be idiosyncratic. For example, in order to interpret correctly we have to know that GM is a car-producing company. (3) There can be many possible semantic relations between a given pair of word constituents. For can be regarded as a as a (4) Interpretation of CNs can be highly context-dependent. For example, “apple juice seat” can be defined as “seat with apple juice on the table in front of it” (cf. (Downing 1977)).</abstract>
<title confidence="0.95178">Genitives The semantic interpretation of genitive constructions</title>
<author confidence="0.909402">Semantic Relation Definition Example</author>
<note confidence="0.961386028571429">1 POSSESSION animate entity possesses (owns) another entity; estate; the girl has a new (Vanderwende 1994) 2 KINSHIP animated entity related by blood, marriage, adoption or strong affinity to another animated entity; (Levi 1979) 3 or quality of an entity/event/state; rose; The thunderstorm was (Levi 1979) 4 AGENT instigator of the action denoted by the predicate; protest; parental approval; The king banished the (Baker, Fillmore, and Lowe 1998) 5 TEMPORAL associated with an event; tea; winter training; the store opens at 9 and Velardi 2003), 6 DEPICTED event/action/entity depicting another event/action/entity; picture of my 7 entity/event/state is part of another entity/event/state knob; door of the (Levi 1979), (Dolan et al. 1993), 8 HYPERNYMY entity/event/state is a subclass of another; flower; Virginia state; large company, such as (Levi 1979), (Dolan et al. 1993) 9 ENTAIL event/state is a logical consequence of another; entails 10 CAUSE event/state makes another event/state to take place; mosquitoes; to die of hunger; The earthquake a (Levi 1979) 11 animated entity creates or manufactures another entity; bees; nuclear powerplant; GM makes (Levi 1979) 12 INSTRUMENT entity used in an event/action as instrument; drainage; the hammer broke the (Levi 1979) 13 relation between two entities or between an event and an entity; includes show; I left the keys in the (Levi 1979), (Dolan et al. 1993) 14 PURPOSE state/action intended to result from a another state/event; was quiet in order not to disturb (Navigli and Velardi 2003) 15 where an entity comes from; got itfrom (Levi 1979) 16 TOPIC object is a topic of another object; report; construction plan; article about (Rosario and Hearst 2001) 17 MANNER way in which an event is performed or takes place; immigrants; enjoy immensely; he died of (Blaheta and Charniak 2000) 18 MEANS means by which an event is performed or takes place; service; Igo to school by (Quirk et al.1985) 19 ACCOMPANIMENT entities accompanying another entity involved in an event; with friends; She came with (Quirk et al.1985) 20 EXPERIENCER animated entity experiencing a state/feeling; was in a state (Sowa 1994) 21 RECIPIENT animated entity for which an event is performed; eggs are for ; includes (Sowa 1994) 22 FREQUENCY of occurrences of an event; meeting; I take the bus every (Sowa 1994) 23 INFLUENCE entity/event that affects other entity/event; The war has an impact on the 24 ASSOCIATED WITH entity/event/state that is in an (undefined) relation with another entity/event/state; 25 MEASURE entity expressing quantity of another entity/event; distance; centennial rite; The jacket cost 26 SYNONYMY a word/concept that means the same or nearly the same as another word/concept; is called (Sowa 1994) 27 ANTONYMY word/concept that is the opposite of another word/concept; is the opposite (Sowa 1994) 28 PROBABILITY OF EXISTENCE the quality/state of being probable; likelihood is little chance of rain (Sowa 1994) 29 POSSIBILITY state/condition of being possible; might go to Opera (Sowa 1994) 30 CERTAINTY state/condition of being certain or without doubt; definitely left the house this 31 THEME an entity that is changed/involved by the action/event denoted by the predicate; lover; John opened the (Sowa 1994) 32 RESULT inanimate result of the action/event denoted by the predicate; includes gases; Ifinished the task (Sowa 1994) 33 STIMULUS of the action or event denoted by the predicate saw [the painting]. sensed [the eagerness] in him. I can see [that you are feeling great].) Fillmore, and Lowe 1998</note>
<abstract confidence="0.987604438461538">34 EXTENT the change of status on a scale (by a percentage or by a value) of some entity; price of oil increased [ten percent]. Oil’s price increased by [ten percent]. (Blaheta and Charniak 2000) 35 PREDICATE expresses the property associated with the subject or the object through the verb; feels [sleepy]. They elected him [treasurer]. (Blaheta and Charniak 2000) Table 1: A list of semantic relations at various syntactic levels (including NP level), their definitions, some examples, and references. is considered problematic by linguists because they involve an implicit relation that seems to allow for a large variety of relational interpretations; for exyear’s picture of my and A characteristic of these constructions is that they are very productive, as the construction can be given various interpretations depending on the context. One such example that can mean the book Kate owns, the book Kate wrote, or the book Kate is very fond of. Thus, the features that contribute to the semantic interpretation of genitives are: the nouns’ semantic classes, the type of genitives, discourse and pragmatic information. Phrases prepositional phrases attached to nouns acting as adjectives (cf. (Semmelmeyer Bolander 1992)). Prepositions play an important role both syntactically and semantically. Semantically speakprepositional constructions can encode various semantic relations, their interpretations being provided most of the time by the underlying context. For instance, preposition can encode different semantic re- (1) It was the girl eyes The baby red ribbon is cute The woman received a lot of attention The conclusion for us is that in addition to the nouns semantic classes, the preposition and the context play important roles here. In order to focus our research, we will concentrate for only on noun noun or adjective noun composiconstructions NP level, ie those whose meaning can be derived from the meaning of the constituent of We don’t consider names (eg, metonymies (eg, proper names (eg, and NPs with coordinate structures in which neither noun is head (eg, However, we check if constructions are (lexicalized) is a matter of convention; e.g., but only for statistical purposes. Fortunately, some of these can be identified with the help of lexicons. 2.3 Corpus Analysis at NP level In order to provide a unified approach for the detection of semantic relations at different NP levels, we analyzed the syntactic and semantic behavior of these constructions on a large open-domain corpora of examples. Our intention to answer questions like: (1) are the semantic reencoded by the NP-level (2) their distribution on a large (3) there a common subset of semantic relations that can be fully paraby all types ofNP (4) many NPs are lexicalized? The data We have assembled a corpus from two sources: Wall Street Journal articles from TREC-9, and eXtended WordNet glosses (XWN) (http://xwn.hlt.utdallas.edu). We used XWN 2.0 since all its glosses are syntactically parsed and their words semantically disambiguated which saved us considerable amount of time. Table 2 shows for each syntactic category the number of randomly selected sentences from each corpus, the number of instances found in these sentences, and finally the number of instances that our group managed to annotate by hand. The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation from those listed in Table 1. Inter-annotator Agreement The annotators, four PhD students in Computational Semantics worked in groups of two, each group focusing on one half of the corpora to annotate. Noun noun (adjective noun, respectively) sequences of words were extracted using the Lauer heuristic (Lauer 1995) which looks for consecutive pairs of nouns that are neither preceded nor succeeded by a noun after each sentence was syntactically parsed with Charniak parser (Charniak 2001) (for XWN we used the gold parse trees). Moreover, they were provided with the sentence in which the pairs occurred along with their corresponding WordNet senses. Whenever the annotators found an example encoding a semantic relation other than those provided or they didn’t know what interpretation to give, they had tag it as Besides the type of relation, the annotators were asked to provide information about the order of the modifier and the head nouns in the syntacconstructions if applicable. For instance, in possessor followed by possessee while in the order is reversed. On average, 30% of the training examples had the nouns in reverse order. Most of the time, one instance was tagged with one semantic relation, but there were also situations in which an example could belong to more than one relation in the context. For example, the genitive of tagged as a and a Overall, there were 608 such cases in the training corpora. Moreover, the annotators were asked to indicate if the instance was lexicalized or not. Also, the judges tagged the NP nouns in the training corpus with their corresponding WordNet senses. The annotators’ agreement was measured using the Kappa statistics, one of the most frequently used measure of inter-annotator agreement for classification tasks: , where is the proportion of times the raters agree and is the probability of agreement by chance. The K coefficient is 1 if there is a total agreement among the annotators, and 0 if there is no agreement other than that expected to occur by chance. Table 3 shows the semantic relations inter-annotator agreement on both training and test corpora for each NP construction. For each construction, the corpus was splint into 80/20 training/testing ratio after agreement. We computed the K coefficient only for those instances tagged with one of the 35 semantic relations. For each pattern, we also computed the number of pairs that were with both annotators, over the number of examples classified in this category by at least one of the judges, averaged by the number of patterns considered. The K coefficient shows a fair to good level of agreement for the training and testing data on the set of 35 relations, taking into consideration the task difficulty. This can be explained by the instructions the annotators received prior to annotation and by their expertise in lexical semantics. There were many heated discussions as well.</abstract>
<note confidence="0.8794135">Wall Street Journal eXtended WordNet 2.0 CNs Genitives Adjective Phrases Complex Nominals NN AdjN ’s of NN No. of sentences 7067 5381 50291 27067 14582 51058 No. of instances 5557 500 2990 4185 3502 12412 No. of annotated instances 2315 383 1816 3404 1341 1651 Table 2: Corpus statistics. Kappa Agreement ( 1 - 35 ) OTHERS</note>
<title confidence="0.610614">Complex Nominals Genitives Phrases</title>
<abstract confidence="0.98515062195122">NN Adj N ’s of 0.55 0.68 0.66 0.65 0.67 0.69 Table 3: The inter-annotator agreement on the semantic annotation of the NP constructions considered on both training and test corpora. For the semantic blend examples, the agreement was done on one of the relations only. 2.4 Distribution of Semantic Relations Even noun phrase constructions are very productive allowing for a large number of possible interpretations, Table 4 shows that a relatively small set of 35 semantic relations covers a significant part of the semantic distribution of these constructions on a large open-domain corpus. Moreover, the distribution of these relations is dependent on the type of NP construction, each type encoding a particular subset. For example, in the case of of-genitives, there were 21 relations found from the total of 35 relations considered. The most frequently occurrelations were and By comparing the subsets of semantic relations in each column we can notice that these semantic spaces are not identical, proving our initial intuition that the NP constructions cannot be alternative ways of packing the same information. Table 4 also shows that there is a subset of semantic relations that can be fully encoded by all of NP constructions. The statistics about the lexare as follows: N-N (30.01%), Adj-N (0%), s-genitive (0%), of-genitive (0%), adjective phrase (1%). From the 30.01% lexicalized noun compounds , 18% were proper names. This simple analysis leads to the important conclusion that the NP constructions must be treated separately as their semantic content is different. This observation is also partially consistent with other recent work in linguistics and computational linguistics on the grammatical variation of the English genitives, noun compounds, and adjective phrases. We can draw from here the following conclusions: 1. Not all semantic relations can be encoded by all NP syntactic constructions. 2. There are semantic relations that have preferences over particular syntactic constructions. 2.5 Models 2.5.1 Mathematical formulation Given each NP syntactic construction considered, the goal is to develop a procedure for the automatic labeling of the semantic relations they encode. The semantic relation derives from the lexical, syntactic, semantic and contextual features of each NP construction. Semantic classification of syntactic patterns in general can be formulated as a learning problem, and thus benefit from the theoretical foundation and experience gained with various learning paradigms. This is a multi-class classification problem since the output can be one of the semantic relations in the set. We cast this as a supervised learning problem where input/ output pairs are available as training data. An important first step is to map the characteristics of each NP construction (usually not numerical) into feature Let’s define with the feature vector of an instance and let be the space of all instances; ie The multi-class classification is performed by a function that maps the feature space into a semantic space , , where is the set of semantic relations from Table 1, ie . Let be the training set of examples or where is the number of examples each accompanied by its semantic relation label . The problem is to decide which semantic to assign to a new, unseen example . In der to classify a given set of examples (members of ), one needs some kind of measure of the similarity (or the difference) between any two given members of . Most of the times it is difficult to explicitly define this function, since can contain features with numerical as well as non-numerical values. Note that the features, thus space , vary from an NP pattern to another and the classification function will be pattern dependent. The novelty of this learning problem is the feature space and the nature ofthe discriminating No. Relations Frequency Examples CNs Genitives Phrases NN AdjN ’s of</abstract>
<note confidence="0.873662868421053">1 POSSESSION 2.91 9.44 14.55 3.89 1.47 2 KINSHIP 0 0 7.94 3.23 0.20 with 3 12.38 7.34 8.96 10.77 4.09 4 AGENT 4.21 10.49 9.75 0.98 3.46 investigation of the 5 TEMPORAL 0.82 0 11.96 0.53 7.97 year’s 6 0 0 1.49 2.86 0.20 picture of my 7 19.68 10.84 27.38 31.70 5.56 girl’s 8 3.95 1.05 0 0.04 1.15 9 ENTAIL 0 0 0 0 0 10 CAUSE 0.08 0 0.23 0.57 1.04 11 1.56 2.09 0.86 0.98 0.63 12 INSTRUMENT 0.65 0 0.07 0.16 0.94 13 7.51 20.28 1.57 3.89 18.04 in 14 PURPOSE 6.69 3.84 0.07 0.20 5.45 15 SOURCE 1.69 11.53 2.51 5.85 2.94 16 TOPIC 4.04 4.54 0.15 4.95 9.13 17 MANNER 0.40 0 0 0 0.20 with 18 MEANS 0.26 0 0 0 0.10 19 ACCOMPANIMENT 0 0 0 0 0.42 with 20 EXPERIENCER 0.08 2.09 0.07 0.16 2.30 of lung 21 RECIPIENT 1.04 0 3.54 2.66 2.51 22 FREQUENCY 0.04 7.00 0 0 0.52 23 INFLUENCE 0.13 0.35 0 0 0.73 24 ASSOCIATED WITH 1.00 0.35 0.15 0.16 2.51 25 MEASURE 1.47 0.35 0.07 13.88 1.36 of 26 SYNONYMY 0 0 0 0 0 27 ANTONYMY 0 0 0 0 0 28 PROBABILITY 0 0 0 0 0 29 POSSIBILITY 0 0 0 0 0 30 CERTAINTY 0 0 0 0 0 31 THEME 6.51 1.75 3.30 6.26 9.75 32 RESULT 0.48 0 0.07 0.41 1.36 33 STIMULUS 0 0 0 0 0 34 EXTENT 0 0 0 0 0 35 PREDICATE 0.04 0 0 0 0.10 OTHERS 23.19 6.64 5.19 5.77 15.73 Total no. of examples 100 (2302) 100 (286) 100 (1271) 100 (2441) 100 (953) Table 4: The distribution of the semantic relations on the annotated corpus after agreement. The number in parentheses</note>
<abstract confidence="0.999438161016949">represent number of examples for a particular pattern. 2.5.2 Feature space An essential aspect of our approach below is the sense disambiguation of the content words (nouns, verbs, adjectives and adverbs). Using a stateof-the-art open-text WSD system, each word is mapped its corresponding 2.0 When disambiguating each word, the WSD algorithm takes into account the surrounding words, and this is one important through which to play a role in the semantic classification of NPs. So far, we have identified and experimented with the NP Semantic class of head noun the WordNet sense (synset) of the head noun and implicitly points to all its hypernyms. It is extracted automatically via a word sense disambiguation module. The NP semantics is influenced heavily by the meaning the noun constituents. Example: a kind of manufacturer that 2. Semantic class of modifier noun specifies the WordNet synset of the modifier noun. In case the modifier is a denominal adjective, we take the synset of the noun from which the adjective is derived. and 2.5.3 Learning Models Several learning models can be used to provide the discriminating function . So far we have experimented with three models: (1) semantic scattering, (2) decision trees, and (3) naive Bayes. The first is described below, the other two are fairly well known from the machine learning literature. Scattering. is a new model developed us particularly useful for the classification of comnominals without nominalization. The mantic relation in this case derives from the semantics of the two noun concepts participating in these constructions as well as the surrounding context. Let us define with and the sets of semantic class features (ie, function derived for each syntactic pattern. WordNet synsets) of the NP modifiers and, respectively NP heads (ie features 2 and 1). The compound nominal semantics is distinctly specified by the feature pair , shortly as . Given feature pair , the probaof a semantic relation , fined as the ratio between the number of occurrences of a the presence of feature pair over the number of occurrences of feature pair in the corpus. The most probable relation is Since the number of possible noun synsets combinations is large, it is difficult to measure the quantities and on a training corpus to . One way of approximating the feature vector is to perform a semantic generalization, by replacing the synsets with their most general hypernyms, followed by a series of specializations for the purpose of eliminating ambiguities in the training data. There are 9 noun hierarchies, thus only 81 possible combinations at the most level. Table 5 shows a row of the probability matrix for . Each entry, there is more than one relation, is other subclasses through an iterative process till there is only one semantic relation per line. This can be achieved by specializing the feature pair’s semantic classes with their immediate WordNet hyponyms. The iterative process stops when new training data does not bring any improvements (see Table 6). 2.5.4 Overview of the Preliminary Results The f-measure results obtained so far are summarized in Table 7. Overall, these results are very encouraging given the complexity of the problem. 2.5.5 Error Analysis An important way of improving the performance of a system is to do a detailed error analysis of the results. We have analyzed the sources of errors in each case and found out that most of them are due to (in decreasing order of importance): (1) errors in automatic sense disambiguation, (2) missing combinations of features that occur in testing but not in the training data, (3) levels of specialization are too high, (4) errors caused by metonymy, (6) errors in the modifier-head order, and others. These errors could be substantially decreased with more research effort. A further analysis of the data led us to consider a different criterion of classification that splits the examples into nominalizations and non-nominalizations. The reason is that nominalization noun phrases seem to call for a different set of learning features than the non-nominalization noun phrases, taking advantage of the underlying verbargument structure. Details about this approach are provided in (Girju et al. 2004)). 3 Applications Semantic relations occur with high frequency in open text, and thus, their discovery is paramount for many applications. One important application is Question Answering. A powerful method of answering more difficult questions is to associate to each question the semantic relation that reflects the meaning of that question and then search for that semantic relation over the candidates of semantically tagged paragraphs. Here is an example. Where have nuclear incidents occurred? the stem word we know the question asks a is found in the complex nomi- Mile the sentence “The Three Mile Island nuclear incident caused a DOE policy leading to the correct answer Mile Q. What did the factory in Howell Michigan make? verb us to look for a which is found in the complex nominal the text: “The car factory in Howell Michigan closed on Dec 22, 1991” which leads to Another important application is building semantically rich ontologies. Last but not least, the discovery of text semantic relations can improve syntactic parsing and even WSD which in turn affects directly the accuracy of other NLP modules and applications. We consider these applications for future work.</abstract>
<note confidence="0.784688888888889">References C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley Project. In of Canada. D. Blaheta and E. Charniak. 2000. Assigning function to parsed text. In of the 1st Annual Meeting of the North American Chapter of the Associafor Computational Linguistics Seattle, WA. E. Charniak. 2001. Immediate-head parsing for language In Toulouse, France. W. Dolan, L. Vanderwende, and S. Richardson. 1993. Automatically deriving structured KBs from on-line In of the Pacific Association Computational Linguistics P. Downing. 1977. On the creation and use of English nouns. 53(4), 810-842. T. Finin. 1980. The Semantic Interpretation of Com-</note>
<affiliation confidence="0.771725">Nominals. University of Illi-</affiliation>
<address confidence="0.6743245">nois, Urbana, Illinois. D. Gildea and D. Jurafsky. 2002. Automatic Labeling of</address>
<abstract confidence="0.838213176470588">Roles. In 28(3). Relation no. 1 2 3 6 7 11 13 15 16 21 25 the rest 0.06103 0.11268 0.00939 0.04225 0.39437 0.01878 0.03286 0.25822 0.04694 0.01878 0.00469 0 Table 5: Sample row from the conditional probability table where the feature pair is entity-entity. The numbers in the top row identify the semantic relations (as in Table 4). Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization.</abstract>
<title confidence="0.880698666666667">Pattern Scattering Decision Tree Bayes Nominals NN AdjN Genitives ’S Of 39.94% 34.72% Adjective Phrases</title>
<note confidence="0.912282">Table 7: F-measure results for the semantic classification of NP patterns obtained with four learning models on a corpus with an 80/20 training/testing ratio. “NA” means not available. D. Gildea and M. Palmer. 2002. The Necessity of Parsing Predicate Argument Recognition. In of the 40th Meeting of the Association for Computa- Linguistics 2002). R. Girju, A. Badulescu, and D. Moldovan. 2003. Learn-</note>
<title confidence="0.980131">ing Semantic Constraints for the Automatic Discovery</title>
<author confidence="0.904558">In of the Human</author>
<note confidence="0.854846212765958">Technology Conference Canada. R. Girju, A.M. Giuglea, M. Olteanu, O. Fortu, and D. Moldovan. 2004. Support Vector Machines Applied to the Classification of Semantic Relations in Nominal- Noun Phrases. In of HLT/NAACL - Computational Lexical Semantics Boston, MA. P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding Annotation to the Penn TreeBank. In Proceedings of the Human Language Technology Confer- 2002), California. M. Lapata. 2002. The Disambiguation of Nominalisa- In Linguistics 357-388. M. Lauer and M. Dras. 1994. A probabilistic model of nouns. In of the 7th Australian Conference on M. Lauer. 1995. Designing Statistical Language Learn- Experiments on Compound Nouns. In Macquarie University, Sidney. Judith Levi. 1979. The Syntax and Semantics of Complex Nominals. New York: Academic Press. R. Navigli and P. Velardi. 2003. Ontology Learning and Its Application to Automated Terminology Transla- In Intelligent J. Pustejovsky, S. Bergler, and P. Anick. 1993. Lexical techniques for corpus analysis. In Computa- 19(2). R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A comprehensive grammar of english language, Longman, Harlow. B. Rosario and M. Hearst. 2001. Classifying the Semantic Relations in Noun Compounds via a Domain- Lexical Hierarchy. In the of the 2001 Conference on Empirical Methods in Natural (EMNLP 2001), Pittsburgh, PA. B. Rosario, M. Hearst, and C. Fillmore. 2002. The Descent of Hierarchy, and Selection in Relational Seman- In the of the Association for Compu- Linguistics University of Pennsylvania. Semmelmeyer and D. Bolander. 1992. New Web- Grammar Lexicon Publications, Inc. J. F. Sowa. 1994. Conceptual Structures: Information Processing in Mind and Machine. Addison Wesley. L. Vanderwende. 1994. Algorithm for automatic inof noun sequences. In of pg. 782-788.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>C Fillmore</author>
<author>J Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLLING/ACL,</booktitle>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of COLLING/ACL, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blaheta</author>
<author>E Charniak</author>
</authors>
<title>Assigning function tags to parsed text.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="4400" citStr="Blaheta and Charniak 2000" startWordPosition="675" endWordPosition="678">uous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART-WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Uanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on sta</context>
<context position="11707" citStr="Blaheta and Charniak 2000" startWordPosition="1800" endWordPosition="1803"> the keys in the car), (Levi 1979), (Dolan et al. 1993) 14 PURPOSE a state/action intended to result from a another state/event; (migraine drug; wine glass; rescue mission; He was quiet in order not to disturb her.) (Navigli and Velardi 2003) 15 SOURCE/FROM place where an entity comes from; (olive oil; I got itfrom China) (Levi 1979) 16 TOPIC an object is a topic of another object; (weather report; construction plan; article about terrorism); (Rosario and Hearst 2001) 17 MANNER a way in which an event is performed or takes place; (hard-working immigrants; enjoy immensely; he died of cancer); (Blaheta and Charniak 2000) 18 MEANS the means by which an event is performed or takes place; (bus service; Igo to school by bus.) (Quirk et al.1985) 19 ACCOMPANIMENT one/more entities accompanying another entity involved in an event; (meeting with friends; She came with us) (Quirk et al.1985) 20 EXPERIENCER an animated entity experiencing a state/feeling; (Mary was in a state ofpanic.); (Sowa 1994) 21 RECIPIENT an animated entity for which an event is performed; (The eggs are for you) ; includes BENEFICIARY; (Sowa 1994) 22 FREQUENCY number of occurrences of an event; (bi-annual meeting; I take the bus every day); (Sowa</context>
<context position="14017" citStr="Blaheta and Charniak 2000" startWordPosition="2160" endWordPosition="2163">e; (music lover; John opened the door.); (Sowa 1994) 32 RESULT the inanimate result of the action/event denoted by the predicate; includes EFFECT and PRODUCT. (combustion gases; Ifinished the task completely.); (Sowa 1994) 33 STIMULUS stimulus of the action or event denoted by the predicate (We saw [the painting]. I sensed [the eagerness] in him. I can see [that you are feeling great].) (Baker, Fillmore, and Lowe 1998) 34 EXTENT the change of status on a scale (by a percentage or by a value) of some entity; (The price of oil increased [ten percent]. Oil’s price increased by [ten percent]. ); (Blaheta and Charniak 2000) 35 PREDICATE expresses the property associated with the subject or the object through the verb; (He feels [sleepy]. They elected him [treasurer]. ) (Blaheta and Charniak 2000) Table 1: A list of semantic relations at various syntactic levels (including NP level), their definitions, some examples, and references. is considered problematic by linguists because they involve an implicit relation that seems to allow for a large variety of relational interpretations; for example: “John’s car”-POSSESSOR-POSSESSEE, “Mary’s brother”-KINSHIP, “last year’s exhibition”-TEMPORAL, “a picture of my nice”-DE</context>
</contexts>
<marker>Blaheta, Charniak, 2000</marker>
<rawString>D. Blaheta and E. Charniak. 2000. Assigning function tags to parsed text. In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="18326" citStr="Charniak 2001" startWordPosition="2831" endWordPosition="2832">d. The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation from those listed in Table 1. Inter-annotator Agreement The annotators, four PhD students in Computational Semantics worked in groups of two, each group focusing on one half of the corpora to annotate. Noun - noun (adjective - noun, respectively) sequences of words were extracted using the Lauer heuristic (Lauer 1995) which looks for consecutive pairs of nouns that are neither preceded nor succeeded by a noun after each sentence was syntactically parsed with Charniak parser (Charniak 2001) (for XWN we used the gold parse trees). Moreover, they were provided with the sentence in which the pairs occurred along with their corresponding WordNet senses. Whenever the annotators found an example encoding a semantic relation other than those provided or they didn’t know what interpretation to give, they had to tag it as “OTHERS”. Besides the type of relation, the annotators were asked to provide information about the order of the modifier and the head nouns in the syntactic constructions if applicable. For instance, in “owner of car”-POSSESSION the possessor owner is followed by the po</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. Immediate-head parsing for language models. In Proceedings ofACL, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Dolan</author>
<author>L Vanderwende</author>
<author>S Richardson</author>
</authors>
<title>Automatically deriving structured KBs from on-line dictionaries.</title>
<date>1993</date>
<booktitle>In Proceedings of the Pacific Association for Computational Linguistics Conference.</booktitle>
<contexts>
<context position="10277" citStr="Dolan et al. 1993" startWordPosition="1576" endWordPosition="1579">s awful.); (Levi 1979) ATTRIBUTE-HOLDER 4 AGENT the doer or instigator of the action denoted by the predicate; (employee protest; parental approval; The king banished the general.); (Baker, Fillmore, and Lowe 1998) 5 TEMPORAL time associated with an event; (5-o’clock tea; winter training; the store opens at 9 am), includes DURATION (Navigli and Velardi 2003), 6 DEPICTION- an event/action/entity depicting another event/action/entity; (A picture of my niece.), DEPICTED 7 PART-WHOLE an entity/event/state is part of another entity/event/state (door knob; door of the car), (MERONYMY) (Levi 1979), (Dolan et al. 1993), 8 HYPERNYMY an entity/event/state is a subclass of another; (daisy flower; Virginia state; large company, such as Microsoft) (IS-A) (Levi 1979), (Dolan et al. 1993) 9 ENTAIL an event/state is a logical consequence of another; (snoring entails sleeping) 10 CAUSE an event/state makes another event/state to take place; (malaria mosquitoes; to die of hunger; The earthquake generated a Tsunami), (Levi 1979) 11 MAKE/PRODUCE an animated entity creates or manufactures another entity; (honey bees; nuclear powerplant; GM makes cars) (Levi 1979) 12 INSTRUMENT an entity used in an event/action as instru</context>
</contexts>
<marker>Dolan, Vanderwende, Richardson, 1993</marker>
<rawString>W. Dolan, L. Vanderwende, and S. Richardson. 1993. Automatically deriving structured KBs from on-line dictionaries. In Proceedings of the Pacific Association for Computational Linguistics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Downing</author>
</authors>
<title>On the creation and use of English compound nouns.</title>
<date>1977</date>
<journal>Language,</journal>
<volume>53</volume>
<issue>4</issue>
<pages>810--842</pages>
<contexts>
<context position="9175" citStr="Downing 1977" startWordPosition="1421" endWordPosition="1422">s with the modifier (eg “GM car” MAKE/PRODUCE, “family car” POSSESSION). (2) CNs’ interpretation is knowledge intensive and can be idiosyncratic. For example, in order to interpret correctly “GM car” we have to know that GM is a car-producing company. (3) There can be many possible semantic relations between a given pair of word constituents. For example, “USA city” can be regarded as a LOCATION as well as a PART-WHOLE relation. (4) Interpretation of CNs can be highly context-dependent. For example, “apple juice seat” can be defined as “seat with apple juice on the table in front of it” (cf. (Downing 1977)). Genitives The semantic interpretation of genitive constructions No. Semantic Definition / Example Relation 1 POSSESSION an animate entity possesses (owns) another entity; (family estate; the girl has a new car.), (Vanderwende 1994) 2 KINSHIP an animated entity related by blood, marriage, adoption or strong affinity to another animated entity; (Mary’s daughter; my sister); (Levi 1979) 3 PROPERTY/ characteristic or quality of an entity/event/state; (red rose; The thunderstorm was awful.); (Levi 1979) ATTRIBUTE-HOLDER 4 AGENT the doer or instigator of the action denoted by the predicate; (empl</context>
</contexts>
<marker>Downing, 1977</marker>
<rawString>P. Downing. 1977. On the creation and use of English compound nouns. Language, 53(4), 810-842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Finin</author>
</authors>
<title>The Semantic Interpretation of Compound Nominals.</title>
<date>1980</date>
<institution>University of Illinois,</institution>
<location>Urbana, Illinois.</location>
<note>Ph.D dissertation,</note>
<contexts>
<context position="5245" citStr="Finin 1980" startWordPosition="807" endWordPosition="808">automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsky et al. 1993). Another popular approach focuses on the interpretation of the underlying semantics. Many researchers that followed this approach relied mostly on hand-coded rules (Finin 1980), (Uanderwende 1994). More recently, (Rosario and Hearst 2001), (Rosario, Hearst, and Fillmore 2002), (Lapata 2002) have proposed automatic methods that analyze and detect noun compounds relations from text. (Rosario and Hearst 2001) focused on the medical domain making use of a lexical ontology and standard machine learning techniques. 2 Approach 2.1 Basic Approach We approach the problem top-down, namely identify and study first the characteristics or feature vectors of each noun phrase linguistic pattern, then develop models for their semantic classification. This is in contrast to our prio</context>
</contexts>
<marker>Finin, 1980</marker>
<rawString>T. Finin. 1980. The Semantic Interpretation of Compound Nominals. Ph.D dissertation, University of Illinois, Urbana, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="4428" citStr="Gildea and Jurafsky 2002" startWordPosition="679" endWordPosition="682"> a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART-WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Uanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer a</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic Labeling of Semantic Roles. In Computational Linguistics, 28(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>M Palmer</author>
</authors>
<title>The Necessity of Parsing for Predicate Argument Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="4454" citStr="Gildea and Palmer 2002" startWordPosition="683" endWordPosition="686">ons, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART-WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Uanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsk</context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>D. Gildea and M. Palmer. 2002. The Necessity of Parsing for Predicate Argument Recognition. In Proceedings of the 40th Meeting of the Association for Computational Linguistics (ACL 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Badulescu</author>
<author>D Moldovan</author>
</authors>
<title>Learning Semantic Constraints for the Automatic Discovery of Part-Whole Relations.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT-03),</booktitle>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>R. Girju, A. Badulescu, and D. Moldovan. 2003. Learning Semantic Constraints for the Automatic Discovery of Part-Whole Relations. In Proceedings of the Human Language Technology Conference (HLT-03), Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A M Giuglea</author>
<author>M Olteanu</author>
<author>O Fortu</author>
<author>D Moldovan</author>
</authors>
<title>Support Vector Machines Applied to the Classification of Semantic Relations in Nominalized Noun Phrases.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL 2004 - Computational Lexical Semantics workshop,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="31829" citStr="Girju et al. 2004" startWordPosition="5083" endWordPosition="5086">vels of specialization are too high, (4) errors caused by metonymy, (6) errors in the modifier-head order, and others. These errors could be substantially decreased with more research effort. A further analysis of the data led us to consider a different criterion of classification that splits the examples into nominalizations and non-nominalizations. The reason is that nominalization noun phrases seem to call for a different set of learning features than the non-nominalization noun phrases, taking advantage of the underlying verbargument structure. Details about this approach are provided in (Girju et al. 2004)). 3 Applications Semantic relations occur with high frequency in open text, and thus, their discovery is paramount for many applications. One important application is Question Answering. A powerful method of answering more difficult questions is to associate to each question the semantic relation that reflects the meaning of that question and then search for that semantic relation over the candidates of semantically tagged paragraphs. Here is an example. Q. Where have nuclear incidents occurred? From the question stem word where, we know the question asks for a LOCATION which is found in the </context>
</contexts>
<marker>Girju, Giuglea, Olteanu, Fortu, Moldovan, 2004</marker>
<rawString>R. Girju, A.M. Giuglea, M. Olteanu, O. Fortu, and D. Moldovan. 2004. Support Vector Machines Applied to the Classification of Semantic Relations in Nominalized Noun Phrases. In Proceedings of HLT/NAACL 2004 - Computational Lexical Semantics workshop, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
<author>M Marcus</author>
</authors>
<title>Adding Semantic Annotation to the Penn TreeBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT 2002),</booktitle>
<location>California.</location>
<contexts>
<context position="4371" citStr="Kingsbury, et al. 2002" startWordPosition="671" endWordPosition="674">erns that are highly ambiguous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART-WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Uanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as sy</context>
</contexts>
<marker>Kingsbury, Palmer, Marcus, 2002</marker>
<rawString>P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding Semantic Annotation to the Penn TreeBank. In Proceedings of the Human Language Technology Conference (HLT 2002), California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>The Disambiguation of Nominalisations.</title>
<date>2002</date>
<journal>In Computational Linguistics</journal>
<volume>28</volume>
<pages>357--388</pages>
<contexts>
<context position="5360" citStr="Lapata 2002" startWordPosition="823" endWordPosition="824">the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsky et al. 1993). Another popular approach focuses on the interpretation of the underlying semantics. Many researchers that followed this approach relied mostly on hand-coded rules (Finin 1980), (Uanderwende 1994). More recently, (Rosario and Hearst 2001), (Rosario, Hearst, and Fillmore 2002), (Lapata 2002) have proposed automatic methods that analyze and detect noun compounds relations from text. (Rosario and Hearst 2001) focused on the medical domain making use of a lexical ontology and standard machine learning techniques. 2 Approach 2.1 Basic Approach We approach the problem top-down, namely identify and study first the characteristics or feature vectors of each noun phrase linguistic pattern, then develop models for their semantic classification. This is in contrast to our prior approach ( (Girju, Badulescu, and Moldovan 2003a)) when we studied one relation at a time, and learned constraint</context>
</contexts>
<marker>Lapata, 2002</marker>
<rawString>M. Lapata. 2002. The Disambiguation of Nominalisations. In Computational Linguistics 28:3, 357-388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lauer</author>
<author>M Dras</author>
</authors>
<title>A probabilistic model of compound nouns.</title>
<date>1994</date>
<booktitle>In Proceedings of the 7th Australian Joint Conference on AI.</booktitle>
<contexts>
<context position="5041" citStr="Lauer and Dras 1994" startWordPosition="775" endWordPosition="778">y 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsky et al. 1993). Another popular approach focuses on the interpretation of the underlying semantics. Many researchers that followed this approach relied mostly on hand-coded rules (Finin 1980), (Uanderwende 1994). More recently, (Rosario and Hearst 2001), (Rosario, Hearst, and Fillmore 2002), (Lapata 2002) have proposed automatic methods that analyze and detect noun compounds relations from text. (Rosario and Hearst 2001) focused on the medical domain making use of a lexical ontology and standard machine learning techniques. 2 Approach 2.1 Basic Approach We approach the problem top</context>
</contexts>
<marker>Lauer, Dras, 1994</marker>
<rawString>M. Lauer and M. Dras. 1994. A probabilistic model of compound nouns. In Proceedings of the 7th Australian Joint Conference on AI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lauer</author>
</authors>
<title>Designing Statistical Language Learners: Experiments on Compound Nouns. In</title>
<date>1995</date>
<tech>PhD thesis,</tech>
<institution>Macquarie University, Sidney.</institution>
<contexts>
<context position="18151" citStr="Lauer 1995" startWordPosition="2804" endWordPosition="2805">andomly selected sentences from each corpus, the number of instances found in these sentences, and finally the number of instances that our group managed to annotate by hand. The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation from those listed in Table 1. Inter-annotator Agreement The annotators, four PhD students in Computational Semantics worked in groups of two, each group focusing on one half of the corpora to annotate. Noun - noun (adjective - noun, respectively) sequences of words were extracted using the Lauer heuristic (Lauer 1995) which looks for consecutive pairs of nouns that are neither preceded nor succeeded by a noun after each sentence was syntactically parsed with Charniak parser (Charniak 2001) (for XWN we used the gold parse trees). Moreover, they were provided with the sentence in which the pairs occurred along with their corresponding WordNet senses. Whenever the annotators found an example encoding a semantic relation other than those provided or they didn’t know what interpretation to give, they had to tag it as “OTHERS”. Besides the type of relation, the annotators were asked to provide information about </context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>M. Lauer. 1995. Designing Statistical Language Learners: Experiments on Compound Nouns. In PhD thesis, Macquarie University, Sidney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Levi</author>
</authors>
<title>The Syntax and Semantics of Complex Nominals.</title>
<date>1979</date>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<contexts>
<context position="4251" citStr="Levi 1979" startWordPosition="656" endWordPosition="657">xamples, and some references. Most of the time, the semantic relations are encoded by lexico-syntactic patterns that are highly ambiguous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART-WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Uanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the </context>
<context position="7806" citStr="Levi 1979" startWordPosition="1204" endWordPosition="1205">agreed in linguistics that concepts can be represented in many ways using various constructions at different syntactic levels. This is in part why we decided to take the syntactico-semantic approach that analyzes semantic relations at different syntactic levels of representation. In this paper we focus only on the behavior of semantic relations at NP level. A thorough understanding of the syntactic and semantic characteristics of NPs provides valuable insights into defining the most representative feature vectors that ultimately drive the discriminating learning models. Complex Nominals Levi (Levi 1979) defines complex nominals (CNs) as expressions that have a head noun preceded by one or more modifying nouns, or by adjectives derived from nouns (usually called denominal adjectives). Most importantly for us, each sequence of nouns, or possibly adjectives and nouns, has a particular meaning as a whole carrying an implicit semantic relation; for example, “spoon handle” (PART-WHOLE) or “musical clock” (MAKE/PRODUCE). CNs have been studied intensively in linguistics, psycho-linguistics, philosophy, and computational linguistics for a long time. The semantic interpretation of CNs proves to be ver</context>
<context position="9564" citStr="Levi 1979" startWordPosition="1476" endWordPosition="1477">CATION as well as a PART-WHOLE relation. (4) Interpretation of CNs can be highly context-dependent. For example, “apple juice seat” can be defined as “seat with apple juice on the table in front of it” (cf. (Downing 1977)). Genitives The semantic interpretation of genitive constructions No. Semantic Definition / Example Relation 1 POSSESSION an animate entity possesses (owns) another entity; (family estate; the girl has a new car.), (Vanderwende 1994) 2 KINSHIP an animated entity related by blood, marriage, adoption or strong affinity to another animated entity; (Mary’s daughter; my sister); (Levi 1979) 3 PROPERTY/ characteristic or quality of an entity/event/state; (red rose; The thunderstorm was awful.); (Levi 1979) ATTRIBUTE-HOLDER 4 AGENT the doer or instigator of the action denoted by the predicate; (employee protest; parental approval; The king banished the general.); (Baker, Fillmore, and Lowe 1998) 5 TEMPORAL time associated with an event; (5-o’clock tea; winter training; the store opens at 9 am), includes DURATION (Navigli and Velardi 2003), 6 DEPICTION- an event/action/entity depicting another event/action/entity; (A picture of my niece.), DEPICTED 7 PART-WHOLE an entity/event/stat</context>
<context position="10819" citStr="Levi 1979" startWordPosition="1658" endWordPosition="1659"> knob; door of the car), (MERONYMY) (Levi 1979), (Dolan et al. 1993), 8 HYPERNYMY an entity/event/state is a subclass of another; (daisy flower; Virginia state; large company, such as Microsoft) (IS-A) (Levi 1979), (Dolan et al. 1993) 9 ENTAIL an event/state is a logical consequence of another; (snoring entails sleeping) 10 CAUSE an event/state makes another event/state to take place; (malaria mosquitoes; to die of hunger; The earthquake generated a Tsunami), (Levi 1979) 11 MAKE/PRODUCE an animated entity creates or manufactures another entity; (honey bees; nuclear powerplant; GM makes cars) (Levi 1979) 12 INSTRUMENT an entity used in an event/action as instrument; (pump drainage; the hammer broke the box) (Levi 1979) 13 LOCATION/SPACE spatial relation between two entities or between an event and an entity; includes DIRECTION; (field mouse; street show; I left the keys in the car), (Levi 1979), (Dolan et al. 1993) 14 PURPOSE a state/action intended to result from a another state/event; (migraine drug; wine glass; rescue mission; He was quiet in order not to disturb her.) (Navigli and Velardi 2003) 15 SOURCE/FROM place where an entity comes from; (olive oil; I got itfrom China) (Levi 1979) 16</context>
</contexts>
<marker>Levi, 1979</marker>
<rawString>Judith Levi. 1979. The Syntax and Semantics of Complex Nominals. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>Ontology Learning and Its Application to Automated Terminology Translation. In</title>
<date>2003</date>
<journal>IEEE Intelligent Systems.</journal>
<contexts>
<context position="10019" citStr="Navigli and Velardi 2003" startWordPosition="1541" endWordPosition="1544">nderwende 1994) 2 KINSHIP an animated entity related by blood, marriage, adoption or strong affinity to another animated entity; (Mary’s daughter; my sister); (Levi 1979) 3 PROPERTY/ characteristic or quality of an entity/event/state; (red rose; The thunderstorm was awful.); (Levi 1979) ATTRIBUTE-HOLDER 4 AGENT the doer or instigator of the action denoted by the predicate; (employee protest; parental approval; The king banished the general.); (Baker, Fillmore, and Lowe 1998) 5 TEMPORAL time associated with an event; (5-o’clock tea; winter training; the store opens at 9 am), includes DURATION (Navigli and Velardi 2003), 6 DEPICTION- an event/action/entity depicting another event/action/entity; (A picture of my niece.), DEPICTED 7 PART-WHOLE an entity/event/state is part of another entity/event/state (door knob; door of the car), (MERONYMY) (Levi 1979), (Dolan et al. 1993), 8 HYPERNYMY an entity/event/state is a subclass of another; (daisy flower; Virginia state; large company, such as Microsoft) (IS-A) (Levi 1979), (Dolan et al. 1993) 9 ENTAIL an event/state is a logical consequence of another; (snoring entails sleeping) 10 CAUSE an event/state makes another event/state to take place; (malaria mosquitoes; t</context>
<context position="11323" citStr="Navigli and Velardi 2003" startWordPosition="1738" endWordPosition="1741">CE an animated entity creates or manufactures another entity; (honey bees; nuclear powerplant; GM makes cars) (Levi 1979) 12 INSTRUMENT an entity used in an event/action as instrument; (pump drainage; the hammer broke the box) (Levi 1979) 13 LOCATION/SPACE spatial relation between two entities or between an event and an entity; includes DIRECTION; (field mouse; street show; I left the keys in the car), (Levi 1979), (Dolan et al. 1993) 14 PURPOSE a state/action intended to result from a another state/event; (migraine drug; wine glass; rescue mission; He was quiet in order not to disturb her.) (Navigli and Velardi 2003) 15 SOURCE/FROM place where an entity comes from; (olive oil; I got itfrom China) (Levi 1979) 16 TOPIC an object is a topic of another object; (weather report; construction plan; article about terrorism); (Rosario and Hearst 2001) 17 MANNER a way in which an event is performed or takes place; (hard-working immigrants; enjoy immensely; he died of cancer); (Blaheta and Charniak 2000) 18 MEANS the means by which an event is performed or takes place; (bus service; Igo to school by bus.) (Quirk et al.1985) 19 ACCOMPANIMENT one/more entities accompanying another entity involved in an event; (meeting</context>
</contexts>
<marker>Navigli, Velardi, 2003</marker>
<rawString>R. Navigli and P. Velardi. 2003. Ontology Learning and Its Application to Automated Terminology Translation. In IEEE Intelligent Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>S Bergler</author>
<author>P Anick</author>
</authors>
<title>Lexical semantic techniques for corpus analysis.</title>
<date>1993</date>
<booktitle>In Computational Linguistics,</booktitle>
<contexts>
<context position="5068" citStr="Pustejovsky et al. 1993" startWordPosition="779" endWordPosition="782">lmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsky et al. 1993). Another popular approach focuses on the interpretation of the underlying semantics. Many researchers that followed this approach relied mostly on hand-coded rules (Finin 1980), (Uanderwende 1994). More recently, (Rosario and Hearst 2001), (Rosario, Hearst, and Fillmore 2002), (Lapata 2002) have proposed automatic methods that analyze and detect noun compounds relations from text. (Rosario and Hearst 2001) focused on the medical domain making use of a lexical ontology and standard machine learning techniques. 2 Approach 2.1 Basic Approach We approach the problem top-down, namely identify and </context>
</contexts>
<marker>Pustejovsky, Bergler, Anick, 1993</marker>
<rawString>J. Pustejovsky, S. Bergler, and P. Anick. 1993. Lexical semantic techniques for corpus analysis. In Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
<author>S Greenbaum</author>
<author>G Leech</author>
<author>J Svartvik</author>
</authors>
<title>A comprehensive grammar of english language,</title>
<date>1985</date>
<location>Longman, Harlow.</location>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A comprehensive grammar of english language, Longman, Harlow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rosario</author>
<author>M Hearst</author>
</authors>
<title>Classifying the Semantic Relations in Noun Compounds via a DomainSpecific Lexical Hierarchy.</title>
<date>2001</date>
<booktitle>In the Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>EMNLP</location>
<contexts>
<context position="4345" citStr="Rosario and Hearst 2001" startWordPosition="667" endWordPosition="670">ed by lexico-syntactic patterns that are highly ambiguous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART-WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Uanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound </context>
<context position="11553" citStr="Rosario and Hearst 2001" startWordPosition="1775" endWordPosition="1778">i 1979) 13 LOCATION/SPACE spatial relation between two entities or between an event and an entity; includes DIRECTION; (field mouse; street show; I left the keys in the car), (Levi 1979), (Dolan et al. 1993) 14 PURPOSE a state/action intended to result from a another state/event; (migraine drug; wine glass; rescue mission; He was quiet in order not to disturb her.) (Navigli and Velardi 2003) 15 SOURCE/FROM place where an entity comes from; (olive oil; I got itfrom China) (Levi 1979) 16 TOPIC an object is a topic of another object; (weather report; construction plan; article about terrorism); (Rosario and Hearst 2001) 17 MANNER a way in which an event is performed or takes place; (hard-working immigrants; enjoy immensely; he died of cancer); (Blaheta and Charniak 2000) 18 MEANS the means by which an event is performed or takes place; (bus service; Igo to school by bus.) (Quirk et al.1985) 19 ACCOMPANIMENT one/more entities accompanying another entity involved in an event; (meeting with friends; She came with us) (Quirk et al.1985) 20 EXPERIENCER an animated entity experiencing a state/feeling; (Mary was in a state ofpanic.); (Sowa 1994) 21 RECIPIENT an animated entity for which an event is performed; (The </context>
</contexts>
<marker>Rosario, Hearst, 2001</marker>
<rawString>B. Rosario and M. Hearst. 2001. Classifying the Semantic Relations in Noun Compounds via a DomainSpecific Lexical Hierarchy. In the Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, (EMNLP 2001), Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rosario</author>
<author>M Hearst</author>
<author>C Fillmore</author>
</authors>
<title>The Descent of Hierarchy, and Selection in Relational Semantics.</title>
<date>2002</date>
<booktitle>In the Proceedings of the Association for Computational Linguistics (ACL-02),</booktitle>
<institution>University of Pennsylvania.</institution>
<marker>Rosario, Hearst, Fillmore, 2002</marker>
<rawString>B. Rosario, M. Hearst, and C. Fillmore. 2002. The Descent of Hierarchy, and Selection in Relational Semantics. In the Proceedings of the Association for Computational Linguistics (ACL-02), University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Semmelmeyer</author>
<author>D Bolander</author>
</authors>
<title>The New Webster’s Grammar Guide.</title>
<date>1992</date>
<publisher>Lexicon Publications, Inc.</publisher>
<contexts>
<context position="1616" citStr="Semmelmeyer and Bolander 1992" startWordPosition="230" endWordPosition="233">puter maker), whereas semantic relations can occur between any constituents, for example in complex nominals (malaria mosquito (CAUSE)), genitives (girl’s mouth (PART-WHOLE)), prepositional phrases attached to nouns (man at the store (LOCATIVE)), or discourse level (The bus was late. As a result, I missed my appointment (CAUSE)). Thus, in a sense, semantic relations are more general than semantic roles and many semantic role types will appear on our list of semantic relations. The following NP level constructions are considered here (cf. the classifications provided by (Quirk et al.1985) and (Semmelmeyer and Bolander 1992)): (1) Compound Nominals consisting of two consecutive nouns (eg night club - a TEMPORAL relation - indicating that club functions at night), (2) Adjective Noun constructions where the adjectival modifier is derived from a noun (eg musical clock - a MAKE/PRODUCE relation), (3) Genitives (eg the door of the car - a PART-WHOLE relation), and (4) Adjective phrases (cf. (Semmelmeyer and Bolander 1992)) in which the modifier noun is expressed by a prepositional phrase which functions as an adjective (eg toy in the box - a LOCATION relation). Example: “Saturday’s snowfall topped a one-day record in </context>
<context position="15255" citStr="Semmelmeyer and Bolander 1992" startWordPosition="2341" endWordPosition="2344">DEPICTED, and “the desert’s oasis”-PART-WHOLE/PLACE-AREA. A characteristic of these constructions is that they are very productive, as the construction can be given various interpretations depending on the context. One such example is “Kate’s book” that can mean the book Kate owns, the book Kate wrote, or the book Kate is very fond of. Thus, the features that contribute to the semantic interpretation of genitives are: the nouns’ semantic classes, the type of genitives, discourse and pragmatic information. Adjective Phrases are prepositional phrases attached to nouns acting as adjectives (cf. (Semmelmeyer and Bolander 1992)). Prepositions play an important role both syntactically and semantically. Semantically speaking, prepositional constructions can encode various semantic relations, their interpretations being provided most of the time by the underlying context. For instance, the preposition “with” can encode different semantic relations: (1) It was the girl with blue eyes (MERONYMY), (2) The baby with the red ribbon is cute (POSSESSION), (3) The woman with triplets received a lot of attention (KINSHIP). The conclusion for us is that in addition to the nouns semantic classes, the preposition and the context p</context>
</contexts>
<marker>Semmelmeyer, Bolander, 1992</marker>
<rawString>M. Semmelmeyer and D. Bolander. 1992. The New Webster’s Grammar Guide. Lexicon Publications, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Sowa</author>
</authors>
<title>Conceptual Structures: Information Processing in Mind and Machine.</title>
<date>1994</date>
<publisher>Addison Wesley.</publisher>
<contexts>
<context position="4284" citStr="Sowa 1994" startWordPosition="660" endWordPosition="661">t of the time, the semantic relations are encoded by lexico-syntactic patterns that are highly ambiguous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART-WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Uanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Sever</context>
<context position="12082" citStr="Sowa 1994" startWordPosition="1862" endWordPosition="1863">er report; construction plan; article about terrorism); (Rosario and Hearst 2001) 17 MANNER a way in which an event is performed or takes place; (hard-working immigrants; enjoy immensely; he died of cancer); (Blaheta and Charniak 2000) 18 MEANS the means by which an event is performed or takes place; (bus service; Igo to school by bus.) (Quirk et al.1985) 19 ACCOMPANIMENT one/more entities accompanying another entity involved in an event; (meeting with friends; She came with us) (Quirk et al.1985) 20 EXPERIENCER an animated entity experiencing a state/feeling; (Mary was in a state ofpanic.); (Sowa 1994) 21 RECIPIENT an animated entity for which an event is performed; (The eggs are for you) ; includes BENEFICIARY; (Sowa 1994) 22 FREQUENCY number of occurrences of an event; (bi-annual meeting; I take the bus every day); (Sowa 1994) 23 INFLUENCE an entity/event that affects other entity/event; (drug-affectedfamilies; The war has an impact on the economy.); 24 ASSOCIATED WITH an entity/event/state that is in an (undefined) relation with another entity/event/state; (Jazz-associated company;) 25 MEASURE an entity expressing quantity of another entity/event; (cup ofsugar; 70-km distance; centennial</context>
<context position="13443" citStr="Sowa 1994" startWordPosition="2066" endWordPosition="2067">; (Sowa 1994) 27 ANTONYMY a word/concept that is the opposite of another word/concept; (empty is the opposite offull); (Sowa 1994) 28 PROBABILITY OF the quality/state of being probable; likelihood EXISTENCE (There is little chance of rain tonight); (Sowa 1994) 29 POSSIBILITY the state/condition of being possible; (I might go to Opera tonight); (Sowa 1994) 30 CERTAINTY the state/condition of being certain or without doubt; (He definitely left the house this morning); 31 THEME an entity that is changed/involved by the action/event denoted by the predicate; (music lover; John opened the door.); (Sowa 1994) 32 RESULT the inanimate result of the action/event denoted by the predicate; includes EFFECT and PRODUCT. (combustion gases; Ifinished the task completely.); (Sowa 1994) 33 STIMULUS stimulus of the action or event denoted by the predicate (We saw [the painting]. I sensed [the eagerness] in him. I can see [that you are feeling great].) (Baker, Fillmore, and Lowe 1998) 34 EXTENT the change of status on a scale (by a percentage or by a value) of some entity; (The price of oil increased [ten percent]. Oil’s price increased by [ten percent]. ); (Blaheta and Charniak 2000) 35 PREDICATE expresses th</context>
</contexts>
<marker>Sowa, 1994</marker>
<rawString>J. F. Sowa. 1994. Conceptual Structures: Information Processing in Mind and Machine. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vanderwende</author>
</authors>
<title>Algorithm for automatic interpretation of noun sequences.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING-94,</booktitle>
<pages>782--788</pages>
<contexts>
<context position="9409" citStr="Vanderwende 1994" startWordPosition="1453" endWordPosition="1454">ar-producing company. (3) There can be many possible semantic relations between a given pair of word constituents. For example, “USA city” can be regarded as a LOCATION as well as a PART-WHOLE relation. (4) Interpretation of CNs can be highly context-dependent. For example, “apple juice seat” can be defined as “seat with apple juice on the table in front of it” (cf. (Downing 1977)). Genitives The semantic interpretation of genitive constructions No. Semantic Definition / Example Relation 1 POSSESSION an animate entity possesses (owns) another entity; (family estate; the girl has a new car.), (Vanderwende 1994) 2 KINSHIP an animated entity related by blood, marriage, adoption or strong affinity to another animated entity; (Mary’s daughter; my sister); (Levi 1979) 3 PROPERTY/ characteristic or quality of an entity/event/state; (red rose; The thunderstorm was awful.); (Levi 1979) ATTRIBUTE-HOLDER 4 AGENT the doer or instigator of the action denoted by the predicate; (employee protest; parental approval; The king banished the general.); (Baker, Fillmore, and Lowe 1998) 5 TEMPORAL time associated with an event; (5-o’clock tea; winter training; the store opens at 9 am), includes DURATION (Navigli and Vel</context>
</contexts>
<marker>Vanderwende, 1994</marker>
<rawString>L. Vanderwende. 1994. Algorithm for automatic interpretation of noun sequences. In Proceedings of COLING-94, pg. 782-788.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>