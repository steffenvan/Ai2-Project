<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.982733">
Kneser-Ney Smoothing on Expected Counts
</title>
<author confidence="0.999073">
Hui Zhang
</author>
<affiliation confidence="0.998584">
Department of Computer Science
University of Southern California
</affiliation>
<email confidence="0.986804">
hzhang@isi.edu
</email>
<author confidence="0.997263">
David Chiang
</author>
<affiliation confidence="0.996385">
Information Sciences Institute
University of Southern California
</affiliation>
<email confidence="0.984038">
chiang@isi.edu
</email>
<sectionHeader confidence="0.981944" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999640421052632">
Widely used in speech and language pro-
cessing, Kneser-Ney (KN) smoothing has
consistently been shown to be one of
the best-performing smoothing methods.
However, KN smoothing assumes integer
counts, limiting its potential uses—for ex-
ample, inside Expectation-Maximization.
In this paper, we propose a generaliza-
tion of KN smoothing that operates on
fractional counts, or, more precisely, on
distributions over counts. We rederive all
the steps of KN smoothing to operate
on count distributions instead of integral
counts, and apply it to two tasks where
KN smoothing was not applicable before:
one in language model adaptation, and the
other in word alignment. In both cases,
our method improves performance signifi-
cantly.
</bodyText>
<sectionHeader confidence="0.995174" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999821883720931">
In speech and language processing, smoothing is
essential to reduce overfitting, and Kneser-Ney
(KN) smoothing (Kneser and Ney, 1995; Chen
and Goodman, 1999) has consistently proven to be
among the best-performing and most widely used
methods. However, KN smoothing assumes inte-
ger counts, whereas in many NLP tasks, training
instances appear with possibly fractional weights.
Such cases have been noted for language model-
ing (Goodman, 2001; Goodman, 2004), domain
adaptation (Tam and Schultz, 2008), grapheme-to-
phoneme conversion (Bisani and Ney, 2008), and
phrase-based translation (Andr´es-Ferrer, 2010;
Wuebker et al., 2012).
For example, in Expectation-Maximization
(Dempster et al., 1977), the Expectation (E) step
computes the posterior distribution over possi-
ble completions of the data, and the Maximiza-
tion (M) step reestimates the model parameters as
if that distribution had actually been observed. In
most cases, the M step is identical to estimating
the model from complete data, except that counts
of observations from the E step are fractional. It
is common to apply add-one smoothing to the
M step, but we cannot apply KN smoothing.
Another example is instance weighting. If we
assign a weight to each training instance to indi-
cate how important it is (say, its relevance to a par-
ticular domain), and the counts are not integral,
then we again cannot train the model using KN
smoothing.
In this paper, we propose a generalization of KN
smoothing (called expected KN smoothing) that
operates on fractional counts, or, more precisely,
on distributions over counts. We rederive all the
steps of KN smoothing to operate on count distri-
butions instead of integral counts. We demonstrate
how to apply expected KN to two tasks where KN
smoothing was not applicable before. One is lan-
guage model domain adaptation, and the other is
word alignment using the IBM models (Brown et
al., 1993). In both tasks, expected KN smoothing
improves performance significantly.
</bodyText>
<sectionHeader confidence="0.683702" genericHeader="introduction">
2 Smoothing on integral counts
</sectionHeader>
<bodyText confidence="0.999971">
Before presenting our method, we review KN
smoothing on integer counts as applied to lan-
guage models, although, as we will demonstrate
in Section 7, KN smoothing is applicable to other
tasks as well.
</bodyText>
<subsectionHeader confidence="0.980328">
2.1 Maximum likelihood estimation
</subsectionHeader>
<bodyText confidence="0.999571333333333">
Let uw stand for an n-gram, where u stands for
the (n − 1) context words and w, the predicted
word. Let c(uw) be the number of occurrences
of uw. We use a bullet (•) to indicate summa-
tion over words, that is, c(u•) _ Ew c(uw). Under
maximum-likelihood estimation (MLE), we max-
</bodyText>
<note confidence="0.807977">
765
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 765–774,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.24329">
imize leaving-one-out log-likelihood
∑L = c(uw) log p(w  |u), ∑Lloo = c(uw) log ploo(w  |u)
uw uw
obtaining the solution
</figure>
<equation confidence="0.995719">
pmle(w  |u) =
∑=
uw|c(uw)&gt;1
c(uw) − 1 − D
c(uw) log
c(u•) − 1
c(uw)
(1)
c(u•) .
∑+ (n1+(u•) − 1)Dqu(w)
uw|c(uw)=1 log
c(u•) − 1
</equation>
<subsectionHeader confidence="0.998047">
2.2 Absolute discounting
</subsectionHeader>
<bodyText confidence="0.999583571428571">
Absolute discounting (Ney et al., 1994) – on which
KN smoothing is based – tries to generalize bet-
ter to unseen data by subtracting a discount from
each seen n-gram’s count and distributing the sub-
tracted discounts to unseen n-grams. For now, we
assume that the discount is a constant D, so that
the smoothed counts are
</bodyText>
<equation confidence="0.482369">
c(uw) − D if c(uw) &gt; 0
n1+(u•)Dqu(w) otherwise
</equation>
<bodyText confidence="0.9999594">
where n1+(u•) = |{w  |c(uw) &gt; 0} |is the number
of word types observed after context u, and qu(w)
specifies how to distribute the subtracted discounts
among unseen n-gram types. Maximizing the like-
lihood of the smoothed counts ˜c, we get
</bodyText>
<equation confidence="0.989856">
p(w  |u) = ⎧ ⎪ ⎪⎪⎪⎪⎪ c(uw) − D (2)
⎨⎪⎪ c(u•) if c(uw) &gt; 0
⎪⎪⎪⎪⎪⎪⎪⎪⎩ n1+(u•)Dqu(w) otherwise.
c(u•)
</equation>
<bodyText confidence="0.9990165">
How to choose D and qu(w) are described in the
next two sections.
</bodyText>
<subsectionHeader confidence="0.998992">
2.3 Estimating D by leaving-one-out
</subsectionHeader>
<bodyText confidence="0.99947725">
The discount D can be chosen by various means;
in absolute discounting, it is chosen by the method
of leaving one out. Given N training instances, we
form the probability of each instance under the
MLE using the other (N − 1) instances as train-
ing data; then we maximize the log-likelihood of
all those instances. The probability of an n-gram
token uw using the other tokens as training data is
</bodyText>
<equation confidence="0.99335575">
ploo(w  |u) =
(n1+(u•) − 1)Dqu(w)
c(uw) = 1.
c(u•) − 1
</equation>
<bodyText confidence="0.907045">
We want to find the D that maximizes the
</bodyText>
<equation confidence="0.968025">
∑= rnr log(r − 1 − D) + n1 log D + C, (3)
r&gt;1
</equation>
<bodyText confidence="0.99998575">
where nr = |{uw  |c(uw) = r} |is the number of n-
gram types appearing r times, and C is a constant
not depending on D. Setting the partial derivative
with respect to D to zero, we have
</bodyText>
<equation confidence="0.9851793">
∂Lloo ∑= − rnr + n1
r&gt;1 r − 1 − D
∂D D
rnr
≥
r − 1 − D
Solving for D, we have
n1
D ≤ .(4)
n1 + 2n2
</equation>
<bodyText confidence="0.999382">
Theoretically, we can use iterative methods to op-
timize D. But in practice, setting D to this upper
bound is effective and simple (Ney et al., 1994;
Chen and Goodman, 1999).
</bodyText>
<subsectionHeader confidence="0.999274">
2.4 Estimating the lower-order distribution
</subsectionHeader>
<bodyText confidence="0.999178">
Finally, qu(w) is defined to be proportional to an
(n − 1)-gram model p′(w  |u′), where u′ is the
(n − 2)-gram suffix of u. That is,
</bodyText>
<equation confidence="0.978533">
qu(w) = γ(u)p′(w  |u′),
</equation>
<bodyText confidence="0.999613428571429">
where γ(u) is an auxiliary function chosen to make
the distribution p(w  |u) in (2) sum to one.
Absolute discounting chooses p′(w  |u′) to be
the maximum-likelihood unigram distribution; un-
der KN smoothing (Kneser and Ney, 1995), it is
chosen to make p in (2) satisfy the following con-
straint for all (n − 1)-grams u′w:
</bodyText>
<equation confidence="0.999371857142857">
∑pmle(u′w) = p(w  |vu′)pmle(vu′). (5)
v
⎨⎪⎪ ⎧
⎪⎪⎩
˜c(uw) =
⎨⎪ ⎪⎪⎪⎪⎪⎪⎪⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎩
n1
D
∑=
r&gt;1
2n2
.
1 − D
</equation>
<bodyText confidence="0.9965435">
Substituting in the definition of pmle from (1) and
p from (2) and canceling terms, we get
</bodyText>
<equation confidence="0.991628363636364">
c(u•) − 1
c(uw) &gt; 1
c(uw) − 1 − D
∑c(u′w) = (c(vu′w) − D)
v|c(vu′w)&gt;0
∑+ n1+(vu′•)Dγ(vu′)p′(w  |u′).
v|c(vu′w)=0
766
Solving for p′(w  |u′), we have
p′(w  |u′) = Ev|c(vu′w)&gt;0 1
Ev|c(vu′w)=0 n1+(vu′•)Y(vu′).
</equation>
<bodyText confidence="0.99483">
Kneser and Ney assume the denominator is con-
stant in w and renormalize to get an approximation
</bodyText>
<equation confidence="0.9183295">
p′ (w  |u′) ≈ n1+(•u′w), (6)
n1+(•u′•)
where
n1+(•u′w) = |{v  |c(vu′w) &gt; 0}|
Zn1+(•u′•) =
w
</equation>
<sectionHeader confidence="0.974727" genericHeader="method">
3 Count distributions
</sectionHeader>
<bodyText confidence="0.999982666666667">
The computation of D and p′ above made use of
nr and nr+, which presupposes integer counts. But
in many applications, the counts are not integral,
but fractional. How do we apply KN smoothing in
such cases? In this section, we introduce count dis-
tributions as a way of circumventing this problem.
</bodyText>
<subsectionHeader confidence="0.995747">
3.1 Definition
</subsectionHeader>
<bodyText confidence="0.999601068965517">
In the E step of EM, we compute a probability dis-
tribution (according to the current model) over all
possible completions of the observed data, and the
expected counts of all types, which may be frac-
tional. However, note that in each completion of
the data, the counts are integral. Although it does
not make sense to compute nr or nr+ on fractional
counts, it does make sense to compute them on
possible completions.
In other situations where fractional counts arise,
we can still think of the counts as expectations un-
der some distribution over possible “realizations”
of the data. For example, if we assign a weight
between zero and one to every instance in a cor-
pus, we can interpret each instance’s weight as the
probability of that instance occurring or not, yield-
ing a distribution over possible subsets of the data.
Let X be a random variable ranging over pos-
sible realizations of the data, and let cX(uw) be
the count of uw in realization X. The expecta-
tion E[cX(uw)] is the familiar fractional expected
count of uw, but we can also compute the proba-
bilities p(cX(uw) = r) for any r. From now on, for
brevity, we drop the subscript X and understand
c(uw) to be a random variable depending on X.
The nr(u•) and nr+(u•) and related quantities also
become random variables depending on X.
For example, suppose that our data consists of
the following bigrams, with their weights:
</bodyText>
<figure confidence="0.724698692307692">
(a) fat cat 0.3
(b) fat cat 0.8
(c) big dog 0.9
We can interpret this as a distribution over eight
subsets (not all distinct), with probabilities:
∅ 0.7 · 0.2 · 0.1 = 0.014
0.3 · 0.2 · 0.1 = 0.006
0.7 · 0.8 · 0.1 = 0.056
{a, b} 0.3 · 0.8 · 0.1 = 0.024
0.7 · 0.2 · 0.9 = 0.126
{a, c} 0.3 · 0.2 · 0.9 = 0.054
{b, c} 0.7 · 0.8 · 0.9 = 0.504
{a, b, c} 0.3 · 0.8 · 0.9 = 0.216
</figure>
<bodyText confidence="0.720991">
Then the count distributions and the E[nr] are:
</bodyText>
<equation confidence="0.97487175">
r = 1 r = 2 r &gt; 0
p(c(fat cat) = r) 0.62 0.24 0.86
p(c(big dog) = r) 0.9 0 0.9
E[nr] 1.52 0.24
</equation>
<subsectionHeader confidence="0.997882">
3.2 Efficient computation
</subsectionHeader>
<bodyText confidence="0.9999927">
How to compute these probabilities and expecta-
tions depends in general on the structure of the
model. If we assume that all occurrences of uw
are independent (although in fact they are not al-
ways), the computation is very easy. If there are
k occurrences of uw, each occurring with proba-
bility pi, the count c(uw) is distributed according
to the Poisson-binomial distribution (Hong, 2013).
The expected count E[c(uw)] is just Ei pi, and the
distribution of c(uw) can be computed as follows:
</bodyText>
<equation confidence="0.984984666666667">
p(c(uw) = r) = s(k, r)
where s(k, r) is defined by the recurrence
{ s(k − 1, r)(1 − pk)
+ s(k − 1,r − 1)pk if 0 ≤ r ≤ k
1 ifk=r=0
0 otherwise.
Z
E[nr(u•)] =
w
Z
E[nr+(u•)] =
w
</equation>
<bodyText confidence="0.96865275">
Since, as we shall see, we only need to compute
these quantities up to a small value of r (2 or 4),
this takes time linear in k.
We can also compute l
</bodyText>
<equation confidence="0.9343125">
p(c(uw) ≥ r) = max{s(m, r), 1 −Zs(m, r′) },
r′&lt;r
</equation>
<bodyText confidence="0.9949175">
the floor operation being needed to protect against
rounding errors, and we can compute
</bodyText>
<equation confidence="0.9647245">
n1+(•u′w).
s(k, r) =
p(c(uw) = r)
p(c(uw) ≥ r).
</equation>
<page confidence="0.45757">
767
</page>
<sectionHeader confidence="0.740854" genericHeader="method">
4 Smoothing on count distributions
</sectionHeader>
<bodyText confidence="0.999997333333333">
We are now ready to describe how to apply KN
smoothing to count distributions. Below, we reca-
pitulate the derivation of KN smoothing presented
in Section 2, using the expected log-likelihood
in place of the log-likelihood and applying KN
smoothing to each possible realization of the data.
</bodyText>
<subsectionHeader confidence="0.997808">
4.1 Maximum likelihood estimation
</subsectionHeader>
<bodyText confidence="0.992232">
The MLE objective function is the expected log-
likelihood,
</bodyText>
<equation confidence="0.990726571428571">
������Z ������
E[L] = E L c(uw) log p(w  |u) �
uw
E[c(uw)] log p(w  |u)
whose maximum is
E[c(uw)] (7)
E[c(u•)] .
</equation>
<subsectionHeader confidence="0.995024">
4.2 Absolute discounting
</subsectionHeader>
<bodyText confidence="0.9999905">
If we apply absolute discounting to every realiza-
tion of the data, the expected smoothed counts are
</bodyText>
<equation confidence="0.9993164">
E[&amp;quot;c(uw)] = Z p(c(uw) = r)(r − D)
r&gt;0
+ p(c(uw) = 0)E[n1+(u•)]Dqu(w)
= E[c(uw)] − p(c(uw) &gt; 0)D
+ p(c(uw) = 0)E[n1+(u•)]Dqu(w) (8)
</equation>
<bodyText confidence="0.999976666666667">
where, to be precise, the expectation E[n1+(u•)]
should be conditioned on c(uw) = 0; in practice, it
seems safe to ignore this. The MLE is then
</bodyText>
<equation confidence="0.9990905">
p(w  |u) = E[c&amp;quot;(uw)]
E[&amp;quot;c(u•)] . (9)
</equation>
<subsectionHeader confidence="0.870284">
4.3 Estimating D by leaving-one-out
</subsectionHeader>
<bodyText confidence="0.999214076923077">
It would not be clear how to perform leaving-
one-out estimation on fractional counts, but here
we have a distribution over realizations of the
data, each with integral counts, and we can
perform leaving-one-out estimation on each of
these. In other words, our goal is to find the D
that maximizes the expected leaving-one-out log-
likelihood, which is just the expected value of (3):
where C is a constant not depending on D. We
have made the assumption that the nr are indepen-
dent.
By exactly the same reasoning as before, we ob-
tain an upper bound for D:
</bodyText>
<equation confidence="0.9971945">
D ≤ E[n1] (10)
E[n1] + 2E[n2].
</equation>
<bodyText confidence="0.8629565">
In our example above, D = 1.52
1.52+2·0.24 = 0.76.
</bodyText>
<subsectionHeader confidence="0.996838">
4.4 Estimating the lower-order distribution
</subsectionHeader>
<bodyText confidence="0.999941666666667">
We again require p′ to satisfy the marginal con-
straint (5). Substituting in (7) and solving for p′ as
in Section 2.4, we obtain the solution
</bodyText>
<equation confidence="0.9933045">
E[n1+(•u′w)]
p′(w  |u′) =
</equation>
<bodyText confidence="0.9587665">
For the example above, the estimates for the un-
igram model p′(w) are
</bodyText>
<equation confidence="0.997727666666667">
p′(cat) = 0.86
0.86+0.9 ≈ 0.489
p′(dog) = 0.9
</equation>
<bodyText confidence="0.505575">
0.86+0.9 ≈ 0.511.
</bodyText>
<subsectionHeader confidence="0.904301">
4.5 Extensions
</subsectionHeader>
<bodyText confidence="0.9998835">
Chen and Goodman (1999) introduce three exten-
sions to Kneser-Ney smoothing which are now
standard. For our experiments, we used all three,
for both integral counts and count distributions.
</bodyText>
<sectionHeader confidence="0.517199" genericHeader="method">
4.5.1 Interpolation
</sectionHeader>
<bodyText confidence="0.999067333333333">
In interpolated KN smoothing, the subtracted dis-
counts are redistributed not only among unseen
events but also seen events. That is,
</bodyText>
<equation confidence="0.967801">
c&amp;quot;(uw) = max{0, c(uw) − D} + n1+(u•)Dp′(w  |u′).
</equation>
<bodyText confidence="0.996830625">
In this case, γ(u) is always equal to one, so that
qu(w) = p′(w  |u′). (Also note that (6) becomes
an exact solution to the marginal constraint.) The-
oretically, this requires us to derive a new estimate
for D. However, as this is not trivial, nearly all im-
plementations simply use the original estimate (4).
On count distributions, the smoothed counts be-
come
</bodyText>
<equation confidence="0.897632111111111">
E[c&amp;quot;(uw)] = E[c(uw)] − p(c(uw) &gt; 0)D
+ E[n1+(u•)]Dp′(w  |u′). (12)
Z=
uw
pmle(w  |u) =
E[n1+(•u′•)] .
(11)
[ Z �rnr log(r − 1 − D) + C In our example, the smoothed counts are:
E[Lloo] = E n1 log D +
</equation>
<table confidence="0.994426962962963">
uw E[&amp;quot;c]
fat cat 1.1 − 0.86 · 0.76 + 0.86 · 0.76 · 0.489 ≈ 0.766
fat dog 0 − 0 · 0.76 + 0.86 · 0.76 · 0.511 ≈ 0.334
big cat 0 − 0 · 0.76 + 0.9 · 0.76 · 0.489 ≈ 0.334
big dog 0.9 − 0.9 · 0.76 + 0.9 · 0.76 · 0.511 ≈ 0.566
r&gt;1
= E[n1] log D
Z+ rE[nr] log(r − 1 − D) + C,
r&gt;1
768
which give the smoothed probability estimates:
0.766 = 0.696
p(cat  |=
fat)
0.766+0.334
0.334 = 0.304
fat)
p(dog  |=
0.766+0.334
0.334 = 0.371
big)
p(dog  |=
0.334+0.556
0.556 = 0.629.
big)
p(cat  |=
0.334+0.556
</table>
<subsubsectionHeader confidence="0.299367">
4.5.2 Modified discounts
</subsubsectionHeader>
<bodyText confidence="0.983543666666667">
Modified KN smoothing uses a different discount
Dr for each count r &lt; 3, and a discount D3+ for
counts r &gt; 3. On count distributions, a similar ar-
gument to the above leads to the estimates:
One side-effect of this change is that (6) is no
longer the correct solution to the marginal con-
straint (Teh, 2006; Sundermeyer et al., 2011). Al-
though this problem can be fixed, standard imple-
mentations simply use (6).
</bodyText>
<subsectionHeader confidence="0.608115">
4.5.3 Recursive smoothing
</subsectionHeader>
<bodyText confidence="0.999926363636364">
In the original KN method, the lower-order
model p′ was estimated using (6); recursive KN
smoothing applies KN smoothing to p′. To do this,
we need to reconstruct counts whose MLE is (6).
On integral counts, this is simple: we generate, for
each n-gram type vu′w, an (n−1)-gram token u′w,
for a total of n1+(•u′w) tokens. We then apply KN
smoothing to these counts.
Analogously, on count distributions, for each n-
gram type vu′w, we generate an (n − 1)-gram to-
ken u′w with probability p(c(vu′w) &gt; 0). Since
</bodyText>
<equation confidence="0.976772">
p(c(vu′w) &gt; 0) = E[n1+(•u′w)],
</equation>
<bodyText confidence="0.9999004">
this has (11) as its MLE and therefore satisfies the
marginal constraint. We then apply expected KN
smoothing to these count distributions.
For the example above, the count distributions
used for the unigram distribution would be:
</bodyText>
<equation confidence="0.796031">
r = 0 r = 1
</equation>
<subsectionHeader confidence="0.839546">
4.6 Summary
</subsectionHeader>
<bodyText confidence="0.996838842105263">
In summary, to perform expected KN smoothing
(either the original version or Chen and Good-
man’s modified version), we perform the steps
listed below:
orig. mod.
compute count distributions §3.2
estimate discount D
estimate lower-order model p′
compute smoothed counts c˜
compute probabilities p (9)
The computational complexity of expected KN
is almost identical to KN on integral counts. The
main addition is computing and storing the count
distributions. Using the dynamic program in Sec-
tion 3.2, computing the distributions for each r is
linear in the number of n-gram types, and we only
need to compute the distributions up to r = 2 (or
r = 4 for modified KN), and store them for r = 0
(or up to r = 2 for modified KN).
</bodyText>
<sectionHeader confidence="0.999839" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999806">
Witten-Bell (WB) smoothing is somewhat easier
than KN to adapt to fractional counts. The SRI-
LM toolkit (Stolcke, 2002) implements a method
which we call fractional WB:
</bodyText>
<equation confidence="0.999344666666667">
p(w  |u) = λ(u)pmle(w  |u) + (1 − λ(u))p′(w  |u′)
E[c(u)]
E[c(u)] + n1+(u•),
</equation>
<bodyText confidence="0.9999058">
where n1+(u•) is the number of word types ob-
served after context u, computed by ignoring all
weights. This method, although simple, inconsis-
tently uses weights for counting tokens but not
types. Moreover, as we will see below, it does not
perform as well as expected KN.
The only previous adaptation of KN smoothing
to fractional counts that we are aware of is that
of Tam and Schultz (2008) and Bisani and Ney
(2008), called fractional KN. This method sub-
tracts D directly from the fractional counts, zero-
ing out counts that are smaller than D. The dis-
count D must be set by minimizing an error metric
on held-out data using a line search (Tam, p. c.) or
Powell’s method (Bisani and Ney, 2008), requiring
repeated estimation and evaluation of the language
model. By contrast, we choose D by leaving-one-
out. Like KN on integral counts, our method has
a closed-form approximation and requires neither
held-out data nor trial and error.
</bodyText>
<equation confidence="0.845380888888889">
D1 &lt; 1 − 2Y E[n2]
E[n1]
D2 &lt; 2 − 3Y E[n3]
E[n2] (13)
D3+ ,zz� 3 − 4Y E[n4]
E[n3]
E[n1]
E[n1] + 2E[n2].
Y =
�
E[c(u′w)] =
v
p(c(cat) = r) 0.14 0.86
p(c(dog) = r) 0.1 0.9
(13)
§4.5.3
(8)
λ(u) =
</equation>
<page confidence="0.625522">
769
</page>
<sectionHeader confidence="0.961083" genericHeader="method">
6 Language model adaptation
</sectionHeader>
<bodyText confidence="0.998939">
N-gram language models are widely used in appli-
cations like machine translation and speech recog-
nition to select fluent output sentences. Although
they can easily be trained on large amounts of data,
in order to perform well, they should be trained on
data containing the right kind of language. For ex-
ample, if we want to model spoken language, then
we should train on spoken language data. If we
train on newswire, then a spoken sentence might
be regarded as ill-formed, because the distribution
of sentences in these two domains are very differ-
ent. In practice, we often have limited-size training
data from a specific domain, and large amounts
of data consisting of language from a variety of
domains (we call this general-domain data). How
can we utilize the large general-domain dataset to
help us train a model on a specific domain?
Many methods (Lin et al., 1997; Gao et al.,
2002; Klakow, 2000; Moore and Lewis, 2010; Ax-
elrod et al., 2011) rank sentences in the general-
domain data according to their similarity to the
in-domain data and select only those with score
higher than some threshold. Such methods are ef-
fective and widely used. However, sometimes it is
hard to say whether a sentence is totally in-domain
or out-of-domain; for example, quoted speech in a
news report might be partly in-domain if the do-
main of interest is broadcast conversation. Here,
we propose to assign each sentence a probability
to indicate how likely it is to belong to the domain
of interest, and train a language model using ex-
pected KN smoothing. We show that this approach
yields models with much better perplexity than the
original sentence-selection approach.
</bodyText>
<subsectionHeader confidence="0.998811">
6.1 Method
</subsectionHeader>
<bodyText confidence="0.998975333333333">
One of the most widely used sentence-selection
approaches is that of Moore and Lewis (2010).
They first train two language models, pin on a set
of in-domain data, and pout on a set of general-
domain data. Then each sentence w is assigned a
score
</bodyText>
<equation confidence="0.996888666666667">
log(pin(w)) − log(pout(w))
H(w) =
|w|
</equation>
<bodyText confidence="0.99975075">
They set a threshold on the score to select a subset.
We adapt this approach as follows. After selec-
tion, for each sentence in the subset, we use a sig-
moid function to map the scores into probabilities:
</bodyText>
<equation confidence="0.933035166666667">
1
p(w is in-domain) =
1 + exp(−H(w)).
0 0.2 0.4 0.6 0.8 1 1.2 1.4
sentences selected (x107)
igure1 : O nt hel anguagem odela daptationt ask,e
</equation>
<bodyText confidence="0.68018">
xpectedK No utperformsa llo therm ethodsa crossa
lls izes o fs electeds ubsets. I ntegralK Ni s a p-p
liedt ou nweightedi nstances, w hile f ractionalW
B,f ractionalK Na nde xpectedK Na rea ppliedt
ow eightedi nstances.T
henwe use theweightedsubsettotraina l an-g
uagem odelw ithe xpected K Ns moothing.6
</bodyText>
<equation confidence="0.382376">
.2E xperimentsM
</equation>
<bodyText confidence="0.99933104">
oorea ndL ewis( 2010) t est t heirm ethodb yp
artitioningt hei n-domaind atai ntot rainingd ataa
ndt estd ata,b otho fw hicha red isjointf romt
heg eneral-domaind ata. T heyu se t hei n-domaint
rainingd atat os electa s ubseto ft heg eneral-d
omaind ata,b uild a l anguage m odelo nt hes e-l
ecteds ubset, a nde valuatei tsp erplexityo nt hei n-d
omaint est d ata. H ere, w ef ollowt his e xperimen-t
alf rameworka ndc ompareM oorea ndL ewis’su
nweightedm ethodt oo urw eightedm ethod.F
oro ure xperiments, w e u sed a llt heE nglishd
ataa llowedf ort heB OLTP hase 1 C hinese-E
nglishe valuation. W et ook6 0ks entences ( 1.7Mw
ords)o fw ebf orumd ataa si n-domaind ata,f
urthers ubdividingi ti nto5 4ks entences ( 1.5Mw
ords)f ort raining, 3 k s entences ( 100kw ords)f
ort esting, a nd3 k s entences ( 100kw ords) f orf u-t
ureu se. T her emaining1 2.7M s entences ( 268Mw
ords)w e t reateda sg eneral-domaind ata.W
et rainedt rigraml anguage m odels a ndc om-p
arede xpectedK Ns moothinga gainst i ntegralK Ns
moothing,f ractionalW Bs moothing, a ndf rac-t
ionalK Ns moothing, m easuringp erplexitya crossv
ariouss ubsets izes ( Figure1 ). F orf ractionalK N,f
ore achs ubsets ize, w e o ptimized D t om ini-7
</bodyText>
<figure confidence="0.979897384615385">
ractionalK Nf
ractionalW Bi
ntegralK Ne
xpectedK NF
.
perplexity 260
240
220
200
180
160
140
70
</figure>
<bodyText confidence="0.9995635">
mize perplexity on the test set to give it the great-
est possible advantage; nevertheless, it is clearly
the worst performer. Expected KN consistently
gives the best perplexity, and, at the optimal sub-
set size, obtains better perplexity (148) than the
other methods (156 for integral KN, 162 for frac-
tional WB and 197 for fractional KN). Finally, we
note that integral KN is very sensitive to the subset
size, whereas expected KN and the other methods
are more robust.
</bodyText>
<sectionHeader confidence="0.918365" genericHeader="method">
7 Word Alignment
</sectionHeader>
<bodyText confidence="0.9999435">
In this section, we show how to apply expected KN
to the IBM word alignment models (Brown et al.,
1993). This illustrates both how to use expected
KN inside EM and how to use it beyond language
modeling. Of course, expected KN can be applied
to other instances of EM besides word alignment.
</bodyText>
<subsectionHeader confidence="0.993125">
7.1 Problem
</subsectionHeader>
<bodyText confidence="0.999972533333333">
Given a French sentence f = f1 f2 · · · fm and its
English translation e = e1e2 · · · en, an alignment a
is a sequence a1, a2, ... , am, where ai is the index
of the English word which generates the French
word fi, or NULL. As is common, we assume that
each French word can only be generated from one
English word or from NULL (Brown et al., 1993;
Och and Ney, 2003; Vogel et al., 1996).
The IBM models and related models define
probability distributions p(a, f  |e, 0), which model
how likely a French sentence f is to be generated
from an English sentence e with word alignment a.
Different models parameterize this probability dis-
tribution in different ways. For example, Model 1
only models the lexical translation probabilities:
</bodyText>
<equation confidence="0.943707666666667">
m
p(a, f  |e, 0) a H p(fj  |eaj).
j=1
</equation>
<bodyText confidence="0.999447444444444">
Models 2–5 and the HMM model introduce addi-
tional components to model word order and fer-
tility. All, however, have the lexical translation
model p(fj  |ei) in common. It also contains most
of the model’s parameters and is where overfit-
ting occurs most. Thus, here we only apply KN
smoothing to the lexical translation probabilities,
leaving the other model components for future
work.
</bodyText>
<subsectionHeader confidence="0.999037">
7.2 Method
</subsectionHeader>
<bodyText confidence="0.999995961538461">
The f and e are observed, while a is a latent vari-
able. Normally, in the E step, we collect expected
counts E[c(e, f)] for each e and f. Then, in the M
step, we find the parameter values that maximize
their likelihood. However, MLE is prone to over-
fitting, one symptom of which is the “garbage col-
lection” phenomenon where a rare English word is
wrongly aligned to many French words.
To reduce overfitting, we use expected KN
smoothing during the M step. That is, during the
E step, we calculate the distribution of c(e, f) for
each e and f, and during the M step, we train a
language model on bigrams e f using expected KN
smoothing (that is, with u = e and w = f). This
gives a smoothed probability estimate for p(f  |e).
One question that arises is: what distribution to
use as the lower-order distribution p′? Following
common practice in language modeling, we use
the unigram distribution p(f) as the lower-order
distribution. We could also use the uniform distri-
bution over word types, or a distribution that as-
signs zero probability to all known word types.
(The latter case is equivalent to a backoff language
model, where, since all bigrams are known, the
lower-order model is never used.) Below, we com-
pare the performance of all three choices.
</bodyText>
<subsectionHeader confidence="0.999042">
7.3 Alignment experiments
</subsectionHeader>
<bodyText confidence="0.999392619047619">
We modified GIZA++ (Och and Ney, 2003) to
perform expected KN smoothing as described
above. Smoothing is enabled or disabled with a
command-line switch, making direct comparisons
simple. Our implementation is publicly available
as open-source software.1
We carried out experiments on two language
pairs: Arabic to English and Czech to English.
For Arabic-English, we used 5.4+4.3 million
words of parallel text from the NIST 2009 con-
strained task,2 and 346 word-aligned sentence
pairs (LDC2006E86) for evaluation. For Czech-
English, we used all 2.0+2.2 million words of
training data from the WMT 2009 shared task,
and 515 word-aligned sentence pairs (Bojar and
Prokopov´a, 2006) for evaluation.
For all methods, we used five iterations of IBM
Models 1, 2, and HMM, followed by three iter-
ations of IBM Models 3 and 4. We applied ex-
pected KN smoothing to all iterations of all mod-
els. We aligned in both the foreign-to-English
</bodyText>
<footnote confidence="0.842535">
1https://github.com/hznlp/giza-kn
</footnote>
<table confidence="0.900488625">
2All data was used except for: United Nations pro-
ceedings (LDC2004E13), ISI Automatically Extracted Par-
allel Text (LDC2007E08), and Ummah newswire text
(LDC2004T18).
771
Smoothing p′ Alignment F1 B Cze-Eng
Ara-Eng Cze-Eng Ara-Eng
none (baseline) – 66.5 67.2 37.0 16.6
variational Bayes uniform 65.7 65.5 36.5 16.6
unigram 60.1 63.7 – –
fractional WB uniform 60.8 66.5 37.8 16.9
zero 60.8 65.2 – –
fractional KN unigram 67.7 70.2 37.2 16.5
unigram 69.7 71.9 38.2 17.0
expected KN uniform 69.4 71.3 – –
zero 69.2 71.9 – –
</table>
<tableCaption confidence="0.999773">
Table 1: Expected KN (interpolating with the unigram distribution) consistently outperforms all other
</tableCaption>
<bodyText confidence="0.978279029411765">
methods. For variational Bayes, we followed Riley and Gildea (2012) in setting α to zero (so that the
choice of p′ is irrelevant). For fractional KN, we chose D to maximize F1 (see Figure 2).
and English-to-foreign directions and then used
the grow-diag-final method to symmetrize them
(Koehn et al., 2003), and evaluated the alignments
using F-measure against gold word alignments.
As shown in Table 1, for KN smoothing, in-
terpolation with the unigram distribution performs
the best, while for WB smoothing, interestingly,
interpolation with the uniform distribution per-
forms the best. The difference can be explained by
the way the two smoothing methods estimate p′.
Consider again a training example with a word e
that occurs nowhere else in the training data. In
WB smoothing, p′(f) is the empirical unigram
distribution. If f contains a word that is much
more frequent than the correct translation of e,
then smoothing may actually encourage the model
to wrongly align e with the frequent word. This
is much less of a problem in KN smoothing,
where p′ is estimated from bigram types rather
than bigram tokens.
We also compared with variational Bayes (Ri-
ley and Gildea, 2012) and fractional KN. Overall,
expected KN performs the best. Variational Bayes
is not consistent across different language pairs.
While fractional KN does beat the baseline for
both language pairs, the value of D, which we op-
timized D to maximize F1, is not consistent across
language pairs: as shown in Figure 2, on Arabic-
English, a smaller D is better, while for Czech-
English, a larger D is better. By contrast, expected
KN uses a closed-form expression for D that out-
performs the best performance of fractional KN.
</bodyText>
<tableCaption confidence="0.556697666666667">
Table 2 shows that, if we apply expected KN
smoothing to only selected stages of training,
adding smoothing always brings an improvement,
</tableCaption>
<figure confidence="0.979570037037037">
0 0.2 0.4 0.6 0.8 1
D
igure2: AlignmentF1 vs. D o ff ractionalK Ns
moothingf orw orda lignment.S
moothedmodels AlignmentF1 1
2 H 3 4 A ra-EngC ze-Eng ◦
◦ ◦ ◦ ◦ 6 6.5 6 7.2 •
◦ ◦ ◦ ◦ 6 7.3 6 7.9 ◦
• ◦ ◦ ◦ 6 8.0 6 8.7 ◦
◦ • ◦ ◦ 6 8.6 7 0.0 ◦
◦ ◦ • ◦ 6 6.9 6 8.4 ◦
◦ ◦ ◦ • 6 7.0 6 8.6 •
• • • • 6 9.77 1.9 T
able2 : S moothingm ores tages o ft rainingm akesa
lignmenta ccuracyg ou p. F ore achr ow,w es
mootheda lli terationso ft hem odels i ndicated.K
ey:H H H MMm odel; • • s moothinge nabled;◦
0 s moothingd isabled.7
alignment F1
72
70
68
66
64
ze-EngA
ra-EngF
72
</figure>
<bodyText confidence="0.999587777777778">
with the best setting being to smooth all stages.
This shows that expected KN smoothing is consis-
tently effective. It is also interesting to note that
smoothing is less helpful for the fertility-based
Models 3 and 4. Whether this is because modeling
fertility makes them less susceptible to “garbage
collection,” or the way they approximate the E step
makes them less amenable to smoothing, or an-
other reason, would require further investigation.
</bodyText>
<subsectionHeader confidence="0.990841">
7.4 Translation experiments
</subsectionHeader>
<bodyText confidence="0.999962043478261">
Finally, we ran MT experiments to see whether the
improved alignments also lead to improved trans-
lations. We used the same training data as before.
For the Arabic-English tasks, we used the NIST
2008 test set as development data and the NIST
2009 test set as test data; for the Czech-English
tasks, we used the WMT 2008 test set as develop-
ment data and the WMT 2009 test set as test data.
We used the Moses toolkit (Koehn et al., 2007)
to build MT systems using various alignments
(for expected KN, we used the one interpolated
with the unigram distribution, and for fractional
WB, we used the one interpolated with the uni-
form distribution). We used a trigram language
model trained on Gigaword (AFP, AP World-
stream, CNA, and Xinhua portions), and minimum
error-rate training (Och, 2003) to tune the feature
weights.
Table 1 shows that, although the relationship
between alignment F1 and B is not very con-
sistent, expected KN smoothing achieves the best
B among all these methods and is significantly
better than the baseline (p &lt; 0.01).
</bodyText>
<sectionHeader confidence="0.994291" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999971166666667">
For a long time, and as noted by many authors,
the usage of KN smoothing has been limited by its
restriction to integer counts. In this paper, we ad-
dressed this issue by treating fractional counts as
distributions over integer counts and generalizing
KN smoothing to operate on these distributions.
This generalization makes KN smoothing, widely
considered to be the best-performing smoothing
method, applicable to many new areas. We have
demonstrated the effectiveness of our method in
two such areas and showed significant improve-
ments in both.
</bodyText>
<sectionHeader confidence="0.992143" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99972725">
We thank Qing Dou, Ashish Vaswani, Wilson Yik-
Cheung Tam, and the anonymous reviewers for
their input to this work. This research was sup-
ported in part by DOI IBC grant D12AP00225.
</bodyText>
<sectionHeader confidence="0.996402" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999861523809524">
Jes´us Andr´es-Ferrer. 2010. Statistical approaches for
natural language modelling and monotone statisti-
cal machine translation. Ph.D. thesis, Universidad
Polit´ecnica de Valencia.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proc. EMNLP, pages 355–362.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434–451.
Ondrˇej Bojar and Magdalena Prokopov´a. 2006.
Czech-English word alignment. In Proc. LREC,
pages 1236–1239.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19:263–311.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
13:359–394.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, Series B, 39:1–38.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to statisti-
cal language modeling for Chinese. ACM Transac-
tions on Asian Language Information, 1:3–33.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling: Extended version. Technical Re-
port MSR-TR-2001-72, Microsoft Research.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proc. HLT-NAACL, pages
305–312.
Yili Hong. 2013. On computing the distribution func-
tion for the Poisson binomial distribution. Compu-
tational Statistics and Data Analysis, 59:41–51.
Dietrich Klakow. 2000. Selecting articles from the
language model training corpus. In Proc. ICASSP,
pages 1695–1698.
</reference>
<page confidence="0.921812">
773
</page>
<bodyText confidence="0.780925571428571">
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In
Proc. ICASSP 1995, pages 181–184.
Joern Wuebker, Mei-Yuh Hwang, and Chris Quirk.
2012. Leave-one-out phrase model training for
large-scale deployment. In Proc. WMT, pages 460–
467.
</bodyText>
<reference confidence="0.999743959183674">
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 127–133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL, Companion Volume, pages 177–180.
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Keh-
Jiann Chen, and Lin-Shan Lee. 1997. Chinese lan-
guage model adaptation based on document classifi-
cation and multiple domain-specific language mod-
els. In Proc. Eurospeech, pages 1463–1466.
Robert Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proc.
ACL, pages 220–224.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On structuring probabilistic dependencies in
stochastic language modelling. Computer Speech
and Language, 8:1–38, 1.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL, pages
160–167.
Darcey Riley and Daniel Gildea. 2012. Improving
the IBM alignment models using variational Bayes.
In Proc. ACL (Volume 2: Short Papers), pages 306–
310.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proc. International Con-
ference on Spoken Language Processing, volume 2,
pages 901–904.
Martin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.
2011. On the estimation of discount parameters for
language model smoothing. In Proc. Interspeech,
pages 1433–1436.
Yik-Cheung Tam and Tanja Schultz. 2008. Correlated
bigram LSA for unsupervised language model adap-
tation. In Proc. NIPS, pages 1633–1640.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proc. COLING-ACL, pages 985–992.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proc. COLING, pages 836–841.
</reference>
<page confidence="0.928103">
774
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.601490">
<title confidence="0.998066">Kneser-Ney Smoothing on Expected Counts</title>
<author confidence="0.994829">Hui</author>
<affiliation confidence="0.999961">Department of Computer University of Southern</affiliation>
<email confidence="0.999069">hzhang@isi.edu</email>
<author confidence="0.841153">David</author>
<affiliation confidence="0.9995975">Information Sciences University of Southern</affiliation>
<email confidence="0.999694">chiang@isi.edu</email>
<abstract confidence="0.98599505">Widely used in speech and language processing, Kneser-Ney (KN) smoothing has consistently been shown to be one of the best-performing smoothing methods. However, KN smoothing assumes integer counts, limiting its potential uses—for example, inside Expectation-Maximization. In this paper, we propose a generalization of KN smoothing that operates on fractional counts, or, more precisely, on counts. We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts, and apply it to two tasks where KN smoothing was not applicable before: one in language model adaptation, and the other in word alignment. In both cases, our method improves performance significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jes´us Andr´es-Ferrer</author>
</authors>
<title>Statistical approaches for natural language modelling and monotone statistical machine translation.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Universidad Polit´ecnica de Valencia.</institution>
<marker>Andr´es-Ferrer, 2010</marker>
<rawString>Jes´us Andr´es-Ferrer. 2010. Statistical approaches for natural language modelling and monotone statistical machine translation. Ph.D. thesis, Universidad Polit´ecnica de Valencia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>355--362</pages>
<contexts>
<context position="17881" citStr="Axelrod et al., 2011" startWordPosition="3193" endWordPosition="3197">l spoken language, then we should train on spoken language data. If we train on newswire, then a spoken sentence might be regarded as ill-formed, because the distribution of sentences in these two domains are very different. In practice, we often have limited-size training data from a specific domain, and large amounts of data consisting of language from a variety of domains (we call this general-domain data). How can we utilize the large general-domain dataset to help us train a model on a specific domain? Many methods (Lin et al., 1997; Gao et al., 2002; Klakow, 2000; Moore and Lewis, 2010; Axelrod et al., 2011) rank sentences in the generaldomain data according to their similarity to the in-domain data and select only those with score higher than some threshold. Such methods are effective and widely used. However, sometimes it is hard to say whether a sentence is totally in-domain or out-of-domain; for example, quoted speech in a news report might be partly in-domain if the domain of interest is broadcast conversation. Here, we propose to assign each sentence a probability to indicate how likely it is to belong to the domain of interest, and train a language model using expected KN smoothing. We sho</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proc. EMNLP, pages 355–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Bisani</author>
<author>Hermann Ney</author>
</authors>
<title>Jointsequence models for grapheme-to-phoneme conversion.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>5</issue>
<contexts>
<context position="1523" citStr="Bisani and Ney, 2008" startWordPosition="216" endWordPosition="219">ases, our method improves performance significantly. 1 Introduction In speech and language processing, smoothing is essential to reduce overfitting, and Kneser-Ney (KN) smoothing (Kneser and Ney, 1995; Chen and Goodman, 1999) has consistently proven to be among the best-performing and most widely used methods. However, KN smoothing assumes integer counts, whereas in many NLP tasks, training instances appear with possibly fractional weights. Such cases have been noted for language modeling (Goodman, 2001; Goodman, 2004), domain adaptation (Tam and Schultz, 2008), grapheme-tophoneme conversion (Bisani and Ney, 2008), and phrase-based translation (Andr´es-Ferrer, 2010; Wuebker et al., 2012). For example, in Expectation-Maximization (Dempster et al., 1977), the Expectation (E) step computes the posterior distribution over possible completions of the data, and the Maximization (M) step reestimates the model parameters as if that distribution had actually been observed. In most cases, the M step is identical to estimating the model from complete data, except that counts of observations from the E step are fractional. It is common to apply add-one smoothing to the M step, but we cannot apply KN smoothing. Ano</context>
<context position="16191" citStr="Bisani and Ney (2008)" startWordPosition="2891" endWordPosition="2894"> easier than KN to adapt to fractional counts. The SRILM toolkit (Stolcke, 2002) implements a method which we call fractional WB: p(w |u) = λ(u)pmle(w |u) + (1 − λ(u))p′(w |u′) E[c(u)] E[c(u)] + n1+(u•), where n1+(u•) is the number of word types observed after context u, computed by ignoring all weights. This method, although simple, inconsistently uses weights for counting tokens but not types. Moreover, as we will see below, it does not perform as well as expected KN. The only previous adaptation of KN smoothing to fractional counts that we are aware of is that of Tam and Schultz (2008) and Bisani and Ney (2008), called fractional KN. This method subtracts D directly from the fractional counts, zeroing out counts that are smaller than D. The discount D must be set by minimizing an error metric on held-out data using a line search (Tam, p. c.) or Powell’s method (Bisani and Ney, 2008), requiring repeated estimation and evaluation of the language model. By contrast, we choose D by leaving-oneout. Like KN on integral counts, our method has a closed-form approximation and requires neither held-out data nor trial and error. D1 &lt; 1 − 2Y E[n2] E[n1] D2 &lt; 2 − 3Y E[n3] E[n2] (13) D3+ ,zz� 3 − 4Y E[n4] E[n3] E</context>
</contexts>
<marker>Bisani, Ney, 2008</marker>
<rawString>Maximilian Bisani and Hermann Ney. 2008. Jointsequence models for grapheme-to-phoneme conversion. Speech Communication, 50(5):434–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrˇej Bojar</author>
<author>Magdalena Prokopov´a</author>
</authors>
<title>Czech-English word alignment.</title>
<date>2006</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>1236--1239</pages>
<marker>Bojar, Prokopov´a, 2006</marker>
<rawString>Ondrˇej Bojar and Magdalena Prokopov´a. 2006. Czech-English word alignment. In Proc. LREC, pages 1236–1239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2867" citStr="Brown et al., 1993" startWordPosition="436" endWordPosition="439"> relevance to a particular domain), and the counts are not integral, then we again cannot train the model using KN smoothing. In this paper, we propose a generalization of KN smoothing (called expected KN smoothing) that operates on fractional counts, or, more precisely, on distributions over counts. We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts. We demonstrate how to apply expected KN to two tasks where KN smoothing was not applicable before. One is language model domain adaptation, and the other is word alignment using the IBM models (Brown et al., 1993). In both tasks, expected KN smoothing improves performance significantly. 2 Smoothing on integral counts Before presenting our method, we review KN smoothing on integer counts as applied to language models, although, as we will demonstrate in Section 7, KN smoothing is applicable to other tasks as well. 2.1 Maximum likelihood estimation Let uw stand for an n-gram, where u stands for the (n − 1) context words and w, the predicted word. Let c(uw) be the number of occurrences of uw. We use a bullet (•) to indicate summation over words, that is, c(u•) _ Ew c(uw). Under maximum-likelihood estimati</context>
<context position="21449" citStr="Brown et al., 1993" startWordPosition="3831" endWordPosition="3834">260 240 220 200 180 160 140 70 mize perplexity on the test set to give it the greatest possible advantage; nevertheless, it is clearly the worst performer. Expected KN consistently gives the best perplexity, and, at the optimal subset size, obtains better perplexity (148) than the other methods (156 for integral KN, 162 for fractional WB and 197 for fractional KN). Finally, we note that integral KN is very sensitive to the subset size, whereas expected KN and the other methods are more robust. 7 Word Alignment In this section, we show how to apply expected KN to the IBM word alignment models (Brown et al., 1993). This illustrates both how to use expected KN inside EM and how to use it beyond language modeling. Of course, expected KN can be applied to other instances of EM besides word alignment. 7.1 Problem Given a French sentence f = f1 f2 · · · fm and its English translation e = e1e2 · · · en, an alignment a is a sequence a1, a2, ... , am, where ai is the index of the English word which generates the French word fi, or NULL. As is common, we assume that each French word can only be generated from one English word or from NULL (Brown et al., 1993; Och and Ney, 2003; Vogel et al., 1996). The IBM mode</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling. Computer Speech and Language,</title>
<date>1999</date>
<pages>13--359</pages>
<contexts>
<context position="1127" citStr="Chen and Goodman, 1999" startWordPosition="158" endWordPosition="161"> In this paper, we propose a generalization of KN smoothing that operates on fractional counts, or, more precisely, on distributions over counts. We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts, and apply it to two tasks where KN smoothing was not applicable before: one in language model adaptation, and the other in word alignment. In both cases, our method improves performance significantly. 1 Introduction In speech and language processing, smoothing is essential to reduce overfitting, and Kneser-Ney (KN) smoothing (Kneser and Ney, 1995; Chen and Goodman, 1999) has consistently proven to be among the best-performing and most widely used methods. However, KN smoothing assumes integer counts, whereas in many NLP tasks, training instances appear with possibly fractional weights. Such cases have been noted for language modeling (Goodman, 2001; Goodman, 2004), domain adaptation (Tam and Schultz, 2008), grapheme-tophoneme conversion (Bisani and Ney, 2008), and phrase-based translation (Andr´es-Ferrer, 2010; Wuebker et al., 2012). For example, in Expectation-Maximization (Dempster et al., 1977), the Expectation (E) step computes the posterior distribution </context>
<context position="5748" citStr="Chen and Goodman, 1999" startWordPosition="972" endWordPosition="975"> tokens as training data is ploo(w |u) = (n1+(u•) − 1)Dqu(w) c(uw) = 1. c(u•) − 1 We want to find the D that maximizes the ∑= rnr log(r − 1 − D) + n1 log D + C, (3) r&gt;1 where nr = |{uw |c(uw) = r} |is the number of ngram types appearing r times, and C is a constant not depending on D. Setting the partial derivative with respect to D to zero, we have ∂Lloo ∑= − rnr + n1 r&gt;1 r − 1 − D ∂D D rnr ≥ r − 1 − D Solving for D, we have n1 D ≤ .(4) n1 + 2n2 Theoretically, we can use iterative methods to optimize D. But in practice, setting D to this upper bound is effective and simple (Ney et al., 1994; Chen and Goodman, 1999). 2.4 Estimating the lower-order distribution Finally, qu(w) is defined to be proportional to an (n − 1)-gram model p′(w |u′), where u′ is the (n − 2)-gram suffix of u. That is, qu(w) = γ(u)p′(w |u′), where γ(u) is an auxiliary function chosen to make the distribution p(w |u) in (2) sum to one. Absolute discounting chooses p′(w |u′) to be the maximum-likelihood unigram distribution; under KN smoothing (Kneser and Ney, 1995), it is chosen to make p in (2) satisfy the following constraint for all (n − 1)-grams u′w: ∑pmle(u′w) = p(w |vu′)pmle(vu′). (5) v ⎨⎪⎪ ⎧ ⎪⎪⎩ ˜c(uw) = ⎨⎪ ⎪⎪⎪⎪⎪⎪⎪⎧ ⎪⎪⎪⎪⎪⎪⎪⎪⎩ n</context>
<context position="12134" citStr="Chen and Goodman (1999)" startWordPosition="2152" endWordPosition="2155"> constant not depending on D. We have made the assumption that the nr are independent. By exactly the same reasoning as before, we obtain an upper bound for D: D ≤ E[n1] (10) E[n1] + 2E[n2]. In our example above, D = 1.52 1.52+2·0.24 = 0.76. 4.4 Estimating the lower-order distribution We again require p′ to satisfy the marginal constraint (5). Substituting in (7) and solving for p′ as in Section 2.4, we obtain the solution E[n1+(•u′w)] p′(w |u′) = For the example above, the estimates for the unigram model p′(w) are p′(cat) = 0.86 0.86+0.9 ≈ 0.489 p′(dog) = 0.9 0.86+0.9 ≈ 0.511. 4.5 Extensions Chen and Goodman (1999) introduce three extensions to Kneser-Ney smoothing which are now standard. For our experiments, we used all three, for both integral counts and count distributions. 4.5.1 Interpolation In interpolated KN smoothing, the subtracted discounts are redistributed not only among unseen events but also seen events. That is, c&amp;quot;(uw) = max{0, c(uw) − D} + n1+(u•)Dp′(w |u′). In this case, γ(u) is always equal to one, so that qu(w) = p′(w |u′). (Also note that (6) becomes an exact solution to the marginal constraint.) Theoretically, this requires us to derive a new estimate for D. However, as this is not </context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13:359–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<pages>39--1</pages>
<contexts>
<context position="1664" citStr="Dempster et al., 1977" startWordPosition="233" endWordPosition="236">fitting, and Kneser-Ney (KN) smoothing (Kneser and Ney, 1995; Chen and Goodman, 1999) has consistently proven to be among the best-performing and most widely used methods. However, KN smoothing assumes integer counts, whereas in many NLP tasks, training instances appear with possibly fractional weights. Such cases have been noted for language modeling (Goodman, 2001; Goodman, 2004), domain adaptation (Tam and Schultz, 2008), grapheme-tophoneme conversion (Bisani and Ney, 2008), and phrase-based translation (Andr´es-Ferrer, 2010; Wuebker et al., 2012). For example, in Expectation-Maximization (Dempster et al., 1977), the Expectation (E) step computes the posterior distribution over possible completions of the data, and the Maximization (M) step reestimates the model parameters as if that distribution had actually been observed. In most cases, the M step is identical to estimating the model from complete data, except that counts of observations from the E step are fractional. It is common to apply add-one smoothing to the M step, but we cannot apply KN smoothing. Another example is instance weighting. If we assign a weight to each training instance to indicate how important it is (say, its relevance to a </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Joshua Goodman</author>
<author>Mingjing Li</author>
<author>KaiFu Lee</author>
</authors>
<title>Toward a unified approach to statistical language modeling for Chinese.</title>
<date>2002</date>
<journal>ACM Transactions on Asian Language Information,</journal>
<pages>1--3</pages>
<contexts>
<context position="17821" citStr="Gao et al., 2002" startWordPosition="3183" endWordPosition="3186">right kind of language. For example, if we want to model spoken language, then we should train on spoken language data. If we train on newswire, then a spoken sentence might be regarded as ill-formed, because the distribution of sentences in these two domains are very different. In practice, we often have limited-size training data from a specific domain, and large amounts of data consisting of language from a variety of domains (we call this general-domain data). How can we utilize the large general-domain dataset to help us train a model on a specific domain? Many methods (Lin et al., 1997; Gao et al., 2002; Klakow, 2000; Moore and Lewis, 2010; Axelrod et al., 2011) rank sentences in the generaldomain data according to their similarity to the in-domain data and select only those with score higher than some threshold. Such methods are effective and widely used. However, sometimes it is hard to say whether a sentence is totally in-domain or out-of-domain; for example, quoted speech in a news report might be partly in-domain if the domain of interest is broadcast conversation. Here, we propose to assign each sentence a probability to indicate how likely it is to belong to the domain of interest, an</context>
</contexts>
<marker>Gao, Goodman, Li, Lee, 2002</marker>
<rawString>Jianfeng Gao, Joshua Goodman, Mingjing Li, and KaiFu Lee. 2002. Toward a unified approach to statistical language modeling for Chinese. ACM Transactions on Asian Language Information, 1:3–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>A bit of progress in language modeling: Extended version.</title>
<date>2001</date>
<tech>Technical Report MSR-TR-2001-72, Microsoft Research.</tech>
<contexts>
<context position="1410" citStr="Goodman, 2001" startWordPosition="203" endWordPosition="204">g was not applicable before: one in language model adaptation, and the other in word alignment. In both cases, our method improves performance significantly. 1 Introduction In speech and language processing, smoothing is essential to reduce overfitting, and Kneser-Ney (KN) smoothing (Kneser and Ney, 1995; Chen and Goodman, 1999) has consistently proven to be among the best-performing and most widely used methods. However, KN smoothing assumes integer counts, whereas in many NLP tasks, training instances appear with possibly fractional weights. Such cases have been noted for language modeling (Goodman, 2001; Goodman, 2004), domain adaptation (Tam and Schultz, 2008), grapheme-tophoneme conversion (Bisani and Ney, 2008), and phrase-based translation (Andr´es-Ferrer, 2010; Wuebker et al., 2012). For example, in Expectation-Maximization (Dempster et al., 1977), the Expectation (E) step computes the posterior distribution over possible completions of the data, and the Maximization (M) step reestimates the model parameters as if that distribution had actually been observed. In most cases, the M step is identical to estimating the model from complete data, except that counts of observations from the E </context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua T. Goodman. 2001. A bit of progress in language modeling: Extended version. Technical Report MSR-TR-2001-72, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Exponential priors for maximum entropy models.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>305--312</pages>
<contexts>
<context position="1426" citStr="Goodman, 2004" startWordPosition="205" endWordPosition="206">cable before: one in language model adaptation, and the other in word alignment. In both cases, our method improves performance significantly. 1 Introduction In speech and language processing, smoothing is essential to reduce overfitting, and Kneser-Ney (KN) smoothing (Kneser and Ney, 1995; Chen and Goodman, 1999) has consistently proven to be among the best-performing and most widely used methods. However, KN smoothing assumes integer counts, whereas in many NLP tasks, training instances appear with possibly fractional weights. Such cases have been noted for language modeling (Goodman, 2001; Goodman, 2004), domain adaptation (Tam and Schultz, 2008), grapheme-tophoneme conversion (Bisani and Ney, 2008), and phrase-based translation (Andr´es-Ferrer, 2010; Wuebker et al., 2012). For example, in Expectation-Maximization (Dempster et al., 1977), the Expectation (E) step computes the posterior distribution over possible completions of the data, and the Maximization (M) step reestimates the model parameters as if that distribution had actually been observed. In most cases, the M step is identical to estimating the model from complete data, except that counts of observations from the E step are fractio</context>
</contexts>
<marker>Goodman, 2004</marker>
<rawString>Joshua Goodman. 2004. Exponential priors for maximum entropy models. In Proc. HLT-NAACL, pages 305–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yili Hong</author>
</authors>
<title>On computing the distribution function for the Poisson binomial distribution.</title>
<date>2013</date>
<booktitle>Computational Statistics and Data Analysis,</booktitle>
<pages>59--41</pages>
<contexts>
<context position="9499" citStr="Hong, 2013" startWordPosition="1670" endWordPosition="1671">· 0.8 · 0.9 = 0.504 {a, b, c} 0.3 · 0.8 · 0.9 = 0.216 Then the count distributions and the E[nr] are: r = 1 r = 2 r &gt; 0 p(c(fat cat) = r) 0.62 0.24 0.86 p(c(big dog) = r) 0.9 0 0.9 E[nr] 1.52 0.24 3.2 Efficient computation How to compute these probabilities and expectations depends in general on the structure of the model. If we assume that all occurrences of uw are independent (although in fact they are not always), the computation is very easy. If there are k occurrences of uw, each occurring with probability pi, the count c(uw) is distributed according to the Poisson-binomial distribution (Hong, 2013). The expected count E[c(uw)] is just Ei pi, and the distribution of c(uw) can be computed as follows: p(c(uw) = r) = s(k, r) where s(k, r) is defined by the recurrence { s(k − 1, r)(1 − pk) + s(k − 1,r − 1)pk if 0 ≤ r ≤ k 1 ifk=r=0 0 otherwise. Z E[nr(u•)] = w Z E[nr+(u•)] = w Since, as we shall see, we only need to compute these quantities up to a small value of r (2 or 4), this takes time linear in k. We can also compute l p(c(uw) ≥ r) = max{s(m, r), 1 −Zs(m, r′) }, r′&lt;r the floor operation being needed to protect against rounding errors, and we can compute n1+(•u′w). s(k, r) = p(c(uw) = r)</context>
</contexts>
<marker>Hong, 2013</marker>
<rawString>Yili Hong. 2013. On computing the distribution function for the Poisson binomial distribution. Computational Statistics and Data Analysis, 59:41–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dietrich Klakow</author>
</authors>
<title>Selecting articles from the language model training corpus.</title>
<date>2000</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>1695--1698</pages>
<contexts>
<context position="17835" citStr="Klakow, 2000" startWordPosition="3187" endWordPosition="3188">uage. For example, if we want to model spoken language, then we should train on spoken language data. If we train on newswire, then a spoken sentence might be regarded as ill-formed, because the distribution of sentences in these two domains are very different. In practice, we often have limited-size training data from a specific domain, and large amounts of data consisting of language from a variety of domains (we call this general-domain data). How can we utilize the large general-domain dataset to help us train a model on a specific domain? Many methods (Lin et al., 1997; Gao et al., 2002; Klakow, 2000; Moore and Lewis, 2010; Axelrod et al., 2011) rank sentences in the generaldomain data according to their similarity to the in-domain data and select only those with score higher than some threshold. Such methods are effective and widely used. However, sometimes it is hard to say whether a sentence is totally in-domain or out-of-domain; for example, quoted speech in a news report might be partly in-domain if the domain of interest is broadcast conversation. Here, we propose to assign each sentence a probability to indicate how likely it is to belong to the domain of interest, and train a lang</context>
</contexts>
<marker>Klakow, 2000</marker>
<rawString>Dietrich Klakow. 2000. Selecting articles from the language model training corpus. In Proc. ICASSP, pages 1695–1698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="25980" citStr="Koehn et al., 2003" startWordPosition="4608" endWordPosition="4611">16.6 unigram 60.1 63.7 – – fractional WB uniform 60.8 66.5 37.8 16.9 zero 60.8 65.2 – – fractional KN unigram 67.7 70.2 37.2 16.5 unigram 69.7 71.9 38.2 17.0 expected KN uniform 69.4 71.3 – – zero 69.2 71.9 – – Table 1: Expected KN (interpolating with the unigram distribution) consistently outperforms all other methods. For variational Bayes, we followed Riley and Gildea (2012) in setting α to zero (so that the choice of p′ is irrelevant). For fractional KN, we chose D to maximize F1 (see Figure 2). and English-to-foreign directions and then used the grow-diag-final method to symmetrize them (Koehn et al., 2003), and evaluated the alignments using F-measure against gold word alignments. As shown in Table 1, for KN smoothing, interpolation with the unigram distribution performs the best, while for WB smoothing, interestingly, interpolation with the uniform distribution performs the best. The difference can be explained by the way the two smoothing methods estimate p′. Consider again a training example with a word e that occurs nowhere else in the training data. In WB smoothing, p′(f) is the empirical unigram distribution. If f contains a word that is much more frequent than the correct translation of </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT-NAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL, Companion Volume,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="28983" citStr="Koehn et al., 2007" startWordPosition="5161" endWordPosition="5164">“garbage collection,” or the way they approximate the E step makes them less amenable to smoothing, or another reason, would require further investigation. 7.4 Translation experiments Finally, we ran MT experiments to see whether the improved alignments also lead to improved translations. We used the same training data as before. For the Arabic-English tasks, we used the NIST 2008 test set as development data and the NIST 2009 test set as test data; for the Czech-English tasks, we used the WMT 2008 test set as development data and the WMT 2009 test set as test data. We used the Moses toolkit (Koehn et al., 2007) to build MT systems using various alignments (for expected KN, we used the one interpolated with the unigram distribution, and for fractional WB, we used the one interpolated with the uniform distribution). We used a trigram language model trained on Gigaword (AFP, AP Worldstream, CNA, and Xinhua portions), and minimum error-rate training (Och, 2003) to tune the feature weights. Table 1 shows that, although the relationship between alignment F1 and B is not very consistent, expected KN smoothing achieves the best B among all these methods and is significantly better than the baseline (p &lt; 0.0</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL, Companion Volume, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung-Chien Lin</author>
<author>Chi-Lung Tsai</author>
<author>Lee-Feng Chien</author>
<author>KehJiann Chen</author>
<author>Lin-Shan Lee</author>
</authors>
<title>Chinese language model adaptation based on document classification and multiple domain-specific language models.</title>
<date>1997</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<pages>1463--1466</pages>
<contexts>
<context position="17803" citStr="Lin et al., 1997" startWordPosition="3179" endWordPosition="3182">ta containing the right kind of language. For example, if we want to model spoken language, then we should train on spoken language data. If we train on newswire, then a spoken sentence might be regarded as ill-formed, because the distribution of sentences in these two domains are very different. In practice, we often have limited-size training data from a specific domain, and large amounts of data consisting of language from a variety of domains (we call this general-domain data). How can we utilize the large general-domain dataset to help us train a model on a specific domain? Many methods (Lin et al., 1997; Gao et al., 2002; Klakow, 2000; Moore and Lewis, 2010; Axelrod et al., 2011) rank sentences in the generaldomain data according to their similarity to the in-domain data and select only those with score higher than some threshold. Such methods are effective and widely used. However, sometimes it is hard to say whether a sentence is totally in-domain or out-of-domain; for example, quoted speech in a news report might be partly in-domain if the domain of interest is broadcast conversation. Here, we propose to assign each sentence a probability to indicate how likely it is to belong to the doma</context>
</contexts>
<marker>Lin, Tsai, Chien, Chen, Lee, 1997</marker>
<rawString>Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, KehJiann Chen, and Lin-Shan Lee. 1997. Chinese language model adaptation based on document classification and multiple domain-specific language models. In Proc. Eurospeech, pages 1463–1466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data. In</title>
<date>2010</date>
<booktitle>Proc. ACL,</booktitle>
<pages>220--224</pages>
<contexts>
<context position="17858" citStr="Moore and Lewis, 2010" startWordPosition="3189" endWordPosition="3192">ple, if we want to model spoken language, then we should train on spoken language data. If we train on newswire, then a spoken sentence might be regarded as ill-formed, because the distribution of sentences in these two domains are very different. In practice, we often have limited-size training data from a specific domain, and large amounts of data consisting of language from a variety of domains (we call this general-domain data). How can we utilize the large general-domain dataset to help us train a model on a specific domain? Many methods (Lin et al., 1997; Gao et al., 2002; Klakow, 2000; Moore and Lewis, 2010; Axelrod et al., 2011) rank sentences in the generaldomain data according to their similarity to the in-domain data and select only those with score higher than some threshold. Such methods are effective and widely used. However, sometimes it is hard to say whether a sentence is totally in-domain or out-of-domain; for example, quoted speech in a news report might be partly in-domain if the domain of interest is broadcast conversation. Here, we propose to assign each sentence a probability to indicate how likely it is to belong to the domain of interest, and train a language model using expect</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proc. ACL, pages 220–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Ute Essen</author>
<author>Reinhard Kneser</author>
</authors>
<title>On structuring probabilistic dependencies in stochastic language modelling.</title>
<date>1994</date>
<journal>Computer Speech and Language,</journal>
<volume>8</volume>
<pages>1</pages>
<contexts>
<context position="3993" citStr="Ney et al., 1994" startWordPosition="624" endWordPosition="627">indicate summation over words, that is, c(u•) _ Ew c(uw). Under maximum-likelihood estimation (MLE), we max765 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 765–774, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics imize leaving-one-out log-likelihood ∑L = c(uw) log p(w |u), ∑Lloo = c(uw) log ploo(w |u) uw uw obtaining the solution pmle(w |u) = ∑= uw|c(uw)&gt;1 c(uw) − 1 − D c(uw) log c(u•) − 1 c(uw) (1) c(u•) . ∑+ (n1+(u•) − 1)Dqu(w) uw|c(uw)=1 log c(u•) − 1 2.2 Absolute discounting Absolute discounting (Ney et al., 1994) – on which KN smoothing is based – tries to generalize better to unseen data by subtracting a discount from each seen n-gram’s count and distributing the subtracted discounts to unseen n-grams. For now, we assume that the discount is a constant D, so that the smoothed counts are c(uw) − D if c(uw) &gt; 0 n1+(u•)Dqu(w) otherwise where n1+(u•) = |{w |c(uw) &gt; 0} |is the number of word types observed after context u, and qu(w) specifies how to distribute the subtracted discounts among unseen n-gram types. Maximizing the likelihood of the smoothed counts ˜c, we get p(w |u) = ⎧ ⎪ ⎪⎪⎪⎪⎪ c(uw) − D (2) ⎨</context>
<context position="5723" citStr="Ney et al., 1994" startWordPosition="968" endWordPosition="971">uw using the other tokens as training data is ploo(w |u) = (n1+(u•) − 1)Dqu(w) c(uw) = 1. c(u•) − 1 We want to find the D that maximizes the ∑= rnr log(r − 1 − D) + n1 log D + C, (3) r&gt;1 where nr = |{uw |c(uw) = r} |is the number of ngram types appearing r times, and C is a constant not depending on D. Setting the partial derivative with respect to D to zero, we have ∂Lloo ∑= − rnr + n1 r&gt;1 r − 1 − D ∂D D rnr ≥ r − 1 − D Solving for D, we have n1 D ≤ .(4) n1 + 2n2 Theoretically, we can use iterative methods to optimize D. But in practice, setting D to this upper bound is effective and simple (Ney et al., 1994; Chen and Goodman, 1999). 2.4 Estimating the lower-order distribution Finally, qu(w) is defined to be proportional to an (n − 1)-gram model p′(w |u′), where u′ is the (n − 2)-gram suffix of u. That is, qu(w) = γ(u)p′(w |u′), where γ(u) is an auxiliary function chosen to make the distribution p(w |u) in (2) sum to one. Absolute discounting chooses p′(w |u′) to be the maximum-likelihood unigram distribution; under KN smoothing (Kneser and Ney, 1995), it is chosen to make p in (2) satisfy the following constraint for all (n − 1)-grams u′w: ∑pmle(u′w) = p(w |vu′)pmle(vu′). (5) v ⎨⎪⎪ ⎧ ⎪⎪⎩ ˜c(uw) </context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>Hermann Ney, Ute Essen, and Reinhard Kneser. 1994. On structuring probabilistic dependencies in stochastic language modelling. Computer Speech and Language, 8:1–38, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="22014" citStr="Och and Ney, 2003" startWordPosition="3945" endWordPosition="3948">o the IBM word alignment models (Brown et al., 1993). This illustrates both how to use expected KN inside EM and how to use it beyond language modeling. Of course, expected KN can be applied to other instances of EM besides word alignment. 7.1 Problem Given a French sentence f = f1 f2 · · · fm and its English translation e = e1e2 · · · en, an alignment a is a sequence a1, a2, ... , am, where ai is the index of the English word which generates the French word fi, or NULL. As is common, we assume that each French word can only be generated from one English word or from NULL (Brown et al., 1993; Och and Ney, 2003; Vogel et al., 1996). The IBM models and related models define probability distributions p(a, f |e, 0), which model how likely a French sentence f is to be generated from an English sentence e with word alignment a. Different models parameterize this probability distribution in different ways. For example, Model 1 only models the lexical translation probabilities: m p(a, f |e, 0) a H p(fj |eaj). j=1 Models 2–5 and the HMM model introduce additional components to model word order and fertility. All, however, have the lexical translation model p(fj |ei) in common. It also contains most of the m</context>
<context position="24131" citStr="Och and Ney, 2003" startWordPosition="4313" endWordPosition="4316"> estimate for p(f |e). One question that arises is: what distribution to use as the lower-order distribution p′? Following common practice in language modeling, we use the unigram distribution p(f) as the lower-order distribution. We could also use the uniform distribution over word types, or a distribution that assigns zero probability to all known word types. (The latter case is equivalent to a backoff language model, where, since all bigrams are known, the lower-order model is never used.) Below, we compare the performance of all three choices. 7.3 Alignment experiments We modified GIZA++ (Och and Ney, 2003) to perform expected KN smoothing as described above. Smoothing is enabled or disabled with a command-line switch, making direct comparisons simple. Our implementation is publicly available as open-source software.1 We carried out experiments on two language pairs: Arabic to English and Czech to English. For Arabic-English, we used 5.4+4.3 million words of parallel text from the NIST 2009 constrained task,2 and 346 word-aligned sentence pairs (LDC2006E86) for evaluation. For CzechEnglish, we used all 2.0+2.2 million words of training data from the WMT 2009 shared task, and 515 word-aligned sen</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="29336" citStr="Och, 2003" startWordPosition="5219" endWordPosition="5220">we used the NIST 2008 test set as development data and the NIST 2009 test set as test data; for the Czech-English tasks, we used the WMT 2008 test set as development data and the WMT 2009 test set as test data. We used the Moses toolkit (Koehn et al., 2007) to build MT systems using various alignments (for expected KN, we used the one interpolated with the unigram distribution, and for fractional WB, we used the one interpolated with the uniform distribution). We used a trigram language model trained on Gigaword (AFP, AP Worldstream, CNA, and Xinhua portions), and minimum error-rate training (Och, 2003) to tune the feature weights. Table 1 shows that, although the relationship between alignment F1 and B is not very consistent, expected KN smoothing achieves the best B among all these methods and is significantly better than the baseline (p &lt; 0.01). 8 Conclusion For a long time, and as noted by many authors, the usage of KN smoothing has been limited by its restriction to integer counts. In this paper, we addressed this issue by treating fractional counts as distributions over integer counts and generalizing KN smoothing to operate on these distributions. This generalization makes KN smoothin</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darcey Riley</author>
<author>Daniel Gildea</author>
</authors>
<title>Improving the IBM alignment models using variational Bayes.</title>
<date>2012</date>
<booktitle>In Proc. ACL (Volume 2: Short Papers),</booktitle>
<pages>306--310</pages>
<contexts>
<context position="25741" citStr="Riley and Gildea (2012)" startWordPosition="4567" endWordPosition="4570">E13), ISI Automatically Extracted Parallel Text (LDC2007E08), and Ummah newswire text (LDC2004T18). 771 Smoothing p′ Alignment F1 B Cze-Eng Ara-Eng Cze-Eng Ara-Eng none (baseline) – 66.5 67.2 37.0 16.6 variational Bayes uniform 65.7 65.5 36.5 16.6 unigram 60.1 63.7 – – fractional WB uniform 60.8 66.5 37.8 16.9 zero 60.8 65.2 – – fractional KN unigram 67.7 70.2 37.2 16.5 unigram 69.7 71.9 38.2 17.0 expected KN uniform 69.4 71.3 – – zero 69.2 71.9 – – Table 1: Expected KN (interpolating with the unigram distribution) consistently outperforms all other methods. For variational Bayes, we followed Riley and Gildea (2012) in setting α to zero (so that the choice of p′ is irrelevant). For fractional KN, we chose D to maximize F1 (see Figure 2). and English-to-foreign directions and then used the grow-diag-final method to symmetrize them (Koehn et al., 2003), and evaluated the alignments using F-measure against gold word alignments. As shown in Table 1, for KN smoothing, interpolation with the unigram distribution performs the best, while for WB smoothing, interestingly, interpolation with the uniform distribution performs the best. The difference can be explained by the way the two smoothing methods estimate p′</context>
</contexts>
<marker>Riley, Gildea, 2012</marker>
<rawString>Darcey Riley and Daniel Gildea. 2012. Improving the IBM alignment models using variational Bayes. In Proc. ACL (Volume 2: Short Papers), pages 306– 310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="15650" citStr="Stolcke, 2002" startWordPosition="2797" endWordPosition="2798"> compute smoothed counts c˜ compute probabilities p (9) The computational complexity of expected KN is almost identical to KN on integral counts. The main addition is computing and storing the count distributions. Using the dynamic program in Section 3.2, computing the distributions for each r is linear in the number of n-gram types, and we only need to compute the distributions up to r = 2 (or r = 4 for modified KN), and store them for r = 0 (or up to r = 2 for modified KN). 5 Related Work Witten-Bell (WB) smoothing is somewhat easier than KN to adapt to fractional counts. The SRILM toolkit (Stolcke, 2002) implements a method which we call fractional WB: p(w |u) = λ(u)pmle(w |u) + (1 − λ(u))p′(w |u′) E[c(u)] E[c(u)] + n1+(u•), where n1+(u•) is the number of word types observed after context u, computed by ignoring all weights. This method, although simple, inconsistently uses weights for counting tokens but not types. Moreover, as we will see below, it does not perform as well as expected KN. The only previous adaptation of KN smoothing to fractional counts that we are aware of is that of Tam and Schultz (2008) and Bisani and Ney (2008), called fractional KN. This method subtracts D directly fr</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proc. International Conference on Spoken Language Processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Ralf Schl¨uter</author>
<author>Hermann Ney</author>
</authors>
<title>On the estimation of discount parameters for language model smoothing. In</title>
<date>2011</date>
<booktitle>Proc. Interspeech,</booktitle>
<pages>1433--1436</pages>
<marker>Sundermeyer, Schl¨uter, Ney, 2011</marker>
<rawString>Martin Sundermeyer, Ralf Schl¨uter, and Hermann Ney. 2011. On the estimation of discount parameters for language model smoothing. In Proc. Interspeech, pages 1433–1436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Tanja Schultz</author>
</authors>
<title>Correlated bigram LSA for unsupervised language model adaptation. In</title>
<date>2008</date>
<booktitle>Proc. NIPS,</booktitle>
<pages>1633--1640</pages>
<contexts>
<context position="1469" citStr="Tam and Schultz, 2008" startWordPosition="209" endWordPosition="212"> adaptation, and the other in word alignment. In both cases, our method improves performance significantly. 1 Introduction In speech and language processing, smoothing is essential to reduce overfitting, and Kneser-Ney (KN) smoothing (Kneser and Ney, 1995; Chen and Goodman, 1999) has consistently proven to be among the best-performing and most widely used methods. However, KN smoothing assumes integer counts, whereas in many NLP tasks, training instances appear with possibly fractional weights. Such cases have been noted for language modeling (Goodman, 2001; Goodman, 2004), domain adaptation (Tam and Schultz, 2008), grapheme-tophoneme conversion (Bisani and Ney, 2008), and phrase-based translation (Andr´es-Ferrer, 2010; Wuebker et al., 2012). For example, in Expectation-Maximization (Dempster et al., 1977), the Expectation (E) step computes the posterior distribution over possible completions of the data, and the Maximization (M) step reestimates the model parameters as if that distribution had actually been observed. In most cases, the M step is identical to estimating the model from complete data, except that counts of observations from the E step are fractional. It is common to apply add-one smoothin</context>
<context position="16165" citStr="Tam and Schultz (2008)" startWordPosition="2886" endWordPosition="2889"> (WB) smoothing is somewhat easier than KN to adapt to fractional counts. The SRILM toolkit (Stolcke, 2002) implements a method which we call fractional WB: p(w |u) = λ(u)pmle(w |u) + (1 − λ(u))p′(w |u′) E[c(u)] E[c(u)] + n1+(u•), where n1+(u•) is the number of word types observed after context u, computed by ignoring all weights. This method, although simple, inconsistently uses weights for counting tokens but not types. Moreover, as we will see below, it does not perform as well as expected KN. The only previous adaptation of KN smoothing to fractional counts that we are aware of is that of Tam and Schultz (2008) and Bisani and Ney (2008), called fractional KN. This method subtracts D directly from the fractional counts, zeroing out counts that are smaller than D. The discount D must be set by minimizing an error metric on held-out data using a line search (Tam, p. c.) or Powell’s method (Bisani and Ney, 2008), requiring repeated estimation and evaluation of the language model. By contrast, we choose D by leaving-oneout. Like KN on integral counts, our method has a closed-form approximation and requires neither held-out data nor trial and error. D1 &lt; 1 − 2Y E[n2] E[n1] D2 &lt; 2 − 3Y E[n3] E[n2] (13) D3+</context>
</contexts>
<marker>Tam, Schultz, 2008</marker>
<rawString>Yik-Cheung Tam and Tanja Schultz. 2008. Correlated bigram LSA for unsupervised language model adaptation. In Proc. NIPS, pages 1633–1640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>985--992</pages>
<contexts>
<context position="13869" citStr="Teh, 2006" startWordPosition="2490" endWordPosition="2491">· 0.511 ≈ 0.566 r&gt;1 = E[n1] log D Z+ rE[nr] log(r − 1 − D) + C, r&gt;1 768 which give the smoothed probability estimates: 0.766 = 0.696 p(cat |= fat) 0.766+0.334 0.334 = 0.304 fat) p(dog |= 0.766+0.334 0.334 = 0.371 big) p(dog |= 0.334+0.556 0.556 = 0.629. big) p(cat |= 0.334+0.556 4.5.2 Modified discounts Modified KN smoothing uses a different discount Dr for each count r &lt; 3, and a discount D3+ for counts r &gt; 3. On count distributions, a similar argument to the above leads to the estimates: One side-effect of this change is that (6) is no longer the correct solution to the marginal constraint (Teh, 2006; Sundermeyer et al., 2011). Although this problem can be fixed, standard implementations simply use (6). 4.5.3 Recursive smoothing In the original KN method, the lower-order model p′ was estimated using (6); recursive KN smoothing applies KN smoothing to p′. To do this, we need to reconstruct counts whose MLE is (6). On integral counts, this is simple: we generate, for each n-gram type vu′w, an (n−1)-gram token u′w, for a total of n1+(•u′w) tokens. We then apply KN smoothing to these counts. Analogously, on count distributions, for each ngram type vu′w, we generate an (n − 1)-gram token u′w w</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proc. COLING-ACL, pages 985–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="22035" citStr="Vogel et al., 1996" startWordPosition="3949" endWordPosition="3952">nment models (Brown et al., 1993). This illustrates both how to use expected KN inside EM and how to use it beyond language modeling. Of course, expected KN can be applied to other instances of EM besides word alignment. 7.1 Problem Given a French sentence f = f1 f2 · · · fm and its English translation e = e1e2 · · · en, an alignment a is a sequence a1, a2, ... , am, where ai is the index of the English word which generates the French word fi, or NULL. As is common, we assume that each French word can only be generated from one English word or from NULL (Brown et al., 1993; Och and Ney, 2003; Vogel et al., 1996). The IBM models and related models define probability distributions p(a, f |e, 0), which model how likely a French sentence f is to be generated from an English sentence e with word alignment a. Different models parameterize this probability distribution in different ways. For example, Model 1 only models the lexical translation probabilities: m p(a, f |e, 0) a H p(fj |eaj). j=1 Models 2–5 and the HMM model introduce additional components to model word order and fertility. All, however, have the lexical translation model p(fj |ei) in common. It also contains most of the model’s parameters and</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proc. COLING, pages 836–841.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>