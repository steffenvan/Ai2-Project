<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014686">
<title confidence="0.998933">
Question Answering Using a Large Text Database:
A Machine Learning Approach
</title>
<author confidence="0.908169333333333">
Hwee Tou Ng*
Jennifer Lai Pheng Kwan
Yiyuan Xia
</author>
<affiliation confidence="0.632433">
DSO National Laboratories
</affiliation>
<address confidence="0.8621">
20 Science Park Drive
Singapore 118230
</address>
<email confidence="0.984804">
Inhweetou, klaiphen, xyiyuanl@dso.org.sg
</email>
<sectionHeader confidence="0.995276" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892076923077">
In this paper, we present a machine learning ap-
proach to question answering. The task is answering
factual questions, where the answers are to be found
in documents in a large text database. We trained
our system on 398 questions from the Remedia cor-
pus, as well as 38 TREC-8 development questions.
We then evaluated our system on 198 questions of
the TREC-8 question answering task. Although our
learning approach only uses 4 features, we are able
to achieve quite competitive accuracy. The results
indicate that such a machine learning approach is
a promising way to build a state-of-the-art question
answering system.
</bodyText>
<sectionHeader confidence="0.997905" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99934736">
Finding information in a large text database is the
traditional subject of study in information retrieval.
When given some keywords, current search engines
retrieve numerous web pages that contain the key-
words, and leave it to a user to sieve through the
large set of returned web pages to find the informa-
tion that he needs. That is, search engines deal more
with whole document retrieval.
However, what a user often wants is really a pre-
cise answer to a question. For instance, given the
question &amp;quot;When was the last US presidential elec-
tion held?&amp;quot;, what he wants is the answer &amp;quot;Nov 7,
2000&amp;quot;, rather than to read through lots of web pages
that contain the words &amp;quot;US&amp;quot;, &amp;quot;presidential&amp;quot;, &amp;quot;elec-
tion&amp;quot;, etc to find the date of election. That is, what
a user needs is information retrieval, rather than the
current document retrieval.
Question answering (QA) has recently attracted
a lot of research activities. This is in part fueled
by the question answering track of TREC. TREC
is an annual exercise to evaluate the performance of
text retrieval systems on common, large real-world
text collections, using a uniform scoring procedure.
The question answering track started in TREC-8
in 1999 (Voorhees, 2000; Voorhees and Tice, 2000,
</bodyText>
<footnote confidence="0.834103">
* Hwee Tou Ng is also affiliated with Department of Com-
puter Science, School of Computing, National University of
Singapore, http://www.comp.nus .edu.sg/nght
</footnote>
<bodyText confidence="0.999797454545455">
TREC). In the question answering track, for each
factual question, the task is to extract the top five
50-byte or 250-byte answers to the question from a
large text database consisting of hundreds of thou-
sands of documents (gigabytes of text).
Another strand of work that deals with question
answering surfaces in the context of reading com-
prehension. The group at MITRE (Hirschman et
al., 1999) initiated work on reading comprehension
of children stories. In particular, the task involves
choosing a sentence from a story that best answers
a question posed to the story. For this task, there
is no need to deal with retrieval from a large text
database, since a question is directed at a particular
story.
In this paper, we address the task of answering
factual questions, where the answers are to be found
in documents in a large text database. We adopt
a machine learning approach to question answering.
In particular, answer candidates are classified and
ranked by a classifier trained on a set of question-
answer pairs.
</bodyText>
<sectionHeader confidence="0.974471" genericHeader="introduction">
2 A Machine Learning Approach
</sectionHeader>
<subsectionHeader confidence="0.872136">
2.1 Natural Language Processing Modules
</subsectionHeader>
<bodyText confidence="0.999783190476191">
Our question answering system utilizes a number of
natural language processing (NLP) modules. They
include sentence segmentation, tokenization, mor-
phological analysis, part-of-speech tagging, noun
phrase chunking, named entity tagging, and seman-
tic class determination. The main goal of these mod-
ules is to identify the boundary and the semantic
class of the noun phrases, so that the necessary fea-
ture values needed to form the training or test ex-
amples can be computed.
Our part-of-speech tagger is a standard statistical
bigram tagger based on the Hidden Markov Model
(HMM) (Church, 1988). Similarly, we built a sta-
tistical HMM-based noun phrase chunker where the
noun phrase boundaries are determined solely based
on the part-of-speech tags assigned to the words in
a sentence. We also implemented a module that as-
signs named entity tags. In particular, the following
named entity types are recognized: human, orga-
nization, location, date, time, percent, and money.
Our named entity tagger uses the HMM approach
of (Bikel et al., 1999), which learns from a tagged
corpus of named entities. Our part-of-speech tag-
ger, noun phrase chunker, and named entity tagger
achieve state-of-the-art accuracy.
The semantic classes defined for noun phrases in
our QA system are: human, organization, location,
date, time, percent, money, and entity. Each of the
7 semantic classes human, organization, ... , money
is a subclass of the semantic class entity, which is a
catch-all semantic class for all noun phrases that are
not of the other 7 defined semantic classes. Each of
these semantic classes is then mapped to a WORD-
NET synset (Miller, 1990). Our semantic class deter-
mination module assumes that the semantic class for
every noun phrase extracted follows the first sense of
the head noun of the noun phrase. Since WORDNET
orders the senses of a noun by their frequency, this
is equivalent to choosing the most frequent sense,
which then determines the semantic class of the noun
phrase through upward traversal of the ISA hierar-
chy Of WORDNET.
</bodyText>
<subsectionHeader confidence="0.966549">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.999829571428571">
The learning task is defined as identifying the best
noun phrase that is most likely to answer a given
question.
Our feature vector consists of 4 features. Each
feature vector is derived from one triple q, n, and s,
where q is the question, n is the noun phrase, and s
is the sentence containing the noun phrase n.
</bodyText>
<listItem confidence="0.977594">
• Question type (QT)
</listItem>
<bodyText confidence="0.999598193548387">
This feature attempts to capture the focus of a
question q, i.e., what a question is asking for.
Its possible values are who, when, where, how,
human, organization, location, date, time, per-
cent, money, and entity.
The value for this feature is determined as fol-
lows. Step through each successive word w in q
(starting from the first word in q), and check if
w is one of the words &amp;quot;who&amp;quot;, &amp;quot;whom&amp;quot;, &amp;quot;whose&amp;quot;,
&amp;quot;when&amp;quot;, &amp;quot;where&amp;quot; and &amp;quot;how&amp;quot; (in that order).
The first match found will determine the ques-
tion type of q. If the first matching word is one
of &amp;quot;who&amp;quot;, &amp;quot;whom&amp;quot;, or &amp;quot;whose&amp;quot;, then the ques-
tion type is who. If the first matching word is
&amp;quot;when&amp;quot;, the question type is when. Similarly
for where and how.
If no match is found, the following heuristic
is used to determine the question type. The
heuristic searches for the first noun phrase in
the question that does not occur after a non-
&amp;quot;be&amp;quot; verb, and whose head word is not the word
&amp;quot;name&amp;quot;. The semantic class of this head word
is then used as the question type. For example,
in the question &amp;quot;What is the name of the man-
aging director of Apricot Computer?&amp;quot;, the first
noun phrase satisfying the conditions is &amp;quot;man-
aging director&amp;quot; and the semantic class of its
head word &amp;quot;director&amp;quot;, which is human, is the
question type.
If no such head word can be found, then the
question is given the type entity.
</bodyText>
<listItem confidence="0.988223">
• Noun phrase semantic class (NPSC)
</listItem>
<bodyText confidence="0.991548857142857">
The possible values of this feature are hu-
man, organization, location, date, time, per-
cent, money, and entity.
If a named entity tag is assigned to the noun
phrase n, then NPSC is assigned the value of the
named entity tag. Otherwise, NPSC is assigned
the value of the semantic class of n.
</bodyText>
<listItem confidence="0.60516">
• Quantitative noun phrase (QNP)
The possible values are true and false.
</listItem>
<bodyText confidence="0.8283488">
If the noun phrase n contains a number and
its NPSC is not date or time, then this value is
true. Otherwise, this value is false. This feature
is indicative of answers to &amp;quot;how many&amp;quot;, &amp;quot;how
far&amp;quot;, &amp;quot;how long&amp;quot;, etc type of &amp;quot;how&amp;quot; questions.
</bodyText>
<listItem confidence="0.99464">
• Diff-from-Max-Word-Match between s and q
(DMWM)
</listItem>
<bodyText confidence="0.9999841">
The possible values are 0, 1, 2, 3, .... This
feature attempts to capture the word overlap
between a question and a sentence. It is similar
to that used in our previous work (Ng et al.,
2000).
To compute the value of this feature, all words
in the question q and the sentence s (which con-
tains the noun phrase n) are reduced to their
morphological roots. Let m be the number of
morphological root words in q which can be
found in s. Stop words are excluded in the
count. Then for all the possible sentences si in
the defined search space&apos;, let mi be the num-
ber of morphological root words in q which can
be found in si. Define M = maxi{mi}. The
value of this feature is then computed as M—m.
That is, for a sentence s that has the maximum
number of words that overlap with q (among all
sentences in the defined search space), its value
for this feature will be 0.
</bodyText>
<subsectionHeader confidence="0.99547">
2.3 Training
</subsectionHeader>
<bodyText confidence="0.9767501">
Given a question q, a noun phrase n that is marked
as an answer to q, as well as the sentence s containing
n, the triple q, n, and s are used to generate a pos-
itive training example, as described in Section 2.2.
To compute the value of DMWM, the search space
1-The list of sentences considered in this search space will be
explained in the following subsections on training and testing.
is defined as all the sentences in d, where d is the
document containing 5.
Negative training examples are generated by ran-
domly selecting one noun phrase (other than n) each
from sentences 5, 5_1 and 5+1, where s_i is the sen-
tence immediately preceding s, and s+i is the sen-
tence immediately following s.
A classifier is then built based on the feature vec-
tors generated from the training questions and an-
swers. The learning algorithm used is C5 with boost-
ing (Freund and Schapire, 1996). C5 is a more recent
version of C4.5 (Quinlan, 1993). We use the default
values for all C5 learning parameters.
</bodyText>
<subsectionHeader confidence="0.99775">
2.4 Testing
</subsectionHeader>
<bodyText confidence="0.99995625">
Before identifying all possible noun phrases for test-
ing, the search space of all documents in a large text
database will have to be efficiently narrowed down
first.
</bodyText>
<subsubsectionHeader confidence="0.751526">
2.4.1 Passage retrieval
</subsubsectionHeader>
<bodyText confidence="0.99996228">
The retrieval system we use is based on that de-
scribed in (Singhal et al., 2000). For a given ques-
tion, query terms (i.e., words in the question) are
first extracted by removing stopwords and punctu-
ation symbols. A term weight is then assigned to
each query term t, using the idf factor: log(1+)
where N is the total number of documents in the
text database, and dt is the number of documents in
the database containing term t.
Within a document, a passage is any contiguous
text string which contains up to a maximum of 5 con-
secutive sentences, or at most 500 bytes. Each candi-
date passage in a document is assigned a score, based
on the sum of the weights of all query terms con-
tained in the passage. See (Singhal et al., 2000) for
more details of how passage is selected and scored.
In our experiments with TREC-8 test data, we
used the top 200 documents per question provided
by AT&amp;T&apos;s retrieval engine. This set of documents
is also provided to all TREC-8 participants. The
3 top-scoring passages of each of the top 200 doc-
uments are selected, and these passages are ranked
in descending order of the passage score. The top
200 passages are then passed to the answer ranking
module described in Section 2.4.2.
</bodyText>
<subsubsectionHeader confidence="0.801063">
2.4.2 Answer ranking
</subsubsectionHeader>
<bodyText confidence="0.999969942857143">
Each noun phrase in the retrieved passages con-
tributes a test example. The question q, the noun
phrase n, and the sentence s containing n are used to
generate a test example. For a given question q, the
value of M needed in the computation of DMWM
is taken over the search space of all the sentences
in the retrieved passages for q. The classifier as-
signs a positive or negative class together with a
confidence value to each test example. The noun
phrases are then sorted according to their classifi-
cation and confidence values. Noun phrases with
positive class are ranked above noun phrases with
negative class. If 2 noun phrases have the same pos-
itive class, then the one with the higher confidence
score is ranked higher. If 2 noun phrases have the
same negative class, then the one with the lower con-
fidence is ranked higher. In the event where 2 or
more noun phrases have the same classification and
confidence value, ties are broken based on the rank
of the passages from which the noun phrases origi-
nate. Ties are further broken based on the position
of the noun phrase in the passage, i.e., noun phrases
occurring earlier in the passage are ranked higher
than those occurring later.
The top ranked noun phrase is then expanded
with its neighboring words in the passage to make
up a 50-byte (or 250-byte) answer string. The 2nd
ranked noun phrase is then checked to ensure that
it does not occur in the earlier chosen 50-byte (or
250-byte) answer strings. If it does, then the next
ranked noun phrase is considered. If it does not,
then the noun phrase is expanded to 50 bytes (or
250 bytes). This process of selecting and expanding
noun phrases continues until finally 5 answer strings
are chosen.
</bodyText>
<sectionHeader confidence="0.997522" genericHeader="background">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999508548387097">
The training data we used to build our classifier
consists of two parts. The first part is the set
of 38 TREC-8 QA track development questions
(Voorhees, 2000). Document collection for this set
of questions is the same as in TREC-8 ad hoc task,
namely the set of documents on TREC disk 4 and
5 minus the Congressional Record documents. In
this development set, we have questions containing
&amp;quot;what&amp;quot;, &amp;quot;who&amp;quot;, &amp;quot;how&amp;quot;, &amp;quot;when&amp;quot;, &amp;quot;where&amp;quot;, and a few
others that do not have any of these words. An-
swers to these questions are also provided by NIST
together with the document id of the documents
where the answers can be found. Answer phrases
were then manual tagged in the specified documents
and training examples were generated according to
the scheme described in Section 2.
The second part of the training data is based on
the questions and stories published by Remedia Pub-
lications (Hirschman et al., 1999). The document
collection for this set consists of stories from grade
2 to 5 with a total of 115 stories. Each of the sto-
ries comes with five questions. However, not all the
questions can be used for training, since some of the
questions cannot be answered by any noun phrase
in the associated story. We also ignore the &amp;quot;Why&amp;quot;
questions, since their answers are typically not noun
phrases. After removing the &amp;quot;Why&amp;quot; questions and
questions without a noun-phrase answer, we have
398 questions left for training.
The collection of stories we used is the copy cre-
ated by the MITRE group with each story manu-
</bodyText>
<table confidence="0.999974">
Question Remedia TREC-8 TREC-8
type develop, set test set
What 101 18 65
Who 98 7 48
How 0 6 31
When 95 3 18
Where 104 2 21
Others 0 2 15
Total 398 38 198
</table>
<tableCaption confidence="0.999982">
Table 1: Questions in the training and test set
</tableCaption>
<bodyText confidence="0.99985125">
ally annotated to indicate which sentence answers
to each of the questions associated with the story.
However, since in our training, we require the answer
noun phrase instead of answer sentence, each story
was then further hand-tagged to indicate the answer
noun phrase. After this stage, the same scheme de-
scribed in Section 2 was employed to generate train-
ing examples.
The test data we used is the set of questions in
the official test set of TREC-8 QA Track (Voorhees,
2000). This set consists of 198 questions which has
no overlap with the 38 development questions. Doc-
ument collection is the same as TREC-8 ad hoc task.
Table 1 shows the number of questions of each type
in the training and test set.
Our evaluation is based on the official evalua-
tion program and answer patterns released by the
TREC-8 QA track organizer (Voorhees and Tice,
2000, SIGIR). In TREC-8, the official evaluation is
done by human assessors. However, since we did
not participate in the TREC-8 QA track, our runs
have not been evaluated by the human assessors who
judged the other TREC-8 official runs. Fortunately,
as demonstrated in (Voorhees and Tice, 2000, SI-
GIR), evaluation based on the evaluation program
and answer patterns gives comparable outcome as
the evaluation done by human judges.
The evaluation metric we used is mean reciprocal
rank (MRR), the same as that used in TREC-8. For
each question, if the rank at which the first correct
answer appears is k, then the question gets a score
of 1/k, where k = 1, 2, 3, 4, 5. If the correct answer
is not found in the top 5 strings returned, then the
question gets a score of 0. The overall score is the
average MRR of all the 198 test questions.
The MRR score for our 50-byte run is 0.357, and
98 questions do not have any answer within the top
5 strings returned. For the 250-byte run, the MRR
score is 0.525, and 71 questions do not have any
answer within the top 5 strings returned. Table 2
shows the percentage of questions answered correctly
(i.e., the answer appears within the top 5 strings) for
the two runs, broken down according to the question
types.
</bodyText>
<figureCaption confidence="0.9999925">
Figure 1: Results for 50-byte run
Figure 2: Results for 250-byte run
</figureCaption>
<bodyText confidence="0.9999043">
We compared our results with all the official runs
submitted to TREC-8. All evaluation, for ours and
for all other official runs, are made using the officially
released evaluation program and answer patterns.
Figure 1 shows our 50-byte run together with all
TREC-8 50-byte runs. Figure 2 shows our 250-byte
run together with all TREC-8 250-byte runs. In the
2 figures, vertical bars indicate MRR scores, whereas
the dotted lines join the data points indicating the
percentage of questions with no answers found in
the top 5 returned strings. The shaded bars in the
figures indicate the MRR scores of our system.
We have also performed statistical significance
test between our runs and each of the TREC-8 offi-
cial runs. The MRR score of each question is com-
pared between two runs. The significance test results
are summarized in Table 3. At 95% confidence level,
our 50-byte run is significantly worse than 2 TREC-
8 runs, better than 13, and no different from the
rest. Our 250-byte run is significantly worse than
</bodyText>
<figure confidence="0.995451583333333">
0 .3
.2
0 .1
1 * 6 10 12 11 16 13 20
R es ults for 50-byte runs
.6
.5
cc
7,7 I
rt D.2
0 .1
nfl
</figure>
<table confidence="0.804862333333333">
What Who How When Where Others
50-byte 38.5 62.5 48.4 66.7 52.4 46.7
250-byte 58.5 68.8 58.1 72.2 66.7 73.3
</table>
<tableCaption confidence="0.972903">
Table 2: Percentage of questions answered correctly
</tableCaption>
<table confidence="0.998204666666667">
Runs worse no diff better
50-byte 2 5 13
250-byte 1 6 18
</table>
<tableCaption confidence="0.9830975">
Table 3: Significance test at 95% confidence level
between our runs versus TREC-8 official runs
</tableCaption>
<table confidence="0.999641833333333">
Features MRR
minus QT 0.203
minus NPSC 0.256
minus QNP 0.329
minus DMWM 0.294
All 0.357
</table>
<tableCaption confidence="0.9903105">
Table 4: Performance of our 50-byte run with one
less feature versus all features
</tableCaption>
<figure confidence="0.99431225">
R R 50 bytes
0.400
0.350
0.300
0.250
0.200
0.150
0.100
0.050
0.000
0 10 20 30 40 50 00 70 80 g0 100
% training data
</figure>
<figureCaption confidence="0.999942">
Figure 3: Learning curve for our 50-byte run
</figureCaption>
<bodyText confidence="0.99873594117647">
only one run, better than 18, and no different from
the rest. This indicates that our QA system has
achieved quite competitive accuracy.
It is interesting to note that our machine learn-
ing approach, which is based on a set of 4 simple
features, can already give quite competitive perfor-
mance. From Table 2, it can be seen that our system
performs quite poorly on &amp;quot;What&amp;quot; and &amp;quot;How&amp;quot; ques-
tions. For &amp;quot;How&amp;quot; questions, the reason is probably
the lack of sufficient training examples. There are
only 6 &amp;quot;How&amp;quot; questions available for training from
the TREC-8 development set, and none from the
Remedia corpus. For &amp;quot;What&amp;quot; questions, the perfor-
mance is low due to the fact that &amp;quot;What&amp;quot; questions
can ask for just about anything, and our current set
of semantic classes is not broad enough to capture
the rich variety. In our future work, we will investi-
gate improvements to overcome these deficiencies in
our current system.
We also investigated the effect of training data
size on the performance of our system, as well as the
effectiveness of the features used. Figure 3 shows the
learning curve of our 50-byte run when trained on
10%, 20%, ... , 100% of the available training data.
The learning curve is obtained by averaging over 10
random trials. It appears that the performance of
our system can be improved given a larger set of
training data, which is encouraging.
Table 4 shows the performance of our 50-byte run
if we remove one feature at a time. In all four cases,
the performance of our system dropped when us-
ing only three features, as opposed to using all four
features. This indicates that all four features con-
tributed to the performance of our system.
</bodyText>
<sectionHeader confidence="0.999547" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999977636363637">
Among the top performing systems at TREC-8 and
TREC-9 QA track (Abney et al., 2000; Clarke et al.,
2001; Cormack et al., 2000; Harabagiu et al., 2001;
Hull, 2000; Moldovan et al., 2000; Singhal et al.,
2000; Srihari and Li, 2000), most are not based on a
machine learning approach. The exceptions are the
work of (Ittycheriah et al., 2001; Prager et al., 2000;
Prager et al., 2001). In (Ittycheriah et al., 2001),
a maximum entropy approach is used to learn the
type of a question from training questions. In our
work, determining the question type is not based on
learning, so this is something we would like to in-
corporate in the future. In the work of (Prager et
al., 2000; Prager et al., 2001), logistic regression is
used to learn the weights to combine scores for fea-
tures, but their set of features is substantially differ-
ent from ours.
In QA work on reading comprehension tests,
again most are not based on a learning approach
(Hirschman et al., 1999; Charniak et al., 2000; Riloff
and Thelen, 2000). (Wang et al., 2000) attempted
a machine learning approach, but with performance
substantially lower than the other non-learning ap-
proaches.
Compared to our own previous work reported in
(Ng et al., 2000), this paper differs in the following
aspects. First, our previous work is only tested on
reading comprehension of children stories, whereas
the current paper scales up to answering questions
based on real-world newspaper documents in TREC-
8. Also, we now had to deal with question answer-
ing using a large text database, and not just an-
swering questions posed to a single short story. In
addition, the answers returned in our current work
is a 50-byte or 250-byte string, and not a sentence.
As such, the features used in this paper are cen-
tered around noun phrases, and not sentences. In
addition, there is now no restriction on the type of
questions asked. In particular, the current work can
answer quantitative &amp;quot;How&amp;quot; questions not addressed
in (Ng et al., 2000). Our current work also uses a
different (simpler) set of 4 features, and does not rely
on hand-tagged coreference information and named
entity tags unlike (Ng et al., 2000).
</bodyText>
<sectionHeader confidence="0.997273" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99866475">
In this paper, we presented a machine learning ap-
proach to question answering. The results indicate
that such an approach is a promising way to build a
state-of-the-art question answering system.
</bodyText>
<sectionHeader confidence="0.997641" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.97982668939394">
Steven Abney, Michael Collins, and Amit Singhal.
2000. Answer extraction. In Proceedings of the
6th Applied Natural Language Processing Confer-
ence and the 1st Meeting of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics (ANLP-NAACL 2000), pages
296-301, Seattle, Washington.
Daniel M. Bikel, Richard Schwartz, and Ralph
M. Weischedel. 1999. An algorithm that learns
what&apos;s in a name. Machine Learning, 34(1-3):211—
231.
Eugene Charniak, Yasemin Altun, Rodrigo de
Salvo Braz, Benjamin Garrett, Margaret Kos-
mala, Tomer Moscovich, Lixin Pang, Changhee
Pyo, Ye Sun, Wei Wy, Zhongfa Yang, Shawn
Zeller, and Lisa Zorn. 2000. Reading comprehen-
sion programs in a statistical-language-processing
class. In Proceedings of the ANLP/NAACL 2000
Workshop on Reading Comprehension Tests as
Evaluation for Computer-Based Language Under-
standing Systems, pages 1-5, Seattle, Washington.
Kenneth Church. 1988. A stochastic parts program
and noun phrase parser for unrestricted text. In
Proceedings of the Second Conference on Applied
Natural Language Processing, pages 136-143.
C.L.A. Clarke, G.V. Cormack, D.I.E. Kisman, and
T.R. Lynam. 2001. Question answering by pas-
sage selection (MultiText experiments for TREC-
9). In Proceedings of the Ninth Text REtrieval
Conference (TREC-9), Gaithersburg, Maryland.
G.V. Cormack, C.L.A. Clarke, C.R. Palmer, and
D.I.E. Kisman. 2000. Fast automatic passage
ranking (MultiText experiments for TREC-8). In
Proceedings of the Eighth Text REtrieval Con-
ference (TREC-8), pages 735-741, Gaithersburg,
Maryland.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proceed-
ings of the Thirteenth International Conference on
Machine Learning, pages 148-156.
Sanda Harabagiu, Dan Moldovan, Marius Pasca,
Rada Mihalcea, Mihai Surdeanu, Razvan
Bunescu, Roxana Girju, Vasile Rus, and Paul
Morarescu. 2001. FALCON: Boosting knowl-
edge for answer engines. In Proceedings of the
Ninth Text REtrieval Conference (TREC-9),
Gaithersburg, Maryland.
Lynette Hirschman, Marc Light, Eric Breck, and
John D. Burger. 1999. Deep Read: A reading
comprehension system. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 325-332, College Park,
Maryland.
David A. Hull. 2000. Xerox TREC-8 question an-
swering track report. In Proceedings of the Eighth
Text REtrieval Conference (TREC-8), pages 743-
752, Gaithersburg, Maryland.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu,
and Adwait Ratnaparkhi. 2001. IBM&apos;s statisti-
cal question answering system. In Proceedings of
the Ninth Text REtrieval Conference (TREC-9),
Gaithersburg, Maryland.
George A. Miller. 1990. WordNet: An online lexical
database. International Journal of Lexicography,
3(4):235-312.
Dan Moldovan, Sanda Harabagiu, Marius Pasca,
Rada Mihalcea, Richard Goodrum, Roxana Girju,
and Vasile Rus. 2000. LASSO: A tool for surfing
the answer net. In Proceedings of the Eighth Text
REtrieval Conference (TREC-8), pages 175-183,
Gaithersburg, Maryland.
Hwee Tou Ng, Leong Hwee Teo, and Jennifer Lai
Pheng Kwan. 2000. A machine learning approach
to answering questions for reading comprehen-
sion tests. In Proceedings of the 2000 Joint SIG-
DAT Conference on Empirical Methods in Natu-
ral Language Processing and Very Large Corpora.
(EMNLP/VLC-2000), pages 124-132.
John Prager, Dragomir Radev, Eric Brown, Anni
Coden, and Valerie Samn. 2000. The use of
predictive annotation for questions answering in
TREC8. In Proceedings of the Eighth Text RE-
trieval Conference (TREC-8), pages 399-409,
Gaithersburg, Maryland.
John Prager, Eric Brown, Dragomir R. Radev, and
Krzysztof Czuba. 2001. One search engine or
two for questions-answering. In Proceedings of
the Ninth Text REtrieval Conference (TREC-9),
Gaithersburg, Maryland.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Fran-
cisco, California.
Ellen Riloff and Michael Thelen. 2000. A rule-based
question answering system for reading comprehen-
sion tests. In Proceedings of the ANLP/NAACL
2000 Workshop on Reading Comprehension Tests
as Evaluation for Computer-Based Language Un-
derstanding Systems, pages 13-19, Seattle, Wash-
ington.
Amit Singhal, Steve Abney, Michiel Bacchiani,
Michael Collins, Donald Hindle, and Fernando
Pereira. 2000. AT&amp;T at TREC-8. In Proceedings
of the Eighth Text REtrieval Conference (TREC-
8), pages 317-330, Gaithersburg, Maryland.
Rohini Srihari and Wei Li. 2000. Information ex-
traction supported question answering. In Pro-
ceedings of the Eighth Text REtrieval Conference
(TREC-8), pages 185-196, Gaithersburg, Mary-
land.
Ellen M. Voorhees. 2000. The TREC-8 question an-
swering track report. In Proceedings of the Eighth
Text REtrieval Conference (TREC-8), pages 77-
82, Gaithersburg, Maryland.
Ellen M. Voorhees and Dawn M. Tice. 2000. The
TREC-8 question answering track evaluation. In
Proceedings of the Eighth Text REtrieval Con-
ference (TREC-8), pages 83-105, Gaithersburg,
Maryland.
Ellen M. Voorhees and Dawn M. Tice. 2000. Build-
ing a question answering test collection. In Pro-
ceedings of the 23rd Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval, pages 200-207, Athens,
Greece.
W. Wang, J. Auer, R. Parasuraman, I. Zubarev,
D. Brandyberry, and M. P. Harper. 2000. A ques-
tion answering system developed as a project in a
natural language processing course. In Proceed-
ings of the ANLP/NAACL 2000 Workshop on
Reading Comprehension Tests as Evaluation for
Computer-Based Language Understanding Sys-
tems, pages 28-35, Seattle, Washington.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.198025">
<title confidence="0.875620333333333">Question Answering Using a Large Text A Machine Learning Approach Hwee Tou</title>
<author confidence="0.884273">Jennifer Lai Pheng Yiyuan</author>
<affiliation confidence="0.892031">DSO National</affiliation>
<address confidence="0.655989">20 Science Park Singapore</address>
<email confidence="0.945896">Inhweetou,klaiphen,xyiyuanl@dso.org.sg</email>
<abstract confidence="0.999344571428571">In this paper, we present a machine learning approach to question answering. The task is answering factual questions, where the answers are to be found in documents in a large text database. We trained our system on 398 questions from the Remedia corpus, as well as 38 TREC-8 development questions. We then evaluated our system on 198 questions of the TREC-8 question answering task. Although our learning approach only uses 4 features, we are able to achieve quite competitive accuracy. The results indicate that such a machine learning approach is a promising way to build a state-of-the-art question answering system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>Michael Collins</author>
<author>Amit Singhal</author>
</authors>
<title>Answer extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference and the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (ANLP-NAACL</booktitle>
<pages>296--301</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="20181" citStr="Abney et al., 2000" startWordPosition="3537" endWordPosition="3540">00% of the available training data. The learning curve is obtained by averaging over 10 random trials. It appears that the performance of our system can be improved given a larger set of training data, which is encouraging. Table 4 shows the performance of our 50-byte run if we remove one feature at a time. In all four cases, the performance of our system dropped when using only three features, as opposed to using all four features. This indicates that all four features contributed to the performance of our system. 4 Related Work Among the top performing systems at TREC-8 and TREC-9 QA track (Abney et al., 2000; Clarke et al., 2001; Cormack et al., 2000; Harabagiu et al., 2001; Hull, 2000; Moldovan et al., 2000; Singhal et al., 2000; Srihari and Li, 2000), most are not based on a machine learning approach. The exceptions are the work of (Ittycheriah et al., 2001; Prager et al., 2000; Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prage</context>
</contexts>
<marker>Abney, Collins, Singhal, 2000</marker>
<rawString>Steven Abney, Michael Collins, and Amit Singhal. 2000. Answer extraction. In Proceedings of the 6th Applied Natural Language Processing Conference and the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (ANLP-NAACL 2000), pages 296-301, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An algorithm that learns what&apos;s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="4394" citStr="Bikel et al., 1999" startWordPosition="705" endWordPosition="708">form the training or test examples can be computed. Our part-of-speech tagger is a standard statistical bigram tagger based on the Hidden Markov Model (HMM) (Church, 1988). Similarly, we built a statistical HMM-based noun phrase chunker where the noun phrase boundaries are determined solely based on the part-of-speech tags assigned to the words in a sentence. We also implemented a module that assigns named entity tags. In particular, the following named entity types are recognized: human, organization, location, date, time, percent, and money. Our named entity tagger uses the HMM approach of (Bikel et al., 1999), which learns from a tagged corpus of named entities. Our part-of-speech tagger, noun phrase chunker, and named entity tagger achieve state-of-the-art accuracy. The semantic classes defined for noun phrases in our QA system are: human, organization, location, date, time, percent, money, and entity. Each of the 7 semantic classes human, organization, ... , money is a subclass of the semantic class entity, which is a catch-all semantic class for all noun phrases that are not of the other 7 defined semantic classes. Each of these semantic classes is then mapped to a WORDNET synset (Miller, 1990)</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel. 1999. An algorithm that learns what&apos;s in a name. Machine Learning, 34(1-3):211— 231.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eugene Charniak</author>
<author>Yasemin Altun</author>
<author>Rodrigo de Salvo Braz</author>
<author>Benjamin Garrett</author>
<author>Margaret Kosmala</author>
</authors>
<title>Tomer Moscovich, Lixin Pang, Changhee Pyo,</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP/NAACL</booktitle>
<pages>1--5</pages>
<location>Ye Sun, Wei Wy, Zhongfa</location>
<contexts>
<context position="21080" citStr="Charniak et al., 2000" startWordPosition="3697" endWordPosition="3700"> Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights to combine scores for features, but their set of features is substantially different from ours. In QA work on reading comprehension tests, again most are not based on a learning approach (Hirschman et al., 1999; Charniak et al., 2000; Riloff and Thelen, 2000). (Wang et al., 2000) attempted a machine learning approach, but with performance substantially lower than the other non-learning approaches. Compared to our own previous work reported in (Ng et al., 2000), this paper differs in the following aspects. First, our previous work is only tested on reading comprehension of children stories, whereas the current paper scales up to answering questions based on real-world newspaper documents in TREC8. Also, we now had to deal with question answering using a large text database, and not just answering questions posed to a singl</context>
</contexts>
<marker>Charniak, Altun, Braz, Garrett, Kosmala, 2000</marker>
<rawString>Eugene Charniak, Yasemin Altun, Rodrigo de Salvo Braz, Benjamin Garrett, Margaret Kosmala, Tomer Moscovich, Lixin Pang, Changhee Pyo, Ye Sun, Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa Zorn. 2000. Reading comprehension programs in a statistical-language-processing class. In Proceedings of the ANLP/NAACL 2000 Workshop on Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems, pages 1-5, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="3946" citStr="Church, 1988" startWordPosition="635" endWordPosition="636">ge Processing Modules Our question answering system utilizes a number of natural language processing (NLP) modules. They include sentence segmentation, tokenization, morphological analysis, part-of-speech tagging, noun phrase chunking, named entity tagging, and semantic class determination. The main goal of these modules is to identify the boundary and the semantic class of the noun phrases, so that the necessary feature values needed to form the training or test examples can be computed. Our part-of-speech tagger is a standard statistical bigram tagger based on the Hidden Markov Model (HMM) (Church, 1988). Similarly, we built a statistical HMM-based noun phrase chunker where the noun phrase boundaries are determined solely based on the part-of-speech tags assigned to the words in a sentence. We also implemented a module that assigns named entity tags. In particular, the following named entity types are recognized: human, organization, location, date, time, percent, and money. Our named entity tagger uses the HMM approach of (Bikel et al., 1999), which learns from a tagged corpus of named entities. Our part-of-speech tagger, noun phrase chunker, and named entity tagger achieve state-of-the-art </context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Kenneth Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L A Clarke</author>
<author>G V Cormack</author>
<author>D I E Kisman</author>
<author>T R Lynam</author>
</authors>
<title>Question answering by passage selection (MultiText experiments for TREC9).</title>
<date>2001</date>
<booktitle>In Proceedings of the Ninth Text REtrieval Conference (TREC-9),</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="20202" citStr="Clarke et al., 2001" startWordPosition="3541" endWordPosition="3544"> training data. The learning curve is obtained by averaging over 10 random trials. It appears that the performance of our system can be improved given a larger set of training data, which is encouraging. Table 4 shows the performance of our 50-byte run if we remove one feature at a time. In all four cases, the performance of our system dropped when using only three features, as opposed to using all four features. This indicates that all four features contributed to the performance of our system. 4 Related Work Among the top performing systems at TREC-8 and TREC-9 QA track (Abney et al., 2000; Clarke et al., 2001; Cormack et al., 2000; Harabagiu et al., 2001; Hull, 2000; Moldovan et al., 2000; Singhal et al., 2000; Srihari and Li, 2000), most are not based on a machine learning approach. The exceptions are the work of (Ittycheriah et al., 2001; Prager et al., 2000; Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logi</context>
</contexts>
<marker>Clarke, Cormack, Kisman, Lynam, 2001</marker>
<rawString>C.L.A. Clarke, G.V. Cormack, D.I.E. Kisman, and T.R. Lynam. 2001. Question answering by passage selection (MultiText experiments for TREC9). In Proceedings of the Ninth Text REtrieval Conference (TREC-9), Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G V Cormack</author>
<author>C L A Clarke</author>
<author>C R Palmer</author>
<author>D I E Kisman</author>
</authors>
<title>Fast automatic passage ranking (MultiText experiments for TREC-8).</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8),</booktitle>
<pages>735--741</pages>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="20224" citStr="Cormack et al., 2000" startWordPosition="3545" endWordPosition="3548">earning curve is obtained by averaging over 10 random trials. It appears that the performance of our system can be improved given a larger set of training data, which is encouraging. Table 4 shows the performance of our 50-byte run if we remove one feature at a time. In all four cases, the performance of our system dropped when using only three features, as opposed to using all four features. This indicates that all four features contributed to the performance of our system. 4 Related Work Among the top performing systems at TREC-8 and TREC-9 QA track (Abney et al., 2000; Clarke et al., 2001; Cormack et al., 2000; Harabagiu et al., 2001; Hull, 2000; Moldovan et al., 2000; Singhal et al., 2000; Srihari and Li, 2000), most are not based on a machine learning approach. The exceptions are the work of (Ittycheriah et al., 2001; Prager et al., 2000; Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is use</context>
</contexts>
<marker>Cormack, Clarke, Palmer, Kisman, 2000</marker>
<rawString>G.V. Cormack, C.L.A. Clarke, C.R. Palmer, and D.I.E. Kisman. 2000. Fast automatic passage ranking (MultiText experiments for TREC-8). In Proceedings of the Eighth Text REtrieval Conference (TREC-8), pages 735-741, Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Experiments with a new boosting algorithm.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth International Conference on Machine Learning,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="9524" citStr="Freund and Schapire, 1996" startWordPosition="1634" endWordPosition="1637">rch space 1-The list of sentences considered in this search space will be explained in the following subsections on training and testing. is defined as all the sentences in d, where d is the document containing 5. Negative training examples are generated by randomly selecting one noun phrase (other than n) each from sentences 5, 5_1 and 5+1, where s_i is the sentence immediately preceding s, and s+i is the sentence immediately following s. A classifier is then built based on the feature vectors generated from the training questions and answers. The learning algorithm used is C5 with boosting (Freund and Schapire, 1996). C5 is a more recent version of C4.5 (Quinlan, 1993). We use the default values for all C5 learning parameters. 2.4 Testing Before identifying all possible noun phrases for testing, the search space of all documents in a large text database will have to be efficiently narrowed down first. 2.4.1 Passage retrieval The retrieval system we use is based on that described in (Singhal et al., 2000). For a given question, query terms (i.e., words in the question) are first extracted by removing stopwords and punctuation symbols. A term weight is then assigned to each query term t, using the idf facto</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1996. Experiments with a new boosting algorithm. In Proceedings of the Thirteenth International Conference on Machine Learning, pages 148-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Dan Moldovan</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Mihai Surdeanu</author>
<author>Razvan Bunescu</author>
<author>Roxana Girju</author>
<author>Vasile Rus</author>
<author>Paul Morarescu</author>
</authors>
<title>FALCON: Boosting knowledge for answer engines.</title>
<date>2001</date>
<booktitle>In Proceedings of the Ninth Text REtrieval Conference (TREC-9),</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="20248" citStr="Harabagiu et al., 2001" startWordPosition="3549" endWordPosition="3552">ned by averaging over 10 random trials. It appears that the performance of our system can be improved given a larger set of training data, which is encouraging. Table 4 shows the performance of our 50-byte run if we remove one feature at a time. In all four cases, the performance of our system dropped when using only three features, as opposed to using all four features. This indicates that all four features contributed to the performance of our system. 4 Related Work Among the top performing systems at TREC-8 and TREC-9 QA track (Abney et al., 2000; Clarke et al., 2001; Cormack et al., 2000; Harabagiu et al., 2001; Hull, 2000; Moldovan et al., 2000; Singhal et al., 2000; Srihari and Li, 2000), most are not based on a machine learning approach. The exceptions are the work of (Ittycheriah et al., 2001; Prager et al., 2000; Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights t</context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, Mihalcea, Surdeanu, Bunescu, Girju, Rus, Morarescu, 2001</marker>
<rawString>Sanda Harabagiu, Dan Moldovan, Marius Pasca, Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu, Roxana Girju, Vasile Rus, and Paul Morarescu. 2001. FALCON: Boosting knowledge for answer engines. In Proceedings of the Ninth Text REtrieval Conference (TREC-9), Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Marc Light</author>
<author>Eric Breck</author>
<author>John D Burger</author>
</authors>
<title>Deep Read: A reading comprehension system.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>325--332</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="2656" citStr="Hirschman et al., 1999" startWordPosition="426" endWordPosition="429">k started in TREC-8 in 1999 (Voorhees, 2000; Voorhees and Tice, 2000, * Hwee Tou Ng is also affiliated with Department of Computer Science, School of Computing, National University of Singapore, http://www.comp.nus .edu.sg/nght TREC). In the question answering track, for each factual question, the task is to extract the top five 50-byte or 250-byte answers to the question from a large text database consisting of hundreds of thousands of documents (gigabytes of text). Another strand of work that deals with question answering surfaces in the context of reading comprehension. The group at MITRE (Hirschman et al., 1999) initiated work on reading comprehension of children stories. In particular, the task involves choosing a sentence from a story that best answers a question posed to the story. For this task, there is no need to deal with retrieval from a large text database, since a question is directed at a particular story. In this paper, we address the task of answering factual questions, where the answers are to be found in documents in a large text database. We adopt a machine learning approach to question answering. In particular, answer candidates are classified and ranked by a classifier trained on a </context>
<context position="13694" citStr="Hirschman et al., 1999" startWordPosition="2370" endWordPosition="2373">TREC disk 4 and 5 minus the Congressional Record documents. In this development set, we have questions containing &amp;quot;what&amp;quot;, &amp;quot;who&amp;quot;, &amp;quot;how&amp;quot;, &amp;quot;when&amp;quot;, &amp;quot;where&amp;quot;, and a few others that do not have any of these words. Answers to these questions are also provided by NIST together with the document id of the documents where the answers can be found. Answer phrases were then manual tagged in the specified documents and training examples were generated according to the scheme described in Section 2. The second part of the training data is based on the questions and stories published by Remedia Publications (Hirschman et al., 1999). The document collection for this set consists of stories from grade 2 to 5 with a total of 115 stories. Each of the stories comes with five questions. However, not all the questions can be used for training, since some of the questions cannot be answered by any noun phrase in the associated story. We also ignore the &amp;quot;Why&amp;quot; questions, since their answers are typically not noun phrases. After removing the &amp;quot;Why&amp;quot; questions and questions without a noun-phrase answer, we have 398 questions left for training. The collection of stories we used is the copy created by the MITRE group with each story ma</context>
<context position="21057" citStr="Hirschman et al., 1999" startWordPosition="3693" endWordPosition="3696">01; Prager et al., 2000; Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights to combine scores for features, but their set of features is substantially different from ours. In QA work on reading comprehension tests, again most are not based on a learning approach (Hirschman et al., 1999; Charniak et al., 2000; Riloff and Thelen, 2000). (Wang et al., 2000) attempted a machine learning approach, but with performance substantially lower than the other non-learning approaches. Compared to our own previous work reported in (Ng et al., 2000), this paper differs in the following aspects. First, our previous work is only tested on reading comprehension of children stories, whereas the current paper scales up to answering questions based on real-world newspaper documents in TREC8. Also, we now had to deal with question answering using a large text database, and not just answering que</context>
</contexts>
<marker>Hirschman, Light, Breck, Burger, 1999</marker>
<rawString>Lynette Hirschman, Marc Light, Eric Breck, and John D. Burger. 1999. Deep Read: A reading comprehension system. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 325-332, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Hull</author>
</authors>
<title>Xerox TREC-8 question answering track report.</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8),</booktitle>
<pages>743--752</pages>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="20260" citStr="Hull, 2000" startWordPosition="3553" endWordPosition="3554"> random trials. It appears that the performance of our system can be improved given a larger set of training data, which is encouraging. Table 4 shows the performance of our 50-byte run if we remove one feature at a time. In all four cases, the performance of our system dropped when using only three features, as opposed to using all four features. This indicates that all four features contributed to the performance of our system. 4 Related Work Among the top performing systems at TREC-8 and TREC-9 QA track (Abney et al., 2000; Clarke et al., 2001; Cormack et al., 2000; Harabagiu et al., 2001; Hull, 2000; Moldovan et al., 2000; Singhal et al., 2000; Srihari and Li, 2000), most are not based on a machine learning approach. The exceptions are the work of (Ittycheriah et al., 2001; Prager et al., 2000; Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights to combine sc</context>
</contexts>
<marker>Hull, 2000</marker>
<rawString>David A. Hull. 2000. Xerox TREC-8 question answering track report. In Proceedings of the Eighth Text REtrieval Conference (TREC-8), pages 743-752, Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Martin Franz</author>
<author>Wei-Jing Zhu</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>IBM&apos;s statistical question answering system.</title>
<date>2001</date>
<booktitle>In Proceedings of the Ninth Text REtrieval Conference (TREC-9),</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="20437" citStr="Ittycheriah et al., 2001" startWordPosition="3582" endWordPosition="3585">nce of our 50-byte run if we remove one feature at a time. In all four cases, the performance of our system dropped when using only three features, as opposed to using all four features. This indicates that all four features contributed to the performance of our system. 4 Related Work Among the top performing systems at TREC-8 and TREC-9 QA track (Abney et al., 2000; Clarke et al., 2001; Cormack et al., 2000; Harabagiu et al., 2001; Hull, 2000; Moldovan et al., 2000; Singhal et al., 2000; Srihari and Li, 2000), most are not based on a machine learning approach. The exceptions are the work of (Ittycheriah et al., 2001; Prager et al., 2000; Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights to combine scores for features, but their set of features is substantially different from ours. In QA work on reading comprehension tests, again most are not based on a learning approach (Hi</context>
</contexts>
<marker>Ittycheriah, Franz, Zhu, Ratnaparkhi, 2001</marker>
<rawString>Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and Adwait Ratnaparkhi. 2001. IBM&apos;s statistical question answering system. In Proceedings of the Ninth Text REtrieval Conference (TREC-9), Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: An online lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="4994" citStr="Miller, 1990" startWordPosition="805" endWordPosition="806">et al., 1999), which learns from a tagged corpus of named entities. Our part-of-speech tagger, noun phrase chunker, and named entity tagger achieve state-of-the-art accuracy. The semantic classes defined for noun phrases in our QA system are: human, organization, location, date, time, percent, money, and entity. Each of the 7 semantic classes human, organization, ... , money is a subclass of the semantic class entity, which is a catch-all semantic class for all noun phrases that are not of the other 7 defined semantic classes. Each of these semantic classes is then mapped to a WORDNET synset (Miller, 1990). Our semantic class determination module assumes that the semantic class for every noun phrase extracted follows the first sense of the head noun of the noun phrase. Since WORDNET orders the senses of a noun by their frequency, this is equivalent to choosing the most frequent sense, which then determines the semantic class of the noun phrase through upward traversal of the ISA hierarchy Of WORDNET. 2.2 Features The learning task is defined as identifying the best noun phrase that is most likely to answer a given question. Our feature vector consists of 4 features. Each feature vector is deriv</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George A. Miller. 1990. WordNet: An online lexical database. International Journal of Lexicography, 3(4):235-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Sanda Harabagiu</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Richard Goodrum</author>
<author>Roxana Girju</author>
<author>Vasile Rus</author>
</authors>
<title>LASSO: A tool for surfing the answer net.</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8),</booktitle>
<pages>175--183</pages>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="20283" citStr="Moldovan et al., 2000" startWordPosition="3555" endWordPosition="3558">ls. It appears that the performance of our system can be improved given a larger set of training data, which is encouraging. Table 4 shows the performance of our 50-byte run if we remove one feature at a time. In all four cases, the performance of our system dropped when using only three features, as opposed to using all four features. This indicates that all four features contributed to the performance of our system. 4 Related Work Among the top performing systems at TREC-8 and TREC-9 QA track (Abney et al., 2000; Clarke et al., 2001; Cormack et al., 2000; Harabagiu et al., 2001; Hull, 2000; Moldovan et al., 2000; Singhal et al., 2000; Srihari and Li, 2000), most are not based on a machine learning approach. The exceptions are the work of (Ittycheriah et al., 2001; Prager et al., 2000; Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights to combine scores for features, but </context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Goodrum, Girju, Rus, 2000</marker>
<rawString>Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Richard Goodrum, Roxana Girju, and Vasile Rus. 2000. LASSO: A tool for surfing the answer net. In Proceedings of the Eighth Text REtrieval Conference (TREC-8), pages 175-183, Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Leong Hwee Teo</author>
<author>Jennifer Lai Pheng Kwan</author>
</authors>
<title>A machine learning approach to answering questions for reading comprehension tests.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora. (EMNLP/VLC-2000),</booktitle>
<pages>124--132</pages>
<contexts>
<context position="7963" citStr="Ng et al., 2000" startWordPosition="1341" endWordPosition="1344"> Otherwise, NPSC is assigned the value of the semantic class of n. • Quantitative noun phrase (QNP) The possible values are true and false. If the noun phrase n contains a number and its NPSC is not date or time, then this value is true. Otherwise, this value is false. This feature is indicative of answers to &amp;quot;how many&amp;quot;, &amp;quot;how far&amp;quot;, &amp;quot;how long&amp;quot;, etc type of &amp;quot;how&amp;quot; questions. • Diff-from-Max-Word-Match between s and q (DMWM) The possible values are 0, 1, 2, 3, .... This feature attempts to capture the word overlap between a question and a sentence. It is similar to that used in our previous work (Ng et al., 2000). To compute the value of this feature, all words in the question q and the sentence s (which contains the noun phrase n) are reduced to their morphological roots. Let m be the number of morphological root words in q which can be found in s. Stop words are excluded in the count. Then for all the possible sentences si in the defined search space&apos;, let mi be the number of morphological root words in q which can be found in si. Define M = maxi{mi}. The value of this feature is then computed as M—m. That is, for a sentence s that has the maximum number of words that overlap with q (among all sente</context>
<context position="21311" citStr="Ng et al., 2000" startWordPosition="3733" endWordPosition="3736">we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights to combine scores for features, but their set of features is substantially different from ours. In QA work on reading comprehension tests, again most are not based on a learning approach (Hirschman et al., 1999; Charniak et al., 2000; Riloff and Thelen, 2000). (Wang et al., 2000) attempted a machine learning approach, but with performance substantially lower than the other non-learning approaches. Compared to our own previous work reported in (Ng et al., 2000), this paper differs in the following aspects. First, our previous work is only tested on reading comprehension of children stories, whereas the current paper scales up to answering questions based on real-world newspaper documents in TREC8. Also, we now had to deal with question answering using a large text database, and not just answering questions posed to a single short story. In addition, the answers returned in our current work is a 50-byte or 250-byte string, and not a sentence. As such, the features used in this paper are centered around noun phrases, and not sentences. In addition, th</context>
</contexts>
<marker>Ng, Teo, Kwan, 2000</marker>
<rawString>Hwee Tou Ng, Leong Hwee Teo, and Jennifer Lai Pheng Kwan. 2000. A machine learning approach to answering questions for reading comprehension tests. In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora. (EMNLP/VLC-2000), pages 124-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Dragomir Radev</author>
<author>Eric Brown</author>
<author>Anni Coden</author>
<author>Valerie Samn</author>
</authors>
<title>The use of predictive annotation for questions answering in TREC8.</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8),</booktitle>
<pages>399--409</pages>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="20458" citStr="Prager et al., 2000" startWordPosition="3586" endWordPosition="3589">we remove one feature at a time. In all four cases, the performance of our system dropped when using only three features, as opposed to using all four features. This indicates that all four features contributed to the performance of our system. 4 Related Work Among the top performing systems at TREC-8 and TREC-9 QA track (Abney et al., 2000; Clarke et al., 2001; Cormack et al., 2000; Harabagiu et al., 2001; Hull, 2000; Moldovan et al., 2000; Singhal et al., 2000; Srihari and Li, 2000), most are not based on a machine learning approach. The exceptions are the work of (Ittycheriah et al., 2001; Prager et al., 2000; Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights to combine scores for features, but their set of features is substantially different from ours. In QA work on reading comprehension tests, again most are not based on a learning approach (Hirschman et al., 1999;</context>
</contexts>
<marker>Prager, Radev, Brown, Coden, Samn, 2000</marker>
<rawString>John Prager, Dragomir Radev, Eric Brown, Anni Coden, and Valerie Samn. 2000. The use of predictive annotation for questions answering in TREC8. In Proceedings of the Eighth Text REtrieval Conference (TREC-8), pages 399-409, Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Eric Brown</author>
<author>Dragomir R Radev</author>
<author>Krzysztof Czuba</author>
</authors>
<title>One search engine or two for questions-answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the Ninth Text REtrieval Conference (TREC-9),</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="20480" citStr="Prager et al., 2001" startWordPosition="3590" endWordPosition="3593"> at a time. In all four cases, the performance of our system dropped when using only three features, as opposed to using all four features. This indicates that all four features contributed to the performance of our system. 4 Related Work Among the top performing systems at TREC-8 and TREC-9 QA track (Abney et al., 2000; Clarke et al., 2001; Cormack et al., 2000; Harabagiu et al., 2001; Hull, 2000; Moldovan et al., 2000; Singhal et al., 2000; Srihari and Li, 2000), most are not based on a machine learning approach. The exceptions are the work of (Ittycheriah et al., 2001; Prager et al., 2000; Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights to combine scores for features, but their set of features is substantially different from ours. In QA work on reading comprehension tests, again most are not based on a learning approach (Hirschman et al., 1999; Charniak et al., 2000</context>
</contexts>
<marker>Prager, Brown, Radev, Czuba, 2001</marker>
<rawString>John Prager, Eric Brown, Dragomir R. Radev, and Krzysztof Czuba. 2001. One search engine or two for questions-answering. In Proceedings of the Ninth Text REtrieval Conference (TREC-9), Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, California.</location>
<contexts>
<context position="9577" citStr="Quinlan, 1993" startWordPosition="1646" endWordPosition="1647"> will be explained in the following subsections on training and testing. is defined as all the sentences in d, where d is the document containing 5. Negative training examples are generated by randomly selecting one noun phrase (other than n) each from sentences 5, 5_1 and 5+1, where s_i is the sentence immediately preceding s, and s+i is the sentence immediately following s. A classifier is then built based on the feature vectors generated from the training questions and answers. The learning algorithm used is C5 with boosting (Freund and Schapire, 1996). C5 is a more recent version of C4.5 (Quinlan, 1993). We use the default values for all C5 learning parameters. 2.4 Testing Before identifying all possible noun phrases for testing, the search space of all documents in a large text database will have to be efficiently narrowed down first. 2.4.1 Passage retrieval The retrieval system we use is based on that described in (Singhal et al., 2000). For a given question, query terms (i.e., words in the question) are first extracted by removing stopwords and punctuation symbols. A term weight is then assigned to each query term t, using the idf factor: log(1+) where N is the total number of documents i</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>John Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Francisco, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Michael Thelen</author>
</authors>
<title>A rule-based question answering system for reading comprehension tests.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP/NAACL</booktitle>
<pages>13--19</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="21106" citStr="Riloff and Thelen, 2000" startWordPosition="3701" endWordPosition="3704">In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights to combine scores for features, but their set of features is substantially different from ours. In QA work on reading comprehension tests, again most are not based on a learning approach (Hirschman et al., 1999; Charniak et al., 2000; Riloff and Thelen, 2000). (Wang et al., 2000) attempted a machine learning approach, but with performance substantially lower than the other non-learning approaches. Compared to our own previous work reported in (Ng et al., 2000), this paper differs in the following aspects. First, our previous work is only tested on reading comprehension of children stories, whereas the current paper scales up to answering questions based on real-world newspaper documents in TREC8. Also, we now had to deal with question answering using a large text database, and not just answering questions posed to a single short story. In addition</context>
</contexts>
<marker>Riloff, Thelen, 2000</marker>
<rawString>Ellen Riloff and Michael Thelen. 2000. A rule-based question answering system for reading comprehension tests. In Proceedings of the ANLP/NAACL 2000 Workshop on Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems, pages 13-19, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Singhal</author>
<author>Steve Abney</author>
<author>Michiel Bacchiani</author>
<author>Michael Collins</author>
<author>Donald Hindle</author>
<author>Fernando Pereira</author>
</authors>
<title>AT&amp;T at TREC-8.</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC8),</booktitle>
<pages>317--330</pages>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="9919" citStr="Singhal et al., 2000" startWordPosition="1703" endWordPosition="1706">+i is the sentence immediately following s. A classifier is then built based on the feature vectors generated from the training questions and answers. The learning algorithm used is C5 with boosting (Freund and Schapire, 1996). C5 is a more recent version of C4.5 (Quinlan, 1993). We use the default values for all C5 learning parameters. 2.4 Testing Before identifying all possible noun phrases for testing, the search space of all documents in a large text database will have to be efficiently narrowed down first. 2.4.1 Passage retrieval The retrieval system we use is based on that described in (Singhal et al., 2000). For a given question, query terms (i.e., words in the question) are first extracted by removing stopwords and punctuation symbols. A term weight is then assigned to each query term t, using the idf factor: log(1+) where N is the total number of documents in the text database, and dt is the number of documents in the database containing term t. Within a document, a passage is any contiguous text string which contains up to a maximum of 5 consecutive sentences, or at most 500 bytes. Each candidate passage in a document is assigned a score, based on the sum of the weights of all query terms con</context>
<context position="20305" citStr="Singhal et al., 2000" startWordPosition="3559" endWordPosition="3562"> performance of our system can be improved given a larger set of training data, which is encouraging. Table 4 shows the performance of our 50-byte run if we remove one feature at a time. In all four cases, the performance of our system dropped when using only three features, as opposed to using all four features. This indicates that all four features contributed to the performance of our system. 4 Related Work Among the top performing systems at TREC-8 and TREC-9 QA track (Abney et al., 2000; Clarke et al., 2001; Cormack et al., 2000; Harabagiu et al., 2001; Hull, 2000; Moldovan et al., 2000; Singhal et al., 2000; Srihari and Li, 2000), most are not based on a machine learning approach. The exceptions are the work of (Ittycheriah et al., 2001; Prager et al., 2000; Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights to combine scores for features, but their set of features </context>
</contexts>
<marker>Singhal, Abney, Bacchiani, Collins, Hindle, Pereira, 2000</marker>
<rawString>Amit Singhal, Steve Abney, Michiel Bacchiani, Michael Collins, Donald Hindle, and Fernando Pereira. 2000. AT&amp;T at TREC-8. In Proceedings of the Eighth Text REtrieval Conference (TREC8), pages 317-330, Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohini Srihari</author>
<author>Wei Li</author>
</authors>
<title>Information extraction supported question answering.</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8),</booktitle>
<pages>185--196</pages>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="20328" citStr="Srihari and Li, 2000" startWordPosition="3563" endWordPosition="3566">stem can be improved given a larger set of training data, which is encouraging. Table 4 shows the performance of our 50-byte run if we remove one feature at a time. In all four cases, the performance of our system dropped when using only three features, as opposed to using all four features. This indicates that all four features contributed to the performance of our system. 4 Related Work Among the top performing systems at TREC-8 and TREC-9 QA track (Abney et al., 2000; Clarke et al., 2001; Cormack et al., 2000; Harabagiu et al., 2001; Hull, 2000; Moldovan et al., 2000; Singhal et al., 2000; Srihari and Li, 2000), most are not based on a machine learning approach. The exceptions are the work of (Ittycheriah et al., 2001; Prager et al., 2000; Prager et al., 2001). In (Ittycheriah et al., 2001), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights to combine scores for features, but their set of features is substantially differ</context>
</contexts>
<marker>Srihari, Li, 2000</marker>
<rawString>Rohini Srihari and Wei Li. 2000. Information extraction supported question answering. In Proceedings of the Eighth Text REtrieval Conference (TREC-8), pages 185-196, Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>The TREC-8 question answering track report.</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8),</booktitle>
<pages>77--82</pages>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="2076" citStr="Voorhees, 2000" startWordPosition="336" endWordPosition="337">2000&amp;quot;, rather than to read through lots of web pages that contain the words &amp;quot;US&amp;quot;, &amp;quot;presidential&amp;quot;, &amp;quot;election&amp;quot;, etc to find the date of election. That is, what a user needs is information retrieval, rather than the current document retrieval. Question answering (QA) has recently attracted a lot of research activities. This is in part fueled by the question answering track of TREC. TREC is an annual exercise to evaluate the performance of text retrieval systems on common, large real-world text collections, using a uniform scoring procedure. The question answering track started in TREC-8 in 1999 (Voorhees, 2000; Voorhees and Tice, 2000, * Hwee Tou Ng is also affiliated with Department of Computer Science, School of Computing, National University of Singapore, http://www.comp.nus .edu.sg/nght TREC). In the question answering track, for each factual question, the task is to extract the top five 50-byte or 250-byte answers to the question from a large text database consisting of hundreds of thousands of documents (gigabytes of text). Another strand of work that deals with question answering surfaces in the context of reading comprehension. The group at MITRE (Hirschman et al., 1999) initiated work on r</context>
<context position="12954" citStr="Voorhees, 2000" startWordPosition="2245" endWordPosition="2246">n the passage to make up a 50-byte (or 250-byte) answer string. The 2nd ranked noun phrase is then checked to ensure that it does not occur in the earlier chosen 50-byte (or 250-byte) answer strings. If it does, then the next ranked noun phrase is considered. If it does not, then the noun phrase is expanded to 50 bytes (or 250 bytes). This process of selecting and expanding noun phrases continues until finally 5 answer strings are chosen. 3 Evaluation The training data we used to build our classifier consists of two parts. The first part is the set of 38 TREC-8 QA track development questions (Voorhees, 2000). Document collection for this set of questions is the same as in TREC-8 ad hoc task, namely the set of documents on TREC disk 4 and 5 minus the Congressional Record documents. In this development set, we have questions containing &amp;quot;what&amp;quot;, &amp;quot;who&amp;quot;, &amp;quot;how&amp;quot;, &amp;quot;when&amp;quot;, &amp;quot;where&amp;quot;, and a few others that do not have any of these words. Answers to these questions are also provided by NIST together with the document id of the documents where the answers can be found. Answer phrases were then manual tagged in the specified documents and training examples were generated according to the scheme described in Sect</context>
<context position="14978" citStr="Voorhees, 2000" startWordPosition="2606" endWordPosition="2607">18 65 Who 98 7 48 How 0 6 31 When 95 3 18 Where 104 2 21 Others 0 2 15 Total 398 38 198 Table 1: Questions in the training and test set ally annotated to indicate which sentence answers to each of the questions associated with the story. However, since in our training, we require the answer noun phrase instead of answer sentence, each story was then further hand-tagged to indicate the answer noun phrase. After this stage, the same scheme described in Section 2 was employed to generate training examples. The test data we used is the set of questions in the official test set of TREC-8 QA Track (Voorhees, 2000). This set consists of 198 questions which has no overlap with the 38 development questions. Document collection is the same as TREC-8 ad hoc task. Table 1 shows the number of questions of each type in the training and test set. Our evaluation is based on the official evaluation program and answer patterns released by the TREC-8 QA track organizer (Voorhees and Tice, 2000, SIGIR). In TREC-8, the official evaluation is done by human assessors. However, since we did not participate in the TREC-8 QA track, our runs have not been evaluated by the human assessors who judged the other TREC-8 officia</context>
</contexts>
<marker>Voorhees, 2000</marker>
<rawString>Ellen M. Voorhees. 2000. The TREC-8 question answering track report. In Proceedings of the Eighth Text REtrieval Conference (TREC-8), pages 77-82, Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Dawn M Tice</author>
</authors>
<title>The TREC-8 question answering track evaluation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8),</booktitle>
<pages>83--105</pages>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="2101" citStr="Voorhees and Tice, 2000" startWordPosition="338" endWordPosition="341">an to read through lots of web pages that contain the words &amp;quot;US&amp;quot;, &amp;quot;presidential&amp;quot;, &amp;quot;election&amp;quot;, etc to find the date of election. That is, what a user needs is information retrieval, rather than the current document retrieval. Question answering (QA) has recently attracted a lot of research activities. This is in part fueled by the question answering track of TREC. TREC is an annual exercise to evaluate the performance of text retrieval systems on common, large real-world text collections, using a uniform scoring procedure. The question answering track started in TREC-8 in 1999 (Voorhees, 2000; Voorhees and Tice, 2000, * Hwee Tou Ng is also affiliated with Department of Computer Science, School of Computing, National University of Singapore, http://www.comp.nus .edu.sg/nght TREC). In the question answering track, for each factual question, the task is to extract the top five 50-byte or 250-byte answers to the question from a large text database consisting of hundreds of thousands of documents (gigabytes of text). Another strand of work that deals with question answering surfaces in the context of reading comprehension. The group at MITRE (Hirschman et al., 1999) initiated work on reading comprehension of c</context>
<context position="15352" citStr="Voorhees and Tice, 2000" startWordPosition="2670" endWordPosition="2673">agged to indicate the answer noun phrase. After this stage, the same scheme described in Section 2 was employed to generate training examples. The test data we used is the set of questions in the official test set of TREC-8 QA Track (Voorhees, 2000). This set consists of 198 questions which has no overlap with the 38 development questions. Document collection is the same as TREC-8 ad hoc task. Table 1 shows the number of questions of each type in the training and test set. Our evaluation is based on the official evaluation program and answer patterns released by the TREC-8 QA track organizer (Voorhees and Tice, 2000, SIGIR). In TREC-8, the official evaluation is done by human assessors. However, since we did not participate in the TREC-8 QA track, our runs have not been evaluated by the human assessors who judged the other TREC-8 official runs. Fortunately, as demonstrated in (Voorhees and Tice, 2000, SIGIR), evaluation based on the evaluation program and answer patterns gives comparable outcome as the evaluation done by human judges. The evaluation metric we used is mean reciprocal rank (MRR), the same as that used in TREC-8. For each question, if the rank at which the first correct answer appears is k,</context>
</contexts>
<marker>Voorhees, Tice, 2000</marker>
<rawString>Ellen M. Voorhees and Dawn M. Tice. 2000. The TREC-8 question answering track evaluation. In Proceedings of the Eighth Text REtrieval Conference (TREC-8), pages 83-105, Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Dawn M Tice</author>
</authors>
<title>Building a question answering test collection.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>200--207</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="2101" citStr="Voorhees and Tice, 2000" startWordPosition="338" endWordPosition="341">an to read through lots of web pages that contain the words &amp;quot;US&amp;quot;, &amp;quot;presidential&amp;quot;, &amp;quot;election&amp;quot;, etc to find the date of election. That is, what a user needs is information retrieval, rather than the current document retrieval. Question answering (QA) has recently attracted a lot of research activities. This is in part fueled by the question answering track of TREC. TREC is an annual exercise to evaluate the performance of text retrieval systems on common, large real-world text collections, using a uniform scoring procedure. The question answering track started in TREC-8 in 1999 (Voorhees, 2000; Voorhees and Tice, 2000, * Hwee Tou Ng is also affiliated with Department of Computer Science, School of Computing, National University of Singapore, http://www.comp.nus .edu.sg/nght TREC). In the question answering track, for each factual question, the task is to extract the top five 50-byte or 250-byte answers to the question from a large text database consisting of hundreds of thousands of documents (gigabytes of text). Another strand of work that deals with question answering surfaces in the context of reading comprehension. The group at MITRE (Hirschman et al., 1999) initiated work on reading comprehension of c</context>
<context position="15352" citStr="Voorhees and Tice, 2000" startWordPosition="2670" endWordPosition="2673">agged to indicate the answer noun phrase. After this stage, the same scheme described in Section 2 was employed to generate training examples. The test data we used is the set of questions in the official test set of TREC-8 QA Track (Voorhees, 2000). This set consists of 198 questions which has no overlap with the 38 development questions. Document collection is the same as TREC-8 ad hoc task. Table 1 shows the number of questions of each type in the training and test set. Our evaluation is based on the official evaluation program and answer patterns released by the TREC-8 QA track organizer (Voorhees and Tice, 2000, SIGIR). In TREC-8, the official evaluation is done by human assessors. However, since we did not participate in the TREC-8 QA track, our runs have not been evaluated by the human assessors who judged the other TREC-8 official runs. Fortunately, as demonstrated in (Voorhees and Tice, 2000, SIGIR), evaluation based on the evaluation program and answer patterns gives comparable outcome as the evaluation done by human judges. The evaluation metric we used is mean reciprocal rank (MRR), the same as that used in TREC-8. For each question, if the rank at which the first correct answer appears is k,</context>
</contexts>
<marker>Voorhees, Tice, 2000</marker>
<rawString>Ellen M. Voorhees and Dawn M. Tice. 2000. Building a question answering test collection. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 200-207, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>J Auer</author>
<author>R Parasuraman</author>
<author>I Zubarev</author>
<author>D Brandyberry</author>
<author>M P Harper</author>
</authors>
<title>A question answering system developed as a project in a natural language processing course.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP/NAACL</booktitle>
<pages>28--35</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="21127" citStr="Wang et al., 2000" startWordPosition="3705" endWordPosition="3708">1), a maximum entropy approach is used to learn the type of a question from training questions. In our work, determining the question type is not based on learning, so this is something we would like to incorporate in the future. In the work of (Prager et al., 2000; Prager et al., 2001), logistic regression is used to learn the weights to combine scores for features, but their set of features is substantially different from ours. In QA work on reading comprehension tests, again most are not based on a learning approach (Hirschman et al., 1999; Charniak et al., 2000; Riloff and Thelen, 2000). (Wang et al., 2000) attempted a machine learning approach, but with performance substantially lower than the other non-learning approaches. Compared to our own previous work reported in (Ng et al., 2000), this paper differs in the following aspects. First, our previous work is only tested on reading comprehension of children stories, whereas the current paper scales up to answering questions based on real-world newspaper documents in TREC8. Also, we now had to deal with question answering using a large text database, and not just answering questions posed to a single short story. In addition, the answers returne</context>
</contexts>
<marker>Wang, Auer, Parasuraman, Zubarev, Brandyberry, Harper, 2000</marker>
<rawString>W. Wang, J. Auer, R. Parasuraman, I. Zubarev, D. Brandyberry, and M. P. Harper. 2000. A question answering system developed as a project in a natural language processing course. In Proceedings of the ANLP/NAACL 2000 Workshop on Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems, pages 28-35, Seattle, Washington.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>