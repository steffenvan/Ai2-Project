<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.966947">
The First International Chinese Word Segmentation Bakeoff
</title>
<author confidence="0.914405">
Richard Sproat
</author>
<affiliation confidence="0.708011">
AT&amp;T Labs – Research
</affiliation>
<address confidence="0.762134">
180 Park Avenue, Florham Park, NJ, 07932, USA
</address>
<email confidence="0.996323">
rws@research.att.com
</email>
<author confidence="0.828901">
Thomas Emerson
</author>
<affiliation confidence="0.763464">
Basis Technology
</affiliation>
<address confidence="0.921923">
150 CambridgePark Drive
Cambridge, MA 02140, USA
</address>
<email confidence="0.998933">
tree@basistech.com
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967615384616">
This paper presents the results from the
ACL-SIGHAN-sponsored First Interna-
tional Chinese Word Segmentation Bake-
off held in 2003 and reported in con-
junction with the Second SIGHAN Work-
shop on Chinese Language Processing,
Sapporo, Japan. We give the motivation
for having an international segmentation
contest (given that there have been two
within-China contests to date) and we re-
port on the results of this first international
contest, analyze these results, and make
some recommendations for the future.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965822916667">
Chinese word segmentation is a difficult problem
that has received a lot of attention in the literature;
reviews of some of the various approaches can be
found in (Wang et al., 1990; Wu and Tseng, 1993;
Sproat and Shih, 2001). The problem with this liter-
ature has always been that it is very hard to compare
systems, due to the lack of any common standard test
set. Thus, an approach that seems very promising
based on its published report is nonetheless hard to
compare fairly with other systems, since the systems
are often tested on their own selected test corpora.
Part of the problem is also that there is no single
accepted segmentation standard: There are several,
including the four standards used in this evaluation.
A number of segmentation contests have been
held in recent years within Mainland China, in the
context of more general evaluations for Chinese-
English machine translation. See (Yao, 2001; Yao,
2002) for the first and second of these; the third eval-
uation will be held in August 2003. The test cor-
pora were segmented according to the Chinese na-
tional standard GB 13715 (GB/T 13715–92, 1993),
though some lenience was granted in the case of
plausible alternative segmentations (Yao, 2001); so
while GB 13715 specifies the segmentation /
for Mao Zedong, was also allowed. Accura-
cies in the mid 80’s to mid 90’s were reported for the
four systems that participated in the first evaluation,
with higher scores (many in the high nineties) being
reported for the second evaluation.
The motivations for holding the current contest
are twofold. First of all, by making the contest in-
ternational, we are encouraging participation from
people and institutions who work on Chinese word
segmentation anywhere in the world. The final set of
participants in the bakeoff include two from Main-
land China, three from Hong Kong, one from Japan,
one from Singapore, one from Taiwan and four from
the United States.
Secondly, as we have already noted, there are at
least four distinct standards in active use in the sense
that large corpora are being developed according to
those standards; see Section 2.1. It has also been
observed that different segmentation standards are
appropriate for different purposes; that the segmen-
tation standard that one might prefer for information
retrieval applications is likely to be different from
the one that one would prefer for text-to-speech syn-
thesis; see (Wu, 2003) for useful discussion. Thus,
while we do not subscribe to the view that any of
the extant standards are, in fact, appropriate for any
particular application, nevertheless, it seems desir-
able to have a contest where people are tested against
more than one standard.
A third point is that we decided early on that we
would not be lenient in our scoring, so that alter-
native segmentations as in the case of Mao
Zedong, cited above, would not be allowed. While
it would be fairly straightforward (in many cases)
to automatically score both alternatives, we felt we
could provide a more objective measure if we went
strictly by the particular segmentation standard be-
ing tested on, and simply did not get into the busi-
ness of deciding upon allowable alternatives.
Comparing segmenters is difficult. This is not
only because of differences in segmentation stan-
dards but also due to differences in the design of
systems: Systems based exclusively (or even pri-
marily) on lexical and grammatical analysis will of-
ten be at a disadvantage during the comparison com-
pared to systems trained exclusively on the training
data. Competitions also may fail to predict the per-
formance of the segmenter on new texts outside the
training and testing sets. The handling of out-of-
vocabulary words becomes a much larger issue in
these situations than is accounted for within the test
environment: A system that performs admirably in
the competition may perform poorly on texts from
different registers.
Another issue that is not accounted for in the
current collection of evaluations is the handling of
short strings with minimal context, such as queries
submitted to a search engine. This has been stud-
ied indirectly through the cross-language informa-
tion retrieval work performed for the TREC 5 and
TREC 6 competitions (Smeaton and Wilkinson,
1997; Wilkinson, 1998).
This report summarizes the results of this First
International Chinese Word Segmentation Bakeoff,
provides some analysis of the results, and makes
specific recommendations for future bakeoffs. One
thing we do not do here is get into the details of spe-
cific systems; each of the participants was required
to provide a four page description of their system
along with detailed discussion of their results, and
these papers are published in this volume.
</bodyText>
<sectionHeader confidence="0.494233" genericHeader="method">
2 Details of the contest
</sectionHeader>
<subsectionHeader confidence="0.958188">
2.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999873689655172">
The corpora are detailed in Table 1. Links
to descriptions of the corpora can be found at
http://www.sighan.org/bakeoff2003/
bakeoff_instr.html; publications on spe-
cific corpora are (Huang et al., 1997) (Academia
Sinica), (Xia, 1999) (Chinese Treebank); the
Beijing University standard is very similar to that
outlined in (GB/T 13715–92, 1993). Table 1 lists
the abbreviations for the four corpora that will be
used throughout this paper. The suffixes “o” and
“c” will be used to denote open and closed tracks,
respectively: Thus “ASo,c” denotes the Academia
Sinica corpus, both open and closed tracks; and
“PKc” denotes the Beijing University corpus, closed
track.
During the course of this bakeoff, a number of
inconsistencies in segmentation were noted in the
CTB corpus by one of the participants. This was
done early enough so that it was possible for the
CTB developers to correct some of the more com-
mon cases, both in the training and the test data.
The revised training data was posted for participants,
and the revised test data was used during the testing
phase.
Inconsistencies were also noted by another par-
ticipant for the AS corpus. Unfortunately this came
too late in the process to correct the data. However,
some informal tests on the revised testing data indi-
cated that the differences were minor.
</bodyText>
<subsectionHeader confidence="0.999581">
2.2 Rules and Procedures
</subsectionHeader>
<bodyText confidence="0.999562428571429">
The contest followed a strict set of guidelines and
a rigid timetable. The detailed instructions for the
bakeoff can be found at http://www.sighan.
org/bakeoff2003/bakeoff_instr.html
(with simplified and traditional Chinese versions
also available). Training material was available
starting March 15, testing material was available
April 22, and the results had to be returned to the
SIGHAN ftp site by April 25 no later than 17:00
EDT.
Upon initial registration sites were required to de-
clare which corpora they would be training and test-
ing on, and whether they would be participating in
the open or closed tracks (or both) on each corpus,
</bodyText>
<table confidence="0.9846314">
Corpus Abbrev. Encoding # Train. Words # Test. Words
Academia Sinica AS Big Five (MS Codepage 950) 5.8M 12K
U. Penn Chinese Treebank CTB EUC-CN (GB 2312-80) 250K 40K
Hong Kong CityU HK Big Five (HKSCS) 240K 35K
Beijing University PK GBK (MS Codepage 936) 1.1M 17K
</table>
<tableCaption confidence="0.99995">
Table 1: Corpora used.
</tableCaption>
<bodyText confidence="0.998723514285714">
where these were defined as follows:
For the open test sites were allowed to train
on the training set for a particular corpus, and
in addition they could use any other mate-
rial including material from other training cor-
pora, proprietary dictionaries, material from
the WWW and so forth. However, if a site
selected the open track the site was required
to explain what percentage of the results came
from which sources. For example, if the sys-
tem did particularly well on out-of-vocabulary
words then the participants were required to ex-
plain if, for example, those results could mostly
be attributed to having a good dictionary.
In the closed test, participants could only use
training material from the training data for the
particular corpus being testing on. No other
material was allowed.
Other obvious restrictions applied: Participants
were prohibited from testing on corpora from their
own sites, and by signing up for a particular track,
participants were declaring implicitly that they had
not previously seen the test corpus for that track.
Scoring was completely automatic. Note that the
scoring software does not correct for cases where a
participant converted from one coding scheme into
another, and any such cases were counted as er-
rors. Results were returned to participants within
a couple of days of submission of the segmented
test data. The script used for scoring can be
downloaded from http://www.sighan.org/
bakeoff2003/score; it is a simple Perl script
that depends upon a version of diff (e.g. GNU diffu-
tils 2.7.2), that supports the -y flag for side-by-side
output format.
</bodyText>
<subsectionHeader confidence="0.99969">
2.3 Participating sites
</subsectionHeader>
<bodyText confidence="0.999931">
Participating sites are shown in Table 2. These are a
subset of the sites who had registered for the bake-
off, as some sites withdrew due to technical difficul-
ties.
</bodyText>
<sectionHeader confidence="0.980981" genericHeader="method">
3 Further details of the corpora
</sectionHeader>
<bodyText confidence="0.99949">
An unfortunate, and sometimes unforseen, complex-
ity in dealing with Chinese text on the computer is
the plethora of character sets and character encod-
ings used throughout Greater China. This is demon-
strated in the Encoding column of Table 1:
</bodyText>
<listItem confidence="0.7439818">
1. Both AS and HK utilize complex-form (or “tra-
ditional”) characters, using variants of the Big
Five character set. The Academia Sinica cor-
pus is composed almost entirely of characters
in pure Big Five (four characters, 0xFB5B,
0xFA76, 0xFB7A, and 0xFAAF are outside
the encoding range of Big Five), while the
City University corpus utilizes 38 (34 unique)
characters from the Hong Kong Supplementary
Character Set (HKSCS) extension to Big Five.
2. The CTB and PK corpora each use simple-form
(or “simplified”) characters, using the EUC-
CN encoding of the GB 2312-80 character set.
However, The PKU corpus includes characters
that are not part of GB 2312-80, but are en-
coded in GBK. GBK is an extension of GB
2312-80 that incorporates some 18,000 hanzi
found in Unicode 2.1 within the GB-2312 code
space. Only Microsoft’s CP936 implements
GBK.
</listItem>
<bodyText confidence="0.9993272">
This variation of encoding is exacerbated by the
usual lack of specific declaration in the files. Gener-
ally a file is said to be “Big Five” or “GB”, when in
actuality the file is encoded in a variation of these.
This is problematic in systems that utilize Unicode
</bodyText>
<note confidence="0.999352307692308">
Site ID Site Name Domain Contact Tracks
S01 Inst. of Comp. Tech.,CAS CN Huaping ZHANG ASo CTBo,c HKc PKo,c
S02 ICL, Beijing U CN Baobao CHANG CTBo,c
S03 HK Polytechnic University HK Qin LU ASo CTBo HKo PKo
S04 U of Hong Kong HK Guohong FU PKo,c
S05 HK CityU HK Chunyu KIT ASc CTBc PKc
S06 Nara IST JP Chooi Ling GOH ASc CTBc HKc PKc
S07 Inst. for Infocomm Research SG Guodong ZHOU PKc
S08 CKIP Ac. Sinica Taiwan TW Wei Yun MA HKo,c PKo,c
S09 UC Berkeley US Aitao CHEN ASc PKc
S10 Microsoft Research US Andi WU CTBo,c PKo,c
S11 SYSTRAN Software, Inc. US Jin YANG ASo CTBo HKo PKo
S12 U Penn US Nianwen XUE ASc HKc
</note>
<tableCaption confidence="0.998505">
Table 2: Participating sites and associated tracks.
</tableCaption>
<bodyText confidence="0.918907">
internally, since transcoding back to the original en-
coding may lose information.
</bodyText>
<sectionHeader confidence="0.999976" genericHeader="method">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.996848">
4.1 Baseline and topline experiments
</subsectionHeader>
<bodyText confidence="0.999977444444444">
We computed a baseline for each of the corpora by
compiling a dictionary of all and only the words in
the training portion of the corpus. We then used this
dictionary with a simple maximum matching algo-
rithm to segment the test corpus. The results of this
experiment are presented in Table 3. In this and sub-
sequent tables, we list the word count for the test
corpus, test recall (R), test precision (P), F score1,
the out-of-vocabulary (OOV) rate for the test corpus,
the recall on OOV words (R ), and the recall on
in-vocabulary (R ) words. Per normal usage, OOV
is defined as the set of words in the test corpus not
occurring in the training corpus.2 We expect sys-
tems to do at least as well as this baseline.
As a nominal topline we ran the same maximum
matching experiments, but this time populating the
dictionary only with words from the test corpus; this
is of course a “cheating” experiment since one could
</bodyText>
<footnote confidence="0.881173555555556">
1We use a balanced F score, so that .
2Note that the OOV recall in Table 3 should in theory be
0.0, but is not always zero because the maximum matching al-
gorithm might get lucky. In particular, if the dictionary con-
tains no word starting with some character , then the maximum
matching algorithm with move on to the next character, leaving
segmented as a word on its own. If it happens that is in fact a
single-character word, then the algorithm will have fortuitously
done the right thing.
</footnote>
<bodyText confidence="0.9995732">
not reasonably know exactly the set of words that
occur in the test corpus. Since this is better than one
could hope for in practice, we would expect systems
to generally underperform this topline. The results
of this “cheating” experiment are given in Table 4.3
</bodyText>
<subsectionHeader confidence="0.9588375">
4.2 Raw scores
4.2.1 Closed Tests
</subsectionHeader>
<bodyText confidence="0.998451">
Results for the closed tests are presented in Ta-
bles 5–8. Column headings are as above, except for
“c ”, and “c ” for which see Section 4.3.
</bodyText>
<subsubsectionHeader confidence="0.816766">
4.2.2 Open Tests
</subsubsectionHeader>
<bodyText confidence="0.896913666666667">
Results for the open tests are presented in Ta-
bles 9–12; again, see Section 4.3 for the explanation
of “c ”, and “c ”.
</bodyText>
<subsectionHeader confidence="0.998892">
4.3 Statistical significance of the results
</subsectionHeader>
<bodyText confidence="0.999941285714286">
Let us assume that the recall rates for the various
system represent the probability that a word will
be successfully identified, and let us further assume
that a binomial distribution is appropriate for this
experiment. Given the Central Limit Theorem for
Bernouilli trials — e.g. (Grinstead and Snell, 1997,
page 330), then the 95% confidence interval is given
</bodyText>
<footnote confidence="0.813658428571428">
3If one did have the exact list of words occurring in the test
corpus, one could still do better than the maximum matching
algorithm, since the maximum matching algorithm cannot in
general correctly resolve cases where more than one segmen-
tation is possible given the dictionary. However as we can see
from the scores in Table 4, such cases constitute at most about
1.5%.
</footnote>
<table confidence="0.9963364">
Corpus word count R P F OOV R R
AS 11,985 0.917 0.912 0.915 0.022 0.000 0.938
CTB 39,922 0.800 0.663 0.725 0.181 0.062 0.962
HK 34,955 0.908 0.830 0.867 0.071 0.037 0.974
PK 17,194 0.909 0.829 0.867 0.069 0.050 0.972
</table>
<tableCaption confidence="0.989232">
Table 3: Baseline scores: Results for maximum matching using only words from training data
</tableCaption>
<table confidence="0.999189">
Corpus word count R P F OOV R R
AS 11,985 0.990 0.993 0.992 0.022 0.988 0.990
CTB 39,922 0.982 0.988 0.985 0.181 0.990 0.980
HK 34,955 0.986 0.991 0.989 0.071 0.996 0.985
PK 17,194 0.995 0.996 0.995 0.069 1.000 0.994
</table>
<tableCaption confidence="0.91253">
Table 4: Topline (“cheating”) scores: Results for maximum matching using only words from testing data
data
</tableCaption>
<bodyText confidence="0.9999292">
as , where is the number of trials
(words). The values for are given
in Tables 5–12, under the heading “c ”. They can be
interpreted as follows: To decide whether two sites
are significantly different (at the 95% confidence
level) in their performance on a particular task, one
just has to compute whether their confidence inter-
vals overlap. Similarly one can treat the precision
rates as the probability that a character string that
has been identified as a word is really a word; these
precision-based confidences are given as “c ” in the
tables.
It seems reasonable to treat two systems as sig-
nificantly different (at the 95% confidence level), if
at least one of their recall-based or precision-based
confidences are different. Using this criterion all
systems are significantly different from each other
except that on PK closed S10 is not significantly dif-
ferent from S09, and S07 is not significantly differ-
ent from S04.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
5 Discussion
</sectionHeader>
<subsectionHeader confidence="0.648731">
5.1 Differences between “open” and “closed”
performance
</subsectionHeader>
<bodyText confidence="0.999807151515151">
In Figure 1 we plot the F scores for all systems, all
tracks. We include as “BASE”, and “TOP” the base-
line and topline scores discussed previously. In most
cases people performed above the baseline, though
well below the ideal topline; note though that the
two participants in the Academia Sinica open track
underperformed the baseline.
Performance on the Penn Chinese Treebank
(CTB) corpus was generally lower than all the other
corpora; omitting S02, which only ran on CTBo,c
the scores for the other systems were uniformly
higher on other corpora than they were on CTB,
the single exception being S11 which did better on
CTBo than on HKo. The baseline for CTB is also
much lower than the baseline for other corpora, so
one might be inclined to ascribe the generally lower
performance to the smaller training data for this cor-
pus. Also, the OOV rate for this corpus is much
higher than all of the other corpora, and since er-
ror rates are generally higher on OOV, this is surely
a contributing factor. However, this would only ex-
plain why CTB showed lower performance on the
closed test; on the open test, one might expect the
size of the training data to matter less, but there were
still large differences between several systems’ per-
formance on CTB and their performance on other
corpora. Note also that the topline for CTB is also
lower than for the other corpora. What all of this
suggests is that the CTB may simply be less con-
sistent than the other corpora in its segmentation;
indeed one of the participants (Andi Wu) noted a
number of inconsistencies in both the training and
the test data (though inconsistencies were also noted
</bodyText>
<table confidence="0.974769166666667">
Site word count R c P c F OOV R R
S09 11,985 0.966 0.0033 0.956 0.0037 0.961 0.022 0.364 0.980
S12 11,985 0.961 0.0035 0.958 0.0037 0.959 0.022 0.729 0.966
S06 11,985 0.944 0.0042 0.945 0.0042 0.945 0.022 0.574 0.952
S05 11,985 0.952 0.0039 0.931 0.0046 0.942 0.022 0.043 0.972
S01 11,985 0.953 0.0039 0.924 0.0048 0.938 0.022 0.178 0.970
</table>
<tableCaption confidence="0.667529">
Table 5: Scores for AS closed, sorted by F.
</tableCaption>
<table confidence="0.998039666666667">
Site word count R c P c F OOV R R
S01 39,922 0.886 0.0032 0.875 0.0033 0.881 0.181 0.705 0.927
S02 39,922 0.892 0.0031 0.856 0.0035 0.874 0.181 0.644 0.947
S10 39,922 0.867 0.0034 0.797 0.0040 0.831 0.181 0.431 0.963
S06 39,922 0.852 0.0036 0.807 0.0040 0.829 0.181 0.412 0.949
S05 39,922 0.800 0.0040 0.674 0.0047 0.732 0.181 0.076 0.959
</table>
<tableCaption confidence="0.983395">
Table 6: Scores for CTB closed, sorted by F.
</tableCaption>
<bodyText confidence="0.998682142857143">
for the AS corpus).4
Systems that ran on both closed and open tracks
for the same corpus generally did better on the open
track, indicating (not surprisingly) that using ad-
ditional data can help. However, the lower-than-
baseline performance of S03 and S11 on ASo may
reflect issues with tuning of these additional re-
sources to the particular standard in question.
Finally note that the top performance of any sys-
tem on any track was S09 on ASc (F=0.961). Since
performances close to our ideal topline have occa-
sionally been reported in the literature it is worth
bearing the results of this bakeoff in mind when
reading such reports.
</bodyText>
<subsectionHeader confidence="0.997548">
5.2 Differences on OOV
</subsectionHeader>
<bodyText confidence="0.9828815">
Figure 2 plots the recall on out-of-vocabulary words
(R ) for all systems and all tracks. For this mea-
</bodyText>
<footnote confidence="0.768302">
4For example, Wu notes that (20th Century) is
consistently segmented as two words in the training data, but
</footnote>
<bodyText confidence="0.99813590625">
as one word in the test data. Similarly ((corporate)
vice president) is segmented as one word in training data but as
two words ( / ) in the testing data. As a final example,
superlatives such as (best) should be segmented as a single
word if the adjective is monosyllabic, and it is not being used
predicatively; however this principle is not consistently applied.
Wu also notes that the test data is different from the training
data in several respects. Most of the training data comprise texts
about Mainland China, whereas most of the testing data is about
Taiwan. The test data contains classes of items, such as URL’s
and English page designations (“p. 64”), that never appeared in
the test data.
sure, the performance of the baseline is only above
0.0 fortuitously, as we noted in Section 4.1. Simi-
larly the topline performance is only less than 1.0 in
cases where there are two or more possible decom-
positions of a string, and where the option with the
longest prefix is not the correct one.
It is with OOV recall that we see the widest varia-
tion among systems, which in turn is consistent with
the observation that dealing with unknown words
is the major outstanding problem of Chinese word
segmentation. While some systems performed lit-
tle better than the baseline, others had a very re-
spectable 0.80 recall on OOV. Again, there was
clearly a benefit for many systems in using addi-
tional resources than what is in the training data: A
number of systems that were run on both closed and
open tracks showed significant improvements in the
open track. For the closed-track entries that did well
on OOV, one must conclude that they have effective
unknown-word detection methods.
</bodyText>
<sectionHeader confidence="0.957366" genericHeader="conclusions">
6 Summary and recommendations
</sectionHeader>
<bodyText confidence="0.999565">
We feel that this First International Chinese Word
Segmentation Bakeoff has been useful in that it has
provided us with a good sense of the range of per-
formance of various systems, both from academic
and industrial institutions. There is clearly no single
best system, insofar as there is no system that con-
</bodyText>
<table confidence="0.9415042">
Site word count R c P c F OOV R R
S08 34,955 0.947 0.0024 0.934 0.0027 0.940 0.071 0.625 0.972
S06 34,955 0.940 0.0025 0.908 0.0031 0.924 0.071 0.415 0.980
S12 34955 0.917 0.0030 0.915 0.0030 0.916 0.071 0.670 0.936
S01 34,955 0.931 0.0027 0.873 0.0036 0.901 0.071 0.243 0.984
Table 7: Scores for HK closed, sorted by F.
Site word count R c P c F OOV R R
S01 17,194 0.962 0.0029 0.940 0.0036 0.951 0.069 0.724 0.979
S10 17,194 0.955 0.0032 0.938 0.0037 0.947 0.069 0.680 0.976
S09 17,194 0.955 0.0032 0.938 0.0037 0.946 0.069 0.647 0.977
S07 17,194 0.936 0.0037 0.945 0.0035 0.940 0.069 0.763 0.949
S04 17,194 0.936 0.0037 0.942 0.0036 0.939 0.069 0.675 0.955
S08 17,194 0.939 0.0037 0.934 0.0038 0.936 0.069 0.642 0.961
S06 17,194 0.933 0.0038 0.916 0.0042 0.924 0.069 0.357 0.975
S05 17,194 0.923 0.0041 0.867 0.0052 0.894 0.069 0.159 0.980
</table>
<tableCaption confidence="0.993168">
Table 8: Scores for PK closed, sorted by F.
</tableCaption>
<bodyText confidence="0.9998885">
sistently outperformed all the others on all tracks.
Even if there were, the most one could say is that for
the four different segmentation standards and asso-
ciated corpora, this particular system outperformed
the others: But there could be no implication that
said system would be the most appropriate for all
applications.
One thing that we have not explicitly discussed in
this paper is which type of approach shows the most
promise, given the different submissions. While we
are familiar with the approaches taken in several of
the tested systems, we leave it up to the individual
participants to describe their approaches and hope-
fully elucidate which aspects of their approaches are
most responsible for their successes and failures; the
participants’ papers all appear in this volume. We
leave it up to the research community as a whole to
decide whether one approach or another shows most
promise.
We believe that there should be future competi-
tions of this kind, possibly not every year, but cer-
tainly every couple of years and we have some spe-
cific recommendations on how things might be im-
proved in such future competitions:
</bodyText>
<listItem confidence="0.725661">
1. It may be a good idea to insist that all partici-
pants participate in all tracks, subject of course
</listItem>
<bodyText confidence="0.998725653846154">
to the restriction that participants may not be
evaluated on data from their own institution.
The decision this time to let people pick and
choose was motivated in part by the concern
that if we insisted that people participate in all
tracks, some participants might be less inclined
to participate. It was also motivated in part
by the different Chinese coding schemes used
by the various corpora, and the possibility that
someone’s system might work on one coding
scheme, but not the other.
However with sufficient planning, perhaps giv-
ing people a longer period of time for train-
ing their systems than was possible with this
contest, it should be possible to impose this re-
striction without scaring away potential partic-
ipants.
2. We would like to see more testing data devel-
oped for the next bakeoff. While the test sets
turned out to be large enough to measure sig-
nificant differences between systems in most
cases, a larger test set would allow even bet-
ter statistics. In some cases, more training data
will also be needed.
Given the problems noted by some of the par-
ticipants with some of the data, we would also
</bodyText>
<table confidence="0.818001">
Site word count R c P c F OOV R R
S11 11,985 0.915 0.0051 0.894 0.0056 0.904 0.022 0.426 0.926
S03 11,985 0.892 0.0057 0.853 0.0065 0.872 0.022 0.236 0.906
Table 9: Scores for AS open, sorted by F.
Site word count R c P c F OOV R R
S02 39,922 0.916 0.0028 0.907 0.0029 0.912 0.181 0.766 0.949
S10 39,922 0.911 0.0029 0.891 0.0031 0.901 0.181 0.738 0.949
S11 39,922 0.891 0.0031 0.877 0.0033 0.884 0.181 0.733 0.925
S01 39,922 0.887 0.0032 0.876 0.0033 0.881 0.181 0.707 0.927
S03 39,922 0.853 0.0035 0.806 0.0040 0.829 0.181 0.578 0.914
</table>
<tableCaption confidence="0.985847">
Table 10: Scores for CTB open, sorted by F.
</tableCaption>
<bodyText confidence="0.998998347826087">
like to see more consistently annotated training
and test data, and test data that is more repre-
sentative of what was seen in the training data.
3. We would like to expand the testing data to in-
clude texts of various lengths, particularly short
strings, in order to emulate query strings seen
in commercial search engines.
4. Finally, one question that we did not ask that
should have been asked was whether the tested
system is used as part of a commercial product
or not. It is often believed of natural language
and speech applications that deployed commer-
cial systems are about a generation behind the
systems being developed in research laborato-
ries. It would be interesting to know if this is
true in the domain of Chinese word segmenta-
tion, which should be possible to find out if we
get a good balance of both.
For the present, we will make the training and test
data for the bakeoff available via http://www.
sighan.org/bakeoff2003 (subject to the re-
strictions of the content providers), so that others can
better study the results of this contest.
</bodyText>
<sectionHeader confidence="0.996516" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.941096740740741">
First and foremost we wish to thank the follow-
ing institutions for providing the training and testing
data for this bakeoff:
Institute of Linguistics, Academia Sinica.
Institute of Computational Linguistics, Beijing
University.
Language Information Sciences Research Cen-
tre, City University of Hong Kong.
The Chinese Treebank Project, University of
Pennsylvania, and the Linguistic Data Consor-
tium.
Without the generous contribution of these re-
sources, this competition would not have been pos-
sible.
We would also like to thank Martha Palmer for
making funds available to pay for translations of the
detailed bakeoff instructions, and to Fu-Dong Chiou,
Susan Converse and Nianwen Xue for their work on
the translations. Andi Wu and Aitao Chen provided
useful feedback on errors in some of the corpora.
The first author wishes to thank Bill DuMouchel of
AT&amp;T Labs for advice on the statistics. We also
wish to thank Professor Tianshun Yao of Northeast
(Dongbei) University for sending us the reports of
the Chinese national competitions. Finally we thank
Fei Xia and Qing Ma for their work on the Second
meeting of SIGHAN of which this bakeoff is a part.
</bodyText>
<sectionHeader confidence="0.989407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.356052333333333">
GB/T 13715–92. 1993. Contemporary Chinese language
word-segmentation specification for information pro-
cessing. Technical report, , Beijing.
</reference>
<table confidence="0.932096583333333">
Site word count R c P c F OOV R R
S08 34,955 0.958 0.0021 0.954 0.0022 0.956 0.071 0.788 0.971
S03 34,955 0.909 0.0031 0.863 0.0037 0.886 0.071 0.579 0.935
S11 34,955 0.898 0.0032 0.860 0.0037 0.879 0.071 0.616 0.920
Table 11: Scores for HK open, sorted by F.
Site word count R c P c F OOV R R
S10 17,194 0.963 0.0029 0.956 0.0031 0.959 0.069 0.799 0.975
S01 17,194 0.963 0.0029 0.943 0.0035 0.953 0.069 0.743 0.980
S08 17,194 0.939 0.0037 0.938 0.0037 0.938 0.069 0.675 0.959
S04 17,194 0.933 0.0038 0.942 0.0036 0.937 0.069 0.712 0.949
S03 17,194 0.940 0.0036 0.911 0.0043 0.925 0.069 0.647 0.962
S11 17,194 0.905 0.0045 0.869 0.0051 0.886 0.069 0.503 0.934
</table>
<tableCaption confidence="0.955066">
Table 12: Scores for PK open, sorted by F.
</tableCaption>
<reference confidence="0.999113028571429">
Charles Grinstead and J. Laurie Snell. 1997. Introduc-
tion to Probability. American Mathematical Society,
Providence, RI, 2nd edition.
Chu-Ren Huang, Keh-Jiann Chen, Chang Lili, and Feng-
yi Chen. 1997. Segmentation standard for Chinese
natural language processing. International Journal
of Computational Linguistics and Chinese Language
Processing, 2(2):47–62.
Alan Smeaton and Ross Wilkinson. 1997. Spanish
and chinese document retrieval at TREC-5. In E.M.
Vorhees and D.K. Harman, editors, Proceedings of the
Fifth Text REtrieval Conference.
Richard Sproat and Chilin Shih. 2001. Corpus-based
methods in Chinese morphology and phonol-
ogy. Technical report, Linguistic Society of
America Summer Institute, Santa Barbara, CA.
http://www.research.att.com/˜rws/
newindex/notes.pdf.
Yongheng Wang, Haiju Su, and Yan Mo. 1990. Auto-
matic processing of Chinese words. Journal of Chi-
nese Information Processing, 4(4):1–11.
Ross Wilkinson. 1998. Chinese document retrieval at
TREC-6. In E.M. Vorhees and D.K. Harman, editors,
Proceedings of the Sixth Text REtrieval Conference.
Zimin Wu and Gwyneth Tseng. 1993. Chinese text seg-
mentation for text retrieval: Achievements and prob-
lems. Journal of the American Societyfor Information
Science, 44(9):532–542.
Andi Wu. 2003. Customizable segmentation of mor-
phologically derived words in Chinese. International
Journal of Computational Linguistics and Chinese
Language Processing, 8(1): forthcoming.
Fei Xia. 1999. Segmentation guideline, Chinese Tree-
bank Project. Technical report, University of Pennsyl-
vania. http://morph.ldc.upenn.edu/ctb/.
</reference>
<figure confidence="0.963521515151515">
Tianshun Yao ( ). 2001.
( ). Technical report, Northeast
University ( ), China, January.
Tianshun Yao ( ). 2002.
( ). Technical report, Northeast Uni-
versity ( ), China, August.
TOP TOP
TOP TOP
TOP TOP
TOP TOP
S09 S10
S12
S08
S01 S01
S06 S08 09S10 8S04
S05 S06 4 S03
S01 S07
S08
S06
BASE BASE S12
S11 S02 S01 S05
S10
S11 S03 S11
S01 S01 S11
S03 S02
BASE BASE BASE BASE
S10 S03
S06
S05
BASE BASE
ASc ASo CTBc CTBo HKc HKo PKc PKo
F 0.75 0.80 0.85 0.90 0.95 1.00
Track
</figure>
<figureCaption confidence="0.999149">
Figure 1: F scores for all systems, all tracks.
</figureCaption>
<figure confidence="0.257086153846154">
TOP TOP TOP TOP TOP TOP TOP TOP
S12 S11 S01 S02 S12 S08 S07 S10
S06 S03 S02 0 S08 S11 S01 S01
S09 BASE S10 S11 S06 S03 10 S04
S01 S06 S01 S01 BASE S04 S08
S05 S05 S03 BASE 9 S03
BASE BASE BASE S08 S11
S06 BASE
S05
BASE
ASc ASo CTBc CTBo HKc HKo PKc PKo
R(OOV) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Track
</figure>
<figureCaption confidence="0.983331">
Figure 2: R scores for all systems, all tracks.
</figureCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.594248">
<title confidence="0.999553">The First International Chinese Word Segmentation Bakeoff</title>
<author confidence="0.998137">Richard</author>
<affiliation confidence="0.991414">AT&amp;T Labs –</affiliation>
<address confidence="0.999162">180 Park Avenue, Florham Park, NJ, 07932,</address>
<email confidence="0.999767">rws@research.att.com</email>
<author confidence="0.720249">Thomas</author>
<affiliation confidence="0.662179">Basis</affiliation>
<address confidence="0.9838405">150 CambridgePark Cambridge, MA 02140,</address>
<email confidence="0.999881">tree@basistech.com</email>
<abstract confidence="0.997810142857143">This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan. We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>GBT</author>
</authors>
<title>Contemporary Chinese language word-segmentation specification for information processing.</title>
<date>1993</date>
<tech>Technical report, ,</tech>
<location>Beijing.</location>
<marker>GBT, 1993</marker>
<rawString>GB/T 13715–92. 1993. Contemporary Chinese language word-segmentation specification for information processing. Technical report, , Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Grinstead</author>
<author>J Laurie Snell</author>
</authors>
<title>Introduction to Probability.</title>
<date>1997</date>
<publisher>American Mathematical Society,</publisher>
<location>Providence, RI,</location>
<note>2nd edition.</note>
<contexts>
<context position="14107" citStr="Grinstead and Snell, 1997" startWordPosition="2356" endWordPosition="2359">lts for the closed tests are presented in Tables 5–8. Column headings are as above, except for “c ”, and “c ” for which see Section 4.3. 4.2.2 Open Tests Results for the open tests are presented in Tables 9–12; again, see Section 4.3 for the explanation of “c ”, and “c ”. 4.3 Statistical significance of the results Let us assume that the recall rates for the various system represent the probability that a word will be successfully identified, and let us further assume that a binomial distribution is appropriate for this experiment. Given the Central Limit Theorem for Bernouilli trials — e.g. (Grinstead and Snell, 1997, page 330), then the 95% confidence interval is given 3If one did have the exact list of words occurring in the test corpus, one could still do better than the maximum matching algorithm, since the maximum matching algorithm cannot in general correctly resolve cases where more than one segmentation is possible given the dictionary. However as we can see from the scores in Table 4, such cases constitute at most about 1.5%. Corpus word count R P F OOV R R AS 11,985 0.917 0.912 0.915 0.022 0.000 0.938 CTB 39,922 0.800 0.663 0.725 0.181 0.062 0.962 HK 34,955 0.908 0.830 0.867 0.071 0.037 0.974 PK</context>
</contexts>
<marker>Grinstead, Snell, 1997</marker>
<rawString>Charles Grinstead and J. Laurie Snell. 1997. Introduction to Probability. American Mathematical Society, Providence, RI, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-Ren Huang</author>
<author>Keh-Jiann Chen</author>
<author>Chang Lili</author>
<author>Fengyi Chen</author>
</authors>
<title>Segmentation standard for Chinese natural language processing.</title>
<date>1997</date>
<journal>International Journal of Computational Linguistics and Chinese Language Processing,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="5741" citStr="Huang et al., 1997" startWordPosition="924" endWordPosition="927">Word Segmentation Bakeoff, provides some analysis of the results, and makes specific recommendations for future bakeoffs. One thing we do not do here is get into the details of specific systems; each of the participants was required to provide a four page description of their system along with detailed discussion of their results, and these papers are published in this volume. 2 Details of the contest 2.1 Corpora The corpora are detailed in Table 1. Links to descriptions of the corpora can be found at http://www.sighan.org/bakeoff2003/ bakeoff_instr.html; publications on specific corpora are (Huang et al., 1997) (Academia Sinica), (Xia, 1999) (Chinese Treebank); the Beijing University standard is very similar to that outlined in (GB/T 13715–92, 1993). Table 1 lists the abbreviations for the four corpora that will be used throughout this paper. The suffixes “o” and “c” will be used to denote open and closed tracks, respectively: Thus “ASo,c” denotes the Academia Sinica corpus, both open and closed tracks; and “PKc” denotes the Beijing University corpus, closed track. During the course of this bakeoff, a number of inconsistencies in segmentation were noted in the CTB corpus by one of the participants. </context>
</contexts>
<marker>Huang, Chen, Lili, Chen, 1997</marker>
<rawString>Chu-Ren Huang, Keh-Jiann Chen, Chang Lili, and Fengyi Chen. 1997. Segmentation standard for Chinese natural language processing. International Journal of Computational Linguistics and Chinese Language Processing, 2(2):47–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Smeaton</author>
<author>Ross Wilkinson</author>
</authors>
<title>Spanish and chinese document retrieval at TREC-5.</title>
<date>1997</date>
<booktitle>Proceedings of the Fifth Text REtrieval Conference.</booktitle>
<editor>In E.M. Vorhees and D.K. Harman, editors,</editor>
<contexts>
<context position="5031" citStr="Smeaton and Wilkinson, 1997" startWordPosition="814" endWordPosition="817">tside the training and testing sets. The handling of out-ofvocabulary words becomes a much larger issue in these situations than is accounted for within the test environment: A system that performs admirably in the competition may perform poorly on texts from different registers. Another issue that is not accounted for in the current collection of evaluations is the handling of short strings with minimal context, such as queries submitted to a search engine. This has been studied indirectly through the cross-language information retrieval work performed for the TREC 5 and TREC 6 competitions (Smeaton and Wilkinson, 1997; Wilkinson, 1998). This report summarizes the results of this First International Chinese Word Segmentation Bakeoff, provides some analysis of the results, and makes specific recommendations for future bakeoffs. One thing we do not do here is get into the details of specific systems; each of the participants was required to provide a four page description of their system along with detailed discussion of their results, and these papers are published in this volume. 2 Details of the contest 2.1 Corpora The corpora are detailed in Table 1. Links to descriptions of the corpora can be found at ht</context>
</contexts>
<marker>Smeaton, Wilkinson, 1997</marker>
<rawString>Alan Smeaton and Ross Wilkinson. 1997. Spanish and chinese document retrieval at TREC-5. In E.M. Vorhees and D.K. Harman, editors, Proceedings of the Fifth Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
</authors>
<title>Corpus-based methods in Chinese morphology and phonology.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>Linguistic Society of America Summer Institute,</institution>
<location>Santa Barbara, CA.</location>
<note>http://www.research.att.com/˜rws/ newindex/notes.pdf.</note>
<contexts>
<context position="1017" citStr="Sproat and Shih, 2001" startWordPosition="151" endWordPosition="154">003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan. We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future. 1 Introduction Chinese word segmentation is a difficult problem that has received a lot of attention in the literature; reviews of some of the various approaches can be found in (Wang et al., 1990; Wu and Tseng, 1993; Sproat and Shih, 2001). The problem with this literature has always been that it is very hard to compare systems, due to the lack of any common standard test set. Thus, an approach that seems very promising based on its published report is nonetheless hard to compare fairly with other systems, since the systems are often tested on their own selected test corpora. Part of the problem is also that there is no single accepted segmentation standard: There are several, including the four standards used in this evaluation. A number of segmentation contests have been held in recent years within Mainland China, in the cont</context>
</contexts>
<marker>Sproat, Shih, 2001</marker>
<rawString>Richard Sproat and Chilin Shih. 2001. Corpus-based methods in Chinese morphology and phonology. Technical report, Linguistic Society of America Summer Institute, Santa Barbara, CA. http://www.research.att.com/˜rws/ newindex/notes.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongheng Wang</author>
<author>Haiju Su</author>
<author>Yan Mo</author>
</authors>
<title>Automatic processing of Chinese words.</title>
<date>1990</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="973" citStr="Wang et al., 1990" startWordPosition="143" endWordPosition="146">ese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan. We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future. 1 Introduction Chinese word segmentation is a difficult problem that has received a lot of attention in the literature; reviews of some of the various approaches can be found in (Wang et al., 1990; Wu and Tseng, 1993; Sproat and Shih, 2001). The problem with this literature has always been that it is very hard to compare systems, due to the lack of any common standard test set. Thus, an approach that seems very promising based on its published report is nonetheless hard to compare fairly with other systems, since the systems are often tested on their own selected test corpora. Part of the problem is also that there is no single accepted segmentation standard: There are several, including the four standards used in this evaluation. A number of segmentation contests have been held in rec</context>
</contexts>
<marker>Wang, Su, Mo, 1990</marker>
<rawString>Yongheng Wang, Haiju Su, and Yan Mo. 1990. Automatic processing of Chinese words. Journal of Chinese Information Processing, 4(4):1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Wilkinson</author>
</authors>
<title>Chinese document retrieval at TREC-6.</title>
<date>1998</date>
<booktitle>Proceedings of the Sixth Text REtrieval Conference.</booktitle>
<editor>In E.M. Vorhees and D.K. Harman, editors,</editor>
<contexts>
<context position="5049" citStr="Wilkinson, 1998" startWordPosition="818" endWordPosition="819">g sets. The handling of out-ofvocabulary words becomes a much larger issue in these situations than is accounted for within the test environment: A system that performs admirably in the competition may perform poorly on texts from different registers. Another issue that is not accounted for in the current collection of evaluations is the handling of short strings with minimal context, such as queries submitted to a search engine. This has been studied indirectly through the cross-language information retrieval work performed for the TREC 5 and TREC 6 competitions (Smeaton and Wilkinson, 1997; Wilkinson, 1998). This report summarizes the results of this First International Chinese Word Segmentation Bakeoff, provides some analysis of the results, and makes specific recommendations for future bakeoffs. One thing we do not do here is get into the details of specific systems; each of the participants was required to provide a four page description of their system along with detailed discussion of their results, and these papers are published in this volume. 2 Details of the contest 2.1 Corpora The corpora are detailed in Table 1. Links to descriptions of the corpora can be found at http://www.sighan.or</context>
</contexts>
<marker>Wilkinson, 1998</marker>
<rawString>Ross Wilkinson. 1998. Chinese document retrieval at TREC-6. In E.M. Vorhees and D.K. Harman, editors, Proceedings of the Sixth Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zimin Wu</author>
<author>Gwyneth Tseng</author>
</authors>
<title>Chinese text segmentation for text retrieval: Achievements and problems.</title>
<date>1993</date>
<journal>Journal of the American Societyfor Information Science,</journal>
<volume>44</volume>
<issue>9</issue>
<contexts>
<context position="993" citStr="Wu and Tseng, 1993" startWordPosition="147" endWordPosition="150">on Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan. We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future. 1 Introduction Chinese word segmentation is a difficult problem that has received a lot of attention in the literature; reviews of some of the various approaches can be found in (Wang et al., 1990; Wu and Tseng, 1993; Sproat and Shih, 2001). The problem with this literature has always been that it is very hard to compare systems, due to the lack of any common standard test set. Thus, an approach that seems very promising based on its published report is nonetheless hard to compare fairly with other systems, since the systems are often tested on their own selected test corpora. Part of the problem is also that there is no single accepted segmentation standard: There are several, including the four standards used in this evaluation. A number of segmentation contests have been held in recent years within Mai</context>
</contexts>
<marker>Wu, Tseng, 1993</marker>
<rawString>Zimin Wu and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: Achievements and problems. Journal of the American Societyfor Information Science, 44(9):532–542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andi Wu</author>
</authors>
<title>Customizable segmentation of morphologically derived words in Chinese.</title>
<date>2003</date>
<journal>International Journal of Computational Linguistics and Chinese Language Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>forthcoming.</pages>
<contexts>
<context position="3205" citStr="Wu, 2003" startWordPosition="514" endWordPosition="515">d China, three from Hong Kong, one from Japan, one from Singapore, one from Taiwan and four from the United States. Secondly, as we have already noted, there are at least four distinct standards in active use in the sense that large corpora are being developed according to those standards; see Section 2.1. It has also been observed that different segmentation standards are appropriate for different purposes; that the segmentation standard that one might prefer for information retrieval applications is likely to be different from the one that one would prefer for text-to-speech synthesis; see (Wu, 2003) for useful discussion. Thus, while we do not subscribe to the view that any of the extant standards are, in fact, appropriate for any particular application, nevertheless, it seems desirable to have a contest where people are tested against more than one standard. A third point is that we decided early on that we would not be lenient in our scoring, so that alternative segmentations as in the case of Mao Zedong, cited above, would not be allowed. While it would be fairly straightforward (in many cases) to automatically score both alternatives, we felt we could provide a more objective measure</context>
</contexts>
<marker>Wu, 2003</marker>
<rawString>Andi Wu. 2003. Customizable segmentation of morphologically derived words in Chinese. International Journal of Computational Linguistics and Chinese Language Processing, 8(1): forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Segmentation guideline, Chinese Treebank Project.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<note>http://morph.ldc.upenn.edu/ctb/.</note>
<contexts>
<context position="5772" citStr="Xia, 1999" startWordPosition="930" endWordPosition="931"> analysis of the results, and makes specific recommendations for future bakeoffs. One thing we do not do here is get into the details of specific systems; each of the participants was required to provide a four page description of their system along with detailed discussion of their results, and these papers are published in this volume. 2 Details of the contest 2.1 Corpora The corpora are detailed in Table 1. Links to descriptions of the corpora can be found at http://www.sighan.org/bakeoff2003/ bakeoff_instr.html; publications on specific corpora are (Huang et al., 1997) (Academia Sinica), (Xia, 1999) (Chinese Treebank); the Beijing University standard is very similar to that outlined in (GB/T 13715–92, 1993). Table 1 lists the abbreviations for the four corpora that will be used throughout this paper. The suffixes “o” and “c” will be used to denote open and closed tracks, respectively: Thus “ASo,c” denotes the Academia Sinica corpus, both open and closed tracks; and “PKc” denotes the Beijing University corpus, closed track. During the course of this bakeoff, a number of inconsistencies in segmentation were noted in the CTB corpus by one of the participants. This was done early enough so t</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fei Xia. 1999. Segmentation guideline, Chinese Treebank Project. Technical report, University of Pennsylvania. http://morph.ldc.upenn.edu/ctb/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>