<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000740">
<title confidence="0.99498">
Automatically creating datasets for measures of semantic relatedness
</title>
<author confidence="0.99123">
Torsten Zesch and Iryna Gurevych
</author>
<affiliation confidence="0.846659666666667">
Department of Telecooperation
Darmstadt University of Technology
D-64289 Darmstadt, Germany
</affiliation>
<email confidence="0.901964">
{zesch,gurevych} (at) tk.informatik.tu-darmstadt.de
</email>
<sectionHeader confidence="0.990327" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999818133333333">
Semantic relatedness is a special form of
linguistic distance between words. Eval-
uating semantic relatedness measures is
usually performed by comparison with hu-
man judgments. Previous test datasets had
been created analytically and were limited
in size. We propose a corpus-based system
for automatically creating test datasets.1
Experiments with human subjects show
that the resulting datasets cover all de-
grees of relatedness. As a result of the
corpus-based approach, test datasets cover
all types of lexical-semantic relations and
contain domain-specific words naturally
occurring in texts.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999841">
Linguistic distance plays an important role in
many applications like information retrieval, word
sense disambiguation, text summarization or
spelling correction. It is defined on different kinds
of textual units, e.g. documents, parts of a docu-
ment (e.g. words and their surrounding context),
words or concepts (Lebart and Rajman, 2000).2
Linguistic distance between words is inverse to
their semantic similarity or relatedness.
Semantic similarity is typically defined via the
lexical relations of synonymy (automobile – car)
and hypernymy (vehicle – car), while semantic
relatedness (SR) is defined to cover any kind of
lexical or functional association that may exist be-
</bodyText>
<footnote confidence="0.98484775">
1In the near future, we are planning to make the software
available to interested researchers.
2In this paper, word denotes the graphemic form of a to-
ken and concept refers to a particular sense of a word.
</footnote>
<bodyText confidence="0.999852647058823">
tween two words (Gurevych, 2005).3 Dissimilar
words can be semantically related, e.g. via func-
tional relationships (night – dark) or when they
are antonyms (high – low). Many NLP applica-
tions require knowledge about semantic related-
ness rather than just similarity (Budanitsky and
Hirst, 2006).
A number of competing approaches for comput-
ing semantic relatedness of words have been de-
veloped (see Section 2). A commonly accepted
method for evaluating these approaches is to com-
pare their results with a gold standard based on
human judgments on word pairs. For that pur-
pose, relatedness scores for each word pair have
to be determined experimentally. Creating test
datasets for such experiments has so far been a
labor-intensive manual process.
We propose a corpus-based system to automat-
ically create test datasets for semantic relatedness
experiments. Previous datasets were created ana-
lytically, preventing their use to gain insights into
the nature of SR and also not necessarily reflecting
the reality found in a corpus. They were also lim-
ited in size. We provide a larger annotated test set
that is used to better analyze the connections and
differences between the approaches for computing
semantic relatedness.
The remainder of this paper is organized as fol-
lows: we first focus on the notion of semantic re-
latedness and how it can be evaluated. Section 3
reviews related work. Section 4 describes our sys-
tem for automatically extracting word pairs from a
corpus. Furthermore, the experimental setup lead-
ing to human judgments of semantic relatedness
</bodyText>
<footnote confidence="0.97196475">
3Nevertheless the two terms are often (mis)used inter-
changeably. We will use semantic relatedness in the remain-
der of this paper, as it is the more general term that subsumes
semantic similarity.
</footnote>
<page confidence="0.972497">
16
</page>
<note confidence="0.698426">
Proceedings of the Workshop on Linguistic Distances, pages 16–24,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998554">
is presented. Section 5 discusses the results, and
finally we draw some conclusions in Section 6.
</bodyText>
<sectionHeader confidence="0.888729" genericHeader="method">
2 Evaluating SR measures
</sectionHeader>
<bodyText confidence="0.999813818181818">
Various approaches for computing semantic re-
latedness of words or concepts have been pro-
posed, e.g. dictionary-based (Lesk, 1986),
ontology-based (Wu and Palmer, 1994; Leacock
and Chodorow, 1998), information-based (Resnik,
1995; Jiang and Conrath, 1997) or distributional
(Weeds and Weir, 2005). The knowledge sources
used for computing relatedness can be as different
as dictionaries, ontologies or large corpora.
According to Budanitsky and Hirst (2006),
there are three prevalent approaches for evaluating
SR measures: mathematical analysis, application-
specific evaluation and comparison with human
judgments.
Mathematical analysis can assess a measure
with respect to some formal properties, e.g.
whether a measure is a metric (Lin, 1998).4 How-
ever, mathematical analysis cannot tell us whether
a measure closely resembles human judgments or
whether it performs best when used in a certain
application.
The latter question is tackled by application-
specific evaluation, where a measure is tested
within the framework of a certain application,
e.g. word sense disambiguation (Patwardhan et
al., 2003) or malapropism detection (Budanitsky
and Hirst, 2006). Lebart and Rajman (2000) ar-
gue for application-specific evaluation of similar-
ity measures, because measures are always used
for some task. But they also note that evaluating
a measure as part of a usually complex applica-
tion only indirectly assesses its quality. A certain
measure may work well in one application, but not
in another. Application-based evaluation can only
state the fact, but give little explanation about the
reasons.
The remaining approach - comparison with hu-
man judgments - is best suited for application
independent evaluation of relatedness measures.
Human annotators are asked to judge the related-
ness of presented word pairs. Results from these
experiments are used as a gold standard for eval-
uation. A further advantage of comparison with
human judgments is the possibility to gain deeper
</bodyText>
<footnote confidence="0.712924">
4That means, whether it fulfills some mathematical crite-
</footnote>
<construct confidence="0.9578935">
ria: d(x, y) &gt; 0; d(x, y) = 0 4* x = y; d(x, y) = d(y, x);
d(x, z) &lt; d(x, y) + d(y, z).
</construct>
<bodyText confidence="0.999468090909091">
insights into the nature of semantic relatedness.
However, creating datasets for evaluation has so
far been limited in a number of respects. Only
a small number of word pairs was manually se-
lected, with semantic similarity instead of related-
ness in mind. Word pairs consisted only of noun-
noun combinations and only general terms were
included. Polysemous and homonymous words
were not disambiguated to concepts, i.e. humans
annotated semantic relatedness of words rather
than concepts.
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="method">
3 Related work
</sectionHeader>
<bodyText confidence="0.999218368421053">
In the seminal work by Rubenstein and Goode-
nough (1965), similarity judgments were obtained
from 51 test subjects on 65 noun pairs written on
paper cards. Test subjects were instructed to order
the cards according to the “similarity of meaning”
and then assign a continuous similarity value (0.0 -
4.0) to each card. Miller and Charles (1991) repli-
cated the experiment with 38 test subjects judg-
ing on a subset of 30 pairs taken from the original
65 pairs. This experiment was again replicated by
Resnik (1995) with 10 subjects. Table 1 summa-
rizes previous experiments.
A comprehensive evaluation of SR measures re-
quires a higher number of word pairs. However,
the original experimental setup is not scalable as
ordering several hundred paper cards is a cum-
bersome task. Furthermore, semantic relatedness
is an intuitive concept and being forced to assign
fine-grained continuous values is felt to overstrain
the test subjects. Gurevych (2005) replicated the
experiment of Rubenstein and Goodenough with
the original 65 word pairs translated into German.
She used an adapted experimental setup where test
subjects had to assign discrete values {0,1,2,3,4}
and word pairs were presented in isolation. This
setup is also scalable to a higher number of word
pairs (350) as was shown in Gurevych (2006).
Finkelstein et al. (2002) annotated a larger set of
word pairs (353), too. They used a 0-10 range of
relatedness scores, but did not give further details
about their experimental setup. In psycholinguis-
tics, relatedness of words can also be determined
through association tests (Schulte im Walde and
Melinger, 2005). Results of such experiments are
hard to quantify and cannot easily serve as the ba-
sis for evaluating SR measures.
Rubenstein and Goodenough selected word
pairs analytically to cover the whole spectrum of
</bodyText>
<page confidence="0.998924">
17
</page>
<table confidence="0.987014176470588">
PAPER LANGUAGE PAIRS POS REL-TYPE
R/G (1965) English 65 N sim
M/C (1991) English 30 N sim
Res (1995) English 30 N sim
Fin (2002) English 353 N, V, A relat
Gur (2005) German 65 N sim
Gur (2006) German 350 N, V, A relat
Z/G (2006) German 328 N, V, A relat
CORRELATION
SCORES # SUBJECTS INTER INTRA
continuous 0–4 51 - .850
continuous 0–4 38 - -
continuous 0–4 10 .903 -
continuous 0–10 16 - -
discrete {0,1,2,3,4} 24 .810 -
discrete {0,1,2,3,4} 8 .690 -
discrete {0,1,2,3,4} 21 .478 .647
</table>
<tableCaption confidence="0.912253">
Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and
Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych
</tableCaption>
<bodyText confidence="0.999777512820513">
similarity from “not similar” to “synonymous”.
This elaborate process is not feasible for a larger
dataset or if domain-specific test sets should be
compiled quickly. Therefore, we automatically
create word pairs using a corpus-based approach.
We assume that due to lexical-semantic cohesion,
texts contain a sufficient number of words re-
lated by means of different lexical and semantic
relations. Resulting from our corpus-based ap-
proach, test sets will also contain domain-specific
terms. Previous studies only included general
terms as opposed to domain-specific vocabularies
and therefore failed to produce datasets that can
be used to evaluate the ability of a measure to cope
with domain-specific or technical terms. This is an
important property if semantic relatedness is used
in information retrieval where users tend to use
specific search terms (Porsche) rather than general
ones (car).
Furthermore, manually selected word pairs
are often biased towards highly related pairs
(Gurevych, 2006), because human annotators tend
to select only highly related pairs connected by re-
lations they are aware of. Automatic corpus-based
selection of word pairs is more objective, leading
to a balanced dataset with pairs connected by all
kinds of lexical-semantic relations. Morris and
Hirst (2004) pointed out that many relations be-
tween words in a text are non-classical (i.e. other
than typical taxonomic relations like synonymy or
hypernymy) and therefore not covered by seman-
tic similarity.
Previous studies only considered semantic re-
latedness (or similarity) of words rather than con-
cepts. However, polysemous or homonymous
words should be annotated on the level of con-
cepts. If we assume that bank has two meanings
(“financial institution” vs. “river bank”)5 and it is
paired with money, the result is two sense quali-
</bodyText>
<footnote confidence="0.339234">
5WordNet lists 10 meanings.
</footnote>
<bodyText confidence="0.999741761904762">
fied pairs (bankfinaneial – money) and (bankriver
– money). It is obvious that the judgments on the
two concept pairs should differ considerably. Con-
cept annotated datasets can be used to test the abil-
ity of a measure to differentiate between senses
when determining the relatedness of polysemous
words. To our knowledge, this study is the first to
include concept pairs and to automatically gener-
ate the test dataset.
In our experiment, we annotated a high number
of pairs similar in size to the test sets by Finkel-
stein (2002) and Gurevych (2006). We used the re-
vised experimental setup (Gurevych, 2005), based
on discrete relatedness scores and presentation of
word pairs in isolation, that is scalable to the
higher number of pairs. We annotated semantic
relatedness instead of similarity and included also
non noun-noun pairs. Additionally, our corpus-
based approach includes domain-specific techni-
cal terms and enables evaluation of the robustness
of a measure.
</bodyText>
<sectionHeader confidence="0.999633" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999939">
4.1 System architecture
</subsectionHeader>
<bodyText confidence="0.9990100625">
Figure 1 gives an overview of our automatic
corpus-based system for creating test datasets for
evaluating SR measures.
In the first step, a source corpus is preprocessed
using tokenization, POS-tagging and lemmatiza-
tion resulting in a list of POS-tagged lemmas.
Randomly generating word pairs from this list
would result in too many unrelated pairs, yielding
an unbalanced dataset. Thus, we assign weights to
each word (e.g. using tf.idf-weighting). The most
important document-specific words get the high-
est weights and due to lexical cohesion of the doc-
uments many related words can be found among
the top rated. Therefore, we randomly generate
a user-defined number of word pairs from the r
words with the highest weights for each document.
</bodyText>
<page confidence="0.997941">
18
</page>
<figureCaption confidence="0.99504">
Figure 1: System architecture for extraction of
concept pairs.
</figureCaption>
<bodyText confidence="0.946082785714286">
In the next step, user defined filters are applied
to the initial list of word pairs. For example, a fil-
ter can remove all pairs containing only uppercase
letters (mostly acronyms). Another filter can en-
force a certain fraction of POS combinations to be
present in the result set.
As we want to obtain judgment scores for se-
mantic relatedness of concepts instead of words,
we have to include all word sense combinations of
a pair in the list. An external dictionary of word
senses is necessary for this step. It is also used to
add a gloss for each word sense that enables test
subjects to distinguish between senses.
If differences in meaning between senses are
very fine-grained, distinguishing between them is
hard even for humans (Mihalcea and Moldovan,
2001).6 Pairs containing such words are not suit-
able for evaluation. To limit their impact on the
experiment, a threshold for the maximal number
of senses can be defined. Words with a number of
senses above the threshold are removed from the
list.
The result of the extraction process is a list of
sense disambiguated, POS-tagged pairs of con-
cepts.
6E.g. the German verb “halten” that can be translated as
hold, maintain, present, sustain, etc. has 26 senses in Ger-
maNet.
</bodyText>
<subsectionHeader confidence="0.9442765">
4.2 Experimental setup
4.2.1 Extraction of concept pairs
</subsectionHeader>
<bodyText confidence="0.999982695652174">
We extracted word pairs from three different
domain-specific corpora (see Table 2). This is
motivated by the aim to enable research in infor-
mation retrieval incorporating SR measures. In
particular, the “Semantic Information Retrieval”
project (SIR Project, 2006) systematically investi-
gates the use of lexical-semantic relations between
words or concepts for improving the performance
of information retrieval systems.
The BERUFEnet (BN) corpus7 consists of de-
scriptions of 5,800 professions in Germany and
therefore contains many terms specific to profes-
sional training. Evaluating semantic relatedness
on a test set based on this corpus may reveal the
ability of a measure to adapt to a very special do-
main. The GIRT (German Indexing and Retrieval
Testdatabase) corpus (Kluck, 2004) is a collec-
tion of abstracts of social science papers. It is a
standard corpus for evaluating German informa-
tion retrieval systems. The third corpus is com-
piled from 106 arbitrarily selected scientific Pow-
erPoint presentations (SPP). They cover a wide
range of topics from bio genetics to computer sci-
ence and contain many technical terms. Due to
the special structure of presentations, this corpus
will be particularly demanding with respect to the
required preprocessing components of an informa-
tion retrieval system.
The three preprocessing steps (tokenization,
POS-tagging, lemmatization) are performed us-
ing TreeTagger (Schmid, 1995). The resulting
list of POS-tagged lemmas is weighted using the
SMART ‘ltc’8 tf.idf-weighting scheme (Salton,
1989).
We implemented a set of filters for word pairs.
One group of filters removed unwanted word
pairs. Word pairs are filtered if they contain at
least one word that a) has less than three letters b)
contains only uppercase letters (mostly acronyms)
or c) can be found in a stoplist. Another fil-
ter enforced a specified fraction of combinations
of nouns (N), verbs (V) and adjectives (A) to be
present in the result set. We used the following pa-
rameters: NN = 0.5, NV = 0.15, NA = 0.15,
V V = 0.1, V A = 0.05, AA = 0.05. That means
50% of the resulting word pairs for each corpus
</bodyText>
<footnote confidence="0.993862666666667">
7http://berufenet.arbeitsagentur.de
8l=logarithmic term frequency, t=logarithmic inverse doc-
ument frequency, c=cosine normalization.
</footnote>
<figure confidence="0.995089">
tf.idf
Word sense
dictionary
Lemmatization
Abbreviations
POS
combinations
Tokenization
POS-tagging
other user
defined filters
Stoplist
Preprocessing
Word-concept
mapping
concept pairs
with glosses
Term
weighting
Word pair
generator
Word pair
filter
Corpus
</figure>
<page confidence="0.979011">
19
</page>
<table confidence="0.999457285714286">
CORPUS # DOCS # TOKENS DOMAIN
BN 9,022 7,728,501 descriptions
of professions
GIRT 151,319 19,645,417 abstracts of social
science papers
SPP 106 144,074 scientific .ppt
presentations
</table>
<tableCaption confidence="0.999162">
Table 2: Corpus statistics.
</tableCaption>
<bodyText confidence="0.999102423076923">
were noun-noun pairs, 15% noun-verb pairs and
so on.
Word pairs containing polysemous words
are expanded to concept pairs using Ger-
maNet (Kunze, 2004), the German equivalent to
WordNet, as a sense inventory for each word. It
is the most complete resource of this type for Ger-
man.
GermaNet contains only a few conceptual
glosses. As they are required to enable test sub-
jects to distinguish between senses, we use artifi-
cial glosses composed from synonyms and hyper-
nyms as a surrogate, e.g. for brother: “brother,
male sibling” vs. “brother, comrade, friend”
(Gurevych, 2005). We removed words which had
more than three senses.
Marginal manual post-processing was neces-
sary, since the lemmatization process introduced
some errors. Foreign words were translated into
German, unless they are common technical termi-
nology. We initially selected 100 word pairs from
each corpus. 11 word pairs were removed be-
cause they comprised non-words. Expanding the
word list to a concept list increased the size of the
list. Thus, the final dataset contained 328 automat-
ically created concept pairs.
</bodyText>
<subsectionHeader confidence="0.728139">
4.2.2 Graphical User Interface
</subsectionHeader>
<bodyText confidence="0.999489533333333">
We developed a web-based interface to obtain
human judgments of semantic relatedness for each
automatically generated concept pair. Test sub-
jects were invited via email to participate in the
experiment. Thus, they were not supervised dur-
ing the experiment.
Gurevych (2006) observed that some annotators
were not familiar with the exact definition of se-
mantic relatedness. Their results differed particu-
larly in cases of antonymy or distributionally re-
lated pairs. We created a manual with a detailed
introduction to SR stressing the crucial points.
The manual was presented to the subjects before
the experiment and could be re-accessed at any
time.
</bodyText>
<figureCaption confidence="0.936259333333333">
Figure 2: Screenshot of the GUI. Polysemous
words are defined by means of synonyms and re-
lated words.
</figureCaption>
<bodyText confidence="0.999889708333333">
During the experiment, one concept pair at a
time was presented to the test subjects in random
ordering. Subjects had to assign a discrete related-
ness value {0,1,2,3,4} to each pair. Figure 2 shows
the system’s GUI.
In case of a polysemous word, synonyms or
related words were presented to enable test sub-
jects to understand the sense of a presented con-
cept. Because this additional information can lead
to undesirable priming effects, test subjects were
instructed to deliberately decide only about the re-
latedness of a concept pair and use the gloss solely
to understand the sense of the presented concept.
Since our corpus-based approach includes
domain-specific vocabulary, we could not assume
that the subjects were familiar with all words.
Thus, they were instructed to look up unknown
words in the German Wikipedia.9
Several test subjects were asked to repeat the
experiment with a minimum break of one day. Re-
sults from the repetition can be used to measure
intra-subject correlation. They can also be used
to obtain some hints on varying difficulty of judg-
ment for special concept pairs or parts-of-speech.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="evaluation">
5 Results and discussion
</sectionHeader>
<bodyText confidence="0.99430125">
21 test subjects (13 males, 8 females) participated
in the experiment, two of them repeated it. The
average age of the subjects was 26 years. Most
subjects had an IT background. The experiment
took 39 minutes on average, leaving about 7 sec-
onds for rating each concept pair.
The summarized inter-subject correlation be-
tween 21 subjects was r=.478 (cf. Table 3), which
</bodyText>
<footnote confidence="0.971978">
9http://www.wikipedia.de
</footnote>
<page confidence="0.98405">
20
</page>
<table confidence="0.995085166666667">
CONCEPTS WORDS
INTER INTRA INTER INTRA
all .478 .647 .490 .675
BN .469 .695 .501 .718
GIRT .451 .598 .463 .625
SPP .535 .649 .523 .679
AA .556 .890 .597 .887
NA .547 .773 .511 .758
NV .510 .658 .540 .647
NN .463 .620 .476 .661
VA .317 .318 .391 .212
VV .278 .494 .301 .476
</table>
<tableCaption confidence="0.800993666666667">
Table 3: Summarized correlation coefficients for
all pairs, grouped by corpus and grouped by POS
combinations.
</tableCaption>
<bodyText confidence="0.999958685714286">
is statistically significant at p &lt; .05. This correla-
tion coefficient is an upper bound of performance
for automatic SR measures applied on the same
dataset.
Resnik (1995) reported a correlation of
r=.9026.10 The results are not directly compara-
ble, because he only used noun-noun pairs, words
instead of concepts, a much smaller dataset, and
measured semantic similarity instead of semantic
relatedness. Finkelstein et al. (2002) did not
report inter-subject correlation for their larger
dataset. Gurevych (2006) reported a correlation
of r=.69. Test subjects were trained students of
computational linguistics, and word pairs were
selected analytically.
Evaluating the influence of using concept pairs
instead of word pairs is complicated because word
level judgments are not directly available. There-
fore, we computed a lower and an upper bound
for correlation coefficients. For the lower bound,
we always selected the concept pair with highest
standard deviation from each set of corresponding
concept pairs. The upper bound is computed by
selecting the concept pair with the lowest standard
deviation. The differences between correlation co-
efficient for concepts and words are not signifi-
cant. Table 3 shows only the lower bounds.
Correlation coefficients for experiments mea-
suring semantic relatedness are expected to be
lower than results for semantic similarity, since the
former also includes additional relations (like co-
occurrence of words) and is thus a more compli-
cated task. Judgments for such relations strongly
depend on experience and cultural background of
the test subjects. While most people may agree
</bodyText>
<footnote confidence="0.983980333333333">
10Note that Resnik used the averaged correlation coeffi-
cient. We computed the summarized correlation coefficient
using a Fisher Z-value transformation.
</footnote>
<figureCaption confidence="0.985979">
Figure 3: Distribution of averaged human judg-
ments.
</figureCaption>
<figure confidence="0.996748285714286">
4
3
2
1
0
0 10 20
Conc
</figure>
<figureCaption confidence="0.9978305">
Figure 4: Distribution of averaged human judg-
ments with standard deviation &lt; 0.8.
</figureCaption>
<bodyText confidence="0.9953134">
that (car – vehicle) are highly related, a strong
connection between (parts – speech) may only be
established by a certain group. Due to the corpus-
based approach, many domain-specific concept
pairs are introduced into the test set. Therefore,
inter-subject correlation is lower than the results
obtained by Gurevych (2006).
In our experiment, intra-subject correlation was
r=.670 for the first and r=.623 for the second in-
dividual who repeated the experiment, yielding
a summarized intra-subject correlation of r=.647.
Rubenstein and Goodenough (1965) reported an
intra-subject correlation of r=.85 for 15 subjects
judging the similarity of a subset (36) of the orig-
inal 65 word pairs. The values may again not be
compared directly. Furthermore, we cannot gen-
eralize from these results, because the number of
participants which repeated our experiment was
too low.
The distribution of averaged human judgments
on the whole test set (see Figure 3) is almost bal-
anced with a slight underrepresentation of highly
related concepts. To create more highly re-
lated concept pairs, more sophisticated weighting
schemes or selection on the basis of lexical chain-
</bodyText>
<page confidence="0.999297">
21
</page>
<figureCaption confidence="0.979296">
Figure 5: Averaged judgments and standard devia-
</figureCaption>
<bodyText confidence="0.981852370967742">
tion for all concept pairs. Low deviations are only
observed for low or high judgments.
ing could be used. However, even with the present
setup, automatic extraction of concept pairs per-
forms remarkably well and can be used to quickly
create balanced test datasets.
Budanitsky and Hirst (2006) pointed out that
distribution plots of judgments for the word pairs
used by Rubenstein and Goodenough display an
empty horizontal band that could be used to sepa-
rate related and unrelated pairs. This empty band
is not observed here. However, Figure 4 shows the
distribution of averaged judgments with the high-
est agreement between annotators (standard devi-
ation &lt; 0.8). The plot clearly shows an empty hor-
izontal band with no judgments. The connection
between averaged judgments and standard devia-
tion is plotted in Figure 5.
When analyzing the concept pairs with lowest
deviation there is a clear tendency for particularly
highly related pairs, e.g. hypernymy: Universität
– Bildungseinrichtung (university – educational
institution); functional relation: Tätigkeit – aus-
führen (task – perform); or pairs that are obviously
not connected, e.g. logisch – Juni (logical – June).
Table 4 lists some example concept pairs along
with averaged judgments and standard deviation.
Concept pairs with high deviations between
judgments often contain polysemous words. For
example, Quelle (source) was disambiguated to
Wasserquelle (spring) and paired with Text
(text). The data shows a clear distinction be-
tween one group that rated the pair low (0) and
another group that rated the pair high (3 or 4). The
latter group obviously missed the point that tex-
tual source was not an option here. High devia-
tions were also common among special technical
terms like (Mips – Core), proper names (Georg –
August – two common first names in German) or
functionally related pairs (agieren – mobil). Hu-
man experience and cultural background clearly
influence the judgment of such pairs.
The effect observed here and the effect noted
by Budanitsky and Hirst is probably caused by the
same underlying principle. Human agreement on
semantic relatedness is only reliable if two words
or concepts are highly related or almost unrelated.
Intuitively, this means that classifying word pairs
as related or unrelated is much easier than numeri-
cally rating semantic relatedness. For an informa-
tion retrieval task, such a classification might be
sufficient.
Differences in correlation coefficients for the
three corpora are not significant indicating that the
phenomenon is not domain-specific. Differences
in correlation coefficients for different parts-of-
speech are significant (see Table 3). Verb-verb and
verb-adjective pairs have the lowest correlation.
A high fraction of these pairs is in the problem-
atic medium relatedness area. Adjective-adjective
pairs have the highest correlation. Most of these
pairs are either highly related or not related at all.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999989615384615">
We proposed a system for automatically creating
datasets for evaluating semantic relatedness mea-
sures. We have shown that our corpus-based ap-
proach enables fast development of large domain-
specific datasets that cover all types of lexical and
semantic relations. We conducted an experiment
to obtain human judgments of semantic related-
ness on concept pairs. Results show that averaged
human judgments cover all degrees of relatedness
with a slight underrepresentation of highly related
concept pairs. More highly related concept pairs
could be generated by using more sophisticated
weighting schemes or selecting concept pairs on
the basis of lexical chaining.
Inter-subject correlation in this experiment is
lower than the results from previous studies due
to several reasons. We measured semantic relat-
edness instead of semantic similarity. The for-
mer is a more complicated task for annotators be-
cause its definition includes all kinds of lexical-
semantic relations not just synonymy. In addition,
concept pairs were automatically selected elimi-
nating the bias towards strong classical relations
with high agreement that is introduced into the
dataset by a manual selection process. Further-
more, our dataset contains many domain-specific
</bodyText>
<page confidence="0.995432">
22
</page>
<table confidence="0.6216835">
PAIR
GERMAN ENGLISH CORPUS AVG ST-DEV
</table>
<tableCaption confidence="0.999675">
Table 4: Example concept pairs with averaged judgments and standard deviation. Only one sense is
listed for polysemous words. Conceptual glosses are omitted due to space limitations.
</tableCaption>
<figure confidence="0.849201266666667">
Universität – Bildungseinrichtung university – educational institution GIRT 3.90 0.30
Tätigkeit – ausführen task – to perform BN 3.67 0.58
strafen – Paragraph to punish – paragraph GIRT 3.00 1.18
Quelle – Text spring – text GIRT 2.43 1.57
Mips – Core mips – core SPP 2.10 1.55
elektronisch – neu electronic – new GIRT
verarbeiten – dichten to manipulate – to caulk BN
Leopold – Institut Leopold – institute SPP
Outfit – Strom outfit – electricity GIRT
logisch – Juni logical – June SPP
1.71 1.15
1.29 1.42
0.81 1.25
0.24 0.44
0.14 0.48
</figure>
<bodyText confidence="0.953604655172414">
concept pairs which have been rated very differ-
ently by test subjects depending on their expe-
rience. Future experiments should ensure that
domain-specific pairs are judged by domain ex-
perts to reduce disagreement between annotators
caused by varying degrees of familiarity with the
domain.
An analysis of the data shows that test sub-
jects more often agreed on highly related or unre-
lated concept pairs, while they often disagreed on
pairs with a medium relatedness value. This result
raises the question whether human judgments of
semantic relatedness with medium scores are re-
liable and should be used for evaluating seman-
tic relatedness measures. We plan to investigate
the impact of this outcome on the evaluation of
semantic relatedness measures. Additionally, for
some applications like information retrieval it may
be sufficient to detect highly related pairs rather
than accurately rating word pairs with medium
values.
There is also a significant difference between
the correlation coefficient for different POS com-
binations. Further investigations are needed to elu-
cidate whether these differences are caused by the
new procedure for corpus-based selection of word
pairs proposed in this paper or are due to inherent
properties of semantic relations existing between
word classes.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998620428571429">
We would like to thank Sabine Schulte im Walde
for her remarks on experimental setups. We are
grateful to the Bundesagentur für Arbeit for pro-
viding the BERUFEnet corpus. This work was
carried out as part of the “Semantic Information
Retrieval” (SIR) project funded by the German
Research Foundation.
</bodyText>
<sectionHeader confidence="0.999143" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99963856097561">
Alexander Budanitsky and Graeme Hirst. 2006. Evaluating
WordNet-based Measures of Semantic Distance. Compu-
tational Linguistics, 32(1).
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud
Rivlin, Zach Solan, and Gadi Wolfman. 2002. Placing
Search in Context: The Concept Revisited. ACM Trans-
actions on Information Systems, 20(1):116–131.
Iryna Gurevych. 2005. Using the Structure of a Conceptual
Network in Computing Semantic Relatedness. In Pro-
ceedings of the 2nd International Joint Conference on Nat-
ural Language Processing, pages 767–778, Jeju Island,
Republic of Korea.
Iryna Gurevych. 2006. Computing Semantic Relatedness
Across Parts of Speech. Technical report, Darmstadt Uni-
versity of Technology, Germany, Department of Computer
Science, Telecooperation.
Jay J. Jiang and David W. Conrath. 1997. Semantic Similar-
ity Based on Corpus Statistics and Lexical Taxonomy. In
Proceedings of the 10th International Conference on Re-
search in Computational Linguistics.
Michael Kluck. 2004. The GIRT Data in the Evaluation of
CLIR Systems - from 1997 Until 2003. Lecture Notes in
Computer Science, 3237:376–390, January.
Claudia Kunze, 2004. Lexikalisch-semantische Wortnetze,
chapter Computerlinguistik und Sprachtechnologie, pages
423–431. Spektrum Akademischer Verlag.
Claudia Leacock and Martin Chodorow, 1998. WordNet: An
Electronic Lexical Database, chapter Combining Local
Context and WordNet Similarity for Word Sense Identi-
fication, pages 265–283. Cambridge: MIT Press.
Ludovic Lebart and Martin Rajman. 2000. Computing Sim-
ilarity. In Robert Dale, editor, Handbook of NLP. Dekker:
Basel.
Michael Lesk. 1986. Automatic Sense Disambiguation Us-
ing Machine Readable Dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the 5th
Annual International Conference on Systems Documenta-
tion, pages 24–26, Toronto, Ontario, Canada.
Dekang Lin. 1998. An Information-Theoretic Definition of
Similarity. In Proceedings of International Conference on
Machine Learning, Madison, Wisconsin.
</reference>
<page confidence="0.970679">
23
</page>
<reference confidence="0.999577282608696">
Rada Mihalcea and Dan Moldovan. 2001. Automatic Gen-
eration of a Coarse Grained WordNet. In Proceedings
of NAACL Workshop on WordNet and Other Lexical Re-
sources, Pittsburgh, PA, June.
George A. Miller and Walter G. Charles. 1991. Contextual
Correlates of Semantic Similarity. Language and Cogni-
tive Processes, 6(1):1–28.
Jane Morris and Graeme Hirst. 2004. Non-Classical Lexical
Semantic Relations. In Workshop on Computational Lex-
ical Semantics, Human Language Technology Conference
of the North American Chapter of the ACL, Boston.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-
sen. 2003. Using Measures of Semantic Relatedness
for Word Sense Disambiguation. In Proceedings of the
Fourth International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, Mexico City.
Philip Resnik. 1995. Using Information Content to Evalu-
ate Semantic Similarity. In Proceedings of the 14th Inter-
national Joint Conference on Artificial Intelligence, pages
448–453, Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual Correlates of Synonymy. Communications of the
ACM, 8(10):627–633.
Gerard Salton. 1989. Automatic Text Processing: the Trans-
formation, Analysis, and Retrieval of Information by Com-
puter. Addison-Wesley Longman Publishing, Boston,
MA, USA.
Helmut Schmid. 1995. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference on
New Methods in Language Processing, Manchester, UK.
Sabine Schulte im Walde and Alissa Melinger. 2005. Iden-
tifying Semantic Relations and Functional Properties of
Human Verb Associations. In Proceedings of the Joint
Conference on Human Language Technology and Empiri-
cal Methods in NLP, pages 612–619, Vancouver, Canada.
SIR Project. 2006. Project ‘Semantic Information
Retrieval’. URL http://www.cre-elearning.
tu-darmstadt.de/elearning/sir/.
Julie Weeds and David Weir. 2005. Co-occurrence Retrieval:
A Flexible Framework For Lexical Distributional Similar-
ity. Computational Linguistics, 31(4):439–475, Decem-
ber.
Zhibiao Wu and Martha Palmer. 1994. Verb Semantics and
Lexical Selection. In 32nd Annual Meeting of the ACL,
pages 133–138, New Mexico State University, Las Cruces,
New Mexico.
</reference>
<page confidence="0.999169">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928875">
<title confidence="0.998053">Automatically creating datasets for measures of semantic relatedness</title>
<author confidence="0.971198">Zesch</author>
<affiliation confidence="0.999191">Department of Darmstadt University of</affiliation>
<address confidence="0.9838">D-64289 Darmstadt,</address>
<email confidence="0.990139">zesch(at)tk.informatik.tu-darmstadt.de</email>
<email confidence="0.990139">gurevych(at)tk.informatik.tu-darmstadt.de</email>
<abstract confidence="0.9989584375">Semantic relatedness is a special form of linguistic distance between words. Evaluating semantic relatedness measures is usually performed by comparison with human judgments. Previous test datasets had been created analytically and were limited in size. We propose a corpus-based system automatically creating test Experiments with human subjects show that the resulting datasets cover all degrees of relatedness. As a result of the corpus-based approach, test datasets cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based Measures of Semantic Distance.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="2038" citStr="Budanitsky and Hirst, 2006" startWordPosition="291" endWordPosition="294">icle – car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be1In the near future, we are planning to make the software available to interested researchers. 2In this paper, word denotes the graphemic form of a token and concept refers to a particular sense of a word. tween two words (Gurevych, 2005).3 Dissimilar words can be semantically related, e.g. via functional relationships (night – dark) or when they are antonyms (high – low). Many NLP applications require knowledge about semantic relatedness rather than just similarity (Budanitsky and Hirst, 2006). A number of competing approaches for computing semantic relatedness of words have been developed (see Section 2). A commonly accepted method for evaluating these approaches is to compare their results with a gold standard based on human judgments on word pairs. For that purpose, relatedness scores for each word pair have to be determined experimentally. Creating test datasets for such experiments has so far been a labor-intensive manual process. We propose a corpus-based system to automatically create test datasets for semantic relatedness experiments. Previous datasets were created analytic</context>
<context position="4216" citStr="Budanitsky and Hirst (2006)" startWordPosition="629" endWordPosition="632">�2006 Association for Computational Linguistics is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application. The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word</context>
<context position="23738" citStr="Budanitsky and Hirst (2006)" startWordPosition="3728" endWordPosition="3731"> distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts. To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain21 Figure 5: Averaged judgments and standard deviation for all concept pairs. Low deviations are only observed for low or high judgments. ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. Budanitsky and Hirst (2006) pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs. This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation &lt; 0.8). The plot clearly shows an empty horizontal band with no judgments. The connection between averaged judgments and standard deviation is plotted in Figure 5. When analyzing the concept pairs with lowest deviation there is a clear t</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based Measures of Semantic Distance. Computational Linguistics, 32(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
</authors>
<title>Placing Search in Context: The Concept Revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="7704" citStr="Finkelstein et al. (2002)" startWordPosition="1185" endWordPosition="1188">scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005). Results of such experiments are hard to quantify and cannot easily serve as the basis for evaluating SR measures. Rubenstein and Goodenough selected word pairs analytically to cover the whole spectrum of 17 PAPER LANGUAGE PAIRS POS REL-TYPE R/G (1965) English 65 N sim M/C (1991) English 30 N sim Res (1995) </context>
<context position="20742" citStr="Finkelstein et al. (2002)" startWordPosition="3264" endWordPosition="3267">.658 .540 .647 NN .463 .620 .476 .661 VA .317 .318 .391 .212 VV .278 .494 .301 .476 Table 3: Summarized correlation coefficients for all pairs, grouped by corpus and grouped by POS combinations. is statistically significant at p &lt; .05. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. Resnik (1995) reported a correlation of r=.9026.10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=.69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically. Evaluating the influence of using concept pairs instead of word pairs is complicated because word level judgments are not directly available. Therefore, we computed a lower and an upper bound for correlation coefficients. For the lower bound, we always selected the concept pair with highest standard deviation from each set of corresponding concept pairs. The upper bound i</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, and Gadi Wolfman. 2002. Placing Search in Context: The Concept Revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
</authors>
<title>Using the Structure of a Conceptual Network in Computing Semantic Relatedness.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd International Joint Conference on Natural Language Processing,</booktitle>
<pages>767--778</pages>
<location>Jeju Island, Republic of</location>
<contexts>
<context position="1777" citStr="Gurevych, 2005" startWordPosition="253" endWordPosition="254">ds or concepts (Lebart and Rajman, 2000).2 Linguistic distance between words is inverse to their semantic similarity or relatedness. Semantic similarity is typically defined via the lexical relations of synonymy (automobile – car) and hypernymy (vehicle – car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be1In the near future, we are planning to make the software available to interested researchers. 2In this paper, word denotes the graphemic form of a token and concept refers to a particular sense of a word. tween two words (Gurevych, 2005).3 Dissimilar words can be semantically related, e.g. via functional relationships (night – dark) or when they are antonyms (high – low). Many NLP applications require knowledge about semantic relatedness rather than just similarity (Budanitsky and Hirst, 2006). A number of competing approaches for computing semantic relatedness of words have been developed (see Section 2). A commonly accepted method for evaluating these approaches is to compare their results with a gold standard based on human judgments on word pairs. For that purpose, relatedness scores for each word pair have to be determin</context>
<context position="7322" citStr="Gurevych (2005)" startWordPosition="1126" endWordPosition="1127">Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. This experiment was again replicated by Resnik (1995) with 10 subjects. Table 1 summarizes previous experiments. A comprehensive evaluation of SR measures requires a higher number of word pairs. However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be de</context>
<context position="11311" citStr="Gurevych, 2005" startWordPosition="1763" endWordPosition="1764">meanings. fied pairs (bankfinaneial – money) and (bankriver – money). It is obvious that the judgments on the two concept pairs should differ considerably. Concept annotated datasets can be used to test the ability of a measure to differentiate between senses when determining the relatedness of polysemous words. To our knowledge, this study is the first to include concept pairs and to automatically generate the test dataset. In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006). We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs. We annotated semantic relatedness instead of similarity and included also non noun-noun pairs. Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure. 4 Experiment 4.1 System architecture Figure 1 gives an overview of our automatic corpus-based system for creating test datasets for evaluating SR measures. In the first step, a source corpus is preprocessed using tokenization, POS-tagging</context>
<context position="17098" citStr="Gurevych, 2005" startWordPosition="2675" endWordPosition="2676">presentations Table 2: Corpus statistics. were noun-noun pairs, 15% noun-verb pairs and so on. Word pairs containing polysemous words are expanded to concept pairs using GermaNet (Kunze, 2004), the German equivalent to WordNet, as a sense inventory for each word. It is the most complete resource of this type for German. GermaNet contains only a few conceptual glosses. As they are required to enable test subjects to distinguish between senses, we use artificial glosses composed from synonyms and hypernyms as a surrogate, e.g. for brother: “brother, male sibling” vs. “brother, comrade, friend” (Gurevych, 2005). We removed words which had more than three senses. Marginal manual post-processing was necessary, since the lemmatization process introduced some errors. Foreign words were translated into German, unless they are common technical terminology. We initially selected 100 word pairs from each corpus. 11 word pairs were removed because they comprised non-words. Expanding the word list to a concept list increased the size of the list. Thus, the final dataset contained 328 automatically created concept pairs. 4.2.2 Graphical User Interface We developed a web-based interface to obtain human judgment</context>
</contexts>
<marker>Gurevych, 2005</marker>
<rawString>Iryna Gurevych. 2005. Using the Structure of a Conceptual Network in Computing Semantic Relatedness. In Proceedings of the 2nd International Joint Conference on Natural Language Processing, pages 767–778, Jeju Island, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
</authors>
<title>Computing Semantic Relatedness Across Parts of Speech.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>Darmstadt University of Technology, Germany, Department of Computer Science, Telecooperation.</institution>
<contexts>
<context position="7677" citStr="Gurevych (2006)" startWordPosition="1183" endWordPosition="1184">tal setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005). Results of such experiments are hard to quantify and cannot easily serve as the basis for evaluating SR measures. Rubenstein and Goodenough selected word pairs analytically to cover the whole spectrum of 17 PAPER LANGUAGE PAIRS POS REL-TYPE R/G (1965) English 65 N sim M/C (1991) E</context>
<context position="9857" citStr="Gurevych, 2006" startWordPosition="1527" endWordPosition="1528">relations. Resulting from our corpus-based approach, test sets will also contain domain-specific terms. Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms. This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car). Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of. Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations. Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e. other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity. Previous studies only considered semantic relatedness (or similarity) of words rather than concepts. However</context>
<context position="11254" citStr="Gurevych (2006)" startWordPosition="1754" endWordPosition="1755">th money, the result is two sense quali5WordNet lists 10 meanings. fied pairs (bankfinaneial – money) and (bankriver – money). It is obvious that the judgments on the two concept pairs should differ considerably. Concept annotated datasets can be used to test the ability of a measure to differentiate between senses when determining the relatedness of polysemous words. To our knowledge, this study is the first to include concept pairs and to automatically generate the test dataset. In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006). We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs. We annotated semantic relatedness instead of similarity and included also non noun-noun pairs. Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure. 4 Experiment 4.1 System architecture Figure 1 gives an overview of our automatic corpus-based system for creating test datasets for evaluating SR measures. In the first step, a sour</context>
<context position="17911" citStr="Gurevych (2006)" startWordPosition="2799" endWordPosition="2800"> unless they are common technical terminology. We initially selected 100 word pairs from each corpus. 11 word pairs were removed because they comprised non-words. Expanding the word list to a concept list increased the size of the list. Thus, the final dataset contained 328 automatically created concept pairs. 4.2.2 Graphical User Interface We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair. Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. Gurevych (2006) observed that some annotators were not familiar with the exact definition of semantic relatedness. Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points. The manual was presented to the subjects before the experiment and could be re-accessed at any time. Figure 2: Screenshot of the GUI. Polysemous words are defined by means of synonyms and related words. During the experiment, one concept pair at a time was presented to the test subjects in random ordering. Subjects had to</context>
<context position="20825" citStr="Gurevych (2006)" startWordPosition="3277" endWordPosition="3278">Summarized correlation coefficients for all pairs, grouped by corpus and grouped by POS combinations. is statistically significant at p &lt; .05. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. Resnik (1995) reported a correlation of r=.9026.10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=.69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically. Evaluating the influence of using concept pairs instead of word pairs is complicated because word level judgments are not directly available. Therefore, we computed a lower and an upper bound for correlation coefficients. For the lower bound, we always selected the concept pair with highest standard deviation from each set of corresponding concept pairs. The upper bound is computed by selecting the concept pair with the lowest standard deviation. The di</context>
<context position="22565" citStr="Gurevych (2006)" startWordPosition="3545" endWordPosition="3546">ik used the averaged correlation coefficient. We computed the summarized correlation coefficient using a Fisher Z-value transformation. Figure 3: Distribution of averaged human judgments. 4 3 2 1 0 0 10 20 Conc Figure 4: Distribution of averaged human judgments with standard deviation &lt; 0.8. that (car – vehicle) are highly related, a strong connection between (parts – speech) may only be established by a certain group. Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set. Therefore, inter-subject correlation is lower than the results obtained by Gurevych (2006). In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647. Rubenstein and Goodenough (1965) reported an intra-subject correlation of r=.85 for 15 subjects judging the similarity of a subset (36) of the original 65 word pairs. The values may again not be compared directly. Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low. The distribution of averaged human judgments on the whole</context>
</contexts>
<marker>Gurevych, 2006</marker>
<rawString>Iryna Gurevych. 2006. Computing Semantic Relatedness Across Parts of Speech. Technical report, Darmstadt University of Technology, Germany, Department of Computer Science, Telecooperation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the 10th International Conference on Research in Computational Linguistics.</booktitle>
<contexts>
<context position="4014" citStr="Jiang and Conrath, 1997" startWordPosition="600" endWordPosition="603">relatedness in the remainder of this paper, as it is the more general term that subsumes semantic similarity. 16 Proceedings of the Workshop on Linguistic Distances, pages 16–24, Sydney, July 2006. c�2006 Association for Computational Linguistics is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. In Proceedings of the 10th International Conference on Research in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kluck</author>
</authors>
<date>2004</date>
<booktitle>The GIRT Data in the Evaluation of CLIR Systems - from</booktitle>
<pages>3237--376</pages>
<contexts>
<context position="14592" citStr="Kluck, 2004" startWordPosition="2285" endWordPosition="2286"> measures. In particular, the “Semantic Information Retrieval” project (SIR Project, 2006) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems. The BERUFEnet (BN) corpus7 consists of descriptions of 5,800 professions in Germany and therefore contains many terms specific to professional training. Evaluating semantic relatedness on a test set based on this corpus may reveal the ability of a measure to adapt to a very special domain. The GIRT (German Indexing and Retrieval Testdatabase) corpus (Kluck, 2004) is a collection of abstracts of social science papers. It is a standard corpus for evaluating German information retrieval systems. The third corpus is compiled from 106 arbitrarily selected scientific PowerPoint presentations (SPP). They cover a wide range of topics from bio genetics to computer science and contain many technical terms. Due to the special structure of presentations, this corpus will be particularly demanding with respect to the required preprocessing components of an information retrieval system. The three preprocessing steps (tokenization, POS-tagging, lemmatization) are pe</context>
</contexts>
<marker>Kluck, 2004</marker>
<rawString>Michael Kluck. 2004. The GIRT Data in the Evaluation of CLIR Systems - from 1997 Until 2003. Lecture Notes in Computer Science, 3237:376–390, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Kunze</author>
</authors>
<title>Lexikalisch-semantische Wortnetze, chapter Computerlinguistik und Sprachtechnologie,</title>
<date>2004</date>
<pages>423--431</pages>
<publisher>Spektrum Akademischer Verlag.</publisher>
<contexts>
<context position="16675" citStr="Kunze, 2004" startWordPosition="2605" endWordPosition="2606">Word sense dictionary Lemmatization Abbreviations POS combinations Tokenization POS-tagging other user defined filters Stoplist Preprocessing Word-concept mapping concept pairs with glosses Term weighting Word pair generator Word pair filter Corpus 19 CORPUS # DOCS # TOKENS DOMAIN BN 9,022 7,728,501 descriptions of professions GIRT 151,319 19,645,417 abstracts of social science papers SPP 106 144,074 scientific .ppt presentations Table 2: Corpus statistics. were noun-noun pairs, 15% noun-verb pairs and so on. Word pairs containing polysemous words are expanded to concept pairs using GermaNet (Kunze, 2004), the German equivalent to WordNet, as a sense inventory for each word. It is the most complete resource of this type for German. GermaNet contains only a few conceptual glosses. As they are required to enable test subjects to distinguish between senses, we use artificial glosses composed from synonyms and hypernyms as a surrogate, e.g. for brother: “brother, male sibling” vs. “brother, comrade, friend” (Gurevych, 2005). We removed words which had more than three senses. Marginal manual post-processing was necessary, since the lemmatization process introduced some errors. Foreign words were tr</context>
</contexts>
<marker>Kunze, 2004</marker>
<rawString>Claudia Kunze, 2004. Lexikalisch-semantische Wortnetze, chapter Computerlinguistik und Sprachtechnologie, pages 423–431. Spektrum Akademischer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>WordNet: An Electronic Lexical Database, chapter Combining Local Context and WordNet Similarity for Word Sense Identification,</title>
<date>1998</date>
<pages>265--283</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge:</location>
<contexts>
<context position="3955" citStr="Leacock and Chodorow, 1998" startWordPosition="593" endWordPosition="596">rms are often (mis)used interchangeably. We will use semantic relatedness in the remainder of this paper, as it is the more general term that subsumes semantic similarity. 16 Proceedings of the Workshop on Linguistic Distances, pages 16–24, Sydney, July 2006. c�2006 Association for Computational Linguistics is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whe</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow, 1998. WordNet: An Electronic Lexical Database, chapter Combining Local Context and WordNet Similarity for Word Sense Identification, pages 265–283. Cambridge: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludovic Lebart</author>
<author>Martin Rajman</author>
</authors>
<title>Computing Similarity.</title>
<date>2000</date>
<booktitle>Handbook of NLP.</booktitle>
<editor>In Robert Dale, editor,</editor>
<publisher>Dekker: Basel.</publisher>
<contexts>
<context position="1202" citStr="Lebart and Rajman, 2000" startWordPosition="159" endWordPosition="162">sets.1 Experiments with human subjects show that the resulting datasets cover all degrees of relatedness. As a result of the corpus-based approach, test datasets cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts. 1 Introduction Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction. It is defined on different kinds of textual units, e.g. documents, parts of a document (e.g. words and their surrounding context), words or concepts (Lebart and Rajman, 2000).2 Linguistic distance between words is inverse to their semantic similarity or relatedness. Semantic similarity is typically defined via the lexical relations of synonymy (automobile – car) and hypernymy (vehicle – car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be1In the near future, we are planning to make the software available to interested researchers. 2In this paper, word denotes the graphemic form of a token and concept refers to a particular sense of a word. tween two words (Gurevych, 2005).3 Dissimilar words can b</context>
<context position="4943" citStr="Lebart and Rajman (2000)" startWordPosition="734" endWordPosition="737">ecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application. The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006). Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task. But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality. A certain measure may work well in one application, but not in another. Application-based evaluation can only state the fact, but give little explanation about the reasons. The remaining approach - comparison with human judgments - is best suited for application independent evaluation of relatedness measures. Human annotators are asked to judge the relatednes</context>
</contexts>
<marker>Lebart, Rajman, 2000</marker>
<rawString>Ludovic Lebart and Martin Rajman. 2000. Computing Similarity. In Robert Dale, editor, Handbook of NLP. Dekker: Basel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th Annual International Conference on Systems Documentation,</booktitle>
<pages>24--26</pages>
<location>Toronto, Ontario, Canada.</location>
<contexts>
<context position="3889" citStr="Lesk, 1986" startWordPosition="586" endWordPosition="587">s of semantic relatedness 3Nevertheless the two terms are often (mis)used interchangeably. We will use semantic relatedness in the remainder of this paper, as it is the more general term that subsumes semantic similarity. 16 Proceedings of the Workshop on Linguistic Distances, pages 16–24, Sydney, July 2006. c�2006 Association for Computational Linguistics is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metr</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the 5th Annual International Conference on Systems Documentation, pages 24–26, Toronto, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An Information-Theoretic Definition of Similarity. In</title>
<date>1998</date>
<booktitle>Proceedings of International Conference on Machine Learning,</booktitle>
<location>Madison, Wisconsin.</location>
<contexts>
<context position="4503" citStr="Lin, 1998" startWordPosition="671" endWordPosition="672">tology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application. The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006). Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task. But they also note that evaluating a measure </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An Information-Theoretic Definition of Similarity. In Proceedings of International Conference on Machine Learning, Madison, Wisconsin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>Automatic Generation of a Coarse Grained WordNet.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources,</booktitle>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="13283" citStr="Mihalcea and Moldovan, 2001" startWordPosition="2078" endWordPosition="2081">ning only uppercase letters (mostly acronyms). Another filter can enforce a certain fraction of POS combinations to be present in the result set. As we want to obtain judgment scores for semantic relatedness of concepts instead of words, we have to include all word sense combinations of a pair in the list. An external dictionary of word senses is necessary for this step. It is also used to add a gloss for each word sense that enables test subjects to distinguish between senses. If differences in meaning between senses are very fine-grained, distinguishing between them is hard even for humans (Mihalcea and Moldovan, 2001).6 Pairs containing such words are not suitable for evaluation. To limit their impact on the experiment, a threshold for the maximal number of senses can be defined. Words with a number of senses above the threshold are removed from the list. The result of the extraction process is a list of sense disambiguated, POS-tagged pairs of concepts. 6E.g. the German verb “halten” that can be translated as hold, maintain, present, sustain, etc. has 26 senses in GermaNet. 4.2 Experimental setup 4.2.1 Extraction of concept pairs We extracted word pairs from three different domain-specific corpora (see Ta</context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>Rada Mihalcea and Dan Moldovan. 2001. Automatic Generation of a Coarse Grained WordNet. In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources, Pittsburgh, PA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<date>1991</date>
<booktitle>Contextual Correlates of Semantic Similarity. Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="6721" citStr="Miller and Charles (1991)" startWordPosition="1028" endWordPosition="1031">rity instead of relatedness in mind. Word pairs consisted only of nounnoun combinations and only general terms were included. Polysemous and homonymous words were not disambiguated to concepts, i.e. humans annotated semantic relatedness of words rather than concepts. 3 Related work In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards. Test subjects were instructed to order the cards according to the “similarity of meaning” and then assign a continuous similarity value (0.0 - 4.0) to each card. Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. This experiment was again replicated by Resnik (1995) with 10 subjects. Table 1 summarizes previous experiments. A comprehensive evaluation of SR measures requires a higher number of word pairs. However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual Correlates of Semantic Similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Non-Classical Lexical Semantic Relations.</title>
<date>2004</date>
<booktitle>In Workshop on Computational Lexical Semantics, Human Language Technology Conference of the North American Chapter of the ACL,</booktitle>
<location>Boston.</location>
<contexts>
<context position="10151" citStr="Morris and Hirst (2004)" startWordPosition="1570" endWordPosition="1573"> to cope with domain-specific or technical terms. This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car). Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of. Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations. Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e. other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity. Previous studies only considered semantic relatedness (or similarity) of words rather than concepts. However, polysemous or homonymous words should be annotated on the level of concepts. If we assume that bank has two meanings (“financial institution” vs. “river bank”)5 and it is paired with money, the result is two sense quali5WordNet lists 10 meanings. fied pairs (bankfinaneial – money) and (bankr</context>
</contexts>
<marker>Morris, Hirst, 2004</marker>
<rawString>Jane Morris and Graeme Hirst. 2004. Non-Classical Lexical Semantic Relations. In Workshop on Computational Lexical Semantics, Human Language Technology Conference of the North American Chapter of the ACL, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Using Measures of Semantic Relatedness for Word Sense Disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<location>Mexico City.</location>
<contexts>
<context position="4863" citStr="Patwardhan et al., 2003" startWordPosition="723" endWordPosition="726">lent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application. The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006). Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task. But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality. A certain measure may work well in one application, but not in another. Application-based evaluation can only state the fact, but give little explanation about the reasons. The remaining approach - comparison with human judgments - is best suited for application independent evalua</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2003. Using Measures of Semantic Relatedness for Word Sense Disambiguation. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using Information Content to Evaluate Semantic Similarity.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="3988" citStr="Resnik, 1995" startWordPosition="598" endWordPosition="599"> use semantic relatedness in the remainder of this paper, as it is the more general term that subsumes semantic similarity. 16 Proceedings of the Workshop on Linguistic Distances, pages 16–24, Sydney, July 2006. c�2006 Association for Computational Linguistics is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles </context>
<context position="6889" citStr="Resnik (1995)" startWordPosition="1060" endWordPosition="1061"> to concepts, i.e. humans annotated semantic relatedness of words rather than concepts. 3 Related work In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards. Test subjects were instructed to order the cards according to the “similarity of meaning” and then assign a continuous similarity value (0.0 - 4.0) to each card. Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. This experiment was again replicated by Resnik (1995) with 10 subjects. Table 1 summarizes previous experiments. A comprehensive evaluation of SR measures requires a higher number of word pairs. However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subje</context>
<context position="20483" citStr="Resnik (1995)" startWordPosition="3228" endWordPosition="3229">s r=.478 (cf. Table 3), which 9http://www.wikipedia.de 20 CONCEPTS WORDS INTER INTRA INTER INTRA all .478 .647 .490 .675 BN .469 .695 .501 .718 GIRT .451 .598 .463 .625 SPP .535 .649 .523 .679 AA .556 .890 .597 .887 NA .547 .773 .511 .758 NV .510 .658 .540 .647 NN .463 .620 .476 .661 VA .317 .318 .391 .212 VV .278 .494 .301 .476 Table 3: Summarized correlation coefficients for all pairs, grouped by corpus and grouped by POS combinations. is statistically significant at p &lt; .05. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. Resnik (1995) reported a correlation of r=.9026.10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=.69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically. Evaluating the influence of using concept pairs instead of word pairs is complicated because word level judgments a</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using Information Content to Evaluate Semantic Similarity. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448–453, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual Correlates of Synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="6434" citStr="Rubenstein and Goodenough (1965)" startWordPosition="980" endWordPosition="984"> &gt; 0; d(x, y) = 0 4* x = y; d(x, y) = d(y, x); d(x, z) &lt; d(x, y) + d(y, z). insights into the nature of semantic relatedness. However, creating datasets for evaluation has so far been limited in a number of respects. Only a small number of word pairs was manually selected, with semantic similarity instead of relatedness in mind. Word pairs consisted only of nounnoun combinations and only general terms were included. Polysemous and homonymous words were not disambiguated to concepts, i.e. humans annotated semantic relatedness of words rather than concepts. 3 Related work In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards. Test subjects were instructed to order the cards according to the “similarity of meaning” and then assign a continuous similarity value (0.0 - 4.0) to each card. Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. This experiment was again replicated by Resnik (1995) with 10 subjects. Table 1 summarizes previous experiments. A comprehensive evaluation of SR measures requires a higher number of word pairs. How</context>
<context position="22794" citStr="Rubenstein and Goodenough (1965)" startWordPosition="3575" endWordPosition="3578">igure 4: Distribution of averaged human judgments with standard deviation &lt; 0.8. that (car – vehicle) are highly related, a strong connection between (parts – speech) may only be established by a certain group. Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set. Therefore, inter-subject correlation is lower than the results obtained by Gurevych (2006). In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647. Rubenstein and Goodenough (1965) reported an intra-subject correlation of r=.85 for 15 subjects judging the similarity of a subset (36) of the original 65 word pairs. The values may again not be compared directly. Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low. The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts. To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chai</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual Correlates of Synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Automatic Text Processing: the Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison-Wesley Longman Publishing,</publisher>
<location>Boston, MA, USA.</location>
<contexts>
<context position="15346" citStr="Salton, 1989" startWordPosition="2397" endWordPosition="2398">d corpus is compiled from 106 arbitrarily selected scientific PowerPoint presentations (SPP). They cover a wide range of topics from bio genetics to computer science and contain many technical terms. Due to the special structure of presentations, this corpus will be particularly demanding with respect to the required preprocessing components of an information retrieval system. The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995). The resulting list of POS-tagged lemmas is weighted using the SMART ‘ltc’8 tf.idf-weighting scheme (Salton, 1989). We implemented a set of filters for word pairs. One group of filters removed unwanted word pairs. Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist. Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set. We used the following parameters: NN = 0.5, NV = 0.15, NA = 0.15, V V = 0.1, V A = 0.05, AA = 0.05. That means 50% of the resulting word pairs for each corpus 7http://berufenet.arbei</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Gerard Salton. 1989. Automatic Text Processing: the Transformation, Analysis, and Retrieval of Information by Computer. Addison-Wesley Longman Publishing, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1995</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="15231" citStr="Schmid, 1995" startWordPosition="2381" endWordPosition="2382">cts of social science papers. It is a standard corpus for evaluating German information retrieval systems. The third corpus is compiled from 106 arbitrarily selected scientific PowerPoint presentations (SPP). They cover a wide range of topics from bio genetics to computer science and contain many technical terms. Due to the special structure of presentations, this corpus will be particularly demanding with respect to the required preprocessing components of an information retrieval system. The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995). The resulting list of POS-tagged lemmas is weighted using the SMART ‘ltc’8 tf.idf-weighting scheme (Salton, 1989). We implemented a set of filters for word pairs. One group of filters removed unwanted word pairs. Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist. Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set. We used the following parameters: NN = 0.5, NV = 0.15, NA = 0.15, V</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>Helmut Schmid. 1995. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Alissa Melinger</author>
</authors>
<title>Identifying Semantic Relations and Functional Properties of Human Verb Associations.</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology and Empirical Methods in NLP,</booktitle>
<pages>612--619</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="7994" citStr="Walde and Melinger, 2005" startWordPosition="1231" endWordPosition="1234">odenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005). Results of such experiments are hard to quantify and cannot easily serve as the basis for evaluating SR measures. Rubenstein and Goodenough selected word pairs analytically to cover the whole spectrum of 17 PAPER LANGUAGE PAIRS POS REL-TYPE R/G (1965) English 65 N sim M/C (1991) English 30 N sim Res (1995) English 30 N sim Fin (2002) English 353 N, V, A relat Gur (2005) German 65 N sim Gur (2006) German 350 N, V, A relat Z/G (2006) German 328 N, V, A relat CORRELATION SCORES # SUBJECTS INTER INTRA continuous 0–4 51 - .850 continuous 0–4 38 - - continuous 0–4 10 .903 - continuous 0–10 16 - - </context>
</contexts>
<marker>Walde, Melinger, 2005</marker>
<rawString>Sabine Schulte im Walde and Alissa Melinger. 2005. Identifying Semantic Relations and Functional Properties of Human Verb Associations. In Proceedings of the Joint Conference on Human Language Technology and Empirical Methods in NLP, pages 612–619, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SIR Project</author>
</authors>
<title>Project ‘Semantic Information Retrieval’.</title>
<date>2006</date>
<note>URL http://www.cre-elearning. tu-darmstadt.de/elearning/sir/.</note>
<contexts>
<context position="14070" citStr="Project, 2006" startWordPosition="2206" endWordPosition="2207">ith a number of senses above the threshold are removed from the list. The result of the extraction process is a list of sense disambiguated, POS-tagged pairs of concepts. 6E.g. the German verb “halten” that can be translated as hold, maintain, present, sustain, etc. has 26 senses in GermaNet. 4.2 Experimental setup 4.2.1 Extraction of concept pairs We extracted word pairs from three different domain-specific corpora (see Table 2). This is motivated by the aim to enable research in information retrieval incorporating SR measures. In particular, the “Semantic Information Retrieval” project (SIR Project, 2006) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems. The BERUFEnet (BN) corpus7 consists of descriptions of 5,800 professions in Germany and therefore contains many terms specific to professional training. Evaluating semantic relatedness on a test set based on this corpus may reveal the ability of a measure to adapt to a very special domain. The GIRT (German Indexing and Retrieval Testdatabase) corpus (Kluck, 2004) is a collection of abstracts of social science papers. It is a standard corpu</context>
</contexts>
<marker>Project, 2006</marker>
<rawString>SIR Project. 2006. Project ‘Semantic Information Retrieval’. URL http://www.cre-elearning. tu-darmstadt.de/elearning/sir/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>Co-occurrence Retrieval: A Flexible Framework For Lexical Distributional Similarity.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="4055" citStr="Weeds and Weir, 2005" startWordPosition="606" endWordPosition="609">as it is the more general term that subsumes semantic similarity. 16 Proceedings of the Workshop on Linguistic Distances, pages 16–24, Sydney, July 2006. c�2006 Association for Computational Linguistics is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain </context>
</contexts>
<marker>Weeds, Weir, 2005</marker>
<rawString>Julie Weeds and David Weir. 2005. Co-occurrence Retrieval: A Flexible Framework For Lexical Distributional Similarity. Computational Linguistics, 31(4):439–475, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb Semantics and Lexical Selection.</title>
<date>1994</date>
<booktitle>In 32nd Annual Meeting of the ACL,</booktitle>
<pages>133--138</pages>
<institution>Mexico State University, Las Cruces,</institution>
<location>New</location>
<contexts>
<context position="3926" citStr="Wu and Palmer, 1994" startWordPosition="589" endWordPosition="592">vertheless the two terms are often (mis)used interchangeably. We will use semantic relatedness in the remainder of this paper, as it is the more general term that subsumes semantic similarity. 16 Proceedings of the Workshop on Linguistic Distances, pages 16–24, Sydney, July 2006. c�2006 Association for Computational Linguistics is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematica</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verb Semantics and Lexical Selection. In 32nd Annual Meeting of the ACL, pages 133–138, New Mexico State University, Las Cruces, New Mexico.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>