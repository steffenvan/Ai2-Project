<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001421">
<title confidence="0.971566">
Agreement and Disputes in Dialogue
</title>
<author confidence="0.997737">
Alex Lascarides Nicholas Asher
</author>
<affiliation confidence="0.999499">
School of Informatics, IRIT
University of Edinburgh Universit´e Paul Sabatier, Toulouse
</affiliation>
<email confidence="0.994816">
alex@inf.ed.ac.uk asher@irit.fr
</email>
<sectionHeader confidence="0.994117" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9925055">
In this paper we define agreement in terms
of shared public commitments, and implicit
agreement is conditioned on the semantics of
the relational speech acts (e.g., Narration, Ex-
planation) that each agent performs. We pro-
vide a consistent interpretation of disputes,
and updating a logical form with the current
utterance always involves extending it and not
revising it, even if the current utterance denies
earlier content.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995814666666667">
A semantic theory of dialogue should account for
what content dialogue agents agree on. This in-
cludes implicit agreement:
</bodyText>
<listItem confidence="0.998456333333333">
(1) a. A: The room went dark.
b. A: Max turned out the light.
c. B: And John drew the blinds.
</listItem>
<bodyText confidence="0.960510266666667">
Intuitively, A and B agree that the room went dark,
that Max turned out the light, and that the latter is
at least part of the reason why the former occurred.
Thus, implicatures can be agreed upon (that (1b)
is part of the cause of (1a) goes beyond composi-
tional semantics), and agreement can be implicated
(B does not repeat (1a) and (1b) nor utter OK to in-
dicate his agreement with A).
In principle, the Grounding Acts Model (GAM,
Traum (1994), Traum and Allen (1994)) supports
implicit agreement. But it demands an acceptance
act for agreement to occur, and its current rules don’t
predict such an act from (1c). Segmented Discourse
Representation Theory (SDRT, Asher and Lascarides
(2003)) errs in the opposite direction. It stipulates
</bodyText>
<page confidence="0.97852">
29
</page>
<bodyText confidence="0.998769111111111">
that lack of disagreement implicates agreement, and
so in (1) too much is agreed upon; e.g., (1c). Thus,
SDRT needs modification to deal with (1), just as
GAM needs supplementation.
Agreement can occur even in the context of cor-
rections or disputes. In (2), A asserts (2a) and B its
negation, but a consistent interpretation of (2) over-
all is a pre-requisite to explaining how A and B end
up agreeing on (2b).
</bodyText>
<listItem confidence="0.998843666666667">
(2) a. A: It’s raining.
b. B: No it’s not.
c. A: OK.
</listItem>
<bodyText confidence="0.999184782608695">
Since a correction negates content in the discourse
context, an obvious strategy for maintaining consis-
tency would be to revise the semantic representation
of the context when updating it with a correction.
But we want to avoid revision, both at the level of
model theory and at the level of composing logi-
cal form. This is for two reasons. Firstly, revision
means that there is in principle no general way of
stating what information is preserved from the pre-
vious discourse state to the current one. But if we
construct logical form in a monotonic way—in our
case, this means that the discourse structure for a
conversation at turn n is an elementary substructure
of the discourse structure at turn n + 1—then stan-
dard preservation results from model theory apply.
Secondly, monotonicity guarantees that interpreta-
tion algorithms can proceed incrementally, combin-
ing information from various sources in a nonde-
structive way (Alshawi and Crouch, 1992).
To our knowledge, there is currently no dynamic
semantics for dialogue that yields adequate interpre-
tations of corrections and implicit agreement. We
will address this gap here. In Section 2, we re-
</bodyText>
<note confidence="0.710069">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 29–36,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999820888888889">
view two existing approaches to motivate our ba-
sic strategy, which we then describe in Section 3.
We will refine SDRT so that it tracks each dialogue
participant’s public commitments. Further, while
identifying a speech act involves default reasoning,
constructing logical form will be monotonic, in the
sense that the logical form of an updated discourse
always extends that of its discourse context, rather
than revising it.
</bodyText>
<sectionHeader confidence="0.990438" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999447857142857">
We will say that a proposition p is grounded just
in case p is agreed by the dialogue agents to be
true. This follows Clark’s terminology, in particu-
lar the concept of grounding a joint action at level 4
(Clark, 1996, p388). Clark’s work focusses almost
entirely on grounding at the so-called ‘lower’ lev-
els; how agents ground an understanding of what
was said, for instance. By contrast, in order to fo-
cus on grounding at the higher level, we will assume
a highly idealised scenario where dialogue agents
understand each other perfectly, resolving ambigu-
ities in the same way. One of Clark’s main claims is
that grounding at all levels occurs only when there
is positive evidence for it, and we aim to explore in
a logically precise manner exactly what amount of
positive evidence suffices for grounding a proposi-
tion. In future work, we intend to demonstrate that
our definition of grounding can model grounding at
the lower levels too; this will involve extending the
framework to represent misunderstandings.
GAM links the speech acts performed with its ef-
fects, including effects on grounding (Traum, 1994).
Each conversational participant builds a conversa-
tional information state (or CIS). Update effects of
particular speech acts (and their preconditions) are
specified in terms of changes to (and conditions on)
the CIS. For example, Figure 1 is the update rule for
the speech act e where B asserts K to A. It updates
the common ground (G) to include an event e&apos; that
B intends A to believe K and a conditional event
e&amp;quot; that should A accept the assertion, then A would
be socially committed to B to believe K (shown via
the attitude SCCOE). The update rules form a hier-
archy, so that more specific acts inherit effects from
more general ones. The speech act in Figure 1 in-
herits that B is SCCOE-ed to A to K, for instance.
Decision trees then predict which speech acts have
been performed.
While it is possible in principle for GAM to in-
clude rules that accurately predict (1c)’s illocution-
ary effects, the rules that are actually provided only
recognise (1c) as an assertion. Consequently, its ef-
fects are under-generated: B is socially committed
to (1c), but not to (1a), (1b) or a causal relation be-
tween them. GAM needs to be supplemented with
rules for inferring that B was also implicitly accept-
ing parts of A’s contribution.
Such acceptances, we argue, should be condi-
tioned on relational speech acts. (1c) continues
</bodyText>
<listItem confidence="0.885495583333333">
(1b) as a narrative, and the narrative so formed ex-
plains (1a). These are relational speech acts (Asher
and Lascarides, 2003): they are speech acts because
continuing a narrative or explaining something are
things that people do with utterances; and they are
relational because the successful performance of the
speech act Explanation, say, is logically dependent
on the content of the utterance (or sequence of ut-
terances) that is being explained (in this case, (1a)).
Thus even though the compositional semantics of
(1c) does not entail (1b) or (1a), its illocutionary
contribution does entail them—or, perhaps more ac-
curately, entails that B is publicly committed to
them. Similarly, through using (1b) as an Explana-
tion of (1a), A is publicly committed to (1a), (1b)
and a causal relationship between them. Thus, what
is grounded amounts to the shared semantic entail-
ments of the rhetorical relations—or speech acts—
that both A and B performed. This explains why
positive evidence for grounding is necessary (Clark,
1996): both agents must perform a speech act with
appropriate semantic consequences for a proposition
to become grounded. An implicit acceptance (or ac-
knowledgement in SDRT terms) is then logically de-
pendent on the formal semantic interpretations of the
relational speech acts performed. For instance, B’s
commitments to (1a) and (1b) stem from Narration
and Explanation acts he performed in uttering (1c).
Since GAM incorporates relational speech acts,
the general principles that we propose here could
extend it. However, we have chosen to use SDRT
because it defines logical form more abstractly, al-
lowing us to exploit its model theory to determine
grounded propositions. In contrast to GAM, we will
not explicitly represent what’s grounded (and what’s
not) in logical form. Doing so would force us to in-
</listItem>
<page confidence="0.990143">
30
</page>
<figure confidence="0.30315525">
Name: Assert
Condition on update: G :: [e : Assert(B, A, K)]
Update G+= [e&apos;]e&apos; : Try(B, As&apos;.s&apos; : Bel(A, K)),
[e&apos;&apos;]e&apos;&apos; : Accept(A, e) ==&gt;- [s|s : SCCOE(A, B, K)]
</figure>
<figureCaption confidence="0.999753">
Figure 1: The update rule for assertion
</figureCaption>
<bodyText confidence="0.999699448275862">
corporate revision should grounded content get dis-
puted, as can happen in a dynamic setting, where
facts and beliefs change as the agents engage in di-
alogue. We will make grounding a property of the
interpretation of a logical form, and not part of its
form.
SDRT offers a formal semantics of relational
speech acts (Asher and Lascarides, 2003). Further-
more, in contrast to theories of discourse interpreta-
tion that equate interpreting a discourse with its ef-
fects on the agents’ beliefs (e.g., Hobbs et al. (1993),
Grosz and Sidner (1990)), SDRT separates the glue
logic (i.e., the logic for constructing a logical form
of what was said) from the logic for interpreting
the logical form (i.e., reasoning about whether what
was said is true, or should be believed). This en-
ables SDRT to maintain a decidable procedure for
computing logical form, even though identifying the
speech acts performed inherently involves common-
sense reasoning, and hence consistency tests. Asher
and Lascarides (2003, p78) argue that it must be de-
cidable to explain why, as Lewis (1969) claims, peo-
ple by and large have a common understanding of
what was said.
SDRT’s current representation of (1) is (1&apos;), where
7r1, 7r2 and 7r3 label the contents of the clauses (1a–
c) respectively, and 7r0 and 7r label the content of the
dialogue segments that are created by the rhetorical
connections:
</bodyText>
<equation confidence="0.999645">
(1&apos;) 7r0 : Explanation(7r1, 7r)
7r : Narration(7r2, 7r3)
</equation>
<bodyText confidence="0.999827818181818">
In words, (1&apos;) implies that the room went dark, and
this was caused by a combination of Max switching
off the light followed by John drawing the blinds.
In the absence of speech acts of denial such as Cor-
rection, SDRT stipulates that all content is grounded
(Asher and Lascarides, 2003, p363). This leads di-
rectly to the wrong predictions for (1).
Unlike GAM, SDRT fails to track the different
commitments of individual speakers. Simply la-
belling each speech act with its speaker doesn’t suf-
fice, as dialogue (3) shows.1
</bodyText>
<listItem confidence="0.9670415">
(3) 7r1. A: John went to Harrods.
7r2. B: He bought a suit.
7r3. A: He then entered the food halls.
7r4. B: He looked for foie gras.
</listItem>
<bodyText confidence="0.999747419354839">
Intuitively, A’s utterance 7r3 publicly commits
him not only to Narration(7r2, 7r3), but also to
Narration(7r1, 7r2) (for this latter speech act entails,
while the former does not, that John bought the suit
at Harrods). And yet B was the speaker who per-
formed the speech act Narration(7r1, 7r2), for it is
B who uttered 7r2. Accordingly, we abandon repre-
senting dialogue with a single SDRS, and replace it
with a tuple of SDRSs—one SDRS per discourse par-
ticipant per turn, representing all his commitments
up to and including that turn. We define grounding
a proposition p in terms of joint entailments from
those commitments, and hence grounding becomes
a semantic property of the logical form. This solves
SDRT’s over-generation problems with grounding.
For instance in (1), A’s public commitments are to
Explanation(7r1, 7r2). B, on the other hand, is com-
mitted to the content expressed by (1&apos;). The shared
public commitments then accurately reflect what A
and B agree on. We also avoid the under-generation
problems of GAM; grounding need not arise from
an acceptance but instead from so-called veridical
rhetorical relations (e.g., Explanation and Narra-
tion) and the logical relationships among their mean-
ings.
Grounded content is not marked as such in logical
form. This makes monotonic construction of logical
form feasible, even when grounded propositions get
disputed. A further part of our strategy for eschew-
ing revision is to assume that the SDRSs for each turn
represent all of A’s and B’s current commitments,
</bodyText>
<footnote confidence="0.854987">
1For simplicity, we use a contructed example here, although
Sacks (1992) attests many similar, naturally occurring dialogues
where the agents build a narrative together.
</footnote>
<page confidence="0.999786">
31
</page>
<bodyText confidence="0.999988541666666">
from the beginning of the dialogue to the end of that
turn. The alternative, where prior but ongoing com-
mitments from turn i −1 are not shown in the repre-
sentation of turn i, and accordingly the input context
for interpreting turn i is the output one from inter-
preting turn i − 1, would condemn us to incorporat-
ing revision into the model theory. This is because
A may commit in turn i to something that is incon-
sistent with his commitments in turn i − 1 (e.g., A’s
utterance (2c)), and without revision the output con-
text from turn i would then be L. We want to avoid
revision while maintaining consistency. Represent-
ing all current commitments in each turn avoids re-
vision in the model theory, because one can com-
pute the current commitments of A and B by dy-
namically interpreting their SDRSs for just the last
turn. One can detect how A’s commitments have
changed during the dialogue, but only by comparing
the SDRSs for the relevant turns.2
We will model disputes by adding non-truth pre-
serving operators over relevant segments in the log-
ical form. This avoids the need for downdating and
revision in both the construction and the interpreta-
tion of logical form.
</bodyText>
<sectionHeader confidence="0.997509" genericHeader="method">
3 Individuating Commitments
</sectionHeader>
<bodyText confidence="0.9759014">
The logical form for a dialogue turn proposed in
Section 2 generalises to dialogues with more than
two agents in the obvious way: the logical form of a
dialogue turn is a set {Sa : a E D}, where Sa is an
SDRS and D is the set of dialogue agents. The log-
ical form of the dialogue overall will be the logical
forms of each of its turns (and all dialogue agents
build all the SDRSs in the logical form, not just the
SDRSs representing their own commitments). We
assume an extremely simple notion of turns, where
turn boundaries occur whenever the speaker changes
(even if this happens mid-clause), and we ignore for
now cases where agents speak simultaneously.
This new logical form for dialogue requires a new
dynamic interpretation. The context Cd of evalua-
tion for interpreting a dialogue turn is a set of dy-
namic contexts for interpreting SDRSs—one for each
2Pr´evot et al. (2006) represent dialogue in terms of commit-
ment slates. Their idea inspired our work, but the details differ
considerably, particularly on monotonic construction.
</bodyText>
<equation confidence="0.8294525">
agent a E D:
Cd = {(Cia, Coa) : a E D}
</equation>
<bodyText confidence="0.9996671">
Thus Cia and Coa are world assignment pairs, given
the definitions from Asher and Lascarides (2003).
For instance, (4) defines the dynamic interpreta-
tion of veridical relations (e.g. Narration, Explana-
tion), where meaning postulates then stipulate the
illocutionary effects �PR(α,β)—e.g., for Narration
they stipulate the spatio-temporal progression of the
events (we gloss the content that’s labelled 7r as Kπ,
and m in [.]m stands for monologue). Equation (5)
defines the dynamic interpretation of Correction.
</bodyText>
<listItem confidence="0.8067465">
(4) (w,f)[R(α,Q)]m(w0,g) iff
(w, f)[Kα n Kβ n cOR(α,β)]m(w0, g)
(5) (w, f)[Correction(α, Q)]m(w0, g) iff
(w, f)[(-Kα) n Kβ n (pC.,,(α,β)]m(w0, g)
</listItem>
<bodyText confidence="0.978502333333333">
The context change potential (CCP) of a dialogue
turn T = {Sa : a E D} is the product of the CCPs
of the individual SDRSs:
</bodyText>
<equation confidence="0.977476">
Cd[T]dC0d iff C0d = {(Cia, Cao) o [Sa]m :
(Cia, Coa) E Cd, a E D}
</equation>
<bodyText confidence="0.974888545454545">
Accordingly, dialogue entailments can be defined in
terms of the entailment relation �=m for SDRSs af-
forded by [.]m:
T �=d 0 iff ba E D, Sa �m 0
This makes �=d the shared entailment of each agent’s
public commitments. And we assume that content 0
is grounded or agreed upon by a dialogue turn T iff
T �=d 0. Finally, given that the SDRSs for a dialogue
turn reflect all an agent’s current commitments, the
interpretation of the dialogue overall is the CCP of
its last turn.
The logical form of (3) is shown in Table 1 (we
have omitted the logical forms of the clauses, la-
belled 7r1 to 7r4). The semantics of the SDRSs for
the last turn correctly predict the following proposi-
tion to be grounded (for it is entailed by them): John
went to Harrods, followed by buying a suit (at Har-
rods), followed by his entering the food halls.
There is a sharing of labels across the SDRSs in
Table 1. This general feature reflects the reality
that one speaker may perform a relational speech act
whose first argument is part of someone else’s turn,
</bodyText>
<page confidence="0.976815">
32
</page>
<table confidence="0.999819833333333">
Turn A’s SDRS B’s SDRS
1 7r1 0
2 7r1 7r2B : Narration(7r1, 7r2)
3 7r3A : Narration(7r1, 7r2) n Narration(7r2, 7r3) 7r2B : Narration(7r1, 7r2)
4 7r3A : Narration(7r1, 7r2) n Narration(7r2, 7r3) 7r4B : Narration(7r1, 7r2) n Narration(7r2, 7r3)n
Narration(7r3, 7r4)
</table>
<tableCaption confidence="0.999962">
Table 1: The logical form of dialogue (3).
</tableCaption>
<bodyText confidence="0.99914771875">
or part of his own previous turns. Sharing labels cap-
tures the intuition that an agent’s speech acts can re-
veal his commitments (or lack of them) to contextual
content, even if this is linguistically implicit.
Including prior but ongoing commitments in the
SDRS for the current turn has consequences for the
general architecture of the theory: we must stipu-
late what commitments persist across turns when
constructing the SDRSs. Consider the fourth turn
of dialogue (3). Intuitively, uttering 7r4 commits
B to the illocutionary content of Narration(7r3, 7r4).
But in addition, he is also committed at this point
to Narration(7r1, 7r2) n Narration(7r2, 7r3), as shown.
Those commitments persist from prior turns; they
are even transferred from one speaker to another.
However, we will shortly examine other examples,
involving corrections and even explicit acknowl-
edgements (or an acceptance in Traum’s (1994) ter-
minology), where the commitments do not persist.
To handle the data, we must make the ‘commitment
persistence’ principle sensitive to distinct relational
speech acts, and it must support a monotonic con-
struction of logical form.
To motivate our persistence principle, consider
how A and B get to the commitments shown in
Table 1. A’s SDRS for the first turn is 7r1 : K,,,
where K,, stands for the representation of John
went to Harrods. Since B hasn’t said anything yet,
his SDRS for the first turn is 0. SDRT’s glue logic
uses default axioms to predict the relation that con-
nects 7r2 to 7r1 (Asher and Lascarides, 2003); here,
these defaults should yield that B is committed to
7r2B : Narration(7r1, 7r2) (we adopt the convention
that the root label of the speaker d’s SDRS for turn j
is named 7rid). A’s SDRS for the second turn is the
same as the first turn: he hasn’t spoken since, and so
his commitments are unchanged.
In the third turn, the glue logic should predict that
A’s utterance 7r3 forms a narrative with 7r2. But sim-
ply adding this to A’s prior SDRS isn’t sufficient.
First, the result is not a well-formed SDRS, because it
won’t contain a single root label. Secondly, it misses
an important interplay between discourse structure
and grounding: adding only Narration(7r2, 7r3) to
A’s existing commitment to K,, makes A commit-
ted to the compositional semantics of 7r2, but not
to its illocutionary contribution conveyed by B (e.g.
that John bought the suit at Harrods). And yet intu-
itively, uttering 7r3 implicates that this (linguistically
implicit) content is agreed on.
Dialogues (1) and (3) feature discourse relations
that occur in monologue as well. Several agents can
use these to build up a narrative together, as noted by
Sacks (1992). Sacks’ observations affirm that such
discourse relations can be used to perform ‘implicit’
acknowledgements, and what’s more they suggest
that the implicit acknowledgement is not only of
the prior contribution’s compositional semantics but
also its illocutionary effects. These observations
lead us to add the following Persistence princi-
ple to the glue logic, together with axioms that iden-
tify undenied commitments (UC(α) stands for the
undenied commitments of the utterance or segment
α):
</bodyText>
<listItem confidence="0.995416">
• Persistence:
</listItem>
<equation confidence="0.401451">
A: R(α, Q) —* A: UC(α)
</equation>
<bodyText confidence="0.998847">
Different glue-logic axioms will then identify the
undenied commitments for different speech acts.
The present case concerns simple left veridical (slv)
relations—those that do not explicitly endorse or
criticise any previous commitments. Note 0 &gt; 0
means “If 0 then normally 0”, and T (d, j, 7r) means
that label 7r is a part of agent d’s SDRS for turn j:
</bodyText>
<listItem confidence="0.973493">
• Undenied Commitments:
</listItem>
<equation confidence="0.999502666666667">
(A : R(α, Q) n T(d1, j, A) n slv(R)n
A&apos; : R&apos;(-y, α) n T (d2, j − 1, A&apos;)) &gt;
(A : UC(α) —* A : R&apos;(-y, α))
</equation>
<page confidence="0.984679">
33
</page>
<bodyText confidence="0.9989205">
Undenied Commitments states that if d1 com-
mits to R(α, Q) where R is simple left veridical and
d2 is already committed to R&apos;(-y, α), then normally
the undenied commitments of α include R&apos;(-y, α).
Examples of simple left veridical relations include
Narration and Explanation but not Acknowledge-
ment (since this explicitly endorses prior content) or
Correction (since this denies prior content).
</bodyText>
<subsectionHeader confidence="0.756356">
Persistence and Undenied
</subsectionHeader>
<bodyText confidence="0.998304391304348">
Commitments predict that A’s SDRS for the third
turn of (3) includes 7r3A : Narration(7r1, 7r2). This is
because default rules yield 7r3A : Narration(7r2, 7r3),
and Narration(7r1, 7r2) is in B’s SDRS.
Persistence and Undenied Commitments
likewise predict that Narration(7r1, 7r2) and
Narration(7r2, 7r3) are a part of B’s SDRS for the
fourth turn, as shown in Table 1.
Undenied Commitments is defeasible. This
is because if the illocutionary contribution of A’s
(left-veridical) speech act R(α, Q) conflicts with
some proposition p that B conveyed by uttering
α, then clearly A’s speech act should not be con-
strued as an implicit acknowledgement of p. This
affects the analysis of (1), whose logical form is
Table 2. B’s SDRS after the second turn does
not include Explanation(7r1,7r2), even though his
utterance 7r3 attaches with the veridical relation
Narration to 7r2, and A’s SDRS for turn 1 in-
cludes Explanation(7r1,7r2). Persistence ap-
plies to this example (for label 7r2) and the an-
tecedent to Undenied Commitments is sat-
isfied, but Explanation(7r1, 7r2) is not an unde-
nied commitment of 7r2 because its (nonmono-
tonic) semantic consequences conflict with those of
Explanation(7r1, 7r), a speech act that the glue logic
must identify as one that B intended to perform (or,
in other words, publicly commit to) as a byproduct
of uttering 7r3. Explanation(7r1,7r2) conflicts with
Explanation(7r1, 7r) because the former nonmono-
tonically entails, via a scalar implicature, that Max
turning out the light was the sole cause of the room
going dark, while the latter (monotonically) entails
it was a strict part of it. This example illustrates how
the default logic rendered by &gt; must be specified in
terms of the consistency in what follows nonmono-
tonically, rather than what follows monotonically.
Undenied Commitments does not apply
for the veridical relation Acknowledgement; i.e.,
utterances of the form OK, I agree, repeat-
ing prior content, and the like. In words,
Acknowledgement(7r1, 7r2) entails K,,, K,, and
that K,, implies K,,; to use the GAM term, it is an
act of explicit acceptance. Dialogue (6) illustrates
why Acknowledgement behaves differently from the
simple left veridical relations like Narration:
</bodyText>
<listItem confidence="0.985448333333333">
(6) 7r1. B: John is not a good speaker
7r2. B: because he’s hard to understand.
7r3. A: I agree he’s hard to understand.
</listItem>
<bodyText confidence="0.999185540540541">
The compositional semantics of 7r3 makes A
explicit about what in B’s turn he acknowl-
edges: A must be committed to (at least)
Acknowledgement(7r2, 7r3). What is outside the
scope of the acknowledgement—namely, B’s pu-
tative explanation for why John is not a good
speaker—is not denied in (6). It would be consistent
to add Explanation(7r1, 7r2) to A’s commitments, but
it’s simply not warranted. Dialogue (6) shows that
when the explicit endorsement conveys sufficiently
specific content, it appears to carry a scalar impli-
cature that this precise content is endorsed, and no
more.
Another reason for excluding explicit acknowl-
edgements from the set of simple left veridical rela-
tions is that such speech acts come with their own
grounding requirements. Acknowledgements can
have scope over implicatures as well as composi-
tional semantic contents, since the first argument
to an Acknowledgement relation can be a label of
an arbitrarily complex SDRS. So by acknowledg-
ing 7rj, we do not thereby acknowledge the impli-
catures of 7rj itself; had we wished to do so, we
would have included them within the scope of the
acknowledgement. That is, we would infer the re-
lation Acknowledgement(7r&apos;j, 7rz), where 7r&apos;j has se-
mantic scope over 7rj, making 7rj and the rhetori-
cal relations it engages in part of what is (explic-
itly) endorsed. It is because the discourse function
of an acknowledgement is precisely to say what one
agent commits to from another agent’s turn—i.e.,
what are the undenied commitments in this case—
that Persistence applies redundantly.
Explicit acknowledgements have been studied
by Traum and Hinkelman (1992), among others.
Here, we will ignore interpretations of an utter-
ance 7r2 (e.g., OK) as an acknowledgement that K,,
</bodyText>
<page confidence="0.995344">
34
</page>
<table confidence="0.99968775">
Turn A’s SDRS B’s SDRS
1 π1A : Explanation(π1, π2) 0
2 π1A : Explanation(π1, π2) π2B : Explanation(π1, π)
π : Narration(π2, π3)
</table>
<tableCaption confidence="0.999804">
Table 2: The logical form of (1).
</tableCaption>
<bodyText confidence="0.9989032">
was said (represented in SDRT with the so-called
metatalk relation Acknowledgement*(π1,π2)), in-
stead focussing entirely on an interpretation of π2
using Acknowledgement (i.e., a commitment to K,,,
which in turn entails a commitment that K,, was
said). But even so there is ambiguity, because lin-
guistic form does not always fully determine what
the acknowledgement has scope over. Let’s assume
that A’s utterance π3 in (7) is an acknowledgement
of content and not just of understanding that content:
</bodyText>
<reference confidence="0.56454975">
(7) π1. B: John is not a good speaker
π2. B: because he is hard to understand.
π3. A: OK.
Acknowledgement(π2, π3) entails K,,. Making π2
</reference>
<bodyText confidence="0.999983838709678">
the only label that’s acknowledged leads to an inter-
pretation where the proposition that π2 explains π1
is not acknowledged. This ‘narrow scope’ attach-
ment permits A to continue by challenging the ex-
planatory link, e.g., by uttering but that’s not why
he’s not a good speaker. Another interpretation
of (7) is that A commits to all of B’s commit-
ments, including the implicatures: this is expressed
by adding Acknowledgement(π1B, π3) to A’s SDRS,
where π1B : Explanation(π1,π2). Indeed, if OK
is all that A says, then one defaults to this wide-
scope interpretation. Even if A follows OK with
He IS hard to understand with high pitch accents
and a falling boundary tone, the preferred interpre-
tation contrasts with (6), to be one where OK is an
Acknowledgement of π1B, and He’s hard to under-
stand is an explanation of that acknowledgement act
(marked with the metatalk relation Explanation* in
SDRT). It is straightforward to add glue-logic ax-
ioms for constructing logical form that reflect these
principles for identifying the first argument of Ac-
knowledgement.
In dialogue (2), A commits to the negation of
his prior commitment. As before, constructing B’s
SDRS for the second turn involves using the glue
logic to identify how π2 connects to π1. So long
as their semantic incompatibility is transferred, in
shallow form, to the glue logic, then the general
principle that the necessary semantic consequences
of a speech act are normally sufficient for inferring
that it was performed will apply, yielding π2B :
Correction(π1,π2) (see Table 3). The cue phrase
OK is then used by the glue logic to infer π3A :
Acknowledgement(π2, π3). This resolves the under-
specified content OK to K,,; and thus as before the
glue logic also yields π3A : Correction(π1, π3), as
shown. It’s not raining is entailed by the SDRSs
for turn 3. The interpretation of each turn is con-
sistent (i.e., the output state is non-empty), although
the SDRSs for turn 2 are mutually inconsistent (A’s
SDRS entails that it’s raining and B’s entails it’s not).
Finally, the content associated with each label does
not change from one turn to the next, making the
construction of logical form monotonic.
Clark (1996) doesn’t make precise exactly what
counts as sufficient positive evidence for grounding.
Similarly, Traum and Allen (1994) don’t provide
rules for inferring when a speaker has performed
an implicit acceptance. Our framework makes
the quantity of positive evidence that’s needed for
grounding propositions logically precise, in terms
of the relational speech acts that both speakers
perform, and the logical relationships between the
semantics of those speech acts. Persistence
and Undenied Commitments capture a gen-
eral class of examples involving implicit agreement.
Sufficient positive evidence for grounding a propo-
sition through explicit endorsements and challenges
rests on the formal semantic interpretation of the rel-
evant speech acts—namely Acknowledgement and
Correction—and the rules by which one determines
the first argument of these relations.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999212333333333">
We have presented a novel treatment of agreements
and disputes in which the construction of logi-
cal form is monotonic in the subsumptive sense
</bodyText>
<page confidence="0.996384">
35
</page>
<table confidence="0.99140325">
Turn A’s SDRS B’s SDRS
1 7r1 : K,, 0
2 7r1 : K,, 7r2B : Correction(7r1, 7r2)
3 7r3A : Correction(7r1, 7r3) n Acknowledgement(7r2, 7r3) 7r2B : Correction(7r1, 7r2)
</table>
<tableCaption confidence="0.999933">
Table 3: The logical form of dialogue (2).
</tableCaption>
<bodyText confidence="0.99993512195122">
(Shieber, 1986); the semantic representation of the
discourse context is an elementary substructure of
the representation of the dialogue updated with the
current utterance, even if the current utterance de-
nies earlier content. However, the logical form re-
mains a product of complex default reasoning, since
identifying the speech acts that were performed in-
volves commonsense reasoning with the linguistic
and non-linguistic context.
The relationship between the grounded proposi-
tions and the interpretation of the dialogue is entirely
transparent and is defined in terms of the model the-
ory of the logical forms. It provides a logical basis
for exploring Clark’s (1996) notion of positive evi-
dence for grounding. A crucial ingredient in our ac-
count was the use of relational speech acts, and the
logical relationships among their semantics.
We believe our definition of grounding as
shared commitment is capable of modelling Clark’s
more central concern—grounding the understand-
ing of what was said. The left-veridical rela-
tions that are the hallmark of grounding at level
4 entail grounding at the lower levels thanks to
the semantics of DSDRSs. Moreover, SDRT’s
metatalk relations—such as Explanation*(α, Q) and
Acknowledgement*(α, Q)—commit an agent to the
fact that Kα was said without committing him Kα.
Thus shared commitments that follow from a repre-
sentation of the dialogue can ground acts at lower
levels without grounding (or denying) acts at level
4. A full model of grounding at lower levels, how-
ever, requires us to extend the framework to handle
misunderstandings.
This paper presents just some first steps towards a
dynamic theory of grounding. For instance, we have
not yet modelled the impact of questions and imper-
atives on public commitments and grounding. We
have started to explore links between public com-
mitments and other attitudes, such as beliefs, prefer-
ences, and intentions (Asher and Lascarides, 2008),
but this also remains a matter of ongoing research.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889588235294">
H. Alshawi and R. Crouch. Monotonic semantic in-
terpretation. In Proceedings ofACL, pages 32–39,
1992.
N. Asher and A. Lascarides. Logics of Conversation.
CUP, 2003.
N. Asher and A. Lascarides. Commitments, beliefs
and intentions in dialogue. In Proceedings of Lon-
dial, 2008.
H. H. Clark. Using Language. CUP, 1996.
B. Grosz and C. Sidner. Plans for discourse. In
J. Morgan P. R. Cohen and M. Pollack, editors, In-
tentions in Communication, pages 365–388. MIT
Press, 1990.
J. R. Hobbs, M. Stickel, D. Appelt, and P. Martin.
Interpretation as abduction. Artificial Intelligence,
63(1–2):69–142, 1993.
D. Lewis. Convention: A Philosophical Study. Har-
vard University Press, 1969.
L. Pr´evot, N. Maudet, and P. Muller. Conversational
game-board and discourse structure. In Proceed-
ings of Constraints in Discourse, Ireland, 2006.
H. Sacks. Lectures on Conversation. Blackwells,
1992.
S. Shieber. An Introduction to Unification-basedAp-
proaches to Grammar. CSLI Publications, 1986.
D. Traum. A Computational Theory of Grounding
in Natural Language Conversation. PhD thesis,
University of Rochester, 1994.
D. Traum and J. Allen. Discourse obligations in di-
alogue processing. In Proceedings ofACL, pages
1–8, 1994.
D. Traum and E. Hinkelman. Conversation acts in
task-oriented spoken dialogue. Computational In-
telligence, 8(3):575–599, 1992.
</reference>
<page confidence="0.998942">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.860146">
<title confidence="0.99971">Agreement and Disputes in Dialogue</title>
<author confidence="0.999958">Alex Lascarides Nicholas Asher</author>
<affiliation confidence="0.9916935">School of Informatics, IRIT University of Edinburgh Universit´e Paul Sabatier, Toulouse</affiliation>
<email confidence="0.963388">alex@inf.ed.ac.ukasher@irit.fr</email>
<abstract confidence="0.991552272727273">In this paper we define agreement in terms of shared public commitments, and implicit agreement is conditioned on the semantics of relational speech acts (e.g., Exthat each agent performs. We provide a consistent interpretation of disputes, and updating a logical form with the current utterance always involves extending it and not revising it, even if the current utterance denies earlier content.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>π1</author>
</authors>
<title>B: John is not a good speaker π2. B: because he is hard to understand.</title>
<booktitle>π3. A: OK. Acknowledgement(π2, π3) entails K,,. Making π2</booktitle>
<marker>π1, </marker>
<rawString>(7) π1. B: John is not a good speaker π2. B: because he is hard to understand. π3. A: OK. Acknowledgement(π2, π3) entails K,,. Making π2</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>R Crouch</author>
</authors>
<title>Monotonic semantic interpretation.</title>
<date>1992</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>32--39</pages>
<contexts>
<context position="3010" citStr="Alshawi and Crouch, 1992" startWordPosition="496" endWordPosition="499"> two reasons. Firstly, revision means that there is in principle no general way of stating what information is preserved from the previous discourse state to the current one. But if we construct logical form in a monotonic way—in our case, this means that the discourse structure for a conversation at turn n is an elementary substructure of the discourse structure at turn n + 1—then standard preservation results from model theory apply. Secondly, monotonicity guarantees that interpretation algorithms can proceed incrementally, combining information from various sources in a nondestructive way (Alshawi and Crouch, 1992). To our knowledge, there is currently no dynamic semantics for dialogue that yields adequate interpretations of corrections and implicit agreement. We will address this gap here. In Section 2, we reProceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 29–36, Columbus, June 2008. c�2008 Association for Computational Linguistics view two existing approaches to motivate our basic strategy, which we then describe in Section 3. We will refine SDRT so that it tracks each dialogue participant’s public commitments. Further, while identifying a speech act involves default reasoning,</context>
</contexts>
<marker>Alshawi, Crouch, 1992</marker>
<rawString>H. Alshawi and R. Crouch. Monotonic semantic interpretation. In Proceedings ofACL, pages 32–39, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Asher</author>
<author>A Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>CUP,</publisher>
<contexts>
<context position="1544" citStr="Asher and Lascarides (2003)" startWordPosition="246" endWordPosition="249">ed out the light, and that the latter is at least part of the reason why the former occurred. Thus, implicatures can be agreed upon (that (1b) is part of the cause of (1a) goes beyond compositional semantics), and agreement can be implicated (B does not repeat (1a) and (1b) nor utter OK to indicate his agreement with A). In principle, the Grounding Acts Model (GAM, Traum (1994), Traum and Allen (1994)) supports implicit agreement. But it demands an acceptance act for agreement to occur, and its current rules don’t predict such an act from (1c). Segmented Discourse Representation Theory (SDRT, Asher and Lascarides (2003)) errs in the opposite direction. It stipulates 29 that lack of disagreement implicates agreement, and so in (1) too much is agreed upon; e.g., (1c). Thus, SDRT needs modification to deal with (1), just as GAM needs supplementation. Agreement can occur even in the context of corrections or disputes. In (2), A asserts (2a) and B its negation, but a consistent interpretation of (2) overall is a pre-requisite to explaining how A and B end up agreeing on (2b). (2) a. A: It’s raining. b. B: No it’s not. c. A: OK. Since a correction negates content in the discourse context, an obvious strategy for m</context>
<context position="6357" citStr="Asher and Lascarides, 2003" startWordPosition="1056" endWordPosition="1059">e for GAM to include rules that accurately predict (1c)’s illocutionary effects, the rules that are actually provided only recognise (1c) as an assertion. Consequently, its effects are under-generated: B is socially committed to (1c), but not to (1a), (1b) or a causal relation between them. GAM needs to be supplemented with rules for inferring that B was also implicitly accepting parts of A’s contribution. Such acceptances, we argue, should be conditioned on relational speech acts. (1c) continues (1b) as a narrative, and the narrative so formed explains (1a). These are relational speech acts (Asher and Lascarides, 2003): they are speech acts because continuing a narrative or explaining something are things that people do with utterances; and they are relational because the successful performance of the speech act Explanation, say, is logically dependent on the content of the utterance (or sequence of utterances) that is being explained (in this case, (1a)). Thus even though the compositional semantics of (1c) does not entail (1b) or (1a), its illocutionary contribution does entail them—or, perhaps more accurately, entails that B is publicly committed to them. Similarly, through using (1b) as an Explanation o</context>
<context position="8609" citStr="Asher and Lascarides, 2003" startWordPosition="1424" endWordPosition="1427">what’s grounded (and what’s not) in logical form. Doing so would force us to in30 Name: Assert Condition on update: G :: [e : Assert(B, A, K)] Update G+= [e&apos;]e&apos; : Try(B, As&apos;.s&apos; : Bel(A, K)), [e&apos;&apos;]e&apos;&apos; : Accept(A, e) ==&gt;- [s|s : SCCOE(A, B, K)] Figure 1: The update rule for assertion corporate revision should grounded content get disputed, as can happen in a dynamic setting, where facts and beliefs change as the agents engage in dialogue. We will make grounding a property of the interpretation of a logical form, and not part of its form. SDRT offers a formal semantics of relational speech acts (Asher and Lascarides, 2003). Furthermore, in contrast to theories of discourse interpretation that equate interpreting a discourse with its effects on the agents’ beliefs (e.g., Hobbs et al. (1993), Grosz and Sidner (1990)), SDRT separates the glue logic (i.e., the logic for constructing a logical form of what was said) from the logic for interpreting the logical form (i.e., reasoning about whether what was said is true, or should be believed). This enables SDRT to maintain a decidable procedure for computing logical form, even though identifying the speech acts performed inherently involves commonsense reasoning, and h</context>
<context position="9977" citStr="Asher and Lascarides, 2003" startWordPosition="1654" endWordPosition="1657"> large have a common understanding of what was said. SDRT’s current representation of (1) is (1&apos;), where 7r1, 7r2 and 7r3 label the contents of the clauses (1a– c) respectively, and 7r0 and 7r label the content of the dialogue segments that are created by the rhetorical connections: (1&apos;) 7r0 : Explanation(7r1, 7r) 7r : Narration(7r2, 7r3) In words, (1&apos;) implies that the room went dark, and this was caused by a combination of Max switching off the light followed by John drawing the blinds. In the absence of speech acts of denial such as Correction, SDRT stipulates that all content is grounded (Asher and Lascarides, 2003, p363). This leads directly to the wrong predictions for (1). Unlike GAM, SDRT fails to track the different commitments of individual speakers. Simply labelling each speech act with its speaker doesn’t suffice, as dialogue (3) shows.1 (3) 7r1. A: John went to Harrods. 7r2. B: He bought a suit. 7r3. A: He then entered the food halls. 7r4. B: He looked for foie gras. Intuitively, A’s utterance 7r3 publicly commits him not only to Narration(7r2, 7r3), but also to Narration(7r1, 7r2) (for this latter speech act entails, while the former does not, that John bought the suit at Harrods). And yet B w</context>
<context position="14415" citStr="Asher and Lascarides (2003)" startWordPosition="2414" endWordPosition="2417">r changes (even if this happens mid-clause), and we ignore for now cases where agents speak simultaneously. This new logical form for dialogue requires a new dynamic interpretation. The context Cd of evaluation for interpreting a dialogue turn is a set of dynamic contexts for interpreting SDRSs—one for each 2Pr´evot et al. (2006) represent dialogue in terms of commitment slates. Their idea inspired our work, but the details differ considerably, particularly on monotonic construction. agent a E D: Cd = {(Cia, Coa) : a E D} Thus Cia and Coa are world assignment pairs, given the definitions from Asher and Lascarides (2003). For instance, (4) defines the dynamic interpretation of veridical relations (e.g. Narration, Explanation), where meaning postulates then stipulate the illocutionary effects �PR(α,β)—e.g., for Narration they stipulate the spatio-temporal progression of the events (we gloss the content that’s labelled 7r as Kπ, and m in [.]m stands for monologue). Equation (5) defines the dynamic interpretation of Correction. (4) (w,f)[R(α,Q)]m(w0,g) iff (w, f)[Kα n Kβ n cOR(α,β)]m(w0, g) (5) (w, f)[Correction(α, Q)]m(w0, g) iff (w, f)[(-Kα) n Kβ n (pC.,,(α,β)]m(w0, g) The context change potential (CCP) of a d</context>
<context position="18038" citStr="Asher and Lascarides, 2003" startWordPosition="3031" endWordPosition="3034">94) terminology), where the commitments do not persist. To handle the data, we must make the ‘commitment persistence’ principle sensitive to distinct relational speech acts, and it must support a monotonic construction of logical form. To motivate our persistence principle, consider how A and B get to the commitments shown in Table 1. A’s SDRS for the first turn is 7r1 : K,,, where K,, stands for the representation of John went to Harrods. Since B hasn’t said anything yet, his SDRS for the first turn is 0. SDRT’s glue logic uses default axioms to predict the relation that connects 7r2 to 7r1 (Asher and Lascarides, 2003); here, these defaults should yield that B is committed to 7r2B : Narration(7r1, 7r2) (we adopt the convention that the root label of the speaker d’s SDRS for turn j is named 7rid). A’s SDRS for the second turn is the same as the first turn: he hasn’t spoken since, and so his commitments are unchanged. In the third turn, the glue logic should predict that A’s utterance 7r3 forms a narrative with 7r2. But simply adding this to A’s prior SDRS isn’t sufficient. First, the result is not a well-formed SDRS, because it won’t contain a single root label. Secondly, it misses an important interplay bet</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>N. Asher and A. Lascarides. Logics of Conversation. CUP, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Asher</author>
<author>A Lascarides</author>
</authors>
<title>Commitments, beliefs and intentions in dialogue.</title>
<date>2008</date>
<booktitle>In Proceedings of Londial,</booktitle>
<publisher>CUP,</publisher>
<marker>Asher, Lascarides, 2008</marker>
<rawString>N. Asher and A. Lascarides. Commitments, beliefs and intentions in dialogue. In Proceedings of Londial, 2008. H. H. Clark. Using Language. CUP, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C Sidner</author>
</authors>
<title>Plans for discourse.</title>
<date>1990</date>
<booktitle>Intentions in Communication,</booktitle>
<pages>365--388</pages>
<editor>In J. Morgan P. R. Cohen and M. Pollack, editors,</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="8804" citStr="Grosz and Sidner (1990)" startWordPosition="1456" endWordPosition="1459"> : Accept(A, e) ==&gt;- [s|s : SCCOE(A, B, K)] Figure 1: The update rule for assertion corporate revision should grounded content get disputed, as can happen in a dynamic setting, where facts and beliefs change as the agents engage in dialogue. We will make grounding a property of the interpretation of a logical form, and not part of its form. SDRT offers a formal semantics of relational speech acts (Asher and Lascarides, 2003). Furthermore, in contrast to theories of discourse interpretation that equate interpreting a discourse with its effects on the agents’ beliefs (e.g., Hobbs et al. (1993), Grosz and Sidner (1990)), SDRT separates the glue logic (i.e., the logic for constructing a logical form of what was said) from the logic for interpreting the logical form (i.e., reasoning about whether what was said is true, or should be believed). This enables SDRT to maintain a decidable procedure for computing logical form, even though identifying the speech acts performed inherently involves commonsense reasoning, and hence consistency tests. Asher and Lascarides (2003, p78) argue that it must be decidable to explain why, as Lewis (1969) claims, people by and large have a common understanding of what was said. </context>
</contexts>
<marker>Grosz, Sidner, 1990</marker>
<rawString>B. Grosz and C. Sidner. Plans for discourse. In J. Morgan P. R. Cohen and M. Pollack, editors, Intentions in Communication, pages 365–388. MIT Press, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
<author>M Stickel</author>
<author>D Appelt</author>
<author>P Martin</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<volume>63</volume>
<issue>1</issue>
<contexts>
<context position="8779" citStr="Hobbs et al. (1993)" startWordPosition="1452" endWordPosition="1455"> Bel(A, K)), [e&apos;&apos;]e&apos;&apos; : Accept(A, e) ==&gt;- [s|s : SCCOE(A, B, K)] Figure 1: The update rule for assertion corporate revision should grounded content get disputed, as can happen in a dynamic setting, where facts and beliefs change as the agents engage in dialogue. We will make grounding a property of the interpretation of a logical form, and not part of its form. SDRT offers a formal semantics of relational speech acts (Asher and Lascarides, 2003). Furthermore, in contrast to theories of discourse interpretation that equate interpreting a discourse with its effects on the agents’ beliefs (e.g., Hobbs et al. (1993), Grosz and Sidner (1990)), SDRT separates the glue logic (i.e., the logic for constructing a logical form of what was said) from the logic for interpreting the logical form (i.e., reasoning about whether what was said is true, or should be believed). This enables SDRT to maintain a decidable procedure for computing logical form, even though identifying the speech acts performed inherently involves commonsense reasoning, and hence consistency tests. Asher and Lascarides (2003, p78) argue that it must be decidable to explain why, as Lewis (1969) claims, people by and large have a common underst</context>
</contexts>
<marker>Hobbs, Stickel, Appelt, Martin, 1993</marker>
<rawString>J. R. Hobbs, M. Stickel, D. Appelt, and P. Martin. Interpretation as abduction. Artificial Intelligence, 63(1–2):69–142, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>Convention: A Philosophical Study.</title>
<date>1969</date>
<publisher>Harvard University Press,</publisher>
<contexts>
<context position="9329" citStr="Lewis (1969)" startWordPosition="1543" endWordPosition="1544"> its effects on the agents’ beliefs (e.g., Hobbs et al. (1993), Grosz and Sidner (1990)), SDRT separates the glue logic (i.e., the logic for constructing a logical form of what was said) from the logic for interpreting the logical form (i.e., reasoning about whether what was said is true, or should be believed). This enables SDRT to maintain a decidable procedure for computing logical form, even though identifying the speech acts performed inherently involves commonsense reasoning, and hence consistency tests. Asher and Lascarides (2003, p78) argue that it must be decidable to explain why, as Lewis (1969) claims, people by and large have a common understanding of what was said. SDRT’s current representation of (1) is (1&apos;), where 7r1, 7r2 and 7r3 label the contents of the clauses (1a– c) respectively, and 7r0 and 7r label the content of the dialogue segments that are created by the rhetorical connections: (1&apos;) 7r0 : Explanation(7r1, 7r) 7r : Narration(7r2, 7r3) In words, (1&apos;) implies that the room went dark, and this was caused by a combination of Max switching off the light followed by John drawing the blinds. In the absence of speech acts of denial such as Correction, SDRT stipulates that all</context>
</contexts>
<marker>Lewis, 1969</marker>
<rawString>D. Lewis. Convention: A Philosophical Study. Harvard University Press, 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pr´evot</author>
<author>N Maudet</author>
<author>P Muller</author>
</authors>
<title>Conversational game-board and discourse structure.</title>
<date>2006</date>
<booktitle>In Proceedings of Constraints in Discourse,</booktitle>
<marker>Pr´evot, Maudet, Muller, 2006</marker>
<rawString>L. Pr´evot, N. Maudet, and P. Muller. Conversational game-board and discourse structure. In Proceedings of Constraints in Discourse, Ireland, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sacks</author>
</authors>
<title>Lectures on Conversation.</title>
<date>1992</date>
<publisher>Blackwells,</publisher>
<contexts>
<context position="11942" citStr="Sacks (1992)" startWordPosition="1981" endWordPosition="1982">avoid the under-generation problems of GAM; grounding need not arise from an acceptance but instead from so-called veridical rhetorical relations (e.g., Explanation and Narration) and the logical relationships among their meanings. Grounded content is not marked as such in logical form. This makes monotonic construction of logical form feasible, even when grounded propositions get disputed. A further part of our strategy for eschewing revision is to assume that the SDRSs for each turn represent all of A’s and B’s current commitments, 1For simplicity, we use a contructed example here, although Sacks (1992) attests many similar, naturally occurring dialogues where the agents build a narrative together. 31 from the beginning of the dialogue to the end of that turn. The alternative, where prior but ongoing commitments from turn i −1 are not shown in the representation of turn i, and accordingly the input context for interpreting turn i is the output one from interpreting turn i − 1, would condemn us to incorporating revision into the model theory. This is because A may commit in turn i to something that is inconsistent with his commitments in turn i − 1 (e.g., A’s utterance (2c)), and without revi</context>
<context position="19175" citStr="Sacks (1992)" startWordPosition="3226" endWordPosition="3227">contain a single root label. Secondly, it misses an important interplay between discourse structure and grounding: adding only Narration(7r2, 7r3) to A’s existing commitment to K,, makes A committed to the compositional semantics of 7r2, but not to its illocutionary contribution conveyed by B (e.g. that John bought the suit at Harrods). And yet intuitively, uttering 7r3 implicates that this (linguistically implicit) content is agreed on. Dialogues (1) and (3) feature discourse relations that occur in monologue as well. Several agents can use these to build up a narrative together, as noted by Sacks (1992). Sacks’ observations affirm that such discourse relations can be used to perform ‘implicit’ acknowledgements, and what’s more they suggest that the implicit acknowledgement is not only of the prior contribution’s compositional semantics but also its illocutionary effects. These observations lead us to add the following Persistence principle to the glue logic, together with axioms that identify undenied commitments (UC(α) stands for the undenied commitments of the utterance or segment α): • Persistence: A: R(α, Q) —* A: UC(α) Different glue-logic axioms will then identify the undenied commitme</context>
</contexts>
<marker>Sacks, 1992</marker>
<rawString>H. Sacks. Lectures on Conversation. Blackwells, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>An Introduction to Unification-basedApproaches to Grammar.</title>
<date>1986</date>
<publisher>CSLI Publications,</publisher>
<marker>Shieber, 1986</marker>
<rawString>S. Shieber. An Introduction to Unification-basedApproaches to Grammar. CSLI Publications, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
</authors>
<title>A Computational Theory of Grounding in Natural Language Conversation.</title>
<date>1994</date>
<tech>PhD thesis,</tech>
<institution>University of Rochester,</institution>
<contexts>
<context position="1297" citStr="Traum (1994)" startWordPosition="211" endWordPosition="212">what content dialogue agents agree on. This includes implicit agreement: (1) a. A: The room went dark. b. A: Max turned out the light. c. B: And John drew the blinds. Intuitively, A and B agree that the room went dark, that Max turned out the light, and that the latter is at least part of the reason why the former occurred. Thus, implicatures can be agreed upon (that (1b) is part of the cause of (1a) goes beyond compositional semantics), and agreement can be implicated (B does not repeat (1a) and (1b) nor utter OK to indicate his agreement with A). In principle, the Grounding Acts Model (GAM, Traum (1994), Traum and Allen (1994)) supports implicit agreement. But it demands an acceptance act for agreement to occur, and its current rules don’t predict such an act from (1c). Segmented Discourse Representation Theory (SDRT, Asher and Lascarides (2003)) errs in the opposite direction. It stipulates 29 that lack of disagreement implicates agreement, and so in (1) too much is agreed upon; e.g., (1c). Thus, SDRT needs modification to deal with (1), just as GAM needs supplementation. Agreement can occur even in the context of corrections or disputes. In (2), A asserts (2a) and B its negation, but a con</context>
<context position="4910" citStr="Traum, 1994" startWordPosition="806" endWordPosition="807">agents understand each other perfectly, resolving ambiguities in the same way. One of Clark’s main claims is that grounding at all levels occurs only when there is positive evidence for it, and we aim to explore in a logically precise manner exactly what amount of positive evidence suffices for grounding a proposition. In future work, we intend to demonstrate that our definition of grounding can model grounding at the lower levels too; this will involve extending the framework to represent misunderstandings. GAM links the speech acts performed with its effects, including effects on grounding (Traum, 1994). Each conversational participant builds a conversational information state (or CIS). Update effects of particular speech acts (and their preconditions) are specified in terms of changes to (and conditions on) the CIS. For example, Figure 1 is the update rule for the speech act e where B asserts K to A. It updates the common ground (G) to include an event e&apos; that B intends A to believe K and a conditional event e&amp;quot; that should A accept the assertion, then A would be socially committed to B to believe K (shown via the attitude SCCOE). The update rules form a hierarchy, so that more specific acts</context>
</contexts>
<marker>Traum, 1994</marker>
<rawString>D. Traum. A Computational Theory of Grounding in Natural Language Conversation. PhD thesis, University of Rochester, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
<author>J Allen</author>
</authors>
<title>Discourse obligations in dialogue processing.</title>
<date>1994</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1321" citStr="Traum and Allen (1994)" startWordPosition="213" endWordPosition="216">ialogue agents agree on. This includes implicit agreement: (1) a. A: The room went dark. b. A: Max turned out the light. c. B: And John drew the blinds. Intuitively, A and B agree that the room went dark, that Max turned out the light, and that the latter is at least part of the reason why the former occurred. Thus, implicatures can be agreed upon (that (1b) is part of the cause of (1a) goes beyond compositional semantics), and agreement can be implicated (B does not repeat (1a) and (1b) nor utter OK to indicate his agreement with A). In principle, the Grounding Acts Model (GAM, Traum (1994), Traum and Allen (1994)) supports implicit agreement. But it demands an acceptance act for agreement to occur, and its current rules don’t predict such an act from (1c). Segmented Discourse Representation Theory (SDRT, Asher and Lascarides (2003)) errs in the opposite direction. It stipulates 29 that lack of disagreement implicates agreement, and so in (1) too much is agreed upon; e.g., (1c). Thus, SDRT needs modification to deal with (1), just as GAM needs supplementation. Agreement can occur even in the context of corrections or disputes. In (2), A asserts (2a) and B its negation, but a consistent interpretation o</context>
</contexts>
<marker>Traum, Allen, 1994</marker>
<rawString>D. Traum and J. Allen. Discourse obligations in dialogue processing. In Proceedings ofACL, pages 1–8, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
<author>E Hinkelman</author>
</authors>
<title>Conversation acts in task-oriented spoken dialogue.</title>
<date>1992</date>
<journal>Computational Intelligence,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="24578" citStr="Traum and Hinkelman (1992)" startWordPosition="4105" endWordPosition="4108">wledge the implicatures of 7rj itself; had we wished to do so, we would have included them within the scope of the acknowledgement. That is, we would infer the relation Acknowledgement(7r&apos;j, 7rz), where 7r&apos;j has semantic scope over 7rj, making 7rj and the rhetorical relations it engages in part of what is (explicitly) endorsed. It is because the discourse function of an acknowledgement is precisely to say what one agent commits to from another agent’s turn—i.e., what are the undenied commitments in this case— that Persistence applies redundantly. Explicit acknowledgements have been studied by Traum and Hinkelman (1992), among others. Here, we will ignore interpretations of an utterance 7r2 (e.g., OK) as an acknowledgement that K,, 34 Turn A’s SDRS B’s SDRS 1 π1A : Explanation(π1, π2) 0 2 π1A : Explanation(π1, π2) π2B : Explanation(π1, π) π : Narration(π2, π3) Table 2: The logical form of (1). was said (represented in SDRT with the so-called metatalk relation Acknowledgement*(π1,π2)), instead focussing entirely on an interpretation of π2 using Acknowledgement (i.e., a commitment to K,,, which in turn entails a commitment that K,, was said). But even so there is ambiguity, because linguistic form does not alw</context>
</contexts>
<marker>Traum, Hinkelman, 1992</marker>
<rawString>D. Traum and E. Hinkelman. Conversation acts in task-oriented spoken dialogue. Computational Intelligence, 8(3):575–599, 1992.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>