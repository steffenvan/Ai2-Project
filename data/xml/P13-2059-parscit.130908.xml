<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.567362">
Sign Language Lexical Recognition With Propositional Dynamic
Logic
Arturo Curiel
Université Paul Sabatier
118 route de Narbonne, IRIT,
31062, Toulouse, France
</note>
<email confidence="0.958397">
curiel@irit.fr
</email>
<sectionHeader confidence="0.985679" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923625">
This paper explores the use of Proposi-
tional Dynamic Logic (PDL) as a suit-
able formal framework for describing
Sign Language (SL), the language of
deaf people, in the context of natu-
ral language processing. SLs are vi-
sual, complete, standalone languages
which are just as expressive as oral lan-
guages. Signs in SL usually correspond
to sequences of highly specific body
postures interleaved with movements,
which make reference to real world ob-
jects, characters or situations. Here we
propose a formal representation of SL
signs, that will help us with the analysis
of automatically-collected hand track-
ing data from French Sign Language
(FSL) video corpora. We further show
how such a representation could help us
with the design of computer aided SL
verification tools, which in turn would
bring us closer to the development of an
automatic recognition system for these
languages.
</bodyText>
<sectionHeader confidence="0.99435" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967615384615">
Sign languages (SL), the vernaculars of deaf
people, are complete, rich, standalone commu-
nication systems which have evolved in paral-
lel with oral languages (Valli and Lucas, 2000).
However, in contrast to the last ones, research
in automatic SL processing has not yet man-
aged to build a complete, formal definition ori-
ented to their automatic recognition (Cuxac
and Dalle, 2007). In SL, both hands and non-
manual features (NMF), e.g. facial muscles,
can convey information with their placements,
configurations and movements. These particu-
lar conditions can difficult the construction of
</bodyText>
<note confidence="0.96499125">
Christophe Collet
Université Paul Sabatier
118 route de Narbonne, IRIT,
31062, Toulouse, France
</note>
<email confidence="0.755674">
collet@irit.fr
</email>
<bodyText confidence="0.996803666666667">
a formal description with common natural lan-
guage processing (NLP) methods, since the ex-
isting modeling techniques are mostly designed
to work with one-channel sound productions
inherent to oral languages, rather than with
the multi-channel partially-synchronized infor-
mation induced by SLs.
Our research strives to address the formal-
ization problem by introducing a logical lan-
guage that lets us represent SL from the lowest
level, so as to render the recognition task more
approachable. For this, we use an instance of
a formal logic, specifically Propositional Dy-
namic Logic (PDL), as a possible description
language for SL signs.
For the rest of this section, we will present a
brief introduction to current research efforts in
the area. Section 2 presents a general descrip-
tion of our formalism, while section 3 shows
how our work can be used when confronted
with real world data. Finally, section 4 present
our final observations and future work.
Images for the examples where taken from
(DictaSign, 2012) corpus.
</bodyText>
<subsectionHeader confidence="0.938848">
1.1 Current Sign Language Research
</subsectionHeader>
<bodyText confidence="0.999870133333333">
Extensive efforts have been made to achieve
efficient automatic capture and representation
of the subtle nuances commonly present in
sign language discourse (Ong and Ranganath,
2005). Research ranges from the development
of hand and body trackers (Dreuw et al., 2009;
Gianni and Dalle, 2009), to the design of high
level SL representation models (Lejeune, 2004;
Lenseigne and Dalle, 2006). Linguistic re-
search in the area has focused on the character-
ization of corporal expressions into meaning-
ful transcriptions (Dreuw et al., 2010; Stokoe,
2005) or common patterns across SL (Aronoff
et al., 2005; Meir et al., 2006; Wittmann,
1991), so as to gain understanding of the un-
</bodyText>
<page confidence="0.639886">
328
</page>
<note confidence="0.7985625">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9800199">
derlying mechanisms of SL communication.
Works like (Losson and Vannobel, 1998) deal
with the creation of a lexical description ori-
ented to computer-based sign animation. Re-
port (Filhol, 2009) describes a lexical specifi-
cation to address the same problem. Both pro-
pose a thoroughly geometrical parametric en-
coding of signs, thus leaving behind meaning-
ful information necessary for recognition and
introducing data beyond the scope of recog-
nition. This complicates the reutilization of
their formal descriptions. Besides, they don’t
take in account the presence of partial informa-
tion. Treating partiality is important for us,
since it is often the case with automatic tools
that incomplete or unrecognizable information
arises. Finally, little to no work has been di-
rected towards the unification of raw collected
data from SL corpora with higher level descrip-
tions (Dalle, 2006).
</bodyText>
<sectionHeader confidence="0.908" genericHeader="method">
2 Propositional Dynamic Logic for
SL
</sectionHeader>
<bodyText confidence="0.998824884615384">
Propositional Dynamic Logic (PDL) is a multi-
modal logic, first defined by (Fischer and Lad-
ner, 1979). It provides a language for describ-
ing programs, their correctness and termina-
tion, by allowing them to be modal operators.
We work with our own variant of this logic,
the Propositional Dynamic Logic for Sign Lan-
guage (PDLSL), which is just an instantiation
of PDL where we take signers’ movements as
programs.
Our sign formalization is based on the ap-
proach of (Liddell and Johnson, 1989) and (Fil-
hol, 2008). They describe signs as sequences of
immutable key postures and movement transi-
tions.
In general, each key posture will be charac-
terized by the concurrent parametric state of
each body articulator over a time-interval. For
us, a body articulator is any relevant body part
involved in signing. The parameters taken in
account can vary from articulator to articula-
tor, but most of the time they comprise their
configurations, orientations and their place-
ment within one or more places of articulation.
Transitions will correspond to the movements
executed between fixed postures.
</bodyText>
<subsectionHeader confidence="0.938651">
2.1 Syntax
</subsectionHeader>
<bodyText confidence="0.993748">
We need to define some primitive sets that will
limit the domain of our logical language.
</bodyText>
<construct confidence="0.970206117647059">
Definition 2.1 (Sign Language primi-
tives). Let BSL = {D, W, R, L} be the set of
relevant body articulators for SL, where D, W,
R and L represent the dominant, weak, right
and left hands, respectively. Both D and W
can be aliases for the right or left hands, but
they change depending on whether the signer
is right-handed or left-handed, or even depend-
ing on the context.
Let Ψ be the two-dimensional projection of
a human body skeleton, seen by the front. We
define the set of places of articulation for SL as
ΛSL = {HEAD, CHEST, NEUTRAL, ...}, such that
for each A ∈ ΛSL, A is a sub-plane of Ψ, as
shown graphically in figure 1.
Let CSL be the set of possible morphological
configurations for a hand.
</construct>
<bodyText confidence="0.8810852">
Let Δ = {↑, %, →, &amp;, ↓,., ←,-} be the set
of relative directions from the signer’s point of
view, where each arrow represents one of eight
possible two-dimensional direction vectors that
share the same origin. For vector S ∈ Δ, we
</bodyText>
<figure confidence="0.2923454">
←−
define vector S as the same as S but with the
←−
inverted abscissa axis, such that S ∈ Δ. Let
vector S indicate movement with respect to the
dominant or weak hand in the following man-
ner:
�
_ S if D ≡ R or W ≡ L
S ←−S if D ≡ L or W ≡ R
</figure>
<bodyText confidence="0.868996181818182">
Finally, let →−v1 and →−v2 be any two vectors with
the same origin. We denote the rotation angle
between the two as θ(−→v1,→−v2).
Now we define the set of atomic propositions
that we will use to characterize fixed states,
and a set of atomic actions to describe move-
ments.
Definition 2.2 (Atomic Propositions for
SL Body Articulators ΦSL). The set of
atomic propositions for SL articulators (ΦSL)
is defined as:
</bodyText>
<equation confidence="0.986134333333333">
ΦSL = {β1δ β2, Ξβ1
λ , Tβ1
β2 ,Fβ1 c , ∠01 }
</equation>
<bodyText confidence="0.853723">
where β1, β2 ∈ BSL, S ∈ Δ, A ∈ ΛSL and
c ∈ CSL.
</bodyText>
<page confidence="0.719307">
329
</page>
<figureCaption confidence="0.97882">
Figure 1: Possible places of articulation in BSL.
</figureCaption>
<equation confidence="0.897769347826087">
Intuitively, β16 indicates that articulator β1
02
is placed in relative direction δ with respect
to articulator β2. Let the current place of
articulation of β2 be the origin point of β2’s
→−
Cartesian system (C02). Let vector β1 de-
scribe the current place of articulation of β1
in C02. Proposition β16 02 holds when ∀−→v ∈ A,
θ(−→β1,δ) ≤ θ(−→β1,→−v ).
Ξ01
� asserts that articulator β1 is located in
λ.
T01 is active whenever articulator β1 physi-
02
cally touches articulator β2.
F01
� indicates that c is the morphological
configuration of articulator β1.
Finally, Z601 means that an articulator β1 is
oriented towards direction δ ∈ A. For hands,
Z6 will hold whenever the vector perpendicu-
01
</equation>
<bodyText confidence="0.856145333333333">
lar to the plane of the palm has the smallest
rotation angle with respect to δ.
Definition 2.3 (Atomic Actions for SL
Body Articulators HSL). The atomic ac-
tions for SL articulators ( HSL) are given by
the following set:
</bodyText>
<equation confidence="0.890329">
HSL = {δ01, +&amp;quot;01}
</equation>
<bodyText confidence="0.797006333333333">
where δ ∈ A and β1 ∈ BSL.
Let β1’s position before movement be the ori-
→−β1be
</bodyText>
<construct confidence="0.69411724">
the position vector of β1 in C01 after moving.
Action δ01 indicates that β1 moves in relative
direction δ in C01 if ∀−→v ∈ A, θ(−→β1, δ) ≤
θ(−→ β1, →− v ).
Action &apos;&amp;quot;01 occurs when articulator β1
moves rapidly and continuously (thrills) with-
out changing it’s current place of articulation.
Definition 2.4 (Action Language for SL
Body Articulators ASL). The action lan-
guage for body articulators (ASL) is given by
the following rule:
α ::= π  |α ∩ α  |α ∪ α  |α;α  |α*
where π ∈ HSL.
Intuitively, α ∩ α indicates the concurrent
execution of two actions, while α ∪ α means
that at least one of two actions will be non-
deterministically executed. Action α; α de-
scribes the sequential execution of two actions.
Finally, action α* indicates the reflexive tran-
sitive closure of α.
Definition 2.5 (Language PDLSL ). The
formulae ϕ of PDLSL are given by the following
rule:
ϕ ::= &gt;  |p  |¬ϕ  |ϕ ∧ ϕ |[α]ϕ
where p ∈ ΦSL, α ∈ ASL.
</construct>
<subsectionHeader confidence="0.96023">
2.2 Semantics
</subsectionHeader>
<bodyText confidence="0.964893307692308">
PDLSL formulas are interpreted over labeled
transition systems (LTS), in the spirit of the
possible worlds model introduced by (Hin-
tikka, 1962). Models correspond to connected
graphs representing key postures and transi-
tions: states are determined by the values of
their propositions, while edges represent sets
of executed movements. Here we present only
a small extract of the logic semantics.
Definition 2.6 (Sign Language Utterance
Model USL). A sign language utterance model
(USL), is a tuple USL = (S, R, J·KΠSL, J·KΦSL)
where:
</bodyText>
<listItem confidence="0.997915375">
• S is a non-empty set of states
• R is a transition relation R ⊆ S×S where,
∀s ∈ S, ∃s&apos; ∈ S such that (s, s&apos;) ∈ R.
• J·KΠSL : HSL → R, denotes the function
mapping actions to the set of binary rela-
tions.
• J·KΦSL : S → 2ΦSL, maps each state to a
set of atomic propositions.
</listItem>
<bodyText confidence="0.8161586">
gin of β1’s Cartesian system (C01) and
330
formulas codify the information tracked in the
previous part. Detected movements are inter-
preted as PDLSL actions between states.
</bodyText>
<figure confidence="0.999053898305085">
..
.
R�
L
.,L
&amp;quot;TORSE
ΞR
R_SIDEOFBODY
¬FR L_CONFIG
¬FL FIST_CONFIG
R T
L
..
_D ∩ a,,,,.G
...
R←
L
svL
&amp;quot;L SIDEOFBODY
`!,R
&amp;quot;R SIDEOFBODY
_
R
FKEY_CONFIG
L
FKEY_CONFIG
¬TLR
..
...
R←
L
.,L
&amp;quot;CKNTEROFBODY
R
ΞR SIDEOFHEAD
_
R
FBEAK_CONFIG
L
FINDEX CONFIG
¬TLR
..
...
R←
L
vL
&amp;quot;L SIDEOFBODY
.Za
&amp;quot;R SIDEOFBODY
R
FOPENPALM_CONFIG
L
FOPENPALM CONFIG
TR
L
...
.L
%L
%L
</figure>
<figureCaption confidence="0.933508333333333">
Figure 3: Example of modeling over four auto-
matically identified frames as possible key pos-
tures.
</figureCaption>
<bodyText confidence="0.994889769230769">
Figure 3 shows an example of the process.
Here, each key posture is codified into propo-
sitions acknowledging the hand positions with
respect to each other (R← L ), their place of artic-
ulation (e.g. “left hand floats over the torse”
with ΞLTORSE), their configuration (e.g. “right
hand is open” with FROPENPALM CONFIG) and their
movements (e.g. “left hand moves to the up-
left direction” with %L).
This module also checks that the generated
graph is correct: it will discard simple track-
ing errors to ensure that the resulting LTS will
remain consistent.
</bodyText>
<figure confidence="0.6689314">
(ΞRFACE∧ΞL
ACE ∧ LR&apos; ∧ FRCLAMP ∧ FLCLAMP ∧TL
R) →
[←R∩ →L](L→ R ∧FR CLAMP ∧ FL CLAMP ∧ ¬T L R )
3.3 Verification Module
</figure>
<bodyText confidence="0.996855">
First of all, the verification module has to be
loaded with a database of sign descriptions en-
coded as PDLSL formulas. These will charac-
terize the specific sequence of key postures that
morphologically describe a sign. For exam-
ple, let’s take the case for sign “route” in FSL,
shown in figure 4, with the following PDLSL
formulation,
</bodyText>
<equation confidence="0.993337">
Example 3.1 (ROUTEFSL formula).
(1)
</equation>
<bodyText confidence="0.9999872">
We also need to define a structure over se-
quences of states to model internal dependen-
cies between them, nevertheless we decided to
omit the rest of our semantics, alongside satis-
faction conditions, for the sake of readability.
</bodyText>
<sectionHeader confidence="0.9805975" genericHeader="method">
3 Use Case: Semi-Automatic Sign
Recognition
</sectionHeader>
<bodyText confidence="0.996898571428572">
We now present an example of how we can use
our formalism in a semi-automatic sign recog-
nition system. Figure 2 shows a simple module
diagram exemplifying information flow in the
system’s architecture. We proceed to briefly
describe each of our modules and how they
work together.
</bodyText>
<figureCaption confidence="0.946975">
Figure 2: Information flow in a semi-automatic
SL lexical recognition system.
</figureCaption>
<bodyText confidence="0.945707714285714">
3.1 Tracking and Segmentation
Module
The process starts by capturing relevant infor-
mation from video corpora. We use an exist-
ing head and hand tracker expressly developed
for SL research (Gonzalez and Collet, 2011).
This tool analyses individual video instances,
and returns the frame-by-frame positions of
the tracked articulators. By using this infor-
mation, the module can immediately calculate
speeds and directions on the fly for each hand.
The module further employs the method
proposed by the authors in (Gonzalez and
Collet, 2012) to achieve sub-lexical segmenta-
tion from the previously calculated data. Like
them, we use the relative velocity between
hands to identify when hands either move at
the same time, independently or don’t move at
all. With these, we can produce a set of possi-
ble key postures and transitions that will serve
as input to the modeling module.
</bodyText>
<subsectionHeader confidence="0.998421">
3.2 Model Extraction Module
</subsectionHeader>
<bodyText confidence="0.9976695">
This module calculates a propositional state
for each static posture, where atomic PDLSL
</bodyText>
<figure confidence="0.989854833333333">
Tracking
and Seg-
mentation
Module
PDLSL
Model
Extraction
Module
PDLSL
Verification
Module
Sign
Proposals
Corpus
PDLSL
Graph
Sign
Formulæ
User
Input
Key
postures &amp;
transitions
331
</figure>
<figureCaption confidence="0.999974">
Figure 4: ROUTEFSL production.
</figureCaption>
<bodyText confidence="0.999761846153846">
Formula (1) describes ROUTEFSL as a sign
with two key postures, connected by a two-
hand simultaneous movement (represented
with operator fl). It also indicates the posi-
tion of each hand, their orientation, whether
they touch and their respective configurations
(in this example, both hold the same CLAMP
configuration).
The module can then verify whether a sign
formula in the lexical database holds in any
sub-sequence of states of the graph generated
in the previous step. Algorithm 1 sums up the
process.
</bodyText>
<table confidence="0.87415425">
Algorithm 1 PDLSL Verification Algorithm
Require: SL model MSL
Require: connected graph GSL
Require: lexical database DBSL
</table>
<listItem confidence="0.982214444444445">
1: Proposals_For[state_qty]
2: for state s E GSL do
3: for sign co E DBSL where s E co do
4: if MSL, s |= co then
5: Proposals_For[s].append(co)
6: end if
7: end for
8: end for
9: return Proposals_For
</listItem>
<bodyText confidence="0.9911192">
For each state, the algorithm returns a set
of possible signs. Expert users (or higher level
algorithms) can further refine the process by
introducing additional information previously
missed by the tracker.
</bodyText>
<sectionHeader confidence="0.97703" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999994216216216">
We have shown how a logical language can
be used to model SL signs for semi-automatic
recognition, albeit with some restrictions. The
traits we have chosen to represent were im-
posed by the limits of the tracking tools we
had to our disposition, most notably working
with 2D coordinates. With these in mind, we
tried to design something flexible that could
be easily adapted by computer scientists and
linguists alike. Our primitive sets, were inten-
tionally defined in a very general fashion due
to the same reason: all of the perceived di-
rections, articulators and places of articulation
can easily change their domains, depending on
the SL we are modeling or the technological
constraints we have to deal with. Proposi-
tions can also be changed, or even induced, by
existing written sign representation languages
such as Zebedee (Filhol, 2008) or HamNoSys
(Hanke, 2004), mainly for the sake of extend-
ability.
From the application side, we still need to
create an extensive sign database codified in
PDLSL and try recognition on other corpora,
with different tracking information. For ver-
ification and model extraction, further opti-
mizations are expected, including the handling
of data inconsistencies and repairing broken
queries when verifying the graph.
Regarding our theoretical issues, future
work will be centered in improving our lan-
guage to better comply with SL research. This
includes adding new features, like incorpo-
rating probability representation to improve
recognition. We also expect to finish the defini-
tion of our formal semantics, as well as proving
correction and complexity of our algorithms.
</bodyText>
<sectionHeader confidence="0.987485" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.785024666666667">
Mark Aronoff, Irit Meir, and Wendy Sandler. 2005.
The paradox of sign language morphology. Lan-
guage, 81(2):301.
Christian Cuxac and Patrice Dalle. 2007. Problé-
matique des chercheurs en traitement automa-
tique des langues des signes, volume 48 of
Traitement Automatique des Langues. Lavoisier,
http://www.editions-hermes.fr/, October.
Patrice Dalle. 2006. High level models for sign lan-
guage analysis by a vision system. In Workshop
on the Representation and Processing of Sign
Language: Lexicographic Matters and Didactic
Scenarios (LREC), Italy, ELDA, page 17–20.
DictaSign. 2012. http://www.dictasign.eu.
Philippe Dreuw, Daniel Stein, and Hermann Ney.
2009. Enhancing a sign language transla-
tion system with vision-based features. In
Miguel Sales Dias, Sylvie Gibet, Marcelo M.
</reference>
<page confidence="0.651319">
332
</page>
<reference confidence="0.991638397959183">
Wanderley, and Rafael Bastos, editors, Gesture-
Based Human-Computer Interaction and Simu-
lation, number 5085 in Lecture Notes in Com-
puter Science, pages 108–113. Springer Berlin
Heidelberg, January.
Philippe Dreuw, Hermann Ney, Gregorio Martinez,
Onno Crasborn, Justus Piater, Jose Miguel
Moya, and Mark Wheatley. 2010. The Sign-
Speak project - bridging the gap between sign-
ers and speakers. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, and et. al., ed-
itors, Proceedings of the Seventh International
Conference on Language Resources and Evalu-
ation (LREC’10), Valletta, Malta, May. Euro-
pean Language Resources Association (ELRA).
Michael Filhol. 2008. Modèle descriptif des signes
pour un traitement automatique des langues des
signes. Ph.D. thesis, Université Paris-sud (Paris
11).
Michael Filhol. 2009. Zebedee: a lexical descrip-
tion model for sign language synthesis. Internal,
LIMSI.
Michael J. Fischer and Richard E. Ladner. 1979.
Propositional dynamic logic of regular pro-
grams. Journal of Computer and System Sci-
ences, 18(2):194–211, April.
Frédéric Gianni and Patrice Dalle. 2009. Ro-
bust tracking for processing of videos of com-
munication’s gestures. Gesture-Based Human-
Computer Interaction and Simulation, page
93–101.
Matilde Gonzalez and Christophe Collet. 2011.
Robust body parts tracking using particle fil-
ter and dynamic template. In 2011 18th IEEE
International Conference on Image Processing
(ICIP), pages 529 –532, September.
Matilde Gonzalez and Christophe Collet. 2012.
Sign segmentation using dynamics and hand
configuration for semi-automatic annotation of
sign language corpora. In Eleni Efthimiou,
Georgios Kouroupetroglou, and Stavroula-Evita
Fotinea, editors, Gesture and Sign Language
in Human-Computer Interaction and Embodied
Communication, number 7206 in Lecture Notes
in Computer Science, pages 204–215. Springer
Berlin Heidelberg, January.
Thomas Hanke. 2004. HamNoSys—Representing
sign language data in language resources and
language processing contexts. In Proceedings of
the Workshop on the Representation and Pro-
cessing of Sign Languages “From SignWriting to
Image Processing. Information, Lisbon, Portu-
gal, 30 May.
Jaakko Hintikka. 1962. Knowledge and Belief.
Ithaca, N.Y.,Cornell University Press.
Fanch Lejeune. 2004. Analyse sémantico-cognitive
d’énoncés en Langue des Signes Fran\ccaise
pour une génération automatique de séquences
gestuelles. Ph.D. thesis, PhD thesis, Orsay Uni-
versity, France.
Boris Lenseigne and Patrice Dalle. 2006. Us-
ing signing space as a representation for sign
language processing. In Sylvie Gibet, Nicolas
Courty, and Jean-François Kamp, editors, Ges-
ture in Human-Computer Interaction and Sim-
ulation, number 3881 in Lecture Notes in Com-
puter Science, pages 25–36. Springer Berlin Hei-
delberg, January.
S. K. Liddell and R. E. Johnson. 1989. American
sign language: The phonological base. Gallaudet
University Press, Washington. DC.
Olivier Losson and Jean-Marc Vannobel. 1998.
Sign language formal description and synthe-
sis. INT.JOURNAL OF VIRTUAL REALITY,
3:27—34.
Irit Meir, Carol Padden, Mark Aronoff, and Wendy
Sandler. 2006. Re-thinking sign language verb
classes: the body as subject. In Sign Languages:
Spinning and Unraveling the Past, Present and
Future. 9th Theoretical Issues in Sign Language
Research Conference, Florianopolis, Brazil, vol-
ume 382.
Sylvie C. W. Ong and Surendra Ranganath. 2005.
Automatic sign language analysis: a survey and
the future beyond lexical meaning. IEEE Trans-
actions on Pattern Analysis and Machine Intel-
ligence, 27(6):873 – 891, June.
William C. Stokoe. 2005. Sign language structure:
An outline of the visual communication systems
of the american deaf. Journal of Deaf Studies
and Deaf Education, 10(1):3–37, January.
Clayton Valli and Ceil Lucas. 2000. Linguistics of
American Sign Language Text, 3rd Edition: An
Introduction. Gallaudet University Press.
Henri Wittmann. 1991. Classification linguis-
tique des langues signées non vocalement. Revue
québécoise de linguistique théorique et appliquée,
10(1):88.
</reference>
<page confidence="0.962267">
333
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.507498">
<title confidence="0.993507">Sign Language Lexical Recognition With Propositional Dynamic Logic</title>
<author confidence="0.997951">Arturo Curiel</author>
<affiliation confidence="0.534281">Université Paul Sabatier</affiliation>
<address confidence="0.992985">118 route de Narbonne, IRIT, 31062, Toulouse, France</address>
<email confidence="0.997318">curiel@irit.fr</email>
<abstract confidence="0.99873816">This paper explores the use of Propositional Dynamic Logic (PDL) as a suitable formal framework for describing Sign Language (SL), the language of deaf people, in the context of natural language processing. SLs are visual, complete, standalone languages which are just as expressive as oral languages. Signs in SL usually correspond to sequences of highly specific body postures interleaved with movements, which make reference to real world objects, characters or situations. Here we propose a formal representation of SL signs, that will help us with the analysis of automatically-collected hand tracking data from French Sign Language (FSL) video corpora. We further show how such a representation could help us with the design of computer aided SL verification tools, which in turn would bring us closer to the development of an automatic recognition system for these languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark Aronoff</author>
<author>Irit Meir</author>
<author>Wendy Sandler</author>
</authors>
<title>The paradox of sign language morphology.</title>
<date>2005</date>
<journal>Language,</journal>
<volume>81</volume>
<issue>2</issue>
<contexts>
<context position="3433" citStr="Aronoff et al., 2005" startWordPosition="530" endWordPosition="533">ign Language Research Extensive efforts have been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics derlying mechanisms of SL communication. Works like (Losson and Vannobel, 1998) deal with the creation of a lexical description oriented to computer-based sign animation. Report (Filhol, 2009) describes a lexical specification to address the same problem. Both propose a thoroughly geometrical parametric encoding of signs, thus leavi</context>
</contexts>
<marker>Aronoff, Meir, Sandler, 2005</marker>
<rawString>Mark Aronoff, Irit Meir, and Wendy Sandler. 2005. The paradox of sign language morphology. Language, 81(2):301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Cuxac</author>
<author>Patrice Dalle</author>
</authors>
<title>Problé-matique des chercheurs en traitement automatique des langues des signes, volume 48 of Traitement Automatique des Langues. Lavoisier,</title>
<date>2007</date>
<location>http://www.editions-hermes.fr/,</location>
<contexts>
<context position="1460" citStr="Cuxac and Dalle, 2007" startWordPosition="225" endWordPosition="228">ra. We further show how such a representation could help us with the design of computer aided SL verification tools, which in turn would bring us closer to the development of an automatic recognition system for these languages. 1 Introduction Sign languages (SL), the vernaculars of deaf people, are complete, rich, standalone communication systems which have evolved in parallel with oral languages (Valli and Lucas, 2000). However, in contrast to the last ones, research in automatic SL processing has not yet managed to build a complete, formal definition oriented to their automatic recognition (Cuxac and Dalle, 2007). In SL, both hands and nonmanual features (NMF), e.g. facial muscles, can convey information with their placements, configurations and movements. These particular conditions can difficult the construction of Christophe Collet Université Paul Sabatier 118 route de Narbonne, IRIT, 31062, Toulouse, France collet@irit.fr a formal description with common natural language processing (NLP) methods, since the existing modeling techniques are mostly designed to work with one-channel sound productions inherent to oral languages, rather than with the multi-channel partially-synchronized information indu</context>
</contexts>
<marker>Cuxac, Dalle, 2007</marker>
<rawString>Christian Cuxac and Patrice Dalle. 2007. Problé-matique des chercheurs en traitement automatique des langues des signes, volume 48 of Traitement Automatique des Langues. Lavoisier, http://www.editions-hermes.fr/, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrice Dalle</author>
</authors>
<title>High level models for sign language analysis by a vision system.</title>
<date>2006</date>
<booktitle>In Workshop on the Representation and Processing of Sign Language: Lexicographic Matters and Didactic Scenarios (LREC),</booktitle>
<pages>17--20</pages>
<location>Italy, ELDA,</location>
<contexts>
<context position="3223" citStr="Dalle, 2006" startWordPosition="498" endWordPosition="499">work can be used when confronted with real world data. Finally, section 4 present our final observations and future work. Images for the examples where taken from (DictaSign, 2012) corpus. 1.1 Current Sign Language Research Extensive efforts have been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics derlying mechanisms of SL communication. Works like (Losson and Vannobel, 1998) deal with the creation of a lexical descript</context>
<context position="4578" citStr="Dalle, 2006" startWordPosition="706" endWordPosition="707">e a thoroughly geometrical parametric encoding of signs, thus leaving behind meaningful information necessary for recognition and introducing data beyond the scope of recognition. This complicates the reutilization of their formal descriptions. Besides, they don’t take in account the presence of partial information. Treating partiality is important for us, since it is often the case with automatic tools that incomplete or unrecognizable information arises. Finally, little to no work has been directed towards the unification of raw collected data from SL corpora with higher level descriptions (Dalle, 2006). 2 Propositional Dynamic Logic for SL Propositional Dynamic Logic (PDL) is a multimodal logic, first defined by (Fischer and Ladner, 1979). It provides a language for describing programs, their correctness and termination, by allowing them to be modal operators. We work with our own variant of this logic, the Propositional Dynamic Logic for Sign Language (PDLSL), which is just an instantiation of PDL where we take signers’ movements as programs. Our sign formalization is based on the approach of (Liddell and Johnson, 1989) and (Filhol, 2008). They describe signs as sequences of immutable key </context>
</contexts>
<marker>Dalle, 2006</marker>
<rawString>Patrice Dalle. 2006. High level models for sign language analysis by a vision system. In Workshop on the Representation and Processing of Sign Language: Lexicographic Matters and Didactic Scenarios (LREC), Italy, ELDA, page 17–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DictaSign</author>
</authors>
<date>2012</date>
<note>http://www.dictasign.eu.</note>
<contexts>
<context position="2791" citStr="DictaSign, 2012" startWordPosition="433" endWordPosition="434">s represent SL from the lowest level, so as to render the recognition task more approachable. For this, we use an instance of a formal logic, specifically Propositional Dynamic Logic (PDL), as a possible description language for SL signs. For the rest of this section, we will present a brief introduction to current research efforts in the area. Section 2 presents a general description of our formalism, while section 3 shows how our work can be used when confronted with real world data. Finally, section 4 present our final observations and future work. Images for the examples where taken from (DictaSign, 2012) corpus. 1.1 Current Sign Language Research Extensive efforts have been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or commo</context>
</contexts>
<marker>DictaSign, 2012</marker>
<rawString>DictaSign. 2012. http://www.dictasign.eu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Dreuw</author>
<author>Daniel Stein</author>
<author>Hermann Ney</author>
</authors>
<title>Enhancing a sign language translation system with vision-based features.</title>
<date>2009</date>
<booktitle>GestureBased Human-Computer Interaction and Simulation, number 5085 in Lecture Notes in Computer Science,</booktitle>
<pages>108--113</pages>
<editor>In Miguel Sales Dias, Sylvie Gibet, Marcelo M. Wanderley, and Rafael Bastos, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="3101" citStr="Dreuw et al., 2009" startWordPosition="477" endWordPosition="480"> current research efforts in the area. Section 2 presents a general description of our formalism, while section 3 shows how our work can be used when confronted with real world data. Finally, section 4 present our final observations and future work. Images for the examples where taken from (DictaSign, 2012) corpus. 1.1 Current Sign Language Research Extensive efforts have been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics de</context>
</contexts>
<marker>Dreuw, Stein, Ney, 2009</marker>
<rawString>Philippe Dreuw, Daniel Stein, and Hermann Ney. 2009. Enhancing a sign language translation system with vision-based features. In Miguel Sales Dias, Sylvie Gibet, Marcelo M. Wanderley, and Rafael Bastos, editors, GestureBased Human-Computer Interaction and Simulation, number 5085 in Lecture Notes in Computer Science, pages 108–113. Springer Berlin Heidelberg, January.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philippe Dreuw</author>
<author>Hermann Ney</author>
<author>Gregorio Martinez</author>
<author>Onno Crasborn</author>
<author>Justus Piater</author>
<author>Jose Miguel Moya</author>
<author>Mark Wheatley</author>
</authors>
<title>The SignSpeak project - bridging the gap between signers and speakers.</title>
<date>2010</date>
<booktitle>In Nicoletta Calzolari</booktitle>
<editor>(Conference Chair), Khalid Choukri, and et. al., editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="3367" citStr="Dreuw et al., 2010" startWordPosition="519" endWordPosition="522">xamples where taken from (DictaSign, 2012) corpus. 1.1 Current Sign Language Research Extensive efforts have been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics derlying mechanisms of SL communication. Works like (Losson and Vannobel, 1998) deal with the creation of a lexical description oriented to computer-based sign animation. Report (Filhol, 2009) describes a lexical specification to address the same problem. Both propose</context>
</contexts>
<marker>Dreuw, Ney, Martinez, Crasborn, Piater, Moya, Wheatley, 2010</marker>
<rawString>Philippe Dreuw, Hermann Ney, Gregorio Martinez, Onno Crasborn, Justus Piater, Jose Miguel Moya, and Mark Wheatley. 2010. The SignSpeak project - bridging the gap between signers and speakers. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, and et. al., editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Filhol</author>
</authors>
<title>Modèle descriptif des signes pour un traitement automatique des langues des signes.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<volume>11</volume>
<institution>Université Paris-sud (Paris</institution>
<contexts>
<context position="5126" citStr="Filhol, 2008" startWordPosition="798" endWordPosition="800">data from SL corpora with higher level descriptions (Dalle, 2006). 2 Propositional Dynamic Logic for SL Propositional Dynamic Logic (PDL) is a multimodal logic, first defined by (Fischer and Ladner, 1979). It provides a language for describing programs, their correctness and termination, by allowing them to be modal operators. We work with our own variant of this logic, the Propositional Dynamic Logic for Sign Language (PDLSL), which is just an instantiation of PDL where we take signers’ movements as programs. Our sign formalization is based on the approach of (Liddell and Johnson, 1989) and (Filhol, 2008). They describe signs as sequences of immutable key postures and movement transitions. In general, each key posture will be characterized by the concurrent parametric state of each body articulator over a time-interval. For us, a body articulator is any relevant body part involved in signing. The parameters taken in account can vary from articulator to articulator, but most of the time they comprise their configurations, orientations and their placement within one or more places of articulation. Transitions will correspond to the movements executed between fixed postures. 2.1 Syntax We need to</context>
<context position="15697" citStr="Filhol, 2008" startWordPosition="2652" endWordPosition="2653">r disposition, most notably working with 2D coordinates. With these in mind, we tried to design something flexible that could be easily adapted by computer scientists and linguists alike. Our primitive sets, were intentionally defined in a very general fashion due to the same reason: all of the perceived directions, articulators and places of articulation can easily change their domains, depending on the SL we are modeling or the technological constraints we have to deal with. Propositions can also be changed, or even induced, by existing written sign representation languages such as Zebedee (Filhol, 2008) or HamNoSys (Hanke, 2004), mainly for the sake of extendability. From the application side, we still need to create an extensive sign database codified in PDLSL and try recognition on other corpora, with different tracking information. For verification and model extraction, further optimizations are expected, including the handling of data inconsistencies and repairing broken queries when verifying the graph. Regarding our theoretical issues, future work will be centered in improving our language to better comply with SL research. This includes adding new features, like incorporating probabil</context>
</contexts>
<marker>Filhol, 2008</marker>
<rawString>Michael Filhol. 2008. Modèle descriptif des signes pour un traitement automatique des langues des signes. Ph.D. thesis, Université Paris-sud (Paris 11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Filhol</author>
</authors>
<title>Zebedee: a lexical description model for sign language synthesis.</title>
<date>2009</date>
<tech>Internal, LIMSI.</tech>
<contexts>
<context position="3891" citStr="Filhol, 2009" startWordPosition="600" endWordPosition="601">aracterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics derlying mechanisms of SL communication. Works like (Losson and Vannobel, 1998) deal with the creation of a lexical description oriented to computer-based sign animation. Report (Filhol, 2009) describes a lexical specification to address the same problem. Both propose a thoroughly geometrical parametric encoding of signs, thus leaving behind meaningful information necessary for recognition and introducing data beyond the scope of recognition. This complicates the reutilization of their formal descriptions. Besides, they don’t take in account the presence of partial information. Treating partiality is important for us, since it is often the case with automatic tools that incomplete or unrecognizable information arises. Finally, little to no work has been directed towards the unifica</context>
</contexts>
<marker>Filhol, 2009</marker>
<rawString>Michael Filhol. 2009. Zebedee: a lexical description model for sign language synthesis. Internal, LIMSI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Fischer</author>
<author>Richard E Ladner</author>
</authors>
<title>Propositional dynamic logic of regular programs.</title>
<date>1979</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="4717" citStr="Fischer and Ladner, 1979" startWordPosition="726" endWordPosition="730">and introducing data beyond the scope of recognition. This complicates the reutilization of their formal descriptions. Besides, they don’t take in account the presence of partial information. Treating partiality is important for us, since it is often the case with automatic tools that incomplete or unrecognizable information arises. Finally, little to no work has been directed towards the unification of raw collected data from SL corpora with higher level descriptions (Dalle, 2006). 2 Propositional Dynamic Logic for SL Propositional Dynamic Logic (PDL) is a multimodal logic, first defined by (Fischer and Ladner, 1979). It provides a language for describing programs, their correctness and termination, by allowing them to be modal operators. We work with our own variant of this logic, the Propositional Dynamic Logic for Sign Language (PDLSL), which is just an instantiation of PDL where we take signers’ movements as programs. Our sign formalization is based on the approach of (Liddell and Johnson, 1989) and (Filhol, 2008). They describe signs as sequences of immutable key postures and movement transitions. In general, each key posture will be characterized by the concurrent parametric state of each body artic</context>
</contexts>
<marker>Fischer, Ladner, 1979</marker>
<rawString>Michael J. Fischer and Richard E. Ladner. 1979. Propositional dynamic logic of regular programs. Journal of Computer and System Sciences, 18(2):194–211, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frédéric Gianni</author>
<author>Patrice Dalle</author>
</authors>
<title>Robust tracking for processing of videos of communication’s gestures. Gesture-Based HumanComputer Interaction and Simulation,</title>
<date>2009</date>
<pages>93--101</pages>
<contexts>
<context position="3126" citStr="Gianni and Dalle, 2009" startWordPosition="481" endWordPosition="484">forts in the area. Section 2 presents a general description of our formalism, while section 3 shows how our work can be used when confronted with real world data. Finally, section 4 present our final observations and future work. Images for the examples where taken from (DictaSign, 2012) corpus. 1.1 Current Sign Language Research Extensive efforts have been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics derlying mechanisms of SL c</context>
</contexts>
<marker>Gianni, Dalle, 2009</marker>
<rawString>Frédéric Gianni and Patrice Dalle. 2009. Robust tracking for processing of videos of communication’s gestures. Gesture-Based HumanComputer Interaction and Simulation, page 93–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matilde Gonzalez</author>
<author>Christophe Collet</author>
</authors>
<title>Robust body parts tracking using particle filter and dynamic template.</title>
<date>2011</date>
<booktitle>In 2011 18th IEEE International Conference on Image Processing (ICIP),</booktitle>
<pages>529--532</pages>
<contexts>
<context position="12796" citStr="Gonzalez and Collet, 2011" startWordPosition="2181" endWordPosition="2184">ability. 3 Use Case: Semi-Automatic Sign Recognition We now present an example of how we can use our formalism in a semi-automatic sign recognition system. Figure 2 shows a simple module diagram exemplifying information flow in the system’s architecture. We proceed to briefly describe each of our modules and how they work together. Figure 2: Information flow in a semi-automatic SL lexical recognition system. 3.1 Tracking and Segmentation Module The process starts by capturing relevant information from video corpora. We use an existing head and hand tracker expressly developed for SL research (Gonzalez and Collet, 2011). This tool analyses individual video instances, and returns the frame-by-frame positions of the tracked articulators. By using this information, the module can immediately calculate speeds and directions on the fly for each hand. The module further employs the method proposed by the authors in (Gonzalez and Collet, 2012) to achieve sub-lexical segmentation from the previously calculated data. Like them, we use the relative velocity between hands to identify when hands either move at the same time, independently or don’t move at all. With these, we can produce a set of possible key postures an</context>
</contexts>
<marker>Gonzalez, Collet, 2011</marker>
<rawString>Matilde Gonzalez and Christophe Collet. 2011. Robust body parts tracking using particle filter and dynamic template. In 2011 18th IEEE International Conference on Image Processing (ICIP), pages 529 –532, September.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Matilde Gonzalez</author>
<author>Christophe Collet</author>
</authors>
<title>Sign segmentation using dynamics and hand configuration for semi-automatic annotation of sign language corpora.</title>
<date>2012</date>
<booktitle>In Eleni Efthimiou, Georgios Kouroupetroglou, and Stavroula-Evita Fotinea, editors, Gesture and Sign Language in Human-Computer Interaction and Embodied Communication, number 7206 in Lecture Notes in Computer Science,</booktitle>
<pages>204--215</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="13119" citStr="Gonzalez and Collet, 2012" startWordPosition="2230" endWordPosition="2233"> together. Figure 2: Information flow in a semi-automatic SL lexical recognition system. 3.1 Tracking and Segmentation Module The process starts by capturing relevant information from video corpora. We use an existing head and hand tracker expressly developed for SL research (Gonzalez and Collet, 2011). This tool analyses individual video instances, and returns the frame-by-frame positions of the tracked articulators. By using this information, the module can immediately calculate speeds and directions on the fly for each hand. The module further employs the method proposed by the authors in (Gonzalez and Collet, 2012) to achieve sub-lexical segmentation from the previously calculated data. Like them, we use the relative velocity between hands to identify when hands either move at the same time, independently or don’t move at all. With these, we can produce a set of possible key postures and transitions that will serve as input to the modeling module. 3.2 Model Extraction Module This module calculates a propositional state for each static posture, where atomic PDLSL Tracking and Segmentation Module PDLSL Model Extraction Module PDLSL Verification Module Sign Proposals Corpus PDLSL Graph Sign Formulæ User In</context>
</contexts>
<marker>Gonzalez, Collet, 2012</marker>
<rawString>Matilde Gonzalez and Christophe Collet. 2012. Sign segmentation using dynamics and hand configuration for semi-automatic annotation of sign language corpora. In Eleni Efthimiou, Georgios Kouroupetroglou, and Stavroula-Evita Fotinea, editors, Gesture and Sign Language in Human-Computer Interaction and Embodied Communication, number 7206 in Lecture Notes in Computer Science, pages 204–215. Springer Berlin Heidelberg, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hanke</author>
</authors>
<title>HamNoSys—Representing sign language data in language resources and language processing contexts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on the Representation and Processing of Sign Languages “From SignWriting to Image Processing. Information,</booktitle>
<location>Lisbon,</location>
<contexts>
<context position="15723" citStr="Hanke, 2004" startWordPosition="2656" endWordPosition="2657"> working with 2D coordinates. With these in mind, we tried to design something flexible that could be easily adapted by computer scientists and linguists alike. Our primitive sets, were intentionally defined in a very general fashion due to the same reason: all of the perceived directions, articulators and places of articulation can easily change their domains, depending on the SL we are modeling or the technological constraints we have to deal with. Propositions can also be changed, or even induced, by existing written sign representation languages such as Zebedee (Filhol, 2008) or HamNoSys (Hanke, 2004), mainly for the sake of extendability. From the application side, we still need to create an extensive sign database codified in PDLSL and try recognition on other corpora, with different tracking information. For verification and model extraction, further optimizations are expected, including the handling of data inconsistencies and repairing broken queries when verifying the graph. Regarding our theoretical issues, future work will be centered in improving our language to better comply with SL research. This includes adding new features, like incorporating probability representation to impr</context>
</contexts>
<marker>Hanke, 2004</marker>
<rawString>Thomas Hanke. 2004. HamNoSys—Representing sign language data in language resources and language processing contexts. In Proceedings of the Workshop on the Representation and Processing of Sign Languages “From SignWriting to Image Processing. Information, Lisbon, Portugal, 30 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaakko Hintikka</author>
</authors>
<title>Knowledge and Belief.</title>
<date>1962</date>
<publisher>University Press.</publisher>
<location>Ithaca, N.Y.,Cornell</location>
<contexts>
<context position="9615" citStr="Hintikka, 1962" startWordPosition="1626" endWordPosition="1628">|α;α |α* where π ∈ HSL. Intuitively, α ∩ α indicates the concurrent execution of two actions, while α ∪ α means that at least one of two actions will be nondeterministically executed. Action α; α describes the sequential execution of two actions. Finally, action α* indicates the reflexive transitive closure of α. Definition 2.5 (Language PDLSL ). The formulae ϕ of PDLSL are given by the following rule: ϕ ::= &gt; |p |¬ϕ |ϕ ∧ ϕ |[α]ϕ where p ∈ ΦSL, α ∈ ASL. 2.2 Semantics PDLSL formulas are interpreted over labeled transition systems (LTS), in the spirit of the possible worlds model introduced by (Hintikka, 1962). Models correspond to connected graphs representing key postures and transitions: states are determined by the values of their propositions, while edges represent sets of executed movements. Here we present only a small extract of the logic semantics. Definition 2.6 (Sign Language Utterance Model USL). A sign language utterance model (USL), is a tuple USL = (S, R, J·KΠSL, J·KΦSL) where: • S is a non-empty set of states • R is a transition relation R ⊆ S×S where, ∀s ∈ S, ∃s&apos; ∈ S such that (s, s&apos;) ∈ R. • J·KΠSL : HSL → R, denotes the function mapping actions to the set of binary relations. • J·</context>
</contexts>
<marker>Hintikka, 1962</marker>
<rawString>Jaakko Hintikka. 1962. Knowledge and Belief. Ithaca, N.Y.,Cornell University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fanch Lejeune</author>
</authors>
<title>Analyse sémantico-cognitive d’énoncés en Langue des Signes Fran\ccaise pour une génération automatique de séquences gestuelles.</title>
<date>2004</date>
<tech>Ph.D. thesis, PhD thesis,</tech>
<institution>Orsay University,</institution>
<contexts>
<context position="3195" citStr="Lejeune, 2004" startWordPosition="494" endWordPosition="495">hile section 3 shows how our work can be used when confronted with real world data. Finally, section 4 present our final observations and future work. Images for the examples where taken from (DictaSign, 2012) corpus. 1.1 Current Sign Language Research Extensive efforts have been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics derlying mechanisms of SL communication. Works like (Losson and Vannobel, 1998) deal with the cr</context>
</contexts>
<marker>Lejeune, 2004</marker>
<rawString>Fanch Lejeune. 2004. Analyse sémantico-cognitive d’énoncés en Langue des Signes Fran\ccaise pour une génération automatique de séquences gestuelles. Ph.D. thesis, PhD thesis, Orsay University, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Lenseigne</author>
<author>Patrice Dalle</author>
</authors>
<title>Using signing space as a representation for sign language processing.</title>
<date>2006</date>
<booktitle>In Sylvie Gibet, Nicolas Courty, and Jean-François Kamp, editors, Gesture in Human-Computer Interaction and Simulation, number 3881 in Lecture Notes in Computer Science,</booktitle>
<pages>25--36</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="3223" citStr="Lenseigne and Dalle, 2006" startWordPosition="496" endWordPosition="499">shows how our work can be used when confronted with real world data. Finally, section 4 present our final observations and future work. Images for the examples where taken from (DictaSign, 2012) corpus. 1.1 Current Sign Language Research Extensive efforts have been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics derlying mechanisms of SL communication. Works like (Losson and Vannobel, 1998) deal with the creation of a lexical descript</context>
</contexts>
<marker>Lenseigne, Dalle, 2006</marker>
<rawString>Boris Lenseigne and Patrice Dalle. 2006. Using signing space as a representation for sign language processing. In Sylvie Gibet, Nicolas Courty, and Jean-François Kamp, editors, Gesture in Human-Computer Interaction and Simulation, number 3881 in Lecture Notes in Computer Science, pages 25–36. Springer Berlin Heidelberg, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K Liddell</author>
<author>R E Johnson</author>
</authors>
<title>American sign language: The phonological base.</title>
<date>1989</date>
<publisher>Gallaudet University Press,</publisher>
<location>Washington. DC.</location>
<contexts>
<context position="5107" citStr="Liddell and Johnson, 1989" startWordPosition="793" endWordPosition="796">he unification of raw collected data from SL corpora with higher level descriptions (Dalle, 2006). 2 Propositional Dynamic Logic for SL Propositional Dynamic Logic (PDL) is a multimodal logic, first defined by (Fischer and Ladner, 1979). It provides a language for describing programs, their correctness and termination, by allowing them to be modal operators. We work with our own variant of this logic, the Propositional Dynamic Logic for Sign Language (PDLSL), which is just an instantiation of PDL where we take signers’ movements as programs. Our sign formalization is based on the approach of (Liddell and Johnson, 1989) and (Filhol, 2008). They describe signs as sequences of immutable key postures and movement transitions. In general, each key posture will be characterized by the concurrent parametric state of each body articulator over a time-interval. For us, a body articulator is any relevant body part involved in signing. The parameters taken in account can vary from articulator to articulator, but most of the time they comprise their configurations, orientations and their placement within one or more places of articulation. Transitions will correspond to the movements executed between fixed postures. 2.</context>
</contexts>
<marker>Liddell, Johnson, 1989</marker>
<rawString>S. K. Liddell and R. E. Johnson. 1989. American sign language: The phonological base. Gallaudet University Press, Washington. DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Losson</author>
<author>Jean-Marc Vannobel</author>
</authors>
<title>Sign language formal description and synthesis.</title>
<date>1998</date>
<journal>INT.JOURNAL OF VIRTUAL REALITY,</journal>
<pages>3--27</pages>
<contexts>
<context position="3778" citStr="Losson and Vannobel, 1998" startWordPosition="580" endWordPosition="583">vel SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics derlying mechanisms of SL communication. Works like (Losson and Vannobel, 1998) deal with the creation of a lexical description oriented to computer-based sign animation. Report (Filhol, 2009) describes a lexical specification to address the same problem. Both propose a thoroughly geometrical parametric encoding of signs, thus leaving behind meaningful information necessary for recognition and introducing data beyond the scope of recognition. This complicates the reutilization of their formal descriptions. Besides, they don’t take in account the presence of partial information. Treating partiality is important for us, since it is often the case with automatic tools that </context>
</contexts>
<marker>Losson, Vannobel, 1998</marker>
<rawString>Olivier Losson and Jean-Marc Vannobel. 1998. Sign language formal description and synthesis. INT.JOURNAL OF VIRTUAL REALITY, 3:27—34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irit Meir</author>
<author>Carol Padden</author>
<author>Mark Aronoff</author>
<author>Wendy Sandler</author>
</authors>
<title>Re-thinking sign language verb classes: the body as subject.</title>
<date>2006</date>
<booktitle>In Sign Languages: Spinning and Unraveling the Past, Present and Future. 9th Theoretical Issues in Sign Language Research Conference,</booktitle>
<volume>volume</volume>
<pages>382</pages>
<location>Florianopolis, Brazil,</location>
<contexts>
<context position="3452" citStr="Meir et al., 2006" startWordPosition="534" endWordPosition="537">Extensive efforts have been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics derlying mechanisms of SL communication. Works like (Losson and Vannobel, 1998) deal with the creation of a lexical description oriented to computer-based sign animation. Report (Filhol, 2009) describes a lexical specification to address the same problem. Both propose a thoroughly geometrical parametric encoding of signs, thus leaving behind meaningfu</context>
</contexts>
<marker>Meir, Padden, Aronoff, Sandler, 2006</marker>
<rawString>Irit Meir, Carol Padden, Mark Aronoff, and Wendy Sandler. 2006. Re-thinking sign language verb classes: the body as subject. In Sign Languages: Spinning and Unraveling the Past, Present and Future. 9th Theoretical Issues in Sign Language Research Conference, Florianopolis, Brazil, volume 382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvie C W Ong</author>
<author>Surendra Ranganath</author>
</authors>
<title>Automatic sign language analysis: a survey and the future beyond lexical meaning.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>27</volume>
<issue>6</issue>
<contexts>
<context position="3017" citStr="Ong and Ranganath, 2005" startWordPosition="463" endWordPosition="466">nguage for SL signs. For the rest of this section, we will present a brief introduction to current research efforts in the area. Section 2 presents a general description of our formalism, while section 3 shows how our work can be used when confronted with real world data. Finally, section 4 present our final observations and future work. Images for the examples where taken from (DictaSign, 2012) corpus. 1.1 Current Sign Language Research Extensive efforts have been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, S</context>
</contexts>
<marker>Ong, Ranganath, 2005</marker>
<rawString>Sylvie C. W. Ong and Surendra Ranganath. 2005. Automatic sign language analysis: a survey and the future beyond lexical meaning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(6):873 – 891, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Stokoe</author>
</authors>
<title>Sign language structure: An outline of the visual communication systems of the american deaf.</title>
<date>2005</date>
<journal>Journal of Deaf Studies and Deaf Education,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="3382" citStr="Stokoe, 2005" startWordPosition="523" endWordPosition="524">from (DictaSign, 2012) corpus. 1.1 Current Sign Language Research Extensive efforts have been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics derlying mechanisms of SL communication. Works like (Losson and Vannobel, 1998) deal with the creation of a lexical description oriented to computer-based sign animation. Report (Filhol, 2009) describes a lexical specification to address the same problem. Both propose a thoroughly g</context>
</contexts>
<marker>Stokoe, 2005</marker>
<rawString>William C. Stokoe. 2005. Sign language structure: An outline of the visual communication systems of the american deaf. Journal of Deaf Studies and Deaf Education, 10(1):3–37, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clayton Valli</author>
<author>Ceil Lucas</author>
</authors>
<title>Linguistics of American Sign Language Text, 3rd Edition: An Introduction.</title>
<date>2000</date>
<publisher>Gallaudet University Press.</publisher>
<contexts>
<context position="1261" citStr="Valli and Lucas, 2000" startWordPosition="192" endWordPosition="195">cters or situations. Here we propose a formal representation of SL signs, that will help us with the analysis of automatically-collected hand tracking data from French Sign Language (FSL) video corpora. We further show how such a representation could help us with the design of computer aided SL verification tools, which in turn would bring us closer to the development of an automatic recognition system for these languages. 1 Introduction Sign languages (SL), the vernaculars of deaf people, are complete, rich, standalone communication systems which have evolved in parallel with oral languages (Valli and Lucas, 2000). However, in contrast to the last ones, research in automatic SL processing has not yet managed to build a complete, formal definition oriented to their automatic recognition (Cuxac and Dalle, 2007). In SL, both hands and nonmanual features (NMF), e.g. facial muscles, can convey information with their placements, configurations and movements. These particular conditions can difficult the construction of Christophe Collet Université Paul Sabatier 118 route de Narbonne, IRIT, 31062, Toulouse, France collet@irit.fr a formal description with common natural language processing (NLP) methods, since</context>
</contexts>
<marker>Valli, Lucas, 2000</marker>
<rawString>Clayton Valli and Ceil Lucas. 2000. Linguistics of American Sign Language Text, 3rd Edition: An Introduction. Gallaudet University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henri Wittmann</author>
</authors>
<title>Classification linguistique des langues signées non vocalement. Revue québécoise de linguistique théorique et appliquée,</title>
<date>1991</date>
<pages>10--1</pages>
<contexts>
<context position="3469" citStr="Wittmann, 1991" startWordPosition="538" endWordPosition="539">ave been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un328 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics derlying mechanisms of SL communication. Works like (Losson and Vannobel, 1998) deal with the creation of a lexical description oriented to computer-based sign animation. Report (Filhol, 2009) describes a lexical specification to address the same problem. Both propose a thoroughly geometrical parametric encoding of signs, thus leaving behind meaningful information nec</context>
</contexts>
<marker>Wittmann, 1991</marker>
<rawString>Henri Wittmann. 1991. Classification linguistique des langues signées non vocalement. Revue québécoise de linguistique théorique et appliquée, 10(1):88.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>