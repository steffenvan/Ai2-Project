<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010147">
<title confidence="0.761899">
ISCAS:A System for Chinese Word Sense Induction Based on
K-means Algorithm
</title>
<author confidence="0.99826">
Zhenzhong Zhang* Le Sun† Wenbo Li†
</author>
<affiliation confidence="0.985934">
*Institute of Software, Graduate University †Institute of Software
Chinese Academy of Sciences Chinese Academy of Sciences
</affiliation>
<email confidence="0.994579">
zhenzhong@nfs.iscas.ac.cn {sunle,wenbo02}@iscas.ac.cn
</email>
<sectionHeader confidence="0.994662" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999786866666667">
This paper presents an unsupervised
method for automatic Chinese word
sense induction. The algorithm is based
on clustering the similar words according
to the contexts in which they occur. First,
the target word which needs to be
disambiguated is represented as the
vector of its contexts. Then, reconstruct
the matrix constituted by the vectors of
target words through singular value
decomposition (SVD) method, and use
the vectors to cluster the similar words.
Our system participants in CLP2010
back off task4-Chinese word sense
induction.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934705882353">
It has been shown that using word senses instead
of surface word forms could improve
performance on many nature language
processing tasks such as information extraction
(Joyce and Alan, 1999), information retrieval
(Ozlem et al., 1999) and machine translation
(David et al., 2005). Historically, word senses
are represented as a fixed-list of definitions
coming from a manually complied dictionary.
However, there seem to be some disadvantages
associated with such fixed-list of senses
paradigm. Since dictionaries usually contain
general definitions and lack explicit semantic,
they can’t reflect the exact content of the context
where the target word appears. Another
disadvantage is that the granularity of sense
distinctions is fixed, so it may not be entirely
suitable for different applications.
In order to overcome these limitations, some
techniques like word sense induction (WSI) have
been proposed for discovering words’ senses
automatically from the unannotated corpus. The
word sense induction algorithms are usually base
on the Distributional Hypothesis, proposed by
(Zellig, 1954), which showed that words with
similar meanings appear in similar contexts
(Michael, 2009). And the hypothesis is also
popularized with the phrase “a word characte-
rized by the company it keeps” (John, 1957).
This concept shows us a method to automatical-
ly discover senses of words by clustering the
target words with similar contexts (Lin, 1998).
The word sense induction can be regarded as an
unsupervised clustering problem. First, select
some features to be used when comparing simi-
larity between words. Second, represent disam-
biguated words as vectors of selected features
according to target words’ contexts. Third, clus-
ter the similar words using the vectors. But
compared with European languages such as Eng-
lish, Chinese language has its own characteris-
tics. For example, Chinese ideographs have
senses while the English alphabets don’t have.
So the methods which work well in English may
not be entirely suitable for Chinese.
This paper proposes a method for Chinese
word sense induction, which contains two stage
processes: features selecting and context cluster-
ing. Chinese ideographs and Chinese words
which have two or more Chinese ideographs are
used different strategies when selecting features.
The vectors of target word’s instances are put
together to constitute a matrix, whose row is in-
stances and column is features. Reconstruct the
matrix through singular value decomposition to
get a new vector for each instance. Then, K-
means clustering algorithm is employed to clus-
ter the vectors of disambiguated words’ contexts.
Each cluster to which some instances belong to
identifies a sense of corresponding target word.
Our system participants in CLP2010 back off
task4 - Chinese word sense induction.
The remainder of this paper is organized as
follows. Section 2 presents the Chinese word
senses induction algorithm. Section 3 presents
the evaluation sheme and the results of our
system. Section 4 gives some discussions and
conclusions.
</bodyText>
<sectionHeader confidence="0.855744" genericHeader="method">
2 Chinese Word Senses Induction
</sectionHeader>
<bodyText confidence="0.9996975">
This section will present the strategies of select-
ing features for disambiguated Chinese words
and k-means algorithm for clustering vectors of
the contexts.
</bodyText>
<subsectionHeader confidence="0.915954">
2.1 Features Selection
</subsectionHeader>
<bodyText confidence="0.999815608695652">
Since the input instances of target words are un-
structured, it&apos;s necessary to select features and
transform them into structured format to fit the
automatic clustering algorithm. Following the
example in (Ted, 2007), words are chosen as
features to represent the contexts where target
words appear. A word w in the context of the
target word can be represented as a vector whose
ith component is the average of the calculated
conditional probabilities of w and wj.
The target words are usually removed from
the corpus in the task of English word sense in-
duction. But Chinese language is very different
from European languages such as English. Chi-
nese ideographs usually have meanings of their
own while English alphabets don’t have. In
Chinese word senses induction tasks, the target
word may be a Chinese word which could have
one or more Chinese ideographs or a Chinese
ideograph. And the meaning of Chinese ideo-
graphs is determined by the Chinese word where
it appears. The following example shows us this
case.
</bodyText>
<listItem confidence="0.856594">
• 我国依靠推广超级稻累计增产稻谷 162
亿公斤。
</listItem>
<equation confidence="0.820286">
在木化石园附件的一处山谷,是大佛寺
近期增加的五百罗汉堂 。
</equation>
<bodyText confidence="0.995474648648649">
In this example, the target word is Chinese
ideograph “谷” displayed in italic in the con-
texts. In the first context, its meaning is paddy
which is determined by the Chinese word “稻
谷”, and similarly in the second context its
meaning is valley determined by “山谷”. Since
the meaning of the Chinese ideograph “谷” is
determined by the word where it appears, it may
not be appropriate to remove it from the con-
texts simply while the others of the word are left.
Different strategies are employed to remove tar-
get words. If the target word contains two or
more Chinese ideographs, it will be removed
from the context. Otherwise it will be kept.
To solve the problem of data sparseness, we
extracted extra 100 instances for each target
word from Sogou Data and also used the
thesauruses (TongYiCi CiLin of HIT) to reduce
the dimensionality of the word space (feature
space). Two filtering heuristics are applied when
selecting features. The first one is the minimum
frequency p1 of words, and the second one is the
maximum frequency p2 of words.
Each selected word (feature) should be as-
signed a weight, which indicates the relative fre-
quency of two co-occurring words. Using condi-
tional probabilities for weighting for object/verb
and subject/verb pairs is better than point-wise
mutual information (Philipp et al., 2005). So we
used conditional probabilities for weighting
words pairs. Let numi,j denote the number of the
instances where the word i and word j co-occur ,
and numi denote the number of the instances in
which the word i appears. Then the jth compo-
nent of the vector of the word i can be calculated
using the following equation.
p j i p i j
</bodyText>
<equation confidence="0.971869333333333">
(  |) (  |)
+
=
2
Where
num
p i j
(  |) =
num j
</equation>
<bodyText confidence="0.961644166666667">
The contexts of each target word are represented
as the centroid of the vectors of the words occur-
ring in the target contexts. Figure 1 shows an
example of context vector, where the Chinese
word “果实” co-occurs with Chinese words “水
果”and “种子
</bodyText>
<equation confidence="0.608372">
”.
wi j,
i j,
</equation>
<figureCaption confidence="0.934945333333333">
Figure 1: An example of a context vector for
“AV”, calculated as the centroid of vectors of
“ &apos;�” and “7fCA”.
</figureCaption>
<subsectionHeader confidence="0.997887">
2.2 Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.999738170731707">
K-means algorithm is applied to cluster the vec-
tors of the target word. It assigns each element to
one of K clusters according to which centroid
the element is close to by the similarity function.
The cosine function is used to measure the simi-
larity between two vectors V and W:
where n is the number of features in each vector.
Before clustering the vectors of instances, we
put together the vectors of instances in the cor-
pus and obtain a co-occurrence matrix of in-
stances and words. Singular value decomposi-
tion is applied to reduce the dimensionality of
the resulting multidimensional space and finds
the major axes of variation in the word space
(Golub and Van Loan, 1989). After the reduc-
tion, the similarity between two instances can be
measured using the cosine function mentioned as
above between the corresponding vectors. The
clustering algorithm stops when the centroid of
each cluster does not change or the iteration of
the algorithm exceed a user-defined threshold p3.
And the number of the clusters is determined by
the corpus where the target word appears. Each
cluster to which some instances belong
represents one senses of the target word
represented by the vector.
We also employed a graph-based clustering
algorithm -Chinese Whispers (CW) (Chris, 2006)
to deal with the task of Chinese WSI. CW does
not require any input parameters and has a good
performance in WSI (Chris, 2006). For more
details about CW algorithm please refer to
(Chris, 2006). We first constructed a graph,
whose vertexes were instances of target word
and edges’ weight was the similarity of the cor-
responding two vertexes. Then we removed the
edges with minimum weight until the percentage
of the kept edges’ sum respect the total was be-
low a threshold p4. CW algorithm was employed
to cluster the graph and each clusters represented
a sense of target word.
</bodyText>
<sectionHeader confidence="0.999101" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9999295">
This section presents the evaluation scheme, set
of parameters and the result of our system.
</bodyText>
<subsectionHeader confidence="0.997728">
3.1 Evaluation Scheme
</subsectionHeader>
<bodyText confidence="0.999977181818182">
We use standard cluster evaluation methods to
measure the performance of our WSI system.
Following the former practice (Zhao and Kary-
pis, 2005), we consider the FScore measure for
assessing WSI methods. The FScore is used in a
similar fashion to Information Retrieval exercis-
es.
Let we assume that the size of a particular
class sr is nr, the size of a particular cluster hj is
nj and the size of their common instances set is
nr,j. The precision can be calculated as follow:
</bodyText>
<equation confidence="0.9826245">
,
P s h
( , ) r j
=
r j
nj
</equation>
<bodyText confidence="0.518884">
The recall value can be defined as:
</bodyText>
<equation confidence="0.972186">
R(sr , hj) = nr,j
nr
</equation>
<bodyText confidence="0.912951">
Then FScore of this class and cluster is defined
to be:
</bodyText>
<equation confidence="0.9225745">
2× P(sr, hj) ×R(sr,hj)
P(sr, hj) +R(sr,hj)
</equation>
<bodyText confidence="0.998797333333333">
The FScore of class sr, F(sr), is the maximum
F(sr, hj) value attained by any cluster, and it is
defined as:
</bodyText>
<equation confidence="0.945072">
F(sr) = max(F(sr, hj))
hj
</equation>
<bodyText confidence="0.968241">
Finally, the FScore of the entire clustering solu-
tion is defined as the weighted average FScore
of each class:
</bodyText>
<equation confidence="0.997375315789474">
n
V •W
i
=
=
1
|V|×|W|
n n
WE W2 i
E
i = 1 i=1
VW
i i
E
( , )
V W =
sim
n
F(sr,hj) =
</equation>
<bodyText confidence="0.998320333333333">
Where q is the number of classes and n is the
total number of the instances where target word
appears.
</bodyText>
<subsectionHeader confidence="0.99996">
3.2 Tuning the Parameters
</subsectionHeader>
<bodyText confidence="0.999931785714286">
We tune the parameters of our system on the
training data. But because of time restrictions,
we do not optimize these parameters. The max-
imum frequency of a word (p2) and the maxi-
mum number of the K-means’ iteration (p3) are
tuned on the training data. The minimum fre-
quency of a word (p1) was set to two following
our intuition. The last parameter K -the number
of the clusters is determined by the test data in
which the target word appears. When tuning pa-
rameters, we first fixed the parameter p3 and
found the best value of parameter p2, which
could lead to the best performance. The results
have been shown in Table 1 and Table 2.
</bodyText>
<table confidence="0.901543">
Parameters FScore
P3=300,p2=35 0.7502
P3=400,p2=40 0.7523
P3=500,p2=40 0.7582
</table>
<tableCaption confidence="0.998544">
Table 1: The results of K-means with SVD
</tableCaption>
<table confidence="0.985056">
Parameters FScore
P3=300,p2=40 0.7454
P3=400,p2=40 0.7493
P3=500,p2=45 0.7404
</table>
<tableCaption confidence="0.998968">
Table 2: The results of K-means
</tableCaption>
<bodyText confidence="0.985765666666667">
The performance of CW algorithm is shown
in Table 3. The parameter p4 is a threshold for
pruning grap
</bodyText>
<tableCaption confidence="0.988022">
Table 3: The results of CW.
</tableCaption>
<bodyText confidence="0.9993883">
The result shows that the K-means algorithm
has a better performance than CW. That may
because CW can’t use the information of the
number of clusters, but K-means could. Another
problem for CW is that the size of corpus is
small and the constructed graph can’t reflect the
inherent relation between the instances.
Based on the result of experiments, we em-
ployed K-means algorithm for our system and
the parameters is shown in Table 4.
</bodyText>
<table confidence="0.998992833333333">
Parameters Value
P1: Minimum frequency of a word 2
P2: Maximum frequency of a word 40
P3: Maximum number of K-means ite- 500
ration
K: the number of the cluster -
</table>
<tableCaption confidence="0.951715">
Table 4: Parameters for the system. The last pa-
rameter K is provided by the test data.
</tableCaption>
<subsectionHeader confidence="0.890796">
3.3 Result
</subsectionHeader>
<bodyText confidence="0.9999672">
Our system participants in the CLP2010 back-
off task4 and disambiguate 100 target words,
total 5000 instances. The F-score of our system
on the test data is 0.7209 against the F-score
0.7933 of the best system.
</bodyText>
<sectionHeader confidence="0.999161" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999996842105263">
We have presented a model for Chinese word
sense induction. Different strategies are applied
to deal with Chinese ideographs and Chinese
words that contain two or more Chinese ideo-
graphs. After selecting the features –words, sin-
gular value decomposition is used to find the
major axes of variation in the feature space and
reconstruct the vector of each context. Then we
employ k-means cluster algorithm to cluster the
vectors of contexts. Result shows that our sys-
tem is able to induce correct senses. One draw-
back of our system is that it overlooks the infre-
quent senses because of lacking enough data.
And our system only uses the information of
word co-occurrences. So in the future we would
like to integrate different kinds of information
such as topical information, syntactic informa-
tion and semantic information, and see if we
could get a better result.
</bodyText>
<sectionHeader confidence="0.976224" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.958085">
This work has been partially funded by National
Natural Science Foundation of China under
grant #60773027, #60736044 and #90920010
</bodyText>
<figure confidence="0.99515105">
( )
sr
nrxF
q
= ∑
FScore
1
n
=
r
Parameter FScore
P4=0.55 0.6325
P4=0.6 0.6321
P4=0.65 0.6278
P4=0.7 0.6393
P4=0.75 0.6289
P4=0.8 0.6345
P4=0.85 0.6326
P4=0.9 0.6342
P4=0.95 0.6355
</figure>
<bodyText confidence="0.954266">
and by “863” Key Projects #2006AA010108,
“863” Projects #2008AA01Z145. We would like
to thank anonymous reviewers for their detailed
comments.
</bodyText>
<sectionHeader confidence="0.98005" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998392888888889">
Chris Biemann, 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to
natural language processing problems, In Pro-
ceedings of TextGraphs, pp. 73–80, New York,
USA.
David Vickrey, Luke Biewald, Marc Teyssley, and
Daphne Koller. 2005. Word-sense disambiguation
for machine translation. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 771-778, Vancouver, British Columbia,
Canada
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics,
volume 2, pages 768-774, Montreal, Quebec, Can-
ada
Golub, G. H. and Van Loan, C. F. 1989. Matrix
Computations. The John Hopkins University Press,
Baltimore, MD
John, R., Firth. 1957. A Synopsis of Linguistic Theory
1930-1955, pages 1-32.
Joyce Yue Chai and Alan W. Biermann. 1999. The
use of word sense disambiguation in an informa-
tion extraction system. In Proceedings of the six-
teenth national conference on Artificial intelli-
gence and the eleventh Innovative applications of
artificial intelligence conference innovative appli-
cations of artificial intelligence, pages 850-855,
Orlando, Florida, United States.
Michael Denkowski. 2009. A Survey of Techniques
for Unsupervised Word Sense Induction.
Ozlem Uzuner, Boris Katz, and Deniz Yuret. 1999.
Word sense disambiguation for information re-
trieval. In Proceedings of the sixteenth national
conference on Artificial intelligence and the ele-
venth Innovative applications of artificial intelli-
gence conference innovative applications of artifi-
cial intelligence, page 985, Orlando, Florida, Unit-
ed States.
Philipp Cimiano, Andreas Hotho, and Steffen Staab,
2005. Learning concept hierarchies from text cor-
pora using formal concept analysis, Journal of Ar-
tificial Intelligence Research (JAIR), 24, 305–339.
Ted Pedersen, 2007. Umnd2: Senseclusters applied to
the sense induction task of senseval-4. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations, pages 394–397, Prague, Czech
Republic.
Zellig Harris. 1954. Distributional Structure, pages
146-162.
Ying Zhao and George Karypis. 2005. Hierarchical
clustering algorithms for document datasets. Data
Mining and Knowledge Discovery, 10(2):141.168.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.187018">
<title confidence="0.9942115">System for Chinese Word Sense Induction Based on K-means Algorithm</title>
<author confidence="0.984712">Zhenzhong Zhang Le_Sun† Wenbo Li†</author>
<affiliation confidence="0.878026">Institute of Software, Graduate University †Institute of Software Chinese Academy of Sciences Chinese Academy of Sciences</affiliation>
<abstract confidence="0.949612470588235">zhenzhong@nfs.iscas.ac.cn {sunle,wenbo02}@iscas.ac.cn Abstract This paper presents an unsupervised method for automatic Chinese word sense induction. The algorithm is based on clustering the similar words according to the contexts in which they occur. First, the target word which needs to be disambiguated is represented as the vector of its contexts. Then, reconstruct the matrix constituted by the vectors of target words through singular value decomposition (SVD) method, and use the vectors to cluster the similar words. Our system participants in CLP2010 back off task4-Chinese word sense induction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems,</title>
<date>2006</date>
<booktitle>In Proceedings of TextGraphs,</booktitle>
<pages>73--80</pages>
<location>New York, USA.</location>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann, 2006. Chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems, In Proceedings of TextGraphs, pp. 73–80, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Luke Biewald</author>
<author>Marc Teyssley</author>
<author>Daphne Koller</author>
</authors>
<title>Word-sense disambiguation for machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>771--778</pages>
<location>Vancouver, British Columbia, Canada</location>
<marker>Vickrey, Biewald, Teyssley, Koller, 2005</marker>
<rawString>David Vickrey, Luke Biewald, Marc Teyssley, and Daphne Koller. 2005. Word-sense disambiguation for machine translation. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 771-778, Vancouver, British Columbia, Canada</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<volume>2</volume>
<pages>768--774</pages>
<location>Montreal, Quebec, Canada</location>
<contexts>
<context position="2294" citStr="Lin, 1998" startWordPosition="334" endWordPosition="335">imitations, some techniques like word sense induction (WSI) have been proposed for discovering words’ senses automatically from the unannotated corpus. The word sense induction algorithms are usually base on the Distributional Hypothesis, proposed by (Zellig, 1954), which showed that words with similar meanings appear in similar contexts (Michael, 2009). And the hypothesis is also popularized with the phrase “a word characterized by the company it keeps” (John, 1957). This concept shows us a method to automatically discover senses of words by clustering the target words with similar contexts (Lin, 1998). The word sense induction can be regarded as an unsupervised clustering problem. First, select some features to be used when comparing similarity between words. Second, represent disambiguated words as vectors of selected features according to target words’ contexts. Third, cluster the similar words using the vectors. But compared with European languages such as English, Chinese language has its own characteristics. For example, Chinese ideographs have senses while the English alphabets don’t have. So the methods which work well in English may not be entirely suitable for Chinese. This paper </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international conference on Computational linguistics, volume 2, pages 768-774, Montreal, Quebec, Canada</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H Golub</author>
<author>C F Van Loan</author>
</authors>
<title>Matrix Computations.</title>
<date>1989</date>
<publisher>The John Hopkins University Press,</publisher>
<location>Baltimore, MD</location>
<marker>Golub, Van Loan, 1989</marker>
<rawString>Golub, G. H. and Van Loan, C. F. 1989. Matrix Computations. The John Hopkins University Press, Baltimore, MD</rawString>
</citation>
<citation valid="true">
<authors>
<author>R John</author>
<author>Firth</author>
</authors>
<title>A Synopsis of Linguistic Theory</title>
<date>1957</date>
<pages>1--32</pages>
<marker>John, Firth, 1957</marker>
<rawString>John, R., Firth. 1957. A Synopsis of Linguistic Theory 1930-1955, pages 1-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joyce Yue Chai</author>
<author>Alan W Biermann</author>
</authors>
<title>The use of word sense disambiguation in an information extraction system.</title>
<date>1999</date>
<booktitle>In Proceedings of the sixteenth national conference on Artificial intelligence and the eleventh Innovative applications of artificial intelligence conference innovative applications of artificial intelligence,</booktitle>
<pages>850--855</pages>
<location>Orlando, Florida, United States.</location>
<marker>Chai, Biermann, 1999</marker>
<rawString>Joyce Yue Chai and Alan W. Biermann. 1999. The use of word sense disambiguation in an information extraction system. In Proceedings of the sixteenth national conference on Artificial intelligence and the eleventh Innovative applications of artificial intelligence conference innovative applications of artificial intelligence, pages 850-855, Orlando, Florida, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
</authors>
<title>A Survey of Techniques for Unsupervised Word Sense Induction.</title>
<date>2009</date>
<marker>Denkowski, 2009</marker>
<rawString>Michael Denkowski. 2009. A Survey of Techniques for Unsupervised Word Sense Induction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozlem Uzuner</author>
<author>Boris Katz</author>
<author>Deniz Yuret</author>
</authors>
<title>Word sense disambiguation for information retrieval.</title>
<date>1999</date>
<booktitle>In Proceedings of the sixteenth national conference on Artificial intelligence and the eleventh Innovative applications of artificial intelligence conference innovative applications of artificial intelligence,</booktitle>
<pages>985</pages>
<location>Orlando, Florida, United States.</location>
<marker>Uzuner, Katz, Yuret, 1999</marker>
<rawString>Ozlem Uzuner, Boris Katz, and Deniz Yuret. 1999. Word sense disambiguation for information retrieval. In Proceedings of the sixteenth national conference on Artificial intelligence and the eleventh Innovative applications of artificial intelligence conference innovative applications of artificial intelligence, page 985, Orlando, Florida, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Andreas Hotho</author>
<author>Steffen Staab</author>
</authors>
<title>Learning concept hierarchies from text corpora using formal concept analysis,</title>
<date>2005</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<volume>24</volume>
<pages>305--339</pages>
<marker>Cimiano, Hotho, Staab, 2005</marker>
<rawString>Philipp Cimiano, Andreas Hotho, and Steffen Staab, 2005. Learning concept hierarchies from text corpora using formal concept analysis, Journal of Artificial Intelligence Research (JAIR), 24, 305–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Umnd2: Senseclusters applied to the sense induction task of senseval-4.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations,</booktitle>
<pages>394--397</pages>
<location>Prague, Czech Republic.</location>
<marker>Pedersen, 2007</marker>
<rawString>Ted Pedersen, 2007. Umnd2: Senseclusters applied to the sense induction task of senseval-4. In Proceedings of the Fourth International Workshop on Semantic Evaluations, pages 394–397, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional Structure,</title>
<date>1954</date>
<pages>146--162</pages>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional Structure, pages 146-162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhao</author>
<author>George Karypis</author>
</authors>
<title>Hierarchical clustering algorithms for document datasets.</title>
<date>2005</date>
<journal>Data Mining and Knowledge Discovery,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="9399" citStr="Zhao and Karypis, 2005" startWordPosition="1522" endWordPosition="1526">whose vertexes were instances of target word and edges’ weight was the similarity of the corresponding two vertexes. Then we removed the edges with minimum weight until the percentage of the kept edges’ sum respect the total was below a threshold p4. CW algorithm was employed to cluster the graph and each clusters represented a sense of target word. 3 Evaluation This section presents the evaluation scheme, set of parameters and the result of our system. 3.1 Evaluation Scheme We use standard cluster evaluation methods to measure the performance of our WSI system. Following the former practice (Zhao and Karypis, 2005), we consider the FScore measure for assessing WSI methods. The FScore is used in a similar fashion to Information Retrieval exercises. Let we assume that the size of a particular class sr is nr, the size of a particular cluster hj is nj and the size of their common instances set is nr,j. The precision can be calculated as follow: , P s h ( , ) r j = r j nj The recall value can be defined as: R(sr , hj) = nr,j nr Then FScore of this class and cluster is defined to be: 2× P(sr, hj) ×R(sr,hj) P(sr, hj) +R(sr,hj) The FScore of class sr, F(sr), is the maximum F(sr, hj) value attained by any cluste</context>
</contexts>
<marker>Zhao, Karypis, 2005</marker>
<rawString>Ying Zhao and George Karypis. 2005. Hierarchical clustering algorithms for document datasets. Data Mining and Knowledge Discovery, 10(2):141.168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>