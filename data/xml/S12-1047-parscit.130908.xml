<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.84045">
SemEval-2012 Task 2: Measuring Degrees of Relational Similarity
</title>
<author confidence="0.860153">
David A. Jurgens
</author>
<affiliation confidence="0.9936145">
Department of Computer Science
University of California, Los Angeles
</affiliation>
<email confidence="0.994691">
jurgens@cs.ucla.edu
</email>
<author confidence="0.859717">
Peter D. Turney
Emerging Technologies
</author>
<affiliation confidence="0.521392">
National Research Council Canada
</affiliation>
<email confidence="0.549549">
peter.turney@nrc-cnrc.gc.ca
</email>
<author confidence="0.389612">
Saif M. Mohammad
Emerging Technologies
</author>
<affiliation confidence="0.270424">
National Research Council Canada
</affiliation>
<email confidence="0.274258">
saif.mohammad@nrc-cnrc.gc.ca
</email>
<author confidence="0.882808">
Keith J. Holyoak
</author>
<affiliation confidence="0.991963">
Department of Psychology
University of California, Los Angeles
</affiliation>
<email confidence="0.999113">
holyoak@lifesci.ucla.edu
</email>
<sectionHeader confidence="0.998592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997309866666667">
Up to now, work on semantic relations has fo-
cused on relation classification: recognizing
whether a given instance (a word pair such as
virus:flu) belongs to a specific relation class
(such as CAUSE:EFFECT). However, instances
of a single relation class may still have signif-
icant variability in how characteristic they are
of that class. We present a new SemEval task
based on identifying the degree of prototypi-
cality for instances within a given class. As
a part of the task, we have assembled the first
dataset of graded relational similarity ratings
across 79 relation categories. Three teams
submitted six systems, which were evaluated
using two methods.
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999909416666667">
Relational similarity measures the degree of corre-
spondence between two relations, where instance
pairs that have high relational similarity are said to
be analogous, i.e., to express the same relation (Tur-
ney, 2006). However, a class of analogous relations
may still have significant variability in the degree of
relational similarity of its members. Consider the
four word pairs dog:bark, cat:meow, floor:squeak,
and car:honk. We could say that these four X:Y
pairs are all instances of the semantic relation EN-
TITY:SOUND; that is, X is an entity that character-
istically makes the sound Y . Within a class of anal-
ogous pairs, certain pairs are more characteristic of
the relation. For example, many would agree that
dog:bark and cat:meow are better prototypes of the
ENTITY:SOUND relation than floor:squeak. Our task
requires automatic systems to quantify the degree of
prototypicality of a target pair by measuring the re-
lational similarity between it and pairs that are given
as defining examples of a particular relation.
So far, most work in semantic relations has fo-
cused on differences between relation categories for
classifying new relation instances. Past SemEval
tasks that use relations have focused largely on dis-
crete classification (Girju et al., 2007; Hendrickx et
al., 2010) and paraphrasing the relations connecting
noun compounds with a verb (Butnariu et al., 2010),
which is also a form of discrete classification due to
the lack of continuous degrees. However, there is
some loss of information in any discrete classifica-
tion of semantic relations. Furthermore, while some
discrete classifiers provide a degree of confidence or
probability for a relation classification, there is no
a priori reason that such values would correspond
to human prototypicality judgments. Our proposed
task is distinct from these past tasks in that we fo-
cus on measuring the degree of relational similarity.1
A graded measure of the degree of relational simi-
larity would tell us that dog:bark is more similar to
cat:meow than to floor:squeak. The discrete classifi-
cation ENTITY:SOUND drops this information.
Systems that are successful at identifying degrees
of relation similarity can have a significant impact
where an application must choose between multi-
ple instances of the same relation. We illustrate
this with two examples. First, consider a rela-
tional search task (Cafarella et al., 2006). A user
of a relational search engine might give the query,
</bodyText>
<footnote confidence="0.9920635">
1Task details and data are available at
https://sites.google.com/site/semeval2012task2/ .
</footnote>
<page confidence="0.975686">
356
</page>
<note confidence="0.98592">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 356–364,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.4793303">
Subcategory Relation name Relation schema Paradigms Responses
8(e) AGENT:GOAL “Y is the goal of X” pilgrim:shrine patient:health
assassin:death runner:finish
climber:peak astronaut:space
5(e) OBJECT:TYPICAL ACTION “an X will typically Y” glass:break ice:melt
soldier:fight lion:roar
juggernaut:crush knife:stab
4(h) DEFECTIVE “an X is is a defect in Y” fallacy:logic pimple:skin
astigmatism:sight ignorance:learning
limp:walk tumor:body
</figure>
<tableCaption confidence="0.996678">
Table 1: Examples of the three manually selected paradigms and the corresponding pairs generated by Turkers.
</tableCaption>
<bodyText confidence="0.999652738095238">
“List all things that are part of a car.” SemEval-
2007 Task 4 proposed that a relational search engine
would use semantic relation classification to answer
queries like this one. For this query, a classifier that
was trained with the relation PART:WHOLE would be
used. However, a system for measuring degrees of
relational similarity would be better suited to rela-
tional search than a discrete classifier, because the
relational search engine could then rank the output
list in order of applicability. For the same query, the
search engine could rank each item X in descending
order of the degree of relational similarity between
X:car and a training set of prototypical examples of
the relation PART:WHOLE. This would be analogous
to how standard search engines rank documents or
web pages in descending order of relevance to the
user’s query.
As a second example, consider the role of rela-
tional similarity in analogical transfer. When faced
with a new situation, we look for an analogous sit-
uation in our past experience, and we use analogi-
cal inference to transfer information from the past
experience (the source domain) to the new situation
(the target domain) (Gentner, 1983; Holyoak, 2012).
Analogy is based on relational similarity (Gentner,
1983; Turney, 2008). The degree of relational sim-
ilarity in an analogy is indicative of the likelihood
that transferred knowledge will be applicable in the
target domain. For example, past experience tells us
that a dog barks to send a signal to other creatures. If
we transfer this knowledge to a new experience with
a cat meowing, we can predict that the cat is sending
a signal, and we can act appropriately with that pre-
diction. If we transfer this knowledge to a new expe-
rience with a floor squeaking, we might predict that
the floor is sending a signal, which might lead us to
act inappropriately. If we have a choice among sev-
eral source analogies, usually the source pair with
the highest degree of relational similarity to the tar-
get pair will prove to be the most useful analogy in
the target domain, providing practical benefits be-
yond discrete relational classification.
</bodyText>
<sectionHeader confidence="0.975697" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.999942">
Here, we describe our task and the two-level hierar-
chy of semantic relation classes used for the task.
</bodyText>
<subsectionHeader confidence="0.918692">
2.1 Objective
</subsectionHeader>
<bodyText confidence="0.999920545454545">
Our task is to rate word pairs by the degree to
which they are prototypical members of a given re-
lation class. The relation class is specified by a
few paradigmatic (highly prototypical) examples of
word pairs that belong to the class and also by a
schematic representation of the relation class. The
task requires comparing a word pair to the paradig-
matic examples and/or the schematic representation.
For example, suppose the relation class is REVERSE.
We may specify this class by the paradigmatic ex-
amples attack:defend, buy:sell, love:hate, and the
schematic representation “X is the reverse act of
Y ” or “X may be undone by Y .” Given a pair
such as repair:break, we compare this pair to the
paradigmatic examples and/or the schematic repre-
sentation, in order to estimate its degree of prototyp-
icality. The challenges are (1) to infer the relation
from the paradigmatic examples and identify what
relational or featural attributes best characterize that
relation, and (2) to identify the relation of the given
pair and rate how similar it is to that shared by the
paradigmatic examples.
</bodyText>
<page confidence="0.996191">
357
</page>
<subsectionHeader confidence="0.991511">
2.2 Relation Categories
</subsectionHeader>
<bodyText confidence="0.999716542857143">
Researchers in psychology and linguistics have con-
sidered many different categorizations of semantic
relations. The particular relation categorization is
often driven by both the type of data and the in-
tended application. Nastase and Szpakowicz (2003)
propose a two-level hierarchy for noun-modifier re-
lations, which has been widely used (Nakov and
Hearst, 2008; Nastase et al., 2006; Turney and
Littman, 2005; Turney, 2005). Others have used
classifications based on the requirements for a spe-
cific task, such as Information Extraction (Pantel
and Pennacchiotti, 2006) or biomedical applications
(Stephens et al., 2001).
We adopt the relation classification scheme of Be-
jar et al. (1991), which includes ten high-level cat-
egories (e.g., CAUSE-PURPOSE and SPACE-TIME).
Each category has between five and ten more re-
fined subcategories (e.g., CAUSE-PURPOSE includes
CAUSE:EFFECT and ACTION:GOAL), for a total of
79 distinct subcategories. Although these cate-
gories do not reflect all possible semantic rela-
tions, they greatly expand the coverage of rela-
tion types from those used in past relation-based
SemEval tasks (Girju et al., 2007; Hendrickx et
al., 2010), which used only seven and nine re-
lation types, respectively. Furthermore, the clas-
sification includes many of the fundamental rela-
tions, e.g., TAXONOMIC and PART:WHOLE, while
also including relations between a variety of parts
of speech and less common relations, such as REF-
ERENCE (e.g., SIGN:SIGNIFICANT) and NONAT-
TRIBUTE (e.g., AGENT:ATYPICAL ACTION). Using
such a large relation class inventory enables evalu-
ating the generality of an approach, while still mea-
suring performance on commonly used relations.
</bodyText>
<sectionHeader confidence="0.97591" genericHeader="method">
3 Task Data
</sectionHeader>
<bodyText confidence="0.997562888888889">
We constructed a new data set for the task, in which
word pairs are manually classified into relation cat-
egories. Word pairs within a category are manually
distinguished according to how well they represent
the category; that is, the degree to which they are
relationally similar to paradigmatic members of the
given semantic relation class. Paradigmatic mem-
bers of a class were taken from examples provided
by Bejar et al. (1991). Due to the large number of
</bodyText>
<figureCaption confidence="0.996719">
Figure 1: An example of the two questions for Phase 1.
</figureCaption>
<bodyText confidence="0.99998612">
annotations needed, we used Amazon Mechanical
Turk (MTurk),2 which is a popular choice in com-
putational linguistics for gathering large numbers of
human responses to linguistic questions (Snow et al.,
2008; Mohammad and Turney, 2010). We refer to
the MTurk workers as Turkers.
The data set was built in two phases. In the first
phase, Turkers were given three paradigmatic exam-
ples of a subcategory and asked to create new pairs
that instantiate the same relation as the paradigms.
In the second phase, people were asked to distin-
guish the new pairs from the first phase according to
the degree to which they are good representatives of
the given subcategory.
Phase 1 In the first phase, we built upon the
paradigmatic examples of Bejar et al. (1991), who
provided one to ten examples for each subcategory.
From these examples, we manually selected three
instances to use as seeds for generating new exam-
ples, adding examples when a subcategory had less
than three. The examples were selected to be bal-
anced across topic domains so as not to bias the
Turkers. For each subcategory, we manually created
a schematic representation of the relation for the ex-
amples. Table 1 gives three examples.
</bodyText>
<footnote confidence="0.807951">
2https://www.mturk.com/
</footnote>
<figureCaption confidence="0.747358">
Question 1: Consider the following word pairs: pil-
grim:shrine, hunter:quarry, assassin:victim, climber:peak.
</figureCaption>
<figure confidence="0.9305856">
What relation best describes these X:Y word pairs?
(1) “X worships/reveres Y ”
(2) “X seeks/desires/aims for Y ”
(3) “X harms/destroys Y ”
(4) “X uses/exploits/employs Y ”
</figure>
<figureCaption confidence="0.878934111111111">
Question 2: Consider the following word pairs: pil-
grim:shrine, hunter:quarry, assassin:victim, climber:peak.
These X:Y pairs share a relation, “X R Y ”. Give four ad-
ditional word pairs that illustrate the same relation, in the
same order (X on the left, Y on the right). Please do not
use phrases composed of two or more words in your ex-
amples (e.g., “racing car”). Please do not use names of
people, places, or things in your examples (e.g., “Europe”,
“Kleenex”).
</figureCaption>
<figure confidence="0.995585">
:
:
:
(4) :
</figure>
<page confidence="0.987311">
358
</page>
<bodyText confidence="0.999841604166667">
To gather new examples of each subcategory,
a two-part questionnaire was presented to Turk-
ers (see Figure 1). In the first part, Turkers were
shown the three paradigm word pairs for a sub-
category along with a list of four relation descrip-
tions (schematic representations of possible rela-
tions). One of the four schematic representations
accurately described the three paradigm pairs and
the other three schematics were distractors (con-
founding descriptions). Turkers were asked to se-
lect which of the four schematic representations best
matched the paradigms. The first part of the ques-
tionnaire serves as quality control by ensuring that
the Turker is capable of recognizing the relation. An
incorrect answer to the question is used to recog-
nize and eliminate confused or negligent responses,
which were approximately 7% of the responses.
In the second part of the Phase 1 questionnaire,
Turkers were shown the three prototypes again and
asked to generate four word pairs that expressed the
same relation. Turkers were directed to be mindful
of the order of the words in each pair, as reversed
orderings can have very different degrees of proto-
typicality in the case of directional relations.
The Turkers provided a total of 3160 additional
examples for the 79 subcategories, 2905 of which
were unique. We applied minor manual correction
to remove spelling errors, which reduced the total
number of examples to 2823. A median of 38 exam-
ples were found per subcategory with a maximum of
40 and minimum of 23. We note that Phase 1 gathers
both high and low quality examples of the relation,
which were all included to capture different degrees
of prototypicality.
We included an additional 395 pairs by randomly
sampling five instances of each subcategory and
creating a new pair from the reversed arguments,
i.e., adding pair Y :X to the subcategory contain-
ing X:Y . Adding reversals was inspired by an ob-
servation during Phase 1 that reversed pairs would
occasionally be added by the Turkers themselves.
We were curious to see what impact reversals would
have on Turker responses and on the output of au-
tomatic systems. Reversals should reveal order sen-
sitivity with a strongly directional relation, such as
PART:WHOLE, but also perhaps there is order sensi-
tivity with more symmetric relations, such as SYN-
ONYMY. Phase 1 produced a total of 3218 pairs.
</bodyText>
<table confidence="0.808086181818182">
Question 1: Consider the following word pairs: pil-
grim:shrine, hunter:quarry, assassin:victim, climber:peak.
What relation best describes these X:Y word pairs?
(1) “X worships/reveres Y ”
(2) “X seeks/desires/aims for Y ”
(3) “X harms/destroys Y ”
(4) “X uses/exploits/employs Y ”
Question 2: Consider the following word pairs: pil-
grim:shrine, hunter:quarry, assassin:victim, climber:peak.
These X:Y pairs share a relation, “X R Y ”. Now consider
the following word pairs:
</table>
<figure confidence="0.81502875">
(1) pig:mud
(2) politician:votes
(3) dog:bone
(4) bird:worm
</figure>
<bodyText confidence="0.5995452">
Which of the above numbered word pairs is the MOST illus-
trative example of the same relation “X R Y ”?
Which of the above numbered word pairs is the LEAST illus-
trative example of the same relation “X R Y ”?
Note: In some cases, a word pair might be in reverse order.
For example, tree:forest is in reverse order for the relation
“X is made from a collection of Y”. The correct order would
be forest:tree; a forest is made from a collection of trees.
You should treat reversed pairs as BAD examples of the given
relation.
</bodyText>
<figureCaption confidence="0.994205">
Figure 2: An example of the two questions for Phase 2.
</figureCaption>
<bodyText confidence="0.999961217391304">
Phase 2 In the second phase, the response pairs
from Phase 1 were ranked according to their pro-
totypicality. We opted to create a ranking using
MaxDiff questions (Louviere, 1991). MaxDiff is a
choice procedure consisting of a question about a
target concept and four or five alternatives. A partic-
ipant must choose both the best and worse answers
from the given alternatives.
MaxDiff is a strong alternative to creating a rank-
ing from standard rating scales, such as the Likert
scale, because it avoids scale biases. Furthermore
MaxDiff is more efficient than other choice proce-
dures such as pairwise comparison, because it does
not require comparing all pairs.
Like Phase 1, Phase 2 was performed using a two-
part questionnaire. The first question was identical
to that of Phase 1: four examples of the same re-
lation subcategory generated in Phase 1 were pre-
sented and the Turker was asked to select the cor-
rect relation from a list of four options. This first
question served as a quality control measure for en-
suring the Turker could properly identify the rela-
tion in question and it also served as a hint, guiding
</bodyText>
<page confidence="0.995727">
359
</page>
<bodyText confidence="0.999962714285714">
the Turker toward the intended understanding of the
shared relation underlying the three paradigms. In
the second part, the Turker selects the most and least
illustrative example of that relation from among
the four examples of pairs generated by Turkers in
Phase 1.
We aimed for five Turker responses for each
MaxDiff question but averaged 4.73 responses for
each MaxDiff question in a subcategory, with a
minimum of 3.45 responses per MaxDiff question.
Turkers answered a total of 48,846 questions over a
period of five months, of which 6,536 (13%) were
rejected due to a missing answer or an incorrect re-
sponse to the first question.
</bodyText>
<subsectionHeader confidence="0.998124">
3.1 Measuring Prototypicality
</subsectionHeader>
<bodyText confidence="0.9995149">
The MaxDiff responses were converted into the
prototypicality scores using a counting procedure
(Orme, 2009). For each word pair, the prototyp-
icality is scored as the percentage of times it is
chosen as most illustrative minus the percentage of
times it is chosen as least illustrative (see Figure 2).
While methods such as hierarchical Bayes models
can be used to compute a numerical rank from the
responses, we found the counting method to produce
very reasonable results.
</bodyText>
<subsectionHeader confidence="0.998581">
3.2 Data Sets
</subsectionHeader>
<bodyText confidence="0.9999605">
The 79 subcategories were divided into training
and testing segments. Ten subcategories were pro-
vided as training with both the Turkers’ MaxDiff
responses and the computed prototypicality ratings.
The ten training subcategories were randomly se-
lected. The remaining 69 subcategories were used
for testing. All data sets are now released on the task
website under the Creative Commons 3.0 license.3
Participants were given the list of all pairs gath-
ered in Phase 1 and the Phase 2 responses for the 10
training subcategories. Phase 2 responses for the 69
test categories were not made available. Participants
also had access to the set of questionnaire materials
provided to the Turkers, the full list of paradigmatic
examples provided by Bejar et al. (1991), and the
confounding schema relations from the initial ques-
tions in Phase 1 and Phase 2, which might serve as
negative training examples.
</bodyText>
<footnote confidence="0.692553">
3http://creativecommons.org/licenses/by/3.0/
</footnote>
<sectionHeader confidence="0.99417" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999185">
Systems are given examples of pairs from a single
category and asked to provide numeric ratings of the
degree of relational similarity for each pair relative
to the relation expressed in that category.
</bodyText>
<subsectionHeader confidence="0.994702">
4.1 Scoring
</subsectionHeader>
<bodyText confidence="0.999972666666667">
Spearman’s rank correlation coefficient, p, and a
MaxDiff score were used to evaluate the systems.
For Spearman’s p, the prototypicality rating of each
pair is used to build a ranking of all pairs in a sub-
category. Spearman’s p is then computed between
the pair rankings of a system and the gold standard
ranking. This evaluation abstracts away from com-
paring the numeric values so that only their relative
ordering in prototypicality is measured.
In the second scoring procedure, we measure the
accuracy of a system at answering the same set of
MaxDiff questions as answered by the Turkers in
Phase 2 (see Figure 2). Given the four word pairs,
the system selects the pair with the lowest numeri-
cal rating as least illustrative and the pair with the
highest numerical rating as most illustrative. Ties
in prototypicality are broken arbitrarily. Accuracy is
measured as the percentage of questions answered
correctly. An answer is considered correct when it
agrees with the majority of the Turkers. In some
cases, two answers may be considered correct. For
example, when five Turkers answer a given MaxD-
iff question, two Turkers might choose one pair as
the most illustrative and two other Turkers might
choose another pair as the most illustrative. In this
case, both pairs would count as correct choices for
the most illustrative pair.
</bodyText>
<subsectionHeader confidence="0.987823">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999684909090909">
We consider two baselines for evaluation: Random
and PMI. The Random baseline rates each pair in a
subcategory randomly. The expected Spearman cor-
relation for Random ratings is zero. The expected
MaxDiff score for Random ratings would be 25%
(because there are four word pairs to choose from
in Phase 2) if there were always a unique majority,
but it is actually about 31%, due to cases where two
pairs both get two votes from the Turkers.
Given a MaxDiff question, a Turker might select
the pair whose words are most strongly associated
</bodyText>
<page confidence="0.996277">
360
</page>
<table confidence="0.994070826086956">
Team Members System Description
Benem´erita Mireya T. Vidal, BUAP Each pair is represented as a vector over multiple features: lexical,
Universidad Darnes V. Ayala, intervening words, WordNet relations between the pair, and syntactic
Aut´onoma de Jose A.R. Ortiz, features such as part of speech and morphology. Prototypicality is
Puebla (M´exico) Azucena M. based on cosine similarity with the class’s pairs.
(BUAP) Rendon,
David Pinto, and
Saul L. Silverio
University of Texas at Bryan Rink and NB Unsupervised learning identifies intervening patterns between all word
Dallas (UTD) Sanda Harabagiu pairs. Each pattern is then ranked according to its subcategory
specificity by learning a generative model from patterns to word pairs.
Prototypicality ratings are based on confidence that the highest scoring
pattern found for a pair belongs to the subcategory.
SVM Intervening patterns are found using the same method as UTD-NB.
Word pairs are then represented as feature vectors of matching
patterns. An SVM classifier is trained using a subcategory’s pairs as
positive training data and all other pairs as negative. Prototypicality
ratings are based on SVM confidence of class inclusion.
University of Ted Pedersen V0 WordNet is used to build the set of concepts connected by WordNet
Minnesota, Duluth relations to the pairs’ words. Prototypicality is estimated using the
(Duluth) vector similarity of the concatenated glosses.
V1 Same procedure as V0, with one further expansion to related concepts.
V2 Same procedure as V0, with two further expansions to related concepts.
</table>
<tableCaption confidence="0.999264">
Table 2: Descriptions of the participating teams and systems.
</tableCaption>
<bodyText confidence="0.999494466666667">
as the most illustrative and the least associated as
the least illustrative. Therefore, we propose a sec-
ond baseline where pairs are rated according to their
Pointwise Mutual Information (PMI) (Church and
Hanks, 1990), which measures the statistical asso-
ciation between two words. For this baseline, the
prototypicality rating given to a word pair is simply
the PMI score for the pair. For two terms x and y,
PMI(x, y) is defined as 1o92 \p (X ,Y)) I where p(·)
denotes the probability of a term or pair of terms.
The PMI score was calculated using the method of
Turney (2001) on a corpus of approximately 50 bil-
lion tokens, indexed by the Wumpus search engine.4
To calculate p(x, y), we recorded all co-occurrences
of both terms within a ten-word window.
</bodyText>
<sectionHeader confidence="0.994853" genericHeader="method">
5 Systems
</sectionHeader>
<bodyText confidence="0.9995525">
Three teams submitted six systems for evaluation.
Table 2 summarizes the teams and systems. Two
teams (BUAP and UTD) based their approaches on
discovering relation-specific patterns for each cat-
egory, while the third team (Duluth) used vector
space comparisons of the glosses related to the pairs.
</bodyText>
<footnote confidence="0.564185">
4http://www.wumpus-search.org/
</footnote>
<bodyText confidence="0.999852583333333">
No single system was able to achieve superior per-
formance on all subcategories. Table 3 reports the
averages across all subcategories for Spearman’s p
and MaxDiff accuracy. Five systems were able to
perform above the Random baseline, while only one
system, UTD-NB, consistently performed above the
PMI baseline.
However, the average performance masks supe-
rior performance on individual subcategories. Ta-
ble 3 also reports the number of subcategories in
which a system obtained a statistically significant
Spearman’s p with the gold standard ranking. De-
spite the low average performance, most models
were able to obtain significant correlation in multi-
ple subcategories. Furthermore, the significant cor-
relations for different systems were not always ob-
tained in the same subcategories. Across all subcat-
egories, 43 had a significant correlation at p &lt; 0.05
and 27 at p &lt; 0.01. The broad coverage of signifi-
cantly correlated subcategories spanned by the com-
bination of all systems and the PMI baseline sug-
gests that high performance on this task may be pos-
sible, but that adapting to each of the specific rela-
tion types may be very beneficial.
</bodyText>
<page confidence="0.994932">
361
</page>
<table confidence="0.9997381">
Team System Spearman’s p # of Subcategories MaxDiff
p &lt; 0.05 p &lt; 0.01
BUAP BUAP 0.014 2 0 31.7
UTD NB 0.229 22 16 39.4
SVM 0.116 11 5 34.7
Duluth V0 0.050 9 3 32.4
V1 0.039 10 4 31.5
V2 0.038 7 3 31.1
Baselines Random 0.018 4 0 31.2
PMI 0.112 15 7 33.9
</table>
<tableCaption confidence="0.9809395">
Table 3: Average Spearman’s p and MaxDiff scores for all system across all 69 test subcategories. Columns 4 and 5
denote the number of subcategories with a Spearman’s p that is statistically significant at the noted level of confidence.
</tableCaption>
<table confidence="0.999943272727273">
Relation Class Random PMI BUAP UTD-NB UTD-SVM Duluth-V0 Duluth-V1 Duluth-V2
Class-Inclusion 0.057 0.221 0.064 0.233 0.093 0.045 0.178 0.168
Part-Whole 0.012 0.144 0.066 0.252 0.142 -0.061 -0.084 -0.054
Similar 0.026 0.094 -0.036 0.214 0.131 0.183 0.208 0.198
Contrast -0.049 0.032 0.000 0.206 0.162 0.142 0.120 0.051
Attribute 0.037 -0.032 -0.095 0.158 0.052 0.044 -0.003 0.008
Non-Attribute -0.070 0.191 0.009 0.098 0.094 0.079 0.066 0.074
Case Relations 0.090 0.168 -0.037 0.241 0.187 -0.011 -0.068 -0.115
Cause-Purpose -0.011 0.130 0.114 0.183 0.060 0.021 0.022 0.042
Space-Time 0.013 0.084 0.035 0.375 0.139 0.055 -0.004 0.040
Reference 0.142 0.125 -0.001 0.346 0.082 0.028 0.074 0.067
</table>
<tableCaption confidence="0.9845375">
Table 4: Average Spearman’s p correlation with the Turker rankings in each of the high-level relation categories, with
the highest average correlation for each subcategory shown in bold.
</tableCaption>
<sectionHeader confidence="0.99681" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999352702702703">
Sensitivity to Pair Association The PMI base-
line performed much better than anticipated, outper-
forming all systems but UTD-NB on many of the
subcategories, despite treating all relations as direc-
tionless. Performance was highest in subcategories
where the X:Y pair might reasonably be expected
to occur together, e.g., FUNCTIONAL or CONTRA-
DICTORY. However, PMI benefits from the design
of our task, which focuses on rating pairs within a
given subcategory. In a different task that mixed
pairs from a variety of subcategories, PMI would
perform poorly, because it would assign high scores
to pairs of strongly associated words, regardless of
whether they belong to a given subcategory.
Difficulty of Specific Subcategories Performance
across the high-level categories was highly varied
between approaches. The category-level summary
shown in Table 4 reveals high-level trends in diffi-
culty across all submitted systems. The submitted
systems performed best for subcategories under the
Similar category, while the systems performed worst
for Non-Attribute subcategories.
As a further possibility of explaining performance
differences between subcategories, we considered
the hypothesis that the difficulty of a subcategory is
inversely proportional to the range of prototypicality
scores, i.e., subcategories with restricted ranges are
more difficult. However, we found that the difficulty
was uncorrelated with both the size of the interval
spanned by prototypicality scores and the standard
deviation of the scores.
Sensitivity to Argument Reversal The direction-
ality of a relation can significantly impact the rated
prototypicality of a pair whose arguments have been
reversed. As an approximate measure of the ef-
fect on prototypicality when a pairs’ arguments are
reversed, we calculated the expected drop in rank
</bodyText>
<page confidence="0.991972">
362
</page>
<table confidence="0.9992543">
Spearman’s p
Team System No Reversals With Reversals
BUAP BUAP -0.003 0.014
UTD NB 0.190 0.229
SVM 0.104 0.116
Duluth V0 0.062 0.050
V1 0.040 0.039
V2 0.046 0.038
Baselines Random 0.004 0.018
PMI 0.143 0.112
</table>
<tableCaption confidence="0.9810305">
Table 5: Average pair ranking correlation for all subcate-
gories when reversed pairs are included and excluded.
</tableCaption>
<bodyText confidence="0.999519411764706">
between a pair and its reversed form. Based on
the Turker rankings, the SEQUENCE (e.g., preg-
nancy:birth) and FUNCTIONAL (e.g., weapon:knife)
subcategories exhibited the strongest sensitivity to
argument reversal, while ATTRIBUTE SIMILARITY
(e.g., rake:fork) and CONTRARY (e.g., happy:sad)
exhibited the least.
The inclusion of reversed pairs potentially adds
a small amount of noise to the relation identifica-
tion process for subcategories with directional rela-
tions. Two teams, BUAP and UTD, accounted for
relation directionality, while Duluth did not, which
resulted in the Duluth systems ranking reversed pairs
the same. Therefore, we conducted a post-hoc anal-
ysis of the impact of reversals by removing the re-
versed pairs from the computed prototypicality rank-
ings. Table 5 reports the resulting Spearman’s p.
With Spearman’s p, we can easily evaluate the im-
pact of the reversals, because we can delete a re-
versed pair without affecting anything else. For the
MaxDiff questions, if there is one reversal in a group
of four choices, then we need to delete the whole
MaxDiff question. Therefore we do not include the
MaxDiff score in Table 5.
Removing reversals decreased performance in the
three systems that were sensitive to pair order-
ing (BUAP, UTD-NB, and UTD-SVM), while only
marginally increasing performance in the three sys-
tems that ignored the ordering. The performance de-
crease in systems that use ordering suggests that the
reversed pairs are easily identified and ranked ap-
propriately low. As a further estimate of the models’
ability to correctly order reversals, we compared the
difference in a reversal’s rank for both a system’s
</bodyText>
<table confidence="0.9509584">
Team System RMSE
BUAP BUAP 256.07
UT Dallas NB 257.15
SVM 209.95
Baseline Random 227.25
</table>
<tableCaption confidence="0.999247">
Table 6: RMSE in estimating the difference in rank be-
</tableCaption>
<subsectionHeader confidence="0.686274">
tween a pair and its reversal in the gold standard.
</subsectionHeader>
<bodyText confidence="0.999986727272727">
ranking and the ranking computed from Turker Re-
sponses. Table 6 reports the Root Mean Squared
Error (RMSE) in ranking difference for the three
systems that took argument order into account. Al-
though not the best performing system, Table 6 indi-
cates that the UTD-SVM system was most able to
appropriately weight reversals’ prototypicality. In
contrast, the UTD-NB system often had many pairs
tied for the lowest rank, which either resulted in pair
and its reversal being tied or having a much smaller
rank difference, thereby increasing its RMSE.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999973238095238">
We have introduced a new task focused on rating the
degrees of prototypicality for word pairs sharing the
same relation. Participants first identify the relation
shared between example pairs and then rate the de-
gree to which each pair expresses that relation. As
a part of the task, we constructed a dataset of proto-
typicality ratings for 3218 word pairs in 79 different
relation categories.
Participating systems used combinations of
corpus-based, syntactic, and WordNet features, with
varying degrees of success. The task also included a
competitive baseline, PMI, which surpassed all but
one system. Several models obtained moderate per-
formance in select relation subcategories, but no one
approach succeeded in general, which introduces
much opportunity for future improvement. We also
hope that both the example pairs and their prototyp-
icality ratings will be a valuable data set for future
research in Linguistics as well as Cognitive Psychol-
ogy. All data sets for this task have been made pub-
licly available on the task website.
</bodyText>
<sectionHeader confidence="0.996518" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.8599255">
This research was supported by ONR grant
N000140810186.
</bodyText>
<page confidence="0.998981">
363
</page>
<sectionHeader confidence="0.995843" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999920423913043">
Isaac I. Bejar, Roger Chaffin, and Susan E. Embretson.
1991. Cognitive and Psychometric Analysis of Ana-
logical Problem Solving. Springer-Verlag.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Dair-
muid O´ S´eaghdha, Stan Szpakowicz, and Tony Veale.
2010. Semeval-2010 task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Proceedings of the 5th International Work-
shop on Semantic Evaluation (SemEval-2010), pages
39–44. Association for Computational Linguistics.
Michael J. Cafarella, Michele Banko, and Oren Etzioni.
2006. Relational web search. In WWW Conference.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.
Dedre Gentner. 1983. Structure-mapping: A theoretical
framework for analogy. Cognitive science, 7(2):155–
170.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 task 04: Classification of semantic re-
lations between nominals. In Proceedings of the
4th International Workshop on Semantic Evaluation
(SemEval-2007).
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. SemEval-2010 task 8: Multi-way classi-
fication of semantic relations between pairs of nom-
inals. In Proceedings of the 5th International Work-
shop on Semantic Evaluation (SemEval-2010), pages
33–38. Association for Computational Linguistics.
Keith J. Holyoak. 2012. Analogy and relational reason-
ing. In Oxford handbook of thinking and reasoning,
pages 234–259. Oxford University Press.
Jordan J. Louviere. 1991. Best-worst scaling: A model
for the largest difference judgments. Working Paper.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
mechanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL-HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, pages 26–34. Association for Com-
putational Linguistics.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proceedings ofACL, volume 8.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285–301. ACL Press Tilburg,, The Nether-
lands.
Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova,
and Stan Szpakowicz. 2006. Learning noun-modifier
semantic relations with corpus-based and wordnet-
based features. In Proceedings of AAAI, volume 21,
page 781.
Bryan Orme. 2009. Maxdiff analysis: Simple counting,
individual-level logit, and hb.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 113–120. Associa-
tion for Computational Linguistics.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254–263. Association for Computational Lin-
guistics.
M. Stephens, M. Palakal, S. Mukhopadhyay, R. Raje,
J. Mostafa, et al. 2001. Detecting gene relations from
medline abstracts. In Pacific Symposium on Biocom-
puting, volume 6, pages 483–495. Citeseer.
Peter D. Turney and Michael L Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1–3):251–278.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelfth European Conference on Machine Learn-
ing (ECML-2001), pages 491–502.
Peter D. Turney. 2005. Measuring semantic similarity
by latent relational analysis. In Proceedings of IJCAI,
pages 1136–1141.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Peter D. Turney. 2008. The latent relation mapping en-
gine: Algorithm and experiments. Journal ofArtificial
Intelligence Research, 33(1):615–655.
</reference>
<page confidence="0.998952">
364
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.177935">
<title confidence="0.998354">SemEval-2012 Task 2: Measuring Degrees of Relational Similarity</title>
<author confidence="0.999453">A David</author>
<affiliation confidence="0.997134">Department of Computer University of California, Los</affiliation>
<email confidence="0.999177">jurgens@cs.ucla.edu</email>
<author confidence="0.982635">D Peter</author>
<email confidence="0.612412">Emerging</email>
<affiliation confidence="0.905059">National Research Council</affiliation>
<email confidence="0.87876">peter.turney@nrc-cnrc.gc.ca</email>
<author confidence="0.553427">M Saif</author>
<email confidence="0.555831">Emerging</email>
<affiliation confidence="0.856962">National Research Council</affiliation>
<email confidence="0.955149">saif.mohammad@nrc-cnrc.gc.ca</email>
<author confidence="0.999598">J Keith</author>
<affiliation confidence="0.9989205">Department of University of California, Los</affiliation>
<email confidence="0.99868">holyoak@lifesci.ucla.edu</email>
<abstract confidence="0.9996034375">Up to now, work on semantic relations has focused on relation classification: recognizing whether a given instance (a word pair such as virus:flu) belongs to a specific relation class as However, instances of a single relation class may still have significant variability in how characteristic they are of that class. We present a new SemEval task based on identifying the degree of prototypicality for instances within a given class. As a part of the task, we have assembled the first dataset of graded relational similarity ratings across 79 relation categories. Three teams submitted six systems, which were evaluated using two methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Isaac I Bejar</author>
<author>Roger Chaffin</author>
<author>Susan E Embretson</author>
</authors>
<title>Cognitive and Psychometric Analysis of Analogical Problem Solving.</title>
<date>1991</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="8508" citStr="Bejar et al. (1991)" startWordPosition="1310" endWordPosition="1314">ent categorizations of semantic relations. The particular relation categorization is often driven by both the type of data and the intended application. Nastase and Szpakowicz (2003) propose a two-level hierarchy for noun-modifier relations, which has been widely used (Nakov and Hearst, 2008; Nastase et al., 2006; Turney and Littman, 2005; Turney, 2005). Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE-PURPOSE and SPACE-TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE-PURPOSE includes CAUSE:EFFECT and ACTION:GOAL), for a total of 79 distinct subcategories. Although these categories do not reflect all possible semantic relations, they greatly expand the coverage of relation types from those used in past relation-based SemEval tasks (Girju et al., 2007; Hendrickx et al., 2010), which used only seven and nine relation types, respectively. Furthermore, the classification includes many of the fundamental </context>
<context position="9935" citStr="Bejar et al. (1991)" startWordPosition="1532" endWordPosition="1535">NT:ATYPICAL ACTION). Using such a large relation class inventory enables evaluating the generality of an approach, while still measuring performance on commonly used relations. 3 Task Data We constructed a new data set for the task, in which word pairs are manually classified into relation categories. Word pairs within a category are manually distinguished according to how well they represent the category; that is, the degree to which they are relationally similar to paradigmatic members of the given semantic relation class. Paradigmatic members of a class were taken from examples provided by Bejar et al. (1991). Due to the large number of Figure 1: An example of the two questions for Phase 1. annotations needed, we used Amazon Mechanical Turk (MTurk),2 which is a popular choice in computational linguistics for gathering large numbers of human responses to linguistic questions (Snow et al., 2008; Mohammad and Turney, 2010). We refer to the MTurk workers as Turkers. The data set was built in two phases. In the first phase, Turkers were given three paradigmatic examples of a subcategory and asked to create new pairs that instantiate the same relation as the paradigms. In the second phase, people were a</context>
<context position="18495" citStr="Bejar et al. (1991)" startWordPosition="2957" endWordPosition="2960">axDiff responses and the computed prototypicality ratings. The ten training subcategories were randomly selected. The remaining 69 subcategories were used for testing. All data sets are now released on the task website under the Creative Commons 3.0 license.3 Participants were given the list of all pairs gathered in Phase 1 and the Phase 2 responses for the 10 training subcategories. Phase 2 responses for the 69 test categories were not made available. Participants also had access to the set of questionnaire materials provided to the Turkers, the full list of paradigmatic examples provided by Bejar et al. (1991), and the confounding schema relations from the initial questions in Phase 1 and Phase 2, which might serve as negative training examples. 3http://creativecommons.org/licenses/by/3.0/ 4 Evaluation Systems are given examples of pairs from a single category and asked to provide numeric ratings of the degree of relational similarity for each pair relative to the relation expressed in that category. 4.1 Scoring Spearman’s rank correlation coefficient, p, and a MaxDiff score were used to evaluate the systems. For Spearman’s p, the prototypicality rating of each pair is used to build a ranking of al</context>
</contexts>
<marker>Bejar, Chaffin, Embretson, 1991</marker>
<rawString>Isaac I. Bejar, Roger Chaffin, and Susan E. Embretson. 1991. Cognitive and Psychometric Analysis of Analogical Problem Solving. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Butnariu</author>
<author>Su Nam Kim</author>
<author>Preslav Nakov</author>
<author>Dairmuid O´ S´eaghdha</author>
<author>Stan Szpakowicz</author>
<author>Tony Veale</author>
</authors>
<title>Semeval-2010 task 9: The interpretation of noun compounds using paraphrasing verbs and prepositions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval-2010),</booktitle>
<pages>39--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Butnariu, Kim, Nakov, S´eaghdha, Szpakowicz, Veale, 2010</marker>
<rawString>Cristina Butnariu, Su Nam Kim, Preslav Nakov, Dairmuid O´ S´eaghdha, Stan Szpakowicz, and Tony Veale. 2010. Semeval-2010 task 9: The interpretation of noun compounds using paraphrasing verbs and prepositions. In Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval-2010), pages 39–44. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Cafarella</author>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>Relational web search.</title>
<date>2006</date>
<booktitle>In WWW Conference.</booktitle>
<contexts>
<context position="3559" citStr="Cafarella et al., 2006" startWordPosition="533" endWordPosition="536">ity judgments. Our proposed task is distinct from these past tasks in that we focus on measuring the degree of relational similarity.1 A graded measure of the degree of relational similarity would tell us that dog:bark is more similar to cat:meow than to floor:squeak. The discrete classification ENTITY:SOUND drops this information. Systems that are successful at identifying degrees of relation similarity can have a significant impact where an application must choose between multiple instances of the same relation. We illustrate this with two examples. First, consider a relational search task (Cafarella et al., 2006). A user of a relational search engine might give the query, 1Task details and data are available at https://sites.google.com/site/semeval2012task2/ . 356 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 356–364, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics Subcategory Relation name Relation schema Paradigms Responses 8(e) AGENT:GOAL “Y is the goal of X” pilgrim:shrine patient:health assassin:death runner:finish climber:peak astronaut:space 5(e) OBJECT:TYPICAL ACTION “an X will typically Y” glass:break ice:melt soldier:fight lio</context>
</contexts>
<marker>Cafarella, Banko, Etzioni, 2006</marker>
<rawString>Michael J. Cafarella, Michele Banko, and Oren Etzioni. 2006. Relational web search. In WWW Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="22656" citStr="Church and Hanks, 1990" startWordPosition="3618" endWordPosition="3621">ed to build the set of concepts connected by WordNet Minnesota, Duluth relations to the pairs’ words. Prototypicality is estimated using the (Duluth) vector similarity of the concatenated glosses. V1 Same procedure as V0, with one further expansion to related concepts. V2 Same procedure as V0, with two further expansions to related concepts. Table 2: Descriptions of the participating teams and systems. as the most illustrative and the least associated as the least illustrative. Therefore, we propose a second baseline where pairs are rated according to their Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which measures the statistical association between two words. For this baseline, the prototypicality rating given to a word pair is simply the PMI score for the pair. For two terms x and y, PMI(x, y) is defined as 1o92 \p (X ,Y)) I where p(·) denotes the probability of a term or pair of terms. The PMI score was calculated using the method of Turney (2001) on a corpus of approximately 50 billion tokens, indexed by the Wumpus search engine.4 To calculate p(x, y), we recorded all co-occurrences of both terms within a ten-word window. 5 Systems Three teams submitted six systems for evaluation. T</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dedre Gentner</author>
</authors>
<title>Structure-mapping: A theoretical framework for analogy.</title>
<date>1983</date>
<journal>Cognitive science,</journal>
<volume>7</volume>
<issue>2</issue>
<pages>170</pages>
<contexts>
<context position="5609" citStr="Gentner, 1983" startWordPosition="838" endWordPosition="839">scending order of the degree of relational similarity between X:car and a training set of prototypical examples of the relation PART:WHOLE. This would be analogous to how standard search engines rank documents or web pages in descending order of relevance to the user’s query. As a second example, consider the role of relational similarity in analogical transfer. When faced with a new situation, we look for an analogous situation in our past experience, and we use analogical inference to transfer information from the past experience (the source domain) to the new situation (the target domain) (Gentner, 1983; Holyoak, 2012). Analogy is based on relational similarity (Gentner, 1983; Turney, 2008). The degree of relational similarity in an analogy is indicative of the likelihood that transferred knowledge will be applicable in the target domain. For example, past experience tells us that a dog barks to send a signal to other creatures. If we transfer this knowledge to a new experience with a cat meowing, we can predict that the cat is sending a signal, and we can act appropriately with that prediction. If we transfer this knowledge to a new experience with a floor squeaking, we might predict that t</context>
</contexts>
<marker>Gentner, 1983</marker>
<rawString>Dedre Gentner. 1983. Structure-mapping: A theoretical framework for analogy. Cognitive science, 7(2):155– 170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>SemEval-2007 task 04: Classification of semantic relations between nominals.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluation (SemEval-2007).</booktitle>
<contexts>
<context position="2427" citStr="Girju et al., 2007" startWordPosition="357" endWordPosition="360">e characteristic of the relation. For example, many would agree that dog:bark and cat:meow are better prototypes of the ENTITY:SOUND relation than floor:squeak. Our task requires automatic systems to quantify the degree of prototypicality of a target pair by measuring the relational similarity between it and pairs that are given as defining examples of a particular relation. So far, most work in semantic relations has focused on differences between relation categories for classifying new relation instances. Past SemEval tasks that use relations have focused largely on discrete classification (Girju et al., 2007; Hendrickx et al., 2010) and paraphrasing the relations connecting noun compounds with a verb (Butnariu et al., 2010), which is also a form of discrete classification due to the lack of continuous degrees. However, there is some loss of information in any discrete classification of semantic relations. Furthermore, while some discrete classifiers provide a degree of confidence or probability for a relation classification, there is no a priori reason that such values would correspond to human prototypicality judgments. Our proposed task is distinct from these past tasks in that we focus on meas</context>
<context position="8955" citStr="Girju et al., 2007" startWordPosition="1378" endWordPosition="1381">ormation Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE-PURPOSE and SPACE-TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE-PURPOSE includes CAUSE:EFFECT and ACTION:GOAL), for a total of 79 distinct subcategories. Although these categories do not reflect all possible semantic relations, they greatly expand the coverage of relation types from those used in past relation-based SemEval tasks (Girju et al., 2007; Hendrickx et al., 2010), which used only seven and nine relation types, respectively. Furthermore, the classification includes many of the fundamental relations, e.g., TAXONOMIC and PART:WHOLE, while also including relations between a variety of parts of speech and less common relations, such as REFERENCE (e.g., SIGN:SIGNIFICANT) and NONATTRIBUTE (e.g., AGENT:ATYPICAL ACTION). Using such a large relation class inventory enables evaluating the generality of an approach, while still measuring performance on commonly used relations. 3 Task Data We constructed a new data set for the task, in whi</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. SemEval-2007 task 04: Classification of semantic relations between nominals. In Proceedings of the 4th International Workshop on Semantic Evaluation (SemEval-2007).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<title>SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval-2010),</booktitle>
<pages>33--38</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2010</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval-2010), pages 33–38. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith J Holyoak</author>
</authors>
<title>Analogy and relational reasoning. In Oxford handbook of thinking and reasoning,</title>
<date>2012</date>
<pages>234--259</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="5625" citStr="Holyoak, 2012" startWordPosition="840" endWordPosition="841">of the degree of relational similarity between X:car and a training set of prototypical examples of the relation PART:WHOLE. This would be analogous to how standard search engines rank documents or web pages in descending order of relevance to the user’s query. As a second example, consider the role of relational similarity in analogical transfer. When faced with a new situation, we look for an analogous situation in our past experience, and we use analogical inference to transfer information from the past experience (the source domain) to the new situation (the target domain) (Gentner, 1983; Holyoak, 2012). Analogy is based on relational similarity (Gentner, 1983; Turney, 2008). The degree of relational similarity in an analogy is indicative of the likelihood that transferred knowledge will be applicable in the target domain. For example, past experience tells us that a dog barks to send a signal to other creatures. If we transfer this knowledge to a new experience with a cat meowing, we can predict that the cat is sending a signal, and we can act appropriately with that prediction. If we transfer this knowledge to a new experience with a floor squeaking, we might predict that the floor is send</context>
</contexts>
<marker>Holyoak, 2012</marker>
<rawString>Keith J. Holyoak. 2012. Analogy and relational reasoning. In Oxford handbook of thinking and reasoning, pages 234–259. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan J Louviere</author>
</authors>
<title>Best-worst scaling: A model for the largest difference judgments. Working Paper.</title>
<date>1991</date>
<contexts>
<context position="15639" citStr="Louviere, 1991" startWordPosition="2485" endWordPosition="2486">rs is the LEAST illustrative example of the same relation “X R Y ”? Note: In some cases, a word pair might be in reverse order. For example, tree:forest is in reverse order for the relation “X is made from a collection of Y”. The correct order would be forest:tree; a forest is made from a collection of trees. You should treat reversed pairs as BAD examples of the given relation. Figure 2: An example of the two questions for Phase 2. Phase 2 In the second phase, the response pairs from Phase 1 were ranked according to their prototypicality. We opted to create a ranking using MaxDiff questions (Louviere, 1991). MaxDiff is a choice procedure consisting of a question about a target concept and four or five alternatives. A participant must choose both the best and worse answers from the given alternatives. MaxDiff is a strong alternative to creating a ranking from standard rating scales, such as the Likert scale, because it avoids scale biases. Furthermore MaxDiff is more efficient than other choice procedures such as pairwise comparison, because it does not require comparing all pairs. Like Phase 1, Phase 2 was performed using a twopart questionnaire. The first question was identical to that of Phase</context>
</contexts>
<marker>Louviere, 1991</marker>
<rawString>Jordan J. Louviere. 1991. Best-worst scaling: A model for the largest difference judgments. Working Paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>26--34</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10252" citStr="Mohammad and Turney, 2010" startWordPosition="1584" endWordPosition="1587">irs within a category are manually distinguished according to how well they represent the category; that is, the degree to which they are relationally similar to paradigmatic members of the given semantic relation class. Paradigmatic members of a class were taken from examples provided by Bejar et al. (1991). Due to the large number of Figure 1: An example of the two questions for Phase 1. annotations needed, we used Amazon Mechanical Turk (MTurk),2 which is a popular choice in computational linguistics for gathering large numbers of human responses to linguistic questions (Snow et al., 2008; Mohammad and Turney, 2010). We refer to the MTurk workers as Turkers. The data set was built in two phases. In the first phase, Turkers were given three paradigmatic examples of a subcategory and asked to create new pairs that instantiate the same relation as the paradigms. In the second phase, people were asked to distinguish the new pairs from the first phase according to the degree to which they are good representatives of the given subcategory. Phase 1 In the first phase, we built upon the paradigmatic examples of Bejar et al. (1991), who provided one to ten examples for each subcategory. From these examples, we ma</context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2010. Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon. In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 26–34. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti A Hearst</author>
</authors>
<title>Solving relational similarity problems using the web as a corpus.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<volume>8</volume>
<contexts>
<context position="8181" citStr="Nakov and Hearst, 2008" startWordPosition="1261" endWordPosition="1264">he paradigmatic examples and identify what relational or featural attributes best characterize that relation, and (2) to identify the relation of the given pair and rate how similar it is to that shared by the paradigmatic examples. 357 2.2 Relation Categories Researchers in psychology and linguistics have considered many different categorizations of semantic relations. The particular relation categorization is often driven by both the type of data and the intended application. Nastase and Szpakowicz (2003) propose a two-level hierarchy for noun-modifier relations, which has been widely used (Nakov and Hearst, 2008; Nastase et al., 2006; Turney and Littman, 2005; Turney, 2005). Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE-PURPOSE and SPACE-TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE-PURPOSE includes CAUSE:EFFECT and ACTION:GOAL), for a total of 79 distinct subcategories. Although these categori</context>
</contexts>
<marker>Nakov, Hearst, 2008</marker>
<rawString>Preslav Nakov and Marti A. Hearst. 2008. Solving relational similarity problems using the web as a corpus. In Proceedings ofACL, volume 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Exploring noun-modifier semantic relations.</title>
<date>2003</date>
<booktitle>In Fifth International Workshop on Computational Semantics (IWCS5),</booktitle>
<pages>285--301</pages>
<publisher>ACL Press</publisher>
<location>Tilburg,, The Netherlands.</location>
<contexts>
<context position="8071" citStr="Nastase and Szpakowicz (2003)" startWordPosition="1244" endWordPosition="1247">presentation, in order to estimate its degree of prototypicality. The challenges are (1) to infer the relation from the paradigmatic examples and identify what relational or featural attributes best characterize that relation, and (2) to identify the relation of the given pair and rate how similar it is to that shared by the paradigmatic examples. 357 2.2 Relation Categories Researchers in psychology and linguistics have considered many different categorizations of semantic relations. The particular relation categorization is often driven by both the type of data and the intended application. Nastase and Szpakowicz (2003) propose a two-level hierarchy for noun-modifier relations, which has been widely used (Nakov and Hearst, 2008; Nastase et al., 2006; Turney and Littman, 2005; Turney, 2005). Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE-PURPOSE and SPACE-TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE-PUR</context>
</contexts>
<marker>Nastase, Szpakowicz, 2003</marker>
<rawString>Vivi Nastase and Stan Szpakowicz. 2003. Exploring noun-modifier semantic relations. In Fifth International Workshop on Computational Semantics (IWCS5), pages 285–301. ACL Press Tilburg,, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Jelber Sayyad-Shirabad</author>
<author>Marina Sokolova</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Learning noun-modifier semantic relations with corpus-based and wordnetbased features.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<volume>21</volume>
<pages>781</pages>
<contexts>
<context position="8203" citStr="Nastase et al., 2006" startWordPosition="1265" endWordPosition="1268"> and identify what relational or featural attributes best characterize that relation, and (2) to identify the relation of the given pair and rate how similar it is to that shared by the paradigmatic examples. 357 2.2 Relation Categories Researchers in psychology and linguistics have considered many different categorizations of semantic relations. The particular relation categorization is often driven by both the type of data and the intended application. Nastase and Szpakowicz (2003) propose a two-level hierarchy for noun-modifier relations, which has been widely used (Nakov and Hearst, 2008; Nastase et al., 2006; Turney and Littman, 2005; Turney, 2005). Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE-PURPOSE and SPACE-TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE-PURPOSE includes CAUSE:EFFECT and ACTION:GOAL), for a total of 79 distinct subcategories. Although these categories do not reflect all </context>
</contexts>
<marker>Nastase, Sayyad-Shirabad, Sokolova, Szpakowicz, 2006</marker>
<rawString>Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova, and Stan Szpakowicz. 2006. Learning noun-modifier semantic relations with corpus-based and wordnetbased features. In Proceedings of AAAI, volume 21, page 781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Orme</author>
</authors>
<title>Maxdiff analysis: Simple counting, individual-level logit, and hb.</title>
<date>2009</date>
<contexts>
<context position="17357" citStr="Orme, 2009" startWordPosition="2773" endWordPosition="2774">rative example of that relation from among the four examples of pairs generated by Turkers in Phase 1. We aimed for five Turker responses for each MaxDiff question but averaged 4.73 responses for each MaxDiff question in a subcategory, with a minimum of 3.45 responses per MaxDiff question. Turkers answered a total of 48,846 questions over a period of five months, of which 6,536 (13%) were rejected due to a missing answer or an incorrect response to the first question. 3.1 Measuring Prototypicality The MaxDiff responses were converted into the prototypicality scores using a counting procedure (Orme, 2009). For each word pair, the prototypicality is scored as the percentage of times it is chosen as most illustrative minus the percentage of times it is chosen as least illustrative (see Figure 2). While methods such as hierarchical Bayes models can be used to compute a numerical rank from the responses, we found the counting method to produce very reasonable results. 3.2 Data Sets The 79 subcategories were divided into training and testing segments. Ten subcategories were provided as training with both the Turkers’ MaxDiff responses and the computed prototypicality ratings. The ten training subca</context>
</contexts>
<marker>Orme, 2009</marker>
<rawString>Bryan Orme. 2009. Maxdiff analysis: Simple counting, individual-level logit, and hb.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>113--120</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8389" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="1292" endWordPosition="1295">red by the paradigmatic examples. 357 2.2 Relation Categories Researchers in psychology and linguistics have considered many different categorizations of semantic relations. The particular relation categorization is often driven by both the type of data and the intended application. Nastase and Szpakowicz (2003) propose a two-level hierarchy for noun-modifier relations, which has been widely used (Nakov and Hearst, 2008; Nastase et al., 2006; Turney and Littman, 2005; Turney, 2005). Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE-PURPOSE and SPACE-TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE-PURPOSE includes CAUSE:EFFECT and ACTION:GOAL), for a total of 79 distinct subcategories. Although these categories do not reflect all possible semantic relations, they greatly expand the coverage of relation types from those used in past relation-based SemEval tasks (Girju et al., 2007; Hendrickx et al., 2010), which u</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 113–120. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>254--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 254–263. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stephens</author>
<author>M Palakal</author>
<author>S Mukhopadhyay</author>
<author>R Raje</author>
<author>J Mostafa</author>
</authors>
<title>Detecting gene relations from medline abstracts.</title>
<date>2001</date>
<booktitle>In Pacific Symposium on Biocomputing,</booktitle>
<volume>6</volume>
<pages>483--495</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="8440" citStr="Stephens et al., 2001" startWordPosition="1299" endWordPosition="1302">s Researchers in psychology and linguistics have considered many different categorizations of semantic relations. The particular relation categorization is often driven by both the type of data and the intended application. Nastase and Szpakowicz (2003) propose a two-level hierarchy for noun-modifier relations, which has been widely used (Nakov and Hearst, 2008; Nastase et al., 2006; Turney and Littman, 2005; Turney, 2005). Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE-PURPOSE and SPACE-TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE-PURPOSE includes CAUSE:EFFECT and ACTION:GOAL), for a total of 79 distinct subcategories. Although these categories do not reflect all possible semantic relations, they greatly expand the coverage of relation types from those used in past relation-based SemEval tasks (Girju et al., 2007; Hendrickx et al., 2010), which used only seven and nine relation types, respectivel</context>
</contexts>
<marker>Stephens, Palakal, Mukhopadhyay, Raje, Mostafa, 2001</marker>
<rawString>M. Stephens, M. Palakal, S. Mukhopadhyay, R. Raje, J. Mostafa, et al. 2001. Detecting gene relations from medline abstracts. In Pacific Symposium on Biocomputing, volume 6, pages 483–495. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Corpusbased learning of analogies and semantic relations.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="8229" citStr="Turney and Littman, 2005" startWordPosition="1269" endWordPosition="1272">ational or featural attributes best characterize that relation, and (2) to identify the relation of the given pair and rate how similar it is to that shared by the paradigmatic examples. 357 2.2 Relation Categories Researchers in psychology and linguistics have considered many different categorizations of semantic relations. The particular relation categorization is often driven by both the type of data and the intended application. Nastase and Szpakowicz (2003) propose a two-level hierarchy for noun-modifier relations, which has been widely used (Nakov and Hearst, 2008; Nastase et al., 2006; Turney and Littman, 2005; Turney, 2005). Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE-PURPOSE and SPACE-TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE-PURPOSE includes CAUSE:EFFECT and ACTION:GOAL), for a total of 79 distinct subcategories. Although these categories do not reflect all possible semantic relation</context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>Peter D. Turney and Michael L Littman. 2005. Corpusbased learning of analogies and semantic relations. Machine Learning, 60(1–3):251–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the Web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001),</booktitle>
<pages>491--502</pages>
<contexts>
<context position="23015" citStr="Turney (2001)" startWordPosition="3687" endWordPosition="3688">the participating teams and systems. as the most illustrative and the least associated as the least illustrative. Therefore, we propose a second baseline where pairs are rated according to their Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which measures the statistical association between two words. For this baseline, the prototypicality rating given to a word pair is simply the PMI score for the pair. For two terms x and y, PMI(x, y) is defined as 1o92 \p (X ,Y)) I where p(·) denotes the probability of a term or pair of terms. The PMI score was calculated using the method of Turney (2001) on a corpus of approximately 50 billion tokens, indexed by the Wumpus search engine.4 To calculate p(x, y), we recorded all co-occurrences of both terms within a ten-word window. 5 Systems Three teams submitted six systems for evaluation. Table 2 summarizes the teams and systems. Two teams (BUAP and UTD) based their approaches on discovering relation-specific patterns for each category, while the third team (Duluth) used vector space comparisons of the glosses related to the pairs. 4http://www.wumpus-search.org/ No single system was able to achieve superior performance on all subcategories. T</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), pages 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Measuring semantic similarity by latent relational analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1136--1141</pages>
<contexts>
<context position="8244" citStr="Turney, 2005" startWordPosition="1273" endWordPosition="1274">utes best characterize that relation, and (2) to identify the relation of the given pair and rate how similar it is to that shared by the paradigmatic examples. 357 2.2 Relation Categories Researchers in psychology and linguistics have considered many different categorizations of semantic relations. The particular relation categorization is often driven by both the type of data and the intended application. Nastase and Szpakowicz (2003) propose a two-level hierarchy for noun-modifier relations, which has been widely used (Nakov and Hearst, 2008; Nastase et al., 2006; Turney and Littman, 2005; Turney, 2005). Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE-PURPOSE and SPACE-TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE-PURPOSE includes CAUSE:EFFECT and ACTION:GOAL), for a total of 79 distinct subcategories. Although these categories do not reflect all possible semantic relations, they greatly</context>
</contexts>
<marker>Turney, 2005</marker>
<rawString>Peter D. Turney. 2005. Measuring semantic similarity by latent relational analysis. In Proceedings of IJCAI, pages 1136–1141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="1376" citStr="Turney, 2006" startWordPosition="192" endWordPosition="194">t variability in how characteristic they are of that class. We present a new SemEval task based on identifying the degree of prototypicality for instances within a given class. As a part of the task, we have assembled the first dataset of graded relational similarity ratings across 79 relation categories. Three teams submitted six systems, which were evaluated using two methods. 1 Introduction Relational similarity measures the degree of correspondence between two relations, where instance pairs that have high relational similarity are said to be analogous, i.e., to express the same relation (Turney, 2006). However, a class of analogous relations may still have significant variability in the degree of relational similarity of its members. Consider the four word pairs dog:bark, cat:meow, floor:squeak, and car:honk. We could say that these four X:Y pairs are all instances of the semantic relation ENTITY:SOUND; that is, X is an entity that characteristically makes the sound Y . Within a class of analogous pairs, certain pairs are more characteristic of the relation. For example, many would agree that dog:bark and cat:meow are better prototypes of the ENTITY:SOUND relation than floor:squeak. Our ta</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>The latent relation mapping engine: Algorithm and experiments.</title>
<date>2008</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="5698" citStr="Turney, 2008" startWordPosition="850" endWordPosition="851"> prototypical examples of the relation PART:WHOLE. This would be analogous to how standard search engines rank documents or web pages in descending order of relevance to the user’s query. As a second example, consider the role of relational similarity in analogical transfer. When faced with a new situation, we look for an analogous situation in our past experience, and we use analogical inference to transfer information from the past experience (the source domain) to the new situation (the target domain) (Gentner, 1983; Holyoak, 2012). Analogy is based on relational similarity (Gentner, 1983; Turney, 2008). The degree of relational similarity in an analogy is indicative of the likelihood that transferred knowledge will be applicable in the target domain. For example, past experience tells us that a dog barks to send a signal to other creatures. If we transfer this knowledge to a new experience with a cat meowing, we can predict that the cat is sending a signal, and we can act appropriately with that prediction. If we transfer this knowledge to a new experience with a floor squeaking, we might predict that the floor is sending a signal, which might lead us to act inappropriately. If we have a ch</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008. The latent relation mapping engine: Algorithm and experiments. Journal ofArtificial Intelligence Research, 33(1):615–655.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>