<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014458">
<title confidence="0.99699">
LCC-WSD: System Description for English Coarse Grained All Words Task
at SemEval 2007
</title>
<author confidence="0.957528">
Adrian Novischi, Munirathnam Srikanth and Andrew Bennett
</author>
<affiliation confidence="0.885046">
Language Computer Corp.
</affiliation>
<address confidence="0.842868">
Richardson, TX
</address>
<email confidence="0.998284">
adrian,srikanth,abennet @languagecomputer.com
</email>
<sectionHeader confidence="0.995627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999727">
This document describes the Word Sense Disam-
biguation system used by Language Computer Cor-
poration at English Coarse Grained All Word Task
at SemEval 2007. The system is based on two su-
pervised machine learning algorithms: Maximum
Entropy and Support Vector Machines. These algo-
rithms were trained on a corpus created from Sem-
Cor, Senseval 2 and 3 all words and lexical sample
corpora and Open Mind Word Expert 1.0 corpus.
We used topical, syntactic and semantic features.
Some semantic features were created using WordNet
glosses with semantic relations tagged manually and
automatically as part of eXtended WordNet project.
We also tried to create more training instances from
the disambiguated WordNet glosses found in XWN
project (XWN, 2003). For words for which we could
not build a sense classifier, we used First Sense in
WordNet as a back-off strategy in order to have cov-
erage of 100%. The precision and recall of the over-
all system is 81.446% placing it in the top 5 systems.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999839809523809">
The performance of a Word Sense Disambiguation
(WSD) system using a finite set of senses depends
greatly on the definition of the word senses. Fine
grained senses are hard to distinguish while coarse
grained senses tend to be more clear. Word Sense
Disambiguation is not a final goal, but it is an in-
termediary step used in other Natural Processing ap-
plications like detection of Semantic Relations, In-
formation Retrieval or Machine Translation. Word
Sense Disambiguation is not useful if it is not per-
formed with high accuracy (Sanderson, 1994). A
coarse grained set of sense gives the opportunity to
make more precise sense distinction and to make a
Word Sense Disambiguation system more useful to
other tasks.
Our goal at SemEval 2007 was to measure the per-
formance of known supervised machine learning al-
gorithm using coarse grained senses. The idea of us-
ing supervised machine learning for WSD is not new
and was used for example in (Ng and Lee, 1996).
We made experiments with two supervised methods:
Maximum Entropy (ME) and Support Vector Ma-
chines (SVM). These supervised algorithms were
used with topical, syntactic and semantic features.
We trained a classifier for each word using both su-
pervised algorithms. New features were added in
3 incremental steps. After an initial set of experi-
ments the algorithm performance was enhanced us-
ing a greedy feature selection algorithm similar to
one in (Mihalcea, 2002). In order to increase the
number of training instances, we tried to use the
disambiguated WordNet glosses from XWN project
(XWN, 2003). Combining other corpora with dis-
ambiguated glosses from XWN did not provide any
improvement so we used XWN as a fall back strat-
egy for 70 words that did not have any training ex-
amples in other corpora but XWN.
Section 2 describes the supervised methods used
by our WSD system, the pre-processing module and
the set of features. Section 3 presents the exper-
iments we performed and their results. Section 4
draws the conclusions.
</bodyText>
<page confidence="0.981173">
223
</page>
<bodyText confidence="0.748028">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 223–226,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.957451" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.998577582417583">
The system contains a preprocessing module used
before computing the values of the features needed
by the machine learning classifiers. The preprocess-
ing module perform the following steps:
Tokenization: using an in house text tokenizer
Named Entity Recognition: using an in house
system
Part of Speech Tagging: normally we use the
Brill tagger, but we took advantage of the part
of speech tags given in the test file
WordNet look-up to check if the word exists
in WordNet and to get its lemma, possible part
of speech for that lemma and if the word has
a single sense or not. For SemEval English
Coarse All Words task we took advantage by
the lemma provided in the test file.
Compound concept detection: using a classifier
based on WordNet
Syntactic Parsing: using an in-house imple-
mentation of Collin’s parser (Glaysher and
Moldovan, 2006)
The Maximum Entropy classifier is a C++ imple-
mentation found on web (Le, 2006). The classifier
was adapted to accept symbolic features for classifi-
cation tasks in Natural Language Processing.
For training SVM classifiers we used LIBSVM
package (Chang and Lin, 2001). Each symbolic fea-
ture can have a single value from a finite set of val-
ues or can be assigned a subset of values from the set
of all possible values. For each value we created a
mapping between the feature value and a dimension
in the N-dimensional classification space and we as-
signed the number 1.0 to that dimension if the fea-
ture had the corresponding value or 0.0 otherwise.
We first performed experiments with our existing
set of features used at Senseval 3 All Words task. We
call this set . Then we made three incremental
changes to improve the performance.
The initial set contains the following features:
current word form (CRT WORD) and part of speech
(CRT POS), contextual features (CTX WORD) in
a window (-3,3) words, collocations in a window
of (-3,3) words (COL WORD), keywords (KEY-
WORDS) and bigrams (BIGRAMS) in a window of
(-3,3) sentences, verb mode (VERB MODE) which
can take 4 values: ACTIVE, INFINITIVE, PAST,
GERUND, verb voice (VERB VOICE) which can
take 2 values ACTIVE, PASSIVE, the parent of the
current verb in the parse tree (CRT PARENT) (ex:
VP, NP), the first ancestor that is not VP in the parse
tree (RAND PARENT) (like S, NP, PP, SBAR) and
a boolean flag indicating if the current verb belongs
to the main clause or not (MAIN CLAUSE).
We added new features to the initial set. We call
this set .
The lemmas of the contextual words in the win-
dow of (-3, 3) words around the target word
(CTX LEMMA).
Collocations formed with the lemma of sur-
rounding words in a window of (-3, 3)
(COL LEMMA)
The parent of the contextual words in the parse
tree in the window of (-3, 3) words around tar-
get word.
Collocations formed with the parents of the sur-
rounding words in the window (-3, 3) words
around the target word (COL PARENT).
Occurrences in the current sentence of the
words that are linked to the current word with
a semantic relation of AGENT or THEME in
WordNet 2.0 glosses (XWN LEMMA).
We used files from XWN project (XWN, 2003)
containing WordNet 2.0 glosses that were sense
disambiguated and tagged with semantic rela-
tions both manually and automatically. For
each word to be disambiguated we created a
signature consisting of the set of words that
are linked with a semantic relation of THEME
or AGENT in all WordNet glosses. For every
word in this set we created a feature showing if
that word appears in the current sentence con-
taining the target word.
Then we added a new feature consisting of all
the named entities in a window of (-5,5) sentences
around the target word. We called this feature
NAMED ENTITIES. We created the feature set
by adding this new feature to .
In the end we applied a greedy feature selection
algorithm to features in inspired by (Mihal-
cea, 2002). Because feature selection was running
very slow, the feature selection algorithm was run
</bodyText>
<page confidence="0.986834">
224
</page>
<table confidence="0.9955706">
CTX WORD 1 CTX WORD -2 CTX LEMMA 1 COL POS -2 0
CTX POS 1 CTX WORD -1 CTX LEMMA 2 COL LEMMA 0 1
CTX WORD 2 COL PARENT -3 -1 CTX LEMMA 3 COL PARENT -2 2
CRT WORD COL PARENT -3 2 NAMED ENTITIES CTX POS 3
CTX WORD -3 CTX WORD 3 COL PARENT -1 1 COL WORD -1 1
</table>
<tableCaption confidence="0.7271415">
Table 1: The feature set obtained from the features most selected by the greedy selection algorithm
applied to all the words in Senseval 2
</tableCaption>
<bodyText confidence="0.9989878">
only for words in Senseval 2 English lexical sample
task and the top 20 features appearing the most often
(at least 5 times) in the selected feature set for each
word were used to create feature set presented
in table 1.
</bodyText>
<sectionHeader confidence="0.995853" genericHeader="evaluation">
3 Experiments and results
</sectionHeader>
<bodyText confidence="0.999677">
For SemEval 2007 we performed several experi-
ments: we tested ME and SVM classifiers on the
4 feature sets described in the previous section and
then we tried to improve the performance using dis-
ambiguated glosses from XWN project. Each set of
experiments together with the final submission is de-
scribed in detail below.
</bodyText>
<subsectionHeader confidence="0.997315">
3.1 Experiments with different feature sets
</subsectionHeader>
<bodyText confidence="0.967111714285714">
Initially we made experiments with the set of fea-
tures used at Senseval 3 All Words task. For training
the ME and SVM classifiers, we used a combined
corpus made from SemCor, Senseval 3 All Words
corpus, Senseval 3 Lexical Sample testing and train-
ing corpora and Senseval 2 Lexical sample train-
ing corpus. For testing we used Senseval 2 Lexi-
</bodyText>
<table confidence="0.812791">
cal Sample corpus. We made 3 experiments for the
first three feature sets ,,
Algorithm
ME 76.03% 75.86% 76.03% 77.56%
SVM 73.30% 71.36% 71.46% 71.90%
</table>
<tableCaption confidence="0.9589295">
Table 2: The precision of ME and SVM classifiers
using 4 sets of features.
</tableCaption>
<bodyText confidence="0.963498666666667">
After the first 3 experiments we noticed that both
ME and SVM classifiers had good results using the
first set of features . This seemed odd since we
</bodyText>
<table confidence="0.9931385">
Corpus Precision
SemCor 79.61%
XWN 57.21%
SemCor+XWN 79.44%
</table>
<tableCaption confidence="0.9871955">
Table 3: The precision using SemCor and disam-
biguated glosses from XWN project
</tableCaption>
<bodyText confidence="0.983648941176471">
expected an increase in performance with the addi-
tional features. This led us to the idea that not all
the features are useful for all words. So we created a
greedy feature selection algorithm based on the per-
formance of the SVM classifier (Mihalcea, 2002).
The feature selection algorithm starts with an empty
set of features , and iteratively adds one feature
from the set of unused features . Initially the set
contains all the features. The algorithm iterates
as long as the overall performance increase. At each
step the algorithm adds tentatively one feature from
the set to the existing feature list and measures
the performance of the classifier on a 10 fold cross
validation on the training corpus. The feature pro-
viding the greatest increase in performance is finally
added to and removed from .
The feature selection algorithm turned out to be
very slow, so we could not use it to train all the
words. Therefore we used it to train only the words
from Senseval 2 Lexical Sample task and then we
computed a global set of features by selecting the
first 20 features that were selected the most (at least
5 times).
This list of features was named . Table 2 that
SVM classifier with did not get a better per-
formance than while ME surprisingly did get
1.53% increase in performance. Given the higher
precision of ME classifier, it was selected for creat-
ing the submission file.
. Both algo-
rithms attempted to disambiguate all the words (cov-
erage=100%) so the precision is equal with recall.
The precision of each algorithm on each feature set
is presented in table 2.
</bodyText>
<page confidence="0.995578">
225
</page>
<subsectionHeader confidence="0.9160445">
3.2 Experiments using disambiguated glosses
from XWN project
</subsectionHeader>
<bodyText confidence="0.9999063">
The ME classifier works well for words with enough
training examples. However we found many words
for which the number of training examples was too
small. We tried to increase the number of training
examples using the disambiguated WordNet glosses
from XWN project. Not all the senses in the dis-
ambiguated glosses were assigned manually and the
text of the glosses is different than normal running
text. However we were curious if we could im-
prove the overall performance by adding more train-
ing examples. We made 3 experiments showed in
table 3. For all three experiments we used Sense-
val 2 English All Words corpus for testing. On the
first experiment we used SemCor for training, on the
second we used disambiguated glosses from XWN
project and on the third we used both. XWN did not
bring an improvement to the overall precision, so we
decided to use XWN as a fall back strategy only for
70 words that did not have training examples is other
corpora.
</bodyText>
<subsectionHeader confidence="0.984792">
3.3 Final Submission
</subsectionHeader>
<bodyText confidence="0.997621333333333">
For final submission we used trained ME models
using feature set for 852 words, representing
1715 instances using SemCor, Senseval 2 and 3
English All Words and Lexical Sample testing and
training and OMWE 1.0. For 50 words represent-
ing 70 instances, we used disambiguated WordNet
glosses from XWN project to train ME classifiers
using feature set . For the rest of 484 words for
which we could not find training examples we used
the First Sense in WordNet strategy. The submitted
answer had a 100% coverage and a 81.446% preci-
sion presented in table 4.
</bodyText>
<table confidence="0.5891095">
LCC-WSD 81.446%
Best submission 83.208%
</table>
<tableCaption confidence="0.907239">
Table 4: The LCC-WSD and the best submission at
SemEval 2007 Coarse All Words Task
</tableCaption>
<sectionHeader confidence="0.999316" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999928357142857">
LCC-WSD team used two supervised approaches
for performing experiments using coarse grained
senses: Maximum Entropy and Support Vector Ma-
chines. We used 4 feature sets: the first one was the
feature set used in Senseval 3 and next two repre-
senting incremental additions. The fourth feature set
represents a global set of features obtained from the
individual feature sets for each word resulted from
the greedy feature selection algorithm used to im-
prove the performance of SVM classifiers. In addi-
tion we used disambiguated WordNet glosses from
XWN to measure the improvement made by adding
additional training examples. The submitted answer
has a coverage of 100% and a precision of 81.446%.
</bodyText>
<sectionHeader confidence="0.999187" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996069375">
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Elliot Glaysher and Dan I. Moldovan. 2006. Speeding up
full syntactic parsing by leveraging partial parsing de-
cisions. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting ofthe Association for Computational Linguis-
tics, pages 295–300, Sydney, Australia. Association
for Computational Linguistics.
Zhang Le, 2006. Maximum Entropy Modeling
Toolkit for Python and C++. Software avail-
able at http://homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html.
Rada Mihalcea. 2002. Instance based learning with au-
tomatic feature selection applied to word sense dis-
ambiguation. In Proceedings of the 19th Interna-
tional Conference on Computational Linguistics COL-
ING 2002, Taiwan.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: an exemplar-based approach. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 40–47, Morristown, NJ,
USA. Association for Computational Linguistics.
Mark Sanderson. 1994. Word sense disambiguation and
information retrieval. In Proceedings of SIGIR-94,
17th ACM International Conference on Research and
Development in Information Retrieval, pages 49–57,
Dublin, IE.
XWN, 2003. eXtended WordNet. Software available at
http://xwn.hlt.utdallas.edu.
</reference>
<page confidence="0.998841">
226
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.743215">
<title confidence="0.9284485">LCC-WSD: System Description for English Coarse Grained All Words Task at SemEval 2007</title>
<author confidence="0.997222">Adrian Novischi</author>
<author confidence="0.997222">Munirathnam Srikanth</author>
<author confidence="0.997222">Andrew Bennett</author>
<affiliation confidence="0.998541">Language Computer Corp.</affiliation>
<address confidence="0.893982">Richardson, TX</address>
<email confidence="0.984012">adrian,srikanth,abennet@languagecomputer.com</email>
<abstract confidence="0.998433238095238">This document describes the Word Sense Disambiguation system used by Language Computer Corporation at English Coarse Grained All Word Task at SemEval 2007. The system is based on two supervised machine learning algorithms: Maximum Entropy and Support Vector Machines. These algorithms were trained on a corpus created from Sem- Cor, Senseval 2 and 3 all words and lexical sample corpora and Open Mind Word Expert 1.0 corpus. We used topical, syntactic and semantic features. Some semantic features were created using WordNet glosses with semantic relations tagged manually and automatically as part of eXtended WordNet project. We also tried to create more training instances from the disambiguated WordNet glosses found in XWN project (XWN, 2003). For words for which we could not build a sense classifier, we used First Sense in WordNet as a back-off strategy in order to have coverage of 100%. The precision and recall of the overall system is 81.446% placing it in the top 5 systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="4524" citStr="Chang and Lin, 2001" startWordPosition="730" endWordPosition="733"> lemma, possible part of speech for that lemma and if the word has a single sense or not. For SemEval English Coarse All Words task we took advantage by the lemma provided in the test file. Compound concept detection: using a classifier based on WordNet Syntactic Parsing: using an in-house implementation of Collin’s parser (Glaysher and Moldovan, 2006) The Maximum Entropy classifier is a C++ implementation found on web (Le, 2006). The classifier was adapted to accept symbolic features for classification tasks in Natural Language Processing. For training SVM classifiers we used LIBSVM package (Chang and Lin, 2001). Each symbolic feature can have a single value from a finite set of values or can be assigned a subset of values from the set of all possible values. For each value we created a mapping between the feature value and a dimension in the N-dimensional classification space and we assigned the number 1.0 to that dimension if the feature had the corresponding value or 0.0 otherwise. We first performed experiments with our existing set of features used at Senseval 3 All Words task. We call this set . Then we made three incremental changes to improve the performance. The initial set contains the foll</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elliot Glaysher</author>
<author>Dan I Moldovan</author>
</authors>
<title>Speeding up full syntactic parsing by leveraging partial parsing decisions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>295--300</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="4258" citStr="Glaysher and Moldovan, 2006" startWordPosition="688" endWordPosition="691">in house text tokenizer Named Entity Recognition: using an in house system Part of Speech Tagging: normally we use the Brill tagger, but we took advantage of the part of speech tags given in the test file WordNet look-up to check if the word exists in WordNet and to get its lemma, possible part of speech for that lemma and if the word has a single sense or not. For SemEval English Coarse All Words task we took advantage by the lemma provided in the test file. Compound concept detection: using a classifier based on WordNet Syntactic Parsing: using an in-house implementation of Collin’s parser (Glaysher and Moldovan, 2006) The Maximum Entropy classifier is a C++ implementation found on web (Le, 2006). The classifier was adapted to accept symbolic features for classification tasks in Natural Language Processing. For training SVM classifiers we used LIBSVM package (Chang and Lin, 2001). Each symbolic feature can have a single value from a finite set of values or can be assigned a subset of values from the set of all possible values. For each value we created a mapping between the feature value and a dimension in the N-dimensional classification space and we assigned the number 1.0 to that dimension if the feature</context>
</contexts>
<marker>Glaysher, Moldovan, 2006</marker>
<rawString>Elliot Glaysher and Dan I. Moldovan. 2006. Speeding up full syntactic parsing by leveraging partial parsing decisions. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pages 295–300, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhang Le</author>
</authors>
<date>2006</date>
<booktitle>Maximum Entropy Modeling Toolkit for Python and C++. Software available at http://homepages.inf.ed.ac.uk/s0450736/ maxent toolkit.html.</booktitle>
<contexts>
<context position="4337" citStr="Le, 2006" startWordPosition="704" endWordPosition="705"> normally we use the Brill tagger, but we took advantage of the part of speech tags given in the test file WordNet look-up to check if the word exists in WordNet and to get its lemma, possible part of speech for that lemma and if the word has a single sense or not. For SemEval English Coarse All Words task we took advantage by the lemma provided in the test file. Compound concept detection: using a classifier based on WordNet Syntactic Parsing: using an in-house implementation of Collin’s parser (Glaysher and Moldovan, 2006) The Maximum Entropy classifier is a C++ implementation found on web (Le, 2006). The classifier was adapted to accept symbolic features for classification tasks in Natural Language Processing. For training SVM classifiers we used LIBSVM package (Chang and Lin, 2001). Each symbolic feature can have a single value from a finite set of values or can be assigned a subset of values from the set of all possible values. For each value we created a mapping between the feature value and a dimension in the N-dimensional classification space and we assigned the number 1.0 to that dimension if the feature had the corresponding value or 0.0 otherwise. We first performed experiments w</context>
</contexts>
<marker>Le, 2006</marker>
<rawString>Zhang Le, 2006. Maximum Entropy Modeling Toolkit for Python and C++. Software available at http://homepages.inf.ed.ac.uk/s0450736/ maxent toolkit.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Instance based learning with automatic feature selection applied to word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics COLING</booktitle>
<contexts>
<context position="2658" citStr="Mihalcea, 2002" startWordPosition="428" endWordPosition="429"> algorithm using coarse grained senses. The idea of using supervised machine learning for WSD is not new and was used for example in (Ng and Lee, 1996). We made experiments with two supervised methods: Maximum Entropy (ME) and Support Vector Machines (SVM). These supervised algorithms were used with topical, syntactic and semantic features. We trained a classifier for each word using both supervised algorithms. New features were added in 3 incremental steps. After an initial set of experiments the algorithm performance was enhanced using a greedy feature selection algorithm similar to one in (Mihalcea, 2002). In order to increase the number of training instances, we tried to use the disambiguated WordNet glosses from XWN project (XWN, 2003). Combining other corpora with disambiguated glosses from XWN did not provide any improvement so we used XWN as a fall back strategy for 70 words that did not have any training examples in other corpora but XWN. Section 2 describes the supervised methods used by our WSD system, the pre-processing module and the set of features. Section 3 presents the experiments we performed and their results. Section 4 draws the conclusions. 223 Proceedings of the 4th Internat</context>
<context position="7236" citStr="Mihalcea, 2002" startWordPosition="1218" endWordPosition="1220">ch word to be disambiguated we created a signature consisting of the set of words that are linked with a semantic relation of THEME or AGENT in all WordNet glosses. For every word in this set we created a feature showing if that word appears in the current sentence containing the target word. Then we added a new feature consisting of all the named entities in a window of (-5,5) sentences around the target word. We called this feature NAMED ENTITIES. We created the feature set by adding this new feature to . In the end we applied a greedy feature selection algorithm to features in inspired by (Mihalcea, 2002). Because feature selection was running very slow, the feature selection algorithm was run 224 CTX WORD 1 CTX WORD -2 CTX LEMMA 1 COL POS -2 0 CTX POS 1 CTX WORD -1 CTX LEMMA 2 COL LEMMA 0 1 CTX WORD 2 COL PARENT -3 -1 CTX LEMMA 3 COL PARENT -2 2 CRT WORD COL PARENT -3 2 NAMED ENTITIES CTX POS 3 CTX WORD -3 CTX WORD 3 COL PARENT -1 1 COL WORD -1 1 Table 1: The feature set obtained from the features most selected by the greedy selection algorithm applied to all the words in Senseval 2 only for words in Senseval 2 English lexical sample task and the top 20 features appearing the most often (at l</context>
<context position="9447" citStr="Mihalcea, 2002" startWordPosition="1621" endWordPosition="1622"> 2: The precision of ME and SVM classifiers using 4 sets of features. After the first 3 experiments we noticed that both ME and SVM classifiers had good results using the first set of features . This seemed odd since we Corpus Precision SemCor 79.61% XWN 57.21% SemCor+XWN 79.44% Table 3: The precision using SemCor and disambiguated glosses from XWN project expected an increase in performance with the additional features. This led us to the idea that not all the features are useful for all words. So we created a greedy feature selection algorithm based on the performance of the SVM classifier (Mihalcea, 2002). The feature selection algorithm starts with an empty set of features , and iteratively adds one feature from the set of unused features . Initially the set contains all the features. The algorithm iterates as long as the overall performance increase. At each step the algorithm adds tentatively one feature from the set to the existing feature list and measures the performance of the classifier on a 10 fold cross validation on the training corpus. The feature providing the greatest increase in performance is finally added to and removed from . The feature selection algorithm turned out to be v</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>Rada Mihalcea. 2002. Instance based learning with automatic feature selection applied to word sense disambiguation. In Proceedings of the 19th International Conference on Computational Linguistics COLING 2002, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2194" citStr="Ng and Lee, 1996" startWordPosition="354" endWordPosition="357">rocessing applications like detection of Semantic Relations, Information Retrieval or Machine Translation. Word Sense Disambiguation is not useful if it is not performed with high accuracy (Sanderson, 1994). A coarse grained set of sense gives the opportunity to make more precise sense distinction and to make a Word Sense Disambiguation system more useful to other tasks. Our goal at SemEval 2007 was to measure the performance of known supervised machine learning algorithm using coarse grained senses. The idea of using supervised machine learning for WSD is not new and was used for example in (Ng and Lee, 1996). We made experiments with two supervised methods: Maximum Entropy (ME) and Support Vector Machines (SVM). These supervised algorithms were used with topical, syntactic and semantic features. We trained a classifier for each word using both supervised algorithms. New features were added in 3 incremental steps. After an initial set of experiments the algorithm performance was enhanced using a greedy feature selection algorithm similar to one in (Mihalcea, 2002). In order to increase the number of training instances, we tried to use the disambiguated WordNet glosses from XWN project (XWN, 2003).</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 40–47, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Sanderson</author>
</authors>
<title>Word sense disambiguation and information retrieval.</title>
<date>1994</date>
<booktitle>In Proceedings of SIGIR-94, 17th ACM International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>49--57</pages>
<location>Dublin, IE.</location>
<contexts>
<context position="1783" citStr="Sanderson, 1994" startWordPosition="283" endWordPosition="284">rall system is 81.446% placing it in the top 5 systems. 1 Introduction The performance of a Word Sense Disambiguation (WSD) system using a finite set of senses depends greatly on the definition of the word senses. Fine grained senses are hard to distinguish while coarse grained senses tend to be more clear. Word Sense Disambiguation is not a final goal, but it is an intermediary step used in other Natural Processing applications like detection of Semantic Relations, Information Retrieval or Machine Translation. Word Sense Disambiguation is not useful if it is not performed with high accuracy (Sanderson, 1994). A coarse grained set of sense gives the opportunity to make more precise sense distinction and to make a Word Sense Disambiguation system more useful to other tasks. Our goal at SemEval 2007 was to measure the performance of known supervised machine learning algorithm using coarse grained senses. The idea of using supervised machine learning for WSD is not new and was used for example in (Ng and Lee, 1996). We made experiments with two supervised methods: Maximum Entropy (ME) and Support Vector Machines (SVM). These supervised algorithms were used with topical, syntactic and semantic feature</context>
</contexts>
<marker>Sanderson, 1994</marker>
<rawString>Mark Sanderson. 1994. Word sense disambiguation and information retrieval. In Proceedings of SIGIR-94, 17th ACM International Conference on Research and Development in Information Retrieval, pages 49–57, Dublin, IE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>XWN</author>
</authors>
<title>eXtended WordNet. Software available at http://xwn.hlt.utdallas.edu.</title>
<date>2003</date>
<contexts>
<context position="982" citStr="XWN, 2003" startWordPosition="144" endWordPosition="145">ask at SemEval 2007. The system is based on two supervised machine learning algorithms: Maximum Entropy and Support Vector Machines. These algorithms were trained on a corpus created from SemCor, Senseval 2 and 3 all words and lexical sample corpora and Open Mind Word Expert 1.0 corpus. We used topical, syntactic and semantic features. Some semantic features were created using WordNet glosses with semantic relations tagged manually and automatically as part of eXtended WordNet project. We also tried to create more training instances from the disambiguated WordNet glosses found in XWN project (XWN, 2003). For words for which we could not build a sense classifier, we used First Sense in WordNet as a back-off strategy in order to have coverage of 100%. The precision and recall of the overall system is 81.446% placing it in the top 5 systems. 1 Introduction The performance of a Word Sense Disambiguation (WSD) system using a finite set of senses depends greatly on the definition of the word senses. Fine grained senses are hard to distinguish while coarse grained senses tend to be more clear. Word Sense Disambiguation is not a final goal, but it is an intermediary step used in other Natural Proces</context>
<context position="2793" citStr="XWN, 2003" startWordPosition="450" endWordPosition="451"> Lee, 1996). We made experiments with two supervised methods: Maximum Entropy (ME) and Support Vector Machines (SVM). These supervised algorithms were used with topical, syntactic and semantic features. We trained a classifier for each word using both supervised algorithms. New features were added in 3 incremental steps. After an initial set of experiments the algorithm performance was enhanced using a greedy feature selection algorithm similar to one in (Mihalcea, 2002). In order to increase the number of training instances, we tried to use the disambiguated WordNet glosses from XWN project (XWN, 2003). Combining other corpora with disambiguated glosses from XWN did not provide any improvement so we used XWN as a fall back strategy for 70 words that did not have any training examples in other corpora but XWN. Section 2 describes the supervised methods used by our WSD system, the pre-processing module and the set of features. Section 3 presents the experiments we performed and their results. Section 4 draws the conclusions. 223 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 223–226, Prague, June 2007. c�2007 Association for Computational Linguisti</context>
<context position="6485" citStr="XWN, 2003" startWordPosition="1088" endWordPosition="1089">ntextual words in the window of (-3, 3) words around the target word (CTX LEMMA). Collocations formed with the lemma of surrounding words in a window of (-3, 3) (COL LEMMA) The parent of the contextual words in the parse tree in the window of (-3, 3) words around target word. Collocations formed with the parents of the surrounding words in the window (-3, 3) words around the target word (COL PARENT). Occurrences in the current sentence of the words that are linked to the current word with a semantic relation of AGENT or THEME in WordNet 2.0 glosses (XWN LEMMA). We used files from XWN project (XWN, 2003) containing WordNet 2.0 glosses that were sense disambiguated and tagged with semantic relations both manually and automatically. For each word to be disambiguated we created a signature consisting of the set of words that are linked with a semantic relation of THEME or AGENT in all WordNet glosses. For every word in this set we created a feature showing if that word appears in the current sentence containing the target word. Then we added a new feature consisting of all the named entities in a window of (-5,5) sentences around the target word. We called this feature NAMED ENTITIES. We created</context>
</contexts>
<marker>XWN, 2003</marker>
<rawString>XWN, 2003. eXtended WordNet. Software available at http://xwn.hlt.utdallas.edu.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>