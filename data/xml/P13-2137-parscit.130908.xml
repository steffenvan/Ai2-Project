<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027577">
<title confidence="0.988652">
Building and Evaluating a Distributional Memory for Croatian
</title>
<author confidence="0.99657">
Jan ˇSnajder* Sebastian Pad´o† ˇZeljko Agi´c$
</author>
<affiliation confidence="0.942989">
*University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
†Heidelberg University, Institut f¨ur Computerlinguistik
69120 Heidelberg, Germany
$University of Zagreb, Faculty of Humanities and Social Sciences
</affiliation>
<address confidence="0.838718">
Ivana Luˇci´ca 3, 10000 Zagreb, Croatia
</address>
<email confidence="0.998278">
jan.snajder@fer.hr pado@cl.uni-heidelberg.de zagic@ffzg.hr
</email>
<sectionHeader confidence="0.997381" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975166666667">
We report on the first structured dis-
tributional semantic model for Croatian,
DM.HR. It is constructed after the model
of the English Distributional Memory (Ba-
roni and Lenci, 2010), from a dependency-
parsed Croatian web corpus, and covers
about 2M lemmas. We give details on the
linguistic processing and the design prin-
ciples. An evaluation shows state-of-the-
art performance on a semantic similarity
task with particularly good performance on
nouns. The resource is freely available.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999641921052631">
Most current work in lexical semantics is based
on the Distributional Hypothesis (Harris, 1954),
which posits a correlation between the degree of
words’ semantic similarity and the similarity of
the contexts in which they occur. Using this hy-
pothesis, word meaning representations can be ex-
tracted from large corpora. Words are typically rep-
resented as vectors whose dimensions correspond
to context features. The vector similarities, which
are interpreted as semantic similarities, are used in
numerous applications (Turney and Pantel, 2010).
Most vector spaces in current use are either word-
based (co-occurrence defined by surface window,
context words as dimensions) or syntax-based (co-
occurrence defined syntactically, syntactic objects
as dimensions). Syntax-based models have sev-
eral desirable properties. First, they are model to
fine-grained types of semantic similarity such as
predicate-argument plausibility (Erk et al., 2010).
Second, they are more versatile – Baroni and Lenci
(2010) have presented a generic framework, the
Distributional Memory (DM), which is applicable
to a wide range of tasks beyond word similarity.
Third, they avoid the “syntactic assumption” in-
herent in word-based models, namely that context
words are relevant iff they are in an n-word window
around the target. This property is particularly rele-
vant for free word order languages with many long
distance dependencies and non-projective structure
(K¨ubler et al., 2009). Their obvious problem, of
course, is that they require a large parsed corpus.
In this paper, we describe the construction of
a Distributional Memory for Croatian (DM.HR),
a free word order language. To do so, we parse
hrWaC (Ljubeˇsi´c and Erjavec, 2011), a 1.2B-token
Croatian web corpus. We evaluate DM.HR on a
synonym choice task, where it outperforms the
standard bag-of-word model for nouns and verbs.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99992345">
Vector space semantic models have been applied
to a number of Slavic languages, including Bul-
garian (Nakov, 2001a), Czech (Smrˇz and Rychl´y,
2001), Polish (Piasecki, 2009; Broda et al., 2008;
Broda and Piasecki, 2008), and Russian (Nakov,
2001b; Mitrofanova et al., 2007). Previous work
on distributional semantic models for Croatian
dealt with similarity prediction (Ljubeˇsi´c et al.,
2008; Jankovi´c et al., 2011) and synonym detec-
tion (Karan et al., 2012), however using only word-
based and not syntactic-based models.
So far the only DM for a language other than
English is the German DM.DE by Pad´o and Utt
(2012), who describe the process of building
DM.DE and the evaluation on a synonym choice
task. Our work is similar, though each language
has its own challenges. Croatian, like other Slavic
languages, has rich inflectional morphology and
free word order, which lead to errors in linguistic
processing and affect the quality of the DM.
</bodyText>
<page confidence="0.974449">
784
</page>
<note confidence="0.5312425">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 784–789,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.996083" genericHeader="method">
3 Distributional Memory
</sectionHeader>
<bodyText confidence="0.999964763157895">
DM represents co-occurrence information in a gen-
eral, non-task-specific manner, as a tensor, i.e., a
three-dimensional matrix, of weighted word-link-
word tuples. Each tuple is mapped onto a number
by scoring function Q: W x L x W -+ R+, that
reflects the strength of the association. When a par-
ticular task is selected, a vector space for this task
can be generated from the tensor by matricization.
Regarding the examples from Section 1, synonym
discovery would use a word by link-word space
(W x LW), which contains vectors for words w
represented by pairs (l, w) of a link and a context
word. Analogy discovery would use a word-word
by link space (WW x L), which represents word
pairs (w1, w2) by vectors over links l.
The links can be chosen to model any relation
of interest between words. However, as noted by
Pad´o and Utt (2012), dependency relations are the
most obvious choice. Baroni and Lenci (2010) in-
troduce three dependency-based DM variants: De-
pDM, LexDM, and TypeDM. DepDM uses links
that correspond to dependency relations, with sub-
categorization for subject (subj tr and subj intr)
and object (obj and iobj). Furthermore, all prepo-
sitions are lexicalized into links (e.g., (sun, on,
Sunday)). Finally, the tensor is symmetrized: for
each tuple (w1, l, w2), its inverse (w2, l−1, w1) is
included. The other two variants are more complex:
LexDM uses more lexicalized links, encoding, e.g.,
lexical material between the words, while TypeDM
extends LexDM with a scoring function based on
lexical variability.
Following the work of Pad´o and Utt (2012), we
build a DepDM variant for DM.HR. Although Ba-
roni and Lenci (2010) show that TypeDM can out-
perform the other two variants, DepDM often per-
forms at a comparable level, while being much
simpler to build and more efficient to compute.
</bodyText>
<sectionHeader confidence="0.996929" genericHeader="method">
4 Building DM.HR
</sectionHeader>
<bodyText confidence="0.9998349">
To build DM.HR, we need to collect co-occurrence
counts from a corpus. Since no sufficiently large
suitable corpus exists for Croatian, we first explain
how we preprocessed, tagged, and parsed the data.
Corpus and preprocessing. We adopted hrWaC,
the 1.2B-token Croatian web corpus (Ljubeˇsi´c and
Erjavec, 2011), as starting point. hrWaC was built
with the aim of obtaining a cleaner-than-usual web
corpus. To this end, a conservative boilerplate re-
moval procedure was used; Ljubeˇsi´c and Erjavec
(2011) report a precision of 97.9% and a recall of
70.7%. Nonetheless, our inspection revealed that,
apart from the unavoidable spelling and grammati-
cal errors, hrWaC still contains non-textual content
(e.g., code snippets and formatting structure), en-
coding errors, and foreign-language content. As
this severely affects linguistic processing, we addi-
tionally filtered the corpus.
First, we removed from hrWaC the content
crawled from main discussion forum and blog web-
sites. This content is highly ungrammatical and
contains a lot of non-diacriticized text, typical for
user-generated content. This step alone removed
one third of the data. We processed the remaining
content with a tokenizer and a sentence segmenter
based on regular expressions, obtaining 66M sen-
tences. Next, we applied a series of heuristic filters
at the document- and sentence-level. At the doc-
ument level, we discard all documents (1) whose
length is below a specified threshold, (2) contain
no diacritics, (3) contain no words from a list of fre-
quent Croatian words, or (4) contain a single word
from lists of distinctive foreign-language words
(for Serbian). The last two steps serve to eliminate
foreign-language content. In particular, the last
step serves to filter out the text in Serbian, which at
the sentence-level is difficult to automatically dis-
criminate from Croatian. At the sentence-level, we
discard sentences that are (1) shorter than a speci-
fied threshold, (2) contain non-standard symbols,
</bodyText>
<listItem confidence="0.7564765">
(3) contain non-diacriticized Croatian words, or
(4) contain too many foreign words from a list of
</listItem>
<bodyText confidence="0.997512066666667">
foreign-language words (for English and Slovene).
The last step filters out specifically the sentences
in English and Slovene, as we found that these of-
ten occur mixed with text in Croatian. The final
filtered version of hrWaC contains 51M sentences
and 1.2B tokens. The corpus is freely available for
download, along with a more detailed description
of the preprocessing steps.1
Tagging, lemmatization, and parsing. For mor-
phosyntactic (MSD) tagging, lemmatization, and
dependency parsing of hrWaC, we use freely avail-
able tools with models trained on the new SETimes
Corpus of Croatian (SETIMES.HR), based on the
Croatian part of the SETimes parallel corpus.2 SE-
TIMES.HR and the derived tools are prototypes
</bodyText>
<footnote confidence="0.995369333333333">
1http://takelab.fer.hr/data
2http://www.nljubesic.net/resources/
corpora/setimes/
</footnote>
<page confidence="0.989619">
785
</page>
<table confidence="0.9986026">
SETIMES.HR Wikipedia
HunPos (POS only) 97.1 94.1
HunPos (full MSD) 87.7 81.5
CST lemmatizer 97.7 96.5
MSTParser 77.5 68.8
</table>
<tableCaption confidence="0.999232">
Table 1: Tagging, lemmatization, and parsing accu-
</tableCaption>
<bodyText confidence="0.9879245">
racy
that are about to be released as parts of another
work. Here we give a general description and a
re-evaluation that we consider relevant for building
DM.HR.
SETIMES.HR consists of 90K tokens and 4K
sentences, manually lemmatized and MSD-tagged
according to Multext East v4 tagset (Erjavec, 2012),
with the help of the Croatian Lemmatization Server
(Tadi´c, 2005). It is used also as a basis for a novel
formalism for syntactic annotation and dependency
parsing of Croatian (Agi´c and Merkler, 2013).
On the basis of previous evaluation for Croa-
tian (Agi´c et al., 2008; Agi´c et al., 2009; Agi´c,
2012) and availability and licensing considerations,
we chose HunPos tagger (Hal´acsy et al., 2007),
CST lemmatizer (Ingason et al., 2008), and MST-
Parser (McDonald et al., 2006) to process hrWaC.
We evaluated the tools on 100-sentence test sets
from SETIMES.HR and Wikipedia; performance
on Wikipedia should be indicative of the perfor-
mance on a cross-domain dataset, such as hrWaC.
In Table 1 we show lemmatization and tagging ac-
curacy, as well as dependency parsing accuracy
in terms of labeled attachment score (LAS). The
results show that lemmatization, tagging and pars-
ing accuracy improves on the state of the art for
Croatian. The SETIMES.HR dependency parsing
models are publicly available.3
Syntactic patterns. We collect the co-occur-
rence counts of tuples using a set of syntactic pat-
terns. The patterns effectively define the link types,
and hence the dimensions of the semantic space.
Similar to previous work, we use two sorts of links:
unlexicalized and lexicalized.
For unlexicalized links, we use ten syntactic pat-
terns. These correspond to the main dependency re-
lations produced by our parser: Pred for predicates,
Atr for attributes, Adv for adverbs, Atv for verbal
complements, Obj for objects, Prep for preposi-
tions, and Pnom for nominal predicates. We sub-
categorized the subject relation into Sub tr (sub-
</bodyText>
<footnote confidence="0.932823">
3http://zeljko.agic.me/resources/
</footnote>
<table confidence="0.9999554375">
Link P(%) R(%) Fl (%)
Unlexicalized
Adv 57.3 52.7 54.9
Atr 85.0 89.3 87.1
Atv 75.3 70.9 73.1
Obj 71.4 71.7 71.5
Pnom 55.7 50.8 53.1
Pred 81.8 70.6 75.8
Prep 50.0 28.6 36.4
Sb tr 67.8 73.8 70.7
Sb intr 64.5 64.8 64.7
Verb 61.6 73.6 67.1
Lexicalized
Prepositions 67.2 67.9 67.5
Verbs 61.6 73.6 67.1
All links 73.7 75.5 74.6
</table>
<tableCaption confidence="0.953827">
Table 2: Tuple extraction performance on SE-
TIMES.HR
</tableCaption>
<bodyText confidence="0.999362235294118">
jects of transitive verbs) and Sub intr (subject of
intransitive verbs). The motivation for this is better
modeling of verb semantics by capturing diathe-
sis alternations. In particular, for many Croatian
verbs reflexivization introduces a meaning shift,
e.g., predati (to hand in/out) vs. predati se (to
surrender). With subject subcategorization, re-
flexive and irreflexive readings will have differ-
ent tensor representations; e.g., (student, Subj tr,
zada´ca) ((student, Subj tr, homework)) vs. (trupe,
Subj intr, napadaˇc) ((troops, Subj intr, invadors)).
Finally, similar to Pad´o and Utt (2012), we use
Verb as an underspecified link between subjects
and objects linked by non-auxiliary verbs.
For lexicalized links, we use two more extraction
patterns for prepositions and verbs. Prepositions
are directly lexicalized as links; e.g., (mjesto, na,
sunce) ((place, on, sun)). The same holds for non-
auxiliary verbs linking subjects to objects; e.g.,
(drˇzava, kupiti, koliˇcina) ((state, buy, amount)).
Tuple extraction and scoring. The overall qual-
ity of the DM.HR depends on the accuracy of ex-
tracted tuples, which is affected by all preprocess-
ing steps. We computed the performance of tu-
ple extraction by evaluating a sample of tuples
extracted from a parsed version of SETIMES.HR
against the tuples extracted from the SETIMES.HR
gold annotations (we use the same sample as for
tagging and parsing performance evaluation). Ta-
ble 2 shows Precision, Recall, and F1 score. Over-
all, we achieve the best performance on the Atr
links, followed by Pred links. The performance is
generally higher on unlexicalized links than on lex-
icalized links (note that performance on unlexical-
</bodyText>
<page confidence="0.996277">
786
</page>
<table confidence="0.998930666666667">
Link Word LMI Link Word LMI
Atv mo´ci 225107 Adv mogu´ce 9669
Atv ˇzeljeti 22049 Atv namjeravati 9095
Obj stan 19997 Obj karta 8936
po cijena 18534 prije godina 8584
Pred kada 14408 Adv nedavno 7842
Obj dionica 13720 Atv odluˇciti 7578
Atv morati 12097 Adv godina 7496
Obj ulaznica 11126 Obj zemljiˇste 7180
</table>
<tableCaption confidence="0.8397295">
Table 3: Top 16 LMI-scored tuples for the verb
kupiti (to buy)
</tableCaption>
<bodyText confidence="0.987214333333333">
ized Verb links is identical to overall performance
on lexicalized verb links). The overall F1 score of
tuple extraction is 74.6%.
Following DM and DM.DE, we score each
extracted tuple using Local Mutual Information
(LMI) (Evert, 2005):
</bodyText>
<equation confidence="0.99265">
P(i, j, k)
LMI(i, j, k) = f(i, j, k) log
P(i)P(j)P(k)
</equation>
<bodyText confidence="0.999953578947368">
For a tuple (w1, l, w2), LMI scores the association
strength between word w1 and word w2 via link l
by comparing their joint distribution against the dis-
tribution under the independence assumption, mul-
tiplied with the observed frequency f(w1, l, w2) to
discount infrequent tuples. The probabilities are
computed from tuple counts as maximum likeli-
hood estimates. We exclude from the tensor all
tuples with a negative LMI score. Finally, we sym-
metrize the tensor by introducing inverse links.
Model statistics. The resulting DM.HR tensor
consists of 2.3M lemmas, 121M links and 165K
link types (including inverse links). On average,
each lemma has 53 links. This makes DM.HR
more sparse than English DM (796 link types), but
less sparse than German DM (220K link types; 22
links per lemma). Table 3 shows an example of
the extracted tuples for the verb kupiti (to buy).
DM.HR tensor is freely available for download.4
</bodyText>
<sectionHeader confidence="0.999583" genericHeader="method">
5 Evaluating DM.HR
</sectionHeader>
<bodyText confidence="0.999983285714286">
Task. We present a pilot evaluation DM.HR on a
standard task from distributional semantics, namely
synonym choice. In contrast to tasks like predict-
ing word similarity We use the dataset created by
Karan et al. (2012), with more than 11,000 syn-
onym choice questions. Each question consists of
one target word (nouns, verbs, and adjectives) with
</bodyText>
<footnote confidence="0.967013">
4http://takelab.fer.hr/dmhr
</footnote>
<table confidence="0.9996082">
Model Accuracy (%) Coverage (%)
N A V N A V
DM.HR 70.0 66.3 63.2 99.9 99.1 100
BOW-LSA 67.2 68.9 61.0 100 100 100
BOW baseline 59.9 65.7 55.9 99.9 99.7 100
</table>
<tableCaption confidence="0.999883">
Table 4: Results on synonym choice task
</tableCaption>
<bodyText confidence="0.991535682926829">
four synonym candidates (one is correct). The ques-
tions were extracted automatically from a machine-
readable dictionary of Croatian. An example item
is teˇzak (farmer): poljoprivrednik (farmer), um-
jetnost (art), radijacija (radiation), bod (point).
We sampled from the dataset questions for nouns,
verbs, and adjectives, with 1000 questions each.5
Additionally, we manually corrected some errors
in the dataset, introduced by the automatic extrac-
tion procedure. To make predictions, we compute
pairwise cosine similarities of the target word vec-
tors with the four candidates and predict the can-
didate(s) with maximal similarity (note that there
may be ties).
Evaluation. Our evaluation follows the scheme
developed by Mohammad et al. (2007), who define
accuracy as the average number of correct predic-
tions per covered question. Each correct prediction
with a single most similar candidate receives a full
credit (A), while ties for maximal similarity are
discounted (B: two-way tie, C: three-way tie, D:
four-way tie): A+ 12B+ 13C+ 14D. We consider a
question item to be covered if the target and at least
one answer word are modeled. In our experiments,
ties occur when vector similarities are zero for all
word pairs (due to vector sparsity). Note that a
random baseline would perform at 0.25 accuracy.
As baseline to compare against the DM.HR, we
build a standard bag-of-word model from the same
corpus. It uses a ±5-word within-sentence con-
text window, and the 10,000 most frequent context
words (nouns, adjectives, and verbs) as dimensions.
We also compare against BOW-LSA, a state-of-
the-art synonym detection model from Karan et
al. (2012), which uses 500 latent dimensions and
paragraphs as contexts. We determine the signifi-
cance of differences between the models by com-
puting 95% confidence intervals with bootstrap re-
sampling (Efron and Tibshirani, 1993).
Results. Table 4 shows the results for the three
considered models on nouns (N), adjectives (A),
</bodyText>
<footnote confidence="0.997891">
5Available at: http://takelab.fer.hr/crosyn
</footnote>
<page confidence="0.994833">
787
</page>
<bodyText confidence="0.999945233333334">
and verbs (V). The performance of BOW-LSA
differs slightly from that reported by Karan et al.
(2012), because we evaluate on a sample of their
dataset. DM.HR outperforms the baseline BOW
model for nouns and verbs (differences are sig-
nificant at p &lt; 0.05). Moreover, on these cate-
gories DM.HR performs slightly better than BOW-
LSA, but the differences are not statistically sig-
nificant. Conversely, on adjectives BOW-LSA per-
forms slightly better than DM.HR, but the differ-
ence is again not statistically significant. All mod-
els achieve comparable and almost perfect cov-
erage on this dataset (BOW-LSA achieves com-
plete coverage because of the way how the original
dataset was filtered).
Overall, the biggest improvement over the base-
line is achieved for nouns. Nouns occur as heads
and dependents of many link types (unlexicalized
and lexicalized), and are thus well represented in
the semantic space. On the other hand, adjectives
seem to be less well modeled. Although the major-
ity of adjectives occur as heads or dependents of
the Atr relation, for which extraction accuracy is
the highest (cf. Table 2), it is likely that a single link
type is not sufficient. As noted by a reviewer, more
insight could perhaps be gained by comparing the
predictions of BOW-LSA and DM.HR models. The
generally low performance on verbs suggests that
their semantic is not fully covered in word- and
syntax-based spaces.
</bodyText>
<sectionHeader confidence="0.996904" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993294117647">
We have described the construction of DM.HR, a
syntax-based distributional memory for Croatian
built from a dependency-parsed web corpus. To the
best of our knowledge, DM.HR is the first freely
available distributional memory for a Slavic lan-
guage. We have conducted a preliminary evalua-
tion of DM.HR on a synonym choice task, where
DM.HR outperformed the bag-of-word model and
performed comparable to an LSA model.
This work provides a starting point for a sys-
tematic study of dependency-based distributional
semantics for Croatian and similar languages. Our
first priority will be to analyze how corpus prepro-
cessing and the choice of link types relates to model
performance on different semantic tasks. Better
modeling of adjectives and verbs is also an impor-
tant topic for future research.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997955">
The first author was supported by the Croatian
Science Foundation (project 02.03/162: “Deriva-
tional Semantic Models for Information Retrieval”).
We thank the reviewers for their constructive com-
ments. Special thanks to Hiko Schamoni, Tae-Gil
Noh, and Mladen Karan for their assistance.
</bodyText>
<sectionHeader confidence="0.99889" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99916">
ˇZeljko Agi´c and Danijela Merkler. 2013. Three syn-
tactic formalisms for data-driven dependency pars-
ing of Croatian. Proceedings of TSD 2013, Lecture
Notes in Artificial Intelligence.
ˇZeljko Agi´c, Marko Tadi´c, and Zdravko Dovedan.
2008. Improving part-of-speech tagging accuracy
for Croatian by morphological analysis. Informat-
ica, 32(4):445–451.
ˇZeljko Agi´c, Marko Tadi´c, and Zdravko Dovedan.
2009. Evaluating full lemmatization of Croatian
texts. In Recent Advances in Intelligent Information
Systems, pages 175–184. EXIT Warsaw.
ˇZeljko Agi´c. 2012. K-best spanning tree dependency
parsing with verb valency lexicon reranking. In Pro-
ceedings of COLING 2012: Posters, pages 1–12,
Bombay, India.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguistics,
36(4):673–721.
Bartosz Broda and Maciej Piasecki. 2008. Superma-
trix: a general tool for lexical semantic knowledge
acquisition. In Speech and Language Technology,
volume 11, pages 239–254. Polish Phonetics Asso-
cation.
Bartosz Broda, Magdalena Derwojedowa, Maciej Pi-
asecki, and Stanisław Szpakowicz. 2008. Corpus-
based semantic relatedness for the construction of
Polish WordNet. In Proceedings of LREC, Mar-
rakech, Morocco.
Bradley Efron and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman and Hall,
New York.
Tomaˇz Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46(1):131–142.
Katrin Erk, Sebastian Pad´o, and Ulrike Pad´o. 2010.
A Flexible, Corpus-driven Model of Regular and In-
verse Selectional Preferences. Computational Lin-
guistics, 36(4):723–763.
</reference>
<page confidence="0.988919">
788
</page>
<reference confidence="0.999681775">
Stefan Evert. 2005. The statistics of word cooccur-
rences. Ph.D. thesis, PhD Dissertation, Stuttgart
University.
P´eter Hal´acsy, Andr´as Kornai, and Csaba Oravecz.
2007. HunPos: An open source trigram tagger. In
Proceedings of ACL 2007, pages 209–212, Prague,
Czech Republic.
Zelig S. Harris. 1954. Distributional structure. Word,
10(23):146–162.
Anton Karl Ingason, Sigr´un Helgad´ottir, Hrafn Lofts-
son, and Eir´ıkur R¨ognvaldsson. 2008. A mixed
method lemmatization algorithm using a hierarchy
of linguistic identities (HOLI). In Proceedings of
GoTAL, pages 205–216.
Vedrana Jankovi´c, Jan &amp;quot;Snajder, and Bojana Dalbelo
Ba&amp;quot;si´c. 2011. Random indexing distributional se-
mantic models for Croatian language. In Proceed-
ings of Text, Speech and Dialogue, pages 411–418,
Plze&amp;quot;n, Czech Republic.
Mladen Karan, Jan &amp;quot;Snajder, and Bojana Dalbelo Ba&amp;quot;si´c.
2012. Distributional semantics approach to detect-
ing synonyms in Croatian language. In Proceedings
of the Language Technologies Conference, Informa-
tion Society, Ljubljana, Slovenia.
Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures
on Human Language Technologies. Morgan &amp; Clay-
pool.
Nikola Ljube&amp;quot;si´c and Toma&amp;quot;z Erjavec. 2011. hrWaC
and slWac: Compiling web corpora for Croatian and
Slovene. In Proceedings of Text, Speech and Dia-
logue, pages 395–402, Plze&amp;quot;n, Czech Republic.
Nikola Ljube&amp;quot;si´c, Damir Boras, Nikola Bakari´c, and Jas-
mina Njavro. 2008. Comparing measures of seman-
tic similarity. In Proceedings of the ITI 2008 30th
International Conference of Information Technology
Interfaces, Cavtat, Croatia.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings of
CoNLL-X, pages 216–220, New York, NY.
Olga Mitrofanova, Anton Mukhin, Polina Panicheva,
and Vyacheslav Savitsky. 2007. Automatic word
clustering in Russian texts. In Proceedings of Text,
Speech and Dialogue, pages 85–91, Plze&amp;quot;n, Czech
Republic.
Saif Mohammad, Iryna Gurevych, Graeme Hirst, and
Torsten Zesch. 2007. Cross-lingual distributional
profiles of concepts for measuring semantic distance.
In Proceedings of EMNLP/CoNLL, pages 571–580,
Prague, Czech Republic.
Preslav Nakov. 2001a. Latent semantic analysis
for Bulgarian literature. In Proceedings of Spring
Conference of Bulgarian Mathematicians Union,
Borovets, Bulgaria.
Preslav Nakov. 2001b. Latent semantic analysis for
Russian literature investigation. In Proceedings of
the 120 years Bulgarian Naval Academy Confer-
ence.
Sebastian Pad´o and Jason Utt. 2012. A distributional
memory for German. In Proceedings of the KON-
VENS 2012 workshop on lexical-semantic resources
and applications, pages 462–470, Vienna, Austria.
Maciej Piasecki. 2009. Automated extraction of lexi-
cal meanings from corpus: A case study of potential-
ities and limitations. In Representing Semantics in
Digital Lexicography. Innovative Solutions for Lexi-
cal Entry Content in Slavic Lexicography, pages 32–
43. Institute of Slavic Studies, Polish Academy of
Sciences.
Pavel Smr&amp;quot;z and Pavel Rychl´y. 2001. Finding semanti-
cally related words in large corpora. In Text, Speech
and Dialogue, pages 108–115. Springer.
Marko Tadi´c. 2005. The Croatian Lemmatization
Server. Southern Journal of Linguistics, 29(1):206–
217.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
</reference>
<page confidence="0.998605">
789
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.198703">
<title confidence="0.999992">Building and Evaluating a Distributional Memory for Croatian</title>
<author confidence="0.995345">Sebastian</author>
<affiliation confidence="0.857682">of Zagreb, Faculty of Electrical Engineering and Unska 3, 10000 Zagreb,</affiliation>
<address confidence="0.7099495">University, Institut f¨ur 69120 Heidelberg, of Zagreb, Faculty of Humanities and Social Ivana Luˇci´ca 3, 10000 Zagreb,</address>
<email confidence="0.810558">jan.snajder@fer.hrpado@cl.uni-heidelberg.dezagic@ffzg.hr</email>
<abstract confidence="0.998236769230769">We report on the first structured distributional semantic model for Croatian, It is constructed after the model of the English Distributional Memory (Baroni and Lenci, 2010), from a dependencyparsed Croatian web corpus, and covers about 2M lemmas. We give details on the linguistic processing and the design principles. An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. The resource is freely available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ˇZeljko Agi´c</author>
<author>Danijela Merkler</author>
</authors>
<title>Three syntactic formalisms for data-driven dependency parsing of Croatian.</title>
<date>2013</date>
<booktitle>Proceedings of TSD 2013, Lecture Notes in Artificial Intelligence.</booktitle>
<marker>Agi´c, Merkler, 2013</marker>
<rawString>ˇZeljko Agi´c and Danijela Merkler. 2013. Three syntactic formalisms for data-driven dependency parsing of Croatian. Proceedings of TSD 2013, Lecture Notes in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ˇZeljko Agi´c</author>
<author>Marko Tadi´c</author>
<author>Zdravko Dovedan</author>
</authors>
<title>Improving part-of-speech tagging accuracy for Croatian by morphological analysis.</title>
<date>2008</date>
<journal>Informatica,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Agi´c, Tadi´c, Dovedan, 2008</marker>
<rawString>ˇZeljko Agi´c, Marko Tadi´c, and Zdravko Dovedan. 2008. Improving part-of-speech tagging accuracy for Croatian by morphological analysis. Informatica, 32(4):445–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ˇZeljko Agi´c</author>
<author>Marko Tadi´c</author>
<author>Zdravko Dovedan</author>
</authors>
<title>Evaluating full lemmatization of Croatian texts.</title>
<date>2009</date>
<booktitle>In Recent Advances in Intelligent Information Systems,</booktitle>
<pages>175--184</pages>
<publisher>EXIT Warsaw.</publisher>
<marker>Agi´c, Tadi´c, Dovedan, 2009</marker>
<rawString>ˇZeljko Agi´c, Marko Tadi´c, and Zdravko Dovedan. 2009. Evaluating full lemmatization of Croatian texts. In Recent Advances in Intelligent Information Systems, pages 175–184. EXIT Warsaw.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ˇZeljko Agi´c</author>
</authors>
<title>K-best spanning tree dependency parsing with verb valency lexicon reranking.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>1--12</pages>
<location>Bombay, India.</location>
<marker>Agi´c, 2012</marker>
<rawString>ˇZeljko Agi´c. 2012. K-best spanning tree dependency parsing with verb valency lexicon reranking. In Proceedings of COLING 2012: Posters, pages 1–12, Bombay, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="645" citStr="Baroni and Lenci, 2010" startWordPosition="79" endWordPosition="83">g a Distributional Memory for Croatian Jan ˇSnajder* Sebastian Pad´o† ˇZeljko Agi´c$ *University of Zagreb, Faculty of Electrical Engineering and Computing Unska 3, 10000 Zagreb, Croatia †Heidelberg University, Institut f¨ur Computerlinguistik 69120 Heidelberg, Germany $University of Zagreb, Faculty of Humanities and Social Sciences Ivana Luˇci´ca 3, 10000 Zagreb, Croatia jan.snajder@fer.hr pado@cl.uni-heidelberg.de zagic@ffzg.hr Abstract We report on the first structured distributional semantic model for Croatian, DM.HR. It is constructed after the model of the English Distributional Memory (Baroni and Lenci, 2010), from a dependencyparsed Croatian web corpus, and covers about 2M lemmas. We give details on the linguistic processing and the design principles. An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. The resource is freely available. 1 Introduction Most current work in lexical semantics is based on the Distributional Hypothesis (Harris, 1954), which posits a correlation between the degree of words’ semantic similarity and the similarity of the contexts in which they occur. Using this hypothesis, word meaning representations </context>
<context position="1960" citStr="Baroni and Lenci (2010)" startWordPosition="272" endWordPosition="275">ions correspond to context features. The vector similarities, which are interpreted as semantic similarities, are used in numerous applications (Turney and Pantel, 2010). Most vector spaces in current use are either wordbased (co-occurrence defined by surface window, context words as dimensions) or syntax-based (cooccurrence defined syntactically, syntactic objects as dimensions). Syntax-based models have several desirable properties. First, they are model to fine-grained types of semantic similarity such as predicate-argument plausibility (Erk et al., 2010). Second, they are more versatile – Baroni and Lenci (2010) have presented a generic framework, the Distributional Memory (DM), which is applicable to a wide range of tasks beyond word similarity. Third, they avoid the “syntactic assumption” inherent in word-based models, namely that context words are relevant iff they are in an n-word window around the target. This property is particularly relevant for free word order languages with many long distance dependencies and non-projective structure (K¨ubler et al., 2009). Their obvious problem, of course, is that they require a large parsed corpus. In this paper, we describe the construction of a Distribut</context>
<context position="4919" citStr="Baroni and Lenci (2010)" startWordPosition="751" endWordPosition="754"> a particular task is selected, a vector space for this task can be generated from the tensor by matricization. Regarding the examples from Section 1, synonym discovery would use a word by link-word space (W x LW), which contains vectors for words w represented by pairs (l, w) of a link and a context word. Analogy discovery would use a word-word by link space (WW x L), which represents word pairs (w1, w2) by vectors over links l. The links can be chosen to model any relation of interest between words. However, as noted by Pad´o and Utt (2012), dependency relations are the most obvious choice. Baroni and Lenci (2010) introduce three dependency-based DM variants: DepDM, LexDM, and TypeDM. DepDM uses links that correspond to dependency relations, with subcategorization for subject (subj tr and subj intr) and object (obj and iobj). Furthermore, all prepositions are lexicalized into links (e.g., (sun, on, Sunday)). Finally, the tensor is symmetrized: for each tuple (w1, l, w2), its inverse (w2, l−1, w1) is included. The other two variants are more complex: LexDM uses more lexicalized links, encoding, e.g., lexical material between the words, while TypeDM extends LexDM with a scoring function based on lexical </context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bartosz Broda</author>
<author>Maciej Piasecki</author>
</authors>
<title>Supermatrix: a general tool for lexical semantic knowledge acquisition.</title>
<date>2008</date>
<journal>Polish Phonetics Assocation.</journal>
<booktitle>In Speech and Language Technology,</booktitle>
<volume>11</volume>
<pages>239--254</pages>
<contexts>
<context position="3064" citStr="Broda and Piasecki, 2008" startWordPosition="446" endWordPosition="449">roblem, of course, is that they require a large parsed corpus. In this paper, we describe the construction of a Distributional Memory for Croatian (DM.HR), a free word order language. To do so, we parse hrWaC (Ljubeˇsi´c and Erjavec, 2011), a 1.2B-token Croatian web corpus. We evaluate DM.HR on a synonym choice task, where it outperforms the standard bag-of-word model for nouns and verbs. 2 Related Work Vector space semantic models have been applied to a number of Slavic languages, including Bulgarian (Nakov, 2001a), Czech (Smrˇz and Rychl´y, 2001), Polish (Piasecki, 2009; Broda et al., 2008; Broda and Piasecki, 2008), and Russian (Nakov, 2001b; Mitrofanova et al., 2007). Previous work on distributional semantic models for Croatian dealt with similarity prediction (Ljubeˇsi´c et al., 2008; Jankovi´c et al., 2011) and synonym detection (Karan et al., 2012), however using only wordbased and not syntactic-based models. So far the only DM for a language other than English is the German DM.DE by Pad´o and Utt (2012), who describe the process of building DM.DE and the evaluation on a synonym choice task. Our work is similar, though each language has its own challenges. Croatian, like other Slavic languages, has </context>
</contexts>
<marker>Broda, Piasecki, 2008</marker>
<rawString>Bartosz Broda and Maciej Piasecki. 2008. Supermatrix: a general tool for lexical semantic knowledge acquisition. In Speech and Language Technology, volume 11, pages 239–254. Polish Phonetics Assocation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bartosz Broda</author>
<author>Magdalena Derwojedowa</author>
<author>Maciej Piasecki</author>
<author>Stanisław Szpakowicz</author>
</authors>
<title>Corpusbased semantic relatedness for the construction of Polish WordNet.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="3037" citStr="Broda et al., 2008" startWordPosition="442" endWordPosition="445">09). Their obvious problem, of course, is that they require a large parsed corpus. In this paper, we describe the construction of a Distributional Memory for Croatian (DM.HR), a free word order language. To do so, we parse hrWaC (Ljubeˇsi´c and Erjavec, 2011), a 1.2B-token Croatian web corpus. We evaluate DM.HR on a synonym choice task, where it outperforms the standard bag-of-word model for nouns and verbs. 2 Related Work Vector space semantic models have been applied to a number of Slavic languages, including Bulgarian (Nakov, 2001a), Czech (Smrˇz and Rychl´y, 2001), Polish (Piasecki, 2009; Broda et al., 2008; Broda and Piasecki, 2008), and Russian (Nakov, 2001b; Mitrofanova et al., 2007). Previous work on distributional semantic models for Croatian dealt with similarity prediction (Ljubeˇsi´c et al., 2008; Jankovi´c et al., 2011) and synonym detection (Karan et al., 2012), however using only wordbased and not syntactic-based models. So far the only DM for a language other than English is the German DM.DE by Pad´o and Utt (2012), who describe the process of building DM.DE and the evaluation on a synonym choice task. Our work is similar, though each language has its own challenges. Croatian, like o</context>
</contexts>
<marker>Broda, Derwojedowa, Piasecki, Szpakowicz, 2008</marker>
<rawString>Bartosz Broda, Magdalena Derwojedowa, Maciej Piasecki, and Stanisław Szpakowicz. 2008. Corpusbased semantic relatedness for the construction of Polish WordNet. In Proceedings of LREC, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap. Chapman and Hall,</title>
<date>1993</date>
<location>New York.</location>
<contexts>
<context position="16925" citStr="Efron and Tibshirani, 1993" startWordPosition="2654" endWordPosition="2657">rsity). Note that a random baseline would perform at 0.25 accuracy. As baseline to compare against the DM.HR, we build a standard bag-of-word model from the same corpus. It uses a ±5-word within-sentence context window, and the 10,000 most frequent context words (nouns, adjectives, and verbs) as dimensions. We also compare against BOW-LSA, a state-ofthe-art synonym detection model from Karan et al. (2012), which uses 500 latent dimensions and paragraphs as contexts. We determine the significance of differences between the models by computing 95% confidence intervals with bootstrap resampling (Efron and Tibshirani, 1993). Results. Table 4 shows the results for the three considered models on nouns (N), adjectives (A), 5Available at: http://takelab.fer.hr/crosyn 787 and verbs (V). The performance of BOW-LSA differs slightly from that reported by Karan et al. (2012), because we evaluate on a sample of their dataset. DM.HR outperforms the baseline BOW model for nouns and verbs (differences are significant at p &lt; 0.05). Moreover, on these categories DM.HR performs slightly better than BOWLSA, but the differences are not statistically significant. Conversely, on adjectives BOW-LSA performs slightly better than DM.H</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>Bradley Efron and Robert J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman and Hall, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomaˇz Erjavec</author>
</authors>
<title>MULTEXT-East: Morphosyntactic resources for Central and Eastern European languages. Language Resources and Evaluation,</title>
<date>2012</date>
<pages>46--1</pages>
<contexts>
<context position="9177" citStr="Erjavec, 2012" startWordPosition="1409" endWordPosition="1410">corpus.2 SETIMES.HR and the derived tools are prototypes 1http://takelab.fer.hr/data 2http://www.nljubesic.net/resources/ corpora/setimes/ 785 SETIMES.HR Wikipedia HunPos (POS only) 97.1 94.1 HunPos (full MSD) 87.7 81.5 CST lemmatizer 97.7 96.5 MSTParser 77.5 68.8 Table 1: Tagging, lemmatization, and parsing accuracy that are about to be released as parts of another work. Here we give a general description and a re-evaluation that we consider relevant for building DM.HR. SETIMES.HR consists of 90K tokens and 4K sentences, manually lemmatized and MSD-tagged according to Multext East v4 tagset (Erjavec, 2012), with the help of the Croatian Lemmatization Server (Tadi´c, 2005). It is used also as a basis for a novel formalism for syntactic annotation and dependency parsing of Croatian (Agi´c and Merkler, 2013). On the basis of previous evaluation for Croatian (Agi´c et al., 2008; Agi´c et al., 2009; Agi´c, 2012) and availability and licensing considerations, we chose HunPos tagger (Hal´acsy et al., 2007), CST lemmatizer (Ingason et al., 2008), and MSTParser (McDonald et al., 2006) to process hrWaC. We evaluated the tools on 100-sentence test sets from SETIMES.HR and Wikipedia; performance on Wikiped</context>
</contexts>
<marker>Erjavec, 2012</marker>
<rawString>Tomaˇz Erjavec. 2012. MULTEXT-East: Morphosyntactic resources for Central and Eastern European languages. Language Resources and Evaluation, 46(1):131–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
<author>Ulrike Pad´o</author>
</authors>
<title>A Flexible,</title>
<date>2010</date>
<booktitle>Corpus-driven Model of Regular and Inverse Selectional Preferences. Computational Linguistics,</booktitle>
<pages>36--4</pages>
<marker>Erk, Pad´o, Pad´o, 2010</marker>
<rawString>Katrin Erk, Sebastian Pad´o, and Ulrike Pad´o. 2010. A Flexible, Corpus-driven Model of Regular and Inverse Selectional Preferences. Computational Linguistics, 36(4):723–763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The statistics of word cooccurrences.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>PhD Dissertation, Stuttgart University.</institution>
<contexts>
<context position="13500" citStr="Evert, 2005" startWordPosition="2101" endWordPosition="2102">al786 Link Word LMI Link Word LMI Atv mo´ci 225107 Adv mogu´ce 9669 Atv ˇzeljeti 22049 Atv namjeravati 9095 Obj stan 19997 Obj karta 8936 po cijena 18534 prije godina 8584 Pred kada 14408 Adv nedavno 7842 Obj dionica 13720 Atv odluˇciti 7578 Atv morati 12097 Adv godina 7496 Obj ulaznica 11126 Obj zemljiˇste 7180 Table 3: Top 16 LMI-scored tuples for the verb kupiti (to buy) ized Verb links is identical to overall performance on lexicalized verb links). The overall F1 score of tuple extraction is 74.6%. Following DM and DM.DE, we score each extracted tuple using Local Mutual Information (LMI) (Evert, 2005): P(i, j, k) LMI(i, j, k) = f(i, j, k) log P(i)P(j)P(k) For a tuple (w1, l, w2), LMI scores the association strength between word w1 and word w2 via link l by comparing their joint distribution against the distribution under the independence assumption, multiplied with the observed frequency f(w1, l, w2) to discount infrequent tuples. The probabilities are computed from tuple counts as maximum likelihood estimates. We exclude from the tensor all tuples with a negative LMI score. Finally, we symmetrize the tensor by introducing inverse links. Model statistics. The resulting DM.HR tensor consist</context>
</contexts>
<marker>Evert, 2005</marker>
<rawString>Stefan Evert. 2005. The statistics of word cooccurrences. Ph.D. thesis, PhD Dissertation, Stuttgart University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P´eter Hal´acsy</author>
<author>Andr´as Kornai</author>
<author>Csaba Oravecz</author>
</authors>
<title>HunPos: An open source trigram tagger.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL 2007,</booktitle>
<pages>209--212</pages>
<location>Prague, Czech Republic.</location>
<marker>Hal´acsy, Kornai, Oravecz, 2007</marker>
<rawString>P´eter Hal´acsy, Andr´as Kornai, and Csaba Oravecz. 2007. HunPos: An open source trigram tagger. In Proceedings of ACL 2007, pages 209–212, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="1059" citStr="Harris, 1954" startWordPosition="144" endWordPosition="145">c@ffzg.hr Abstract We report on the first structured distributional semantic model for Croatian, DM.HR. It is constructed after the model of the English Distributional Memory (Baroni and Lenci, 2010), from a dependencyparsed Croatian web corpus, and covers about 2M lemmas. We give details on the linguistic processing and the design principles. An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. The resource is freely available. 1 Introduction Most current work in lexical semantics is based on the Distributional Hypothesis (Harris, 1954), which posits a correlation between the degree of words’ semantic similarity and the similarity of the contexts in which they occur. Using this hypothesis, word meaning representations can be extracted from large corpora. Words are typically represented as vectors whose dimensions correspond to context features. The vector similarities, which are interpreted as semantic similarities, are used in numerous applications (Turney and Pantel, 2010). Most vector spaces in current use are either wordbased (co-occurrence defined by surface window, context words as dimensions) or syntax-based (cooccurr</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zelig S. Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Karl Ingason</author>
<author>Sigr´un Helgad´ottir</author>
<author>Hrafn Loftsson</author>
<author>Eir´ıkur R¨ognvaldsson</author>
</authors>
<title>A mixed method lemmatization algorithm using a hierarchy of linguistic identities (HOLI).</title>
<date>2008</date>
<booktitle>In Proceedings of GoTAL,</booktitle>
<pages>205--216</pages>
<marker>Ingason, Helgad´ottir, Loftsson, R¨ognvaldsson, 2008</marker>
<rawString>Anton Karl Ingason, Sigr´un Helgad´ottir, Hrafn Loftsson, and Eir´ıkur R¨ognvaldsson. 2008. A mixed method lemmatization algorithm using a hierarchy of linguistic identities (HOLI). In Proceedings of GoTAL, pages 205–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vedrana Jankovi´c</author>
</authors>
<title>Snajder, and Bojana Dalbelo Ba&amp;quot;si´c.</title>
<date></date>
<booktitle>In Proceedings of Text, Speech and Dialogue,</booktitle>
<pages>411--418</pages>
<location>Plze&amp;quot;n, Czech Republic.</location>
<marker>Jankovi´c, </marker>
<rawString>Vedrana Jankovi´c, Jan &amp;quot;Snajder, and Bojana Dalbelo Ba&amp;quot;si´c. 2011. Random indexing distributional semantic models for Croatian language. In Proceedings of Text, Speech and Dialogue, pages 411–418, Plze&amp;quot;n, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Ba&amp;quot;si´c.</title>
<date></date>
<booktitle>In Proceedings of the Language Technologies Conference, Information Society,</booktitle>
<location>Ljubljana, Slovenia.</location>
<marker>Karan, </marker>
<rawString>Mladen Karan, Jan &amp;quot;Snajder, and Bojana Dalbelo Ba&amp;quot;si´c. 2012. Distributional semantics approach to detecting synonyms in Croatian language. In Proceedings of the Language Technologies Conference, Information Society, Ljubljana, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing. Synthesis Lectures on Human Language Technologies.</title>
<date>2009</date>
<publisher>Morgan &amp; Claypool.</publisher>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra K¨ubler, Ryan McDonald, and Joakim Nivre. 2009. Dependency Parsing. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikola Ljubesi´c</author>
<author>Tomaz Erjavec</author>
</authors>
<title>hrWaC and slWac: Compiling web corpora for Croatian and Slovene.</title>
<date>2011</date>
<booktitle>In Proceedings of Text, Speech and Dialogue,</booktitle>
<pages>395--402</pages>
<location>Plze&amp;quot;n, Czech Republic.</location>
<marker>Ljubesi´c, Erjavec, 2011</marker>
<rawString>Nikola Ljube&amp;quot;si´c and Toma&amp;quot;z Erjavec. 2011. hrWaC and slWac: Compiling web corpora for Croatian and Slovene. In Proceedings of Text, Speech and Dialogue, pages 395–402, Plze&amp;quot;n, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikola Ljubesi´c</author>
<author>Damir Boras</author>
<author>Nikola Bakari´c</author>
<author>Jasmina Njavro</author>
</authors>
<title>Comparing measures of semantic similarity.</title>
<date>2008</date>
<booktitle>In Proceedings of the ITI 2008 30th International Conference of Information Technology Interfaces,</booktitle>
<location>Cavtat, Croatia.</location>
<marker>Ljubesi´c, Boras, Bakari´c, Njavro, 2008</marker>
<rawString>Nikola Ljube&amp;quot;si´c, Damir Boras, Nikola Bakari´c, and Jasmina Njavro. 2008. Comparing measures of semantic similarity. In Proceedings of the ITI 2008 30th International Conference of Information Technology Interfaces, Cavtat, Croatia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X,</booktitle>
<pages>216--220</pages>
<location>New York, NY.</location>
<contexts>
<context position="9656" citStr="McDonald et al., 2006" startWordPosition="1485" endWordPosition="1488">.HR. SETIMES.HR consists of 90K tokens and 4K sentences, manually lemmatized and MSD-tagged according to Multext East v4 tagset (Erjavec, 2012), with the help of the Croatian Lemmatization Server (Tadi´c, 2005). It is used also as a basis for a novel formalism for syntactic annotation and dependency parsing of Croatian (Agi´c and Merkler, 2013). On the basis of previous evaluation for Croatian (Agi´c et al., 2008; Agi´c et al., 2009; Agi´c, 2012) and availability and licensing considerations, we chose HunPos tagger (Hal´acsy et al., 2007), CST lemmatizer (Ingason et al., 2008), and MSTParser (McDonald et al., 2006) to process hrWaC. We evaluated the tools on 100-sentence test sets from SETIMES.HR and Wikipedia; performance on Wikipedia should be indicative of the performance on a cross-domain dataset, such as hrWaC. In Table 1 we show lemmatization and tagging accuracy, as well as dependency parsing accuracy in terms of labeled attachment score (LAS). The results show that lemmatization, tagging and parsing accuracy improves on the state of the art for Croatian. The SETIMES.HR dependency parsing models are publicly available.3 Syntactic patterns. We collect the co-occurrence counts of tuples using a set</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proceedings of CoNLL-X, pages 216–220, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Mitrofanova</author>
<author>Anton Mukhin</author>
<author>Polina Panicheva</author>
<author>Vyacheslav Savitsky</author>
</authors>
<title>Automatic word clustering in Russian texts.</title>
<date>2007</date>
<booktitle>In Proceedings of Text, Speech and Dialogue,</booktitle>
<pages>85--91</pages>
<location>Plze&amp;quot;n, Czech Republic.</location>
<contexts>
<context position="3118" citStr="Mitrofanova et al., 2007" startWordPosition="454" endWordPosition="457"> corpus. In this paper, we describe the construction of a Distributional Memory for Croatian (DM.HR), a free word order language. To do so, we parse hrWaC (Ljubeˇsi´c and Erjavec, 2011), a 1.2B-token Croatian web corpus. We evaluate DM.HR on a synonym choice task, where it outperforms the standard bag-of-word model for nouns and verbs. 2 Related Work Vector space semantic models have been applied to a number of Slavic languages, including Bulgarian (Nakov, 2001a), Czech (Smrˇz and Rychl´y, 2001), Polish (Piasecki, 2009; Broda et al., 2008; Broda and Piasecki, 2008), and Russian (Nakov, 2001b; Mitrofanova et al., 2007). Previous work on distributional semantic models for Croatian dealt with similarity prediction (Ljubeˇsi´c et al., 2008; Jankovi´c et al., 2011) and synonym detection (Karan et al., 2012), however using only wordbased and not syntactic-based models. So far the only DM for a language other than English is the German DM.DE by Pad´o and Utt (2012), who describe the process of building DM.DE and the evaluation on a synonym choice task. Our work is similar, though each language has its own challenges. Croatian, like other Slavic languages, has rich inflectional morphology and free word order, whic</context>
</contexts>
<marker>Mitrofanova, Mukhin, Panicheva, Savitsky, 2007</marker>
<rawString>Olga Mitrofanova, Anton Mukhin, Polina Panicheva, and Vyacheslav Savitsky. 2007. Automatic word clustering in Russian texts. In Proceedings of Text, Speech and Dialogue, pages 85–91, Plze&amp;quot;n, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Iryna Gurevych</author>
<author>Graeme Hirst</author>
<author>Torsten Zesch</author>
</authors>
<title>Cross-lingual distributional profiles of concepts for measuring semantic distance.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP/CoNLL,</booktitle>
<pages>571--580</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="15800" citStr="Mohammad et al. (2007)" startWordPosition="2472" endWordPosition="2475">y of Croatian. An example item is teˇzak (farmer): poljoprivrednik (farmer), umjetnost (art), radijacija (radiation), bod (point). We sampled from the dataset questions for nouns, verbs, and adjectives, with 1000 questions each.5 Additionally, we manually corrected some errors in the dataset, introduced by the automatic extraction procedure. To make predictions, we compute pairwise cosine similarities of the target word vectors with the four candidates and predict the candidate(s) with maximal similarity (note that there may be ties). Evaluation. Our evaluation follows the scheme developed by Mohammad et al. (2007), who define accuracy as the average number of correct predictions per covered question. Each correct prediction with a single most similar candidate receives a full credit (A), while ties for maximal similarity are discounted (B: two-way tie, C: three-way tie, D: four-way tie): A+ 12B+ 13C+ 14D. We consider a question item to be covered if the target and at least one answer word are modeled. In our experiments, ties occur when vector similarities are zero for all word pairs (due to vector sparsity). Note that a random baseline would perform at 0.25 accuracy. As baseline to compare against the</context>
</contexts>
<marker>Mohammad, Gurevych, Hirst, Zesch, 2007</marker>
<rawString>Saif Mohammad, Iryna Gurevych, Graeme Hirst, and Torsten Zesch. 2007. Cross-lingual distributional profiles of concepts for measuring semantic distance. In Proceedings of EMNLP/CoNLL, pages 571–580, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Latent semantic analysis for Bulgarian literature.</title>
<date>2001</date>
<booktitle>In Proceedings of Spring Conference of Bulgarian Mathematicians</booktitle>
<location>Union, Borovets, Bulgaria.</location>
<contexts>
<context position="2958" citStr="Nakov, 2001" startWordPosition="432" endWordPosition="433">g distance dependencies and non-projective structure (K¨ubler et al., 2009). Their obvious problem, of course, is that they require a large parsed corpus. In this paper, we describe the construction of a Distributional Memory for Croatian (DM.HR), a free word order language. To do so, we parse hrWaC (Ljubeˇsi´c and Erjavec, 2011), a 1.2B-token Croatian web corpus. We evaluate DM.HR on a synonym choice task, where it outperforms the standard bag-of-word model for nouns and verbs. 2 Related Work Vector space semantic models have been applied to a number of Slavic languages, including Bulgarian (Nakov, 2001a), Czech (Smrˇz and Rychl´y, 2001), Polish (Piasecki, 2009; Broda et al., 2008; Broda and Piasecki, 2008), and Russian (Nakov, 2001b; Mitrofanova et al., 2007). Previous work on distributional semantic models for Croatian dealt with similarity prediction (Ljubeˇsi´c et al., 2008; Jankovi´c et al., 2011) and synonym detection (Karan et al., 2012), however using only wordbased and not syntactic-based models. So far the only DM for a language other than English is the German DM.DE by Pad´o and Utt (2012), who describe the process of building DM.DE and the evaluation on a synonym choice task. Our</context>
</contexts>
<marker>Nakov, 2001</marker>
<rawString>Preslav Nakov. 2001a. Latent semantic analysis for Bulgarian literature. In Proceedings of Spring Conference of Bulgarian Mathematicians Union, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Latent semantic analysis for Russian literature investigation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 120 years Bulgarian Naval Academy Conference.</booktitle>
<contexts>
<context position="2958" citStr="Nakov, 2001" startWordPosition="432" endWordPosition="433">g distance dependencies and non-projective structure (K¨ubler et al., 2009). Their obvious problem, of course, is that they require a large parsed corpus. In this paper, we describe the construction of a Distributional Memory for Croatian (DM.HR), a free word order language. To do so, we parse hrWaC (Ljubeˇsi´c and Erjavec, 2011), a 1.2B-token Croatian web corpus. We evaluate DM.HR on a synonym choice task, where it outperforms the standard bag-of-word model for nouns and verbs. 2 Related Work Vector space semantic models have been applied to a number of Slavic languages, including Bulgarian (Nakov, 2001a), Czech (Smrˇz and Rychl´y, 2001), Polish (Piasecki, 2009; Broda et al., 2008; Broda and Piasecki, 2008), and Russian (Nakov, 2001b; Mitrofanova et al., 2007). Previous work on distributional semantic models for Croatian dealt with similarity prediction (Ljubeˇsi´c et al., 2008; Jankovi´c et al., 2011) and synonym detection (Karan et al., 2012), however using only wordbased and not syntactic-based models. So far the only DM for a language other than English is the German DM.DE by Pad´o and Utt (2012), who describe the process of building DM.DE and the evaluation on a synonym choice task. Our</context>
</contexts>
<marker>Nakov, 2001</marker>
<rawString>Preslav Nakov. 2001b. Latent semantic analysis for Russian literature investigation. In Proceedings of the 120 years Bulgarian Naval Academy Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Jason Utt</author>
</authors>
<title>A distributional memory for German.</title>
<date>2012</date>
<booktitle>In Proceedings of the KONVENS 2012 workshop on lexical-semantic resources and applications,</booktitle>
<pages>462--470</pages>
<location>Vienna, Austria.</location>
<marker>Pad´o, Utt, 2012</marker>
<rawString>Sebastian Pad´o and Jason Utt. 2012. A distributional memory for German. In Proceedings of the KONVENS 2012 workshop on lexical-semantic resources and applications, pages 462–470, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maciej Piasecki</author>
</authors>
<title>Automated extraction of lexical meanings from corpus: A case study of potentialities and limitations.</title>
<date>2009</date>
<booktitle>In Representing Semantics in Digital Lexicography. Innovative Solutions for Lexical Entry Content in Slavic Lexicography,</booktitle>
<pages>pages</pages>
<contexts>
<context position="3017" citStr="Piasecki, 2009" startWordPosition="440" endWordPosition="441">ubler et al., 2009). Their obvious problem, of course, is that they require a large parsed corpus. In this paper, we describe the construction of a Distributional Memory for Croatian (DM.HR), a free word order language. To do so, we parse hrWaC (Ljubeˇsi´c and Erjavec, 2011), a 1.2B-token Croatian web corpus. We evaluate DM.HR on a synonym choice task, where it outperforms the standard bag-of-word model for nouns and verbs. 2 Related Work Vector space semantic models have been applied to a number of Slavic languages, including Bulgarian (Nakov, 2001a), Czech (Smrˇz and Rychl´y, 2001), Polish (Piasecki, 2009; Broda et al., 2008; Broda and Piasecki, 2008), and Russian (Nakov, 2001b; Mitrofanova et al., 2007). Previous work on distributional semantic models for Croatian dealt with similarity prediction (Ljubeˇsi´c et al., 2008; Jankovi´c et al., 2011) and synonym detection (Karan et al., 2012), however using only wordbased and not syntactic-based models. So far the only DM for a language other than English is the German DM.DE by Pad´o and Utt (2012), who describe the process of building DM.DE and the evaluation on a synonym choice task. Our work is similar, though each language has its own challeng</context>
</contexts>
<marker>Piasecki, 2009</marker>
<rawString>Maciej Piasecki. 2009. Automated extraction of lexical meanings from corpus: A case study of potentialities and limitations. In Representing Semantics in Digital Lexicography. Innovative Solutions for Lexical Entry Content in Slavic Lexicography, pages 32– 43. Institute of Slavic Studies, Polish Academy of Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Smrz</author>
<author>Pavel Rychl´y</author>
</authors>
<title>Finding semantically related words in large corpora.</title>
<date>2001</date>
<booktitle>In Text, Speech and Dialogue,</booktitle>
<pages>108--115</pages>
<publisher>Springer.</publisher>
<marker>Smrz, Rychl´y, 2001</marker>
<rawString>Pavel Smr&amp;quot;z and Pavel Rychl´y. 2001. Finding semantically related words in large corpora. In Text, Speech and Dialogue, pages 108–115. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marko Tadi´c</author>
</authors>
<title>The Croatian Lemmatization Server.</title>
<date>2005</date>
<journal>Southern Journal of Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>217</pages>
<marker>Tadi´c, 2005</marker>
<rawString>Marko Tadi´c. 2005. The Croatian Lemmatization Server. Southern Journal of Linguistics, 29(1):206– 217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1506" citStr="Turney and Pantel, 2010" startWordPosition="208" endWordPosition="211">larly good performance on nouns. The resource is freely available. 1 Introduction Most current work in lexical semantics is based on the Distributional Hypothesis (Harris, 1954), which posits a correlation between the degree of words’ semantic similarity and the similarity of the contexts in which they occur. Using this hypothesis, word meaning representations can be extracted from large corpora. Words are typically represented as vectors whose dimensions correspond to context features. The vector similarities, which are interpreted as semantic similarities, are used in numerous applications (Turney and Pantel, 2010). Most vector spaces in current use are either wordbased (co-occurrence defined by surface window, context words as dimensions) or syntax-based (cooccurrence defined syntactically, syntactic objects as dimensions). Syntax-based models have several desirable properties. First, they are model to fine-grained types of semantic similarity such as predicate-argument plausibility (Erk et al., 2010). Second, they are more versatile – Baroni and Lenci (2010) have presented a generic framework, the Distributional Memory (DM), which is applicable to a wide range of tasks beyond word similarity. Third, t</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>