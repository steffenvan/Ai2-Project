<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000861">
<title confidence="0.9981815">
Finite-state Approximation of Constraint-based Grammars using
Left-corner Grammar Transforms
</title>
<author confidence="0.980474">
Mark Johnson*
</author>
<affiliation confidence="0.888572">
Cognitive and Linguistic Sciences, Box 1978
Brown University
</affiliation>
<email confidence="0.992675">
MarkJohnson@Brown.edu
</email>
<sectionHeader confidence="0.988062" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999908571428571">
This paper describes how to construct a finite-state
machine (FSM) approximating a &apos;unification-based&apos;
grammar using a left-corner grammar transform.
The approximation is presented as a series of gram-
mar transforms, and is exact for left-linear and right-
linear CFGs, and for trees up to a user-specified
depth of center-embedding.
</bodyText>
<sectionHeader confidence="0.993787" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999916766666667">
This paper describes a method for approximat-
ing grammars with finite-state machines. Unlike
the method derived from the LR(k) parsing algo-
rithm described in Pereira and Wright (1991), these
methods use grammar transformations based on the
left-corner grammar transform (Rosenkrantz and
Lewis II, 1970; Aho and Ullman, 1972). One ad-
vantage of the left corner methods is that they gen-
eralize straightforwardly to complex feature &amp;quot;unifi-
cation based&amp;quot; grammars, unlike the LR(k) based ap-
proach. For example, the implementation described
here translates a DCG version of the example gram-
mar given by Pereira and Wright (1991) directly into
a FSM without constructing an approximating CFG.
Left-corner based techniques are natural for this
kind of application because (with the simple opti-
mization described below) they can parse pure left-
branching or pure right-branching structures with
a stack depth of one (two if terminals are pushed
and popped from the stack). Higher stack depth
occurs with center-embedded structures, which hu-
mans find difficult to comprehend. This suggests
that we may get a finite-state approximation to hu-
man performance by simply imposing a stack depth
bound. We provide a simple tree-geometric descrip-
tion of the configurations that cause an increase in
a left corner parser&apos;s stack depth below.
The rest of this paper is structured as follows.
The remainder of this section outlines the &amp;quot;gram-
mar transform&amp;quot; approach, summarizes the top-down
</bodyText>
<listItem confidence="0.92344975">
• This research was supported by NSF grant SBR526978. I
began this research while I was on sabbatical at the Xerox
Research Centre in Grenoble, France. I would like to thank
them and my colleages at Brown for their support.
</listItem>
<bodyText confidence="0.9987541875">
parsing algorithm and discusses how finite state
approximations of top-down parsers can be con-
structed. The fact that this approximation is not ex-
act for left linear grammars (which define finite-state
languages) motivates a finite-state approximation
based on the left-corner parsing algorithm (which
is presented as a grammar transform in section 2).
In its standard form the approximation based on the
left-corner parsing algorithm suffers from the com-
plementary problem to the top-down approximation:
it is not exact for right-linear grammars, but the
&amp;quot;optimized&amp;quot; variants presented in section 3 over-
come this deficiency, resulting in finite-state CFG
approximations which are exact for left-linear and
right-linear grammars. Section 4 discusses how these
techniques can be combined in an implementation.
</bodyText>
<subsectionHeader confidence="0.973893">
1.1 Parsing strategies as grammar
transformations
</subsectionHeader>
<bodyText confidence="0.999967545454545">
The parsing algorithms discussed here are presented
as grammar transformations, i.e., functions T that
map a context-free grammar G into another context-
free grammar T(G). The transforms have the prop-
erty that a top-down parse using the transformed
grammar is isomorphic to some other kind of parse
using the original grammar. Thus grammar trans-
forms provide a simple, compact way of describing
various parsing algorithms, as a top-down parser us-
ing T(G) behaves identically to the kind of parser
we want to study using G.
</bodyText>
<subsectionHeader confidence="0.998239">
1.2 Mappings from trees to trees
</subsectionHeader>
<bodyText confidence="0.983862818181818">
The transformations presented here can also be un-
derstood as isomorphisms from the set of parse trees
of the source grammar G to parse trees of the trans-
formed grammar which preserve terminal strings.
Thus it is convenient to explain the transforms in
terms of their effect on parse trees. We call a parse
tree with respect to the source grammar G an anal-
ysis tree, in order to distinguish it from parse trees
with respect to some transform of G. The analy-
sis tree t in Figure 1 will be used as an example
throughout this paper.
</bodyText>
<page confidence="0.949701">
619
</page>
<equation confidence="0.896447413793103">
= S
£C1(t)
t =
NP VP
/NN
DET N V ADV
the dog ran fast
.CC2
DET S-DET
I
the N S-NP
I /\
dog S-S
V
ran
fast
(t) = S
DET S-DET
I /\
the N S-NP
I I
dog VP
V VP-V
I
ran ADV
fast
VP-V
ADV VP-VP
.CC4(t) =
</equation>
<figure confidence="0.966651428571429">
DET S-DET
I
the N S-NP
ZN
dog v VP-V
ran ADV
fast
</figure>
<figureCaption confidence="0.929479666666667">
Figure 1: The analysis tree t used as a running example below, and its left-corner transforms LCi(t). Note
that the phonological forms are treated here as annotations on the nodes drawn above them, rather than
independent nodes. That is, DET (annotated with the) is a terminal node.
</figureCaption>
<subsectionHeader confidence="0.999489">
1.3 Top-down parsers and parse trees
</subsectionHeader>
<bodyText confidence="0.999984363636364">
The &amp;quot;predictive&amp;quot; or &amp;quot;top-down&amp;quot; recognition algo-
rithm is one of the simplest CFG recognition al-
gorithms. Given a CFG G = (N ,T , P, S), a (top-
down) stack state is a sequence of terminals and
nonterminals. Let Q = (N UT)* be the set of stack
states for G. The start state qo E Q is the sequence
S, and the final state qf E Q is the empty sequence e.
The state transition function 8: Q x (TU {e}) 1-4 2Q
maps a state and a terminal or epsilon into a set of
states. It is the smallest function 8 that satisfies the
following conditions:
</bodyText>
<equation confidence="0.979715">
E (a7 , a) :a ET,7E (N U T)* •
07 E (A-y , E) :A E N,76 (N U T)* , A E P.
</equation>
<bodyText confidence="0.999968125">
A string w is accepted by the top-down recognition
algorithm if qf E (5* (q0, w), where 8* is the reflex-
ive transitive closure of 5 with respect to epsilon
moves. Extending this top-down parsing algorithm
to a &apos;unification-based&apos; grammar is straight-forward,
and described in many textbooks, such as Pereira
and Shieber (1987).
It is easy to read off the stack states of a top-
down parser constructing a parse tree from the tree
itself. For any node X in the tree, the stack contents
of a top-down parser just before the construction
of X consists of (the label of) X followed by the
sequence of labels on the right siblings of the nodes
encountered on the path from X back to the root.
It is easy to check that a top-down parser requires a
stack of depth 3 to construct the tree t depicted in
</bodyText>
<figureCaption confidence="0.548344">
Figure 1.
</figureCaption>
<subsectionHeader confidence="0.9891">
1.4 Finite-state approximations
</subsectionHeader>
<bodyText confidence="0.999935357142857">
We obtain a finite-state approximation to a top-
down parser by restricting attention to only a finite
number of possible stack states. The system imple-
mented here imposes a stack depth restriction, i.e.,
the transition function is modified so that there are
no transitions to any stack state whose size is larger
than some user-specified limit.&apos; This restriction en-
sures that there is only a finite number of possible
stack states, and hence that the top down parser
is an finite-state machine. The resulting finite-state
machine accepts a subset of the language generated
by the original grammar.
The situation becomes more complicated when we
move to &apos;unification-based&apos; grammars, since there
may be an unbounded number of different categories
appearing in the accessible stack states. In the sys-
tem implemented here we used restriction (Shieber,
1985) on the stack states to restrict attention to a
finite number of distinct stack states for any given
stack depth. Since the restriction operation maps
a stack state to a more general one, it produces a
finite-state approximation which accepts a superset
of the language generated by the original unification
grammar. Thus for general constraint-based gram-
mars the language accepted by our finite-state ap-
proximation is not guaranteed to be either a superset
or a subset of the language generated by the input
grammar.
</bodyText>
<sectionHeader confidence="0.732542" genericHeader="method">
2 The left-corner transform
</sectionHeader>
<bodyText confidence="0.972918846153846">
While conceptually simple, the top-down parsing al-
gorithm presented in the last section suffers from
a number of drawbacks for a finite-state approxi-
mation. For example, the number of distinct ac-
cessible stack states is unbounded if the grammar
is left-recursive, yet left-linear grammars always
generate regular languages. This section presents
&apos;With the optimized left-corner transforms described be-
low we obtain acceptable approximations with a stack size
limit of 5 or less. In many useful cases, including the example
grammar provided by Pereira and Wright (1991), this stack
bound is never reached and the system reports that the FSA
it returns is exact.
</bodyText>
<page confidence="0.988349">
620
</page>
<bodyText confidence="0.994648607142857">
the standard left-corner grammar transformation
(Rosenkrantz and Lewis II, 1970; Aho and Ull-
man, 1972); these references should be consulted for
proofs of correctness. This transform serves as the
basis for the further transforms described in the next
section; these transforms have the property that the
output grammar induces a finite number of distinct
accessible stack states if their input is a left-recursive
left-linear grammar.
Given an input grammar G with nonterminals
N and terminals T, these transforms a i produce
grammars with an enlarged set of nonterminals N&apos; =
N U (N x (N UT)). The new &amp;quot;pair&amp;quot; categories in
N x (N U T) are written A-X, where A is a non-
terminal of G and X is either a terminal or non-
terminal of G. It turns out that if A = X-y then
A-X =.*cci(G) -y, i.e., a non-terminal A-X in the
transformed grammar derives the difference between
A and X in the original grammar, and the notation
is meant to be suggestive of this.
The left-corner transform of a CFG G =
(N,T, P, S) is a grammar LCi (G) = (N&apos; , , S)
where P1 contains all productions of the form (1.a-
1.c). This paper assumes that N n T = 0, as is
standard. To save space we assume that P does not
contain any epsilon productions (but it is straight-
forward to deal with them).
A -4 a A-a : A E N,a E T. (1.a)
</bodyText>
<equation confidence="0.779673">
A-X A-B : A E N,B -+ X E P. (1.b)
A-A--€:AE N. (1.c)
</equation>
<bodyText confidence="0.999960739130435">
Informally, the productions (1.a) start the left-
corner recognition of A by recognizing a terminal
a as a possible left-corner of A. The actual left-
corner recognition is performed by the productions
(1.b), which extend the left-corner from X to its
parent B by recognizing 0; these productions are
used repeatedly to construct increasingly larger left-
corners. Finally, the productions (1.c) terminate the
recognition of A when this left-corner construction
process has constructed an A.
The left-corner transform preserves the number
of parses of a string, so it defines an isomorphism
from analysis trees (i.e., parse trees with respect to
G) to parse trees with respect to LCi(G). If t is a
parse tree with respect to G then (abusing notation)
LC1(t) is the corresponding parse tree with respect
to LC1(G). Figure 1 shows the effect of this map-
ping on a simple tree. The transformed tree is con-
siderably more complex: it has double the number
of nodes of the original tree. In a top-down parse
of the tree LCi (t) in Figure 1 the maximum stack
depth is 3, which occurs at the recognition of the
terminals ran and fast.
</bodyText>
<subsectionHeader confidence="0.969357">
2.1 Filtering useless categories
</subsectionHeader>
<bodyText confidence="0.999896909090909">
In general the grammar produced by the transform
LCi(G) contains a large number of useless nonter-
minals, i.e., non-terminals which can never appear
in any complete derivation, even if the grammar G is
fully pruned (i.e., contains no useless productions).
While CC1(G) can be pruned using standard algo-
rithms, given the observation about the relationship
between the pair non-terminals in LC1(G) and non-
terminals in G, it is clear that certain productions
can be discarded immediately as useless. Define the
left-corner relation 4 C (N U T) x N as follows:
</bodyText>
<equation confidence="0.705255">
X &lt; A if 30. A -4 X [3 E P,
</equation>
<bodyText confidence="0.999492714285714">
Let a* be the reflexive and transitive closure of 4.
It is easy to show that a category A-X is useless
in LCi (G) (i.e., derives no sequence of terminals)
unless X a* A. Thus we can restrict the productions
in (1.a-1.c) without affecting the language (strongly)
generated to those that only contain pair categories
A-X where X1* A.
</bodyText>
<subsectionHeader confidence="0.986221">
2.2 Unification grammars
</subsectionHeader>
<bodyText confidence="0.999984807692308">
One of the main advantages of left-corner parsing
algorithms over LR(k) based parsing algorithms is
that they extend straight-forwardly to complex fea-
ture based &amp;quot;unification&amp;quot; grammars. The transfor-
mation LC1 itself can be encoded in several lines of
Prolog (Matsumoto et al., 1983; Pereira and Shieber,
1987). This contrasts with the LR(k) methods. In
LR(k) parsing a single LR state may correspond
to several items or dotted rules, so it is not clear
how the feature &amp;quot;unification&amp;quot; constraints should be
associated with transitions from LR state to LR
state (see Nakazawa (1995) for one proposal). In
contrast, extending the techniques described here
to complex feature based &amp;quot;unification&amp;quot; grammar is
straight-forward.
The main complication is the filter on useless non-
terminals and productions just discussed. General-
izing the left-corner closure filter on pair categories
to complex feature &amp;quot;unification&amp;quot; grammars in an ef-
ficient way is complicated, and is the primary diffi-
culty in using left-corner methods with complex fea-
ture based grammars. van Noord (1997) provides
a detailed discussion of methods for using such a
&amp;quot;left-corner filter&amp;quot; in unification-grammar parsing,
and the methods he discusses are used in the imple-
mentation described below.
</bodyText>
<sectionHeader confidence="0.961467" genericHeader="method">
3 Extended left-corner transforms
</sectionHeader>
<bodyText confidence="0.999899">
This section presents some simple extensions to the
basic left-corner transform presented above. The
&apos;tail-recursion&apos; optimization permits bounded-stack
parsing of both left and right linear constructions.
Further manipulation of this transform puts it into a
form in which we can identify precisely the tree con-
figurations in the original grammar which cause the
stack size of a left-corner parser to increase. These
</bodyText>
<page confidence="0.997449">
621
</page>
<bodyText confidence="0.9999385">
observations motivate the special binarization meth-
ods described in the next section, which minimize
stack depth in grammars that contain productions
of length no greater than two.
</bodyText>
<subsectionHeader confidence="0.999467">
3.1 A tail-recursion optimization
</subsectionHeader>
<bodyText confidence="0.999458772727273">
If G is a left-linear grammar, a top-down parser us-
ing GC1(G) can recognize any string generated by G
with a constant-bounded stack size. However, the
corresponding operation with right-linear grammars
requires a stack of size proportional to the length
of the string, since the stack fills with paired cate-
gories A—A for each non-left-corner nonterminal in
the analysis tree.
The &apos;tail recursion&apos; or &apos;composition&apos; optimiza-
tion (Abney and Johnson, 1991; Resnik, 1992) per-
mits right-branching structures to be parsed with
bounded stack depth. It is the result of epsilon re-
moval applied to the output of GC1, and can be de-
scribed in terms of resolution or partial evaluation
of the transformed grammar with respect to pro-
ductions (1.c). In effect, the schema (1.b) is split
into two cases, depending on whether or not the
rightmost nonterminal A—B is expanded by the ep-
silon rules produced by schema (1.c). This expansion
yields a grammar £C2 (G) = (N&apos;, T, P2 S ) , where P2
contains all productions of the form (2.a-2.c). (In
these schemata A, B E N; a E T; X E NUT and
</bodyText>
<equation confidence="0.588336">
E (N U T)*).
</equation>
<bodyText confidence="0.906028944444445">
A -4 a A—a
A—X A—B B X E P.
A—X-4/3:A-4X0EP.
Figure 1 shows the effect of the transform 12C2 on
the example tree. The maximum stack depth re-
quired for this tree is 2. When this &apos;tail recursion&apos;
optimization is applied, pair categories in the trans-
formed grammar encode proper left-corner relation-
ships between nodes in the analysis tree. This lets
us strengthen the &apos;useless category&apos; filter described
above as follows. Let a+ be the transitive closure of
the left-corner relation a defined above. It is easy
to show that a category A—X is useless in 12C2 (G)
(i.e., derives no sequence of terminals) unless Xa+ A.
Thus we can restrict the productions in (2.a-2.b)
without affecting the language (strongly) generated
to just those that only contain pair categories A—X
where X a+ A.
</bodyText>
<subsectionHeader confidence="0.99995">
3.2 The special case of binary productions
</subsectionHeader>
<bodyText confidence="0.999473">
We can get a better idea of the properties of transfor-
mation LC2 if we investigate the special case where
the productions of G are unary or binary. In this
situation, transformation £C2(G) can be more ex-
</bodyText>
<footnote confidence="0.69385925">
plicitly written as CCa(G) = (N&apos;,T, P3 , S ) , where
P3 contains all instances of the production schemata
(3.a-3.e). (In these schemata, a E T; A, B E N and
X,YENUT).
</footnote>
<figure confidence="0.9792115">
a
A—X aC—a A—B (41)
</figure>
<figureCaption confidence="0.9343265">
Figure 2: The highly distinctive &amp;quot;zig-zag&amp;quot; or &amp;quot;light-
ning bolt&amp;quot; configuration of nodes in the analysis tree
</figureCaption>
<bodyText confidence="0.8419015">
characteristic of the use of production schema (4.f)
in transform LC4. This is the only configuration
which causes an increase in stack depth in a top-
down parser using a grammar transformed with LC4.
</bodyText>
<figure confidence="0.8466748">
A a A—a. (3.a)
A—X A—B B E P. (3.b)
A—X-4e:A—*XE P. (3.c)
A—X A—B B X Y P. (3.d)
A—X 1&apos; : A -4 X Y E P. (3.e)
</figure>
<bodyText confidence="0.9858879375">
Productions (3.b-3.c) and (3.d-3.e) correspond to
unary and binary productions respectively in the
original grammar. Now, note that nonterminals
from N only appear in the right hand sides of pro-
ductions of type (3.d) and (3.e). Moreover, any such
nonterminals must be immediately expanded by a
production of type (3.a). Thus these non-terminals
are eliminable by resolving them with (3.a); the
only remaining nonterminal is the start symbol S.
This expansion yields a new transform LC4, where
£C4(G) = ({S} u (N x (N U T)), P4, S). P4 de-
fined in (4.a-4.g), still contains productions of type
(3.a), but these only expand the start symbol, as all
occurences of nonterminals in N have been resolved
away. (In these schemata a E T; A,B,C,D E N
andXENUT).
</bodyText>
<figure confidence="0.941765857142857">
S a S—a. (4.a)
A—X (4.b)
(4.c)
A—X a A—B B X a E P. (4.d)
A—X --&gt;a:A-4Xae P. (4.e)
A—X A—B B X C E P. (4.f)
A—X —+ aC—a : A -4 X C P. (4.g)
</figure>
<bodyText confidence="0.999224545454545">
In the production schemata defining LC4, (4.a-4.c)
are copied directly from (3.a-3.c) respectively. The
schemata (4.d-4.e) are obtained by instantiating Y
in (3.d-3.e) to a terminal a E T, while the other two
schemata (4.f-4.g) are obtained by instantiating Y in
(3.d-3.e) with the right hand sides of (3.a). Figure 1
shows the result of applying the transformation £C4
to the example analysis tree t.
The transform also simplifies the specification of
finite-state machine approximations. Because all
terminals are introduced as the left-most symbols in
</bodyText>
<page confidence="0.994943">
622
</page>
<bodyText confidence="0.999991606060606">
their productions, there is no need for terminal sym-
bols to appear on the parser&apos;s stack, saving an ep-
silon transition associated with a stack push and an
immediately following stack pop with respect to the
standard left-corner algorithm. Productions (4.a)
and (4.d-4.g) can be understood as transitions over
a terminal a that replace the top stack element with
a sequence of other elements, while the other produc-
tions can be interpreted as epsilon transitions that
manipulate the stack contents accordingly.
Note that the right hand sides of all of these
productions except for schema (4.f) are right-linear.
Thus instances of this schema are the only produc-
tions that can increase the stack size in a top-down
parse with LC4(G), and the stack depth required
to parse an analysis tree is the maximum number
of &amp;quot;zig-zag&amp;quot; patterns in the path in the analysis
tree from any terminal node to the root. Figure 2
sketches the configuration of nodes in the analysis
trees in which instances of schemata (4.f) would be
used in a parse using LC4(G). This highly distinc-
tive &amp;quot;zig-zag&amp;quot; or &amp;quot;lightning bolt&amp;quot; pattern does not
occur at all in the example tree tin Figure 1, so the
maximum required stack depth is 2. (Recall that in
a traditional top-down parser terminals are pushed
onto the stack and popped later, so initialization
productions (4.a) cause two symbols to be pushed
onto the stack). It follows that this finite state ap-
proximation is exact for left-linear and right-linear
CFGs. Indeed, analysis trees that consist simply of a
left-branching subtree followed by a right-branching
subtree, such as the example tree t, are transformed
into strictly right-branching trees by LC4.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.999966409090909">
This section provides further details of the finite-
state approximator implemented in this research.
The approximator is written in Sicstus Prolog. It
takes a user-specifier Definite Clause Grammar G
(without Prolog annotations) as input, which it bi-
narizes and then applies transform LC4 to.
The implementation annotates each transition
with the production it corresponds to (represented
as a pair of a LC4 schema number and a produc-
tion number from G), so the finite-state approxima-
tion actually defines a transducer which transduces
a lexical input to a sequence of productions which
specify a parse of that input with respect to LC4(G).
A following program inverts the tree transform LC4,
returning a corresponding parse tree with respect
to G. This parse tree can be checked by perform-
ing complete unifications with respect to the orig-
inal grammar productions if so desired. Thus the
finite-state approximation provides an efficient way
of determining if an analysis of a given input string
with respect to a unification grammar G exists, and
if so, it can be used to suggest such analyses.
</bodyText>
<sectionHeader confidence="0.99936" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999953111111111">
This paper surveyed the issues arising in the con-
struction of finite-state approximations of left-corner
parsers. The different kinds of parsers were pre-
sented as grammar transforms, which let us abstract
away from the algorithmic details of parsing algo-
rithms themselves. It derived the various forms of
the left-corner parsing algorithms in terms of gram-
mar transformations from the original left-corner
grammar transform.
</bodyText>
<sectionHeader confidence="0.999402" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999538266666667">
Stephen Abney and Mark Johnson. 1991. Mem-
ory requirements and local ambiguities of parsing
strategies. Journal of Psycholinguistic Research,
20(3):233-250.
Alfred V. Aho and Jeffery D. Ullman. 1972. The
Theory of Parsing, Translation and Compiling;
Volume I: Parsing. Prentice-Hall, Englewood
Cliffs, New Jersey.
Yuji Matsumoto, Hozumi Tanaka, Hideki Hirakawa,
Hideo Miyoshi, and Hideki Yasukawa. 1983.
BUP: A bottom-up parser embedded in Prolog.
New Generation Computing, 1(2):145-158.
Tsuneko Nakazawa. 1995. Construction of LR pars-
ing tables for grammars using feature-based syn-
tactic categories. In Jennifer Cole, Georgia M.
Green, and Jerry L. Morgan, editors, Linguis-
tics and Computation, number 52 in CSLI Lecture
Notes Series, pages 199-219, Stanford, California.
CSLI Publications.
Fernando C.N. Pereira and Stuart M. Shieber. 1987.
Prolog and Natural Language Analysis. Num-
ber 10 in CSLI Lecture Notes Series. Chicago Uni-
versity Press, Chicago.
Fernando C. N. Pereira and Rebecca N. Wright.
1991. Finite state approximation of phrase struc-
ture grammars. In The Proceedings of the 29th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 246-255.
Philip Resnik. 1992. Left-corner parsing and psy-
chological plausibility. In The Proceedings of the
fifteenth International Conference on Computa-
tional Linguistics, COLING-92, volume 1, pages
191-197.
Stanley J. Rosenkrantz and Philip M. Lewis II.
1970. Deterministic left corner parser. In IEEE
Conference Record of the 11th Annual Symposium
on Switching and Automata, pages 139-152.
Stuart M. Shieber. 1985. Using Restriction to ex-
tend parsing algorithms for unification-based for-
malisms. In Proceedings of the 23rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 145-152, Chicago.
Gertjan van Noord. 1997. An efficient implemen-
tation of the head-corner parser. Computational
Linguistics, 23(3) :425-456.
</reference>
<page confidence="0.999159">
623
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.537925">
<title confidence="0.998551">Finite-state Approximation of Constraint-based Grammars using Left-corner Grammar Transforms</title>
<author confidence="0.999974">Mark Johnson</author>
<pubnum confidence="0.554884">Cognitive and Linguistic Sciences, Box 1978</pubnum>
<affiliation confidence="0.999916">Brown University</affiliation>
<email confidence="0.991067">MarkJohnson@Brown.edu</email>
<abstract confidence="0.997561">This paper describes how to construct a finite-state machine (FSM) approximating a &apos;unification-based&apos; grammar using a left-corner grammar transform. The approximation is presented as a series of grammar transforms, and is exact for left-linear and rightlinear CFGs, and for trees up to a user-specified depth of center-embedding.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephen Abney</author>
<author>Mark Johnson</author>
</authors>
<title>Memory requirements and local ambiguities of parsing strategies.</title>
<date>1991</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>20--3</pages>
<contexts>
<context position="14074" citStr="Abney and Johnson, 1991" startWordPosition="2362" endWordPosition="2365">on methods described in the next section, which minimize stack depth in grammars that contain productions of length no greater than two. 3.1 A tail-recursion optimization If G is a left-linear grammar, a top-down parser using GC1(G) can recognize any string generated by G with a constant-bounded stack size. However, the corresponding operation with right-linear grammars requires a stack of size proportional to the length of the string, since the stack fills with paired categories A—A for each non-left-corner nonterminal in the analysis tree. The &apos;tail recursion&apos; or &apos;composition&apos; optimization (Abney and Johnson, 1991; Resnik, 1992) permits right-branching structures to be parsed with bounded stack depth. It is the result of epsilon removal applied to the output of GC1, and can be described in terms of resolution or partial evaluation of the transformed grammar with respect to productions (1.c). In effect, the schema (1.b) is split into two cases, depending on whether or not the rightmost nonterminal A—B is expanded by the epsilon rules produced by schema (1.c). This expansion yields a grammar £C2 (G) = (N&apos;, T, P2 S ) , where P2 contains all productions of the form (2.a-2.c). (In these schemata A, B E N; a</context>
</contexts>
<marker>Abney, Johnson, 1991</marker>
<rawString>Stephen Abney and Mark Johnson. 1991. Memory requirements and local ambiguities of parsing strategies. Journal of Psycholinguistic Research, 20(3):233-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffery D Ullman</author>
</authors>
<title>The Theory of Parsing, Translation and Compiling; Volume I: Parsing. Prentice-Hall, Englewood Cliffs,</title>
<date>1972</date>
<location>New Jersey.</location>
<contexts>
<context position="868" citStr="Aho and Ullman, 1972" startWordPosition="116" endWordPosition="119">tate machine (FSM) approximating a &apos;unification-based&apos; grammar using a left-corner grammar transform. The approximation is presented as a series of grammar transforms, and is exact for left-linear and rightlinear CFGs, and for trees up to a user-specified depth of center-embedding. 1 Introduction This paper describes a method for approximating grammars with finite-state machines. Unlike the method derived from the LR(k) parsing algorithm described in Pereira and Wright (1991), these methods use grammar transformations based on the left-corner grammar transform (Rosenkrantz and Lewis II, 1970; Aho and Ullman, 1972). One advantage of the left corner methods is that they generalize straightforwardly to complex feature &amp;quot;unification based&amp;quot; grammars, unlike the LR(k) based approach. For example, the implementation described here translates a DCG version of the example grammar given by Pereira and Wright (1991) directly into a FSM without constructing an approximating CFG. Left-corner based techniques are natural for this kind of application because (with the simple optimization described below) they can parse pure leftbranching or pure right-branching structures with a stack depth of one (two if terminals ar</context>
<context position="8357" citStr="Aho and Ullman, 1972" startWordPosition="1396" endWordPosition="1400">oximation. For example, the number of distinct accessible stack states is unbounded if the grammar is left-recursive, yet left-linear grammars always generate regular languages. This section presents &apos;With the optimized left-corner transforms described below we obtain acceptable approximations with a stack size limit of 5 or less. In many useful cases, including the example grammar provided by Pereira and Wright (1991), this stack bound is never reached and the system reports that the FSA it returns is exact. 620 the standard left-corner grammar transformation (Rosenkrantz and Lewis II, 1970; Aho and Ullman, 1972); these references should be consulted for proofs of correctness. This transform serves as the basis for the further transforms described in the next section; these transforms have the property that the output grammar induces a finite number of distinct accessible stack states if their input is a left-recursive left-linear grammar. Given an input grammar G with nonterminals N and terminals T, these transforms a i produce grammars with an enlarged set of nonterminals N&apos; = N U (N x (N UT)). The new &amp;quot;pair&amp;quot; categories in N x (N U T) are written A-X, where A is a nonterminal of G and X is either a </context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffery D. Ullman. 1972. The Theory of Parsing, Translation and Compiling; Volume I: Parsing. Prentice-Hall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Hozumi Tanaka</author>
<author>Hideki Hirakawa</author>
<author>Hideo Miyoshi</author>
<author>Hideki Yasukawa</author>
</authors>
<title>BUP: A bottom-up parser embedded in Prolog.</title>
<date>1983</date>
<journal>New Generation Computing,</journal>
<pages>1--2</pages>
<contexts>
<context position="11977" citStr="Matsumoto et al., 1983" startWordPosition="2044" endWordPosition="2047">eflexive and transitive closure of 4. It is easy to show that a category A-X is useless in LCi (G) (i.e., derives no sequence of terminals) unless X a* A. Thus we can restrict the productions in (1.a-1.c) without affecting the language (strongly) generated to those that only contain pair categories A-X where X1* A. 2.2 Unification grammars One of the main advantages of left-corner parsing algorithms over LR(k) based parsing algorithms is that they extend straight-forwardly to complex feature based &amp;quot;unification&amp;quot; grammars. The transformation LC1 itself can be encoded in several lines of Prolog (Matsumoto et al., 1983; Pereira and Shieber, 1987). This contrasts with the LR(k) methods. In LR(k) parsing a single LR state may correspond to several items or dotted rules, so it is not clear how the feature &amp;quot;unification&amp;quot; constraints should be associated with transitions from LR state to LR state (see Nakazawa (1995) for one proposal). In contrast, extending the techniques described here to complex feature based &amp;quot;unification&amp;quot; grammar is straight-forward. The main complication is the filter on useless nonterminals and productions just discussed. Generalizing the left-corner closure filter on pair categories to com</context>
</contexts>
<marker>Matsumoto, Tanaka, Hirakawa, Miyoshi, Yasukawa, 1983</marker>
<rawString>Yuji Matsumoto, Hozumi Tanaka, Hideki Hirakawa, Hideo Miyoshi, and Hideki Yasukawa. 1983. BUP: A bottom-up parser embedded in Prolog. New Generation Computing, 1(2):145-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsuneko Nakazawa</author>
</authors>
<title>Construction of LR parsing tables for grammars using feature-based syntactic categories.</title>
<date>1995</date>
<booktitle>Linguistics and Computation, number 52 in CSLI Lecture Notes Series,</booktitle>
<pages>199--219</pages>
<editor>In Jennifer Cole, Georgia M. Green, and Jerry L. Morgan, editors,</editor>
<publisher>CSLI Publications.</publisher>
<location>Stanford, California.</location>
<contexts>
<context position="12275" citStr="Nakazawa (1995)" startWordPosition="2095" endWordPosition="2096">here X1* A. 2.2 Unification grammars One of the main advantages of left-corner parsing algorithms over LR(k) based parsing algorithms is that they extend straight-forwardly to complex feature based &amp;quot;unification&amp;quot; grammars. The transformation LC1 itself can be encoded in several lines of Prolog (Matsumoto et al., 1983; Pereira and Shieber, 1987). This contrasts with the LR(k) methods. In LR(k) parsing a single LR state may correspond to several items or dotted rules, so it is not clear how the feature &amp;quot;unification&amp;quot; constraints should be associated with transitions from LR state to LR state (see Nakazawa (1995) for one proposal). In contrast, extending the techniques described here to complex feature based &amp;quot;unification&amp;quot; grammar is straight-forward. The main complication is the filter on useless nonterminals and productions just discussed. Generalizing the left-corner closure filter on pair categories to complex feature &amp;quot;unification&amp;quot; grammars in an efficient way is complicated, and is the primary difficulty in using left-corner methods with complex feature based grammars. van Noord (1997) provides a detailed discussion of methods for using such a &amp;quot;left-corner filter&amp;quot; in unification-grammar parsing, a</context>
</contexts>
<marker>Nakazawa, 1995</marker>
<rawString>Tsuneko Nakazawa. 1995. Construction of LR parsing tables for grammars using feature-based syntactic categories. In Jennifer Cole, Georgia M. Green, and Jerry L. Morgan, editors, Linguistics and Computation, number 52 in CSLI Lecture Notes Series, pages 199-219, Stanford, California. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Stuart M Shieber</author>
</authors>
<title>Prolog and Natural Language Analysis.</title>
<date>1987</date>
<journal>Number</journal>
<booktitle>in CSLI Lecture Notes Series.</booktitle>
<volume>10</volume>
<publisher>Chicago University Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="5682" citStr="Pereira and Shieber (1987)" startWordPosition="957" endWordPosition="960">al state qf E Q is the empty sequence e. The state transition function 8: Q x (TU {e}) 1-4 2Q maps a state and a terminal or epsilon into a set of states. It is the smallest function 8 that satisfies the following conditions: E (a7 , a) :a ET,7E (N U T)* • 07 E (A-y , E) :A E N,76 (N U T)* , A E P. A string w is accepted by the top-down recognition algorithm if qf E (5* (q0, w), where 8* is the reflexive transitive closure of 5 with respect to epsilon moves. Extending this top-down parsing algorithm to a &apos;unification-based&apos; grammar is straight-forward, and described in many textbooks, such as Pereira and Shieber (1987). It is easy to read off the stack states of a topdown parser constructing a parse tree from the tree itself. For any node X in the tree, the stack contents of a top-down parser just before the construction of X consists of (the label of) X followed by the sequence of labels on the right siblings of the nodes encountered on the path from X back to the root. It is easy to check that a top-down parser requires a stack of depth 3 to construct the tree t depicted in Figure 1. 1.4 Finite-state approximations We obtain a finite-state approximation to a topdown parser by restricting attention to only</context>
<context position="12005" citStr="Pereira and Shieber, 1987" startWordPosition="2048" endWordPosition="2051">closure of 4. It is easy to show that a category A-X is useless in LCi (G) (i.e., derives no sequence of terminals) unless X a* A. Thus we can restrict the productions in (1.a-1.c) without affecting the language (strongly) generated to those that only contain pair categories A-X where X1* A. 2.2 Unification grammars One of the main advantages of left-corner parsing algorithms over LR(k) based parsing algorithms is that they extend straight-forwardly to complex feature based &amp;quot;unification&amp;quot; grammars. The transformation LC1 itself can be encoded in several lines of Prolog (Matsumoto et al., 1983; Pereira and Shieber, 1987). This contrasts with the LR(k) methods. In LR(k) parsing a single LR state may correspond to several items or dotted rules, so it is not clear how the feature &amp;quot;unification&amp;quot; constraints should be associated with transitions from LR state to LR state (see Nakazawa (1995) for one proposal). In contrast, extending the techniques described here to complex feature based &amp;quot;unification&amp;quot; grammar is straight-forward. The main complication is the filter on useless nonterminals and productions just discussed. Generalizing the left-corner closure filter on pair categories to complex feature &amp;quot;unification&amp;quot; g</context>
</contexts>
<marker>Pereira, Shieber, 1987</marker>
<rawString>Fernando C.N. Pereira and Stuart M. Shieber. 1987. Prolog and Natural Language Analysis. Number 10 in CSLI Lecture Notes Series. Chicago University Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Rebecca N Wright</author>
</authors>
<title>Finite state approximation of phrase structure grammars.</title>
<date>1991</date>
<booktitle>In The Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>246--255</pages>
<contexts>
<context position="727" citStr="Pereira and Wright (1991)" startWordPosition="96" endWordPosition="99">son* Cognitive and Linguistic Sciences, Box 1978 Brown University MarkJohnson@Brown.edu Abstract This paper describes how to construct a finite-state machine (FSM) approximating a &apos;unification-based&apos; grammar using a left-corner grammar transform. The approximation is presented as a series of grammar transforms, and is exact for left-linear and rightlinear CFGs, and for trees up to a user-specified depth of center-embedding. 1 Introduction This paper describes a method for approximating grammars with finite-state machines. Unlike the method derived from the LR(k) parsing algorithm described in Pereira and Wright (1991), these methods use grammar transformations based on the left-corner grammar transform (Rosenkrantz and Lewis II, 1970; Aho and Ullman, 1972). One advantage of the left corner methods is that they generalize straightforwardly to complex feature &amp;quot;unification based&amp;quot; grammars, unlike the LR(k) based approach. For example, the implementation described here translates a DCG version of the example grammar given by Pereira and Wright (1991) directly into a FSM without constructing an approximating CFG. Left-corner based techniques are natural for this kind of application because (with the simple opti</context>
<context position="8158" citStr="Pereira and Wright (1991)" startWordPosition="1364" endWordPosition="1367">erated by the input grammar. 2 The left-corner transform While conceptually simple, the top-down parsing algorithm presented in the last section suffers from a number of drawbacks for a finite-state approximation. For example, the number of distinct accessible stack states is unbounded if the grammar is left-recursive, yet left-linear grammars always generate regular languages. This section presents &apos;With the optimized left-corner transforms described below we obtain acceptable approximations with a stack size limit of 5 or less. In many useful cases, including the example grammar provided by Pereira and Wright (1991), this stack bound is never reached and the system reports that the FSA it returns is exact. 620 the standard left-corner grammar transformation (Rosenkrantz and Lewis II, 1970; Aho and Ullman, 1972); these references should be consulted for proofs of correctness. This transform serves as the basis for the further transforms described in the next section; these transforms have the property that the output grammar induces a finite number of distinct accessible stack states if their input is a left-recursive left-linear grammar. Given an input grammar G with nonterminals N and terminals T, these</context>
</contexts>
<marker>Pereira, Wright, 1991</marker>
<rawString>Fernando C. N. Pereira and Rebecca N. Wright. 1991. Finite state approximation of phrase structure grammars. In The Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 246-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Left-corner parsing and psychological plausibility.</title>
<date>1992</date>
<booktitle>In The Proceedings of the fifteenth International Conference on Computational Linguistics, COLING-92,</booktitle>
<volume>1</volume>
<pages>191--197</pages>
<contexts>
<context position="14089" citStr="Resnik, 1992" startWordPosition="2366" endWordPosition="2367">he next section, which minimize stack depth in grammars that contain productions of length no greater than two. 3.1 A tail-recursion optimization If G is a left-linear grammar, a top-down parser using GC1(G) can recognize any string generated by G with a constant-bounded stack size. However, the corresponding operation with right-linear grammars requires a stack of size proportional to the length of the string, since the stack fills with paired categories A—A for each non-left-corner nonterminal in the analysis tree. The &apos;tail recursion&apos; or &apos;composition&apos; optimization (Abney and Johnson, 1991; Resnik, 1992) permits right-branching structures to be parsed with bounded stack depth. It is the result of epsilon removal applied to the output of GC1, and can be described in terms of resolution or partial evaluation of the transformed grammar with respect to productions (1.c). In effect, the schema (1.b) is split into two cases, depending on whether or not the rightmost nonterminal A—B is expanded by the epsilon rules produced by schema (1.c). This expansion yields a grammar £C2 (G) = (N&apos;, T, P2 S ) , where P2 contains all productions of the form (2.a-2.c). (In these schemata A, B E N; a E T; X E NUT a</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Philip Resnik. 1992. Left-corner parsing and psychological plausibility. In The Proceedings of the fifteenth International Conference on Computational Linguistics, COLING-92, volume 1, pages 191-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley J Rosenkrantz</author>
<author>Philip M Lewis</author>
</authors>
<title>Deterministic left corner parser.</title>
<date>1970</date>
<booktitle>In IEEE Conference Record of the 11th Annual Symposium on Switching and Automata,</booktitle>
<pages>139--152</pages>
<marker>Rosenkrantz, Lewis, 1970</marker>
<rawString>Stanley J. Rosenkrantz and Philip M. Lewis II. 1970. Deterministic left corner parser. In IEEE Conference Record of the 11th Annual Symposium on Switching and Automata, pages 139-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Using Restriction to extend parsing algorithms for unification-based formalisms.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>145--152</pages>
<location>Chicago.</location>
<contexts>
<context position="7044" citStr="Shieber, 1985" startWordPosition="1192" endWordPosition="1193">so that there are no transitions to any stack state whose size is larger than some user-specified limit.&apos; This restriction ensures that there is only a finite number of possible stack states, and hence that the top down parser is an finite-state machine. The resulting finite-state machine accepts a subset of the language generated by the original grammar. The situation becomes more complicated when we move to &apos;unification-based&apos; grammars, since there may be an unbounded number of different categories appearing in the accessible stack states. In the system implemented here we used restriction (Shieber, 1985) on the stack states to restrict attention to a finite number of distinct stack states for any given stack depth. Since the restriction operation maps a stack state to a more general one, it produces a finite-state approximation which accepts a superset of the language generated by the original unification grammar. Thus for general constraint-based grammars the language accepted by our finite-state approximation is not guaranteed to be either a superset or a subset of the language generated by the input grammar. 2 The left-corner transform While conceptually simple, the top-down parsing algori</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Stuart M. Shieber. 1985. Using Restriction to extend parsing algorithms for unification-based formalisms. In Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics, pages 145-152, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>An efficient implementation of the head-corner parser.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>425--456</pages>
<marker>van Noord, 1997</marker>
<rawString>Gertjan van Noord. 1997. An efficient implementation of the head-corner parser. Computational Linguistics, 23(3) :425-456.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>