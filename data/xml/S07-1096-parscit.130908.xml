<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.034826">
<title confidence="0.976742">
UPV-SI: Word Sense Induction using Self Term Expansion*
</title>
<author confidence="0.999362">
David Pinto(&apos;,2) and Paolo Rosso&apos;
</author>
<affiliation confidence="0.999608">
&apos;Polytechnic University of Valencia
</affiliation>
<address confidence="0.719499666666667">
DSIC, Valencia, Spain, 46022
2B. Autonomous University of Puebla
FCC, Puebla, Mexico, 72570
</address>
<email confidence="0.997874">
{dpinto, prosso}@dsic.upv.es
</email>
<author confidence="0.992484">
Hector Jimenez-Salazar
</author>
<affiliation confidence="0.995519">
Autonomous Metropolitan University
Department of Information Technologies
</affiliation>
<address confidence="0.937251">
Cuajimalpa, DF, Mexico, 11850
</address>
<email confidence="0.999054">
hgimenezs@gmail.com
</email>
<sectionHeader confidence="0.995486" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.968703">
In this paper we are reporting the re-
sults obtained participating in the “Eval-
uating Word Sense Induction and Dis-
crimination Systems” task of Semeval
2007. Our totally unsupervised system
performed an automatic self-term expan-
sion process by mean of co-ocurrence
terms and, thereafter, it executed the
unsupervised KStar clustering method.
Two ranking tables with different eval-
uation measures were calculated by the
task organizers, every table with two
baselines and six runs submitted by dif-
ferent teams. We were ranked third
place in both ranking tables obtaining a
better performance than three different
baselines, and outperforming the average
score.
</bodyText>
<sectionHeader confidence="0.998824" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.966006454545454">
Word Sense Disambiguation (WSD) is a partic-
ular problem of computational linguistics which
consists in determining the correct sense for a
given ambiguous word. It is well-known that su-
pervised algorithms have obtained the best re-
sults in public evaluations, but their accuracy
is close related with the amount of hand-tagged
data available. The construction of that kind
of training data is difficult for real applications.
The unsupervised WSD overcomes this draw-
back by using clustering algorithms which do
</bodyText>
<footnote confidence="0.975197666666667">
This work has been partially supported by the MCyT
TIN2006-15265-C06-04 project, as well as by the BUAP-
701 PROMEP/103.5/05/1536 grant
</footnote>
<bodyText confidence="0.984607944444444">
not need training data in order to determine the
possible sense for a given ambiguous word.
This paper describes a simple technique for
unsupervised sense induction for ambiguous
words. The approach is based on a self term ex-
pansion technique which constructs a set of co-
ocurrence terms and, thereafter, it uses this set
to expand the target dataset. The implemented
system was performed in the task “SemEval-
2007 Task 2: Evaluating Word Sense Induc-
tion and Discrimination Systems&amp;quot;(Agirre and
A., 2007). The aim of the task was to per-
mit a comparison across sense-induction and dis-
crimination systems. Moreover, the comparison
with other supervised and knowledge-based sys-
tems may be also done, since the test corpus was
borrowed from the well known “English lexical-
sample” task in SemEval-2007, with the usual
training + test split.
The self term expansion method consists in
replacing terms of a document by a set of co-
related terms. The goal is to improve natu-
ral language processing tasks such as cluster-
ing narrow-domain short texts. This process
may be done by mean of different ways, of-
ten just by using a knowledge database. In
information retrieval, for instance, the expan-
sion of query terms is a very investigated topic
which has shown to improve results with respect
to when query expansion is not employed (Qiu
and Frei, 1993; Ruge, 1992; R.Baeza-Yates and
Ribeiro-Neto, 1999; Grefenstette, 1994; Rijsber-
gen, 1979).
The availability of Machine Readable Re-
sources (MRR) like “Dictionaries”, “Thesauri”
and “Lexicons” has allowed to apply term ex-
</bodyText>
<page confidence="0.981183">
430
</page>
<bodyText confidence="0.990673426666667">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 430–433,
Prague, June 2007. c�2007 Association for Computational Linguistics
pansion to other fields of natural language pro-
cessing like WSD. In (Banerjee and Pedersen,
2002) we may see the typical example of using
a external knowledge database for determining
the correct sense of a word given in some con-
text. In this approach, every word close to the
one we would like to determine its correct sense
is expanded with its different senses by using
the WordNet lexicon (Fellbaum, 1998). Then,
an overlapping factor is calculated in order to
determine the correct sense of the ambiguous
word. Different other approaches have made use
of a similar procedure. By using dictionaries,
the proposals presented in (Lesk, 1986; Wilks et
al., 1990; Nancy and Wronis, 1990) are the most
sucessful in WSD. Yarowsky (Yarowsky, 1992)
used instead thesauri for their experiments. Fi-
nally, in (Sussna, 1993; Resnik, 1995; Baner-
jee and Pedersen, 2002) the use of lexicons in
WSD has been investigated. Although in some
cases the knowledge resource seems not to be
used strictly for term expansion, the aplication
of co-occurrence terms is included in their algo-
rithms. Like in information retrieval, the appli-
cation of term expansion in WSD by using co-
related terms has shown to improve the baseline
results if we carefully select the external resource
to use, with a priori knowledge of the domain
and the broadness of the corpus (wide or nar-
row domain). Evenmore, we have to be sure that
the Lexical Data Base (LDB) has been suitable
constructed. Due to the last facts, we consider
that the use of a self automatically constructed
LDB (using the same test corpora), may be of
high benefit. This assumption is based on the
intrinsic properties extracted from the corpus it-
self. Our proposal is related somehow with the
investigations presented in (Schiitze, 1998) and
(Purandare and Pedersen, 2004), where words
are also expanded with co-ocurrence terms for
word sense discrimination. The main difference
consists in the use of the same corpora for con-
structing the co-ocurrence list.
Following we describe the self term expan-
sion method used and, thereafter, the results
obtained in the task #2 of Semeval 2007 com-
petition.
2 The Self Term Expansion Method
In literature, co-ocurrence terms is the most
common technique used for automatic construc-
tion of LDBs (Grefenstette, 1994; Frakes and
Baeza-Yates, 1992). A simple approach may use
n-grams, which allows to predict a word from
previous words in a sample of text. The fre-
quency of each n-gram is calculated and then
filtered according to some threshold. The re-
sulting n-grams constitutes a LDB which may
be used as an “expansion dictionary” for each
term.
On the other hand, an information theory-
based co-ocurrence measure is discussed in
(Manning and Schutze, 2003). This measure
is named pointwise Mutual Information (MI),
and its applications for finding collocations are
analysed by determining the co-ocurrence de-
gree among two terms. This may be done by cal-
culating the ratio between the number of times
that both terms appear together (in the same
context and not necessarily in the same order)
and the product of the number of times that
each term ocurrs alone. Given two terms X1
and X2, the pointwise mutual information be-
tween X1 and X2 can be calculated as follows:
</bodyText>
<equation confidence="0.9852365">
P(X1X2)
MI(X1, X2) = log2 P(X1) x P(X2)
</equation>
<bodyText confidence="0.999865666666667">
The numerator could be modified in order to
take into account only bigrams, as presented
in (Pinto et al., 2006), where an improvement
of clustering short texts in narrow domains has
been obtained.
We have used the pointwise MI for obtaining
a co-ocurrence list from the same target dataset.
This list is then used to expand every term of the
original data. Since the co-ocurrence formula
captures relations between related terms, it is
possible to see that the self term expansion mag-
nifies less the noisy than the meaninful informa-
tion. Therefore, the execution of the clustering
algorithm in the expanded corpus should out-
perform the one executed over the non-expanded
data.
In order to fully appreciate the self term ex-
pansion method, in Table 1 we show the co-
</bodyText>
<page confidence="0.995711">
431
</page>
<bodyText confidence="0.9867965">
ocurrence list for some words related with the
verb “kill” of the test corpus. Since the MI
is calculated after preprocessing the corpus, we
present the stemmed version of the terms.
</bodyText>
<tableCaption confidence="0.998572">
Table 1: An example of co-ocurrence terms
</tableCaption>
<bodyText confidence="0.999985764705883">
For the task #2 of Semeval 2007, a set of 100
ambiguous words (35 nouns and 65 verbs) were
provided. We preprocessed this original dataset
by eliminating stopwords and then applying the
Porter stemmer (Porter, 1980). Thereafter,
when we used the pointwise MI, we determined
that the single ocurrence of each term should
be at least three (see (Manning and Schiitze,
2003)), whereas the maximum separation among
the two terms was five. Finally, we selected
the unsupervised KStar clustering method (Shin
and Han, 2003) for our experiments, defining the
average of similarities among all the sentences
for a given ambiguous word as the stop criterion
for this clustering method. The input similarity
matrix for the clustering method was calculated
by using the Jaccard coefficient.
</bodyText>
<sectionHeader confidence="0.998825" genericHeader="introduction">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9986106">
The task organizers decided to use two differ-
ent measures for evaluating the runs submitted
to the task. The first measure is called unsuper-
vised one, and it is based on the Fscore measure.
Whereas the second measure is called supervised
recall. For further information on how these
measures are calculated refer to (Agirre et al.,
2006a; Agirre et al., 2006b). Since these mea-
sures give conflicting information, two different
evaluation results are reported in this paper.
In Table 2 we may see our ranking and the Fs-
core measure obtained (UPV-SI). We also show
the best and worst team Fscores; as well as the
total average and two baselines proposed by the
task organizers. The first baseline (Baseline1)
assumes that each ambiguous word has only one
sense, whereas the second baseline (Baseline2) is
a random assignation of senses. We are ranked
as third place and our results are better scored
than the other teams except for the best team
score. However, given the similar values with
the “Baseline1&amp;quot;, we may assume that that team
presented one cluster per ambiguous word as its
result as the Baseline1 did; whereas we obtained
9.03 senses per ambiguous word in average.
</bodyText>
<table confidence="0.999727714285714">
Name Rank All Nouns Verbs
Baseline1 1 78.9 80.7 76.8
Best Team 2 78.7 80.8 76.3
UPV-SI 3 66.3 69.9 62.2
Average - 63.6 66.5 60.3
Worst Team 7 56.1 65.8 45.1
Baseline2 8 37.8 38.0 37.6
</table>
<tableCaption confidence="0.8422745">
Table 2: Unsupervised evaluation (Fscore per-
formance).
</tableCaption>
<bodyText confidence="0.997569777777778">
In Table 3 we show our ranking and the super-
vised recall obtained (UPV-SI). We again show
the best and worst team recalls. The total av-
erage and one baseline is also presented (the
other baseline obtained the same Fscore). In
this case, the baseline tags each test instance
with the most frequent sense obtained in a train
split. We are ranked again in third place and
our score is slightly above the baseline.
</bodyText>
<table confidence="0.999808714285714">
Name Rank All Nouns Verbs
Best Team 1 81.6 86.8 76.2
UPV-SI 3 79.1 82.5 75.3
Average - 79.1 82.8 75.0
Baseline 4 78.7 80.9 76.2
Worst Team 6a 78.5 81.8 74.9
Worst Team 6b 78.5 81.4 75.2
</table>
<tableCaption confidence="0.999759">
Table 3: Supervised evaluation (Recall).
</tableCaption>
<bodyText confidence="0.9996578">
The results show that the technique employed
have learned, since our simple approach ob-
tained a better performance than the baselines,
especially the one that have chosen the most fre-
quent sense as baseline.
</bodyText>
<figure confidence="0.840280642857143">
Word
Co-ocurrence terms
kill
women think shoot peopl old man
kill death beat
todai live guerrilla fight explod
shoot run rape person peopl outsid
murder life lebanon kill convict...
tuesdai peopl least kill earthquak
soldier
rape
grenad
death
temblor
</figure>
<page confidence="0.996927">
432
</page>
<sectionHeader confidence="0.997756" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999970076923077">
We have reported the performance of a single
approach based on self term expansion. The
technique uses the pointwise mutual information
for calculating a set of co-ocurrence terms which
then are used to expand the original dataset.
Once the expansion has been done, the unsu-
pervised KStar clustering method was used to
induce the sense for the different ocurrences of
each ambiguous word. We obtained the third
place in the two measures proposed in the task.
We will further investigate whether an improve-
ment may be obtained by applying term selec-
tion methods to the expanded corpus.
</bodyText>
<sectionHeader confidence="0.998916" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99971405">
E. Agirre and Soroa A. 2007. SemEval-2007 Task 2:
Evaluating Word Sense Induction and Discrimina-
tion Systems. In SemEval-2007. Association for
Computational Linguistics.
E. Agirre, O. Lopez de Lacalle Lekuona, D. Mar-
tinez, and A. Soroa. 2006a. Evaluating and opti-
mizing the parameters of an unsupervised graph-
based WSD algorithm. In Textgraphs 2006 work-
shop, NAACL06, pages 89–96.
E. Agirre, O. Lopez de Lacalle Lekuona, D. Mar-
tinez, and A. Soroa. 2006b. Two graph-based
algorithms for state-of-the-art WSD. In EMNLP,
pages 585–593. ACL.
S. Banerjee and T. Pedersen. 2002. An Adapted
Lesk Algorithm for Word Sense Disambiguation
Using WordNet. In CICLing 2002 Conference,
volume 3878 of LNCS, pages 136–145. Springer-
Verlang.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
W. B. Frakes and R. A. Baeza-Yates. 1992. Infor-
mation Retrieval: Data Structures &amp; Algorithms.
Prentice-Hall.
G. Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic.
M. Lesk. 1986. Automatic sense disambiguation:
How to tell a pine cone from an ice cream cone.
In ACM SIGDOC Conference, pages 24–26. ACM
Press.
D. C. Manning and H. Schutze. 2003. Foundations
of Statistical Natural Language Processing. MIT
Press. Revised version May 1999.
I. Nancy and J. Veronis. 1990. Mapping dictionar-
ies: A spreading activation approach. In 6th An-
nual Conference of the Centre for the New Oxford
English Dictionary, pages 52–64.
D. Pinto, H. Jimenez-Salazar, and P. Rosso. 2006.
Clustering abstracts of scientific texts using the
transition point technique. In CICLing, volume
3878 of LNCS, pages 536–546. Springer-Verlang.
M. F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3).
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing, pages 41–48, Boston, MA.
Y. Qiu and H. P. Frei. 1993. Concept based Query
Expansion. In ACM SIGIR on R&amp;D in informa-
tion retrieval, pages 160–169. ACM Press.
R.Baeza-Yates and B. Ribeiro-Neto. 1999. Mod-
ern information retrieval. New York: ACM Press;
Addison-Wesley.
P. Resnik. 1995. Disambiguating Noun Groupings
with Respect to WordNet Senses. In 3rd Work-
shop on Very Large Corpora, pages 54–68. ACL.
C. J. Van Rijsbergen. 1979. Information Retrieval,
2nd edition. Dept. of Computer Science, Univer-
sity of Glasgow.
G. Ruge. 1992. Experiments on linguistically-
based term associations. Information Processing
&amp; Management, 28(3):317–332.
H. Schutze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97–123.
K. Shin and S. Y. Han. 2003. Fast clustering algo-
rithm for information organization. In CICLing,
volume 2588 of LNCS, pages 619–622. Springer-
Verlang.
M. Sussna. 1993. Word sense disambiguation for
free-test indexing using a massive semantic net-
work. In 2nd International Conference on Infor-
mation and Knowledge Management, pages 67–74.
Y. Wilks, D. Fass, C. Guo, J. McDonald, T. Plate,
and B. Slator. 1990. Providing machine tractable
dictionary tools. Machine Translation, 5(2):99–
154.
D. Yarowsky. 1992. Word-sense disambiguation us-
ing statistical models of Rogets categories trained
on large corpora. In 14th Conference on Compu-
tational Linguistics, pages 454–460. ACL.
</reference>
<page confidence="0.999443">
433
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.730291">
<title confidence="0.912806">Word Sense Induction using Self Term</title>
<affiliation confidence="0.99969">University of Valencia</affiliation>
<address confidence="0.993898">DSIC, Valencia, Spain, 46022</address>
<affiliation confidence="0.987805">Autonomous University of Puebla</affiliation>
<address confidence="0.996481">FCC, Puebla, Mexico, 72570</address>
<author confidence="0.994756">Hector Jimenez-Salazar</author>
<affiliation confidence="0.9938225">Autonomous Metropolitan University Department of Information Technologies</affiliation>
<address confidence="0.998099">Cuajimalpa, DF, Mexico, 11850</address>
<email confidence="0.998693">hgimenezs@gmail.com</email>
<abstract confidence="0.998328789473684">In this paper we are reporting the results obtained participating in the “Evaluating Word Sense Induction and Discrimination Systems” task of Semeval 2007. Our totally unsupervised system performed an automatic self-term expansion process by mean of co-ocurrence terms and, thereafter, it executed the unsupervised KStar clustering method. Two ranking tables with different evaluation measures were calculated by the task organizers, every table with two baselines and six runs submitted by different teams. We were ranked third place in both ranking tables obtaining a better performance than three different baselines, and outperforming the average score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>SemEval-2007 Task 2: Evaluating Word Sense Induction and Discrimination Systems.</title>
<date>2007</date>
<booktitle>In SemEval-2007. Association for Computational Linguistics.</booktitle>
<marker>Agirre, Soroa, 2007</marker>
<rawString>E. Agirre and Soroa A. 2007. SemEval-2007 Task 2: Evaluating Word Sense Induction and Discrimination Systems. In SemEval-2007. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>O Lopez de Lacalle Lekuona</author>
<author>D Martinez</author>
<author>A Soroa</author>
</authors>
<title>Evaluating and optimizing the parameters of an unsupervised graphbased WSD algorithm.</title>
<date>2006</date>
<booktitle>In Textgraphs 2006 workshop, NAACL06,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="8854" citStr="Agirre et al., 2006" startWordPosition="1421" endWordPosition="1424">, 2003) for our experiments, defining the average of similarities among all the sentences for a given ambiguous word as the stop criterion for this clustering method. The input similarity matrix for the clustering method was calculated by using the Jaccard coefficient. 3 Evaluation The task organizers decided to use two different measures for evaluating the runs submitted to the task. The first measure is called unsupervised one, and it is based on the Fscore measure. Whereas the second measure is called supervised recall. For further information on how these measures are calculated refer to (Agirre et al., 2006a; Agirre et al., 2006b). Since these measures give conflicting information, two different evaluation results are reported in this paper. In Table 2 we may see our ranking and the Fscore measure obtained (UPV-SI). We also show the best and worst team Fscores; as well as the total average and two baselines proposed by the task organizers. The first baseline (Baseline1) assumes that each ambiguous word has only one sense, whereas the second baseline (Baseline2) is a random assignation of senses. We are ranked as third place and our results are better scored than the other teams except for the be</context>
</contexts>
<marker>Agirre, Lekuona, Martinez, Soroa, 2006</marker>
<rawString>E. Agirre, O. Lopez de Lacalle Lekuona, D. Martinez, and A. Soroa. 2006a. Evaluating and optimizing the parameters of an unsupervised graphbased WSD algorithm. In Textgraphs 2006 workshop, NAACL06, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>O Lopez de Lacalle Lekuona</author>
<author>D Martinez</author>
<author>A Soroa</author>
</authors>
<title>Two graph-based algorithms for state-of-the-art WSD. In</title>
<date>2006</date>
<booktitle>EMNLP,</booktitle>
<pages>585--593</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="8854" citStr="Agirre et al., 2006" startWordPosition="1421" endWordPosition="1424">, 2003) for our experiments, defining the average of similarities among all the sentences for a given ambiguous word as the stop criterion for this clustering method. The input similarity matrix for the clustering method was calculated by using the Jaccard coefficient. 3 Evaluation The task organizers decided to use two different measures for evaluating the runs submitted to the task. The first measure is called unsupervised one, and it is based on the Fscore measure. Whereas the second measure is called supervised recall. For further information on how these measures are calculated refer to (Agirre et al., 2006a; Agirre et al., 2006b). Since these measures give conflicting information, two different evaluation results are reported in this paper. In Table 2 we may see our ranking and the Fscore measure obtained (UPV-SI). We also show the best and worst team Fscores; as well as the total average and two baselines proposed by the task organizers. The first baseline (Baseline1) assumes that each ambiguous word has only one sense, whereas the second baseline (Baseline2) is a random assignation of senses. We are ranked as third place and our results are better scored than the other teams except for the be</context>
</contexts>
<marker>Agirre, Lekuona, Martinez, Soroa, 2006</marker>
<rawString>E. Agirre, O. Lopez de Lacalle Lekuona, D. Martinez, and A. Soroa. 2006b. Two graph-based algorithms for state-of-the-art WSD. In EMNLP, pages 585–593. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet.</title>
<date>2002</date>
<booktitle>In CICLing 2002 Conference,</booktitle>
<volume>3878</volume>
<pages>136--145</pages>
<publisher>SpringerVerlang.</publisher>
<contexts>
<context position="3546" citStr="Banerjee and Pedersen, 2002" startWordPosition="536" endWordPosition="539">ms is a very investigated topic which has shown to improve results with respect to when query expansion is not employed (Qiu and Frei, 1993; Ruge, 1992; R.Baeza-Yates and Ribeiro-Neto, 1999; Grefenstette, 1994; Rijsbergen, 1979). The availability of Machine Readable Resources (MRR) like “Dictionaries”, “Thesauri” and “Lexicons” has allowed to apply term ex430 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 430–433, Prague, June 2007. c�2007 Association for Computational Linguistics pansion to other fields of natural language processing like WSD. In (Banerjee and Pedersen, 2002) we may see the typical example of using a external knowledge database for determining the correct sense of a word given in some context. In this approach, every word close to the one we would like to determine its correct sense is expanded with its different senses by using the WordNet lexicon (Fellbaum, 1998). Then, an overlapping factor is calculated in order to determine the correct sense of the ambiguous word. Different other approaches have made use of a similar procedure. By using dictionaries, the proposals presented in (Lesk, 1986; Wilks et al., 1990; Nancy and Wronis, 1990) are the m</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>S. Banerjee and T. Pedersen. 2002. An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet. In CICLing 2002 Conference, volume 3878 of LNCS, pages 136–145. SpringerVerlang.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3858" citStr="Fellbaum, 1998" startWordPosition="593" endWordPosition="594">ons” has allowed to apply term ex430 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 430–433, Prague, June 2007. c�2007 Association for Computational Linguistics pansion to other fields of natural language processing like WSD. In (Banerjee and Pedersen, 2002) we may see the typical example of using a external knowledge database for determining the correct sense of a word given in some context. In this approach, every word close to the one we would like to determine its correct sense is expanded with its different senses by using the WordNet lexicon (Fellbaum, 1998). Then, an overlapping factor is calculated in order to determine the correct sense of the ambiguous word. Different other approaches have made use of a similar procedure. By using dictionaries, the proposals presented in (Lesk, 1986; Wilks et al., 1990; Nancy and Wronis, 1990) are the most sucessful in WSD. Yarowsky (Yarowsky, 1992) used instead thesauri for their experiments. Finally, in (Sussna, 1993; Resnik, 1995; Banerjee and Pedersen, 2002) the use of lexicons in WSD has been investigated. Although in some cases the knowledge resource seems not to be used strictly for term expansion, the</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Frakes</author>
<author>R A Baeza-Yates</author>
</authors>
<title>Information Retrieval: Data Structures &amp; Algorithms.</title>
<date>1992</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="5769" citStr="Frakes and Baeza-Yates, 1992" startWordPosition="903" endWordPosition="906">proposal is related somehow with the investigations presented in (Schiitze, 1998) and (Purandare and Pedersen, 2004), where words are also expanded with co-ocurrence terms for word sense discrimination. The main difference consists in the use of the same corpora for constructing the co-ocurrence list. Following we describe the self term expansion method used and, thereafter, the results obtained in the task #2 of Semeval 2007 competition. 2 The Self Term Expansion Method In literature, co-ocurrence terms is the most common technique used for automatic construction of LDBs (Grefenstette, 1994; Frakes and Baeza-Yates, 1992). A simple approach may use n-grams, which allows to predict a word from previous words in a sample of text. The frequency of each n-gram is calculated and then filtered according to some threshold. The resulting n-grams constitutes a LDB which may be used as an “expansion dictionary” for each term. On the other hand, an information theorybased co-ocurrence measure is discussed in (Manning and Schutze, 2003). This measure is named pointwise Mutual Information (MI), and its applications for finding collocations are analysed by determining the co-ocurrence degree among two terms. This may be don</context>
</contexts>
<marker>Frakes, Baeza-Yates, 1992</marker>
<rawString>W. B. Frakes and R. A. Baeza-Yates. 1992. Information Retrieval: Data Structures &amp; Algorithms. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic.</publisher>
<contexts>
<context position="3127" citStr="Grefenstette, 1994" startWordPosition="479" endWordPosition="480">2007, with the usual training + test split. The self term expansion method consists in replacing terms of a document by a set of corelated terms. The goal is to improve natural language processing tasks such as clustering narrow-domain short texts. This process may be done by mean of different ways, often just by using a knowledge database. In information retrieval, for instance, the expansion of query terms is a very investigated topic which has shown to improve results with respect to when query expansion is not employed (Qiu and Frei, 1993; Ruge, 1992; R.Baeza-Yates and Ribeiro-Neto, 1999; Grefenstette, 1994; Rijsbergen, 1979). The availability of Machine Readable Resources (MRR) like “Dictionaries”, “Thesauri” and “Lexicons” has allowed to apply term ex430 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 430–433, Prague, June 2007. c�2007 Association for Computational Linguistics pansion to other fields of natural language processing like WSD. In (Banerjee and Pedersen, 2002) we may see the typical example of using a external knowledge database for determining the correct sense of a word given in some context. In this approach, every word close to the o</context>
<context position="5738" citStr="Grefenstette, 1994" startWordPosition="901" endWordPosition="902"> corpus itself. Our proposal is related somehow with the investigations presented in (Schiitze, 1998) and (Purandare and Pedersen, 2004), where words are also expanded with co-ocurrence terms for word sense discrimination. The main difference consists in the use of the same corpora for constructing the co-ocurrence list. Following we describe the self term expansion method used and, thereafter, the results obtained in the task #2 of Semeval 2007 competition. 2 The Self Term Expansion Method In literature, co-ocurrence terms is the most common technique used for automatic construction of LDBs (Grefenstette, 1994; Frakes and Baeza-Yates, 1992). A simple approach may use n-grams, which allows to predict a word from previous words in a sample of text. The frequency of each n-gram is calculated and then filtered according to some threshold. The resulting n-grams constitutes a LDB which may be used as an “expansion dictionary” for each term. On the other hand, an information theorybased co-ocurrence measure is discussed in (Manning and Schutze, 2003). This measure is named pointwise Mutual Information (MI), and its applications for finding collocations are analysed by determining the co-ocurrence degree a</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>G. Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In ACM SIGDOC Conference,</booktitle>
<pages>24--26</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="4091" citStr="Lesk, 1986" startWordPosition="629" endWordPosition="630">al language processing like WSD. In (Banerjee and Pedersen, 2002) we may see the typical example of using a external knowledge database for determining the correct sense of a word given in some context. In this approach, every word close to the one we would like to determine its correct sense is expanded with its different senses by using the WordNet lexicon (Fellbaum, 1998). Then, an overlapping factor is calculated in order to determine the correct sense of the ambiguous word. Different other approaches have made use of a similar procedure. By using dictionaries, the proposals presented in (Lesk, 1986; Wilks et al., 1990; Nancy and Wronis, 1990) are the most sucessful in WSD. Yarowsky (Yarowsky, 1992) used instead thesauri for their experiments. Finally, in (Sussna, 1993; Resnik, 1995; Banerjee and Pedersen, 2002) the use of lexicons in WSD has been investigated. Although in some cases the knowledge resource seems not to be used strictly for term expansion, the aplication of co-occurrence terms is included in their algorithms. Like in information retrieval, the application of term expansion in WSD by using corelated terms has shown to improve the baseline results if we carefully select the</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation: How to tell a pine cone from an ice cream cone. In ACM SIGDOC Conference, pages 24–26. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Manning</author>
<author>H Schutze</author>
</authors>
<title>Revised version</title>
<date>2003</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6180" citStr="Manning and Schutze, 2003" startWordPosition="973" endWordPosition="976">of Semeval 2007 competition. 2 The Self Term Expansion Method In literature, co-ocurrence terms is the most common technique used for automatic construction of LDBs (Grefenstette, 1994; Frakes and Baeza-Yates, 1992). A simple approach may use n-grams, which allows to predict a word from previous words in a sample of text. The frequency of each n-gram is calculated and then filtered according to some threshold. The resulting n-grams constitutes a LDB which may be used as an “expansion dictionary” for each term. On the other hand, an information theorybased co-ocurrence measure is discussed in (Manning and Schutze, 2003). This measure is named pointwise Mutual Information (MI), and its applications for finding collocations are analysed by determining the co-ocurrence degree among two terms. This may be done by calculating the ratio between the number of times that both terms appear together (in the same context and not necessarily in the same order) and the product of the number of times that each term ocurrs alone. Given two terms X1 and X2, the pointwise mutual information between X1 and X2 can be calculated as follows: P(X1X2) MI(X1, X2) = log2 P(X1) x P(X2) The numerator could be modified in order to take</context>
</contexts>
<marker>Manning, Schutze, 2003</marker>
<rawString>D. C. Manning and H. Schutze. 2003. Foundations of Statistical Natural Language Processing. MIT Press. Revised version May 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Nancy</author>
<author>J Veronis</author>
</authors>
<title>Mapping dictionaries: A spreading activation approach.</title>
<date>1990</date>
<booktitle>In 6th Annual Conference of the Centre for the New Oxford English Dictionary,</booktitle>
<pages>52--64</pages>
<marker>Nancy, Veronis, 1990</marker>
<rawString>I. Nancy and J. Veronis. 1990. Mapping dictionaries: A spreading activation approach. In 6th Annual Conference of the Centre for the New Oxford English Dictionary, pages 52–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pinto</author>
<author>H Jimenez-Salazar</author>
<author>P Rosso</author>
</authors>
<title>Clustering abstracts of scientific texts using the transition point technique.</title>
<date>2006</date>
<journal>Program,</journal>
<booktitle>In CICLing,</booktitle>
<volume>3878</volume>
<pages>536--546</pages>
<contexts>
<context position="6844" citStr="Pinto et al., 2006" startWordPosition="1089" endWordPosition="1092">rmation (MI), and its applications for finding collocations are analysed by determining the co-ocurrence degree among two terms. This may be done by calculating the ratio between the number of times that both terms appear together (in the same context and not necessarily in the same order) and the product of the number of times that each term ocurrs alone. Given two terms X1 and X2, the pointwise mutual information between X1 and X2 can be calculated as follows: P(X1X2) MI(X1, X2) = log2 P(X1) x P(X2) The numerator could be modified in order to take into account only bigrams, as presented in (Pinto et al., 2006), where an improvement of clustering short texts in narrow domains has been obtained. We have used the pointwise MI for obtaining a co-ocurrence list from the same target dataset. This list is then used to expand every term of the original data. Since the co-ocurrence formula captures relations between related terms, it is possible to see that the self term expansion magnifies less the noisy than the meaninful information. Therefore, the execution of the clustering algorithm in the expanded corpus should outperform the one executed over the non-expanded data. In order to fully appreciate the s</context>
</contexts>
<marker>Pinto, Jimenez-Salazar, Rosso, 2006</marker>
<rawString>D. Pinto, H. Jimenez-Salazar, and P. Rosso. 2006. Clustering abstracts of scientific texts using the transition point technique. In CICLing, volume 3878 of LNCS, pages 536–546. Springer-Verlang. M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Purandare</author>
<author>T Pedersen</author>
</authors>
<title>Word sense discrimination by clustering contexts in vector and similarity spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning,</booktitle>
<pages>41--48</pages>
<location>Boston, MA.</location>
<contexts>
<context position="5256" citStr="Purandare and Pedersen, 2004" startWordPosition="822" endWordPosition="825">shown to improve the baseline results if we carefully select the external resource to use, with a priori knowledge of the domain and the broadness of the corpus (wide or narrow domain). Evenmore, we have to be sure that the Lexical Data Base (LDB) has been suitable constructed. Due to the last facts, we consider that the use of a self automatically constructed LDB (using the same test corpora), may be of high benefit. This assumption is based on the intrinsic properties extracted from the corpus itself. Our proposal is related somehow with the investigations presented in (Schiitze, 1998) and (Purandare and Pedersen, 2004), where words are also expanded with co-ocurrence terms for word sense discrimination. The main difference consists in the use of the same corpora for constructing the co-ocurrence list. Following we describe the self term expansion method used and, thereafter, the results obtained in the task #2 of Semeval 2007 competition. 2 The Self Term Expansion Method In literature, co-ocurrence terms is the most common technique used for automatic construction of LDBs (Grefenstette, 1994; Frakes and Baeza-Yates, 1992). A simple approach may use n-grams, which allows to predict a word from previous words</context>
</contexts>
<marker>Purandare, Pedersen, 2004</marker>
<rawString>A. Purandare and T. Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In Proceedings of the Conference on Computational Natural Language Learning, pages 41–48, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Qiu</author>
<author>H P Frei</author>
</authors>
<title>Concept based Query Expansion.</title>
<date>1993</date>
<booktitle>In ACM SIGIR on R&amp;D in information retrieval,</booktitle>
<pages>160--169</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="3057" citStr="Qiu and Frei, 1993" startWordPosition="469" endWordPosition="472"> borrowed from the well known “English lexicalsample” task in SemEval-2007, with the usual training + test split. The self term expansion method consists in replacing terms of a document by a set of corelated terms. The goal is to improve natural language processing tasks such as clustering narrow-domain short texts. This process may be done by mean of different ways, often just by using a knowledge database. In information retrieval, for instance, the expansion of query terms is a very investigated topic which has shown to improve results with respect to when query expansion is not employed (Qiu and Frei, 1993; Ruge, 1992; R.Baeza-Yates and Ribeiro-Neto, 1999; Grefenstette, 1994; Rijsbergen, 1979). The availability of Machine Readable Resources (MRR) like “Dictionaries”, “Thesauri” and “Lexicons” has allowed to apply term ex430 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 430–433, Prague, June 2007. c�2007 Association for Computational Linguistics pansion to other fields of natural language processing like WSD. In (Banerjee and Pedersen, 2002) we may see the typical example of using a external knowledge database for determining the correct sense of a w</context>
</contexts>
<marker>Qiu, Frei, 1993</marker>
<rawString>Y. Qiu and H. P. Frei. 1993. Concept based Query Expansion. In ACM SIGIR on R&amp;D in information retrieval, pages 160–169. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Baeza-Yates</author>
<author>B Ribeiro-Neto</author>
</authors>
<title>Modern information retrieval.</title>
<date>1999</date>
<publisher>ACM Press; Addison-Wesley.</publisher>
<location>New York:</location>
<contexts>
<context position="3107" citStr="Baeza-Yates and Ribeiro-Neto, 1999" startWordPosition="475" endWordPosition="478">lish lexicalsample” task in SemEval-2007, with the usual training + test split. The self term expansion method consists in replacing terms of a document by a set of corelated terms. The goal is to improve natural language processing tasks such as clustering narrow-domain short texts. This process may be done by mean of different ways, often just by using a knowledge database. In information retrieval, for instance, the expansion of query terms is a very investigated topic which has shown to improve results with respect to when query expansion is not employed (Qiu and Frei, 1993; Ruge, 1992; R.Baeza-Yates and Ribeiro-Neto, 1999; Grefenstette, 1994; Rijsbergen, 1979). The availability of Machine Readable Resources (MRR) like “Dictionaries”, “Thesauri” and “Lexicons” has allowed to apply term ex430 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 430–433, Prague, June 2007. c�2007 Association for Computational Linguistics pansion to other fields of natural language processing like WSD. In (Banerjee and Pedersen, 2002) we may see the typical example of using a external knowledge database for determining the correct sense of a word given in some context. In this approach, every</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>R.Baeza-Yates and B. Ribeiro-Neto. 1999. Modern information retrieval. New York: ACM Press; Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Disambiguating Noun Groupings with Respect to WordNet Senses.</title>
<date>1995</date>
<booktitle>In 3rd Workshop on Very Large Corpora,</booktitle>
<pages>54--68</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4278" citStr="Resnik, 1995" startWordPosition="659" endWordPosition="660"> in some context. In this approach, every word close to the one we would like to determine its correct sense is expanded with its different senses by using the WordNet lexicon (Fellbaum, 1998). Then, an overlapping factor is calculated in order to determine the correct sense of the ambiguous word. Different other approaches have made use of a similar procedure. By using dictionaries, the proposals presented in (Lesk, 1986; Wilks et al., 1990; Nancy and Wronis, 1990) are the most sucessful in WSD. Yarowsky (Yarowsky, 1992) used instead thesauri for their experiments. Finally, in (Sussna, 1993; Resnik, 1995; Banerjee and Pedersen, 2002) the use of lexicons in WSD has been investigated. Although in some cases the knowledge resource seems not to be used strictly for term expansion, the aplication of co-occurrence terms is included in their algorithms. Like in information retrieval, the application of term expansion in WSD by using corelated terms has shown to improve the baseline results if we carefully select the external resource to use, with a priori knowledge of the domain and the broadness of the corpus (wide or narrow domain). Evenmore, we have to be sure that the Lexical Data Base (LDB) has</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Disambiguating Noun Groupings with Respect to WordNet Senses. In 3rd Workshop on Very Large Corpora, pages 54–68. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Van Rijsbergen</author>
</authors>
<title>Information Retrieval, 2nd edition.</title>
<date>1979</date>
<institution>Dept. of Computer Science, University of Glasgow.</institution>
<marker>Van Rijsbergen, 1979</marker>
<rawString>C. J. Van Rijsbergen. 1979. Information Retrieval, 2nd edition. Dept. of Computer Science, University of Glasgow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ruge</author>
</authors>
<title>Experiments on linguisticallybased term associations.</title>
<date>1992</date>
<journal>Information Processing &amp; Management,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="3069" citStr="Ruge, 1992" startWordPosition="473" endWordPosition="474">ell known “English lexicalsample” task in SemEval-2007, with the usual training + test split. The self term expansion method consists in replacing terms of a document by a set of corelated terms. The goal is to improve natural language processing tasks such as clustering narrow-domain short texts. This process may be done by mean of different ways, often just by using a knowledge database. In information retrieval, for instance, the expansion of query terms is a very investigated topic which has shown to improve results with respect to when query expansion is not employed (Qiu and Frei, 1993; Ruge, 1992; R.Baeza-Yates and Ribeiro-Neto, 1999; Grefenstette, 1994; Rijsbergen, 1979). The availability of Machine Readable Resources (MRR) like “Dictionaries”, “Thesauri” and “Lexicons” has allowed to apply term ex430 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 430–433, Prague, June 2007. c�2007 Association for Computational Linguistics pansion to other fields of natural language processing like WSD. In (Banerjee and Pedersen, 2002) we may see the typical example of using a external knowledge database for determining the correct sense of a word given in</context>
</contexts>
<marker>Ruge, 1992</marker>
<rawString>G. Ruge. 1992. Experiments on linguisticallybased term associations. Information Processing &amp; Management, 28(3):317–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schutze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Schutze, 1998</marker>
<rawString>H. Schutze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shin</author>
<author>S Y Han</author>
</authors>
<title>Fast clustering algorithm for information organization.</title>
<date>2003</date>
<booktitle>In CICLing,</booktitle>
<volume>2588</volume>
<pages>619--622</pages>
<publisher>SpringerVerlang.</publisher>
<contexts>
<context position="8242" citStr="Shin and Han, 2003" startWordPosition="1322" endWordPosition="1325">ng the corpus, we present the stemmed version of the terms. Table 1: An example of co-ocurrence terms For the task #2 of Semeval 2007, a set of 100 ambiguous words (35 nouns and 65 verbs) were provided. We preprocessed this original dataset by eliminating stopwords and then applying the Porter stemmer (Porter, 1980). Thereafter, when we used the pointwise MI, we determined that the single ocurrence of each term should be at least three (see (Manning and Schiitze, 2003)), whereas the maximum separation among the two terms was five. Finally, we selected the unsupervised KStar clustering method (Shin and Han, 2003) for our experiments, defining the average of similarities among all the sentences for a given ambiguous word as the stop criterion for this clustering method. The input similarity matrix for the clustering method was calculated by using the Jaccard coefficient. 3 Evaluation The task organizers decided to use two different measures for evaluating the runs submitted to the task. The first measure is called unsupervised one, and it is based on the Fscore measure. Whereas the second measure is called supervised recall. For further information on how these measures are calculated refer to (Agirre </context>
</contexts>
<marker>Shin, Han, 2003</marker>
<rawString>K. Shin and S. Y. Han. 2003. Fast clustering algorithm for information organization. In CICLing, volume 2588 of LNCS, pages 619–622. SpringerVerlang.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sussna</author>
</authors>
<title>Word sense disambiguation for free-test indexing using a massive semantic network.</title>
<date>1993</date>
<booktitle>In 2nd International Conference on Information and Knowledge Management,</booktitle>
<pages>67--74</pages>
<contexts>
<context position="4264" citStr="Sussna, 1993" startWordPosition="657" endWordPosition="658">f a word given in some context. In this approach, every word close to the one we would like to determine its correct sense is expanded with its different senses by using the WordNet lexicon (Fellbaum, 1998). Then, an overlapping factor is calculated in order to determine the correct sense of the ambiguous word. Different other approaches have made use of a similar procedure. By using dictionaries, the proposals presented in (Lesk, 1986; Wilks et al., 1990; Nancy and Wronis, 1990) are the most sucessful in WSD. Yarowsky (Yarowsky, 1992) used instead thesauri for their experiments. Finally, in (Sussna, 1993; Resnik, 1995; Banerjee and Pedersen, 2002) the use of lexicons in WSD has been investigated. Although in some cases the knowledge resource seems not to be used strictly for term expansion, the aplication of co-occurrence terms is included in their algorithms. Like in information retrieval, the application of term expansion in WSD by using corelated terms has shown to improve the baseline results if we carefully select the external resource to use, with a priori knowledge of the domain and the broadness of the corpus (wide or narrow domain). Evenmore, we have to be sure that the Lexical Data </context>
</contexts>
<marker>Sussna, 1993</marker>
<rawString>M. Sussna. 1993. Word sense disambiguation for free-test indexing using a massive semantic network. In 2nd International Conference on Information and Knowledge Management, pages 67–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
<author>D Fass</author>
<author>C Guo</author>
<author>J McDonald</author>
<author>T Plate</author>
<author>B Slator</author>
</authors>
<title>Providing machine tractable dictionary tools.</title>
<date>1990</date>
<journal>Machine Translation,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>154</pages>
<contexts>
<context position="4111" citStr="Wilks et al., 1990" startWordPosition="631" endWordPosition="634">processing like WSD. In (Banerjee and Pedersen, 2002) we may see the typical example of using a external knowledge database for determining the correct sense of a word given in some context. In this approach, every word close to the one we would like to determine its correct sense is expanded with its different senses by using the WordNet lexicon (Fellbaum, 1998). Then, an overlapping factor is calculated in order to determine the correct sense of the ambiguous word. Different other approaches have made use of a similar procedure. By using dictionaries, the proposals presented in (Lesk, 1986; Wilks et al., 1990; Nancy and Wronis, 1990) are the most sucessful in WSD. Yarowsky (Yarowsky, 1992) used instead thesauri for their experiments. Finally, in (Sussna, 1993; Resnik, 1995; Banerjee and Pedersen, 2002) the use of lexicons in WSD has been investigated. Although in some cases the knowledge resource seems not to be used strictly for term expansion, the aplication of co-occurrence terms is included in their algorithms. Like in information retrieval, the application of term expansion in WSD by using corelated terms has shown to improve the baseline results if we carefully select the external resource t</context>
</contexts>
<marker>Wilks, Fass, Guo, McDonald, Plate, Slator, 1990</marker>
<rawString>Y. Wilks, D. Fass, C. Guo, J. McDonald, T. Plate, and B. Slator. 1990. Providing machine tractable dictionary tools. Machine Translation, 5(2):99– 154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Rogets categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In 14th Conference on Computational Linguistics,</booktitle>
<pages>454--460</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4193" citStr="Yarowsky, 1992" startWordPosition="646" endWordPosition="647">of using a external knowledge database for determining the correct sense of a word given in some context. In this approach, every word close to the one we would like to determine its correct sense is expanded with its different senses by using the WordNet lexicon (Fellbaum, 1998). Then, an overlapping factor is calculated in order to determine the correct sense of the ambiguous word. Different other approaches have made use of a similar procedure. By using dictionaries, the proposals presented in (Lesk, 1986; Wilks et al., 1990; Nancy and Wronis, 1990) are the most sucessful in WSD. Yarowsky (Yarowsky, 1992) used instead thesauri for their experiments. Finally, in (Sussna, 1993; Resnik, 1995; Banerjee and Pedersen, 2002) the use of lexicons in WSD has been investigated. Although in some cases the knowledge resource seems not to be used strictly for term expansion, the aplication of co-occurrence terms is included in their algorithms. Like in information retrieval, the application of term expansion in WSD by using corelated terms has shown to improve the baseline results if we carefully select the external resource to use, with a priori knowledge of the domain and the broadness of the corpus (wide</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>D. Yarowsky. 1992. Word-sense disambiguation using statistical models of Rogets categories trained on large corpora. In 14th Conference on Computational Linguistics, pages 454–460. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>