<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022640">
<title confidence="0.985161">
Selecting Corpus-Semantic Models for Neurolinguistic Decoding
</title>
<author confidence="0.996707">
Brian Murphy
</author>
<affiliation confidence="0.962975666666667">
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, USA
</affiliation>
<email confidence="0.996995">
brianmurphy@cmu.edu
</email>
<author confidence="0.970397">
Partha Talukdar
</author>
<affiliation confidence="0.956713333333333">
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, USA
</affiliation>
<email confidence="0.996823">
ppt@cs.cmu.edu
</email>
<author confidence="0.994551">
Tom Mitchell
</author>
<affiliation confidence="0.962320666666667">
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, USA
</affiliation>
<email confidence="0.996571">
tom.mitchell@cs.cmu.edu
</email>
<sectionHeader confidence="0.980014" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999793761904762">
Neurosemantics aims to learn the mapping
between concepts and the neural activity
which they elicit during neuroimaging ex-
periments. Different approaches have been
used to represent individual concepts, but
current state-of-the-art techniques require
extensive manual intervention to scale to
arbitrary words and domains. To over-
come this challenge, we initiate a system-
atic comparison of automatically-derived
corpus representations, based on various
types of textual co-occurrence. We find
that dependency parse-based features are
the most effective, achieving accuracies
similar to the leading semi-manual ap-
proaches and higher than any published
for a corpus-based model. We also find
that simple word features enriched with
directional information provide a close-to-
optimal solution at much lower computa-
tional cost.
</bodyText>
<sectionHeader confidence="0.997337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978395833334">
The cognitive plausibility of computational
models of word meaning has typically been
tested using behavioural benchmarks, such as
identification of synonyms among close asso-
ciates (the TOEFL task for language learners,
see e.g. Landauer and Dumais, 1997); emulating
elicited judgments of pairwise similarity (such as
Rubenstein and Goodenough, 1965); judgments
of category membership (e.g. Battig and Mon-
tague, 1969); and word priming effects (Lund
and Burgess, 1996). Mitchell et al. (2008) in-
troduced a new task in neurosemantic decoding
– using models of semantics to learn the map-
ping between concepts and the neural activity
which they elicit during neuroimaging experi-
ments. This was achieved with a linear model
which used training data to find neural basis im-
ages that correspond to the assumed semantic
dimensions (for instance, one such basis image
might be the activity of the brain for words rep-
resenting animate concepts), and subsequently
used these general patterns and known seman-
tic dimensions to infer the fMRI activity that
should be elicited by an unseen stimulus con-
cept. Follow-on work has experimented with
other neuroimaging modalities (Murphy et al.,
2009), and with a range of semantic models in-
cluding elicited property norms (Chang et al.,
2011), corpus derived models (Devereux and
Kelly, 2010; Pereira et al., 2011) and structured
ontologies (Jelodar et al., 2010).
The current state-of-the-art performance on
this task is achieved using models that are hand-
tailored in some respect, whether using manual
annotation tasks (Palatucci et al., 2009), use of
a domain-appropriate curated corpus (Pereira
et al., 2011), or selection of particular collocates
to suit the concepts to be described (Mitchell
et al., 2008). While these approaches are clearly
very successful, it is questionable whether they
are a general solution to describe the vari-
ous parts-of-speech and semantic domains that
make up a speaker’s vocabulary. The Mitchell
et al. (2008) 25-verb model would probably have
to be extended to describe the lexicon at large,
and it is unclear whether such a compact model
could be maintained. While Wikipedia (Pereira
et al., 2011) has very broad and increasing cov-
</bodyText>
<page confidence="0.979444">
114
</page>
<note confidence="0.9727415">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 114–123,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999887529411765">
erage, it is possible that it will remain inad-
equate for specialist vocabularies, or for less-
studied languages. And while the method used
by Palatucci et al. (2009) distributes the anno-
tation task efficiently by crowd-sourcing, it still
requires that appropriate questions are compiled
by researchers, a task that is both difficult to
perform in a systematic way, and which may not
generalize to more abstract concepts.
In this paper we examine a representative set
of corpus-derived models of meaning, that re-
quire no manual intervention, and are applicable
to any syntactic and semantic domain. We con-
centrate on which types of basic corpus pattern
perform well on the neurosemantic decoding
task: LSA-style word-region co-occurrences,
and various HAL-style word-collocate features
including raw tokens, POS tags, and a full de-
pendency parse. Otherwise a common feature
extraction and preprocessing pipeline is used: a
co-occurrence frequency cutoff, application of a
frequency normalization weighting, and dimen-
sionality reduction with SVD.
The following section describes how the brain
activity data was gathered and processed; the
construction of several corpus-derived models
of meaning; and the regression-based meth-
ods used to predict one from the other, evalu-
ated with a brain-image matching task (Mitchell
et al., 2008). In section 3 we report the re-
sults, and in the Conclusion we discuss both the
practical implications, and what this works sug-
gests for the cognitive plausibility of distribu-
tional models of meaning.
</bodyText>
<sectionHeader confidence="0.997014" genericHeader="introduction">
2 Methods
</sectionHeader>
<subsectionHeader confidence="0.999261">
2.1 Brain activity features
</subsectionHeader>
<bodyText confidence="0.999869375">
The dataset used here is that described in detail
in (Mitchell et al., 2008) and released publicly&apos;
in conjunction with the NAACL 2010 Work-
shop on Computational Neurolinguistics (Mur-
phy et al., 2010). Functional MRI (fMRI) data
was collected from 9 participants while they per-
formed a property generation task. The stimuli
were line-drawings, accompanied by their text
</bodyText>
<footnote confidence="0.9939015">
1http://www.cs.cmu.edu/afs/cs/project/theo-
73/www/science2008/data.html
</footnote>
<bodyText confidence="0.999934791666667">
label, of everyday concrete concepts, with 5 ex-
emplars of each of 12 semantic classes (mam-
mals, body parts, buildings, building parts,
clothes, furniture, insects, kitchen utensils, mis-
cellaneous functional artifacts, work tools, veg-
etables, and vehicles). Stimuli remained on
screen for three seconds, and each was each pre-
sented six times, in random order, to give a total
of 360 image presentations in the session.
The fMRI images were recorded with 3.0T
scanner at 1 second intervals, with a spatial reso-
lution of 3x3x6mm. The resulting data was pre-
processed with the SPM package (Friston et al.,
2007); the blood-oxygen-level response was ap-
proximated by taking a boxcar average over a
sequence of brain images in each trial; percent
signal change was calculated relative to rest pe-
riods, and the data from each of the six repeti-
tions of each stimulus were averaged to yield a
single brain image for each concept. Finally, a
grey-matter anatomical mask was used to select
only those voxels (three-dimensional pixels) that
overlap with cortex, yielding approximately 20
thousand features per participant.
</bodyText>
<subsectionHeader confidence="0.999944">
2.2 Models of semantics
</subsectionHeader>
<bodyText confidence="0.999021045454546">
Our objective is to compare current semantic
representations that get state-of-the-art perfor-
mance on the neuro-semantics task with repre-
sentative distributional models of semantics that
can be derived from arbitrary corpora, using
varying degrees of linguistic preprocessing. A
series of candidate models were selected to rep-
resent the variety of ways in which basic textual
features can be extracted and represented, in-
cluding token co-occurrence in a small local win-
dow, dependency parses of whole sentences, and
document co-occurrence, among others. Other
parameters were kept fixed in a way that the
literature suggests would be neutral to the var-
ious models, and so allow a fair comparison
among them (Sahlgren, 2006; Bullinaria and
Levy, 2007; Turney and Pantel, 2010).
All textual statistics were gathered from a set
of 50m English-language web-page documents
consisting of 16 billion words. Where a fixed
text window was used, we chose an extent of
±4 lower-case tokens either side of the target
</bodyText>
<page confidence="0.998405">
115
</page>
<bodyText confidence="0.98538505">
word of interest, which is in the mid-range of
optimal values found by various authors (Lund
and Burgess, 1996; Rapp, 2003; Sahlgren, 2006).
Positive pointwise-mutual-information (1,2) was
used as an association measure to normalize
the observed co-occurrence frequency p(w, f) for
the varying frequency of the target word p(w)
and its features p(f). PPMI up-weights co-
occurrences between rare words, yielding posi-
tive values for collocations that are more com-
mon than would be expected by chance (i.e. if
word distributions were independent), and dis-
cards negative values that represent patterns of
co-occurrences that are rarer than one would ex-
pect by chance. It has been shown to perform
well generally, with both word- and document-
level statistics, in raw and dimensionality re-
duced forms (Bullinaria and Levy, 2007; Turney
and Pantel, 2010).2
dimension-
</bodyText>
<footnote confidence="0.928177666666667">
2Preliminary analyses confirmed that PPMI per-
formed as well or better than alternatives including log-
likelihood, TF-IDF, and log-entropy.
</footnote>
<bodyText confidence="0.998980745454546">
ality ranged widely, from about 500 thousand,
to tens of millions. A singular value decompo-
sition (SVD) was applied to identify the 1000
dimensions within each model with the great-
est explanatory power, which also has the ef-
fect of combining similar dimensions (such as
synonyms, inflectional variants, topically simi-
lar documents) into common components, and
discarding more noisy dimensions in the data.
Again there is variation in the number of di-
mension that authors use: here we experiment
with 300 and 1000. For decomposition we used
a sparse SVD method, the Implicitly Restarted
Arnoldi Method (Lehoucq et al., 1998; Jones
et al., 2001), which was coherent with the PPMI
normalization used, since a zero value repre-
sented both negative target-feature associations,
and those that were not observed or fell below
the frequency cut-off. We also streamlined the
task by reducing the input data C (of n target
words by m co-occurrence features) to a square
matrix CCT of size n x n, taking advantage of
the equality of their left singular vectors U. For
SVD to generalize well over the many input fea-
tures, it is also important to have more training
cases that the small set of 60 concrete nouns
used in our evaluation task. Consequently we
gathered all statistics over a set of the 40,000
most frequent word-forms found in the Ameri-
can National Corpus (Nancy Ide and Keith Su-
derman, 2006), which should approximate the
scale and composition of the vocabulary of a
university-educated speaker of English (Nation
and Waring, 1997), and over 95% of tokens typ-
ically encountered in English.
A frequency threshold is commonly applied
for three reasons: low-frequency co-occurrence
counts are more noisy; PMI is positively bi-
ased towards hapax co-occurrences; and due
to Zipfian distributions acut-off dramatically
reduces the amount of data to be processed.
Many authors use a threshold of approximately
50-100 occurrences for word-collocate models
(Lund and Burgess, 1996; Lin, 1998; Rapp,
2003). Since Bullinaria and Levy (2007) find
improving performance with models using pro-
gressively lower cutoffs we explored two cut-offs
of 20 and 50 which equate to low co-occurrences
thresholds of 0.00125 or 0.003125 per million re-
spectively; for the word-region model we chose
a threshold of 2 occurrences of a target term in
a document, to keep the input features to a rea-
sonable dimensionality (Bradford, 2008).
After applying these operations to the input
data from each model, the resulting
</bodyText>
<subsubsectionHeader confidence="0.738121">
2.2.1 Hand-tailored benchmarks
</subsubsectionHeader>
<bodyText confidence="0.999876909090909">
The state-of-the-art models on this brain ac-
tivity prediction task are both hand-tailored.
Mitchell et al. (2008) used a model of seman-
tics based on co-occurrence in the Google 1T 5-
gram corpus of English (Brants and Franz, 2006)
with a small set of 25 Verbs chosen to rep-
resent everyday sensory-motor interaction with
concrete objects, such as see, move, listen. We
recreated this using our current parameters (web
document corpus, co-occurrence frequency cut-
off, PPMI normalization). The second hand-
</bodyText>
<equation confidence="0.887626166666667">
PMI&amp;quot;f = log �p(w, f) (2)
p(w)p(f)
�PMI&amp;quot;f if PMI&amp;quot;f &gt; 0
0 otherwise
(1)
PPMI&amp;quot;f =
</equation>
<page confidence="0.980393">
116
</page>
<bodyText confidence="0.999968538461538">
tailored dataset we used was a set of Elicited
Properties inspired by the 20 Questions game,
and gathered using Mechanical Turk (Palatucci
et al., 2009; Palatucci, 2011). Multiple infor-
mants were asked to answer one or more of 218
questions “related to size, shape, surface prop-
erties, and typical usage” such as Do you see
it daily?, Is it wild?, Is it man-made? with a
scalar response ranging from 1 to 5. The re-
sulting responses were then averaged over infor-
mants, and then the values of each question were
grouped into 5 bins, giving all dimensions simi-
lar mean and variance.
</bodyText>
<subsubsectionHeader confidence="0.923602">
2.2.2 Word-Region Model
</subsubsectionHeader>
<bodyText confidence="0.999496083333333">
Latent Semantic Analysis (Deerwester et al.,
1990; Landauer and Dumais, 1997), and its
probabilistic cousins (Blei et al., 2003; Grif-
fiths et al., 2007), express the meaning of a
word as a distribution of co-occurrence across
a set of documents, or other text-regions such
as paragraphs. This word-region matrix in-
stantiates the assumption that words that share
a topical domain (such as medicine, entertain-
ment, philosophy) would be expected to appear
in similar sub-sets of text-regions. In such a
model, the nearest neighbors of a target word
are syntagmatically related (i.e. appear along-
side each other), and for judge might include
lawyer, court, crime, or prison.
The Document model used here is loosely
based on LSA, taking the frequency of occur-
rence of each of our 40,000 vocabulary words
in each of 50 million documents as its input
data, and it follows Bullinaria and Levy (2007);
Turney and Pantel (2010) in using PPMI as a
normalization function. We have not investi-
gated variations on the decomposition algorithm
in any detail, such as those using non-negative
matrix factorization, probabilistic LSA or LDA
topic models, as the objective in this paper is
to provide a direct comparison between the dif-
ferent types of basic collocation information en-
coded in corpora, rather than evaluate the best
algorithmic means for discovering latent dimen-
sions in those co-occurrences. Nor have we eval-
uated performance on a more structured corpus
input (Pereira et al., 2011). However prelimi-
nary tests with the Wikipedia corpus, and with
LDA, using the Gensim package (Rehurek and
Sojka, 2010) yielded similar performances.
</bodyText>
<subsubsectionHeader confidence="0.824743">
2.2.3 Word-Collocate Models
</subsubsectionHeader>
<bodyText confidence="0.999987093023256">
Word-collocate models make a complemen-
tary assumption to that of the document model:
that words with closely-related categorical or
taxonomic properties should appear in the same
position of similar sentences. In a basic word-
collocate model, based on a word-word co-
occurrence matrix, the nearest neighbors of
judge might be athlete, singer, or ire-ighter,
reflecting paradigmatic relatedness (i.e. substi-
tutability). Word-collocate models are further
differentiated by the amount of linguistic anno-
tation attached to word features, ranging from
simple word-form features in a fixed-width win-
dow around the concept word, to more elaborate
word sequence patterns and parses including
parts of speech and dependency relation tags.
Among these alternatives, we might expect a
dependency model to be the most powerful. In-
tuitively, the information that John is sentient
is similarly encoded in the text John likes cake
and John seems to really really like cake, and a
suitably effective parser should be able to gen-
eralize over this variation, to extract the same
dependency relationship of John-subject-like. In
contrast a narrow window-based model might
exclude informative features (such as like in the
second example), while including presumably
uninformative ones (such as really). However
parsers have the disadvantage of being computa-
tionally expensive (meaning that they typically
are applied to smaller corpora) and usually in-
troduce some noise through their errors. Conse-
quently, simpler window-based models have of-
ten been found to be as effective.
The most basic model considered is the
Word-Form model, in which all lower-case to-
kens (word forms and punctuation) found within
four positions left and right of the target word
are recorded, yielding simple features such as
{john, likes}. It may also be termed a `flat&apos;
model in contrast to those which assign a vari-
able weight to collocates, progressively lower as
one moves further than the target position (e.g.
</bodyText>
<page confidence="0.990522">
117
</page>
<bodyText confidence="0.99989272">
Lund et al., 1995). We did not use a stop-list, as
Bullinaria and Levy (2007) found co-occurrence
with very high frequency words also to be infor-
mative for semantic tasks. We also expect that
the subsequent steps of normalizing with PPMI,
reduction with SVD, and use of regularised re-
gression should be able to recognize when such
high-frequency words are not informative and
then discount these, without the need for such
assumptions upfront.
The Stemmed model is a slight variation on
the Word-Form model, where the same statistics
are aggregated after applying Lancaster stem-
ming (Paice, 1990; Loper and Bird, 2002).
The Directional model, inspired by Schutze
and Pedersen (1993), is also derived from the
word-form model, but differentiates between co-
occurrence to the left or to the right of the target
word, with features such as {john L, cake R}.
The Part-of-Speech model (Kanejiya et al.,
2003; Widdows, 2003) replaces each lower-
case word-token with its part-of-speech disam-
biguated form (e.g. likes VBZ, cake NN). These
annotations were extracted from the full depen-
dency parse described below.
The Sequence model draws on a range of
work that uses word sequence patterns (Lin and
Pantel, 2001; Almuhareb and Poesio, 2004; Ba-
roni et al., 2010), and may also be considered an
approximation of models that use shallow syn-
tactic analysis (Grefenstette, 1994; Curran and
Moens, 2002). All distinct token sequences up
to length 4 either side of the target word were
counted.
Finally the Dependency model uses a full
dependency parse, which might be considered
the most informed representation of the word-
collocate relationships instantiated in corpus
sentences, and this approach has been used by
several authors (Lin, 1998; Padd and Lapata,
2007; Baroni and Lenci, 2010). The features
used are pairs of dependency relation and lex-
eme corresponding to each edge linked to a tar-
get word of interest (e.g. likes subj). The parser
used here was Malt, which achieves accuracies of
85% when deriving labelled dependencies on En-
glish text (Hall et al., 2007). The features pro-
duced by this module are much more limited,
to those words that have a direct dependency
relation with the word of interest.
</bodyText>
<subsectionHeader confidence="0.987123">
2.3 Linear Learning Model
</subsectionHeader>
<bodyText confidence="0.999966">
A linear regression model will allow us to eval-
uate how well a given model of word semantics
can be used to predict brain activity. We fol-
low the analysis in Mitchell et al. (2008) and
subsequently adopted by several other research
groups (see Murphy et al., 2010). For each par-
ticipant and selected fMRI feature (i.e. each
voxel, which records the time-course of neural
activity at a fixed location in the brain), we train
a model where the level of activation of the latter
(the blood oxygenation level) in response to dif-
ferent concepts is approximated by a regularised
linear combination of their semantic features:
</bodyText>
<equation confidence="0.997453">
f = CQ + A||Q||2 (3)
</equation>
<bodyText confidence="0.999916428571429">
where f is the vector of activations of a spe-
cific fMRI feature for different concepts, the ma-
trix C contains the values of the semantic fea-
tures for the same concepts, Q is the vector of
weights we must learn for each of those (corpus-
derived) features, and A tunes the degree of reg-
ularisation. We can illustrate this with a toy
example, containing several stimulus concepts
and their attributes on three semantic dimen-
sions: cat (+animate, -big, +moving); phone
(-animate, -big, -moving); elephant (+animate,
+big, +moving); skate-board (-animate, -big,
+moving). After training over all the voxels in
our fMRI data with this simple semantic model,
we can derive whole brain images that are typ-
ical of each of the semantic dimensions. The
power of the model is its ability to predict ac-
tivity for concepts that were not in the training
set – for instance the brain activation elicited by
the word car might be approximated by combin-
ing the images see for -animate, +big, +moving,
even though this combination of properties was
not observed during training.
The linear model was estimated with a
least squared errors method and L2 regularisa-
tion, selecting the lambda parameter from the
range 0.0001 to 5000 using Generalized Cross-
Validation (see Hastie et al., 2011, p.244). The
</bodyText>
<page confidence="0.993725">
118
</page>
<bodyText confidence="0.999991037037037">
activation of each fMRI voxel in response to a
new concept that was not in the training data
was predicted by a Q-weighted sum of the val-
ues on each semantic dimension, building a pic-
ture of expected the global neural activity re-
sponse for an arbitrary concept. Again follow-
ing Mitchell et al. (2008) we use a leave-2-out
paradigm in which a linear model for each neu-
ral feature is trained in turn on all concepts mi-
nus 2, having selected the 500 most stable voxels
in the training set using the same correlational
measure across stimulus presentations. For each
of the 2 left-out concepts, we predict the global
neural activation pattern, as just described. We
then try to correctly match the predicted and
observed activations, by measuring the cosine
distance between the model-generated estimate
of fMRI activity and the that observed in the ex-
periment. If the sum of the matched cosine dis-
tances is lower than the sum of the mismatched
distances, we consider the prediction successful
– otherwise as failed. At chance levels, expected
matching accuracy is 50%, and significant per-
formance above chance can be estimated using
the binomial test, once variance had been veri-
fied over independent trials (i.e. where no single
stimulus concept is shared between pairs).
</bodyText>
<sectionHeader confidence="0.999842" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.997657055555556">
Table 1 shows the main results of the leave-
two-out brain-image matching task. They show
the mean classification performance over 1770
word pairs (60 select 2) by 9 participants. All of
these classification accuracies are highly signif-
icant at p « 0.001 over test trials (binomial,
chance 50%, n=1770*9) and p &lt; 0.001 over
words (binomial, chance 50%, n=60). There
were some significant differences between mod-
els when making inferences over trials, but for
the small set of words used here it is not possible
to make firm conclusions about the superiority
of one model over the other, that could be confi-
dently expected to generalize to other stimuli or
experiments. However, we do achieve classifica-
tion accuracies that are as high, or higher than
any previously published (Palatucci et al., 2009;
Pereira et al., 2011), while models based on very
</bodyText>
<table confidence="0.9998694">
Semantic Models Features Accuracy
25 Verbs 25 78.5
Elicited Properties 218 83.5
Document (f2) 1000 76.2
Word Form 1000 80.0
Stemmed 1000 76.2
Direction 1000 80.2
Part-of-Speech 1000 80.0
Sequence 1000 78.5
Dependency 1000 83.1
</table>
<tableCaption confidence="0.999286666666667">
Table 1: Brain activity prediction accuracy on leave-
2-out pair-matching task. A frequency cutoff of 20
was used for all 1000 dimensional models.
</tableCaption>
<table confidence="0.999837875">
Semantic Models 300 Feats. 1000 Feats.
Document (f2) 79.9 76.2
Word Form 78.1 80.0
Stemmed 77.9 76.2
Direction 80.0 80.2
Part-of-Speech 77.9 80.0
Sequence 72.9 78.5
Dependency 81.6 83.1
</table>
<tableCaption confidence="0.991716">
Table 2: Effect of SVD dimensionality in the leave-
2-out pair-matching setting; frequency cutoff of 20.
</tableCaption>
<bodyText confidence="0.620838">
different basic features (directional word-forms;
dependency relations; document co-occurrence)
yield very similar performance.
</bodyText>
<subsectionHeader confidence="0.998765">
3.1 Effect of Number of Dimensions
</subsectionHeader>
<bodyText confidence="0.9999831">
Here we evaluate what effect the number of SVD
dimensions used has on the final performance
of various semantic models. Experimental re-
sults comparing 300 and 1000 dimensions are
presented in Table 2, all based on a frequency
cutoff of 20. We observe that performance im-
proves in 5 out of 7 semantic models compared,
with the highest performance achieved by the
Dependency model when 1000 SVD dimensions
were used.
</bodyText>
<subsectionHeader confidence="0.999495">
3.2 Effect of Frequency Cutoff
</subsectionHeader>
<bodyText confidence="0.999918333333333">
In this section, we evaluate what effect frequency
cutoff has on the brain prediction accuracy of
various semantic models. From the results in
Table 3, we observe only marginal changes as
the frequency cutoff varied from 20 to 50. This
suggests that the semantic models of this set of
</bodyText>
<page confidence="0.997513">
119
</page>
<table confidence="0.999623375">
Semantic Models Cutoff = 50 Cutoff = 20
Document (f2) 79.9 79.9
Word Form 78.5 78.1
Stemmed 78.2 77.9
Direction 80.8 80.0
Part-of-Speech 77.5 77.9
Sequence 74.4 72.9
Dependency 81.3 81.6
</table>
<tableCaption confidence="0.98121">
Table 3: Effect of frequency cutoff in the leave-2-out
pair-matching setting; 300 SVD dimensions.
</tableCaption>
<bodyText confidence="0.9934438">
words are not very sensitive to variations in the
frequency cutoff under current experimental set-
tings, and do not benefit clearly from the de-
crease in sparsity and increase in noise that a
lower threshold produces.
</bodyText>
<subsectionHeader confidence="0.973961">
3.3 Information Overlap Analysis
</subsectionHeader>
<bodyText confidence="0.999847555555556">
To verify that the models are in fact substan-
tially different, we performed a follow-on analy-
sis that measured the informational overlap be-
tween the corpus-derived models. Given two
models A and B, both with dimensionality 40
thousand words by 300 SVD dimensions, we can
evaluate the extent to which A (used as the
predictor semantic representation) contains the
information encoded in B (the explained rep-
resentation). As shown in (4), for each SVD
component c, we take the left singular vector
bc as a dependent variable and fit it with a lin-
ear model, using the matrix A (all left singu-
lar vectors) as independent variables. The ex-
plained variance for this column is weighted by
its squared singular value s2c in B, and the sum of
these component-wise variances gives the total
variance explained R2A→B.
</bodyText>
<equation confidence="0.9496945">
2
RA→B =
</equation>
<bodyText confidence="0.9010025">
Figure 1 indicates that the first three models,
which are all derived from token occurrences in a
±4 window, are close to identical. The sequence
and document models are relatively dissimilar,
and the dependency model occupies a middle
ground, with some similarity to all the models.
It is also interesting to note that the among the
first cluster of word-form derived models, the
</bodyText>
<figureCaption confidence="0.9769645">
Figure 1: Informational Overlap between Corpus-
Derived Datasets, in R2
</figureCaption>
<bodyText confidence="0.8862515">
directional one has the highest similarity to the
dependency model.
</bodyText>
<sectionHeader confidence="0.994925" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999989384615385">
The main result of this study was that we
achieved classification accuracies as high as
any published, and within a fraction of a per-
centage point of the human benchmark 20
Questions data, using completely unsupervised,
data-driven models of semantics based on a large
random sample of web-text. The most linguisti-
cally informed among the models (and so, per-
haps the most psychologically plausible), based
on dependency parses, is the most successful.
Still the performance of sometimes radically dif-
ferent models, from Document-based (syntag-
matic) and Word-Form-based (paradigmatic), is
surprisingly similar. One reason for this may be
that we have reached a ceiling in performance
on the fMRI data, due to its inherent noise – in
this regard it is interesting to note that an at-
tempt to classify individual concepts using this
data directly, without an intervening model of
semantics, also achieves about 80% (though on a
different task, Shinkareva et al., 2008). Another
possible explanation is that both methods reveal
equivalent sets of underlying semantic dimen-
sions, but figure 1 suggests not. Alternatively,
it may be that the small set of 60 words exam-
ined here may be as well-distinguished by means
</bodyText>
<figure confidence="0.568049166666667">
300
�
c=1
s2
c
sc RA→b� (4)
</figure>
<page confidence="0.97684">
120
</page>
<bodyText confidence="0.999990989583334">
of their taxonomic differences, as by their top-
ical differences, a suggestion supported by the
results in Pereira et al. (2011, see Figure 2A).
From the perspective of computational effi-
ciency however, some of the models have clearer
advantages. The Dependency and Part-of-
Speech models are processing-intensive, since
the broad vocabulary considered requires that
the very large quantities of text pass through
a parsing or tagging pipeline (though these
tasks can be parallelized). The Sequence and
Document models conversely require very large
amounts of memory to store all their features
during SVD. In comparison, the Direction model
is impressive, as it achieves close to optimal per-
formance, despite being very cheap to produce
in terms of processor time and memory foot-
print. Its relatively superior performance may
be due to the relatively fixed word-order of En-
glish, making it a good approximation of a De-
pendency model. For instance, given the nar-
row ±4 token windows used here, the Direction
features shaky Left and donate Right (relative
to a target noun) are probably nearly identical
to the Dependency features shaky Adj and do-
nate Subj. The Sequence model might also be
seen as an approximate Dependency model, but
one with the addition of more superficial colloca-
tions such as “fish and chips” or “Judge Judy”,
which are less relevant to our semantic task.
The evidence for the influence of the scal-
ing parameters (number of SVD dimensions,
frequency cutoff) is mixed: cut-off appears to
have little effect either way, and increasing the
number of dimensions can help or hinder (com-
pare the Sequence and Document models). We
can speculate that the Document model is al-
ready “saturated” with 300 dimensions/topics,
but that the other models based on properties
have a higher inherent dimensionality. It may
also be a lower cut-off and higher dimensional-
ity would show clearer benefits over a larger set
of semantic/syntactic domains, including lower-
frequency words (the lowest frequency work in
the set of 60 used here was igloo, which has an
incidence of 0.3 per million words in the ANC).
PPMI appears to be both effective, and par-
simonious with assumptions one might make
about conceptual representations, where it
would be cognitively onerous and unnecessary
to encode all negative features (such as the facts
that dogs do not have wheels, are not commu-
nication events, and do not belong in the avi-
ation domain). But while SVD is certainly ef-
fective in dealing with the pervasive synonymy
and polysemy seen in corpus-feature sets, it is
less clear that it reveals psychologically plausi-
ble dimensions of meaning. Alternatives such as
non-negative matrix factorization (Lee and Se-
ung, 1999) or Latent Dirichlet Allocation (Blei
et al., 2003) might extract more readily inter-
pretable dimensions; or alternative regularisa-
tion methods such as Elastic Nets, Lasso (Hastie
et al., 2011), or Network Regularisation (Sandler
et al., 2009) might even be capable of identifying
meaningful clusters of features when learning di-
rectly on co-occurrence data. Finally, we should
consider whether more derived datasets could be
used as input data in place of the basic corpus
features used here, such as the full facts learned
by the NELL system (Carlson et al., 2010), or
crowd-sourced data which can be easily gathered
for any word (e.g. association norms, Kiss et al.,
1973), though different algorithmic means would
be needed to deal with their extreme degree of
sparsity.
The results also suggest a series of follow-on
analyses. A priority should be to test these
models against a wider range of neuroimaging
data modalities (e.g. MEG, EEG) and stim-
ulus sets, including abstract kinds (see Mur-
phy et al. 2012, for a preliminary study), and
parts-of-speech beyond nouns. It may be that a
putative complementarity between word-region
and word-collocate models is only revealed when
we look at a broader sample of the human
lexicon. And beyond establishing what infor-
mational content is required to make semantic
distinctions, other factorisation methods (e.g.
sparse or non-negative decompositions) could be
applied to yield more interpretable dimensions.
Other classification tasks might also be more
sensitive for detecting differences between mod-
els, such as the test of word identification among
a set by rank accuracy, as used in (Shinkareva
et al., 2008).
</bodyText>
<page confidence="0.997286">
121
</page>
<sectionHeader confidence="0.975275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998858030612245">
Almuhareb, A. and Poesio, M. (2004). Attribute-
based and value-based clustering: An evaluation.
In Proceedings of EMNLP, pages 158–165.
Baroni, M. and Lenci, A. (2010). Distributional
Memory: A General Framework for Corpus-Based
Semantics. Computational Linguistics, 36(4):673–
721.
Baroni, M., Murphy, B., Barbu, E., and Poesio, M.
(2010). Strudel: A corpus-based semantic model
based on properties and types. Cognitive Science,
34(2):222–254.
Battig, W. F. and Montague, W. E. (1969). Cate-
gory Norms for Verbal Items in 56 Categories: A
Replication and Extension of the Connecticut Cat-
egory Norms. Journal of Experimental Psychology
Monographs, 80(3):1–46.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003).
Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3(4-5):993–1022.
Bradford, R. B. (2008). An empirical study of re-
quired dimensionality for large-scale latent seman-
tic indexing applications. Proceeding of the 17th
ACM conference on Information and knowledge
mining CIKM 08, pages 153–162.
Brants, T. and Franz, A. (2006). Web 1T 5-gram
Version 1.
Bullinaria, J. A. and Levy, J. P. (2007). Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39(3):510–526.
Carlson, A., Betteridge, J., Kisiel, B., Settles, B.,
Jr., E. R. H., and Mitchell, T. M. (2010). To-
ward an Architecture for Never-Ending Language
Learning. Artificial Intelligence, 2(4):3–3.
Chang, K.-m. K., Mitchell, T., and Just, M. A.
(2011). Quantitative modeling of the neural repre-
sentation of objects: how semantic feature norms
can account for fMRI activation. NeuroImage,
56(2):716–727.
Curran, J. R. and Moens, M. (2002). Improvements
in automatic thesaurus extraction. In SIGLEX,
pages 59–66.
Deerwester, S., Dumais, S., Landauer, T., Furnas,
G., and Harshman, R. (1990). Indexing by La-
tent Semantic Analysis. Journal of the American
Society of Information Science, 41(6):391 – 407.
Devereux, B. and Kelly, C. (2010). Using fMRI ac-
tivation to conceptual stimuli to evaluate meth-
ods for extracting conceptual representations from
corpora. In Murphy, B., Korhonen, A., and
Chang, K. K.-M., editors, 1st Workshop on Com-
putational Neurolinguistics.
Friston, K. J., Ashburner, J. T., Kiebel, S. J.,
Nichols, T. E., and Penny, W. D. (2007). Statis-
tical Parametric Mapping: The Analysis of Func-
tional Brain Images, volume 8. Academic Press.
Grefenstette, G. (1994). Explorations in Automatic
Thesaurus Discovery. Kluwer, Dordrecht.
Griffiths, T. L., Steyvers, M., and Tenenbaum, J. B.
(2007). Topics in semantic representation. Psy-
chological Review, 114(2):211–244.
Hall, J., Nilsson, J., Nivre, J., Eryigit, G., Megyesi,
B., Nilsson, M., and Saers, M. (2007). Single Malt
or Blended? A Study in Multilingual Parser Op-
timization. CoNLL Shared Task Session, pages
933–939.
Hastie, T., Tibshirani, R., and Friedman, J. (2011).
The Elements of Statistical Learning, volume 18 of
Springer Series in Statistics. Springer, 5th edition.
Jelodar, A. B., Alizadeh, M., and Khadivi, S. (2010).
WordNet Based Features for Predicting Brain Ac-
tivity associated with meanings of nouns. In Mur-
phy, B., Korhonen, A., and Chang, K. K.-M., ed-
itors, 1st Workshop on Computational Neurolin-
guistics, pages 18–26.
Jones, E., Oliphant, T., Peterson, P., and Et Al.
(2001). SciPy: Open source scientific tools for
Python.
Kanejiya, D., Kumar, A., and Prasad, S. (2003).
Automatic evaluation of students’ answers using
syntactically enhanced LSA. Building educational
applications, NAACL, 2:53–60.
Kiss, G. R., Armstrong, C., Milroy, R., and Piper, J.
(1973). An associative thesaurus of English and its
computer analysis. In Aitken, A. J., Bailey, R. W.,
and Hamilton-Smith, N., editors, The Computer
and Literary Studies. Edinburgh University Press.
Landauer, T. and Dumais, S. (1997). A solution to
Plato’s problem: the latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211–240.
Lee, D. D. and Seung, H. S. (1999). Learning the
parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788–91.
Lehoucq, R. B., Sorensen, D. C., and Yang, C.
(1998). Arpack users’ guide: Solution of large
scale eigenvalue problems with implicitly restarted
Arnoldi methods. SIAM.
</reference>
<page confidence="0.971023">
122
</page>
<reference confidence="0.999645797979798">
Lin, D. (1998). Automatic Retrieval and Clustering
of Similar Words. In COLING-ACL, pages 768–
774.
Lin, D. and Pantel, P. (2001). DIRT – discovery
of inference rules from text. Proceedings of the
seventh ACM SIGKDD international conference
on Knowledge discovery and data mining KDD 01,
datamining:323–328.
Loper, E. and Bird, S. (2002). {NLTK}: The natu-
ral language toolkit. In ACL Workshop, volume 1,
pages 63–70. Association for Computational Lin-
guistics.
Lund, K. and Burgess, C. (1996). Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28:203–208.
Lund, K., Burgess, C., and Atchley, R. (1995). Se-
mantic and associative priming in high dimen-
sional semantic space. In Proceedings of the 17th
Cognitive Science Society Meeting, pages 660–665.
Mitchell, T. M., Shinkareva, S. V., Carlson, A.,
Chang, K.-M., Malave, V. L., Mason, R. A., and
Just, M. A. (2008). Predicting Human Brain Ac-
tivity Associated with the Meanings of Nouns. Sci-
ence, 320:1191–1195.
Murphy, B., Baroni, M., and Poesio, M. (2009). EEG
responds to conceptual stimuli and corpus seman-
tics. In Proceedings of EMNLP, pages 619–627.
ACL.
Murphy, B., Korhonen, A., and Chang, K. K.-
M., editors (2010). Proceedings of the 1st Work-
shop on Computational Neurolinguistics, NAACL-
HLT, Los Angeles. ACL.
Murphy, B., Talukdar, P., and Mitchell, T. (2012).
Comparing Abstract and Concrete Conceptual
Representations using Neurosemantic Decoding.
In NAACL Workshop on Cognitive Modelling and
Computational Linguistics.
Nancy Ide and Keith Suderman (2006). The Amer-
ican National Corpus First Release. Proceedings
of the Fifth Language Resources and Evaluation
Conference (LREC).
Nation, P. and Waring, R. (1997). Vocabulary size,
text coverage and word lists. In Schmitt, N. and
McCarthy, M., editors, Vocabulary Description ac-
quisition and pedagogy, pages 6–19. Cambridge
University Press.
Pado, S. and Lapata, M. (2007). Dependency-based
construction of semantic space models. Computa-
tional Linguistics, 33(2):161–199.
Paice, C. D. (1990). Another stemmer. SIGIR Fo-
rum, 24(3):56–61.
Palatucci, M., Hinton, G., Pomerleau, D., and
Mitchell, T. M. (2009). Zero-Shot Learning with
Semantic Output Codes. Advances in Neural In-
formation Processing Systems, 22:1–9.
Palatucci, M. M. (2011). Thought Recognition: Pre-
dicting and Decoding Brain Activity Using the
Zero-Shot Learning Model. PhD thesis, Carnegie
Mellon University.
Pereira, F., Detre, G., and Botvinick, M. (2011).
Generating Text from Functional Brain Images.
Frontiers in Human Neuroscience, 5:1–11.
Rapp, R. (2003). Word Sense Discovery Based on
Sense Descriptor Dissimilarity. Proceedings of the
Ninth Machine Translation Summit, pp:315–322.
Rehurek, R. and Sojka, P. (2010). Software Frame-
work for Topic Modelling with Large Corpora. In
New Challenges, LREC 2010, pages 45–50. ELRA.
Rubenstein, H. and Goodenough, J. B. (1965). Con-
textual correlates of synonymy. Communications
of the ACM, 8(10):627–633.
Sahlgren, M. (2006). The Word-Space Model: Using
distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Dissertation, Stock-
holm University.
Sandler, T., Talukdar, P. P., Ungar, L. H., and
Blitzer, J. (2009). Regularized Learning with Net-
works of Features. Advances in Neural Informa-
tion Processing Systems 21, 4:1401–1408.
Schiitze, H. and Pedersen, J. (1993). A Vector Model
for syntagmatic and paradigmatic relatedness. In
Making Sense of Words Proceedings of the 9th
Annual Conference of the University of Waterloo
Centre for the New OED and Text Research, pages
104–113.
Shinkareva, S. V., Mason, R. A., Malave, V. L.,
Wang, W., Mitchell, T. M., and Just, M. A.
(2008). Using fMRI Brain Activation to Iden-
tify Cognitive States Associated with Perception
of Tools and Dwellings. PloS ONE, 3(1).
Turney, P. D. and Pantel, P. (2010). From Frequency
to Meaning: Vector Space Models of Semantics.
Artificial Intelligence, 37(1):141–188.
Widdows, D. (2003). Unsupervised methods for de-
veloping taxonomies by combining syntactic and
statistical information. In NAACL, pages 197–
204. Association for Computational Linguistics.
</reference>
<page confidence="0.998948">
123
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.103340">
<title confidence="0.99999">Selecting Corpus-Semantic Models for Neurolinguistic Decoding</title>
<author confidence="0.989868">Brian</author>
<affiliation confidence="0.859567333333333">Machine Learning Carnegie Mellon Pittsburgh,</affiliation>
<email confidence="0.999758">brianmurphy@cmu.edu</email>
<author confidence="0.523372">Partha</author>
<affiliation confidence="0.842123333333333">Machine Learning Carnegie Mellon Pittsburgh,</affiliation>
<email confidence="0.999792">ppt@cs.cmu.edu</email>
<author confidence="0.94576">Tom</author>
<affiliation confidence="0.866969">Machine Learning Carnegie Mellon Pittsburgh,</affiliation>
<email confidence="0.999907">tom.mitchell@cs.cmu.edu</email>
<abstract confidence="0.992249954545455">Neurosemantics aims to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments. Different approaches have been used to represent individual concepts, but current state-of-the-art techniques require extensive manual intervention to scale to arbitrary words and domains. To overcome this challenge, we initiate a systematic comparison of automatically-derived corpus representations, based on various types of textual co-occurrence. We find that dependency parse-based features are the most effective, achieving accuracies similar to the leading semi-manual approaches and higher than any published for a corpus-based model. We also find that simple word features enriched with directional information provide a close-tooptimal solution at much lower computational cost.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Almuhareb</author>
<author>M Poesio</author>
</authors>
<title>Attributebased and value-based clustering: An evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>158--165</pages>
<contexts>
<context position="17299" citStr="Almuhareb and Poesio, 2004" startWordPosition="2691" endWordPosition="2694">nd Bird, 2002). The Directional model, inspired by Schutze and Pedersen (1993), is also derived from the word-form model, but differentiates between cooccurrence to the left or to the right of the target word, with features such as {john L, cake R}. The Part-of-Speech model (Kanejiya et al., 2003; Widdows, 2003) replaces each lowercase word-token with its part-of-speech disambiguated form (e.g. likes VBZ, cake NN). These annotations were extracted from the full dependency parse described below. The Sequence model draws on a range of work that uses word sequence patterns (Lin and Pantel, 2001; Almuhareb and Poesio, 2004; Baroni et al., 2010), and may also be considered an approximation of models that use shallow syntactic analysis (Grefenstette, 1994; Curran and Moens, 2002). All distinct token sequences up to length 4 either side of the target word were counted. Finally the Dependency model uses a full dependency parse, which might be considered the most informed representation of the wordcollocate relationships instantiated in corpus sentences, and this approach has been used by several authors (Lin, 1998; Padd and Lapata, 2007; Baroni and Lenci, 2010). The features used are pairs of dependency relation an</context>
</contexts>
<marker>Almuhareb, Poesio, 2004</marker>
<rawString>Almuhareb, A. and Poesio, M. (2004). Attributebased and value-based clustering: An evaluation. In Proceedings of EMNLP, pages 158–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Lenci</author>
</authors>
<title>Distributional Memory: A General Framework for Corpus-Based Semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<pages>721</pages>
<contexts>
<context position="17844" citStr="Baroni and Lenci, 2010" startWordPosition="2778" endWordPosition="2781"> uses word sequence patterns (Lin and Pantel, 2001; Almuhareb and Poesio, 2004; Baroni et al., 2010), and may also be considered an approximation of models that use shallow syntactic analysis (Grefenstette, 1994; Curran and Moens, 2002). All distinct token sequences up to length 4 either side of the target word were counted. Finally the Dependency model uses a full dependency parse, which might be considered the most informed representation of the wordcollocate relationships instantiated in corpus sentences, and this approach has been used by several authors (Lin, 1998; Padd and Lapata, 2007; Baroni and Lenci, 2010). The features used are pairs of dependency relation and lexeme corresponding to each edge linked to a target word of interest (e.g. likes subj). The parser used here was Malt, which achieves accuracies of 85% when deriving labelled dependencies on English text (Hall et al., 2007). The features produced by this module are much more limited, to those words that have a direct dependency relation with the word of interest. 2.3 Linear Learning Model A linear regression model will allow us to evaluate how well a given model of word semantics can be used to predict brain activity. We follow the anal</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Baroni, M. and Lenci, A. (2010). Distributional Memory: A General Framework for Corpus-Based Semantics. Computational Linguistics, 36(4):673– 721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>B Murphy</author>
<author>E Barbu</author>
<author>M Poesio</author>
</authors>
<title>Strudel: A corpus-based semantic model based on properties and types.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="17321" citStr="Baroni et al., 2010" startWordPosition="2695" endWordPosition="2699">nal model, inspired by Schutze and Pedersen (1993), is also derived from the word-form model, but differentiates between cooccurrence to the left or to the right of the target word, with features such as {john L, cake R}. The Part-of-Speech model (Kanejiya et al., 2003; Widdows, 2003) replaces each lowercase word-token with its part-of-speech disambiguated form (e.g. likes VBZ, cake NN). These annotations were extracted from the full dependency parse described below. The Sequence model draws on a range of work that uses word sequence patterns (Lin and Pantel, 2001; Almuhareb and Poesio, 2004; Baroni et al., 2010), and may also be considered an approximation of models that use shallow syntactic analysis (Grefenstette, 1994; Curran and Moens, 2002). All distinct token sequences up to length 4 either side of the target word were counted. Finally the Dependency model uses a full dependency parse, which might be considered the most informed representation of the wordcollocate relationships instantiated in corpus sentences, and this approach has been used by several authors (Lin, 1998; Padd and Lapata, 2007; Baroni and Lenci, 2010). The features used are pairs of dependency relation and lexeme corresponding</context>
</contexts>
<marker>Baroni, Murphy, Barbu, Poesio, 2010</marker>
<rawString>Baroni, M., Murphy, B., Barbu, E., and Poesio, M. (2010). Strudel: A corpus-based semantic model based on properties and types. Cognitive Science, 34(2):222–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W F Battig</author>
<author>W E Montague</author>
</authors>
<title>Category Norms for Verbal Items in 56 Categories: A Replication and Extension of the Connecticut Category Norms.</title>
<date>1969</date>
<journal>Journal of Experimental Psychology Monographs,</journal>
<volume>80</volume>
<issue>3</issue>
<contexts>
<context position="1624" citStr="Battig and Montague, 1969" startWordPosition="211" endWordPosition="215">er than any published for a corpus-based model. We also find that simple word features enriched with directional information provide a close-tooptimal solution at much lower computational cost. 1 Introduction The cognitive plausibility of computational models of word meaning has typically been tested using behavioural benchmarks, such as identification of synonyms among close associates (the TOEFL task for language learners, see e.g. Landauer and Dumais, 1997); emulating elicited judgments of pairwise similarity (such as Rubenstein and Goodenough, 1965); judgments of category membership (e.g. Battig and Montague, 1969); and word priming effects (Lund and Burgess, 1996). Mitchell et al. (2008) introduced a new task in neurosemantic decoding – using models of semantics to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments. This was achieved with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to in</context>
</contexts>
<marker>Battig, Montague, 1969</marker>
<rawString>Battig, W. F. and Montague, W. E. (1969). Category Norms for Verbal Items in 56 Categories: A Replication and Extension of the Connecticut Category Norms. Journal of Experimental Psychology Monographs, 80(3):1–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--4</pages>
<contexts>
<context position="12563" citStr="Blei et al., 2003" startWordPosition="1941" endWordPosition="1944">chanical Turk (Palatucci et al., 2009; Palatucci, 2011). Multiple informants were asked to answer one or more of 218 questions “related to size, shape, surface properties, and typical usage” such as Do you see it daily?, Is it wild?, Is it man-made? with a scalar response ranging from 1 to 5. The resulting responses were then averaged over informants, and then the values of each question were grouped into 5 bins, giving all dimensions similar mean and variance. 2.2.2 Word-Region Model Latent Semantic Analysis (Deerwester et al., 1990; Landauer and Dumais, 1997), and its probabilistic cousins (Blei et al., 2003; Griffiths et al., 2007), express the meaning of a word as a distribution of co-occurrence across a set of documents, or other text-regions such as paragraphs. This word-region matrix instantiates the assumption that words that share a topical domain (such as medicine, entertainment, philosophy) would be expected to appear in similar sub-sets of text-regions. In such a model, the nearest neighbors of a target word are syntagmatically related (i.e. appear alongside each other), and for judge might include lawyer, court, crime, or prison. The Document model used here is loosely based on LSA, ta</context>
<context position="29792" citStr="Blei et al., 2003" startWordPosition="4750" endWordPosition="4753">and parsimonious with assumptions one might make about conceptual representations, where it would be cognitively onerous and unnecessary to encode all negative features (such as the facts that dogs do not have wheels, are not communication events, and do not belong in the aviation domain). But while SVD is certainly effective in dealing with the pervasive synonymy and polysemy seen in corpus-feature sets, it is less clear that it reveals psychologically plausible dimensions of meaning. Alternatives such as non-negative matrix factorization (Lee and Seung, 1999) or Latent Dirichlet Allocation (Blei et al., 2003) might extract more readily interpretable dimensions; or alternative regularisation methods such as Elastic Nets, Lasso (Hastie et al., 2011), or Network Regularisation (Sandler et al., 2009) might even be capable of identifying meaningful clusters of features when learning directly on co-occurrence data. Finally, we should consider whether more derived datasets could be used as input data in place of the basic corpus features used here, such as the full facts learned by the NELL system (Carlson et al., 2010), or crowd-sourced data which can be easily gathered for any word (e.g. association no</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research, 3(4-5):993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R B Bradford</author>
</authors>
<title>An empirical study of required dimensionality for large-scale latent semantic indexing applications.</title>
<date>2008</date>
<booktitle>Proceeding of the 17th ACM conference on Information and knowledge mining CIKM 08,</booktitle>
<pages>153--162</pages>
<contexts>
<context position="11133" citStr="Bradford, 2008" startWordPosition="1705" endWordPosition="1706">acut-off dramatically reduces the amount of data to be processed. Many authors use a threshold of approximately 50-100 occurrences for word-collocate models (Lund and Burgess, 1996; Lin, 1998; Rapp, 2003). Since Bullinaria and Levy (2007) find improving performance with models using progressively lower cutoffs we explored two cut-offs of 20 and 50 which equate to low co-occurrences thresholds of 0.00125 or 0.003125 per million respectively; for the word-region model we chose a threshold of 2 occurrences of a target term in a document, to keep the input features to a reasonable dimensionality (Bradford, 2008). After applying these operations to the input data from each model, the resulting 2.2.1 Hand-tailored benchmarks The state-of-the-art models on this brain activity prediction task are both hand-tailored. Mitchell et al. (2008) used a model of semantics based on co-occurrence in the Google 1T 5- gram corpus of English (Brants and Franz, 2006) with a small set of 25 Verbs chosen to represent everyday sensory-motor interaction with concrete objects, such as see, move, listen. We recreated this using our current parameters (web document corpus, co-occurrence frequency cutoff, PPMI normalization).</context>
</contexts>
<marker>Bradford, 2008</marker>
<rawString>Bradford, R. B. (2008). An empirical study of required dimensionality for large-scale latent semantic indexing applications. Proceeding of the 17th ACM conference on Information and knowledge mining CIKM 08, pages 153–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<title>Web 1T 5-gram Version 1.</title>
<date>2006</date>
<contexts>
<context position="11477" citStr="Brants and Franz, 2006" startWordPosition="1758" endWordPosition="1761"> of 20 and 50 which equate to low co-occurrences thresholds of 0.00125 or 0.003125 per million respectively; for the word-region model we chose a threshold of 2 occurrences of a target term in a document, to keep the input features to a reasonable dimensionality (Bradford, 2008). After applying these operations to the input data from each model, the resulting 2.2.1 Hand-tailored benchmarks The state-of-the-art models on this brain activity prediction task are both hand-tailored. Mitchell et al. (2008) used a model of semantics based on co-occurrence in the Google 1T 5- gram corpus of English (Brants and Franz, 2006) with a small set of 25 Verbs chosen to represent everyday sensory-motor interaction with concrete objects, such as see, move, listen. We recreated this using our current parameters (web document corpus, co-occurrence frequency cutoff, PPMI normalization). The second handPMI&amp;quot;f = log �p(w, f) (2) p(w)p(f) �PMI&amp;quot;f if PMI&amp;quot;f &gt; 0 0 otherwise (1) PPMI&amp;quot;f = 116 tailored dataset we used was a set of Elicited Properties inspired by the 20 Questions game, and gathered using Mechanical Turk (Palatucci et al., 2009; Palatucci, 2011). Multiple informants were asked to answer one or more of 218 questions “rel</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Brants, T. and Franz, A. (2006). Web 1T 5-gram Version 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Bullinaria</author>
<author>J P Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="7471" citStr="Bullinaria and Levy, 2007" startWordPosition="1117" endWordPosition="1120">sk with representative distributional models of semantics that can be derived from arbitrary corpora, using varying degrees of linguistic preprocessing. A series of candidate models were selected to represent the variety of ways in which basic textual features can be extracted and represented, including token co-occurrence in a small local window, dependency parses of whole sentences, and document co-occurrence, among others. Other parameters were kept fixed in a way that the literature suggests would be neutral to the various models, and so allow a fair comparison among them (Sahlgren, 2006; Bullinaria and Levy, 2007; Turney and Pantel, 2010). All textual statistics were gathered from a set of 50m English-language web-page documents consisting of 16 billion words. Where a fixed text window was used, we chose an extent of ±4 lower-case tokens either side of the target 115 word of interest, which is in the mid-range of optimal values found by various authors (Lund and Burgess, 1996; Rapp, 2003; Sahlgren, 2006). Positive pointwise-mutual-information (1,2) was used as an association measure to normalize the observed co-occurrence frequency p(w, f) for the varying frequency of the target word p(w) and its feat</context>
<context position="10756" citStr="Bullinaria and Levy (2007)" startWordPosition="1641" endWordPosition="1644">uld approximate the scale and composition of the vocabulary of a university-educated speaker of English (Nation and Waring, 1997), and over 95% of tokens typically encountered in English. A frequency threshold is commonly applied for three reasons: low-frequency co-occurrence counts are more noisy; PMI is positively biased towards hapax co-occurrences; and due to Zipfian distributions acut-off dramatically reduces the amount of data to be processed. Many authors use a threshold of approximately 50-100 occurrences for word-collocate models (Lund and Burgess, 1996; Lin, 1998; Rapp, 2003). Since Bullinaria and Levy (2007) find improving performance with models using progressively lower cutoffs we explored two cut-offs of 20 and 50 which equate to low co-occurrences thresholds of 0.00125 or 0.003125 per million respectively; for the word-region model we chose a threshold of 2 occurrences of a target term in a document, to keep the input features to a reasonable dimensionality (Bradford, 2008). After applying these operations to the input data from each model, the resulting 2.2.1 Hand-tailored benchmarks The state-of-the-art models on this brain activity prediction task are both hand-tailored. Mitchell et al. (2</context>
<context position="13327" citStr="Bullinaria and Levy (2007)" startWordPosition="2068" endWordPosition="2071">ns such as paragraphs. This word-region matrix instantiates the assumption that words that share a topical domain (such as medicine, entertainment, philosophy) would be expected to appear in similar sub-sets of text-regions. In such a model, the nearest neighbors of a target word are syntagmatically related (i.e. appear alongside each other), and for judge might include lawyer, court, crime, or prison. The Document model used here is loosely based on LSA, taking the frequency of occurrence of each of our 40,000 vocabulary words in each of 50 million documents as its input data, and it follows Bullinaria and Levy (2007); Turney and Pantel (2010) in using PPMI as a normalization function. We have not investigated variations on the decomposition algorithm in any detail, such as those using non-negative matrix factorization, probabilistic LSA or LDA topic models, as the objective in this paper is to provide a direct comparison between the different types of basic collocation information encoded in corpora, rather than evaluate the best algorithmic means for discovering latent dimensions in those co-occurrences. Nor have we evaluated performance on a more structured corpus input (Pereira et al., 2011). However p</context>
<context position="16146" citStr="Bullinaria and Levy (2007)" startWordPosition="2506" endWordPosition="2509">y introduce some noise through their errors. Consequently, simpler window-based models have often been found to be as effective. The most basic model considered is the Word-Form model, in which all lower-case tokens (word forms and punctuation) found within four positions left and right of the target word are recorded, yielding simple features such as {john, likes}. It may also be termed a `flat&apos; model in contrast to those which assign a variable weight to collocates, progressively lower as one moves further than the target position (e.g. 117 Lund et al., 1995). We did not use a stop-list, as Bullinaria and Levy (2007) found co-occurrence with very high frequency words also to be informative for semantic tasks. We also expect that the subsequent steps of normalizing with PPMI, reduction with SVD, and use of regularised regression should be able to recognize when such high-frequency words are not informative and then discount these, without the need for such assumptions upfront. The Stemmed model is a slight variation on the Word-Form model, where the same statistics are aggregated after applying Lancaster stemming (Paice, 1990; Loper and Bird, 2002). The Directional model, inspired by Schutze and Pedersen (</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>Bullinaria, J. A. and Levy, J. P. (2007). Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39(3):510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
<author>B Kisiel</author>
<author>B Settles</author>
<author>E R H</author>
<author>T M Mitchell</author>
</authors>
<title>Toward an Architecture for Never-Ending Language Learning.</title>
<date>2010</date>
<journal>Artificial Intelligence,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="30306" citStr="Carlson et al., 2010" startWordPosition="4832" endWordPosition="4835"> non-negative matrix factorization (Lee and Seung, 1999) or Latent Dirichlet Allocation (Blei et al., 2003) might extract more readily interpretable dimensions; or alternative regularisation methods such as Elastic Nets, Lasso (Hastie et al., 2011), or Network Regularisation (Sandler et al., 2009) might even be capable of identifying meaningful clusters of features when learning directly on co-occurrence data. Finally, we should consider whether more derived datasets could be used as input data in place of the basic corpus features used here, such as the full facts learned by the NELL system (Carlson et al., 2010), or crowd-sourced data which can be easily gathered for any word (e.g. association norms, Kiss et al., 1973), though different algorithmic means would be needed to deal with their extreme degree of sparsity. The results also suggest a series of follow-on analyses. A priority should be to test these models against a wider range of neuroimaging data modalities (e.g. MEG, EEG) and stimulus sets, including abstract kinds (see Murphy et al. 2012, for a preliminary study), and parts-of-speech beyond nouns. It may be that a putative complementarity between word-region and word-collocate models is on</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, H, Mitchell, 2010</marker>
<rawString>Carlson, A., Betteridge, J., Kisiel, B., Settles, B., Jr., E. R. H., and Mitchell, T. M. (2010). Toward an Architecture for Never-Ending Language Learning. Artificial Intelligence, 2(4):3–3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-m K Chang</author>
<author>T Mitchell</author>
<author>M A Just</author>
</authors>
<title>Quantitative modeling of the neural representation of objects: how semantic feature norms can account for fMRI activation.</title>
<date>2011</date>
<journal>NeuroImage,</journal>
<volume>56</volume>
<issue>2</issue>
<contexts>
<context position="2481" citStr="Chang et al., 2011" startWordPosition="352" endWordPosition="355"> neuroimaging experiments. This was achieved with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept. Follow-on work has experimented with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly very successful, it is questionable whether they are a general solution to describe the vario</context>
</contexts>
<marker>Chang, Mitchell, Just, 2011</marker>
<rawString>Chang, K.-m. K., Mitchell, T., and Just, M. A. (2011). Quantitative modeling of the neural representation of objects: how semantic feature norms can account for fMRI activation. NeuroImage, 56(2):716–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
<author>M Moens</author>
</authors>
<title>Improvements in automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In SIGLEX,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="17457" citStr="Curran and Moens, 2002" startWordPosition="2717" endWordPosition="2720"> to the left or to the right of the target word, with features such as {john L, cake R}. The Part-of-Speech model (Kanejiya et al., 2003; Widdows, 2003) replaces each lowercase word-token with its part-of-speech disambiguated form (e.g. likes VBZ, cake NN). These annotations were extracted from the full dependency parse described below. The Sequence model draws on a range of work that uses word sequence patterns (Lin and Pantel, 2001; Almuhareb and Poesio, 2004; Baroni et al., 2010), and may also be considered an approximation of models that use shallow syntactic analysis (Grefenstette, 1994; Curran and Moens, 2002). All distinct token sequences up to length 4 either side of the target word were counted. Finally the Dependency model uses a full dependency parse, which might be considered the most informed representation of the wordcollocate relationships instantiated in corpus sentences, and this approach has been used by several authors (Lin, 1998; Padd and Lapata, 2007; Baroni and Lenci, 2010). The features used are pairs of dependency relation and lexeme corresponding to each edge linked to a target word of interest (e.g. likes subj). The parser used here was Malt, which achieves accuracies of 85% whe</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>Curran, J. R. and Moens, M. (2002). Improvements in automatic thesaurus extraction. In SIGLEX, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S Dumais</author>
<author>T Landauer</author>
<author>G Furnas</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>407</pages>
<contexts>
<context position="12485" citStr="Deerwester et al., 1990" startWordPosition="1929" endWordPosition="1932"> set of Elicited Properties inspired by the 20 Questions game, and gathered using Mechanical Turk (Palatucci et al., 2009; Palatucci, 2011). Multiple informants were asked to answer one or more of 218 questions “related to size, shape, surface properties, and typical usage” such as Do you see it daily?, Is it wild?, Is it man-made? with a scalar response ranging from 1 to 5. The resulting responses were then averaged over informants, and then the values of each question were grouped into 5 bins, giving all dimensions similar mean and variance. 2.2.2 Word-Region Model Latent Semantic Analysis (Deerwester et al., 1990; Landauer and Dumais, 1997), and its probabilistic cousins (Blei et al., 2003; Griffiths et al., 2007), express the meaning of a word as a distribution of co-occurrence across a set of documents, or other text-regions such as paragraphs. This word-region matrix instantiates the assumption that words that share a topical domain (such as medicine, entertainment, philosophy) would be expected to appear in similar sub-sets of text-regions. In such a model, the nearest neighbors of a target word are syntagmatically related (i.e. appear alongside each other), and for judge might include lawyer, cou</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Deerwester, S., Dumais, S., Landauer, T., Furnas, G., and Harshman, R. (1990). Indexing by Latent Semantic Analysis. Journal of the American Society of Information Science, 41(6):391 – 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Devereux</author>
<author>C Kelly</author>
</authors>
<title>Using fMRI activation to conceptual stimuli to evaluate methods for extracting conceptual representations from corpora.</title>
<date>2010</date>
<booktitle>1st Workshop on Computational Neurolinguistics.</booktitle>
<editor>In Murphy, B., Korhonen, A., and Chang, K. K.-M., editors,</editor>
<contexts>
<context position="2530" citStr="Devereux and Kelly, 2010" startWordPosition="359" endWordPosition="362"> with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept. Follow-on work has experimented with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly very successful, it is questionable whether they are a general solution to describe the various parts-of-speech and semantic domains that make</context>
</contexts>
<marker>Devereux, Kelly, 2010</marker>
<rawString>Devereux, B. and Kelly, C. (2010). Using fMRI activation to conceptual stimuli to evaluate methods for extracting conceptual representations from corpora. In Murphy, B., Korhonen, A., and Chang, K. K.-M., editors, 1st Workshop on Computational Neurolinguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Friston</author>
<author>J T Ashburner</author>
<author>S J Kiebel</author>
<author>T E Nichols</author>
<author>W D Penny</author>
</authors>
<title>Statistical Parametric Mapping: The Analysis of Functional Brain Images,</title>
<date>2007</date>
<volume>8</volume>
<publisher>Academic Press.</publisher>
<contexts>
<context position="6195" citStr="Friston et al., 2007" startWordPosition="919" endWordPosition="922">2008/data.html label, of everyday concrete concepts, with 5 exemplars of each of 12 semantic classes (mammals, body parts, buildings, building parts, clothes, furniture, insects, kitchen utensils, miscellaneous functional artifacts, work tools, vegetables, and vehicles). Stimuli remained on screen for three seconds, and each was each presented six times, in random order, to give a total of 360 image presentations in the session. The fMRI images were recorded with 3.0T scanner at 1 second intervals, with a spatial resolution of 3x3x6mm. The resulting data was preprocessed with the SPM package (Friston et al., 2007); the blood-oxygen-level response was approximated by taking a boxcar average over a sequence of brain images in each trial; percent signal change was calculated relative to rest periods, and the data from each of the six repetitions of each stimulus were averaged to yield a single brain image for each concept. Finally, a grey-matter anatomical mask was used to select only those voxels (three-dimensional pixels) that overlap with cortex, yielding approximately 20 thousand features per participant. 2.2 Models of semantics Our objective is to compare current semantic representations that get sta</context>
</contexts>
<marker>Friston, Ashburner, Kiebel, Nichols, Penny, 2007</marker>
<rawString>Friston, K. J., Ashburner, J. T., Kiebel, S. J., Nichols, T. E., and Penny, W. D. (2007). Statistical Parametric Mapping: The Analysis of Functional Brain Images, volume 8. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="17432" citStr="Grefenstette, 1994" startWordPosition="2715" endWordPosition="2716">between cooccurrence to the left or to the right of the target word, with features such as {john L, cake R}. The Part-of-Speech model (Kanejiya et al., 2003; Widdows, 2003) replaces each lowercase word-token with its part-of-speech disambiguated form (e.g. likes VBZ, cake NN). These annotations were extracted from the full dependency parse described below. The Sequence model draws on a range of work that uses word sequence patterns (Lin and Pantel, 2001; Almuhareb and Poesio, 2004; Baroni et al., 2010), and may also be considered an approximation of models that use shallow syntactic analysis (Grefenstette, 1994; Curran and Moens, 2002). All distinct token sequences up to length 4 either side of the target word were counted. Finally the Dependency model uses a full dependency parse, which might be considered the most informed representation of the wordcollocate relationships instantiated in corpus sentences, and this approach has been used by several authors (Lin, 1998; Padd and Lapata, 2007; Baroni and Lenci, 2010). The features used are pairs of dependency relation and lexeme corresponding to each edge linked to a target word of interest (e.g. likes subj). The parser used here was Malt, which achie</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, G. (1994). Explorations in Automatic Thesaurus Discovery. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>J B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="12588" citStr="Griffiths et al., 2007" startWordPosition="1945" endWordPosition="1949">tucci et al., 2009; Palatucci, 2011). Multiple informants were asked to answer one or more of 218 questions “related to size, shape, surface properties, and typical usage” such as Do you see it daily?, Is it wild?, Is it man-made? with a scalar response ranging from 1 to 5. The resulting responses were then averaged over informants, and then the values of each question were grouped into 5 bins, giving all dimensions similar mean and variance. 2.2.2 Word-Region Model Latent Semantic Analysis (Deerwester et al., 1990; Landauer and Dumais, 1997), and its probabilistic cousins (Blei et al., 2003; Griffiths et al., 2007), express the meaning of a word as a distribution of co-occurrence across a set of documents, or other text-regions such as paragraphs. This word-region matrix instantiates the assumption that words that share a topical domain (such as medicine, entertainment, philosophy) would be expected to appear in similar sub-sets of text-regions. In such a model, the nearest neighbors of a target word are syntagmatically related (i.e. appear alongside each other), and for judge might include lawyer, court, crime, or prison. The Document model used here is loosely based on LSA, taking the frequency of occ</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Griffiths, T. L., Steyvers, M., and Tenenbaum, J. B. (2007). Topics in semantic representation. Psychological Review, 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hall</author>
<author>J Nilsson</author>
<author>J Nivre</author>
<author>G Eryigit</author>
<author>B Megyesi</author>
<author>M Nilsson</author>
<author>M Saers</author>
</authors>
<title>Single Malt or Blended? A Study in Multilingual Parser Optimization. CoNLL Shared Task Session,</title>
<date>2007</date>
<pages>933--939</pages>
<contexts>
<context position="18125" citStr="Hall et al., 2007" startWordPosition="2827" endWordPosition="2830">er side of the target word were counted. Finally the Dependency model uses a full dependency parse, which might be considered the most informed representation of the wordcollocate relationships instantiated in corpus sentences, and this approach has been used by several authors (Lin, 1998; Padd and Lapata, 2007; Baroni and Lenci, 2010). The features used are pairs of dependency relation and lexeme corresponding to each edge linked to a target word of interest (e.g. likes subj). The parser used here was Malt, which achieves accuracies of 85% when deriving labelled dependencies on English text (Hall et al., 2007). The features produced by this module are much more limited, to those words that have a direct dependency relation with the word of interest. 2.3 Linear Learning Model A linear regression model will allow us to evaluate how well a given model of word semantics can be used to predict brain activity. We follow the analysis in Mitchell et al. (2008) and subsequently adopted by several other research groups (see Murphy et al., 2010). For each participant and selected fMRI feature (i.e. each voxel, which records the time-course of neural activity at a fixed location in the brain), we train a model</context>
</contexts>
<marker>Hall, Nilsson, Nivre, Eryigit, Megyesi, Nilsson, Saers, 2007</marker>
<rawString>Hall, J., Nilsson, J., Nivre, J., Eryigit, G., Megyesi, B., Nilsson, M., and Saers, M. (2007). Single Malt or Blended? A Study in Multilingual Parser Optimization. CoNLL Shared Task Session, pages 933–939.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>R Tibshirani</author>
<author>J Friedman</author>
</authors>
<title>The Elements of Statistical Learning,</title>
<date>2011</date>
<volume>18</volume>
<note>Springer Series in Statistics. Springer, 5th edition.</note>
<contexts>
<context position="20202" citStr="Hastie et al., 2011" startWordPosition="3181" endWordPosition="3184">l, we can derive whole brain images that are typical of each of the semantic dimensions. The power of the model is its ability to predict activity for concepts that were not in the training set – for instance the brain activation elicited by the word car might be approximated by combining the images see for -animate, +big, +moving, even though this combination of properties was not observed during training. The linear model was estimated with a least squared errors method and L2 regularisation, selecting the lambda parameter from the range 0.0001 to 5000 using Generalized CrossValidation (see Hastie et al., 2011, p.244). The 118 activation of each fMRI voxel in response to a new concept that was not in the training data was predicted by a Q-weighted sum of the values on each semantic dimension, building a picture of expected the global neural activity response for an arbitrary concept. Again following Mitchell et al. (2008) we use a leave-2-out paradigm in which a linear model for each neural feature is trained in turn on all concepts minus 2, having selected the 500 most stable voxels in the training set using the same correlational measure across stimulus presentations. For each of the 2 left-out c</context>
<context position="29933" citStr="Hastie et al., 2011" startWordPosition="4771" endWordPosition="4774"> encode all negative features (such as the facts that dogs do not have wheels, are not communication events, and do not belong in the aviation domain). But while SVD is certainly effective in dealing with the pervasive synonymy and polysemy seen in corpus-feature sets, it is less clear that it reveals psychologically plausible dimensions of meaning. Alternatives such as non-negative matrix factorization (Lee and Seung, 1999) or Latent Dirichlet Allocation (Blei et al., 2003) might extract more readily interpretable dimensions; or alternative regularisation methods such as Elastic Nets, Lasso (Hastie et al., 2011), or Network Regularisation (Sandler et al., 2009) might even be capable of identifying meaningful clusters of features when learning directly on co-occurrence data. Finally, we should consider whether more derived datasets could be used as input data in place of the basic corpus features used here, such as the full facts learned by the NELL system (Carlson et al., 2010), or crowd-sourced data which can be easily gathered for any word (e.g. association norms, Kiss et al., 1973), though different algorithmic means would be needed to deal with their extreme degree of sparsity. The results also s</context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2011</marker>
<rawString>Hastie, T., Tibshirani, R., and Friedman, J. (2011). The Elements of Statistical Learning, volume 18 of Springer Series in Statistics. Springer, 5th edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Jelodar</author>
<author>M Alizadeh</author>
<author>S Khadivi</author>
</authors>
<title>WordNet Based Features for Predicting Brain Activity associated with meanings of nouns.</title>
<date>2010</date>
<booktitle>1st Workshop on Computational Neurolinguistics,</booktitle>
<pages>18--26</pages>
<editor>In Murphy, B., Korhonen, A., and Chang, K. K.-M., editors,</editor>
<contexts>
<context position="2602" citStr="Jelodar et al., 2010" startWordPosition="370" endWordPosition="373">hat correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept. Follow-on work has experimented with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly very successful, it is questionable whether they are a general solution to describe the various parts-of-speech and semantic domains that make up a speaker’s vocabulary. The Mitchell et al. (2008) 25-verb model wou</context>
</contexts>
<marker>Jelodar, Alizadeh, Khadivi, 2010</marker>
<rawString>Jelodar, A. B., Alizadeh, M., and Khadivi, S. (2010). WordNet Based Features for Predicting Brain Activity associated with meanings of nouns. In Murphy, B., Korhonen, A., and Chang, K. K.-M., editors, 1st Workshop on Computational Neurolinguistics, pages 18–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Jones</author>
<author>T Oliphant</author>
<author>P Peterson</author>
<author>Et Al</author>
</authors>
<title>SciPy: Open source scientific tools for Python.</title>
<date>2001</date>
<contexts>
<context position="9370" citStr="Jones et al., 2001" startWordPosition="1417" endWordPosition="1420">m about 500 thousand, to tens of millions. A singular value decomposition (SVD) was applied to identify the 1000 dimensions within each model with the greatest explanatory power, which also has the effect of combining similar dimensions (such as synonyms, inflectional variants, topically similar documents) into common components, and discarding more noisy dimensions in the data. Again there is variation in the number of dimension that authors use: here we experiment with 300 and 1000. For decomposition we used a sparse SVD method, the Implicitly Restarted Arnoldi Method (Lehoucq et al., 1998; Jones et al., 2001), which was coherent with the PPMI normalization used, since a zero value represented both negative target-feature associations, and those that were not observed or fell below the frequency cut-off. We also streamlined the task by reducing the input data C (of n target words by m co-occurrence features) to a square matrix CCT of size n x n, taking advantage of the equality of their left singular vectors U. For SVD to generalize well over the many input features, it is also important to have more training cases that the small set of 60 concrete nouns used in our evaluation task. Consequently we</context>
</contexts>
<marker>Jones, Oliphant, Peterson, Al, 2001</marker>
<rawString>Jones, E., Oliphant, T., Peterson, P., and Et Al. (2001). SciPy: Open source scientific tools for Python.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kanejiya</author>
<author>A Kumar</author>
<author>S Prasad</author>
</authors>
<title>Automatic evaluation of students’ answers using syntactically enhanced LSA. Building educational applications,</title>
<date>2003</date>
<pages>2--53</pages>
<contexts>
<context position="16970" citStr="Kanejiya et al., 2003" startWordPosition="2639" endWordPosition="2642">d regression should be able to recognize when such high-frequency words are not informative and then discount these, without the need for such assumptions upfront. The Stemmed model is a slight variation on the Word-Form model, where the same statistics are aggregated after applying Lancaster stemming (Paice, 1990; Loper and Bird, 2002). The Directional model, inspired by Schutze and Pedersen (1993), is also derived from the word-form model, but differentiates between cooccurrence to the left or to the right of the target word, with features such as {john L, cake R}. The Part-of-Speech model (Kanejiya et al., 2003; Widdows, 2003) replaces each lowercase word-token with its part-of-speech disambiguated form (e.g. likes VBZ, cake NN). These annotations were extracted from the full dependency parse described below. The Sequence model draws on a range of work that uses word sequence patterns (Lin and Pantel, 2001; Almuhareb and Poesio, 2004; Baroni et al., 2010), and may also be considered an approximation of models that use shallow syntactic analysis (Grefenstette, 1994; Curran and Moens, 2002). All distinct token sequences up to length 4 either side of the target word were counted. Finally the Dependency</context>
</contexts>
<marker>Kanejiya, Kumar, Prasad, 2003</marker>
<rawString>Kanejiya, D., Kumar, A., and Prasad, S. (2003). Automatic evaluation of students’ answers using syntactically enhanced LSA. Building educational applications, NAACL, 2:53–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Kiss</author>
<author>C Armstrong</author>
<author>R Milroy</author>
<author>J Piper</author>
</authors>
<title>An associative thesaurus of English and its computer analysis.</title>
<date>1973</date>
<booktitle>The Computer and Literary Studies.</booktitle>
<editor>In Aitken, A. J., Bailey, R. W., and Hamilton-Smith, N., editors,</editor>
<publisher>Edinburgh University Press.</publisher>
<contexts>
<context position="30415" citStr="Kiss et al., 1973" startWordPosition="4850" endWordPosition="4853">t extract more readily interpretable dimensions; or alternative regularisation methods such as Elastic Nets, Lasso (Hastie et al., 2011), or Network Regularisation (Sandler et al., 2009) might even be capable of identifying meaningful clusters of features when learning directly on co-occurrence data. Finally, we should consider whether more derived datasets could be used as input data in place of the basic corpus features used here, such as the full facts learned by the NELL system (Carlson et al., 2010), or crowd-sourced data which can be easily gathered for any word (e.g. association norms, Kiss et al., 1973), though different algorithmic means would be needed to deal with their extreme degree of sparsity. The results also suggest a series of follow-on analyses. A priority should be to test these models against a wider range of neuroimaging data modalities (e.g. MEG, EEG) and stimulus sets, including abstract kinds (see Murphy et al. 2012, for a preliminary study), and parts-of-speech beyond nouns. It may be that a putative complementarity between word-region and word-collocate models is only revealed when we look at a broader sample of the human lexicon. And beyond establishing what informational</context>
</contexts>
<marker>Kiss, Armstrong, Milroy, Piper, 1973</marker>
<rawString>Kiss, G. R., Armstrong, C., Milroy, R., and Piper, J. (1973). An associative thesaurus of English and its computer analysis. In Aitken, A. J., Bailey, R. W., and Hamilton-Smith, N., editors, The Computer and Literary Studies. Edinburgh University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S Dumais</author>
</authors>
<title>A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="1462" citStr="Landauer and Dumais, 1997" startWordPosition="190" endWordPosition="193">al co-occurrence. We find that dependency parse-based features are the most effective, achieving accuracies similar to the leading semi-manual approaches and higher than any published for a corpus-based model. We also find that simple word features enriched with directional information provide a close-tooptimal solution at much lower computational cost. 1 Introduction The cognitive plausibility of computational models of word meaning has typically been tested using behavioural benchmarks, such as identification of synonyms among close associates (the TOEFL task for language learners, see e.g. Landauer and Dumais, 1997); emulating elicited judgments of pairwise similarity (such as Rubenstein and Goodenough, 1965); judgments of category membership (e.g. Battig and Montague, 1969); and word priming effects (Lund and Burgess, 1996). Mitchell et al. (2008) introduced a new task in neurosemantic decoding – using models of semantics to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments. This was achieved with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis i</context>
<context position="12513" citStr="Landauer and Dumais, 1997" startWordPosition="1933" endWordPosition="1936">es inspired by the 20 Questions game, and gathered using Mechanical Turk (Palatucci et al., 2009; Palatucci, 2011). Multiple informants were asked to answer one or more of 218 questions “related to size, shape, surface properties, and typical usage” such as Do you see it daily?, Is it wild?, Is it man-made? with a scalar response ranging from 1 to 5. The resulting responses were then averaged over informants, and then the values of each question were grouped into 5 bins, giving all dimensions similar mean and variance. 2.2.2 Word-Region Model Latent Semantic Analysis (Deerwester et al., 1990; Landauer and Dumais, 1997), and its probabilistic cousins (Blei et al., 2003; Griffiths et al., 2007), express the meaning of a word as a distribution of co-occurrence across a set of documents, or other text-regions such as paragraphs. This word-region matrix instantiates the assumption that words that share a topical domain (such as medicine, entertainment, philosophy) would be expected to appear in similar sub-sets of text-regions. In such a model, the nearest neighbors of a target word are syntagmatically related (i.e. appear alongside each other), and for judge might include lawyer, court, crime, or prison. The Do</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, T. and Dumais, S. (1997). A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lee</author>
<author>H S Seung</author>
</authors>
<title>Learning the parts of objects by non-negative matrix factorization.</title>
<date>1999</date>
<journal>Nature,</journal>
<volume>401</volume>
<issue>6755</issue>
<contexts>
<context position="29741" citStr="Lee and Seung, 1999" startWordPosition="4741" endWordPosition="4745">ords in the ANC). PPMI appears to be both effective, and parsimonious with assumptions one might make about conceptual representations, where it would be cognitively onerous and unnecessary to encode all negative features (such as the facts that dogs do not have wheels, are not communication events, and do not belong in the aviation domain). But while SVD is certainly effective in dealing with the pervasive synonymy and polysemy seen in corpus-feature sets, it is less clear that it reveals psychologically plausible dimensions of meaning. Alternatives such as non-negative matrix factorization (Lee and Seung, 1999) or Latent Dirichlet Allocation (Blei et al., 2003) might extract more readily interpretable dimensions; or alternative regularisation methods such as Elastic Nets, Lasso (Hastie et al., 2011), or Network Regularisation (Sandler et al., 2009) might even be capable of identifying meaningful clusters of features when learning directly on co-occurrence data. Finally, we should consider whether more derived datasets could be used as input data in place of the basic corpus features used here, such as the full facts learned by the NELL system (Carlson et al., 2010), or crowd-sourced data which can b</context>
</contexts>
<marker>Lee, Seung, 1999</marker>
<rawString>Lee, D. D. and Seung, H. S. (1999). Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R B Lehoucq</author>
<author>D C Sorensen</author>
<author>C Yang</author>
</authors>
<title>Arpack users’ guide: Solution of large scale eigenvalue problems with implicitly restarted Arnoldi methods.</title>
<date>1998</date>
<publisher>SIAM.</publisher>
<contexts>
<context position="9349" citStr="Lehoucq et al., 1998" startWordPosition="1413" endWordPosition="1416">ity ranged widely, from about 500 thousand, to tens of millions. A singular value decomposition (SVD) was applied to identify the 1000 dimensions within each model with the greatest explanatory power, which also has the effect of combining similar dimensions (such as synonyms, inflectional variants, topically similar documents) into common components, and discarding more noisy dimensions in the data. Again there is variation in the number of dimension that authors use: here we experiment with 300 and 1000. For decomposition we used a sparse SVD method, the Implicitly Restarted Arnoldi Method (Lehoucq et al., 1998; Jones et al., 2001), which was coherent with the PPMI normalization used, since a zero value represented both negative target-feature associations, and those that were not observed or fell below the frequency cut-off. We also streamlined the task by reducing the input data C (of n target words by m co-occurrence features) to a square matrix CCT of size n x n, taking advantage of the equality of their left singular vectors U. For SVD to generalize well over the many input features, it is also important to have more training cases that the small set of 60 concrete nouns used in our evaluation </context>
</contexts>
<marker>Lehoucq, Sorensen, Yang, 1998</marker>
<rawString>Lehoucq, R. B., Sorensen, D. C., and Yang, C. (1998). Arpack users’ guide: Solution of large scale eigenvalue problems with implicitly restarted Arnoldi methods. SIAM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="10709" citStr="Lin, 1998" startWordPosition="1636" endWordPosition="1637">ith Suderman, 2006), which should approximate the scale and composition of the vocabulary of a university-educated speaker of English (Nation and Waring, 1997), and over 95% of tokens typically encountered in English. A frequency threshold is commonly applied for three reasons: low-frequency co-occurrence counts are more noisy; PMI is positively biased towards hapax co-occurrences; and due to Zipfian distributions acut-off dramatically reduces the amount of data to be processed. Many authors use a threshold of approximately 50-100 occurrences for word-collocate models (Lund and Burgess, 1996; Lin, 1998; Rapp, 2003). Since Bullinaria and Levy (2007) find improving performance with models using progressively lower cutoffs we explored two cut-offs of 20 and 50 which equate to low co-occurrences thresholds of 0.00125 or 0.003125 per million respectively; for the word-region model we chose a threshold of 2 occurrences of a target term in a document, to keep the input features to a reasonable dimensionality (Bradford, 2008). After applying these operations to the input data from each model, the resulting 2.2.1 Hand-tailored benchmarks The state-of-the-art models on this brain activity prediction </context>
<context position="17796" citStr="Lin, 1998" startWordPosition="2772" endWordPosition="2773">odel draws on a range of work that uses word sequence patterns (Lin and Pantel, 2001; Almuhareb and Poesio, 2004; Baroni et al., 2010), and may also be considered an approximation of models that use shallow syntactic analysis (Grefenstette, 1994; Curran and Moens, 2002). All distinct token sequences up to length 4 either side of the target word were counted. Finally the Dependency model uses a full dependency parse, which might be considered the most informed representation of the wordcollocate relationships instantiated in corpus sentences, and this approach has been used by several authors (Lin, 1998; Padd and Lapata, 2007; Baroni and Lenci, 2010). The features used are pairs of dependency relation and lexeme corresponding to each edge linked to a target word of interest (e.g. likes subj). The parser used here was Malt, which achieves accuracies of 85% when deriving labelled dependencies on English text (Hall et al., 2007). The features produced by this module are much more limited, to those words that have a direct dependency relation with the word of interest. 2.3 Linear Learning Model A linear regression model will allow us to evaluate how well a given model of word semantics can be us</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. (1998). Automatic Retrieval and Clustering of Similar Words. In COLING-ACL, pages 768– 774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>DIRT – discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining KDD 01,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="17271" citStr="Lin and Pantel, 2001" startWordPosition="2687" endWordPosition="2690"> (Paice, 1990; Loper and Bird, 2002). The Directional model, inspired by Schutze and Pedersen (1993), is also derived from the word-form model, but differentiates between cooccurrence to the left or to the right of the target word, with features such as {john L, cake R}. The Part-of-Speech model (Kanejiya et al., 2003; Widdows, 2003) replaces each lowercase word-token with its part-of-speech disambiguated form (e.g. likes VBZ, cake NN). These annotations were extracted from the full dependency parse described below. The Sequence model draws on a range of work that uses word sequence patterns (Lin and Pantel, 2001; Almuhareb and Poesio, 2004; Baroni et al., 2010), and may also be considered an approximation of models that use shallow syntactic analysis (Grefenstette, 1994; Curran and Moens, 2002). All distinct token sequences up to length 4 either side of the target word were counted. Finally the Dependency model uses a full dependency parse, which might be considered the most informed representation of the wordcollocate relationships instantiated in corpus sentences, and this approach has been used by several authors (Lin, 1998; Padd and Lapata, 2007; Baroni and Lenci, 2010). The features used are pai</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Lin, D. and Pantel, P. (2001). DIRT – discovery of inference rules from text. Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining KDD 01, datamining:323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Loper</author>
<author>S Bird</author>
</authors>
<title>{NLTK}: The natural language toolkit.</title>
<date>2002</date>
<booktitle>In ACL Workshop,</booktitle>
<volume>1</volume>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16687" citStr="Loper and Bird, 2002" startWordPosition="2592" endWordPosition="2595">17 Lund et al., 1995). We did not use a stop-list, as Bullinaria and Levy (2007) found co-occurrence with very high frequency words also to be informative for semantic tasks. We also expect that the subsequent steps of normalizing with PPMI, reduction with SVD, and use of regularised regression should be able to recognize when such high-frequency words are not informative and then discount these, without the need for such assumptions upfront. The Stemmed model is a slight variation on the Word-Form model, where the same statistics are aggregated after applying Lancaster stemming (Paice, 1990; Loper and Bird, 2002). The Directional model, inspired by Schutze and Pedersen (1993), is also derived from the word-form model, but differentiates between cooccurrence to the left or to the right of the target word, with features such as {john L, cake R}. The Part-of-Speech model (Kanejiya et al., 2003; Widdows, 2003) replaces each lowercase word-token with its part-of-speech disambiguated form (e.g. likes VBZ, cake NN). These annotations were extracted from the full dependency parse described below. The Sequence model draws on a range of work that uses word sequence patterns (Lin and Pantel, 2001; Almuhareb and </context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Loper, E. and Bird, S. (2002). {NLTK}: The natural language toolkit. In ACL Workshop, volume 1, pages 63–70. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, and Computers,</journal>
<pages>28--203</pages>
<contexts>
<context position="1675" citStr="Lund and Burgess, 1996" startWordPosition="220" endWordPosition="223">o find that simple word features enriched with directional information provide a close-tooptimal solution at much lower computational cost. 1 Introduction The cognitive plausibility of computational models of word meaning has typically been tested using behavioural benchmarks, such as identification of synonyms among close associates (the TOEFL task for language learners, see e.g. Landauer and Dumais, 1997); emulating elicited judgments of pairwise similarity (such as Rubenstein and Goodenough, 1965); judgments of category membership (e.g. Battig and Montague, 1969); and word priming effects (Lund and Burgess, 1996). Mitchell et al. (2008) introduced a new task in neurosemantic decoding – using models of semantics to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments. This was achieved with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an</context>
<context position="7841" citStr="Lund and Burgess, 1996" startWordPosition="1179" endWordPosition="1182">ole sentences, and document co-occurrence, among others. Other parameters were kept fixed in a way that the literature suggests would be neutral to the various models, and so allow a fair comparison among them (Sahlgren, 2006; Bullinaria and Levy, 2007; Turney and Pantel, 2010). All textual statistics were gathered from a set of 50m English-language web-page documents consisting of 16 billion words. Where a fixed text window was used, we chose an extent of ±4 lower-case tokens either side of the target 115 word of interest, which is in the mid-range of optimal values found by various authors (Lund and Burgess, 1996; Rapp, 2003; Sahlgren, 2006). Positive pointwise-mutual-information (1,2) was used as an association measure to normalize the observed co-occurrence frequency p(w, f) for the varying frequency of the target word p(w) and its features p(f). PPMI up-weights cooccurrences between rare words, yielding positive values for collocations that are more common than would be expected by chance (i.e. if word distributions were independent), and discards negative values that represent patterns of co-occurrences that are rarer than one would expect by chance. It has been shown to perform well generally, wi</context>
<context position="10698" citStr="Lund and Burgess, 1996" startWordPosition="1632" endWordPosition="1635">Corpus (Nancy Ide and Keith Suderman, 2006), which should approximate the scale and composition of the vocabulary of a university-educated speaker of English (Nation and Waring, 1997), and over 95% of tokens typically encountered in English. A frequency threshold is commonly applied for three reasons: low-frequency co-occurrence counts are more noisy; PMI is positively biased towards hapax co-occurrences; and due to Zipfian distributions acut-off dramatically reduces the amount of data to be processed. Many authors use a threshold of approximately 50-100 occurrences for word-collocate models (Lund and Burgess, 1996; Lin, 1998; Rapp, 2003). Since Bullinaria and Levy (2007) find improving performance with models using progressively lower cutoffs we explored two cut-offs of 20 and 50 which equate to low co-occurrences thresholds of 0.00125 or 0.003125 per million respectively; for the word-region model we chose a threshold of 2 occurrences of a target term in a document, to keep the input features to a reasonable dimensionality (Bradford, 2008). After applying these operations to the input data from each model, the resulting 2.2.1 Hand-tailored benchmarks The state-of-the-art models on this brain activity </context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Lund, K. and Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers, 28:203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
<author>R Atchley</author>
</authors>
<title>Semantic and associative priming in high dimensional semantic space.</title>
<date>1995</date>
<booktitle>In Proceedings of the 17th Cognitive Science Society Meeting,</booktitle>
<pages>660--665</pages>
<contexts>
<context position="16087" citStr="Lund et al., 1995" startWordPosition="2495" endWordPosition="2498">ypically are applied to smaller corpora) and usually introduce some noise through their errors. Consequently, simpler window-based models have often been found to be as effective. The most basic model considered is the Word-Form model, in which all lower-case tokens (word forms and punctuation) found within four positions left and right of the target word are recorded, yielding simple features such as {john, likes}. It may also be termed a `flat&apos; model in contrast to those which assign a variable weight to collocates, progressively lower as one moves further than the target position (e.g. 117 Lund et al., 1995). We did not use a stop-list, as Bullinaria and Levy (2007) found co-occurrence with very high frequency words also to be informative for semantic tasks. We also expect that the subsequent steps of normalizing with PPMI, reduction with SVD, and use of regularised regression should be able to recognize when such high-frequency words are not informative and then discount these, without the need for such assumptions upfront. The Stemmed model is a slight variation on the Word-Form model, where the same statistics are aggregated after applying Lancaster stemming (Paice, 1990; Loper and Bird, 2002)</context>
</contexts>
<marker>Lund, Burgess, Atchley, 1995</marker>
<rawString>Lund, K., Burgess, C., and Atchley, R. (1995). Semantic and associative priming in high dimensional semantic space. In Proceedings of the 17th Cognitive Science Society Meeting, pages 660–665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Mitchell</author>
<author>S V Shinkareva</author>
<author>A Carlson</author>
<author>K-M Chang</author>
<author>V L Malave</author>
<author>R A Mason</author>
<author>M A Just</author>
</authors>
<title>Predicting Human Brain Activity Associated with the Meanings of Nouns.</title>
<date>2008</date>
<journal>Science,</journal>
<pages>320--1191</pages>
<contexts>
<context position="1699" citStr="Mitchell et al. (2008)" startWordPosition="224" endWordPosition="227">eatures enriched with directional information provide a close-tooptimal solution at much lower computational cost. 1 Introduction The cognitive plausibility of computational models of word meaning has typically been tested using behavioural benchmarks, such as identification of synonyms among close associates (the TOEFL task for language learners, see e.g. Landauer and Dumais, 1997); emulating elicited judgments of pairwise similarity (such as Rubenstein and Goodenough, 1965); judgments of category membership (e.g. Battig and Montague, 1969); and word priming effects (Lund and Burgess, 1996). Mitchell et al. (2008) introduced a new task in neurosemantic decoding – using models of semantics to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments. This was achieved with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept</context>
<context position="2951" citStr="Mitchell et al., 2008" startWordPosition="423" endWordPosition="426">ted with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly very successful, it is questionable whether they are a general solution to describe the various parts-of-speech and semantic domains that make up a speaker’s vocabulary. The Mitchell et al. (2008) 25-verb model would probably have to be extended to describe the lexicon at large, and it is unclear whether such a compact model could be maintained. While Wikipedia (Pereira et al., 2011) has very broad and increasing cov114 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 114–123, Montr´eal, Canada, June 7-8, 2012. c�2012 Association </context>
<context position="4910" citStr="Mitchell et al., 2008" startWordPosition="720" endWordPosition="723">le word-region co-occurrences, and various HAL-style word-collocate features including raw tokens, POS tags, and a full dependency parse. Otherwise a common feature extraction and preprocessing pipeline is used: a co-occurrence frequency cutoff, application of a frequency normalization weighting, and dimensionality reduction with SVD. The following section describes how the brain activity data was gathered and processed; the construction of several corpus-derived models of meaning; and the regression-based methods used to predict one from the other, evaluated with a brain-image matching task (Mitchell et al., 2008). In section 3 we report the results, and in the Conclusion we discuss both the practical implications, and what this works suggests for the cognitive plausibility of distributional models of meaning. 2 Methods 2.1 Brain activity features The dataset used here is that described in detail in (Mitchell et al., 2008) and released publicly&apos; in conjunction with the NAACL 2010 Workshop on Computational Neurolinguistics (Murphy et al., 2010). Functional MRI (fMRI) data was collected from 9 participants while they performed a property generation task. The stimuli were line-drawings, accompanied by the</context>
<context position="11360" citStr="Mitchell et al. (2008)" startWordPosition="1736" endWordPosition="1739">ia and Levy (2007) find improving performance with models using progressively lower cutoffs we explored two cut-offs of 20 and 50 which equate to low co-occurrences thresholds of 0.00125 or 0.003125 per million respectively; for the word-region model we chose a threshold of 2 occurrences of a target term in a document, to keep the input features to a reasonable dimensionality (Bradford, 2008). After applying these operations to the input data from each model, the resulting 2.2.1 Hand-tailored benchmarks The state-of-the-art models on this brain activity prediction task are both hand-tailored. Mitchell et al. (2008) used a model of semantics based on co-occurrence in the Google 1T 5- gram corpus of English (Brants and Franz, 2006) with a small set of 25 Verbs chosen to represent everyday sensory-motor interaction with concrete objects, such as see, move, listen. We recreated this using our current parameters (web document corpus, co-occurrence frequency cutoff, PPMI normalization). The second handPMI&amp;quot;f = log �p(w, f) (2) p(w)p(f) �PMI&amp;quot;f if PMI&amp;quot;f &gt; 0 0 otherwise (1) PPMI&amp;quot;f = 116 tailored dataset we used was a set of Elicited Properties inspired by the 20 Questions game, and gathered using Mechanical Turk </context>
<context position="18474" citStr="Mitchell et al. (2008)" startWordPosition="2891" endWordPosition="2894">atures used are pairs of dependency relation and lexeme corresponding to each edge linked to a target word of interest (e.g. likes subj). The parser used here was Malt, which achieves accuracies of 85% when deriving labelled dependencies on English text (Hall et al., 2007). The features produced by this module are much more limited, to those words that have a direct dependency relation with the word of interest. 2.3 Linear Learning Model A linear regression model will allow us to evaluate how well a given model of word semantics can be used to predict brain activity. We follow the analysis in Mitchell et al. (2008) and subsequently adopted by several other research groups (see Murphy et al., 2010). For each participant and selected fMRI feature (i.e. each voxel, which records the time-course of neural activity at a fixed location in the brain), we train a model where the level of activation of the latter (the blood oxygenation level) in response to different concepts is approximated by a regularised linear combination of their semantic features: f = CQ + A||Q||2 (3) where f is the vector of activations of a specific fMRI feature for different concepts, the matrix C contains the values of the semantic fe</context>
<context position="20520" citStr="Mitchell et al. (2008)" startWordPosition="3239" endWordPosition="3242">te, +big, +moving, even though this combination of properties was not observed during training. The linear model was estimated with a least squared errors method and L2 regularisation, selecting the lambda parameter from the range 0.0001 to 5000 using Generalized CrossValidation (see Hastie et al., 2011, p.244). The 118 activation of each fMRI voxel in response to a new concept that was not in the training data was predicted by a Q-weighted sum of the values on each semantic dimension, building a picture of expected the global neural activity response for an arbitrary concept. Again following Mitchell et al. (2008) we use a leave-2-out paradigm in which a linear model for each neural feature is trained in turn on all concepts minus 2, having selected the 500 most stable voxels in the training set using the same correlational measure across stimulus presentations. For each of the 2 left-out concepts, we predict the global neural activation pattern, as just described. We then try to correctly match the predicted and observed activations, by measuring the cosine distance between the model-generated estimate of fMRI activity and the that observed in the experiment. If the sum of the matched cosine distances</context>
</contexts>
<marker>Mitchell, Shinkareva, Carlson, Chang, Malave, Mason, Just, 2008</marker>
<rawString>Mitchell, T. M., Shinkareva, S. V., Carlson, A., Chang, K.-M., Malave, V. L., Mason, R. A., and Just, M. A. (2008). Predicting Human Brain Activity Associated with the Meanings of Nouns. Science, 320:1191–1195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Murphy</author>
<author>M Baroni</author>
<author>M Poesio</author>
</authors>
<title>EEG responds to conceptual stimuli and corpus semantics.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>619--627</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2389" citStr="Murphy et al., 2009" startWordPosition="336" endWordPosition="339">antics to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments. This was achieved with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept. Follow-on work has experimented with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly v</context>
</contexts>
<marker>Murphy, Baroni, Poesio, 2009</marker>
<rawString>Murphy, B., Baroni, M., and Poesio, M. (2009). EEG responds to conceptual stimuli and corpus semantics. In Proceedings of EMNLP, pages 619–627. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Murphy</author>
<author>A Korhonen</author>
<author>K Chang</author>
</authors>
<date>2010</date>
<booktitle>Proceedings of the 1st Workshop on Computational Neurolinguistics, NAACLHLT,</booktitle>
<editor>K.-M., editors</editor>
<publisher>ACL.</publisher>
<location>Los Angeles.</location>
<contexts>
<context position="5348" citStr="Murphy et al., 2010" startWordPosition="792" endWordPosition="796">n of several corpus-derived models of meaning; and the regression-based methods used to predict one from the other, evaluated with a brain-image matching task (Mitchell et al., 2008). In section 3 we report the results, and in the Conclusion we discuss both the practical implications, and what this works suggests for the cognitive plausibility of distributional models of meaning. 2 Methods 2.1 Brain activity features The dataset used here is that described in detail in (Mitchell et al., 2008) and released publicly&apos; in conjunction with the NAACL 2010 Workshop on Computational Neurolinguistics (Murphy et al., 2010). Functional MRI (fMRI) data was collected from 9 participants while they performed a property generation task. The stimuli were line-drawings, accompanied by their text 1http://www.cs.cmu.edu/afs/cs/project/theo73/www/science2008/data.html label, of everyday concrete concepts, with 5 exemplars of each of 12 semantic classes (mammals, body parts, buildings, building parts, clothes, furniture, insects, kitchen utensils, miscellaneous functional artifacts, work tools, vegetables, and vehicles). Stimuli remained on screen for three seconds, and each was each presented six times, in random order, </context>
<context position="18558" citStr="Murphy et al., 2010" startWordPosition="2904" endWordPosition="2907">ked to a target word of interest (e.g. likes subj). The parser used here was Malt, which achieves accuracies of 85% when deriving labelled dependencies on English text (Hall et al., 2007). The features produced by this module are much more limited, to those words that have a direct dependency relation with the word of interest. 2.3 Linear Learning Model A linear regression model will allow us to evaluate how well a given model of word semantics can be used to predict brain activity. We follow the analysis in Mitchell et al. (2008) and subsequently adopted by several other research groups (see Murphy et al., 2010). For each participant and selected fMRI feature (i.e. each voxel, which records the time-course of neural activity at a fixed location in the brain), we train a model where the level of activation of the latter (the blood oxygenation level) in response to different concepts is approximated by a regularised linear combination of their semantic features: f = CQ + A||Q||2 (3) where f is the vector of activations of a specific fMRI feature for different concepts, the matrix C contains the values of the semantic features for the same concepts, Q is the vector of weights we must learn for each of t</context>
</contexts>
<marker>Murphy, Korhonen, Chang, 2010</marker>
<rawString>Murphy, B., Korhonen, A., and Chang, K. K.-M., editors (2010). Proceedings of the 1st Workshop on Computational Neurolinguistics, NAACLHLT, Los Angeles. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Murphy</author>
<author>P Talukdar</author>
<author>T Mitchell</author>
</authors>
<title>Comparing Abstract and Concrete Conceptual Representations using Neurosemantic Decoding.</title>
<date>2012</date>
<booktitle>In NAACL Workshop on Cognitive Modelling and Computational Linguistics.</booktitle>
<contexts>
<context position="30751" citStr="Murphy et al. 2012" startWordPosition="4905" endWordPosition="4909">r more derived datasets could be used as input data in place of the basic corpus features used here, such as the full facts learned by the NELL system (Carlson et al., 2010), or crowd-sourced data which can be easily gathered for any word (e.g. association norms, Kiss et al., 1973), though different algorithmic means would be needed to deal with their extreme degree of sparsity. The results also suggest a series of follow-on analyses. A priority should be to test these models against a wider range of neuroimaging data modalities (e.g. MEG, EEG) and stimulus sets, including abstract kinds (see Murphy et al. 2012, for a preliminary study), and parts-of-speech beyond nouns. It may be that a putative complementarity between word-region and word-collocate models is only revealed when we look at a broader sample of the human lexicon. And beyond establishing what informational content is required to make semantic distinctions, other factorisation methods (e.g. sparse or non-negative decompositions) could be applied to yield more interpretable dimensions. Other classification tasks might also be more sensitive for detecting differences between models, such as the test of word identification among a set by r</context>
</contexts>
<marker>Murphy, Talukdar, Mitchell, 2012</marker>
<rawString>Murphy, B., Talukdar, P., and Mitchell, T. (2012). Comparing Abstract and Concrete Conceptual Representations using Neurosemantic Decoding. In NAACL Workshop on Cognitive Modelling and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Keith Suderman</author>
</authors>
<title>The American National Corpus First Release.</title>
<date>2006</date>
<booktitle>Proceedings of the Fifth Language Resources and Evaluation Conference (LREC).</booktitle>
<marker>Ide, Suderman, 2006</marker>
<rawString>Nancy Ide and Keith Suderman (2006). The American National Corpus First Release. Proceedings of the Fifth Language Resources and Evaluation Conference (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nation</author>
<author>R Waring</author>
</authors>
<title>Vocabulary size, text coverage and word lists.</title>
<date>1997</date>
<pages>6--19</pages>
<editor>In Schmitt, N. and McCarthy, M., editors, Vocabulary</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10259" citStr="Nation and Waring, 1997" startWordPosition="1568" endWordPosition="1571">rget words by m co-occurrence features) to a square matrix CCT of size n x n, taking advantage of the equality of their left singular vectors U. For SVD to generalize well over the many input features, it is also important to have more training cases that the small set of 60 concrete nouns used in our evaluation task. Consequently we gathered all statistics over a set of the 40,000 most frequent word-forms found in the American National Corpus (Nancy Ide and Keith Suderman, 2006), which should approximate the scale and composition of the vocabulary of a university-educated speaker of English (Nation and Waring, 1997), and over 95% of tokens typically encountered in English. A frequency threshold is commonly applied for three reasons: low-frequency co-occurrence counts are more noisy; PMI is positively biased towards hapax co-occurrences; and due to Zipfian distributions acut-off dramatically reduces the amount of data to be processed. Many authors use a threshold of approximately 50-100 occurrences for word-collocate models (Lund and Burgess, 1996; Lin, 1998; Rapp, 2003). Since Bullinaria and Levy (2007) find improving performance with models using progressively lower cutoffs we explored two cut-offs of 2</context>
</contexts>
<marker>Nation, Waring, 1997</marker>
<rawString>Nation, P. and Waring, R. (1997). Vocabulary size, text coverage and word lists. In Schmitt, N. and McCarthy, M., editors, Vocabulary Description acquisition and pedagogy, pages 6–19. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pado, Lapata, 2007</marker>
<rawString>Pado, S. and Lapata, M. (2007). Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Paice</author>
</authors>
<title>Another stemmer.</title>
<date>1990</date>
<journal>SIGIR Forum,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="16664" citStr="Paice, 1990" startWordPosition="2590" endWordPosition="2591">ition (e.g. 117 Lund et al., 1995). We did not use a stop-list, as Bullinaria and Levy (2007) found co-occurrence with very high frequency words also to be informative for semantic tasks. We also expect that the subsequent steps of normalizing with PPMI, reduction with SVD, and use of regularised regression should be able to recognize when such high-frequency words are not informative and then discount these, without the need for such assumptions upfront. The Stemmed model is a slight variation on the Word-Form model, where the same statistics are aggregated after applying Lancaster stemming (Paice, 1990; Loper and Bird, 2002). The Directional model, inspired by Schutze and Pedersen (1993), is also derived from the word-form model, but differentiates between cooccurrence to the left or to the right of the target word, with features such as {john L, cake R}. The Part-of-Speech model (Kanejiya et al., 2003; Widdows, 2003) replaces each lowercase word-token with its part-of-speech disambiguated form (e.g. likes VBZ, cake NN). These annotations were extracted from the full dependency parse described below. The Sequence model draws on a range of work that uses word sequence patterns (Lin and Pante</context>
</contexts>
<marker>Paice, 1990</marker>
<rawString>Paice, C. D. (1990). Another stemmer. SIGIR Forum, 24(3):56–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palatucci</author>
<author>G Hinton</author>
<author>D Pomerleau</author>
<author>T M Mitchell</author>
</authors>
<date>2009</date>
<booktitle>Zero-Shot Learning with Semantic Output Codes. Advances in Neural Information Processing Systems,</booktitle>
<pages>22--1</pages>
<contexts>
<context position="2784" citStr="Palatucci et al., 2009" startWordPosition="397" endWordPosition="400">used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept. Follow-on work has experimented with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly very successful, it is questionable whether they are a general solution to describe the various parts-of-speech and semantic domains that make up a speaker’s vocabulary. The Mitchell et al. (2008) 25-verb model would probably have to be extended to describe the lexicon at large, and it is unclear whether such a compact model could be maintained. While Wikipedia (Pereira et al., 2011) has very </context>
<context position="11983" citStr="Palatucci et al., 2009" startWordPosition="1842" endWordPosition="1845">used a model of semantics based on co-occurrence in the Google 1T 5- gram corpus of English (Brants and Franz, 2006) with a small set of 25 Verbs chosen to represent everyday sensory-motor interaction with concrete objects, such as see, move, listen. We recreated this using our current parameters (web document corpus, co-occurrence frequency cutoff, PPMI normalization). The second handPMI&amp;quot;f = log �p(w, f) (2) p(w)p(f) �PMI&amp;quot;f if PMI&amp;quot;f &gt; 0 0 otherwise (1) PPMI&amp;quot;f = 116 tailored dataset we used was a set of Elicited Properties inspired by the 20 Questions game, and gathered using Mechanical Turk (Palatucci et al., 2009; Palatucci, 2011). Multiple informants were asked to answer one or more of 218 questions “related to size, shape, surface properties, and typical usage” such as Do you see it daily?, Is it wild?, Is it man-made? with a scalar response ranging from 1 to 5. The resulting responses were then averaged over informants, and then the values of each question were grouped into 5 bins, giving all dimensions similar mean and variance. 2.2.2 Word-Region Model Latent Semantic Analysis (Deerwester et al., 1990; Landauer and Dumais, 1997), and its probabilistic cousins (Blei et al., 2003; Griffiths et al., </context>
<context position="22297" citStr="Palatucci et al., 2009" startWordPosition="3532" endWordPosition="3535">s. All of these classification accuracies are highly significant at p « 0.001 over test trials (binomial, chance 50%, n=1770*9) and p &lt; 0.001 over words (binomial, chance 50%, n=60). There were some significant differences between models when making inferences over trials, but for the small set of words used here it is not possible to make firm conclusions about the superiority of one model over the other, that could be confidently expected to generalize to other stimuli or experiments. However, we do achieve classification accuracies that are as high, or higher than any previously published (Palatucci et al., 2009; Pereira et al., 2011), while models based on very Semantic Models Features Accuracy 25 Verbs 25 78.5 Elicited Properties 218 83.5 Document (f2) 1000 76.2 Word Form 1000 80.0 Stemmed 1000 76.2 Direction 1000 80.2 Part-of-Speech 1000 80.0 Sequence 1000 78.5 Dependency 1000 83.1 Table 1: Brain activity prediction accuracy on leave2-out pair-matching task. A frequency cutoff of 20 was used for all 1000 dimensional models. Semantic Models 300 Feats. 1000 Feats. Document (f2) 79.9 76.2 Word Form 78.1 80.0 Stemmed 77.9 76.2 Direction 80.0 80.2 Part-of-Speech 77.9 80.0 Sequence 72.9 78.5 Dependency </context>
</contexts>
<marker>Palatucci, Hinton, Pomerleau, Mitchell, 2009</marker>
<rawString>Palatucci, M., Hinton, G., Pomerleau, D., and Mitchell, T. M. (2009). Zero-Shot Learning with Semantic Output Codes. Advances in Neural Information Processing Systems, 22:1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Palatucci</author>
</authors>
<title>Thought Recognition: Predicting and Decoding Brain Activity Using the Zero-Shot Learning Model.</title>
<date>2011</date>
<tech>PhD thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="12001" citStr="Palatucci, 2011" startWordPosition="1846" endWordPosition="1847">s based on co-occurrence in the Google 1T 5- gram corpus of English (Brants and Franz, 2006) with a small set of 25 Verbs chosen to represent everyday sensory-motor interaction with concrete objects, such as see, move, listen. We recreated this using our current parameters (web document corpus, co-occurrence frequency cutoff, PPMI normalization). The second handPMI&amp;quot;f = log �p(w, f) (2) p(w)p(f) �PMI&amp;quot;f if PMI&amp;quot;f &gt; 0 0 otherwise (1) PPMI&amp;quot;f = 116 tailored dataset we used was a set of Elicited Properties inspired by the 20 Questions game, and gathered using Mechanical Turk (Palatucci et al., 2009; Palatucci, 2011). Multiple informants were asked to answer one or more of 218 questions “related to size, shape, surface properties, and typical usage” such as Do you see it daily?, Is it wild?, Is it man-made? with a scalar response ranging from 1 to 5. The resulting responses were then averaged over informants, and then the values of each question were grouped into 5 bins, giving all dimensions similar mean and variance. 2.2.2 Word-Region Model Latent Semantic Analysis (Deerwester et al., 1990; Landauer and Dumais, 1997), and its probabilistic cousins (Blei et al., 2003; Griffiths et al., 2007), express the</context>
</contexts>
<marker>Palatucci, 2011</marker>
<rawString>Palatucci, M. M. (2011). Thought Recognition: Predicting and Decoding Brain Activity Using the Zero-Shot Learning Model. PhD thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>G Detre</author>
<author>M Botvinick</author>
</authors>
<title>Generating Text from Functional Brain Images. Frontiers in Human Neuroscience,</title>
<date>2011</date>
<pages>5--1</pages>
<contexts>
<context position="2553" citStr="Pereira et al., 2011" startWordPosition="363" endWordPosition="366"> used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept. Follow-on work has experimented with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly very successful, it is questionable whether they are a general solution to describe the various parts-of-speech and semantic domains that make up a speaker’s vocabul</context>
<context position="13916" citStr="Pereira et al., 2011" startWordPosition="2161" endWordPosition="2164">llows Bullinaria and Levy (2007); Turney and Pantel (2010) in using PPMI as a normalization function. We have not investigated variations on the decomposition algorithm in any detail, such as those using non-negative matrix factorization, probabilistic LSA or LDA topic models, as the objective in this paper is to provide a direct comparison between the different types of basic collocation information encoded in corpora, rather than evaluate the best algorithmic means for discovering latent dimensions in those co-occurrences. Nor have we evaluated performance on a more structured corpus input (Pereira et al., 2011). However preliminary tests with the Wikipedia corpus, and with LDA, using the Gensim package (Rehurek and Sojka, 2010) yielded similar performances. 2.2.3 Word-Collocate Models Word-collocate models make a complementary assumption to that of the document model: that words with closely-related categorical or taxonomic properties should appear in the same position of similar sentences. In a basic wordcollocate model, based on a word-word cooccurrence matrix, the nearest neighbors of judge might be athlete, singer, or ire-ighter, reflecting paradigmatic relatedness (i.e. substitutability). Word-</context>
<context position="22320" citStr="Pereira et al., 2011" startWordPosition="3536" endWordPosition="3539">cation accuracies are highly significant at p « 0.001 over test trials (binomial, chance 50%, n=1770*9) and p &lt; 0.001 over words (binomial, chance 50%, n=60). There were some significant differences between models when making inferences over trials, but for the small set of words used here it is not possible to make firm conclusions about the superiority of one model over the other, that could be confidently expected to generalize to other stimuli or experiments. However, we do achieve classification accuracies that are as high, or higher than any previously published (Palatucci et al., 2009; Pereira et al., 2011), while models based on very Semantic Models Features Accuracy 25 Verbs 25 78.5 Elicited Properties 218 83.5 Document (f2) 1000 76.2 Word Form 1000 80.0 Stemmed 1000 76.2 Direction 1000 80.2 Part-of-Speech 1000 80.0 Sequence 1000 78.5 Dependency 1000 83.1 Table 1: Brain activity prediction accuracy on leave2-out pair-matching task. A frequency cutoff of 20 was used for all 1000 dimensional models. Semantic Models 300 Feats. 1000 Feats. Document (f2) 79.9 76.2 Word Form 78.1 80.0 Stemmed 77.9 76.2 Direction 80.0 80.2 Part-of-Speech 77.9 80.0 Sequence 72.9 78.5 Dependency 81.6 83.1 Table 2: Effe</context>
<context position="27157" citStr="Pereira et al. (2011" startWordPosition="4322" endWordPosition="4325">is interesting to note that an attempt to classify individual concepts using this data directly, without an intervening model of semantics, also achieves about 80% (though on a different task, Shinkareva et al., 2008). Another possible explanation is that both methods reveal equivalent sets of underlying semantic dimensions, but figure 1 suggests not. Alternatively, it may be that the small set of 60 words examined here may be as well-distinguished by means 300 � c=1 s2 c sc RA→b� (4) 120 of their taxonomic differences, as by their topical differences, a suggestion supported by the results in Pereira et al. (2011, see Figure 2A). From the perspective of computational efficiency however, some of the models have clearer advantages. The Dependency and Part-ofSpeech models are processing-intensive, since the broad vocabulary considered requires that the very large quantities of text pass through a parsing or tagging pipeline (though these tasks can be parallelized). The Sequence and Document models conversely require very large amounts of memory to store all their features during SVD. In comparison, the Direction model is impressive, as it achieves close to optimal performance, despite being very cheap to</context>
</contexts>
<marker>Pereira, Detre, Botvinick, 2011</marker>
<rawString>Pereira, F., Detre, G., and Botvinick, M. (2011). Generating Text from Functional Brain Images. Frontiers in Human Neuroscience, 5:1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Word Sense Discovery Based on Sense Descriptor Dissimilarity.</title>
<date>2003</date>
<booktitle>Proceedings of the Ninth Machine Translation Summit,</booktitle>
<pages>315--322</pages>
<contexts>
<context position="7853" citStr="Rapp, 2003" startWordPosition="1183" endWordPosition="1184">ent co-occurrence, among others. Other parameters were kept fixed in a way that the literature suggests would be neutral to the various models, and so allow a fair comparison among them (Sahlgren, 2006; Bullinaria and Levy, 2007; Turney and Pantel, 2010). All textual statistics were gathered from a set of 50m English-language web-page documents consisting of 16 billion words. Where a fixed text window was used, we chose an extent of ±4 lower-case tokens either side of the target 115 word of interest, which is in the mid-range of optimal values found by various authors (Lund and Burgess, 1996; Rapp, 2003; Sahlgren, 2006). Positive pointwise-mutual-information (1,2) was used as an association measure to normalize the observed co-occurrence frequency p(w, f) for the varying frequency of the target word p(w) and its features p(f). PPMI up-weights cooccurrences between rare words, yielding positive values for collocations that are more common than would be expected by chance (i.e. if word distributions were independent), and discards negative values that represent patterns of co-occurrences that are rarer than one would expect by chance. It has been shown to perform well generally, with both word</context>
<context position="10722" citStr="Rapp, 2003" startWordPosition="1638" endWordPosition="1639">n, 2006), which should approximate the scale and composition of the vocabulary of a university-educated speaker of English (Nation and Waring, 1997), and over 95% of tokens typically encountered in English. A frequency threshold is commonly applied for three reasons: low-frequency co-occurrence counts are more noisy; PMI is positively biased towards hapax co-occurrences; and due to Zipfian distributions acut-off dramatically reduces the amount of data to be processed. Many authors use a threshold of approximately 50-100 occurrences for word-collocate models (Lund and Burgess, 1996; Lin, 1998; Rapp, 2003). Since Bullinaria and Levy (2007) find improving performance with models using progressively lower cutoffs we explored two cut-offs of 20 and 50 which equate to low co-occurrences thresholds of 0.00125 or 0.003125 per million respectively; for the word-region model we chose a threshold of 2 occurrences of a target term in a document, to keep the input features to a reasonable dimensionality (Bradford, 2008). After applying these operations to the input data from each model, the resulting 2.2.1 Hand-tailored benchmarks The state-of-the-art models on this brain activity prediction task are both</context>
</contexts>
<marker>Rapp, 2003</marker>
<rawString>Rapp, R. (2003). Word Sense Discovery Based on Sense Descriptor Dissimilarity. Proceedings of the Ninth Machine Translation Summit, pp:315–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rehurek</author>
<author>P Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In New Challenges, LREC 2010,</booktitle>
<pages>45--50</pages>
<publisher>ELRA.</publisher>
<contexts>
<context position="14035" citStr="Rehurek and Sojka, 2010" startWordPosition="2180" endWordPosition="2183">vestigated variations on the decomposition algorithm in any detail, such as those using non-negative matrix factorization, probabilistic LSA or LDA topic models, as the objective in this paper is to provide a direct comparison between the different types of basic collocation information encoded in corpora, rather than evaluate the best algorithmic means for discovering latent dimensions in those co-occurrences. Nor have we evaluated performance on a more structured corpus input (Pereira et al., 2011). However preliminary tests with the Wikipedia corpus, and with LDA, using the Gensim package (Rehurek and Sojka, 2010) yielded similar performances. 2.2.3 Word-Collocate Models Word-collocate models make a complementary assumption to that of the document model: that words with closely-related categorical or taxonomic properties should appear in the same position of similar sentences. In a basic wordcollocate model, based on a word-word cooccurrence matrix, the nearest neighbors of judge might be athlete, singer, or ire-ighter, reflecting paradigmatic relatedness (i.e. substitutability). Word-collocate models are further differentiated by the amount of linguistic annotation attached to word features, ranging f</context>
</contexts>
<marker>Rehurek, Sojka, 2010</marker>
<rawString>Rehurek, R. and Sojka, P. (2010). Software Framework for Topic Modelling with Large Corpora. In New Challenges, LREC 2010, pages 45–50. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="1557" citStr="Rubenstein and Goodenough, 1965" startWordPosition="202" endWordPosition="205">hieving accuracies similar to the leading semi-manual approaches and higher than any published for a corpus-based model. We also find that simple word features enriched with directional information provide a close-tooptimal solution at much lower computational cost. 1 Introduction The cognitive plausibility of computational models of word meaning has typically been tested using behavioural benchmarks, such as identification of synonyms among close associates (the TOEFL task for language learners, see e.g. Landauer and Dumais, 1997); emulating elicited judgments of pairwise similarity (such as Rubenstein and Goodenough, 1965); judgments of category membership (e.g. Battig and Montague, 1969); and word priming effects (Lund and Burgess, 1996). Mitchell et al. (2008) introduced a new task in neurosemantic decoding – using models of semantics to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments. This was achieved with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequen</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Rubenstein, H. and Goodenough, J. B. (1965). Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
</authors>
<title>The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces. Dissertation,</title>
<date>2006</date>
<location>Stockholm University.</location>
<contexts>
<context position="7444" citStr="Sahlgren, 2006" startWordPosition="1115" endWordPosition="1116">uro-semantics task with representative distributional models of semantics that can be derived from arbitrary corpora, using varying degrees of linguistic preprocessing. A series of candidate models were selected to represent the variety of ways in which basic textual features can be extracted and represented, including token co-occurrence in a small local window, dependency parses of whole sentences, and document co-occurrence, among others. Other parameters were kept fixed in a way that the literature suggests would be neutral to the various models, and so allow a fair comparison among them (Sahlgren, 2006; Bullinaria and Levy, 2007; Turney and Pantel, 2010). All textual statistics were gathered from a set of 50m English-language web-page documents consisting of 16 billion words. Where a fixed text window was used, we chose an extent of ±4 lower-case tokens either side of the target 115 word of interest, which is in the mid-range of optimal values found by various authors (Lund and Burgess, 1996; Rapp, 2003; Sahlgren, 2006). Positive pointwise-mutual-information (1,2) was used as an association measure to normalize the observed co-occurrence frequency p(w, f) for the varying frequency of the ta</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Sahlgren, M. (2006). The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces. Dissertation, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sandler</author>
<author>P P Talukdar</author>
<author>L H Ungar</author>
<author>J Blitzer</author>
</authors>
<date>2009</date>
<booktitle>Regularized Learning with Networks of Features. Advances in Neural Information Processing Systems 21,</booktitle>
<pages>4--1401</pages>
<contexts>
<context position="29983" citStr="Sandler et al., 2009" startWordPosition="4778" endWordPosition="4781">that dogs do not have wheels, are not communication events, and do not belong in the aviation domain). But while SVD is certainly effective in dealing with the pervasive synonymy and polysemy seen in corpus-feature sets, it is less clear that it reveals psychologically plausible dimensions of meaning. Alternatives such as non-negative matrix factorization (Lee and Seung, 1999) or Latent Dirichlet Allocation (Blei et al., 2003) might extract more readily interpretable dimensions; or alternative regularisation methods such as Elastic Nets, Lasso (Hastie et al., 2011), or Network Regularisation (Sandler et al., 2009) might even be capable of identifying meaningful clusters of features when learning directly on co-occurrence data. Finally, we should consider whether more derived datasets could be used as input data in place of the basic corpus features used here, such as the full facts learned by the NELL system (Carlson et al., 2010), or crowd-sourced data which can be easily gathered for any word (e.g. association norms, Kiss et al., 1973), though different algorithmic means would be needed to deal with their extreme degree of sparsity. The results also suggest a series of follow-on analyses. A priority </context>
</contexts>
<marker>Sandler, Talukdar, Ungar, Blitzer, 2009</marker>
<rawString>Sandler, T., Talukdar, P. P., Ungar, L. H., and Blitzer, J. (2009). Regularized Learning with Networks of Features. Advances in Neural Information Processing Systems 21, 4:1401–1408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schiitze</author>
<author>J Pedersen</author>
</authors>
<title>A Vector Model for syntagmatic and paradigmatic relatedness.</title>
<date>1993</date>
<booktitle>In Making Sense of Words Proceedings of the 9th Annual Conference of the University of Waterloo Centre for the New OED and Text Research,</booktitle>
<pages>104--113</pages>
<marker>Schiitze, Pedersen, 1993</marker>
<rawString>Schiitze, H. and Pedersen, J. (1993). A Vector Model for syntagmatic and paradigmatic relatedness. In Making Sense of Words Proceedings of the 9th Annual Conference of the University of Waterloo Centre for the New OED and Text Research, pages 104–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V Shinkareva</author>
<author>R A Mason</author>
<author>V L Malave</author>
<author>W Wang</author>
<author>T M Mitchell</author>
<author>M A Just</author>
</authors>
<title>Using fMRI Brain Activation to Identify Cognitive States Associated with Perception of Tools and Dwellings.</title>
<date>2008</date>
<journal>PloS ONE,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="26754" citStr="Shinkareva et al., 2008" startWordPosition="4252" endWordPosition="4255">odels (and so, perhaps the most psychologically plausible), based on dependency parses, is the most successful. Still the performance of sometimes radically different models, from Document-based (syntagmatic) and Word-Form-based (paradigmatic), is surprisingly similar. One reason for this may be that we have reached a ceiling in performance on the fMRI data, due to its inherent noise – in this regard it is interesting to note that an attempt to classify individual concepts using this data directly, without an intervening model of semantics, also achieves about 80% (though on a different task, Shinkareva et al., 2008). Another possible explanation is that both methods reveal equivalent sets of underlying semantic dimensions, but figure 1 suggests not. Alternatively, it may be that the small set of 60 words examined here may be as well-distinguished by means 300 � c=1 s2 c sc RA→b� (4) 120 of their taxonomic differences, as by their topical differences, a suggestion supported by the results in Pereira et al. (2011, see Figure 2A). From the perspective of computational efficiency however, some of the models have clearer advantages. The Dependency and Part-ofSpeech models are processing-intensive, since the b</context>
</contexts>
<marker>Shinkareva, Mason, Malave, Wang, Mitchell, Just, 2008</marker>
<rawString>Shinkareva, S. V., Mason, R. A., Malave, V. L., Wang, W., Mitchell, T. M., and Just, M. A. (2008). Using fMRI Brain Activation to Identify Cognitive States Associated with Perception of Tools and Dwellings. PloS ONE, 3(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From Frequency to Meaning: Vector Space Models of Semantics.</title>
<date>2010</date>
<journal>Artificial Intelligence,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="7497" citStr="Turney and Pantel, 2010" startWordPosition="1121" endWordPosition="1124">ributional models of semantics that can be derived from arbitrary corpora, using varying degrees of linguistic preprocessing. A series of candidate models were selected to represent the variety of ways in which basic textual features can be extracted and represented, including token co-occurrence in a small local window, dependency parses of whole sentences, and document co-occurrence, among others. Other parameters were kept fixed in a way that the literature suggests would be neutral to the various models, and so allow a fair comparison among them (Sahlgren, 2006; Bullinaria and Levy, 2007; Turney and Pantel, 2010). All textual statistics were gathered from a set of 50m English-language web-page documents consisting of 16 billion words. Where a fixed text window was used, we chose an extent of ±4 lower-case tokens either side of the target 115 word of interest, which is in the mid-range of optimal values found by various authors (Lund and Burgess, 1996; Rapp, 2003; Sahlgren, 2006). Positive pointwise-mutual-information (1,2) was used as an association measure to normalize the observed co-occurrence frequency p(w, f) for the varying frequency of the target word p(w) and its features p(f). PPMI up-weights</context>
<context position="13353" citStr="Turney and Pantel (2010)" startWordPosition="2072" endWordPosition="2075">word-region matrix instantiates the assumption that words that share a topical domain (such as medicine, entertainment, philosophy) would be expected to appear in similar sub-sets of text-regions. In such a model, the nearest neighbors of a target word are syntagmatically related (i.e. appear alongside each other), and for judge might include lawyer, court, crime, or prison. The Document model used here is loosely based on LSA, taking the frequency of occurrence of each of our 40,000 vocabulary words in each of 50 million documents as its input data, and it follows Bullinaria and Levy (2007); Turney and Pantel (2010) in using PPMI as a normalization function. We have not investigated variations on the decomposition algorithm in any detail, such as those using non-negative matrix factorization, probabilistic LSA or LDA topic models, as the objective in this paper is to provide a direct comparison between the different types of basic collocation information encoded in corpora, rather than evaluate the best algorithmic means for discovering latent dimensions in those co-occurrences. Nor have we evaluated performance on a more structured corpus input (Pereira et al., 2011). However preliminary tests with the </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Turney, P. D. and Pantel, P. (2010). From Frequency to Meaning: Vector Space Models of Semantics. Artificial Intelligence, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
</authors>
<title>Unsupervised methods for developing taxonomies by combining syntactic and statistical information.</title>
<date>2003</date>
<booktitle>In NAACL,</booktitle>
<pages>197--204</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16986" citStr="Widdows, 2003" startWordPosition="2643" endWordPosition="2644">able to recognize when such high-frequency words are not informative and then discount these, without the need for such assumptions upfront. The Stemmed model is a slight variation on the Word-Form model, where the same statistics are aggregated after applying Lancaster stemming (Paice, 1990; Loper and Bird, 2002). The Directional model, inspired by Schutze and Pedersen (1993), is also derived from the word-form model, but differentiates between cooccurrence to the left or to the right of the target word, with features such as {john L, cake R}. The Part-of-Speech model (Kanejiya et al., 2003; Widdows, 2003) replaces each lowercase word-token with its part-of-speech disambiguated form (e.g. likes VBZ, cake NN). These annotations were extracted from the full dependency parse described below. The Sequence model draws on a range of work that uses word sequence patterns (Lin and Pantel, 2001; Almuhareb and Poesio, 2004; Baroni et al., 2010), and may also be considered an approximation of models that use shallow syntactic analysis (Grefenstette, 1994; Curran and Moens, 2002). All distinct token sequences up to length 4 either side of the target word were counted. Finally the Dependency model uses a fu</context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>Widdows, D. (2003). Unsupervised methods for developing taxonomies by combining syntactic and statistical information. In NAACL, pages 197– 204. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>