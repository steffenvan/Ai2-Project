<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.853498">
UniPi: Recognition of Mentions of Disorders in Clinical Text
</title>
<author confidence="0.653966">
Giuseppe Attardi, Vittoria Cozza, Daniele Sartiano
</author>
<note confidence="0.54572375">
Dipartimento di Informatica
Università di Pisa
Largo B. Pontecorvo, 3
I-56127 Pisa, Italy
</note>
<email confidence="0.979839">
{attardi, cozza, sartiano}@di.unipi.it
</email>
<sectionHeader confidence="0.993799" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999838538461538">
The paper describes our experiments ad-
dressing the SemEval 2014 task on the
Analysis of Clinical text. Our approach
consists in extending the techniques of
NE recognition, based on sequence label-
ling, to address the special issues of this
task, i.e. the presence of overlapping and
discontiguous mentions and the require-
ment to map the mentions to unique iden-
tifiers. We explored using supervised
methods in combination with word em-
beddings generated from unannotated da-
ta.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.975899089285714">
Clinical records provide detailed information on
examination and findings of a patient consulta-
tion expressed in a narrative style. Such records
abound in mentions of clinical conditions, ana-
tomical sites, medications, and procedures,
whose accurate identification is crucial for any
further activity of text mining. Many different
surface forms are used to represent the same
concept and the mentions are interleaved with
modifiers, e.g. adjectives, verb or adverbs, or are
abbreviated involving implicit terms.
For example, in
Abdomen is soft, nontender,
nondistended, negative bruits
the mention occurrences are “Abdomen
nontender” and “Abdomen bruits”, which
refer to the disorders: “nontender abdomen”
and “abdomininal bruit”, with only the sec-
ond having a corresponding UMLS Concept
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
Unique Identifier (CUI). In this case the two
mentions overlap and both are interleaved with
other terms, not part of the mentions.
Secondly, mentions can be nested, as in this
example:
left pleural and parenchymal
calcifications
where the mention calcifications is nested
within pleural calcifications.
Mentions of this kind are a considerable de-
parture from those dealt in typical Named Entity
recognition, which are contiguous and non-
overlapping, and therefore they represents a new
challenge for text analysis.
The analysis of clinical records poses addi-
tional difficulties with respect to other biomedi-
cal NER tasks, which use corpora from the med-
ical literature. Clinical records are entered by
medical personnel on the fly and so they contain
misspellings and inconsistent use of capitaliza-
tion.
The task 7 at SemEval 2014, Analysis of
Clinical Text, addresses the problem of recogni-
tion of mentions of disorders and is divided in
two parts:
A. recognition of mentions of bio-medical
concepts that belong to the UMLS se-
mantic group disorders;
B. mapping of each disorder mention to a
unique UMLS CUI (Concept Unique
Identifiers).
The challenge organizers provided the following
resources:
</bodyText>
<listItem confidence="0.994075857142857">
• A training corpus of clinical notes from
MIMIC II database manually annotated
for disorder mentions and normalized to
an UMLS CUI, consisting of 9432 sen-
tences, with 5816 annotations.
• A collection of unannotated notes, consist-
ing of 1,611,080 sentences.
</listItem>
<page confidence="0.977369">
754
</page>
<note confidence="0.7310475">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 754–760,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.985501">
We also had access to the UMLS ontology (Bo-
denreider, 2004).
Our approach to portion A of the task was to
adapt a sequence labeller, which provides good
accuracy in Named Entity recognition in the
newswire domain, to handle the peculiarities of
the clinical domain.
We performed mention recognition in two
steps:
</bodyText>
<listItem confidence="0.956314">
1. identifying contiguous portions of a men-
tion;
2. combining separated portions of mentions
into a full mention.
</listItem>
<bodyText confidence="0.986136529411765">
In order to use a traditional sequence tagger for
the first step, we had to convert the input data
into a suitable format, in particular, we dealt with
nested mentions by transforming them into non-
overlapping sequences, through replication.
For recombining discontiguous mentions, we
employed a classifier, trained to recognize
whether pairs of mentions belong to the same
entity. The classifier was trained using also fea-
tures extracted from the dependency tree of a
sentence, in particular the distance of terms along
the tree path. Terms related by a dependency
have distance 1 and terms having a common
head have distance 2. By limiting the pairs1 to be
considered for combination to those within dis-
tance 3, we both ensure that only plausible com-
binations are performed and reduce the cost of
the algorithm.
For dealing with portion B of the task, we ap-
ply fuzzy matching (Fraiser, 2011) between the
extracted mentions and the textual description of
entities present in selected sections of UMLS
disorders. The CUI from the match with highest
score is chosen.
In the following sections, we describe how we
carried out the experiments, starting with the pre-
processing of the data, then with the training of
several versions of NE recognizer, the training of
the classifier for mention combination. We then
report on the results and discuss some error anal-
ysis on the results.
2 Preprocessing of the annotated data
The training data was pre-processed, in order to
obtain corpora in a suitable format for:
</bodyText>
<listItem confidence="0.8802105">
1. training a sequence tagger
2. training the classifier for mention com-
bination.
1 Not implemented in the submitted runs.
</listItem>
<bodyText confidence="0.996167">
Annotations in the training data adopt a pipe-
delimited stand-off character-offset format. The
example in the introduction has these annota-
tions:
</bodyText>
<equation confidence="0.997045">
00098-016139-
DISCHARGE_SUMMARY.txt  ||Dis-
ease_Disorder  ||C0221755 ||
1141  ||1148  ||1192  ||1198
00098-016139-
DISCHARGE_SUMMARY.txt  ||Dis-
ease_Disorder  ||CUI-less ||
1141  ||1148  ||1158  ||1167
</equation>
<bodyText confidence="0.999895625">
The first annotation marks Disease_Disorder
as annotation type, C0221755 as CUI, while the
remaining pairs of numbers represent character
offsets within the original text that correspond to
spans of texts containing the mention, i.e. Abdo-
men nondistended. The second annotation is
similar and refers to Abdomen bruits.
In order to prepare the training corpus for a
NE tagger, the data had to be transformed and
converted into IOB2 notation. However a stand-
ard IOB notation does not convey information
about overlapping or discontiguous mentions.
In order to deal with overlapping mentions, as
is the case for word “Abdomen” in our earlier
example, multiple copies of the sentence are pro-
duced, each one annotated with disjoint men-
tions. If two mentions overlap, two versions are
generated, one annotated with just the first men-
tion and one with the second. If several overlap-
ping mentions are present in a sentence, copies
are generated for all possible combinations of
non-overlapping mentions.
For dealing with discontiguous mentions, each
annotated entity is assigned an id, uniquely iden-
tifying the mention within the sentence. This id
is added as an extra attribute to each token, rep-
resented as an extra column in the tab separated
IOB file format for the NE tagger.
We processed with the Tanl pipeline (Attardi
et al., 2009; Attardi et al., 2010). We first ex-
tracted the text from the training corpus in XML
format and added the mentions annotations as
tags enclosing them, with spans and mentions id
as attributes. We then applied sentence splitting,
tokenization, PoS tagging and dependency pars-
ing using DeSR (Attardi, 2006).
The tags were converted to IOB format.
Here are two sample tokens in the resulting
annotation, with attributes id, form, pos, head,
deprel, entity, entity id:
</bodyText>
<footnote confidence="0.92018">
2 http://en.wikipedia.org/wiki/Inside_Outside_Beginning
</footnote>
<page confidence="0.994822">
755
</page>
<figure confidence="0.992675333333333">
1 Abdomen NNP 2 SBJ B-DISO 1
...
5 nontender NN 10 NMOD B-DISO 1
</figure>
<sectionHeader confidence="0.694971" genericHeader="introduction">
3 Named Entity Tagging
</sectionHeader>
<bodyText confidence="0.999943666666667">
The core of our approach relies on an initial
stage of Named Entity recognition. We per-
formed several experiments, using different NE
taggers in different configurations and using both
features from the training corpus and features
obtained from the unannotated data.
</bodyText>
<subsectionHeader confidence="0.997269">
3.1 Tanl NER
</subsectionHeader>
<bodyText confidence="0.999969904761905">
We performed several experiments using the
Tanl NE Tagger (Attardi et al., 2009), a generic,
customizable statistical sequence labeller, suita-
ble for many tasks of sequence labelling, such as
POS tagging or Named Entity Recognition.
The tagger implements a Conditional Markov
Model and can be configured to use different
classification algorithms and to specify feature
templates for extracting features. In our experi-
ments we used a linear SVM classification algo-
rithm.
We experimented with several configurations,
all including a set of word shape features, as in
(Attardi et al., 2009): (1) the previous word is
capitalized; (2) the following word is capitalized;
(3) the current word is in upper case; (4) the cur-
rent word is in mixed case; (5) the current word
is a single uppercase character; (6) the current
word is a uppercase character and a dot; (7) the
current word contains digits; (8) the current word
is two digits; (9) the current word is four digits;
</bodyText>
<listItem confidence="0.573018">
(10) the current word is made of digits and “T”;
(11) the current word contains “$”; (12) the cur-
rent word contains “%”; (13) the current word
contains an apostrophe; (14) the current word is
made of digits and dots.
</listItem>
<bodyText confidence="0.999451">
A number of dictionary features were also
used, including prefix and suffix dictionaries,
bigrams, last words, first word and frequent
words, all extracted from the training corpus.
Additionally, a dictionary of disease terms was
used, consisting of about 22,000 terms extracted
from the preferred terms for CUIs belonging to
the UMLS semantic type “Disease or Syn-
drome”.
The first character of the POS tag was also
used as feature, extracted from a window of to-
kens before and after the current token.
Finally attribute features are extracted from
attributes (Form, PoS, Lemma, NE, Disease) of
surrounding tokens, denoted by their relative po-
sition to the current token. The best combination
of Attribute features obtained with runs on the
development set was the following:
</bodyText>
<table confidence="0.884678">
Feature Tokens
POS[0] wi-2 wi–1 wi wi+1
DISEASE wi wi+1 wi+2
</table>
<tableCaption confidence="0.999649">
Table 1. Attribute features used in the runs.
</tableCaption>
<subsectionHeader confidence="0.998357">
3.2 Word Embeddings
</subsectionHeader>
<bodyText confidence="0.998292277777778">
We explored ways to use the unannotated data in
NE recognition by exploiting word embeddings
(Collobert et al, 2011). In a paper published after
our submission, Tang et al. (2014) show that
word embeddings are beneficial to Biomedical
NER.
We used the word embeddings for 100,000
terms created through deep learning on the Eng-
lish Wikipedia by Al-Rfou et al. (2013). We then
built, with the same procedure, embedding for
terms from the supplied unlabelled data. The
corpus was split, tokenized and normalized and a
vocabulary was created with the most frequent
words not already present among the Wikipedia
word embeddings. Four versions of the embed-
dings were created, varying the size of the vo-
cabulary and the size of the context window, as
described in Table 1.
</bodyText>
<table confidence="0.999873833333333">
Run1 Run2 Run3 Run4
Vocabulary size 50,000 50,000 30,000 30,000
Context 5 2 5 2
Hidden Layers 32 32 32 32
Learning Rate 0.1 0.1 0.1 0.1
Embedding size 64 64 64 64
</table>
<tableCaption confidence="0.999412">
Table 2. Word Embedding Parameters.
</tableCaption>
<bodyText confidence="0.999970411764706">
We developed and trained a Deep Learning NE
tagger (nlpnet, 2014) based on the SENNA archi-
tecture (SENNA, 2011) using these word em-
beddings.
As an alternative to using the embeddings di-
rectly as features, we created clusters of word
embeddings using the Dbscan algorithm (Ester et
al., 1996) implemented in the sklearn library. We
carried out several experiments, varying the pa-
rameters of the algorithm. The configuration that
produced the largest number of clusters had 572
clusters. The clusters turned out not to be much
significant, since a single cluster had about
29,000 words, another had 5,000 words, and the
others had few, unusual words.
We added the clusters as a dictionary feature
to our NE tagger. Unfortunately, most of the
</bodyText>
<page confidence="0.993108">
756
</page>
<bodyText confidence="0.9913825">
terms fell within 4 clusters, so the feature turned
out to be little discriminative.
</bodyText>
<subsectionHeader confidence="0.997987">
3.3 Stanford NER
</subsectionHeader>
<bodyText confidence="0.9999962">
We performed experiments also with a tagger
based on a different statistical approach: the
Stanford Named Entity Recognizer. This tagger
is based on the Conditional Random Fields
(CRF) statistical model and uses Gibbs sampling
instead of other dynamic programming tech-
niques for inference on sequence models (Finkel
et al., 2005). This tagger normally works well
enough using just the form of tokens as feature
and we applied it so.
</bodyText>
<subsectionHeader confidence="0.921594">
3.4 NER accuracy
</subsectionHeader>
<bodyText confidence="0.999585714285714">
We report the accuracy of the various NE taggers
we tested on the development set, using the scor-
er from the CoNLL Shared Task 2003 (Tjong
Kim Sang and De Meulder, 2003).
We include here also the results with
CRFsuite, the CRF tagger used in (Tang et al.,
2014).
</bodyText>
<table confidence="0.941176833333333">
NER Precision Recall F- score
Tanl 80.41 65.08 71.94
Tanl+clusters 80.43 64.48 71.58
nlpnet 80.29 62.51 70.29
Stanford 80.30 64.89 71.78
CRFsuite 79.69 61.97 69.72
</table>
<tableCaption confidence="0.9849745">
Table 3. Accuracy of various NE taggers on the
development set.
</tableCaption>
<bodyText confidence="0.999714222222222">
Based on these results we chose the Tanl tagger
and the Stanford NER for our submitted runs.
All these taggers are known to be capable of
achieving state of the art performance or close to
it (89.57 F1) in the CoNLL 2003 shared task on
the WSJ Penn Treebank.
The accuracy on the current benchmark is
much lower, despite the fact that there is only
one category and the terminology for disorders is
drawn from a restricted vocabulary.
It has been noted by Dingare et al. (2005) that
NER over biomedical texts achieves lower accu-
racy compared to other domains, quite within the
range of the above results. Indeed, compared
with the newswire domain or other domains, the
entities in the biomedical domain tend to be more
complex, without the distinctive shape features
of the newswire categories.
</bodyText>
<sectionHeader confidence="0.998255" genericHeader="method">
4 Discontiguous mentions
</sectionHeader>
<bodyText confidence="0.999929">
Discontiguous mention detection can be formu-
lated as a problem of deciding whether two con-
tiguous mentions belong to the same mention. As
such, it can be cast into a classification problem.
A similar approach was used successfully for the
coreference resolution task at SemEval 2010 (At-
tardi, Dei Rossi et al., 2010)
</bodyText>
<subsectionHeader confidence="0.996915">
4.1 Mentions detection
</subsectionHeader>
<bodyText confidence="0.953215833333333">
We trained a Maximum Entropy classifier
(Ratnaparkhi, 1996) to recognize whether two
terms belong to the same mention.
The training instances for the pair-wise learner
consist of each pair of terms within a sentence
annotated as disorders. A positive instance is
created if the terms belong to the same mention,
negative otherwise.
The classifier was trained using the following
features, extracted for each pair of words for dis-
eases.
Distance features
</bodyText>
<listItem confidence="0.9193065">
• Token distance: quantized distance be-
tween the two words;
• Ancestor distance: quantized distance be-
tween the words in the parse tree if one is
the ancestor of the other
Syntax features
• Head: whether the two words have the
same head;
• DepPath: concatenation of the dependen-
cy relations of the two words to their
common parent
Dictionary features
• UMLS: whether the two words are both
present in an UMLS definition
</listItem>
<bodyText confidence="0.99971025">
The last feature is motivated by the fact that, ac-
cording to the task description, most of the dis-
order mentions correspond to diseases in the
SNOMED terminology.
</bodyText>
<subsectionHeader confidence="0.999583">
4.2 Merging of mentions
</subsectionHeader>
<bodyText confidence="0.999977777777778">
The mentions detected in the first phase are
merged using the following process. Sentence
are parsed and then for each pair of words that
are tagged as disorder, features are extracted and
passed to the classifier.
If the classifier assigns a probability greater
than a given threshold the two words are com-
bined into a larger mention. The process is then
repeated trying to further extend each mention
</bodyText>
<page confidence="0.993526">
757
</page>
<bodyText confidence="0.872191">
with additional terms by combining mentions
that share a word.
</bodyText>
<sectionHeader confidence="0.623886" genericHeader="method">
5 Mapping entities to GUIs
</sectionHeader>
<bodyText confidence="0.999880896551724">
Task B requires mapping each recognized entity
to a concept in the SNOMED-CT terminology,
assigning to it a unique UMLS CUI, if possible,
or else marking it as CUI-less. The CUIs are
limited to those corresponding to SNOMED
codes and belonging to the following UMLS se-
mantic types: “Acquired Abnormality&amp;quot; or “Con-
genital Abnormality&amp;quot;, “Injury or Poisoning&amp;quot;,
&amp;quot;Pathologic Function&amp;quot;, &amp;quot;Disease or Syndrome&amp;quot;,
&amp;quot;Mental or Behavioral Dysfunction&amp;quot;, &amp;quot;Cell or
Molecular Dysfunction&amp;quot;, &amp;quot;Experimental Model
of Disease&amp;quot; or &amp;quot;Anatomical Abnormality&amp;quot;, “Ne-
oplastic Process&amp;quot; or &amp;quot;Sign or Symptom&amp;quot;.
In order to speed up search, we created two
indices: an inverted index from words in the def-
inition of a CUI to the corresponding CUI and a
forward index from a CUI to its definition.
For assigning a CUI to a mention, we search
in the dictionary of CUI preferred terms, first for
an exact match, then for a normalized mention
and finally for a fuzzy match (Fraiser, 2011).
Normalization entails dropping punctuation and
stop words. Fuzzy matching is sometimes too
liberal, for example it matches “chronic ob-
structive pulmonary” with “chronic ob-
structive lung disease”; so we also put a
ceiling on the edit distance between the phrases.
The effectiveness of the process is summa-
rized in these results on the development set:
</bodyText>
<table confidence="0.943257">
Exact Normalized Fuzzy No
matches matches matches matches
1352 868 304 5488
</table>
<tableCaption confidence="0.996916">
Table 4. CUI identifications on the devel set.
</tableCaption>
<sectionHeader confidence="0.997104" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.998990625">
The training corpus for the submission consisted
of the merge of the train and development sets.
We submitted three runs, using different or
differently configured NE tagger.
Two runs were submitted using the Tanl tag-
ger using the features listed in Table 5, where
DISEASE and CLUSTER meaning is explained
earlier.
</bodyText>
<table confidence="0.77640875">
Feature UniPI_run0 UniPI_ run1
POS[0] wi-2 wi–1 wi wi+1 wi-2 wi–1 wi wi+1
CLUSTER wi wi+1 wi wi+1
DISEASE wi wi+1 wi+2
</table>
<tableCaption confidence="0.964534">
Table 5. Attribute features used in the runs.
</tableCaption>
<bodyText confidence="0.9995604">
Since the clustering produced few large clusters,
the inclusion of this feature did not affect sub-
stantially the results.
A third run (UniPi_run_2) was performed us-
ing the Stanford NER with default settings.
</bodyText>
<sectionHeader confidence="0.999702" genericHeader="method">
7 Results
</sectionHeader>
<bodyText confidence="0.995334">
The results obtained in the three submitted runs,
are summarized in Table 6, in terms of accuracy,
precision, recall and F-score. For comparison,
also the results obtained by the best performing
systems are included.
</bodyText>
<table confidence="0.943657875">
Run Precision Recall F- score
Task A
Unipi_run0 0.539 0.684 0.602
Unipi_run1 0.659 0.612 0.635
Unipi_run2 0.712 0.601 0.652
SemEval best 0.843 0.786 0813
Task A relaxed
Unipi_run0 0.778 0.885 0.828
Unipi_run1 0.902 0.775 0.834
Unipi_run2 0.897 0.766 0.826
SemEval best 0.936 0.866 0.900
Table 6. UniPI Task A results, compared to the
best submission.
Run Accuracy
Task B
Unipi_run0 0.467
Unipi_run1 0.428
Unipi_run2 0.417
SemEval best 0.741
Task B relaxed
Unipi_run0 0.683
Unipi_run1 0.699
Unipi_run2 0.693
SemEval best 0.873
</table>
<tableCaption confidence="0.972098">
Table 7. UniPI Task B results, compared to the
best submission.
</tableCaption>
<sectionHeader confidence="0.993752" genericHeader="method">
8 Error analysis
</sectionHeader>
<bodyText confidence="0.9998897">
Since the core step of our approach is the NE
recognition, we tried to analyze possible causes
of its errors.
Some errors might be due to mistakes by the
POS tagger. For example, often some words oc-
cur in full upper case, leading to classify adjec-
tives like ABDOMINAL as NNP instead of JJ.
Training our POS tagger on the GENIA corpus
or using the GENIA POS tagger might have
helped a little. Spelling errors like abdominla
</bodyText>
<page confidence="0.993368">
758
</page>
<bodyText confidence="0.999953222222222">
instead of abdominal could also have been cor-
rected.
Another choice that might have affected the
NER accuracy was our decision to duplicate the
sentences in order to remove mention overlaps.
An alternative solution might have been to use
two categories in the IOB annotation: one cate-
gory for full contiguous disorder mentions and
another for partial disorder mentions. This might
have reduced the confusion in the tagger, since
isolated words like abdomen get tagged as dis-
order, having been so annotated in the training
set. Distinguishing the two cases, abdomen
would become a disorder mention in the step of
mention merging. Counting the errors in the de-
velopment set we found that 939 out of the 1757
errors were indeed individual words incorrectly
identified as disorders.
</bodyText>
<subsectionHeader confidence="0.968308">
8.1 After submission experiments
</subsectionHeader>
<bodyText confidence="0.9997234">
After the submission, we changed the algorithm
for merging mentions, in order to avoid nested
spans, retaining only the larger one. Tests on the
development set show that this change leads to a
small improvement in the strict evaluation:
</bodyText>
<figure confidence="0.796852">
Run
devel_run1
devel run1_after
devel_run1
devel run1_after
</figure>
<tableCaption confidence="0.962227">
Table 8. UniPI Task A post submission results.
</tableCaption>
<sectionHeader confidence="0.994642" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999987785714286">
We reported our participation to SemEval 2014
on the Analysis of Clinical Text. Our approach is
based on using a NER, for identifying contiguous
mentions and on a Maximum Entropy classifier
for merging discontiguous ones.
The training data was transformed into a for-
mat suitable for a standard NE tagger, that does
not accept discontiguous or nested mentions. Our
measurements on the development set showed
that different NE tagger reach a similar accuracy.
We explored using word embeddings as fea-
tures, generated from the unsupervised data pro-
vided, but they did not improve the accuracy of
the NE tagger.
</bodyText>
<sectionHeader confidence="0.997141" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.953295">
Partial support for this work was provided by
project RIS (POR RIS of the Regione Toscana,
CUP n° 6408.30122011.026000160).
</bodyText>
<sectionHeader confidence="0.984894" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999756673469388">
Rami Al-Rfou’, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed Word Representations
for Multilingual NLP. In Proceedings of Confer-
ence on Computational Natural Language Learn-
ing, CoNLL ‘13, pages 183-192, Sofia, Bulgaria.
Giuseppe Attardi. 2006. Experiments with a Mul-
tilanguage Non-Projective Dependency Parser. In
Proceedings of the Tenth Conference on Natural
Language Learning, CoNLL ‘06, pages 166-170,
New York, NY.
Giuseppe Attardi et al., 2009. Tanl (Text Analytics
and Natural Language Processing). SemaWiki pro-
ject: http://medialab.di.unipi.it/wiki/SemaWiki.
Giuseppe Attardi, Stefano Dei Rossi, Felice Dell&apos;Or-
letta and Eva Maria Vecchi. 2009. The Tanl
Named Entity Recognizer at Evalita 2009. In Pro-
ceedings of Workshop Evalita’09 - Evaluation of
NLP and Speech Tools for Italian, Reggio Emilia,
ISBN 978-88-903581-1-1.
Giuseppe Attardi, Felice Dell&apos;Orletta, Maria Simi and
Joseph Turian. 2009. Accurate Dependency Pars-
ing with a Stacked Multilayer Perceptron. In Pro-
ceedings of Workshop Evalita’09 - Evaluation of
NLP and Speech Tools for Italian, Reggio Emilia,
ISBN 978-88-903581-1-1.
Giuseppe Attardi, Stefano Dei Rossi and Maria Simi.
2010. The Tanl Pipeline. In Proceedings of LREC
Workshop on Web Services and Processing Pipe-
lines in HLT, WSPP, La Valletta, Malta, pages 14-
21
Giuseppe Attardi, Stefano Dei Rossi and Maria Simi.
2010. TANL-1: Coreference Resolution by Parse
Analysis and Similarity Clustering. In Proceedings
of the 5th International Workshop on Semantic
Evaluation, SemEval 2010, Uppsala, Sweden, pag-
es 108-111
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical
terminology. Nucleic Acids Research, vol. 32, no.
supplement 1, pages D267–D270.
Ronan Collobert et al. 2011. Natural Language Pro-
cessing (Almost) from Scratch. Journal of Machine
Learning Research, 12, pages 2461–2505.
Shipra Dingare, Malvina Nissim, Jenny Finkel, Chris-
topher Manning and Claire Grover. 2005. A Sys-
tem for Identifying Named Entities in Biomedical
Text: how Results From two Evaluations Reflect
on Both the System and the Evaluations. Comp
Funct Genomics. Feb-Mar; 6(1-2): pages 77–85.
</reference>
<figure confidence="0.880530428571429">
Precision Recall F- score
Task A
0.596 0.653 0.624
0.668 0.637 0.652
Task A relaxed
0.865 0.850 0.858
0.864 0.831 0.847
</figure>
<page confidence="0.984064">
759
</page>
<reference confidence="0.998479689655172">
Martin Ester, et al. 1996. A density-based algorithm
for discovering clusters in large spatial databases
with noise. In Proceedings of 2nd International
Conference on Knowledge Discovery and Data Mi-
ing, KDD 96, pages 226–231.
Jenny Rose Finkel, Trond Grenager and Christopher
Manning 2005. Incorporating Non-local Infor-
mation into Information Extraction Systems by
Gibbs Sampling. In Proceedings of the 43nd Annu-
al Meeting of the Association for Computational
Linguistics, 2005, pages 363–370.
Neil Fraser. 2011. Diff, Match and Patch libraries for
Plain Text. (Based on Myer&apos;s diff algorithm).
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Part-Of-Speech Tagger. In Proceedings of the Em-
pirical Methods in Natural Language Processing
Conference, EMNLP ’96, pages 17-18.
Buzhou Tang, Hongxin Cao, Xiaolong Wang, Qingcai
Chen, and Hua Xu. 2014. Evaluating Word Repre-
sentation Features in Biomedical Named Entity
Recognition Tasks. BioMed Research Internation-
al, Volume 2014, Article ID 240403.
Erik F. Tjong Kim Sang and Fien De Meulder 2003.
Introduction to the CoNLL ‘03 Shared Task: Lan-
guage-Independent Named Entity Recognition. In:
Proceedings of CoNLL ‘03, Edmonton, Canada,
pages 142-147.
SENNA. 2011. http://ml.nec-labs.com/senna/
nlpnet. 2014. https://github.com/attardi/nlpnet
</reference>
<page confidence="0.99641">
760
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.833238">
<title confidence="0.9987">UniPi: Recognition of Mentions of Disorders in Clinical Text</title>
<author confidence="0.982129">Giuseppe Attardi</author>
<author confidence="0.982129">Vittoria Cozza</author>
<author confidence="0.982129">Daniele</author>
<affiliation confidence="0.981540666666667">Dipartimento di Università di Largo B. Pontecorvo,</affiliation>
<address confidence="0.999877">I-56127 Pisa, Italy</address>
<email confidence="0.99399">attardi@di.unipi.it</email>
<email confidence="0.99399">cozza@di.unipi.it</email>
<email confidence="0.99399">sartiano@di.unipi.it</email>
<abstract confidence="0.992373714285714">The paper describes our experiments addressing the SemEval 2014 task on the Analysis of Clinical text. Our approach consists in extending the techniques of NE recognition, based on sequence labelling, to address the special issues of this task, i.e. the presence of overlapping and discontiguous mentions and the requirement to map the mentions to unique identifiers. We explored using supervised methods in combination with word embeddings generated from unannotated data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rami Al-Rfou’</author>
<author>Bryan Perozzi</author>
<author>Steven Skiena</author>
</authors>
<title>Polyglot: Distributed Word Representations for Multilingual NLP.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference on Computational Natural Language Learning, CoNLL ‘13,</booktitle>
<pages>183--192</pages>
<location>Sofia, Bulgaria.</location>
<marker>Al-Rfou’, Perozzi, Skiena, 2013</marker>
<rawString>Rami Al-Rfou’, Bryan Perozzi, and Steven Skiena. 2013. Polyglot: Distributed Word Representations for Multilingual NLP. In Proceedings of Conference on Computational Natural Language Learning, CoNLL ‘13, pages 183-192, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
</authors>
<title>Experiments with a Multilanguage Non-Projective Dependency Parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Natural Language Learning, CoNLL ‘06,</booktitle>
<pages>166--170</pages>
<location>New York, NY.</location>
<contexts>
<context position="7362" citStr="Attardi, 2006" startWordPosition="1141" endWordPosition="1142">iguous mentions, each annotated entity is assigned an id, uniquely identifying the mention within the sentence. This id is added as an extra attribute to each token, represented as an extra column in the tab separated IOB file format for the NE tagger. We processed with the Tanl pipeline (Attardi et al., 2009; Attardi et al., 2010). We first extracted the text from the training corpus in XML format and added the mentions annotations as tags enclosing them, with spans and mentions id as attributes. We then applied sentence splitting, tokenization, PoS tagging and dependency parsing using DeSR (Attardi, 2006). The tags were converted to IOB format. Here are two sample tokens in the resulting annotation, with attributes id, form, pos, head, deprel, entity, entity id: 2 http://en.wikipedia.org/wiki/Inside_Outside_Beginning 755 1 Abdomen NNP 2 SBJ B-DISO 1 ... 5 nontender NN 10 NMOD B-DISO 1 3 Named Entity Tagging The core of our approach relies on an initial stage of Named Entity recognition. We performed several experiments, using different NE taggers in different configurations and using both features from the training corpus and features obtained from the unannotated data. 3.1 Tanl NER We perform</context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>Giuseppe Attardi. 2006. Experiments with a Multilanguage Non-Projective Dependency Parser. In Proceedings of the Tenth Conference on Natural Language Learning, CoNLL ‘06, pages 166-170, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
</authors>
<title>Tanl (Text Analytics and Natural Language Processing). SemaWiki project: http://medialab.di.unipi.it/wiki/SemaWiki.</title>
<date>2009</date>
<marker>Attardi, 2009</marker>
<rawString>Giuseppe Attardi et al., 2009. Tanl (Text Analytics and Natural Language Processing). SemaWiki project: http://medialab.di.unipi.it/wiki/SemaWiki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
<author>Stefano Dei Rossi</author>
<author>Felice Dell&apos;Orletta</author>
<author>Eva Maria Vecchi</author>
</authors>
<title>The Tanl Named Entity Recognizer at Evalita</title>
<date>2009</date>
<booktitle>In Proceedings of Workshop Evalita’09 - Evaluation of NLP and Speech Tools for Italian, Reggio Emilia, ISBN</booktitle>
<pages>978--88</pages>
<contexts>
<context position="7058" citStr="Attardi et al., 2009" startWordPosition="1090" endWordPosition="1093">ed with disjoint mentions. If two mentions overlap, two versions are generated, one annotated with just the first mention and one with the second. If several overlapping mentions are present in a sentence, copies are generated for all possible combinations of non-overlapping mentions. For dealing with discontiguous mentions, each annotated entity is assigned an id, uniquely identifying the mention within the sentence. This id is added as an extra attribute to each token, represented as an extra column in the tab separated IOB file format for the NE tagger. We processed with the Tanl pipeline (Attardi et al., 2009; Attardi et al., 2010). We first extracted the text from the training corpus in XML format and added the mentions annotations as tags enclosing them, with spans and mentions id as attributes. We then applied sentence splitting, tokenization, PoS tagging and dependency parsing using DeSR (Attardi, 2006). The tags were converted to IOB format. Here are two sample tokens in the resulting annotation, with attributes id, form, pos, head, deprel, entity, entity id: 2 http://en.wikipedia.org/wiki/Inside_Outside_Beginning 755 1 Abdomen NNP 2 SBJ B-DISO 1 ... 5 nontender NN 10 NMOD B-DISO 1 3 Named En</context>
<context position="8540" citStr="Attardi et al., 2009" startWordPosition="1321" endWordPosition="1324">e unannotated data. 3.1 Tanl NER We performed several experiments using the Tanl NE Tagger (Attardi et al., 2009), a generic, customizable statistical sequence labeller, suitable for many tasks of sequence labelling, such as POS tagging or Named Entity Recognition. The tagger implements a Conditional Markov Model and can be configured to use different classification algorithms and to specify feature templates for extracting features. In our experiments we used a linear SVM classification algorithm. We experimented with several configurations, all including a set of word shape features, as in (Attardi et al., 2009): (1) the previous word is capitalized; (2) the following word is capitalized; (3) the current word is in upper case; (4) the current word is in mixed case; (5) the current word is a single uppercase character; (6) the current word is a uppercase character and a dot; (7) the current word contains digits; (8) the current word is two digits; (9) the current word is four digits; (10) the current word is made of digits and “T”; (11) the current word contains “$”; (12) the current word contains “%”; (13) the current word contains an apostrophe; (14) the current word is made of digits and dots. A nu</context>
</contexts>
<marker>Attardi, Rossi, Dell&apos;Orletta, Vecchi, 2009</marker>
<rawString>Giuseppe Attardi, Stefano Dei Rossi, Felice Dell&apos;Orletta and Eva Maria Vecchi. 2009. The Tanl Named Entity Recognizer at Evalita 2009. In Proceedings of Workshop Evalita’09 - Evaluation of NLP and Speech Tools for Italian, Reggio Emilia, ISBN 978-88-903581-1-1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
<author>Felice Dell&apos;Orletta</author>
<author>Maria Simi</author>
<author>Joseph Turian</author>
</authors>
<title>Accurate Dependency Parsing with a Stacked Multilayer Perceptron.</title>
<date>2009</date>
<booktitle>In Proceedings of Workshop Evalita’09 - Evaluation of NLP and Speech Tools for Italian, Reggio Emilia, ISBN</booktitle>
<pages>978--88</pages>
<contexts>
<context position="7058" citStr="Attardi et al., 2009" startWordPosition="1090" endWordPosition="1093">ed with disjoint mentions. If two mentions overlap, two versions are generated, one annotated with just the first mention and one with the second. If several overlapping mentions are present in a sentence, copies are generated for all possible combinations of non-overlapping mentions. For dealing with discontiguous mentions, each annotated entity is assigned an id, uniquely identifying the mention within the sentence. This id is added as an extra attribute to each token, represented as an extra column in the tab separated IOB file format for the NE tagger. We processed with the Tanl pipeline (Attardi et al., 2009; Attardi et al., 2010). We first extracted the text from the training corpus in XML format and added the mentions annotations as tags enclosing them, with spans and mentions id as attributes. We then applied sentence splitting, tokenization, PoS tagging and dependency parsing using DeSR (Attardi, 2006). The tags were converted to IOB format. Here are two sample tokens in the resulting annotation, with attributes id, form, pos, head, deprel, entity, entity id: 2 http://en.wikipedia.org/wiki/Inside_Outside_Beginning 755 1 Abdomen NNP 2 SBJ B-DISO 1 ... 5 nontender NN 10 NMOD B-DISO 1 3 Named En</context>
<context position="8540" citStr="Attardi et al., 2009" startWordPosition="1321" endWordPosition="1324">e unannotated data. 3.1 Tanl NER We performed several experiments using the Tanl NE Tagger (Attardi et al., 2009), a generic, customizable statistical sequence labeller, suitable for many tasks of sequence labelling, such as POS tagging or Named Entity Recognition. The tagger implements a Conditional Markov Model and can be configured to use different classification algorithms and to specify feature templates for extracting features. In our experiments we used a linear SVM classification algorithm. We experimented with several configurations, all including a set of word shape features, as in (Attardi et al., 2009): (1) the previous word is capitalized; (2) the following word is capitalized; (3) the current word is in upper case; (4) the current word is in mixed case; (5) the current word is a single uppercase character; (6) the current word is a uppercase character and a dot; (7) the current word contains digits; (8) the current word is two digits; (9) the current word is four digits; (10) the current word is made of digits and “T”; (11) the current word contains “$”; (12) the current word contains “%”; (13) the current word contains an apostrophe; (14) the current word is made of digits and dots. A nu</context>
</contexts>
<marker>Attardi, Dell&apos;Orletta, Simi, Turian, 2009</marker>
<rawString>Giuseppe Attardi, Felice Dell&apos;Orletta, Maria Simi and Joseph Turian. 2009. Accurate Dependency Parsing with a Stacked Multilayer Perceptron. In Proceedings of Workshop Evalita’09 - Evaluation of NLP and Speech Tools for Italian, Reggio Emilia, ISBN 978-88-903581-1-1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
<author>Stefano Dei Rossi</author>
<author>Maria Simi</author>
</authors>
<title>The Tanl Pipeline.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC Workshop on Web Services and Processing Pipelines in HLT, WSPP,</booktitle>
<pages>14--21</pages>
<location>La Valletta, Malta,</location>
<contexts>
<context position="7081" citStr="Attardi et al., 2010" startWordPosition="1094" endWordPosition="1097">ons. If two mentions overlap, two versions are generated, one annotated with just the first mention and one with the second. If several overlapping mentions are present in a sentence, copies are generated for all possible combinations of non-overlapping mentions. For dealing with discontiguous mentions, each annotated entity is assigned an id, uniquely identifying the mention within the sentence. This id is added as an extra attribute to each token, represented as an extra column in the tab separated IOB file format for the NE tagger. We processed with the Tanl pipeline (Attardi et al., 2009; Attardi et al., 2010). We first extracted the text from the training corpus in XML format and added the mentions annotations as tags enclosing them, with spans and mentions id as attributes. We then applied sentence splitting, tokenization, PoS tagging and dependency parsing using DeSR (Attardi, 2006). The tags were converted to IOB format. Here are two sample tokens in the resulting annotation, with attributes id, form, pos, head, deprel, entity, entity id: 2 http://en.wikipedia.org/wiki/Inside_Outside_Beginning 755 1 Abdomen NNP 2 SBJ B-DISO 1 ... 5 nontender NN 10 NMOD B-DISO 1 3 Named Entity Tagging The core o</context>
</contexts>
<marker>Attardi, Rossi, Simi, 2010</marker>
<rawString>Giuseppe Attardi, Stefano Dei Rossi and Maria Simi. 2010. The Tanl Pipeline. In Proceedings of LREC Workshop on Web Services and Processing Pipelines in HLT, WSPP, La Valletta, Malta, pages 14-21</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
<author>Stefano Dei Rossi</author>
<author>Maria Simi</author>
</authors>
<title>TANL-1: Coreference Resolution by Parse Analysis and Similarity Clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval</booktitle>
<pages>108--111</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="7081" citStr="Attardi et al., 2010" startWordPosition="1094" endWordPosition="1097">ons. If two mentions overlap, two versions are generated, one annotated with just the first mention and one with the second. If several overlapping mentions are present in a sentence, copies are generated for all possible combinations of non-overlapping mentions. For dealing with discontiguous mentions, each annotated entity is assigned an id, uniquely identifying the mention within the sentence. This id is added as an extra attribute to each token, represented as an extra column in the tab separated IOB file format for the NE tagger. We processed with the Tanl pipeline (Attardi et al., 2009; Attardi et al., 2010). We first extracted the text from the training corpus in XML format and added the mentions annotations as tags enclosing them, with spans and mentions id as attributes. We then applied sentence splitting, tokenization, PoS tagging and dependency parsing using DeSR (Attardi, 2006). The tags were converted to IOB format. Here are two sample tokens in the resulting annotation, with attributes id, form, pos, head, deprel, entity, entity id: 2 http://en.wikipedia.org/wiki/Inside_Outside_Beginning 755 1 Abdomen NNP 2 SBJ B-DISO 1 ... 5 nontender NN 10 NMOD B-DISO 1 3 Named Entity Tagging The core o</context>
</contexts>
<marker>Attardi, Rossi, Simi, 2010</marker>
<rawString>Giuseppe Attardi, Stefano Dei Rossi and Maria Simi. 2010. TANL-1: Coreference Resolution by Parse Analysis and Similarity Clustering. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval 2010, Uppsala, Sweden, pages 108-111</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Bodenreider</author>
</authors>
<title>The Unified Medical Language System (UMLS): integrating biomedical terminology.</title>
<date>2004</date>
<journal>Nucleic Acids Research,</journal>
<volume>32</volume>
<pages>267--270</pages>
<contexts>
<context position="3394" citStr="Bodenreider, 2004" startWordPosition="505" endWordPosition="507">up disorders; B. mapping of each disorder mention to a unique UMLS CUI (Concept Unique Identifiers). The challenge organizers provided the following resources: • A training corpus of clinical notes from MIMIC II database manually annotated for disorder mentions and normalized to an UMLS CUI, consisting of 9432 sentences, with 5816 annotations. • A collection of unannotated notes, consisting of 1,611,080 sentences. 754 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 754–760, Dublin, Ireland, August 23-24, 2014. We also had access to the UMLS ontology (Bodenreider, 2004). Our approach to portion A of the task was to adapt a sequence labeller, which provides good accuracy in Named Entity recognition in the newswire domain, to handle the peculiarities of the clinical domain. We performed mention recognition in two steps: 1. identifying contiguous portions of a mention; 2. combining separated portions of mentions into a full mention. In order to use a traditional sequence tagger for the first step, we had to convert the input data into a suitable format, in particular, we dealt with nested mentions by transforming them into nonoverlapping sequences, through repl</context>
</contexts>
<marker>Bodenreider, 2004</marker>
<rawString>Olivier Bodenreider. 2004. The Unified Medical Language System (UMLS): integrating biomedical terminology. Nucleic Acids Research, vol. 32, no. supplement 1, pages D267–D270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Natural Language Processing (Almost) from Scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2461--2505</pages>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert et al. 2011. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12, pages 2461–2505.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shipra Dingare</author>
<author>Malvina Nissim</author>
<author>Jenny Finkel</author>
<author>Christopher Manning</author>
<author>Claire Grover</author>
</authors>
<title>A System for Identifying Named Entities</title>
<date>2005</date>
<booktitle>in Biomedical Text: how Results From two Evaluations Reflect on Both the System and the Evaluations. Comp Funct Genomics. Feb-Mar;</booktitle>
<pages>6--1</pages>
<contexts>
<context position="13267" citStr="Dingare et al. (2005)" startWordPosition="2125" endWordPosition="2128">8 nlpnet 80.29 62.51 70.29 Stanford 80.30 64.89 71.78 CRFsuite 79.69 61.97 69.72 Table 3. Accuracy of various NE taggers on the development set. Based on these results we chose the Tanl tagger and the Stanford NER for our submitted runs. All these taggers are known to be capable of achieving state of the art performance or close to it (89.57 F1) in the CoNLL 2003 shared task on the WSJ Penn Treebank. The accuracy on the current benchmark is much lower, despite the fact that there is only one category and the terminology for disorders is drawn from a restricted vocabulary. It has been noted by Dingare et al. (2005) that NER over biomedical texts achieves lower accuracy compared to other domains, quite within the range of the above results. Indeed, compared with the newswire domain or other domains, the entities in the biomedical domain tend to be more complex, without the distinctive shape features of the newswire categories. 4 Discontiguous mentions Discontiguous mention detection can be formulated as a problem of deciding whether two contiguous mentions belong to the same mention. As such, it can be cast into a classification problem. A similar approach was used successfully for the coreference resolu</context>
</contexts>
<marker>Dingare, Nissim, Finkel, Manning, Grover, 2005</marker>
<rawString>Shipra Dingare, Malvina Nissim, Jenny Finkel, Christopher Manning and Claire Grover. 2005. A System for Identifying Named Entities in Biomedical Text: how Results From two Evaluations Reflect on Both the System and the Evaluations. Comp Funct Genomics. Feb-Mar; 6(1-2): pages 77–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Ester</author>
</authors>
<title>A density-based algorithm for discovering clusters in large spatial databases with noise.</title>
<date>1996</date>
<booktitle>In Proceedings of 2nd International Conference on Knowledge Discovery and Data Miing, KDD 96,</booktitle>
<pages>226--231</pages>
<marker>Ester, 1996</marker>
<rawString>Martin Ester, et al. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proceedings of 2nd International Conference on Knowledge Discovery and Data Miing, KDD 96, pages 226–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
</authors>
<title>Trond Grenager and Christopher Manning 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<marker>Finkel, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager and Christopher Manning 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics, 2005, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil Fraser</author>
</authors>
<title>Diff, Match and Patch libraries for Plain Text. (Based on Myer&apos;s diff algorithm).</title>
<date>2011</date>
<marker>Fraser, 2011</marker>
<rawString>Neil Fraser. 2011. Diff, Match and Patch libraries for Plain Text. (Based on Myer&apos;s diff algorithm).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Part-Of-Speech Tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference, EMNLP ’96,</booktitle>
<pages>17--18</pages>
<contexts>
<context position="14009" citStr="Ratnaparkhi, 1996" startWordPosition="2244" endWordPosition="2245">ts. Indeed, compared with the newswire domain or other domains, the entities in the biomedical domain tend to be more complex, without the distinctive shape features of the newswire categories. 4 Discontiguous mentions Discontiguous mention detection can be formulated as a problem of deciding whether two contiguous mentions belong to the same mention. As such, it can be cast into a classification problem. A similar approach was used successfully for the coreference resolution task at SemEval 2010 (Attardi, Dei Rossi et al., 2010) 4.1 Mentions detection We trained a Maximum Entropy classifier (Ratnaparkhi, 1996) to recognize whether two terms belong to the same mention. The training instances for the pair-wise learner consist of each pair of terms within a sentence annotated as disorders. A positive instance is created if the terms belong to the same mention, negative otherwise. The classifier was trained using the following features, extracted for each pair of words for diseases. Distance features • Token distance: quantized distance between the two words; • Ancestor distance: quantized distance between the words in the parse tree if one is the ancestor of the other Syntax features • Head: whether t</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-Of-Speech Tagger. In Proceedings of the Empirical Methods in Natural Language Processing Conference, EMNLP ’96, pages 17-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Buzhou Tang</author>
<author>Hongxin Cao</author>
<author>Xiaolong Wang</author>
<author>Qingcai Chen</author>
<author>Hua Xu</author>
</authors>
<title>Evaluating Word Representation Features in Biomedical Named Entity Recognition Tasks.</title>
<date>2014</date>
<booktitle>BioMed Research International, Volume 2014, Article ID</booktitle>
<pages>240403</pages>
<contexts>
<context position="10218" citStr="Tang et al. (2014)" startWordPosition="1605" endWordPosition="1608">ns before and after the current token. Finally attribute features are extracted from attributes (Form, PoS, Lemma, NE, Disease) of surrounding tokens, denoted by their relative position to the current token. The best combination of Attribute features obtained with runs on the development set was the following: Feature Tokens POS[0] wi-2 wi–1 wi wi+1 DISEASE wi wi+1 wi+2 Table 1. Attribute features used in the runs. 3.2 Word Embeddings We explored ways to use the unannotated data in NE recognition by exploiting word embeddings (Collobert et al, 2011). In a paper published after our submission, Tang et al. (2014) show that word embeddings are beneficial to Biomedical NER. We used the word embeddings for 100,000 terms created through deep learning on the English Wikipedia by Al-Rfou et al. (2013). We then built, with the same procedure, embedding for terms from the supplied unlabelled data. The corpus was split, tokenized and normalized and a vocabulary was created with the most frequent words not already present among the Wikipedia word embeddings. Four versions of the embeddings were created, varying the size of the vocabulary and the size of the context window, as described in Table 1. Run1 Run2 Run</context>
<context position="12561" citStr="Tang et al., 2014" startWordPosition="2002" endWordPosition="2005">anford Named Entity Recognizer. This tagger is based on the Conditional Random Fields (CRF) statistical model and uses Gibbs sampling instead of other dynamic programming techniques for inference on sequence models (Finkel et al., 2005). This tagger normally works well enough using just the form of tokens as feature and we applied it so. 3.4 NER accuracy We report the accuracy of the various NE taggers we tested on the development set, using the scorer from the CoNLL Shared Task 2003 (Tjong Kim Sang and De Meulder, 2003). We include here also the results with CRFsuite, the CRF tagger used in (Tang et al., 2014). NER Precision Recall F- score Tanl 80.41 65.08 71.94 Tanl+clusters 80.43 64.48 71.58 nlpnet 80.29 62.51 70.29 Stanford 80.30 64.89 71.78 CRFsuite 79.69 61.97 69.72 Table 3. Accuracy of various NE taggers on the development set. Based on these results we chose the Tanl tagger and the Stanford NER for our submitted runs. All these taggers are known to be capable of achieving state of the art performance or close to it (89.57 F1) in the CoNLL 2003 shared task on the WSJ Penn Treebank. The accuracy on the current benchmark is much lower, despite the fact that there is only one category and the t</context>
</contexts>
<marker>Tang, Cao, Wang, Chen, Xu, 2014</marker>
<rawString>Buzhou Tang, Hongxin Cao, Xiaolong Wang, Qingcai Chen, and Hua Xu. 2014. Evaluating Word Representation Features in Biomedical Named Entity Recognition Tasks. BioMed Research International, Volume 2014, Article ID 240403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder</title>
<date>2003</date>
<booktitle>Proceedings of CoNLL ‘03,</booktitle>
<pages>142--147</pages>
<location>Edmonton, Canada,</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder 2003. Introduction to the CoNLL ‘03 Shared Task: Language-Independent Named Entity Recognition. In: Proceedings of CoNLL ‘03, Edmonton, Canada, pages 142-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SENNA</author>
</authors>
<title>http://ml.nec-labs.com/senna/ nlpnet.</title>
<date>2011</date>
<note>https://github.com/attardi/nlpnet</note>
<contexts>
<context position="11115" citStr="SENNA, 2011" startWordPosition="1762" endWordPosition="1763">e corpus was split, tokenized and normalized and a vocabulary was created with the most frequent words not already present among the Wikipedia word embeddings. Four versions of the embeddings were created, varying the size of the vocabulary and the size of the context window, as described in Table 1. Run1 Run2 Run3 Run4 Vocabulary size 50,000 50,000 30,000 30,000 Context 5 2 5 2 Hidden Layers 32 32 32 32 Learning Rate 0.1 0.1 0.1 0.1 Embedding size 64 64 64 64 Table 2. Word Embedding Parameters. We developed and trained a Deep Learning NE tagger (nlpnet, 2014) based on the SENNA architecture (SENNA, 2011) using these word embeddings. As an alternative to using the embeddings directly as features, we created clusters of word embeddings using the Dbscan algorithm (Ester et al., 1996) implemented in the sklearn library. We carried out several experiments, varying the parameters of the algorithm. The configuration that produced the largest number of clusters had 572 clusters. The clusters turned out not to be much significant, since a single cluster had about 29,000 words, another had 5,000 words, and the others had few, unusual words. We added the clusters as a dictionary feature to our NE tagger</context>
</contexts>
<marker>SENNA, 2011</marker>
<rawString>SENNA. 2011. http://ml.nec-labs.com/senna/ nlpnet. 2014. https://github.com/attardi/nlpnet</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>