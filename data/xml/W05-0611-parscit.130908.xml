<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001496">
<title confidence="0.973662">
Improving sequence segmentation learning by predicting trigrams
</title>
<author confidence="0.710291">
Antal van den Bosch
</author>
<affiliation confidence="0.871279">
ILK / Computational Linguistics and AI
Tilburg University
Tilburg, The Netherlands
</affiliation>
<email confidence="0.992189">
Antal.vdnBosch@uvt.nl
</email>
<sectionHeader confidence="0.993739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999803142857143">
Symbolic machine-learning classifiers are
known to suffer from near-sightedness
when performing sequence segmentation
(chunking) tasks in natural language pro-
cessing: without special architectural ad-
ditions they are oblivious of the decisions
they made earlier when making new ones.
We introduce a new pointwise-prediction
single-classifier method that predicts tri-
grams of class labels on the basis of win-
dowed input sequences, and uses a simple
voting mechanism to decide on the labels
in the final output sequence. We apply
the method to maximum-entropy, sparse-
winnow, and memory-based classifiers us-
ing three different sentence-level chunk-
ing tasks, and show that the method is able
to boost generalization performance in
most experiments, attaining error reduc-
tions of up to 51%. We compare and com-
bine the method with two known alterna-
tive methods to combat near-sightedness,
viz. a feedback-loop method and a stack-
ing method, using the memory-based clas-
sifier. The combination with a feedback
loop suffers from the label bias problem,
while the combination with a stacking
method produces the best overall results.
</bodyText>
<sectionHeader confidence="0.929837" genericHeader="method">
1 Optimizing output sequences
</sectionHeader>
<bodyText confidence="0.9995262">
Many tasks in natural language processing have the
full sentence as their domain. Chunking tasks, for
example, deal with segmenting the full sentence into
chunks of some type, for example constituents or
named entities, and possibly labeling each identified
</bodyText>
<page confidence="0.979782">
80
</page>
<author confidence="0.861807">
Walter Daelemans
</author>
<affiliation confidence="0.910124666666667">
CNTS, Department of Linguistics
University of Antwerp
Antwerp, Belgium
</affiliation>
<email confidence="0.874723">
Walter.Daelemans@ua.ac.be
</email>
<figureCaption confidence="0.982409">
Figure 1: Standard windowing process. Sequences
</figureCaption>
<bodyText confidence="0.946691">
of input symbols and output symbols are converted
into windows of fixed-width input symbols each as-
sociated with one output symbol.
chunk. The latter typically involves disambigua-
tion among alternative labels (e.g. syntactic role la-
beling, or semantic type assignment). Both tasks,
whether seen as separate tasks or as one, involve the
use of contextual knowledge from the available in-
put (e.g. words with part-of-speech tags), but also
the coordination of segmentations and disambigua-
tions over the sentence as a whole.
Many machine-learning approaches to chunking
tasks use windowing, a standard representational ap-
proach to generate cases that can be sequentially
processed. Each case produces one element of the
output sequence. The simplest method to process
these cases is that each case is classified in isolation,
generating a so-called point-wise prediction; the se-
quence of subsequent predictions can be concate-
nated to form the entire output analysis of the sen-
tence. Within a window, fixed-width subsequences
of adjacent input symbols, representing a certain
contextual scope, are mapped to one output symbol,
typically associated with one of the input symbols,
for example the middle one. Figure 1 displays this
standard version of the windowing process.
</bodyText>
<note confidence="0.9451175">
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 80–87, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999944893617022">
The fact that the point-wise classifier is only
trained to associate subsequences of input symbols
to single output symbols as accurately as possible
is a problematic restriction: it may easily cause the
classifier to produce invalid or impossible output se-
quences, since it is incapable of taking into account
any decisions it has made earlier. This well-known
problem has triggered at least the following three
main types of solutions.
Feedback loop Each training or test example may
represent not only the regular windowed input, but
also a copy of previously made classifications, to al-
low the classifier to be more consistent with its pre-
vious decisions. Direct feedback loops that copy
a predicted output label to the input representa-
tion of the next example have been used in sym-
bolic machine-learning architectures such as the the
maximum-entropy tagger described by Ratnaparkhi
(1996) and the memory-based tagger (MBT) pro-
posed by Daelemans et al. (1996). This solution as-
sumes that processing is directed, e.g. from left to
right. A noted problem of this approach is the label
bias problem (Lafferty et al., 2001), which is that a
feedback-loop classifier may be driven to be consis-
tent with its previous decision also in the case this
decision was wrong; sequences of errors may result.
Stacking, boosting, and voting The partly incor-
rect concatenated output sequence of a single classi-
fier may serve as input to a second-stage classifier in
a stacking architecture, a common machine-learning
optimization technique (Wolpert, 1992). Although
less elegant than a monolithic single-classifier ar-
chitecture, this method is known to be capable of
recognizing recurring errors of the first-stage clas-
sifier and correcting them (Veenstra, 1998). Boost-
ing (Freund and Schapire, 1996) has been applied to
optimize chunking systems (Carreras et al., 2002),
as well as voting over sets of different classifiers
(Florian et al., 2003). Punyakanok and Roth (2001)
present two methods for combining the predictions
of different classifiers according to constraints that
ensure that the resulting output is made more coher-
ent.
Output sequence optimization Rather than bas-
ing classifications only on model parameters esti-
mated from co-occurrences between input and out-
put symbols employed for maximizing the likeli-
hood of point-wise single-label predictions at the
output level, classifier output may be augmented by
an optimization over the output sequence as a whole
using optimization techniques such as beam search-
ing in the space of a conditional markov model’s
output (Ratnaparkhi, 1996) or hidden markov mod-
els (Skut and Brants, 1998). Maximum-entropy
markov models (McCallum et al., 2000) and con-
ditional random fields (Lafferty et al., 2001) opti-
mize the likelihood of segmentations of output sym-
bol sequences through variations of Viterbi search.
A non-stochastic, non-generative method for output
sequence optimization is presented by Argamon et
al. (1999), who propose a memory-based sequence
learner that finds alternative chunking analyses of a
sequence, and produces one best-guess analysis by a
tiling algorithm that finds an optimal joining of the
alternative analyses.
In this paper we introduce a symbolic machine-
learning method that can be likened to the ap-
proaches of the latter type of output sequence op-
timizers, but which does not perform a search in
a space of possible analyses. The approach is to
have a point-wise symbolic machine-learning clas-
sifier predict series of overlapping n-grams (in the
current study, trigrams) of class symbols, and have
a simple voting mechanism decide on the final out-
put sequence based on the overlapping predicted tri-
grams. We show that the approach has similar posi-
tive effects when applied to a memory-based classi-
fier and a maximum-entropy classifier, while yield-
ing mixed effects with a sparse-winnow classifier.
We then proceed to compare the trigram prediction
method to a feedback-loop method and a stacking
method applied using the memory-based classifier.
The three methods attain comparable error reduc-
tions. Finally, we combine the trigram-prediction
method with each of the two other methods. We
show that the combination of the trigram-prediction
method and the feedback-loop method does not
improve performance due to the label bias prob-
lem. In contrast, the combination of the trigram-
prediction method and the stacking method leads to
the overall best results, indicating that the latter two
methods solve complementary aspects of the near-
sightedness problem.
The structure of the paper is as follows. First,
</bodyText>
<page confidence="0.997527">
81
</page>
<bodyText confidence="0.993694111111111">
we introduce the three chunking sequence segmen-
tation tasks studied in this paper and explain the au-
tomatic algorithmic model selection method for the
three machine-learning classifiers used in our study,
in Section 2. The subsequent three sections report
on empirical results for the different methods pro-
posed for correcting the near-sightedness of classi-
fiers: the new class-trigrams method, a feedback-
loop approach in combination with single classes
and class trigrams, and two types of stacking in com-
bination with single classes and class trigrams. Sec-
tion 6 sums up and discusses the main results of the
comparison.
2 Data and methodology
The three data sets we used for this study repre-
sent a varied set of sentence-level chunking tasks
of both syntactic and semantic nature: English
base phrase chunking (henceforth CHUNK), En-
glish named-entity recognition (NER), and disflu-
ency chunking in transcribed spoken Dutch utter-
ances (DISFL).
CHUNK is the task of splitting sentences into
non-overlapping syntactic phrases or constituents.
The used data set, extracted from the WSJ Penn
Treebank, contains 211,727 training examples and
47,377 test instances. The examples represent
seven-word windows of words and their respective
(predicted) part-of-speech tags, and each example
is labeled with a class using the IOB type of seg-
mentation coding as introduced by Ramshaw and
Marcus (1995), marking whether the middle word
is inside (I), outside (O), or at the beginning (B)
of a chunk. Words occuring less than ten times in
the training material are attenuated (converted into a
more general string that retains some of the word’s
surface form). Generalization performance is mea-
sured by the F-score on correctly identified and la-
beled constituents in test data, using the evaluation
method originally used in the “shared task” sub-
event of the CoNLL-2000 conference (Tjong Kim
Sang and Buchholz, 2000) in which this particu-
lar training and test set were used. An example
sentence with base phrases marked and labeled is
the following: [He]NP [reckons]V P [the current account
deficit]NP [will narrow]V P [to]PP [only $ 1.8 billion]NP
[in]PP [September]NP .
NER, named-entity recognition, is to recognize
and type named entities in text. We employ the En-
glish NER shared task data set used in the CoNLL-
2003 conference, again using the same evaluation
method as originally used in the shared task (Tjong
Kim Sang and De Meulder, 2003). This data set
discriminates four name types: persons, organiza-
tions, locations, and a rest category of “miscellany
names”. The data set is a collection of newswire ar-
ticles from the Reuters Corpus, RCV11. The given
training set contains 203,621 examples; as test set
we use the “testb” evaluation set which contains
46,435 examples. Examples represent seven-word
windows of unattenuated words with their respec-
tive predicted part-of-speech tags. No other task-
specific features such as capitalization identifiers or
seed list features were used. Class labels use the
IOB segmentation coding coupled with the four pos-
sible name type labels. Analogous to the CHUNK
task, generalization performance is measured by the
F-score on correctly identified and labeled named
entities in test data. An example sentence with
the named entities segmented and typed is the fol-
lowing: [U.N.]organization official [Ekeus]person heads for
[Baghdad]location.
DISFL, disfluency chunking, is the task of rec-
ognizing subsequences of words in spoken utter-
ances such as fragmented words, laughter, self-
corrections, stammering, repetitions, abandoned
constituents, hesitations, and filled pauses, that are
not part of the syntactic core of the spoken utter-
ance. We use data introduced by Lendvai et al.
(2003), who extracted the data from a part of the
Spoken Dutch Corpus of spontaneous speech2 that
is both transcribed and syntactically annotated. All
words and multi-word subsequences judged not to
be part of the syntactic tree are defined as disfluent
chunks. We used a single 90% – 10% split of the
data, producing a training set of 303,385 examples
and a test set of 37,160 examples. Each example
represents a window of nine words (attenuated be-
low an occurrence threshold of 100) and 22 binary
features representing various string overlaps (to en-
code possible repetitions); for details, cf. (Lendvai
</bodyText>
<footnote confidence="0.993645">
1Reuters Corpus, Volume 1, English language, 1996-08-20
to 1997-08-19.
2CGN, Spoken Dutch Corpus, version 1.0,
http://lands.let.kun.nl/cgn/ehome.htm.
</footnote>
<page confidence="0.998726">
82
</page>
<bodyText confidence="0.99831928125">
et al., 2003). Generalization performance is mea-
sured by the F-score on correctly identified disfluent
chunks in test data. An example of a chunked Spo-
ken Dutch Corpus sentence is the following (“uh” is
a filled pause; without the disfluencies, the sentence
means “I have followed this process with a certain
amount of scepticism for about a year”): [ik uh] ik heb
met de nodige scepsis [uh] deze gang van zaken [zo’n]
zo’n jaar aangekeken.
We perform our experiments on the three tasks us-
ing three machine-learning algorithms: the memory-
based learning or k-nearest neighbor algorithm as
implemented in the TiMBL software package (ver-
sion 5.1) (Daelemans et al., 2004), henceforth re-
ferred to as MBL; maximum-entropy classification
(Guiasu and Shenitzer, 1985) as implemented in
the maxent software package (version 20040930)
by Zhang Le3, henceforth MAXENT; and a sparse-
winnow network (Littlestone, 1988) as implemented
in the SNoW software package (version 3.0.5) by
Carlson et al. (1999), henceforth WINNOW. All
three algorithms have algorithmic parameters that
bias their performance; to allow for a fair compar-
ison we optimized each algorithm on each task us-
ing wrapped progressive sampling (Van den Bosch,
2004) (WPS), a heuristic automatic procedure that,
on the basis of validation experiments internal to
the training material, searches among algorithmic
parameter combinations for a combination likely to
yield optimal generalization performance on unseen
data. We used wrapped progressive sampling in all
experiments.
</bodyText>
<sectionHeader confidence="0.985168" genericHeader="method">
3 Predicting class trigrams
</sectionHeader>
<bodyText confidence="0.999968636363636">
There is no intrinsic bound to what is packed into
a class label associated to a windowed example.
For example, complex class labels can span over
trigrams of singular class labels. A classifier that
learns to produce trigrams of class labels will at least
produce syntactically valid trigrams from the train-
ing material, which might partly solve some near-
sightedness problems of the single-class classifier.
Although simple and appealing, the lurking disad-
vantage of the trigram idea is that the number of
class labels increases explosively when moving from
</bodyText>
<footnote confidence="0.989830666666667">
3Maximum Entropy Modeling Toolkit for Python
and C++, http://homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html.
</footnote>
<figureCaption confidence="0.912875">
Figure 2: Windowing process with trigrams of class
symbols. Sequences of input symbols and output
symbols are converted into windows of fixed-width
input symbols each associated with, in this example,
trigrams of output symbols.
</figureCaption>
<bodyText confidence="0.96889075862069">
single class labels to wider trigrams. The CHUNK
data, for example, has 22 classes (“IOB” codes as-
sociated with chunk types); in the same training set,
846 different trigrams of these 22 classes and the
start/end context symbol occur. The eight original
classes of NER combine to 138 occurring trigrams.
DISFL only has two classes, but 18 trigram classes.
Figure 2 illustrates the procedure by which win-
dows are created with, as an example, class trigrams.
Each windowed instance maps to a class label that
incorporates three atomic class labels, namely the
focus class label that was the original unigram label,
plus its immediate left and right neighboring class
labels.
While creating instances this way is trivial, it is
not entirely trivial how the output of overlapping
class trigrams recombines into a normal string of
class sequences. When the example illustrated in
Figure 2 is followed, each single class label in the
output sequence is effectively predicted three times;
first, as the right label of a trigram, next as the mid-
dle label, and finally as the left label. Although
it would be possible to avoid overlaps and classify
only every three words, there is an interesting prop-
erty of overlapping class label n-grams: it is pos-
sible to vote over them. To pursue our example of
trigram classes, the following voting procedure can
be followed to decide about the resulting unigram
class label sequence:
</bodyText>
<listItem confidence="0.994182666666667">
1. When all three votes are unanimous, their com-
mon class label is returned;
2. When two out of three votes are for the same
</listItem>
<page confidence="0.992089">
83
</page>
<table confidence="0.9825754">
MBL MAXENT WINNOW
Task Baseline Trigram red. Baseline Trigram red. Baseline Trigram red.
CHUNK 91.9 92.7 10 90.3 91.9 17 89.5 88.3 -11
NER 77.2 80.2 17 47.5 74.5 51 68.9 70.1 4
DISFL 77.9 81.7 17 75.3 80.7 22 70.5 65.3 -17
</table>
<tableCaption confidence="0.99944">
Table 1: Comparison of generalization performances of three machine-learning algorithms in terms of F-
</tableCaption>
<bodyText confidence="0.976727483870968">
score on the three test sets without and with class trigrams. Each third column displays the error reduction
in F-score by the class trigrams method over the other method. The best performances per task are printed
in bold.
class label, this class label is returned;
3. When all three votes disagree (i.e., when ma-
jority voting ties), the class label is returned of
which the classifier is most confident.
Classifier confidence, needed for the third tie-
breaking rule, can be heuristically estimated by tak-
ing the distance of the nearest neighbor in MBL, the
estimated probability value of the most likely class
produced by the MAXENT classifier, or the activa-
tion level of the most active unit of the WINNOW
network.
Clearly this scheme is one out of many possible
schemes, using variants of voting as well as variants
of n (and having multiple classifiers with different n,
so that some back-off procedure could be followed).
For now we use this procedure with trigrams as an
example. To measure its effect we apply it to the se-
quence tasks CHUNK, NER, and DISFL. The results
of this experiment, where in each case WPS was used
to find optimal algorithmic parameters of all three
algorithms, are listed in Table 1. We find rather posi-
tive effects of the trigram method both with MBL and
MAXENT; we observe relative error reductions in the
F-score on chunking ranging between 10% and a re-
markable 51% error reduction, with MAXENT on the
NER task. With WINNOW, we observe decreases in
performance on CHUNK and DISFL, and a minor er-
ror reduction of 4% on NER.
</bodyText>
<sectionHeader confidence="0.991753" genericHeader="method">
4 The feedback-loop method versus class
trigrams
</sectionHeader>
<bodyText confidence="0.9998798">
An alternative method for providing a classifier ac-
cess to its previous decisions is a feedback-loop ap-
proach, which extends the windowing approach by
feeding previous decisions of the classifier as fea-
tures into the current input of the classifier. This
</bodyText>
<table confidence="0.99961675">
Task Baseline Feedback Trigrams Feed+Tri
CHUNK 91.9 93.0 92.7 89.8
NER 77.2 78.1 80.2 77.5
DISFL 77.9 78.6 81.7 79.1
</table>
<tableCaption confidence="0.998192">
Table 2: Comparison of generalization perfor-
</tableCaption>
<bodyText confidence="0.967652115384615">
mances in terms of F-score of MBL on the three test
sets, with and without a feedback loop, and the error
reduction attained by the feedback-loop method, the
F-score of the trigram-class method, and the F-score
of the combination of the two methods.
approach was proposed in the context of memory-
based learning for part-of-speech tagging as MBT
(Daelemans et al., 1996). The number of decisions
fed back into the input can be varied. In the exper-
iments described here, the feedback loop iteratively
updates a memory of the three most recent predic-
tions.
The feedback-loop approach can be combined
both with single class and class trigram output. In
the latter case, the full trigram class labels are copied
to the input, retaining at any time the three most re-
cently predicted labels in the input. Table 2 shows
the results for both options on the three chunking
tasks. The feedback-loop method outperforms the
trigram-class method on CHUNK, but not on the
other two tasks. It does consistently outperform
the baseline single-class classifier. Interestingly, the
combination of the two methods performs worse
than the baseline classifier on CHUNK, and also per-
forms worse than the trigram-class method on the
other two tasks.
</bodyText>
<page confidence="0.998525">
84
</page>
<figureCaption confidence="0.983828">
Figure 3: The windowing process after a first-stage
</figureCaption>
<bodyText confidence="0.936745833333333">
classifier has produced a predicted output sequence.
Sequences of input symbols, predicted output sym-
bols, and real output symbols are converted into win-
dows of fixed-width input symbols and predicted
output symbols, each associated with one output
symbol.
</bodyText>
<sectionHeader confidence="0.905232" genericHeader="method">
5 Stacking versus class trigrams
</sectionHeader>
<bodyText confidence="0.999691230769231">
Stacking, a term popularized by Wolpert (1992) in
an artificial neural network context, refers to a class
of meta-learning systems that learn to correct er-
rors made by lower-level classifiers. We implement
stacking by adding a windowed sequence of previ-
ous and subsequent output class labels to the origi-
nal input features (here, we copy a window of seven
predictions to the input, centered around the middle
position), and providing these enriched examples as
training material to a second-stage classifier. Fig-
ure 3 illustrates the procedure. Given the (possibly
erroneous) output of a first classifier on an input se-
quence, a certain window of class symbols from that
predicted sequence is copied to the input, to act as
predictive features for the real class label.
To generate the output of a first-stage classifier,
two options are open. We name these options per-
fect and adaptive. They differ in the way they create
training material for the second-stage classifier:
Perfect – the training material is created straight
from the training material of the first-stage classi-
fier, by windowing over the real class sequences.
In doing so, the class label of each window is ex-
cluded from the input window, since it is always
the same as the class to be predicted. In training,
this focus feature would receive an unrealistically
</bodyText>
<table confidence="0.9525064">
Perfect Adaptive
Task Baseline stacking stacking
CHUNK 91.9 92.0 92.6
NER 77.2 78.3 78.9
DISFL 77.9 80.5 81.6
</table>
<tableCaption confidence="0.999566">
Table 3: Comparison of generalization perfor-
</tableCaption>
<bodyText confidence="0.966102282051282">
mances in terms of F-score of MBL on the three test
sets, without stacking, and with perfect and adaptive
stacking.
high weight, especially considering that in testing
this feature would contain errors. To assign a very
high weight to a feature that may contain an erro-
neous value does not seem a good idea in view of
the label bias problem.
Adaptive – the training material is created in-
directly by running an internal 10-fold cross-
validation experiment on the first-stage training set,
concatenating the predicted output class labels on all
of the ten test partitions, and converting this out-
put to class windows. In contrast with the perfect
variant, we do include the focus class feature in the
copied class label window. The adaptive approach
can in principle learn from recurring classification
errors in the input, and predict the correct class in
case an error re-occurs.
Table 3 lists the comparative results on the
CHUNK, NER, and DISFL tasks introduced earlier.
They show that both types of stacking improve per-
formance on the three tasks, and that the adaptive
stacking variant produces higher relative gains than
the perfect variant; in terms of error reduction in F-
score as compared to the baseline single-class clas-
sifier, the gains are 9% for CHUNK, 7% for NER,
and 17% for DISFL. There appears to be more use-
ful information in training data derived from cross-
validated output with errors, than in training data
with error-free material.
Stacking and class trigrams can be combined.
One possible straightforward combination is that of
a first-stage classifier that predicts trigrams, and a
second-stage stacked classifier that also predicts tri-
grams (we use the adaptive variant, since it produced
the best results), while including a centered seven-
positions-wide window of first-stage trigram class
labels in the input. Table 4 compares the results
</bodyText>
<page confidence="0.999179">
85
</page>
<table confidence="0.9902074">
Adaptive
Task stacking Trigram Combination
CHUNK 92.6 92.8 93.1
NER 78.9 80.2 80.6
DISFL 81.6 81.7 81.9
</table>
<tableCaption confidence="0.8030345">
Table 4: Comparison of generalization perfor-
mances in terms of F-score by MBL on the three test
sets, with adaptive stacking, trigram classes, and the
combination of the two.
</tableCaption>
<bodyText confidence="0.999842347826087">
of adaptive stacking and trigram classes with those
of the combination of the two. As can be seen, the
combination produces even better results than both
the stacking and the trigram-class methods individ-
ually, on all three tasks. Compared to the baseline
single-class classifier, the error reductions are 15%
for CHUNK, 15% for NER, and 18% for DISFL.
As an additional analysis, we inspected the pre-
dictions made by the trigram-class method and its
combinations with the stacking and the feedback-
loop methods on the CHUNK task to obtain a bet-
ter view on the amount of disagreements between
the trigrams. We found that with the trigram-class
method, in 6.3% of all votes some disagreement
among the overlapping trigrams occurs. A slightly
higher percentage of disagreements, 7.1%, is ob-
served with the combination of the trigram-class and
the stacking method. Interestingly, in the combina-
tion of the trigram-class and feedback-loop methods,
only 0.1% of all trigram votes are not unanimous.
This clearly illustrates that in the latter combination
the resulting sequence of trigrams is internally very
consistent – also in its errors.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999974593220339">
Classifiers trained on chunking tasks that make iso-
lated. near-sighted decisions on output symbols and
that do not optimize the resulting output sequences
afterwards or internally through a feedback loop,
tend to produce weak models for sequence process-
ing tasks. To combat this weakness, we have pro-
posed a new method that uses a single symbolic
machine-learning classifier predicting trigrams of
classes, using a simple voting mechanism to reduce
the sequence of predicted overlapping trigrams to a
sequence of single output symbols. Compared to
their near-sighted counterparts, error reductions are
attained of 10 to 51% with MBL and MAXENT on
three chunking tasks. We found weaker results with
a WINNOW classifier, suggesting that the latter is
more sensitive to the division of the class space in
more classes, likely due to the relatively sparser co-
occurrences between feature values and class labels
on which WINNOW network connection weights are
based.
We have contrasted the trigram-class method
against a feedback-loop method (MBT) and a stack-
ing method, all using a memory-based classifier
(but the methods generalize to any machine-learning
classifier). With the feedback-loop method, modest
error reductions of 3%, 4%, and 17% are measured;
stacking attains comparable improvements of 7%,
9%, and 17% error reductions in the chunking F-
score. We then combined the trigram-class method
with the two other methods. The combination with
the feedback-loop system led to relatively low per-
formance results. A closer analysis indicated that
the two methods appear to render each other ineffec-
tive: by feeding back predicted trigrams in the input,
the classifier is very much geared towards predicting
a next trigram that will be in accordance with the
two partly overlapping trigrams in the input, as sug-
gested by overwhelming evidence in this direction
in training material – this problem is also known as
the label bias problem (Lafferty et al., 2001). (The
fact that maximum-entropy markov models also suf-
fer from this problem prompted Lafferty et al. to
propose conditional random fields.)
We also observed that the positive effects of the
trigram-class and stacking variants do not mute each
other when combined. The overall highest error re-
ductions are attained with the combination: 15%
for CHUNK, 15% for NER, and 18% for DISFL.
The combination of the two methods solve more er-
rors than the individual methods do. Apparently,
they both introduce complementary disagreements
in overlapping trigrams, which the simple voting
mechanism can convert to more correct predictions
than the two methods do individually.
Further research should focus on a deep quan-
titative and qualitative analysis of the different er-
rors the different methods correct when compared
to the baseline single-class classifier, as well as
the errors they may introduce. Alternatives to the
</bodyText>
<page confidence="0.988306">
86
</page>
<bodyText confidence="0.999983384615385">
IOB-style encoding should also be incorporated in
these experiments (Tjong Kim Sang, 2000). Ad-
ditionally, a broader comparison with point-wise
predictors (Kashima and Tsuboi, 2004) as well as
Viterbi-based probabilistic models (McCallum et al.,
2000; Lafferty et al., 2001; Sha and Pereira, 2003)
in large-scale comparative studies is warranted.
Also, the scope of the study may be broadened to
all sequential language processing tasks, including
tasks in which no segmentation takes place (e.g.
part-of-speech tagging), and tasks at the morpho-
phonological level (e.g. grapheme-phoneme conver-
sion and morphological analysis).
</bodyText>
<sectionHeader confidence="0.99628" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999442">
The authors wish to thank Sander Canisius for dis-
cussions and suggestions. The work of the first au-
thor is funded by NWO, the Netherlands Organi-
sation for Scientific Research; the second author’s
work is partially funded by the EU BioMinT project.
</bodyText>
<sectionHeader confidence="0.998081" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999770932584269">
S. Argamon, I. Dagan, and Y. Krymolowski. 1999. A
memory-based approach to learning shallow natural
language patterns. Journal ofExperimental and Theo-
retical Artificial Intelligence, 10:1–22.
A. J. Carlson, C. M. Cumby, J. L. Rosen, and D. Roth.
1999. Snow user guide. Technical Report UIUCDCS-
R-99-2101, Cognitive Computation Group, Computer
Science Department, University of Illinois, Urbana,
Illinois.
X. Carreras, L. M`arques, and L. Padr´o. 2002. Named
entity extraction using AdaBoost. In Proceedings of
CoNLL-2002, pages 167–170. Taipei, Taiwan.
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996.
MBT: A memory-based part of speech tagger genera-
tor. In E. Ejerhed and I. Dagan, editors, Proceedings
of WVLC, pages 14–27. ACL SIGDAT.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2004. TiMBL: Tilburg memory based learner,
version 5.1.0, reference guide. Technical Report ILK
04-02, ILK Research Group, Tilburg University.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In W. Daelemans and M. Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 168–171. Edmonton,
Canada.
Y. Freund and R. E. Schapire. 1996. Experiments with a
new boosting algorithm. In L. Saitta, editor, Proceed-
ings of ICML-96, pages 148–156, San Francisco, CA.
Morgan Kaufmann.
S. Guiasu and A. Shenitzer. 1985. The principle of max-
imum entropy. The Mathematical Intelligencer, 7(1).
H. Kashima and Y. Tsuboi. 2004. Kernel-based discrim-
inative learning algorithms for labeling sequences,
trees and graphs. In Proceedings of ICML-2004,
Banff, Canada.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
ofICML-01, Williamstown, MA.
P. Lendvai, A. van den Bosch, and E. Krahmer. 2003.
Memory-based disfluency chunking. In Proceedings
ofDISS’03), Gothenburg, Sweden, pages 63–66.
N. Littlestone. 1988. Learning quickly when irrelevant
attributes abound: A new linear-threshold algorithm.
Machine Learning, 2:285–318.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proceedings of ICML-00,
Stanford, CA.
V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS-13; The 2000 Con-
ference on Advances in Neural Information Processing
Systems, pages 995–1001. The MIT Press.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of WVLC-95, Cambridge, MA, pages 82–94.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of EMNLP, May 17-18,
1996, University of Pennsylvania.
F. Sha and F. Pereira. 2003. Shallow parsing with Condi-
tional Random Fields. In Proceedings ofHLT-NAACL
2003, Edmonton, Canada.
W. Skut and T. Brants. 1998. Chunk tagger: statistical
recognition of noun phrases. In ESSLLI-1998 Work-
shop on Automated Acquisition of Syntax and Parsing.
E. Tjong Kim Sang and S. Buchholz. 2000. Introduction
to the CoNLL-2000 shared task: Chunking. In Pro-
ceedings of CoNLL-2000 and LLL-2000, pages 127–
132.
E. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In W. Daele-
mans and M. Osborne, editors, Proceedings of CoNLL-
2003, pages 142–147. Edmonton, Canada.
E. Tjong Kim Sang. 2000. Noun phrase recognition by
system combination. In Proceedings ofANLP-NAACL
2000, pages 50–55. Seattle, Washington, USA. Mor-
gan Kaufman Publishers.
A. van den Bosch. 2004. Wrapped progressive sampling
search for optimizing learning algorithm parameters.
In R. Verbrugge, N. Taatgen, and L. Schomaker, edi-
tors, Proceedings of the 16th Belgian-Dutch AI Con-
ference, pages 219–226, Groningen, The Netherlands.
J. Veenstra. 1998. Fast NP chunking using memory-
based learning techniques. In Proceedings of BENE-
LEARN’98, pages 71–78, Wageningen, The Nether-
lands.
D. H. Wolpert. 1992. Stacked Generalization. Neural
Networks, 5:241–259.
</reference>
<page confidence="0.999474">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000155">
<title confidence="0.999672">Improving sequence segmentation learning by predicting trigrams</title>
<author confidence="0.999714">Antal van_den</author>
<affiliation confidence="0.833145">ILK / Computational Linguistics and Tilburg</affiliation>
<address confidence="0.92781">Tilburg, The</address>
<email confidence="0.766626">Antal.vdnBosch@uvt.nl</email>
<abstract confidence="0.976270216216216">Symbolic machine-learning classifiers are known to suffer from near-sightedness when performing sequence segmentation (chunking) tasks in natural language processing: without special architectural additions they are oblivious of the decisions they made earlier when making new ones. We introduce a new pointwise-prediction single-classifier method that predicts trigrams of class labels on the basis of windowed input sequences, and uses a simple voting mechanism to decide on the labels in the final output sequence. We apply the method to maximum-entropy, sparsewinnow, and memory-based classifiers using three different sentence-level chunking tasks, and show that the method is able to boost generalization performance in most experiments, attaining error reductions of up to 51%. We compare and combine the method with two known alternative methods to combat near-sightedness, viz. a feedback-loop method and a stacking method, using the memory-based classifier. The combination with a feedback loop suffers from the label bias problem, while the combination with a stacking method produces the best overall results. 1 Optimizing output sequences Many tasks in natural language processing have the full sentence as their domain. Chunking tasks, for example, deal with segmenting the full sentence into chunks of some type, for example constituents or named entities, and possibly labeling each identified 80 Walter</abstract>
<affiliation confidence="0.758162333333333">CNTS, Department of University of Antwerp,</affiliation>
<email confidence="0.623764">Walter.Daelemans@ua.ac.be</email>
<abstract confidence="0.996930234567902">Figure 1: Standard windowing process. Sequences of input symbols and output symbols are converted into windows of fixed-width input symbols each associated with one output symbol. chunk. The latter typically involves disambiguation among alternative labels (e.g. syntactic role labeling, or semantic type assignment). Both tasks, whether seen as separate tasks or as one, involve the use of contextual knowledge from the available input (e.g. words with part-of-speech tags), but also the coordination of segmentations and disambiguations over the sentence as a whole. Many machine-learning approaches to chunking tasks use windowing, a standard representational approach to generate cases that can be sequentially processed. Each case produces one element of the output sequence. The simplest method to process these cases is that each case is classified in isolation, generating a so-called point-wise prediction; the sequence of subsequent predictions can be concatenated to form the entire output analysis of the sentence. Within a window, fixed-width subsequences of adjacent input symbols, representing a certain contextual scope, are mapped to one output symbol, typically associated with one of the input symbols, for example the middle one. Figure 1 displays this standard version of the windowing process. of the 9th Conference on Computational Natural Language Learning 80–87, Ann Arbor, June 2005. Association for Computational Linguistics The fact that the point-wise classifier is only trained to associate subsequences of input symbols to single output symbols as accurately as possible is a problematic restriction: it may easily cause the classifier to produce invalid or impossible output sequences, since it is incapable of taking into account any decisions it has made earlier. This well-known problem has triggered at least the following three main types of solutions. loop training or test example may represent not only the regular windowed input, but also a copy of previously made classifications, to allow the classifier to be more consistent with its previous decisions. Direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by Ratnaparkhi and the memory-based tagger proposed by Daelemans et al. (1996). This solution assumes that processing is directed, e.g. from left to A noted problem of this approach is the problem et al., 2001), which is that a feedback-loop classifier may be driven to be consistent with its previous decision also in the case this decision was wrong; sequences of errors may result. boosting, and voting partly incorrect concatenated output sequence of a single classifier may serve as input to a second-stage classifier in a stacking architecture, a common machine-learning optimization technique (Wolpert, 1992). Although less elegant than a monolithic single-classifier architecture, this method is known to be capable of recognizing recurring errors of the first-stage classifier and correcting them (Veenstra, 1998). Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al., 2002), as well as voting over sets of different classifiers (Florian et al., 2003). Punyakanok and Roth (2001) present two methods for combining the predictions of different classifiers according to constraints that ensure that the resulting output is made more coherent. sequence optimization than basing classifications only on model parameters estifrom co-occurrences between input and output symbols employed for maximizing the likelihood of point-wise single-label predictions at the output level, classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov model’s output (Ratnaparkhi, 1996) or hidden markov models (Skut and Brants, 1998). Maximum-entropy markov models (McCallum et al., 2000) and conditional random fields (Lafferty et al., 2001) optimize the likelihood of segmentations of output symbol sequences through variations of Viterbi search. A non-stochastic, non-generative method for output sequence optimization is presented by Argamon et al. (1999), who propose a memory-based sequence learner that finds alternative chunking analyses of a sequence, and produces one best-guess analysis by a tiling algorithm that finds an optimal joining of the alternative analyses. In this paper we introduce a symbolic machinelearning method that can be likened to the approaches of the latter type of output sequence optimizers, but which does not perform a search in a space of possible analyses. The approach is to have a point-wise symbolic machine-learning claspredict series of overlapping (in the current study, trigrams) of class symbols, and have a simple voting mechanism decide on the final output sequence based on the overlapping predicted trigrams. We show that the approach has similar positive effects when applied to a memory-based classifier and a maximum-entropy classifier, while yielding mixed effects with a sparse-winnow classifier. We then proceed to compare the trigram prediction method to a feedback-loop method and a stacking method applied using the memory-based classifier. The three methods attain comparable error reductions. Finally, we combine the trigram-prediction method with each of the two other methods. We show that the combination of the trigram-prediction method and the feedback-loop method does not improve performance due to the label bias problem. In contrast, the combination of the trigramprediction method and the stacking method leads to the overall best results, indicating that the latter two methods solve complementary aspects of the nearsightedness problem. The structure of the paper is as follows. First, 81 we introduce the three chunking sequence segmentation tasks studied in this paper and explain the automatic algorithmic model selection method for the three machine-learning classifiers used in our study, in Section 2. The subsequent three sections report on empirical results for the different methods proposed for correcting the near-sightedness of classifiers: the new class-trigrams method, a feedbackloop approach in combination with single classes and class trigrams, and two types of stacking in combination with single classes and class trigrams. Section 6 sums up and discusses the main results of the comparison. 2 Data and methodology The three data sets we used for this study represent a varied set of sentence-level chunking tasks of both syntactic and semantic nature: English phrase chunking (henceforth Ennamed-entity recognition and disfluency chunking in transcribed spoken Dutch utterthe task of splitting sentences into non-overlapping syntactic phrases or constituents. The used data set, extracted from the WSJ Penn Treebank, contains 211,727 training examples and 47,377 test instances. The examples represent seven-word windows of words and their respective (predicted) part-of-speech tags, and each example is labeled with a class using the IOB type of segmentation coding as introduced by Ramshaw and Marcus (1995), marking whether the middle word is inside (I), outside (O), or at the beginning (B) of a chunk. Words occuring less than ten times in the training material are attenuated (converted into a more general string that retains some of the word’s surface form). Generalization performance is measured by the F-score on correctly identified and labeled constituents in test data, using the evaluation method originally used in the “shared task” subevent of the CoNLL-2000 conference (Tjong Kim Sang and Buchholz, 2000) in which this particular training and test set were used. An example sentence with base phrases marked and labeled is following: Pcurrent account P$ 1.8 named-entity recognition, is to recognize and type named entities in text. We employ the Entask data set used in the CoNLL- 2003 conference, again using the same evaluation method as originally used in the shared task (Tjong Kim Sang and De Meulder, 2003). This data set discriminates four name types: persons, organizations, locations, and a rest category of “miscellany names”. The data set is a collection of newswire arfrom the Reuters Corpus, The given training set contains 203,621 examples; as test set we use the “testb” evaluation set which contains 46,435 examples. Examples represent seven-word windows of unattenuated words with their respective predicted part-of-speech tags. No other taskspecific features such as capitalization identifiers or seed list features were used. Class labels use the IOB segmentation coding coupled with the four posname type labels. Analogous to the task, generalization performance is measured by the F-score on correctly identified and labeled named entities in test data. An example sentence with the named entities segmented and typed is the folofficialheads for disfluency chunking, is the task of recognizing subsequences of words in spoken utterances such as fragmented words, laughter, selfcorrections, stammering, repetitions, abandoned constituents, hesitations, and filled pauses, that are not part of the syntactic core of the spoken utterance. We use data introduced by Lendvai et al. (2003), who extracted the data from a part of the Dutch Corpus of spontaneous that is both transcribed and syntactically annotated. All words and multi-word subsequences judged not to be part of the syntactic tree are defined as disfluent chunks. We used a single 90% – 10% split of the data, producing a training set of 303,385 examples and a test set of 37,160 examples. Each example represents a window of nine words (attenuated below an occurrence threshold of 100) and 22 binary features representing various string overlaps (to encode possible repetitions); for details, cf. (Lendvai Corpus, Volume 1, English language, 1996-08-20 to 1997-08-19. Spoken Dutch Corpus, version 1.0, 82 et al., 2003). Generalization performance is measured by the F-score on correctly identified disfluent chunks in test data. An example of a chunked Spoken Dutch Corpus sentence is the following (“uh” is a filled pause; without the disfluencies, the sentence means “I have followed this process with a certain of scepticism for about a year”): heb de nodige scepsis gang van zaken jaar We perform our experiments on the three tasks using three machine-learning algorithms: the memorylearning or neighbor algorithm as implemented in the TiMBL software package (version 5.1) (Daelemans et al., 2004), henceforth reto as maximum-entropy classification (Guiasu and Shenitzer, 1985) as implemented in the maxent software package (version 20040930) Zhang henceforth and a sparsewinnow network (Littlestone, 1988) as implemented in the SNoW software package (version 3.0.5) by et al. (1999), henceforth All three algorithms have algorithmic parameters that bias their performance; to allow for a fair comparison we optimized each algorithm on each task using wrapped progressive sampling (Van den Bosch, a heuristic automatic procedure that, on the basis of validation experiments internal to the training material, searches among algorithmic parameter combinations for a combination likely to yield optimal generalization performance on unseen data. We used wrapped progressive sampling in all experiments. 3 Predicting class trigrams There is no intrinsic bound to what is packed into a class label associated to a windowed example. For example, complex class labels can span over trigrams of singular class labels. A classifier that learns to produce trigrams of class labels will at least produce syntactically valid trigrams from the training material, which might partly solve some nearsightedness problems of the single-class classifier. Although simple and appealing, the lurking disadvantage of the trigram idea is that the number of class labels increases explosively when moving from Entropy Modeling Toolkit for Python C++, Figure 2: Windowing process with trigrams of class symbols. Sequences of input symbols and output symbols are converted into windows of fixed-width input symbols each associated with, in this example, trigrams of output symbols. class labels to wider trigrams. The data, for example, has 22 classes (“IOB” codes associated with chunk types); in the same training set, 846 different trigrams of these 22 classes and the start/end context symbol occur. The eight original of to 138 occurring trigrams. has two classes, but 18 trigram classes. Figure 2 illustrates the procedure by which windows are created with, as an example, class trigrams. Each windowed instance maps to a class label that incorporates three atomic class labels, namely the focus class label that was the original unigram label, plus its immediate left and right neighboring class labels. While creating instances this way is trivial, it is not entirely trivial how the output of overlapping class trigrams recombines into a normal string of class sequences. When the example illustrated in Figure 2 is followed, each single class label in the output sequence is effectively predicted three times; first, as the right label of a trigram, next as the middle label, and finally as the left label. Although it would be possible to avoid overlaps and classify only every three words, there is an interesting propof overlapping class label it is possible to vote over them. To pursue our example of trigram classes, the following voting procedure can be followed to decide about the resulting unigram class label sequence: 1. When all three votes are unanimous, their common class label is returned; 2. When two out of three votes are for the same 83 MBL MAXENT WINNOW Task Baseline Trigram red. Baseline Trigram red. Baseline Trigram red. CHUNK 91.9 92.7 10 90.3 91.9 17 89.5 88.3 -11 NER 77.2 80.2 17 47.5 74.5 51 68.9 70.1 4 DISFL 77.9 81.7 17 75.3 80.7 22 70.5 65.3 -17 Table 1: Comparison of generalization performances of three machine-learning algorithms in terms of Fscore on the three test sets without and with class trigrams. Each third column displays the error reduction in F-score by the class trigrams method over the other method. The best performances per task are printed in bold. class label, this class label is returned; 3. When all three votes disagree (i.e., when majority voting ties), the class label is returned of which the classifier is most confident. Classifier confidence, needed for the third tiebreaking rule, can be heuristically estimated by takthe distance of the nearest neighbor in the estimated probability value of the most likely class by the or the activalevel of the most active unit of the network. Clearly this scheme is one out of many possible schemes, using variants of voting as well as variants having multiple classifiers with different so that some back-off procedure could be followed). For now we use this procedure with trigrams as an example. To measure its effect we apply it to the setasks and The results this experiment, where in each case used to find optimal algorithmic parameters of all three algorithms, are listed in Table 1. We find rather posieffects of the trigram method both with we observe relative error reductions in the F-score on chunking ranging between 10% and a re- 51% error reduction, with the With we observe decreases in on and a minor erreduction of 4% on 4 The feedback-loop method versus class trigrams An alternative method for providing a classifier access to its previous decisions is a feedback-loop approach, which extends the windowing approach by feeding previous decisions of the classifier as features into the current input of the classifier. This Task Feedback Trigrams CHUNK 91.9 93.0 92.7 89.8 NER 77.2 78.1 80.2 77.5 DISFL 77.9 78.6 81.7 79.1 Table 2: Comparison of generalization perforin terms of F-score of the three test sets, with and without a feedback loop, and the error reduction attained by the feedback-loop method, the F-score of the trigram-class method, and the F-score of the combination of the two methods. approach was proposed in the context of memorylearning for part-of-speech tagging as (Daelemans et al., 1996). The number of decisions fed back into the input can be varied. In the experiments described here, the feedback loop iteratively updates a memory of the three most recent predictions. The feedback-loop approach can be combined both with single class and class trigram output. In the latter case, the full trigram class labels are copied to the input, retaining at any time the three most recently predicted labels in the input. Table 2 shows the results for both options on the three chunking tasks. The feedback-loop method outperforms the method on but not on the other two tasks. It does consistently outperform the baseline single-class classifier. Interestingly, the combination of the two methods performs worse the baseline classifier on and also performs worse than the trigram-class method on the other two tasks. 84 Figure 3: The windowing process after a first-stage classifier has produced a predicted output sequence. Sequences of input symbols, predicted output symbols, and real output symbols are converted into windows of fixed-width input symbols and predicted output symbols, each associated with one output symbol. 5 Stacking versus class trigrams Stacking, a term popularized by Wolpert (1992) in an artificial neural network context, refers to a class of meta-learning systems that learn to correct errors made by lower-level classifiers. We implement stacking by adding a windowed sequence of previous and subsequent output class labels to the original input features (here, we copy a window of seven predictions to the input, centered around the middle position), and providing these enriched examples as training material to a second-stage classifier. Figure 3 illustrates the procedure. Given the (possibly erroneous) output of a first classifier on an input sequence, a certain window of class symbols from that predicted sequence is copied to the input, to act as predictive features for the real class label. To generate the output of a first-stage classifier, options are open. We name these options per- They differ in the way they create training material for the second-stage classifier: the training material is created straight from the training material of the first-stage classifier, by windowing over the real class sequences. In doing so, the class label of each window is excluded from the input window, since it is always the same as the class to be predicted. In training, this focus feature would receive an unrealistically Perfect Adaptive Task Baseline stacking stacking CHUNK 91.9 92.0 92.6 NER 77.2 78.3 78.9 DISFL 77.9 80.5 81.6 Table 3: Comparison of generalization perforin terms of F-score of the three test sets, without stacking, and with perfect and adaptive stacking. high weight, especially considering that in testing this feature would contain errors. To assign a very high weight to a feature that may contain an erroneous value does not seem a good idea in view of the label bias problem. the training material is created indirectly by running an internal 10-fold crossvalidation experiment on the first-stage training set, concatenating the predicted output class labels on all of the ten test partitions, and converting this output to class windows. In contrast with the perfect variant, we do include the focus class feature in the copied class label window. The adaptive approach can in principle learn from recurring classification errors in the input, and predict the correct class in case an error re-occurs. Table 3 lists the comparative results on the and introduced earlier. They show that both types of stacking improve performance on the three tasks, and that the adaptive stacking variant produces higher relative gains than the perfect variant; in terms of error reduction in Fscore as compared to the baseline single-class clasthe gains are 9% for 7% for 17% for There appears to be more useful information in training data derived from crossvalidated output with errors, than in training data with error-free material. Stacking and class trigrams can be combined. One possible straightforward combination is that of a first-stage classifier that predicts trigrams, and a second-stage stacked classifier that also predicts trigrams (we use the adaptive variant, since it produced the best results), while including a centered sevenpositions-wide window of first-stage trigram class labels in the input. Table 4 compares the results 85 Adaptive Task stacking Trigram Combination CHUNK 92.6 92.8 93.1 NER 78.9 80.2 80.6 DISFL 81.6 81.7 81.9 Table 4: Comparison of generalization perforin terms of F-score by the three test sets, with adaptive stacking, trigram classes, and the combination of the two. of adaptive stacking and trigram classes with those of the combination of the two. As can be seen, the combination produces even better results than both the stacking and the trigram-class methods individually, on all three tasks. Compared to the baseline single-class classifier, the error reductions are 15% 15% for and 18% for As an additional analysis, we inspected the predictions made by the trigram-class method and its combinations with the stacking and the feedbackmethods on the to obtain a better view on the amount of disagreements between the trigrams. We found that with the trigram-class method, in 6.3% of all votes some disagreement among the overlapping trigrams occurs. A slightly higher percentage of disagreements, 7.1%, is observed with the combination of the trigram-class and the stacking method. Interestingly, in the combination of the trigram-class and feedback-loop methods, only 0.1% of all trigram votes are not unanimous. This clearly illustrates that in the latter combination the resulting sequence of trigrams is internally very consistent – also in its errors. 6 Conclusion Classifiers trained on chunking tasks that make isolated. near-sighted decisions on output symbols and that do not optimize the resulting output sequences afterwards or internally through a feedback loop, tend to produce weak models for sequence processing tasks. To combat this weakness, we have proposed a new method that uses a single symbolic machine-learning classifier predicting trigrams of classes, using a simple voting mechanism to reduce the sequence of predicted overlapping trigrams to a sequence of single output symbols. Compared to their near-sighted counterparts, error reductions are of 10 to 51% with three chunking tasks. We found weaker results with suggesting that the latter is more sensitive to the division of the class space in more classes, likely due to the relatively sparser cooccurrences between feature values and class labels which connection weights are based. We have contrasted the trigram-class method a feedback-loop method and a stacking method, all using a memory-based classifier (but the methods generalize to any machine-learning classifier). With the feedback-loop method, modest error reductions of 3%, 4%, and 17% are measured; stacking attains comparable improvements of 7%, 9%, and 17% error reductions in the chunking Fscore. We then combined the trigram-class method with the two other methods. The combination with the feedback-loop system led to relatively low performance results. A closer analysis indicated that the two methods appear to render each other ineffective: by feeding back predicted trigrams in the input, the classifier is very much geared towards predicting a next trigram that will be in accordance with the two partly overlapping trigrams in the input, as suggested by overwhelming evidence in this direction in training material – this problem is also known as the label bias problem (Lafferty et al., 2001). (The fact that maximum-entropy markov models also suffrom this problem prompted Lafferty al. propose conditional random fields.) We also observed that the positive effects of the trigram-class and stacking variants do not mute each other when combined. The overall highest error reductions are attained with the combination: 15% 15% for and 18% for The combination of the two methods solve more errors than the individual methods do. Apparently, they both introduce complementary disagreements in overlapping trigrams, which the simple voting mechanism can convert to more correct predictions than the two methods do individually. Further research should focus on a deep quantitative and qualitative analysis of the different errors the different methods correct when compared to the baseline single-class classifier, as well as the errors they may introduce. Alternatives to the 86 IOB-style encoding should also be incorporated in these experiments (Tjong Kim Sang, 2000). Additionally, a broader comparison with point-wise predictors (Kashima and Tsuboi, 2004) as well as Viterbi-based probabilistic models (McCallum et al., 2000; Lafferty et al., 2001; Sha and Pereira, 2003) in large-scale comparative studies is warranted. Also, the scope of the study may be broadened to all sequential language processing tasks, including tasks in which no segmentation takes place (e.g. part-of-speech tagging), and tasks at the morphophonological level (e.g. grapheme-phoneme conversion and morphological analysis). Acknowledgements The authors wish to thank Sander Canisius for discussions and suggestions. The work of the first author is funded by NWO, the Netherlands Organisation for Scientific Research; the second author’s work is partially funded by the EU BioMinT project.</abstract>
<note confidence="0.727641378787879">References S. Argamon, I. Dagan, and Y. Krymolowski. 1999. A memory-based approach to learning shallow natural patterns. ofExperimental and Theo- Artificial 10:1–22. A. J. Carlson, C. M. Cumby, J. L. Rosen, and D. Roth. 1999. Snow user guide. Technical Report UIUCDCS- R-99-2101, Cognitive Computation Group, Computer Science Department, University of Illinois, Urbana, Illinois. X. Carreras, L. M`arques, and L. Padr´o. 2002. Named extraction using AdaBoost. In of pages 167–170. Taipei, Taiwan. W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. A memory-based part of speech tagger genera- In E. Ejerhed and I. Dagan, editors, pages 14–27. ACL SIGDAT. W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. 2004. TiMBL: Tilburg memory based learner, version 5.1.0, reference guide. Technical Report ILK 04-02, ILK Research Group, Tilburg University. R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003. Named entity recognition through classifier combina- In W. Daelemans and M. Osborne, editors, Proof pages 168–171. Edmonton, Canada. Y. Freund and R. E. Schapire. 1996. Experiments with a boosting algorithm. In L. Saitta, editor, Proceedof pages 148–156, San Francisco, CA. Morgan Kaufmann. S. Guiasu and A. Shenitzer. 1985. The principle of maxentropy. Mathematical 7(1). H. Kashima and Y. Tsuboi. 2004. Kernel-based discriminative learning algorithms for labeling sequences, and graphs. In of Banff, Canada. J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segand labeling sequence data. In Williamstown, MA. P. Lendvai, A. van den Bosch, and E. Krahmer. 2003. disfluency chunking. In Gothenburg, Sweden, pages 63–66. N. Littlestone. 1988. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. 2:285–318. A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy Markov models for information extracand segmentation. In of Stanford, CA. V. Punyakanok and D. Roth. 2001. The use of classifiers sequential inference. In The 2000 Conference on Advances in Neural Information Processing pages 995–1001. The MIT Press. L.A. Ramshaw and M.P. Marcus. 1995. Text chunking transformation-based learning. In Cambridge, MA, pages 82–94. A. Ratnaparkhi. 1996. A maximum entropy part-oftagger. In of EMNLP, May 17-18, University of F. Sha and F. Pereira. 2003. Shallow parsing with Condi- Random Fields. In ofHLT-NAACL Edmonton, Canada. W. Skut and T. Brants. 1998. Chunk tagger: statistical of noun phrases. In Workon Automated Acquisition of Syntax and</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>I Dagan</author>
<author>Y Krymolowski</author>
</authors>
<title>A memory-based approach to learning shallow natural language patterns.</title>
<date>1999</date>
<journal>Journal ofExperimental and Theoretical Artificial Intelligence,</journal>
<pages>10--1</pages>
<contexts>
<context position="6174" citStr="Argamon et al. (1999)" startWordPosition="930" endWordPosition="933">predictions at the output level, classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov model’s output (Ratnaparkhi, 1996) or hidden markov models (Skut and Brants, 1998). Maximum-entropy markov models (McCallum et al., 2000) and conditional random fields (Lafferty et al., 2001) optimize the likelihood of segmentations of output symbol sequences through variations of Viterbi search. A non-stochastic, non-generative method for output sequence optimization is presented by Argamon et al. (1999), who propose a memory-based sequence learner that finds alternative chunking analyses of a sequence, and produces one best-guess analysis by a tiling algorithm that finds an optimal joining of the alternative analyses. In this paper we introduce a symbolic machinelearning method that can be likened to the approaches of the latter type of output sequence optimizers, but which does not perform a search in a space of possible analyses. The approach is to have a point-wise symbolic machine-learning classifier predict series of overlapping n-grams (in the current study, trigrams) of class symbols,</context>
</contexts>
<marker>Argamon, Dagan, Krymolowski, 1999</marker>
<rawString>S. Argamon, I. Dagan, and Y. Krymolowski. 1999. A memory-based approach to learning shallow natural language patterns. Journal ofExperimental and Theoretical Artificial Intelligence, 10:1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Carlson</author>
<author>C M Cumby</author>
<author>J L Rosen</author>
<author>D Roth</author>
</authors>
<title>Snow user guide.</title>
<date>1999</date>
<tech>Technical Report UIUCDCSR-99-2101,</tech>
<institution>Cognitive Computation Group, Computer Science Department, University of Illinois,</institution>
<location>Urbana, Illinois.</location>
<contexts>
<context position="13257" citStr="Carlson et al. (1999)" startWordPosition="2040" endWordPosition="2043">de nodige scepsis [uh] deze gang van zaken [zo’n] zo’n jaar aangekeken. We perform our experiments on the three tasks using three machine-learning algorithms: the memorybased learning or k-nearest neighbor algorithm as implemented in the TiMBL software package (version 5.1) (Daelemans et al., 2004), henceforth referred to as MBL; maximum-entropy classification (Guiasu and Shenitzer, 1985) as implemented in the maxent software package (version 20040930) by Zhang Le3, henceforth MAXENT; and a sparsewinnow network (Littlestone, 1988) as implemented in the SNoW software package (version 3.0.5) by Carlson et al. (1999), henceforth WINNOW. All three algorithms have algorithmic parameters that bias their performance; to allow for a fair comparison we optimized each algorithm on each task using wrapped progressive sampling (Van den Bosch, 2004) (WPS), a heuristic automatic procedure that, on the basis of validation experiments internal to the training material, searches among algorithmic parameter combinations for a combination likely to yield optimal generalization performance on unseen data. We used wrapped progressive sampling in all experiments. 3 Predicting class trigrams There is no intrinsic bound to wh</context>
</contexts>
<marker>Carlson, Cumby, Rosen, Roth, 1999</marker>
<rawString>A. J. Carlson, C. M. Cumby, J. L. Rosen, and D. Roth. 1999. Snow user guide. Technical Report UIUCDCSR-99-2101, Cognitive Computation Group, Computer Science Department, University of Illinois, Urbana, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arques</author>
<author>L Padr´o</author>
</authors>
<title>Named entity extraction using AdaBoost.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>167--170</pages>
<location>Taipei, Taiwan.</location>
<marker>Carreras, M`arques, Padr´o, 2002</marker>
<rawString>X. Carreras, L. M`arques, and L. Padr´o. 2002. Named entity extraction using AdaBoost. In Proceedings of CoNLL-2002, pages 167–170. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>MBT: A memory-based part of speech tagger generator.</title>
<date>1996</date>
<booktitle>Proceedings of WVLC,</booktitle>
<pages>14--27</pages>
<editor>In E. Ejerhed and I. Dagan, editors,</editor>
<publisher>ACL SIGDAT.</publisher>
<contexts>
<context position="4174" citStr="Daelemans et al. (1996)" startWordPosition="623" endWordPosition="626">lier. This well-known problem has triggered at least the following three main types of solutions. Feedback loop Each training or test example may represent not only the regular windowed input, but also a copy of previously made classifications, to allow the classifier to be more consistent with its previous decisions. Direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by Ratnaparkhi (1996) and the memory-based tagger (MBT) proposed by Daelemans et al. (1996). This solution assumes that processing is directed, e.g. from left to right. A noted problem of this approach is the label bias problem (Lafferty et al., 2001), which is that a feedback-loop classifier may be driven to be consistent with its previous decision also in the case this decision was wrong; sequences of errors may result. Stacking, boosting, and voting The partly incorrect concatenated output sequence of a single classifier may serve as input to a second-stage classifier in a stacking architecture, a common machine-learning optimization technique (Wolpert, 1992). Although less elega</context>
<context position="18987" citStr="Daelemans et al., 1996" startWordPosition="2980" endWordPosition="2983">vious decisions of the classifier as features into the current input of the classifier. This Task Baseline Feedback Trigrams Feed+Tri CHUNK 91.9 93.0 92.7 89.8 NER 77.2 78.1 80.2 77.5 DISFL 77.9 78.6 81.7 79.1 Table 2: Comparison of generalization performances in terms of F-score of MBL on the three test sets, with and without a feedback loop, and the error reduction attained by the feedback-loop method, the F-score of the trigram-class method, and the F-score of the combination of the two methods. approach was proposed in the context of memorybased learning for part-of-speech tagging as MBT (Daelemans et al., 1996). The number of decisions fed back into the input can be varied. In the experiments described here, the feedback loop iteratively updates a memory of the three most recent predictions. The feedback-loop approach can be combined both with single class and class trigram output. In the latter case, the full trigram class labels are copied to the input, retaining at any time the three most recently predicted labels in the input. Table 2 shows the results for both options on the three chunking tasks. The feedback-loop method outperforms the trigram-class method on CHUNK, but not on the other two ta</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. MBT: A memory-based part of speech tagger generator. In E. Ejerhed and I. Dagan, editors, Proceedings of WVLC, pages 14–27. ACL SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>TiMBL: Tilburg memory based learner, version 5.1.0, reference guide.</title>
<date>2004</date>
<tech>Technical Report ILK 04-02,</tech>
<institution>ILK Research Group, Tilburg University.</institution>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2004</marker>
<rawString>W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. 2004. TiMBL: Tilburg memory based learner, version 5.1.0, reference guide. Technical Report ILK 04-02, ILK Research Group, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>T Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>Proceedings of CoNLL-2003,</booktitle>
<pages>168--171</pages>
<editor>In W. Daelemans and M. Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5145" citStr="Florian et al., 2003" startWordPosition="776" endWordPosition="779">ing, and voting The partly incorrect concatenated output sequence of a single classifier may serve as input to a second-stage classifier in a stacking architecture, a common machine-learning optimization technique (Wolpert, 1992). Although less elegant than a monolithic single-classifier architecture, this method is known to be capable of recognizing recurring errors of the first-stage classifier and correcting them (Veenstra, 1998). Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al., 2002), as well as voting over sets of different classifiers (Florian et al., 2003). Punyakanok and Roth (2001) present two methods for combining the predictions of different classifiers according to constraints that ensure that the resulting output is made more coherent. Output sequence optimization Rather than basing classifications only on model parameters estimated from co-occurrences between input and output symbols employed for maximizing the likelihood of point-wise single-label predictions at the output level, classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of </context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003. Named entity recognition through classifier combination. In W. Daelemans and M. Osborne, editors, Proceedings of CoNLL-2003, pages 168–171. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Experiments with a new boosting algorithm.</title>
<date>1996</date>
<booktitle>Proceedings of ICML-96,</booktitle>
<pages>148--156</pages>
<editor>In L. Saitta, editor,</editor>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="4998" citStr="Freund and Schapire, 1996" startWordPosition="752" endWordPosition="755">fier may be driven to be consistent with its previous decision also in the case this decision was wrong; sequences of errors may result. Stacking, boosting, and voting The partly incorrect concatenated output sequence of a single classifier may serve as input to a second-stage classifier in a stacking architecture, a common machine-learning optimization technique (Wolpert, 1992). Although less elegant than a monolithic single-classifier architecture, this method is known to be capable of recognizing recurring errors of the first-stage classifier and correcting them (Veenstra, 1998). Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al., 2002), as well as voting over sets of different classifiers (Florian et al., 2003). Punyakanok and Roth (2001) present two methods for combining the predictions of different classifiers according to constraints that ensure that the resulting output is made more coherent. Output sequence optimization Rather than basing classifications only on model parameters estimated from co-occurrences between input and output symbols employed for maximizing the likelihood of point-wise single-label predictions at the output level, classifier o</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Y. Freund and R. E. Schapire. 1996. Experiments with a new boosting algorithm. In L. Saitta, editor, Proceedings of ICML-96, pages 148–156, San Francisco, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Guiasu</author>
<author>A Shenitzer</author>
</authors>
<title>The principle of maximum entropy.</title>
<date>1985</date>
<journal>The Mathematical Intelligencer,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="13027" citStr="Guiasu and Shenitzer, 1985" startWordPosition="2005" endWordPosition="2008">e of a chunked Spoken Dutch Corpus sentence is the following (“uh” is a filled pause; without the disfluencies, the sentence means “I have followed this process with a certain amount of scepticism for about a year”): [ik uh] ik heb met de nodige scepsis [uh] deze gang van zaken [zo’n] zo’n jaar aangekeken. We perform our experiments on the three tasks using three machine-learning algorithms: the memorybased learning or k-nearest neighbor algorithm as implemented in the TiMBL software package (version 5.1) (Daelemans et al., 2004), henceforth referred to as MBL; maximum-entropy classification (Guiasu and Shenitzer, 1985) as implemented in the maxent software package (version 20040930) by Zhang Le3, henceforth MAXENT; and a sparsewinnow network (Littlestone, 1988) as implemented in the SNoW software package (version 3.0.5) by Carlson et al. (1999), henceforth WINNOW. All three algorithms have algorithmic parameters that bias their performance; to allow for a fair comparison we optimized each algorithm on each task using wrapped progressive sampling (Van den Bosch, 2004) (WPS), a heuristic automatic procedure that, on the basis of validation experiments internal to the training material, searches among algorith</context>
</contexts>
<marker>Guiasu, Shenitzer, 1985</marker>
<rawString>S. Guiasu and A. Shenitzer. 1985. The principle of maximum entropy. The Mathematical Intelligencer, 7(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kashima</author>
<author>Y Tsuboi</author>
</authors>
<title>Kernel-based discriminative learning algorithms for labeling sequences, trees and graphs.</title>
<date>2004</date>
<booktitle>In Proceedings of ICML-2004,</booktitle>
<location>Banff, Canada.</location>
<contexts>
<context position="28026" citStr="Kashima and Tsuboi, 2004" startWordPosition="4427" endWordPosition="4430">rently, they both introduce complementary disagreements in overlapping trigrams, which the simple voting mechanism can convert to more correct predictions than the two methods do individually. Further research should focus on a deep quantitative and qualitative analysis of the different errors the different methods correct when compared to the baseline single-class classifier, as well as the errors they may introduce. Alternatives to the 86 IOB-style encoding should also be incorporated in these experiments (Tjong Kim Sang, 2000). Additionally, a broader comparison with point-wise predictors (Kashima and Tsuboi, 2004) as well as Viterbi-based probabilistic models (McCallum et al., 2000; Lafferty et al., 2001; Sha and Pereira, 2003) in large-scale comparative studies is warranted. Also, the scope of the study may be broadened to all sequential language processing tasks, including tasks in which no segmentation takes place (e.g. part-of-speech tagging), and tasks at the morphophonological level (e.g. grapheme-phoneme conversion and morphological analysis). Acknowledgements The authors wish to thank Sander Canisius for discussions and suggestions. The work of the first author is funded by NWO, the Netherlands</context>
</contexts>
<marker>Kashima, Tsuboi, 2004</marker>
<rawString>H. Kashima and Y. Tsuboi. 2004. Kernel-based discriminative learning algorithms for labeling sequences, trees and graphs. In Proceedings of ICML-2004, Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings ofICML-01,</booktitle>
<location>Williamstown, MA.</location>
<contexts>
<context position="4334" citStr="Lafferty et al., 2001" startWordPosition="651" endWordPosition="654">nly the regular windowed input, but also a copy of previously made classifications, to allow the classifier to be more consistent with its previous decisions. Direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by Ratnaparkhi (1996) and the memory-based tagger (MBT) proposed by Daelemans et al. (1996). This solution assumes that processing is directed, e.g. from left to right. A noted problem of this approach is the label bias problem (Lafferty et al., 2001), which is that a feedback-loop classifier may be driven to be consistent with its previous decision also in the case this decision was wrong; sequences of errors may result. Stacking, boosting, and voting The partly incorrect concatenated output sequence of a single classifier may serve as input to a second-stage classifier in a stacking architecture, a common machine-learning optimization technique (Wolpert, 1992). Although less elegant than a monolithic single-classifier architecture, this method is known to be capable of recognizing recurring errors of the first-stage classifier and correc</context>
<context position="5957" citStr="Lafferty et al., 2001" startWordPosition="899" endWordPosition="902">Output sequence optimization Rather than basing classifications only on model parameters estimated from co-occurrences between input and output symbols employed for maximizing the likelihood of point-wise single-label predictions at the output level, classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov model’s output (Ratnaparkhi, 1996) or hidden markov models (Skut and Brants, 1998). Maximum-entropy markov models (McCallum et al., 2000) and conditional random fields (Lafferty et al., 2001) optimize the likelihood of segmentations of output symbol sequences through variations of Viterbi search. A non-stochastic, non-generative method for output sequence optimization is presented by Argamon et al. (1999), who propose a memory-based sequence learner that finds alternative chunking analyses of a sequence, and produces one best-guess analysis by a tiling algorithm that finds an optimal joining of the alternative analyses. In this paper we introduce a symbolic machinelearning method that can be likened to the approaches of the latter type of output sequence optimizers, but which does</context>
<context position="26928" citStr="Lafferty et al., 2001" startWordPosition="4259" endWordPosition="4262"> chunking Fscore. We then combined the trigram-class method with the two other methods. The combination with the feedback-loop system led to relatively low performance results. A closer analysis indicated that the two methods appear to render each other ineffective: by feeding back predicted trigrams in the input, the classifier is very much geared towards predicting a next trigram that will be in accordance with the two partly overlapping trigrams in the input, as suggested by overwhelming evidence in this direction in training material – this problem is also known as the label bias problem (Lafferty et al., 2001). (The fact that maximum-entropy markov models also suffer from this problem prompted Lafferty et al. to propose conditional random fields.) We also observed that the positive effects of the trigram-class and stacking variants do not mute each other when combined. The overall highest error reductions are attained with the combination: 15% for CHUNK, 15% for NER, and 18% for DISFL. The combination of the two methods solve more errors than the individual methods do. Apparently, they both introduce complementary disagreements in overlapping trigrams, which the simple voting mechanism can convert </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings ofICML-01, Williamstown, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lendvai</author>
<author>A van den Bosch</author>
<author>E Krahmer</author>
</authors>
<title>Memory-based disfluency chunking.</title>
<date>2003</date>
<booktitle>In Proceedings ofDISS’03), Gothenburg, Sweden,</booktitle>
<pages>63--66</pages>
<marker>Lendvai, van den Bosch, Krahmer, 2003</marker>
<rawString>P. Lendvai, A. van den Bosch, and E. Krahmer. 2003. Memory-based disfluency chunking. In Proceedings ofDISS’03), Gothenburg, Sweden, pages 63–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Littlestone</author>
</authors>
<title>Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm.</title>
<date>1988</date>
<booktitle>Machine Learning,</booktitle>
<pages>2--285</pages>
<contexts>
<context position="13172" citStr="Littlestone, 1988" startWordPosition="2028" endWordPosition="2029">rocess with a certain amount of scepticism for about a year”): [ik uh] ik heb met de nodige scepsis [uh] deze gang van zaken [zo’n] zo’n jaar aangekeken. We perform our experiments on the three tasks using three machine-learning algorithms: the memorybased learning or k-nearest neighbor algorithm as implemented in the TiMBL software package (version 5.1) (Daelemans et al., 2004), henceforth referred to as MBL; maximum-entropy classification (Guiasu and Shenitzer, 1985) as implemented in the maxent software package (version 20040930) by Zhang Le3, henceforth MAXENT; and a sparsewinnow network (Littlestone, 1988) as implemented in the SNoW software package (version 3.0.5) by Carlson et al. (1999), henceforth WINNOW. All three algorithms have algorithmic parameters that bias their performance; to allow for a fair comparison we optimized each algorithm on each task using wrapped progressive sampling (Van den Bosch, 2004) (WPS), a heuristic automatic procedure that, on the basis of validation experiments internal to the training material, searches among algorithmic parameter combinations for a combination likely to yield optimal generalization performance on unseen data. We used wrapped progressive sampl</context>
</contexts>
<marker>Littlestone, 1988</marker>
<rawString>N. Littlestone. 1988. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML-00,</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="5903" citStr="McCallum et al., 2000" startWordPosition="890" endWordPosition="893">sure that the resulting output is made more coherent. Output sequence optimization Rather than basing classifications only on model parameters estimated from co-occurrences between input and output symbols employed for maximizing the likelihood of point-wise single-label predictions at the output level, classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov model’s output (Ratnaparkhi, 1996) or hidden markov models (Skut and Brants, 1998). Maximum-entropy markov models (McCallum et al., 2000) and conditional random fields (Lafferty et al., 2001) optimize the likelihood of segmentations of output symbol sequences through variations of Viterbi search. A non-stochastic, non-generative method for output sequence optimization is presented by Argamon et al. (1999), who propose a memory-based sequence learner that finds alternative chunking analyses of a sequence, and produces one best-guess analysis by a tiling algorithm that finds an optimal joining of the alternative analyses. In this paper we introduce a symbolic machinelearning method that can be likened to the approaches of the lat</context>
<context position="28095" citStr="McCallum et al., 2000" startWordPosition="4437" endWordPosition="4440">igrams, which the simple voting mechanism can convert to more correct predictions than the two methods do individually. Further research should focus on a deep quantitative and qualitative analysis of the different errors the different methods correct when compared to the baseline single-class classifier, as well as the errors they may introduce. Alternatives to the 86 IOB-style encoding should also be incorporated in these experiments (Tjong Kim Sang, 2000). Additionally, a broader comparison with point-wise predictors (Kashima and Tsuboi, 2004) as well as Viterbi-based probabilistic models (McCallum et al., 2000; Lafferty et al., 2001; Sha and Pereira, 2003) in large-scale comparative studies is warranted. Also, the scope of the study may be broadened to all sequential language processing tasks, including tasks in which no segmentation takes place (e.g. part-of-speech tagging), and tasks at the morphophonological level (e.g. grapheme-phoneme conversion and morphological analysis). Acknowledgements The authors wish to thank Sander Canisius for discussions and suggestions. The work of the first author is funded by NWO, the Netherlands Organisation for Scientific Research; the second author’s work is pa</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proceedings of ICML-00, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In NIPS-13; The 2000 Conference on Advances in Neural Information Processing Systems,</booktitle>
<pages>995--1001</pages>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5173" citStr="Punyakanok and Roth (2001)" startWordPosition="780" endWordPosition="783">tly incorrect concatenated output sequence of a single classifier may serve as input to a second-stage classifier in a stacking architecture, a common machine-learning optimization technique (Wolpert, 1992). Although less elegant than a monolithic single-classifier architecture, this method is known to be capable of recognizing recurring errors of the first-stage classifier and correcting them (Veenstra, 1998). Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al., 2002), as well as voting over sets of different classifiers (Florian et al., 2003). Punyakanok and Roth (2001) present two methods for combining the predictions of different classifiers according to constraints that ensure that the resulting output is made more coherent. Output sequence optimization Rather than basing classifications only on model parameters estimated from co-occurrences between input and output symbols employed for maximizing the likelihood of point-wise single-label predictions at the output level, classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov model’s</context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>V. Punyakanok and D. Roth. 2001. The use of classifiers in sequential inference. In NIPS-13; The 2000 Conference on Advances in Neural Information Processing Systems, pages 995–1001. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of WVLC-95,</booktitle>
<pages>82--94</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="9185" citStr="Ramshaw and Marcus (1995)" startWordPosition="1401" endWordPosition="1404"> semantic nature: English base phrase chunking (henceforth CHUNK), English named-entity recognition (NER), and disfluency chunking in transcribed spoken Dutch utterances (DISFL). CHUNK is the task of splitting sentences into non-overlapping syntactic phrases or constituents. The used data set, extracted from the WSJ Penn Treebank, contains 211,727 training examples and 47,377 test instances. The examples represent seven-word windows of words and their respective (predicted) part-of-speech tags, and each example is labeled with a class using the IOB type of segmentation coding as introduced by Ramshaw and Marcus (1995), marking whether the middle word is inside (I), outside (O), or at the beginning (B) of a chunk. Words occuring less than ten times in the training material are attenuated (converted into a more general string that retains some of the word’s surface form). Generalization performance is measured by the F-score on correctly identified and labeled constituents in test data, using the evaluation method originally used in the “shared task” subevent of the CoNLL-2000 conference (Tjong Kim Sang and Buchholz, 2000) in which this particular training and test set were used. An example sentence with bas</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L.A. Ramshaw and M.P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of WVLC-95, Cambridge, MA, pages 82–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-ofspeech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4104" citStr="Ratnaparkhi (1996)" startWordPosition="613" endWordPosition="614">is incapable of taking into account any decisions it has made earlier. This well-known problem has triggered at least the following three main types of solutions. Feedback loop Each training or test example may represent not only the regular windowed input, but also a copy of previously made classifications, to allow the classifier to be more consistent with its previous decisions. Direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by Ratnaparkhi (1996) and the memory-based tagger (MBT) proposed by Daelemans et al. (1996). This solution assumes that processing is directed, e.g. from left to right. A noted problem of this approach is the label bias problem (Lafferty et al., 2001), which is that a feedback-loop classifier may be driven to be consistent with its previous decision also in the case this decision was wrong; sequences of errors may result. Stacking, boosting, and voting The partly incorrect concatenated output sequence of a single classifier may serve as input to a second-stage classifier in a stacking architecture, a common machin</context>
<context position="5800" citStr="Ratnaparkhi, 1996" startWordPosition="876" endWordPosition="877">two methods for combining the predictions of different classifiers according to constraints that ensure that the resulting output is made more coherent. Output sequence optimization Rather than basing classifications only on model parameters estimated from co-occurrences between input and output symbols employed for maximizing the likelihood of point-wise single-label predictions at the output level, classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov model’s output (Ratnaparkhi, 1996) or hidden markov models (Skut and Brants, 1998). Maximum-entropy markov models (McCallum et al., 2000) and conditional random fields (Lafferty et al., 2001) optimize the likelihood of segmentations of output symbol sequences through variations of Viterbi search. A non-stochastic, non-generative method for output sequence optimization is presented by Argamon et al. (1999), who propose a memory-based sequence learner that finds alternative chunking analyses of a sequence, and produces one best-guess analysis by a tiling algorithm that finds an optimal joining of the alternative analyses. In thi</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy part-ofspeech tagger. In Proceedings of EMNLP, May 17-18, 1996, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with Conditional Random Fields.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL 2003,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="28142" citStr="Sha and Pereira, 2003" startWordPosition="4445" endWordPosition="4448">convert to more correct predictions than the two methods do individually. Further research should focus on a deep quantitative and qualitative analysis of the different errors the different methods correct when compared to the baseline single-class classifier, as well as the errors they may introduce. Alternatives to the 86 IOB-style encoding should also be incorporated in these experiments (Tjong Kim Sang, 2000). Additionally, a broader comparison with point-wise predictors (Kashima and Tsuboi, 2004) as well as Viterbi-based probabilistic models (McCallum et al., 2000; Lafferty et al., 2001; Sha and Pereira, 2003) in large-scale comparative studies is warranted. Also, the scope of the study may be broadened to all sequential language processing tasks, including tasks in which no segmentation takes place (e.g. part-of-speech tagging), and tasks at the morphophonological level (e.g. grapheme-phoneme conversion and morphological analysis). Acknowledgements The authors wish to thank Sander Canisius for discussions and suggestions. The work of the first author is funded by NWO, the Netherlands Organisation for Scientific Research; the second author’s work is partially funded by the EU BioMinT project. Refer</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with Conditional Random Fields. In Proceedings ofHLT-NAACL 2003, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Skut</author>
<author>T Brants</author>
</authors>
<title>Chunk tagger: statistical recognition of noun phrases.</title>
<date>1998</date>
<booktitle>In ESSLLI-1998 Workshop on Automated Acquisition of Syntax and Parsing.</booktitle>
<contexts>
<context position="5848" citStr="Skut and Brants, 1998" startWordPosition="883" endWordPosition="886"> different classifiers according to constraints that ensure that the resulting output is made more coherent. Output sequence optimization Rather than basing classifications only on model parameters estimated from co-occurrences between input and output symbols employed for maximizing the likelihood of point-wise single-label predictions at the output level, classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov model’s output (Ratnaparkhi, 1996) or hidden markov models (Skut and Brants, 1998). Maximum-entropy markov models (McCallum et al., 2000) and conditional random fields (Lafferty et al., 2001) optimize the likelihood of segmentations of output symbol sequences through variations of Viterbi search. A non-stochastic, non-generative method for output sequence optimization is presented by Argamon et al. (1999), who propose a memory-based sequence learner that finds alternative chunking analyses of a sequence, and produces one best-guess analysis by a tiling algorithm that finds an optimal joining of the alternative analyses. In this paper we introduce a symbolic machinelearning </context>
</contexts>
<marker>Skut, Brants, 1998</marker>
<rawString>W. Skut and T. Brants. 1998. Chunk tagger: statistical recognition of noun phrases. In ESSLLI-1998 Workshop on Automated Acquisition of Syntax and Parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong Kim Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL-2000 and LLL-2000,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="9698" citStr="Sang and Buchholz, 2000" startWordPosition="1485" endWordPosition="1488">le is labeled with a class using the IOB type of segmentation coding as introduced by Ramshaw and Marcus (1995), marking whether the middle word is inside (I), outside (O), or at the beginning (B) of a chunk. Words occuring less than ten times in the training material are attenuated (converted into a more general string that retains some of the word’s surface form). Generalization performance is measured by the F-score on correctly identified and labeled constituents in test data, using the evaluation method originally used in the “shared task” subevent of the CoNLL-2000 conference (Tjong Kim Sang and Buchholz, 2000) in which this particular training and test set were used. An example sentence with base phrases marked and labeled is the following: [He]NP [reckons]V P [the current account deficit]NP [will narrow]V P [to]PP [only $ 1.8 billion]NP [in]PP [September]NP . NER, named-entity recognition, is to recognize and type named entities in text. We employ the English NER shared task data set used in the CoNLL2003 conference, again using the same evaluation method as originally used in the shared task (Tjong Kim Sang and De Meulder, 2003). This data set discriminates four name types: persons, organizations</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>E. Tjong Kim Sang and S. Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of CoNLL-2000 and LLL-2000, pages 127– 132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong Kim Sang</author>
<author>F De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition.</title>
<date>2003</date>
<booktitle>Proceedings of CoNLL2003,</booktitle>
<pages>142--147</pages>
<editor>In W. Daelemans and M. Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<marker>Sang, De Meulder, 2003</marker>
<rawString>E. Tjong Kim Sang and F. De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. In W. Daelemans and M. Osborne, editors, Proceedings of CoNLL2003, pages 142–147. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong Kim Sang</author>
</authors>
<title>Noun phrase recognition by system combination.</title>
<date>2000</date>
<booktitle>In Proceedings ofANLP-NAACL 2000,</booktitle>
<pages>50--55</pages>
<publisher>Morgan Kaufman Publishers.</publisher>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="27936" citStr="Sang, 2000" startWordPosition="4417" endWordPosition="4418">on of the two methods solve more errors than the individual methods do. Apparently, they both introduce complementary disagreements in overlapping trigrams, which the simple voting mechanism can convert to more correct predictions than the two methods do individually. Further research should focus on a deep quantitative and qualitative analysis of the different errors the different methods correct when compared to the baseline single-class classifier, as well as the errors they may introduce. Alternatives to the 86 IOB-style encoding should also be incorporated in these experiments (Tjong Kim Sang, 2000). Additionally, a broader comparison with point-wise predictors (Kashima and Tsuboi, 2004) as well as Viterbi-based probabilistic models (McCallum et al., 2000; Lafferty et al., 2001; Sha and Pereira, 2003) in large-scale comparative studies is warranted. Also, the scope of the study may be broadened to all sequential language processing tasks, including tasks in which no segmentation takes place (e.g. part-of-speech tagging), and tasks at the morphophonological level (e.g. grapheme-phoneme conversion and morphological analysis). Acknowledgements The authors wish to thank Sander Canisius for d</context>
</contexts>
<marker>Sang, 2000</marker>
<rawString>E. Tjong Kim Sang. 2000. Noun phrase recognition by system combination. In Proceedings ofANLP-NAACL 2000, pages 50–55. Seattle, Washington, USA. Morgan Kaufman Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A van den Bosch</author>
</authors>
<title>Wrapped progressive sampling search for optimizing learning algorithm parameters. In</title>
<date>2004</date>
<booktitle>Proceedings of the 16th Belgian-Dutch AI Conference,</booktitle>
<pages>219--226</pages>
<editor>R. Verbrugge, N. Taatgen, and L. Schomaker, editors,</editor>
<location>Groningen, The Netherlands.</location>
<marker>van den Bosch, 2004</marker>
<rawString>A. van den Bosch. 2004. Wrapped progressive sampling search for optimizing learning algorithm parameters. In R. Verbrugge, N. Taatgen, and L. Schomaker, editors, Proceedings of the 16th Belgian-Dutch AI Conference, pages 219–226, Groningen, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veenstra</author>
</authors>
<title>Fast NP chunking using memorybased learning techniques.</title>
<date>1998</date>
<booktitle>In Proceedings of BENELEARN’98,</booktitle>
<pages>71--78</pages>
<location>Wageningen, The Netherlands.</location>
<contexts>
<context position="4960" citStr="Veenstra, 1998" startWordPosition="748" endWordPosition="749">that a feedback-loop classifier may be driven to be consistent with its previous decision also in the case this decision was wrong; sequences of errors may result. Stacking, boosting, and voting The partly incorrect concatenated output sequence of a single classifier may serve as input to a second-stage classifier in a stacking architecture, a common machine-learning optimization technique (Wolpert, 1992). Although less elegant than a monolithic single-classifier architecture, this method is known to be capable of recognizing recurring errors of the first-stage classifier and correcting them (Veenstra, 1998). Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al., 2002), as well as voting over sets of different classifiers (Florian et al., 2003). Punyakanok and Roth (2001) present two methods for combining the predictions of different classifiers according to constraints that ensure that the resulting output is made more coherent. Output sequence optimization Rather than basing classifications only on model parameters estimated from co-occurrences between input and output symbols employed for maximizing the likelihood of point-wise single-label predict</context>
</contexts>
<marker>Veenstra, 1998</marker>
<rawString>J. Veenstra. 1998. Fast NP chunking using memorybased learning techniques. In Proceedings of BENELEARN’98, pages 71–78, Wageningen, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Wolpert</author>
</authors>
<title>Stacked Generalization. Neural Networks,</title>
<date>1992</date>
<pages>5--241</pages>
<contexts>
<context position="4753" citStr="Wolpert, 1992" startWordPosition="718" endWordPosition="719">proposed by Daelemans et al. (1996). This solution assumes that processing is directed, e.g. from left to right. A noted problem of this approach is the label bias problem (Lafferty et al., 2001), which is that a feedback-loop classifier may be driven to be consistent with its previous decision also in the case this decision was wrong; sequences of errors may result. Stacking, boosting, and voting The partly incorrect concatenated output sequence of a single classifier may serve as input to a second-stage classifier in a stacking architecture, a common machine-learning optimization technique (Wolpert, 1992). Although less elegant than a monolithic single-classifier architecture, this method is known to be capable of recognizing recurring errors of the first-stage classifier and correcting them (Veenstra, 1998). Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al., 2002), as well as voting over sets of different classifiers (Florian et al., 2003). Punyakanok and Roth (2001) present two methods for combining the predictions of different classifiers according to constraints that ensure that the resulting output is made more coherent. Output sequence op</context>
<context position="20235" citStr="Wolpert (1992)" startWordPosition="3181" endWordPosition="3182">rm the baseline single-class classifier. Interestingly, the combination of the two methods performs worse than the baseline classifier on CHUNK, and also performs worse than the trigram-class method on the other two tasks. 84 Figure 3: The windowing process after a first-stage classifier has produced a predicted output sequence. Sequences of input symbols, predicted output symbols, and real output symbols are converted into windows of fixed-width input symbols and predicted output symbols, each associated with one output symbol. 5 Stacking versus class trigrams Stacking, a term popularized by Wolpert (1992) in an artificial neural network context, refers to a class of meta-learning systems that learn to correct errors made by lower-level classifiers. We implement stacking by adding a windowed sequence of previous and subsequent output class labels to the original input features (here, we copy a window of seven predictions to the input, centered around the middle position), and providing these enriched examples as training material to a second-stage classifier. Figure 3 illustrates the procedure. Given the (possibly erroneous) output of a first classifier on an input sequence, a certain window of</context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>D. H. Wolpert. 1992. Stacked Generalization. Neural Networks, 5:241–259.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>