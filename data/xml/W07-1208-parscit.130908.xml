<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.114097">
<title confidence="0.9913065">
Self- or Pre-Tuning?
Deep linguistic processing of language variants
</title>
<author confidence="0.998385">
Antonio Branco Francisco Costa
</author>
<affiliation confidence="0.980325">
Universidade de Lisboa Universidade de Lisboa
</affiliation>
<email confidence="0.978598">
Antonio.Branco@di.fc.ul.pt fcosta@di.fc.ul.pt
</email>
<sectionHeader confidence="0.995184" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992675">
This paper proposes a design strategy
for deep language processing grammars
to appropriately handle language vari-
ants. It allows a grammar to be re-
stricted as to what language variant it is
tuned to, but also to detect the variant
a given input pertains to. This is eval-
uated and compared to results obtained
with an alternative strategy by which the
relevant variant is detected with current
language identification methods in a pre-
processing step.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955236842105">
This paper addresses the issue of handling dif-
ferent variants of a given language by a deep
language processing grammar for that language.
In the benefit of generalization and grammar
writing economy, it is desirable that a grammar
can handle language variants – that share most
grammatical structures and lexicon – in order to
avoid endless multiplication of individual gram-
mars, motivated by inessential differences.
From the viewpoint of analysis, however, in-
creased variant coverage typically opens the way
to increased spurious overgeneration. Conse-
quently, the ability for the grammar to be tuned
to the relevant dialect of the input is impor-
tant to control overgeneration arising from its
flexibility.
Control on what is generated is also desirable.
In general one wants to be able to parse as much
variants as possible, but at the same time be se-
lective in generation, by consistently generating
only in a given selected variant.
Closely related to the setting issue (addressed
in the next Section 2) is the tuning issue: if a
system can be restricted to a particular variety,
what is the best way to detect the variety of the
input? We discuss two approaches to this issue.
One of them consists in using pre-processing
components that can detect the language variety
at stake. This pre-tuning approach explores the
hypothesis that methods developed for language
identification can be used also to detect language
variants (Section 5).
The other approach is to have the computa-
tional grammar prepared for self-tuning to the
language variant of the input in the course of
processing that input (Section 4).
We evaluate the two approaches and compare
them (last Section 6).
</bodyText>
<sectionHeader confidence="0.995574" genericHeader="method">
2 Variant-sensitive Grammar
</sectionHeader>
<bodyText confidence="0.999926277777778">
In this Section, we discuss the design options for
a deep linguistic processing grammar allowing
for its appropriate tuning to different language
variants. For the sake of concreteness of the dis-
cussion, we assume the HPSG framework (Pol-
lard and Sag, 1994) and a grammar that handles
two close variants of the same language, Euro-
pean and Brazilian Portuguese. These assump-
tions are merely instrumental, and the results
obtained can be easily extended to other lan-
guages and variants, and to other grammatical
frameworks for deep linguistic processing.
A stretch of text from a language L can dis-
play grammatical features common to all vari-
ants of L, or contain a construction that per-
tains to some or only one of its variants. Hence,
undesirable overgeneration due to the grammar
readiness to cope with all language variants can
</bodyText>
<page confidence="0.983969">
57
</page>
<note confidence="0.9226885">
Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 57–64,
Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics
</note>
<figure confidence="0.985865666666667">
variant
ep-variant single-variant bp-variant
european-portuguese portuguese brazilian-portuguese
</figure>
<figureCaption confidence="0.999985">
Figure 1: Type hierarchy under variant.
</figureCaption>
<bodyText confidence="0.999309957446809">
be put in check by restricting the grammar
to produce variant-“consistent” analyses. More
precisely, if the input string contains an element
that can only be found in variety v1 and that in-
put string yields ambiguity in a different stretch
but only in varieties vk other than v1, this ambi-
guity will not give rise to multiple analyses if the
grammar can be designed so that it can be con-
strained to accept strings with marked elements
of at most one variety, v1.
The approach we propose seeks to implement
this mode of operation in analysis, with the im-
portant effect of permitting also to control the
variant under which generation should be per-
formed. It relies on the use of a feature VARIANT
to model variation. This feature is appropriate
for all signs and declared to be of type variant.
Given the working language variants assumed
here, its values are presented in Figure 1.
This attribute is constrained to take the ap-
propriate value in lexical items and construc-
tions specific to one of the two varieties. For
example, a hypothetical lexical entry for the lex-
ical item autocarro (bus, exclusive to European
Portuguese) would include the constraint that
the attribute VARIANT has the value ep-variant
and the corresponding Brazilian Portuguese en-
try for onibus would constrain the same feature
to bear the value bp-variant. The only two types
that are used to mark signs are ep-variant and
bp-variant. The remaining types presented in
Figure 1 are used to constrain grammar behav-
ior, as explained below.
Lexical items are not the only elements that
can have marked values in the VARIANT fea-
ture. Lexical and syntax rules can have them,
too. Such constraints model constructions that
markedly pertain to one of the dialects.
Feature VARIANT is structure-shared among
all signs comprised in a full parse tree. This
is achieved by having all lexical or syntactic
rules unifying their VARIANT feature with the
VARIANT feature of their daughters.
If two signs (e.g. from lexical items and syn-
tax rules) in the same parse tree have different
values for feature VARIANT (one has ep-variant
and the other bp-variant), they will unify to por-
tuguese, as can be seen from Figure 1. This type
means that lexical items or constructions spe-
cific to two different varieties are used together.
Furthermore, since this feature is shared among
all signs, it will be visible everywhere, for in-
stance in the root node.
It is possible to constrain feature VARIANT in
the root condition of the grammar so that the
grammar works in a variant-”consistent” fash-
ion: this feature just has to be constrained to
be of type single-variant (in root nodes) and
the grammar will accept either European Por-
tuguese or Brazilian Portuguese. Furthermore,
in the non natural condition where the input
string bears marked properties of both vari-
ants, that string will receive no analysis: feature
VARIANT will have the value portuguese in this
case, and there is no unifier for portuguese and
single-variant.
If this feature is constrained to be of type
european-portuguese in the root node, the gram-
mar will not accept any sentence with fea-
tures of Brazilian Portuguese, since they will be
marked to have a VARIANT of type bp-variant,
which is incompatible with european-portuguese.
It is also possible to have the grammar re-
ject European Portuguese (using type brazilian-
portuguese) or to ignore variation completely by
not constraining this feature in the start symbol.
With this grammar design it is thus possi-
ble to control beforehand the mode of operation
for the grammar, either for it to handle only
one variant or several. But it is also possible
to use the grammar to detect to which variety
input happens to belong. This self-tuning of
the grammar to the relevant variant is done by
parsing that input and placing no constraint on
feature VARIANT of root nodes, and then read-
ing the value of attribute VARIANT from the re-
sulting feature structure: values ep-variant and
bp-variant result from parsing text with proper-
ties specific to European Portuguese or Brazilian
Portuguese respectively; value variant indicates
that no marked elements were detected and the
text can be from both variants. Also here where
the language variant of the input is detected by
the grammar, the desired variant-”consistent”
</bodyText>
<page confidence="0.996865">
58
</page>
<bodyText confidence="0.9996615">
behavior of the grammar is enforced.
If the input can be known to be specifically
European or Brazilian Portuguese before it is
parsed, the constraints on feature VARIANT can
be set accordingly to improve efficiency: When
parsing text known to be European Portuguese,
there is no need to explore analyses that are
markedly Brazilian Portuguese, for instance.
It is thus important to discuss what meth-
ods for language variant detection can be put
in place that support a possible pre-processing
step aimed at pre-tuning the grammar for the
relevant variant of the input. It is also impor-
tant to gain insight on the quality of the per-
formance of this method and on how the perfor-
mance of this pre-tuning setup compares with
the self-tuning approach. This is addressed in
the next Sections.
</bodyText>
<sectionHeader confidence="0.997883" genericHeader="method">
3 Experimental setup
</sectionHeader>
<bodyText confidence="0.9999734">
Before reporting on the results obtained with the
experiments on the performance of the two ap-
proaches (self- and pre-tuning), it is important
to introduce the experimental conditions under
which such exercises were conducted.
</bodyText>
<subsectionHeader confidence="0.998238">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999975961038961">
To experiment with any of these two approaches
to variant-tuning, two corpora of newspaper text
were used, CETEMPublico (204M tokens) and
CETENFolha (32M tokens). The first contains
text from the European newspaper O Publico,
and the latter from the South American Folha
de Sdo Paulo. These corpora are only minimally
annotated (paragraph and sentence boundaries,
inter alia), but are very large.
Some preprocessing was carried out: XML-
like tags, like the &lt;s&gt; and &lt;Is&gt; tags marking
sentence boundaries, were removed and each in-
dividual sentence was put in a single line.
Some heuristics were also employed to remove
loose lines (parts of lists, etc.) so that only lines
ending in ., ! and ?, and containing more than 5
tokens (whitespace delimited) were considered.
Other character sequences that were judged ir-
relevant and potential misguiders for the pur-
pose at hand were normalized: URLs were re-
placed by the sequence URL, e-mail addresses
by MAIL, hours and dates by HORA and DATA,
etc. Names at the beginning of lines indicating
speaker (in an interview, for instance) were re-
moved, since they are frequent and the grammar
that will be used is not intended to parse name
plus sentence strings.
The remaining lines were ordered by length in
terms of words and the smallest 200K lines from
each of the two corpora were selected. Small
lines were preferred as they are more likely to
receive an analysis by the grammar.
Given the methods we will be employing for
pre-tuning reportedly perform well even with
small training sets (Section 5), only a modest
portion of text from these corpora was needed.
In the benefit of comparability of the two
approaches for grammar tuning, it is impor-
tant that all the lines in the working data are
parsable by the grammar. Otherwise, even if
in the pre-tuning approach the pre-processor
gets the classification right for non parsable sen-
tences, this will be of no use since the grammar
will not produce any result out of that. 90K lines
of text were thus randomly selected from each
corpus and checked as to whether they could be
parsed by the grammar. 25K of parsable lines of
the American corpus and 21K of parsable lines
of the European corpus were obtained (46K lines
out of 180K, representing 26% rate of parsabil-
ity for the grammar used – more details on this
grammar in the next Section).
It is worth noting that the use of two corpora,
one from an European newspaper and the other
from an American newspaper, without further
annotation, does not allow their appropriate use
in the present set of experiments. The reason
is that if a sentence is found in the European
corpus, one can have almost absolute certainty
that it is possible in European Portuguese, but
one does not know if it is Brazilian Portuguese,
too. The same is true of any sentences in the
American corpus — it can also be a sentence
of European Portuguese in case it only contains
words and structures common to both variants.
In order to prepare the data, a native speaker
of European Portuguese was asked to manually
decide from sentences found in the American
corpus whether they are markedly Brazilian Por-
tuguese. Conversely, a Brazilian informant de-
tected markedly European Portuguese sentences
from the European corpus.
From these parsed lines we drew around 1800
random lines of text from each corpus, and had
them annotated. The lines coming from the
American corpus were annotated for whether
they are markedly Brazilian Portuguese, and
</bodyText>
<page confidence="0.993334">
59
</page>
<bodyText confidence="0.999676842105263">
vice-versa for the other corpus. Thus a three-
way classification is obtained: any sentence
was classified as being markedly Brazilian Por-
tuguese, European Portuguese or common to
both variants.
The large majority of the sentences were
judged to be possible in both European and
Brazilian Portuguese. 16% of the sentences in
the European corpus were considered not be-
longing to Brazilian Portuguese, and 21% of the
sentences in the American corpus were judged as
not being European Portuguese.&apos; Overall, 81%
of the text was common to both varieties.
10KB of text from each one of the three classes
were obtained. 140 lines, approximately 5KB,
were reserved for training and another 140 for
test. In total, the 30 K corpus included 116, 170,
493 and 41 sentence tokens for, respectively, 8,
7, 6 and 5 word length sentence types.
</bodyText>
<subsectionHeader confidence="0.996685">
3.2 Variation
</subsectionHeader>
<bodyText confidence="0.999137">
These training corpora were submitted to man-
ual inspection in order to identify and quantify
the sources of variant specificity. This is impor-
tant to help interpret the experimental results
and to gain insight on the current coverage of
the grammar used in the experiment.
This analysis was performed over the 140 lines
selected as markedly Brazilian Portuguese, and
assumed that the sources of variant specificity
should have broadly the same distribution in
the other 140K lines markedly European Por-
tuguese.
</bodyText>
<listItem confidence="0.99980875">
1. Mere orthographic differences (24%) e.g.
agdo vs. acVdo (action)
2. Phonetic variants reflected in orthography
(9.3%) e.g. ironico vs. ironico (ironic)
</listItem>
<figureCaption confidence="0.829309375">
lA hypothetical explanation for this asymmetry (16%
vs. 21%) is that one of the most pervasive differences
between European and Brazilian Portuguese, clitic place-
ment, is attenuated in writing: Brazilian text often dis-
plays word order between clitic and verb similar to Euro-
pean Portuguese, and different from oral Brazilian Por-
tuguese. Therefore, European text displaying European
clitic order tends not be seen as markedly European. In
fact, we looked at the European sentences with clitic
placement characteristic of European Portuguese that
were judged possible in Brazilian Portuguese. If they
were included in the markedly European sentences, 23%
of the European text would be unacceptable Brazilian
Portuguese, a number closer to the 21% sentences judged
to be exclusively Brazilian Portuguese in the American
corpus.
</figureCaption>
<listItem confidence="0.9974295625">
3. Lexical differences (26.9% of differences)
(a) Different form, same meaning (22.5%)
e.g. time vs. equipa (team)
(b) Same form, different meaning (4.4%)
e.g. policial (policeman/criminal novel
4. Syntactic differences (39.7%)
(a) Possessives w/out articles (12.2%)
(b) In subcategorization frames (9.8%)
(c) Clitic placement (6.4%)
(d) Singular bare NPs (5.4%)
(e) In subcat and word sense (1.9%)
(f) Universal todo + article (0.9%)
(g) Contractions of Prep+article (0.9%)
(h) Questions w/out SV inversion (0.9%)
(i) Postverbal negation (0.5%)
(j) other (0.5%)
</listItem>
<bodyText confidence="0.9994239">
About 1/3 of the differences found would dis-
appear if a unified orthography was adopted.
Differences that are reflected in spelling can be
modeled via multiple lexical entries, with con-
straints on feature VARIANT reflecting the vari-
ety in which the item with that spelling is used.
Interestingly, 40% of the differences are syn-
tactic in nature. These cases are expected to
be more difficult to detect with stochastic ap-
proaches than with a grammar.
</bodyText>
<sectionHeader confidence="0.991948" genericHeader="method">
4 Self-tuning
</sectionHeader>
<subsectionHeader confidence="0.998494">
4.1 Grammar and baseline
</subsectionHeader>
<bodyText confidence="0.999755055555556">
The experiments on the self-tuning approach
were carried out with a computational grammar
for Portuguese developed with the LKB plat-
form (Copestake, 2002) that uses MRS for se-
mantic representation (Copestake et al., 2001)
(Branco and Costa, 2005). At the time of the
experiments reported here, this grammar was
of modest size. In terms of linguistic phenom-
ena, it covered basic declarative sentential struc-
tures and basic phrase structure of all cate-
gories, with a fully detailed account of the struc-
ture of NPs. It contained 42 syntax rules, 37
lexical rules (mostly inflectional) and a total
of 2988 types, with 417 types for lexical en-
tries. There were 2630 hand-built lexical entries,
mostly nouns, with 1000 entries. It was coupled
with a POS tagger for Portuguese, with 97% ac-
curacy (Branco and Silva, 2004).
</bodyText>
<page confidence="0.992579">
60
</page>
<bodyText confidence="0.999153538461538">
In terms of the sources of variant specificity
identified above, this grammar was specifically
designed to handle the co-occurrence of prenom-
inal possessives and determiners and most of the
syntactic constructions related to clitic-verb or-
der. As revealed by the study of the training
corpus, these constructions are responsible for
almost 20% of marked sentences.
The lexicon contained lexical items markedly
European Portuguese and markedly Brazilian
Portuguese. These were taken from the Por-
tuguese Wiktionary, where this information is
available. Leaving aside the very infrequent
items, around 740 marked lexical items were
coded. Items that are variant specific found in
the training corpora (80 more) were also entered
in the lexicon.
These items, markedly belonging to one vari-
ant, were declined into their inflected forms and
the resulting set Lexbsl was used in the following
baseline for dialect tuning: for a sentence s and
Nep, resp. Nbp, the number of tokens of items
in Lexbsl markedly European, resp. Brazilian
Portuguese, occurring in s, s is tagged as Euro-
pean Portuguese if Nep &gt; Nbp, or vice-versa, or
else, ”common” Portuguese if Nep = Nbp = 0.
</bodyText>
<table confidence="0.998987833333333">
Known Predicted class
class EP BP Common Recall
EP 45 0 95 0.32
BP 3 45 92 0.32
Common 4 4 132 0.94
Precision 0.87 0.98 0.41
</table>
<tableCaption confidence="0.999334">
Table 1: Baseline: Confusion matrix.
</tableCaption>
<bodyText confidence="0.9995915">
For this baseline, the figure of 0.53 of overall
accuracy was obtained, detailed in Table 1.2
</bodyText>
<subsectionHeader confidence="0.992322">
4.2 Results with self-tuning
</subsectionHeader>
<bodyText confidence="0.99866019047619">
The results obtained for the self-tuning mode
of operation are presented in Table 2.3 When
the grammar produced multiple analyses for a
2Naturally, extending the operation of this baseline
method beyond the terms of comparability with gram-
mars that handle each sentence at a time, namely by
increasingly extending the number of sentences in the
stretch of text being classified, will virtually lead it to
reach optimal accuracy.
3These figures concern the test corpus, with the three
conditions represented by 1/3 of the sentences, which are
all parsable. Hence, actual recall over a naturally occur-
ring text is expected to be lower. Using the estimate that
only 26% of input receives a parse, that figure for recall
would lie somewhere around 0.15 (= 0.57 x 0.26).
given sentence, that sentence was classified as
markedly European, resp. Brazilian, Portuguese
if all the parses produced VARIANT with type ep-
variant, resp. bp-variant. In all other cases, the
sentence would be classified as common to both
variants.
</bodyText>
<table confidence="0.999319">
Known Predicted class
class EP BP Common Recall
EP 53 1 86 0.38
BP 6 61 73 0.44
Common 14 1 125 0.89
Precision 0.73 0.97 0.44
</table>
<tableCaption confidence="0.995939">
Table 2: Self-tuning: Confusion matrix.
</tableCaption>
<bodyText confidence="0.999925444444444">
Every sentence in the test data was classified,
and the figure of 0.57 was obtained for over-
all accuracy. The analysis of errors shows that
the sentence belonging to Brazilian Portuguese
or to ”common” Portuguese wrongly classified
as European Portuguese contain clitics follow-
ing the European Portuguese syntax, and some
misspellings conforming to the European Por-
tuguese orthography.
</bodyText>
<sectionHeader confidence="0.981318" genericHeader="evaluation">
5 Pre-tuning
</sectionHeader>
<subsectionHeader confidence="0.999255">
5.1 Language Detection Methods
</subsectionHeader>
<bodyText confidence="0.999961458333333">
Methods have been developed to detect the lan-
guage a given text is written in. They have
also been used to discriminate varieties of the
same language, although less often. (Lins and
Gon¸calves, 2004) look up words in dictionaries
to discriminate among languages, and (Oakes,
2003) runs stochastic tests on token frequencies,
like the chi-square test, in order to differentiate
between European and American English.
Many methods are based on frequency of byte
n-grams in text because they can simultaneously
detect language and character encoding (Li and
Momoi, 2001), and can reliably classify short
portions of text. They have been applied in web
browsers (to identify character encodings) and
information retrieval systems.
We are going to focus on methods based on
character n-grams. Because all information used
for classification is taken from characters, and
they can be found in text in much larger quanti-
ties than words or phrases, problems of scarcity
of data are attenuated. Besides, training data
can also be easily found in large amounts be-
cause corpora do not need to be annotated (it is
</bodyText>
<page confidence="0.998431">
61
</page>
<bodyText confidence="0.982512090909091">
only necessary to know the language they belong
to). More importantly, methods based on char-
acter n-grams can reliably classify small portions
of text. The literature on automatic language
identification mentions training corpora as small
as 2K producing classifiers that perform with al-
most perfect accuracy for test strings as little as
500 Bytes (Dunning, 1994) and considering sev-
eral languages. With more training data (20K-
50K of text), similar quality can be achieved for
smaller test strings (Prager, 1999).
Many n-gram based methods have been ex-
plored besides the one we opted for.4 Many
can achieve perfect or nearly perfect classifica-
tion with small training corpora on small texts.
In previous work (Branco and Costa, 2007),
we did a comparative study on two classifiers
that use approaches very well understood in
language processing and information retrieval,
namely Vector Space and Bayesian models. We
retain here the latter as this one scored compar-
atively better for the current purposes.
In order to know which language Li E L gen-
erated string s, Bayesian methods can be used
to calculate the probabilities P(s|Li) of string s
appearing in language Li for all Li E L, the con-
sidered language set, and decide for the language
with the highest score (Dunning, 1994). That is,
in order to compute P(Li|s), we only compute
P(s|Li). The Bayes rule allows us to cast the
problem in terms of P(s|Li)P (Li)
P(s) , but as is stan-
dard practice, the denominator is dropped since
we are only interested here in getting the highest
probability, not its exact value. The prior P(Li)
is also ignored, corresponding to the simplify-
ing assumption that all languages are equally
probable for the operation of the classifier. The
way P(s|Li) is calculated is also the standard
way to do it, namely assuming independence
and just multiplying the probabilities of charac-
ter ci given the preceding n-1 characters (using
n-grams), for all characters in the input (esti-
mated from n-gram counts in the training set).
For our experiments, we implemented the al-
gorithm described in (Dunning, 1994). Other
common strategies were also used, like prepend-
ing n−1 special characters to the input string to
harmonize calculations, summing logs of proba-
bilities instead of multiplying them to avoid un-
&apos;See (Sibun and Reynar, 1996) and (Hughes et al.,
2006) for surveys.
derflow errors, and using Laplace smoothing to
reserve probability mass to events not seen in
training.
</bodyText>
<subsectionHeader confidence="0.999689">
5.2 Calibrating the implementation
5.2.1 Detection of languages
</subsectionHeader>
<bodyText confidence="0.999834666666667">
First of all, we want to check that the lan-
guage identification methods we are using, and
have implemented, are in fact reliable to identify
different languages. Hence, we run the classifier
on three languages showing strikingly different
characters and character sequences. This is a
deliberately easy test to get insight into the ap-
propriate setting of the two parameters at stake
here, size of of the n-gram in the training phase,
and size of the input in the running phase.
For this test, we used the Universal Declara-
tion of Human Rights texts.The languages used
were Finnish, Portuguese and Welsh.5
Several tests were conducted, splitting the
test data in chunks 1, 5, 10 and 20 lines long.
The classifier obtained perfect accuracy on all
test conditions (all chunk sizes), for all values of
n between 1 and 7 (inclusively). For n = 8 and
n = 9 there were errors only when classifying 1
line long items.
The average line length for the test corpora
was 138 characters for Finnish, 141 for Por-
tuguese and 121 for Welsh (133 overall). In the
corpora we will be using in the following experi-
ments, average line length is much lower (around
40 characters per line). To become closer to
our experimental conditions, we also evaluated
this classifiers with the same test corpora, but
truncated each line beyond the first 50 charac-
ters, yielding test corpora with an average line
length around 38 characters (since some were
smaller than that). The results are similar. The
Bayesian classifier performed with less than per-
fect accuracy also with n = 7 when classifying 1
line at a time.
Our classifier was thus performing well at dis-
criminating languages with short values of n,
and can classify short bits of text, even with
incomplete words.
</bodyText>
<footnote confidence="0.95736025">
5The Preamble and Articles 1–19 were used for train-
ing (8.1K of Finnish, 6.9K of Portuguese, and 6.1K of
Welsh), and Articles 20–30 for testing (4.6K of Finnish,
4.7K of Portuguese, and 4.0K of Welsh).
</footnote>
<page confidence="0.997707">
62
</page>
<subsectionHeader confidence="0.868177">
5.2.2 Detection of originating corpus
</subsectionHeader>
<bodyText confidence="0.999807714285714">
In order to study its suitability to discrimi-
nate also the two Portuguese variants, we ex-
perimented our implementation of the Bayesian
classifiers on 200K lines of text from each of the
two corpora. We randomly chose 20K lines for
testing and the remaining 180K for training. A
classification is considered correct if the classi-
fier can guess the newspaper the text was taken
from.
The average line length of the test sentences is
43 characters. Several input lengths were tried
out by dividing the test data into various sets
with varying size. Table 3 summarizes the re-
sults obtained.
</bodyText>
<table confidence="0.98783125">
Length of Test Item
1line 5lines 10 lines 20 lines
2 0.84 0.99 1 1
3 0.96 0.99 1 1
4 0.96 1 1 1
5 0.94 1 1 1
6 0.92 0.99 1 1
7 0.89 0.98 0.99 1
</table>
<tableCaption confidence="0.999903">
Table 3: Originating corpora: Accuracy
</tableCaption>
<bodyText confidence="0.99956725">
The accuracy of the classifier is surprisingly
high given that the sentences that cannot be at-
tributed to a single variety are estimated to be
around 81%.
</bodyText>
<subsubsectionHeader confidence="0.84578">
5.2.3 Scaling down the training data
</subsubsectionHeader>
<bodyText confidence="0.998508684210526">
A final check was made with the classifier
to gain further insight on the comparability of
the results obtained under the two tuning ap-
proaches. It was trained on the data prepared
for the actual experiment, made of the 10K
with lines that have the shortest length and are
parsable, but using only the markedly European
and Brazilian Portuguese data (leaving aside the
sentences judged to be common to both). This
way the two setups can be compared, since in
the test of the Subsection just above much more
data was available for training.
Results are in Table 4. As expected, with
a much smaller amount of training data there
is an overall drop in the accuracy, with a no-
ticed bias at classifying items as European Por-
tuguese. The performance of the classifier de-
grades with larger values of n. Nevertheless, the
classifier is still very good with bigrams, with an
</bodyText>
<table confidence="0.9819834">
Length of Test Item
1line 5lines 10 lines 20 lines
n = 2 0.86 0.98 0.96 1
n = 3 0.82 0.73 0.64 0.5
n = 4 0.68 0.55 0.5 0.5
</table>
<tableCaption confidence="0.999612">
Table 4: Two-way classification: Accuracy
</tableCaption>
<bodyText confidence="0.9976454">
almost optimal performance, only slightly worse
than the one observed in the previous Subsec-
tion, when it was trained with more data.
From these preliminary tests, we learned that
we could expect a quasi optimal performance of
the classifier we implemented to act as a prepro-
cessor in the pre-tuning approach, when n = 2
and it is run under conditions very close to the
ones it will encounter in the actual experiment
aimed at comparing the two tuning approaches.
</bodyText>
<subsectionHeader confidence="0.981406">
5.3 Results with pre-tuning
</subsectionHeader>
<bodyText confidence="0.999762125">
In the final experiment, the classifier should
discriminate between three classes, deciding
whether the input is either specifically Euro-
pean or Brazilian Portuguese, or else whether
it belongs to both variants. It was trained over
the 15K tokens/420 lines of training data, and
tested over the held out test data of identical
size.
</bodyText>
<table confidence="0.9739862">
Length of Test Item
1line 5lines 10 lines 20 lines
n = 2 0.59 0.67 0.76 0.76
n = 3 0.55 0.52 0.45 0.33
n = 4 0.48 0.39 0.33 0.33
</table>
<tableCaption confidence="0.999458">
Table 5: Three-way classification: Accuracy
</tableCaption>
<bodyText confidence="0.9997562">
The results are in Table 5. As expected, the
classifier based in bigrams has the best perfor-
mance for every size of the input, which im-
proves from 0.59 to 0.76 as the size of the input
gets from 1 line to 20 lines.
</bodyText>
<sectionHeader confidence="0.996995" genericHeader="conclusions">
6 Discussion and conclusions
</sectionHeader>
<bodyText confidence="0.999979555555556">
From the results above for pre-tuning, it is the
value 0.59, obtained for 1 line of input, that can
be put on a par with the value of 0.57 obtained
for self-tuning — both of them to be appreciated
against the baseline of 0.53.
Interestingly, the performance of both ap-
proaches are quite similar, and quite encour-
aging given the limitations under which the
present pilot exercise was executed. But this is
</bodyText>
<equation confidence="0.9999485">
n =
n =
n =
n =
n =
n =
</equation>
<page confidence="0.996668">
63
</page>
<bodyText confidence="0.999909537037037">
also the reason why they should be considered
with the appropriate grano salis.
Note that there is much room for improve-
ment in both approaches. From the several
sources of variant specificity, the grammar used
was prepared to cope only with grammatical
constructs that are responsible for at most 20%
of them. Also the lexicon, that included a little
more than 800 variant-distinctive items, can be
largely improved.
As to the classifier used for pre-tuning, it im-
plements methods that may achieve optimal ac-
curacy with training data sets of modest size but
that need to be nevertheless larger than the very
scarce 15K tokens used this time. Using backoff
and interpolation will help to improve as well.
Some features potentially distinguish, how-
ever, the pre-tuning based on Bayesian classifier
from the self-tuning by the grammar.
Language detection methods are easy to scale
up with respect to the number of variants used.
In contrast, the size of the type hierarchy under
variant is exponential on the number of language
variants if all combinations of variants are taken
into account, as it seems reasonable to do.
N-grams based methods are efficient and can
be very accurate. On the other hand, like any
stochastic method, they are sensitive to training
data and tend to be much more affected than the
grammar in self-tuning by a change of text do-
main. Also in dialogue settings with turns from
different language variants, hence with small
lengths of texts available to classify and suc-
cessive alternation between language variants,
n-grams are likely to show less advantage than
self-tuning by fully fledged grammars.
These are issues over which more acute insight
will be gained in future work, which will seek
to improve the contributions put forward in the
present paper.
Summing up, a major contribution of the
present paper is a design strategy for type-
feature grammars that allows them to be appro-
priately set to the specific language variant of a
given input. Concomitantly, this design allows
the grammars either to be pre-tuned or to self-
tune to that dialect – which, to the best of our
knowledge, consists in a new kind of approach to
handling language variation in deep processing.
In addition, we undertook a pilot experiment
which can be taken as setting the basis for a
methodology to comparatively assess the perfor-
mance of these different tuning approaches and
their future improvements.
</bodyText>
<sectionHeader confidence="0.996246" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999721902439024">
Ant´onio Branco and Francisco Costa. 2005. LX-
GRAM – deep linguistic processing of Portuguese
with HSPG. Technical report, Dept. of Informat-
ics, University of Lisbon.
Ant´onio Branco and Francisco Costa. 2007. Han-
dling language variation in deep processing. In
Proc. CLIN2007.
Ant´onio Branco and Jo˜ao Silva. 2004. Evaluat-
ing solutions for the rapid development of state-
of-the-art POS taggers for Portuguese. In Proc.
LREC2004.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan Sag. 2001. Minimal Recursion Semantics:
An introduction. Language and Computation, 3.
Ann Copestake. 2002. Implementing typed feature
structure grammars. CSLI.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS-94-273, Comput-
ing Research Lab, New Mexico State Univ.
Baden Hughes, Timothy Baldwin, Steven Bird,
Jeremy Nicholson, and Andrew MacKinlay. 2006.
Reconsidering language identification for written
language resources. In Proc. LREC2006.
Shanjian Li and Katsuhiko Momoi. 2001. A com-
posite approach to language/encoding detection.
In Proc. 19th International Unicode Conference.
Rafael Lins and Paulo Gon¸calves. 2004. Automatic
language identification of written texts. In Proc.
2004 ACM Symposium on Applied Computing.
Michael P. Oakes. 2003. Text categorization: Auto-
matic discrimination between US and UK English
using the chi-square test and high ratio pairs. Re-
search in Language, 1.
Carl Pollard and Ivan Sag. 1994. Head-driven phrase
structure grammar. CSLI.
John M. Prager. 1999. Linguini: Language iden-
tification for multilingual documents. Journal of
Management Information Systems, 16(3).
Penelope Sibun and Jeffrey C. Reynar. 1996. Lan-
guage identification: Examining the issues. In 5th
Symposium on Document Analysis and IR.
</reference>
<page confidence="0.999417">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.381225">
<title confidence="0.9944855">Selfor Pre-Tuning? Deep linguistic processing of language variants</title>
<author confidence="0.999941">Antonio Branco Francisco Costa</author>
<affiliation confidence="0.995215">Universidade de Lisboa Universidade de Lisboa</affiliation>
<email confidence="0.414101">Antonio.Branco@di.fc.ul.ptfcosta@di.fc.ul.pt</email>
<abstract confidence="0.994817">This paper proposes a design strategy for deep language processing grammars to appropriately handle language variants. It allows a grammar to be restricted as to what language variant it is tuned to, but also to detect the variant a given input pertains to. This is evaluated and compared to results obtained with an alternative strategy by which the relevant variant is detected with current language identification methods in a preprocessing step.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ant´onio Branco</author>
<author>Francisco Costa</author>
</authors>
<title>LXGRAM – deep linguistic processing of Portuguese with HSPG.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Dept. of Informatics, University of Lisbon.</institution>
<contexts>
<context position="15935" citStr="Branco and Costa, 2005" startWordPosition="2591" endWordPosition="2594">eflected in spelling can be modeled via multiple lexical entries, with constraints on feature VARIANT reflecting the variety in which the item with that spelling is used. Interestingly, 40% of the differences are syntactic in nature. These cases are expected to be more difficult to detect with stochastic approaches than with a grammar. 4 Self-tuning 4.1 Grammar and baseline The experiments on the self-tuning approach were carried out with a computational grammar for Portuguese developed with the LKB platform (Copestake, 2002) that uses MRS for semantic representation (Copestake et al., 2001) (Branco and Costa, 2005). At the time of the experiments reported here, this grammar was of modest size. In terms of linguistic phenomena, it covered basic declarative sentential structures and basic phrase structure of all categories, with a fully detailed account of the structure of NPs. It contained 42 syntax rules, 37 lexical rules (mostly inflectional) and a total of 2988 types, with 417 types for lexical entries. There were 2630 hand-built lexical entries, mostly nouns, with 1000 entries. It was coupled with a POS tagger for Portuguese, with 97% accuracy (Branco and Silva, 2004). 60 In terms of the sources of v</context>
</contexts>
<marker>Branco, Costa, 2005</marker>
<rawString>Ant´onio Branco and Francisco Costa. 2005. LXGRAM – deep linguistic processing of Portuguese with HSPG. Technical report, Dept. of Informatics, University of Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ant´onio Branco</author>
<author>Francisco Costa</author>
</authors>
<title>Handling language variation in deep processing.</title>
<date>2007</date>
<booktitle>In Proc. CLIN2007.</booktitle>
<contexts>
<context position="21410" citStr="Branco and Costa, 2007" startWordPosition="3483" endWordPosition="3486">eliably classify small portions of text. The literature on automatic language identification mentions training corpora as small as 2K producing classifiers that perform with almost perfect accuracy for test strings as little as 500 Bytes (Dunning, 1994) and considering several languages. With more training data (20K50K of text), similar quality can be achieved for smaller test strings (Prager, 1999). Many n-gram based methods have been explored besides the one we opted for.4 Many can achieve perfect or nearly perfect classification with small training corpora on small texts. In previous work (Branco and Costa, 2007), we did a comparative study on two classifiers that use approaches very well understood in language processing and information retrieval, namely Vector Space and Bayesian models. We retain here the latter as this one scored comparatively better for the current purposes. In order to know which language Li E L generated string s, Bayesian methods can be used to calculate the probabilities P(s|Li) of string s appearing in language Li for all Li E L, the considered language set, and decide for the language with the highest score (Dunning, 1994). That is, in order to compute P(Li|s), we only compu</context>
</contexts>
<marker>Branco, Costa, 2007</marker>
<rawString>Ant´onio Branco and Francisco Costa. 2007. Handling language variation in deep processing. In Proc. CLIN2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ant´onio Branco</author>
<author>Jo˜ao Silva</author>
</authors>
<title>Evaluating solutions for the rapid development of stateof-the-art POS taggers for Portuguese. In</title>
<date>2004</date>
<booktitle>Proc. LREC2004.</booktitle>
<contexts>
<context position="16502" citStr="Branco and Silva, 2004" startWordPosition="2688" endWordPosition="2691">tation (Copestake et al., 2001) (Branco and Costa, 2005). At the time of the experiments reported here, this grammar was of modest size. In terms of linguistic phenomena, it covered basic declarative sentential structures and basic phrase structure of all categories, with a fully detailed account of the structure of NPs. It contained 42 syntax rules, 37 lexical rules (mostly inflectional) and a total of 2988 types, with 417 types for lexical entries. There were 2630 hand-built lexical entries, mostly nouns, with 1000 entries. It was coupled with a POS tagger for Portuguese, with 97% accuracy (Branco and Silva, 2004). 60 In terms of the sources of variant specificity identified above, this grammar was specifically designed to handle the co-occurrence of prenominal possessives and determiners and most of the syntactic constructions related to clitic-verb order. As revealed by the study of the training corpus, these constructions are responsible for almost 20% of marked sentences. The lexicon contained lexical items markedly European Portuguese and markedly Brazilian Portuguese. These were taken from the Portuguese Wiktionary, where this information is available. Leaving aside the very infrequent items, aro</context>
</contexts>
<marker>Branco, Silva, 2004</marker>
<rawString>Ant´onio Branco and Jo˜ao Silva. 2004. Evaluating solutions for the rapid development of stateof-the-art POS taggers for Portuguese. In Proc. LREC2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Minimal Recursion Semantics: An introduction.</title>
<date>2001</date>
<journal>Language and Computation,</journal>
<volume>3</volume>
<contexts>
<context position="15910" citStr="Copestake et al., 2001" startWordPosition="2587" endWordPosition="2590">d. Differences that are reflected in spelling can be modeled via multiple lexical entries, with constraints on feature VARIANT reflecting the variety in which the item with that spelling is used. Interestingly, 40% of the differences are syntactic in nature. These cases are expected to be more difficult to detect with stochastic approaches than with a grammar. 4 Self-tuning 4.1 Grammar and baseline The experiments on the self-tuning approach were carried out with a computational grammar for Portuguese developed with the LKB platform (Copestake, 2002) that uses MRS for semantic representation (Copestake et al., 2001) (Branco and Costa, 2005). At the time of the experiments reported here, this grammar was of modest size. In terms of linguistic phenomena, it covered basic declarative sentential structures and basic phrase structure of all categories, with a fully detailed account of the structure of NPs. It contained 42 syntax rules, 37 lexical rules (mostly inflectional) and a total of 2988 types, with 417 types for lexical entries. There were 2630 hand-built lexical entries, mostly nouns, with 1000 entries. It was coupled with a POS tagger for Portuguese, with 97% accuracy (Branco and Silva, 2004). 60 In </context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2001</marker>
<rawString>Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan Sag. 2001. Minimal Recursion Semantics: An introduction. Language and Computation, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Implementing typed feature structure grammars.</title>
<date>2002</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="15843" citStr="Copestake, 2002" startWordPosition="2578" endWordPosition="2579">es found would disappear if a unified orthography was adopted. Differences that are reflected in spelling can be modeled via multiple lexical entries, with constraints on feature VARIANT reflecting the variety in which the item with that spelling is used. Interestingly, 40% of the differences are syntactic in nature. These cases are expected to be more difficult to detect with stochastic approaches than with a grammar. 4 Self-tuning 4.1 Grammar and baseline The experiments on the self-tuning approach were carried out with a computational grammar for Portuguese developed with the LKB platform (Copestake, 2002) that uses MRS for semantic representation (Copestake et al., 2001) (Branco and Costa, 2005). At the time of the experiments reported here, this grammar was of modest size. In terms of linguistic phenomena, it covered basic declarative sentential structures and basic phrase structure of all categories, with a fully detailed account of the structure of NPs. It contained 42 syntax rules, 37 lexical rules (mostly inflectional) and a total of 2988 types, with 417 types for lexical entries. There were 2630 hand-built lexical entries, mostly nouns, with 1000 entries. It was coupled with a POS tagger</context>
</contexts>
<marker>Copestake, 2002</marker>
<rawString>Ann Copestake. 2002. Implementing typed feature structure grammars. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Statistical identification of language.</title>
<date>1994</date>
<tech>Technical Report MCCS-94-273,</tech>
<institution>Computing Research Lab, New Mexico State Univ.</institution>
<contexts>
<context position="21040" citStr="Dunning, 1994" startWordPosition="3424" endWordPosition="3425">cters, and they can be found in text in much larger quantities than words or phrases, problems of scarcity of data are attenuated. Besides, training data can also be easily found in large amounts because corpora do not need to be annotated (it is 61 only necessary to know the language they belong to). More importantly, methods based on character n-grams can reliably classify small portions of text. The literature on automatic language identification mentions training corpora as small as 2K producing classifiers that perform with almost perfect accuracy for test strings as little as 500 Bytes (Dunning, 1994) and considering several languages. With more training data (20K50K of text), similar quality can be achieved for smaller test strings (Prager, 1999). Many n-gram based methods have been explored besides the one we opted for.4 Many can achieve perfect or nearly perfect classification with small training corpora on small texts. In previous work (Branco and Costa, 2007), we did a comparative study on two classifiers that use approaches very well understood in language processing and information retrieval, namely Vector Space and Bayesian models. We retain here the latter as this one scored compa</context>
<context position="22764" citStr="Dunning, 1994" startWordPosition="3713" endWordPosition="3714">opped since we are only interested here in getting the highest probability, not its exact value. The prior P(Li) is also ignored, corresponding to the simplifying assumption that all languages are equally probable for the operation of the classifier. The way P(s|Li) is calculated is also the standard way to do it, namely assuming independence and just multiplying the probabilities of character ci given the preceding n-1 characters (using n-grams), for all characters in the input (estimated from n-gram counts in the training set). For our experiments, we implemented the algorithm described in (Dunning, 1994). Other common strategies were also used, like prepending n−1 special characters to the input string to harmonize calculations, summing logs of probabilities instead of multiplying them to avoid un&apos;See (Sibun and Reynar, 1996) and (Hughes et al., 2006) for surveys. derflow errors, and using Laplace smoothing to reserve probability mass to events not seen in training. 5.2 Calibrating the implementation 5.2.1 Detection of languages First of all, we want to check that the language identification methods we are using, and have implemented, are in fact reliable to identify different languages. Henc</context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>Ted Dunning. 1994. Statistical identification of language. Technical Report MCCS-94-273, Computing Research Lab, New Mexico State Univ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baden Hughes</author>
<author>Timothy Baldwin</author>
<author>Steven Bird</author>
<author>Jeremy Nicholson</author>
<author>Andrew MacKinlay</author>
</authors>
<title>Reconsidering language identification for written language resources.</title>
<date>2006</date>
<booktitle>In Proc. LREC2006.</booktitle>
<contexts>
<context position="23016" citStr="Hughes et al., 2006" startWordPosition="3752" endWordPosition="3755">ier. The way P(s|Li) is calculated is also the standard way to do it, namely assuming independence and just multiplying the probabilities of character ci given the preceding n-1 characters (using n-grams), for all characters in the input (estimated from n-gram counts in the training set). For our experiments, we implemented the algorithm described in (Dunning, 1994). Other common strategies were also used, like prepending n−1 special characters to the input string to harmonize calculations, summing logs of probabilities instead of multiplying them to avoid un&apos;See (Sibun and Reynar, 1996) and (Hughes et al., 2006) for surveys. derflow errors, and using Laplace smoothing to reserve probability mass to events not seen in training. 5.2 Calibrating the implementation 5.2.1 Detection of languages First of all, we want to check that the language identification methods we are using, and have implemented, are in fact reliable to identify different languages. Hence, we run the classifier on three languages showing strikingly different characters and character sequences. This is a deliberately easy test to get insight into the appropriate setting of the two parameters at stake here, size of of the n-gram in the </context>
</contexts>
<marker>Hughes, Baldwin, Bird, Nicholson, MacKinlay, 2006</marker>
<rawString>Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy Nicholson, and Andrew MacKinlay. 2006. Reconsidering language identification for written language resources. In Proc. LREC2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shanjian Li</author>
<author>Katsuhiko Momoi</author>
</authors>
<title>A composite approach to language/encoding detection.</title>
<date>2001</date>
<booktitle>In Proc. 19th International Unicode Conference.</booktitle>
<contexts>
<context position="20138" citStr="Li and Momoi, 2001" startWordPosition="3276" endWordPosition="3279">raphy. 5 Pre-tuning 5.1 Language Detection Methods Methods have been developed to detect the language a given text is written in. They have also been used to discriminate varieties of the same language, although less often. (Lins and Gon¸calves, 2004) look up words in dictionaries to discriminate among languages, and (Oakes, 2003) runs stochastic tests on token frequencies, like the chi-square test, in order to differentiate between European and American English. Many methods are based on frequency of byte n-grams in text because they can simultaneously detect language and character encoding (Li and Momoi, 2001), and can reliably classify short portions of text. They have been applied in web browsers (to identify character encodings) and information retrieval systems. We are going to focus on methods based on character n-grams. Because all information used for classification is taken from characters, and they can be found in text in much larger quantities than words or phrases, problems of scarcity of data are attenuated. Besides, training data can also be easily found in large amounts because corpora do not need to be annotated (it is 61 only necessary to know the language they belong to). More impo</context>
</contexts>
<marker>Li, Momoi, 2001</marker>
<rawString>Shanjian Li and Katsuhiko Momoi. 2001. A composite approach to language/encoding detection. In Proc. 19th International Unicode Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael Lins</author>
<author>Paulo Gon¸calves</author>
</authors>
<title>Automatic language identification of written texts.</title>
<date>2004</date>
<booktitle>In Proc. 2004 ACM Symposium on Applied Computing.</booktitle>
<marker>Lins, Gon¸calves, 2004</marker>
<rawString>Rafael Lins and Paulo Gon¸calves. 2004. Automatic language identification of written texts. In Proc. 2004 ACM Symposium on Applied Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael P Oakes</author>
</authors>
<title>Text categorization: Automatic discrimination between US and UK English using the chi-square test and high ratio pairs.</title>
<date>2003</date>
<journal>Research in Language,</journal>
<volume>1</volume>
<contexts>
<context position="19851" citStr="Oakes, 2003" startWordPosition="3235" endWordPosition="3236">uracy. The analysis of errors shows that the sentence belonging to Brazilian Portuguese or to ”common” Portuguese wrongly classified as European Portuguese contain clitics following the European Portuguese syntax, and some misspellings conforming to the European Portuguese orthography. 5 Pre-tuning 5.1 Language Detection Methods Methods have been developed to detect the language a given text is written in. They have also been used to discriminate varieties of the same language, although less often. (Lins and Gon¸calves, 2004) look up words in dictionaries to discriminate among languages, and (Oakes, 2003) runs stochastic tests on token frequencies, like the chi-square test, in order to differentiate between European and American English. Many methods are based on frequency of byte n-grams in text because they can simultaneously detect language and character encoding (Li and Momoi, 2001), and can reliably classify short portions of text. They have been applied in web browsers (to identify character encodings) and information retrieval systems. We are going to focus on methods based on character n-grams. Because all information used for classification is taken from characters, and they can be fo</context>
</contexts>
<marker>Oakes, 2003</marker>
<rawString>Michael P. Oakes. 2003. Text categorization: Automatic discrimination between US and UK English using the chi-square test and high ratio pairs. Research in Language, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Head-driven phrase structure grammar.</title>
<date>1994</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="2630" citStr="Pollard and Sag, 1994" startWordPosition="413" endWordPosition="417">thods developed for language identification can be used also to detect language variants (Section 5). The other approach is to have the computational grammar prepared for self-tuning to the language variant of the input in the course of processing that input (Section 4). We evaluate the two approaches and compare them (last Section 6). 2 Variant-sensitive Grammar In this Section, we discuss the design options for a deep linguistic processing grammar allowing for its appropriate tuning to different language variants. For the sake of concreteness of the discussion, we assume the HPSG framework (Pollard and Sag, 1994) and a grammar that handles two close variants of the same language, European and Brazilian Portuguese. These assumptions are merely instrumental, and the results obtained can be easily extended to other languages and variants, and to other grammatical frameworks for deep linguistic processing. A stretch of text from a language L can display grammatical features common to all variants of L, or contain a construction that pertains to some or only one of its variants. Hence, undesirable overgeneration due to the grammar readiness to cope with all language variants can 57 Proceedings of the ACL 2</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan Sag. 1994. Head-driven phrase structure grammar. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Prager</author>
</authors>
<title>Linguini: Language identification for multilingual documents.</title>
<date>1999</date>
<journal>Journal of Management Information Systems,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="21189" citStr="Prager, 1999" startWordPosition="3448" endWordPosition="3449">ata can also be easily found in large amounts because corpora do not need to be annotated (it is 61 only necessary to know the language they belong to). More importantly, methods based on character n-grams can reliably classify small portions of text. The literature on automatic language identification mentions training corpora as small as 2K producing classifiers that perform with almost perfect accuracy for test strings as little as 500 Bytes (Dunning, 1994) and considering several languages. With more training data (20K50K of text), similar quality can be achieved for smaller test strings (Prager, 1999). Many n-gram based methods have been explored besides the one we opted for.4 Many can achieve perfect or nearly perfect classification with small training corpora on small texts. In previous work (Branco and Costa, 2007), we did a comparative study on two classifiers that use approaches very well understood in language processing and information retrieval, namely Vector Space and Bayesian models. We retain here the latter as this one scored comparatively better for the current purposes. In order to know which language Li E L generated string s, Bayesian methods can be used to calculate the pr</context>
</contexts>
<marker>Prager, 1999</marker>
<rawString>John M. Prager. 1999. Linguini: Language identification for multilingual documents. Journal of Management Information Systems, 16(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Sibun</author>
<author>Jeffrey C Reynar</author>
</authors>
<title>Language identification: Examining the issues.</title>
<date>1996</date>
<booktitle>In 5th Symposium on Document Analysis and IR.</booktitle>
<contexts>
<context position="22990" citStr="Sibun and Reynar, 1996" startWordPosition="3747" endWordPosition="3750"> the operation of the classifier. The way P(s|Li) is calculated is also the standard way to do it, namely assuming independence and just multiplying the probabilities of character ci given the preceding n-1 characters (using n-grams), for all characters in the input (estimated from n-gram counts in the training set). For our experiments, we implemented the algorithm described in (Dunning, 1994). Other common strategies were also used, like prepending n−1 special characters to the input string to harmonize calculations, summing logs of probabilities instead of multiplying them to avoid un&apos;See (Sibun and Reynar, 1996) and (Hughes et al., 2006) for surveys. derflow errors, and using Laplace smoothing to reserve probability mass to events not seen in training. 5.2 Calibrating the implementation 5.2.1 Detection of languages First of all, we want to check that the language identification methods we are using, and have implemented, are in fact reliable to identify different languages. Hence, we run the classifier on three languages showing strikingly different characters and character sequences. This is a deliberately easy test to get insight into the appropriate setting of the two parameters at stake here, siz</context>
</contexts>
<marker>Sibun, Reynar, 1996</marker>
<rawString>Penelope Sibun and Jeffrey C. Reynar. 1996. Language identification: Examining the issues. In 5th Symposium on Document Analysis and IR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>