<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001091">
<title confidence="0.99801">
Maximum Entropy Translation Model
in Dependency-Based MT Framework
</title>
<author confidence="0.993396">
David Mareˇcek, Martin Popel, Zdenˇek ˇZabokrtsk´y
</author>
<affiliation confidence="0.902923">
Charles University in Prague, Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.399331">
Malostransk´e n´am. 25, Praha 1, CZ-118 00, Czech Republic
</address>
<email confidence="0.996301">
{marecek,popel,zabokrtsky}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.99381" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998828043478261">
Maximum Entropy Principle has been
used successfully in various NLP tasks. In
this paper we propose a forward transla-
tion model consisting of a set of maxi-
mum entropy classifiers: a separate clas-
sifier is trained for each (sufficiently fre-
quent) source-side lemma. In this way
the estimates of translation probabilities
can be sensitive to a large number of fea-
tures derived from the source sentence (in-
cluding non-local features, features mak-
ing use of sentence syntactic structure,
etc.). When integrated into English-to-
Czech dependency-based translation sce-
nario implemented in the TectoMT frame-
work, the new translation model signif-
icantly outperforms the baseline model
(MLE) in terms of BLEU. The perfor-
mance is further boosted in a configuration
inspired by Hidden Tree Markov Mod-
els which combines the maximum entropy
translation model with the target-language
dependency tree model.
</bodyText>
<sectionHeader confidence="0.999147" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9854245">
The principle of maximum entropy states that,
given known constraints, the probability distri-
bution which best represents the current state of
knowledge is the one with the largest entropy.
Maximum entropy models based on this princi-
ple have been widely used in Natural Language
Processing, e.g. for tagging (Ratnaparkhi, 1996),
parsing (Charniak, 2000), and named entity recog-
nition (Bender et al., 2003). Maximum entropy
models have the following form
where fi is a feature function, Ai is its weight, and
Z(x) is the normalizing factor
</bodyText>
<equation confidence="0.9866995">
Z(x) = � �exp Aifi(x, y)
y i
</equation>
<bodyText confidence="0.999716676470588">
In statistical machine translation (SMT), trans-
lation model (TM) p(t|s) is the probability that the
string t from the target language is the translation
of the string s from the source language. Typical
approach in SMT is to use backward translation
model p(s|t) according to Bayes’ rule and noisy-
channel model. However, in this paper we deal
only with the forward (direct) model.1
The idea of using maximum entropy for con-
structing forward translation models is not new. It
naturally allows to make use of various features
potentially important for correct choice of target-
language expressions. Let us adopt a motivat-
ing example of such a feature from (Berger et al.,
1996) (which contains the first usage of maxent
translation model we are aware of): “If house ap-
pears within the next three words (e.g., the phrases
in the house and in the red house), then dans might
be a more likely [French] translation [of in].”
Incorporating non-local features extracted from
the source sentence into the standard noisy-
channel model in which only the backward trans-
lation model is available, is not possible. This
drawback of the noisy-channel approach is typi-
cally compensated by using large target-language
n-gram models, which can – in a result – play a
role similar to that of a more elaborate (more con-
text sensitive) forward translation model. How-
ever, we expect that it would be more beneficial to
exploit both the parallel data and the monolingual
data in a more balance fashion, rather than extract
only a reduced amount of information from the
parallel data and compensate it by large language
model on the target side.
</bodyText>
<footnote confidence="0.9400455">
1A backward translation model is used only for pruning
training data in this paper.
</footnote>
<equation confidence="0.955648333333333">
1 �
p(y|x) = Z(x)exp Aifi(x, y)
i
</equation>
<page confidence="0.953681">
201
</page>
<note confidence="0.4434095">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9933969">
A deeper discussion on the potential advantages
of maximum entropy approach over the noisy-
channel approach can be found in (Foster, 2000)
and (Och and Ney, 2002), in which another suc-
cessful applications of maxent translation models
are shown. Log-linear translation models (instead
of MLE) with rich feature sets are used also in
(Ittycheriah and Roukos, 2007) and (Gimpel and
Smith, 2009); the idea can be traced back to (Pap-
ineni et al., 1997).
What makes our approach different from the
previously published works is that
1. we show how the maximum entropy trans-
lation model can be used in a dependency
framework; we use deep-syntactic depen-
dency trees (as defined in the Prague Depen-
dency Treebank (Hajiˇc et al., 2006)) as the
transfer layer,
2. we combine the maximum entropy transla-
tion model with target-language dependency
tree model and use tree-modified Viterbi
search for finding the optimal lemmas label-
ing of the target-tree nodes.
The rest of the paper is structured as follows. In
Section 2 we give a brief overview of the trans-
lation framework TectoMT in which the experi-
ments are implemented. In Section 3 we describe
how our translation models are constructed. Sec-
tion 4 summarizes the experimental results, and
Section 5 contains a summary.
</bodyText>
<sectionHeader confidence="0.968885" genericHeader="method">
2 Translation framework
</sectionHeader>
<bodyText confidence="0.99995420754717">
We use tectogrammatical (deep-syntactic) layer of
language representation as the transfer layer in the
presented MT experiments. Tectogrammatics was
introduced in (Sgall, 1967) and further elaborated
within the Prague Dependency Treebank project
(Hajiˇc et al., 2006). On this layer, each sentence
is represented as a tectogrammatical tree, whose
main properties (from the MT viewpoint) are fol-
lowing: (1) nodes represent autosemantic words,
(2) edges represent semantic dependencies (a node
is an argument or a modifier of its parent), (3) there
are no functional words (prepositions, auxiliary
words) in the tree, and the autosemantic words ap-
pear only in their base forms (lemmas). Morpho-
logically indispensable categories (such as number
with nouns or tense with verbs, but not number
with verbs as it is only imposed by agreement) are
stored in separate node attributes (grammatemes).
The intuition behind the decision to use tec-
togrammatics for MT is the following: we be-
lieve that (1) tectogrammatics largely abstracts
from language-specific means (inflection, agglu-
tination, functional words etc.) of expressing
non-lexical meanings and thus tectogrammatical
trees are supposed to be highly similar across lan-
guages,2 (2) it enables a natural transfer factor-
ization,3 (3) and local tree contexts in tectogram-
matical trees carry more information (especially
for lexical choice) than local linear contexts in the
original sentences.4
In order to facilitate transfer of sentence ‘syn-
tactization’, we work with tectogrammatical nodes
enhanced with the formeme attribute (ˇZabokrtsk´y
et al., 2008), which captures the surface mor-
phosyntactic form of a given tectogrammatical
node in a compact fashion. For example, the
value n:pˇred+4 is used to label semantic nouns
that should appear in an accusative form in a
prepositional group with the preposition pˇred in
Czech. For English we use formemes such as
n:subj (semantic noun (SN) in subject position),
n:for+X (SN with preposition for), n:X+ago (SN
with postposition ago), n:poss (possessive form of
SN), v:because+fin (semantic verb (SV) as a sub-
ordinating finite clause introduced by because),
v:without+ger (SV as a gerund after without), adj:attr
(semantic adjective (SA) in attributive position),
adj:compl (SA in complement position).
We have implemented our experiments in the
TectoMT software framework, which already of-
fers tool chains for analysis and synthesis of Czech
and English sentences (ˇZabokrtsk´y et al., 2008).
The translation scenario proceeds as follows.
</bodyText>
<listItem confidence="0.9733374">
1. The input English text is segmented into sen-
tences and tokens.
2. The tokens are lemmatized and tagged with
Penn Treebank tags using the Morce tagger
(Spoustov´a et al., 2007).
</listItem>
<bodyText confidence="0.739463214285714">
2This claim is supported by error analysis of output of
tectogrammatics-based MT system presented in (Popel and
ˇZabok/rtsk´y, 2009), which shows that only 8 % of translation
errors are caused by the (obviously too strong) assumption
that the tectogrammatical tree of a sentence and the tree rep-
resenting its translation are isomorphic.
3Morphological categories can be translated almost inde-
pendently from lemmas, which makes parallel training data
‘denser’, especially when translating from/to a language with
rich inflection such as Czech.
4Recall the house-is-somewhere-around feature in the in-
troduction; again, the fact that we know the dominating (or
dependent) word should allow to construct a more compact
translation model, compared to n-gram models.
</bodyText>
<page confidence="0.996026">
202
</page>
<figureCaption confidence="0.848537333333333">
Figure 1: Intermediate sentence representations when translating the English sentence “However, this
very week, he tried to find refuge in Brazil.”, leading to the Czech translation “Pˇresto se tento pr´avˇe
t´yden snaˇzil najit ´utoˇciˇstˇe v Braz´ılii.”.
</figureCaption>
<bodyText confidence="0.794238571428571">
3. Then the Maximum Spanning Tree parser
(McDonald et al., 2005) is applied and a
surface-syntax dependency tree (analytical
tree in the PDT terminology) is created for
each sentence (Figure 1a).
4. This tree is converted to a tectogrammatical
tree (Figure 1b). Each autosemantic word
with its associated functional words is col-
lapsed into a single tectogrammatical node,
labeled with lemma, formeme, and seman-
tically indispensable morphologically cate-
gories; coreference is also resolved. Collaps-
ing edges are depicted by wider lines in the
Figure 1a.
</bodyText>
<listItem confidence="0.831812875">
5. The transfer phase follows, whose most dif-
ficult part consists in labeling the tree with
target-side lemmas and formemes5 (changes
of tree topology are required relatively infre-
quently). See Figure 1c.
6. Finally, surface sentence shape (Figure 1d) is
synthesized from the tectogrammatical tree,
which is basically a reverse operation for the
</listItem>
<footnote confidence="0.879283666666667">
5In this paper we focus on using maximum entropy
for translating lemmas, but it can be used for translating
formemes as well.
</footnote>
<bodyText confidence="0.999211333333333">
tectogrammatical analysis: adding punctua-
tion and functional words, spreading mor-
phological categories according to grammat-
ical agreement, performing inflection (using
Czech morphology database (Hajiˇc, 2004)),
arranging word order etc.
</bodyText>
<sectionHeader confidence="0.952636" genericHeader="method">
3 Training the two models
</sectionHeader>
<bodyText confidence="0.999953090909091">
In this section we describe two translation mod-
els used in the experiments: a baseline translation
model based on maximum likelihood estimates
(3.2), and a maximum entropy based model (3.3).
Both models are trained using the same data (3.1).
In addition, we describe a target-language tree
model (3.4), which can be combined with both
the translation models using the Hidden Tree
Markov Model approach and tree-modified Viterbi
search, similarly to the approach of (ˇZabokrtsk´y
and Popel, 2009).
</bodyText>
<subsectionHeader confidence="0.935776">
3.1 Data preprocessing common for both
models
</subsectionHeader>
<bodyText confidence="0.9986734">
We used Czech-English parallel corpus CzEng 0.9
(Bojar and ˇZabokrtsk´y, 2009) for training the
translation models. CzEng 0.9 contains about
8 million sentence pairs, and also their tectogram-
matical analyses and node-wise alignment.
</bodyText>
<page confidence="0.994919">
203
</page>
<bodyText confidence="0.999812428571428">
We used only trees from training sections (about
80 % of the whole data), which contain around 30
million pairs of aligned tectogrammatical nodes.
From each pair of aligned tectogrammatical
nodes, we extracted triples containing the source
(English) lemma, the target (Czech) lemma, and
the feature vector.
In order to reduce noise in the training data,
we pruned the data in two ways. First, we dis-
regarded all triples whose lemma pair did not oc-
cur at least twice in the whole data. Second,
we computed forward and backward maximum
likelihood (ML) translation models (target lemma
given source lemma and vice versa) and deleted
all triples whose probability according to one of
the two models was lower than the threshold 0.01.
Then the forward ML translation model was
reestimated using only the remaining data.
For a given pair of aligned nodes, the feature
vector was of course derived only from the source-
side node or from the tree which it belongs to. As
already mentioned in the introduction, the advan-
tage of the maximum entropy approach is that a
rich and diverse set of features can be used, with-
out limiting oneself to linearly local context. The
following features (or, better to say, feature tem-
plates, as each categorical feature is in fact con-
verted to a number of 0-1 features) were used:
</bodyText>
<listItem confidence="0.999356">
• formeme and morphological categories of the
given node,
• lemma, formeme and morphological cate-
gories of the governing node,
• lemmas and formemes of all child nodes,
• lemmas and formemes of the nearest linearly
preceding and following nodes.
</listItem>
<subsectionHeader confidence="0.997547">
3.2 Baseline translation model
</subsectionHeader>
<bodyText confidence="0.999982307692308">
The baseline TM is basically the ML translation
model resulting from the previous section, lin-
early interpolated with several translation models
making use of regular word-formative derivations,
which can be helpful for translating some less fre-
quent (but regularly derived) lemmas. For exam-
ple, one of the derivation-based models estimates
the probability p(zaj´ımavˇejinterestingly) (possibly
unseen pair of deadjectival adverbs) by the value
of p(zaj´ımav´yjinteresting). More detailed descrip-
tion of these models goes beyond the scope of this
paper; their weights in the interpolation are very
small anyway.
</bodyText>
<subsectionHeader confidence="0.997724">
3.3 MaxEnt translation model
</subsectionHeader>
<bodyText confidence="0.995444">
The MaxEnt TM was created as follows:
</bodyText>
<listItem confidence="0.87579715">
1. training triples (source lemma, target lemma,
feature vector) were disregarded if the source
lemma was not seen at least 50 times (only
the baseline model will be used for such lem-
mas),
2. the remaining triples were grouped by the En-
glish lemma (over 16 000 groups),
3. due to computational issues, the maximum
number of triples in a group was reduced to
1000 by random selection,
4. a separate maximum entropy classifier
was trained for each group (i.e., one
classifier per source-side lemma) using
AI::MaxEntropy Perl module,6
5. due to the more aggressive pruning of the
training data, coverage of this model is
smaller than that of the baseline model; in or-
der not to loose the coverage, the two mod-
els were combined using linear interpolation
(1:1).
</listItem>
<bodyText confidence="0.982328285714286">
Selected properties of the maximum entropy
translation model (before the linear interpolation
with the baseline model) are shown in Figure 2.
We increased the size of the training data from
10 000 training triples up to 31 million and eval-
uated three relative quantities characterizing the
translation models:
</bodyText>
<listItem confidence="0.997875285714286">
• coverage - relative frequency of source lem-
mas for which the translation model offers at
least one translation,
• first - relative frequency of source lemmas for
which the target lemmas offered as the first
by the model (argmax) are the correct ones,
• oracle - relative frequency of source lemmas
</listItem>
<bodyText confidence="0.991488857142857">
for which the correct target lemma is among
the lemmas offered by the translation model.
As mentioned in Section 3.1, there are context
features making use both of local linear context
and local tree context. After training the MaxEnt
model, there are about 4.5 million features with
non-zero weight, out of which 1.1 million features
</bodyText>
<footnote confidence="0.992707">
6http://search.cpan.org/perldoc?AI::
MaxEntropy
</footnote>
<page confidence="0.995922">
204
</page>
<figureCaption confidence="0.86816925">
Figure 2: Three measures characterizing the Max-
Ent translation model performance, depending on
the training data size. Evaluated on aligned node
pairs from the dtest portion of CzEng 0.9.
</figureCaption>
<bodyText confidence="0.999831617647059">
are derived from the linear context and 2.4 million
features are derived from the tree context. This
shows that the MaxEnt translation model employs
the dependency structure intensively.
A preliminary analysis of feature weights seems
to support our intuition that the linear context
is preferred especially in the case of more sta-
ble collocations. For example, the most impor-
tant features for translating the lemma bare are
based on the lemma of the following noun: tar-
get lemma bos´y (barefooted) is preferred if the fol-
lowing noun on the source side is foot, while hol´y
(naked, unprotected) is preferred if hand follows.
The contribution of dependency-based features
can be illustrated on translating the word drop.
The greatest weight for choosing kapka (a droplet)
as the translation is assigned to the feature captur-
ing the presence of a node with formeme n:of+X
among the node’s children. The greatest weights
in favor of odhodit (throw aside) are assigned to
features capturing the presence of words such as
gun or weapon, while the greatest weights in favor
of klesnout (to come down) are assigned to fea-
tures saying that there is the lemma percent or the
percent sign among the children.
Of course, the lexical choice is influenced also
by the governing lemmas, as can be illustrated
with the word native. One can find a high-
value feature for rodil´y (native-born) saying that
the source-side parent is speaker; similarly for
mateˇrsk´y (mother) with governing tongue, and
rodn´y (home) with land.
Linear and tree features are occasionally used
simultaneously: there are high-valued positive
</bodyText>
<table confidence="0.9598498">
configuration
baseline TM
MaxEnt TM
baseline TM + TreeLM
MaxEnt TM + TreeLM
</table>
<tableCaption confidence="0.991467">
Table 1: BLEU and NIST evaluation of four con-
</tableCaption>
<bodyText confidence="0.944673142857143">
figurations of our MT system; the WMT 2010 test
set was used.
weights for translating order as objednat (reserve,
give an order for st.) assigned both to tree-based
features saying that there are words such as pizza,
meal or goods and to linear features saying that the
very following word is some or two.
</bodyText>
<subsectionHeader confidence="0.995759">
3.4 Target-language tree model
</subsectionHeader>
<bodyText confidence="0.999968">
Although the MaxEnt TM captures some contex-
tual dependencies that are covered by language
models in the standard noisy-channel SMT, it may
still be beneficial to exploit target-language mod-
els, because these can be trained on huge mono-
lingual corpora. We use a target-language depen-
dency tree model differing from standard n-gram
model in two aspects:
</bodyText>
<listItem confidence="0.936544666666667">
• it uses tree context instead of linear context,
• it predicts tectogrammatical attributes (lem-
mas and formemes) instead of word forms.
</listItem>
<bodyText confidence="0.999994">
In particular, our target-language tree model
(TreeLM) predicts the probability of node’s
lemma and formeme given its parent’s lemma and
formeme. The optimal (lemma and formeme) la-
beling is found by tree-modified Viterbi search;
for details see (ˇZabokrtsk´y and Popel, 2009).
</bodyText>
<sectionHeader confidence="0.999834" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999875083333333">
When included into the above described transla-
tion scenario, the MaxEnt TM outperforms the
baseline TM, be it used together with or with-
out TreeLM. The results are summarized in Ta-
ble 1. The improvement is statistically signif-
icant according to paired bootstrap resampling
test (Koehn, 2004). In the configuration without
TreeLM the improvement is greater (1.33 BLEU)
than with TreeLM (0.81 BLEU), which confirms
our hypothesis that MaxEnt TM captures some of
the contextual dependencies resolved otherwise by
language models.
</bodyText>
<figure confidence="0.6425536">
BLEU NIST
10.44 4.795
11.77 5.135
11.77 5.038
12.58 5.250
</figure>
<page confidence="0.99527">
205
</page>
<sectionHeader confidence="0.999304" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999859">
We have introduced a maximum entropy transla-
tion model in dependency-based MT which en-
ables exploiting a large number of feature func-
tions in order to obtain more accurate translations.
The BLEU evaluation proved significant improve-
ment over the baseline solution based on the trans-
lation model with maximum likelihood estimates.
However, the performance of this system still be-
low the state of the art (which is around BLEU 16
for the English-to-Czech direction).
</bodyText>
<sectionHeader confidence="0.997388" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998950166666667">
This research was supported by the grants
MSM0021620838, MˇSMT ˇCR LC536, FP7-ICT-
2009-4-247762 (Faust), FP7-ICT-2007-3-231720
(EuroMatrix Plus), GA201/09/H057, and GAUK
116310. We thank two anonymous reviewers for
helpful comments.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9990901125">
Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum entropy models for named entity
recognition. In Proceedings of CoNLL 2003, pages
148–151.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39–71.
Ondˇrej Bojar and Zdenˇek ˇZabokrtsk´y. 2009. CzEng
0.9, Building a Large Czech-English Automatic Par-
allel Treebank. The Prague Bulletin of Mathemati-
cal Linguistics, 92:63–83.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the ACL conference, pages
132–139, San Francisco, USA.
George Foster. 2000. A maximum entropy/minimum
divergence translation model. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 45–52, Morristown, USA.
Association for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2009. Feature-
rich translation by quasi-synchronous lattice pars-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 219–228, Morristown, USA. Association for
Computational Linguistics.
Jan Hajiˇc et al. 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T01, Philadelphia.
Jan Hajiˇc. 2004. Disambiguation of Rich Inflection –
Computational Morphology of Czech. Charles Uni-
versity – The Karolinum Press, Prague.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Candace L. Sidner, Tanja
Schultz, Matthew Stone, and ChengXiang Zhai, edi-
tors, HLT-NAACL, pages 57–64. The Association for
Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, volume 4, pages 388–395.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of HLT / EMNLP, pages 523–530, Vancouver,
Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL,
pages 295–302.
Kishore A. Papineni, Salim Roukos, and Todd R.
Ward. 1997. Feature-based language understand-
ing. In European Conference on Speech Commu-
nication and Technology (EUROSPEECH), pages
1435–1438, Rhodes, Greece, September.
Martin Popel and Zdenˇek ˇZabok/rtsk´y. 2009.
Improving English-Czech Tectogrammatical MT.
The Prague Bulletin of Mathematical Linguistics,
(92):1–20.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In In Proceedings
of EMNLP’96, pages 133–142.
Petr Sgall. 1967. Generativnipopis jazyka a ˇcesk´a
deklinace. Academia, Prague.
Drahomira Spoustov´a, Jan Hajiˇc, Jan Votrubec, Pavel
Krbec, and Pavel Kvˇetoˇn. 2007. The Best of Two
Worlds: Cooperation of Statistical and Rule-Based
Taggers for Czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67–74, Praha.
Zdenˇek ˇZabokrtsk´y and Martin Popel. 2009. Hidden
markov tree model in dependency-based machine
translation. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 145–148, Sun-
tec, Singapore.
Zdenˇek ˇZabokrtsk´y, Jan Pt´aˇcek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the 3rd Workshop on Statistical Machine
Translation, ACL, pages 167–170.
</reference>
<page confidence="0.998869">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.570387">
<title confidence="0.9954085">Maximum Entropy Translation in Dependency-Based MT Framework</title>
<author confidence="0.723486">Martin Popel Mareˇcek</author>
<author confidence="0.723486">Zdenˇek</author>
<affiliation confidence="0.683709">Charles University in Prague, Institute of Formal and Applied</affiliation>
<address confidence="0.652011">Malostransk´e n´am. 25, Praha 1, CZ-118 00, Czech</address>
<abstract confidence="0.999708958333333">Maximum Entropy Principle has been used successfully in various NLP tasks. In this paper we propose a forward translation model consisting of a set of maximum entropy classifiers: a separate classifier is trained for each (sufficiently frequent) source-side lemma. In this way the estimates of translation probabilities can be sensitive to a large number of features derived from the source sentence (including non-local features, features making use of sentence syntactic structure, etc.). When integrated into English-to- Czech dependency-based translation scenario implemented in the TectoMT framework, the new translation model significantly outperforms the baseline model (MLE) in terms of BLEU. The performance is further boosted in a configuration inspired by Hidden Tree Markov Models which combines the maximum entropy translation model with the target-language dependency tree model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Oliver Bender</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Maximum entropy models for named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<pages>148--151</pages>
<contexts>
<context position="1616" citStr="Bender et al., 2003" startWordPosition="233" endWordPosition="236"> BLEU. The performance is further boosted in a configuration inspired by Hidden Tree Markov Models which combines the maximum entropy translation model with the target-language dependency tree model. 1 Introduction The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy. Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996), parsing (Charniak, 2000), and named entity recognition (Bender et al., 2003). Maximum entropy models have the following form where fi is a feature function, Ai is its weight, and Z(x) is the normalizing factor Z(x) = � �exp Aifi(x, y) y i In statistical machine translation (SMT), translation model (TM) p(t|s) is the probability that the string t from the target language is the translation of the string s from the source language. Typical approach in SMT is to use backward translation model p(s|t) according to Bayes’ rule and noisychannel model. However, in this paper we deal only with the forward (direct) model.1 The idea of using maximum entropy for constructing forw</context>
</contexts>
<marker>Bender, Och, Ney, 2003</marker>
<rawString>Oliver Bender, Franz Josef Och, and Hermann Ney. 2003. Maximum entropy models for named entity recognition. In Proceedings of CoNLL 2003, pages 148–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="2453" citStr="Berger et al., 1996" startWordPosition="376" endWordPosition="379">model (TM) p(t|s) is the probability that the string t from the target language is the translation of the string s from the source language. Typical approach in SMT is to use backward translation model p(s|t) according to Bayes’ rule and noisychannel model. However, in this paper we deal only with the forward (direct) model.1 The idea of using maximum entropy for constructing forward translation models is not new. It naturally allows to make use of various features potentially important for correct choice of targetlanguage expressions. Let us adopt a motivating example of such a feature from (Berger et al., 1996) (which contains the first usage of maxent translation model we are aware of): “If house appears within the next three words (e.g., the phrases in the house and in the red house), then dans might be a more likely [French] translation [of in].” Incorporating non-local features extracted from the source sentence into the standard noisychannel model in which only the backward translation model is available, is not possible. This drawback of the noisy-channel approach is typically compensated by using large target-language n-gram models, which can – in a result – play a role similar to that of a m</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<title>CzEng 0.9, Building a Large Czech-English Automatic Parallel Treebank.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>92--63</pages>
<marker>Bojar, ˇZabokrtsk´y, 2009</marker>
<rawString>Ondˇrej Bojar and Zdenˇek ˇZabokrtsk´y. 2009. CzEng 0.9, Building a Large Czech-English Automatic Parallel Treebank. The Prague Bulletin of Mathematical Linguistics, 92:63–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the ACL conference,</booktitle>
<pages>132--139</pages>
<location>San Francisco, USA.</location>
<contexts>
<context position="1564" citStr="Charniak, 2000" startWordPosition="226" endWordPosition="227">utperforms the baseline model (MLE) in terms of BLEU. The performance is further boosted in a configuration inspired by Hidden Tree Markov Models which combines the maximum entropy translation model with the target-language dependency tree model. 1 Introduction The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy. Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996), parsing (Charniak, 2000), and named entity recognition (Bender et al., 2003). Maximum entropy models have the following form where fi is a feature function, Ai is its weight, and Z(x) is the normalizing factor Z(x) = � �exp Aifi(x, y) y i In statistical machine translation (SMT), translation model (TM) p(t|s) is the probability that the string t from the target language is the translation of the string s from the source language. Typical approach in SMT is to use backward translation model p(s|t) according to Bayes’ rule and noisychannel model. However, in this paper we deal only with the forward (direct) model.1 The</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st North American chapter of the ACL conference, pages 132–139, San Francisco, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
</authors>
<title>A maximum entropy/minimum divergence translation model.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>45--52</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, USA.</location>
<contexts>
<context position="3846" citStr="Foster, 2000" startWordPosition="607" endWordPosition="608">a more balance fashion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side. 1A backward translation model is used only for pruning training data in this paper. 1 � p(y|x) = Z(x)exp Aifi(x, y) i 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the tran</context>
</contexts>
<marker>Foster, 2000</marker>
<rawString>George Foster. 2000. A maximum entropy/minimum divergence translation model. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 45–52, Morristown, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Featurerich translation by quasi-synchronous lattice parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>219--228</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, USA.</location>
<contexts>
<context position="4099" citStr="Gimpel and Smith, 2009" startWordPosition="645" endWordPosition="648">paper. 1 � p(y|x) = Z(x)exp Aifi(x, y) i 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combine the maximum entropy translation model with target-language dependency tree model and use tree-modified Viterbi search for finding the optimal lemmas labeling of the target-tree nodes. The rest of the paper is structured as foll</context>
</contexts>
<marker>Gimpel, Smith, 2009</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2009. Featurerich translation by quasi-synchronous lattice parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219–228, Morristown, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<date>2006</date>
<booktitle>Prague Dependency Treebank 2.0. CD-ROM, Linguistic Data Consortium, LDC Catalog No.: LDC2006T01,</booktitle>
<location>Philadelphia.</location>
<marker>Hajiˇc, 2006</marker>
<rawString>Jan Hajiˇc et al. 2006. Prague Dependency Treebank 2.0. CD-ROM, Linguistic Data Consortium, LDC Catalog No.: LDC2006T01, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<date>2004</date>
<booktitle>Disambiguation of Rich Inflection – Computational Morphology of Czech.</booktitle>
<publisher>Charles University – The Karolinum Press,</publisher>
<location>Prague.</location>
<marker>Hajiˇc, 2004</marker>
<rawString>Jan Hajiˇc. 2004. Disambiguation of Rich Inflection – Computational Morphology of Czech. Charles University – The Karolinum Press, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>Direct translation model 2.</title>
<date>2007</date>
<pages>57--64</pages>
<editor>In Candace L. Sidner, Tanja Schultz, Matthew Stone, and ChengXiang Zhai, editors, HLT-NAACL,</editor>
<publisher>The Association for Computational Linguistics.</publisher>
<contexts>
<context position="4070" citStr="Ittycheriah and Roukos, 2007" startWordPosition="640" endWordPosition="643"> for pruning training data in this paper. 1 � p(y|x) = Z(x)exp Aifi(x, y) i 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combine the maximum entropy translation model with target-language dependency tree model and use tree-modified Viterbi search for finding the optimal lemmas labeling of the target-tree nodes. The rest of th</context>
</contexts>
<marker>Ittycheriah, Roukos, 2007</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2007. Direct translation model 2. In Candace L. Sidner, Tanja Schultz, Matthew Stone, and ChengXiang Zhai, editors, HLT-NAACL, pages 57–64. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>4</volume>
<pages>388--395</pages>
<contexts>
<context position="18137" citStr="Koehn, 2004" startWordPosition="2855" endWordPosition="2856">memes) instead of word forms. In particular, our target-language tree model (TreeLM) predicts the probability of node’s lemma and formeme given its parent’s lemma and formeme. The optimal (lemma and formeme) labeling is found by tree-modified Viterbi search; for details see (ˇZabokrtsk´y and Popel, 2009). 4 Experiments When included into the above described translation scenario, the MaxEnt TM outperforms the baseline TM, be it used together with or without TreeLM. The results are summarized in Table 1. The improvement is statistically significant according to paired bootstrap resampling test (Koehn, 2004). In the configuration without TreeLM the improvement is greater (1.33 BLEU) than with TreeLM (0.81 BLEU), which confirms our hypothesis that MaxEnt TM captures some of the contextual dependencies resolved otherwise by language models. BLEU NIST 10.44 4.795 11.77 5.135 11.77 5.038 12.58 5.250 205 5 Conclusions We have introduced a maximum entropy translation model in dependency-based MT which enables exploiting a large number of feature functions in order to obtain more accurate translations. The BLEU evaluation proved significant improvement over the baseline solution based on the translation</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP, volume 4, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT / EMNLP,</booktitle>
<pages>523--530</pages>
<location>Vancouver, Canada.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT / EMNLP, pages 523–530, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="3870" citStr="Och and Ney, 2002" startWordPosition="610" endWordPosition="613">ion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side. 1A backward translation model is used only for pruning training data in this paper. 1 � p(y|x) = Z(x)exp Aifi(x, y) i 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combin</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore A Papineni</author>
<author>Salim Roukos</author>
<author>Todd R Ward</author>
</authors>
<title>Feature-based language understanding.</title>
<date>1997</date>
<booktitle>In European Conference on Speech Communication and Technology (EUROSPEECH),</booktitle>
<pages>1435--1438</pages>
<location>Rhodes, Greece,</location>
<contexts>
<context position="4155" citStr="Papineni et al., 1997" startWordPosition="656" endWordPosition="660">of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combine the maximum entropy translation model with target-language dependency tree model and use tree-modified Viterbi search for finding the optimal lemmas labeling of the target-tree nodes. The rest of the paper is structured as follows. In Section 2 we give a brief overview of the transl</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1997</marker>
<rawString>Kishore A. Papineni, Salim Roukos, and Todd R. Ward. 1997. Feature-based language understanding. In European Conference on Speech Communication and Technology (EUROSPEECH), pages 1435–1438, Rhodes, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Popel</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<date>2009</date>
<booktitle>Improving English-Czech Tectogrammatical MT. The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>92--1</pages>
<marker>Popel, ˇZabokrtsk´y, 2009</marker>
<rawString>Martin Popel and Zdenˇek ˇZabok/rtsk´y. 2009. Improving English-Czech Tectogrammatical MT. The Prague Bulletin of Mathematical Linguistics, (92):1–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging. In</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP’96,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="1538" citStr="Ratnaparkhi, 1996" startWordPosition="223" endWordPosition="224">slation model significantly outperforms the baseline model (MLE) in terms of BLEU. The performance is further boosted in a configuration inspired by Hidden Tree Markov Models which combines the maximum entropy translation model with the target-language dependency tree model. 1 Introduction The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy. Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996), parsing (Charniak, 2000), and named entity recognition (Bender et al., 2003). Maximum entropy models have the following form where fi is a feature function, Ai is its weight, and Z(x) is the normalizing factor Z(x) = � �exp Aifi(x, y) y i In statistical machine translation (SMT), translation model (TM) p(t|s) is the probability that the string t from the target language is the translation of the string s from the source language. Typical approach in SMT is to use backward translation model p(s|t) according to Bayes’ rule and noisychannel model. However, in this paper we deal only with the fo</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In In Proceedings of EMNLP’96, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
</authors>
<title>Generativnipopis jazyka a ˇcesk´a deklinace.</title>
<date>1967</date>
<location>Academia, Prague.</location>
<contexts>
<context position="5171" citStr="Sgall, 1967" startWordPosition="820" endWordPosition="821">e tree-modified Viterbi search for finding the optimal lemmas labeling of the target-tree nodes. The rest of the paper is structured as follows. In Section 2 we give a brief overview of the translation framework TectoMT in which the experiments are implemented. In Section 3 we describe how our translation models are constructed. Section 4 summarizes the experimental results, and Section 5 contains a summary. 2 Translation framework We use tectogrammatical (deep-syntactic) layer of language representation as the transfer layer in the presented MT experiments. Tectogrammatics was introduced in (Sgall, 1967) and further elaborated within the Prague Dependency Treebank project (Hajiˇc et al., 2006). On this layer, each sentence is represented as a tectogrammatical tree, whose main properties (from the MT viewpoint) are following: (1) nodes represent autosemantic words, (2) edges represent semantic dependencies (a node is an argument or a modifier of its parent), (3) there are no functional words (prepositions, auxiliary words) in the tree, and the autosemantic words appear only in their base forms (lemmas). Morphologically indispensable categories (such as number with nouns or tense with verbs, bu</context>
</contexts>
<marker>Sgall, 1967</marker>
<rawString>Petr Sgall. 1967. Generativnipopis jazyka a ˇcesk´a deklinace. Academia, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahomira Spoustov´a</author>
<author>Jan Hajiˇc</author>
<author>Jan Votrubec</author>
<author>Pavel Krbec</author>
<author>Pavel Kvˇetoˇn</author>
</authors>
<title>The Best of Two Worlds: Cooperation of Statistical and Rule-Based Taggers for Czech.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing, ACL</booktitle>
<pages>67--74</pages>
<location>Praha.</location>
<marker>Spoustov´a, Hajiˇc, Votrubec, Krbec, Kvˇetoˇn, 2007</marker>
<rawString>Drahomira Spoustov´a, Jan Hajiˇc, Jan Votrubec, Pavel Krbec, and Pavel Kvˇetoˇn. 2007. The Best of Two Worlds: Cooperation of Statistical and Rule-Based Taggers for Czech. In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing, ACL 2007, pages 67–74, Praha.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdenˇek ˇZabokrtsk´y</author>
<author>Martin Popel</author>
</authors>
<title>Hidden markov tree model in dependency-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>145--148</pages>
<location>Suntec, Singapore.</location>
<marker>ˇZabokrtsk´y, Popel, 2009</marker>
<rawString>Zdenˇek ˇZabokrtsk´y and Martin Popel. 2009. Hidden markov tree model in dependency-based machine translation. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 145–148, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdenˇek ˇZabokrtsk´y</author>
<author>Jan Pt´aˇcek</author>
<author>Petr Pajas</author>
</authors>
<title>TectoMT: Highly Modular MT System with Tectogrammatics Used as Transfer Layer.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd Workshop on Statistical Machine Translation, ACL,</booktitle>
<pages>167--170</pages>
<marker>ˇZabokrtsk´y, Pt´aˇcek, Pajas, 2008</marker>
<rawString>Zdenˇek ˇZabokrtsk´y, Jan Pt´aˇcek, and Petr Pajas. 2008. TectoMT: Highly Modular MT System with Tectogrammatics Used as Transfer Layer. In Proceedings of the 3rd Workshop on Statistical Machine Translation, ACL, pages 167–170.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>