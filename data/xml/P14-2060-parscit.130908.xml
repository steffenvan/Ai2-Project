<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.039792">
<title confidence="0.966534">
Hippocratic Abbreviation Expansion
</title>
<author confidence="0.834693">
Brian Roark and Richard Sproat
</author>
<affiliation confidence="0.614882">
Google, Inc, 79 Ninth Avenue, New York, NY 10011
</affiliation>
<email confidence="0.980925">
{roark,rws}@google.com
</email>
<sectionHeader confidence="0.997127" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99967235">
Incorrect normalization of text can be par-
ticularly damaging for applications like
text-to-speech synthesis (TTS) or typing
auto-correction, where the resulting nor-
malization is directly presented to the user,
versus feeding downstream applications.
In this paper, we focus on abbreviation
expansion for TTS, which requires a “do
no harm”, high precision approach yield-
ing few expansion errors at the cost of
leaving relatively many abbreviations un-
expanded. In the context of a large-
scale, real-world TTS scenario, we present
methods for training classifiers to establish
whether a particular expansion is apt. We
achieve a large increase in correct abbrevi-
ation expansion when combined with the
baseline text normalization component of
the TTS system, together with a substan-
tial reduction in incorrect expansions.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998380253968254">
Text normalization (Sproat et al., 2001) is an im-
portant initial phase for many natural language and
speech applications. The basic task of text normal-
ization is to convert non-standard words (NSWs)
— numbers, abbreviations, dates, etc. — into stan-
dard words, though depending on the task and the
domain a greater or lesser number of these NSWs
may need to be normalized. Perhaps the most de-
manding such application is text-to-speech synthe-
sis (TTS) since, while for parsing, machine trans-
lation and information retrieval it may be accept-
able to leave such things as numbers and abbre-
viations unexpanded, for TTS all tokens need to
be read, and for that it is necessary to know how
to pronounce them. Which normalizations are re-
quired depends very much on the application.
What is also very application-dependent is the
cost of errors in normalization. For some applica-
tions, where the normalized string is an interme-
diate stage in a larger application such as trans-
lation or information retrieval, overgeneration of
normalized alternatives is often a beneficial strat-
egy, to the extent that it may improve the accu-
racy of what is eventually being presented to the
user. In other applications, such as TTS or typing
auto-correction, the resulting normalized string it-
self is directly presented to the user; hence errors
in normalization can have a very high cost relative
to leaving tokens unnormalized.
In this paper we concentrate on abbreviations,
which we define as alphabetic NSWs that it would
be normal to pronounce as their expansion. This
class of NSWs is particularly common in personal
ads, product reviews, and so forth. For example:
home health care svcs stat home health llc
osceola aquatic ctr stars rating write
audi vw repair ser quality and customer
Each of the examples above contains an abbrevi-
ation that, unlike, e.g., conventionalized state ab-
breviations such as ca for California, is either only
slightly standard (ctr for center) or not standard at
all (ser for service).
An important principle in text normalization for
TTS is do no harm. If a system is unable to re-
liably predict the correct reading for a string, it is
better to leave the string alone and have it default
to, say, a character-by-character reading, than to
expand it to something wrong. This is particularly
true in accessibility applications for users who rely
on TTS for most or all of their information needs.
Ideally a navigation system should read turn on
30N correctly as turn on thirty north; but if it can-
not resolve the ambiguity in 30N, it is far better to
read it as thirty N than as thirty Newtons, since lis-
teners can more easily recover from the first kind
of error than the second.
We present methods for learning abbreviation
expansion models that favor high precision (incor-
rect expansions &lt; 2%). Unannotated data is used
to collect evidence for contextual disambiguation
and to train an abbreviation model. Then a small
amount of annotated data is used to build models
to determine whether to accept a candidate expan-
</bodyText>
<page confidence="0.981567">
364
</page>
<note confidence="0.861075">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 364–369,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999825111111111">
sion of an abbreviation based on these features.
The data we report on are taken from Google
MapsTM and web pages associated with its map en-
tries, but the methods can be applied to any data
source that is relatively abbreviation rich.
We note in passing that similar issues arise
in automatic spelling correction work (Wilcox-
O’Hearn et al., 2008), where it is better to leave
a word alone than to “correct” it wrongly.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9999156875">
There has been a lot of interest in recent years on
“normalization” of social media such as Twitter,
but that work defines normalization much more
broadly than we do here (Xia et al., 2006; Choud-
hury et al., 2007; Kobus et al., 2008; Beaufort et
al., 2010; Kaufmann, 2010; Liu et al., 2011; Pen-
nell and Liu, 2011; Aw and Lee, 2012; Liu et al.,
2012a; Liu et al., 2012b; Hassan and Menezes,
2013; Yang and Eisenstein, 2013). There is a good
reason for us to focus more narrowly. For Twit-
ter, much of the normalization task involves non-
standard language such as ur website suxx brah
(from Yang and Eisenstein (2013)). Expanding the
latter to your website sucks, brother certainly nor-
malizes it to standard English, but one could argue
that in so doing one is losing information that the
writer is trying to convey using an informal style.
On the other hand, someone who writes svc ctr
for service center in a product review is probably
merely trying to save time and so expanding the
abbreviations in that case is neutral with respect to
preserving the intent of the original text.
One other difference between the work we re-
port from much of the recent work cited above is
that that work focuses on getting high F scores,
whereas we are most concerned with getting high
precision. While this may seem like a trivial
trade off between precision and recall, our goal
motivates developing measures that minimize the
“risk” of expanding a term, something that is im-
portant in an application such as TTS, where one
cannot correct a misexpansion after it is spoken.
</bodyText>
<sectionHeader confidence="0.998651" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999955571428572">
Since our target application is text-to-speech, we
define the task in terms of an existing TTS lexi-
con. If a word is already in the lexicon, it is left
unprocessed, since there is an existing pronuncia-
tion for it; if a word is out-of-vocabulary (OOV),
we consider expanding it to a word in the lexicon.
We consider a possible expansion for an abbrevi-
ation to be any word in the lexicon from which
the abbreviation can be derived by only deletion of
letters.1 For present purposes we use the Google
English text-to-speech lexicon, consisting of over
430 thousand words. Given an OOV item (possi-
ble abbreviation) in context, we make use of fea-
tures of the context and of the OOV item itself to
enumerate and score candidate expansions.
Our data consists of 15.1 billion words of text
data from Google MapsTM, lower-cased and tok-
enized to remove punctuation symbols. We used
this data in several ways. First, we used it to boot-
strap a model for assigning a probability of an ab-
breviation/expansion pair. Second, we used it to
extract contextual n-gram features for predicting
possible expansions. Finally, we sampled just over
14 thousand OOV items in context and had them
manually labeled with a number of categories, in-
cluding ‘abbreviation’. OOVs labeled as abbrevia-
tions were also labeled with the correct expansion.
We present each of these uses in turn.
</bodyText>
<subsectionHeader confidence="0.999663">
3.1 Abbreviation modeling
</subsectionHeader>
<bodyText confidence="0.980533740740741">
We collect potential abbreviation/full-word pairs
by looking for terms that could be abbreviations
of full words that occur in the same context. Thus:
the svc/service center
heating clng/cooling system
dry clng/cleaning system
contributes evidence that svc is an abbreviation
of service. Similarly instances of clng in con-
texts that can contain cooling or cleaning are evi-
dence that clng could be an abbreviation of either
of these words. (The same contextual information
of course is used later on to disambiguate which
of the expansions is appropriate for the context.)
To compute the initial guess as to what can be a
possible abbreviation, a Thrax grammar (Roark et
al., 2012) is used that, among other things, speci-
fies that: the abbreviation must start with the same
letter as the full word; if a vowel is deleted, all ad-
jacent vowels should also be deleted; consonants
may be deleted in a cluster, but not the last one;
and a (string) suffix may be deleted.2 We count
a pair of words as ‘co-occurring’ if they are ob-
served in the same context. For a given context C,
e.g., the center, let We be the set of words found
in that context. Then, for any pair of words u, v,
we can assign a pair count based on the count of
contexts where both occur:
</bodyText>
<equation confidence="0.972078">
c(u,v) = {C : u E WC and vEWcJJ
</equation>
<footnote confidence="0.767755833333333">
1We do not deal here with phonetic spellings in abbrevia-
tions such as 4get, or cases where letters have been transposed
due to typographical errors (scv).
2This Thrax grammar can be found at
http://openfst.cs.nyu.edu/twiki/bin/
view/Contrib/ThraxContrib
</footnote>
<page confidence="0.998584">
365
</page>
<tableCaption confidence="0.8115138">
blvd boulevard rd road yrs years
ca california fl florida ctr center
mins minutes def definitely ste suite
Table 1: Examples of automatically mined abbrevia-
tion/expansion pairs.
</tableCaption>
<bodyText confidence="0.999763244444445">
Let c(u) be defined as Ev c(u, v). From these
counts, we can define a 2×2 table and calculate
statistics such as the log likelihood statistic (Dun-
ning, 1993), which we use to rank possible abbre-
viation/expansion pairs. Scores derived from these
type (rather than token) counts highly rank pairs of
in-vocabulary words and OOV possible abbrevia-
tions that are substitutable in many contexts.
We further filter the potential abbreviations by
removing ones that have a lot of potential expan-
sions, where we set the cutoff at 10. This removes
mostly short abbreviations that are highly ambigu-
ous. The resulting ranked list of abbreviation ex-
pansion pairs is then thresholded before building
the abbreviation model (see below) to provide a
smaller but more confident training set. For this
paper, we used 5-gram contexts (two words on ei-
ther side) to extract abbreviations and their expan-
sions. See Table 1 for some examples.
Our abbreviation model is a pair character lan-
guage model (LM), also known as a joint multi-
gram model (Bisani and Ney, 2008), whereby
aligned symbols are treated as a single token and
a smoothed n-gram model is estimated. This de-
fines a joint distribution over input and output
sequences, and can be efficiently encoded as a
weighted finite-state transducer. The extracted
abbreviation/expansion pairs are character-aligned
and a 7-gram pair character LM is built over
the alignments using the OpenGrm n-gram library
(Roark et al., 2012). For example:
c:c c:e c:n t:t c:e r:r
Note that, as we’ve defined it, the alignments from
abbreviation to expansion allow only identity and
insertion, no deletions or substitutions. The cost
from this LM, normalized by the length of the ex-
pansion, serves as a score for the quality of a pu-
tative expansion for an abbreviation.
For a small set of frequent, conventionalized
abbreviations (e.g., ca for California — 63 pairs
in total — mainly state abbreviations and similar
items), we assign an fixed pair LM score, since
these examples are in effect irregular cases, where
the regularities of the productive abbreviation pro-
cess do not capture their true cost.
</bodyText>
<subsectionHeader confidence="0.999633">
3.2 Contextual features
</subsectionHeader>
<bodyText confidence="0.999994076923077">
To predict the expansion given the context, we ex-
tract n-gram observations for full words in the TTS
lexicon. We do this in two ways. First, we sim-
ply train a smoothed n-gram LM from the data.
Because of the size of the data set, this is heav-
ily pruned using relative entropy pruning (Stolcke,
1998). Second, we use log likelihood and log odds
ratios (this time using standardly defined n-gram
counts) to extract reliable bigram and trigram con-
texts for words. Space precludes a detailed treat-
ment of these two statistics, but, briefly, both can
be derived from contingency table values calcu-
lated from the frequencies of (1) the word in the
particular context; (2) the word in any context; (3)
the context with any word; and (4) all words in
the corpus. See Agresti (2002), Dunning (1993)
and Monroe et al. (2008) for useful overviews of
how to calculate these and other statistics to de-
rive reliable associations. In our case, we use them
to derive associations between contexts and words
occuring in those contexts. The contexts include
trigrams with the target word in any of the three
positions, and bigrams with the target word in ei-
ther position. We filter the set of n-grams based on
both their log likelihood and log odds ratios, and
provide those scores as features.
</bodyText>
<subsectionHeader confidence="0.999817">
3.3 Manual annotations
</subsectionHeader>
<bodyText confidence="0.99972705882353">
We randomly selected 14,434 OOVs in their full
context, and had them manually annotated as
falling within one of 8 categories, along with the
expansion if the category was ‘abbreviation’. Note
that these are relatively lightweight annotations
that do not require extensive linguistics expertise.
The abbreviation class is defined as cases where
pronouncing as the expansion would be normal.
Other categories included letter sequence (expan-
sion would not be normal, e.g., TV); partial let-
ter sequence (e.g., PurePictureTV); misspelling;
leave as is (part of a URL or pronounced as a
word, e.g., NATO); foreign; don’t know; and junk.
Abbreviations accounted for nearly 23% of the
cases, and about 3/5 of these abbreviations were
instances from the set of 63 conventional abbrevi-
ation/expansion pairs mentioned in Section 3.1.
</bodyText>
<subsectionHeader confidence="0.991878">
3.4 Abbreviation expansion systems
</subsectionHeader>
<bodyText confidence="0.999990916666667">
We have three base systems that we compare here.
The first is the hand-built TTS normalization sys-
tem. This system includes some manually built
patterns and an address parser to find common ab-
breviations that occur in a recognizable context.
For example, the grammar covers several hundred
city-state combinations, such as Fairbanks AK,
yielding good performance on such cases.
The other two systems were built using data ex-
tracted as described above. Both systems make
use of the pair LM outlined in Section 3.1, but
differ in how they model context. The first sys-
</bodyText>
<page confidence="0.995583">
366
</page>
<bodyText confidence="0.999982866666667">
tem, which we call “N-gram”, uses a pruned Katz
(1987) smoothed trigram model. The second sys-
tem, which we call “SVM”, uses a Support Vec-
tor Machine (Cortes and Vapnik, 1995) to classify
candidate expansions as being correct or not. For
both systems, for any given input OOV, the pos-
sible expansion with the highest score is output,
along with the decision of whether to expand.
For the “N-gram” system, n-gram negative log
probabilities are extracted as follows. Let wi be
the position of the target expansion. We extract the
part of the n-gram probability of the string that is
not constant across all competing expansions, and
normalize by the number of words in that window.
Thus the score of the word is:
</bodyText>
<equation confidence="0.926116">
log P(wj  |wj−1wj−2)
</equation>
<bodyText confidence="0.99948702173913">
In our experiments, k = 2 since we have a trigram
model, though in cases where the target word is the
last word in the string, k = 1, because there only
the end-of-string symbol must be predicted in ad-
dition to the expansion. We then take the Bayesian
fusion of this model with the pair LM, by adding
them in the log space, to get prediction from both
the context and abbreviation model.
For the “SVM” model, we extract features from
the log likelihood and log odds scores associated
with contextual n-grams, as well as from the pair
LM probability and characteristics of the abbrevi-
ation itself. We train a linear model on a subset of
the annotated data (see section 4). Multiple con-
textual n-grams may be observed, and we take the
maximum log likelihood and log odds scores for
each candidate expansion in the observed context.
We then quantize these scores down into 16 bins,
using the histogram in the training data to define
bin thresholds so as to partition the training in-
stances evenly. We also create 16 bins for the pair
LM score. A binary feature is defined for each
bin that is set to 1 if the current candidate’s score
is less than the threshold of that bin, otherwise 0.
Thus multiple bin features can be active for a given
candidate expansion of the abbreviation.
We also have features that fire for each type of
contextual feature (e.g., trigram with expansion as
middle word, etc.), including ‘no context’, where
none of the trigrams or bigrams from the current
example that include the candidate expansion are
present in our list. Further, we have features for
the length of the abbreviation (shorter abbrevia-
tions have more ambiguity, hence are more risky
to expand); membership in the list of frequent,
conventionalized abbreviations mentioned earlier;
and some combinations of these, along with bias
features. We train the model using standard op-
tions with Google internal SVM training tools.
Note that the number of n-grams in the two
models differs. The N-gram system has around
200M n-grams after pruning; while the SVM
model uses around a quarter of that. We also tried
a more heavily pruned n-gram model, and the re-
sults are only very slightly worse, certainly accept-
able for a low-resource scenario.
</bodyText>
<sectionHeader confidence="0.998264" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999863235294118">
We split the 3,209 labeled abbreviations into a
training set of 2,209 examples and a held aside de-
velopment set of 1,000 examples. We first evaluate
on the development set, then perform a final 10-
fold cross validation over the entire set of labeled
examples. We evaluate in terms of the percent-
age of abbreviations that were correctly expanded
(true positives, TP) and that were incorrectly ex-
panded (false positives, FP).
Results are shown in Table 2. The first two rows
show the baseline TTS system and SVM model.
On the development set, both systems have a false
positive rate near 3%, i.e., three abbreviations are
expanded incorrectly for every 100 examples; and
over 50% true positive rate, i.e., more than half of
the abbreviations are expanded correctly. To re-
port true and false positive rates for the N-gram
system we would need to select an arbitrary de-
cision threshold operating point, unlike the deter-
ministic TTS baseline and the SVM model with
its decision threshold of 0. Rather than tune such a
meta-parameter to the development set, we instead
present an ROC curve comparison of the N-gram
and SVM models, and then propose a method
for “intersecting” their output without requiring a
tuned decision threshold.
Figure 1 presents an ROC curve for the N-gram
and SVM systems, and for the simple Bayesian
fusion (sum in log space) of their scores. We can
see that the SVM model has very high precision
for its highest ranked examples, yielding nearly
20% of the correct expansions without any in-
correct expansions. However the N-gram system
achieves higher true positive rates when the false
</bodyText>
<table confidence="0.996068428571428">
System Percent set of abbreviations set
dev FP full FP
TP TP
TTS baseline 55.0 3.1 40.0 3.0
SVM model 52.6 3.3 53.3 2.6
SVM ∩ N-gram 50.6 1.1 50.3 0.9
SVM ∩ N-gram, then TTS 73.5 1.9 74.5 1.5
</table>
<tableCaption confidence="0.99608225">
Table 2: Results on held-out labeled data, and with final
10-fold cross-validation over the entire labeled set. Percent-
age of abbreviations expanded correctly (TP) and percentage
expanded incorrectly (FP) are reported for each system.
</tableCaption>
<equation confidence="0.8993426">
i+k
1
S(wi) = −
k + 1
j=i
</equation>
<page confidence="0.935158">
367
</page>
<figure confidence="0.9960892">
hold
VM
0 1 2 3 4
�
Incorrect expansion percentage (FP)
</figure>
<figureCaption confidence="0.958318666666667">
Figure 1: ROC curve plotting true positive (correct expan-
sion) percentages versus false positive (incorrect expansion)
percentages for several systems on the development set.
</figureCaption>
<bodyText confidence="0.985615780821918">
positive rate falls between 1 and 3 percent, though
both systems reach roughly the same performance
at the SVM’s decision threshold corresponding to
around 3.3% false positive rate. The simple com-
bination of their scores achieves strong improve-
ments over either model, with an operating point
associated with the SVM decision boundary that
yields a couple of points improvement in true pos-
itives and a full 1% reduction in false positive rate.
One simple way to combine these two system
outputs in a way that does not require tuning a de-
cision threshold is to expand the abbreviation if
and only if (1) both the SVM model and the N-
gram model agree on the best expansion; and (2)
the SVM model score is greater than zero. In a
slight abuse of the term ‘intersection’, we call this
combination ‘SVM intersect N-gram’ (or ‘SVM
∩ N-gram’ in Table 2). Using this approach, our
true positive rate on the development set declines
a bit to just over 50%, but our false positive rate
declines over two full percentage points to 1.1%,
yielding a very high precision system.
Taking this very high precision system combi-
nation of the N-gram and SVM models, we then
combine with the baseline TTS system as follows.
First we apply our system, and expand the item if
it scores above threshold; for those items left un-
expanded, we let the TTS system process it in its
own way. In this way, we actually reduce the false
positive rate on the development set over the base-
line TTS system by over 1% absolute to less than
2%, while also increasing the true positive rate to
73.5%, an increase of 18.5% absolute.
Of course, at test time, we will not know
whether an OOV is an abbreviation or not, so
we also looked at the performance on the rest
of the collected data, to see how often it erro-
neously suggests an expansion from that set. Of
the 11,157-examples that were hand-labeled-as
non-abbreviations, our SVM ∩ N-gram systemmex-
pandedi45 items,iwhich;isroa false positive rate
of 0.4% under thetassumption that noneelof them
should benexpanded. In fact,imanualatinspection
found that 20% of these werescorrect expansions
of abbreviations that had beennmis-labeled.
We also experimented withmantnumberiof alter-
native highfprecisionsapproaches that space pre-
cludesiourlpresenting1in detailShere,tincluding:
pruningerthe number of expansionocandidatessbased
ondthe pair LM score; only allowingivabbreviation
expansion when at leastnone extractedon-gram con-
text is present for that expansion in that context;
and CART tree (Breiman et al., 1984) training
with real valued scores. Some of these yielded
very high precision systems, though at the cost
of leaving many more abbreviations unexpanded.
We found that, for use in combination with the
baseline TTS system, large overall reductions in
FP rate were achieved by using an initial system
with substantially higher TP and somewhat higher
FP rates, since far fewer abbreviations were then
passed along unexpanded to the baseline system,
with its relatively high 3% FP rate.
To ensure that we did not overtune our systems
to the development set through experimentation,
we performed 10-fold cross validation over the full
set of abbreviations. These results are presented
in Table 2. Most notably, the TTS baseline system
has a much lower true positive rate; yet we find our
systems achieve performance very close to that for
the development set, so that our final combination
with the TTS baseline was actually slighly better
than the numbers on the development set.
</bodyText>
<sectionHeader confidence="0.919325" genericHeader="conclusions">
5 Conclusions w a
</sectionHeader>
<bodyText confidence="0.987492153846154">
In this paper we have presented methods for high
precision abbreviation expansion for a TTS appli-
cation. The methods are largely self-organizing,
using in-domain unannotated data, and depend on
only a small amount of annotated data. Since the
SVM features relate to general properties of ab-
breviations, expansions and contexts, the classi-
fier parameters will likely carry over to new (En-
glish) domains. We demonstrate that in combi-
nation with a hand-built TTS baseline, the meth-
ods afford dramatic improvement in the TP rate
(to about 74% from a starting point of about 40%)
and a reduction of FP to below our goal of 2%.
</bodyText>
<sectionHeader confidence="0.99412" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.86628575">
We would like to thank Daan van Esch and the
Google Speech Data Operations team for their
work on preparing the annotated data. We also
thank the reviewers for their comments.
</bodyText>
<figure confidence="0.999280771929824">
r
a
m
p
a
e
a
e
m
v
men
r
s
e
a
u
v
o
y
w
n
��
o u tp
a
x
o
c
n
e
m
a
n
x
e
a
s
m
�
��
s
c om
u
n
�
v
t
p
�
p
�
o
y
e
m
y
e
a
w
a
o
n
Correct expansion percentage (TP)
��
�
�
�
�
�
60 N ­gram-N--gram SVMSVM SVM+N.gram SVM interned Ngmm
50
�
40
30
20
10
�
0
INam
�
b
ntin
b
S
b
f
%
i
M
itv
VM de
i
c om b
V
i
F
s
h
y
b
he t
V
e 2)
u
b
ed w
r
hres
SVM
b
N
%b
i
h
h
</figure>
<page confidence="0.99005">
368
</page>
<sectionHeader confidence="0.995225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998893009174312">
Alan Agresti. 2002. Categorical data analysis. John
Wiley &amp; Sons, 2nd edition.
Ai Ti Aw and Lian Hau Lee. 2012. Personalized nor-
malization for a multilingual chat system. In Pro-
ceedings of the ACL 2012 System Demonstrations,
pages 31–36, Jeju Island, Korea, July. Association
for Computational Linguistics.
Richard Beaufort, Sophie Roekhaut, Louise-Am´elie
Cougnon, and C´edrick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normal-
izing SMS messages. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 770–779, Uppsala, Sweden, July.
Association for Computational Linguistics.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434–451.
Leo Breiman, Jerome H. Friedman, Richard A. Olshen,
and Charles J. Stone. 1984. Classification and Re-
gression Trees. Wadsworth &amp; Brooks, Pacific Grove
CA.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Sudesha
Sarkar, and Anupam Basu. 2007. Investigation and
modeling of the structure of texting language. Int. J.
Doc. Anal. Recognit., 10:157–174.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273–297.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61–74.
Hany Hassan and Arul Menezes. 2013. Social text nor-
malization using contextual graph random walks. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1577–
1586.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recogniser. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3):400–401.
Max Kaufmann. 2010. Syntactic normalization of
Twitter messages. In International Conference on
NLP.
Catherine Kobus, Franc¸ois Yvon, and G´eraldine
Damnati. 2008. Normalizing SMS: are two
metaphors better than one? In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 441–448, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution? Nor-
malizing text messages without pre-categorization
nor supervision. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 71–
76, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012a. A
broad-coverage normalization system for social me-
dia language. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1035–
1044, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012b. Joint
inference of named entity recognition and nor-
malization for tweets. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
526–535, Jeju Island, Korea, July. Association for
Computational Linguistics.
Burt L Monroe, Michael P Colaresi, and Kevin M
Quinn. 2008. Fightin’words: Lexical feature se-
lection and evaluation for identifying the content of
political conflict. Political Analysis, 16(4):372–403.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
SMS abbreviations. In IJCNLP. Papers/pennell-
liu3.pdf.
Brian Roark, Michael Riley, Cyril Allauzen, Terry Tai,
and Richard Sproat. 2012. The OpenGrm open-
source finite-state grammar software libraries. In
ACL, Jeju Island, Korea.
Richard Sproat, Alan Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287–333.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proc. DARPA Broad-
cast News Transcription and Understanding Work-
shop, pages 270–274.
Amber Wilcox-O’Hearn, Graeme Hirst, and Alexander
Budanitsky. 2008. Real-word spelling correction
with trigrams: A reconsideration of the Mays, Dam-
erau, and Mercer model. In CICLing 2008, volume
4919 of LNCS, pages 605–616, Berlin. Springer.
Yunqing Xia, Kam-Fai Wong, and Wenjie Li. 2006.
A phonetic-based approach to Chinese chat text nor-
malization. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 993–1000, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 61–72.
</reference>
<page confidence="0.999123">
369
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.819168">
<title confidence="0.999978">Hippocratic Abbreviation Expansion</title>
<author confidence="0.999253">Brian Roark</author>
<author confidence="0.999253">Richard Sproat</author>
<address confidence="0.838872">Google, Inc, 79 Ninth Avenue, New York, NY</address>
<abstract confidence="0.998889714285714">Incorrect normalization of text can be particularly damaging for applications like text-to-speech synthesis (TTS) or typing auto-correction, where the resulting normalization is directly presented to the user, versus feeding downstream applications. In this paper, we focus on abbreviation expansion for TTS, which requires a “do no harm”, high precision approach yielding few expansion errors at the cost of leaving relatively many abbreviations unexpanded. In the context of a largescale, real-world TTS scenario, we present methods for training classifiers to establish whether a particular expansion is apt. We achieve a large increase in correct abbreviation expansion when combined with the baseline text normalization component of the TTS system, together with a substantial reduction in incorrect expansions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan Agresti</author>
</authors>
<title>Categorical data analysis.</title>
<date>2002</date>
<publisher>John Wiley &amp; Sons,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="12244" citStr="Agresti (2002)" startWordPosition="2051" endWordPosition="2052">smoothed n-gram LM from the data. Because of the size of the data set, this is heavily pruned using relative entropy pruning (Stolcke, 1998). Second, we use log likelihood and log odds ratios (this time using standardly defined n-gram counts) to extract reliable bigram and trigram contexts for words. Space precludes a detailed treatment of these two statistics, but, briefly, both can be derived from contingency table values calculated from the frequencies of (1) the word in the particular context; (2) the word in any context; (3) the context with any word; and (4) all words in the corpus. See Agresti (2002), Dunning (1993) and Monroe et al. (2008) for useful overviews of how to calculate these and other statistics to derive reliable associations. In our case, we use them to derive associations between contexts and words occuring in those contexts. The contexts include trigrams with the target word in any of the three positions, and bigrams with the target word in either position. We filter the set of n-grams based on both their log likelihood and log odds ratios, and provide those scores as features. 3.3 Manual annotations We randomly selected 14,434 OOVs in their full context, and had them manu</context>
</contexts>
<marker>Agresti, 2002</marker>
<rawString>Alan Agresti. 2002. Categorical data analysis. John Wiley &amp; Sons, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ai Ti Aw</author>
<author>Lian Hau Lee</author>
</authors>
<title>Personalized normalization for a multilingual chat system.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>31--36</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="4986" citStr="Aw and Lee, 2012" startWordPosition="816" endWordPosition="819">ut the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is proba</context>
</contexts>
<marker>Aw, Lee, 2012</marker>
<rawString>Ai Ti Aw and Lian Hau Lee. 2012. Personalized normalization for a multilingual chat system. In Proceedings of the ACL 2012 System Demonstrations, pages 31–36, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Beaufort</author>
<author>Sophie Roekhaut</author>
<author>Louise-Am´elie Cougnon</author>
<author>C´edrick Fairon</author>
</authors>
<title>A hybrid rule/model-based finite-state framework for normalizing SMS messages.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>770--779</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="4911" citStr="Beaufort et al., 2010" startWordPosition="801" endWordPosition="804">on are taken from Google MapsTM and web pages associated with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand,</context>
</contexts>
<marker>Beaufort, Roekhaut, Cougnon, Fairon, 2010</marker>
<rawString>Richard Beaufort, Sophie Roekhaut, Louise-Am´elie Cougnon, and C´edrick Fairon. 2010. A hybrid rule/model-based finite-state framework for normalizing SMS messages. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 770–779, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Bisani</author>
<author>Hermann Ney</author>
</authors>
<title>Jointsequence models for grapheme-to-phoneme conversion.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>5</issue>
<contexts>
<context position="10372" citStr="Bisani and Ney, 2008" startWordPosition="1735" endWordPosition="1738">bbreviations by removing ones that have a lot of potential expansions, where we set the cutoff at 10. This removes mostly short abbreviations that are highly ambiguous. The resulting ranked list of abbreviation expansion pairs is then thresholded before building the abbreviation model (see below) to provide a smaller but more confident training set. For this paper, we used 5-gram contexts (two words on either side) to extract abbreviations and their expansions. See Table 1 for some examples. Our abbreviation model is a pair character language model (LM), also known as a joint multigram model (Bisani and Ney, 2008), whereby aligned symbols are treated as a single token and a smoothed n-gram model is estimated. This defines a joint distribution over input and output sequences, and can be efficiently encoded as a weighted finite-state transducer. The extracted abbreviation/expansion pairs are character-aligned and a 7-gram pair character LM is built over the alignments using the OpenGrm n-gram library (Roark et al., 2012). For example: c:c c:e c:n t:t c:e r:r Note that, as we’ve defined it, the alignments from abbreviation to expansion allow only identity and insertion, no deletions or substitutions. The </context>
</contexts>
<marker>Bisani, Ney, 2008</marker>
<rawString>Maximilian Bisani and Hermann Ney. 2008. Jointsequence models for grapheme-to-phoneme conversion. Speech Communication, 50(5):434–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
<author>Jerome H Friedman</author>
<author>Richard A Olshen</author>
<author>Charles J Stone</author>
</authors>
<title>Classification and Regression Trees.</title>
<date>1984</date>
<publisher>Wadsworth &amp; Brooks,</publisher>
<location>Pacific Grove CA.</location>
<contexts>
<context position="21999" citStr="Breiman et al., 1984" startWordPosition="3680" endWordPosition="3683">ystemmexpandedi45 items,iwhich;isroa false positive rate of 0.4% under thetassumption that noneelof them should benexpanded. In fact,imanualatinspection found that 20% of these werescorrect expansions of abbreviations that had beennmis-labeled. We also experimented withmantnumberiof alternative highfprecisionsapproaches that space precludesiourlpresenting1in detailShere,tincluding: pruningerthe number of expansionocandidatessbased ondthe pair LM score; only allowingivabbreviation expansion when at leastnone extractedon-gram context is present for that expansion in that context; and CART tree (Breiman et al., 1984) training with real valued scores. Some of these yielded very high precision systems, though at the cost of leaving many more abbreviations unexpanded. We found that, for use in combination with the baseline TTS system, large overall reductions in FP rate were achieved by using an initial system with substantially higher TP and somewhat higher FP rates, since far fewer abbreviations were then passed along unexpanded to the baseline system, with its relatively high 3% FP rate. To ensure that we did not overtune our systems to the development set through experimentation, we performed 10-fold cro</context>
</contexts>
<marker>Breiman, Friedman, Olshen, Stone, 1984</marker>
<rawString>Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. 1984. Classification and Regression Trees. Wadsworth &amp; Brooks, Pacific Grove CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Sudesha Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Investigation and modeling of the structure of texting language.</title>
<date>2007</date>
<journal>Int. J. Doc. Anal. Recognit.,</journal>
<pages>10--157</pages>
<contexts>
<context position="4868" citStr="Choudhury et al., 2007" startWordPosition="792" endWordPosition="796">based on these features. The data we report on are taken from Google MapsTM and web pages associated with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey </context>
</contexts>
<marker>Choudhury, Saraf, Jain, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Sudesha Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. Int. J. Doc. Anal. Recognit., 10:157–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="14373" citStr="Cortes and Vapnik, 1995" startWordPosition="2395" endWordPosition="2398">udes some manually built patterns and an address parser to find common abbreviations that occur in a recognizable context. For example, the grammar covers several hundred city-state combinations, such as Fairbanks AK, yielding good performance on such cases. The other two systems were built using data extracted as described above. Both systems make use of the pair LM outlined in Section 3.1, but differ in how they model context. The first sys366 tem, which we call “N-gram”, uses a pruned Katz (1987) smoothed trigram model. The second system, which we call “SVM”, uses a Support Vector Machine (Cortes and Vapnik, 1995) to classify candidate expansions as being correct or not. For both systems, for any given input OOV, the possible expansion with the highest score is output, along with the decision of whether to expand. For the “N-gram” system, n-gram negative log probabilities are extracted as follows. Let wi be the position of the target expansion. We extract the part of the n-gram probability of the string that is not constant across all competing expansions, and normalize by the number of words in that window. Thus the score of the word is: log P(wj |wj−1wj−2) In our experiments, k = 2 since we have a tr</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="9485" citStr="Dunning, 1993" startWordPosition="1590" endWordPosition="1592"> : u E WC and vEWcJJ 1We do not deal here with phonetic spellings in abbreviations such as 4get, or cases where letters have been transposed due to typographical errors (scv). 2This Thrax grammar can be found at http://openfst.cs.nyu.edu/twiki/bin/ view/Contrib/ThraxContrib 365 blvd boulevard rd road yrs years ca california fl florida ctr center mins minutes def definitely ste suite Table 1: Examples of automatically mined abbreviation/expansion pairs. Let c(u) be defined as Ev c(u, v). From these counts, we can define a 2×2 table and calculate statistics such as the log likelihood statistic (Dunning, 1993), which we use to rank possible abbreviation/expansion pairs. Scores derived from these type (rather than token) counts highly rank pairs of in-vocabulary words and OOV possible abbreviations that are substitutable in many contexts. We further filter the potential abbreviations by removing ones that have a lot of potential expansions, where we set the cutoff at 10. This removes mostly short abbreviations that are highly ambiguous. The resulting ranked list of abbreviation expansion pairs is then thresholded before building the abbreviation model (see below) to provide a smaller but more confid</context>
<context position="12260" citStr="Dunning (1993)" startWordPosition="2053" endWordPosition="2054">LM from the data. Because of the size of the data set, this is heavily pruned using relative entropy pruning (Stolcke, 1998). Second, we use log likelihood and log odds ratios (this time using standardly defined n-gram counts) to extract reliable bigram and trigram contexts for words. Space precludes a detailed treatment of these two statistics, but, briefly, both can be derived from contingency table values calculated from the frequencies of (1) the word in the particular context; (2) the word in any context; (3) the context with any word; and (4) all words in the corpus. See Agresti (2002), Dunning (1993) and Monroe et al. (2008) for useful overviews of how to calculate these and other statistics to derive reliable associations. In our case, we use them to derive associations between contexts and words occuring in those contexts. The contexts include trigrams with the target word in any of the three positions, and bigrams with the target word in either position. We filter the set of n-grams based on both their log likelihood and log odds ratios, and provide those scores as features. 3.3 Manual annotations We randomly selected 14,434 OOVs in their full context, and had them manually annotated a</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hany Hassan</author>
<author>Arul Menezes</author>
</authors>
<title>Social text normalization using contextual graph random walks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1577--1586</pages>
<contexts>
<context position="5050" citStr="Hassan and Menezes, 2013" startWordPosition="828" endWordPosition="831"> relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying to save time and so expanding the abbreviation</context>
</contexts>
<marker>Hassan, Menezes, 2013</marker>
<rawString>Hany Hassan and Arul Menezes. 2013. Social text normalization using contextual graph random walks. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1577– 1586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recogniser.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="14253" citStr="Katz (1987)" startWordPosition="2376" endWordPosition="2377">e base systems that we compare here. The first is the hand-built TTS normalization system. This system includes some manually built patterns and an address parser to find common abbreviations that occur in a recognizable context. For example, the grammar covers several hundred city-state combinations, such as Fairbanks AK, yielding good performance on such cases. The other two systems were built using data extracted as described above. Both systems make use of the pair LM outlined in Section 3.1, but differ in how they model context. The first sys366 tem, which we call “N-gram”, uses a pruned Katz (1987) smoothed trigram model. The second system, which we call “SVM”, uses a Support Vector Machine (Cortes and Vapnik, 1995) to classify candidate expansions as being correct or not. For both systems, for any given input OOV, the possible expansion with the highest score is output, along with the decision of whether to expand. For the “N-gram” system, n-gram negative log probabilities are extracted as follows. Let wi be the position of the target expansion. We extract the part of the n-gram probability of the string that is not constant across all competing expansions, and normalize by the number </context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recogniser. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Kaufmann</author>
</authors>
<title>Syntactic normalization of Twitter messages.</title>
<date>2010</date>
<booktitle>In International Conference on NLP.</booktitle>
<contexts>
<context position="4927" citStr="Kaufmann, 2010" startWordPosition="805" endWordPosition="806">e MapsTM and web pages associated with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who wri</context>
</contexts>
<marker>Kaufmann, 2010</marker>
<rawString>Max Kaufmann. 2010. Syntactic normalization of Twitter messages. In International Conference on NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Kobus</author>
<author>Franc¸ois Yvon</author>
<author>G´eraldine Damnati</author>
</authors>
<title>Normalizing SMS: are two metaphors better than one?</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>441--448</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="4888" citStr="Kobus et al., 2008" startWordPosition="797" endWordPosition="800"> The data we report on are taken from Google MapsTM and web pages associated with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal st</context>
</contexts>
<marker>Kobus, Yvon, Damnati, 2008</marker>
<rawString>Catherine Kobus, Franc¸ois Yvon, and G´eraldine Damnati. 2008. Normalizing SMS: are two metaphors better than one? In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 441–448, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Bingqing Wang</author>
<author>Yang Liu</author>
</authors>
<title>Insertion, deletion, or substitution? Normalizing text messages without pre-categorization nor supervision.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>71--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="4945" citStr="Liu et al., 2011" startWordPosition="807" endWordPosition="810"> pages associated with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for se</context>
</contexts>
<marker>Liu, Weng, Wang, Liu, 2011</marker>
<rawString>Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu. 2011. Insertion, deletion, or substitution? Normalizing text messages without pre-categorization nor supervision. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 71– 76, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Xiao Jiang</author>
</authors>
<title>A broad-coverage normalization system for social media language.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1035--1044</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="5004" citStr="Liu et al., 2012" startWordPosition="820" endWordPosition="823"> be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying </context>
</contexts>
<marker>Liu, Weng, Jiang, 2012</marker>
<rawString>Fei Liu, Fuliang Weng, and Xiao Jiang. 2012a. A broad-coverage normalization system for social media language. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1035– 1044, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Xiangyang Zhou</author>
<author>Zhongyang Fu</author>
<author>Furu Wei</author>
</authors>
<title>Joint inference of named entity recognition and normalization for tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>526--535</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="5004" citStr="Liu et al., 2012" startWordPosition="820" endWordPosition="823"> be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying </context>
</contexts>
<marker>Liu, Zhou, Zhou, Fu, Wei, 2012</marker>
<rawString>Xiaohua Liu, Ming Zhou, Xiangyang Zhou, Zhongyang Fu, and Furu Wei. 2012b. Joint inference of named entity recognition and normalization for tweets. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 526–535, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burt L Monroe</author>
<author>Michael P Colaresi</author>
<author>Kevin M Quinn</author>
</authors>
<title>Fightin’words: Lexical feature selection and evaluation for identifying the content of political conflict.</title>
<date>2008</date>
<journal>Political Analysis,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="12285" citStr="Monroe et al. (2008)" startWordPosition="2056" endWordPosition="2059">ecause of the size of the data set, this is heavily pruned using relative entropy pruning (Stolcke, 1998). Second, we use log likelihood and log odds ratios (this time using standardly defined n-gram counts) to extract reliable bigram and trigram contexts for words. Space precludes a detailed treatment of these two statistics, but, briefly, both can be derived from contingency table values calculated from the frequencies of (1) the word in the particular context; (2) the word in any context; (3) the context with any word; and (4) all words in the corpus. See Agresti (2002), Dunning (1993) and Monroe et al. (2008) for useful overviews of how to calculate these and other statistics to derive reliable associations. In our case, we use them to derive associations between contexts and words occuring in those contexts. The contexts include trigrams with the target word in any of the three positions, and bigrams with the target word in either position. We filter the set of n-grams based on both their log likelihood and log odds ratios, and provide those scores as features. 3.3 Manual annotations We randomly selected 14,434 OOVs in their full context, and had them manually annotated as falling within one of 8</context>
</contexts>
<marker>Monroe, Colaresi, Quinn, 2008</marker>
<rawString>Burt L Monroe, Michael P Colaresi, and Kevin M Quinn. 2008. Fightin’words: Lexical feature selection and evaluation for identifying the content of political conflict. Political Analysis, 16(4):372–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deana Pennell</author>
<author>Yang Liu</author>
</authors>
<title>A character-level machine translation approach for normalization of SMS abbreviations.</title>
<date>2011</date>
<booktitle>In IJCNLP. Papers/pennellliu3.pdf.</booktitle>
<contexts>
<context position="4968" citStr="Pennell and Liu, 2011" startWordPosition="811" endWordPosition="815">with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a produ</context>
</contexts>
<marker>Pennell, Liu, 2011</marker>
<rawString>Deana Pennell and Yang Liu. 2011. A character-level machine translation approach for normalization of SMS abbreviations. In IJCNLP. Papers/pennellliu3.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Michael Riley</author>
<author>Cyril Allauzen</author>
<author>Terry Tai</author>
<author>Richard Sproat</author>
</authors>
<title>The OpenGrm opensource finite-state grammar software libraries.</title>
<date>2012</date>
<booktitle>In ACL, Jeju Island,</booktitle>
<contexts>
<context position="8288" citStr="Roark et al., 2012" startWordPosition="1376" endWordPosition="1379">for terms that could be abbreviations of full words that occur in the same context. Thus: the svc/service center heating clng/cooling system dry clng/cleaning system contributes evidence that svc is an abbreviation of service. Similarly instances of clng in contexts that can contain cooling or cleaning are evidence that clng could be an abbreviation of either of these words. (The same contextual information of course is used later on to disambiguate which of the expansions is appropriate for the context.) To compute the initial guess as to what can be a possible abbreviation, a Thrax grammar (Roark et al., 2012) is used that, among other things, specifies that: the abbreviation must start with the same letter as the full word; if a vowel is deleted, all adjacent vowels should also be deleted; consonants may be deleted in a cluster, but not the last one; and a (string) suffix may be deleted.2 We count a pair of words as ‘co-occurring’ if they are observed in the same context. For a given context C, e.g., the center, let We be the set of words found in that context. Then, for any pair of words u, v, we can assign a pair count based on the count of contexts where both occur: c(u,v) = {C : u E WC and vEW</context>
<context position="10785" citStr="Roark et al., 2012" startWordPosition="1798" endWordPosition="1801">de) to extract abbreviations and their expansions. See Table 1 for some examples. Our abbreviation model is a pair character language model (LM), also known as a joint multigram model (Bisani and Ney, 2008), whereby aligned symbols are treated as a single token and a smoothed n-gram model is estimated. This defines a joint distribution over input and output sequences, and can be efficiently encoded as a weighted finite-state transducer. The extracted abbreviation/expansion pairs are character-aligned and a 7-gram pair character LM is built over the alignments using the OpenGrm n-gram library (Roark et al., 2012). For example: c:c c:e c:n t:t c:e r:r Note that, as we’ve defined it, the alignments from abbreviation to expansion allow only identity and insertion, no deletions or substitutions. The cost from this LM, normalized by the length of the expansion, serves as a score for the quality of a putative expansion for an abbreviation. For a small set of frequent, conventionalized abbreviations (e.g., ca for California — 63 pairs in total — mainly state abbreviations and similar items), we assign an fixed pair LM score, since these examples are in effect irregular cases, where the regularities of the pr</context>
</contexts>
<marker>Roark, Riley, Allauzen, Tai, Sproat, 2012</marker>
<rawString>Brian Roark, Michael Riley, Cyril Allauzen, Terry Tai, and Richard Sproat. 2012. The OpenGrm opensource finite-state grammar software libraries. In ACL, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Alan Black</author>
<author>Stanley Chen</author>
<author>Shankar Kumar</author>
<author>Mari Ostendorf</author>
<author>Christopher Richards</author>
</authors>
<title>Normalization of non-standard words.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="1019" citStr="Sproat et al., 2001" startWordPosition="145" endWordPosition="148"> this paper, we focus on abbreviation expansion for TTS, which requires a “do no harm”, high precision approach yielding few expansion errors at the cost of leaving relatively many abbreviations unexpanded. In the context of a largescale, real-world TTS scenario, we present methods for training classifiers to establish whether a particular expansion is apt. We achieve a large increase in correct abbreviation expansion when combined with the baseline text normalization component of the TTS system, together with a substantial reduction in incorrect expansions. 1 Introduction Text normalization (Sproat et al., 2001) is an important initial phase for many natural language and speech applications. The basic task of text normalization is to convert non-standard words (NSWs) — numbers, abbreviations, dates, etc. — into standard words, though depending on the task and the domain a greater or lesser number of these NSWs may need to be normalized. Perhaps the most demanding such application is text-to-speech synthesis (TTS) since, while for parsing, machine translation and information retrieval it may be acceptable to leave such things as numbers and abbreviations unexpanded, for TTS all tokens need to be read,</context>
</contexts>
<marker>Sproat, Black, Chen, Kumar, Ostendorf, Richards, 2001</marker>
<rawString>Richard Sproat, Alan Black, Stanley Chen, Shankar Kumar, Mari Ostendorf, and Christopher Richards. 2001. Normalization of non-standard words. Computer Speech and Language, 15(3):287–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proc. DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<contexts>
<context position="11770" citStr="Stolcke, 1998" startWordPosition="1970" endWordPosition="1971">ed abbreviations (e.g., ca for California — 63 pairs in total — mainly state abbreviations and similar items), we assign an fixed pair LM score, since these examples are in effect irregular cases, where the regularities of the productive abbreviation process do not capture their true cost. 3.2 Contextual features To predict the expansion given the context, we extract n-gram observations for full words in the TTS lexicon. We do this in two ways. First, we simply train a smoothed n-gram LM from the data. Because of the size of the data set, this is heavily pruned using relative entropy pruning (Stolcke, 1998). Second, we use log likelihood and log odds ratios (this time using standardly defined n-gram counts) to extract reliable bigram and trigram contexts for words. Space precludes a detailed treatment of these two statistics, but, briefly, both can be derived from contingency table values calculated from the frequencies of (1) the word in the particular context; (2) the word in any context; (3) the context with any word; and (4) all words in the corpus. See Agresti (2002), Dunning (1993) and Monroe et al. (2008) for useful overviews of how to calculate these and other statistics to derive reliab</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amber Wilcox-O’Hearn</author>
<author>Graeme Hirst</author>
<author>Alexander Budanitsky</author>
</authors>
<title>Real-word spelling correction with trigrams: A reconsideration of the Mays, Damerau, and Mercer model.</title>
<date>2008</date>
<booktitle>In CICLing 2008,</booktitle>
<volume>4919</volume>
<pages>605--616</pages>
<publisher>Springer.</publisher>
<location>Berlin.</location>
<marker>Wilcox-O’Hearn, Hirst, Budanitsky, 2008</marker>
<rawString>Amber Wilcox-O’Hearn, Graeme Hirst, and Alexander Budanitsky. 2008. Real-word spelling correction with trigrams: A reconsideration of the Mays, Damerau, and Mercer model. In CICLing 2008, volume 4919 of LNCS, pages 605–616, Berlin. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunqing Xia</author>
<author>Kam-Fai Wong</author>
<author>Wenjie Li</author>
</authors>
<title>A phonetic-based approach to Chinese chat text normalization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>993--1000</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="4844" citStr="Xia et al., 2006" startWordPosition="788" endWordPosition="791">f an abbreviation based on these features. The data we report on are taken from Google MapsTM and web pages associated with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the wri</context>
</contexts>
<marker>Xia, Wong, Li, 2006</marker>
<rawString>Yunqing Xia, Kam-Fai Wong, and Wenjie Li. 2006. A phonetic-based approach to Chinese chat text normalization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 993–1000, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Yang</author>
<author>Jacob Eisenstein</author>
</authors>
<title>A log-linear model for unsupervised text normalization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="5078" citStr="Yang and Eisenstein, 2013" startWordPosition="832" endWordPosition="835">ich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying to save time and so expanding the abbreviations in that case is neutral wi</context>
</contexts>
<marker>Yang, Eisenstein, 2013</marker>
<rawString>Yi Yang and Jacob Eisenstein. 2013. A log-linear model for unsupervised text normalization. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 61–72.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>