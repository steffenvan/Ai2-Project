<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029361">
<title confidence="0.993328">
Corry: A System for Coreference Resolution
</title>
<author confidence="0.992458">
Olga Uryupina
</author>
<affiliation confidence="0.995815">
CiMeC, University of Trento
</affiliation>
<email confidence="0.988851">
uryupina@gmail.com
</email>
<sectionHeader confidence="0.988031" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.8014650625">
Corry is a system for coreference resolution
in English. It supports both local (Soon et
al. (2001)-style) and global (Integer Linear
Programming, Denis and Baldridge (2007)-
style) models of coreference. Corry relies on a
rich linguistically motivated feature set, which
has, however, been manually reduced to 64
features for efficiency reasons. Three runs
have been submitted for the SemEval task 1
on Coreference Resolution (Recasens et al.,
2010), optimizing Corry’s performance for
BLANC (Recasens and Hovy, in prep), MUC
(Vilain et al., 1995) and CEAF (Luo, 2005).
Corry runs have shown the best performance
level among all the systems in their track for
the corresponding metric.
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999907304347826">
Corry is a system for coreference resolution in En-
glish. It supports both local (Soon et al. (2001)-style)
and global (ILP, Denis and Baldridge (2007)-style)
models of coreference. The backbone of the system
is a family of SVM classifiers for pairs of mentions:
each mention type receives its own classifier. A sep-
arate anaphoricity classifier is learned for the ILP
setting. Corry relies on a rich linguistically moti-
vated feature set, which has, however, been manu-
ally reduced to 64 features for efficiency reasons.
Corry has only participated in the “open” setting,
as it has already a number of preprocessing mod-
ules integrated into the system: the Stanford NLP
toolkit for parsing (Klein and Manning, 2003) and
NE-tagging (Finkel et al., 2005), Wordnet for se-
mantic classes and the U.S. census data for assigning
gender values to person names.
Three runs have been submitted for the Se-
mEval task 1 on Coreference Resolution, optimizing
Corry’s performance for BLANC, MUC and CEAF.
The runs differ with respect to the model (local for
BLANC, global for MUC and CEAF) and the defi-
nition of mention types.
</bodyText>
<sectionHeader confidence="0.795208" genericHeader="introduction">
2 Preprocessing and Mention Extraction
</sectionHeader>
<bodyText confidence="0.998574421052632">
In our previous study (Uryupina, 2008) we have
shown that up to 35% recall and 20% precision er-
rors in coreference resolution for MUC corpora are
due to inaccurate mention detection. We have there-
fore invested substantial efforts into our mention de-
tection module.
Most state-of-the-art coreference resolution sys-
tems operate either on gold markables or on the
output of an ACE-style mention detection module.
We are not aware of extensive studies on mention
extraction algorithms for such datasets as SemEval
(OntoNotes) where mentions are complex NPs not
constrained with respect to their semantic types.
We rely on the Stanford NLP toolkit for extract-
ing named entities (Finkel et al., 2005) and parse
trees for each sentence (Klein and Manning, 2003).
We then merge the output of the NE-tagger and the
parser to create a list of mentions in the following
way:
</bodyText>
<listItem confidence="0.999158571428572">
1. Named entities are considered mentions if
they correspond to a sequence of parsing con-
straints.
2. Pronouns are considered mentions if they are
not a part of an NE-mention.
3. NPs are considered “candidate mentions” if
they are not a part of an NE-mention. The set of
</listItem>
<page confidence="0.865193">
100
</page>
<bodyText confidence="0.952673444444444">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 100–103,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
candidate mentions is then filtered to eliminate
pairs of NPs with the same head noun (coor-
dinate NPs receive unique artificial heads). For
possessive NPs we adjust the boundaries and
the head to exclude the “’s” token. The remain-
ing candidates are aligned with NE-mentions –
if an NE and an NP have the same last word,
they are considered the same mention of a spe-
cial type. Finally, the list of candidates is op-
tionally filtered using a small stop-list (for ex-
ample, all the “there” NPs in “There is ..” are
discarded).
We rely on the Stanford NLP toolkit, WordNet
and the U.S. census data to assign numerous proper-
ties to our mentions: semantic type, number, gender
and others.
</bodyText>
<sectionHeader confidence="0.99962" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999934416666667">
Corry relies on two SVM1 classifiers for coreference
and anaphoricity. The former determines whether
two given mentions Mi and Mj are coreferent or
not. The latter determines whether a given mention
Mi is anaphoric or discourse new. In Section 4 we
show how these classifiers help us build coreference
chains. We use the SVM-Light package (Joachims,
1999) for learning our classifiers.
The strength of our system lies in its rich fea-
ture set for the coreference classifier. In our previous
studies (Uryupina, 2006; 2007) we have tested up to
351 nominal/continuous (1096 boolean/continuous)
features showing significant improvements over ba-
sic feature sets advocated in the literature. For the
SemEval task 1, we have reduced our rich feature set
to 64 nominal/continuous features for efficiency rea-
sons: on the one hand, our new set is large enough to
cover complex linguistic patterns of coreference, on
the other hand, it allows us to test different settings
and investigate possibilities for global modeling.
Our anaphoricity classifier is used by the ILP
model. It relies on 26 boolean/continuous features.
More details on the classifier itself can be found in
(Uryupina, 2003).
</bodyText>
<footnote confidence="0.889917666666667">
1Corry supports a number of machine learning algorithms:
C4.5, TiMBL, Ripper, MaxEnt and SVM. See Uryupina (2006)
for a comparison of Corry’s performance with different learners.
</footnote>
<sectionHeader confidence="0.974972" genericHeader="method">
4 Modeling
</sectionHeader>
<bodyText confidence="0.999975684210527">
Corry supports both global and local views of coref-
erence. Our evaluation experiments (cf. Section 5)
show that the choice of a particular model should be
motivated by the desired scoring metric.
Our local model of coreference is a reimplementa-
tion of the algorithm, proposed by Soon et al. (2001)
with an extended feature set. The core of Soon et
al.’s (2001) approach is a link-based classifier: it
determines whether a given pair of markables are
coreferent or not. During testing, a greedy cluster-
ing algorithm (link-first) is next used to build coref-
erence chains on the output of the classifier.
We have slightly extended this model to allow
separate classifiers for different mention types: each
candidate anaphor receives a type (e.g. “pronoun”)
and is processed with a corresponding classifier. We,
thus, rely on a family of classifiers, with the same
feature set and the same machine learner. The ex-
act definition of mention types is a parameter to be
determined empirically on the development set.
Our global model is largely motivated by Denis
and Baldridge (2007; 2008) and Finkel and Manning
(2008). Following these studies, we use Integer Lin-
ear Programming to find the most globally optimal
solution, given the decisions made by our corefer-
ence and anaphoricity classifiers.
In general, an ILP problem is determined by an
objective function to be maximized (or minimized)
and a set of task-specific constraints. The function
is defined by costs link&lt;i,j&gt;, and dnewj reflecting
potential gains and losses for committing to specific
variable assignments. We assume that costs can be
positive (for pairs of markables that are likely to be
coreferent) or negative (for pairs of markables that
are unlikely to be coreferent). The costs are com-
puted by an external module (such as a family of lo-
cal classifiers described above). The objective func-
tion then takes the form:
</bodyText>
<equation confidence="0.836383333333333">
� max link&lt;i,j&gt; * L&lt;i,j&gt; − )dnewj * Dj
&lt;i,j&gt; j
(1)
</equation>
<bodyText confidence="0.7663706">
Binary variables L&lt;i,j&gt; indicate that two mark-
ables Mi and Mj are coreferent in the output assign-
ment. Binary variables Dj indicate that the mark-
able Mj is considered anaphoric in the output as-
signment. The ILP solver thus assigns values to
</bodyText>
<page confidence="0.991254">
101
</page>
<bodyText confidence="0.966923916666667">
L&lt;i,j&gt;, bi, j : i &lt; j and Dj, bj whilst maximizing
the objective in (1). We take the transitive closure of
all the proposed L&lt;i,j&gt; to build the output partition.
Note that the objective in (1) is not constrained
in any way and will thus allow illegal variable as-
signments. For example it does not constrain the
assignment of L and D variables to be consistent
with one another and does not enforce transitivity.
The following constraints suggested in the literature
(Denis and Baldridge, 2007; Denis and Baldridge,
2008; Finkel and Manning, 2008) ensure that these
and other coreference properties are respected:
</bodyText>
<equation confidence="0.905168545454545">
1. Best-link constraint
B :� L&lt;i,j&gt; &lt; 1, bj (2)
i
2. Transitivity constraints
bi,j,k : i &lt; j &lt; k
T : L&lt;i,j&gt; + L&lt;j,k&gt; − 1 &lt; L&lt;i,k&gt; (3)
L : L&lt;j,k&gt; + L&lt;i,k&gt; − 1 &lt; L&lt;i,j&gt; (4)
R : L&lt;i,j&gt; + L&lt;i,k&gt; − 1 &lt; L&lt;j,k&gt; (5)
3. Anaphoricity constraints
A : &amp; L&lt;i,j&gt; &gt;= Dj bj (6)
D : L&lt;i,j&gt; &lt; Dj bi, j (7)
</equation>
<bodyText confidence="0.999991333333333">
We refer the reader to the above-mentioned pa-
pers for detailed discussions of these constraints and
their impact on coreference resolution. As we show
in Section 5 below, the usability of a particular con-
straint should be determined experimentally based
on the desired system behaviour.
</bodyText>
<sectionHeader confidence="0.998328" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.925002">
5.1 Development
</subsectionHeader>
<bodyText confidence="0.9999506">
Corry has participated in the gold and regular open
settings for English. We have collected a number of
runs on the development data to optimize the per-
formance level for a particular score: BLANC (Re-
casens and Hovy, in prep), MUC (Vilain et al., 1995)
or CEAF (Luo, 2005). The runs differ with respect to
the model (local vs. global with varying sets of con-
straints) and the definition of mention types. We de-
liberately left the B-CUBE score (Bagga and Bald-
win, 1998) completely out of our preliminary ex-
periments. The official SemEval scorer was used for
these experiments.
Our experiments on the development set show
that no configuration is able to produce equally re-
liable scores according to all the metrics (note, for
example, that on the test set the BLANC difference
between Corry-M and Corry-B in the gold setting
is almost 10%). We believe that it is a challenging
point for future research.
We have selected the best configurations for each
score and submitted them as separate runs. The
Corry-C system, optimized for CEAF-φ4, is a global
model with the L, D and A constraints. For the gold
setting, mention types are defined as pronouns and
non-pronouns. For the regular setting, the system
distinguishes between “speech” pronouns, 3rd per-
son pronouns, names and nominals.
Corry-M, optimized for MUC, is a global model
with the D constraint and separate classifiers for
pronouns, names and nominals. Note that, compared
to Corry-C, this setting allows for more coreference
links – it is well known from the literature (cf., for
example, Bagga and Baldwin (1998)) that the MUC
metric is biased towards recall.
Finally, Corry-B, optimized for BLANC, is a
local model that distinguishes between pronouns,
nominals and names. The fact that such a simple
model is able to outperform much more complex
versions of Corry strengthens the importance of fea-
ture engineering.
</bodyText>
<subsectionHeader confidence="0.999504">
5.2 Testing
</subsectionHeader>
<bodyText confidence="0.99981625">
Table 1 shows the SemEval task 1 scores for the
gold/regular open setting. Corry has shown reliable
performance for both mention detection and coref-
erence resolution. For mention detection, Corry’s F-
score is 4% higher than the one of the competing ap-
proach. For coreference, all the Corry runs yielded
the best performance level for a score under opti-
mization.
Finally, for the B-CUBE metric that had not been
optimized at all, Corry lost only marginally to the
RelaxCor system in the gold setting and came first
in the regular setting.
</bodyText>
<sectionHeader confidence="0.996708" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999914">
We have presented Corry – a system for coreference
resolution in English. Our plans include extending it
to cover multiple languages. However, as the main
strength of Corry lies in its rich linguistically moti-
vated feature set, this remains an issue.
</bodyText>
<page confidence="0.992966">
102
</page>
<table confidence="0.9999365">
Mention detection CEAF MUC B3 BLANC
R P F1 R P F1 R P F1 R P F1 R P F1
Language: en, Information: open, Annotation: gold
Corry-B 100 100 100 77.5 77.5 77.5 56.1 57.5 56.8 82.6 85.7 84.1 69.3 75.3 71.8
Corry-C 100 100 100 77.7 77.7 77.7 57.4 58.3 57.9 83.1 84.7 83.9 71.3 71.6 71.5
Corry-M 100 100 100 73.8 73.8 73.8 62.5 56.2 59.2 85.5 78.6 81.9 76.2 58.8 62.7
RelaxCor 100 100 100 75.8 75.8 75.8 22.6 70.5 34.2 75.2 96.7 84.6 58.0 83.8 62.7
Language: en, Information: open, Annotation: regular
BART 76.1 69.8 72.8 70.1 64.3 67.1 62.8 52.4 57.1 74.9 67.7 71.1 55.3 73.2 57.7
Corry-B 79.8 76.4 78.1 70.4 67.4 68.9 55.0 54.2 54.6 73.7 74.1 73.9 57.1 75.7 60.6
Corry-C 79.8 76.4 78.1 70.9 67.9 69.4 54.7 55.5 55.1 73.8 73.1 73.5 57.4 63.8 59.4
Corry-M 79.8 76.4 78.1 66.3 63.5 64.8 61.5 53.4 57.2 76.8 66.5 71.3 58.5 56.2 57.1
</table>
<tableCaption confidence="0.99986">
Table 1: System scores for the gold/regular open setting. The best F-score for each metric shown in bold.
</tableCaption>
<bodyText confidence="0.999863285714286">
An important advantage of Corry is its flexibility:
the system allows for a number of modeling solu-
tions that can be tested on the development set to
optimize the performance level for a particular ob-
jective. Our SemEval task 1 results confirm that a
system might benefit a lot from a direct optimization
for a given performance metric.
</bodyText>
<sectionHeader confidence="0.998431" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997848591549296">
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of the
Linguistic Coreference Workshop at the International
Conference on Language Resources and Evaluation
(LREC-1998), pages 563–566.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of the An-
nual Meeting of the North American Chapter ofthe As-
sociation for Computational Linguistics -Human Lan-
guage Technology Conference (NAACL/HLT-2007).
Pascal Denis and Jason Baldridge. 2008. Corefer-
ence with named entity classification and transitiv-
ity constraints and evaluation with MUC, B-CUBED,
and CEAF. In Proceedings of Corpus-Based Ap-
proaches to Coreference Resolution in Romance Lan-
guages (CBA 2008).
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 2008), Short
Papers, pages 45–48.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
363–370.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Sch¨olkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT-Press.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 423–430.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of the Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics - Human Language Tech-
nology Conference (NAACL/HLT-2005), pages 25–32.
Marta Recasens and Eduard Hovy. in prep. BLANC: Im-
plementing the rand index for coreference evaluation.
Marta Recasens, Lluis M`arquez, Emili Sapena,
M.Ant`onia Marti, Mariona Taul´e, V´eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 Task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010), Uppsala, Sweden.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics (Special Issue on Computational Anaphora
Resolution), 27(4):521–544.
Olga Uryupina. 2003. High-precision identification of
discourse-new and unique noun phrases. In Proceed-
ings of the ACL’03 Student Workshop, pages 80–86.
Olga Uryupina. 2006. Coreference resolution with and
without linguistic knowledge. In Proceedings of the
Language Resources and Evaluation Conference.
Olga Uryupina. 2007. Knowledge Acquisition for Coref-
erence Resolution. Ph.D. thesis, Saarland University.
Olga Uryupina. 2008. Error analysis for learning-based
coreference resolution. In Proceedings of the Lan-
guage Resources and Evaluation Conference.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of the 6th Message Understanding Conference, pages
45–52.
</reference>
<page confidence="0.999296">
103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.832414">
<title confidence="0.992194">Corry: A System for Coreference Resolution</title>
<author confidence="0.981271">Olga Uryupina</author>
<affiliation confidence="0.999193">CiMeC, University of Trento</affiliation>
<email confidence="0.999734">uryupina@gmail.com</email>
<abstract confidence="0.97421">Corry is a system for coreference resolution in English. It supports both local (Soon et al. (2001)-style) and global (Integer Linear Programming, Denis and Baldridge (2007)style) models of coreference. Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons. Three runs have been submitted for the SemEval task 1 on Coreference Resolution (Recasens et al., 2010), optimizing Corry’s performance for BLANC (Recasens and Hovy, in prep), MUC (Vilain et al., 1995) and CEAF (Luo, 2005). Corry runs have shown the best performance level among all the systems in their track for the corresponding metric.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the Linguistic Coreference Workshop at the International Conference on Language Resources and Evaluation (LREC-1998),</booktitle>
<pages>563--566</pages>
<contexts>
<context position="9209" citStr="Bagga and Baldwin, 1998" startWordPosition="1527" endWordPosition="1531">n 5 below, the usability of a particular constraint should be determined experimentally based on the desired system behaviour. 5 Evaluation 5.1 Development Corry has participated in the gold and regular open settings for English. We have collected a number of runs on the development data to optimize the performance level for a particular score: BLANC (Recasens and Hovy, in prep), MUC (Vilain et al., 1995) or CEAF (Luo, 2005). The runs differ with respect to the model (local vs. global with varying sets of constraints) and the definition of mention types. We deliberately left the B-CUBE score (Bagga and Baldwin, 1998) completely out of our preliminary experiments. The official SemEval scorer was used for these experiments. Our experiments on the development set show that no configuration is able to produce equally reliable scores according to all the metrics (note, for example, that on the test set the BLANC difference between Corry-M and Corry-B in the gold setting is almost 10%). We believe that it is a challenging point for future research. We have selected the best configurations for each score and submitted them as separate runs. The Corry-C system, optimized for CEAF-φ4, is a global model with the L,</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the Linguistic Coreference Workshop at the International Conference on Language Resources and Evaluation (LREC-1998), pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the North American Chapter ofthe Association for Computational Linguistics -Human Language Technology Conference (NAACL/HLT-2007).</booktitle>
<contexts>
<context position="965" citStr="Denis and Baldridge (2007)" startWordPosition="143" endWordPosition="146">h linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons. Three runs have been submitted for the SemEval task 1 on Coreference Resolution (Recasens et al., 2010), optimizing Corry’s performance for BLANC (Recasens and Hovy, in prep), MUC (Vilain et al., 1995) and CEAF (Luo, 2005). Corry runs have shown the best performance level among all the systems in their track for the corresponding metric. 1 Introduction Corry is a system for coreference resolution in English. It supports both local (Soon et al. (2001)-style) and global (ILP, Denis and Baldridge (2007)-style) models of coreference. The backbone of the system is a family of SVM classifiers for pairs of mentions: each mention type receives its own classifier. A separate anaphoricity classifier is learned for the ILP setting. Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons. Corry has only participated in the “open” setting, as it has already a number of preprocessing modules integrated into the system: the Stanford NLP toolkit for parsing (Klein and Manning, 2003) and NE-tagging (Finkel et al., 2005), </context>
<context position="6420" citStr="Denis and Baldridge (2007" startWordPosition="1036" endWordPosition="1039">coreferent or not. During testing, a greedy clustering algorithm (link-first) is next used to build coreference chains on the output of the classifier. We have slightly extended this model to allow separate classifiers for different mention types: each candidate anaphor receives a type (e.g. “pronoun”) and is processed with a corresponding classifier. We, thus, rely on a family of classifiers, with the same feature set and the same machine learner. The exact definition of mention types is a parameter to be determined empirically on the development set. Our global model is largely motivated by Denis and Baldridge (2007; 2008) and Finkel and Manning (2008). Following these studies, we use Integer Linear Programming to find the most globally optimal solution, given the decisions made by our coreference and anaphoricity classifiers. In general, an ILP problem is determined by an objective function to be maximized (or minimized) and a set of task-specific constraints. The function is defined by costs link&lt;i,j&gt;, and dnewj reflecting potential gains and losses for committing to specific variable assignments. We assume that costs can be positive (for pairs of markables that are likely to be coreferent) or negative</context>
<context position="8016" citStr="Denis and Baldridge, 2007" startWordPosition="1308" endWordPosition="1311">riables Dj indicate that the markable Mj is considered anaphoric in the output assignment. The ILP solver thus assigns values to 101 L&lt;i,j&gt;, bi, j : i &lt; j and Dj, bj whilst maximizing the objective in (1). We take the transitive closure of all the proposed L&lt;i,j&gt; to build the output partition. Note that the objective in (1) is not constrained in any way and will thus allow illegal variable assignments. For example it does not constrain the assignment of L and D variables to be consistent with one another and does not enforce transitivity. The following constraints suggested in the literature (Denis and Baldridge, 2007; Denis and Baldridge, 2008; Finkel and Manning, 2008) ensure that these and other coreference properties are respected: 1. Best-link constraint B :� L&lt;i,j&gt; &lt; 1, bj (2) i 2. Transitivity constraints bi,j,k : i &lt; j &lt; k T : L&lt;i,j&gt; + L&lt;j,k&gt; − 1 &lt; L&lt;i,k&gt; (3) L : L&lt;j,k&gt; + L&lt;i,k&gt; − 1 &lt; L&lt;i,j&gt; (4) R : L&lt;i,j&gt; + L&lt;i,k&gt; − 1 &lt; L&lt;j,k&gt; (5) 3. Anaphoricity constraints A : &amp; L&lt;i,j&gt; &gt;= Dj bj (6) D : L&lt;i,j&gt; &lt; Dj bi, j (7) We refer the reader to the above-mentioned papers for detailed discussions of these constraints and their impact on coreference resolution. As we show in Section 5 below, the usability of a p</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis and Jason Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In Proceedings of the Annual Meeting of the North American Chapter ofthe Association for Computational Linguistics -Human Language Technology Conference (NAACL/HLT-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Coreference with named entity classification and transitivity constraints and evaluation with MUC, B-CUBED, and CEAF.</title>
<date>2008</date>
<booktitle>In Proceedings of Corpus-Based Approaches to Coreference Resolution in Romance Languages (CBA</booktitle>
<contexts>
<context position="8043" citStr="Denis and Baldridge, 2008" startWordPosition="1312" endWordPosition="1315">e markable Mj is considered anaphoric in the output assignment. The ILP solver thus assigns values to 101 L&lt;i,j&gt;, bi, j : i &lt; j and Dj, bj whilst maximizing the objective in (1). We take the transitive closure of all the proposed L&lt;i,j&gt; to build the output partition. Note that the objective in (1) is not constrained in any way and will thus allow illegal variable assignments. For example it does not constrain the assignment of L and D variables to be consistent with one another and does not enforce transitivity. The following constraints suggested in the literature (Denis and Baldridge, 2007; Denis and Baldridge, 2008; Finkel and Manning, 2008) ensure that these and other coreference properties are respected: 1. Best-link constraint B :� L&lt;i,j&gt; &lt; 1, bj (2) i 2. Transitivity constraints bi,j,k : i &lt; j &lt; k T : L&lt;i,j&gt; + L&lt;j,k&gt; − 1 &lt; L&lt;i,k&gt; (3) L : L&lt;j,k&gt; + L&lt;i,k&gt; − 1 &lt; L&lt;i,j&gt; (4) R : L&lt;i,j&gt; + L&lt;i,k&gt; − 1 &lt; L&lt;j,k&gt; (5) 3. Anaphoricity constraints A : &amp; L&lt;i,j&gt; &gt;= Dj bj (6) D : L&lt;i,j&gt; &lt; Dj bi, j (7) We refer the reader to the above-mentioned papers for detailed discussions of these constraints and their impact on coreference resolution. As we show in Section 5 below, the usability of a particular constraint should</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Pascal Denis and Jason Baldridge. 2008. Coreference with named entity classification and transitivity constraints and evaluation with MUC, B-CUBED, and CEAF. In Proceedings of Corpus-Based Approaches to Coreference Resolution in Romance Languages (CBA 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Enforcing transitivity in coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL 2008), Short Papers,</booktitle>
<pages>45--48</pages>
<contexts>
<context position="6457" citStr="Finkel and Manning (2008)" startWordPosition="1042" endWordPosition="1045">greedy clustering algorithm (link-first) is next used to build coreference chains on the output of the classifier. We have slightly extended this model to allow separate classifiers for different mention types: each candidate anaphor receives a type (e.g. “pronoun”) and is processed with a corresponding classifier. We, thus, rely on a family of classifiers, with the same feature set and the same machine learner. The exact definition of mention types is a parameter to be determined empirically on the development set. Our global model is largely motivated by Denis and Baldridge (2007; 2008) and Finkel and Manning (2008). Following these studies, we use Integer Linear Programming to find the most globally optimal solution, given the decisions made by our coreference and anaphoricity classifiers. In general, an ILP problem is determined by an objective function to be maximized (or minimized) and a set of task-specific constraints. The function is defined by costs link&lt;i,j&gt;, and dnewj reflecting potential gains and losses for committing to specific variable assignments. We assume that costs can be positive (for pairs of markables that are likely to be coreferent) or negative (for pairs of markables that are unl</context>
<context position="8070" citStr="Finkel and Manning, 2008" startWordPosition="1316" endWordPosition="1319"> anaphoric in the output assignment. The ILP solver thus assigns values to 101 L&lt;i,j&gt;, bi, j : i &lt; j and Dj, bj whilst maximizing the objective in (1). We take the transitive closure of all the proposed L&lt;i,j&gt; to build the output partition. Note that the objective in (1) is not constrained in any way and will thus allow illegal variable assignments. For example it does not constrain the assignment of L and D variables to be consistent with one another and does not enforce transitivity. The following constraints suggested in the literature (Denis and Baldridge, 2007; Denis and Baldridge, 2008; Finkel and Manning, 2008) ensure that these and other coreference properties are respected: 1. Best-link constraint B :� L&lt;i,j&gt; &lt; 1, bj (2) i 2. Transitivity constraints bi,j,k : i &lt; j &lt; k T : L&lt;i,j&gt; + L&lt;j,k&gt; − 1 &lt; L&lt;i,k&gt; (3) L : L&lt;j,k&gt; + L&lt;i,k&gt; − 1 &lt; L&lt;i,j&gt; (4) R : L&lt;i,j&gt; + L&lt;i,k&gt; − 1 &lt; L&lt;j,k&gt; (5) 3. Anaphoricity constraints A : &amp; L&lt;i,j&gt; &gt;= Dj bj (6) D : L&lt;i,j&gt; &lt; Dj bi, j (7) We refer the reader to the above-mentioned papers for detailed discussions of these constraints and their impact on coreference resolution. As we show in Section 5 below, the usability of a particular constraint should be determined experimental</context>
</contexts>
<marker>Finkel, Manning, 2008</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2008. Enforcing transitivity in coreference resolution. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL 2008), Short Papers, pages 45–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="1563" citStr="Finkel et al., 2005" startWordPosition="240" endWordPosition="243">s and Baldridge (2007)-style) models of coreference. The backbone of the system is a family of SVM classifiers for pairs of mentions: each mention type receives its own classifier. A separate anaphoricity classifier is learned for the ILP setting. Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons. Corry has only participated in the “open” setting, as it has already a number of preprocessing modules integrated into the system: the Stanford NLP toolkit for parsing (Klein and Manning, 2003) and NE-tagging (Finkel et al., 2005), Wordnet for semantic classes and the U.S. census data for assigning gender values to person names. Three runs have been submitted for the SemEval task 1 on Coreference Resolution, optimizing Corry’s performance for BLANC, MUC and CEAF. The runs differ with respect to the model (local for BLANC, global for MUC and CEAF) and the definition of mention types. 2 Preprocessing and Mention Extraction In our previous study (Uryupina, 2008) we have shown that up to 35% recall and 20% precision errors in coreference resolution for MUC corpora are due to inaccurate mention detection. We have therefore </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning. MIT-Press.</booktitle>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<contexts>
<context position="4326" citStr="Joachims, 1999" startWordPosition="703" endWordPosition="704">sing a small stop-list (for example, all the “there” NPs in “There is ..” are discarded). We rely on the Stanford NLP toolkit, WordNet and the U.S. census data to assign numerous properties to our mentions: semantic type, number, gender and others. 3 Features Corry relies on two SVM1 classifiers for coreference and anaphoricity. The former determines whether two given mentions Mi and Mj are coreferent or not. The latter determines whether a given mention Mi is anaphoric or discourse new. In Section 4 we show how these classifiers help us build coreference chains. We use the SVM-Light package (Joachims, 1999) for learning our classifiers. The strength of our system lies in its rich feature set for the coreference classifier. In our previous studies (Uryupina, 2006; 2007) we have tested up to 351 nominal/continuous (1096 boolean/continuous) features showing significant improvements over basic feature sets advocated in the literature. For the SemEval task 1, we have reduced our rich feature set to 64 nominal/continuous features for efficiency reasons: on the one hand, our new set is large enough to cover complex linguistic patterns of coreference, on the other hand, it allows us to test different se</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT-Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="1526" citStr="Klein and Manning, 2003" startWordPosition="234" endWordPosition="237">t al. (2001)-style) and global (ILP, Denis and Baldridge (2007)-style) models of coreference. The backbone of the system is a family of SVM classifiers for pairs of mentions: each mention type receives its own classifier. A separate anaphoricity classifier is learned for the ILP setting. Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons. Corry has only participated in the “open” setting, as it has already a number of preprocessing modules integrated into the system: the Stanford NLP toolkit for parsing (Klein and Manning, 2003) and NE-tagging (Finkel et al., 2005), Wordnet for semantic classes and the U.S. census data for assigning gender values to person names. Three runs have been submitted for the SemEval task 1 on Coreference Resolution, optimizing Corry’s performance for BLANC, MUC and CEAF. The runs differ with respect to the model (local for BLANC, global for MUC and CEAF) and the definition of mention types. 2 Preprocessing and Mention Extraction In our previous study (Uryupina, 2008) we have shown that up to 35% recall and 20% precision errors in coreference resolution for MUC corpora are due to inaccurate </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics - Human Language Technology Conference (NAACL/HLT-2005),</booktitle>
<pages>25--32</pages>
<contexts>
<context position="682" citStr="Luo, 2005" startWordPosition="99" endWordPosition="100">ty of Trento uryupina@gmail.com Abstract Corry is a system for coreference resolution in English. It supports both local (Soon et al. (2001)-style) and global (Integer Linear Programming, Denis and Baldridge (2007)- style) models of coreference. Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons. Three runs have been submitted for the SemEval task 1 on Coreference Resolution (Recasens et al., 2010), optimizing Corry’s performance for BLANC (Recasens and Hovy, in prep), MUC (Vilain et al., 1995) and CEAF (Luo, 2005). Corry runs have shown the best performance level among all the systems in their track for the corresponding metric. 1 Introduction Corry is a system for coreference resolution in English. It supports both local (Soon et al. (2001)-style) and global (ILP, Denis and Baldridge (2007)-style) models of coreference. The backbone of the system is a family of SVM classifiers for pairs of mentions: each mention type receives its own classifier. A separate anaphoricity classifier is learned for the ILP setting. Corry relies on a rich linguistically motivated feature set, which has, however, been manua</context>
<context position="9013" citStr="Luo, 2005" startWordPosition="1495" endWordPosition="1496">L&lt;i,j&gt; &lt; Dj bi, j (7) We refer the reader to the above-mentioned papers for detailed discussions of these constraints and their impact on coreference resolution. As we show in Section 5 below, the usability of a particular constraint should be determined experimentally based on the desired system behaviour. 5 Evaluation 5.1 Development Corry has participated in the gold and regular open settings for English. We have collected a number of runs on the development data to optimize the performance level for a particular score: BLANC (Recasens and Hovy, in prep), MUC (Vilain et al., 1995) or CEAF (Luo, 2005). The runs differ with respect to the model (local vs. global with varying sets of constraints) and the definition of mention types. We deliberately left the B-CUBE score (Bagga and Baldwin, 1998) completely out of our preliminary experiments. The official SemEval scorer was used for these experiments. Our experiments on the development set show that no configuration is able to produce equally reliable scores according to all the metrics (note, for example, that on the test set the BLANC difference between Corry-M and Corry-B in the gold setting is almost 10%). We believe that it is a challeng</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics - Human Language Technology Conference (NAACL/HLT-2005), pages 25–32.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marta</author>
</authors>
<title>Recasens and Eduard Hovy. in prep. BLANC: Implementing the rand index for coreference evaluation.</title>
<marker>Marta, </marker>
<rawString>Marta Recasens and Eduard Hovy. in prep. BLANC: Implementing the rand index for coreference evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
</authors>
<title>Lluis M`arquez, Emili Sapena, M.Ant`onia Marti, Mariona Taul´e, V´eronique Hoste, Massimo Poesio, and Yannick Versley.</title>
<date>2010</date>
<marker>Recasens, 2010</marker>
<rawString>Marta Recasens, Lluis M`arquez, Emili Sapena, M.Ant`onia Marti, Mariona Taul´e, V´eronique Hoste, Massimo Poesio, and Yannick Versley. 2010.</rawString>
</citation>
<citation valid="true">
<title>SemEval-2010 Task 1: Coreference resolution in multiple languages.</title>
<date></date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010),</booktitle>
<location>Uppsala,</location>
<marker></marker>
<rawString>SemEval-2010 Task 1: Coreference resolution in multiple languages. In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics (Special Issue on Computational Anaphora Resolution),</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="914" citStr="Soon et al. (2001)" startWordPosition="136" endWordPosition="139">odels of coreference. Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons. Three runs have been submitted for the SemEval task 1 on Coreference Resolution (Recasens et al., 2010), optimizing Corry’s performance for BLANC (Recasens and Hovy, in prep), MUC (Vilain et al., 1995) and CEAF (Luo, 2005). Corry runs have shown the best performance level among all the systems in their track for the corresponding metric. 1 Introduction Corry is a system for coreference resolution in English. It supports both local (Soon et al. (2001)-style) and global (ILP, Denis and Baldridge (2007)-style) models of coreference. The backbone of the system is a family of SVM classifiers for pairs of mentions: each mention type receives its own classifier. A separate anaphoricity classifier is learned for the ILP setting. Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons. Corry has only participated in the “open” setting, as it has already a number of preprocessing modules integrated into the system: the Stanford NLP toolkit for parsing (Klein and Ma</context>
<context position="5642" citStr="Soon et al. (2001)" startWordPosition="909" endWordPosition="912">he ILP model. It relies on 26 boolean/continuous features. More details on the classifier itself can be found in (Uryupina, 2003). 1Corry supports a number of machine learning algorithms: C4.5, TiMBL, Ripper, MaxEnt and SVM. See Uryupina (2006) for a comparison of Corry’s performance with different learners. 4 Modeling Corry supports both global and local views of coreference. Our evaluation experiments (cf. Section 5) show that the choice of a particular model should be motivated by the desired scoring metric. Our local model of coreference is a reimplementation of the algorithm, proposed by Soon et al. (2001) with an extended feature set. The core of Soon et al.’s (2001) approach is a link-based classifier: it determines whether a given pair of markables are coreferent or not. During testing, a greedy clustering algorithm (link-first) is next used to build coreference chains on the output of the classifier. We have slightly extended this model to allow separate classifiers for different mention types: each candidate anaphor receives a type (e.g. “pronoun”) and is processed with a corresponding classifier. We, thus, rely on a family of classifiers, with the same feature set and the same machine lea</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics (Special Issue on Computational Anaphora Resolution), 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
</authors>
<title>High-precision identification of discourse-new and unique noun phrases.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL’03 Student Workshop,</booktitle>
<pages>80--86</pages>
<contexts>
<context position="5153" citStr="Uryupina, 2003" startWordPosition="833" endWordPosition="834">096 boolean/continuous) features showing significant improvements over basic feature sets advocated in the literature. For the SemEval task 1, we have reduced our rich feature set to 64 nominal/continuous features for efficiency reasons: on the one hand, our new set is large enough to cover complex linguistic patterns of coreference, on the other hand, it allows us to test different settings and investigate possibilities for global modeling. Our anaphoricity classifier is used by the ILP model. It relies on 26 boolean/continuous features. More details on the classifier itself can be found in (Uryupina, 2003). 1Corry supports a number of machine learning algorithms: C4.5, TiMBL, Ripper, MaxEnt and SVM. See Uryupina (2006) for a comparison of Corry’s performance with different learners. 4 Modeling Corry supports both global and local views of coreference. Our evaluation experiments (cf. Section 5) show that the choice of a particular model should be motivated by the desired scoring metric. Our local model of coreference is a reimplementation of the algorithm, proposed by Soon et al. (2001) with an extended feature set. The core of Soon et al.’s (2001) approach is a link-based classifier: it determi</context>
</contexts>
<marker>Uryupina, 2003</marker>
<rawString>Olga Uryupina. 2003. High-precision identification of discourse-new and unique noun phrases. In Proceedings of the ACL’03 Student Workshop, pages 80–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
</authors>
<title>Coreference resolution with and without linguistic knowledge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="4484" citStr="Uryupina, 2006" startWordPosition="729" endWordPosition="730"> to assign numerous properties to our mentions: semantic type, number, gender and others. 3 Features Corry relies on two SVM1 classifiers for coreference and anaphoricity. The former determines whether two given mentions Mi and Mj are coreferent or not. The latter determines whether a given mention Mi is anaphoric or discourse new. In Section 4 we show how these classifiers help us build coreference chains. We use the SVM-Light package (Joachims, 1999) for learning our classifiers. The strength of our system lies in its rich feature set for the coreference classifier. In our previous studies (Uryupina, 2006; 2007) we have tested up to 351 nominal/continuous (1096 boolean/continuous) features showing significant improvements over basic feature sets advocated in the literature. For the SemEval task 1, we have reduced our rich feature set to 64 nominal/continuous features for efficiency reasons: on the one hand, our new set is large enough to cover complex linguistic patterns of coreference, on the other hand, it allows us to test different settings and investigate possibilities for global modeling. Our anaphoricity classifier is used by the ILP model. It relies on 26 boolean/continuous features. M</context>
</contexts>
<marker>Uryupina, 2006</marker>
<rawString>Olga Uryupina. 2006. Coreference resolution with and without linguistic knowledge. In Proceedings of the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
</authors>
<title>Knowledge Acquisition for Coreference Resolution.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Saarland University.</institution>
<marker>Uryupina, 2007</marker>
<rawString>Olga Uryupina. 2007. Knowledge Acquisition for Coreference Resolution. Ph.D. thesis, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
</authors>
<title>Error analysis for learning-based coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="2000" citStr="Uryupina, 2008" startWordPosition="315" endWordPosition="316">s it has already a number of preprocessing modules integrated into the system: the Stanford NLP toolkit for parsing (Klein and Manning, 2003) and NE-tagging (Finkel et al., 2005), Wordnet for semantic classes and the U.S. census data for assigning gender values to person names. Three runs have been submitted for the SemEval task 1 on Coreference Resolution, optimizing Corry’s performance for BLANC, MUC and CEAF. The runs differ with respect to the model (local for BLANC, global for MUC and CEAF) and the definition of mention types. 2 Preprocessing and Mention Extraction In our previous study (Uryupina, 2008) we have shown that up to 35% recall and 20% precision errors in coreference resolution for MUC corpora are due to inaccurate mention detection. We have therefore invested substantial efforts into our mention detection module. Most state-of-the-art coreference resolution systems operate either on gold markables or on the output of an ACE-style mention detection module. We are not aware of extensive studies on mention extraction algorithms for such datasets as SemEval (OntoNotes) where mentions are complex NPs not constrained with respect to their semantic types. We rely on the Stanford NLP too</context>
</contexts>
<marker>Uryupina, 2008</marker>
<rawString>Olga Uryupina. 2008. Error analysis for learning-based coreference resolution. In Proceedings of the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Message Understanding Conference,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="661" citStr="Vilain et al., 1995" startWordPosition="93" endWordPosition="96">n Olga Uryupina CiMeC, University of Trento uryupina@gmail.com Abstract Corry is a system for coreference resolution in English. It supports both local (Soon et al. (2001)-style) and global (Integer Linear Programming, Denis and Baldridge (2007)- style) models of coreference. Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons. Three runs have been submitted for the SemEval task 1 on Coreference Resolution (Recasens et al., 2010), optimizing Corry’s performance for BLANC (Recasens and Hovy, in prep), MUC (Vilain et al., 1995) and CEAF (Luo, 2005). Corry runs have shown the best performance level among all the systems in their track for the corresponding metric. 1 Introduction Corry is a system for coreference resolution in English. It supports both local (Soon et al. (2001)-style) and global (ILP, Denis and Baldridge (2007)-style) models of coreference. The backbone of the system is a family of SVM classifiers for pairs of mentions: each mention type receives its own classifier. A separate anaphoricity classifier is learned for the ILP setting. Corry relies on a rich linguistically motivated feature set, which has</context>
<context position="8993" citStr="Vilain et al., 1995" startWordPosition="1489" endWordPosition="1492">A : &amp; L&lt;i,j&gt; &gt;= Dj bj (6) D : L&lt;i,j&gt; &lt; Dj bi, j (7) We refer the reader to the above-mentioned papers for detailed discussions of these constraints and their impact on coreference resolution. As we show in Section 5 below, the usability of a particular constraint should be determined experimentally based on the desired system behaviour. 5 Evaluation 5.1 Development Corry has participated in the gold and regular open settings for English. We have collected a number of runs on the development data to optimize the performance level for a particular score: BLANC (Recasens and Hovy, in prep), MUC (Vilain et al., 1995) or CEAF (Luo, 2005). The runs differ with respect to the model (local vs. global with varying sets of constraints) and the definition of mention types. We deliberately left the B-CUBE score (Bagga and Baldwin, 1998) completely out of our preliminary experiments. The official SemEval scorer was used for these experiments. Our experiments on the development set show that no configuration is able to produce equally reliable scores according to all the metrics (note, for example, that on the test set the BLANC difference between Corry-M and Corry-B in the gold setting is almost 10%). We believe t</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference, pages 45–52.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>