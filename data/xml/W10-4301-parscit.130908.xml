<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002445">
<title confidence="0.986072">
Towards Incremental Speech Generation in Dialogue Systems
</title>
<author confidence="0.961717">
Gabriel Skantze Anna Hjalmarsson
</author>
<affiliation confidence="0.708851">
Dept. of Speech Music and Hearing Dept. of Speech Music and Hearing
KTH, Stockholm, Sweden KTH, Stockholm, Sweden
</affiliation>
<email confidence="0.974612">
gabriel@speech.kth.se annah@speech.kth.se
</email>
<sectionHeader confidence="0.993157" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987266666667">
We present a first step towards a model of
speech generation for incremental dialogue
systems. The model allows a dialogue system
to incrementally interpret spoken input, while
simultaneously planning, realising and self-
monitoring the system response. The model
has been implemented in a general dialogue
system framework. Using this framework, we
have implemented a specific application and
tested it in a Wizard-of-Oz setting, comparing
it with a non-incremental version of the same
system. The results show that the incremental
version, while producing longer utterances,
has a shorter response time and is perceived
as more efficient by the users.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948103448276">
Speakers in dialogue produce speech in a piece-
meal fashion and on-line as the dialogue pro-
gresses. When starting to speak, dialogue partici-
pants typically do not have a complete plan of
how to say something or even what to say. Yet,
they manage to rapidly integrate information
from different sources in parallel and simultane-
ously plan and realize new dialogue contribu-
tions. Moreover, interlocutors continuously self-
monitor the actual production processes in order
to facilitate self-corrections (Levelt, 1989). Con-
trary to this, most spoken dialogue systems use a
silence threshold to determine when the user has
stopped speaking. The user utterance is then
processed by one module at a time, after which a
complete system utterance is produced and real-
ised by a speech synthesizer.
This paper has two purposes. First, to present
an initial step towards a model of speech genera-
tion that allows a dialogue system to incremen-
tally interpret spoken input, while simultaneously
planning, realising and self-monitoring the sys-
tem response. The model has been implemented
in a general dialogue system framework. This is
described in Section 2 and 3. The second purpose
is to evaluate the usefulness of incremental
speech generation in a Wizard-of-Oz setting, us-
ing the proposed model. This is described in Sec-
tion 4.
</bodyText>
<subsectionHeader confidence="0.980368">
1.1 Motivation
</subsectionHeader>
<bodyText confidence="0.99991940625">
A non-incremental dialogue system waits until
the user has stopped speaking (using a silence
threshold to determine this) before starting to
process the utterance and then produce a system
response. If processing takes time, for example
because an external resource is being accessed,
this may result in a confusing response delay. An
incremental system may instead continuously
build a tentative plan of what to say as the user is
speaking. When it detects that the user’s utter-
ance has ended, it may start to asynchronously
realise this plan while processing continues, with
the possibility to revise the plan if needed.
There are many potential reasons for why dia-
logue systems may need additional time for
processing. For example, it has been assumed
that ASR processing has to be done in real-time,
in order to avoid long and confusing response
delays. Yet, if we allow the system to start
speaking before input is complete, we can allow
more accurate (and time-consuming) ASR proc-
essing (for example by broadening the beam). In
this paper, we will explore incremental speech
generation in a Wizard-of-oz setting. A common
problem in such settings is the time it takes for
the Wizard to interpret the user’s utterance
and/or decide on the next system action, resulting
in unacceptable response delays (Fraser &amp; Gil-
bert, 1991). Thus, it would be useful if the sys-
tem could start to speak as soon as the user has
finished speaking, based on the Wizard’s actions
so far.
</bodyText>
<note confidence="0.6251135">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 1–8,
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</note>
<page confidence="0.9851">
1
</page>
<sectionHeader confidence="0.987513" genericHeader="introduction">
1.2 Related work
</sectionHeader>
<bodyText confidence="0.99997905">
Incremental speech generation has been studied
from different perspectives. From a psycholin-
guistic perspective, Levelt (1989) and others
have studied how speakers incrementally pro-
duce utterances while self-monitoring the output,
both overtly (listening to oneself speaking) and
covertly (mentally monitoring what is about to
be said). As deviations from the desired output is
detected, the speaker may initiate self-repairs. If
the item to be repaired has already been spoken,
an overt repair is needed (for example by using
an editing term, such as “sorry”). If not, the ut-
terance plan may be altered to accommodate the
repair, a so-called covert repair. Central to the
concept of incremental speech generation is that
the realization of overt speech can be initiated
before the speaker has a complete plan of what to
say. An option for a speaker who does not know
what to say (but wants to claim the floor) is to
use hesitation phenomena such as filled pauses
(“eh”) or cue phrases such as “let’s see”.
A dialogue system may not need to self-
monitor its output for the same reasons as hu-
mans do. For example, there is no risk of articu-
latory errors (with current speech synthesis tech-
nology). However, a dialogue system may utilize
the same mechanisms of self-repair and hesita-
tion phenomena to simultaneously plan and real-
ise the spoken output, as there is always a risk
for revision in the input to an incremental mod-
ule (as described in Section 2.1).
There is also another aspect of self-monitoring
that is important for dialogue systems. In a sys-
tem with modules operating asynchronously, the
dialogue manager cannot know whether the in-
tended output is actually realized, as the user
may interrupt the system. Also, the timing of the
synthesized speech is important, as the user may
give feedback in the middle of a system utter-
ance. Thus, an incremental, asynchronous system
somehow needs to self-monitor its own output.
From a syntactic perspective, Kempen &amp;
Hoenkamp (1987) and Kilger &amp; Finkler (1995)
have studied how to syntactically formulate sen-
tences incrementally under time constraints.
Dohsaka &amp; Shimazu (1997) describes a system
architecture for incremental speech generation.
However, there is no account for revision of the
input (as discussed in Section 2.1) and there is no
evaluation with users. Skantze &amp; Schlangen
(2009) describe an incremental system that partly
supports incremental output and that is evaluated
with users, but the domain is limited to number
dictation.
In this study, the focus is not on syntactic con-
struction of utterances, but on how to build prac-
tical incremental dialogue systems within limited
domains that can handle revisions and produce
convincing, flexible and varied speech output in
on-line interaction with users.
</bodyText>
<sectionHeader confidence="0.850944" genericHeader="method">
2 The Jindigo framework
</sectionHeader>
<bodyText confidence="0.999927">
The proposed model has been implemented in
Jindigo – a Java-based open source framework
for implementing and experimenting with incre-
mental dialogue systems (www.jindigo.net). We
will here briefly describe this framework and the
model of incremental dialogue processing that it
is based on.
</bodyText>
<subsectionHeader confidence="0.925657">
2.1 Incremental units
</subsectionHeader>
<bodyText confidence="0.966510321428571">
Schlangen &amp; Skantze (2009) describes a general,
abstract model of incremental dialogue process-
ing, which Jindigo is based on. In this model, a
system consists of a network of processing mod-
ules. Each module has a left buffer, a processor,
and a right buffer, where the normal mode of
processing is to receive input from the left
buffer, process it, and provide output in the right
buffer, from where it is forwarded to the next
module’s left buffer. An example is shown in
Figure 1. Modules exchange incremental units
(IUs), which are the smallest ‘chunks’ of infor-
mation that can trigger connected modules into
action (such as words, phrases, communicative
acts, etc). IUs are typically part of larger units:
individual words are parts of an utterance; con-
cepts are part of the representation of an utter-
ance meaning. This relation of being part of the
same larger unit is recorded through same-level
links. In the example below, IU2 has a same-level
link to IU1 of type PREDECESSOR, meaning that
they are linearly ordered. The information that
was used in creating a given IU is linked to it via
grounded-in links. In the example, IU3 is
grounded in IU1 and IU2, while IU4 is grounded
in IU3.
left buffer processor right buffer
left buffer processor right buffer
</bodyText>
<figureCaption confidence="0.999098">
Figure 1: Two connected modules.
</figureCaption>
<figure confidence="0.9969235">
Module A
IU1 IU2
IU1
IU3
IU2
IU3
IU3
IUa
Module B
IUa
</figure>
<page confidence="0.995524">
2
</page>
<table confidence="0.999591666666667">
String Right buffer Update
message
t1: one w1 one w2 [w1, w2]
t2: one five w1 one w2 five w3 [w1, w3]
t3: one w1 one w2 five w3 [w1, w2]
t4: one four five w1 one w2 five w3 five w5 [w1, w5]
four w4
t5: [commit] w1 one w2 five w3 five w5 [w5,w5]
four w4
</table>
<tableCaption confidence="0.9885545">
Table 1: The right buffer of an ASR module, and up-
date messages at different time-steps.
</tableCaption>
<bodyText confidence="0.999860538461538">
A challenge for incremental systems is to han-
dle revisions. For example, as the first part of the
word “forty” is recognised, the best hypothesis
might be “four”. As the speech recogniser re-
ceives more input, it might need to revise its pre-
vious output, which might cause a chain of revi-
sions in all subsequent modules. To cope with
this, modules have to be able to react to three
basic situations: that IUs are added to a buffer,
which triggers processing; that IUs that were er-
roneously hypothesized by an earlier module are
revoked, which may trigger a revision of a mod-
ule’s own output; and that modules signal that
they commit to an IU, that is, won’t revoke it
anymore.
Jindigo implements an efficient model for
communicating these updates. In this model, IUs
are associated with edges in a graph, as shown in
Table 1. The graph may be incrementally
amended without actually removing edges or
vertices, even if revision occurs. At each time-
step, a new update message is sent to the con-
suming module. The update message contains a
pair of pointers [C, A]: (C) the vertex from which
the currently committed hypothesis can be con-
structed, and (A) the vertex from which the cur-
</bodyText>
<figureCaption confidence="0.997479">
Figure 2: A typical Jindigo system architecture.
</figureCaption>
<bodyText confidence="0.9897135">
rently best tentative hypothesis can be con-
structed. In Jindigo, all modules run as threads
within a single Java process, and therefore have
access to the same memory space.
</bodyText>
<subsectionHeader confidence="0.996088">
2.2 A typical architecture
</subsectionHeader>
<bodyText confidence="0.994072947368421">
A typical Jindigo system architecture is shown in
Figure 2. The word buffer from the Recognizer
module is parsed by the Interpreter module
which tries to find an optimal sequence of top
phrases and their semantic representations. These
phrases are then interpreted in light of the current
dialogue context by the Contextualizer module
and are packaged as Communicative Acts (CAs).
As can be seen in Figure 2, the Contextualizer
also self-monitors Concepts from the system as
they are spoken by the Vocalizer, which makes it
possible to contextually interpret user responses
to system utterances. This also makes it possible
for the system to know whether an intended ut-
terance actually was produced, or if it was inter-
rupted. The current context is sent to the Action
Manager, which generates a SpeechPlan that is
sent to the Vocalizer. This is described in detail
in the next section.
</bodyText>
<figure confidence="0.999472491525424">
Speech
User
SpeechPlan
ActionManager
Vocalizer
Speech
Concept
Utterance
Segment
Context
Contextualizer
Interpreter
ASR
Word
Phrase
VAD
ASR
Contextualizer
Interpreter
User System
W W W W
P
Utterance
Segment
CA
C
Utterance
Phrase
W
US
W
CA
Concept
W
Utterance
US
W
W
Response
To
Other
Delay
Speech
Segment
SU
SS
SU
SU
SpeechPlan
Self
Delay
SU
SS
SS
SU
Concept
SU
Action Manager
Vocalizer
</figure>
<figureCaption confidence="0.999326">
Figure 3: Incremental Units at different levels of processing. Some grounded-in relations are shown with dotted
lines. W=Word, SS=SpeechSegment, SU=SpeechUnit, CA=Communicative Act.
</figureCaption>
<page confidence="0.996157">
3
</page>
<sectionHeader confidence="0.993547" genericHeader="method">
3 Incremental speech generation
</sectionHeader>
<subsectionHeader confidence="0.969538">
3.1 Incremental units of speech
</subsectionHeader>
<bodyText confidence="0.999847410714286">
In order for user and system utterances to be in-
terpreted and produced incrementally, they need
to be decomposed into smaller units of process-
ing (IUs). This decomposition is shown in Figure
3. Using a standard voice activity detector
(VAD) in the ASR, the user’s speech is chunked
into Utterance-units. The Utterance bounda-
ries determine when the ASR hypothesis is
committed. However, for the system to be able to
respond quickly, the end silence threshold of
these Utterances are typically too long. Therefore
smaller units of the type UtteranceSegment
(US) are detected, using a much shorter silence
threshold of about 50ms. Such short silence
thresholds allow the system to give very fast re-
sponses (such as backchannels). Information
about US boundaries is sent directly from the
ASR to the Vocalizer. As Figure 3 illustrates, the
grounded-in links can be followed to derive the
timing of IUs at different levels of processing.
The system output is also modelled using IUs
at different processing levels. The widest-
spanning IU on the output side is the
SpeechPlan. The rendering of a SpeechPlan
will result in a sequence of SpeechSegment’s,
where each SpeechSegment represents a con-
tinuous audio rendering of speech, either as a
synthesised string or a pre-recorded audio file.
For example, the plan may be to say “okay, a red
doll, here is a nice doll”, consisting of three seg-
ments. Now, there are two requirements that we
need to meet. First, the output should be varied:
the system should not give exactly the same re-
sponse every time to the same request. But, as
we will see, the output in an incremental system
must also be flexible, as speech plans are incre-
mentally produced and amended. In order to re-
lieve the Action Manager of the burden of vary-
ing the output and making time-critical adjust-
ments, we model the SpeechPlan as a directed
graph, where each edge is associated with a
SpeechSegment, as shown in Figure 4. Thus, the
Action Manager may asynchronously plan (a set
of possible) responses, while the Vocalizer se-
lects the rendering path in the graph and takes
care of time-critical synchronization. To control
the rendering, each SpeechSegment has the
properties optional, committing, selfDelay
and otherDelay, as described in the next sec-
tion. It must also be possible for an incremental
system to interrupt and make self-repairs in the
middle of a SpeechSegment. Therefore, each
SpeechSegment may also be decomposed into an
array of SpeechUnit’s, where each SpeechUnit
contains pointers to the audio rendering in the
SpeechSegment.
</bodyText>
<subsectionHeader confidence="0.999685">
3.2 Producing and consuming SpeechPlans
</subsectionHeader>
<bodyText confidence="0.999907714285714">
The SpeechPlan does not need to be complete
before the system starts to speak. An example of
this is shown in Figure 4. As more words are
recognised by the ASR, the Action Manager may
add more SpeechSegment’s to the graph. Thus,
the system may start to say “it costs” before it
knows which object is being talked about.
</bodyText>
<figureCaption confidence="0.9924836">
Figure 4: The right buffer of an ASR (top) and the
SpeechPlan that is incrementally produced (bottom).
Vertex s1 is associated with w1, s3 with w3, etc. Op-
tional, non-committing SpeechSegment’s are marked
with dashed outline.
</figureCaption>
<bodyText confidence="0.999596">
The SpeechPlan has a pointer called
finalVertex. When the Vocalizer reaches the
finalVertex, the SpeechPlan is completely
realised. If finalVertex is not set, it means that
the SpeechPlan is not yet completely con-
structed. The SpeechSegment property
optional tells whether the segment needs to be
realised or if it could be skipped if the
finalVertex is in sight. This makes it possible
to insert floor-keeping SpeechSegment’s (such
as “eh”) in the graph, which are only realised if
needed. The Vocalizer also keeps track of which
SpeechSegment’s it has realised before, so that it
can look ahead in the graph and realise a more
varied output. Each SpeechSegment may carry a
semantic representation of the segment (a
Concept). This is sent by the Vocalizer to the
Contextualizer as soon as the segment has been
realised.
The SpeechSegment properties selfDelay
and otherDelay regulate the timing of the out-
put (as illustrated in Figure 3). They specify the
number of milliseconds that should pass before
the Vocalizer starts to play the segment, depend-
ing on the previous speaker. By setting the
otherDelay of a segment, the Action Manager
may delay the response depending on how cer-
tain it is that it is appropriate to speak, for exam-
ple by considering pitch and semantic complete-
ness. (See Raux &amp; Eskenazi (2008) for a study
</bodyText>
<figure confidence="0.985998166666667">
w1
how
well
eh
w2
much
s1
w3 is w4 the w5 doll w6
you can have it for
let’s say
it costs
s3 40 crowns s6
</figure>
<page confidence="0.98437">
4
</page>
<bodyText confidence="0.999865230769231">
on how such dynamic delays can be derived us-
ing machine learning.)
If the user starts to speak (i.e., a new
UtteranceSegment is initiated) as the system is
speaking, the Vocalizer pauses (at a SpeechUnit
boundary) and waits until it has received a new
response from the Action Manager. The Action
Manager may then choose to generate a new re-
sponse or simply ignore the last input, in which
case the Vocalizer continues from the point of
interruption. This may happen if, for example,
the UtteranceSegment was identified as a back-
channel, cough, or similar.
</bodyText>
<subsectionHeader confidence="0.999641">
3.3 Self-repairs
</subsectionHeader>
<bodyText confidence="0.983437">
As Figure 3 shows, a SpeechPlan may be
grounded in a user CA (i.e., it is a response to
this CA). If this CA is revoked, or if the
SpeechPlan is revised, the Vocalizer may initial-
ize a self-repair. The Vocalizer keeps a list of the
SpeechSegment’s it has realised so far. If the
SpeechPlan is revised when it has been partly
realised, the Vocalizer compares the history with
the new graph and chooses one of the different
repair strategies shown in Table 2. In the best
case, it may smoothly switch to the new plan
without the user noticing it (covert repair). In
case of a unit repair, the Vocalizer searches for a
zero-crossing point in the audio segment, close to
the boundary pointed out by the SpeechUnit.
covert you are right it is blue
segment
repair
you are right they are blue
overt sorry
segment
repair
you are right it is blue
you are wrong it is red
covert
unit
repair
you are right it is blue
you are wrong it is red
overt
unit
repair
you are right it is blue
sorry
you are wrong it is red
</bodyText>
<tableCaption confidence="0.957297333333333">
Table 2: Different types of self-repairs. The shaded
boxes show which SpeechUnit’s have been realised,
or are about to be realised, at the point of revision.
</tableCaption>
<bodyText confidence="0.996306944444444">
The SpeechSegment property committing
tells whether it needs to be repaired if the
SpeechPlan is revised. For example, a filled
pause such as “eh” is not committing (there is no
need to insert an editing term after it), while a
request or an assertion usually is. If (parts of) a
committing segment has already been realised
and it cannot be part of the new plan, an overt
repaired is made with the help of an editing term
(e.g., “sorry”). When comparing the history with
the new graph, the Vocalizer searches the graph
and tries to find a path so that it may avoid mak-
ing an overt repair. For example if the graph in
Figure 4 is replaced with a corresponding one
that ends with “60 crowns”, and it has so far
partly realised “it costs”, it may choose the cor-
responding path in the new SpeechPlan, making
a covert repair.
</bodyText>
<sectionHeader confidence="0.99578" genericHeader="method">
4 A Wizard-of-Oz experiment
</sectionHeader>
<bodyText confidence="0.986660952380952">
A Wizard-of-Oz experiment was conducted to
test the usefulness of the model outlined above.
All modules in the system were fully functional,
except for the ASR, since not enough data had
been collected to build language models. Thus,
instead of using ASR, the users’ speech was
transcribed by a Wizard. As discussed in section
1.1, a common problem is the time it takes for
the Wizard to transcribe incoming utterances,
and thus for the system to respond. Therefore,
this is an interesting test-case for our model. In
order to let the system respond as soon as the
user finished speaking, even if the Wizard hasn’t
completed the transcription yet, a VAD is used.
The setting is shown in Figure 5 (compare with
Figure 2). The Wizard may start to type as soon
as the user starts to speak and may alter whatever
he has typed until the return key is pressed and
the hypothesis is committed. The word buffer is
updated in exactly the same manner as if it had
been the output of an ASR.
</bodyText>
<figureCaption confidence="0.999015">
Figure 5: The system architecture used in the Wizard-
of-Oz experiment.
</figureCaption>
<bodyText confidence="0.999151">
For comparison, we also configured a non-
incremental version of the same system, where
nothing was sent from the Wizard until he com-
</bodyText>
<figure confidence="0.995494583333333">
Speech
User
ActionManager
Vocalizer
Speech
VAD
Utterance
Segment
Contextualizer
Interpreter
Wizard
Word
</figure>
<page confidence="0.9627">
5
</page>
<bodyText confidence="0.9998558">
mitted by pressing the return key. Since we did
not have mature models for the Interpreter either,
the Wizard was allowed to adapt the transcrip-
tion of the utterances to match the models, while
preserving the semantic content.
</bodyText>
<subsectionHeader confidence="0.983506">
4.1 The DEAL domain
</subsectionHeader>
<bodyText confidence="0.9999786">
The system that was used in the experiment was
a spoken dialogue system for second language
learners of Swedish under development at KTH,
called DEAL (Hjalmarsson et al., 2007). The
scene of DEAL is set at a flea market where a
talking agent is the owner of a shop selling used
goods. The student is given a mission to buy
items at the flea market getting the best possible
price from the shop-keeper. The shop-keeper can
talk about the properties of goods for sale and
negotiate about the price. The price can be re-
duced if the user points out a flaw of an object,
argues that something is too expensive, or offers
lower bids. However, if the user is too persistent
haggling, the agent gets frustrated and closes the
shop. Then the user has failed to complete the
task.
For the experiment, DEAL was re-
implemented using the Jindigo framework. Fig-
ure 6 shows the GUI that was shown to the user.
</bodyText>
<figureCaption confidence="0.92307675">
Figure 6: The user interface in DEAL. The object on
the table is the one currently in focus. Example ob-
jects are shown on the shelf. Current game score,
money and bought objects are shown on the right.
</figureCaption>
<subsectionHeader confidence="0.999208">
4.2 Speech segments in DEAL
</subsectionHeader>
<bodyText confidence="0.970717466666667">
In a previous data collection of human-human
interaction in the DEAL domain (Hjalmarsson,
2008) it was noted that about 40% of the speaker
turns were initiated with standardized lexical ex-
pressions (cue phrases) or filled pauses. Such
speech segments commit very little semantically
to the rest of the utterance and are therefore very
useful as initiations of utterances, since such
speech segments can be produced immediately
after the user has stopped speaking, allowing the
Wizard to exploit the additional time to tran-
scribe the rest of the utterance.
The DEAL corpus was used to create utter-
ance initial speech segments for the experiment.
The motivation to use speech segments derived
from human recordings was to make the system
sound convincing in terms of both lexical choice
and intonation. In particular, we wanted a reper-
toire of different types of filled pauses and feed-
back expression such as “eh” and “mm” in order
to avoid a system that sounds monotone and re-
petitive. First, a number of feedback expression
such as “ja”, “a”, “mm” (Eng: “yes”), filled
pauses such as “eh”, “ehm” and expressions used
to initiate different domain specific speech acts
(for example “it costs” and “let me see”) were
extracted. The segments were re-synthesized
using Expros, a tool for experimentation with
prosody in diphone voices (Gustafson &amp; Edlund,
2008). Based on manual transcriptions and sound
files, Expros automatically extracts pitch, dura-
tion and intensity from the human voice and cre-
ates a synthetic version using these parameters.
In the speech plan, these canned segments were
mixed with generated text segments (for example
references to objects, prices, etc) that were syn-
thesized and generated on-line with the same
diphone voice.
An example interaction with the incremental
version of the system is shown in Table 3. S.11
exemplifies a self-correction, where the system
prepares to present another bid, but then realizes
that the user’s bid is too low to even consider. A
video (with subtitles) showing an interaction
with one of the users can be seen at
http://www.youtube.com/watch?v=cQQmgItIMvs.
S.1 [welcome] [how may I help you]
U.2 I want to buy a doll
S.3 [eh] [here is] [a doll]
U.4 how much is it?
S.5 [eh] [it costs] [120 crowns]
U.6 that is too expensive
how much is the teddy bear?
S.7 [well] [you can have it for] [let’s see]
[40 crowns]
U.8 I can give you 30 crowns
S.9 [you could have it for] [37 crowns]
U.10 I can give you 10 crowns
S.11 [let’s say] [or, I mean] [that is way too
little]
</bodyText>
<tableCaption confidence="0.996677">
Table 3: An example DEAL dialogue (translated from
Swedish). Speech segments are marked in brackets.
</tableCaption>
<page confidence="0.99758">
6
</page>
<subsectionHeader confidence="0.976777">
4.3 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999994466666667">
In order to compare the incremental and non-
incremental versions of the system, we con-
ducted an experiment with 10 participants, 4
male and 6 female. The participants were given a
mission: to buy three items (with certain charac-
teristics) in DEAL at the best possible price from
the shop-keeper. The participants were further
instructed to evaluate two different versions of
the system, System A and System B. However,
they were not informed how the versions dif-
fered. The participants were lead to believe that
they were interacting with a fully working dia-
logue system and were not aware of the Wizard-
of-Oz set up. Each participant interacted with the
system four times, first two times with each ver-
sion of the system, after which a questionnaire
was completed. Then they interacted with the
two versions again, after which they filled out a
second questionnaire with the same questions.
The order of the versions was balanced between
subjects.
The mid-experiment questionnaire was used to
collect the participants’ first opinions of the two
versions and to make them aware of what type of
characteristics they should consider when inter-
acting with the system the second time. When
filling out the second questionnaire, the partici-
pants were asked to base their ratings on their
overall experience with the two system versions.
Thus, the analysis of the results is based on the
second questionnaire. In the questionnaires, they
were requested to rate which one of the two ver-
sions was most prominent according to 8 differ-
ent dimensions: which version they preferred;
which was more human-like, polite, efficient, and
intelligent; which gave a faster response and bet-
ter feedback; and with which version it was eas-
ier to know when to speak. All ratings were done
on a continuous horizontal line with System A on
the left end and System B on the right end. The
centre of the line was labelled with “no differ-
ence”.
The participants were recorded during their in-
teraction with the system, and all messages in the
system were logged.
</bodyText>
<sectionHeader confidence="0.734287" genericHeader="evaluation">
4.4 Results
</sectionHeader>
<bodyText confidence="0.931202933333333">
Figure 7 shows the difference in response time
between the two versions. As expected, the in-
cremental version started to speak more quickly
(M=0.58s, SD=1.20) than the non-incremental
version (M=2.84s, SD=1.17), while producing
longer utterances. It was harder to anticipate
whether it would take more or less time for the
incremental version to finish utterances. Both
versions received the final input at the same
time. On the one hand, the incremental version
initiates utterances with speech segments that
contain little or no semantic information. Thus, if
the system is in the middle of such a segment
when receiving the complete input from the
Wizard, the system may need to complete this
segment before producing the rest of the utter-
ance. Moreover, if an utterance is initiated and
the Wizard alters the input, the incremental ver-
sion needs to make a repair which takes addi-
tional time. On the other hand, it may also start
to produce speech segments that are semantically
relevant, based on the incremental input, which
allows it to finish the utterance more quickly. As
the figure shows, it turns out that the average
response completion time for the incremental
version (M=5.02s, SD=1.54) is about 600ms
faster than the average for non-incremental ver-
sion (M=5.66s, SD=1.50), (t(704)=5.56,
p&lt;0.001).
start end length
</bodyText>
<figureCaption confidence="0.637537">
Figure 7: The first two column pairs show the average
time from the end of the user’s utterance to the start
of the system’s response, and from the end of the
user’s utterance to the end of the system’s response.
The third column pair shows the average total system
utterance length (end minus start).
</figureCaption>
<bodyText confidence="0.999928333333333">
In general, subjects reported that the system
worked very well. After the first interaction with
the two versions, the participants found it hard to
point out the difference, as they were focused on
solving the task. The marks on the horizontal
continuous lines on the questionnaire were
measured with a ruler based on their distance
from the midpoint (labelled with “no difference”)
and normalized to a scale from -1 to 1, each ex-
treme representing one system version. A Wil-
coxon Signed Ranks Test was carried out, using
these rankings as differences. The results are
shown in Table 4. As the table shows, the two
versions differed significantly in three dimen-
sions, all in favour of the incremental version.
</bodyText>
<figure confidence="0.9962361">
Seconds
4,00
0,00
6,00
5,00
3,00
2,00
1,00
inc
non
</figure>
<page confidence="0.997444">
7
</page>
<bodyText confidence="0.9985276">
Hence, the incremental version was rated as
more polite, more efficient, and better at indicat-
ing when to speak.
Oz paradigm, and thereby for practical develop-
ment of dialogue systems in general.
</bodyText>
<table confidence="0.935722222222222">
diff z-value p-value
preferred 0.23 -1.24 0.214
human-like 0.15 -0.76 0.445
polite 0.40 -2.19 0.028*
efficient 0.29 -2.08 0.038*
intelligent 0.11 -0.70 0.484
faster response 0.26 -1.66 0.097
feedback 0.08 -0.84 0.400
when to speak 0.35 -2.38 0.017*
</table>
<tableCaption confidence="0.823690333333333">
Table 4: The results from the second questionnaire.
All differences are positive, meaning that they are in
favour of the incremental version.
</tableCaption>
<bodyText confidence="0.9999794">
A well known phenomena in dialogue is that
of entrainment (or adaptation or alignment), that
is, speakers (in both human-human and human-
computer dialogue) tend to adapt the conversa-
tional behaviour to their interlocutor (e.g., Bell,
2003). In order to examine whether the different
versions affected the user’s behaviour, we ana-
lyzed both the user utterance length and user re-
sponse time, but found no significant differences
between the interactions with the two versions.
</bodyText>
<sectionHeader confidence="0.999216" genericHeader="conclusions">
5 Conclusions &amp; Future work
</sectionHeader>
<bodyText confidence="0.999971153846154">
This paper has presented a first step towards in-
cremental speech generation in dialogue systems.
The results are promising: when there are delays
in the processing of the dialogue, it is possible to
incrementally produce utterances that make the
interaction more efficient and pleasant for the
user.
As this is a first step, there are several ways to
improve the model. First, the edges in the
SpeechPlan could have probabilities, to guide
the path planning. Second, when the user has
finished speaking, it should (in some cases) be
possible to anticipate how long it will take until
the processing is completed and thereby choose a
more optimal path (by taking the length of the
SpeechSegment’s into consideration). Third, a
lot of work could be done on the dynamic gen-
eration of SpeechSegment’s, considering syntac-
tic and pragmatic constraints, although this
would require a speech synthesizer that was bet-
ter at convincingly produce conversational
speech.
The experiment also shows that it is possible
to achieve fast turn-taking and convincing re-
sponses in a Wizard-of-Oz setting. We think that
this opens up new possibilities for the Wizard-of-
</bodyText>
<sectionHeader confidence="0.999045" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.997385">
This research was funded by the Swedish research
council project GENDIAL (VR #2007-6431).
</bodyText>
<sectionHeader confidence="0.999128" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999653863636364">
Bell, L. (2003). Linguistic adaptations in spoken hu-
man-computer dialogues. Empirical studies of user
behavior. Doctoral dissertation, Department of
Speech, Music and Hearing, KTH, Stockholm.
Dohsaka, K., &amp; Shimazu, A. (1997). System architec-
ture for spoken utterance production in collaborative
dialogue. In Working Notes of IJCAI 1997 Work-
shop on Collaboration, Cooperation and Conflict in
Dialogue Systems.
Fraser, N. M., &amp; Gilbert, G. N. (1991). Simulating
speech systems. Computer Speech and Language,
5(1), 81-99.
Gustafson, J., &amp; Edlund, J. (2008). expros: a toolkit
for exploratory experimentation with prosody in
customized diphone voices. In Proceedings of Per-
ception and Interactive Technologies for Speech-
Based Systems (PIT 2008) (pp. 293-296). Ber-
lin/Heidelberg: Springer.
Hjalmarsson, A., Wik, P., &amp; Brusk, J. (2007). Dealing
with DEAL: a dialogue system for conversation
training. In Proceedings of SigDial (pp. 132-135).
Antwerp, Belgium.
Hjalmarsson, A. (2008). Speaking without knowing
what to say... or when to end. In Proceedings of
SIGDial 2008. Columbus, Ohio, USA.
Kempen, G., &amp; Hoenkamp, E. (1987). An incremental
procedural grammar for sentence formulation. Cog-
nitive Science, 11(2), 201-258.
Kilger, A., &amp; Finkler, W. (1995). Incremental Gen-
eration for Real-Time Applications. Technical Re-
port RR-95-11, German Research Center for Artifi-
cial Intelligence.
Levelt, W. J. M. (1989). Speaking: From Intention to
Articulation. Cambridge, Mass., USA: MIT Press.
Raux, A., &amp; Eskenazi, M. (2008). Optimizing end-
pointing thresholds using dialogue features in a
spoken dialogue system. In Proceedings of SIGdial
2008. Columbus, OH, USA.
Schlangen, D., &amp; Skantze, G. (2009). A general, ab-
stract model of incremental dialogue processing. In
Proceedings of EACL-09. Athens, Greece.
Skantze, G., &amp; Schlangen, D. (2009). Incremental
dialogue processing in a micro-domain. In Proceed-
ings of EACL-09. Athens, Greece.
</reference>
<page confidence="0.998487">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.755893">
<title confidence="0.999922">Towards Incremental Speech Generation in Dialogue Systems</title>
<author confidence="0.999493">Gabriel Skantze Anna Hjalmarsson</author>
<affiliation confidence="0.987721">Dept. of Speech Music and Hearing Dept. of Speech Music and Hearing</affiliation>
<address confidence="0.9217">KTH, Stockholm, Sweden KTH, Stockholm, Sweden</address>
<email confidence="0.80323">gabriel@speech.kth.seannah@speech.kth.se</email>
<abstract confidence="0.9997709375">We present a first step towards a model of speech generation for incremental dialogue systems. The model allows a dialogue system to incrementally interpret spoken input, while simultaneously planning, realising and selfmonitoring the system response. The model has been implemented in a general dialogue system framework. Using this framework, we have implemented a specific application and tested it in a Wizard-of-Oz setting, comparing it with a non-incremental version of the same system. The results show that the incremental version, while producing longer utterances, has a shorter response time and is perceived as more efficient by the users.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bell</author>
</authors>
<title>Linguistic adaptations in spoken human-computer dialogues. Empirical studies of user behavior.</title>
<date>2003</date>
<institution>Department of Speech, Music</institution>
<note>Doctoral dissertation,</note>
<contexts>
<context position="29449" citStr="Bell, 2003" startWordPosition="4946" endWordPosition="4947">ue preferred 0.23 -1.24 0.214 human-like 0.15 -0.76 0.445 polite 0.40 -2.19 0.028* efficient 0.29 -2.08 0.038* intelligent 0.11 -0.70 0.484 faster response 0.26 -1.66 0.097 feedback 0.08 -0.84 0.400 when to speak 0.35 -2.38 0.017* Table 4: The results from the second questionnaire. All differences are positive, meaning that they are in favour of the incremental version. A well known phenomena in dialogue is that of entrainment (or adaptation or alignment), that is, speakers (in both human-human and humancomputer dialogue) tend to adapt the conversational behaviour to their interlocutor (e.g., Bell, 2003). In order to examine whether the different versions affected the user’s behaviour, we analyzed both the user utterance length and user response time, but found no significant differences between the interactions with the two versions. 5 Conclusions &amp; Future work This paper has presented a first step towards incremental speech generation in dialogue systems. The results are promising: when there are delays in the processing of the dialogue, it is possible to incrementally produce utterances that make the interaction more efficient and pleasant for the user. As this is a first step, there are s</context>
</contexts>
<marker>Bell, 2003</marker>
<rawString>Bell, L. (2003). Linguistic adaptations in spoken human-computer dialogues. Empirical studies of user behavior. Doctoral dissertation, Department of Speech, Music and Hearing, KTH, Stockholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dohsaka</author>
<author>A Shimazu</author>
</authors>
<title>System architecture for spoken utterance production in collaborative dialogue.</title>
<date>1997</date>
<booktitle>In Working Notes of IJCAI</booktitle>
<contexts>
<context position="6091" citStr="Dohsaka &amp; Shimazu (1997)" startWordPosition="973" endWordPosition="976"> that is important for dialogue systems. In a system with modules operating asynchronously, the dialogue manager cannot know whether the intended output is actually realized, as the user may interrupt the system. Also, the timing of the synthesized speech is important, as the user may give feedback in the middle of a system utterance. Thus, an incremental, asynchronous system somehow needs to self-monitor its own output. From a syntactic perspective, Kempen &amp; Hoenkamp (1987) and Kilger &amp; Finkler (1995) have studied how to syntactically formulate sentences incrementally under time constraints. Dohsaka &amp; Shimazu (1997) describes a system architecture for incremental speech generation. However, there is no account for revision of the input (as discussed in Section 2.1) and there is no evaluation with users. Skantze &amp; Schlangen (2009) describe an incremental system that partly supports incremental output and that is evaluated with users, but the domain is limited to number dictation. In this study, the focus is not on syntactic construction of utterances, but on how to build practical incremental dialogue systems within limited domains that can handle revisions and produce convincing, flexible and varied spee</context>
</contexts>
<marker>Dohsaka, Shimazu, 1997</marker>
<rawString>Dohsaka, K., &amp; Shimazu, A. (1997). System architecture for spoken utterance production in collaborative dialogue. In Working Notes of IJCAI 1997 Workshop on Collaboration, Cooperation and Conflict in Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N M Fraser</author>
<author>G N Gilbert</author>
</authors>
<title>Simulating speech systems.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<volume>5</volume>
<issue>1</issue>
<pages>81--99</pages>
<contexts>
<context position="3583" citStr="Fraser &amp; Gilbert, 1991" startWordPosition="559" endWordPosition="563">or processing. For example, it has been assumed that ASR processing has to be done in real-time, in order to avoid long and confusing response delays. Yet, if we allow the system to start speaking before input is complete, we can allow more accurate (and time-consuming) ASR processing (for example by broadening the beam). In this paper, we will explore incremental speech generation in a Wizard-of-oz setting. A common problem in such settings is the time it takes for the Wizard to interpret the user’s utterance and/or decide on the next system action, resulting in unacceptable response delays (Fraser &amp; Gilbert, 1991). Thus, it would be useful if the system could start to speak as soon as the user has finished speaking, based on the Wizard’s actions so far. Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 1–8, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 1 1.2 Related work Incremental speech generation has been studied from different perspectives. From a psycholinguistic perspective, Levelt (1989) and others have studied how speakers incrementally produce utterances while self-monitoring t</context>
</contexts>
<marker>Fraser, Gilbert, 1991</marker>
<rawString>Fraser, N. M., &amp; Gilbert, G. N. (1991). Simulating speech systems. Computer Speech and Language, 5(1), 81-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gustafson</author>
<author>J Edlund</author>
</authors>
<title>expros: a toolkit for exploratory experimentation with prosody in customized diphone voices.</title>
<date>2008</date>
<booktitle>In Proceedings of Perception and Interactive Technologies for SpeechBased Systems (PIT</booktitle>
<pages>293--296</pages>
<publisher>Berlin/Heidelberg: Springer.</publisher>
<contexts>
<context position="22894" citStr="Gustafson &amp; Edlund, 2008" startWordPosition="3852" endWordPosition="3855">stem sound convincing in terms of both lexical choice and intonation. In particular, we wanted a repertoire of different types of filled pauses and feedback expression such as “eh” and “mm” in order to avoid a system that sounds monotone and repetitive. First, a number of feedback expression such as “ja”, “a”, “mm” (Eng: “yes”), filled pauses such as “eh”, “ehm” and expressions used to initiate different domain specific speech acts (for example “it costs” and “let me see”) were extracted. The segments were re-synthesized using Expros, a tool for experimentation with prosody in diphone voices (Gustafson &amp; Edlund, 2008). Based on manual transcriptions and sound files, Expros automatically extracts pitch, duration and intensity from the human voice and creates a synthetic version using these parameters. In the speech plan, these canned segments were mixed with generated text segments (for example references to objects, prices, etc) that were synthesized and generated on-line with the same diphone voice. An example interaction with the incremental version of the system is shown in Table 3. S.11 exemplifies a self-correction, where the system prepares to present another bid, but then realizes that the user’s bi</context>
</contexts>
<marker>Gustafson, Edlund, 2008</marker>
<rawString>Gustafson, J., &amp; Edlund, J. (2008). expros: a toolkit for exploratory experimentation with prosody in customized diphone voices. In Proceedings of Perception and Interactive Technologies for SpeechBased Systems (PIT 2008) (pp. 293-296). Berlin/Heidelberg: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hjalmarsson</author>
<author>P Wik</author>
<author>J Brusk</author>
</authors>
<title>Dealing with DEAL: a dialogue system for conversation training.</title>
<date>2007</date>
<booktitle>In Proceedings of SigDial</booktitle>
<pages>132--135</pages>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="20590" citStr="Hjalmarsson et al., 2007" startWordPosition="3455" endWordPosition="3458">a nonincremental version of the same system, where nothing was sent from the Wizard until he comSpeech User ActionManager Vocalizer Speech VAD Utterance Segment Contextualizer Interpreter Wizard Word 5 mitted by pressing the return key. Since we did not have mature models for the Interpreter either, the Wizard was allowed to adapt the transcription of the utterances to match the models, while preserving the semantic content. 4.1 The DEAL domain The system that was used in the experiment was a spoken dialogue system for second language learners of Swedish under development at KTH, called DEAL (Hjalmarsson et al., 2007). The scene of DEAL is set at a flea market where a talking agent is the owner of a shop selling used goods. The student is given a mission to buy items at the flea market getting the best possible price from the shop-keeper. The shop-keeper can talk about the properties of goods for sale and negotiate about the price. The price can be reduced if the user points out a flaw of an object, argues that something is too expensive, or offers lower bids. However, if the user is too persistent haggling, the agent gets frustrated and closes the shop. Then the user has failed to complete the task. For t</context>
</contexts>
<marker>Hjalmarsson, Wik, Brusk, 2007</marker>
<rawString>Hjalmarsson, A., Wik, P., &amp; Brusk, J. (2007). Dealing with DEAL: a dialogue system for conversation training. In Proceedings of SigDial (pp. 132-135). Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hjalmarsson</author>
</authors>
<title>Speaking without knowing what to say... or when to end.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGDial 2008.</booktitle>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="21633" citStr="Hjalmarsson, 2008" startWordPosition="3648" endWordPosition="3649">ive, or offers lower bids. However, if the user is too persistent haggling, the agent gets frustrated and closes the shop. Then the user has failed to complete the task. For the experiment, DEAL was reimplemented using the Jindigo framework. Figure 6 shows the GUI that was shown to the user. Figure 6: The user interface in DEAL. The object on the table is the one currently in focus. Example objects are shown on the shelf. Current game score, money and bought objects are shown on the right. 4.2 Speech segments in DEAL In a previous data collection of human-human interaction in the DEAL domain (Hjalmarsson, 2008) it was noted that about 40% of the speaker turns were initiated with standardized lexical expressions (cue phrases) or filled pauses. Such speech segments commit very little semantically to the rest of the utterance and are therefore very useful as initiations of utterances, since such speech segments can be produced immediately after the user has stopped speaking, allowing the Wizard to exploit the additional time to transcribe the rest of the utterance. The DEAL corpus was used to create utterance initial speech segments for the experiment. The motivation to use speech segments derived from</context>
</contexts>
<marker>Hjalmarsson, 2008</marker>
<rawString>Hjalmarsson, A. (2008). Speaking without knowing what to say... or when to end. In Proceedings of SIGDial 2008. Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kempen</author>
<author>E Hoenkamp</author>
</authors>
<title>An incremental procedural grammar for sentence formulation.</title>
<date>1987</date>
<journal>Cognitive Science,</journal>
<volume>11</volume>
<issue>2</issue>
<pages>201--258</pages>
<contexts>
<context position="5946" citStr="Kempen &amp; Hoenkamp (1987)" startWordPosition="952" endWordPosition="955">s always a risk for revision in the input to an incremental module (as described in Section 2.1). There is also another aspect of self-monitoring that is important for dialogue systems. In a system with modules operating asynchronously, the dialogue manager cannot know whether the intended output is actually realized, as the user may interrupt the system. Also, the timing of the synthesized speech is important, as the user may give feedback in the middle of a system utterance. Thus, an incremental, asynchronous system somehow needs to self-monitor its own output. From a syntactic perspective, Kempen &amp; Hoenkamp (1987) and Kilger &amp; Finkler (1995) have studied how to syntactically formulate sentences incrementally under time constraints. Dohsaka &amp; Shimazu (1997) describes a system architecture for incremental speech generation. However, there is no account for revision of the input (as discussed in Section 2.1) and there is no evaluation with users. Skantze &amp; Schlangen (2009) describe an incremental system that partly supports incremental output and that is evaluated with users, but the domain is limited to number dictation. In this study, the focus is not on syntactic construction of utterances, but on how </context>
</contexts>
<marker>Kempen, Hoenkamp, 1987</marker>
<rawString>Kempen, G., &amp; Hoenkamp, E. (1987). An incremental procedural grammar for sentence formulation. Cognitive Science, 11(2), 201-258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilger</author>
<author>W Finkler</author>
</authors>
<title>Incremental Generation for Real-Time Applications.</title>
<date>1995</date>
<tech>Technical Report RR-95-11,</tech>
<institution>German Research Center for Artificial Intelligence.</institution>
<contexts>
<context position="5974" citStr="Kilger &amp; Finkler (1995)" startWordPosition="957" endWordPosition="960">in the input to an incremental module (as described in Section 2.1). There is also another aspect of self-monitoring that is important for dialogue systems. In a system with modules operating asynchronously, the dialogue manager cannot know whether the intended output is actually realized, as the user may interrupt the system. Also, the timing of the synthesized speech is important, as the user may give feedback in the middle of a system utterance. Thus, an incremental, asynchronous system somehow needs to self-monitor its own output. From a syntactic perspective, Kempen &amp; Hoenkamp (1987) and Kilger &amp; Finkler (1995) have studied how to syntactically formulate sentences incrementally under time constraints. Dohsaka &amp; Shimazu (1997) describes a system architecture for incremental speech generation. However, there is no account for revision of the input (as discussed in Section 2.1) and there is no evaluation with users. Skantze &amp; Schlangen (2009) describe an incremental system that partly supports incremental output and that is evaluated with users, but the domain is limited to number dictation. In this study, the focus is not on syntactic construction of utterances, but on how to build practical increment</context>
</contexts>
<marker>Kilger, Finkler, 1995</marker>
<rawString>Kilger, A., &amp; Finkler, W. (1995). Incremental Generation for Real-Time Applications. Technical Report RR-95-11, German Research Center for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J M Levelt</author>
</authors>
<title>Speaking: From Intention to Articulation.</title>
<date>1989</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass., USA:</location>
<contexts>
<context position="1435" citStr="Levelt, 1989" startWordPosition="209" endWordPosition="210">erances, has a shorter response time and is perceived as more efficient by the users. 1 Introduction Speakers in dialogue produce speech in a piecemeal fashion and on-line as the dialogue progresses. When starting to speak, dialogue participants typically do not have a complete plan of how to say something or even what to say. Yet, they manage to rapidly integrate information from different sources in parallel and simultaneously plan and realize new dialogue contributions. Moreover, interlocutors continuously selfmonitor the actual production processes in order to facilitate self-corrections (Levelt, 1989). Contrary to this, most spoken dialogue systems use a silence threshold to determine when the user has stopped speaking. The user utterance is then processed by one module at a time, after which a complete system utterance is produced and realised by a speech synthesizer. This paper has two purposes. First, to present an initial step towards a model of speech generation that allows a dialogue system to incrementally interpret spoken input, while simultaneously planning, realising and self-monitoring the system response. The model has been implemented in a general dialogue system framework. Th</context>
<context position="4089" citStr="Levelt (1989)" startWordPosition="641" endWordPosition="642">and/or decide on the next system action, resulting in unacceptable response delays (Fraser &amp; Gilbert, 1991). Thus, it would be useful if the system could start to speak as soon as the user has finished speaking, based on the Wizard’s actions so far. Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 1–8, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 1 1.2 Related work Incremental speech generation has been studied from different perspectives. From a psycholinguistic perspective, Levelt (1989) and others have studied how speakers incrementally produce utterances while self-monitoring the output, both overtly (listening to oneself speaking) and covertly (mentally monitoring what is about to be said). As deviations from the desired output is detected, the speaker may initiate self-repairs. If the item to be repaired has already been spoken, an overt repair is needed (for example by using an editing term, such as “sorry”). If not, the utterance plan may be altered to accommodate the repair, a so-called covert repair. Central to the concept of incremental speech generation is that the </context>
</contexts>
<marker>Levelt, 1989</marker>
<rawString>Levelt, W. J. M. (1989). Speaking: From Intention to Articulation. Cambridge, Mass., USA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>M Eskenazi</author>
</authors>
<title>Optimizing endpointing thresholds using dialogue features in a spoken dialogue system.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGdial 2008.</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="16174" citStr="Raux &amp; Eskenazi (2008)" startWordPosition="2661" endWordPosition="2664">ntation of the segment (a Concept). This is sent by the Vocalizer to the Contextualizer as soon as the segment has been realised. The SpeechSegment properties selfDelay and otherDelay regulate the timing of the output (as illustrated in Figure 3). They specify the number of milliseconds that should pass before the Vocalizer starts to play the segment, depending on the previous speaker. By setting the otherDelay of a segment, the Action Manager may delay the response depending on how certain it is that it is appropriate to speak, for example by considering pitch and semantic completeness. (See Raux &amp; Eskenazi (2008) for a study w1 how well eh w2 much s1 w3 is w4 the w5 doll w6 you can have it for let’s say it costs s3 40 crowns s6 4 on how such dynamic delays can be derived using machine learning.) If the user starts to speak (i.e., a new UtteranceSegment is initiated) as the system is speaking, the Vocalizer pauses (at a SpeechUnit boundary) and waits until it has received a new response from the Action Manager. The Action Manager may then choose to generate a new response or simply ignore the last input, in which case the Vocalizer continues from the point of interruption. This may happen if, for examp</context>
</contexts>
<marker>Raux, Eskenazi, 2008</marker>
<rawString>Raux, A., &amp; Eskenazi, M. (2008). Optimizing endpointing thresholds using dialogue features in a spoken dialogue system. In Proceedings of SIGdial 2008. Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schlangen</author>
<author>G Skantze</author>
</authors>
<title>A general, abstract model of incremental dialogue processing.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL-09.</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="7099" citStr="Schlangen &amp; Skantze (2009)" startWordPosition="1127" endWordPosition="1130">dy, the focus is not on syntactic construction of utterances, but on how to build practical incremental dialogue systems within limited domains that can handle revisions and produce convincing, flexible and varied speech output in on-line interaction with users. 2 The Jindigo framework The proposed model has been implemented in Jindigo – a Java-based open source framework for implementing and experimenting with incremental dialogue systems (www.jindigo.net). We will here briefly describe this framework and the model of incremental dialogue processing that it is based on. 2.1 Incremental units Schlangen &amp; Skantze (2009) describes a general, abstract model of incremental dialogue processing, which Jindigo is based on. In this model, a system consists of a network of processing modules. Each module has a left buffer, a processor, and a right buffer, where the normal mode of processing is to receive input from the left buffer, process it, and provide output in the right buffer, from where it is forwarded to the next module’s left buffer. An example is shown in Figure 1. Modules exchange incremental units (IUs), which are the smallest ‘chunks’ of information that can trigger connected modules into action (such a</context>
</contexts>
<marker>Schlangen, Skantze, 2009</marker>
<rawString>Schlangen, D., &amp; Skantze, G. (2009). A general, abstract model of incremental dialogue processing. In Proceedings of EACL-09. Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Skantze</author>
<author>D Schlangen</author>
</authors>
<title>Incremental dialogue processing in a micro-domain.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL-09.</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="6309" citStr="Skantze &amp; Schlangen (2009)" startWordPosition="1007" endWordPosition="1010">o, the timing of the synthesized speech is important, as the user may give feedback in the middle of a system utterance. Thus, an incremental, asynchronous system somehow needs to self-monitor its own output. From a syntactic perspective, Kempen &amp; Hoenkamp (1987) and Kilger &amp; Finkler (1995) have studied how to syntactically formulate sentences incrementally under time constraints. Dohsaka &amp; Shimazu (1997) describes a system architecture for incremental speech generation. However, there is no account for revision of the input (as discussed in Section 2.1) and there is no evaluation with users. Skantze &amp; Schlangen (2009) describe an incremental system that partly supports incremental output and that is evaluated with users, but the domain is limited to number dictation. In this study, the focus is not on syntactic construction of utterances, but on how to build practical incremental dialogue systems within limited domains that can handle revisions and produce convincing, flexible and varied speech output in on-line interaction with users. 2 The Jindigo framework The proposed model has been implemented in Jindigo – a Java-based open source framework for implementing and experimenting with incremental dialogue </context>
</contexts>
<marker>Skantze, Schlangen, 2009</marker>
<rawString>Skantze, G., &amp; Schlangen, D. (2009). Incremental dialogue processing in a micro-domain. In Proceedings of EACL-09. Athens, Greece.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>