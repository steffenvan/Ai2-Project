<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000249">
<title confidence="0.818747">
Tree-based Analysis of Simple Recurrent Network Learning
</title>
<author confidence="0.395248">
Ivelin Stoianov
</author>
<note confidence="0.747785333333333">
Dept. Alfa-Informatica, Faculty of Arts, Groningen University, POBox 716, 9700 AS Groningen,
The Netherlands, Email:stoianovelet.rug.n1
1 Simple recurrent networks for natural
</note>
<subsectionHeader confidence="0.922387">
language phonotactics analysis.
</subsectionHeader>
<bodyText confidence="0.997057464285715">
In searching for a cormectionist paradigm capable of
natural language processing, many researchers have
explored the Simple Recurrent Network (SRN) such
as Elman(1990), Cleermance(1993), Reilly(1995)
and Lavvrence(1996). SRNs have a context layer
that keeps track of the past hidden neuron
activations and enables them to deal with sequential
data. The events in Natural Language span time so
SRNs are needed to deal with them.
Among the various levels of language proce-
ssing, a phonological level can be distinguished. The
Phonology deals with phonemes or graphemes — the
latter in the case when one works with orthographic
word representations. The principles governing the
combinations of these symbols is called phonotactics
(Laver&apos;1994). It is a good starting point for
cormectionist language analysis because there are
not too many basic entities. The number of the
symbols varies between 26 (for the Latin
graphemes) and 50 &apos;(for the phonemes).
Recently, some experiments considering
phonotactics modelling with SRNs have been carried
out by Stoianov(1997), Rodd(1997). The neural
network in Stoianov(1997) was trained to study the
phonotactics of a large Dutch word corpus. This
problem was implemented as an SRN learning task —
to predict the symbol following the left context given
to the input layer so far. Words were applied to the
network, symbol by symbol, which in turn were
encoded orthogonally, that is, one node standing for
one symbol (Fig.1). An extra symbol (&apos;#&apos;) was used
as a delimiter. After the training, the network
responded to the input with different neuron
activations at the output layer. The more active a
given output neuron is, the higher the probability is
that it is a successor. The authors used a so-called
optimal threshold method for establishing the
threshold which determines the possible successors.
This method was based on examining the network
for Dutch, and up to at most 100 in other languages.
response to a test corpus of words belonging to the
trained language and a random corpus, built up from
random strings. Two error functions dependent on a
threshold were computed, for the test and the
random corpora, respectively. The threshold at
which both errors had minimal value was selected as
an optimal threshold. Using this approach, an SRN,
trained to the phonotactics of a Dutch monosyllabic
corpus containing 4500 words, was reported to
distinguish words from non-words with 7% error.
Since the phonotactics of a given language is
represented by the constraints allowing a given
sequence to be a word or not, and the SRN managed
to distinguish words from random strings with
tolerable error, the authors claim that SRNs are able
to learn the phonotactics of Dutch language.
</bodyText>
<sectionHeader confidence="0.4667985" genericHeader="method">
OutnutUver: 27
NIEI MNIOIRIK
</sectionHeader>
<bodyText confidence="0.999795294117647">
Fig.!. SRN and mechanism of sequence
processing. A character is provided to the input
and the next one is used for training. In turn, it
has to be predicted during the test phase.
In the present report, alternative evaluation
procedures are proposed. The network evaluation
methods introduced are based on examining the
network response to each left context, available in
the training corpus. An effective way to represent
and use the complete set of context strings is a tree-
based data structure. Therefore, these methods are
termed tree-based analysis. Two possible
approaches are proposed for measuring the SRN
response accuracy to each left context. The first uses
the idea mentioned above of searching a threshold
that distinguishes permitted successors from
impossible ones. An error as a function of the
</bodyText>
<figure confidence="0.4252295">
SRN
bum Laver: 27
</figure>
<page confidence="0.973739">
1502
</page>
<bodyText confidence="0.989969317073171">
threshold is computed. Its minimum value
corresponds to the SRN learning error rate. The
second approach computes the local proximity
between the network response and a vector
containing the empirical symbol probabilities that a
given symbol would follow the current left context.
Two measures are used: L2 norm and normalised
vector multiplication. The mean of these local
proximities measures how close the network
responses are to the desired responses.
2 Tree-based corpus representation.
There are diverse methods to represent a given set of
words (corpus). Lists is the simplest, but they are
not optimal with regard to the memory complexity
and the time complexity of the operations working
with the data. A more effective method is the tree-
based representation. Each node in this tree has a
maximum of 26 possible children (successors), if we
work with orthographic word representations. The
root is empty, it does not represent a symbol. It is
the beginning of a word. The leaves do not have
successors and they always represent the end of a
word. A word can end somewhere between the root
and the leaves as well. This manner of corpus
representation, termed trie, is one of the most
compact representations and is very effective for
different operations with words from the corpus.
In addition to the symbol at each node, we can
keep additional information, for example the
frequency of a word, if this node is the end of a
word. Another useful piece of information is the
frequency of each node C, that is, the frequency of
each left context. It is computed recursively as a
sum of the frequencies of all successors and the
frequency of the word ending at this node, provided
that such a word exists. These frequencies give us an
instant evaluation of the empirical distribution for
each successor. In order to compute the successors&apos;
empirical distribution vector r(.), we have to
normalise the successors&apos; frequencies with respect to
their sum.
</bodyText>
<sectionHeader confidence="0.990292" genericHeader="method">
3 Tree-based evaluation of SRN learning.
</sectionHeader>
<bodyText confidence="0.999957333333333">
During the training of a word, only one output
neuron is forced to be active in response to the
context presented so far. But usually, in the entire
corpus there are several successors following a given
context. Therefore, the training should result in
output neurons, reproducing the successors&apos;
probability distribution. Following this reasoning,
we can derive a test procedure that verifies whether
the SRN output activations correspond to these local
distributions. Another approach related to the
practical implementation of a trained SRN is to
search for a cue, giving an answer to the question
whether given symbol can follow the context
provided to the input layer so far. As in the optimal
threshold method we can search for a threshold that
distinguishes these neurons.
The tree-based learning examination methods
are recursive procedures that process each tree node,
performing an in-order (or depth-first) tree
traversal. This kind of traversal algorithms start
from the root and process each sub-tree completely.
At each node, a comparison between the SRNs
reaction to the input, and the empirical characters
distribution is made. Apart from this evaluation, the
SRN state, that is, the context layer, has to be kept
before moving to one of the sub-trees, in order for it
to be reused after traversing this sub-tree.
On the basis of above ideas, two methods for
network evaluation are performed at each tree node
C. The first one computes an error function f(t)
dependent on a threshold t. This function gives the
error rate for each threshold t, that is, the ratio of
erroneous predictions given I The values of r(t) are
high for close to zero and close to one thresholds,
since almost all neurons would permit the
correspondent symbols to be successors in the first
case, and would not allow any successor in the
second case. The minimum will occur somewhere in
the middle, where only a few neurons would have an
activation higher than this threshold. The training
adjusts the weights of the network so that only
neurons corresponding to actual successors are
active. The SRN evaluation is - based on the mean
F(t) of these local error functions (Fig.2a).
The second evaluation method computes the
proximity DC = I WO ,To I between the network
response WO and the local empirical distributions
vector r(.) at each tree node. The final evaluation
of the SRN training is the mean D of IY for all tree
nodes. Two measures are used to compute D. The
first one is 42 norm (1):
</bodyText>
<sectionHeader confidence="0.408193" genericHeader="method">
(1) I Nc(-) ,Tc(.) IL2 = ET.I-M (NC(X)-TC(X))21112
</sectionHeader>
<page confidence="0.942416">
1503
</page>
<bodyText confidence="0.9295832">
The second is a vector multiplication, normali-
sed with respect to the vector&apos;s length (cosine) (2):
(2)INc0 ,Tc(-) I v .(INcOliTc01/11.1-ki (Nc(x)Tc(x))
where hi is the vector size, that is, the number of
possible successors (e.g. 27) (see Fig. 2b).
</bodyText>
<sectionHeader confidence="0.985811" genericHeader="method">
4 Results,
</sectionHeader>
<bodyText confidence="0.997361666666667">
Well-trained SRNs were examined with both the
optimal threshold method and the tree-based
approaches. A network with 30 hidden neurons
predicted about 11% of the characters erroneously.
The same network had mean L2 distance 0.056 and
mean vector-multiplication proximity 0.851. At the
same time, the optimal threshold method rated the
learning at 7% error. Not surprisingly, the tree-
based evaluations methods gave higher error rate —
they do not examine the SRN response to non-
existent left contexts, which in turn are used in the
optimal threshold method.
</bodyText>
<sectionHeader confidence="0.8438" genericHeader="conclusions">
Discussion and conclusions.
</sectionHeader>
<bodyText confidence="0.999680571428572">
Alternative evaluation methods for SRN learning are
proposed. They examine the network response only
to the training input data, which in turn is
represented in a tree-based structure. In contrast,
previous methods examined trained SRNs with test
and random corpora. Both methods give a good idea
about the learning attained. Methods used previously
estimate the SRN recognition capabilities, while the
methods presented here evaluate how close the
network response is to the desired response — but for
familiar input sequences. The desired response is
considered to be the successors&apos; empirical
probability distribution. Hence, one of the methods
proposed compares the local empirical probabilities
</bodyText>
<figure confidence="0.876457333333333">
30
25
20
(01.6
10
5
</figure>
<bodyText confidence="0.999884333333333">
to the network response. The other approach
searches for a threshold that minimises the
prediction error function. The proposed methods
have been employed in the evaluation of
phonotactics learning, but they can be used in
various other tasks as well, wherever the data can be
organised hierarchically. I hope, that the proposed
analysis will contribute to our understanding of
learning carried out in SRNs.
</bodyText>
<sectionHeader confidence="0.997384" genericHeader="references">
References.
</sectionHeader>
<reference confidence="0.985134666666667">
Cleeremans, Axel (1993). Mechanisms of Implicit
Learning.MIT Press.
Elman, J.L. (1990). Finding structure in time. Cognitive
Science, 14, pp.179-211.
Elman, J.L., et al. (1996). Rethinking Innates. A
Bradford Book, The Mit Press.
Haykin, Simon. (1994). Neural Networks, Macmillan
College Publisher.
Laver,John.(1994).Principles of phonetics,Cambr.Un.Pr.
Lawrence, S., et al.(1996).NL Gramatical Inference A
Comparison of RNN and ML Methods. Con-
nectionist. statistical and symbolic approaches to
learning for NLP, Springer-Verlag,pp.33-47
Nerbonne, John, et al (1996). Phonetic Distance between
Dutch Dialects. In G.Dureux, W.Daelle-mans &amp;
S.Gillis(eds) Proc.of CLIN, pp.185-202
Reilly, Ronan G.(1995).Sandy Ideas and Coloured Days:
Some Computational Implications of Embodiment.
Art Intellig. Review,9: 305-322.,Kluver Ac. Publ.,NL.
Rodd, Jenifer. (1997). Recurrent Neural-Network
Learning of Phonological Regula-ri ties in Turkish,
ACL&apos;97 Workshop: Computational Natural language
learning, pp. 97-106.
Stoianov, I.P., John Nerbonne and Huub Houma (1997).
Modelling the phonotactic structure of natural
language words with Simple Recurrent Networks,
Proc. of 7-th CLIN&apos;97 (in press)
</reference>
<figure confidence="0.9963096">
0.46 •
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
00 0.1
0
0 2 4 6 TFreiaold12 14
litrEn-(0.004)41.17.1
(b)
16 16 20
</figure>
<figureCaption confidence="0.968132">
Fig.2. SRN evaluation by: (a.) minimising the error function F(t). (b.) measuring the SRN matching to the
empirical successor distributions. The distributions of L2 distance and cosine are given (see the text).
</figureCaption>
<page confidence="0.6758655">
0.2
0.3
</page>
<figure confidence="0.74615525">
0.4 0.5 0.6
Diatom
0.7
0.9 0.9 1
</figure>
<page confidence="0.97561">
1504
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000598">
<title confidence="0.990118">Tree-based Analysis of Simple Recurrent Network Learning</title>
<author confidence="0.766201">Ivelin Stoianov</author>
<affiliation confidence="0.916375">Dept. Alfa-Informatica, Faculty of Arts, Groningen University, POBox 716, 9700 AS Groningen,</affiliation>
<address confidence="0.959831">The Netherlands, Email:stoianovelet.rug.n1</address>
<abstract confidence="0.991943246636771">1 Simple recurrent networks for natural language phonotactics analysis. In searching for a cormectionist paradigm capable of natural language processing, many researchers have explored the Simple Recurrent Network (SRN) such as Elman(1990), Cleermance(1993), Reilly(1995) and Lavvrence(1996). SRNs have a context layer that keeps track of the past hidden neuron activations and enables them to deal with sequential data. The events in Natural Language span time so SRNs are needed to deal with them. Among the various levels of language procea phonological level can The Phonology deals with phonemes or graphemes — the latter in the case when one works with orthographic word representations. The principles governing the combinations of these symbols is called phonotactics (Laver&apos;1994). It is a good starting point for cormectionist language analysis because there are not too many basic entities. The number of the symbols varies between 26 (for the Latin graphemes) and 50 &apos;(for the phonemes). Recently, some experiments considering phonotactics modelling with SRNs have been carried out by Stoianov(1997), Rodd(1997). The neural network in Stoianov(1997) was trained to study the phonotactics of a large Dutch word corpus. This problem was implemented as an SRN learning task — to predict the symbol following the left context given to the input layer so far. Words were applied to the network, symbol by symbol, which in turn were encoded orthogonally, that is, one node standing for one symbol (Fig.1). An extra symbol (&apos;#&apos;) was used as a delimiter. After the training, the network responded to the input with different neuron activations at the output layer. The more active a given output neuron is, the higher the probability is that it is a successor. The authors used a so-called threshold method establishing the threshold which determines the possible successors. This method was based on examining the network for Dutch, and up to at most 100 in other languages. response to a test corpus of words belonging to the trained language and a random corpus, built up from random strings. Two error functions dependent on a were computed, for the the random corpora, respectively. The threshold at which both errors had minimal value was selected as an optimal threshold. Using this approach, an SRN, trained to the phonotactics of a Dutch monosyllabic corpus containing 4500 words, was reported to distinguish words from non-words with 7% error. Since the phonotactics of a given language is represented by the constraints allowing a given to word or not, and the SRN managed to distinguish words from random strings with error, the authors claim that able to learn the phonotactics of Dutch language. OutnutUver: 27 NIEI MNIOIRIK Fig.!. SRN and mechanism of sequence processing. A character is provided to the input and the next one is used for training. In turn, it has to be predicted during the test phase. In the present report, alternative evaluation procedures are proposed. The network evaluation methods introduced are based on examining the network response to each left context, available in the training corpus. An effective way to represent and use the complete set of context strings is a treebased data structure. Therefore, these methods are analysis. possible approaches are proposed for measuring the SRN response accuracy to each left context. The first uses the idea mentioned above of searching a threshold that distinguishes permitted successors from impossible ones. An error as a function of the SRN bum Laver: 27 1502 threshold is computed. Its minimum value corresponds to the SRN learning error rate. The second approach computes the local proximity between the network response and a vector containing the empirical symbol probabilities that a given symbol would follow the current left context. Two measures are used: L2 norm and normalised vector multiplication. The mean of these local proximities measures how close the network responses are to the desired responses. 2 Tree-based corpus representation. There are diverse methods to represent a given set of words (corpus). Lists is the simplest, but they are not optimal with regard to the memory complexity and the time complexity of the operations working with the data. A more effective method is the treebased representation. Each node in this tree has a maximum of 26 possible children (successors), if we work with orthographic word representations. The root is empty, it does not represent a symbol. It is the beginning of a word. The leaves do not have successors and they always represent the end of a word. A word can end somewhere between the root and the leaves as well. This manner of corpus termed one of the most compact representations and is very effective for different operations with words from the corpus. In addition to the symbol at each node, we can keep additional information, for example the frequency of a word, if this node is the end of a word. Another useful piece of information is the frequency of each node C, that is, the frequency of each left context. It is computed recursively as a sum of the frequencies of all successors and the frequency of the word ending at this node, provided that such a word exists. These frequencies give us an instant evaluation of the empirical distribution for each successor. In order to compute the successors&apos; empirical distribution vector r(.), we have to normalise the successors&apos; frequencies with respect to their sum. 3 Tree-based evaluation of SRN learning. During the training of a word, only one output neuron is forced to be active in response to the context presented so far. But usually, in the entire corpus there are several successors following a given context. Therefore, the training should result in output neurons, reproducing the successors&apos; probability distribution. Following this reasoning, we can derive a test procedure that verifies whether the SRN output activations correspond to these local distributions. Another approach related to the practical implementation of a trained SRN is to search for a cue, giving an answer to the question whether given symbol can follow the context to the input layer so far. As in the method can search for a threshold that distinguishes these neurons. The tree-based learning examination methods are recursive procedures that process each tree node, an traversal. This kind of traversal algorithms start from the root and process each sub-tree completely. a comparison between the SRNs reaction to the input, and the empirical characters distribution is made. Apart from this evaluation, the SRN state, that is, the context layer, has to be kept before moving to one of the sub-trees, in order for it to be reused after traversing this sub-tree. On the basis of above ideas, two methods for network evaluation are performed at each tree node C. The first one computes an error function f(t) dependent on a threshold t. This function gives the error rate for each threshold t, that is, the ratio of erroneous predictions given I The values of r(t) are high for close to zero and close to one thresholds, since almost all neurons would permit the correspondent symbols to be successors in the first case, and would not allow any successor in the second case. The minimum will occur somewhere in the middle, where only a few neurons would have an activation higher than this threshold. The training adjusts the weights of the network so that only neurons corresponding to actual successors are active. The SRN evaluation is based on the mean F(t) of these local error functions (Fig.2a). The second evaluation method computes the DC = I WO between the network response WO and the local empirical distributions vector r(.) at each tree node. The final evaluation of the SRN training is the mean D of IY for all tree Two measures are used to compute D.The first one is 42 norm (1): I Nc(-) ,Tc(.)= 1503 The second is a vector multiplication, normalised with respect to the vector&apos;s length (cosine) (2): I where hi is the vector size, that is, the number of possible successors (e.g. 27) (see Fig. 2b). 4 Results, Well-trained SRNs were examined with both the threshold method the network with 30 hidden neurons predicted about 11% of the characters erroneously. same network had mean 0.056 and mean vector-multiplication proximity 0.851. At the time, the threshold method the learning at 7% error. Not surprisingly, the treebased evaluations methods gave higher error rate — do not examine the to nonexistent left contexts, which in turn are used in the optimal threshold method. Discussion and conclusions. Alternative evaluation methods for SRN learning are proposed. They examine the network response only to the training input data, which in turn is represented in a tree-based structure. In contrast, previous methods examined trained SRNs with test and random corpora. Both methods give a good idea about the learning attained. Methods used previously the capabilities, while the methods presented here evaluate how close the network response is to the desired response — but for familiar input sequences. The desired response is considered to be the successors&apos; empirical probability distribution. Hence, one of the methods proposed compares the local empirical probabilities 30 25 20 10 5 to the network response. The other approach searches for a threshold that minimises the prediction error function. The proposed methods have been employed in the evaluation of phonotactics learning, but they can be used in other tasks as well, wherever the data can organised hierarchically. I hope, that the proposed analysis will contribute to our understanding of carried out in References.</abstract>
<note confidence="0.801412">Axel (1993). of Implicit J.L. (1990). Finding structure in time. pp.179-211. J.L., et al. (1996). Innates. Bradford Book, The Mit Press. Simon. (1994). Networks, College Publisher. phonetics,Cambr.Un.Pr. Lawrence, S., et al.(1996).NL Gramatical Inference A of RNN and ML Methods. Conand symbolic approaches to for Nerbonne, John, et al (1996). Phonetic Distance between Dutch Dialects. In G.Dureux, W.Daelle-mans &amp; CLIN, pp.185-202 Reilly, Ronan G.(1995).Sandy Ideas and Coloured Days: Some Computational Implications of Embodiment. Intellig. Review,9: Ac. Publ.,NL. Rodd, Jenifer. (1997). Recurrent Neural-Network Learning of Phonological Regula-ri ties in Turkish, Workshop: Natural language 97-106. Stoianov, I.P., John Nerbonne and Huub Houma (1997).</note>
<abstract confidence="0.642171333333333">Modelling the phonotactic structure of natural language words with Simple Recurrent Networks, of press) 0.46 • 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 2 614 litrEn-(0.004)41.17.1 (b) 16 16 20 by: (a.) minimising the error function measuring the SRN matching to the successor distributions. The distributions of and cosine are given (see the text). 0.3 0.4 0.5 0.6 Diatom 0.7 0.9 0.9 1</abstract>
<date confidence="0.55405">1504</date>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Axel Cleeremans</author>
</authors>
<title>Mechanisms of Implicit</title>
<date>1993</date>
<publisher>Learning.MIT Press.</publisher>
<marker>Cleeremans, 1993</marker>
<rawString>Cleeremans, Axel (1993). Mechanisms of Implicit Learning.MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive Science,</journal>
<volume>14</volume>
<pages>179--211</pages>
<marker>Elman, 1990</marker>
<rawString>Elman, J.L. (1990). Finding structure in time. Cognitive Science, 14, pp.179-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Rethinking Innates.</title>
<date>1996</date>
<publisher>A Bradford Book, The Mit Press.</publisher>
<marker>Elman, 1996</marker>
<rawString>Elman, J.L., et al. (1996). Rethinking Innates. A Bradford Book, The Mit Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Haykin</author>
</authors>
<title>Neural Networks, Macmillan College Publisher. Laver,John.(1994).Principles of phonetics,Cambr.Un.Pr.</title>
<date>1994</date>
<marker>Haykin, 1994</marker>
<rawString>Haykin, Simon. (1994). Neural Networks, Macmillan College Publisher. Laver,John.(1994).Principles of phonetics,Cambr.Un.Pr.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Lawrence</author>
</authors>
<title>et al.(1996).NL Gramatical Inference A Comparison of RNN and ML Methods. Connectionist. statistical and symbolic approaches to learning for NLP,</title>
<pages>33--47</pages>
<marker>Lawrence, </marker>
<rawString>Lawrence, S., et al.(1996).NL Gramatical Inference A Comparison of RNN and ML Methods. Connectionist. statistical and symbolic approaches to learning for NLP, Springer-Verlag,pp.33-47</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Nerbonne</author>
</authors>
<title>Phonetic Distance between Dutch Dialects.</title>
<date>1996</date>
<booktitle>In G.Dureux, W.Daelle-mans &amp; S.Gillis(eds) Proc.of CLIN,</booktitle>
<pages>185--202</pages>
<marker>Nerbonne, 1996</marker>
<rawString>Nerbonne, John, et al (1996). Phonetic Distance between Dutch Dialects. In G.Dureux, W.Daelle-mans &amp; S.Gillis(eds) Proc.of CLIN, pp.185-202</rawString>
</citation>
<citation valid="false">
<authors>
<author>Reilly</author>
</authors>
<title>Ronan G.(1995).Sandy Ideas and Coloured Days: Some Computational Implications of Embodiment. Art Intellig. Review,9: 305-322.,Kluver Ac.</title>
<publisher>Publ.,NL.</publisher>
<marker>Reilly, </marker>
<rawString>Reilly, Ronan G.(1995).Sandy Ideas and Coloured Days: Some Computational Implications of Embodiment. Art Intellig. Review,9: 305-322.,Kluver Ac. Publ.,NL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenifer Rodd</author>
</authors>
<title>Recurrent Neural-Network Learning of Phonological Regula-ri ties in Turkish,</title>
<date>1997</date>
<booktitle>ACL&apos;97 Workshop: Computational Natural language learning,</booktitle>
<pages>97--106</pages>
<marker>Rodd, 1997</marker>
<rawString>Rodd, Jenifer. (1997). Recurrent Neural-Network Learning of Phonological Regula-ri ties in Turkish, ACL&apos;97 Workshop: Computational Natural language learning, pp. 97-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I P Stoianov</author>
<author>John</author>
</authors>
<title>Nerbonne and Huub Houma</title>
<date>1997</date>
<booktitle>Proc. of 7-th CLIN&apos;97</booktitle>
<note>(in press)</note>
<marker>Stoianov, John, 1997</marker>
<rawString>Stoianov, I.P., John Nerbonne and Huub Houma (1997). Modelling the phonotactic structure of natural language words with Simple Recurrent Networks, Proc. of 7-th CLIN&apos;97 (in press)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>