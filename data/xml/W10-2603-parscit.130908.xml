<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.212979">
<title confidence="0.989372">
Domain Adaptation to Summarize Human Conversations
</title>
<author confidence="0.994239">
Oana Sandu, Giuseppe Carenini, Gabriel Murray, and Raymond Ng
</author>
<affiliation confidence="0.78865">
University of British Columbia
Vancouver, Canada
</affiliation>
<email confidence="0.998622">
{oanas,carenini,gabrielm,rng}@cs.ubc.ca
</email>
<sectionHeader confidence="0.993892" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956523809524">
We are interested in improving the sum-
marization of conversations by using
domain adaptation. Since very few email
corpora have been annotated for summa-
rization purposes, we attempt to leverage
the labeled data available in the multi-
party meetings domain for the summari-
zation of email threads. In this paper, we
compare several approaches to super-
vised domain adaptation using out-of-
domain labeled data, and also try to use
unlabeled data in the target domain
through semi-supervised domain adapta-
tion. From the results of our experiments,
we conclude that with some in-domain
labeled data, training in-domain with no
adaptation is most effective, but that
when there is no labeled in-domain data,
domain adaptation algorithms such as
structural correspondence learning can
improve summarization.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986280701754">
On a given day, many people engage in conver-
sations via several modalities, including face-to-
face speech, telephone, email, SMS, chat, and
blogs. Being able to produce automatic summa-
ries of multi-party conversations occurring in one
or several of these modalities would enable the
parties involved to keep track of and make sense
of this diverse data. However, summarizing spo-
ken dialogue is more challenging than summariz-
ing written monologues such as books and arti-
cles, as speech tends to be more fragmented and
disfluent.
We are interested in using both fully and semi-
supervised techniques to produce extractive
summaries for conversations, where each sen-
tence of a text is labeled with its informativeness,
and a subset of sentences are concatenated into
an extractive summary of the text. In previous
work (Murray and Carenini, 2008), it has been
shown that conversations in different modalities
can be effectively characterized by a set of “con-
versational” features that are useful in detecting
informativeness for the task of extractive sum-
marization. However, because of privacy con-
cerns, annotated corpora are rarely publicly
available for conversational data, including for
the email domain. One promising solution to this
problem is domain adaptation, which aims to use
labeled data in a well-studied source domain and
a limited amount of labeled data from a different
target domain to train a model that performs well
in that target domain. In this work, we investi-
gate using domain adaptation that leverages la-
beled data in the domain of meetings along with
labeled and unlabeled email data for summariz-
ing email threads. We evaluate several domain
adaptation algorithms, using both a small set of
conversational features and a large set of simple
lexical features to determine what settings will
yield the best results for summarizing email con-
versations. In our experiments, we do not get a
significant improvement from using out-of-
domain data in addition to in-domain data in su-
pervised domain adaptation, though in the setting
where only unlabeled in-domain data is avail-
able, we gain from using it through structural
correspondence learning. We also observe that
conversational features are more useful in super-
vised methods, whereas lexical features are bet-
ter leveraged in semi-supervised adaptation.
The next section surveys past research in do-
main adaptation and in summarizing conversa-
tional data. In section 3 we present the corpora
and feature sets we used, and we describe our
experimental setting in section 4. We then com-
pare the performance of different methods in sec-
tion 5 and draw conclusions in section 6.
</bodyText>
<page confidence="0.974601">
16
</page>
<note confidence="0.625915">
Proceedings of the 2010 Workshop on Domain Ada tation for Natural Language Processing, ACL 2010, pages 16–22,
Uppsala, Sweden, 15 July 2010. 82010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999165">
email thread as a conversation. For summarizing
email threads, Rambow (2004) used lexical fea-
tures such as tf.idf, features that considered the
thread to be a sequence of turns, and email-
specific features such as number of recipients
and the subject line. Asynchronous multi-party
conversations were successfully represented for
summarization through a small number of con-
versational features by Murray and Carenini
(2008). This paved the way to cross-domain
conversation summarization by representing both
email threads and meetings with a set of common
conversational features. The work we present
here investigates using data from both emails and
meetings in summarizing emails, and compares
using conversational versus lexical features.
</bodyText>
<sectionHeader confidence="0.969078" genericHeader="method">
3 Summarization setting
</sectionHeader>
<bodyText confidence="0.9995">
Because the meetings domain has a large corpus,
AMI, annotated for summarization, we will use it
as the source domain for adaptation and the
email domain as the target, with data from the
Enron corpus as unlabeled email data, and the
BC3 corpus as test data.
</bodyText>
<subsectionHeader confidence="0.990691">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.998729388888889">
The AMI meeting corpus: We use the scenario
portion of the AMI corpus (Carletta et al., 2005),
for which groups of four participants take part in
a series of four meetings and play roles within a
fictitious company. While the scenario given to
them is artificial, the speech and the actions are
completely spontaneous and natural. The dataset
contains approximately 115000 dialogue act
(DA) segments. For the annotation, annotators
wrote abstract summaries of each meeting and
extracted transcript DA segments that best con-
veyed or supported the information in the ab-
stracts. A many-to-many mapping between tran-
script DAs and sentences from the human ab-
stract was obtained for each annotator, with three
annotators assigned to each meeting. We con-
sider a dialogue act to be a positive example if it
is linked to a given human summary, and a nega-
tive example otherwise. Approximately 13% of
the total DAs are ultimately labeled as positive.
The BC3 email corpus1: composed of 40
email threads from the World Wide Web Con-
sortium (W3C) mailing list which feature a vari-
ety of topics such as web accessibility and plan-
ning face-to-face meetings. Each thread is anno-
tated similarly to the AMI corpus, with three an-
&apos; http://www.cs.ubc.ca/labs/lci/bc3.html
notators authoring abstracts and linking email
thread sentences to the abstract sentences.
The Enron email corpus2: a collection of
emails released as part of the investigation into
the Enron corporation, it has become a popular
corpus for NLP research due to being realistic,
naturally-occurring data from a corporate envi-
ronment. We use 39 threads from this corpus to
supplement the BC3 email data.
</bodyText>
<subsectionHeader confidence="0.99821">
3.2 Features Used
</subsectionHeader>
<bodyText confidence="0.999803676470588">
We consider two sets of features for each sen-
tence: a small set of conversational structure fea-
tures, and a large set of lexical features.
Conversational features: We extract 24 con-
versational features from both the email and
meetings domain, and which consider both
emails and meetings to be conversations com-
prised of turns between multiple participants. For
an email thread, a turn consists of a single email
fragment in the exchange. Similarly, for meet-
ings, a turn is a sequence of dialogue acts by the
same speaker. The conversational features,
which are described in detail in (Murray and
Carenini, 2008), include sentence length, sen-
tence position in the conversation and in the cur-
rent turn, pause-style features, lexical cohesion,
centroid scores, and features that measure how
terms cluster between conversation participants
and conversation turns.
Lexical features: We derive an extensive set of
lexical features, originally proposed in (Murray
et al., 2010) from the AMI and BC3 datasets, and
then compute their occurrence in the Enron cor-
pus. After throwing out features that occur less
than five times, we end up with approximately
200,000 features. The features derived are: char-
acter trigrams, word bigrams, POS tag bigrams,
word pairs, POS pairs, and varying instantiation
ngram (VIN) features. For word pairs, we extract
the ordered pairs of words that occur in the same
sentence, and similarly for POS pairs. To derive
VIN features, we take each word bigram w1,w2
and further represent it as two patterns p1,w2 and
w1,p2 each consisting of a word and a POS tag.
</bodyText>
<subsectionHeader confidence="0.989296">
3.3 Classifier
</subsectionHeader>
<bodyText confidence="0.9999884">
In all of our experiments, we train logistic re-
gression classifiers using the liblinear toolkit3.
This choice was partly motivated by our earlier
summarization research, where logistic regres-
sion classifiers were compared alongside support
</bodyText>
<footnote confidence="0.999145">
2 http://www.cs.cmu.edu/˜enron/
3 http://www.csie.ntu.edu.tw/˜cjlin/liblinear/
</footnote>
<page confidence="0.998388">
18
</page>
<bodyText confidence="0.999777666666667">
vector machines. The two types of classifier
yielded very similar results, with logistic regres-
sion classifiers being much faster to train.
</bodyText>
<subsectionHeader confidence="0.946214">
3.4 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.999978666666667">
Given the predicted labels on a test set and the
existing gold-standard labels of the test set data,
in each of our experiments we compute the area
under the receiver operator curve as a measure of
performance. The area under the ROC (auROC)
is a common summary statistic used to measure
the quality of binary classification, where a per-
fect classifier would achieve an auROC of 1.0,
and a random classifier, near 0.5.
</bodyText>
<sectionHeader confidence="0.99992" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.981443">
4.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.999986551724138">
The available labeled BC3 data totals about 3000
sentences, and the available labeled AMI data
totals over 100,000 sentences, so for both effi-
ciency and to not overwhelm the in-domain data,
in each of our runs we subsample 10,000 sen-
tences from the AMI data to use for training. Af-
ter some initial experiments, where increasing
the amount of target data beyond this did not im-
prove accuracy, we decided not to incur the run-
time cost of training on larger amounts of source
data. Similarly, given that we extracted about
200,000 lexical features from our corpora, from
our initial experiments trading off auROC and
runtime, we decided to select a subset of 10,000
lexical features chosen by having the top mutual
information with respect to the summarization
labels. We did 5-fold cross-validation to split the
target set into training and testing portions, and
ran all the domain adaptation methods using the
same split. We report the auROC performance of
each method averaged over three runs of the 5-
fold cross-validation. To test for significant dif-
ferences between the performances of the various
methods, we compute pairwise t-tests between
the auROC values obtained on the same run. To
account for an increased chance of false positives
in reporting results of several pairwise t-tests, we
report significance for p-values &lt; 0.005 rather
than at the customary 0.05 level.
</bodyText>
<subsectionHeader confidence="0.971074">
4.2 Methods Implemented
</subsectionHeader>
<bodyText confidence="0.999953901639345">
We compare supervised domain adaptation me-
thods to the baseline INDOMAIN, in which only
the training folds of the target data are used for
training. In the MERGE method, we simply
combine the labeled source and target sets and
train on their combination. For ENSEMBLE, we
train a classifier on the source training data, a
classifier on the target training data, run each of
them on the target test data, and for each test in-
stance compute the average of the two probabili-
ties predicted by the classifiers and use it to
make a label prediction. We could vary the trade-
off between the contribution of the source and
target classifier in ENSEMBLE and determine
the optimal parameter by cross-validation,
though for simplicity we used 0.5 which pro-
duced satisfying results. For the PRED approach,
we use the source data to train a classifier, use it
to make a prediction for the label of each point in
the target data, and add the predicted probability
as an additional feature to an in-domain trained
classifier. The final supervised method FEAT-
COPY (Daume, 2007) takes the existing features
and extends the feature space by making a gen-
eral, a source-specific, and a target-specific ver-
sion of each feature. Hence, a sentence with fea-
tures (x) gets represented as (x, x, 0) if it comes
from the source domain, and as (x, 0, x) if it
comes from the target domain.
For semi-supervised domain adaptation meth-
ods, our baseline does not exploit any unlabeled
target data. We train a classifier on the source
data only, and call this TRANSFER. In contrast
our two semi-supervised methods try to leverage
unlabeled target data to help a classifier trained
with labeled source data be more suited to the
target domain.
For the SCL approach, we implemented Blit-
zer’s structural correspondence learning (SCL)
algorithm. An important part of the algorithm is
training a classifier for each of a set of m selected
pivot features to determine the correlations of the
other features with respect to the pivot. The m
models’ weights are combined in a matrix, and
its SVD with truncation factor of k is then ap-
plied to the data to yield k new features for the
data, that are added to the existing features. For
the larger set of lexical features, we ran SCL
with Blitzer’s original choice of m=1000 and
k=50, but since the computation was extremely
time consuming we scale down m to 100. For the
tests with conversational features, since the
number of features is 24, we picked m=24 and
k=24. We also test SCLSMALL, which uses the
same algorithm as SCL to find augmented fea-
tures, except it then uses only these k features to
train, not adding them to the original features.
This possibility was suggested in (Blitzer 2008).
As a second semi-supervised method, we im-
plemented SELFTRAIN. The standard self-
training algorithm we implemented, inspired by
</bodyText>
<page confidence="0.998258">
19
</page>
<bodyText confidence="0.999826285714286">
Blum and Mitchell (1998), is to start with a la-
beled training set T, create a subset of a fixed
size of the unlabeled data U, and then iterate
training a classifier on T, making a prediction on
the data in U, and take the highest-confidence
positive p predictions and highest-confidence
negative n predictions from U with their pre-
dicted labels to add to T before replenishing U
from the rest of the unlabeled data. We picked
the size of the subset U as 200, and to select the
top p=3 and bottom n=17 predictions at each step
in order to achieve a ratio of summary to total
sentences of 15%, which is near to the known
ratio of the labels for AMI.
</bodyText>
<table confidence="0.988757">
method indomain merge ense featcopy pred transfer selftrain scl sclsmall
ble
using conversational features
auROC 0.838 0.747 0.751 0.839 0.838 0.677 0.678 0.663 0.646
time(s) 0.79 2.42 2.64 8.44 5.38 2.08 100.2 52.85 66.74
using lexical features
auROC 0.623 0.638 0.667 0.615 0.625 0.636 0.636 0.651 0.742
time(s) 4.87 13.64 13.77 78.63 30.99 9.73 448.8 813.7 828.3
</table>
<tableCaption confidence="0.999847">
Table 1. Performance and time of domain adaptation methods with the two feature sets
</tableCaption>
<sectionHeader confidence="0.999393" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999946360655738">
In our first experiment, we ran all the domain
adaptation methods on the data with conversa-
tional features; in our second experiment, we did
the same on the data with lexical features. We
computed the average of the auROCs and run-
ning times obtained for each method in each ex-
periment. Table 1 lists the results of the super-
vised methods MERGE, ENSEMBLE, and
FEATCOPY with baseline INDOMAIN, and the
semi-supervised methods SELFTRAIN, SCL,
and SCLSMALL with baseline TRANSFER.
The best results for supervised methods (and
overall) are achieved by FEATCOPY, PRED,
and INDOMAIN with the conversational fea-
tures, with a similar performance that is signifi-
cantly better than for MERGE and ENSEMBLE.
However, for lexical features MERGE and EN-
SEMBLE beat their performance, with the sig-
nificant differences from the baseline INDO-
MAIN being those of ENSEMBLE and FEAT-
COPY, the latter now being the worst performer.
For the set of lexical features, all semi-
supervised methods improve on TRANSFER. In
this setting, all of the differences are significant,
with SCLSMALL generating a considerable gain
of 10%. For the set of conversational features,
SELFTRAIN yields an auROC similar to
TRANSFER, and the small difference between
the two is not significant. Unlike when using
lexical features, SCL and SCLSMALL perform
significantly worse than TRANSFER, though
this is not unexpected. Because it relies on de-
termining correlation between features, we be-
lieve that structural correspondence learning is
more appropriate in a high rather than low-
dimensional feature space.
Figure 1 shows, for each of the methods, a dark
grey bar representing the auROC obtained with
the set of conversational features next to a lighter
grey one for the lexical features. For the super-
vised methods on the left (INDOMAIN to
PRED), the conversational features yield better
performance, and this by an absolute ROC dif-
ference of more than 5%. However, notice that
no method outperform the baseline INDOMAIN.
For the semi-supervised methods on the right, the
difference in performance between the two fea-
ture sets is less marked, although the auROC of
SCLSMALL with lexical features is exception-
ally larger.
As shown in Table 1, every one of the domain
adaptation methods has a higher average time
with lexical features than with conversational
features. The semi-supervised methods take
longer than the fully supervised methods, and
this is due to their algorithms involving more
steps. Both SCL and SELFTRAIN take minutes
instead of seconds to make a prediction, though
their running times are more reasonable than
with the initial parameter settings we used in pre-
liminary experiments.
</bodyText>
<page confidence="0.976257">
20
</page>
<figureCaption confidence="0.9831695">
Figure 1. Comparison of auROCs of all domain
adaptation methods and baselines
</figureCaption>
<sectionHeader confidence="0.997274" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999983476190476">
This paper is a comparative study of the per-
formance of several domain adaptation methods
on the task of summarizing conversational data
when a large amount of annotated data is avail-
able in the domain of meetings and a smaller (or
no) amount of annotation exists in the target do-
main of email threads.
One surprising finding of our experiments is
that of the methods we implemented, the best
performance is achieved by training on in-
domain data using conversational features.
Hence, it seems that when sufficient labeled in-
domain data is available, supervised domain ad-
aptation is not useful for summarization of
emails with the features and amounts of labeled
data we used.
However, semi-supervised methods using unla-
beled data and labeled out-of-domain data are
useful in the absence of these labels, with the
SCLSMALL method greatly outperforming the
baseline. This is a promising result for using an-
notated corpora in well-studied domains or con-
versational modalities to summarize data in new
domains.
In our experiments, we have explored the effec-
tiveness of conversational and lexical features
separately. The two sets of features differ in their
impact on domain adaptation: with conversa-
tional features, no method improves significantly
over the baseline, whereas with lexical features,
the semi-supervised methods given no labeled
target data perform better than the supervised
baseline of training in-domain. One hypothesis to
explain this is that lexical features behave simi-
larly in the two domains, so training on the larger
amount of labeled target data is beneficial, while
conversational features are more domain spe-
cific, likely because emails and meetings are
structured differently. As the next step in our
work, we intend to combine the two sets of fea-
tures. In doing this, we will have to ensure that
the conversational features are not washed out by
a very large number of lexical features.
A scenario of practical interest in domain adap-
tation for new domains is when the target domain
has a considerable amount of unlabeled data and
a subset of this data can easily be annotated by
hand, for example five threads in the email do-
main. We are currently exploring injecting a
small amount of labeled target data into the semi-
supervised methods we have implemented to ac-
count for differences that cannot be observed in
the unlabeled data. Blitzer (2008) did such an
adjustment to SCL using a small amount of la-
beled target data to correct misaligned features
and thus improve accuracy.
Finally, it may be worth investigating how to
combine several of the methods, for example by
adding the feature of PRED based on training a
classifier on the source, alongside augmented
features using more unlabeled data through SCL,
and adding the highest-confidence labels from
SELFTRAIN to the training set.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999893458333333">
Blitzer, J. (2008). Domain Adaptation of Natural
Language Processing Systems. PhD Thesis.
Blitzer, J., Dredze, M., &amp; Pereira, F. (2007). Biogra-
phies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification.
In Proc. of ACL 2007.
Blitzer, J., McDonald, R., &amp; Pereira, F. (2006). Do-
main adaptation with structural correspondence
learning. In Proc. of EMNLP 2006.
Blum, A., &amp; Mitchell, T. (1998). Combining labeled
and unlabeled data with co-training. In Proc. CLT.
Carletta, J., Ashby, S., Bourban, S., Flynn, M.,
Guillemot, M. et al. (2005). The AMI meeting cor-
pus: A pre-announcement. In Proc. of MLMI 2005.
Chelba, C., &amp; Acero, A. (2006). Adaptation of maxi-
mum entropy capitalizer: Little data can help a lot.
Computer Speech &amp; Language, 20(4), 382-399.
Daume III, H. (2007). Frustratingly easy domain ad-
aptation. In Proc. of ACL 2007.
Daume III, H., &amp; Marcu, D. (2006). Domain Adapta-
tion for Statistical Classifiers. Journal of Artificial
Intelligence Research, 26, 101–126.
Florian, R., Hassan, H., Ittycheriah, A., Jing, H.,
Kambhatla, N., Luo, X., et al. (2004). A statistical
</reference>
<page confidence="0.983072">
21
</page>
<reference confidence="0.999592875">
model for multilingual entity detection and track-
ing. In Proc. HLT-NAACL 2004.
Galley, M. (2006). A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proc. of EMNLP 2006.
McClosky, D., Charniak, E., &amp; Johnson, M. (2006).
Effective self-training for parsing. In Proc. of HLT-
NAACL 2006 .
Murray, G., &amp; Carenini, G. (2008). Summarizing
spoken and written conversations. In Proc. of
EMNLP 2008.
Murray, G., Carenini, G., &amp; Ng, R. (2010). Interpreta-
tion and transformation for abstracting conversa-
tions. In Proc. of HLT-NAACL 2010.
Murray, G., Renals, S., Moore, J., &amp; Carletta, J.
(2006). Incorporating speaker and discourse fea-
tures into speech summarization. In Proc. of HLT-
NAACL 2006.
Plank, B. (2009). Structural correspondence learning
for parse disambiguation. In Proc. of EACL 2009:
Student Research Workshop.
Rambow, O., Shrestha, L., &amp; Chen, J. (2004). Sum-
marizing email threads. In Proc. of HLT-NAACL
2004.
</reference>
<page confidence="0.999027">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.871822">
<title confidence="0.997859">Domain Adaptation to Summarize Human Conversations</title>
<author confidence="0.904007">Oana Sandu</author>
<author confidence="0.904007">Giuseppe Carenini</author>
<author confidence="0.904007">Gabriel Murray</author>
<author confidence="0.904007">Raymond</author>
<affiliation confidence="0.999799">University of British</affiliation>
<address confidence="0.987762">Vancouver, Canada</address>
<email confidence="0.998038">oanas@cs.ubc.ca</email>
<email confidence="0.998038">carenini@cs.ubc.ca</email>
<email confidence="0.998038">gabrielm@cs.ubc.ca</email>
<email confidence="0.998038">rng@cs.ubc.ca</email>
<abstract confidence="0.999019545454545">We are interested in improving the summarization of conversations by using domain adaptation. Since very few email corpora have been annotated for summarization purposes, we attempt to leverage the labeled data available in the multiparty meetings domain for the summarization of email threads. In this paper, we compare several approaches to supervised domain adaptation using out-ofdomain labeled data, and also try to use unlabeled data in the target domain through semi-supervised domain adaptation. From the results of our experiments, we conclude that with some in-domain labeled data, training in-domain with no adaptation is most effective, but that when there is no labeled in-domain data, domain adaptation algorithms such as structural correspondence learning can improve summarization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Blitzer</author>
</authors>
<title>Domain Adaptation of Natural Language Processing Systems.</title>
<date>2008</date>
<tech>PhD Thesis.</tech>
<contexts>
<context position="13209" citStr="Blitzer 2008" startWordPosition="2125" endWordPosition="2126">plied to the data to yield k new features for the data, that are added to the existing features. For the larger set of lexical features, we ran SCL with Blitzer’s original choice of m=1000 and k=50, but since the computation was extremely time consuming we scale down m to 100. For the tests with conversational features, since the number of features is 24, we picked m=24 and k=24. We also test SCLSMALL, which uses the same algorithm as SCL to find augmented features, except it then uses only these k features to train, not adding them to the original features. This possibility was suggested in (Blitzer 2008). As a second semi-supervised method, we implemented SELFTRAIN. The standard selftraining algorithm we implemented, inspired by 19 Blum and Mitchell (1998), is to start with a labeled training set T, create a subset of a fixed size of the unlabeled data U, and then iterate training a classifier on T, making a prediction on the data in U, and take the highest-confidence positive p predictions and highest-confidence negative n predictions from U with their predicted labels to add to T before replenishing U from the rest of the unlabeled data. We picked the size of the subset U as 200, and to sel</context>
</contexts>
<marker>Blitzer, 2008</marker>
<rawString>Blitzer, J. (2008). Domain Adaptation of Natural Language Processing Systems. PhD Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>Blitzer, J., Dredze, M., &amp; Pereira, F. (2007). Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proc. of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP</booktitle>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>Blitzer, J., McDonald, R., &amp; Pereira, F. (2006). Domain adaptation with structural correspondence learning. In Proc. of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proc. CLT.</booktitle>
<contexts>
<context position="13364" citStr="Blum and Mitchell (1998)" startWordPosition="2146" endWordPosition="2149">SCL with Blitzer’s original choice of m=1000 and k=50, but since the computation was extremely time consuming we scale down m to 100. For the tests with conversational features, since the number of features is 24, we picked m=24 and k=24. We also test SCLSMALL, which uses the same algorithm as SCL to find augmented features, except it then uses only these k features to train, not adding them to the original features. This possibility was suggested in (Blitzer 2008). As a second semi-supervised method, we implemented SELFTRAIN. The standard selftraining algorithm we implemented, inspired by 19 Blum and Mitchell (1998), is to start with a labeled training set T, create a subset of a fixed size of the unlabeled data U, and then iterate training a classifier on T, making a prediction on the data in U, and take the highest-confidence positive p predictions and highest-confidence negative n predictions from U with their predicted labels to add to T before replenishing U from the rest of the unlabeled data. We picked the size of the subset U as 200, and to select the top p=3 and bottom n=17 predictions at each step in order to achieve a ratio of summary to total sentences of 15%, which is near to the known ratio</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Blum, A., &amp; Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In Proc. CLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>S Ashby</author>
<author>S Bourban</author>
<author>M Flynn</author>
<author>M Guillemot</author>
</authors>
<title>The AMI meeting corpus: A pre-announcement.</title>
<date>2005</date>
<booktitle>In Proc. of MLMI</booktitle>
<contexts>
<context position="4987" citStr="Carletta et al., 2005" startWordPosition="770" endWordPosition="773">mail threads and meetings with a set of common conversational features. The work we present here investigates using data from both emails and meetings in summarizing emails, and compares using conversational versus lexical features. 3 Summarization setting Because the meetings domain has a large corpus, AMI, annotated for summarization, we will use it as the source domain for adaptation and the email domain as the target, with data from the Enron corpus as unlabeled email data, and the BC3 corpus as test data. 3.1 Datasets The AMI meeting corpus: We use the scenario portion of the AMI corpus (Carletta et al., 2005), for which groups of four participants take part in a series of four meetings and play roles within a fictitious company. While the scenario given to them is artificial, the speech and the actions are completely spontaneous and natural. The dataset contains approximately 115000 dialogue act (DA) segments. For the annotation, annotators wrote abstract summaries of each meeting and extracted transcript DA segments that best conveyed or supported the information in the abstracts. A many-to-many mapping between transcript DAs and sentences from the human abstract was obtained for each annotator, </context>
</contexts>
<marker>Carletta, Ashby, Bourban, Flynn, Guillemot, 2005</marker>
<rawString>Carletta, J., Ashby, S., Bourban, S., Flynn, M., Guillemot, M. et al. (2005). The AMI meeting corpus: A pre-announcement. In Proc. of MLMI 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>A Acero</author>
</authors>
<title>Adaptation of maximum entropy capitalizer: Little data can help a lot.</title>
<date>2006</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>382--399</pages>
<marker>Chelba, Acero, 2006</marker>
<rawString>Chelba, C., &amp; Acero, A. (2006). Adaptation of maximum entropy capitalizer: Little data can help a lot. Computer Speech &amp; Language, 20(4), 382-399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="11564" citStr="Daume, 2007" startWordPosition="1838" endWordPosition="1839">ge of the two probabilities predicted by the classifiers and use it to make a label prediction. We could vary the tradeoff between the contribution of the source and target classifier in ENSEMBLE and determine the optimal parameter by cross-validation, though for simplicity we used 0.5 which produced satisfying results. For the PRED approach, we use the source data to train a classifier, use it to make a prediction for the label of each point in the target data, and add the predicted probability as an additional feature to an in-domain trained classifier. The final supervised method FEATCOPY (Daume, 2007) takes the existing features and extends the feature space by making a general, a source-specific, and a target-specific version of each feature. Hence, a sentence with features (x) gets represented as (x, x, 0) if it comes from the source domain, and as (x, 0, x) if it comes from the target domain. For semi-supervised domain adaptation methods, our baseline does not exploit any unlabeled target data. We train a classifier on the source data only, and call this TRANSFER. In contrast our two semi-supervised methods try to leverage unlabeled target data to help a classifier trained with labeled </context>
</contexts>
<marker>Daume, 2007</marker>
<rawString>Daume III, H. (2007). Frustratingly easy domain adaptation. In Proc. of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daume</author>
<author>D Marcu</author>
</authors>
<title>Domain Adaptation for Statistical Classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>26</volume>
<pages>101--126</pages>
<marker>Daume, Marcu, 2006</marker>
<rawString>Daume III, H., &amp; Marcu, D. (2006). Domain Adaptation for Statistical Classifiers. Journal of Artificial Intelligence Research, 26, 101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Hassan</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>X Luo</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, 2004</marker>
<rawString>Florian, R., Hassan, H., Ittycheriah, A., Jing, H., Kambhatla, N., Luo, X., et al. (2004). A statistical model for multilingual entity detection and tracking. In Proc. HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
</authors>
<title>A skip-chain conditional random field for ranking meeting utterances by importance.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP</booktitle>
<marker>Galley, 2006</marker>
<rawString>Galley, M. (2006). A skip-chain conditional random field for ranking meeting utterances by importance. In Proc. of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proc. of HLTNAACL</booktitle>
<pages>.</pages>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>McClosky, D., Charniak, E., &amp; Johnson, M. (2006). Effective self-training for parsing. In Proc. of HLTNAACL 2006 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>G Carenini</author>
</authors>
<title>Summarizing spoken and written conversations.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="1864" citStr="Murray and Carenini, 2008" startWordPosition="278" endWordPosition="281">ns occurring in one or several of these modalities would enable the parties involved to keep track of and make sense of this diverse data. However, summarizing spoken dialogue is more challenging than summarizing written monologues such as books and articles, as speech tends to be more fragmented and disfluent. We are interested in using both fully and semisupervised techniques to produce extractive summaries for conversations, where each sentence of a text is labeled with its informativeness, and a subset of sentences are concatenated into an extractive summary of the text. In previous work (Murray and Carenini, 2008), it has been shown that conversations in different modalities can be effectively characterized by a set of “conversational” features that are useful in detecting informativeness for the task of extractive summarization. However, because of privacy concerns, annotated corpora are rarely publicly available for conversational data, including for the email domain. One promising solution to this problem is domain adaptation, which aims to use labeled data in a well-studied source domain and a limited amount of labeled data from a different target domain to train a model that performs well in that </context>
<context position="4279" citStr="Murray and Carenini (2008)" startWordPosition="657" endWordPosition="660"> section 6. 16 Proceedings of the 2010 Workshop on Domain Ada tation for Natural Language Processing, ACL 2010, pages 16–22, Uppsala, Sweden, 15 July 2010. 82010 Association for Computational Linguistics email thread as a conversation. For summarizing email threads, Rambow (2004) used lexical features such as tf.idf, features that considered the thread to be a sequence of turns, and emailspecific features such as number of recipients and the subject line. Asynchronous multi-party conversations were successfully represented for summarization through a small number of conversational features by Murray and Carenini (2008). This paved the way to cross-domain conversation summarization by representing both email threads and meetings with a set of common conversational features. The work we present here investigates using data from both emails and meetings in summarizing emails, and compares using conversational versus lexical features. 3 Summarization setting Because the meetings domain has a large corpus, AMI, annotated for summarization, we will use it as the source domain for adaptation and the email domain as the target, with data from the Enron corpus as unlabeled email data, and the BC3 corpus as test data</context>
<context position="7174" citStr="Murray and Carenini, 2008" startWordPosition="1125" endWordPosition="1128"> Features Used We consider two sets of features for each sentence: a small set of conversational structure features, and a large set of lexical features. Conversational features: We extract 24 conversational features from both the email and meetings domain, and which consider both emails and meetings to be conversations comprised of turns between multiple participants. For an email thread, a turn consists of a single email fragment in the exchange. Similarly, for meetings, a turn is a sequence of dialogue acts by the same speaker. The conversational features, which are described in detail in (Murray and Carenini, 2008), include sentence length, sentence position in the conversation and in the current turn, pause-style features, lexical cohesion, centroid scores, and features that measure how terms cluster between conversation participants and conversation turns. Lexical features: We derive an extensive set of lexical features, originally proposed in (Murray et al., 2010) from the AMI and BC3 datasets, and then compute their occurrence in the Enron corpus. After throwing out features that occur less than five times, we end up with approximately 200,000 features. The features derived are: character trigrams, </context>
</contexts>
<marker>Murray, Carenini, 2008</marker>
<rawString>Murray, G., &amp; Carenini, G. (2008). Summarizing spoken and written conversations. In Proc. of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>G Carenini</author>
<author>R Ng</author>
</authors>
<title>Interpretation and transformation for abstracting conversations.</title>
<date>2010</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<contexts>
<context position="7533" citStr="Murray et al., 2010" startWordPosition="1176" endWordPosition="1179">pants. For an email thread, a turn consists of a single email fragment in the exchange. Similarly, for meetings, a turn is a sequence of dialogue acts by the same speaker. The conversational features, which are described in detail in (Murray and Carenini, 2008), include sentence length, sentence position in the conversation and in the current turn, pause-style features, lexical cohesion, centroid scores, and features that measure how terms cluster between conversation participants and conversation turns. Lexical features: We derive an extensive set of lexical features, originally proposed in (Murray et al., 2010) from the AMI and BC3 datasets, and then compute their occurrence in the Enron corpus. After throwing out features that occur less than five times, we end up with approximately 200,000 features. The features derived are: character trigrams, word bigrams, POS tag bigrams, word pairs, POS pairs, and varying instantiation ngram (VIN) features. For word pairs, we extract the ordered pairs of words that occur in the same sentence, and similarly for POS pairs. To derive VIN features, we take each word bigram w1,w2 and further represent it as two patterns p1,w2 and w1,p2 each consisting of a word and</context>
</contexts>
<marker>Murray, Carenini, Ng, 2010</marker>
<rawString>Murray, G., Carenini, G., &amp; Ng, R. (2010). Interpretation and transformation for abstracting conversations. In Proc. of HLT-NAACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Moore</author>
<author>J Carletta</author>
</authors>
<title>Incorporating speaker and discourse features into speech summarization.</title>
<date>2006</date>
<booktitle>In Proc. of HLTNAACL</booktitle>
<marker>Murray, Renals, Moore, Carletta, 2006</marker>
<rawString>Murray, G., Renals, S., Moore, J., &amp; Carletta, J. (2006). Incorporating speaker and discourse features into speech summarization. In Proc. of HLTNAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Plank</author>
</authors>
<title>Structural correspondence learning for parse disambiguation.</title>
<date>2009</date>
<booktitle>In Proc. of EACL 2009: Student Research Workshop.</booktitle>
<marker>Plank, 2009</marker>
<rawString>Plank, B. (2009). Structural correspondence learning for parse disambiguation. In Proc. of EACL 2009: Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Rambow</author>
<author>L Shrestha</author>
<author>J Chen</author>
</authors>
<title>Summarizing email threads.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<marker>Rambow, Shrestha, Chen, 2004</marker>
<rawString>Rambow, O., Shrestha, L., &amp; Chen, J. (2004). Summarizing email threads. In Proc. of HLT-NAACL 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>