<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007099">
<title confidence="0.9940545">
Using the Distribution of Performance for Studying Statistical
NLP Systems and Corpora
</title>
<author confidence="0.998407">
Yuval Krymolowski
</author>
<affiliation confidence="0.816243">
Department of Mathematics and Computer Science
Bar-Ilan University
52900 Ramat Gan, Israel
</affiliation>
<sectionHeader confidence="0.85884" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999685470588235">
Statistical NLP systems are fre-
quently evaluated and compared on
the basis of their performances on
a single split of training and test
data. Results obtained using a single
split are, however, subject to sam-
pling noise. In this paper we ar-
gue in favour of reporting a distri-
bution of performance figures, ob-
tained by resampling the training
data, rather than a single num-
ber. The additional information
from distributions can be used to
make statistically quantified state-
ments about differences across pa-
rameter settings, systems, and cor-
pora.
</bodyText>
<sectionHeader confidence="0.99734" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.942197944444444">
The common practice in evaluating statistical
NLP systems is using a standard corpus (e.g.,
Penn TreeBank for parsing, Reuters for text
categorization) along with a standard split be-
tween training and test data. As systems im-
prove, it becomes harder to achieve additional
improvements, and the performance of vari-
ous state-of-the-art systems is approximately
identical. This makes performance compar-
isons difficult.
In this paper, we argue in favour of studying
the distribution of performance, and present
conclusions drawn from studying the recall
distribution. This distribution provides mea-
sures for answering the following questions:
Q1: Comparing systems on given data: Is
classifier A better than classifier B for
given training and test data?
</bodyText>
<listItem confidence="0.965835">
Q2: Adequacy of training data to test data:
Is a system trained on dataset X ade-
quate for analysing dataset Y ? Are fea-
tures from X indicative in Y ?
Q3: Comparing data sets with a given sys-
tem: If a different training set improves
the result of system A on dataset Y1, will
</listItem>
<bodyText confidence="0.981904043478261">
this be the case on dataset Y2 as well?
The answers to these questions can provide
useful insight into statistical NLP systems.
In particular, about sensitivity to features in
the training data, and transferability. These
properties can be different even when similar
performance is reported.
A statistical treatment of Question 1 is
presented by Yeh (2000). He tests for the
significance of performance differences on
fixed training and test data sets. In other
related works, Martin and Hirschberg (1996)
provides an overview of significance tests
of error differences in small samples, and
Dietterich (1998) discusses results of a num-
ber of tests.
Questions 2 and 3 have been frequently
raised in NLP, but not explicitly addressed,
since the prevailing evaluation methods pro-
vide no means of addressing them. In this
paper we propose addressing all three ques-
tions with a single experimental methodology,
which uses the distribution of recall.
</bodyText>
<sectionHeader confidence="0.956515" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999692634146341">
Words, parts-of-speech (POS), words, or any
feature in text may be regarded as outcomes
of a statistical process. Therefore, word
counts, count ratios, and other data used in
creating statistical NLP models are statisti-
cal quantities as well, and as such prone to
sampling noise. Sampling noise results from
the finiteness of the data, and the particular
choice of training and test data.
A model is an approximation or a more ab-
stract representation of training data. One
may look at a model as a collection of es-
timators analogous, e.g., to the slope calcu-
lated by linear regression. These estimators
are statistics with a distribution related to the
way they were obtained, which may be very
complicated. The performance figures, being
dependent on these estimators, have a distri-
bution function which may be difficult to find
theoretically. This distribution gives rise to
intrinsic noise.
Performance comparisons based on a single
run or a few runs do not take these noises into
account. Because we cannot assign the result-
ing statements a confidence measure, they are
more qualitative than quantitative. The de-
gree to which we can accept such statements
depends on the noise level and more generally,
on the distribution of performance.
In this paper, we use recall as a perfor-
mance measure (cf. Section 4.4 and Section
3.2 in (Yeh, 2000)). Recall samples are ob-
tained by resampling from training data and
training classifiers on these samples.
The resampling methods used here are
cross-validation and bootstrap (Efron and
Gong, 1983; Efron and Tibshirani, 1993,
cf. Section 3). Section 4 presents the experi-
mental goals and setup. Results are presented
and discussed in Section 5, and a summary is
provided in Section 6.
</bodyText>
<sectionHeader confidence="0.987757" genericHeader="method">
3 The Bootstrap Method
</sectionHeader>
<bodyText confidence="0.999784818181818">
The bootstrap is a re-sampling technique
designed for obtaining empirical distribu-
tions of estimators. It can be thought
of as a smoothed version of k-fold cross-
validation (CV). The method has been ap-
plied to decision tree and bayesian classifiers
by Kohavi (1995) and to neural networks by,
e.g., LeBaron and Weigend (1998).
In this paper, we use the bootstrap method
to obtain the distribution of performance of a
system which learns to identify non-recursive
noun-phrases (base-NPs). While there are a
few refinements of the method, the intention
of this paper is to present the benefits of ob-
taining distributions, rather than optimising
bias or variance. We do not aim to study the
properties of bootstrap estimation.
Let a statistic S = S(x1, ... , xn) be a func-
tion of the independent observations {xi}ni=1
of a statistical variable X. The bootstrap
method constructs the distribution function
of S by successively re-sampling x with re-
placements.
After B samples, we have a set of bootstrap
samples {xb1, ... , xbn}Bb=1, each of which yields
an estimate ˆSb for S. The distribution of Sˆ is
the bootstrap estimate for the distribution of
S. That distribution is mostly used for esti-
mating the standard deviation, bias, or confi-
dence interval of S.
In the present work, xi are the base-NP in-
stances in a given corpus, and the statistic S
is the recall on a test set.
</bodyText>
<sectionHeader confidence="0.998715" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999786888888889">
The aim of our experiments is to test whether
the recall distribution can be helpful in an-
swering the questions Q1–Q3 mentioned in
the introduction of this paper.
The data and learning algorithms are pre-
sented in Sections 4.1 and 4.2. Section 4.3
describes the sampling method in detail. Sec-
tion 4.4 motivates the use of recall and de-
scribes the experiments.
</bodyText>
<subsectionHeader confidence="0.973861">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999659909090909">
We used Penn-Treebank (Marcus et al., 1993)
data, presented in Table 1. Wall-Street Jour-
nal (WSJ) Sections 15-18 and 20 were used
by Ramshaw and Marcus (1995) as training
and test data respectively for evaluating their
base-NP chunker. These data have since be-
come a standard for evaluating base-NP sys-
tems.
The WSJ texts are economic newspaper
reports, which often include elaborated sen-
tences containing about six base-NPs on the
</bodyText>
<table confidence="0.997452142857143">
Source Sentences Words Base
NPs
WSJ 15-18 8936 229598 54760
WSJ 20 2012 51401 12335
ATIS 190 2046 613
WSJ 20a 100 2479 614
WSJ 20b 93 2661 619
</table>
<tableCaption confidence="0.999566">
Table 1: Data sources
</tableCaption>
<bodyText confidence="0.998109347826087">
average.
The ATIS data, on the other hand, are
a collection of customer requests related to
flight schedules. These typically include short
sentences which contain only three base-NPs
on the average. For example:
I have a friend living in Denver
that would like to visit me
here in Washington DC .
The structure of sentences in the ATIS data
differs significantly from that in the WSJ
data. We expect this difference to be reflected
in the recall of systems tested on both data
sets.
The small size of the ATIS data can influ-
ence the results as well. To distinguish the
size effect from the structural differences, we
drew two equally small samples from WSJ
Section 20. These samples, WSJ20a and
WSJ20b, consist of the first 100 and the fol-
lowing 93 sentences respectively. There is a
slight difference in size because sentences were
kept complete, as explained Section 4.3.
</bodyText>
<subsectionHeader confidence="0.998318">
4.2 Learning Algorithms
</subsectionHeader>
<bodyText confidence="0.999851352941176">
We evaluated base-NP learning systems based
on two algorithms: MBSL (Argamon et al.,
1999) and SNoW (Mu˜noz et al., 1999).
MBSL is a memory-based system which
records, for each POS sequence containing a
border (left, right, or both) of a base-NP, the
number of times it appears with that border
vs. the number of times it appears without
it. It is possible to set an upper limit on the
length of the POS sequences.
Given a sentence, represented by a sequence
of POS tags, the system examines each sub-
sequence for being a base-NP. This is done
by attempting to tile it using POS sequences
that appeared in the training data with the
base-NP borders at the same locations.
For the purpose of the present work, suffice
it to mention that one of the parameters is
the context size (c). It denotes the maximal
number of words considered before or after a
base-NP when recording sub-sequences con-
taining a border.
SNoW (Roth, 1998, “Sparse Network of
Winnow”) is a network architecture of Win-
now classifiers (Littlestone, 1988). Winnow
is a mistake-driven algorithm for learning a
linear separator, in which feature weights are
updated by multiplication. The Winnow al-
gorithm is known for being able to learn well
even in the presence of many noisy features.
The features consist of one to four consec-
utive POSs in a 3-word window around each
POS. Each word is classified as a beginning of
a base-NP, as an end, or neither.
</bodyText>
<subsectionHeader confidence="0.998031">
4.3 Sampling Method
</subsectionHeader>
<bodyText confidence="0.999636703703704">
In generating the training samples we sampled
complete sentences. In MBSL, an un-marked
boundary may be counted as a negative ex-
ample for the POS-subsequences which con-
tains it. Therefore, sampling only part of the
base-NPs in a sentence will generate negative
examples.
For SNoW, each word is an example, but
most of the words are neither a beginning nor
an end of a base-NP. Random sampling of
words might generate a sample with an im-
proper balance between the three classes.
To avoid these problems, we sampled
full sentences instead of words or instances.
Within a good approximation, it can be as-
sumed that base-NP patterns in a sentence do
not correlate. The base-NP instances drawn
from the sampled sentences can therefore be
regarded as independent.
As described at the end of Sec. 4.1, the
WSJ20a and WSJ20b data were created so
that they contain 613 instances, like the ATIS
data. In practice, the number of instances
exceeds 613 slightly due to the full-sentence
constraint. For the purpose of this work, it is
enough that their size is very close to the size
of ATIS.
</bodyText>
<table confidence="0.999402333333333">
Dataset Sentences Base-NPs
Training 8938 f 48 54763 f 2
Unique: 5648 f 34
</table>
<tableCaption confidence="0.803225">
Table 2: Sentence and instant counts for the
bootstrap samples. The second line refers to
unique sentences in the training data.
</tableCaption>
<bodyText confidence="0.995837583333333">
We used the WSJ15-18 dataset for train-
ing. This dataset contains n0 = 54760 base-
NP instances. The number of instances in a
bootstrap sample depends on the number of
instances in the last sampled sentence. As
Table 2 shows, it is slightly more than n0.
For k-CV sampling, the data were divided
into k random distinct parts, each containing
k f2 instances. Table 3 shows the number of
recall samples in each experiment (MBSL and
SNoW experiments were carried out seper-
ately).
</bodyText>
<table confidence="0.998629666666667">
Method MBSL SNoW
Bootstrap 2200 1000
CV (total folds) 1500 1000
</table>
<tableCaption confidence="0.9863825">
Table 3: Number of bootstrap samples and
total CV folds.
</tableCaption>
<subsectionHeader confidence="0.805278">
4.4 Experiments
</subsectionHeader>
<bodyText confidence="0.98110985483871">
We trained SNoW and MBSL; the latter us-
ing context sizes of c=1 and c=3. Data sets
WSJ20, ATIS, WSJ20a, and WSJ20b were
used for testing. MBSL runs with the two
c values were conducted on the same training
samples, therefore it is possible to compare
their results directly.
Each run yielded recall and precision. Re-
call may be viewed as the expected 0-1 loss-
function on the given test sample of instances.
Precision, on the other hand, may be viewed
as the expected 0-1 loss on the sample of in-
stances detected by the learning system. Care
should be taken when discussing the distribu-
tion of precision values because this sample
varies from run to run. We will therefore only
analyse the distribution of recall in this work.
In the following, r1 and r3 denote recall
samples of MBSL with c = 1 and c = 3,
with standard deviations a1 and a3. ρ13 de-
notes the cross-correlation between r1 and r3.
SNoW recall and standard deviation will be
SN
denoted by rSN and a
To approach the questions raised in the in-
troduction we made the following measure-
ments:
Q1: System comparison was addressed by
comparing r1 and r3 on the same test data.
With samples at hand, we obtained an esti-
mate of P(r3 &gt; r1).
Q2: We studied training and test adequacy
through the effect of more specific features on
recall, and on its standard deviation.
Setting c = 3 takes into account sequences
with context of two and three words in ad-
dition to those with c = 1. Sequences with
larger context are more specific, and an im-
provement in recall implies that they are in-
formative in the test data as well.
For particular choices of parameters and
test data, the recall spread yields an estimate
of the training sampling noise. On inade-
quate data, where the statistics differ signif-
icantly from those in the training data, even
small changes in the model can lead to a no-
ticeable difference in recall. This is because
the model relies on statistics which appear
relatively rarely in the test data. Not only
do these statistics provide little information
about the problem, but even small differences
in weighting them are relatively influential.
Therefore, the more training and test data
differ from each other, the more spread we can
expect in results.
Q3: For comparing test data sets with a
system, we used cross-correlations between r1,
r3, or rSN samples obtained on these data sets.
We know that WSJ data are different from
ATIS data, and so expect the results on WSJ
to correlate with ATIS results less than with
other WSJ results.
</bodyText>
<sectionHeader confidence="0.997876" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.990502934426229">
For each of the five test datasets, Table 4 re-
ports averages and standard deviations of r1,
r3, and rSN obtained by 3, 5, 10, and 20-fold
cross-validation, and by bootstrap. ρ13 and
P(r3 &gt; r1) are reported as well.
We discuss our results by considering to
what extent they provide information for an-
swering the three questions:
Q1 – Comparing systems on given data:
For the WSJ data sets, the difference between
r3 and r1 was well above their standard de-
viations, and r3 &gt; r1 nearly always. For
ATIS, the standard deviation of the differ-
ence (σ2r3−r1 = (σ1)2 + (σ3)2 − 2σ1σ3 · p13)
was small due to the high p13, and r1 &gt; r3
nearly always.
Q2 – The adequacy of training and test
sets: It is clear that adding more specific
features, by increasing the context, improved
recall on the WSJ test data and degraded it
on the ATIS data. This is likely to be an indi-
cation of the difference in syntactic structure
between ATIS and WSJ texts.
Another evidence of structural difference
comes from standard deviations. The spread
of the ATIS results always exceeded that of
the WSJ results, with all three experiments.
That difference cannot be solely attributed
to the small size of ATIS, since WSJ20a
and WSJ20b results displayed a much smaller
spread. Indeed, these results had a wider
standard deviation than WSJ20, probably
due to the smaller size, but not as wide as
ATIS. This indicates that base-NPs in ATIS
text have different characteristics than those
in WSJ texts.
Q3 – Comparing datasets by a system:
Table 5 reports, for each pair of datasets, the
correlation between the 5-fold CV recall sam-
ples of each experiment on these datasets.
The correlations change with CV fold num-
ber, 5-fold results were chosen as they repre-
sent intermediary values.
Both MBSL experiments yielded negligible
correlations of ATIS results with any WSJ
data set, whether large or small. These corre-
lations were always weaker than with WSJ20a
and WSJ20b, which are about the same size.
This is due to ATIS being a different kind
of text. The correlation between WSJ20a and
WSJ20b results was also weak. This may be
due to their small sizes; these texts might not
share enough features to make a significant
correlation.
SNoW results were highly correlated for all
pairs. That behaviour is markedly different
from the MBSL results, and indicates a high
level of noise in the SNoW features. Indeed,
Winnow is able to learn well in the presence
of noise, but that noise causes the high corre-
lations observed here.
</bodyText>
<subsectionHeader confidence="0.982309">
5.1 Further Observations
</subsectionHeader>
<bodyText confidence="0.999866777777778">
The decrease of p13 with CV fold number is
related to stabilization of the system. As the
folds become larger, training samples become
more similar to each other, and the spread of
results decreases. This effect was not visible
in the SNoW data, most likely due to the high
level of noise in the features. This noise also
contributes to the higher standard deviation
of SNoW results.
</bodyText>
<sectionHeader confidence="0.827666" genericHeader="conclusions">
6 Summary and Further Research
</sectionHeader>
<bodyText confidence="0.999955333333333">
In this work, we used the distribution of re-
call to address questions concerning base-NP
learning systems and corpora. Two of these
questions, of training and test adequacy, and
of comparing data sets using NLP systems,
were not addressed before.
The recall distributions were obtained using
CV and bootstrap resampling.
We found differences between algorithms
with similar recall, related to the features they
use.
We demonstrated that using an inadequate
test set may lead to noisy performance results.
This effect was observed with two different
learning algorithms. We also reported a case
when changing a parameter of a learning al-
gorithm improved results on one dataset but
degraded results on another.
We used classifiers as “similarity rulers”,
for producing a similarity measure between
datasets. Classifiers may have various prop-
erties as similarity rulers, even when their re-
calls are similar. Each classifier should be
scaled differently according to its noise level.
This demonstrates the way we can use clas-
sifiers to study data, as well as use data to
study classifiers.
</bodyText>
<table confidence="0.999908185185185">
Test Method MBSL SNoW
data
E(r1) f Q1 E(r3) f Q3 ρ13 P(r3 &gt; r1) E(rSN) f QSN
3-CV 89.64 f 0.16 91.26 f 0.12 0.36 100% 90.18 f 1.01
5-CV 89.75 f 0.14 91.43 f 0.10 0.30 100% 90.37 f 1.03
WSJ 20 10-CV 89.80 f 0.12 91.53 f 0.08 0.25 100% 90.47 f 1.11
20-CV 89.81 f 0.11 91.56 f 0.07 0.28 100% 90.51 f 1.19
Bootstrap 89.58 f 0.17 91.16 f 0.14 0.42 100% 89.83 f 0.93
E(·) 89.74 91.58 91.23
3-CV 85.70 f 2.03 83.99 f 1.87 0.82 3% 83.70 f 4.11
5-CV 85.76 f 1.87 83.69 f 1.57 0.79 1% 83.53 f 4.52
ATIS 10-CV 85.90 f 1.31 84.78 f 0.92 0.78 4% 83.38 f 5.14
20-CV 85.78 f 1.16 83.28 f 0.85 0.77 0% 83.23 f 5.36
Bootstrap 85.72 f 1.95 84.69 f 1.95 0.81 16% 83.50 f 3.35
E(·) 85.81 83.20 85.48
3-CV 89.45 f 0.42 91.25 f 0.56 0.33 100% 90.84 f 1.04
5-CV 89.66 f 0.36 91.64 f 0.54 0.32 100% 91.07 f 1.15
WSJ 20a 10-CV 89.79 f 0.28 91.85 f 0.49 0.20 100% 91.14 f 1.26
20-CV 89.82 f 0.23 91.89 f 0.44 0.18 100% 91.11 f 1.39
Bootstrap 89.42 f 0.47 91.55 f 0.57 0.33 99% 90.76 f 1.00
E(·) 89.73 92.18 90.07
3-CV 88.95 f 0.41 90.12 f 0.39 0.37 99% 89.79 f 0.81
5-CV 89.03 f 0.36 90.15 f 0.31 0.31 99% 89.81 f 0.84
WSJ 20b 10-CV 89.06 f 0.33 90.14 f 0.22 0.28 99% 89.83 f 0.86
20-CV 89.07 f 0.27 90.13 f 0.18 0.22 100% 89.87 f 0.88
Bootstrap 89.00 f 0.44 90.17 f 0.44 0.38 98% 89.93 f 0.80
E(·) 89.01 91.55 90.79
</table>
<tableCaption confidence="0.960344">
Table 4: Recall statistic summary for MBSL with contexts c = 1 and c = 3, and SNoW. The
E(·) figures were obtained using the full training set. Note the monotonic change of standard
deviation with fold number. The s.d. of the bootstrap samples are closest to those of low-fold
CV samples.
</tableCaption>
<table confidence="0.9994916">
5-CV WSJ 20b WSJ 20a ATIS
r1 r3 rSN r1 r3 rSN r1 r3 rSN
WSJ 20 0.33 0.19 0.72 0.26 0.29 0.78 0.08 0.02 0.76
ATIS -0.01 0.00 0.59 0.02 -0.01 0.63
WSJ 20a 0.07 0.04 0.59
</table>
<tableCaption confidence="0.9915265">
Table 5: Cross-correlations between recalls of the three experiments on the test data for five-fold
CV. Correlations of r1 capture dataset similarity in the best way.
</tableCaption>
<bodyText confidence="0.999979333333333">
By using MBSL with different context sizes,
our results provide insights into the relation
between training and test data sets, in terms
of general and specific features. That issue be-
comes important when one plans to use a sys-
tem trained on certain data set for analysing
an arbitrary text. Another approach to this
topic, examining the effect of using lexical
bigram information, which is very corpus-
specific, appears in (Gildea, 2001).
In our experiments with systems trained on
WSJ data, there was a clear difference be-
tween their behaviour on other WSJ data and
on the ATIS data set, in which the structure
of base-NPs is different. That difference was
observed with correlations and standard devi-
ations. This shows that resampling the train-
ing data is essential for noticing these struc-
ture differences.
To control the effect of small size of the
ATIS dataset, we provided two equally-small
WSJ data sets. The effect of different genres
was stronger than that of the small-size.
In future study, it would be helpful to study
the distribution of recall using training and
test data from a few genres, across genres,
and on combinations (e.g. “known-similarity
corpora” (Kilgarriff and Rose, 1998)). This
will provide a measure of the transferability
of a model.
We would like to study whether there is a
relation between bootstrap and 2 or 3-CV re-
sults. The average number of unique base-
NPs in a random bootstrap training sample
is about 63% of the total training instances
(Table 2). That corresponds roughly to the
size of a 3-CV training sample. More work is
required to see whether this relation between
bootstrap and low-fold CV is meaningful.
We also plan to study the distribution of
precision. As mentioned in Sec. 4.4, the pre-
cisions of different runs are now taken from
different sample spaces. This makes the boot-
strap estimator unsuitable, and more study is
required to overcome this problem.
</bodyText>
<sectionHeader confidence="0.992304" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999884376811594">
S. Argamon, I. Dagan, and Y. Krymolowski. 1999.
A memory-based approach to learning shallow
natural language patterns. Journal of Experi-
mental and Theoretical AI, 11:369–390. CMP-
LG/9806011.
T. G. Dietterich. 1998. Approximate statisti-
cal tests for comparing supervised classifica-
tion learning algorithms. Neural Computation,
10(7).
Bradley Efron and Gail Gong. 1983. A leisurely
look at the bootstrap, the jackknife, and cross-
validation. Am. Stat., 37(1):36–48.
Bradley Efron and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman and
Hall, New York.
Daniel Gildea. 2001. Corpus variation and parser
performance. In Proc. 2001 Conf. on Empir-
ical Methods in Natural Language Processing
(EMNLP–2001), Carnegie Mellon University,
Pittsburgh, June. ACL-SIGDAT.
Adam Kilgarriff and Tony Rose. 1998. Mea-
sures for corpus similarity and homogeneity. In
Proc. 3rd Conf. on Empirical Methods in Nat-
ural Language Processing (EMNLP–3), pages
46–52, Granada, Spain, June. ACL-SIGDAT.
Ron Kohavi. 1995. A study of cross-validation
and bootstrap for accuracy estimation and
model selection. In proceedings of the Inter-
national Joint Conference on Artificial Intelli-
gence, pages 1137–1145.
B. LeBaron and A. S. Weigend. 1998. A boot-
strap evaluation of the effect of data splitting
on financial time series. IEEE Transactions on
Neural Networks, 9(1):213–220, January.
N. Littlestone. 1988. Learning quickly when
irrelevant attributes abound: A new linear-
threshold algorithm. Machine Learning, 2:285–
318.
M. P. Marcus, B. Santorini, and
M. Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313–330,
June.
J. Martin and D. Hirschberg. 1996. Small sample
statistics for classification error rates II: Confi-
dence intervals and significance tests. Technical
report, Dept. of Information and Computer Sci-
ence, University of California, Irvine. Technical
Report 96-22.
M. Mu˜noz, V. Punyakanok, D. Roth, and D. Zi-
mak. 1999. A learning approach to shallow
parsing. In EMNLP-VLC’99, the Joint SIG-
DAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Cor-
pora, pages 168–178, June.
L. A. Ramshaw and M. P. Marcus. 1995. Text
chunking using transformation-based learning.
In Proceedings of the Third Workshop on Very
Large Corpora.
D. Roth. 1998. Learning to resolve natural lan-
guage ambiguities: A unified approach. In
proc. of the Fifteenth National Conference on
Artificial Intelligence, pages 806–813, Menlo
Park, CA, USA, July. AAAI Press.
Alexander Yeh. 2000. More accurate tests for
the statistical significance of result differences.
In 18th International Conference on Computa-
tional Linguistics (COLING), pages 947–953,
July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.336402">
<title confidence="0.962533">Using the Distribution of Performance for Studying Statistical NLP Systems and Corpora</title>
<author confidence="0.751095">Yuval</author>
<affiliation confidence="0.7509375">Department of Mathematics and Computer Bar-Ilan</affiliation>
<address confidence="0.986958">52900 Ramat Gan, Israel</address>
<abstract confidence="0.9915715">Statistical NLP systems are frequently evaluated and compared on the basis of their performances on a single split of training and test data. Results obtained using a single split are, however, subject to sampling noise. In this paper we argue in favour of reporting a distribution of performance figures, obtained by resampling the training data, rather than a single number. The additional information from distributions can be used to make statistically quantified statements about differences across parameter settings, systems, and corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>I Dagan</author>
<author>Y Krymolowski</author>
</authors>
<title>A memory-based approach to learning shallow natural language patterns.</title>
<date>1999</date>
<booktitle>Journal of Experimental and Theoretical AI,</booktitle>
<pages>11--369</pages>
<contexts>
<context position="7853" citStr="Argamon et al., 1999" startWordPosition="1293" endWordPosition="1296"> that in the WSJ data. We expect this difference to be reflected in the recall of systems tested on both data sets. The small size of the ATIS data can influence the results as well. To distinguish the size effect from the structural differences, we drew two equally small samples from WSJ Section 20. These samples, WSJ20a and WSJ20b, consist of the first 100 and the following 93 sentences respectively. There is a slight difference in size because sentences were kept complete, as explained Section 4.3. 4.2 Learning Algorithms We evaluated base-NP learning systems based on two algorithms: MBSL (Argamon et al., 1999) and SNoW (Mu˜noz et al., 1999). MBSL is a memory-based system which records, for each POS sequence containing a border (left, right, or both) of a base-NP, the number of times it appears with that border vs. the number of times it appears without it. It is possible to set an upper limit on the length of the POS sequences. Given a sentence, represented by a sequence of POS tags, the system examines each subsequence for being a base-NP. This is done by attempting to tile it using POS sequences that appeared in the training data with the base-NP borders at the same locations. For the purpose of </context>
</contexts>
<marker>Argamon, Dagan, Krymolowski, 1999</marker>
<rawString>S. Argamon, I. Dagan, and Y. Krymolowski. 1999. A memory-based approach to learning shallow natural language patterns. Journal of Experimental and Theoretical AI, 11:369–390. CMPLG/9806011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<volume>10</volume>
<issue>7</issue>
<contexts>
<context position="2402" citStr="Dietterich (1998)" startWordPosition="378" endWordPosition="379">, will this be the case on dataset Y2 as well? The answers to these questions can provide useful insight into statistical NLP systems. In particular, about sensitivity to features in the training data, and transferability. These properties can be different even when similar performance is reported. A statistical treatment of Question 1 is presented by Yeh (2000). He tests for the significance of performance differences on fixed training and test data sets. In other related works, Martin and Hirschberg (1996) provides an overview of significance tests of error differences in small samples, and Dietterich (1998) discusses results of a number of tests. Questions 2 and 3 have been frequently raised in NLP, but not explicitly addressed, since the prevailing evaluation methods provide no means of addressing them. In this paper we propose addressing all three questions with a single experimental methodology, which uses the distribution of recall. 2 Motivation Words, parts-of-speech (POS), words, or any feature in text may be regarded as outcomes of a statistical process. Therefore, word counts, count ratios, and other data used in creating statistical NLP models are statistical quantities as well, and as </context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>T. G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10(7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Gail Gong</author>
</authors>
<title>A leisurely look at the bootstrap, the jackknife, and crossvalidation.</title>
<date>1983</date>
<journal>Am. Stat.,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="4292" citStr="Efron and Gong, 1983" startWordPosition="685" endWordPosition="688">sed on a single run or a few runs do not take these noises into account. Because we cannot assign the resulting statements a confidence measure, they are more qualitative than quantitative. The degree to which we can accept such statements depends on the noise level and more generally, on the distribution of performance. In this paper, we use recall as a performance measure (cf. Section 4.4 and Section 3.2 in (Yeh, 2000)). Recall samples are obtained by resampling from training data and training classifiers on these samples. The resampling methods used here are cross-validation and bootstrap (Efron and Gong, 1983; Efron and Tibshirani, 1993, cf. Section 3). Section 4 presents the experimental goals and setup. Results are presented and discussed in Section 5, and a summary is provided in Section 6. 3 The Bootstrap Method The bootstrap is a re-sampling technique designed for obtaining empirical distributions of estimators. It can be thought of as a smoothed version of k-fold crossvalidation (CV). The method has been applied to decision tree and bayesian classifiers by Kohavi (1995) and to neural networks by, e.g., LeBaron and Weigend (1998). In this paper, we use the bootstrap method to obtain the distr</context>
</contexts>
<marker>Efron, Gong, 1983</marker>
<rawString>Bradley Efron and Gail Gong. 1983. A leisurely look at the bootstrap, the jackknife, and crossvalidation. Am. Stat., 37(1):36–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap. Chapman and Hall,</title>
<date>1993</date>
<location>New York.</location>
<contexts>
<context position="4320" citStr="Efron and Tibshirani, 1993" startWordPosition="689" endWordPosition="692"> a few runs do not take these noises into account. Because we cannot assign the resulting statements a confidence measure, they are more qualitative than quantitative. The degree to which we can accept such statements depends on the noise level and more generally, on the distribution of performance. In this paper, we use recall as a performance measure (cf. Section 4.4 and Section 3.2 in (Yeh, 2000)). Recall samples are obtained by resampling from training data and training classifiers on these samples. The resampling methods used here are cross-validation and bootstrap (Efron and Gong, 1983; Efron and Tibshirani, 1993, cf. Section 3). Section 4 presents the experimental goals and setup. Results are presented and discussed in Section 5, and a summary is provided in Section 6. 3 The Bootstrap Method The bootstrap is a re-sampling technique designed for obtaining empirical distributions of estimators. It can be thought of as a smoothed version of k-fold crossvalidation (CV). The method has been applied to decision tree and bayesian classifiers by Kohavi (1995) and to neural networks by, e.g., LeBaron and Weigend (1998). In this paper, we use the bootstrap method to obtain the distribution of performance of a </context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>Bradley Efron and Robert J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman and Hall, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proc. 2001 Conf. on Empirical Methods in Natural Language Processing (EMNLP–2001),</booktitle>
<publisher>ACL-SIGDAT.</publisher>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh,</location>
<contexts>
<context position="19994" citStr="Gildea, 2001" startWordPosition="3456" endWordPosition="3457">.04 0.59 Table 5: Cross-correlations between recalls of the three experiments on the test data for five-fold CV. Correlations of r1 capture dataset similarity in the best way. By using MBSL with different context sizes, our results provide insights into the relation between training and test data sets, in terms of general and specific features. That issue becomes important when one plans to use a system trained on certain data set for analysing an arbitrary text. Another approach to this topic, examining the effect of using lexical bigram information, which is very corpusspecific, appears in (Gildea, 2001). In our experiments with systems trained on WSJ data, there was a clear difference between their behaviour on other WSJ data and on the ATIS data set, in which the structure of base-NPs is different. That difference was observed with correlations and standard deviations. This shows that resampling the training data is essential for noticing these structure differences. To control the effect of small size of the ATIS dataset, we provided two equally-small WSJ data sets. The effect of different genres was stronger than that of the small-size. In future study, it would be helpful to study the di</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proc. 2001 Conf. on Empirical Methods in Natural Language Processing (EMNLP–2001), Carnegie Mellon University, Pittsburgh, June. ACL-SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Tony Rose</author>
</authors>
<title>Measures for corpus similarity and homogeneity.</title>
<date>1998</date>
<booktitle>In Proc. 3rd Conf. on Empirical Methods in Natural Language Processing (EMNLP–3),</booktitle>
<pages>46--52</pages>
<publisher>ACL-SIGDAT.</publisher>
<location>Granada, Spain,</location>
<contexts>
<context position="20758" citStr="Kilgarriff and Rose, 1998" startWordPosition="3580" endWordPosition="3583">TIS data set, in which the structure of base-NPs is different. That difference was observed with correlations and standard deviations. This shows that resampling the training data is essential for noticing these structure differences. To control the effect of small size of the ATIS dataset, we provided two equally-small WSJ data sets. The effect of different genres was stronger than that of the small-size. In future study, it would be helpful to study the distribution of recall using training and test data from a few genres, across genres, and on combinations (e.g. “known-similarity corpora” (Kilgarriff and Rose, 1998)). This will provide a measure of the transferability of a model. We would like to study whether there is a relation between bootstrap and 2 or 3-CV results. The average number of unique baseNPs in a random bootstrap training sample is about 63% of the total training instances (Table 2). That corresponds roughly to the size of a 3-CV training sample. More work is required to see whether this relation between bootstrap and low-fold CV is meaningful. We also plan to study the distribution of precision. As mentioned in Sec. 4.4, the precisions of different runs are now taken from different sample</context>
</contexts>
<marker>Kilgarriff, Rose, 1998</marker>
<rawString>Adam Kilgarriff and Tony Rose. 1998. Measures for corpus similarity and homogeneity. In Proc. 3rd Conf. on Empirical Methods in Natural Language Processing (EMNLP–3), pages 46–52, Granada, Spain, June. ACL-SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kohavi</author>
</authors>
<title>A study of cross-validation and bootstrap for accuracy estimation and model selection.</title>
<date>1995</date>
<booktitle>In proceedings of the International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1137--1145</pages>
<contexts>
<context position="4768" citStr="Kohavi (1995)" startWordPosition="766" endWordPosition="767">data and training classifiers on these samples. The resampling methods used here are cross-validation and bootstrap (Efron and Gong, 1983; Efron and Tibshirani, 1993, cf. Section 3). Section 4 presents the experimental goals and setup. Results are presented and discussed in Section 5, and a summary is provided in Section 6. 3 The Bootstrap Method The bootstrap is a re-sampling technique designed for obtaining empirical distributions of estimators. It can be thought of as a smoothed version of k-fold crossvalidation (CV). The method has been applied to decision tree and bayesian classifiers by Kohavi (1995) and to neural networks by, e.g., LeBaron and Weigend (1998). In this paper, we use the bootstrap method to obtain the distribution of performance of a system which learns to identify non-recursive noun-phrases (base-NPs). While there are a few refinements of the method, the intention of this paper is to present the benefits of obtaining distributions, rather than optimising bias or variance. We do not aim to study the properties of bootstrap estimation. Let a statistic S = S(x1, ... , xn) be a function of the independent observations {xi}ni=1 of a statistical variable X. The bootstrap method </context>
</contexts>
<marker>Kohavi, 1995</marker>
<rawString>Ron Kohavi. 1995. A study of cross-validation and bootstrap for accuracy estimation and model selection. In proceedings of the International Joint Conference on Artificial Intelligence, pages 1137–1145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B LeBaron</author>
<author>A S Weigend</author>
</authors>
<title>A bootstrap evaluation of the effect of data splitting on financial time series.</title>
<date>1998</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="4828" citStr="LeBaron and Weigend (1998)" startWordPosition="774" endWordPosition="777"> The resampling methods used here are cross-validation and bootstrap (Efron and Gong, 1983; Efron and Tibshirani, 1993, cf. Section 3). Section 4 presents the experimental goals and setup. Results are presented and discussed in Section 5, and a summary is provided in Section 6. 3 The Bootstrap Method The bootstrap is a re-sampling technique designed for obtaining empirical distributions of estimators. It can be thought of as a smoothed version of k-fold crossvalidation (CV). The method has been applied to decision tree and bayesian classifiers by Kohavi (1995) and to neural networks by, e.g., LeBaron and Weigend (1998). In this paper, we use the bootstrap method to obtain the distribution of performance of a system which learns to identify non-recursive noun-phrases (base-NPs). While there are a few refinements of the method, the intention of this paper is to present the benefits of obtaining distributions, rather than optimising bias or variance. We do not aim to study the properties of bootstrap estimation. Let a statistic S = S(x1, ... , xn) be a function of the independent observations {xi}ni=1 of a statistical variable X. The bootstrap method constructs the distribution function of S by successively re</context>
</contexts>
<marker>LeBaron, Weigend, 1998</marker>
<rawString>B. LeBaron and A. S. Weigend. 1998. A bootstrap evaluation of the effect of data splitting on financial time series. IEEE Transactions on Neural Networks, 9(1):213–220, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Littlestone</author>
</authors>
<title>Learning quickly when irrelevant attributes abound: A new linearthreshold algorithm.</title>
<date>1988</date>
<booktitle>Machine Learning,</booktitle>
<pages>2--285</pages>
<contexts>
<context position="8784" citStr="Littlestone, 1988" startWordPosition="1458" endWordPosition="1459"> sequences. Given a sentence, represented by a sequence of POS tags, the system examines each subsequence for being a base-NP. This is done by attempting to tile it using POS sequences that appeared in the training data with the base-NP borders at the same locations. For the purpose of the present work, suffice it to mention that one of the parameters is the context size (c). It denotes the maximal number of words considered before or after a base-NP when recording sub-sequences containing a border. SNoW (Roth, 1998, “Sparse Network of Winnow”) is a network architecture of Winnow classifiers (Littlestone, 1988). Winnow is a mistake-driven algorithm for learning a linear separator, in which feature weights are updated by multiplication. The Winnow algorithm is known for being able to learn well even in the presence of many noisy features. The features consist of one to four consecutive POSs in a 3-word window around each POS. Each word is classified as a beginning of a base-NP, as an end, or neither. 4.3 Sampling Method In generating the training samples we sampled complete sentences. In MBSL, an un-marked boundary may be counted as a negative example for the POS-subsequences which contains it. There</context>
</contexts>
<marker>Littlestone, 1988</marker>
<rawString>N. Littlestone. 1988. Learning quickly when irrelevant attributes abound: A new linearthreshold algorithm. Machine Learning, 2:285– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="6310" citStr="Marcus et al., 1993" startWordPosition="1030" endWordPosition="1033">estimating the standard deviation, bias, or confidence interval of S. In the present work, xi are the base-NP instances in a given corpus, and the statistic S is the recall on a test set. 4 Experimental Setup The aim of our experiments is to test whether the recall distribution can be helpful in answering the questions Q1–Q3 mentioned in the introduction of this paper. The data and learning algorithms are presented in Sections 4.1 and 4.2. Section 4.3 describes the sampling method in detail. Section 4.4 motivates the use of recall and describes the experiments. 4.1 Data We used Penn-Treebank (Marcus et al., 1993) data, presented in Table 1. Wall-Street Journal (WSJ) Sections 15-18 and 20 were used by Ramshaw and Marcus (1995) as training and test data respectively for evaluating their base-NP chunker. These data have since become a standard for evaluating base-NP systems. The WSJ texts are economic newspaper reports, which often include elaborated sentences containing about six base-NPs on the Source Sentences Words Base NPs WSJ 15-18 8936 229598 54760 WSJ 20 2012 51401 12335 ATIS 190 2046 613 WSJ 20a 100 2479 614 WSJ 20b 93 2661 619 Table 1: Data sources average. The ATIS data, on the other hand, are</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Martin</author>
<author>D Hirschberg</author>
</authors>
<title>Small sample statistics for classification error rates II: Confidence intervals and significance tests.</title>
<date>1996</date>
<tech>Technical report,</tech>
<institution>Dept. of Information and Computer Science, University of California, Irvine.</institution>
<contexts>
<context position="2298" citStr="Martin and Hirschberg (1996)" startWordPosition="361" endWordPosition="364"> Comparing data sets with a given system: If a different training set improves the result of system A on dataset Y1, will this be the case on dataset Y2 as well? The answers to these questions can provide useful insight into statistical NLP systems. In particular, about sensitivity to features in the training data, and transferability. These properties can be different even when similar performance is reported. A statistical treatment of Question 1 is presented by Yeh (2000). He tests for the significance of performance differences on fixed training and test data sets. In other related works, Martin and Hirschberg (1996) provides an overview of significance tests of error differences in small samples, and Dietterich (1998) discusses results of a number of tests. Questions 2 and 3 have been frequently raised in NLP, but not explicitly addressed, since the prevailing evaluation methods provide no means of addressing them. In this paper we propose addressing all three questions with a single experimental methodology, which uses the distribution of recall. 2 Motivation Words, parts-of-speech (POS), words, or any feature in text may be regarded as outcomes of a statistical process. Therefore, word counts, count ra</context>
</contexts>
<marker>Martin, Hirschberg, 1996</marker>
<rawString>J. Martin and D. Hirschberg. 1996. Small sample statistics for classification error rates II: Confidence intervals and significance tests. Technical report, Dept. of Information and Computer Science, University of California, Irvine. Technical Report 96-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mu˜noz</author>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>D Zimak</author>
</authors>
<title>A learning approach to shallow parsing.</title>
<date>1999</date>
<booktitle>In EMNLP-VLC’99, the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>168--178</pages>
<marker>Mu˜noz, Punyakanok, Roth, Zimak, 1999</marker>
<rawString>M. Mu˜noz, V. Punyakanok, D. Roth, and D. Zimak. 1999. A learning approach to shallow parsing. In EMNLP-VLC’99, the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 168–178, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="6425" citStr="Ramshaw and Marcus (1995)" startWordPosition="1050" endWordPosition="1053">instances in a given corpus, and the statistic S is the recall on a test set. 4 Experimental Setup The aim of our experiments is to test whether the recall distribution can be helpful in answering the questions Q1–Q3 mentioned in the introduction of this paper. The data and learning algorithms are presented in Sections 4.1 and 4.2. Section 4.3 describes the sampling method in detail. Section 4.4 motivates the use of recall and describes the experiments. 4.1 Data We used Penn-Treebank (Marcus et al., 1993) data, presented in Table 1. Wall-Street Journal (WSJ) Sections 15-18 and 20 were used by Ramshaw and Marcus (1995) as training and test data respectively for evaluating their base-NP chunker. These data have since become a standard for evaluating base-NP systems. The WSJ texts are economic newspaper reports, which often include elaborated sentences containing about six base-NPs on the Source Sentences Words Base NPs WSJ 15-18 8936 229598 54760 WSJ 20 2012 51401 12335 ATIS 190 2046 613 WSJ 20a 100 2479 614 WSJ 20b 93 2661 619 Table 1: Data sources average. The ATIS data, on the other hand, are a collection of customer requests related to flight schedules. These typically include short sentences which conta</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: A unified approach.</title>
<date>1998</date>
<booktitle>In proc. of the Fifteenth National Conference on Artificial Intelligence,</booktitle>
<pages>806--813</pages>
<publisher>AAAI Press.</publisher>
<location>Menlo Park, CA, USA,</location>
<contexts>
<context position="8687" citStr="Roth, 1998" startWordPosition="1444" endWordPosition="1445">imes it appears without it. It is possible to set an upper limit on the length of the POS sequences. Given a sentence, represented by a sequence of POS tags, the system examines each subsequence for being a base-NP. This is done by attempting to tile it using POS sequences that appeared in the training data with the base-NP borders at the same locations. For the purpose of the present work, suffice it to mention that one of the parameters is the context size (c). It denotes the maximal number of words considered before or after a base-NP when recording sub-sequences containing a border. SNoW (Roth, 1998, “Sparse Network of Winnow”) is a network architecture of Winnow classifiers (Littlestone, 1988). Winnow is a mistake-driven algorithm for learning a linear separator, in which feature weights are updated by multiplication. The Winnow algorithm is known for being able to learn well even in the presence of many noisy features. The features consist of one to four consecutive POSs in a 3-word window around each POS. Each word is classified as a beginning of a base-NP, as an end, or neither. 4.3 Sampling Method In generating the training samples we sampled complete sentences. In MBSL, an un-marke</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In proc. of the Fifteenth National Conference on Artificial Intelligence, pages 806–813, Menlo Park, CA, USA, July. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In 18th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>947--953</pages>
<contexts>
<context position="2149" citStr="Yeh (2000)" startWordPosition="340" endWordPosition="341">ng data to test data: Is a system trained on dataset X adequate for analysing dataset Y ? Are features from X indicative in Y ? Q3: Comparing data sets with a given system: If a different training set improves the result of system A on dataset Y1, will this be the case on dataset Y2 as well? The answers to these questions can provide useful insight into statistical NLP systems. In particular, about sensitivity to features in the training data, and transferability. These properties can be different even when similar performance is reported. A statistical treatment of Question 1 is presented by Yeh (2000). He tests for the significance of performance differences on fixed training and test data sets. In other related works, Martin and Hirschberg (1996) provides an overview of significance tests of error differences in small samples, and Dietterich (1998) discusses results of a number of tests. Questions 2 and 3 have been frequently raised in NLP, but not explicitly addressed, since the prevailing evaluation methods provide no means of addressing them. In this paper we propose addressing all three questions with a single experimental methodology, which uses the distribution of recall. 2 Motivati</context>
<context position="4096" citStr="Yeh, 2000" startWordPosition="658" endWordPosition="659">eing dependent on these estimators, have a distribution function which may be difficult to find theoretically. This distribution gives rise to intrinsic noise. Performance comparisons based on a single run or a few runs do not take these noises into account. Because we cannot assign the resulting statements a confidence measure, they are more qualitative than quantitative. The degree to which we can accept such statements depends on the noise level and more generally, on the distribution of performance. In this paper, we use recall as a performance measure (cf. Section 4.4 and Section 3.2 in (Yeh, 2000)). Recall samples are obtained by resampling from training data and training classifiers on these samples. The resampling methods used here are cross-validation and bootstrap (Efron and Gong, 1983; Efron and Tibshirani, 1993, cf. Section 3). Section 4 presents the experimental goals and setup. Results are presented and discussed in Section 5, and a summary is provided in Section 6. 3 The Bootstrap Method The bootstrap is a re-sampling technique designed for obtaining empirical distributions of estimators. It can be thought of as a smoothed version of k-fold crossvalidation (CV). The method has</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In 18th International Conference on Computational Linguistics (COLING), pages 947–953, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>