<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.659171">
Summarization Through Submodularity and Dispersion
</title>
<author confidence="0.63752">
Anirban Dasgupta Ravi Kumar Sujith Ravi
</author>
<affiliation confidence="0.527711">
Yahoo! Labs Google Google
</affiliation>
<address confidence="0.557195">
Sunnyvale, CA 95054 Mountain View, CA 94043 Mountain View, CA 94043
</address>
<email confidence="0.89651">
anirban@yahoo-inc.com tintin@google.com sravi@gooogle.com
</email>
<sectionHeader confidence="0.992985" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999819578947369">
We propose a new optimization frame-
work for summarization by generalizing
the submodular framework of (Lin and
Bilmes, 2011). In our framework the sum-
marization desideratum is expressed as a
sum of a submodular function and a non-
submodular function, which we call dis-
persion; the latter uses inter-sentence dis-
similarities in different ways in order to
ensure non-redundancy of the summary.
We consider three natural dispersion func-
tions and show that a greedy algorithm
can obtain an approximately optimal sum-
mary in all three cases. We conduct ex-
periments on two corpora—DUC 2004
and user comments on news articles—and
show that the performance of our algo-
rithm outperforms those that rely only on
submodularity.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999705">
Summarization is a classic text processing prob-
lem. Broadly speaking, given one or more doc-
uments, the goal is to obtain a concise piece
of text that contains the most salient points in
the given document(s). Thanks to the om-
nipresent information overload facing all of us,
the importance of summarization is gaining; semi-
automatically summarized content is increasingly
becoming user-facing: many newspapers equip
editors with automated tools to aid them in choos-
ing a subset of user comments to show. Summa-
rization has been studied for the past in various
settings—a large single document, multiple docu-
ments on the same topic, and user-generated con-
tent.
Each domain throws up its own set of idiosyn-
crasies and challenges for the summarization task.
On one hand, in the multi-document case (say, dif-
ferent news reports on the same event), the text is
often very long and detailed. The precision/recall
requirements are higher in this domain and a se-
mantic representation of the text might be needed
to avoid redundancy. On the other hand, in the
case of user-generated content (say, comments on
a news article), even though the text is short, one
is faced with a different set of problems: volume
(popular articles generate more than 10,000 com-
ments), noise (most comments are vacuous, lin-
guistically deficient, and tangential to the article),
and redundancy (similar views are expressed by
multiple commenters). In both cases, there is a
delicate balance between choosing the salient, rel-
evant, popular, and diverse points (e.g., sentences)
versus minimizing syntactic and semantic redun-
dancy.
While there have been many approaches to au-
tomatic summarization (see Section 2), our work
is directly inspired by the recent elegant frame-
work of (Lin and Bilmes, 2011). They employed
the powerful theory of submodular functions for
summarization: submodularity embodies the “di-
minishing returns” property and hence is a natural
vocabulary to express the summarization desider-
ata. In this framework, each of the constraints (rel-
evance, redundancy, etc.) is captured as a submod-
ular function and the objective is to maximize their
sum. A simple greedy algorithm is guaranteed to
produce an approximately optimal summary. They
used this framework to obtain the best results on
the DUC 2004 corpus.
Even though the submodularity framework is
quite general, it has limitations in its expressiv-
ity. In particular, it cannot capture redundancy
constraints that depend on pairwise dissimilarities
between sentences. For example, a natural con-
straint on the summary is that the sum or the mini-
mum of pairwise dissimilarities between sentences
chosen in the summary should be maximized; this,
unfortunately, is not a submodular function. We
call functions that depend on inter-sentence pair-
</bodyText>
<page confidence="0.963185">
1014
</page>
<note confidence="0.9137035">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1014–1022,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999398">
wise dissimilarities in the summary as dispersion
functions. Our focus in this work is on signif-
icantly furthering the submodularity-based sum-
marization framework to incorporate such disper-
sion functions.
We propose a very general graph-based sum-
marization framework that combines a submod-
ular function with a non-submodular dispersion
function. We consider three natural dispersion
functions on the sentences in a summary: sum
of all-pair sentence dissimilarities, the weight of
the minimum spanning tree on the sentences, and
the minimum of all-pair sentence dissimilarities.
These three functions represent three different
ways of using the sentence dissimilarities. We
then show that a greedy algorithm can obtain ap-
proximately optimal summary in each of the three
cases; the proof exploits some nice combinatorial
properties satisfied by the three dispersion func-
tions. We then conduct experiments on two cor-
pora: the DUC 2004 corpus and a corpus of user
comments on news articles. On DUC 2004, we
obtain performance that matches (Lin and Bilmes,
2011), without any serious parameter tuning; note
that their framework does not have the dispersion
function. On the comment corpus, we outperform
their method, demonstrating that value of disper-
sion functions. As part of our methodology, we
also use a new structured representation for sum-
maries.
</bodyText>
<sectionHeader confidence="0.999731" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.98866405">
Automatic summarization is a well-studied prob-
lem in the literature. Several methods have been
proposed for single- and multi-document summa-
rization (Carbonell and Goldstein, 1998; Con-
roy and O’Leary, 2001; Takamura and Okumura,
2009; Shen and Li, 2010).
Related concepts have also been used in several
other scenarios such as query-focused summariza-
tion in information retrieval (Daum´e and Marcu,
2006), microblog summarization (Sharifi et al.,
2010), event summarization (Filatova, 2004), and
others (Riedhammer et al., 2010; Qazvinian et al.,
2010; Yatani et al., 2011).
Graph-based methods have been used for sum-
marization (Ganesan et al., 2010), but in a dif-
ferent context—using paths in graphs to produce
very short abstractive summaries. For a detailed
survey on existing automatic summarization tech-
niques and other related topics, see (Kim et al.,
2011; Nenkova and McKeown, 2012).
</bodyText>
<sectionHeader confidence="0.988512" genericHeader="method">
3 Framework
</sectionHeader>
<bodyText confidence="0.9999507">
In this section we present the summarization
framework. We start by describing a generic ob-
jective function that can be widely applied to sev-
eral summarization scenarios. This objective func-
tion is the sum of a monotone submodular cov-
erage function and a non-submodular dispersion
function. We then describe a simple greedy algo-
rithm for optimizing this objective function with
provable approximation guarantees for three natu-
ral dispersion functions.
</bodyText>
<subsectionHeader confidence="0.995937">
3.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.987855806451613">
Let C be a collection of texts. Depending on the
summarization application, C can refer to the set
of documents (e.g., newswire) related to a partic-
ular topic as in standard summarization; in other
scenarios (e.g., user-generated content), it is a col-
lection of comments associated with a news article
or a blog post, etc. For each document c E C,
let S(c) denote the set of sentences in c. Let
U = UcECS(c) be the universe of all sentences;
without loss of generality, we assume each sen-
tence is unique to a document. For a sentence
u E U, let C(u) be the document corresponding
to u.
Each u E U is associated with a weight w(u),
which might indicate, for instance, how similar u
is to the main article (and/or the query, in query-
dependent settings). Each pair u, v E U is as-
sociated with a similarity s(u, v) E [0, 1]. This
similarity can then be used to define an inter-
sentence distance d(·,·) as follows: let d&apos;(u, v) =
1 − s(u, v) and define d(u, v) to be the shortest
path distance from u to v in the graph where the
weight of each edge (u, v) is d&apos;(u, v). Note that
d(·, ·) is a metric unlike d&apos;(·, ·), which may not be
a metric. (In addition to being intuitive, d(·, ·) be-
ing a metric helps us obtain guarantees on the al-
gorithm’s output.) For a set S, and a point u E� S,
define d(u, S) = minvES d(u, v).
Let k &gt; 0 be fixed. A summary of U is a subset
S C U, |S |= k. Our aim is to find a summary that
maximizes
</bodyText>
<equation confidence="0.999267">
f(S) = g(S) + δh(S), (1)
</equation>
<bodyText confidence="0.985931">
where g(S) is the coverage function that is non-
negative, monotone, and submodular1, h(S) is a
</bodyText>
<footnote confidence="0.811612">
1A function f : U → R is submodular if for every
</footnote>
<page confidence="0.995345">
1015
</page>
<bodyText confidence="0.982721789473684">
dispersion function, and 6 ≥ 0 is a parameter that
can be used to scale the range of h(·) to be com-
parable to that of g(·).
For two sets S and T, let P be the set of un-
ordered pairs {u, v} where u ∈ S and v ∈ T. Our
focus is on the following dispersion functions: the
sum measure hs(S, T) = E{u,v}EP d(u, v), the
spanning tree measure ht(S, T) given by the cost
of the minimum spanning tree of the set S∪T, and
the min measure hm(S, T) = min{u,v}EP d(u, v).
Note that these functions span from consider-
ing the entire set of distances in S to consider-
ing only the minimum distance in S; also it is
easy to construct examples to show that none of
these functions is submodular. Define h*(u, S) =
h*({u}, S) and h*(S) = h*(S, S).
Let O be the optimal solution of the function
f. A summary S˜ is a γ-approximation if f(˜S) ≥
γf(O).
</bodyText>
<subsectionHeader confidence="0.988808">
3.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.989543071428572">
Maximizing (1) is NP-hard even if 6 = 0 or if
g(·) = 0 (Chandra and Halld´orsson, 2001). For
the special case 6 = 0, since g(·) is submodular,
a classical greedy algorithm obtains a (1 − 1/e)-
approximation (Nemhauser et al., 1978). But if
6 &gt; 0, since the dispersion function h(·) is not
submodular, the combined objective f(·) is not
submodular as well. Despite this, we show that
a simple greedy algorithm achieves a provable ap-
proximation factor for (1). This is possible due to
some nice structural properties of the dispersion
functions we consider.
Algorithm 2 Greedy algorithm, parametrized by
the dispersion function h; here, U, k, g, 6 are fixed.
</bodyText>
<equation confidence="0.99734525">
S0 ← ∅; i ← 0
for i = 0,...,k − 1 do
v ← arg maxuEU\Si g(Si +u)+6h(Si +u)
Si+1 ← Si ∪ {v}
</equation>
<subsectionHeader confidence="0.704062">
end for
3.3 Analysis
</subsectionHeader>
<bodyText confidence="0.99983925">
In this section we obtain a provable approximation
for the greedy algorithm. First, we show that a
greedy choice is well-behaved with respect to the
dispersion function h·(·).
</bodyText>
<construct confidence="0.7000275">
Lemma 1. Let O be any set with |O |= k. If S is
such that |S |= E &lt; k, then
</construct>
<equation confidence="0.670368666666667">
(i) EuEO\S hs(u, S) ≥ |O \ S |�hs(O)
k(k−1);
A, B C U,wehave f(A)+ f(B) ≥ f(AUB)+ f(AnB).
(ii) EuEO\S d(u, S) ≥ 12ht(O) − ht(S); and
(iii) there exists u ∈ O \ S such that hm(u, S) ≥
hm(O)/2.
</equation>
<bodyText confidence="0.986133">
Proof. The proof for (i) follows directly from
Lemma 1 in (Borodin et al., 2012).
To prove (ii) let T be the tree obtained by adding
all points of O \ S directly to their respective clos-
est points on the minimum spanning tree of S. T
is a spanning tree, and hence a Steiner tree, for the
points in set S ∪ O. Hence, cost(T) = ht(S) +
</bodyText>
<equation confidence="0.714557">
E
</equation>
<bodyText confidence="0.949513166666667">
uEO\S d(u, S). Let smt(S) denote the cost of
a minimum Steiner tree of S. Thus, cost(T) ≥
smt(O ∪ S). Since a Steiner tree of O ∪ S is also
a Steiner tree of O, smt(O ∪ S) ≥ smt(O). Since
this is a metric space, smt(O) ≥12ht(O) (see, for
example, (Cieslik, 2001)). Thus,
</bodyText>
<equation confidence="0.718540833333333">
� 1
ht(S) + d(u, S) ≥ 2ht(O)
uEO\S
� 1
⇒ d(u, S) ≥ 2ht(O) − ht(S).
uEO\S
</equation>
<bodyText confidence="0.9993588">
To prove (iii), let O = {u1, ... , uk}. By def-
inition, for every i =6 j, d(ui, uj) ≥ hm(O).
Consider the (open) ball Bi of radius hm(O)/2
around each element ui. By construction for each
i, Bi ∩ O = {ui} and for each pair i =6 j,
Bi ∩ Bj = ∅. Since |S |&lt; k, and there are k balls
Bi, there exists k−E balls Bi such that S∩Bi = ∅,
proving (iii).
We next show that the tree created by the greedy
algorithm for h = ht is not far from the optimum.
</bodyText>
<construct confidence="0.491115666666667">
Lemma 2. Let u1, ... , uk be a sequence ofpoints
and let Si = {uj, j ≤ i}. Then, ht(Sk) ≥
1/logk E2&lt;j&lt;k d(uj, Sj−1).
</construct>
<bodyText confidence="0.999908833333333">
Proof. The proof follows by noting that we get a
spanning tree by connecting each ui to its closest
point in Si−1. The cost of this spanning tree is
E2&lt;j&lt;k d(uj, Sj−1) and this tree is also the re-
sult of the greedy algorithm run in an online fash-
ion on the input sequence {u1, ... , uk}. Using the
result of (Imase and Waxman, 1991), the compet-
itive ratio of this algorithm is log k, and hence the
proof.
We now state and prove the main result about
the quality of approximation of the greedy algo-
rithm.
</bodyText>
<page confidence="0.968265">
1016
</page>
<bodyText confidence="0.987597357142857">
Theorem 3. Fork &gt; 1, there is a polynomial-time
algorithm that obtains a γ-approximation to f(S),
where γ = 1/2 for h = hs, γ = 1/4 for h = hm,
and γ = 1/3 log k for h = ht.
Proof. For hs and ht, we run Algorithm 1 using
a new dispersion function h&apos;, which is a slightly
modified version of h. In particular, for h = hs,
we use h&apos;(S) = 2hs(S). For h = ht, we
abuse notation and define h&apos; to be a function over
an ordered set S = {u1, ... , uk} as follows:
h&apos;(S) = Ej≤|S |d(uj, Sj−1), where Sj−1 =
{u1,... , uj−1}. Let f&apos;(S) = g(S) + δh&apos;(S).
Consider the ith iteration of the algorithm. By
the submodularity of g(·),
</bodyText>
<equation confidence="0.996721">
� g(Si ∪ {u}) − g(Si) (2)
uEO\Si
≥ g(O ∪ Si) − g(Si) ≥ g(O) − g(Sk),
</equation>
<bodyText confidence="0.996552142857143">
where we use monotonicity of g(·) to infer g(O ∪
Si) ≥ g(O) and g(Si) ≤ g(Sk).
For h = hs, the proof follows by Lemma 1(i)
and by Theorem 1 in (Borodin et al., 2012).
For ht, using the above argument of submodu-
larity and monotonicity of g, and the result from
Lemma 1(ii), we have
</bodyText>
<equation confidence="0.9853516">
� g(Si ∪ u) − g(Si) + δd(u, Si)
uEO\Si
≥ g(O) − g(Si) + δ(ht(O)/2 − ht(Si))
≥ (g(O) + δht(O)/2) − (g(Si) + δht(Si))
≥ f(O)/2 − (g(Si) + δht(Si)).
</equation>
<bodyText confidence="0.92031625">
Also, ht(Si) ≤ 2 smt(Si) since this is a met-
ric space. Using the monotonicity of the Steiner
tree cost, smt(Si) ≤ smt(Sk) ≤ ht(Sk). Hence,
ht(Si) ≤ 2ht(Sk). Thus,
</bodyText>
<equation confidence="0.970747777777778">
� g(Si ∪ u) − g(Si) + δd(u, Si)
uEO\Si
≥ f(O)/2 − (g(Si) + δht(Si))
≥ f(O)/2 − (g(Sk) + 2δht(Sk))
≥ f(O)/2 − 2f(Sk). (3)
By the greedy choice of ui+1,
f&apos;(Si ∪ ui+1) − f&apos;(Si)
= g(Si ∪ ui+1) − g(Si) + δd(ui+1, Si)
≥ (f(O)/2 − 2f(Sk))/|O \ Si|
1
≥ k (f(O)/2 − 2f(Sk)).
Summing over all i ∈ [1, k − 1],
f&apos;(Sk) ≥ (k−1)/k(f(O)/2 − 2f(Sk)). (4)
Using Lemma 2 we obtain
f(Sk) = g(Sk) + δht(Sk) ≥ f&apos;(Sk)
log k
1 − 1/k
≥ log k (f(O)/2 − 2f(Sk)).
</equation>
<bodyText confidence="0.976241928571428">
By simplifying, we obtain f(Sk) ≥ f(O)/3 log k.
Finally for hm, we run Algorithm 1 twice: once
with g as given and h ≡ 0, and the second
time with g ≡ 0 and h ≡ hm. Let Sg and
Sh be the solutions in the two cases. Let Og
and Oh be the corresponding optimal solutions.
By the submodularity and monotonicity of g(·),
g(Sg) ≥ (1 − 1/e)g(Og) ≥ g(O9)/2. Similarly,
using Lemma 1(iii), hm(Sh) ≥ hm(Oh)/2 since
in any iteration i &lt; k we can choose an ele-
ment ui+1 such that hm(ui+1, Si) ≥ hm(Oh)/2.
Let S = arg maxXE{S9,Sh} f(X). Using an av-
eraging argument, since g and hm are both non-
negative,
</bodyText>
<equation confidence="0.936848">
f(X) ≥ (f(S9)+f(Sh))/2 ≥ (g(O9)+δhm(Oh))/4.
</equation>
<bodyText confidence="0.8855185">
Since by definition g(Og) ≥ g(O) and hm(Oh) ≥
hm(O), we have a 1/4-approximation.
</bodyText>
<subsectionHeader confidence="0.968065">
3.4 A universal constant-factor
approximation
</subsectionHeader>
<bodyText confidence="0.9999447">
Using the above algorithm that we used for hm,
it is possible to give a universal algorithm that
gives a constant-factor approximation to each of
the above objectives. By running the Algorithm 1
once for g ≡ 0 and next for h ≡ 0 and taking
the best of the two solutions, we can argue that the
resulting set gives a constant factor approximation
to f. We do not use this algorithm in our exper-
iments, as it is oblivious of the actual dispersion
functions used.
</bodyText>
<sectionHeader confidence="0.972519" genericHeader="method">
4 Using the Framework
</sectionHeader>
<bodyText confidence="0.999920428571428">
Next, we describe how the framework described
in Section 3 can be applied to our tasks of interest,
i.e., summarizing documents or user-generated
content (in our case, comments). First, we repre-
sent the elements of interest (i.e., sentences within
comments) in a structured manner by using depen-
dency trees. We then use this representation to
</bodyText>
<page confidence="0.981749">
1017
</page>
<bodyText confidence="0.9998635">
generate a graph and instantiate our summariza-
tion objective function with specific components
that capture the desiderata of a given summariza-
tion task.
</bodyText>
<subsectionHeader confidence="0.987531">
4.1 Structured representation for sentences
</subsectionHeader>
<bodyText confidence="0.999963363636364">
In order to instantiate the summarization graph
(nodes and edges), we first need to model each
sentence (in multi-document summarization) or
comment (i.e., set of sentences) as nodes in the
graph. Sentences have been typically modeled
using standard ngrams (unigrams or bigrams) in
previous summarization work. Instead, we model
sentences using a structured representation, i.e., its
syntax structure using dependency parse trees. We
first use a dependency parser (de Marneffe et al.,
2006) to parse each sentence and extract the set
of dependency relations associated with the sen-
tence. For example, the sentence “I adore tennis”
is represented by the dependency relations (nsubj:
adore, I) and (dobj: adore, tennis).
Each sentence represents a single node u in
the graph (unless otherwise specified) and is com-
prised of a set of dependency relations (or ngrams)
present in the sentence. Furthermore, the edge
weights s(u, v) represent pairwise similarity be-
tween sentences or comments (e.g., similarity be-
tween views expressed in different comments).
The edge weights are then used to define the
inter-sentence distance metric d(u, v) for the dif-
ferent dispersion functions. We identify simi-
lar views/opinions by computing semantic simi-
larity rather than using standard similarity mea-
sures (such as cosine similarity based on ex-
act lexical matches between different nodes in
the graph). For each pair of nodes (u, v) in
the graph, we compute the semantic similarity
score (using WordNet) between every pair of
dependency relation (rel: a, b) in u and v as:
</bodyText>
<equation confidence="0.969768666666667">
�s(u, v) = WN(ai, aj) x WN(bi, bj),
reli∈u,relj ∈v
reli=relj
</equation>
<bodyText confidence="0.944528913043478">
where rel is a relation type (e.g., nsubj) and a, b
are the two arguments present in the dependency
relation (b does not exist for some relations).
WN(wi, wj) is defined as the WordNet similar-
ity score between words wi and wj.2 The edge
weights are then normalized across all edges in the
2There exists various semantic relatedness measures
based on WordNet (Patwardhan and Pedersen, 2006). In our
experiments, for WN we pick one that is based on the path
length between the two words in the WordNet graph.
graph.
This allows us to perform approximate match-
ing of syntactic treelets obtained from the depen-
dency parses using semantic (WordNet) similar-
ity. For example, the sentences “I adore tennis”
and “Everyone likes tennis” convey the same view
and should be assigned a higher similarity score
as opposed to “I hate tennis”. Using the syntac-
tic structure along with semantic similarity helps
us identify useful (valid) nuggets of information
within comments (or documents), avoid redun-
dancies, and identify similar views in a semantic
space.
</bodyText>
<subsectionHeader confidence="0.997747">
4.2 Components of the coverage function
</subsectionHeader>
<bodyText confidence="0.969905375">
Our coverage function is a linear combination of
the following.
(i) Popularity. One of the requirements for a good
summary (especially, for user-generated content)
is that it should include (or rather not miss) the
popular views or opinions expressed by several
users across multiple documents or comments. We
model this property in our objective function as
follows.
For each node u, we define w(u) as the num-
ber of documents |Curel C C |from the collection
such that at least one of the dependency relations
rel E u appeared in a sentence within some doc-
ument c E Curel. The popularity scores are then
normalized across all nodes in the graph. We then
add this component to our objective function as
</bodyText>
<equation confidence="0.633357">
w(S) = EuES w(u).
</equation>
<bodyText confidence="0.956622875">
(ii) Cluster contribution. This term captures the
fact that we do not intend to include multiple sen-
tences from the same comment (or document).
Define B to be the clustering induced by the sen-
tence to comment relation, i.e., two sentences in
the same comment belong to the same cluster. The
corresponding contribution to the objective func-
tion is EBEB |S n B|1/2.
</bodyText>
<listItem confidence="0.735239818181818">
(iii) Content contribution. This term promotes the
diversification of content. We look at the graph of
sentences where the weight of each edge is s(u, v).
This graph is then partitioned based on a local
random walk based method to give us clusters
D = {D1, ... , Dn}. The corresponding contribu-
tion to the objective function is EDED |S nD|1/2.
(iv) Cover contribution. We also measure the
cover of the set S as follows: for each element
s in U first define cover of an element u by a
set S&apos; as cov(u, S&apos;) = EvES, s(u, v).
</listItem>
<bodyText confidence="0.573627">
Then, the
</bodyText>
<page confidence="0.923577">
1018
</page>
<bodyText confidence="0.91685">
cover value of the set S is defined as cov(S) =
</bodyText>
<equation confidence="0.9735545">
&amp; E U)).3
s min(cov(u, s), 0.25cov(u,
</equation>
<bodyText confidence="0.9962494">
Thus, the final coverage function is: g(S) =
w(S) + α EBEB |S ∩ B|1/2 + 0 EDED |S ∩
D|1/2 + Acov(S), where α, 0, A are non-negative
constants. By using the monotone submodularity
of each of the component functions, and the fact
that addition preserves submodularity, the follow-
ing is immediate.
Fact 4. g(S) is a monotone, non-negative, sub-
modular function.
We then apply Algorithm 1 to optimize (1).
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.919678">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999791615384615">
Multi-document summarization. We use the
DUC 2004 corpus4 that comprises 50 clusters (i.e.,
50 different summarization tasks) with 10 docu-
ments per cluster on average. Each document con-
tains multiple sentences and the goal is to produce
a summary of all the documents for a given cluster.
Comments summarization. We extracted a set
of news articles and corresponding user comments
from Yahoo! News website. Our corpus contains a
set of 34 articles and each article is associated with
anywhere from 100–500 comments. Each com-
ment contains more than three sentences and 36
words per sentence on average.
</bodyText>
<subsectionHeader confidence="0.992722">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.969611071428571">
For each summarization task, we compare the
system output (i.e., summaries automatically pro-
duced by the algorithm) against the human-
generated summaries and evaluate the perfor-
mance in terms of ROUGE score (Lin, 2004), a
standard recall-based evaluation measure used in
summarization. A system that produces higher
ROUGE scores generates better quality summary
and vice versa.
We use the following evaluation settings in our
experiments for each summarization task:
(1) For multi-document summarization, we
compute the ROUGE-15 scores that was the main
evaluation criterion for DUC 2004 evaluations.
</bodyText>
<footnote confidence="0.9983332">
3The choice of the value 0.25 in the cover component is
inspired by the observations made by (Lin and Bilmes, 2011)
for the α value used in their cover function.
4http://duc.nist.gov/duc2004/tasks.html
5ROUGE v1.5.5 with options: -a -c 95 -b 665 -m -n 4 -w
</footnote>
<page confidence="0.801246">
1.2
</page>
<bodyText confidence="0.999926166666667">
(2) For comment summarization, the collection
of user comments associated with a given arti-
cle is typically much larger. Additionally, indi-
vidual comments are noisy, wordy, diverse, and
informally written. Hence for this task, we use
a slightly different evaluation criterion that is in-
spired from the DUC 2005-2007 summarization
evaluation tasks.
We represent the content within each comment
c (i.e., all sentences S(c) comprising the com-
ment) as a single node in the graph. We then run
our summarization algorithm on the instantiated
graph to produce a summary for each news article.
In addition, each news article and corresponding
set of comments were presented to three human
annotators. They were asked to select a subset of
comments (at most 20 comments) that best rep-
resented a summary capturing the most popular
as well as diverse set of views and opinions ex-
pressed by different users that are relevant to the
given news article. We then compare the auto-
matically generated comment summaries against
the human-generated summaries and compute the
ROUGE-1 and ROUGE-2 scores.6
This summarization task is particularly hard for
even human annotators since user-generated com-
ments are typically noisy and there are several
hundreds of comments per article. Similar to ex-
isting work in the literature (Sekine and Nobata,
2003), we computed inter-annotator agreement for
the humans by comparing their summaries against
each other on a small held-out set of articles. The
average ROUGE-1 F-scores observed for humans
was much higher (59.7) than that of automatic sys-
tems measured against the human-generated sum-
maries (our best system achieved a score of 28.9
ROUGE-1 on the same dataset). This shows that
even though this is a new type of summariza-
tion task, humans tend to generate more consistent
summaries and hence their annotations are reliable
for evaluation purposes as in multi-document sum-
marization.
</bodyText>
<subsectionHeader confidence="0.894775">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.9986146">
Multi-document summarization. (1) Table 1
compares the performance of our system with
the previous best reported system that partici-
pated in the DUC 2004 competition. We also in-
clude for comparison another baseline—a version
</bodyText>
<footnote confidence="0.9768905">
6ROUGE v1.5.5 with options: -a -n 2 -x -m -2 4 -u -c 95
-r 1000 -f A -p 0.5 -t 0 -d -l 150
</footnote>
<page confidence="0.997636">
1019
</page>
<bodyText confidence="0.996402647058823">
of our system that approximates the submodular
objective function proposed by (Lin and Bilmes,
2011).7 As shown in the results, our best system8
which uses the hg dispersion function achieves a
better ROUGE-1 F-score than all other systems.
(2) We observe that the hm and ht dispersion func-
tions produce slightly lower scores than hg, which
may be a characteristic of this particular summa-
rization task. We believe that the empirical results
achieved by different dispersion functions depend
on the nature of the summarization tasks and there
are task settings under which hm or ht perform
better than hg. For example, we show later how us-
ing the ht dispersion function yields the best per-
formance on the comments summarization task.
Regardless, the theoretical guarantees presented in
this paper cover all these cases.
</bodyText>
<listItem confidence="0.731952142857143">
(3) We also analyze the contributions of individ-
ual components of the new objective function to-
wards summarization performance by selectively
setting certain parameters to 0. Table 2 illustrates
these results. We clearly see that each component
(popularity, cluster contribution, dispersion) indi-
vidually yields a reasonable summarization per-
</listItem>
<bodyText confidence="0.919161428571429">
formance but the best result is achieved by the
combined system (row 5 in the table). We also
contrast the performance of the full system with
and without the dispersion component (row 4 ver-
sus row 5). The results show that optimizing for
dispersion yields an improvement in summariza-
tion performance.
</bodyText>
<listItem confidence="0.640729333333333">
(4) To understand the effect of utilizing syntactic
structure and semantic similarity for constructing
the summarization graph, we ran the experiments
using just the unigrams and bigrams; we obtained
a ROUGE-1 F-score of 37.1. Thus, modeling
the syntactic structure (using relations extracted
</listItem>
<bodyText confidence="0.990929517241379">
7Note that Lin &amp; Bilmes (2011) report a slightly higher
ROUGE-1 score (F-score 38.90) on DUC 2004. This is be-
cause their system was tuned for the particular summarization
task using the DUC 2003 corpus. On the other hand, even
without any parameter tuning our method yields good perfor-
mance, as evidenced by results on the two different summa-
rization tasks. However, since individual components within
our objective function are parametrized it is easy to tune them
for a specific task or genre.
8For the full system, we weight certain parameters per-
taining to cluster contributions and dispersion higher (α =
β = S = 5) compared to the rest of the objective function
(A = 1). Lin &amp; Bilmes (2011) also observed a similar find-
ing (albeit via parameter tuning) where weighting the cluster
contribution component higher yielded better performance.
If the maximum number of sentences/comments chosen were
k, we brought both he and ht to the same approximate scale
as hm by dividing he by k(k − 1)/2 and ht by k − 1.
from dependency parse tree) along with comput-
ing similarity in semantic spaces (using WordNet)
clearly produces an improvement in the summa-
rization quality (+1.4 improvement in ROUGE-1
F-score). However, while the structured represen-
tation is beneficial, we observed that dispersion
(and other individual components) contribute sim-
ilar performance gains even when using ngrams
alone. So the improvements obtained from the
structured representation and dispersion are com-
plementary.
</bodyText>
<table confidence="0.9997875">
System ROUGE-1 F
Best system in DUC 2004 37.9
(Lin and Bilmes, 2011), no tuning 37.47
Our algorithm with h = hm 37.5
h = hg 38.5
h = ht 36.8
</table>
<tableCaption confidence="0.999909">
Table 1: Performance on DUC 2004.
</tableCaption>
<bodyText confidence="0.99990803125">
Comments summarization. (1) Table 3 com-
pares the performance of our system against a
baseline system that is constructed by picking
comments in order of decreasing length, i.e., we
first pick the longest comment (comprising the
most number of characters), then the next longest
comment and so on, to create an ordered set of
comments. The intuition behind this baseline is
that longer comments contain more content and
possibly cover more topics than short ones.
From the table, we observe that the new sys-
tem (using either dispersion function) outperforms
the baseline by a huge margin (+44% relative
improvement in ROUGE-1 and much bigger im-
provements in ROUGE-2 scores). One reason be-
hind the lower ROUGE-2 scores for the baseline
might be that while long comments provide more
content (in terms of size), they also add noise and
irrelevant information to the generated summaries.
Our system models sentences using the syntactic
structure and semantics and jointly optimizes for
multiple summarization criteria (including disper-
sion) which helps weed out the noise and identify
relevant, useful information within the comments
thereby producing better quality summaries. The
95% confidence interval scores for the best system
on this task is [36.5–46.9].
(2) Unlike the multi-document summarization,
here we observe that the ht dispersion function
yields the best empirical performance for this
task. This observation supports our claim that the
choice of the specific dispersion function depends
</bodyText>
<page confidence="0.932826">
1020
</page>
<note confidence="0.311307">
Objective function components ROUGE-1 F
</note>
<equation confidence="0.9603782">
α = /3 = A = δ = 0 35.7
w(S) = /3 = A = δ = 0 35.1
h = hs,w(S) = α = /3 = A = 0 37.1
δ = 0 37.4
w(S), α, /3, A, δ &gt; 0 38.5
</equation>
<tableCaption confidence="0.594957">
Table 2: Performance with different parameters
(DUC).
</tableCaption>
<bodyText confidence="0.998048384615385">
on the summarization task and that the dispersion
functions proposed in this paper have a wider va-
riety of use cases.
(3) Results showing contributions from individual
components of the new summarization objective
function are listed in Table 4. We observe a sim-
ilar pattern as with multi-document summariza-
tion. The full system using all components out-
perform all other parameter settings, achieving the
best ROUGE-1 and ROUGE-2 scores. The table
also shows that incorporating dispersion into the
objective function yields an improvement in sum-
marization quality (row 4 versus row 5).
</bodyText>
<table confidence="0.99829">
System ROUGE-1 ROUGE-2
Baseline (decreasing length) 28.9 2.9
Our algorithm with h = hm 39.2 13.2
h = hs 40.9 15.0
h = ht 41.6 16.2
</table>
<tableCaption confidence="0.9690965">
Table 3: Performance on comments summariza-
tion.
</tableCaption>
<table confidence="0.7213505">
Objective function ROUGE-1 ROUGE-2
components
</table>
<equation confidence="0.8210998">
α = /3 = A = δ = 0 36.1 9.4
w(S) = /3 = A = δ = 0 32.1 4.9
h = ht,w(S) = α = /3 = A = 0 37.8 11.2
δ = 0 38.0 11.6
w(S), α, /3, A, δ &gt; 0 41.6 16.2
</equation>
<tableCaption confidence="0.8592505">
Table 4: Performance with different parameters
(comments).
</tableCaption>
<sectionHeader confidence="0.998919" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999993024390244">
We introduced a new general-purpose graph-based
summarization framework that combines a sub-
modular coverage function with a non-submodular
dispersion function. We presented three natural
dispersion functions that represent three different
ways of ensuring non-redundancy (using sentence
dissimilarities) for summarization and proved that
a simple greedy algorithm can obtain an approxi-
mately optimal summary in all these cases. Exper-
iments on two different summarization tasks show
that our algorithm outperforms algorithms that
rely only on submodularity. Finally, we demon-
strated that using a structured representation to
model sentences in the graph improves summa-
rization quality.
For future work, it would be interesting to in-
vestigate other related developments in this area
and perhaps combine them with our approach to
see if further improvements are possible. Firstly,
it would interesting to see if dispersion offers sim-
ilar improvements over a tuned version of the sub-
modular framework of Lin and Bilmes (2011). In a
very recent work, Lin and Bilmes (2012) demon-
strate a further improvement in performance for
document summarization by using mixtures of
submodular shells. This is an interesting exten-
sion of their previous submodular framework and
while the new formulation permits more complex
functions, the resulting function is still submodu-
lar and hence can be combined with the dispersion
measures proposed in this paper. A different body
of work uses determinantal point processes (DPP)
to model subset selection problems and adapt it
for document summarization (Kulesza and Taskar,
2011). Note that DPPs use similarity kernels for
performing inference whereas our measures are
combinatorial and not kernel-representable. While
approximation guarantees for DPPs are open, it
would be interesting to investigate the empiri-
cal gains by combining DPPs with dispersion-like
functions.
</bodyText>
<sectionHeader confidence="0.998436" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999758">
We thank the anonymous reviewers for their many
useful comments.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998665">
Allan Borodin, Hyun Chul Lee, and Yuli Ye. 2012.
Max-sum diversification, monotone submodular
functions and dynamic updates. In Proc. PODS,
pages 155–166.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proc. SIGIR,
pages 335–336.
Barun Chandra and Magn´us Halld´orsson. 2001. Facil-
ity dispersion and remote subgraphs. J. Algorithms,
38(2):438–465.
Dietmar Cieslik. 2001. The Steiner Ratio. Springer.
</reference>
<page confidence="0.924362">
1021
</page>
<bodyText confidence="0.433947857142857">
John M. Conroy and Dianne P. O’Leary. 2001. Text
summarization via hidden Markov models. In Proc.
SIGIR, pages 406–407.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
¨Ozg¨ur. 2010. Citation summarization through
keyphrase extraction. In Proc. COLING, pages 895–
903.
</bodyText>
<reference confidence="0.999186591549296">
Hal Daum´e, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proc. COL-
ING/ACL, pages 305–312.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. LREC, pages 449–454.
Elena Filatova. 2004. Event-based extractive summa-
rization. In Proc. ACL Workshop on Summarization,
pages 104–111.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: A graph based approach to abstrac-
tive summarization of highly redundant opinions. In
Proc. COLING.
Makoto Imase and Bernard M. Waxman. 1991. Dy-
namic Steiner tree problem. SIAM J. Discrete Math-
ematics, 4(3):369–384.
Hyun Duk Kim, Kavita Ganesan, Parikshit Sondhi, and
ChengXiang Zhai. 2011. Comprehensive review of
opinion summarization. Technical report, Univer-
sity of Illinois at Urbana-Champaign.
Alex Kulesza and Ben Taskar. 2011. Learning deter-
minantal point processes. In Proc. UAI, pages 419–
427.
Hui Lin and Jeff Bilmes. 2011. A class of submodu-
lar functions for document summarization. In Proc.
ACL, pages 510–520.
Hui Lin and Jeff Bilmes. 2012. Learning mixtures
of submodular shells with application to document
summarization. In Proc. UAI, pages 479–490.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Workshop on Text
Summarization Branches Out: Proc. ACL Work-
shop, pages 74–81.
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher.
1978. An analysis of approximations for maximiz-
ing submodular set functions I. Mathematical Pro-
gramming, 14(1):265–294.
Ani Nenkova and Kathleen McKeown. 2012. A survey
of text summarization techniques. In Charu C. Ag-
garwal and ChengXiang Zhai, editors, Mining Text
Data, pages 43–76. Springer.
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing WordNet-based context vectors to estimate the
semantic relatedness of concepts. In Proc. EACL
Workshop on Making Sense of Sense: Bringing
Computational Linguistics and Psycholinguistics
Together, pages 1–8.
Korbinian Riedhammer, Benoit Favre, and Dilek
Hakkani-T¨ur. 2010. Long story short—Global
unsupervised models for keyphrase based meeting
summarization. Speech Commun., 52(10):801–815.
Satoshi Sekine and Chikashi Nobata. 2003. A survey
for multi-document summarization. In Proc. HLT-
NAACL Workshop on Text Summarization, pages
65–72.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. HLT/NAACL, pages 685–688.
Chao Shen and Tao Li. 2010. Multi-document summa-
rization via the minimum dominating set. In Proc.
COLING, pages 984–992.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In Proc. EACL, pages 781–
789.
Koji Yatani, Michael Novati, Andrew Trusty, and
Khai N. Truong. 2011. Review spotlight: A user in-
terface for summarizing user-generated reviews us-
ing adjective-noun word pairs. In Proc. CHI, pages
1541–1550.
</reference>
<page confidence="0.994105">
1022
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.655389">
<title confidence="0.999757">Summarization Through Submodularity and Dispersion</title>
<author confidence="0.904485">Anirban Dasgupta Ravi Kumar Sujith Ravi</author>
<affiliation confidence="0.70652">Yahoo! Labs Google Google</affiliation>
<address confidence="0.997034">Sunnyvale, CA 95054 Mountain View, CA 94043 Mountain View, CA 94043</address>
<email confidence="0.995826">anirban@yahoo-inc.comtintin@google.comsravi@gooogle.com</email>
<abstract confidence="0.9972437">We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonfunction, which we call disthe latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. We conduct experiments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algorithm outperforms those that rely only on submodularity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Allan Borodin</author>
<author>Hyun Chul Lee</author>
<author>Yuli Ye</author>
</authors>
<title>Max-sum diversification, monotone submodular functions and dynamic updates.</title>
<date>2012</date>
<booktitle>In Proc. PODS,</booktitle>
<pages>155--166</pages>
<contexts>
<context position="10422" citStr="Borodin et al., 2012" startWordPosition="1767" endWordPosition="1770">for i = 0,...,k − 1 do v ← arg maxuEU\Si g(Si +u)+6h(Si +u) Si+1 ← Si ∪ {v} end for 3.3 Analysis In this section we obtain a provable approximation for the greedy algorithm. First, we show that a greedy choice is well-behaved with respect to the dispersion function h·(·). Lemma 1. Let O be any set with |O |= k. If S is such that |S |= E &lt; k, then (i) EuEO\S hs(u, S) ≥ |O \ S |�hs(O) k(k−1); A, B C U,wehave f(A)+ f(B) ≥ f(AUB)+ f(AnB). (ii) EuEO\S d(u, S) ≥ 12ht(O) − ht(S); and (iii) there exists u ∈ O \ S such that hm(u, S) ≥ hm(O)/2. Proof. The proof for (i) follows directly from Lemma 1 in (Borodin et al., 2012). To prove (ii) let T be the tree obtained by adding all points of O \ S directly to their respective closest points on the minimum spanning tree of S. T is a spanning tree, and hence a Steiner tree, for the points in set S ∪ O. Hence, cost(T) = ht(S) + E uEO\S d(u, S). Let smt(S) denote the cost of a minimum Steiner tree of S. Thus, cost(T) ≥ smt(O ∪ S). Since a Steiner tree of O ∪ S is also a Steiner tree of O, smt(O ∪ S) ≥ smt(O). Since this is a metric space, smt(O) ≥12ht(O) (see, for example, (Cieslik, 2001)). Thus, � 1 ht(S) + d(u, S) ≥ 2ht(O) uEO\S � 1 ⇒ d(u, S) ≥ 2ht(O) − ht(S). uEO\S </context>
<context position="12942" citStr="Borodin et al., 2012" startWordPosition="2304" endWordPosition="2307">spersion function h&apos;, which is a slightly modified version of h. In particular, for h = hs, we use h&apos;(S) = 2hs(S). For h = ht, we abuse notation and define h&apos; to be a function over an ordered set S = {u1, ... , uk} as follows: h&apos;(S) = Ej≤|S |d(uj, Sj−1), where Sj−1 = {u1,... , uj−1}. Let f&apos;(S) = g(S) + δh&apos;(S). Consider the ith iteration of the algorithm. By the submodularity of g(·), � g(Si ∪ {u}) − g(Si) (2) uEO\Si ≥ g(O ∪ Si) − g(Si) ≥ g(O) − g(Sk), where we use monotonicity of g(·) to infer g(O ∪ Si) ≥ g(O) and g(Si) ≤ g(Sk). For h = hs, the proof follows by Lemma 1(i) and by Theorem 1 in (Borodin et al., 2012). For ht, using the above argument of submodularity and monotonicity of g, and the result from Lemma 1(ii), we have � g(Si ∪ u) − g(Si) + δd(u, Si) uEO\Si ≥ g(O) − g(Si) + δ(ht(O)/2 − ht(Si)) ≥ (g(O) + δht(O)/2) − (g(Si) + δht(Si)) ≥ f(O)/2 − (g(Si) + δht(Si)). Also, ht(Si) ≤ 2 smt(Si) since this is a metric space. Using the monotonicity of the Steiner tree cost, smt(Si) ≤ smt(Sk) ≤ ht(Sk). Hence, ht(Si) ≤ 2ht(Sk). Thus, � g(Si ∪ u) − g(Si) + δd(u, Si) uEO\Si ≥ f(O)/2 − (g(Si) + δht(Si)) ≥ f(O)/2 − (g(Sk) + 2δht(Sk)) ≥ f(O)/2 − 2f(Sk). (3) By the greedy choice of ui+1, f&apos;(Si ∪ ui+1) − f&apos;(Si) =</context>
</contexts>
<marker>Borodin, Lee, Ye, 2012</marker>
<rawString>Allan Borodin, Hyun Chul Lee, and Yuli Ye. 2012. Max-sum diversification, monotone submodular functions and dynamic updates. In Proc. PODS, pages 155–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>335--336</pages>
<contexts>
<context position="5499" citStr="Carbonell and Goldstein, 1998" startWordPosition="842" endWordPosition="845">ra: the DUC 2004 corpus and a corpus of user comments on news articles. On DUC 2004, we obtain performance that matches (Lin and Bilmes, 2011), without any serious parameter tuning; note that their framework does not have the dispersion function. On the comment corpus, we outperform their method, demonstrating that value of dispersion functions. As part of our methodology, we also use a new structured representation for summaries. 2 Related Work Automatic summarization is a well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. SIGIR, pages 335–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barun Chandra</author>
<author>Magn´us Halld´orsson</author>
</authors>
<title>Facility dispersion and remote subgraphs.</title>
<date>2001</date>
<journal>J. Algorithms,</journal>
<volume>38</volume>
<issue>2</issue>
<marker>Chandra, Halld´orsson, 2001</marker>
<rawString>Barun Chandra and Magn´us Halld´orsson. 2001. Facility dispersion and remote subgraphs. J. Algorithms, 38(2):438–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dietmar Cieslik</author>
</authors>
<date>2001</date>
<publisher>The Steiner Ratio. Springer.</publisher>
<contexts>
<context position="10940" citStr="Cieslik, 2001" startWordPosition="1878" endWordPosition="1879">, S) ≥ hm(O)/2. Proof. The proof for (i) follows directly from Lemma 1 in (Borodin et al., 2012). To prove (ii) let T be the tree obtained by adding all points of O \ S directly to their respective closest points on the minimum spanning tree of S. T is a spanning tree, and hence a Steiner tree, for the points in set S ∪ O. Hence, cost(T) = ht(S) + E uEO\S d(u, S). Let smt(S) denote the cost of a minimum Steiner tree of S. Thus, cost(T) ≥ smt(O ∪ S). Since a Steiner tree of O ∪ S is also a Steiner tree of O, smt(O ∪ S) ≥ smt(O). Since this is a metric space, smt(O) ≥12ht(O) (see, for example, (Cieslik, 2001)). Thus, � 1 ht(S) + d(u, S) ≥ 2ht(O) uEO\S � 1 ⇒ d(u, S) ≥ 2ht(O) − ht(S). uEO\S To prove (iii), let O = {u1, ... , uk}. By definition, for every i =6 j, d(ui, uj) ≥ hm(O). Consider the (open) ball Bi of radius hm(O)/2 around each element ui. By construction for each i, Bi ∩ O = {ui} and for each pair i =6 j, Bi ∩ Bj = ∅. Since |S |&lt; k, and there are k balls Bi, there exists k−E balls Bi such that S∩Bi = ∅, proving (iii). We next show that the tree created by the greedy algorithm for h = ht is not far from the optimum. Lemma 2. Let u1, ... , uk be a sequence ofpoints and let Si = {uj, j ≤ i}.</context>
</contexts>
<marker>Cieslik, 2001</marker>
<rawString>Dietmar Cieslik. 2001. The Steiner Ratio. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian query-focused summarization.</title>
<date>2006</date>
<booktitle>In Proc. COLING/ACL,</booktitle>
<pages>305--312</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e, III and Daniel Marcu. 2006. Bayesian query-focused summarization. In Proc. COLING/ACL, pages 305–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill Maccartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, Maccartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proc. LREC, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
</authors>
<title>Event-based extractive summarization.</title>
<date>2004</date>
<booktitle>In Proc. ACL Workshop on Summarization,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="5810" citStr="Filatova, 2004" startWordPosition="889" endWordPosition="890"> dispersion functions. As part of our methodology, we also use a new structured representation for summaries. 2 Related Work Automatic summarization is a well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related topics, see (Kim et al., 2011; Nenkova and McKeown, 2012). 3 Framework In this section we present the summarization framework. We start by describing a generic objective function that can be widely applied to several summarization scenarios. This objective </context>
</contexts>
<marker>Filatova, 2004</marker>
<rawString>Elena Filatova. 2004. Event-based extractive summarization. In Proc. ACL Workshop on Summarization, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kavita Ganesan</author>
<author>ChengXiang Zhai</author>
<author>Jiawei Han</author>
</authors>
<title>Opinosis: A graph based approach to abstractive summarization of highly redundant opinions.</title>
<date>2010</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="5970" citStr="Ganesan et al., 2010" startWordPosition="913" endWordPosition="916">well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related topics, see (Kim et al., 2011; Nenkova and McKeown, 2012). 3 Framework In this section we present the summarization framework. We start by describing a generic objective function that can be widely applied to several summarization scenarios. This objective function is the sum of a monotone submodular coverage function and a non-submodular dispersion function. We then describe a simple greedy algorithm for optimizi</context>
</contexts>
<marker>Ganesan, Zhai, Han, 2010</marker>
<rawString>Kavita Ganesan, ChengXiang Zhai, and Jiawei Han. 2010. Opinosis: A graph based approach to abstractive summarization of highly redundant opinions. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Imase</author>
<author>Bernard M Waxman</author>
</authors>
<title>Dynamic Steiner tree problem.</title>
<date>1991</date>
<journal>SIAM J. Discrete Mathematics,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="11915" citStr="Imase and Waxman, 1991" startWordPosition="2087" endWordPosition="2090">e k balls Bi, there exists k−E balls Bi such that S∩Bi = ∅, proving (iii). We next show that the tree created by the greedy algorithm for h = ht is not far from the optimum. Lemma 2. Let u1, ... , uk be a sequence ofpoints and let Si = {uj, j ≤ i}. Then, ht(Sk) ≥ 1/logk E2&lt;j&lt;k d(uj, Sj−1). Proof. The proof follows by noting that we get a spanning tree by connecting each ui to its closest point in Si−1. The cost of this spanning tree is E2&lt;j&lt;k d(uj, Sj−1) and this tree is also the result of the greedy algorithm run in an online fashion on the input sequence {u1, ... , uk}. Using the result of (Imase and Waxman, 1991), the competitive ratio of this algorithm is log k, and hence the proof. We now state and prove the main result about the quality of approximation of the greedy algorithm. 1016 Theorem 3. Fork &gt; 1, there is a polynomial-time algorithm that obtains a γ-approximation to f(S), where γ = 1/2 for h = hs, γ = 1/4 for h = hm, and γ = 1/3 log k for h = ht. Proof. For hs and ht, we run Algorithm 1 using a new dispersion function h&apos;, which is a slightly modified version of h. In particular, for h = hs, we use h&apos;(S) = 2hs(S). For h = ht, we abuse notation and define h&apos; to be a function over an ordered se</context>
</contexts>
<marker>Imase, Waxman, 1991</marker>
<rawString>Makoto Imase and Bernard M. Waxman. 1991. Dynamic Steiner tree problem. SIAM J. Discrete Mathematics, 4(3):369–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyun Duk Kim</author>
<author>Kavita Ganesan</author>
<author>Parikshit Sondhi</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Comprehensive review of opinion summarization.</title>
<date>2011</date>
<tech>Technical report,</tech>
<institution>University of Illinois at Urbana-Champaign.</institution>
<contexts>
<context position="6182" citStr="Kim et al., 2011" startWordPosition="947" endWordPosition="950">, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related topics, see (Kim et al., 2011; Nenkova and McKeown, 2012). 3 Framework In this section we present the summarization framework. We start by describing a generic objective function that can be widely applied to several summarization scenarios. This objective function is the sum of a monotone submodular coverage function and a non-submodular dispersion function. We then describe a simple greedy algorithm for optimizing this objective function with provable approximation guarantees for three natural dispersion functions. 3.1 Preliminaries Let C be a collection of texts. Depending on the summarization application, C can refer </context>
</contexts>
<marker>Kim, Ganesan, Sondhi, Zhai, 2011</marker>
<rawString>Hyun Duk Kim, Kavita Ganesan, Parikshit Sondhi, and ChengXiang Zhai. 2011. Comprehensive review of opinion summarization. Technical report, University of Illinois at Urbana-Champaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Ben Taskar</author>
</authors>
<title>Learning determinantal point processes.</title>
<date>2011</date>
<booktitle>In Proc. UAI,</booktitle>
<pages>419--427</pages>
<marker>Kulesza, Taskar, 2011</marker>
<rawString>Alex Kulesza and Ben Taskar. 2011. Learning determinantal point processes. In Proc. UAI, pages 419– 427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>A class of submodular functions for document summarization.</title>
<date>2011</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>510--520</pages>
<contexts>
<context position="2749" citStr="Lin and Bilmes, 2011" startWordPosition="429" endWordPosition="432">one is faced with a different set of problems: volume (popular articles generate more than 10,000 comments), noise (most comments are vacuous, linguistically deficient, and tangential to the article), and redundancy (similar views are expressed by multiple commenters). In both cases, there is a delicate balance between choosing the salient, relevant, popular, and diverse points (e.g., sentences) versus minimizing syntactic and semantic redundancy. While there have been many approaches to automatic summarization (see Section 2), our work is directly inspired by the recent elegant framework of (Lin and Bilmes, 2011). They employed the powerful theory of submodular functions for summarization: submodularity embodies the “diminishing returns” property and hence is a natural vocabulary to express the summarization desiderata. In this framework, each of the constraints (relevance, redundancy, etc.) is captured as a submodular function and the objective is to maximize their sum. A simple greedy algorithm is guaranteed to produce an approximately optimal summary. They used this framework to obtain the best results on the DUC 2004 corpus. Even though the submodularity framework is quite general, it has limitati</context>
<context position="5012" citStr="Lin and Bilmes, 2011" startWordPosition="769" endWordPosition="772">air sentence dissimilarities, the weight of the minimum spanning tree on the sentences, and the minimum of all-pair sentence dissimilarities. These three functions represent three different ways of using the sentence dissimilarities. We then show that a greedy algorithm can obtain approximately optimal summary in each of the three cases; the proof exploits some nice combinatorial properties satisfied by the three dispersion functions. We then conduct experiments on two corpora: the DUC 2004 corpus and a corpus of user comments on news articles. On DUC 2004, we obtain performance that matches (Lin and Bilmes, 2011), without any serious parameter tuning; note that their framework does not have the dispersion function. On the comment corpus, we outperform their method, demonstrating that value of dispersion functions. As part of our methodology, we also use a new structured representation for summaries. 2 Related Work Automatic summarization is a well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used </context>
<context position="21761" citStr="Lin and Bilmes, 2011" startWordPosition="3827" endWordPosition="3830">ced by the algorithm) against the humangenerated summaries and evaluate the performance in terms of ROUGE score (Lin, 2004), a standard recall-based evaluation measure used in summarization. A system that produces higher ROUGE scores generates better quality summary and vice versa. We use the following evaluation settings in our experiments for each summarization task: (1) For multi-document summarization, we compute the ROUGE-15 scores that was the main evaluation criterion for DUC 2004 evaluations. 3The choice of the value 0.25 in the cover component is inspired by the observations made by (Lin and Bilmes, 2011) for the α value used in their cover function. 4http://duc.nist.gov/duc2004/tasks.html 5ROUGE v1.5.5 with options: -a -c 95 -b 665 -m -n 4 -w 1.2 (2) For comment summarization, the collection of user comments associated with a given article is typically much larger. Additionally, individual comments are noisy, wordy, diverse, and informally written. Hence for this task, we use a slightly different evaluation criterion that is inspired from the DUC 2005-2007 summarization evaluation tasks. We represent the content within each comment c (i.e., all sentences S(c) comprising the comment) as a sing</context>
<context position="24253" citStr="Lin and Bilmes, 2011" startWordPosition="4232" endWordPosition="4235">is a new type of summarization task, humans tend to generate more consistent summaries and hence their annotations are reliable for evaluation purposes as in multi-document summarization. 5.3 Results Multi-document summarization. (1) Table 1 compares the performance of our system with the previous best reported system that participated in the DUC 2004 competition. We also include for comparison another baseline—a version 6ROUGE v1.5.5 with options: -a -n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d -l 150 1019 of our system that approximates the submodular objective function proposed by (Lin and Bilmes, 2011).7 As shown in the results, our best system8 which uses the hg dispersion function achieves a better ROUGE-1 F-score than all other systems. (2) We observe that the hm and ht dispersion functions produce slightly lower scores than hg, which may be a characteristic of this particular summarization task. We believe that the empirical results achieved by different dispersion functions depend on the nature of the summarization tasks and there are task settings under which hm or ht perform better than hg. For example, we show later how using the ht dispersion function yields the best performance on</context>
<context position="27472" citStr="Lin and Bilmes, 2011" startWordPosition="4747" endWordPosition="4750"> approximate scale as hm by dividing he by k(k − 1)/2 and ht by k − 1. from dependency parse tree) along with computing similarity in semantic spaces (using WordNet) clearly produces an improvement in the summarization quality (+1.4 improvement in ROUGE-1 F-score). However, while the structured representation is beneficial, we observed that dispersion (and other individual components) contribute similar performance gains even when using ngrams alone. So the improvements obtained from the structured representation and dispersion are complementary. System ROUGE-1 F Best system in DUC 2004 37.9 (Lin and Bilmes, 2011), no tuning 37.47 Our algorithm with h = hm 37.5 h = hg 38.5 h = ht 36.8 Table 1: Performance on DUC 2004. Comments summarization. (1) Table 3 compares the performance of our system against a baseline system that is constructed by picking comments in order of decreasing length, i.e., we first pick the longest comment (comprising the most number of characters), then the next longest comment and so on, to create an ordered set of comments. The intuition behind this baseline is that longer comments contain more content and possibly cover more topics than short ones. From the table, we observe tha</context>
<context position="31352" citStr="Lin and Bilmes (2011)" startWordPosition="5398" endWordPosition="5401">summary in all these cases. Experiments on two different summarization tasks show that our algorithm outperforms algorithms that rely only on submodularity. Finally, we demonstrated that using a structured representation to model sentences in the graph improves summarization quality. For future work, it would be interesting to investigate other related developments in this area and perhaps combine them with our approach to see if further improvements are possible. Firstly, it would interesting to see if dispersion offers similar improvements over a tuned version of the submodular framework of Lin and Bilmes (2011). In a very recent work, Lin and Bilmes (2012) demonstrate a further improvement in performance for document summarization by using mixtures of submodular shells. This is an interesting extension of their previous submodular framework and while the new formulation permits more complex functions, the resulting function is still submodular and hence can be combined with the dispersion measures proposed in this paper. A different body of work uses determinantal point processes (DPP) to model subset selection problems and adapt it for document summarization (Kulesza and Taskar, 2011). Note that DP</context>
<context position="25940" citStr="Lin &amp; Bilmes (2011)" startWordPosition="4497" endWordPosition="4500">ce but the best result is achieved by the combined system (row 5 in the table). We also contrast the performance of the full system with and without the dispersion component (row 4 versus row 5). The results show that optimizing for dispersion yields an improvement in summarization performance. (4) To understand the effect of utilizing syntactic structure and semantic similarity for constructing the summarization graph, we ran the experiments using just the unigrams and bigrams; we obtained a ROUGE-1 F-score of 37.1. Thus, modeling the syntactic structure (using relations extracted 7Note that Lin &amp; Bilmes (2011) report a slightly higher ROUGE-1 score (F-score 38.90) on DUC 2004. This is because their system was tuned for the particular summarization task using the DUC 2003 corpus. On the other hand, even without any parameter tuning our method yields good performance, as evidenced by results on the two different summarization tasks. However, since individual components within our objective function are parametrized it is easy to tune them for a specific task or genre. 8For the full system, we weight certain parameters pertaining to cluster contributions and dispersion higher (α = β = S = 5) compared </context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proc. ACL, pages 510–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Learning mixtures of submodular shells with application to document summarization.</title>
<date>2012</date>
<booktitle>In Proc. UAI,</booktitle>
<pages>479--490</pages>
<contexts>
<context position="31398" citStr="Lin and Bilmes (2012)" startWordPosition="5407" endWordPosition="5410"> different summarization tasks show that our algorithm outperforms algorithms that rely only on submodularity. Finally, we demonstrated that using a structured representation to model sentences in the graph improves summarization quality. For future work, it would be interesting to investigate other related developments in this area and perhaps combine them with our approach to see if further improvements are possible. Firstly, it would interesting to see if dispersion offers similar improvements over a tuned version of the submodular framework of Lin and Bilmes (2011). In a very recent work, Lin and Bilmes (2012) demonstrate a further improvement in performance for document summarization by using mixtures of submodular shells. This is an interesting extension of their previous submodular framework and while the new formulation permits more complex functions, the resulting function is still submodular and hence can be combined with the dispersion measures proposed in this paper. A different body of work uses determinantal point processes (DPP) to model subset selection problems and adapt it for document summarization (Kulesza and Taskar, 2011). Note that DPPs use similarity kernels for performing infer</context>
</contexts>
<marker>Lin, Bilmes, 2012</marker>
<rawString>Hui Lin and Jeff Bilmes. 2012. Learning mixtures of submodular shells with application to document summarization. In Proc. UAI, pages 479–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Workshop on Text Summarization Branches Out: Proc. ACL Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="21263" citStr="Lin, 2004" startWordPosition="3754" endWordPosition="3755">s to produce a summary of all the documents for a given cluster. Comments summarization. We extracted a set of news articles and corresponding user comments from Yahoo! News website. Our corpus contains a set of 34 articles and each article is associated with anywhere from 100–500 comments. Each comment contains more than three sentences and 36 words per sentence on average. 5.2 Evaluation For each summarization task, we compare the system output (i.e., summaries automatically produced by the algorithm) against the humangenerated summaries and evaluate the performance in terms of ROUGE score (Lin, 2004), a standard recall-based evaluation measure used in summarization. A system that produces higher ROUGE scores generates better quality summary and vice versa. We use the following evaluation settings in our experiments for each summarization task: (1) For multi-document summarization, we compute the ROUGE-15 scores that was the main evaluation criterion for DUC 2004 evaluations. 3The choice of the value 0.25 in the cover component is inspired by the observations made by (Lin and Bilmes, 2011) for the α value used in their cover function. 4http://duc.nist.gov/duc2004/tasks.html 5ROUGE v1.5.5 w</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Workshop on Text Summarization Branches Out: Proc. ACL Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G L Nemhauser</author>
<author>L A Wolsey</author>
<author>M L Fisher</author>
</authors>
<title>An analysis of approximations for maximizing submodular set functions I.</title>
<date>1978</date>
<booktitle>Mathematical Programming,</booktitle>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="9361" citStr="Nemhauser et al., 1978" startWordPosition="1560" endWordPosition="1563">(u, v). Note that these functions span from considering the entire set of distances in S to considering only the minimum distance in S; also it is easy to construct examples to show that none of these functions is submodular. Define h*(u, S) = h*({u}, S) and h*(S) = h*(S, S). Let O be the optimal solution of the function f. A summary S˜ is a γ-approximation if f(˜S) ≥ γf(O). 3.2 Algorithm Maximizing (1) is NP-hard even if 6 = 0 or if g(·) = 0 (Chandra and Halld´orsson, 2001). For the special case 6 = 0, since g(·) is submodular, a classical greedy algorithm obtains a (1 − 1/e)- approximation (Nemhauser et al., 1978). But if 6 &gt; 0, since the dispersion function h(·) is not submodular, the combined objective f(·) is not submodular as well. Despite this, we show that a simple greedy algorithm achieves a provable approximation factor for (1). This is possible due to some nice structural properties of the dispersion functions we consider. Algorithm 2 Greedy algorithm, parametrized by the dispersion function h; here, U, k, g, 6 are fixed. S0 ← ∅; i ← 0 for i = 0,...,k − 1 do v ← arg maxuEU\Si g(Si +u)+6h(Si +u) Si+1 ← Si ∪ {v} end for 3.3 Analysis In this section we obtain a provable approximation for the gree</context>
</contexts>
<marker>Nemhauser, Wolsey, Fisher, 1978</marker>
<rawString>G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. 1978. An analysis of approximations for maximizing submodular set functions I. Mathematical Programming, 14(1):265–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>A survey of text summarization techniques.</title>
<date>2012</date>
<booktitle>In Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data,</booktitle>
<pages>43--76</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6210" citStr="Nenkova and McKeown, 2012" startWordPosition="951" endWordPosition="954">oncepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related topics, see (Kim et al., 2011; Nenkova and McKeown, 2012). 3 Framework In this section we present the summarization framework. We start by describing a generic objective function that can be widely applied to several summarization scenarios. This objective function is the sum of a monotone submodular coverage function and a non-submodular dispersion function. We then describe a simple greedy algorithm for optimizing this objective function with provable approximation guarantees for three natural dispersion functions. 3.1 Preliminaries Let C be a collection of texts. Depending on the summarization application, C can refer to the set of documents (e.g</context>
</contexts>
<marker>Nenkova, McKeown, 2012</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2012. A survey of text summarization techniques. In Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data, pages 43–76. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>Using WordNet-based context vectors to estimate the semantic relatedness of concepts.</title>
<date>2006</date>
<booktitle>In Proc. EACL Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="17601" citStr="Patwardhan and Pedersen, 2006" startWordPosition="3123" endWordPosition="3126">e graph). For each pair of nodes (u, v) in the graph, we compute the semantic similarity score (using WordNet) between every pair of dependency relation (rel: a, b) in u and v as: �s(u, v) = WN(ai, aj) x WN(bi, bj), reli∈u,relj ∈v reli=relj where rel is a relation type (e.g., nsubj) and a, b are the two arguments present in the dependency relation (b does not exist for some relations). WN(wi, wj) is defined as the WordNet similarity score between words wi and wj.2 The edge weights are then normalized across all edges in the 2There exists various semantic relatedness measures based on WordNet (Patwardhan and Pedersen, 2006). In our experiments, for WN we pick one that is based on the path length between the two words in the WordNet graph. graph. This allows us to perform approximate matching of syntactic treelets obtained from the dependency parses using semantic (WordNet) similarity. For example, the sentences “I adore tennis” and “Everyone likes tennis” convey the same view and should be assigned a higher similarity score as opposed to “I hate tennis”. Using the syntactic structure along with semantic similarity helps us identify useful (valid) nuggets of information within comments (or documents), avoid redun</context>
</contexts>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>Siddharth Patwardhan and Ted Pedersen. 2006. Using WordNet-based context vectors to estimate the semantic relatedness of concepts. In Proc. EACL Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Korbinian Riedhammer</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Long story short—Global unsupervised models for keyphrase based meeting summarization.</title>
<date>2010</date>
<journal>Speech Commun.,</journal>
<volume>52</volume>
<issue>10</issue>
<marker>Riedhammer, Favre, Hakkani-T¨ur, 2010</marker>
<rawString>Korbinian Riedhammer, Benoit Favre, and Dilek Hakkani-T¨ur. 2010. Long story short—Global unsupervised models for keyphrase based meeting summarization. Speech Commun., 52(10):801–815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Chikashi Nobata</author>
</authors>
<title>A survey for multi-document summarization.</title>
<date>2003</date>
<booktitle>In Proc. HLTNAACL Workshop on Text Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="23237" citStr="Sekine and Nobata, 2003" startWordPosition="4063" endWordPosition="4066">ed to select a subset of comments (at most 20 comments) that best represented a summary capturing the most popular as well as diverse set of views and opinions expressed by different users that are relevant to the given news article. We then compare the automatically generated comment summaries against the human-generated summaries and compute the ROUGE-1 and ROUGE-2 scores.6 This summarization task is particularly hard for even human annotators since user-generated comments are typically noisy and there are several hundreds of comments per article. Similar to existing work in the literature (Sekine and Nobata, 2003), we computed inter-annotator agreement for the humans by comparing their summaries against each other on a small held-out set of articles. The average ROUGE-1 F-scores observed for humans was much higher (59.7) than that of automatic systems measured against the human-generated summaries (our best system achieved a score of 28.9 ROUGE-1 on the same dataset). This shows that even though this is a new type of summarization task, humans tend to generate more consistent summaries and hence their annotations are reliable for evaluation purposes as in multi-document summarization. 5.3 Results Multi</context>
</contexts>
<marker>Sekine, Nobata, 2003</marker>
<rawString>Satoshi Sekine and Chikashi Nobata. 2003. A survey for multi-document summarization. In Proc. HLTNAACL Workshop on Text Summarization, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beaux Sharifi</author>
<author>Mark-Anthony Hutton</author>
<author>Jugal Kalita</author>
</authors>
<title>Summarizing microblogs automatically.</title>
<date>2010</date>
<booktitle>In Proc. HLT/NAACL,</booktitle>
<pages>685--688</pages>
<contexts>
<context position="5772" citStr="Sharifi et al., 2010" startWordPosition="883" endWordPosition="886">rm their method, demonstrating that value of dispersion functions. As part of our methodology, we also use a new structured representation for summaries. 2 Related Work Automatic summarization is a well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related topics, see (Kim et al., 2011; Nenkova and McKeown, 2012). 3 Framework In this section we present the summarization framework. We start by describing a generic objective function that can be widely applied to several su</context>
</contexts>
<marker>Sharifi, Hutton, Kalita, 2010</marker>
<rawString>Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita. 2010. Summarizing microblogs automatically. In Proc. HLT/NAACL, pages 685–688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Shen</author>
<author>Tao Li</author>
</authors>
<title>Multi-document summarization via the minimum dominating set.</title>
<date>2010</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>984--992</pages>
<contexts>
<context position="5573" citStr="Shen and Li, 2010" startWordPosition="855" endWordPosition="858">we obtain performance that matches (Lin and Bilmes, 2011), without any serious parameter tuning; note that their framework does not have the dispersion function. On the comment corpus, we outperform their method, demonstrating that value of dispersion functions. As part of our methodology, we also use a new structured representation for summaries. 2 Related Work Automatic summarization is a well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related topics, see (Kim et </context>
</contexts>
<marker>Shen, Li, 2010</marker>
<rawString>Chao Shen and Tao Li. 2010. Multi-document summarization via the minimum dominating set. In Proc. COLING, pages 984–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Text summarization model based on maximum coverage problem and its variant.</title>
<date>2009</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>781--789</pages>
<contexts>
<context position="5553" citStr="Takamura and Okumura, 2009" startWordPosition="851" endWordPosition="854">news articles. On DUC 2004, we obtain performance that matches (Lin and Bilmes, 2011), without any serious parameter tuning; note that their framework does not have the dispersion function. On the comment corpus, we outperform their method, demonstrating that value of dispersion functions. As part of our methodology, we also use a new structured representation for summaries. 2 Related Work Automatic summarization is a well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related </context>
</contexts>
<marker>Takamura, Okumura, 2009</marker>
<rawString>Hiroya Takamura and Manabu Okumura. 2009. Text summarization model based on maximum coverage problem and its variant. In Proc. EACL, pages 781– 789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koji Yatani</author>
<author>Michael Novati</author>
<author>Andrew Trusty</author>
<author>Khai N Truong</author>
</authors>
<title>Review spotlight: A user interface for summarizing user-generated reviews using adjective-noun word pairs.</title>
<date>2011</date>
<booktitle>In Proc. CHI,</booktitle>
<pages>1541--1550</pages>
<contexts>
<context position="5893" citStr="Yatani et al., 2011" startWordPosition="901" endWordPosition="904">d representation for summaries. 2 Related Work Automatic summarization is a well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related topics, see (Kim et al., 2011; Nenkova and McKeown, 2012). 3 Framework In this section we present the summarization framework. We start by describing a generic objective function that can be widely applied to several summarization scenarios. This objective function is the sum of a monotone submodular coverage function and a non-submodular</context>
</contexts>
<marker>Yatani, Novati, Trusty, Truong, 2011</marker>
<rawString>Koji Yatani, Michael Novati, Andrew Trusty, and Khai N. Truong. 2011. Review spotlight: A user interface for summarizing user-generated reviews using adjective-noun word pairs. In Proc. CHI, pages 1541–1550.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>