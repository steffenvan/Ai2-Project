<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003706">
<title confidence="0.984095">
The RWTH System Combination System for WMT 2009
</title>
<author confidence="0.998075">
Gregor Leusch, Evgeny Matusov, and Hermann Ney
</author>
<affiliation confidence="0.889578">
RWTH Aachen University
Aachen, Germany
</affiliation>
<sectionHeader confidence="0.968637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.965398">
RWTH participated in the System Combi-
nation task of the Fourth Workshop on Sta-
tistical Machine Translation (WMT 2009).
Hypotheses from 9 German—*English MT
systems were combined into a consen-
sus translation. This consensus transla-
tion scored 2.1% better in BLEU and 2.3%
better in TER (abs.) than the best sin-
gle system. In addition, cross-lingual
output from 10 French, German, and
Spanish—*English systems was combined
into a consensus translation, which gave
an improvement of 2.0% in BLEU/3.5% in
TER (abs.) over the best single system.
</bodyText>
<sectionHeader confidence="0.998729" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949666666667">
The RWTH approach to MT system combination
is a refined version of the ROVER approach in
ASR (Fiscus, 1997), with additional steps to cope
with reordering between different hypotheses, and
to use true casing information from the input hy-
potheses. The basic concept of the approach has
been described by Matusov et al. (2006). Several
improvements have been added later (Matusov et
al., 2008). This approach includes an enhanced
alignment and reordering framework. In con-
trast to existing approaches (Jayaraman and Lavie,
2005; Rosti et al., 2007), the context of the whole
corpus rather than a single sentence is considered
in this iterative, unsupervised procedure, yielding
a more reliable alignment. Majority voting on the
generated lattice is performed using the prior prob-
abilities for each system as well as other statistical
models such as a special n-gram language model.
</bodyText>
<sectionHeader confidence="0.983969" genericHeader="method">
2 System Combination Algorithm
</sectionHeader>
<bodyText confidence="0.994796">
In this section we present the details of our system
combination method. Figure 1 gives an overview
of the system combination architecture described
in this section. After preprocessing the MT hy-
potheses, pairwise alignments between the hy-
potheses are calculated. The hypotheses are then
reordered to match the word order of a selected
primary hypothesis. From this, we create a confu-
sion network (CN), which we then rescore using
</bodyText>
<figureCaption confidence="0.999843">
Figure 1: The system combination architecture.
</figureCaption>
<bodyText confidence="0.894597333333333">
system prior weights and a language model (LM).
The single best path in this CN then constitutes the
consensus translation.
</bodyText>
<subsectionHeader confidence="0.933874">
2.1 Word Alignment
</subsectionHeader>
<bodyText confidence="0.999044848484848">
The proposed alignment approach is a statistical
one. It takes advantage of multiple translations for
a whole corpus to compute a consensus translation
for each sentence in this corpus. It also takes ad-
vantage of the fact that the sentences to be aligned
are in the same language.
For each source sentence F in the test corpus,
we select one of its translations En, n=1, ... , M,
as the primary hypothesis. Then we align the sec-
ondary hypotheses Ent(m = 1, ... , M; n =� m)
with En to match the word order in En. Since it is
not clear which hypothesis should be primary, i. e.
has the “best” word order, we let every hypothesis
play the role of the primary translation, and align
all pairs of hypotheses (En, Ent); n =� m.
The word alignment is trained in analogy to
the alignment training procedure in statistical MT.
The difference is that the two sentences that have
to be aligned are in the same language. We use the
IBM Model 1 (Brown et al., 1993) and the Hid-
den Markov Model (HMM, (Vogel et al., 1996))
to estimate the alignment model.
The alignment training corpus is created from a
test corpus1 of effectively M · (M − 1) · N sen-
tences translated by the involved MT engines. The
single-word based lexicon probabilities p(e|e&apos;) are
initialized from normalized lexicon counts col-
lected over the sentence pairs (Ent, En) on this
corpus. Since all of the hypotheses are in the same
language, we count co-occurring identical words,
i. e. whether ent,7 is the same word as en,i for some
i and j. In addition, we add a fraction of a count
for words with identical prefixes.
</bodyText>
<footnote confidence="0.998657666666667">
1A test corpus can be used directly because the align-
ment training is unsupervised and only automatically pro-
duced translations are considered.
</footnote>
<note confidence="0.4984985">
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 51–55,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999274">
51
</page>
<bodyText confidence="0.99992924">
The model parameters are trained iteratively us-
ing the GIZA++ toolkit (Och and Ney, 2003). The
training is performed in the directions Em —* En
and En —* Em. After each iteration, the updated
lexicon tables from the two directions are interpo-
lated. The final alignments are determined using
a cost matrix C for each sentence pair (Em, En).
Elements of this matrix are the local costs C(j, i)
of aligning a word em,7 from Em to a word en,z
from En. Following Matusov et al. (2004), we
compute these local costs by interpolating the
negated logarithms of the state occupation proba-
bilities from the “source-to-target” and “target-to-
source” training of the HMM model. Two differ-
ent alignments are computed using the cost matrix
C: the alignment a used for reordering each sec-
ondary translation Em, and the alignment a used
to build the confusion network.
In addition to the GIZA++ alignments, we have
also conducted preliminary experiments follow-
ing He et al. (2008) to exploit character-based
similarity, as well as estimating p(e|e&apos;) .=
E f p(e|f)p(f|e&apos;) directly from a bilingual lexi-
con. But we were not able to find improvements
over the GIZA++ alignments so far.
</bodyText>
<subsectionHeader confidence="0.9934965">
2.2 Word Reordering and Confusion
Network Generation
</subsectionHeader>
<bodyText confidence="0.999839">
After reordering each secondary hypothesis Em
and the rows of the corresponding alignment cost
matrix according to a, we determine M −1 mono-
tone one-to-one alignments between En as the pri-
mary translation and Em, m = 1, ... , M; m =� n.
We then construct the confusion network. In case
of many-to-one connections in a of words in Em
to a single word from En, we only keep the con-
nection with the lowest alignment costs.
The use of the one-to-one alignment a implies
that some words in the secondary translation will
not have a correspondence in the primary transla-
tion and vice versa. We consider these words to
have a null alignment with the empty word E. In
the corresponding confusion network, the empty
word will be transformed to an e-arc.
M − 1 monotone one-to-one alignments can
then be transformed into a confusion network. We
follow the approach of Bangalore et al. (2001)
with some extensions. Multiple insertions with re-
gard to the primary hypothesis are sub-aligned to
each other, as described by Matusov et al. (2008).
Figure 2 gives an example for the alignment.
</bodyText>
<subsectionHeader confidence="0.991965">
2.3 Voting in the confusion network
</subsectionHeader>
<bodyText confidence="0.999983294117647">
Instead of choosing a fixed sentence to define the
word order for the consensus translation, we gen-
erate confusion networks for all hypotheses as pri-
mary, and unite them into a single lattice. In our
experience, this approach is advantageous in terms
of translation quality, e.g. by 0.7% in BLEU com-
pared to a minimum Bayes risk primary (Rosti et
al., 2007). Weighted majority voting on a single
confusion network is straightforward and analo-
gous to ROVER (Fiscus, 1997). We sum up the
probabilities of the arcs which are labeled with the
same word and have the same start state and the
same end state. To exploit the true casing abilities
of the input MT systems, we sum up the scores of
arcs bearing the same word but in different cases.
Here, we leave the decision about upper or lower
case to the language model.
</bodyText>
<subsectionHeader confidence="0.995206">
2.4 Language Models
</subsectionHeader>
<bodyText confidence="0.999993636363636">
The lattice representing a union of several confu-
sion networks can then be directly rescored with
an n-gram language model (LM). A transforma-
tion of the lattice is required, since LM history has
to be memorized.
We train a trigram LM on the outputs of the sys-
tems involved in system combination. For LM
training, we took the system hypotheses for the
same test corpus for which the consensus trans-
lations are to be produced. Using this “adapted”
LM for lattice rescoring thus gives bonus to n-
grams from the original system hypotheses, in
most cases from the original phrases. Presum-
ably, many of these phrases have a correct word or-
der, since they are extracted from the training data.
Previous experimental results show that using this
LM in rescoring together with a word penalty (to
counteract any bias towards short sentences) no-
tably improves translation quality. This even re-
sults in better translations than using a “classical”
LM trained on a monolingual training corpus. We
attribute this to the fact that most of the systems
we combine are phrase-based systems, which al-
ready include such general LMs. Since we are us-
ing a true-cased LM trained on the hypotheses, we
can exploit true casing information from the in-
put systems by using this LM to disambiguate be-
tween the separate arcs generated for the variants
(see Section 2.3).
After LM rescoring, we add the probabilities of
identical partial paths to improve the estimation
of the score for the best hypothesis. This is done
through determinization of the lattice.
</bodyText>
<subsectionHeader confidence="0.978668">
2.5 Extracting Consensus Translations
</subsectionHeader>
<bodyText confidence="0.9999855">
To generate our consensus translation, we extract
the single-best path within the rescored confusion
network. With our approach, we could also extract
N-best hypotheses. In a subsequent step, these N-
best lists could be rescored with additional statis-
tical models (Matusov et al., 2008). But as we did
not have the resources in the WMT 2009 evalua-
tion, this step was dropped for our submission.
</bodyText>
<sectionHeader confidence="0.952243" genericHeader="method">
3 Tuning system weights
</sectionHeader>
<bodyText confidence="0.998553666666667">
System weights, LM factor, and word penalty
need to be tuned to produce good consensus trans-
lations. We optimize these parameters using the
</bodyText>
<page confidence="0.992581">
52
</page>
<table confidence="0.99875625">
system 0.25 would your like coffee or tea
hypotheses 0.35 have you tea or Coffee
0.10 would like your coffee or
0.30 I have some coffee tea would you like
alignment have|would you|your $|like Coffee|coffee or|or tea|tea
and would|would your|your like|like coffee|coffee or|or $|tea
reordering I|$ would|would you|your like|like have|$ some|$ coffee|coffee $|or tea|tea
confusion $ would your like $ $ coffee or tea
network $ have you $ $ $ Coffee or tea
$ would your like $ $ coffee or $
I would you like have some coffee $ tea
voting $ would you $ $ $ coffee or tea
(normalized) 0.7 0.65 0.65 0.35 0.7 0.7 0.5 0.7 0.9
I have your like have some Coffee $ $
0.3 0.35 0.35 0.65 0.3 0.3 0.5 0.3 0.1
consensus translation would you like coffee or tea
</table>
<figureCaption confidence="0.904850666666667">
Figure 2: Example of creating a confusion network from monotone one-to-one word alignments (denoted
with symbol |). The words of the primary hypothesis are printed in bold. The symbol $ denotes a null
alignment or an e-arc in the corresponding part of the confusion network.
</figureCaption>
<tableCaption confidence="0.839286666666667">
Table 1: Systems combined for the WMT 2009
task. Systems written in oblique were also used in
the Cross Lingual task (rbmt3 for FR-*EN).
</tableCaption>
<table confidence="0.791865833333333">
DE→EN google, liu, rbmt3, rwth, stutt-
gart, systran, uedin, uka, umd
ES→EN google, nict, rbmt4, rwth,
talp-upc, uedin
FR→EN dcu, google, jhu, limsi, lium-
systran, rbmt4, rwth, uedin, uka
</table>
<bodyText confidence="0.919579636363636">
publicly available CONDOR optimization toolkit
(Berghen and Bersini, 2005). For the WMT
2009 Workshop, we selected a linear combina-
tion of BLEU (Papineni et al., 2002) and TER
(Snover et al., 2006) as optimization criterion,
O := argmaxp {(2 · BLEU) − TER}, based on
previous experience (Mauser et al., 2008). We
used the whole dev set as a tuning set. For more
stable results, we used the case-insensitive variants
for both measures, despite the explicit use of case
information in our approach.
</bodyText>
<sectionHeader confidence="0.998535" genericHeader="method">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.999965">
Due to the large number of submissions (71 in
total for the language pairs DE-*EN, ES-*EN,
FR-*EN), we had to select a reasonable number
of systems to be able to tune the parameters in
a reliable way. Based on previous experience,
we manually selected the systems with the best
BLEU/TER score, and tried different variations of
this selection, e.g. by removing systems which
had low weights after optimization, or by adding
promising systems, like rule based systems.
Table 1 lists the systems which made it into
our final submission. In our experience, if a large
number of systems is available, using n-best trans-
lations does not give better results than using sin-
gle best translations, but raises optimization time
significantly. Consequently, we only used single
best translations from all systems.
The results also confirm another observation:
Even though rule-based systems by itself may
have significantly lower automatic evaluation
scores (e.g. by 2% or more in BLEU on DE-*EN),
they are often very important in system combina-
tion, and can improve the consensus translation
e.g. by 0.5% in BLEU.
Having submitted our translations to the WMT
workshop, we calculated scores on the WMT 2009
test set, to verify the results on the tuning data.
Both the results on the tuning set and on the test
set can be found in the following tables.
</bodyText>
<subsectionHeader confidence="0.998135">
4.1 The Google Problem
</subsectionHeader>
<bodyText confidence="0.999942">
One particular thing we noticed is that in the lan-
guage pairs of FR-*EN and ES-*EN, the trans-
lations from one provided single system (Google)
were much better in terms of BLEU and TER than
those of all other systems – in the former case
by more than 4% in BLEU. In our experience,
our system combination approach requires at least
three “comparably good” systems to be able to
achieve significant improvements. This was con-
firmed in the WMT 2009 task as well: Neither in
FR-*EN nor in ES-*EN we were able to achieve
an improvement over the Google system. For this
reason, we did not submit consensus translations
for these two language pairs. On the other hand,
we would have achieved significant improvements
over all (remaining) systems leaving out Google.
</bodyText>
<subsectionHeader confidence="0.979378">
4.2 German-*English (DE-*EN)
</subsectionHeader>
<bodyText confidence="0.999183666666667">
Table 2 lists the scores on the tuning and test set
for the DE-*EN task. We can see that the best
systems are rather close to each other in terms
of BLEU. Also, the rule-based translation system
(RBMT), here SYSTRAN, scores rather well. As
a consequence, we find a large improvement using
system combination: 2.9%/2.7% abs. on the tun-
ing set, and still 2.1%/2.3% on test, which means
that system combination generalizes well here.
</bodyText>
<subsectionHeader confidence="0.723103">
4.3 Spanish-*English (ES-*EN),
French-*English (FR-*EN)
</subsectionHeader>
<bodyText confidence="0.972378333333333">
In Table 3, we see that on the ES-*EN and
FR-*EN tasks, a single system – Google – scores
significantly better on the TUNE set than any other
</bodyText>
<page confidence="0.999624">
53
</page>
<tableCaption confidence="0.68706675">
Table 2: German-*English task: case-insensitive
scores. Best single system was Google, second
best UKA, best RBMT Systran. SC stands for sys-
tem combination output.
</tableCaption>
<table confidence="0.999920375">
German-*English TUNE TER TEST TER
BLEU BLEU
Best single 23.2 59.5 21.3 61.3
Second best single 23.0 58.8 21.0 61.7
Best RBMT 21.3 61.3 18.9 63.7
SC (9 systems) 26.1 56.8 23.4 59.0
w/o RBMT 24.5 57.3 22.5 59.2
w/o Google 24.9 57.4 23.0 59.1
</table>
<tableCaption confidence="0.997059">
Table 3: Spanish-*English and French-*English
</tableCaption>
<bodyText confidence="0.6882884">
task: scores on the tuning set after system combi-
nation weight tuning (case-insensitive). Best sin-
gle system was Google, second best was Uedin
(Spanish) and UKA (French). No results on TEST
were generated.
</bodyText>
<table confidence="0.999124">
Spanish-*English ES-*EN TER FR-*EN TER
BLEU BLEU
Best single 29.5 53.6 32.2 50.1
Second best single 26.9 56.1 28.0 54.6
SC (6/9 systems) 28.7 53.6 30.7 52.5
w/o Google 27.5 55.6 30.0 52.8
</table>
<bodyText confidence="0.999963058823529">
system, namely by 2.6%/4.2% resp. in BLEU. As
a result, a combination of these systems scores
better than any other system, even when leaving
out the Google system. But it gives worse scores
than the single best system. This is explainable,
because system combination is trying to find a
consensus translation. For example, in one case,
the majority of the systems leave the French term
“wagon-lit” untranslated; spurious translations in-
clude “baggage car”, “sleeping car”, and “alive”.
As a result, the consensus translation also contains
“wagon-lit”, not the correct translation “sleeper”
which only the Google system provides. Even tun-
ing all other system weights to zero would not re-
sult in pure Google translations, as these weights
neither affect the LM nor the selection of the pri-
mary hypothesis in our approach.
</bodyText>
<subsectionHeader confidence="0.99929">
4.4 Cross-Lingual-*English (XX-*EN)
</subsectionHeader>
<bodyText confidence="0.987734818181818">
Finally, we have conducted experiments on cross-
lingual system combination, namely combining
the output from DE-*EN, ES-*EN, and FR-*EN
systems to a single English consensus transla-
tion. Some interesting results can be found in
Table 4. We see that this consensus translation
scores 2.0%/3.5% better than the best single sys-
tem, and 4.4%/5.6% better than the second best
single system. While this is only 0.8%/2.5% bet-
ter than the combination of only the three Google
systems, the combination of the non-Google sys-
</bodyText>
<tableCaption confidence="0.935003">
Table 4: Cross-lingual task: combination
</tableCaption>
<table confidence="0.983718733333333">
of German-*English, Spanish-*English, and
French-*English. Case-insensitive scores. Best
single system was Google for all language pairs.
Cross-lingual TUNE TER TEST TER
-* English BLEU BLEU
Best single German 23.2 59.5 21.3 61.3
Best single Spanish 29.5 53.6 28.7 53.8
Best single French 32.2 50.1 31.1 51.7
SC (10 systems) 35.5 46.4 33.1 48.2
w/o RBMT 35.1 46.5 32.7 48.3
w/o Google 32.3 48.8 29.9 50.5
3 Google systems 34.2 48.0 32.3 49.2
w/o German 34.0 49.3 31.5 50.9
w/o Spanish 33.4 49.8 31.0 51.9
w/o French 30.5 51.4 28.6 52.3
</table>
<bodyText confidence="0.999381214285714">
tems leads to translations that could compete with
the FR-*EN Google system. Again, we see that
RBMT systems lead to a small improvement of
0.4% in BLEU, although their scores are signif-
icantly worse than those of the competing SMT
systems.
Regarding languages, we see that despite the
large differences in the quality of the systems (10
points between DE-*EN and FR-*EN), all lan-
guages seem to provide significant information to
the consensus translation: While FR-*EN cer-
tainly has the largest influence (−4.5% in BLEU
when left out), even DE-*EN “contributes” 1.6
BLEU points to the final submission.
</bodyText>
<sectionHeader confidence="0.999567" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999980230769231">
We have shown that our system combination sys-
tem can lead to significant improvements over
single best MT output where a significant num-
ber of comparably good translations is available
on a single language pair. For cross-lingual sys-
tem combination, we observe even larger improve-
ments, even if the quality in terms of BLEU or
TER between the systems of different language
pairs varies significantly. While the input of high-
quality SMT systems has the largest weight for the
consensus translation quality, we find that RBMT
systems can give important additional information
leading to better translations.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997609833333333">
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This work was
partly supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023.
</bodyText>
<page confidence="0.997725">
54
</page>
<sectionHeader confidence="0.990282" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999553487179487">
S. Bangalore, G. Bordel, and G. Riccardi. 2001.
Computing consensus translation from multiple ma-
chine translation systems. In IEEE Automatic
Speech Recognition and Understanding Workshop,
Madonna di Campiglio, Italy, December.
F. V. Berghen and H. Bersini. 2005. CONDOR,
a new parallel, constrained extension of Powell’s
UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157–
175.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263–311, June.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Workshop on Au-
tomatic Speech Recognition and Understanding.
X. He, M. Yang, J. Gao, P. Nguyen, and R. Moore.
2008. Indirect-HMM-based hypothesis alignment
for combining outputs from machine translation sys-
tems. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 98–107, Honolulu, Hawaii, October.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of the 10th Annual Conf. of the European
Association for Machine Translation (EAMT), pages
143–152, Budapest, Hungary, May.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation.
In COLING ’04: The 20th Int. Conf. on Computa-
tional Linguistics, pages 219–225, Geneva, Switzer-
land, August.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33–40, Trento, Italy, April.
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee,
J. B. Marino, M. Paulik, S. Roukos, H. Schwenk,
and H. Ney. 2008. System combination for machine
translation of spoken and written language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222–1237, September.
A. Mauser, S. Hasan, and H. Ney. 2008. Automatic
evaluation measures for statistical machine transla-
tion system optimization. In International Confer-
ence on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51, March.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311–318, Philadelphia, PA, July.
A. V. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 312–319, Prague, Czech Re-
public, June.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Error
Rate with Targeted Human Annotation. In Proc. of
the 7th Conf. of the Association for Machine Trans-
lation in the Americas (AMTA), pages 223–231,
Boston, MA, August.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ’96: The 16th Int. Conf. on Computational
Linguistics, pages 836–841, Copenhagen, Denmark,
August.
</reference>
<page confidence="0.999044">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.890721">
<title confidence="0.923413">The RWTH System Combination System for WMT 2009</title>
<author confidence="0.993192">Gregor Leusch</author>
<author confidence="0.993192">Evgeny Matusov</author>
<author confidence="0.993192">Hermann</author>
<affiliation confidence="0.981167">RWTH Aachen</affiliation>
<address confidence="0.993377">Aachen, Germany</address>
<abstract confidence="0.999348733333333">RWTH participated in the System Combination task of the Fourth Workshop on Statistical Machine Translation (WMT 2009). from 9 MT systems were combined into a consensus translation. This consensus translascored 2.1% better in 2.3% in than the best single system. In addition, cross-lingual output from 10 French, German, and systems was combined into a consensus translation, which gave improvement of 2.0% in in over the best single system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>G Bordel</author>
<author>G Riccardi</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems.</title>
<date>2001</date>
<booktitle>In IEEE Automatic Speech Recognition and Understanding Workshop, Madonna di Campiglio,</booktitle>
<location>Italy,</location>
<contexts>
<context position="6230" citStr="Bangalore et al. (2001)" startWordPosition="1043" endWordPosition="1046"> confusion network. In case of many-to-one connections in a of words in Em to a single word from En, we only keep the connection with the lowest alignment costs. The use of the one-to-one alignment a implies that some words in the secondary translation will not have a correspondence in the primary translation and vice versa. We consider these words to have a null alignment with the empty word E. In the corresponding confusion network, the empty word will be transformed to an e-arc. M − 1 monotone one-to-one alignments can then be transformed into a confusion network. We follow the approach of Bangalore et al. (2001) with some extensions. Multiple insertions with regard to the primary hypothesis are sub-aligned to each other, as described by Matusov et al. (2008). Figure 2 gives an example for the alignment. 2.3 Voting in the confusion network Instead of choosing a fixed sentence to define the word order for the consensus translation, we generate confusion networks for all hypotheses as primary, and unite them into a single lattice. In our experience, this approach is advantageous in terms of translation quality, e.g. by 0.7% in BLEU compared to a minimum Bayes risk primary (Rosti et al., 2007). Weighted </context>
</contexts>
<marker>Bangalore, Bordel, Riccardi, 2001</marker>
<rawString>S. Bangalore, G. Bordel, and G. Riccardi. 2001. Computing consensus translation from multiple machine translation systems. In IEEE Automatic Speech Recognition and Understanding Workshop, Madonna di Campiglio, Italy, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F V Berghen</author>
<author>H Bersini</author>
</authors>
<title>CONDOR, a new parallel, constrained extension of Powell’s UOBYQA algorithm: Experimental results and comparison with the DFO algorithm.</title>
<date>2005</date>
<journal>Journal of Computational and Applied Mathematics,</journal>
<volume>181</volume>
<pages>175</pages>
<contexts>
<context position="10846" citStr="Berghen and Bersini, 2005" startWordPosition="1830" endWordPosition="1833">work from monotone one-to-one word alignments (denoted with symbol |). The words of the primary hypothesis are printed in bold. The symbol $ denotes a null alignment or an e-arc in the corresponding part of the confusion network. Table 1: Systems combined for the WMT 2009 task. Systems written in oblique were also used in the Cross Lingual task (rbmt3 for FR-*EN). DE→EN google, liu, rbmt3, rwth, stuttgart, systran, uedin, uka, umd ES→EN google, nict, rbmt4, rwth, talp-upc, uedin FR→EN dcu, google, jhu, limsi, liumsystran, rbmt4, rwth, uedin, uka publicly available CONDOR optimization toolkit (Berghen and Bersini, 2005). For the WMT 2009 Workshop, we selected a linear combination of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as optimization criterion, O := argmaxp {(2 · BLEU) − TER}, based on previous experience (Mauser et al., 2008). We used the whole dev set as a tuning set. For more stable results, we used the case-insensitive variants for both measures, despite the explicit use of case information in our approach. 4 Experimental results Due to the large number of submissions (71 in total for the language pairs DE-*EN, ES-*EN, FR-*EN), we had to select a reasonable number of systems to be </context>
</contexts>
<marker>Berghen, Bersini, 2005</marker>
<rawString>F. V. Berghen and H. Bersini. 2005. CONDOR, a new parallel, constrained extension of Powell’s UOBYQA algorithm: Experimental results and comparison with the DFO algorithm. Journal of Computational and Applied Mathematics, 181:157– 175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3184" citStr="Brown et al., 1993" startWordPosition="524" endWordPosition="527">ect one of its translations En, n=1, ... , M, as the primary hypothesis. Then we align the secondary hypotheses Ent(m = 1, ... , M; n =� m) with En to match the word order in En. Since it is not clear which hypothesis should be primary, i. e. has the “best” word order, we let every hypothesis play the role of the primary translation, and align all pairs of hypotheses (En, Ent); n =� m. The word alignment is trained in analogy to the alignment training procedure in statistical MT. The difference is that the two sentences that have to be aligned are in the same language. We use the IBM Model 1 (Brown et al., 1993) and the Hidden Markov Model (HMM, (Vogel et al., 1996)) to estimate the alignment model. The alignment training corpus is created from a test corpus1 of effectively M · (M − 1) · N sentences translated by the involved MT engines. The single-word based lexicon probabilities p(e|e&apos;) are initialized from normalized lexicon counts collected over the sentence pairs (Ent, En) on this corpus. Since all of the hypotheses are in the same language, we count co-occurring identical words, i. e. whether ent,7 is the same word as en,i for some i and j. In addition, we add a fraction of a count for words wi</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER).</title>
<date>1997</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="806" citStr="Fiscus, 1997" startWordPosition="126" endWordPosition="127">f the Fourth Workshop on Statistical Machine Translation (WMT 2009). Hypotheses from 9 German—*English MT systems were combined into a consensus translation. This consensus translation scored 2.1% better in BLEU and 2.3% better in TER (abs.) than the best single system. In addition, cross-lingual output from 10 French, German, and Spanish—*English systems was combined into a consensus translation, which gave an improvement of 2.0% in BLEU/3.5% in TER (abs.) over the best single system. 1 Introduction The RWTH approach to MT system combination is a refined version of the ROVER approach in ASR (Fiscus, 1997), with additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses. The basic concept of the approach has been described by Matusov et al. (2006). Several improvements have been added later (Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. In contrast to existing approaches (Jayaraman and Lavie, 2005; Rosti et al., 2007), the context of the whole corpus rather than a single sentence is considered in this iterative, unsupervised procedure, yielding a more reliable alignment. Majo</context>
<context position="6932" citStr="Fiscus, 1997" startWordPosition="1162" endWordPosition="1163">aligned to each other, as described by Matusov et al. (2008). Figure 2 gives an example for the alignment. 2.3 Voting in the confusion network Instead of choosing a fixed sentence to define the word order for the consensus translation, we generate confusion networks for all hypotheses as primary, and unite them into a single lattice. In our experience, this approach is advantageous in terms of translation quality, e.g. by 0.7% in BLEU compared to a minimum Bayes risk primary (Rosti et al., 2007). Weighted majority voting on a single confusion network is straightforward and analogous to ROVER (Fiscus, 1997). We sum up the probabilities of the arcs which are labeled with the same word and have the same start state and the same end state. To exploit the true casing abilities of the input MT systems, we sum up the scores of arcs bearing the same word but in different cases. Here, we leave the decision about upper or lower case to the language model. 2.4 Language Models The lattice representing a union of several confusion networks can then be directly rescored with an n-gram language model (LM). A transformation of the lattice is required, since LM history has to be memorized. We train a trigram LM</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>J. Fiscus. 1997. A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER). In IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X He</author>
<author>M Yang</author>
<author>J Gao</author>
<author>P Nguyen</author>
<author>R Moore</author>
</authors>
<title>Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>98--107</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="5093" citStr="He et al. (2008)" startWordPosition="846" endWordPosition="849">s of this matrix are the local costs C(j, i) of aligning a word em,7 from Em to a word en,z from En. Following Matusov et al. (2004), we compute these local costs by interpolating the negated logarithms of the state occupation probabilities from the “source-to-target” and “target-tosource” training of the HMM model. Two different alignments are computed using the cost matrix C: the alignment a used for reordering each secondary translation Em, and the alignment a used to build the confusion network. In addition to the GIZA++ alignments, we have also conducted preliminary experiments following He et al. (2008) to exploit character-based similarity, as well as estimating p(e|e&apos;) .= E f p(e|f)p(f|e&apos;) directly from a bilingual lexicon. But we were not able to find improvements over the GIZA++ alignments so far. 2.2 Word Reordering and Confusion Network Generation After reordering each secondary hypothesis Em and the rows of the corresponding alignment cost matrix according to a, we determine M −1 monotone one-to-one alignments between En as the primary translation and Em, m = 1, ... , M; m =� n. We then construct the confusion network. In case of many-to-one connections in a of words in Em to a single</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>X. He, M. Yang, J. Gao, P. Nguyen, and R. Moore. 2008. Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 98–107, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jayaraman</author>
<author>A Lavie</author>
</authors>
<title>Multi-engine machine translation guided by explicit word matching.</title>
<date>2005</date>
<booktitle>In Proc. of the 10th Annual Conf. of the European Association for Machine Translation (EAMT),</booktitle>
<pages>143--152</pages>
<location>Budapest, Hungary,</location>
<contexts>
<context position="1224" citStr="Jayaraman and Lavie, 2005" startWordPosition="188" endWordPosition="191"> gave an improvement of 2.0% in BLEU/3.5% in TER (abs.) over the best single system. 1 Introduction The RWTH approach to MT system combination is a refined version of the ROVER approach in ASR (Fiscus, 1997), with additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses. The basic concept of the approach has been described by Matusov et al. (2006). Several improvements have been added later (Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. In contrast to existing approaches (Jayaraman and Lavie, 2005; Rosti et al., 2007), the context of the whole corpus rather than a single sentence is considered in this iterative, unsupervised procedure, yielding a more reliable alignment. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models such as a special n-gram language model. 2 System Combination Algorithm In this section we present the details of our system combination method. Figure 1 gives an overview of the system combination architecture described in this section. After preprocessing the MT hypotheses, pairwise </context>
</contexts>
<marker>Jayaraman, Lavie, 2005</marker>
<rawString>S. Jayaraman and A. Lavie. 2005. Multi-engine machine translation guided by explicit word matching. In Proc. of the 10th Annual Conf. of the European Association for Machine Translation (EAMT), pages 143–152, Budapest, Hungary, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Symmetric word alignments for statistical machine translation.</title>
<date>2004</date>
<booktitle>In COLING ’04: The 20th Int. Conf. on Computational Linguistics,</booktitle>
<pages>219--225</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="4609" citStr="Matusov et al. (2004)" startWordPosition="769" endWordPosition="772"> Statistical Machine Translation , pages 51–55, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 51 The model parameters are trained iteratively using the GIZA++ toolkit (Och and Ney, 2003). The training is performed in the directions Em —* En and En —* Em. After each iteration, the updated lexicon tables from the two directions are interpolated. The final alignments are determined using a cost matrix C for each sentence pair (Em, En). Elements of this matrix are the local costs C(j, i) of aligning a word em,7 from Em to a word en,z from En. Following Matusov et al. (2004), we compute these local costs by interpolating the negated logarithms of the state occupation probabilities from the “source-to-target” and “target-tosource” training of the HMM model. Two different alignments are computed using the cost matrix C: the alignment a used for reordering each secondary translation Em, and the alignment a used to build the confusion network. In addition to the GIZA++ alignments, we have also conducted preliminary experiments following He et al. (2008) to exploit character-based similarity, as well as estimating p(e|e&apos;) .= E f p(e|f)p(f|e&apos;) directly from a bilingual</context>
</contexts>
<marker>Matusov, Zens, Ney, 2004</marker>
<rawString>E. Matusov, R. Zens, and H. Ney. 2004. Symmetric word alignments for statistical machine translation. In COLING ’04: The 20th Int. Conf. on Computational Linguistics, pages 219–225, Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>33--40</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="1023" citStr="Matusov et al. (2006)" startWordPosition="159" endWordPosition="162">EU and 2.3% better in TER (abs.) than the best single system. In addition, cross-lingual output from 10 French, German, and Spanish—*English systems was combined into a consensus translation, which gave an improvement of 2.0% in BLEU/3.5% in TER (abs.) over the best single system. 1 Introduction The RWTH approach to MT system combination is a refined version of the ROVER approach in ASR (Fiscus, 1997), with additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses. The basic concept of the approach has been described by Matusov et al. (2006). Several improvements have been added later (Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. In contrast to existing approaches (Jayaraman and Lavie, 2005; Rosti et al., 2007), the context of the whole corpus rather than a single sentence is considered in this iterative, unsupervised procedure, yielding a more reliable alignment. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models such as a special n-gram language model. 2 System Combination Algorithm In this secti</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>E. Matusov, N. Ueffing, and H. Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 33–40, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>G Leusch</author>
<author>R E Banchs</author>
<author>N Bertoldi</author>
<author>D Dechelotte</author>
<author>M Federico</author>
<author>M Kolss</author>
<author>Y S Lee</author>
<author>J B Marino</author>
<author>M Paulik</author>
<author>S Roukos</author>
<author>H Schwenk</author>
<author>H Ney</author>
</authors>
<title>System combination for machine translation of spoken and written language.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>16</volume>
<issue>7</issue>
<contexts>
<context position="1090" citStr="Matusov et al., 2008" startWordPosition="169" endWordPosition="172">dition, cross-lingual output from 10 French, German, and Spanish—*English systems was combined into a consensus translation, which gave an improvement of 2.0% in BLEU/3.5% in TER (abs.) over the best single system. 1 Introduction The RWTH approach to MT system combination is a refined version of the ROVER approach in ASR (Fiscus, 1997), with additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses. The basic concept of the approach has been described by Matusov et al. (2006). Several improvements have been added later (Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. In contrast to existing approaches (Jayaraman and Lavie, 2005; Rosti et al., 2007), the context of the whole corpus rather than a single sentence is considered in this iterative, unsupervised procedure, yielding a more reliable alignment. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models such as a special n-gram language model. 2 System Combination Algorithm In this section we present the details of our system combination method. Figure </context>
<context position="6379" citStr="Matusov et al. (2008)" startWordPosition="1067" endWordPosition="1070">ent costs. The use of the one-to-one alignment a implies that some words in the secondary translation will not have a correspondence in the primary translation and vice versa. We consider these words to have a null alignment with the empty word E. In the corresponding confusion network, the empty word will be transformed to an e-arc. M − 1 monotone one-to-one alignments can then be transformed into a confusion network. We follow the approach of Bangalore et al. (2001) with some extensions. Multiple insertions with regard to the primary hypothesis are sub-aligned to each other, as described by Matusov et al. (2008). Figure 2 gives an example for the alignment. 2.3 Voting in the confusion network Instead of choosing a fixed sentence to define the word order for the consensus translation, we generate confusion networks for all hypotheses as primary, and unite them into a single lattice. In our experience, this approach is advantageous in terms of translation quality, e.g. by 0.7% in BLEU compared to a minimum Bayes risk primary (Rosti et al., 2007). Weighted majority voting on a single confusion network is straightforward and analogous to ROVER (Fiscus, 1997). We sum up the probabilities of the arcs which</context>
<context position="9152" citStr="Matusov et al., 2008" startWordPosition="1536" endWordPosition="1539">ems by using this LM to disambiguate between the separate arcs generated for the variants (see Section 2.3). After LM rescoring, we add the probabilities of identical partial paths to improve the estimation of the score for the best hypothesis. This is done through determinization of the lattice. 2.5 Extracting Consensus Translations To generate our consensus translation, we extract the single-best path within the rescored confusion network. With our approach, we could also extract N-best hypotheses. In a subsequent step, these Nbest lists could be rescored with additional statistical models (Matusov et al., 2008). But as we did not have the resources in the WMT 2009 evaluation, this step was dropped for our submission. 3 Tuning system weights System weights, LM factor, and word penalty need to be tuned to produce good consensus translations. We optimize these parameters using the 52 system 0.25 would your like coffee or tea hypotheses 0.35 have you tea or Coffee 0.10 would like your coffee or 0.30 I have some coffee tea would you like alignment have|would you|your $|like Coffee|coffee or|or tea|tea and would|would your|your like|like coffee|coffee or|or $|tea reordering I|$ would|would you|your like|l</context>
</contexts>
<marker>Matusov, Leusch, Banchs, Bertoldi, Dechelotte, Federico, Kolss, Lee, Marino, Paulik, Roukos, Schwenk, Ney, 2008</marker>
<rawString>E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee, J. B. Marino, M. Paulik, S. Roukos, H. Schwenk, and H. Ney. 2008. System combination for machine translation of spoken and written language. IEEE Transactions on Audio, Speech and Language Processing, 16(7):1222–1237, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mauser</author>
<author>S Hasan</author>
<author>H Ney</author>
</authors>
<title>Automatic evaluation measures for statistical machine translation system optimization.</title>
<date>2008</date>
<booktitle>In International Conference on Language Resources and Evaluation,</booktitle>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="11080" citStr="Mauser et al., 2008" startWordPosition="1872" endWordPosition="1875">ystems combined for the WMT 2009 task. Systems written in oblique were also used in the Cross Lingual task (rbmt3 for FR-*EN). DE→EN google, liu, rbmt3, rwth, stuttgart, systran, uedin, uka, umd ES→EN google, nict, rbmt4, rwth, talp-upc, uedin FR→EN dcu, google, jhu, limsi, liumsystran, rbmt4, rwth, uedin, uka publicly available CONDOR optimization toolkit (Berghen and Bersini, 2005). For the WMT 2009 Workshop, we selected a linear combination of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as optimization criterion, O := argmaxp {(2 · BLEU) − TER}, based on previous experience (Mauser et al., 2008). We used the whole dev set as a tuning set. For more stable results, we used the case-insensitive variants for both measures, despite the explicit use of case information in our approach. 4 Experimental results Due to the large number of submissions (71 in total for the language pairs DE-*EN, ES-*EN, FR-*EN), we had to select a reasonable number of systems to be able to tune the parameters in a reliable way. Based on previous experience, we manually selected the systems with the best BLEU/TER score, and tried different variations of this selection, e.g. by removing systems which had low weigh</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2008</marker>
<rawString>A. Mauser, S. Hasan, and H. Ney. 2008. Automatic evaluation measures for statistical machine translation system optimization. In International Conference on Language Resources and Evaluation, Marrakech, Morocco, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4219" citStr="Och and Ney, 2003" startWordPosition="697" endWordPosition="700"> are in the same language, we count co-occurring identical words, i. e. whether ent,7 is the same word as en,i for some i and j. In addition, we add a fraction of a count for words with identical prefixes. 1A test corpus can be used directly because the alignment training is unsupervised and only automatically produced translations are considered. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 51–55, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 51 The model parameters are trained iteratively using the GIZA++ toolkit (Och and Ney, 2003). The training is performed in the directions Em —* En and En —* Em. After each iteration, the updated lexicon tables from the two directions are interpolated. The final alignments are determined using a cost matrix C for each sentence pair (Em, En). Elements of this matrix are the local costs C(j, i) of aligning a word em,7 from Em to a word en,z from En. Following Matusov et al. (2004), we compute these local costs by interpolating the negated logarithms of the state occupation probabilities from the “source-to-target” and “target-tosource” training of the HMM model. Two different alignments</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="10939" citStr="Papineni et al., 2002" startWordPosition="1847" endWordPosition="1850">ypothesis are printed in bold. The symbol $ denotes a null alignment or an e-arc in the corresponding part of the confusion network. Table 1: Systems combined for the WMT 2009 task. Systems written in oblique were also used in the Cross Lingual task (rbmt3 for FR-*EN). DE→EN google, liu, rbmt3, rwth, stuttgart, systran, uedin, uka, umd ES→EN google, nict, rbmt4, rwth, talp-upc, uedin FR→EN dcu, google, jhu, limsi, liumsystran, rbmt4, rwth, uedin, uka publicly available CONDOR optimization toolkit (Berghen and Bersini, 2005). For the WMT 2009 Workshop, we selected a linear combination of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as optimization criterion, O := argmaxp {(2 · BLEU) − TER}, based on previous experience (Mauser et al., 2008). We used the whole dev set as a tuning set. For more stable results, we used the case-insensitive variants for both measures, despite the explicit use of case information in our approach. 4 Experimental results Due to the large number of submissions (71 in total for the language pairs DE-*EN, ES-*EN, FR-*EN), we had to select a reasonable number of systems to be able to tune the parameters in a reliable way. Based on previous experience, we manually sele</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Rosti</author>
<author>S Matsoukas</author>
<author>R Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<pages>312--319</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1245" citStr="Rosti et al., 2007" startWordPosition="192" endWordPosition="195">% in BLEU/3.5% in TER (abs.) over the best single system. 1 Introduction The RWTH approach to MT system combination is a refined version of the ROVER approach in ASR (Fiscus, 1997), with additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses. The basic concept of the approach has been described by Matusov et al. (2006). Several improvements have been added later (Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. In contrast to existing approaches (Jayaraman and Lavie, 2005; Rosti et al., 2007), the context of the whole corpus rather than a single sentence is considered in this iterative, unsupervised procedure, yielding a more reliable alignment. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models such as a special n-gram language model. 2 System Combination Algorithm In this section we present the details of our system combination method. Figure 1 gives an overview of the system combination architecture described in this section. After preprocessing the MT hypotheses, pairwise alignments between th</context>
<context position="6819" citStr="Rosti et al., 2007" startWordPosition="1143" endWordPosition="1146">ach of Bangalore et al. (2001) with some extensions. Multiple insertions with regard to the primary hypothesis are sub-aligned to each other, as described by Matusov et al. (2008). Figure 2 gives an example for the alignment. 2.3 Voting in the confusion network Instead of choosing a fixed sentence to define the word order for the consensus translation, we generate confusion networks for all hypotheses as primary, and unite them into a single lattice. In our experience, this approach is advantageous in terms of translation quality, e.g. by 0.7% in BLEU compared to a minimum Bayes risk primary (Rosti et al., 2007). Weighted majority voting on a single confusion network is straightforward and analogous to ROVER (Fiscus, 1997). We sum up the probabilities of the arcs which are labeled with the same word and have the same start state and the same end state. To exploit the true casing abilities of the input MT systems, we sum up the scores of arcs bearing the same word but in different cases. Here, we leave the decision about upper or lower case to the language model. 2.4 Language Models The lattice representing a union of several confusion networks can then be directly rescored with an n-gram language mod</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>A. V. Rosti, S. Matsoukas, and R. Schwartz. 2007. Improved word-level system combination for machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL), pages 312–319, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A Study of Translation Error Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proc. of the 7th Conf. of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>223--231</pages>
<location>Boston, MA,</location>
<contexts>
<context position="10969" citStr="Snover et al., 2006" startWordPosition="1853" endWordPosition="1856">he symbol $ denotes a null alignment or an e-arc in the corresponding part of the confusion network. Table 1: Systems combined for the WMT 2009 task. Systems written in oblique were also used in the Cross Lingual task (rbmt3 for FR-*EN). DE→EN google, liu, rbmt3, rwth, stuttgart, systran, uedin, uka, umd ES→EN google, nict, rbmt4, rwth, talp-upc, uedin FR→EN dcu, google, jhu, limsi, liumsystran, rbmt4, rwth, uedin, uka publicly available CONDOR optimization toolkit (Berghen and Bersini, 2005). For the WMT 2009 Workshop, we selected a linear combination of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as optimization criterion, O := argmaxp {(2 · BLEU) − TER}, based on previous experience (Mauser et al., 2008). We used the whole dev set as a tuning set. For more stable results, we used the case-insensitive variants for both measures, despite the explicit use of case information in our approach. 4 Experimental results Due to the large number of submissions (71 in total for the language pairs DE-*EN, ES-*EN, FR-*EN), we had to select a reasonable number of systems to be able to tune the parameters in a reliable way. Based on previous experience, we manually selected the systems with the best</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A Study of Translation Error Rate with Targeted Human Annotation. In Proc. of the 7th Conf. of the Association for Machine Translation in the Americas (AMTA), pages 223–231, Boston, MA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMMbased word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING ’96: The 16th Int. Conf. on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="3239" citStr="Vogel et al., 1996" startWordPosition="535" endWordPosition="538">imary hypothesis. Then we align the secondary hypotheses Ent(m = 1, ... , M; n =� m) with En to match the word order in En. Since it is not clear which hypothesis should be primary, i. e. has the “best” word order, we let every hypothesis play the role of the primary translation, and align all pairs of hypotheses (En, Ent); n =� m. The word alignment is trained in analogy to the alignment training procedure in statistical MT. The difference is that the two sentences that have to be aligned are in the same language. We use the IBM Model 1 (Brown et al., 1993) and the Hidden Markov Model (HMM, (Vogel et al., 1996)) to estimate the alignment model. The alignment training corpus is created from a test corpus1 of effectively M · (M − 1) · N sentences translated by the involved MT engines. The single-word based lexicon probabilities p(e|e&apos;) are initialized from normalized lexicon counts collected over the sentence pairs (Ent, En) on this corpus. Since all of the hypotheses are in the same language, we count co-occurring identical words, i. e. whether ent,7 is the same word as en,i for some i and j. In addition, we add a fraction of a count for words with identical prefixes. 1A test corpus can be used direc</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMMbased word alignment in statistical translation. In COLING ’96: The 16th Int. Conf. on Computational Linguistics, pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>