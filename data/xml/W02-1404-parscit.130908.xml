<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000745">
<title confidence="0.978272">
Alignment and Extraction of Bilingual Legal Terminology from
Context Profiles
</title>
<author confidence="0.7918065">
Oi Yee Kwong, Benjamin K. Tsou, Tom B.Y. Lai, Robert W.P. Lukt,
Lawrence Y.L. Cheung and Francis C.Y. Chik
</author>
<affiliation confidence="0.809464666666667">
Language Information Sciences Research Centre tDepartment of Computing
City University of Hong Kong Hong Kong Polytechnic University
Tat Chee Avenue, Kowloon, Hong Kong Hung Hom, Kowloon, Hong Kong
</affiliation>
<address confidence="0.413521">
{ rlolivia,rlbtsou,cttomlai,rlylc,r1fchik}Ocityu.edu.hk csrlukO comp .polyu.edu.hk
</address>
<sectionHeader confidence="0.959015" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977210526316">
In this study, we propose a knowledge-
independent method for aligning terms and thus
extracting translations from a small, domain-
specific corpus consisting of parallel English
and Chinese court judgments from Hong Kong.
With a sentence-aligned corpus, translation
equivalences are suggested by analysing the fre-
quency profiles of parallel concordances. The
method overcomes the limitations of conven-
tional statistical methods which require large
corpora to be effective, and lexical approaches
which depend on existing bilingual dictionaries.
Pilot testing on a parallel corpus of about 113K
Chinese words and 120K English words gives
an encouraging 85% precision and 45% recall.
Future work includes fine-tuning the algorithm
upon the analysis of the errors, and acquiring a
translation lexicon for legal terminology by fil-
tering out general terms.
</bodyText>
<sectionHeader confidence="0.998732" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999751340425532">
Machine translation, parallel text alignment,
and translation lexicons are the vertices in their
tightly bound triangular relation. The mutual
relation between translation lexicons and par-
allel text alignment is especially close as they
together provide foundational resources for the
research of machine translation.
Conventional statistical methods for bilingual
word alignment and extraction of translation
lexicon require large corpora to be effective.
Lexical approaches, on the other hand, depend
on existing bilingual dictionaries which often
only cover general terms. In any case, both
methods will probably fall short with a small
and domain-specific corpus, as the one in our
study.
The parallel corpus in this study consists of
bilingual Hong Kong court judgments, in En-
glish and Chinese. It is a small and domain-
specific corpus. Thus on the one hand, we
can imagine that existing bilingual dictionaries
would be of little help in the alignment process,
as general lexicons do not often cover legal ter-
minologies and their translations. On the other
hand, the effectiveness of statistical or proba-
bilistic approaches might not be realised, given
the limited corpus size. Hence, neither approach
fits the data ideally. Moreover, English and Chi-
nese are from different language families. Also,
legal terms are not limited to single words.
Hence, we propose a method for aligning
terms in this small corpus of legal texts and ex-
tracting a translation lexicon therefrom, based
on the consistency observed in legal texts. The
method only requires a sentence-aligned corpus
and suggests translation equivalences from the
frequency profiles of parallel concordances. Pi-
lot testing shows that the method is effective for
fulfilling the two purposes, i.e. term alignment
and lexicon extraction, simultaneously.
In the following, we will first review related
studies in Section 2. Then we will describe the
properties of our corpus in Section 3 and our
method in Section 4. A pilot experiment with
the proposed approach will be reported in Sec-
tion 5 with results discussed in Section 6, fol-
lowed by a conclusion in Section 7.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999938404255319">
Conventional bilingual sentence alignment is of-
ten based statistically on sentence length (e.g.
Gale and Church, 1991), or lexically on cognates
(e.g. Simard et al., 1992) and correspondence
of word position (e.g. Kay and Roescheisen,
1993; Piperidis et al., 1997). Such criteria, how-
ever, are mostly applicable to Indo-European
language pairs. Although Wu (1994) found the
length criterion applied surprisingly well be-
tween English and Chinese, he supplemented
the statistical, length-based method with lexical
criteria. Fixed words or phrases with consistent
translations, like month names, were identified
first, and he observed an improvement of the
results.
The derivation of bilingual dictionaries often
follows text alignment (possibly at the word
level) based on some frequency criterion. Nev-
ertheless, in practice sentence alignment is not
always distinctly separated from word align-
ment, and neither is word alignment and the
derivation of translation lexicons thereby. In
fact, apart from Gale and Church&apos;s length-based
method, most other methods worked at the lex-
ical level to some extent.
Word alignment can be done statistically by
learning the translation association or token co-
occurrences between the source language and
the target language (e.g. Wu and Xia, 1995;
Melamed, 1997). The acquisition of bilingual
translation equivalences sometimes follows the
acquisition of monolingual collocations (e.g. Wu
and Xia, 1995; Smadja et al., 1996). Wu and
Xia (1995), for instance, made use of terms ex-
tracted by CXtract, a Chinese term extraction
algorithm, to learn collocation translations for
English words from the bilingual Hong Kong
LegCo proceedings, reporting a precision of
about 90%.
Others make use of existing bilingual dictio-
naries for word alignment, which is useful when
the corpus is too small for statistical meth-
ods and contains many general words. For ex-
ample, Ker and Chang (1997) worked with a
small bilingual corpus (English-Chinese) and at-
tempted to overcome the limitation of statistical
methods by class-based rules, with reference to
both an English and a Chinese machine read-
able dictionary.
When word alignment relies on existing lexi-
cal resources, however, the coverage tends to-
ward the low end, probably due to the in-
herent limited coverage of existing resources.
Huang and Choi (2000) used several linguistic
resources, including bilingual and monolingual
resources, for word alignment between Chinese
and Korean texts. Even when they combined
three algorithms together, there was still little
improvement in coverage.
Using a third, pivot language as a bridge in
word alignment may also improve the perfor-
mance (e.g. Bonin, 2000; Mann and Yarowsky,
2001). However, unlike Slavic languages or
Indo-European languages, it seems difficult to
imagine an effective bridge between English and
Chinese.
Hybrid methods are also used (e.g. Piperidis
et al., 1997; Huang and Choi, 2000), and are
believed to produce better alignment results.
Fung (1998), in contrast to the above which
worked with parallel corpora, tried to extract
bilingual lexicons from non-parallel corpora,
which is more difficult. She discussed an algo-
rithm called Convec, which compares the con-
text vector of a given English word with the con-
text vectors of all Chinese words for the most
similar candidate. During the process, a bilin-
gual dictionary is used to map the context words
in the two languages. The method was about
30% accurate if the top-one candidate was con-
sidered, although the accuracy was more than
doubled if the top-20 candidates were taken.
While most translation lexicon extraction
methods do not particularly address domain-
specificity, Resnik and Melamed (1997) sug-
gested that a domain-specific translation lexi-
con could be obtained by filtering out general
terms from the results. They, for instance, com-
pared their extracted lexicon entries against a
machine readable dictionary and discarded the
terms in common.
In the next section, we will discuss our prob-
lem in this study and explain why existing
methods are insufficient to solve it.
</bodyText>
<sectionHeader confidence="0.621926" genericHeader="method">
3 Characteristics of the Corpus
</sectionHeader>
<bodyText confidence="0.9996805">
As mentioned earlier, we are working with a
parallel corpus of bilingual Hong Kong court
judgments. The following properties of the cor-
pus render many existing word alignment and
translation lexicon extraction methods limited
in one way or another.
</bodyText>
<listItem confidence="0.997051">
• Small corpus size
</listItem>
<bodyText confidence="0.999406083333333">
The amount of bilingual Hong Kong court
judgments is potentially growing, as long
as there are legal proceedings. However,
the part that is ready for use in this study
only contains about 200K Chinese charac-
ters (about 113K word tokens upon seg-
mentation) and about 120K English word
tokens. This size is considered small, in
view of the many large corpora in general
domains available for natural language pro-
cessing research, and may therefore not be
ideal for many statistical methods.
</bodyText>
<listItem confidence="0.961774">
• Domain-specific corpus
</listItem>
<bodyText confidence="0.9949523">
The corpus consists of legal texts, and is
thus very domain-specific. It would be a
good resource from which to derive a le-
gal translation lexicon. However, a lexi-
cal approach to alignment based on exist-
ing general bilingual dictionaries might be
limited, because such dictionaries do not
always cover legal terminologies and their
translations. Even some general terms may
be translated in special ways in legal texts.
</bodyText>
<listItem confidence="0.974344">
• Different language families
</listItem>
<bodyText confidence="0.6010462">
English and Chinese are from different lan-
guage families and have little resemblance
of each other. As a result, lexical criteria
like cognates, or alignment via a bridging
language will not be applicable.
</bodyText>
<listItem confidence="0.928758">
• Unpredictable word complexity
</listItem>
<bodyText confidence="0.9977560625">
In many of the studies reviewed in Sec-
tion 2, the alignment was confined to single
words, at least for one of the languages in
question (e.g. Wu and Xia, 1995; Melamed,
1997; Resnik and Melamed, 1997; Fung,
1998; Huang and Choi, 2000). For about
150 years, the legal system in Hong Kong
operated through English only. So, many
legal concepts may not be as precisely lex-
icalised in Chinese. The lengths and com-
plexity of legal translations between En-
glish and Chinese are therefore not nec-
essarily correlated. Aligning legal terms
should therefore take care of the varying
lengths and complexity of a source term
and its translation equivalence.
</bodyText>
<subsectionHeader confidence="0.994231">
3.1 Two Assumptions
</subsectionHeader>
<bodyText confidence="0.999795733333333">
On the other hand, we also have some advan-
tages from the bilingual legal texts. Legal trans-
lations are well-known for their preciseness and
consistency. Thus we make the following as-
sumptions for the current study:
tence alignments are expected to be neat,
with few insertions and deletions.
2. Though not necessarily one sense per dis-
course, legal terms tend to be translated
more consistently than common terms.
Based on these two assumptions, we propose
a method, which depends minimally on prior
knowledge, for aligning words and expressions
in the corpus and thus extracting translation
equivalences, as described in the next section.
</bodyText>
<sectionHeader confidence="0.9726365" genericHeader="method">
4 Bilingual Term Alignment and
Extraction
</sectionHeader>
<subsectionHeader confidence="0.89953">
4.1 Task Definition
</subsectionHeader>
<bodyText confidence="0.998292125">
As Huang and Choi (2000) pointed out, the
&amp;quot;alignment&amp;quot; problem is often not explicitly
defined and apparently everyone understands
what is and should be going on. They, however,
adopted their own definition. In the current
study, we take &amp;quot;word alignment&amp;quot; as the more
viable task of &amp;quot;translation spotting&amp;quot;, as in the
ARCADE project.&apos;
</bodyText>
<subsectionHeader confidence="0.988308">
4.2 Our Approach
</subsectionHeader>
<bodyText confidence="0.99969475">
Our method is similar to Piperidis et al.&apos;s as we
share the observation that the source word and
the target word should have similar frequencies
if they are bilingual equivalence, except for func-
tion words. However, their method compares all
source-target associations for every possible pair
of words between the source sentence and the
target sentence, and located the correct trans-
lation from the local maximum. We, on the
other hand, adopt a simpler comparison, pay-
ing utmost attention to one term at a time.
Our approach starts with a sentence-aligned
bilingual corpus. As said, bilingual corpora in
the legal domain are relatively clean corpora.
Sentences can often be one-to-one aligned.
Given that legal terms are not always cross-
lingually lexicalised in similar ways, as discussed
in Section 3, term length and position in a sen-
tence might not be reliable parameters for word
alignment. Instead, we deal with one term at
a time, taking all sentences containing the term
into consideration; and we refer to the group
of sentences containing a given term as &amp;quot;con-
cordance lines&amp;quot; of that term. Thus within its
</bodyText>
<listItem confidence="0.55491">
1. Bilingual legal texts form relatively clean
</listItem>
<bodyText confidence="0.952354846153846">
ihttp://www.up.univ-mrs.frrveronis/arcade/index-
parallel corpora, in the sense that the sen- en.html
concordance lines, a given term often has higher
frequency than other co-occurring words, except
for function words. Since terms in legal texts are
more likely to be consistently translated, trans-
lation equivalences in the target concordances
should share a similar frequency with the source
word. Hence, by analysing the frequency pro-
files, we can identify the words in the target
language which are likely to be expressing the
concept of the source word.
Our algorithm is as follows:
</bodyText>
<listItem confidence="0.994394866666667">
1. Extract salient compound terms from the
word-segmented Chinese texts and treat
them as single words in subsequent steps.
2. Mark up stop words from both the Chinese
and English texts, and lemmatise the En-
glish words.
3. Scan through the Chinese texts. For each
un-aligned word, retrieve all sentences con-
taining the word to give the source concor-
dances, and all corresponding, aligned sen-
tences from the English texts to give the
target concordances.
4. Perform a word frequency count from the
concordances and rank the results.
5. Pick the words from target concordances
</listItem>
<bodyText confidence="0.9618135">
above some frequency threshold. The
longest string containing one or more of
these words and spanning within a small
window size in each target concordance
gives a candidate of translation equiva-
lence. (The setting of the threshold and
the window size will be discussed in Sec-
tion 5.2.)
</bodyText>
<listItem confidence="0.8287985">
6. Repeat Steps 3 to 5 until all Chinese words
are processed.
</listItem>
<bodyText confidence="0.9989322">
Thus our method is not restricted to aligning
single words. It does not inherently require a
large corpus to start with. Also, no prior knowl-
edge source like existing bilingual dictionaries is
required.
</bodyText>
<sectionHeader confidence="0.979058" genericHeader="method">
5 Pilot Experiment
</sectionHeader>
<subsectionHeader confidence="0.991918">
5.1 Test Materials
</subsectionHeader>
<bodyText confidence="0.999649666666667">
The corpus we used is a domain-specific one,
consisting of parallel texts of Hong Kong court
judgments, in English and Chinese.2 Each judg-
ment has a header containing the basic informa-
tion of the case (e.g. case number, judges, etc.),
the main judgment text, and a footer where
judges sign. Only the main text was used in
this experiment. For the current study, the Chi-
nese texts contain about 200K characters, and
about 113K word tokens and 7K word types
upon segmentation. The parallel English texts
contain about 120K word tokens, which corre-
spond to about 7K word types. The corpus was
aligned to the sentence level, and there were
4750 groups of aligned sentences.
</bodyText>
<subsectionHeader confidence="0.996721">
5.2 Method
</subsectionHeader>
<bodyText confidence="0.9998203">
The algorithm proposed in Section 4.2 was ap-
plied to the sentence-aligned corpus. The sen-
tence alignment was manually verified to ensure
the idiosyncratic cases were rectified. The Chi-
nese term extraction was done with the algo-
rithm in Kwong and Tsou (2001). The English
words were stemmed by applying the Porter
(1980) stemmer. Only the Chinese terms oc-
curring more than 5 times in the corpus were
tackled. For the frequency threshold (in Step
5), we first look for words with frequency over
0.8* source frequency (where source frequency
is the frequency of the source term within the
source concordances). If no words cross this
threshold, we pick the word with the highest
frequency and over 0.5* source frequency. For
the window size, we took the empirically opti-
mal n + 1 from Kwong (2002), where n is the
number of English words crossing the frequency
threshold.
</bodyText>
<subsectionHeader confidence="0.96363">
5.3 Performance Measures
</subsectionHeader>
<bodyText confidence="0.999856333333333">
Alignment outcomes were classified into four
types: correct, partially correct, incorrect, and
empty, defined as follows:
</bodyText>
<listItem confidence="0.968571875">
• Correct: All relevant content words are
within the suggested translation equiva-
lence, allowing incomplete verb forms or
missing stop words before or after.
• Partially Correct: Under- or over-
aligned, with some content words outside
or some irrelevant words inside the trans-
lation candidate.
</listItem>
<footnote confidence="0.777412">
2The authors acknowledge the Hong Kong Judiciary
for providing the judgment texts.
</footnote>
<listItem confidence="0.999134333333333">
• Incorrect: Completely mis-aligned words.
• Empty: No candidate translation equiva-
lence is suggested.
</listItem>
<subsectionHeader confidence="0.678039">
5.4 Results
</subsectionHeader>
<bodyText confidence="0.9999219375">
The Chinese term extraction yielded 683 poten-
tial compound terms (length &gt; 4 characters and
without numerals), and 462 remained after hu-
man verification. Thus step 1 of the algorithm
(see Section 4.2) re-segmented the Chinese texts
with this list of compound terms.
As said earlier, the corpus contains 4750
aligned sentences. We evaluated the alignment
of terms in 50 randomly selected sentences. Sen-
tence lengths range from 2 to 45 Chinese words,
and the average sentence length is 18.6 words.
Of the total 932 Chinese words in the 50 sen-
tences, 359 were filtered as stop words, and 59
occurred five times or less. Hence, there were
514 words subject to alignment. The outcomes
are summarised below.
</bodyText>
<equation confidence="0.834069">
Correct 213 (41%)
Partially Correct 19 (4%)
Incorrect 41(8%)
Empty 241 (47%)
</equation>
<bodyText confidence="0.999435083333334">
Considering all cases where a translation equiv-
alence was suggested, our method attained 53%
coverage and 85% precision (percentage of cor-
rect and partially correct equivalences among
all suggested equivalences). The recall (per-
centage of correct and partially correct equiv-
alences among all test instances) was 45%. Our
method gives relatively high coverage and com-
parable precision, as compared to existing lexi-
cally based methods, which is especially encour-
aging in view of its knowledge independence.
Figure 1 shows an excerpt of the output.
Words in the aligned sentences were numbered
as seen at the top of Figure 1. Stop words and
words with too few occurrences were not pro-
cessed. The suggested alignment was listed af-
ter the whole Chinese sentence was processed.
In the figure, for example, c6 was aligned to e4-
e6. The resultant alignment of the sentence is
drawn in Figure 2. In fact, we can find the var-
ious types of alignment outcomes in this exam-
ple. The links (except the dotted one) indicate
correct alignment. The dotted link for c14 is an
incorrect match. There were no suggestions for
</bodyText>
<table confidence="0.956414833333333">
Sentence No. 4660
c1=ft c2=NA c3=A c4=g c5=4*
c6=1.-fAir1T15 c7=.1-Ak c8=f-tal
c11=Ez. c12=11111 c13=n c14=_E#
el=He e2=now e3=seeks e4=leave e5=to
e6=appeal e7=against e8=conviction e9=and
</table>
<figure confidence="0.9474315">
el0=sentence
c1,ft (stop)
c2,NA,27
1,now,20
c3,r(-11 (stop)
c4,-g ,213
1,court,160
c5,4*,62
c6,_E#N1 1A-,17
1,appea1,22
2,1eav,17
3,applic,13
c7,,38
c8,MT,,325
1,aga nst,57
c10,11W,5 (too few)
c11,Ez. (stop)
c12,JP.M,49
1,sentenc,50
c13,n (stop)
c14,L-#,183
1,appea1,226
Alignment:
c2:e2/!!c4:!!c5:!!c6:e4-e6/!!c7:!!c8:!!c9:e7/!!c10:!!c12:e10/!!c14:e6/!!
</figure>
<figureCaption confidence="0.999997">
Figure 1: Excerpt of Alignment Output
</figureCaption>
<bodyText confidence="0.9998999375">
c4, c5, c7, c8, and c10. Nevertheless, there were
in fact no literal correspondences in the English
sentence for c4, c7, c8, and c14. So some of the
empty alignments are correct. Further classify-
ing the empty outcomes and taking the correct
ones into account, the accuracy of our method is
over 55% as far as the word alignment per se is
concerned. The results will be further analysed
in the next section.
As for the derivation of a translation lexi-
con, we thus obtained 213 suggested translation
equivalences from the correct word alignments.
These equivalences are potential entries of a
bilingual legal term translation lexicon, upon
further filtering and processing. Figure 3 shows
some examples.
</bodyText>
<sectionHeader confidence="0.998195" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99950625">
In the last section, we have seen the effective-
ness of our method from a pilot experiment.
This finding is significant because the method
overcomes the limitations of existing statisti-
</bodyText>
<figure confidence="0.911405222222222">
Terms in Chinese English Equivalence
Appellant
Common law
Enforcement
Satisfied
Requisition
Plaintiff’s solicitors
Findings
Customary marriage
</figure>
<bodyText confidence="0.999842818181818">
gal terminology. This would require further
processing of the translation equivalences sug-
gested during the alignment process. For in-
stance, entries in a lexicon should make sense
even when out of context, so they should be free
of anaphors like definite descriptions, and they
should be in root form. Also, apart from filter-
ing using a general dictionary as in Resnik and
Melamed (1997), we may also filter the results
against the terms found in a general-domain
corpus.
</bodyText>
<sectionHeader confidence="0.99859" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999989214285714">
In this study, we have introduced and tested
a simple but effective method for word align-
ment and translation extraction between par-
allel English and Chinese legal texts, based on
frequency profiles of parallel concordances. The
method does not require any prior knowledge,
and thus overcomes the limitations of existing
statistically based and lexically based methods.
It is especially designed for working with small,
domain-specific, and sentence-aligned parallel
corpora. About 85% precision and 45% recall
were obtained from the pilot experiment. Fu-
ture work will be on the fine-tuning of the algo-
rithm and the acquired translation lexicon.
</bodyText>
<sectionHeader confidence="0.99887" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999775586666667">
L. Bonn. 2000. You&apos;ll take the high road and I&apos;ll take
the low road: Using a third language to improve bilin-
gual word alignment. In Proceedings of the 18th In-
ternational Conference on Computational Linguistics,
pages 97-103, Saarbriicken, Germany.
P. Fung. 1998. A statistical view on bilingual lexicon
extraction: from parallel corpora to non-parallel cor-
pora. Lecture Notes in Artificial Intelligence, 1529:1-
17.
W.A. Gale and K.W. Church. 1991. A program for
aligning sentences in bilingual corpora. In Proceedings
of the 29th Annual Conference of the Association for
Computational Linguistics (ACL &apos;91), pages 177-184,
Berkeley, USA.
J-X. Huang and K-S. Choi. 2000. Chinese-Korean word
alignment based on linguistic comparison. In Proceed-
ings of the 38th Annual Meeting of the Association for
Computational Linguistics (ACL-2000), pages 392-
399, Hong Kong.
M. Kay and M. Roescheisen. 1993. Text-translation
alignment. Computational Linguistics, 19(1):121-142.
S.J. Ker and J.S. Chang. 1997. Aligning more words
with high precision for small bilingual corpora. Com-
putational Linguistics and Chinese Language Process-
ing, 2(2):63-96.
O.Y. Kwong and B.K. Tsou. 2001. Automatic corpus-
based extraction of Chinese legal terms. In Pro-
ceedings of the 6th Natural Language Processing Pa-
cific Rim Symposium (NLPRS 2001), pages 669-676,
Tokyo, Japan.
O.Y. Kwong. 2002. Toward a bilingual legal term glos-
sary from context profiles. In Proceedings of the 16th
Pacific Asia Conference on Language, Information
and Computation (PACLIC 16), pages 249-258, Jeju,
Korea.
G.S. Mann and D. Yarowsky. 2001. Multipath transla-
tion lexicon induction via bridge languages. In Pro-
ceedings of the 2nd Meeting of the North Ameri-
can Chapter of the Association for Computational
Linguistics (NAACL 2001), pages 151-158, Carnegie
Mellon University, Pittsburgh, USA.
I.D. Melamed. 1997. A word-to-word model of transla-
tional equivalence. In Proceedings of the 35th Con-
ference of the Association for Computational Lin-
guistics and 8th Conference of the European Chap-
ter of the Association for Computational Linguistics
(ACL/EACL&apos;97), Madrid, Spain.
S. Piperidis, S. Boutsis, and I. Demiros. 1997. Auto-
matic translation lexicon generation from multilingual
texts. In Proceedings of the 2nd Workshop on Multi-
linguality in Software Industry: The Al Contribution
(MULSAIC&apos;97), Nagoya, Japan.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130-137, July.
P. Resnik and I.D. Melamed. 1997. Semi-automatic ac-
quisition of domain-specific translation lexicons. In
Proceedings of the Fifth Conference on Applied Nat-
ural Language Processing (ANLP&apos;97), Washington,
DC.
M. Simard, G.F. Foster, and P. Isabelle. 1992. Using
cognates to align sentences in bilingual corpora. In
Proceedings of the Fourth International Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI-92), pages 67-81.
F.Z. Smadja, K. McKeown, and V. Hatzivassiloglou.
1996. Translating collocations for bilingual lexicons:
A statistical approach. Computational Linguistics,
22(1):1-38.
D. Wu and X. Xia. 1995. Large-scale automatic extrac-
tion of an English-Chinese translation lexicon. Ma-
chine Translation, 9(3-4285-313.
D. Wu. 1994. Aligning a parallel English-Chinese corpus
statistically with lexical criteria. In Proceedings of the
32nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL-94), Las Cruces, NM.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.289038">
<title confidence="0.9992">Alignment and Extraction of Bilingual Legal Terminology Context Profiles</title>
<author confidence="0.9759395">Oi Yee Kwong</author>
<author confidence="0.9759395">Benjamin K Tsou</author>
<author confidence="0.9759395">Tom B Y Lai</author>
<author confidence="0.9759395">Robert W P Lawrence Y L Cheung</author>
<author confidence="0.9759395">Francis C Y Chik</author>
<affiliation confidence="0.996164">Information Sciences Research Centre Computing City University of Hong Kong Hong Kong Polytechnic University</affiliation>
<address confidence="0.584583">Tat Chee Avenue, Kowloon, Hong Kong Hung Hom, Kowloon, Hong Kong</address>
<email confidence="0.572231">rloliviaOcityu.edu.hkcsrlukOcomp.polyu.edu.hk</email>
<email confidence="0.572231">rlbtsouOcityu.edu.hkcsrlukOcomp.polyu.edu.hk</email>
<email confidence="0.572231">cttomlaiOcityu.edu.hkcsrlukOcomp.polyu.edu.hk</email>
<email confidence="0.572231">rlylcOcityu.edu.hkcsrlukOcomp.polyu.edu.hk</email>
<email confidence="0.572231">r1fchikOcityu.edu.hkcsrlukOcomp.polyu.edu.hk</email>
<abstract confidence="0.9966619">In this study, we propose a knowledgeindependent method for aligning terms and thus extracting translations from a small, domainspecific corpus consisting of parallel English and Chinese court judgments from Hong Kong. With a sentence-aligned corpus, translation equivalences are suggested by analysing the frequency profiles of parallel concordances. The method overcomes the limitations of conventional statistical methods which require large corpora to be effective, and lexical approaches which depend on existing bilingual dictionaries. Pilot testing on a parallel corpus of about 113K Chinese words and 120K English words gives an encouraging 85% precision and 45% recall. Future work includes fine-tuning the algorithm upon the analysis of the errors, and acquiring a translation lexicon for legal terminology by filtering out general terms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bonn</author>
</authors>
<title>You&apos;ll take the high road and I&apos;ll take the low road: Using a third language to improve bilingual word alignment.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>97--103</pages>
<location>Saarbriicken, Germany.</location>
<marker>Bonn, 2000</marker>
<rawString>L. Bonn. 2000. You&apos;ll take the high road and I&apos;ll take the low road: Using a third language to improve bilingual word alignment. In Proceedings of the 18th International Conference on Computational Linguistics, pages 97-103, Saarbriicken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
</authors>
<title>A statistical view on bilingual lexicon extraction: from parallel corpora to non-parallel corpora.</title>
<date>1998</date>
<journal>Lecture Notes in Artificial Intelligence,</journal>
<pages>1529--1</pages>
<contexts>
<context position="6468" citStr="Fung (1998)" startWordPosition="978" endWordPosition="979">ingual and monolingual resources, for word alignment between Chinese and Korean texts. Even when they combined three algorithms together, there was still little improvement in coverage. Using a third, pivot language as a bridge in word alignment may also improve the performance (e.g. Bonin, 2000; Mann and Yarowsky, 2001). However, unlike Slavic languages or Indo-European languages, it seems difficult to imagine an effective bridge between English and Chinese. Hybrid methods are also used (e.g. Piperidis et al., 1997; Huang and Choi, 2000), and are believed to produce better alignment results. Fung (1998), in contrast to the above which worked with parallel corpora, tried to extract bilingual lexicons from non-parallel corpora, which is more difficult. She discussed an algorithm called Convec, which compares the context vector of a given English word with the context vectors of all Chinese words for the most similar candidate. During the process, a bilingual dictionary is used to map the context words in the two languages. The method was about 30% accurate if the top-one candidate was considered, although the accuracy was more than doubled if the top-20 candidates were taken. While most transl</context>
<context position="9267" citStr="Fung, 1998" startWordPosition="1434" endWordPosition="1435">always cover legal terminologies and their translations. Even some general terms may be translated in special ways in legal texts. • Different language families English and Chinese are from different language families and have little resemblance of each other. As a result, lexical criteria like cognates, or alignment via a bridging language will not be applicable. • Unpredictable word complexity In many of the studies reviewed in Section 2, the alignment was confined to single words, at least for one of the languages in question (e.g. Wu and Xia, 1995; Melamed, 1997; Resnik and Melamed, 1997; Fung, 1998; Huang and Choi, 2000). For about 150 years, the legal system in Hong Kong operated through English only. So, many legal concepts may not be as precisely lexicalised in Chinese. The lengths and complexity of legal translations between English and Chinese are therefore not necessarily correlated. Aligning legal terms should therefore take care of the varying lengths and complexity of a source term and its translation equivalence. 3.1 Two Assumptions On the other hand, we also have some advantages from the bilingual legal texts. Legal translations are well-known for their preciseness and consis</context>
</contexts>
<marker>Fung, 1998</marker>
<rawString>P. Fung. 1998. A statistical view on bilingual lexicon extraction: from parallel corpora to non-parallel corpora. Lecture Notes in Artificial Intelligence, 1529:1-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics (ACL &apos;91),</booktitle>
<pages>177--184</pages>
<location>Berkeley, USA.</location>
<contexts>
<context position="3599" citStr="Gale and Church, 1991" startWordPosition="539" endWordPosition="542">ofiles of parallel concordances. Pilot testing shows that the method is effective for fulfilling the two purposes, i.e. term alignment and lexicon extraction, simultaneously. In the following, we will first review related studies in Section 2. Then we will describe the properties of our corpus in Section 3 and our method in Section 4. A pilot experiment with the proposed approach will be reported in Section 5 with results discussed in Section 6, followed by a conclusion in Section 7. 2 Related Work Conventional bilingual sentence alignment is often based statistically on sentence length (e.g. Gale and Church, 1991), or lexically on cognates (e.g. Simard et al., 1992) and correspondence of word position (e.g. Kay and Roescheisen, 1993; Piperidis et al., 1997). Such criteria, however, are mostly applicable to Indo-European language pairs. Although Wu (1994) found the length criterion applied surprisingly well between English and Chinese, he supplemented the statistical, length-based method with lexical criteria. Fixed words or phrases with consistent translations, like month names, were identified first, and he observed an improvement of the results. The derivation of bilingual dictionaries often follows </context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>W.A. Gale and K.W. Church. 1991. A program for aligning sentences in bilingual corpora. In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics (ACL &apos;91), pages 177-184, Berkeley, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-X Huang</author>
<author>K-S Choi</author>
</authors>
<title>Chinese-Korean word alignment based on linguistic comparison.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000),</booktitle>
<pages>392--399</pages>
<location>Hong Kong.</location>
<contexts>
<context position="5808" citStr="Huang and Choi (2000)" startWordPosition="878" endWordPosition="881">Others make use of existing bilingual dictionaries for word alignment, which is useful when the corpus is too small for statistical methods and contains many general words. For example, Ker and Chang (1997) worked with a small bilingual corpus (English-Chinese) and attempted to overcome the limitation of statistical methods by class-based rules, with reference to both an English and a Chinese machine readable dictionary. When word alignment relies on existing lexical resources, however, the coverage tends toward the low end, probably due to the inherent limited coverage of existing resources. Huang and Choi (2000) used several linguistic resources, including bilingual and monolingual resources, for word alignment between Chinese and Korean texts. Even when they combined three algorithms together, there was still little improvement in coverage. Using a third, pivot language as a bridge in word alignment may also improve the performance (e.g. Bonin, 2000; Mann and Yarowsky, 2001). However, unlike Slavic languages or Indo-European languages, it seems difficult to imagine an effective bridge between English and Chinese. Hybrid methods are also used (e.g. Piperidis et al., 1997; Huang and Choi, 2000), and a</context>
<context position="9290" citStr="Huang and Choi, 2000" startWordPosition="1436" endWordPosition="1439"> legal terminologies and their translations. Even some general terms may be translated in special ways in legal texts. • Different language families English and Chinese are from different language families and have little resemblance of each other. As a result, lexical criteria like cognates, or alignment via a bridging language will not be applicable. • Unpredictable word complexity In many of the studies reviewed in Section 2, the alignment was confined to single words, at least for one of the languages in question (e.g. Wu and Xia, 1995; Melamed, 1997; Resnik and Melamed, 1997; Fung, 1998; Huang and Choi, 2000). For about 150 years, the legal system in Hong Kong operated through English only. So, many legal concepts may not be as precisely lexicalised in Chinese. The lengths and complexity of legal translations between English and Chinese are therefore not necessarily correlated. Aligning legal terms should therefore take care of the varying lengths and complexity of a source term and its translation equivalence. 3.1 Two Assumptions On the other hand, we also have some advantages from the bilingual legal texts. Legal translations are well-known for their preciseness and consistency. Thus we make the</context>
</contexts>
<marker>Huang, Choi, 2000</marker>
<rawString>J-X. Huang and K-S. Choi. 2000. Chinese-Korean word alignment based on linguistic comparison. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000), pages 392-399, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
<author>M Roescheisen</author>
</authors>
<date>1993</date>
<booktitle>Text-translation alignment. Computational Linguistics,</booktitle>
<pages>19--1</pages>
<contexts>
<context position="3720" citStr="Kay and Roescheisen, 1993" startWordPosition="558" endWordPosition="561">. term alignment and lexicon extraction, simultaneously. In the following, we will first review related studies in Section 2. Then we will describe the properties of our corpus in Section 3 and our method in Section 4. A pilot experiment with the proposed approach will be reported in Section 5 with results discussed in Section 6, followed by a conclusion in Section 7. 2 Related Work Conventional bilingual sentence alignment is often based statistically on sentence length (e.g. Gale and Church, 1991), or lexically on cognates (e.g. Simard et al., 1992) and correspondence of word position (e.g. Kay and Roescheisen, 1993; Piperidis et al., 1997). Such criteria, however, are mostly applicable to Indo-European language pairs. Although Wu (1994) found the length criterion applied surprisingly well between English and Chinese, he supplemented the statistical, length-based method with lexical criteria. Fixed words or phrases with consistent translations, like month names, were identified first, and he observed an improvement of the results. The derivation of bilingual dictionaries often follows text alignment (possibly at the word level) based on some frequency criterion. Nevertheless, in practice sentence alignme</context>
</contexts>
<marker>Kay, Roescheisen, 1993</marker>
<rawString>M. Kay and M. Roescheisen. 1993. Text-translation alignment. Computational Linguistics, 19(1):121-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Ker</author>
<author>J S Chang</author>
</authors>
<title>Aligning more words with high precision for small bilingual corpora.</title>
<date>1997</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<pages>2--2</pages>
<contexts>
<context position="5393" citStr="Ker and Chang (1997)" startWordPosition="812" endWordPosition="815">997). The acquisition of bilingual translation equivalences sometimes follows the acquisition of monolingual collocations (e.g. Wu and Xia, 1995; Smadja et al., 1996). Wu and Xia (1995), for instance, made use of terms extracted by CXtract, a Chinese term extraction algorithm, to learn collocation translations for English words from the bilingual Hong Kong LegCo proceedings, reporting a precision of about 90%. Others make use of existing bilingual dictionaries for word alignment, which is useful when the corpus is too small for statistical methods and contains many general words. For example, Ker and Chang (1997) worked with a small bilingual corpus (English-Chinese) and attempted to overcome the limitation of statistical methods by class-based rules, with reference to both an English and a Chinese machine readable dictionary. When word alignment relies on existing lexical resources, however, the coverage tends toward the low end, probably due to the inherent limited coverage of existing resources. Huang and Choi (2000) used several linguistic resources, including bilingual and monolingual resources, for word alignment between Chinese and Korean texts. Even when they combined three algorithms together</context>
</contexts>
<marker>Ker, Chang, 1997</marker>
<rawString>S.J. Ker and J.S. Chang. 1997. Aligning more words with high precision for small bilingual corpora. Computational Linguistics and Chinese Language Processing, 2(2):63-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Y Kwong</author>
<author>B K Tsou</author>
</authors>
<title>Automatic corpusbased extraction of Chinese legal terms.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium (NLPRS</booktitle>
<pages>669--676</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="14623" citStr="Kwong and Tsou (2001)" startWordPosition="2306" endWordPosition="2309">d in this experiment. For the current study, the Chinese texts contain about 200K characters, and about 113K word tokens and 7K word types upon segmentation. The parallel English texts contain about 120K word tokens, which correspond to about 7K word types. The corpus was aligned to the sentence level, and there were 4750 groups of aligned sentences. 5.2 Method The algorithm proposed in Section 4.2 was applied to the sentence-aligned corpus. The sentence alignment was manually verified to ensure the idiosyncratic cases were rectified. The Chinese term extraction was done with the algorithm in Kwong and Tsou (2001). The English words were stemmed by applying the Porter (1980) stemmer. Only the Chinese terms occurring more than 5 times in the corpus were tackled. For the frequency threshold (in Step 5), we first look for words with frequency over 0.8* source frequency (where source frequency is the frequency of the source term within the source concordances). If no words cross this threshold, we pick the word with the highest frequency and over 0.5* source frequency. For the window size, we took the empirically optimal n + 1 from Kwong (2002), where n is the number of English words crossing the frequency</context>
</contexts>
<marker>Kwong, Tsou, 2001</marker>
<rawString>O.Y. Kwong and B.K. Tsou. 2001. Automatic corpusbased extraction of Chinese legal terms. In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium (NLPRS 2001), pages 669-676, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Y Kwong</author>
</authors>
<title>Toward a bilingual legal term glossary from context profiles.</title>
<date>2002</date>
<booktitle>In Proceedings of the 16th Pacific Asia Conference on Language, Information and Computation (PACLIC 16),</booktitle>
<pages>249--258</pages>
<location>Jeju,</location>
<contexts>
<context position="15160" citStr="Kwong (2002)" startWordPosition="2401" endWordPosition="2402">hinese term extraction was done with the algorithm in Kwong and Tsou (2001). The English words were stemmed by applying the Porter (1980) stemmer. Only the Chinese terms occurring more than 5 times in the corpus were tackled. For the frequency threshold (in Step 5), we first look for words with frequency over 0.8* source frequency (where source frequency is the frequency of the source term within the source concordances). If no words cross this threshold, we pick the word with the highest frequency and over 0.5* source frequency. For the window size, we took the empirically optimal n + 1 from Kwong (2002), where n is the number of English words crossing the frequency threshold. 5.3 Performance Measures Alignment outcomes were classified into four types: correct, partially correct, incorrect, and empty, defined as follows: • Correct: All relevant content words are within the suggested translation equivalence, allowing incomplete verb forms or missing stop words before or after. • Partially Correct: Under- or overaligned, with some content words outside or some irrelevant words inside the translation candidate. 2The authors acknowledge the Hong Kong Judiciary for providing the judgment texts. • </context>
</contexts>
<marker>Kwong, 2002</marker>
<rawString>O.Y. Kwong. 2002. Toward a bilingual legal term glossary from context profiles. In Proceedings of the 16th Pacific Asia Conference on Language, Information and Computation (PACLIC 16), pages 249-258, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Multipath translation lexicon induction via bridge languages.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL</booktitle>
<pages>151--158</pages>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="6179" citStr="Mann and Yarowsky, 2001" startWordPosition="933" endWordPosition="936">th an English and a Chinese machine readable dictionary. When word alignment relies on existing lexical resources, however, the coverage tends toward the low end, probably due to the inherent limited coverage of existing resources. Huang and Choi (2000) used several linguistic resources, including bilingual and monolingual resources, for word alignment between Chinese and Korean texts. Even when they combined three algorithms together, there was still little improvement in coverage. Using a third, pivot language as a bridge in word alignment may also improve the performance (e.g. Bonin, 2000; Mann and Yarowsky, 2001). However, unlike Slavic languages or Indo-European languages, it seems difficult to imagine an effective bridge between English and Chinese. Hybrid methods are also used (e.g. Piperidis et al., 1997; Huang and Choi, 2000), and are believed to produce better alignment results. Fung (1998), in contrast to the above which worked with parallel corpora, tried to extract bilingual lexicons from non-parallel corpora, which is more difficult. She discussed an algorithm called Convec, which compares the context vector of a given English word with the context vectors of all Chinese words for the most s</context>
</contexts>
<marker>Mann, Yarowsky, 2001</marker>
<rawString>G.S. Mann and D. Yarowsky. 2001. Multipath translation lexicon induction via bridge languages. In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2001), pages 151-158, Carnegie Mellon University, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>A word-to-word model of translational equivalence.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Conference of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics (ACL/EACL&apos;97),</booktitle>
<location>Madrid,</location>
<contexts>
<context position="4777" citStr="Melamed, 1997" startWordPosition="717" endWordPosition="718">ilingual dictionaries often follows text alignment (possibly at the word level) based on some frequency criterion. Nevertheless, in practice sentence alignment is not always distinctly separated from word alignment, and neither is word alignment and the derivation of translation lexicons thereby. In fact, apart from Gale and Church&apos;s length-based method, most other methods worked at the lexical level to some extent. Word alignment can be done statistically by learning the translation association or token cooccurrences between the source language and the target language (e.g. Wu and Xia, 1995; Melamed, 1997). The acquisition of bilingual translation equivalences sometimes follows the acquisition of monolingual collocations (e.g. Wu and Xia, 1995; Smadja et al., 1996). Wu and Xia (1995), for instance, made use of terms extracted by CXtract, a Chinese term extraction algorithm, to learn collocation translations for English words from the bilingual Hong Kong LegCo proceedings, reporting a precision of about 90%. Others make use of existing bilingual dictionaries for word alignment, which is useful when the corpus is too small for statistical methods and contains many general words. For example, Ker </context>
<context position="7173" citStr="Melamed (1997)" startWordPosition="1092" endWordPosition="1093">exicons from non-parallel corpora, which is more difficult. She discussed an algorithm called Convec, which compares the context vector of a given English word with the context vectors of all Chinese words for the most similar candidate. During the process, a bilingual dictionary is used to map the context words in the two languages. The method was about 30% accurate if the top-one candidate was considered, although the accuracy was more than doubled if the top-20 candidates were taken. While most translation lexicon extraction methods do not particularly address domainspecificity, Resnik and Melamed (1997) suggested that a domain-specific translation lexicon could be obtained by filtering out general terms from the results. They, for instance, compared their extracted lexicon entries against a machine readable dictionary and discarded the terms in common. In the next section, we will discuss our problem in this study and explain why existing methods are insufficient to solve it. 3 Characteristics of the Corpus As mentioned earlier, we are working with a parallel corpus of bilingual Hong Kong court judgments. The following properties of the corpus render many existing word alignment and translat</context>
<context position="9229" citStr="Melamed, 1997" startWordPosition="1428" endWordPosition="1429">imited, because such dictionaries do not always cover legal terminologies and their translations. Even some general terms may be translated in special ways in legal texts. • Different language families English and Chinese are from different language families and have little resemblance of each other. As a result, lexical criteria like cognates, or alignment via a bridging language will not be applicable. • Unpredictable word complexity In many of the studies reviewed in Section 2, the alignment was confined to single words, at least for one of the languages in question (e.g. Wu and Xia, 1995; Melamed, 1997; Resnik and Melamed, 1997; Fung, 1998; Huang and Choi, 2000). For about 150 years, the legal system in Hong Kong operated through English only. So, many legal concepts may not be as precisely lexicalised in Chinese. The lengths and complexity of legal translations between English and Chinese are therefore not necessarily correlated. Aligning legal terms should therefore take care of the varying lengths and complexity of a source term and its translation equivalence. 3.1 Two Assumptions On the other hand, we also have some advantages from the bilingual legal texts. Legal translations are well-</context>
<context position="19769" citStr="Melamed (1997)" startWordPosition="3111" endWordPosition="3112">ing is significant because the method overcomes the limitations of existing statistiTerms in Chinese English Equivalence Appellant Common law Enforcement Satisfied Requisition Plaintiff’s solicitors Findings Customary marriage gal terminology. This would require further processing of the translation equivalences suggested during the alignment process. For instance, entries in a lexicon should make sense even when out of context, so they should be free of anaphors like definite descriptions, and they should be in root form. Also, apart from filtering using a general dictionary as in Resnik and Melamed (1997), we may also filter the results against the terms found in a general-domain corpus. 7 Conclusion In this study, we have introduced and tested a simple but effective method for word alignment and translation extraction between parallel English and Chinese legal texts, based on frequency profiles of parallel concordances. The method does not require any prior knowledge, and thus overcomes the limitations of existing statistically based and lexically based methods. It is especially designed for working with small, domain-specific, and sentence-aligned parallel corpora. About 85% precision and 45</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I.D. Melamed. 1997. A word-to-word model of translational equivalence. In Proceedings of the 35th Conference of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics (ACL/EACL&apos;97), Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Piperidis</author>
<author>S Boutsis</author>
<author>I Demiros</author>
</authors>
<title>Automatic translation lexicon generation from multilingual texts.</title>
<date>1997</date>
<booktitle>In Proceedings of the 2nd Workshop on Multilinguality in Software Industry: The Al Contribution (MULSAIC&apos;97),</booktitle>
<location>Nagoya, Japan.</location>
<contexts>
<context position="3745" citStr="Piperidis et al., 1997" startWordPosition="562" endWordPosition="565">n extraction, simultaneously. In the following, we will first review related studies in Section 2. Then we will describe the properties of our corpus in Section 3 and our method in Section 4. A pilot experiment with the proposed approach will be reported in Section 5 with results discussed in Section 6, followed by a conclusion in Section 7. 2 Related Work Conventional bilingual sentence alignment is often based statistically on sentence length (e.g. Gale and Church, 1991), or lexically on cognates (e.g. Simard et al., 1992) and correspondence of word position (e.g. Kay and Roescheisen, 1993; Piperidis et al., 1997). Such criteria, however, are mostly applicable to Indo-European language pairs. Although Wu (1994) found the length criterion applied surprisingly well between English and Chinese, he supplemented the statistical, length-based method with lexical criteria. Fixed words or phrases with consistent translations, like month names, were identified first, and he observed an improvement of the results. The derivation of bilingual dictionaries often follows text alignment (possibly at the word level) based on some frequency criterion. Nevertheless, in practice sentence alignment is not always distinct</context>
<context position="6378" citStr="Piperidis et al., 1997" startWordPosition="962" endWordPosition="965">overage of existing resources. Huang and Choi (2000) used several linguistic resources, including bilingual and monolingual resources, for word alignment between Chinese and Korean texts. Even when they combined three algorithms together, there was still little improvement in coverage. Using a third, pivot language as a bridge in word alignment may also improve the performance (e.g. Bonin, 2000; Mann and Yarowsky, 2001). However, unlike Slavic languages or Indo-European languages, it seems difficult to imagine an effective bridge between English and Chinese. Hybrid methods are also used (e.g. Piperidis et al., 1997; Huang and Choi, 2000), and are believed to produce better alignment results. Fung (1998), in contrast to the above which worked with parallel corpora, tried to extract bilingual lexicons from non-parallel corpora, which is more difficult. She discussed an algorithm called Convec, which compares the context vector of a given English word with the context vectors of all Chinese words for the most similar candidate. During the process, a bilingual dictionary is used to map the context words in the two languages. The method was about 30% accurate if the top-one candidate was considered, although</context>
</contexts>
<marker>Piperidis, Boutsis, Demiros, 1997</marker>
<rawString>S. Piperidis, S. Boutsis, and I. Demiros. 1997. Automatic translation lexicon generation from multilingual texts. In Proceedings of the 2nd Workshop on Multilinguality in Software Industry: The Al Contribution (MULSAIC&apos;97), Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<pages>14--3</pages>
<contexts>
<context position="14685" citStr="Porter (1980)" startWordPosition="2318" endWordPosition="2319"> about 200K characters, and about 113K word tokens and 7K word types upon segmentation. The parallel English texts contain about 120K word tokens, which correspond to about 7K word types. The corpus was aligned to the sentence level, and there were 4750 groups of aligned sentences. 5.2 Method The algorithm proposed in Section 4.2 was applied to the sentence-aligned corpus. The sentence alignment was manually verified to ensure the idiosyncratic cases were rectified. The Chinese term extraction was done with the algorithm in Kwong and Tsou (2001). The English words were stemmed by applying the Porter (1980) stemmer. Only the Chinese terms occurring more than 5 times in the corpus were tackled. For the frequency threshold (in Step 5), we first look for words with frequency over 0.8* source frequency (where source frequency is the frequency of the source term within the source concordances). If no words cross this threshold, we pick the word with the highest frequency and over 0.5* source frequency. For the window size, we took the empirically optimal n + 1 from Kwong (2002), where n is the number of English words crossing the frequency threshold. 5.3 Performance Measures Alignment outcomes were c</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M.F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130-137, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>I D Melamed</author>
</authors>
<title>Semi-automatic acquisition of domain-specific translation lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP&apos;97),</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="7173" citStr="Resnik and Melamed (1997)" startWordPosition="1090" endWordPosition="1093">bilingual lexicons from non-parallel corpora, which is more difficult. She discussed an algorithm called Convec, which compares the context vector of a given English word with the context vectors of all Chinese words for the most similar candidate. During the process, a bilingual dictionary is used to map the context words in the two languages. The method was about 30% accurate if the top-one candidate was considered, although the accuracy was more than doubled if the top-20 candidates were taken. While most translation lexicon extraction methods do not particularly address domainspecificity, Resnik and Melamed (1997) suggested that a domain-specific translation lexicon could be obtained by filtering out general terms from the results. They, for instance, compared their extracted lexicon entries against a machine readable dictionary and discarded the terms in common. In the next section, we will discuss our problem in this study and explain why existing methods are insufficient to solve it. 3 Characteristics of the Corpus As mentioned earlier, we are working with a parallel corpus of bilingual Hong Kong court judgments. The following properties of the corpus render many existing word alignment and translat</context>
<context position="9255" citStr="Resnik and Melamed, 1997" startWordPosition="1430" endWordPosition="1433"> such dictionaries do not always cover legal terminologies and their translations. Even some general terms may be translated in special ways in legal texts. • Different language families English and Chinese are from different language families and have little resemblance of each other. As a result, lexical criteria like cognates, or alignment via a bridging language will not be applicable. • Unpredictable word complexity In many of the studies reviewed in Section 2, the alignment was confined to single words, at least for one of the languages in question (e.g. Wu and Xia, 1995; Melamed, 1997; Resnik and Melamed, 1997; Fung, 1998; Huang and Choi, 2000). For about 150 years, the legal system in Hong Kong operated through English only. So, many legal concepts may not be as precisely lexicalised in Chinese. The lengths and complexity of legal translations between English and Chinese are therefore not necessarily correlated. Aligning legal terms should therefore take care of the varying lengths and complexity of a source term and its translation equivalence. 3.1 Two Assumptions On the other hand, we also have some advantages from the bilingual legal texts. Legal translations are well-known for their precisenes</context>
<context position="19769" citStr="Resnik and Melamed (1997)" startWordPosition="3109" endWordPosition="3112">. This finding is significant because the method overcomes the limitations of existing statistiTerms in Chinese English Equivalence Appellant Common law Enforcement Satisfied Requisition Plaintiff’s solicitors Findings Customary marriage gal terminology. This would require further processing of the translation equivalences suggested during the alignment process. For instance, entries in a lexicon should make sense even when out of context, so they should be free of anaphors like definite descriptions, and they should be in root form. Also, apart from filtering using a general dictionary as in Resnik and Melamed (1997), we may also filter the results against the terms found in a general-domain corpus. 7 Conclusion In this study, we have introduced and tested a simple but effective method for word alignment and translation extraction between parallel English and Chinese legal texts, based on frequency profiles of parallel concordances. The method does not require any prior knowledge, and thus overcomes the limitations of existing statistically based and lexically based methods. It is especially designed for working with small, domain-specific, and sentence-aligned parallel corpora. About 85% precision and 45</context>
</contexts>
<marker>Resnik, Melamed, 1997</marker>
<rawString>P. Resnik and I.D. Melamed. 1997. Semi-automatic acquisition of domain-specific translation lexicons. In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP&apos;97), Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Simard</author>
<author>G F Foster</author>
<author>P Isabelle</author>
</authors>
<title>Using cognates to align sentences in bilingual corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-92),</booktitle>
<pages>67--81</pages>
<contexts>
<context position="3652" citStr="Simard et al., 1992" startWordPosition="548" endWordPosition="551">at the method is effective for fulfilling the two purposes, i.e. term alignment and lexicon extraction, simultaneously. In the following, we will first review related studies in Section 2. Then we will describe the properties of our corpus in Section 3 and our method in Section 4. A pilot experiment with the proposed approach will be reported in Section 5 with results discussed in Section 6, followed by a conclusion in Section 7. 2 Related Work Conventional bilingual sentence alignment is often based statistically on sentence length (e.g. Gale and Church, 1991), or lexically on cognates (e.g. Simard et al., 1992) and correspondence of word position (e.g. Kay and Roescheisen, 1993; Piperidis et al., 1997). Such criteria, however, are mostly applicable to Indo-European language pairs. Although Wu (1994) found the length criterion applied surprisingly well between English and Chinese, he supplemented the statistical, length-based method with lexical criteria. Fixed words or phrases with consistent translations, like month names, were identified first, and he observed an improvement of the results. The derivation of bilingual dictionaries often follows text alignment (possibly at the word level) based on </context>
</contexts>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>M. Simard, G.F. Foster, and P. Isabelle. 1992. Using cognates to align sentences in bilingual corpora. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-92), pages 67-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Z Smadja</author>
<author>K McKeown</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="4939" citStr="Smadja et al., 1996" startWordPosition="737" endWordPosition="740">ent is not always distinctly separated from word alignment, and neither is word alignment and the derivation of translation lexicons thereby. In fact, apart from Gale and Church&apos;s length-based method, most other methods worked at the lexical level to some extent. Word alignment can be done statistically by learning the translation association or token cooccurrences between the source language and the target language (e.g. Wu and Xia, 1995; Melamed, 1997). The acquisition of bilingual translation equivalences sometimes follows the acquisition of monolingual collocations (e.g. Wu and Xia, 1995; Smadja et al., 1996). Wu and Xia (1995), for instance, made use of terms extracted by CXtract, a Chinese term extraction algorithm, to learn collocation translations for English words from the bilingual Hong Kong LegCo proceedings, reporting a precision of about 90%. Others make use of existing bilingual dictionaries for word alignment, which is useful when the corpus is too small for statistical methods and contains many general words. For example, Ker and Chang (1997) worked with a small bilingual corpus (English-Chinese) and attempted to overcome the limitation of statistical methods by class-based rules, with</context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>F.Z. Smadja, K. McKeown, and V. Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>X Xia</author>
</authors>
<title>Large-scale automatic extraction of an English-Chinese translation lexicon.</title>
<date>1995</date>
<journal>Machine Translation,</journal>
<pages>9--3</pages>
<contexts>
<context position="4761" citStr="Wu and Xia, 1995" startWordPosition="713" endWordPosition="716">he derivation of bilingual dictionaries often follows text alignment (possibly at the word level) based on some frequency criterion. Nevertheless, in practice sentence alignment is not always distinctly separated from word alignment, and neither is word alignment and the derivation of translation lexicons thereby. In fact, apart from Gale and Church&apos;s length-based method, most other methods worked at the lexical level to some extent. Word alignment can be done statistically by learning the translation association or token cooccurrences between the source language and the target language (e.g. Wu and Xia, 1995; Melamed, 1997). The acquisition of bilingual translation equivalences sometimes follows the acquisition of monolingual collocations (e.g. Wu and Xia, 1995; Smadja et al., 1996). Wu and Xia (1995), for instance, made use of terms extracted by CXtract, a Chinese term extraction algorithm, to learn collocation translations for English words from the bilingual Hong Kong LegCo proceedings, reporting a precision of about 90%. Others make use of existing bilingual dictionaries for word alignment, which is useful when the corpus is too small for statistical methods and contains many general words. F</context>
<context position="9214" citStr="Wu and Xia, 1995" startWordPosition="1424" endWordPosition="1427">onaries might be limited, because such dictionaries do not always cover legal terminologies and their translations. Even some general terms may be translated in special ways in legal texts. • Different language families English and Chinese are from different language families and have little resemblance of each other. As a result, lexical criteria like cognates, or alignment via a bridging language will not be applicable. • Unpredictable word complexity In many of the studies reviewed in Section 2, the alignment was confined to single words, at least for one of the languages in question (e.g. Wu and Xia, 1995; Melamed, 1997; Resnik and Melamed, 1997; Fung, 1998; Huang and Choi, 2000). For about 150 years, the legal system in Hong Kong operated through English only. So, many legal concepts may not be as precisely lexicalised in Chinese. The lengths and complexity of legal translations between English and Chinese are therefore not necessarily correlated. Aligning legal terms should therefore take care of the varying lengths and complexity of a source term and its translation equivalence. 3.1 Two Assumptions On the other hand, we also have some advantages from the bilingual legal texts. Legal transla</context>
</contexts>
<marker>Wu, Xia, 1995</marker>
<rawString>D. Wu and X. Xia. 1995. Large-scale automatic extraction of an English-Chinese translation lexicon. Machine Translation, 9(3-4285-313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Aligning a parallel English-Chinese corpus statistically with lexical criteria.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL-94),</booktitle>
<location>Las Cruces, NM.</location>
<contexts>
<context position="3844" citStr="Wu (1994)" startWordPosition="578" endWordPosition="579">escribe the properties of our corpus in Section 3 and our method in Section 4. A pilot experiment with the proposed approach will be reported in Section 5 with results discussed in Section 6, followed by a conclusion in Section 7. 2 Related Work Conventional bilingual sentence alignment is often based statistically on sentence length (e.g. Gale and Church, 1991), or lexically on cognates (e.g. Simard et al., 1992) and correspondence of word position (e.g. Kay and Roescheisen, 1993; Piperidis et al., 1997). Such criteria, however, are mostly applicable to Indo-European language pairs. Although Wu (1994) found the length criterion applied surprisingly well between English and Chinese, he supplemented the statistical, length-based method with lexical criteria. Fixed words or phrases with consistent translations, like month names, were identified first, and he observed an improvement of the results. The derivation of bilingual dictionaries often follows text alignment (possibly at the word level) based on some frequency criterion. Nevertheless, in practice sentence alignment is not always distinctly separated from word alignment, and neither is word alignment and the derivation of translation l</context>
</contexts>
<marker>Wu, 1994</marker>
<rawString>D. Wu. 1994. Aligning a parallel English-Chinese corpus statistically with lexical criteria. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL-94), Las Cruces, NM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>