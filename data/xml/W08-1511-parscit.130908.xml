<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000720">
<title confidence="0.996757">
A Small-Vocabulary Shared Task for Medical Speech Translation
</title>
<author confidence="0.990354">
Manny Rayner&apos;, Pierrette Bouillon&apos;, Glenn Flores2, Farzad Ehsani3
Marianne Starlander&apos;, Beth Ann Hockey4, Jane Brotanek2, Lukas Biewald5
</author>
<affiliation confidence="0.996789">
&apos; University of Geneva, TIM/ISSCO, 40 bvd du Pont-d’Arve, CH-1211 Geneva 4, Switzerland
</affiliation>
<email confidence="0.7044445">
{Emmanuel.Rayner,Pierrette.Bouillon}@issco.unige.ch
Marianne.Starlander@eti.unige.ch
</email>
<address confidence="0.466104">
2 UT Southwestern Medical Center, Children’s Medical Center of Dallas
</address>
<email confidence="0.982044">
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
</email>
<sectionHeader confidence="0.345532" genericHeader="abstract">
3 Fluential, Inc, 1153 Bordeaux Drive, Suite 211, Sunnyvale, CA 94089, USA
</sectionHeader>
<email confidence="0.848083">
farzad@fluentialinc.com
</email>
<note confidence="0.272262">
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035–1000
</note>
<email confidence="0.946177">
bahockey@ucsc.edu
</email>
<sectionHeader confidence="0.913111" genericHeader="keywords">
5 Dolores Labs
</sectionHeader>
<email confidence="0.938736">
lukeab@gmail.com
</email>
<sectionHeader confidence="0.997064" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.999958866666667">
We outline a possible small-vocabulary
shared task for the emerging medical
speech translation community. Data would
consist of about 2000 recorded and tran-
scribed utterances collected during an eval-
uation of an English H Spanish version
of the Open Source MedSLT system; the
vocabulary covered consisted of about 450
words in English, and 250 in Spanish. The
key problem in defining the task is to agree
on a scoring system which is acceptable
both to medical professionals and to the
speech and language community. We sug-
gest a framework for defining and admin-
istering a scoring system of this kind.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999716">
In computer science research, a “shared task” is a
competition between interested teams, where the
goal is to achieve as good performance as possible
on a well-defined problem that everyone agrees to
work on. The shared task has three main compo-
nents: training data, test data, and an evaluation
metric. Both test and training data are divided
up into sets of items, which are to be processed.
</bodyText>
<note confidence="0.723195">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967215666666667">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999759814814815">
The evaluation metric defines a score for each pro-
cessed item. Competitors are first given the train-
ing data, which they use to construct and/or train
their systems. They are then evaluated on the test
data, which they have not previously seen.
In many areas of speech and language process-
ing, agreement on a shared task has been a major
step forward. Often, it has in effect created a new
subfield, since it allows objective comparison of
results between different groups. For example, it
is very common at speech conference to have spe-
cial sessions devoted to recognition within a par-
ticular shared task database. In fact, a conference
without at least a couple of such sessions would
be an anomaly. A recent success story in language
processing is the Recognizing Textual Entailment
(RTE) task1. Since its inception in 2004, this has
become extremely popular; the yearly RTE work-
shop now attracts around 40 submissions, and error
rates on the task have more than halved.
Automatic medical speech translation would
clearly benefit from a shared task. As was made
apparent at the initial 2006 workshop in New
York2, nearly every group has both a unique ar-
chitecture and a unique set of data, essentially
making comparisons impossible. In this note, we
will suggest an initial small-vocabulary medical
</bodyText>
<footnote confidence="0.99978325">
1http://www.pascal-network.org/
Challenges/RTE/
2http://www.issco.unige.ch/pub/
SLT workshop proceedings book.pdf
</footnote>
<page confidence="0.95766">
60
</page>
<note confidence="0.602057">
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 60–63
</note>
<bodyText confidence="0.963043388888889">
Manchester, August 2008
shared task. The aspect of the task that is hard-
est to define is the evaluation metric, since there
unfortunately appears to be considerable tension
between the preferences of medical professionals
and speech system implementers. Medical profes-
sionals would prefer to carry out a “deep” evalu-
ation, in terms of possible clinical consequences
following from a mistranslation. System evalua-
tors will on the other hand prefer an evaluation
method that can be carried out quickly, enabling
frequent evaluations of evolving systems. The plan
we will sketch out is intended to be a compromise
between these two opposing positions.
The rest of the note is organised as follows.
Section 2 describes the data we propose to use,
and Section 3 discusses our approach to evaluation
metrics. Section 4 concludes.
</bodyText>
<sectionHeader confidence="0.994462" genericHeader="method">
2 Data
</sectionHeader>
<bodyText confidence="0.998539866666667">
The data we would use in the task is for the English
↔ Spanish language pair, and was collected us-
ing two different versions of the MedSLT system3.
In each case, the scenario imagines an English-
speaking doctor conducting a verbal examination
of a Spanish-speaking patient, who was assumed
to be have visited the doctor because they were
displaying symptoms which included a sore throat.
The doctor’s task was to use the translation sys-
tem to determine the likely reason for the patient’s
symptoms.
The two versions of the system differed in
terms of the linguistic coverage offered. The
more restricted version supported a minimal range
of English questions (vocabulary size, about 200
words), and only allowed the patient to respond
using short phrases (vocabulary size, 100 words).
Thus for example the doctor could ask “How long
have you had a sore throat?”, and the patient would
respond Hace dos dias (“for two days”). The
less restricted version supported a broader range
of doctor questions (vocabulary size, about 450
words), and allowed the patient to respond using
both short phrases and complete sentences (vocab-
ulary size, about 225 words). Thus in response
to “How long have you had a sore throat?”, the
patient could say either Hace dos dias (“for two
days”) or Tengo dolor en la garganta hace dos dias
(“I have had a sore throat for two days”).
Data was collected in 64 sessions, carried out
</bodyText>
<footnote confidence="0.94956">
3http://www.issco.unige.ch/projects/
medslt/
</footnote>
<bodyText confidence="0.99995">
over two days in February 2008 at the University
of Texas Medical Center, Dallas. In each session,
the part of the “doctor” was played by a real physi-
cian, and the part of the “patient” by a Spanish-
speaking interpreter. This resulted in 1005 En-
glish utterances, and 967 Spanish utterances. All
speech data is available in SPHERE-headed form,
and totals about 90 MB. A master file, organised in
spreadsheet form, lists metadata for each recorded
file. This includes a transcription, a possible valid
translation (verified by a bilingual translator), IDs
for the “doctor”, the “patient”, the session and the
system version, and the preceding context. Con-
text is primarily required for short answers, and
consists of the most recent preceding doctor ques-
tion.
</bodyText>
<sectionHeader confidence="0.999305" genericHeader="evaluation">
3 Evaluation metrics
</sectionHeader>
<bodyText confidence="0.99998525">
The job of the evaluation component in the shared
task is to assign a score to each translated utter-
ance. Our basic model will be the usual one for
shared tasks in speech and language. Each pro-
cessed utterance will be assigned to a category;
each category will be associated with a specified
score; the score for a complete testset will the sum
of the scores for all of its utterances. We thus have
three sub-problems: deciding what the categories
are, deciding how to assign a category to a pro-
cessing utterance, and deciding what scores to as-
sociate with each category.
</bodyText>
<subsectionHeader confidence="0.998369">
3.1 Defining categories
</subsectionHeader>
<bodyText confidence="0.999960263157895">
If the system attempts to translate an utterance,
there are a priori three things that can happen:
it can produce a correct translation, an incorrect
translation, or no translation. Medical speech
translation is a safety-critical problem; a mistrans-
lation may have serious consequences, up to and
including the death of the patient. This implies
that the negative score for an incorrect translation
should be high in comparison to the positive score
for a correct translation. So a naive scoring func-
tion might be “1 point for a correct translation, 0
points for no translation, –1000 points for an in-
correct translation.”
However, since the high negative score for a
mistranslation is justified by the possible serious
consequences, not all mistranslations are equal;
some are much more likely than others to result in
clinical consequences. For example, consider the
possible consequences of two different mistrans-
</bodyText>
<page confidence="0.996647">
61
</page>
<bodyText confidence="0.99953730952381">
lations of the Spanish sentence La penicilina me 4. Nonsense.
da alergias. Ideally, we would like the system to 5. No translation produced, but later rephrased
translate this as “I am allergic to penicillin”. If it in a way the system handled adequately.
instead says “I am allergic to the penicillin”, the 6. No translation produced, but not rephrased in
translation is slightly imperfect, but it is hard to see a way the system handled adequately.
any important misunderstanding arising as a result.
In contrast, the translation “I am not allergic to
penicillin”, which might be produced as the result
of a mistake in speech recognition, could have very
serious consequences indeed. (Note in passing that
both errors are single-word insertions). Another
type of result is a nonsensical translation, perhaps
due to an internal system error. For instance, sup-
pose the translation of our sample sentence were
“The allergy penicillin does me”. In this case, it
is not clear what will happen. Most users will
probably dismiss the output as meaningless; a few
might be tempted to try and decipher it, with un-
predictable results.
Examples like these show that it is important for
the scoring metric to differentiate between differ-
ent classes of mistranslations, with the differentia-
tion based on possible clinical consequences of the
error. For similar reasons, it is important to think
about the clinical consequences when the system
produces correct translations, or fails to produce
a translation. For example, when the system cor-
rectly translates “Hello” as Buenas dias, there are
not likely to be any clinical consequences, so it is
reasonable to reward it with a lower score than the
one assigned to a clinically contentful utterance.
When no translation is produced, it also seems cor-
rect to distinguish the case where the user was able
recover by a suitably rephrasing the utterance from
the one where they simply gave up. For example,
if the system failed to translate “How long has this
cough been troubling you?”, but correctly handled
the simpler formulation “How long have you had a
cough?”, we would give this a small positive score,
rather than a simple zero.
Summarising, we propose to classify transla-
tions into the following seven categories:
</bodyText>
<listItem confidence="0.931608285714286">
1. Perfect translation, useful clinical conse-
quences.
2. Perfect translation, no useful clinical conse-
quences.
3. Imperfect translation, but not dangerous in
terms of clinical consequences.
3.2 Assigning utterances to categories
</listItem>
<bodyText confidence="0.995673910714286">
At the moment, medical professionals will only
accept the validity of category assignments made
by trained physicians. In the worst case, it is
clearly true that a layman, even one who has re-
ceived some training, will not be able to determine
whether or not a mistranslation has clinical signif-
icance.
Physician time is, however, a scarce and valu-
able resource, and, as usual, typical case and worst
case may be very different. Particularly for routine
testing during system development, it is clearly not
possible to rely on expert physician assessments.
We consequently suggest a compromise strategy.
We will first carry out an evaluation using medical
experts, in order to establish a gold standard. We
will then repeat this evaluation using non-experts,
and determine how large the differential is in prac-
tice.
We initially intend to experiment with two dif-
ferent groups of non-experts. At Geneva Uni-
versity, we will use students from the School of
Translation. These students will be selected for
competence in English and Spanish, and will re-
ceive a few hours of training on determination of
clinical significance in translation, using guide-
lines developed in collaboration with Glenn Flores
and his colleagues at the UT Southwestern Medi-
cal Center, Texas. Given that the corpus material
is simple and sterotypical, we think that this ap-
proach should yield a useful approximation to ex-
pert judgements.
Although translation students are far cheaper
than doctors, they are still quite expensive, and
evaluation turn-around will be slow. For these rea-
sons, we also propose to investigate the idea of per-
forming evaluations using Amazon’s Mechanical
Turk4. This will be done by Dolores Labs, a new
startup specialising in Turk-based crowdsourcing.
3.3 Scores for categories
We have not yet agreed on exact scores for the
different categories, and this is something that is
7. Imperfect translation, potentially dangerous. 62 4http://www.mturk.com/mturk/welcome
probably best decided after mutual discussion at
the workshop. Some basic principles will be evi-
dent from the preceding discussion. The scale will
be normalised so that failure to produce a trans-
lation is counted as zero; potentially dangerous
mistranslations will be associated with a negative
score large in comparison to the positive score for
a useful correct translation. Inability to communi-
cate can certainly be dangerous (this is the point of
having a translation system in the first place), but
mistakenly believing that one has communicated
is usually much worse. As Mark Twain put it: “It
ain’t what you don’t know that gets you into trou-
ble. It’s what you know for sure that just ain’t so”.
</bodyText>
<subsectionHeader confidence="0.988666">
3.4 Discarding uncertain responses
</subsectionHeader>
<bodyText confidence="0.999986833333334">
Given that both speech recognition and machine
translation are uncertain technologies, a high
penalty for mistranslations means that systems
which attempt to translate everything may eas-
ily end up with an average negative score - in
other words, they would score worse than a system
which did nothing! For the shared task to be in-
teresting, we must address this problem, and in the
doctor to patient direction there is a natural way
to do so. Since the doctor can reasonably be as-
sumed to be a trained professional who has had
time to learn to operate the system, we can say that
he has the option of aborting any translation where
the machine does not appear to have understood
correctly.
We thus relativise the task with respect to a “fil-
ter”: for each utterance, we produce both a transla-
tion in the target language, and a “reference trans-
lation” in the source language, which in some way
gives information about what the machine has un-
derstood. The simplest way to produce this “ref-
erence translation” is to show the words produced
by speech recognition. When scoring, we evaluate
both translations, and ignore all examples where
the reference translation is evaluated as incorrect.
To go back to the “penicillin” example, suppose
that Spanish source-language speech recognition
has incorrectly recognised La penicilina me da
alergias as La penicilina no me da alergias. Even
if this produces the seriously incorrect translation
“I am not allergic to penicillin”, we can score it
as a zero rather than a negative, on the grounds
that the speech recognition result already shows
the Spanish-speaking doctor that something has
gone wrong before any translation has happened.
The reference translation may also be produced in
a more elaborate way; a common approach is to
translate back from the target language result into
the source language.
Although the “filtered” version of the medical
speech translation task makes good sense in the
doctor to patient direction, it is less clear how
meaningful it is in the patient to doctor direction.
Most patients will not have used the system before,
and may be distressed or in pain. It is consequently
less reasonable to expect them to be able to pay at-
tention to the reference translation when using the
system.
</bodyText>
<sectionHeader confidence="0.99652" genericHeader="conclusions">
4 Summary and conclusions
</sectionHeader>
<bodyText confidence="0.999989173913044">
The preceding notes are intended to form a frame-
work which will serve as a basis for discussion at
the workshop. As already indicated, the key chal-
lenge here is to arrive at metrics which are ac-
ceptable to both the medical and the speech and
language community. This will certainly require
more negotiation. We are however encouraged by
the fact that the proposal, as presented here, has
been developed jointly by representatives of both
communities, and that we appear to be fairly near
agreement. Another important parameter which
we have intentionally left blank is the duration of
the task; we think it will be more productive to de-
termine this based on the schedules of interested
parties.
Realistically, the initial definition of the metric
can hardly be more than a rough guess. Experi-
mentation during the course of the shared task will
probably show that some adjustment will be desir-
able, in order to make it conform more closely to
the requirements of the medical community. If we
do this, we will, in the interests of fairness, score
competing systems using all versions of the metric.
</bodyText>
<page confidence="0.999225">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.123337">
<title confidence="0.999799">A Small-Vocabulary Shared Task for Medical Speech Translation</title>
<author confidence="0.9961555">Pierrette Glenn Farzad Beth Ann Jane Lukas</author>
<affiliation confidence="0.900425666666667">of Geneva, TIM/ISSCO, 40 bvd du Pont-d’Arve, CH-1211 Geneva 4, Marianne.Starlander@eti.unige.ch 2UT Southwestern Medical Center, Children’s Medical Center of</affiliation>
<address confidence="0.565775">3Fluential, Inc, 1153 Bordeaux Drive, Suite 211, Sunnyvale, CA 94089,</address>
<email confidence="0.999039">farzad@fluentialinc.com</email>
<author confidence="0.430402">UCSC UARC</author>
<author confidence="0.430402">NASA Ames Research Center</author>
<author confidence="0.430402">Moffett Field</author>
<author confidence="0.430402">CA</author>
<email confidence="0.769983">bahockey@ucsc.edu</email>
<note confidence="0.781224">5Dolores Labs</note>
<email confidence="0.996708">lukeab@gmail.com</email>
<abstract confidence="0.9987429375">We outline a possible small-vocabulary shared task for the emerging medical speech translation community. Data would consist of about 2000 recorded and transcribed utterances collected during an evalof an English version of the Open Source MedSLT system; the vocabulary covered consisted of about 450 words in English, and 250 in Spanish. The key problem in defining the task is to agree on a scoring system which is acceptable both to medical professionals and to the speech and language community. We suggest a framework for defining and administering a scoring system of this kind.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>