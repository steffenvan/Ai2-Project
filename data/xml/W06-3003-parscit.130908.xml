<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005571">
<title confidence="0.83543">
Modeling Reference Interviews as a Basis for Improving Automatic QA
Systems
</title>
<note confidence="0.8145735">
Nancy J. McCracken, Anne R. Diekema, Grant Ingersoll, Sarah C.
Harwell, Eileen E. Allen, Ozgur Yilmazel, Elizabeth D. Liddy
</note>
<bodyText confidence="0.95829675">
Center for Natural Language Processing
Syracuse University
Syracuse, NY 13244
{ njmccrac, diekemar, gsingers, scharwel, eeallen, oyilmaz, liddy}@syr.edu
</bodyText>
<sectionHeader confidence="0.988511" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.96741192">
The automatic QA system described in
this paper uses a reference interview
model to allow the user to guide and
contribute to the QA process. A set of
system capabilities was designed and
implemented that defines how the user’s
contributions can help improve the
system. These include tools, called the
Query Template Builder and the
Knowledge Base Builder, that tailor the
document processing and QA system to
a particular domain by allowing a
Subject Matter Expert to contribute to
the query representation and to the
domain knowledge. During the QA
process, the system can interact with the
user to improve query terminology by
using Spell Checking, Answer Type
verification, Expansions and Acronym
Clarifications. The system also has
capabilities that depend upon, and
expand the user’s history of interaction
with the system, including a User
Profile, Reference Resolution, and
Question Similarity modules
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999792230769231">
Reference librarians have successfully fielded
questions of all types for years using the Reference
Interview to clarify an unfocused question, narrow
a broad question, and suggest further information
that the user might not have thought to ask for.
The reference interview tries to elicit sufficient
information about the user’s real need to enable a
librarian to understand the question enough to
begin searching. The question is clarified, made
more specific, and contextualized with relevant
detail. Real questions from real users are often
“ill-formed’ with respect to the information
system; that is, they do not match the structure of
‘expectations’ of the system (Ross et al., 2002). A
reference interview translates the user’s question
into a representation that the librarian and the
library systems can interpret correctly. The human
reference interview process provides an ideal,
well-tested model of how questioner and answerer
work together co-operatively and, we believe, can
be successfully applied to the digital environment.
The findings of researchers applying this model in
online situations (Bates, 1989, Straw, 2004) have
enabled us to understand how a system might work
with the user to provide accurate and relevant
answers to complex questions.
Our long term goal in developing Question-
Answering (QA) systems for various user groups is
to permit, and encourage users to positively
contribute to the QA process, to more nearly
mirror what occurs in the reference interview, and
to develop an automatic QA system that provides
fuller, more appropriate, individually tailored
responses than has been available to date.
Building on our Natural Language
Processing (NLP) experience in a range of
information access applications, we have focused
our QA work in two areas: 1) modeling the subject
domain of the collections of interest to a set of
</bodyText>
<page confidence="0.99185">
17
</page>
<note confidence="0.9178535">
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 17–24,
New York City, NY, USA. June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.997483">
users for whom we are developing the QA system,
and; 2) modeling the query clarification and
negotiation interaction between the information
seeker and the information provider. Examples of
these implementation environments are:
</bodyText>
<listItem confidence="0.547359428571429">
1. Undergraduate aerospace engineering students
working in collaborative teams on course
projects designing reusable launch vehicles,
who use a QA system in their course-related
research.
2. Customers of online business sites who use a
QA system to learn more about the products or
</listItem>
<bodyText confidence="0.994396895348838">
services provided by the company, or who
wish to resolve issues concerning products or
service delivery.
In this paper, we describe the capabilities we
have developed for these specific projects in order
to explicate a more general picture of how we
model and utilize both the domains of inquiry and
typical interaction processes observed in these
diverse user groups.
2 Background and related research
Our work in this paper is based on two premises:
1) user questions and responsive answers need to
be understood within a larger model of the user’s
information needs and requirements, and, 2) a
good interactive QA system facilitates a dialogue
with its users to ensure it understands and satisfies
these information needs. The first premise is based
on the long-tested and successful model of the
reference interview (Bates, 1997, Straw, 2004),
which was again validated by the findings of an
ARDA-sponsored workshop to increase the
research community’s understanding of the
information seeking needs and cognitive processes
of intelligence analysts (Liddy, 2003). The second
premise instantiates this model within the digital
and distributed information environment.
Interactive QA assumes an interaction
between the human and the computer, typically
through a combination of a clarification dialogue
and user modeling to capture previous interactions
of users with the system. De Boni et al. (2005)
view the clarification dialogue mainly as the
presence or absence of a relationship between the
question from the user and the answer provided by
the system. For example, a user may ask a
question, receive an answer and ask another
question in order to clarify the meaning, or, the
user may ask an additional question which expands
on the previous answer. In their research De Boni
et al. (2005) try to determine automatically
whether or not there exists a relationship between a
current question and preceding questions, and if
there is a relationship, they use this additional
information in order to determine the correct
answer.
We prefer to view the clarification dialogue
as more two-sided, where the system and the user
actually enter a dialogue, similar to the reference
interview as carried out by reference librarians
(Diekema et al., 2004). The traditional reference
interview is a cyclical process in which the
questioner poses their question, the librarian (or the
system) questions the questioner, then locates the
answer based on information provided by the
questioner, and returns an answer to the user who
then determines whether this has satisfied their
information need or whether further clarification or
further questions are needed. The HITIQA
system’s (Small et al., 2004) view of a clarification
system is closely related to ours—their dialogue
aligns the understanding of the question between
system and user. Their research describes three
types of dialogue strategies: 1) narrowing the
dialogue, 2) broadening the dialogue, and 3) a fact
seeking dialogue.
Similar research was carried out by Hori et
al. (2003), although their system automatically
determines whether there is a need for a dialogue,
not the user. The system identifies ambiguous
questions (i.e. questions to which the system could
not find an answer). By gathering additional
information, the researchers believe that the system
can find answers to these questions. Clarifying
questions are automatically generated based on the
ambiguous question to solicit additional
information from the user. This process is
completely automated and based on templates that
generate the questions. Still, removing the
cognitive burden from the user through automation
is not easy to implement and can be the cause of
error or misunderstanding. Increasing user
involvement may help to reduce this error.
As described above, it can be seen that
interactive QA systems have various levels of
dialogue automation ranging from fully automatic
(De Boni et al., 2004, Hori et al., 2004) to a strong
</bodyText>
<page confidence="0.998493">
18
</page>
<bodyText confidence="0.999840875">
user involvement (Small et al., 2004, Diekema et
al., 2004). Some research suggests that
clarification dialogues in open-domain systems are
more unpredictable than those in restricted domain
systems, the latter lending itself better to
automation (Hori et al., 2003, Jönsson et al., 2004).
Incorporating the user’s inherent knowledge of the
intention of their query is quite feasible in
restricted domain systems and should improve the
quality of answers returned, and make the
experience of the user a less frustrating one. While
many of the systems described above are
promising in terms of IQA, we believe that
incorporating knowledge of the user in the
question negotiation dialogue is key to developing
a more accurate and satisfying QA system.
</bodyText>
<sectionHeader confidence="0.993942" genericHeader="method">
3 System Capabilities
</sectionHeader>
<bodyText confidence="0.999909">
In order to increase the contribution of users to our
question answering system, we expanded our
traditional domain independent QA system by
adding new capabilities that support system-user
interaction.
</bodyText>
<subsectionHeader confidence="0.997008">
3.1 Domain Independent QA
</subsectionHeader>
<bodyText confidence="0.999905853333333">
Our traditional domain-independent QA capability
functions in two stages, the first information
retrieval stage selecting a set of candidate
documents, the second stage doing the answer
finding within the filtered set. The answer finding
process draws on models of question types and
document-based knowledge to seek answers
without additional feedback from the user. Again,
drawing on the modeling of questions as they
interact with the domain representation, the system
returns answers of variable lengths on the fly in
response to the nature of the question since factoid
questions may be answered with a short answer,
but complex questions often require longer
answers. In addition, since our QA projects were
based on closed collections, and since closed
collections may not provide enough redundancy to
allow for short answers to be returned, the variable
length answer capability assists in finding answers
to factoid questions. The QA system provides
answers in the form of short answers, sentences,
and answer-providing passages, as well as links to
the full answer-providing documents. The user can
provide relevance feedback by selecting the full
documents that offer the best information. Using
this feedback, the system can reformulate the
question and look for a better set of documents
from which to find an answer to the question.
Multiple answers can be returned, giving the user a
more complete picture of the information held
within the collection.
One of our first tactics to assist in both
question and domain modeling for specific user
needs was to develop tools for Subject Matter
Experts (SMEs) to tailor our QA systems to a
particular domain. Of particular interest to the
interactive QA community is the Query Template
Builder (QTB) and the Knowledge Base Builder
(KBB).
Both tools allow a priori alterations to
question and domain modeling for a community,
but are not sensitive to particular users. Then the
interactive QA system permits question- and user-
specific tailoring of system behavior simply
because it allows subject matter experts to change
the way the system understands their need at the
time of the search.
Question Template Builder (QTB) allows
a subject matter expert to fine tune a question
representation by adding or removing stopwords
on a question-by-question basis, adding or masking
expansions, or changing the answer focus. The
QTB displays a list of Question-Answer types,
allows the addition of new Answer Types, and
allows users to select the expected answer type for
specific questions. For example, the subject matter
expert may want to adjust particular “who”
questions as to whether the expected answer type is
“person” or “organization”. The QTB enables
organizations to identify questions for which they
want human intervention and to build specialized
term expansion sets for terms in the collection.
They can also adjust the stop word list, and refine
and build the Frequently or Previously Asked
Question (FAQ/PAQ) collection.
Knowledge Base Builder (KBB) is a suite
of tools developed for both commercial and
government customers. It allows the users to view
and extract terminology that resides in their
document collections. It provides useful statistics
about the corpus that may indicate portions that
require attention in customization. It collects
frequent / important terms with categorizations to
enable ontology building (semi-automatic,
permitting human review), term collocation for use
</bodyText>
<page confidence="0.996294">
19
</page>
<bodyText confidence="0.999733">
in identifying which sense of a word is used in the
collection for use in term expansion and
categorization review. KBB allows companies to
tailor the QA system to the domain vocabulary and
important concept types for their market. Users
are able to customize their QA applications
through human-assisted automatic procedures.
The Knowledge Bases built with the tools are
</bodyText>
<figureCaption confidence="0.999376">
Figure 1. System overview
</figureCaption>
<figure confidence="0.9194059">
User
Spell Expansion Answer
checking Clarification Type
Verification
Question
Processing
Session
Tracking
User Profile
Reference
Resolution
Question
Similarity
IR
QTB
Domain Modeling
KBB
Answer
Providers
Answer
</figure>
<bodyText confidence="0.998791">
primarily lexical semantic taxonomic resources.
These are used by the system in creating frame
representations of the text. Using automatically
harvested data, customers can review and alter
categorization of names and entities and expand
the underlying category taxonomy to the domain of
interest. For example, in the NASA QA system,
experts added categories like “material”, “fuel”,
“spacecraft” and “RLV”, (Reusable Launch
Vehicles). They also could specify that “RLV” is a
subcategory of “spacecraft” and that space shuttles
like “Atlantis” have category “RLV”. The KBB
works in tandem with the QTB, where the user can
find terms in either documents or example queries
</bodyText>
<subsectionHeader confidence="0.902889">
3.2 Interactive QA Development
</subsectionHeader>
<bodyText confidence="0.999926269230769">
In our current NASA phase, developed for
undergraduate aerospace engineering students to
quickly find information in the course of their
studies on reusable launch vehicles, the user can
view immediate results, thus bypassing the
Reference Interviewer, or they may take the
opportunity to utilize its increased functionality
and interact with the QA system. The capabilities
we have developed, represented by modules added
to the system, fall into two groups. Group One
includes capabilities that draw on direct interaction
with the user to clarify what is being asked and that
address terminological issues. It includes Spell
Checking, Expansion Clarification, and Answer
Type Verification. Answers change dynamically as
the user provides more input about what was
meant. Group Two capabilities are dependent
upon, and expand upon the user’s history of
interaction with the system and include User
Profile, Session Tracking, Reference Resolution,
Question Similarity and User Frustration
Recognition modules. These gather knowledge
about the user, help provide co-reference
resolution within an extended dialogue, and
monitor the level of frustration a user is
experiencing.
</bodyText>
<page confidence="0.973753">
20
</page>
<bodyText confidence="0.996834333333333">
The capabilities are explained in greater
detail below. Figure 1 captures the NASA system
process and flow.
</bodyText>
<subsectionHeader confidence="0.928647">
Group One:
</subsectionHeader>
<bodyText confidence="0.990393641791045">
In this group of interactive capabilities, after the
user asks a query, answers are returned as in a
typical system. If the answers presented aren’t
satisfactory, the system will embark on a series of
interactive steps (described below) in which
alternative spelling, answertypes, clarifications and
expansions will be suggested. The user can
choose from the system’s suggestions or type in
their own. The system will then revise the query
and return a new set of answers. If those answers
aren’t satisfactory, the user can continue
interacting with the system until appropriate
answers are found.
Spell checking: Terms not found in the
index of the document collection are displayed as
potentially misspelled words. In this preliminary
phase, spelling is checked and users have the
opportunity to select correct and/or alternative
spellings.
AnswerType verification: The interactive
QA system displays the type of answer that the
system is looking for in order to answer the
question. For example for the question, Who
piloted the first space shuttle?, the answer type is
‘person’, and the system will limit the search for
candidate short answers in the collection to those
that are a person’s name. The user can either
accept the system’s understanding of the question
or reject the type it suggests. This is particularly
useful in semantically ambiguous questions such as
“Who makes Mountain Dew?” where the system
might interpret the question as needing a person,
but the questioner actually wants the name of a
company.
Expansion: This capability allows users to
review the possible relevant terms (synonyms and
group members) that could enhance the question-
answering process. The user can either select or
deselect terms of interest which do or do not
express the intent of the question. For example, if
the user asks: How will aerobraking change the
orbit size? then the system can bring back the
following expansions for “aerobraking”: By
aerobraking do you mean the following: 1)
aeroassist, 2) aerocapture, 3) aeromaneuvering, 4)
interplanetary transfer orbits, or 5) transfer orbits.
Acronym Clarification: For abbreviations
or acronyms within a query, the full explications
known by the system for the term can be displayed
back to the user. The clarifications implemented
are a priori limited to those that are relevant to the
domain. In the aerospace domain for example, if
the question was What is used for the TPS of the
RLV?, the clarifications of TPS would be thermal
protection system, thermal protection subsystem,
test preparation sheet, or twisted pair shielded, and
the clarification of RLV would be reusable launch
vehicle. The appropriate clarifications can be
selected to assist in improving the search. For a
more generic domain, the system would offer
broader choices. For example, if the user types in
the question: What educational programs does the
AIAA offer?, then the system might return: By
AIAA, do you mean (a) American Institute of
Aeronautics and Astronautics (b) Australia
Indonesia Arts Alliance or (c) Americans for
International Aid &amp; Adoption?
</bodyText>
<subsectionHeader confidence="0.942091">
Group Two:
</subsectionHeader>
<bodyText confidence="0.9990506">
User Profile: The User Profile keeps track of more
permanent information about the user. The profile
includes a small standard set of user attributes,
such as the user’s name and / or research interests.
In our commercially funded work, selected
information gleaned from the question about the
user was also captured in the profile. For example,
if a user asks “How much protein should my
husband be getting every day?”, the fact that the
user is married can be added to their profile for
future marketing, or for a new line of dialogue to
ask his name or age. This information is then
made available as context information for the QA
system to resolve references that the user makes to
themselves and their own attributes.
For the NASA question-answering
capability, to assist students in organizing their
questions and results, there is an area for users to
save their searches as standing queries, along with
the results of searching (Davidson, 2006). This
information, representing topics and areas of
interest, can help to focus answer finding for new
questions the user asks.
Not yet implemented, but of interest, is the
ability to save information such as a user’s
</bodyText>
<page confidence="0.998065">
21
</page>
<bodyText confidence="0.987905132653062">
preferences (format, reliability, sources), that could
be used as filters in the answer finding process.
Reference Resolution: A basic feature of
an interactive QA system is the requirement to
understand the user’s questions and responsive
answers as one session. The sequence of questions
and answers forms a natural language dialogue
between the user and the system. This necessitates
NLP processing at the discourse level, a primary
task of which is to resolve references across the
session. Building on previous work in this area
done for the Context Track of TREC 2001
(Harabagiu et al, 2001) and additional work (Chai
and Jin, 2004) suggesting discourse structures are
needed to understand the question/answer
sequence, we have developed session-based
reference resolution capability. In a dialogue, the
user naturally includes referring phrases that
require several types of resolution.
The simplest case is that of referring
pronouns, where the user is asking a follow-up
question, for example:
Q1: When did Madonna enter the music business?
A1: Madonna&apos;s first album, Madonna, came out in
1983 and since then she&apos;s had a string of hits, been
a major influence in the music industry and
become an international icon.
Q2: When did she first move to NYC?
In this question sequence, the second
question contains a pronoun, “she”, that refers to
the person “Madonna” mentioned both in the
previous question and its answer. Reference
resolution would transform the question into
“When did Madonna first move to NYC?”
Another type of referring phrase is the
definite common noun phrase, as seen in the next
example:
Q1: If my doctor wants me to take Acyclovir, is it
expensive?
A1: Glaxo-Wellcome, Inc., the company that
makes Acyclovir, has a program to assist
individuals that have HIV and Herpes.
Q2: Does this company have other assistance
programs?
The second question has a definite noun
phrase “this company” that refers to “Glaxo-
Wellcome, Inc.” in the previous answer, thus
transforming the question to “Does Glaxo-
Wellcome, Inc. have other assistance programs?”
Currently, we capture a log of the
question/answer interaction, and the reference
resolution capability will resolve any references in
the current question that it can by using linguistic
techniques on the discourse of the current session.
This is almost the same as the narrative
coreference resolution used in documents, with the
addition of the need to understand first and second
person pronouns from the dialogue context. The
coreference resolution algorithm is based on
standard linguistic discourse processing techniques
where referring phrases and candidate resolvents
are analyzed along a set of features that typically
includes gender, animacy, number, person and the
distance between the referring phrase and the
candidate resolvent.
Question Similarity: Question Similarity is
the task of identifying when two or more questions
are related. Previous studies (Boydell et al., 2005,
Balfe and Smyth, 2005) on information retrieval
have shown that using previously asked questions
to enhance the current question is often useful for
improving results among like-minded users.
Identifying related questions is useful for finding
matches to Frequently Asked Questions (FAQs)
and Previously Asked Questions (PAQs) as well as
detecting when a user is failing to find adequate
answers and may be getting frustrated.
Furthermore, similar questions can be used during
the reference interview process to present
questions that other users with similar information
needs have used and any answers that they
considered useful.
CNLP’s question similarity capability
comprises a suite of algorithms designed to
identify when two or more questions are related.
The system works by analyzing each query using
our Language-to-Logic (L2L) module to identify
and weight keywords in the query, provide
expansions and clarifications, as well as determine
the focus of the question and the type of answer the
user is expecting (Liddy et al., 2003). We then
compute a series of similarity measures on two or
more L2L queries. Our measures adopt a variety
of approaches, including those that are based on
keywords in the query: cosine similarity, keyword
string matching, expansion analysis, and spelling
variations. In addition, two measures are based on
the representation of the whole query:answer type
</bodyText>
<page confidence="0.986669">
22
</page>
<bodyText confidence="0.9994006">
and answer frame analysis. An answer frame is our
representation of the meaningful extractions
contained in the query, along with metadata about
where they occur and any other extractions that
relate to in the query.
Our system will then combine the weighted
scores of two or more of these measures to
determine a composite score for the two queries,
giving more weight to a measure that testing has
determined to be more useful for a particular task.
We have utilized our question similarity
module for two main tasks. For FAQ/PAQ (call it
XAQ) matching, we use question similarity to
compare the incoming question with our database
of XAQs. Through empirical testing, we
determined a threshold above which we consider
two questions to be similar.
Our other use of question similarity is in the
area of frustration detection. The goal of
frustration detection is to identify the signs a user
may be giving that they are not finding relevant
answers so that the system can intervene and offer
alternatives before the user leaves the system, such
as similar questions from other users that have
been successful.
</bodyText>
<sectionHeader confidence="0.999378" genericHeader="method">
4 Implementations:
</sectionHeader>
<bodyText confidence="0.999924409090909">
The refinements to our Question Answering
system and the addition of interactive elements
have been implemented in three different, but
related working systems, one of which is strictly an
enhanced IR system. None of the three
incorporates all of these capabilities. In our work
for MySentient, Ltd, we developed the session-
based reference resolution capability, implemented
the variable length and multiple answer capability,
modified our processing to facilitate the building
of a user profile, added FAQ/PAQ capability, and
our Question Similarity capability for both
FAQ/PAQ matching and frustration detection. A
related project, funded by Syracuse Research
Corporation, extended the user tools capability to
include a User Interface for the KBB and basic
processing technology. Our NASA project has
seen several phases. As the project progressed, we
added the relevant developed capabilities for
improved performance. In the current phase, we
are implementing the capabilities which draw on
user choice.
</bodyText>
<sectionHeader confidence="0.99515" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999993756756757">
The reference interview has been
implemented as an interactive dialogue between
the system and the user, and the full system is near
completion. We are currently working on two
types of evaluation of our interactive QA
capabilities. One is a system-based evaluation in
the form of unit tests, the other is a user-based
evaluation. The unit tests are designed to verify
whether each module is working correctly and
whether any changes to the system adversely affect
results or performance. Crafting unit tests for
complex questions has proved challenging, as no
gold standard for this type of question has yet been
created. As the data becomes available, this type
of evaluation will be ongoing and part of regular
system development.
As appropriate for this evolutionary work
within specific domains for which there are not
gold standard test sets, our evaluation of the QA
systems has focused on qualitative assessments.
What has been a particularly interesting outcome is
what we have learned in elicitation from graduate
students using the NASA QA system, namely that
they have multiple dimensions on which they
evaluate a QA system, not just traditional recall
and precision (Liddy et al, 2004). The high level
dimensions identified include system performance,
answers, database content, display, and
expectations. Therefore the evaluation criteria we
believe appropriate for IQA systems are centered
around the display (UI) category as described in
Liddy et al, (2004). We will evaluate aspects of
the UI input subcategory, including question
understanding, information need understanding,
querying style, and question formulation
assistance. Based on this user evaluation the
system will be improved and retested.
</bodyText>
<sectionHeader confidence="0.999007" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999654">
Evelyn Balfe and Barry Smyth. 2005. An Analysis
of Query Similarity in Collaborative Web
Search. In Proceedings of the 27th European
Conference on Information Retrieval. Santiago
de Compostela, Spain.
</reference>
<page confidence="0.988464">
23
</page>
<table confidence="0.5273032">
Spoken Interactive ODQA System. In ICASSP.
Hongkong, I: 624-627.
Marcia J. Bates. 1989. The Design of Browsing
and Berrypicking Techniques for the Online
Search Interface. Online Review, 13: 407-424.
</table>
<reference confidence="0.99965264935065">
Mary Ellen Bates. 1997. The Art of the Reference
Interview. Online World. September 15.
Oisín Boydell, Barry Smyth, Cathal Gurrin, and
Alan F. Smeaton. 2005. A Study of Selection
Noise in Collaborative Web Search. In
Proceedings of the 19th International Joint
Conference on Artificial Intelligence. Edinburgh,
Scotland.
http://www.ijcai.org/papers/post-0214.pdf
Joyce Y. Chai, and Rong Jin. 2004. Discourse
Structure for Context Question Answering. In
Proceedings of the Workshp on the Pragmatics
of Quesiton Answering, HST-NAACL, Boston.
http://www.cse.msu.edu/~rongjin/publications/H
LTQAWorkshop04.pdf
Barry D. Davidson. 2006. An Advanced Interactive
Discovery Learning Environment for
Engineering Education: Final Report.
Submitted to R. E. Gillian, National Aeronautics
and Space Administration.
Marco De Boni and Suresh Manandhar. 2005.
Implementing Clarification Dialogues in Open
Domain Question Answering. Natural Language
Engineering 11(4): 343-361.
Anne R. Diekema, Ozgur Yilmazel, Jiangping
Chen, Sarah Harwell, Lan He, and Elizabeth D.
Liddy. 2004. Finding Answers to Complex
Questions. In New Directions in Question
Answering. (Ed.) Mark T. Maybury. The MIT
Press, 141-152.
Sanda Harabagiu, Dan Moldovan, Marius Pa�ca,
Mihai Surdeanu, Rada Mihalcea, Roxana Gîrju,
Vasile Rus, Finley Lacatu�u, Paul Morarescu,
Razvan Bunescu. 2001. Answering Complex,
List and Context Questions with LCC’s
Question-Answering Server, TREC 2001.
Chiori Hori, Takaaki Hori., Hideki Isozaki, Eisaku
Maeda, Shigeru Katagiri, and Sadaoki Furui.
2003. Deriving Disambiguous Queries in a
Arne Jönsson, Frida Andén, Lars Degerstedt,
Annika Flycht-Eriksson, Magnus Merkel, and
Sara Norberg. 2004. Experiences from
Combining Dialogue System Development With
Information Extraction Techniques. In New
Directions in Question Answering. (Ed.) Mark T.
Maybury. The MIT Press, 153-164.
Elizabeth D. Liddy. 2003. Question Answering in
Contexts. Invited Keynote Speaker. ARDA
AQUAINT Annual Meeting. Washington, DC.
Dec 2-5, 2003.
Elizabeth D. Liddy, Anne R. Diekema, Jiangping
Chen, Sarah Harwell, Ozgur Yilmazel, and Lan
He. 2003. What do You Mean? Finding Answers
to Complex Questions. Proceedings of New
Directions in Question Answering. AAAI Spring
Symposium, March 24-26.
Elizabeth D. Liddy, Anne R. Diekema, and Ozgur
Yilmazel. 2004. Context-Based Question-
Answering Evaluation. In Proceedings of the 27th
Annual ACM-SIGIR Conference. Sheffield,
England
Catherine S. Ross, Kirsti Nilsen, and Patricia
Dewdney. 2002. Conducting the Reference
Interview. Neal-Schuman, New York, NY.
Sharon Small, Tomek Strzalkowski, Ting Liu,
Nobuyuki Shimizu, and Boris Yamrom. 2004. A
Data Driven Approach to Interactive QA. In New
Directions in Question Answering. (Ed.) Mark T.
Maybury. The MIT Press, 129-140.
Joseph E. Straw. 2004. Expecting the Stars but
Getting the Moon: Negotiating around Patron
Expectations in the Digital Reference
Environment. In The Virtual Reference
Experience: Integrating Theory into Practice.
Eds. R. David Lankes, Joseph Janes, Linda C.
Smith, and Christina M. Finneran. Neal-
Schuman, New York, NY.
</reference>
<page confidence="0.999179">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.131901">
<title confidence="0.9998155">Modeling Reference Interviews as a Basis for Improving Automatic QA Systems</title>
<author confidence="0.7512015">Nancy J McCracken</author>
<author confidence="0.7512015">Anne R Diekema</author>
<author confidence="0.7512015">Grant Ingersoll</author>
<author confidence="0.7512015">Sarah Harwell</author>
<author confidence="0.7512015">Eileen E Allen</author>
<author confidence="0.7512015">Ozgur Yilmazel</author>
<author confidence="0.7512015">D Elizabeth</author>
<affiliation confidence="0.808233">Center for Natural Language Syracuse</affiliation>
<address confidence="0.98113">Syracuse, NY</address>
<email confidence="0.995356">njmccrac@syr.edu</email>
<email confidence="0.995356">diekemar@syr.edu</email>
<email confidence="0.995356">gsingers@syr.edu</email>
<email confidence="0.995356">scharwel@syr.edu</email>
<email confidence="0.995356">eeallen@syr.edu</email>
<email confidence="0.995356">oyilmaz@syr.edu</email>
<email confidence="0.995356">liddy@syr.edu</email>
<abstract confidence="0.9715024">The automatic QA system described in this paper uses a reference interview model to allow the user to guide and contribute to the QA process. A set of system capabilities was designed and implemented that defines how the user’s contributions can help improve the system. These include tools, called the Query Template Builder and the Knowledge Base Builder, that tailor the document processing and QA system to a particular domain by allowing a Subject Matter Expert to contribute to the query representation and to the domain knowledge. During the QA process, the system can interact with the user to improve query terminology by using Spell Checking, Answer Type verification, Expansions and Acronym Clarifications. The system also has capabilities that depend upon, and expand the user’s history of interaction with the system, including a User Profile, Reference Resolution, and</abstract>
<intro confidence="0.751627">Question Similarity modules</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Evelyn Balfe</author>
<author>Barry Smyth</author>
</authors>
<title>An Analysis of Query Similarity in Collaborative Web Search.</title>
<date>2005</date>
<booktitle>In Proceedings of the 27th European Conference on Information Retrieval. Santiago de Compostela,</booktitle>
<contexts>
<context position="22160" citStr="Balfe and Smyth, 2005" startWordPosition="3420" endWordPosition="3423">resolution used in documents, with the addition of the need to understand first and second person pronouns from the dialogue context. The coreference resolution algorithm is based on standard linguistic discourse processing techniques where referring phrases and candidate resolvents are analyzed along a set of features that typically includes gender, animacy, number, person and the distance between the referring phrase and the candidate resolvent. Question Similarity: Question Similarity is the task of identifying when two or more questions are related. Previous studies (Boydell et al., 2005, Balfe and Smyth, 2005) on information retrieval have shown that using previously asked questions to enhance the current question is often useful for improving results among like-minded users. Identifying related questions is useful for finding matches to Frequently Asked Questions (FAQs) and Previously Asked Questions (PAQs) as well as detecting when a user is failing to find adequate answers and may be getting frustrated. Furthermore, similar questions can be used during the reference interview process to present questions that other users with similar information needs have used and any answers that they consider</context>
</contexts>
<marker>Balfe, Smyth, 2005</marker>
<rawString>Evelyn Balfe and Barry Smyth. 2005. An Analysis of Query Similarity in Collaborative Web Search. In Proceedings of the 27th European Conference on Information Retrieval. Santiago de Compostela, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Ellen Bates</author>
</authors>
<title>The Art of the Reference Interview. Online World.</title>
<date>1997</date>
<contexts>
<context position="4674" citStr="Bates, 1997" startWordPosition="711" endWordPosition="712">re general picture of how we model and utilize both the domains of inquiry and typical interaction processes observed in these diverse user groups. 2 Background and related research Our work in this paper is based on two premises: 1) user questions and responsive answers need to be understood within a larger model of the user’s information needs and requirements, and, 2) a good interactive QA system facilitates a dialogue with its users to ensure it understands and satisfies these information needs. The first premise is based on the long-tested and successful model of the reference interview (Bates, 1997, Straw, 2004), which was again validated by the findings of an ARDA-sponsored workshop to increase the research community’s understanding of the information seeking needs and cognitive processes of intelligence analysts (Liddy, 2003). The second premise instantiates this model within the digital and distributed information environment. Interactive QA assumes an interaction between the human and the computer, typically through a combination of a clarification dialogue and user modeling to capture previous interactions of users with the system. De Boni et al. (2005) view the clarification dialo</context>
</contexts>
<marker>Bates, 1997</marker>
<rawString>Mary Ellen Bates. 1997. The Art of the Reference Interview. Online World. September 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oisín Boydell</author>
<author>Barry Smyth</author>
<author>Cathal Gurrin</author>
<author>Alan F Smeaton</author>
</authors>
<title>A Study of Selection Noise in Collaborative Web Search.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence.</booktitle>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="22136" citStr="Boydell et al., 2005" startWordPosition="3416" endWordPosition="3419">narrative coreference resolution used in documents, with the addition of the need to understand first and second person pronouns from the dialogue context. The coreference resolution algorithm is based on standard linguistic discourse processing techniques where referring phrases and candidate resolvents are analyzed along a set of features that typically includes gender, animacy, number, person and the distance between the referring phrase and the candidate resolvent. Question Similarity: Question Similarity is the task of identifying when two or more questions are related. Previous studies (Boydell et al., 2005, Balfe and Smyth, 2005) on information retrieval have shown that using previously asked questions to enhance the current question is often useful for improving results among like-minded users. Identifying related questions is useful for finding matches to Frequently Asked Questions (FAQs) and Previously Asked Questions (PAQs) as well as detecting when a user is failing to find adequate answers and may be getting frustrated. Furthermore, similar questions can be used during the reference interview process to present questions that other users with similar information needs have used and any an</context>
</contexts>
<marker>Boydell, Smyth, Gurrin, Smeaton, 2005</marker>
<rawString>Oisín Boydell, Barry Smyth, Cathal Gurrin, and Alan F. Smeaton. 2005. A Study of Selection Noise in Collaborative Web Search. In Proceedings of the 19th International Joint Conference on Artificial Intelligence. Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joyce Y Chai</author>
<author>Rong Jin</author>
</authors>
<title>Discourse Structure for Context Question Answering.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshp on the Pragmatics of Quesiton Answering, HST-NAACL,</booktitle>
<location>Boston.</location>
<note>http://www.cse.msu.edu/~rongjin/publications/H LTQAWorkshop04.pdf</note>
<contexts>
<context position="19811" citStr="Chai and Jin, 2004" startWordPosition="3059" endWordPosition="3062">, reliability, sources), that could be used as filters in the answer finding process. Reference Resolution: A basic feature of an interactive QA system is the requirement to understand the user’s questions and responsive answers as one session. The sequence of questions and answers forms a natural language dialogue between the user and the system. This necessitates NLP processing at the discourse level, a primary task of which is to resolve references across the session. Building on previous work in this area done for the Context Track of TREC 2001 (Harabagiu et al, 2001) and additional work (Chai and Jin, 2004) suggesting discourse structures are needed to understand the question/answer sequence, we have developed session-based reference resolution capability. In a dialogue, the user naturally includes referring phrases that require several types of resolution. The simplest case is that of referring pronouns, where the user is asking a follow-up question, for example: Q1: When did Madonna enter the music business? A1: Madonna&apos;s first album, Madonna, came out in 1983 and since then she&apos;s had a string of hits, been a major influence in the music industry and become an international icon. Q2: When did </context>
</contexts>
<marker>Chai, Jin, 2004</marker>
<rawString>Joyce Y. Chai, and Rong Jin. 2004. Discourse Structure for Context Question Answering. In Proceedings of the Workshp on the Pragmatics of Quesiton Answering, HST-NAACL, Boston. http://www.cse.msu.edu/~rongjin/publications/H LTQAWorkshop04.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry D Davidson</author>
</authors>
<title>An Advanced Interactive Discovery Learning Environment for Engineering Education: Final Report.</title>
<date>2006</date>
<note>Submitted to</note>
<contexts>
<context position="18951" citStr="Davidson, 2006" startWordPosition="2922" endWordPosition="2923">xample, if a user asks “How much protein should my husband be getting every day?”, the fact that the user is married can be added to their profile for future marketing, or for a new line of dialogue to ask his name or age. This information is then made available as context information for the QA system to resolve references that the user makes to themselves and their own attributes. For the NASA question-answering capability, to assist students in organizing their questions and results, there is an area for users to save their searches as standing queries, along with the results of searching (Davidson, 2006). This information, representing topics and areas of interest, can help to focus answer finding for new questions the user asks. Not yet implemented, but of interest, is the ability to save information such as a user’s 21 preferences (format, reliability, sources), that could be used as filters in the answer finding process. Reference Resolution: A basic feature of an interactive QA system is the requirement to understand the user’s questions and responsive answers as one session. The sequence of questions and answers forms a natural language dialogue between the user and the system. This nece</context>
</contexts>
<marker>Davidson, 2006</marker>
<rawString>Barry D. Davidson. 2006. An Advanced Interactive Discovery Learning Environment for Engineering Education: Final Report. Submitted to R. E. Gillian, National Aeronautics and Space Administration.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco De Boni</author>
<author>Suresh Manandhar</author>
</authors>
<title>Implementing Clarification Dialogues in Open Domain Question Answering.</title>
<date>2005</date>
<journal>Natural Language Engineering</journal>
<volume>11</volume>
<issue>4</issue>
<pages>343--361</pages>
<marker>De Boni, Manandhar, 2005</marker>
<rawString>Marco De Boni and Suresh Manandhar. 2005. Implementing Clarification Dialogues in Open Domain Question Answering. Natural Language Engineering 11(4): 343-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne R Diekema</author>
<author>Ozgur Yilmazel</author>
<author>Jiangping Chen</author>
<author>Sarah Harwell</author>
<author>Lan He</author>
<author>Elizabeth D Liddy</author>
</authors>
<title>Finding Answers to Complex Questions.</title>
<date>2004</date>
<booktitle>In New Directions in Question</booktitle>
<pages>141--152</pages>
<publisher>The MIT Press,</publisher>
<contexts>
<context position="6096" citStr="Diekema et al., 2004" startWordPosition="930" endWordPosition="933">er question in order to clarify the meaning, or, the user may ask an additional question which expands on the previous answer. In their research De Boni et al. (2005) try to determine automatically whether or not there exists a relationship between a current question and preceding questions, and if there is a relationship, they use this additional information in order to determine the correct answer. We prefer to view the clarification dialogue as more two-sided, where the system and the user actually enter a dialogue, similar to the reference interview as carried out by reference librarians (Diekema et al., 2004). The traditional reference interview is a cyclical process in which the questioner poses their question, the librarian (or the system) questions the questioner, then locates the answer based on information provided by the questioner, and returns an answer to the user who then determines whether this has satisfied their information need or whether further clarification or further questions are needed. The HITIQA system’s (Small et al., 2004) view of a clarification system is closely related to ours—their dialogue aligns the understanding of the question between system and user. Their research </context>
<context position="7880" citStr="Diekema et al., 2004" startWordPosition="1205" endWordPosition="1208"> on the ambiguous question to solicit additional information from the user. This process is completely automated and based on templates that generate the questions. Still, removing the cognitive burden from the user through automation is not easy to implement and can be the cause of error or misunderstanding. Increasing user involvement may help to reduce this error. As described above, it can be seen that interactive QA systems have various levels of dialogue automation ranging from fully automatic (De Boni et al., 2004, Hori et al., 2004) to a strong 18 user involvement (Small et al., 2004, Diekema et al., 2004). Some research suggests that clarification dialogues in open-domain systems are more unpredictable than those in restricted domain systems, the latter lending itself better to automation (Hori et al., 2003, Jönsson et al., 2004). Incorporating the user’s inherent knowledge of the intention of their query is quite feasible in restricted domain systems and should improve the quality of answers returned, and make the experience of the user a less frustrating one. While many of the systems described above are promising in terms of IQA, we believe that incorporating knowledge of the user in the qu</context>
</contexts>
<marker>Diekema, Yilmazel, Chen, Harwell, He, Liddy, 2004</marker>
<rawString>Anne R. Diekema, Ozgur Yilmazel, Jiangping Chen, Sarah Harwell, Lan He, and Elizabeth D. Liddy. 2004. Finding Answers to Complex Questions. In New Directions in Question Answering. (Ed.) Mark T. Maybury. The MIT Press, 141-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Dan Moldovan</author>
<author>Marius Pa�ca</author>
</authors>
<title>Mihai Surdeanu, Rada Mihalcea, Roxana Gîrju, Vasile Rus, Finley Lacatu�u, Paul Morarescu, Razvan Bunescu.</title>
<date>2001</date>
<location>TREC</location>
<marker>Harabagiu, Moldovan, Pa�ca, 2001</marker>
<rawString>Sanda Harabagiu, Dan Moldovan, Marius Pa�ca, Mihai Surdeanu, Rada Mihalcea, Roxana Gîrju, Vasile Rus, Finley Lacatu�u, Paul Morarescu, Razvan Bunescu. 2001. Answering Complex, List and Context Questions with LCC’s Question-Answering Server, TREC 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chiori Hori</author>
</authors>
<title>Takaaki Hori., Hideki Isozaki, Eisaku Maeda, Shigeru Katagiri, and Sadaoki Furui.</title>
<date>2003</date>
<note>in a</note>
<marker>Hori, 2003</marker>
<rawString>Chiori Hori, Takaaki Hori., Hideki Isozaki, Eisaku Maeda, Shigeru Katagiri, and Sadaoki Furui. 2003. Deriving Disambiguous Queries in a</rawString>
</citation>
<citation valid="false">
<authors>
<author>Arne Jönsson</author>
</authors>
<title>Frida Andén, Lars Degerstedt, Annika Flycht-Eriksson,</title>
<location>Magnus Merkel, and</location>
<marker>Jönsson, </marker>
<rawString>Arne Jönsson, Frida Andén, Lars Degerstedt, Annika Flycht-Eriksson, Magnus Merkel, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Norberg</author>
</authors>
<title>Experiences from Combining Dialogue System Development With Information Extraction Techniques.</title>
<date>2004</date>
<booktitle>In New Directions in Question Answering. (Ed.) Mark</booktitle>
<pages>153--164</pages>
<publisher>The MIT Press,</publisher>
<marker>Norberg, 2004</marker>
<rawString>Sara Norberg. 2004. Experiences from Combining Dialogue System Development With Information Extraction Techniques. In New Directions in Question Answering. (Ed.) Mark T. Maybury. The MIT Press, 153-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth D Liddy</author>
</authors>
<title>Question Answering in Contexts. Invited Keynote Speaker.</title>
<date>2003</date>
<booktitle>ARDA AQUAINT Annual Meeting.</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="4908" citStr="Liddy, 2003" startWordPosition="743" endWordPosition="744"> user questions and responsive answers need to be understood within a larger model of the user’s information needs and requirements, and, 2) a good interactive QA system facilitates a dialogue with its users to ensure it understands and satisfies these information needs. The first premise is based on the long-tested and successful model of the reference interview (Bates, 1997, Straw, 2004), which was again validated by the findings of an ARDA-sponsored workshop to increase the research community’s understanding of the information seeking needs and cognitive processes of intelligence analysts (Liddy, 2003). The second premise instantiates this model within the digital and distributed information environment. Interactive QA assumes an interaction between the human and the computer, typically through a combination of a clarification dialogue and user modeling to capture previous interactions of users with the system. De Boni et al. (2005) view the clarification dialogue mainly as the presence or absence of a relationship between the question from the user and the answer provided by the system. For example, a user may ask a question, receive an answer and ask another question in order to clarify t</context>
</contexts>
<marker>Liddy, 2003</marker>
<rawString>Elizabeth D. Liddy. 2003. Question Answering in Contexts. Invited Keynote Speaker. ARDA AQUAINT Annual Meeting. Washington, DC. Dec 2-5, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth D Liddy</author>
<author>Anne R Diekema</author>
<author>Jiangping Chen</author>
<author>Sarah Harwell</author>
<author>Ozgur Yilmazel</author>
<author>Lan He</author>
</authors>
<title>What do You Mean? Finding Answers to Complex Questions.</title>
<date>2003</date>
<booktitle>Proceedings of New Directions in Question Answering.</booktitle>
<publisher>AAAI Spring Symposium,</publisher>
<contexts>
<context position="23181" citStr="Liddy et al., 2003" startWordPosition="3575" endWordPosition="3578">d. Furthermore, similar questions can be used during the reference interview process to present questions that other users with similar information needs have used and any answers that they considered useful. CNLP’s question similarity capability comprises a suite of algorithms designed to identify when two or more questions are related. The system works by analyzing each query using our Language-to-Logic (L2L) module to identify and weight keywords in the query, provide expansions and clarifications, as well as determine the focus of the question and the type of answer the user is expecting (Liddy et al., 2003). We then compute a series of similarity measures on two or more L2L queries. Our measures adopt a variety of approaches, including those that are based on keywords in the query: cosine similarity, keyword string matching, expansion analysis, and spelling variations. In addition, two measures are based on the representation of the whole query:answer type 22 and answer frame analysis. An answer frame is our representation of the meaningful extractions contained in the query, along with metadata about where they occur and any other extractions that relate to in the query. Our system will then co</context>
</contexts>
<marker>Liddy, Diekema, Chen, Harwell, Yilmazel, He, 2003</marker>
<rawString>Elizabeth D. Liddy, Anne R. Diekema, Jiangping Chen, Sarah Harwell, Ozgur Yilmazel, and Lan He. 2003. What do You Mean? Finding Answers to Complex Questions. Proceedings of New Directions in Question Answering. AAAI Spring Symposium, March 24-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth D Liddy</author>
<author>Anne R Diekema</author>
<author>Ozgur Yilmazel</author>
</authors>
<title>Context-Based QuestionAnswering Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th Annual ACM-SIGIR Conference.</booktitle>
<location>Sheffield, England</location>
<marker>Liddy, Diekema, Yilmazel, 2004</marker>
<rawString>Elizabeth D. Liddy, Anne R. Diekema, and Ozgur Yilmazel. 2004. Context-Based QuestionAnswering Evaluation. In Proceedings of the 27th Annual ACM-SIGIR Conference. Sheffield, England</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine S Ross</author>
<author>Kirsti Nilsen</author>
<author>Patricia Dewdney</author>
</authors>
<title>Conducting the Reference Interview. Neal-Schuman,</title>
<date>2002</date>
<location>New York, NY.</location>
<contexts>
<context position="1978" citStr="Ross et al., 2002" startWordPosition="294" endWordPosition="297">r years using the Reference Interview to clarify an unfocused question, narrow a broad question, and suggest further information that the user might not have thought to ask for. The reference interview tries to elicit sufficient information about the user’s real need to enable a librarian to understand the question enough to begin searching. The question is clarified, made more specific, and contextualized with relevant detail. Real questions from real users are often “ill-formed’ with respect to the information system; that is, they do not match the structure of ‘expectations’ of the system (Ross et al., 2002). A reference interview translates the user’s question into a representation that the librarian and the library systems can interpret correctly. The human reference interview process provides an ideal, well-tested model of how questioner and answerer work together co-operatively and, we believe, can be successfully applied to the digital environment. The findings of researchers applying this model in online situations (Bates, 1989, Straw, 2004) have enabled us to understand how a system might work with the user to provide accurate and relevant answers to complex questions. Our long term goal i</context>
</contexts>
<marker>Ross, Nilsen, Dewdney, 2002</marker>
<rawString>Catherine S. Ross, Kirsti Nilsen, and Patricia Dewdney. 2002. Conducting the Reference Interview. Neal-Schuman, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Small</author>
<author>Tomek Strzalkowski</author>
<author>Ting Liu</author>
<author>Nobuyuki Shimizu</author>
<author>Boris Yamrom</author>
</authors>
<title>A Data Driven Approach to Interactive QA.</title>
<date>2004</date>
<journal>In New Directions in Question</journal>
<pages>129--140</pages>
<publisher>The MIT Press,</publisher>
<contexts>
<context position="6541" citStr="Small et al., 2004" startWordPosition="997" endWordPosition="1000">e as more two-sided, where the system and the user actually enter a dialogue, similar to the reference interview as carried out by reference librarians (Diekema et al., 2004). The traditional reference interview is a cyclical process in which the questioner poses their question, the librarian (or the system) questions the questioner, then locates the answer based on information provided by the questioner, and returns an answer to the user who then determines whether this has satisfied their information need or whether further clarification or further questions are needed. The HITIQA system’s (Small et al., 2004) view of a clarification system is closely related to ours—their dialogue aligns the understanding of the question between system and user. Their research describes three types of dialogue strategies: 1) narrowing the dialogue, 2) broadening the dialogue, and 3) a fact seeking dialogue. Similar research was carried out by Hori et al. (2003), although their system automatically determines whether there is a need for a dialogue, not the user. The system identifies ambiguous questions (i.e. questions to which the system could not find an answer). By gathering additional information, the researche</context>
<context position="7857" citStr="Small et al., 2004" startWordPosition="1201" endWordPosition="1204">ally generated based on the ambiguous question to solicit additional information from the user. This process is completely automated and based on templates that generate the questions. Still, removing the cognitive burden from the user through automation is not easy to implement and can be the cause of error or misunderstanding. Increasing user involvement may help to reduce this error. As described above, it can be seen that interactive QA systems have various levels of dialogue automation ranging from fully automatic (De Boni et al., 2004, Hori et al., 2004) to a strong 18 user involvement (Small et al., 2004, Diekema et al., 2004). Some research suggests that clarification dialogues in open-domain systems are more unpredictable than those in restricted domain systems, the latter lending itself better to automation (Hori et al., 2003, Jönsson et al., 2004). Incorporating the user’s inherent knowledge of the intention of their query is quite feasible in restricted domain systems and should improve the quality of answers returned, and make the experience of the user a less frustrating one. While many of the systems described above are promising in terms of IQA, we believe that incorporating knowledg</context>
</contexts>
<marker>Small, Strzalkowski, Liu, Shimizu, Yamrom, 2004</marker>
<rawString>Sharon Small, Tomek Strzalkowski, Ting Liu, Nobuyuki Shimizu, and Boris Yamrom. 2004. A Data Driven Approach to Interactive QA. In New Directions in Question Answering. (Ed.) Mark T. Maybury. The MIT Press, 129-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph E Straw</author>
</authors>
<title>Expecting the Stars but Getting the Moon: Negotiating around Patron Expectations in the Digital Reference Environment.</title>
<date>2004</date>
<booktitle>In The Virtual Reference Experience: Integrating Theory into</booktitle>
<location>Linda C.</location>
<contexts>
<context position="2426" citStr="Straw, 2004" startWordPosition="359" endWordPosition="360">al users are often “ill-formed’ with respect to the information system; that is, they do not match the structure of ‘expectations’ of the system (Ross et al., 2002). A reference interview translates the user’s question into a representation that the librarian and the library systems can interpret correctly. The human reference interview process provides an ideal, well-tested model of how questioner and answerer work together co-operatively and, we believe, can be successfully applied to the digital environment. The findings of researchers applying this model in online situations (Bates, 1989, Straw, 2004) have enabled us to understand how a system might work with the user to provide accurate and relevant answers to complex questions. Our long term goal in developing QuestionAnswering (QA) systems for various user groups is to permit, and encourage users to positively contribute to the QA process, to more nearly mirror what occurs in the reference interview, and to develop an automatic QA system that provides fuller, more appropriate, individually tailored responses than has been available to date. Building on our Natural Language Processing (NLP) experience in a range of information access app</context>
<context position="4688" citStr="Straw, 2004" startWordPosition="713" endWordPosition="714">cture of how we model and utilize both the domains of inquiry and typical interaction processes observed in these diverse user groups. 2 Background and related research Our work in this paper is based on two premises: 1) user questions and responsive answers need to be understood within a larger model of the user’s information needs and requirements, and, 2) a good interactive QA system facilitates a dialogue with its users to ensure it understands and satisfies these information needs. The first premise is based on the long-tested and successful model of the reference interview (Bates, 1997, Straw, 2004), which was again validated by the findings of an ARDA-sponsored workshop to increase the research community’s understanding of the information seeking needs and cognitive processes of intelligence analysts (Liddy, 2003). The second premise instantiates this model within the digital and distributed information environment. Interactive QA assumes an interaction between the human and the computer, typically through a combination of a clarification dialogue and user modeling to capture previous interactions of users with the system. De Boni et al. (2005) view the clarification dialogue mainly as </context>
</contexts>
<marker>Straw, 2004</marker>
<rawString>Joseph E. Straw. 2004. Expecting the Stars but Getting the Moon: Negotiating around Patron Expectations in the Digital Reference Environment. In The Virtual Reference Experience: Integrating Theory into Practice. Eds. R. David Lankes, Joseph Janes, Linda C.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Smith</author>
<author>Christina M Finneran</author>
</authors>
<publisher>NealSchuman,</publisher>
<location>New York, NY.</location>
<marker>Smith, Finneran, </marker>
<rawString>Smith, and Christina M. Finneran. NealSchuman, New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>