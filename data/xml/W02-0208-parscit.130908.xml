<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.033609">
<note confidence="0.9942815">
Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
Philadelphia, July 2002, pp. 50-59. Association for Computational Linguistics.
</note>
<title confidence="0.938315">
DialogueView: An Annotation Tool for Dialogue
</title>
<author confidence="0.944454">
Peter A. Heeman and Fan Yang and Susan E. Strayer
</author>
<affiliation confidence="0.854531333333333">
Computer Science and Engineering
OGI School of Science and Engineering
Oregon Health &amp; Science University
</affiliation>
<address confidence="0.919359">
20000 NW Walker Rd., Beaverton OR, 97006
</address>
<email confidence="0.998527">
heeman@cse.ogi.edu yangf@cse.ogi.edu susan strayer@yahoo.com
</email>
<sectionHeader confidence="0.980072" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999379375">
This paper describes DialogueView, a
tool for annotating dialogues with utter-
ance boundaries, speech repairs, speech
act tags, and discourse segments. The
tool provides several views of the data,
including a word view that is time-
aligned with the audio signal, and an ut-
terance view that shows the dialogue as
if it were a script for a play. The ut-
terance view abstracts away from lower
level details that are coded in the word
view. This allows the annotator to have
a simpler view of the dialogue when cod-
ing speech act tags and discourse struc-
ture, but still have access to the details
when needed.
</bodyText>
<sectionHeader confidence="0.996304" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999124">
There is a growing interest in annotating human-
human dialogue. Annotated dialogues are useful
for formulating and verifying theories of dialogue
and for building statistical models. In addition to
orthographic word transcription, one might want
the following dialogue annotations.
e Annotation of the speech repairs. Speech re-
pairs are a type of disfluency where speakers
go back and change or repeat something they
just said.
</bodyText>
<listItem confidence="0.971745">
• Segmentation of the speech of each speaker
into utterance units, with a single ordering of
the utterances. We refer to this as linearizing
the dialogue (Heeman and Allen, 1995a).
</listItem>
<bodyText confidence="0.995332355932203">
e Each utterance tagged with its speech act
function.
e The utterances grouped into hierarchical dis-
course segments.
There are tools that address subsets of the above
tasks. However, we feel that doing dialogue an-
notation is very difficult. Part of this difficulty
is due to the interactions between the annotation
tasks. An error at a lower level can have a large
impact on the higher level annotations. For in-
stance, there can be ambiguity as to whether an
&amp;quot;okay&amp;quot; is part of a speech repair; this will im-
pact the segmentation of the speech into utter-
ance units and the speech act coding. Sometimes,
it is only by considering the higher level annota-
tions that one can make sense of what is going on
at the lower levels, especially when there is over-
lapping speech. Hence, a tool is needed that lets
users examine and code the dialogue at all lev-
els. The second reason why dialogue annotation
is difficult is because it is difficult to follow what
is occurring in the dialogue, especially for coding
discourse structure. A dialogue annotation tool
needs to help the user deal with this overload.
In this paper, we describe our dialogue anno-
tation tool, DialogueView. This tool displays the
dialogue at three different levels of abstraction.
The word level shows the words time-aligned with
the audio signal. The utterance level shows the
dialogue as a sequence of utterance, as if it were a
script for a play. It abstracts away from the exact
timing of the words and even skips words that do
not impact the progression of the dialogue. The
block level shows the dialogue as a hierarchy of
discourse segment purposes, and abstracts away
from the exact utterances that were said. Anno-
tations are done at the view that is most appropri-
ate for what is being annotated. The tool allows
the user to easily navigate between the three views
and automatically updates the higher level views
when changes are made in the lower level views.
Because the higher levels abstract away lower level
details, it is easier for the user to understand what
is happening in the dialogue.&apos; Yet, the user can
easily refer to the lower level views to see what
This approach bears resemblance to the work of
Jonsson and Dahlbdck (2000) in which they distill
human-human dialogues by removing those parts that
would not occur in human-computer dialogue. They
do this to create training data for their spoken dia-
logue systems.
actually occurred when necessary.
In the rest of this paper, we first discuss other
annotation tools. We then describe the three lev-
els of abstraction in our annotation tool. We then
discuss audio playback. Next, we discuss how the
tool can be customized for different annotation
schemes. Finally, we discuss the implementation
of the tool.
</bodyText>
<sectionHeader confidence="0.952947" genericHeader="introduction">
2 Existing Tools
</sectionHeader>
<bodyText confidence="0.999938844827586">
There are a number of tools that let users annotate
speech audio files. These include Emu (Cassidy
and Harrington, 2001), SpeechView (Sutton et al.,
1998) and Waves+ (Ent, 1993). These tools often
allow multiple text annotation files to be associ-
ated with the waveform and allow users an easy fa-
cility to capture time alignments. For instance, for
the ToBI annotation scheme (Pitrelli et al., 1994),
one can have the word alignment in one text file,
intonation features in a second, break indices in a
third, and miscellaneous features in a fourth. The
audio annotation tools often have powerful signal
analysis packages for displaying such phenomena
as spectrograms and voicing. These tools, how-
ever, do not have any built-in facility to group
words into utterances nor group utterances into
hierarchical discourse segments.
Several tools allow users to annotate higher
level structure in dialogue. The annotation tool
DAT from the University of Rochester (Fergu-
son, 1998) allows users to annotate utterances or
groups of utterances with a set of hard-coded an-
notation tags for the DAMSL annotation scheme
(Allen and Core, 1997; Core and Allen, 1997).
The tool Nb from MIT (Flammia, 1995; Flammia,
1998) allows users to impose a hierarchical group-
ing on a sequence of utterances, and hence anno-
tate discourse structure. Both DAT and Nb take
as input a linearization of the speaker utterances.
Mistakes in this input cannot be fixed. Whether
there are errors or not, users cannot see the exact
timing interactions between the speakers&apos; words or
the length of pauses. This simplification can make
it difficult for the annotator to truly understand
what is happening in the dialogue, especially for
overlapping speech, where speakers fight over the
turn or make back-channels.
The tools Transcriber (Barras et al., 2001) and
Mate (McKelvie et al., 2001) allow multiple views
of the data: a word view with words time-aligned
to the audio signal and an utterance view. How-
ever, Transcriber is geared to single channel data
and has a weak ability to handle overlapping
speech and Mate only allows one view to be shown
at a time. The author of a competing tool has
remarked that &amp;quot;speed and stability of [Mate] are
both still problematic for real annotation. Also,
the highly generic approach increases the initial
effort to set up the tool since you basically have
to write your own tool using the Mate style-sheet
language&amp;quot; (Kipp, 2001). Hence, he developed a
new tool Anvil that he claims is a better trade-off
among generality, functionality, and complexity.
This tool offers multi-modal annotation support,
but like Transcriber, does not allow detailed an-
notation of dialogue phenomena, such as overlap-
ping speech and abandoned speech, and has no
abstraction mechanism.
</bodyText>
<sectionHeader confidence="0.94301" genericHeader="method">
3 Word V iew
</sectionHeader>
<bodyText confidence="0.99998125">
Our dialogue annotation tool, DialogueView,
gives the user three views of the data. The low-
est level view is called WordView and takes as
input the words said by each speaker and their
start and stop times and shows them time-aligned
with the audio signal. Figure 1 shows an ex-
cerpt from the Trains corpus (Heeman and Allen,
1995b) in WordView. This view is ideal for view-
ing the exact timing of speech, especially overlap-
ping speech. As will be discussed below, we use
it for segmenting the speech into utterances, and
annotating communicative status and speech re-
pairs.2 These annotations will allow us to build a
simpler representation of what is happening in the
speech for the utterance view, which is discussed
in the next section.
</bodyText>
<subsectionHeader confidence="0.998975">
3.1 Utterance Segmentation
</subsectionHeader>
<bodyText confidence="0.984269863636364">
As can be seen in Figure 1, WordView shows the
words segmented into utterances, which for our
purpose is simply a grouping of consecutive words
by a single speaker, in which there is minimal
effect from the other speaker. Consider the ex-
change in Figure 1, where the upper speaker says
&amp;quot;and then take the remaining boxcar down to&amp;quot; fol-
lowed by the lower speaker saying &amp;quot;right, to du-&amp;quot;,
followed by the upper speaker saying &amp;quot;Corning&amp;quot;.
Although one could consider the upper speaker&apos;s
speech as one utterance, we preclude that due to
the interaction from the lower speaker. A full def-
inition of `utterance&apos; is beyond the scope of this
paper, and is left to the users to specify in their
annotation scheme.
Utterance boundaries in WordView are only
shown by their start times. The end of the utter-
ance is either the start of the next utterance by the
There currently is no support for changing the
word transcription. There are already a number of
tools that do an excellent job at this task. Hence,
adding this capability is a low priority for us.
</bodyText>
<figureCaption confidence="0.998509">
Figure 1: Utterance Segmentation in WordView
</figureCaption>
<bodyText confidence="0.996535583333333">
same speaker or the end of the file. With Word-
View, the user can easily move, insert or delete
utterance boundaries. The tool ensures that the
boundaries never fall in the middle of a word by
the speaker.
The start times of the utterances are used to
derive a single ordering of the utterances of the
two speakers. This linearization of the dialogue
captures how the speakers are sequentially adding
to the dialogue. The linearization is used by the
next view to give an abstraction away from the
exact timing of the speech.
</bodyText>
<subsectionHeader confidence="0.999619">
3.2 Communicative Status
</subsectionHeader>
<bodyText confidence="0.999970512195122">
WordView allows features of the utterances to be
annotated. In practice, however, we only anno-
tate features related to its communicative status,
based on the DAMSL annotation scheme (Allen
and Core, 1997). Below, we give the utterance
tags that we assign in WordView.
Abandoned: The speaker abandoned what
they were saying and it had no impact on the rest
of the dialogue. This mainly happens where one
speaker loses the fight over who speaks next. Fig-
ure 2 gives an example where the upper speaker
tries to take the turn by saying &amp;quot;takes,&amp;quot; and than
abandons this effort.
Incomplete: The speaker did not make a com-
plete utterance, either prosodically, syntactically,
or semantically. Unlike the previous category, the
partial utterance did impact the dialogue behav-
ior. Figure 1 gives two examples. The first is
where the upper speaker says &amp;quot;and than take the
remaining boxcar down to&amp;quot;, and the second is
where the lower speaker said &amp;quot;to du-,&amp;quot; which was
than completed by the upper speaker.
Overlap: The speech in the utterance overlaps
with the speech from the prior utterance. For
instance, in Figure 1, the lower speaker utters
&amp;quot;okay&amp;quot; during the middle of an utterance, perhaps
to tell the upper speaker that they are understand-
ing everything so far. However, a simple lineariza-
tion would make it seem that the &amp;quot;okay&amp;quot; is an
acknowledgment of the entire utterance, which is
not the case. Hence, we tag the &amp;quot;okay&amp;quot; utterance
with the overlap tag. The next view, Utterance-
View, will take the overlap tag into account in
displaying the utterances, as will be discussed in
Section 4.3.
Not all overlapping speech needs to be anno-
tated with the overlap tag. In Figure 1, the sec-
ond instance of &amp;quot;Corning&amp;quot; overlaps slightly with
the first instance of &amp;quot;Corning&amp;quot;. However, viewing
it sequentially does not alter the analysis of the
exchange.
</bodyText>
<subsectionHeader confidence="0.999331">
3.3 Reordering Overlapping Speech
</subsectionHeader>
<bodyText confidence="0.9997818">
Consider the example in Figure 2. Before the ex-
cerpt shown, the lower speaker had just finished
an utterance and then paused for over a second.
The upper speaker then acknowledged the utter-
ance with &amp;quot;okay&amp;quot;, but this happened a fraction of
a second after the bottom speaker started speak-
ing again. A simple linearization of the dialogue
would have the &amp;quot;okay&amp;quot; following the wrong stretch
of speech &amp;quot;and than th(at)- takes w- what three
hours.&amp;quot; A solution to this would be to anno-
</bodyText>
<figureCaption confidence="0.995448">
Figure 2: Utterance Ordering and Speech Repairs in WordView
</figureCaption>
<bodyText confidence="0.999960315789474">
tate the &amp;quot;okay&amp;quot; with the overlap tag. However,
this &amp;quot;Okay&amp;quot; is more similar to the overlapping in-
stances of &amp;quot;Corning&amp;quot; in Figure 1. The fact that
&amp;quot;Okay&amp;quot; overlaps with the start of the next utter-
ance is not critical to understanding what is oc-
curring in the dialogue, as long as we linearize
the &amp;quot;Okay&amp;quot; to occur before the other utterance.
WordView allows the user to change the lineariza-
tion by moving the start times of utterances. This
can be done provided that the speaker was silent
in the time interval preceding where the other per-
son started talking.
In summary, overlapping speech can be handled
in three ways. The utterance can be explicitly
tagged as overlapping; the overlap can be ignored
if it is not critical in understanding what is going
on in the dialogue; or the start time of the utter-
ance can be changed so that the overlap does not
need to be tagged.
</bodyText>
<subsectionHeader confidence="0.995896">
3.4 Speech Repairs
</subsectionHeader>
<bodyText confidence="0.983058818181818">
WordView also allows users to annotate speech
repairs. A speech repair is where a user goes back
and repeats or changes something that was just
said (Heeman and Allen, 1999). Below we give an
example of a speech repair and show its principle
components: reparandum, interruption point, and
editing term.
Example 1
why don&apos;t we take
���� reparandum ip
The reparandum is the speech that is being re-
placed, the interruption point is the end of the
reparandum, and the editing term consists of
words such as &amp;quot;um&amp;quot;, &amp;quot;uh&amp;quot;, &amp;quot;okay&amp;quot;, &amp;quot;let&apos;s see&amp;quot; that
help signal the repair.
To annotate a repair, the user highlights a se-
quence of words and then tags it as a reparandum
or an editing term of a repair. The user can also
specify the type of repair. Figure 2 shows how
speech repairs are displayed in WordView. The
words in the reparandum and editing term are un-
derlined and displayed in a special color.
</bodyText>
<subsectionHeader confidence="0.993607">
3.5 Speech Repairs and Utterances
</subsectionHeader>
<bodyText confidence="0.972213541666667">
Some phenomena can be marked as either a speech
repair, or could be marked using the utterance
tags of incomplete or abandon. This is especially
true for fresh starts (Heeman and Allen, 1999),
where a speaker abandons the current utterance
and starts over. To avoid having multiple ways of
annotating the same phenomena, we impose the
following restrictions in our annotation scheme.
. There cannot be an utterance boundary in-
side of a reparandum, inside of an editing
term, at the interruption point, nor at the
end of the editing term. Hence, something
annotated as a reparandum cannot also be
annotated as an abandoned utterance.
. Abandoned or incomplete utterances cannot
be followed by an utterance by the same
speaker.
. All word fragments must either be the last
word of a reparandum or the last word of an
utterance that is marked as abandoned or in-
complete.
um take two boxcars
����
et
</bodyText>
<figureCaption confidence="0.99322">
Figure 3: UtteranceView: Segmented Utterances in UtteranceView
</figureCaption>
<bodyText confidence="0.999768666666667">
. Abandoned or incomplete utterances can end
with an editing term, which would be marked
as the editing term of an abridged repair.
</bodyText>
<subsectionHeader confidence="0.892679">
3.6 Summary
</subsectionHeader>
<bodyText confidence="0.999985">
There are a number of reasons why we annotate
utterance boundaries, speech repairs, and commu-
nicative status in WordView. Annotating utter-
ance boundaries and overlapping speech requires
the user to take into account the exact timing of
the utterances, which is best done in this view.
Speech repairs also require fine tuned listening to
the speech and have strong interactions with ut-
terance boundaries. Furthermore, all three types
of annotations can be used to build a simpler view
of what is happening in the dialogue, as will be ex-
plained in the next section.
</bodyText>
<sectionHeader confidence="0.992596" genericHeader="method">
4 Utterance View
</sectionHeader>
<bodyText confidence="0.999832466666666">
The annotations from the word view are used to
build the next view, which we refer to as Utter-
anceView. The dialogue excerpts from Figures 1
and 2 are shown in the utterance view in Figures 3
and 4, respectively. The utterance view abstracts
away from the detailed timing information and in-
dividual words that were spoken. Instead, it fo-
cuses on the sequence of utterances between the
speakers. By removing details that were anno-
tated in the word view, we still preserve the im-
portant details that are needed to annotate speech
act types for the utterances and to annotate dis-
course segments. Of course, if the user wants to
see the exact timing of the words in the utter-
ances, they can examine the word view, as it is
</bodyText>
<figureCaption confidence="0.987728">
Figure 4: UtteranceView: Reordered and Abandoned Utterances in UtteranceView
</figureCaption>
<bodyText confidence="0.9999005">
displayed alongside the utterance view. There are
also navigation buttons on each view that allow
the user to easily reposition the portion of the di-
alogue in the other view. Furthermore, changes
made in the word view are immediately propa-
gated into the utterance view, and hence the user
will immediately see the impact of their annota-
tions.
</bodyText>
<subsectionHeader confidence="0.986517">
4.1 Utterance Ordering
</subsectionHeader>
<bodyText confidence="0.999974133333333">
Utterance ordering in the utterance view is deter-
mined by the start times of the utterances as spec-
ified in WordView. As was explained earlier, alter-
ing the start time of an utterance can be used to
simplify some cases of overlapping speech, where
the overlap is not critical to understanding the
role of the utterance in the dialogue. Figure 2
gave the word view of such an example. Rather
than code it as an overlap, we moved the start
time of the &amp;quot;okay&amp;quot; utterance so that it precedes
the overlapping speech by the other speaker. Fig-
ure 4 shows how this looks in the utterance view.
Here, the annotator would view the &amp;quot;okay&amp;quot; as an
acknowledgment that occurred between the two
utterances of the lower speaker.
</bodyText>
<subsectionHeader confidence="0.976898">
4.2 Speech Repairs
</subsectionHeader>
<bodyText confidence="0.999884703703704">
In the word view, the user annotates the reparan-
dum and editing term of speech repairs. If the
reparandum and editing term are removed, the
resulting utterance reflects what the speaker in-
tended to say. Speech repairs do carry informa-
tion.
. Their occurrence can signal a lack of certainty
of the speaker.
. The reparandum of a repair can have an
anaphoric reference, as in &amp;quot;Peter was, well
he was fired.&amp;quot;
However, removing the reparandum and editing
term of speech repairs from utterances in the ut-
terance view leads to a simpler representation of
what is occurring in the dialogue. Hence, in the
utterance view, we clean up the speech repairs, as
shown in Figures 2 and 4. Figure 2, which shows
the word view, contains the utterance &amp;quot;and then
th(at) that takes w- what three hours&amp;quot;; whereas
Figure 4, which shows the utterance view, con-
tains &amp;quot;and then that takes what three hours.&amp;quot; Of
course, a user can always refer to the word view
when annotating in the utterance view if they
want to see the exact speech that was said. In
most cases, we feel that this will not be neces-
sary for annotating speech act tags and discourse
segments.
</bodyText>
<subsectionHeader confidence="0.987938">
4.3 Communicative Status
</subsectionHeader>
<bodyText confidence="0.9999583">
The communicative status coded in the word view
is used in formatting the utterance view. Utter-
ances tagged as overlapping are indented and dis-
played with `+&apos; on either side, as shown in Fig-
ure 3. Utterances tagged as abandoned are not
shown, as can be seen in Figure 4, in which the
abandoned utterance &amp;quot;takes&amp;quot; made by the upper
speaker is not included. Utterances tagged as in-
complete are shown with a trailing &amp;quot;...&amp;quot; as shown
in Figure 3.
</bodyText>
<subsectionHeader confidence="0.999295">
4.4 Annotating Utterance Tags
</subsectionHeader>
<bodyText confidence="0.999948142857143">
In the utterance view, one can also annotate the
utterances with various tags. For our work, we
use a subset of the DAMSL tags corresponding
to forward and backward functions (Allen and
Core, 1997). Forward functions include state-
ment, request information, and suggestion. Back-
ward functions include answer, acknowledgment,
and agreement. Although these utterance tags
could be annotated in the word view, doing it in
the utterance view allows us to see more context,
which is needed to give the utterance the proper
tags. When necessary, the annotator can easily
refer to the word view to see the exact local con-
text.
</bodyText>
<subsectionHeader confidence="0.996781">
4.5 Annotating Blocks of Utterances
</subsectionHeader>
<bodyText confidence="0.999928066666667">
In the utterance view, the user can also annotate
hierarchical groupings of utterances.3 We use the
utterance blocks to annotate discourse structure
(Grosz and Sidner, 1986). This is similar to what
Flammia&apos;s tool allows (Flammia, 1995). Rather
than showing it with indentation and color, we
draw boxes around segments. Figure 3 shows a
dialogue excerpt with three utterance blocks in-
side of a larger block. To create a segment, the
user highlights a sequence of utterances and then
presses the &amp;quot;make segment&amp;quot; button. The user can
change the boundaries of the blocks by simply
dragging either the top or bottom edge of the box.
Blocks can also be deleted. The tool ensures that
if two blocks have utterances in common then one
block is entirely contained in the other.
Tags can also be assigned to the blocks. We
have just started using the tool for discourse seg-
mentation, and so we are still refining these tags.
In Grosz and Sidner&apos;s theory of discourse struc-
ture (1986), the speaker who initiates the block
does so to accomplish a certain purpose. We have
a tag for annotating this purpose. We also have
tags to categorize the block as a greeting, spec-
ify goal, construct plan, summarize plan, verify
plan, give info, request info, or other (Strayer and
Heeman, 2001).
The utterance view also allows the user to open
or close a block. When a block is open (the de-
fault), all of its utterances and sub-blocks are dis-
</bodyText>
<footnote confidence="0.7433285">
3We do not allow segments to be interleaved. It is
unclear if such phenomena actually occur.
</footnote>
<bodyText confidence="0.999911285714286">
played. When it is closed, its utterances and sub-
blocks are replaced by the single line purpose.
Opening and closing blocks is useful as it allows
the user to control the level of detail that is shown.
Consider the third embedded block shown in Fig-
ure 3, in which the conversants take seven utter-
ances to jointly make a suggestion. After we have
analyzed it, we can close it and just see the pur-
pose. This will make it easier to determine the
segmentation of the blocks that contain it.
We are experimenting with a special type of
dialogue block. Consider the example from the
previous paragraph, in which the conversants
take seven utterances to jointly make a sugges-
tion. This is related to the shared turns of
Schiffrin (1987), the co-operative completions of
Linell (1998), and the grounding units of Traum
and Nakatani (1999). We are experimenting with
how to support the annotation of such phenom-
ena. We have added a tag to indicate whether the
utterances in the block are being used to build a
single contribution. For these single contributions,
we also supply a concise paraphrase of what was
said. We have found that this paraphrase can be
built from a sequential subset of the words in the
utterances of the block. For instance, the para-
phrase of our example block is &amp;quot;and then take the
remaining boxcar down to Corning.&amp;quot;
</bodyText>
<sectionHeader confidence="0.959583" genericHeader="method">
5 Block View
</sectionHeader>
<bodyText confidence="0.999947666666667">
We are experimenting with a third view of the
dialogue. This view, which we refer to as Block-
View, abstracts away from the individual utter-
ances, and shows the hierarchical structure of the
discourse segments. This gives a very concise view
of the dialogue. The block view is also convenient
for it provides an index to the whole dialogue.
This allows the user to quickly move around the
dialogue.
</bodyText>
<sectionHeader confidence="0.988747" genericHeader="method">
6 Audio Playback
</sectionHeader>
<bodyText confidence="0.999523">
Each view gives the user the ability to select a re-
gion of the dialogue and to play it. In the word
view, the user can play each speaker channel indi-
vidually or both combined.4 This ability is espe-
cially useful for overlapping speech, where the an-
notator would want to listen to what each speaker
said individually, as well as hear the timing be-
tween the speaker utterances.
Just as each view provides a visual abstraction
from the previous one, we also do the same with
audio playback. In the word view, which has
</bodyText>
<footnote confidence="0.985126">
4In order to play each speaker individually, we re-
quire a separate audio channel for each speaker.
</footnote>
<table confidence="0.8392808">
wordViewUtt =&gt; atmostoneof abandoned incomplete overlap
uttViewUtt =&gt; anyof forward backward comment
uttViewUtt.forward =&gt; oneof statement question suggestion other
uttViewUtt.backward =&gt; oneof agreement understanding answer other
uttViewUtt.comment =&gt; other
</table>
<figureCaption confidence="0.999902">
Figure 5: Sample Specification of Utterance Tags
Figure 6: Sample Utterance Annotation Panel
</figureCaption>
<bodyText confidence="0.999923375">
the speech repairs annotated, the user can play
back the speech cleanly of either speaker, where
the stretches of speech annotated as the reparan-
dum or editing term of a repair are skipped. We
have found this to be of great assistance in verify-
ing if something should be annotated as a repair
or not. It gives us an audio means to verify the
speech repair annotations. If we have annotated
the repair correctly, the edited audio signal should
sound fairly natural.
In formatting the utterance view, we take into
account whether utterances have been marked as
abandoned or overlapped. We provide a special
playback in the utterance view that takes this
into account. We build an audio file in which we
skip over repairs, skip over abandoned speech, and
shorten large silences. If there is overlap that is
not marked as significant, we linearize it by con-
catenating the utterances together. If the overlap
is marked as significant, we keep the overlap. We
are finding that this gives us an audio means to
ensure that our markings of abandonment, over-
lap and our linearization is correct.
We are also experimenting with even further
simplifying the audio output. For blocks that have
a paraphrase, and the block is closed, we play the
paraphrase by constructing it from the words said
in the block. For blocks that are closed that do
not have a paraphrase, we use the text-to-speech
engine in the CSLU toolkit (Colton et al., 1996;
Sutton et al., 1997) to say the purpose, as if there
was a narrator.
</bodyText>
<sectionHeader confidence="0.983739" genericHeader="method">
7 Customizations
</sectionHeader>
<bodyText confidence="0.998609886363636">
Some aspects of the tool are built in, such as the
notion of utterances, speech repairs, and hierar-
chical grouping of utterances into blocks. How-
ever, the annotations of these phenomena and how
they are displayed can be customized through a
configuration file. This allows us to easily exper-
iment as we revise our annotation scheme; to use
domain specific tags; and to make the tool useful
for other researchers who might use different tags.
Speech repair tags, utterance tags, and block
tags are specified in the configuration file. Fig-
ure 5 gives a sample of how the annotation
tags for an utterance are specified. The two top
level entries in the figure are &amp;quot;wordViewUtt&amp;quot; and
&amp;quot;uttViewUtt&amp;quot;, which specify the utterance anno-
tation tags in WordView and UtteranceView, re-
spectively. The decomposition can be of one of
three types.
atmostoneof: at most one of the attributes can
be specified
oneof. exactly one of the attributes must be spec-
ified
anyof. there is no restriction on which attributes
can be specified
The subcomponents can either be terminals as is
the case for the decomposition of &amp;quot;wordViewUtt&amp;quot;,
or can be non-terminals, as is the case for each
of the three subcomponents of &amp;quot;uttViewUtt&amp;quot;.
Hence, hierarchical tags are supported. Termi-
nals are assumed to be of binary type, except for
&amp;quot;other&amp;quot;, which is assumed to be a string. The
configuration file determines how the annotation
panel is generated. For the annotation scheme
specified in Figure 5, Figure 6 shows the annota-
tion panel that would be automatically generated
for the utterance view.
As we explained earlier, some of the utterance
tags affect how the word view and utterance view
are formatted. Rather than hard code this func-
tionality, it is specified in the configuration file.
We are still experimenting with the best way to
code this functionality. Figure 7 gives an exam-
ple of how we code the utterance tag function-
ality. The first line indicates that the utterance
</bodyText>
<figureCaption confidence="0.944955571428571">
wordViewUtt.abandoned do wordView color red
wordViewUtt.abandoned uttView ignore
wordViewUtt.incomplete wordView color yellow
wordViewUtt.incomplete uttView trailsoff
wordViewUtt.overlap wordView color blue
wordViewUtt.overlap uttView overlap
Figure 7: Sample Utterance Display Specification
</figureCaption>
<bodyText confidence="0.99979775">
tag of &amp;quot;abandoned&amp;quot; coded in WordView should
be displayed in red in WordView. The second line
indicates that it should not be displayed in Utter-
anceView.
</bodyText>
<sectionHeader confidence="0.994393" genericHeader="method">
8 Implementation
</sectionHeader>
<bodyText confidence="0.99981375">
DialogueView is written in Tcl/Tk. We also use
utilities from the CSLU Speech Toolkit (Colton
et al., 1996; Sutton et al., 1997), including audio
and wave handling and speech synthesis. We have
rewritten the tool to use an object-oriented exten-
sion of Tcl called Tclpp, designed and developed
by Stefan Simnige. This is allowing us to better
manage the growing complexity of the tool as well
as reuse pieces of the software in our annotation
comparison tool (Yang et al., 2002). It should also
help in expanding the tool so that it can handle
any number of speakers.
</bodyText>
<sectionHeader confidence="0.979778" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.99998632">
In this paper, we described a dialogue annotation
tool that we are developing for segmenting dia-
logue into utterances, annotating speech repairs,
tagging speech acts, and segmenting dialogue into
hierarchical discourse segments. The tool presents
the dialogue at different levels of abstraction al-
lowing the user to both see in detail what is go-
ing on and see the higher level structure that is
being built. The higher levels not only abstract
away from the exact timing, but also can skip over
words, whole utterances, and even simplify seg-
ments to a single line paraphrase. Along with the
visual presentation, the audio can also be played
at these different levels of abstraction. We feel
that these capabilities should help annotators bet-
ter code dialogue.
This tool is still under active development. In
particular, we are currently refining how blocks
are displayed, improving the ability to customize
the tool for different tagsets, and improving the
audio playback facilities. As we develop this tool,
we are also doing dialogue annotation, and refin-
ing our scheme for annotating dialogue in order to
better capture the salient features of dialogue and
improve the inter-coder reliability.
</bodyText>
<sectionHeader confidence="0.995091" genericHeader="acknowledgments">
10 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9889545">
The authors acknowledgment funding from the In-
tel Research Council.
</bodyText>
<sectionHeader confidence="0.980712" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.994479583333333">
James F. Allen and Mark G. Core. 1997. Damsl:
Dialog annotation markup in several layers.
Unpublished Manuscript.
Claude Barras, Edouard Geoffrois, Zhibiao Wu,
and Mark Liberman. 2001. Transcriber: devel-
opment and use of a tool for assisting speech
corpora production. Speech Communications,
33:5-22.
Steve Cassidy and Jonathan Harrington. 2001.
Multi-level annotation in the Emu speech
database management system. Speech Commu-
nications, 33:61{77.
</bodyText>
<reference confidence="0.988619234693877">
Don Colton, Ron Cole, David G. Novick, and
Stephen Sutton. 1996. A laboratory course
for designing and testing spoken dialogue sys-
tems. In Proceedings of the International Con-
ference on Audio, Speech and Signal Processing
(ICASSP), pages 1129{1132.
Mark G. Core and James F. Allen. 1997. Coding
dialogs with the DAMSL annotation scheme.
In Working notes of the AAAI Fall Symposium
on Communicative Action in Humans and Ma-
chines.
Entropic Research Laboratory, Inc., 1993.
WAVES+ Reference Manual. Version 5.0.
George Ferguson. 1998. DAT: Dialogue annota-
tion tool. Available from www.cs.rochester.edu
in the subdirectory research/speech/damsl.
Giovanni Flammia. 1995. N.b.: A graphical user
interface for annotating spoken dialogue. In
AAAI Spring Symposium on Empirical Meth-
ods in Discourse Interpretation and Generation,
pages pages 40{46, Stanford, CA.
Giovanni Flammia. 1998. Discourse segmenta-
tion of spoken dialogue: an empirical approach.
Doctoral dissertation, Department of Electrical
and Computer Science, Massachusetts Institute
of Technology.
Barbara J. Grosz and Candace L. Sidner. 1986.
Attention, intentions, and the structure of dis-
course. Computational Linguistics, 12(3):175-
204.
Peter A. Heeman and James Allen. 1995a. Dia-
logue transcription tools. Trains Technical Note
94-1, Department of Computer Science, Univer-
sity of Rochester, March. Revised.
Peter A. Heeman and James F. Allen. 1995b. The
Trains spoken dialog corpus. CD-ROM, Lin-
guistics Data Consortium, April.
Peter A. Heeman and James F. Allen. 1999.
Speech repairs, intonational phrases and dis-
course markers: Modeling speakers&apos; utterances
in spoken dialog. Computational Linguistics,
25(4):527-572.
Arne J6nsson and Nils Dahlback. 2000. Distill-
ing dialogues a method using natural di-
alogue corpora for dialogue systems develop-
ment. In Proceedings of the 6th Applied Natural
Language Processing Conference, pages 44{51,
Seattle.
Michael Kipp. 2001. Anvil: A generic annotation
tool for multimodal dialogue. In Proceedings of
the &apos;rth European Conference on Speech Com-
munication and Technology (Eurospeech).
Per Linell. 1998. Approaching Dialogue: Talk,
Interaction and Contexts in Dialogical Perspec-
tives. John Benjamins Publishing.
David McKelvie, Amy Isard, Andreas Mengel,
Morten Baun Muller, Michael Grosse, and Mar-
ion Klein. 2001. The MATE workbench an
annotation tool for XML coded speech corpora.
Speech Communications, 33:97{112.
John F. Pitrelli, Mary E. Beckman, and Ju-
lia Hirschberg. 1994. Evaluation of prosodic
transcription labeling reliability in the ToBI
framework. In Proceedings of the 3rd Interna-
tional Conference on Spoken Language Process-
ing (ICSLP-94), Yokohama, September.
Deborah Schiffrin. 1987. Discourse Markers.
Cambridge University Press, New York.
Susan E. Strayer and Peter A. Heeman. 2001.
Dialogue structure and mixed initiative. In
Second workshop of the Special Interest Group
on Dialogue, pages 153{161, Aalborg Denmark,
September.
Stephen Sutton, Ed Kaiser, Andrew Cronk, and
Ronald Cole. 1997. Bringing spoken language
systems to the classroom. In Proceedings of the
5th European Conference on Speech Commu-
nication and Technology (Eurospeech), Rhodes,
Greece.
S. Sutton, R. Cole, J. de Villiers, J. Schalkwyk,
P. Vermeulen, M. Macon, Y. Yan, E. Kaiser,
R. Rundle, K. Shobaki, P. Hosom, A. Kain,
J. Wouters, M. Massaro, and M. Cohen. 1998.
Universal speech tools: the cslu toolkit. In Pro-
ceedings of the 5th International Conference on
Spoken Language Processing (ICSLP-98), pages
3221{3224, Sydney Australia, November.
David R. Traum and Christine H. Nakatani. 1999.
A two-level approach to coding dialogue for dis-
course structure: Activities of the 1998 working
group on higher-level structures. In Proceed-
ings of the ACL&apos;99 Workshop Towards Stan-
dards and Tools for Discourse Tagging, pages
101{108, June.
Fan Yang, Susan E. Strayer, and Peter A. Hee-
man. 2002. ACT: a graphical dialogue anno-
tation comparison tool. Submitted for publica-
tion.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.311636">
<note confidence="0.4351375">Proceedings of the Third SIGdial Workshop on Discourse and Dialogue, Philadelphia, July 2002, pp. 50-59. Association for Computational Linguistics.</note>
<title confidence="0.984864">DialogueView: An Annotation Tool for Dialogue</title>
<author confidence="0.93771">A Heeman Yang E</author>
<affiliation confidence="0.998442">Computer Science and OGI School of Science and Oregon Health &amp; Science</affiliation>
<address confidence="0.958814">20000 NW Walker Rd., Beaverton OR,</address>
<email confidence="0.998653">heeman@cse.ogi.eduyangf@cse.ogi.edususanstrayer@yahoo.com</email>
<abstract confidence="0.998431882352941">This paper describes DialogueView, a tool for annotating dialogues with utterance boundaries, speech repairs, speech act tags, and discourse segments. The tool provides several views of the data, including a word view that is timealigned with the audio signal, and an utterance view that shows the dialogue as if it were a script for a play. The utterance view abstracts away from lower level details that are coded in the word view. This allows the annotator to have a simpler view of the dialogue when coding speech act tags and discourse structure, but still have access to the details when needed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Don Colton</author>
<author>Ron Cole</author>
<author>David G Novick</author>
<author>Stephen Sutton</author>
</authors>
<title>A laboratory course for designing and testing spoken dialogue systems.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Audio, Speech and Signal Processing (ICASSP),</booktitle>
<pages>1129--1132</pages>
<contexts>
<context position="25378" citStr="Colton et al., 1996" startWordPosition="4319" endWordPosition="4322">marked as significant, we linearize it by concatenating the utterances together. If the overlap is marked as significant, we keep the overlap. We are finding that this gives us an audio means to ensure that our markings of abandonment, overlap and our linearization is correct. We are also experimenting with even further simplifying the audio output. For blocks that have a paraphrase, and the block is closed, we play the paraphrase by constructing it from the words said in the block. For blocks that are closed that do not have a paraphrase, we use the text-to-speech engine in the CSLU toolkit (Colton et al., 1996; Sutton et al., 1997) to say the purpose, as if there was a narrator. 7 Customizations Some aspects of the tool are built in, such as the notion of utterances, speech repairs, and hierarchical grouping of utterances into blocks. However, the annotations of these phenomena and how they are displayed can be customized through a configuration file. This allows us to easily experiment as we revise our annotation scheme; to use domain specific tags; and to make the tool useful for other researchers who might use different tags. Speech repair tags, utterance tags, and block tags are specified in th</context>
<context position="27981" citStr="Colton et al., 1996" startWordPosition="4735" endWordPosition="4738">ality. The first line indicates that the utterance wordViewUtt.abandoned do wordView color red wordViewUtt.abandoned uttView ignore wordViewUtt.incomplete wordView color yellow wordViewUtt.incomplete uttView trailsoff wordViewUtt.overlap wordView color blue wordViewUtt.overlap uttView overlap Figure 7: Sample Utterance Display Specification tag of &amp;quot;abandoned&amp;quot; coded in WordView should be displayed in red in WordView. The second line indicates that it should not be displayed in UtteranceView. 8 Implementation DialogueView is written in Tcl/Tk. We also use utilities from the CSLU Speech Toolkit (Colton et al., 1996; Sutton et al., 1997), including audio and wave handling and speech synthesis. We have rewritten the tool to use an object-oriented extension of Tcl called Tclpp, designed and developed by Stefan Simnige. This is allowing us to better manage the growing complexity of the tool as well as reuse pieces of the software in our annotation comparison tool (Yang et al., 2002). It should also help in expanding the tool so that it can handle any number of speakers. 9 Conclusion In this paper, we described a dialogue annotation tool that we are developing for segmenting dialogue into utterances, annotat</context>
</contexts>
<marker>Colton, Cole, Novick, Sutton, 1996</marker>
<rawString>Don Colton, Ron Cole, David G. Novick, and Stephen Sutton. 1996. A laboratory course for designing and testing spoken dialogue systems. In Proceedings of the International Conference on Audio, Speech and Signal Processing (ICASSP), pages 1129{1132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>James F Allen</author>
</authors>
<title>Coding dialogs with the DAMSL annotation scheme.</title>
<date>1997</date>
<contexts>
<context position="5594" citStr="Core and Allen, 1997" startWordPosition="920" endWordPosition="923">tures in a fourth. The audio annotation tools often have powerful signal analysis packages for displaying such phenomena as spectrograms and voicing. These tools, however, do not have any built-in facility to group words into utterances nor group utterances into hierarchical discourse segments. Several tools allow users to annotate higher level structure in dialogue. The annotation tool DAT from the University of Rochester (Ferguson, 1998) allows users to annotate utterances or groups of utterances with a set of hard-coded annotation tags for the DAMSL annotation scheme (Allen and Core, 1997; Core and Allen, 1997). The tool Nb from MIT (Flammia, 1995; Flammia, 1998) allows users to impose a hierarchical grouping on a sequence of utterances, and hence annotate discourse structure. Both DAT and Nb take as input a linearization of the speaker utterances. Mistakes in this input cannot be fixed. Whether there are errors or not, users cannot see the exact timing interactions between the speakers&apos; words or the length of pauses. This simplification can make it difficult for the annotator to truly understand what is happening in the dialogue, especially for overlapping speech, where speakers fight over the turn</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Mark G. Core and James F. Allen. 1997. Coding dialogs with the DAMSL annotation scheme.</rawString>
</citation>
<citation valid="false">
<booktitle>In Working notes of the AAAI Fall Symposium on Communicative Action in Humans and Machines.</booktitle>
<marker></marker>
<rawString>In Working notes of the AAAI Fall Symposium on Communicative Action in Humans and Machines.</rawString>
</citation>
<citation valid="true">
<date>1993</date>
<journal>WAVES+ Reference Manual. Version</journal>
<volume>5</volume>
<institution>Entropic Research Laboratory, Inc.,</institution>
<marker>1993</marker>
<rawString>Entropic Research Laboratory, Inc., 1993. WAVES+ Reference Manual. Version 5.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Ferguson</author>
</authors>
<title>DAT: Dialogue annotation tool. Available from www.cs.rochester.edu in the subdirectory research/speech/damsl.</title>
<date>1998</date>
<contexts>
<context position="5416" citStr="Ferguson, 1998" startWordPosition="891" endWordPosition="893">nnotation scheme (Pitrelli et al., 1994), one can have the word alignment in one text file, intonation features in a second, break indices in a third, and miscellaneous features in a fourth. The audio annotation tools often have powerful signal analysis packages for displaying such phenomena as spectrograms and voicing. These tools, however, do not have any built-in facility to group words into utterances nor group utterances into hierarchical discourse segments. Several tools allow users to annotate higher level structure in dialogue. The annotation tool DAT from the University of Rochester (Ferguson, 1998) allows users to annotate utterances or groups of utterances with a set of hard-coded annotation tags for the DAMSL annotation scheme (Allen and Core, 1997; Core and Allen, 1997). The tool Nb from MIT (Flammia, 1995; Flammia, 1998) allows users to impose a hierarchical grouping on a sequence of utterances, and hence annotate discourse structure. Both DAT and Nb take as input a linearization of the speaker utterances. Mistakes in this input cannot be fixed. Whether there are errors or not, users cannot see the exact timing interactions between the speakers&apos; words or the length of pauses. This s</context>
</contexts>
<marker>Ferguson, 1998</marker>
<rawString>George Ferguson. 1998. DAT: Dialogue annotation tool. Available from www.cs.rochester.edu in the subdirectory research/speech/damsl.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giovanni Flammia</author>
</authors>
<title>N.b.: A graphical user interface for annotating spoken dialogue.</title>
<date>1995</date>
<booktitle>In AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation,</booktitle>
<pages>40--46</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="5631" citStr="Flammia, 1995" startWordPosition="929" endWordPosition="930"> often have powerful signal analysis packages for displaying such phenomena as spectrograms and voicing. These tools, however, do not have any built-in facility to group words into utterances nor group utterances into hierarchical discourse segments. Several tools allow users to annotate higher level structure in dialogue. The annotation tool DAT from the University of Rochester (Ferguson, 1998) allows users to annotate utterances or groups of utterances with a set of hard-coded annotation tags for the DAMSL annotation scheme (Allen and Core, 1997; Core and Allen, 1997). The tool Nb from MIT (Flammia, 1995; Flammia, 1998) allows users to impose a hierarchical grouping on a sequence of utterances, and hence annotate discourse structure. Both DAT and Nb take as input a linearization of the speaker utterances. Mistakes in this input cannot be fixed. Whether there are errors or not, users cannot see the exact timing interactions between the speakers&apos; words or the length of pauses. This simplification can make it difficult for the annotator to truly understand what is happening in the dialogue, especially for overlapping speech, where speakers fight over the turn or make back-channels. The tools Tra</context>
<context position="19948" citStr="Flammia, 1995" startWordPosition="3385" endWordPosition="3386">ward functions include answer, acknowledgment, and agreement. Although these utterance tags could be annotated in the word view, doing it in the utterance view allows us to see more context, which is needed to give the utterance the proper tags. When necessary, the annotator can easily refer to the word view to see the exact local context. 4.5 Annotating Blocks of Utterances In the utterance view, the user can also annotate hierarchical groupings of utterances.3 We use the utterance blocks to annotate discourse structure (Grosz and Sidner, 1986). This is similar to what Flammia&apos;s tool allows (Flammia, 1995). Rather than showing it with indentation and color, we draw boxes around segments. Figure 3 shows a dialogue excerpt with three utterance blocks inside of a larger block. To create a segment, the user highlights a sequence of utterances and then presses the &amp;quot;make segment&amp;quot; button. The user can change the boundaries of the blocks by simply dragging either the top or bottom edge of the box. Blocks can also be deleted. The tool ensures that if two blocks have utterances in common then one block is entirely contained in the other. Tags can also be assigned to the blocks. We have just started using</context>
</contexts>
<marker>Flammia, 1995</marker>
<rawString>Giovanni Flammia. 1995. N.b.: A graphical user interface for annotating spoken dialogue. In AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, pages pages 40{46, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giovanni Flammia</author>
</authors>
<title>Discourse segmentation of spoken dialogue: an empirical approach.</title>
<date>1998</date>
<institution>Department of Electrical and Computer Science, Massachusetts Institute of Technology.</institution>
<note>Doctoral dissertation,</note>
<contexts>
<context position="5647" citStr="Flammia, 1998" startWordPosition="931" endWordPosition="932">erful signal analysis packages for displaying such phenomena as spectrograms and voicing. These tools, however, do not have any built-in facility to group words into utterances nor group utterances into hierarchical discourse segments. Several tools allow users to annotate higher level structure in dialogue. The annotation tool DAT from the University of Rochester (Ferguson, 1998) allows users to annotate utterances or groups of utterances with a set of hard-coded annotation tags for the DAMSL annotation scheme (Allen and Core, 1997; Core and Allen, 1997). The tool Nb from MIT (Flammia, 1995; Flammia, 1998) allows users to impose a hierarchical grouping on a sequence of utterances, and hence annotate discourse structure. Both DAT and Nb take as input a linearization of the speaker utterances. Mistakes in this input cannot be fixed. Whether there are errors or not, users cannot see the exact timing interactions between the speakers&apos; words or the length of pauses. This simplification can make it difficult for the annotator to truly understand what is happening in the dialogue, especially for overlapping speech, where speakers fight over the turn or make back-channels. The tools Transcriber (Barras</context>
</contexts>
<marker>Flammia, 1998</marker>
<rawString>Giovanni Flammia. 1998. Discourse segmentation of spoken dialogue: an empirical approach. Doctoral dissertation, Department of Electrical and Computer Science, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--3</pages>
<contexts>
<context position="19885" citStr="Grosz and Sidner, 1986" startWordPosition="3373" endWordPosition="3376">d functions include statement, request information, and suggestion. Backward functions include answer, acknowledgment, and agreement. Although these utterance tags could be annotated in the word view, doing it in the utterance view allows us to see more context, which is needed to give the utterance the proper tags. When necessary, the annotator can easily refer to the word view to see the exact local context. 4.5 Annotating Blocks of Utterances In the utterance view, the user can also annotate hierarchical groupings of utterances.3 We use the utterance blocks to annotate discourse structure (Grosz and Sidner, 1986). This is similar to what Flammia&apos;s tool allows (Flammia, 1995). Rather than showing it with indentation and color, we draw boxes around segments. Figure 3 shows a dialogue excerpt with three utterance blocks inside of a larger block. To create a segment, the user highlights a sequence of utterances and then presses the &amp;quot;make segment&amp;quot; button. The user can change the boundaries of the blocks by simply dragging either the top or bottom edge of the box. Blocks can also be deleted. The tool ensures that if two blocks have utterances in common then one block is entirely contained in the other. Tags</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
<author>James Allen</author>
</authors>
<title>Dialogue transcription tools.</title>
<date>1995</date>
<tech>Trains Technical Note 94-1,</tech>
<publisher>Revised.</publisher>
<institution>Department of Computer Science, University of Rochester,</institution>
<contexts>
<context position="1682" citStr="Heeman and Allen, 1995" startWordPosition="262" endWordPosition="265">d. 1 Introduction There is a growing interest in annotating humanhuman dialogue. Annotated dialogues are useful for formulating and verifying theories of dialogue and for building statistical models. In addition to orthographic word transcription, one might want the following dialogue annotations. e Annotation of the speech repairs. Speech repairs are a type of disfluency where speakers go back and change or repeat something they just said. • Segmentation of the speech of each speaker into utterance units, with a single ordering of the utterances. We refer to this as linearizing the dialogue (Heeman and Allen, 1995a). e Each utterance tagged with its speech act function. e The utterances grouped into hierarchical discourse segments. There are tools that address subsets of the above tasks. However, we feel that doing dialogue annotation is very difficult. Part of this difficulty is due to the interactions between the annotation tasks. An error at a lower level can have a large impact on the higher level annotations. For instance, there can be ambiguity as to whether an &amp;quot;okay&amp;quot; is part of a speech repair; this will impact the segmentation of the speech into utterance units and the speech act coding. Someti</context>
<context position="7553" citStr="Heeman and Allen, 1995" startWordPosition="1248" endWordPosition="1251">a better trade-off among generality, functionality, and complexity. This tool offers multi-modal annotation support, but like Transcriber, does not allow detailed annotation of dialogue phenomena, such as overlapping speech and abandoned speech, and has no abstraction mechanism. 3 Word V iew Our dialogue annotation tool, DialogueView, gives the user three views of the data. The lowest level view is called WordView and takes as input the words said by each speaker and their start and stop times and shows them time-aligned with the audio signal. Figure 1 shows an excerpt from the Trains corpus (Heeman and Allen, 1995b) in WordView. This view is ideal for viewing the exact timing of speech, especially overlapping speech. As will be discussed below, we use it for segmenting the speech into utterances, and annotating communicative status and speech repairs.2 These annotations will allow us to build a simpler representation of what is happening in the speech for the utterance view, which is discussed in the next section. 3.1 Utterance Segmentation As can be seen in Figure 1, WordView shows the words segmented into utterances, which for our purpose is simply a grouping of consecutive words by a single speaker,</context>
</contexts>
<marker>Heeman, Allen, 1995</marker>
<rawString>Peter A. Heeman and James Allen. 1995a. Dialogue transcription tools. Trains Technical Note 94-1, Department of Computer Science, University of Rochester, March. Revised.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
<author>James F Allen</author>
</authors>
<title>The Trains spoken dialog corpus. CD-ROM, Linguistics Data Consortium,</title>
<date>1995</date>
<contexts>
<context position="1682" citStr="Heeman and Allen, 1995" startWordPosition="262" endWordPosition="265">d. 1 Introduction There is a growing interest in annotating humanhuman dialogue. Annotated dialogues are useful for formulating and verifying theories of dialogue and for building statistical models. In addition to orthographic word transcription, one might want the following dialogue annotations. e Annotation of the speech repairs. Speech repairs are a type of disfluency where speakers go back and change or repeat something they just said. • Segmentation of the speech of each speaker into utterance units, with a single ordering of the utterances. We refer to this as linearizing the dialogue (Heeman and Allen, 1995a). e Each utterance tagged with its speech act function. e The utterances grouped into hierarchical discourse segments. There are tools that address subsets of the above tasks. However, we feel that doing dialogue annotation is very difficult. Part of this difficulty is due to the interactions between the annotation tasks. An error at a lower level can have a large impact on the higher level annotations. For instance, there can be ambiguity as to whether an &amp;quot;okay&amp;quot; is part of a speech repair; this will impact the segmentation of the speech into utterance units and the speech act coding. Someti</context>
<context position="7553" citStr="Heeman and Allen, 1995" startWordPosition="1248" endWordPosition="1251">a better trade-off among generality, functionality, and complexity. This tool offers multi-modal annotation support, but like Transcriber, does not allow detailed annotation of dialogue phenomena, such as overlapping speech and abandoned speech, and has no abstraction mechanism. 3 Word V iew Our dialogue annotation tool, DialogueView, gives the user three views of the data. The lowest level view is called WordView and takes as input the words said by each speaker and their start and stop times and shows them time-aligned with the audio signal. Figure 1 shows an excerpt from the Trains corpus (Heeman and Allen, 1995b) in WordView. This view is ideal for viewing the exact timing of speech, especially overlapping speech. As will be discussed below, we use it for segmenting the speech into utterances, and annotating communicative status and speech repairs.2 These annotations will allow us to build a simpler representation of what is happening in the speech for the utterance view, which is discussed in the next section. 3.1 Utterance Segmentation As can be seen in Figure 1, WordView shows the words segmented into utterances, which for our purpose is simply a grouping of consecutive words by a single speaker,</context>
</contexts>
<marker>Heeman, Allen, 1995</marker>
<rawString>Peter A. Heeman and James F. Allen. 1995b. The Trains spoken dialog corpus. CD-ROM, Linguistics Data Consortium, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
<author>James F Allen</author>
</authors>
<title>Speech repairs, intonational phrases and discourse markers: Modeling speakers&apos; utterances in spoken dialog.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--4</pages>
<contexts>
<context position="13095" citStr="Heeman and Allen, 1999" startWordPosition="2202" endWordPosition="2205"> be done provided that the speaker was silent in the time interval preceding where the other person started talking. In summary, overlapping speech can be handled in three ways. The utterance can be explicitly tagged as overlapping; the overlap can be ignored if it is not critical in understanding what is going on in the dialogue; or the start time of the utterance can be changed so that the overlap does not need to be tagged. 3.4 Speech Repairs WordView also allows users to annotate speech repairs. A speech repair is where a user goes back and repeats or changes something that was just said (Heeman and Allen, 1999). Below we give an example of a speech repair and show its principle components: reparandum, interruption point, and editing term. Example 1 why don&apos;t we take ���� reparandum ip The reparandum is the speech that is being replaced, the interruption point is the end of the reparandum, and the editing term consists of words such as &amp;quot;um&amp;quot;, &amp;quot;uh&amp;quot;, &amp;quot;okay&amp;quot;, &amp;quot;let&apos;s see&amp;quot; that help signal the repair. To annotate a repair, the user highlights a sequence of words and then tags it as a reparandum or an editing term of a repair. The user can also specify the type of repair. Figure 2 shows how speech repairs a</context>
</contexts>
<marker>Heeman, Allen, 1999</marker>
<rawString>Peter A. Heeman and James F. Allen. 1999. Speech repairs, intonational phrases and discourse markers: Modeling speakers&apos; utterances in spoken dialog. Computational Linguistics, 25(4):527-572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne J6nsson</author>
<author>Nils Dahlback</author>
</authors>
<title>Distilling dialogues a method using natural dialogue corpora for dialogue systems development.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference,</booktitle>
<pages>44--51</pages>
<location>Seattle.</location>
<marker>J6nsson, Dahlback, 2000</marker>
<rawString>Arne J6nsson and Nils Dahlback. 2000. Distilling dialogues a method using natural dialogue corpora for dialogue systems development. In Proceedings of the 6th Applied Natural Language Processing Conference, pages 44{51, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kipp</author>
</authors>
<title>Anvil: A generic annotation tool for multimodal dialogue.</title>
<date>2001</date>
<booktitle>In Proceedings of the &apos;rth European Conference on Speech Communication and Technology (Eurospeech).</booktitle>
<contexts>
<context position="6874" citStr="Kipp, 2001" startWordPosition="1137" endWordPosition="1138">nd Mate (McKelvie et al., 2001) allow multiple views of the data: a word view with words time-aligned to the audio signal and an utterance view. However, Transcriber is geared to single channel data and has a weak ability to handle overlapping speech and Mate only allows one view to be shown at a time. The author of a competing tool has remarked that &amp;quot;speed and stability of [Mate] are both still problematic for real annotation. Also, the highly generic approach increases the initial effort to set up the tool since you basically have to write your own tool using the Mate style-sheet language&amp;quot; (Kipp, 2001). Hence, he developed a new tool Anvil that he claims is a better trade-off among generality, functionality, and complexity. This tool offers multi-modal annotation support, but like Transcriber, does not allow detailed annotation of dialogue phenomena, such as overlapping speech and abandoned speech, and has no abstraction mechanism. 3 Word V iew Our dialogue annotation tool, DialogueView, gives the user three views of the data. The lowest level view is called WordView and takes as input the words said by each speaker and their start and stop times and shows them time-aligned with the audio s</context>
</contexts>
<marker>Kipp, 2001</marker>
<rawString>Michael Kipp. 2001. Anvil: A generic annotation tool for multimodal dialogue. In Proceedings of the &apos;rth European Conference on Speech Communication and Technology (Eurospeech).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Per Linell</author>
</authors>
<title>Approaching Dialogue: Talk, Interaction and Contexts in Dialogical Perspectives.</title>
<date>1998</date>
<publisher>John Benjamins Publishing.</publisher>
<contexts>
<context position="22000" citStr="Linell (1998)" startWordPosition="3747" endWordPosition="3748">rol the level of detail that is shown. Consider the third embedded block shown in Figure 3, in which the conversants take seven utterances to jointly make a suggestion. After we have analyzed it, we can close it and just see the purpose. This will make it easier to determine the segmentation of the blocks that contain it. We are experimenting with a special type of dialogue block. Consider the example from the previous paragraph, in which the conversants take seven utterances to jointly make a suggestion. This is related to the shared turns of Schiffrin (1987), the co-operative completions of Linell (1998), and the grounding units of Traum and Nakatani (1999). We are experimenting with how to support the annotation of such phenomena. We have added a tag to indicate whether the utterances in the block are being used to build a single contribution. For these single contributions, we also supply a concise paraphrase of what was said. We have found that this paraphrase can be built from a sequential subset of the words in the utterances of the block. For instance, the paraphrase of our example block is &amp;quot;and then take the remaining boxcar down to Corning.&amp;quot; 5 Block View We are experimenting with a th</context>
</contexts>
<marker>Linell, 1998</marker>
<rawString>Per Linell. 1998. Approaching Dialogue: Talk, Interaction and Contexts in Dialogical Perspectives. John Benjamins Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McKelvie</author>
<author>Amy Isard</author>
<author>Andreas Mengel</author>
<author>Morten Baun Muller</author>
<author>Michael Grosse</author>
<author>Marion Klein</author>
</authors>
<title>The MATE workbench an annotation tool for XML coded speech corpora.</title>
<date>2001</date>
<journal>Speech Communications,</journal>
<pages>33--97</pages>
<contexts>
<context position="6294" citStr="McKelvie et al., 2001" startWordPosition="1034" endWordPosition="1037"> hierarchical grouping on a sequence of utterances, and hence annotate discourse structure. Both DAT and Nb take as input a linearization of the speaker utterances. Mistakes in this input cannot be fixed. Whether there are errors or not, users cannot see the exact timing interactions between the speakers&apos; words or the length of pauses. This simplification can make it difficult for the annotator to truly understand what is happening in the dialogue, especially for overlapping speech, where speakers fight over the turn or make back-channels. The tools Transcriber (Barras et al., 2001) and Mate (McKelvie et al., 2001) allow multiple views of the data: a word view with words time-aligned to the audio signal and an utterance view. However, Transcriber is geared to single channel data and has a weak ability to handle overlapping speech and Mate only allows one view to be shown at a time. The author of a competing tool has remarked that &amp;quot;speed and stability of [Mate] are both still problematic for real annotation. Also, the highly generic approach increases the initial effort to set up the tool since you basically have to write your own tool using the Mate style-sheet language&amp;quot; (Kipp, 2001). Hence, he develope</context>
</contexts>
<marker>McKelvie, Isard, Mengel, Muller, Grosse, Klein, 2001</marker>
<rawString>David McKelvie, Amy Isard, Andreas Mengel, Morten Baun Muller, Michael Grosse, and Marion Klein. 2001. The MATE workbench an annotation tool for XML coded speech corpora. Speech Communications, 33:97{112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John F Pitrelli</author>
<author>Mary E Beckman</author>
<author>Julia Hirschberg</author>
</authors>
<title>Evaluation of prosodic transcription labeling reliability in the ToBI framework.</title>
<date>1994</date>
<booktitle>In Proceedings of the 3rd International Conference on Spoken Language Processing (ICSLP-94),</booktitle>
<location>Yokohama,</location>
<contexts>
<context position="4841" citStr="Pitrelli et al., 1994" startWordPosition="801" endWordPosition="804">s of abstraction in our annotation tool. We then discuss audio playback. Next, we discuss how the tool can be customized for different annotation schemes. Finally, we discuss the implementation of the tool. 2 Existing Tools There are a number of tools that let users annotate speech audio files. These include Emu (Cassidy and Harrington, 2001), SpeechView (Sutton et al., 1998) and Waves+ (Ent, 1993). These tools often allow multiple text annotation files to be associated with the waveform and allow users an easy facility to capture time alignments. For instance, for the ToBI annotation scheme (Pitrelli et al., 1994), one can have the word alignment in one text file, intonation features in a second, break indices in a third, and miscellaneous features in a fourth. The audio annotation tools often have powerful signal analysis packages for displaying such phenomena as spectrograms and voicing. These tools, however, do not have any built-in facility to group words into utterances nor group utterances into hierarchical discourse segments. Several tools allow users to annotate higher level structure in dialogue. The annotation tool DAT from the University of Rochester (Ferguson, 1998) allows users to annotate</context>
</contexts>
<marker>Pitrelli, Beckman, Hirschberg, 1994</marker>
<rawString>John F. Pitrelli, Mary E. Beckman, and Julia Hirschberg. 1994. Evaluation of prosodic transcription labeling reliability in the ToBI framework. In Proceedings of the 3rd International Conference on Spoken Language Processing (ICSLP-94), Yokohama, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah Schiffrin</author>
</authors>
<title>Discourse Markers.</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="21953" citStr="Schiffrin (1987)" startWordPosition="3741" endWordPosition="3742">ing blocks is useful as it allows the user to control the level of detail that is shown. Consider the third embedded block shown in Figure 3, in which the conversants take seven utterances to jointly make a suggestion. After we have analyzed it, we can close it and just see the purpose. This will make it easier to determine the segmentation of the blocks that contain it. We are experimenting with a special type of dialogue block. Consider the example from the previous paragraph, in which the conversants take seven utterances to jointly make a suggestion. This is related to the shared turns of Schiffrin (1987), the co-operative completions of Linell (1998), and the grounding units of Traum and Nakatani (1999). We are experimenting with how to support the annotation of such phenomena. We have added a tag to indicate whether the utterances in the block are being used to build a single contribution. For these single contributions, we also supply a concise paraphrase of what was said. We have found that this paraphrase can be built from a sequential subset of the words in the utterances of the block. For instance, the paraphrase of our example block is &amp;quot;and then take the remaining boxcar down to Cornin</context>
</contexts>
<marker>Schiffrin, 1987</marker>
<rawString>Deborah Schiffrin. 1987. Discourse Markers. Cambridge University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan E Strayer</author>
<author>Peter A Heeman</author>
</authors>
<title>Dialogue structure and mixed initiative.</title>
<date>2001</date>
<booktitle>In Second workshop of the Special Interest Group on Dialogue,</booktitle>
<pages>153--161</pages>
<location>Aalborg Denmark,</location>
<contexts>
<context position="20983" citStr="Strayer and Heeman, 2001" startWordPosition="3564" endWordPosition="3567"> deleted. The tool ensures that if two blocks have utterances in common then one block is entirely contained in the other. Tags can also be assigned to the blocks. We have just started using the tool for discourse segmentation, and so we are still refining these tags. In Grosz and Sidner&apos;s theory of discourse structure (1986), the speaker who initiates the block does so to accomplish a certain purpose. We have a tag for annotating this purpose. We also have tags to categorize the block as a greeting, specify goal, construct plan, summarize plan, verify plan, give info, request info, or other (Strayer and Heeman, 2001). The utterance view also allows the user to open or close a block. When a block is open (the default), all of its utterances and sub-blocks are dis3We do not allow segments to be interleaved. It is unclear if such phenomena actually occur. played. When it is closed, its utterances and subblocks are replaced by the single line purpose. Opening and closing blocks is useful as it allows the user to control the level of detail that is shown. Consider the third embedded block shown in Figure 3, in which the conversants take seven utterances to jointly make a suggestion. After we have analyzed it, </context>
</contexts>
<marker>Strayer, Heeman, 2001</marker>
<rawString>Susan E. Strayer and Peter A. Heeman. 2001. Dialogue structure and mixed initiative. In Second workshop of the Special Interest Group on Dialogue, pages 153{161, Aalborg Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Sutton</author>
<author>Ed Kaiser</author>
<author>Andrew Cronk</author>
<author>Ronald Cole</author>
</authors>
<title>Bringing spoken language systems to the classroom.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology (Eurospeech),</booktitle>
<location>Rhodes, Greece.</location>
<contexts>
<context position="25400" citStr="Sutton et al., 1997" startWordPosition="4323" endWordPosition="4326">, we linearize it by concatenating the utterances together. If the overlap is marked as significant, we keep the overlap. We are finding that this gives us an audio means to ensure that our markings of abandonment, overlap and our linearization is correct. We are also experimenting with even further simplifying the audio output. For blocks that have a paraphrase, and the block is closed, we play the paraphrase by constructing it from the words said in the block. For blocks that are closed that do not have a paraphrase, we use the text-to-speech engine in the CSLU toolkit (Colton et al., 1996; Sutton et al., 1997) to say the purpose, as if there was a narrator. 7 Customizations Some aspects of the tool are built in, such as the notion of utterances, speech repairs, and hierarchical grouping of utterances into blocks. However, the annotations of these phenomena and how they are displayed can be customized through a configuration file. This allows us to easily experiment as we revise our annotation scheme; to use domain specific tags; and to make the tool useful for other researchers who might use different tags. Speech repair tags, utterance tags, and block tags are specified in the configuration file. </context>
<context position="28003" citStr="Sutton et al., 1997" startWordPosition="4739" endWordPosition="4742"> indicates that the utterance wordViewUtt.abandoned do wordView color red wordViewUtt.abandoned uttView ignore wordViewUtt.incomplete wordView color yellow wordViewUtt.incomplete uttView trailsoff wordViewUtt.overlap wordView color blue wordViewUtt.overlap uttView overlap Figure 7: Sample Utterance Display Specification tag of &amp;quot;abandoned&amp;quot; coded in WordView should be displayed in red in WordView. The second line indicates that it should not be displayed in UtteranceView. 8 Implementation DialogueView is written in Tcl/Tk. We also use utilities from the CSLU Speech Toolkit (Colton et al., 1996; Sutton et al., 1997), including audio and wave handling and speech synthesis. We have rewritten the tool to use an object-oriented extension of Tcl called Tclpp, designed and developed by Stefan Simnige. This is allowing us to better manage the growing complexity of the tool as well as reuse pieces of the software in our annotation comparison tool (Yang et al., 2002). It should also help in expanding the tool so that it can handle any number of speakers. 9 Conclusion In this paper, we described a dialogue annotation tool that we are developing for segmenting dialogue into utterances, annotating speech repairs, ta</context>
</contexts>
<marker>Sutton, Kaiser, Cronk, Cole, 1997</marker>
<rawString>Stephen Sutton, Ed Kaiser, Andrew Cronk, and Ronald Cole. 1997. Bringing spoken language systems to the classroom. In Proceedings of the 5th European Conference on Speech Communication and Technology (Eurospeech), Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sutton</author>
<author>R Cole</author>
<author>J de Villiers</author>
<author>J Schalkwyk</author>
<author>P Vermeulen</author>
<author>M Macon</author>
<author>Y Yan</author>
<author>E Kaiser</author>
<author>R Rundle</author>
<author>K Shobaki</author>
<author>P Hosom</author>
<author>A Kain</author>
<author>J Wouters</author>
<author>M Massaro</author>
<author>M Cohen</author>
</authors>
<title>Universal speech tools: the cslu toolkit.</title>
<date>1998</date>
<booktitle>In Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP-98),</booktitle>
<pages>3221--3224</pages>
<location>Sydney Australia,</location>
<marker>Sutton, Cole, de Villiers, Schalkwyk, Vermeulen, Macon, Yan, Kaiser, Rundle, Shobaki, Hosom, Kain, Wouters, Massaro, Cohen, 1998</marker>
<rawString>S. Sutton, R. Cole, J. de Villiers, J. Schalkwyk, P. Vermeulen, M. Macon, Y. Yan, E. Kaiser, R. Rundle, K. Shobaki, P. Hosom, A. Kain, J. Wouters, M. Massaro, and M. Cohen. 1998. Universal speech tools: the cslu toolkit. In Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP-98), pages 3221{3224, Sydney Australia, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Christine H Nakatani</author>
</authors>
<title>A two-level approach to coding dialogue for discourse structure: Activities of the 1998 working group on higher-level structures.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL&apos;99 Workshop Towards Standards and Tools for Discourse Tagging,</booktitle>
<pages>101--108</pages>
<contexts>
<context position="22054" citStr="Traum and Nakatani (1999)" startWordPosition="3754" endWordPosition="3757">sider the third embedded block shown in Figure 3, in which the conversants take seven utterances to jointly make a suggestion. After we have analyzed it, we can close it and just see the purpose. This will make it easier to determine the segmentation of the blocks that contain it. We are experimenting with a special type of dialogue block. Consider the example from the previous paragraph, in which the conversants take seven utterances to jointly make a suggestion. This is related to the shared turns of Schiffrin (1987), the co-operative completions of Linell (1998), and the grounding units of Traum and Nakatani (1999). We are experimenting with how to support the annotation of such phenomena. We have added a tag to indicate whether the utterances in the block are being used to build a single contribution. For these single contributions, we also supply a concise paraphrase of what was said. We have found that this paraphrase can be built from a sequential subset of the words in the utterances of the block. For instance, the paraphrase of our example block is &amp;quot;and then take the remaining boxcar down to Corning.&amp;quot; 5 Block View We are experimenting with a third view of the dialogue. This view, which we refer to</context>
</contexts>
<marker>Traum, Nakatani, 1999</marker>
<rawString>David R. Traum and Christine H. Nakatani. 1999. A two-level approach to coding dialogue for discourse structure: Activities of the 1998 working group on higher-level structures. In Proceedings of the ACL&apos;99 Workshop Towards Standards and Tools for Discourse Tagging, pages 101{108, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Yang</author>
<author>Susan E Strayer</author>
<author>Peter A Heeman</author>
</authors>
<title>ACT: a graphical dialogue annotation comparison tool.</title>
<date>2002</date>
<note>Submitted for publication.</note>
<contexts>
<context position="28352" citStr="Yang et al., 2002" startWordPosition="4798" endWordPosition="4801">ordView should be displayed in red in WordView. The second line indicates that it should not be displayed in UtteranceView. 8 Implementation DialogueView is written in Tcl/Tk. We also use utilities from the CSLU Speech Toolkit (Colton et al., 1996; Sutton et al., 1997), including audio and wave handling and speech synthesis. We have rewritten the tool to use an object-oriented extension of Tcl called Tclpp, designed and developed by Stefan Simnige. This is allowing us to better manage the growing complexity of the tool as well as reuse pieces of the software in our annotation comparison tool (Yang et al., 2002). It should also help in expanding the tool so that it can handle any number of speakers. 9 Conclusion In this paper, we described a dialogue annotation tool that we are developing for segmenting dialogue into utterances, annotating speech repairs, tagging speech acts, and segmenting dialogue into hierarchical discourse segments. The tool presents the dialogue at different levels of abstraction allowing the user to both see in detail what is going on and see the higher level structure that is being built. The higher levels not only abstract away from the exact timing, but also can skip over wo</context>
</contexts>
<marker>Yang, Strayer, Heeman, 2002</marker>
<rawString>Fan Yang, Susan E. Strayer, and Peter A. Heeman. 2002. ACT: a graphical dialogue annotation comparison tool. Submitted for publication.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>