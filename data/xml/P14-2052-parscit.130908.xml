<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.613819">
Single Document Summarization based on Nested Tree Structure
</title>
<author confidence="0.761547">
Yuta Kikuchi† Tsutomu Hirao$ Hiroya Takamura† Manabu Okumura† Masaaki Nagata$
</author>
<affiliation confidence="0.973406">
†Tokyo Institute of technology
</affiliation>
<address confidence="0.664758">
4295, Nagatsuta, Midori-ku, Yokohama, 226-8503, Japan
</address>
<email confidence="0.985979">
{kikuchi,takamura,oku}@lr.pi.titech.ac.jp
</email>
<note confidence="0.895497">
$NTT Communication Science Laboratories, NTT Corporation
</note>
<address confidence="0.698847">
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
</address>
<email confidence="0.986407">
{hirao.tsutomu,nagata.masaaki}@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.98422" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952913043478">
Many methods of text summarization
combining sentence selection and sen-
tence compression have recently been pro-
posed. Although the dependency between
words has been used in most of these
methods, the dependency between sen-
tences, i.e., rhetorical structures, has not
been exploited in such joint methods. We
used both dependency between words and
dependency between sentences by con-
structing a nested tree, in which nodes
in the document tree representing depen-
dency between sentences were replaced by
a sentence tree representing dependency
between words. We formulated a sum-
marization task as a combinatorial opti-
mization problem, in which the nested
tree was trimmed without losing impor-
tant content in the source document. The
results from an empirical evaluation re-
vealed that our method based on the trim-
ming of the nested tree significantly im-
proved the summarization of texts.
</bodyText>
<sectionHeader confidence="0.99248" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906905660378">
Extractive summarization is one well-known ap-
proach to text summarization and extractive meth-
ods represent a document (or a set of documents)
as a set of some textual units (e.g., sentences,
clauses, and words) and select their subset as a
summary. Formulating extractive summarization
as a combinational optimization problem greatly
improves the quality of summarization (McDon-
ald, 2007; Filatova and Hatzivassiloglou, 2004;
Takamura and Okumura, 2009). There has re-
cently been increasing attention focused on ap-
proaches that jointly optimize sentence extraction
and sentence compression (Tomita et al., 2009;
Qian and Liu, 2013; Morita et al., 2013; Gillick
and Favre, 2009; Almeida and Martins, 2013;
Berg-Kirkpatrick et al., 2011). We can only ex-
tract important content by trimming redundant
parts from sentences.
However, as these methods did not include the
discourse structures of documents, the generated
summaries lacked coherence. It is important for
generated summaries to have a discourse struc-
ture that is similar to that of the source docu-
ment. Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988) is one way of introduc-
ing the discourse structure of a document to a
summarization task (Marcu, 1998; Daum´e III and
Marcu, 2002; Hirao et al., 2013). Hirao et al.
recently transformed RST trees into dependency
trees and used them for single document summa-
rization (Hirao et al., 2013). They formulated the
summarization problem as a tree knapsack prob-
lem with constraints represented by the depen-
dency trees.
We propose a method of summarizing a single
document that utilizes dependency between sen-
tences obtained from rhetorical structures and de-
pendency between words obtained from a depen-
dency parser. We have explained our method with
an example in Figure 1. First, we represent a doc-
ument as a nested tree, which is composed of two
types of tree structures: a document tree and a
sentence tree. The document tree is a tree that has
sentences as nodes and head modifier relationships
between sentences obtained by RST as edges. The
sentence tree is a tree that has words as nodes
and head modifier relationships between words
obtained by the dependency parser as edges. We
can build the nested tree by regarding each node of
the document tree as a sentence tree. Finally, we
formulate the problem of single document sum-
marization as that of combinatorial optimization,
which is based on the trimming of the nested tree.
</bodyText>
<page confidence="0.725864">
315
</page>
<note confidence="0.669845">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 315–320,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.730997666666667">
*
Mike said he is training for a race.
The race is held next month.
He looks very tired.
John was running on a track in the park.
Source document
</figure>
<bodyText confidence="0.82339925">
John was running on a track in the park.
He looks very tired.
Mike said he is training for a race.
The race is held next month.
Summary
John was running on a track.
he is training for a race. *
The race is held next month.
</bodyText>
<figureCaption confidence="0.9210515">
Figure 1: Overview of our method. The source document is represented as a nested tree. Our method
simultaneously selects a rooted document subtree and sentence subtree from each node.
</figureCaption>
<bodyText confidence="0.975858285714286">
Our method jointly utilizes relations between sen-
tences and relations between words, and extracts
a rooted document subtree from a document tree
whose nodes are arbitrary subtrees of the sentence
tree.
Elementary Discourse Units (EDUs) in RST are
defined as the minimal building blocks of dis-
course. EDUs roughly correspond to clauses.
Most methods of summarization based on RST use
EDUs as extraction textual units. We converted
the rhetorical relations between EDUs to the re-
lations between sentences to build the nested tree
structure. Weucould thus take into account both
relations between sentences and relations between
</bodyText>
<equation confidence="0.467812333333333">
words.
�
ou
</equation>
<sectionHeader confidence="0.307845" genericHeader="introduction">
2 Related work 6
</sectionHeader>
<bodyText confidence="0.54321">
Number
frm
Extracting a subtree from the dependency tree of
</bodyText>
<equation confidence="0.31452925">
4
words is one approach to sentence compression
(Tomita et al., 2009; Qian and Liu, 2013; Morita
2
</equation>
<bodyText confidence="0.979133076923077">
et al., 2013; Gillick and Favre, 2009). However,
these studies have only extracted rooted subtrees
0
from sentences. We allowed our model to extract
a subtree that did not inc
the sentence with an asterisk * intFigure 1). T
method of Filippova and Strube (2008) allows the
model to extract non-rooted subtrees in sentence
compression tasks that compress a single sentence
with a given compression ratio. However, it is not
trivial to apply their method to text summariza-
tion because no compression ratio is given to sen-
tences. None of these methods use the discourse
structures of documents.
Daum´e III and Marcu (2002) proposed a noisy-
channel model that used RST. Although their
method generated a well-organized summary, no
optimality of information coverage was guaran-
teed and their method could not accept large texts
because of the high computational cost. In addi-
l
- The scare over Alar, a growth regulator
- that makes apples redder and crunchier
- but may be carcinogenic,
- made consumers shy away from the Delicious,
- though they were less affected than the McIntosh.
</bodyText>
<figureCaption confidence="0.966868">
Figure 2: Example of one sentence. Each line cor-
responds to one EDU.
</figureCaption>
<bodyText confidence="0.992789285714286">
�
tion, their method required large sets of data to cal-
culate the accurate probability. There have been
some studies that have used discourse structures
locally to optimize the order of selected sentences
(Nishikawa et al., 2010; Christensen et al., 2013).
��
</bodyText>
<sectionHeader confidence="0.632357" genericHeader="method">
3 Generating summary from nested tree
</sectionHeader>
<subsectionHeader confidence="0.999383">
3.1 Building Nested Tree with RST
</subsectionHeader>
<bodyText confidence="0.995667333333333">
A document in RST is segmented into EDUs and
adjacent EDUs are linked with rhetorical relations
to build an RST-Discourse Tree (RST-DT) that has
a hierarchical structure of the relations. There are
78 types of rhetorical relations between two spans,
and each span has one of two aspects of a nu-
cleus and a satellite. The nucleus is more salient
to the discourse structure, while the other span, the
tree whose terminal nodes correspond
selection summa
to EDUs and whose nonterminal nodes indicate
the relations. Hirao et al. converted RST-DTs
into dependency-based discourse trees (DEP-DTs)
whose nodes corresponded to EDUs and whose
edges corresponded to the head modifier relation-
ships of EDUs. See Hirao et al. for details (Hirao
et al., 2013).
Our model requires sentence-level dependency.
Fortunately we can simply convert DEP-DTs to
obtain dependency trees between sentences. We
specifically merge EDUs that belong to the same
sentence. Each sentence has only one root EDU
that is the parent of all the other EDUs in the sen-
tence. Each root EDU in a sentence has the parent
</bodyText>
<figure confidence="0.949009083333333">
s
n c
t
d!.-sy
st
s
u do
u m
ude the root word (See satellite represents supporting information RST-
he ection
DT is a
316
</figure>
<equation confidence="0.996408571428571">
s.t. ∑i �j izij ≤ L; (1)
xparent(i) ≥ xi; ∀i (2)
zparent(i,j) − zij + rij ≥ 0; ∀i, j (3)
xi ≥ zij; ∀i, j (4)
∑Mi jzij ≥ min(θ, len(i))xi; ∀i (5)
∑�i
jrij = xi; ∀i (6)
∑
j �&amp;(i) rij = 0; ∀i (7)
rij ≤ zij; ∀i, j (8)
rij + zparent(i,j) ≤ 1; ∀i, j (9)
riroot(i) = ziroot(i); ∀i (10)
∑jEsub(i) zij ≥ xi; ∀i (11)
∑jEobj(i) zij ≥ xi; ∀i (12)
</equation>
<figureCaption confidence="0.805229">
Figure 3: ILP formulation (xi, zij, rij ∈ {0,1})
</figureCaption>
<bodyText confidence="0.9960455">
EDU in another sentence. Hence, we can deter-
mine the parent-child relations between sentences.
As a result, we obtain a tree that represents the
parent-child relations of sentences, and we can use
it as a document tree. After the document tree is
obtained, we use a dependency parser to obtain the
syntactic dependency trees of sentences. Finally,
we obtain a nested tree.
</bodyText>
<subsectionHeader confidence="0.977627">
3.2 ILP formulation
</subsectionHeader>
<bodyText confidence="0.999983102040817">
Our method generates a summary by trimming a
nested tree. In particular, we extract a rooted docu-
ment subtree from the document tree, and sentence
subtrees from sentence trees in the document tree.
We formulate our problem of optimization in this
section as that of integer linear programming. Our
model is shown in Figure 3.
Let us denote by wij the term weight of word
ij (word j in sentence i). xi is a variable that
is one if sentence i is selected as part of a sum-
mary, and zij is a variable that is one if word ij
is selected as part of a summary. According to the
objective function, the score for the resulting sum-
mary is the sum of the term weights wij that are
included in the summary. We denote by rij the
variable that is one if word ij is selected as a root
of an extracting sentence subtree. Constraint (1)
guarantees that the summary length will be less
than or equal to limit L. Constraints (2) and (3)
are tree constraints for a document tree and sen-
tence trees. rij in Constraint (3) allows the system
to extract non-rooted sentence subtrees, as we pre-
viously mentioned. Function parent(i) returns the
parent of sentence i and function parent(i, j) re-
turns the parent of word ij. Constraint (4) guaran-
tees that words are only selected from a selected
sentence. Constraint (5) guarantees that each se-
lected sentence subtree has at least θ words. Func-
tion len(i) returns the number of words in sentence
i. Constraints (6)-(10) allow the model to extract
subtrees that have an arbitrary root node. Con-
straint (6) guarantees that there is only one root
per selected sentence. We can set the candidate
for the root node of the subtree by using constraint
(7). The R,(i) returns a set of the nodes that are
the candidates of the root nodes in sentence i. It
returned the parser’s root node and the verb nodes
in this study. Constraint (8) maintains consistency
between zij and rij. Constraint (9) prevents the
system from selecting the parent node of the root
node. Constraint (10) guarantees that the parser’s
root node will only be selected when the system
extracts a rooted sentence subtree. The root(i) re-
turns the word index of the parser’s root. Con-
straints (11) and (12) guarantee that the selected
sentence subtree has at least one subject and one
object if it has any. The sub(i) and obj(i) return
the word indices whose dependency tag is “SUB”
and “OBJ”.
</bodyText>
<subsectionHeader confidence="0.997617">
3.3 Additional constraint for grammaticality
</subsectionHeader>
<bodyText confidence="0.990185666666667">
We added two types of constraints to our model
to extract a grammatical sentence subtree from a
dependency tree:
</bodyText>
<equation confidence="0.997747">
zik = zil, (13)
∑ zik = |s(i,j)|xi. (14)
kEs(i,j)
</equation>
<bodyText confidence="0.9693528">
Equation (13) means that words zik and zil have
to be selected together, i.e., a word whose depen-
dency tag is PMOD or VC and its parent word, a
negation and its parent word, a word whose de-
pendency tag is SUB or OBJ and its parent verb,
a comparative (JJR) or superlative (JJS) adjective
and its parent word, an article (a/the) and its par-
ent word, and the word “to” and its parent word.
Equation (14) means that the sequence of words
has to be selected together, i.e., a proper noun se-
quence whose POS tag is PRP$, WP%, or POS
and a possessive word and its parent word and the
words between them. The s(i, j) returns the set of
word indices that are selected together with word
ij.
</bodyText>
<figure confidence="0.855345">
∑n
i
max.
∑Mi
j
wijzij
317
</figure>
<tableCaption confidence="0.790286">
Table 1: ROUGE score of each model. Note that
the top two rows are both our proposals.
</tableCaption>
<table confidence="0.974307142857143">
ROUGE-1
Sentence subtree 0.354
Rooted sentence subtree 0.352
Sentence selection 0.254
EDU selection (Hirao et al., 2013) 0.321
LEADEDU 0.240
LEADsnt 0.157
</table>
<sectionHeader confidence="0.993811" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999131">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999947366666667">
We experimentally evaluated the test collection for
single document summarization contained in the
RST Discourse Treebank (RST-DTB) (Carlson et
al., 2001) distributed by the Linguistic Data Con-
sortium (LDC) 1. The RST-DTB Corpus includes
385 Wall Street Journal articles with RST anno-
tations, and 30 of these documents also have one
manually prepared reference summary. We set the
length constraint, L, as the number of words in
each reference summary. The average length of
the reference summaries corresponded to approxi-
mately 10% of the length of the source document.
This dataset was first used by Marcu et al. for
evaluating a text summarization system (Marcu,
1998). We used ROUGE (Lin, 2004) as an eval-
uation criterion.
We compared our method (sentence subtree)
with that of EDU selection (Hirao et al., 2013).
We examined two other methods, i.e., rooted sen-
tence subtree and sentence selection. These two
are different from our method in the way that they
select a sentence subtree. Rooted sentence subtree
only selects rooted sentence subtrees 2. Sentence
selection does not trim sentence trees. It simply
selects full sentences from a document tree3. We
built all document trees from the RST-DTs that
were annotated in the corpus.
We set the term weight, wij, for our model as:
where tfij is the term frequency of word ij in a
document and depth(i) is the depth of sentence
</bodyText>
<footnote confidence="0.949986">
1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2002T07
2We achieved this by making R,(i) only return the
parser’s root node in Figure 7.
3We achieved this by setting 0 to a very large number.
</footnote>
<bodyText confidence="0.996949666666667">
i within the sentence-level DEP-DT that we de-
scribed in Section 3.1. For Constraint (5), we set
0 to eight.
</bodyText>
<subsectionHeader confidence="0.7685235">
4.2 Results and Discussion
4.2.1 Comparing ROUGE scores
</subsectionHeader>
<bodyText confidence="0.999988571428572">
We have summarized the Recall-Oriented Under-
study for Gisting Evaluation (ROUGE) scores for
each method in Table 1. The score for sentence
selection is low (0.254). However, introducing
sentence compression to the system greatly im-
proved the ROUGE score (0.354). The score is
also higher than that with EDU selection, which
is a state-of-the-art method. We applied a multi-
ple test by using Holm’s method and found that
our method significantly outperformed EDU se-
lection and sentence selection. The difference be-
tween the sentence subtree and the rooted sentence
subtree methods was fairly small. We therefore
qualitatively analyzed some actual examples that
will be discussed in Section 4.2.2. We also exam-
ined the ROUGE scores of two LEAD4 methods
with different textual units: EDUs (LEADEDU)
and sentences (LEADSNT). Although LEAD
works well and often obtains high ROUGE scores
for news articles, the scores for LEADEDU and
LEADSNT were very low.
</bodyText>
<subsectionHeader confidence="0.9716305">
4.2.2 Qualitative Evaluation of Sentence
Subtree Selection
</subsectionHeader>
<bodyText confidence="0.999832705882353">
This subsection compares the methods of subtree
selection and rooted subtree selection. Figure 4
has two example sentences for which both meth-
ods selected a subtree as part of a summary. The
1·1 indicates the parser’s root word. The [·� indi-
cates the word that the system selected as the root
of the subtree. Subtree selection selected a root in
both examples that differed from the parser’s root.
As we can see, subtree selection only selected im-
portant subtrees that did not include the parser’s
root, e.g., purpose-clauses and that-clauses. This
capability is very effective because we have to
contain important content in summaries within
given length limits, especially when the compres-
sion ratio is high (i.e., the method has to gener-
ate much shorter summaries than the source docu-
ments).
</bodyText>
<figure confidence="0.741422526315789">
4LEAD methods simply take the first K textual units from
a source document until the summary length reaches L.
wij =
depth(i)2 , (15)
l0g(1 + tfij)
318
Original sentence John Kriz, a Moody’s vice president, {said} Boston Safe Deposit’s performance has been
hurt this year by a mismatch in the maturities of its assets and liabilities.
Rooted subtree selection John Kriz a Moody’s vice president [{said}] Boston Safe Deposit’s performance has been
hurt this year
Subtree selection Boston Safe Deposit’s performance has [been] hurt this year
Original sentence Recent surveys by Leo J. Shapiro &amp; Associates, a market research firm in Chicago,
S dt
{suggest} that Sears is having a tough time attracting shoppers because its hasn’t yet done
enough to improve service or its selection of merchandise.
Rooted subtree selection surveys [{suggest}] that Sears is having a time
oks
Subtree selection Sears [is] having a tough time attracting shoppers
he
</figure>
<figureCaption confidence="0.997343">
Figure 4: Example sentences and subtrees selected by each method.
</figureCaption>
<note confidence="0.60763">
Te rac is held on next month. The
</note>
<tableCaption confidence="0.949118">
Table 2: Average number of words that individual
extracted textual units contained.
</tableCaption>
<table confidence="0.916322">
Subtree Sentence EDU
15.29 18.96 9.98
</table>
<subsectionHeader confidence="0.981404">
4.2.3 Fragmentation of Information
</subsectionHeader>
<bodyText confidence="0.999982225806451">
Many studies that have utilized RST have simply
adopted EDUs as textual units (Mann and Thomp-
son, 1988; Daum´e III and Marcu, 2002; Hirao et
al., 2013; Knight and Marcu, 2000). While EDUs
are textual units for RST, they are too fine grained
as textual units for methods of extractive summa-
rization. Therefore, the models have tended to se-
lect small fragments from many sentences to max-
imize objective functions and have led to frag-
mented summaries being generated. Figure 2 has
an example of EDUs. A fragmented summary
is generated when small fragments are selected
from many sentences. Hence, the number of sen-
tences in the source document included in the re-
sulting summary can be an indicator to measure
the fragmentation of information. We counted
the number of sentences in the source document
that each method used to generate a summary5.
The average for our method was 4.73 and its me-
dian was four sentences. In contrast, methods
of EDU selection had an average of 5.77 and a
median of five sentences. This meant that our
method generated a summary with a significantly
smaller number of sentences6. In other words, our
method relaxed fragmentation without decreasing
the ROUGE score. There are boxplots of the num-
bers of selected sentences in Figure 5. Table 2 lists
the number of words in each textual unit extracted
by each method. It indicates that EDUs are shorter
than the other textual units. Hence, the number of
sentences tends to be large.
</bodyText>
<footnote confidence="0.566948">
5Note that the number for the EDU method is not equal to
selected textual units because a sentence in the source docu-
ment may contain multiple EDUs.
6We used the Wilcoxon signed-rank test (p &lt; 0.05).
</footnote>
<figure confidence="0.978221133333333">
16
14
12
10
8
6
4
2
0
EDU
DU選択 sentence subtree
提案手法 sentence
文選択 reference
参照要約
selsection selection selection summary
</figure>
<figureCaption confidence="0.9764225">
Figure 5: Number of sentences that each method
selected.
</figureCaption>
<sectionHeader confidence="0.993248" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99999562962963">
We proposed a method of summarizing a sin-
gle document that included relations between sen-
tences and relations between words. We built a
nested tree and formulated the problem of summa-
rization as that of integer linear programming. Our
method significantly improved the ROUGE score
with significantly fewer sentences than the method
of EDU selection. The results suggest that our
method relaxed the fragmentation of information.
We also discussed the effectiveness of sentence
subtree selection that did not restrict rooted sub-
trees. Although ROUGE scores are widely used
as evaluation metrics for text summarization sys-
tems, they cannot take into consideration linguis-
tic qualities such as human readability. Hence, we
plan to conduct evaluations with people7.
We only used the rhetorical structures between
sentences in this study. However, there were also
rhetorical structures between EDUs inside individ-
ual sentences. Hence, utilizing these for sentence
compression has been left for future work. In addi-
tion, we used rhetorical structures that were man-
ually annotated. There have been related studies
on building RST parsers (duVerle and Prendinger,
2009; Hernault et al., 2010) and by using such
parsers, we should be able to apply our model to
other corpora or to multi-document settings.
</bodyText>
<footnote confidence="0.41143325">
7For example, the quality question metric from the Docu-
ment Understanding Conference (DUC).
Number of selected sentences
from source document
</footnote>
<page confidence="0.810386">
319
</page>
<sectionHeader confidence="0.966157" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999770109589042">
Miguel Almeida and Andre Martins. 2013. Fast
and robust compressive summarization with dual de-
composition and multi-task learning. In ACL, pages
196–206, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
ACL, pages 481–490, Portland, Oregon, USA, June.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In SIGDIAL, pages 1–10.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In NAACL:HLT, pages
1163–1173.
Hal Daum´e III and Daniel Marcu. 2002. A noisy-
channel model for document compression. ACL,
pages 449–456.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine
classification. In IJCNLP, pages 665–673.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In COLING.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In INLG,
pages 25–32.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In ILP, pages 10–18.
Hugo Hernault, Helmut Prendinger, David duVerle,
and Mitsuru Ishizuka. 2010. Hilda: A discourse
parser using support vector machine classification.
Dialogue &amp; Discourse, 1(3):1–30.
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,
Norihito Yasuda, and Masaaki Nagata. 2013.
Single-document summarization as a tree knapsack
problem. In EMNLP, pages 1515–1520.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In National Conference on Artificial Intelli-
gence (AAAI), pages 703–710.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out, pages 74–81.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, pages 243–281.
Daniel Marcu. 1998. Improving summarization
through rhetorical parsing tuning. In In Proc. of the
6th Workshop on Very Large Corpora, pages 206–
215.
Ryan T. McDonald. 2007. A study of global infer-
ence algorithms in multi-document summarization.
In ECIR, pages 557–564.
Hajime Morita, Ryohei Sasano, Hiroya Takamura, and
Manabu Okumura. 2013. Subtree extractive sum-
marization via submodular maximization. In ACL,
pages 1023–1032.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-
suo, and Genichiro Kikui. 2010. Opinion summa-
rization with integer linear programming formula-
tion for sentence extraction and ordering. In COL-
ING, pages 910–918.
Xian Qian and Yang Liu. 2013. Fast joint compres-
sion and summarization via graph cuts. In EMNLP,
pages 1492–1502.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on the budgeted median
problem. In CIKM, pages 1589–1592.
Kohei Tomita, Hiroya Takamura, and Manabu Oku-
mura. 2009. A new approach of extractive sum-
marization combining sentence selection and com-
pression. IPSJ SIG Notes, pages 13–20.
</reference>
<page confidence="0.903349">
320
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.381106">
<title confidence="0.882224">Single Document Summarization based on Nested Tree Structure</title>
<affiliation confidence="0.965591">Institute of</affiliation>
<address confidence="0.991562">4295, Nagatsuta, Midori-ku, Yokohama, 226-8503,</address>
<affiliation confidence="0.489727">Communication Science Laboratories, NTT</affiliation>
<address confidence="0.875107">2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237,</address>
<abstract confidence="0.998988666666667">Many methods of text summarization combining sentence selection and sentence compression have recently been proposed. Although the dependency between words has been used in most of these methods, the dependency between sentences, i.e., rhetorical structures, has not been exploited in such joint methods. We used both dependency between words and dependency between sentences by constructing a nested tree, in which nodes in the document tree representing dependency between sentences were replaced by a sentence tree representing dependency between words. We formulated a summarization task as a combinatorial optimization problem, in which the nested tree was trimmed without losing important content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Miguel Almeida</author>
<author>Andre Martins</author>
</authors>
<title>Fast and robust compressive summarization with dual decomposition and multi-task learning.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>196--206</pages>
<contexts>
<context position="2041" citStr="Almeida and Martins, 2013" startWordPosition="286" endWordPosition="289"> extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013). Hirao et al. recently transforme</context>
</contexts>
<marker>Almeida, Martins, 2013</marker>
<rawString>Miguel Almeida and Andre Martins. 2013. Fast and robust compressive summarization with dual decomposition and multi-task learning. In ACL, pages 196–206, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>481--490</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2073" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="290" endWordPosition="293">nt a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013). Hirao et al. recently transformed RST trees into dependency tree</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In ACL, pages 481–490, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory. In</title>
<date>2001</date>
<booktitle>SIGDIAL,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="12553" citStr="Carlson et al., 2001" startWordPosition="2108" endWordPosition="2111">POS and a possessive word and its parent word and the words between them. The s(i, j) returns the set of word indices that are selected together with word ij. ∑n i max. ∑Mi j wijzij 317 Table 1: ROUGE score of each model. Note that the top two rows are both our proposals. ROUGE-1 Sentence subtree 0.354 Rooted sentence subtree 0.352 Sentence selection 0.254 EDU selection (Hirao et al., 2013) 0.321 LEADEDU 0.240 LEADsnt 0.157 4 Experiment 4.1 Experimental Settings We experimentally evaluated the test collection for single document summarization contained in the RST Discourse Treebank (RST-DTB) (Carlson et al., 2001) distributed by the Linguistic Data Consortium (LDC) 1. The RST-DTB Corpus includes 385 Wall Street Journal articles with RST annotations, and 30 of these documents also have one manually prepared reference summary. We set the length constraint, L, as the number of words in each reference summary. The average length of the reference summaries corresponded to approximately 10% of the length of the source document. This dataset was first used by Marcu et al. for evaluating a text summarization system (Marcu, 1998). We used ROUGE (Lin, 2004) as an evaluation criterion. We compared our method (sen</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In SIGDIAL, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janara Christensen</author>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Towards coherent multidocument summarization.</title>
<date>2013</date>
<booktitle>In NAACL:HLT,</booktitle>
<pages>1163--1173</pages>
<contexts>
<context position="6768" citStr="Christensen et al., 2013" startWordPosition="1073" endWordPosition="1076">d their method could not accept large texts because of the high computational cost. In addil - The scare over Alar, a growth regulator - that makes apples redder and crunchier - but may be carcinogenic, - made consumers shy away from the Delicious, - though they were less affected than the McIntosh. Figure 2: Example of one sentence. Each line corresponds to one EDU. � tion, their method required large sets of data to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). �� 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations to build an RST-Discourse Tree (RST-DT) that has a hierarchical structure of the relations. There are 78 types of rhetorical relations between two spans, and each span has one of two aspects of a nucleus and a satellite. The nucleus is more salient to the discourse structure, while the other span, the tree whose terminal nodes correspond selection summa to EDUs and whose nonterminal nodes indicate the relations. Hirao et al</context>
</contexts>
<marker>Christensen, Mausam, Etzioni, 2013</marker>
<rawString>Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2013. Towards coherent multidocument summarization. In NAACL:HLT, pages 1163–1173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisychannel model for document compression.</title>
<date>2002</date>
<pages>449--456</pages>
<publisher>ACL,</publisher>
<marker>Daum´e, Marcu, 2002</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2002. A noisychannel model for document compression. ACL, pages 449–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David duVerle</author>
<author>Helmut Prendinger</author>
</authors>
<title>A novel discourse parser based on support vector machine classification.</title>
<date>2009</date>
<booktitle>In IJCNLP,</booktitle>
<pages>665--673</pages>
<marker>duVerle, Prendinger, 2009</marker>
<rawString>David duVerle and Helmut Prendinger. 2009. A novel discourse parser based on support vector machine classification. In IJCNLP, pages 665–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>A formal model for information selection in multisentence text extraction.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="1763" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="243" endWordPosition="246">rtant content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts. 1 Introduction Extractive summarization is one well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is simil</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. A formal model for information selection in multisentence text extraction. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Dependency tree based sentence compression.</title>
<date>2008</date>
<booktitle>In INLG,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="5626" citStr="Filippova and Strube (2008)" startWordPosition="884" endWordPosition="887">ations between EDUs to the relations between sentences to build the nested tree structure. Weucould thus take into account both relations between sentences and relations between words. � ou 2 Related work 6 Number frm Extracting a subtree from the dependency tree of 4 words is one approach to sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita 2 et al., 2013; Gillick and Favre, 2009). However, these studies have only extracted rooted subtrees 0 from sentences. We allowed our model to extract a subtree that did not inc the sentence with an asterisk * intFigure 1). T method of Filippova and Strube (2008) allows the model to extract non-rooted subtrees in sentence compression tasks that compress a single sentence with a given compression ratio. However, it is not trivial to apply their method to text summarization because no compression ratio is given to sentences. None of these methods use the discourse structures of documents. Daum´e III and Marcu (2002) proposed a noisychannel model that used RST. Although their method generated a well-organized summary, no optimality of information coverage was guaranteed and their method could not accept large texts because of the high computational cost.</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Dependency tree based sentence compression. In INLG, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In ILP,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="2014" citStr="Gillick and Favre, 2009" startWordPosition="282" endWordPosition="285">to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013). Hirao</context>
<context position="5403" citStr="Gillick and Favre, 2009" startWordPosition="846" endWordPosition="849"> (EDUs) in RST are defined as the minimal building blocks of discourse. EDUs roughly correspond to clauses. Most methods of summarization based on RST use EDUs as extraction textual units. We converted the rhetorical relations between EDUs to the relations between sentences to build the nested tree structure. Weucould thus take into account both relations between sentences and relations between words. � ou 2 Related work 6 Number frm Extracting a subtree from the dependency tree of 4 words is one approach to sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita 2 et al., 2013; Gillick and Favre, 2009). However, these studies have only extracted rooted subtrees 0 from sentences. We allowed our model to extract a subtree that did not inc the sentence with an asterisk * intFigure 1). T method of Filippova and Strube (2008) allows the model to extract non-rooted subtrees in sentence compression tasks that compress a single sentence with a given compression ratio. However, it is not trivial to apply their method to text summarization because no compression ratio is given to sentences. None of these methods use the discourse structures of documents. Daum´e III and Marcu (2002) proposed a noisych</context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In ILP, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Helmut Prendinger</author>
<author>David duVerle</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Hilda: A discourse parser using support vector machine classification.</title>
<date>2010</date>
<journal>Dialogue &amp; Discourse,</journal>
<volume>1</volume>
<issue>3</issue>
<marker>Hernault, Prendinger, duVerle, Ishizuka, 2010</marker>
<rawString>Hugo Hernault, Helmut Prendinger, David duVerle, and Mitsuru Ishizuka. 2010. Hilda: A discourse parser using support vector machine classification. Dialogue &amp; Discourse, 1(3):1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsutomu Hirao</author>
<author>Yasuhisa Yoshida</author>
<author>Masaaki Nishino</author>
<author>Norihito Yasuda</author>
<author>Masaaki Nagata</author>
</authors>
<title>Single-document summarization as a tree knapsack problem.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1515--1520</pages>
<contexts>
<context position="2607" citStr="Hirao et al., 2013" startWordPosition="377" endWordPosition="380">3; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013). Hirao et al. recently transformed RST trees into dependency trees and used them for single document summarization (Hirao et al., 2013). They formulated the summarization problem as a tree knapsack problem with constraints represented by the dependency trees. We propose a method of summarizing a single document that utilizes dependency between sentences obtained from rhetorical structures and dependency between words obtained from a dependency parser. We have explained our method with an example in Figure 1. First, we represent a document as a nested tree, which is composed of two types of tr</context>
<context position="7591" citStr="Hirao et al., 2013" startWordPosition="1208" endWordPosition="1211">(RST-DT) that has a hierarchical structure of the relations. There are 78 types of rhetorical relations between two spans, and each span has one of two aspects of a nucleus and a satellite. The nucleus is more salient to the discourse structure, while the other span, the tree whose terminal nodes correspond selection summa to EDUs and whose nonterminal nodes indicate the relations. Hirao et al. converted RST-DTs into dependency-based discourse trees (DEP-DTs) whose nodes corresponded to EDUs and whose edges corresponded to the head modifier relationships of EDUs. See Hirao et al. for details (Hirao et al., 2013). Our model requires sentence-level dependency. Fortunately we can simply convert DEP-DTs to obtain dependency trees between sentences. We specifically merge EDUs that belong to the same sentence. Each sentence has only one root EDU that is the parent of all the other EDUs in the sentence. Each root EDU in a sentence has the parent s n c t d!.-sy st s u do u m ude the root word (See satellite represents supporting information RSThe ection DT is a 316 s.t. ∑i �j izij ≤ L; (1) xparent(i) ≥ xi; ∀i (2) zparent(i,j) − zij + rij ≥ 0; ∀i, j (3) xi ≥ zij; ∀i, j (4) ∑Mi jzij ≥ min(θ, len(i))xi; ∀i (5) </context>
<context position="12325" citStr="Hirao et al., 2013" startWordPosition="2077" endWordPosition="2080">rent word, an article (a/the) and its parent word, and the word “to” and its parent word. Equation (14) means that the sequence of words has to be selected together, i.e., a proper noun sequence whose POS tag is PRP$, WP%, or POS and a possessive word and its parent word and the words between them. The s(i, j) returns the set of word indices that are selected together with word ij. ∑n i max. ∑Mi j wijzij 317 Table 1: ROUGE score of each model. Note that the top two rows are both our proposals. ROUGE-1 Sentence subtree 0.354 Rooted sentence subtree 0.352 Sentence selection 0.254 EDU selection (Hirao et al., 2013) 0.321 LEADEDU 0.240 LEADsnt 0.157 4 Experiment 4.1 Experimental Settings We experimentally evaluated the test collection for single document summarization contained in the RST Discourse Treebank (RST-DTB) (Carlson et al., 2001) distributed by the Linguistic Data Consortium (LDC) 1. The RST-DTB Corpus includes 385 Wall Street Journal articles with RST annotations, and 30 of these documents also have one manually prepared reference summary. We set the length constraint, L, as the number of words in each reference summary. The average length of the reference summaries corresponded to approximate</context>
<context position="17313" citStr="Hirao et al., 2013" startWordPosition="2878" endWordPosition="2881">ne enough to improve service or its selection of merchandise. Rooted subtree selection surveys [{suggest}] that Sears is having a time oks Subtree selection Sears [is] having a tough time attracting shoppers he Figure 4: Example sentences and subtrees selected by each method. Te rac is held on next month. The Table 2: Average number of words that individual extracted textual units contained. Subtree Sentence EDU 15.29 18.96 9.98 4.2.3 Fragmentation of Information Many studies that have utilized RST have simply adopted EDUs as textual units (Mann and Thompson, 1988; Daum´e III and Marcu, 2002; Hirao et al., 2013; Knight and Marcu, 2000). While EDUs are textual units for RST, they are too fine grained as textual units for methods of extractive summarization. Therefore, the models have tended to select small fragments from many sentences to maximize objective functions and have led to fragmented summaries being generated. Figure 2 has an example of EDUs. A fragmented summary is generated when small fragments are selected from many sentences. Hence, the number of sentences in the source document included in the resulting summary can be an indicator to measure the fragmentation of information. We counted</context>
</contexts>
<marker>Hirao, Yoshida, Nishino, Yasuda, Nagata, 2013</marker>
<rawString>Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino, Norihito Yasuda, and Masaaki Nagata. 2013. Single-document summarization as a tree knapsack problem. In EMNLP, pages 1515–1520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization - step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>703--710</pages>
<contexts>
<context position="17338" citStr="Knight and Marcu, 2000" startWordPosition="2882" endWordPosition="2885"> service or its selection of merchandise. Rooted subtree selection surveys [{suggest}] that Sears is having a time oks Subtree selection Sears [is] having a tough time attracting shoppers he Figure 4: Example sentences and subtrees selected by each method. Te rac is held on next month. The Table 2: Average number of words that individual extracted textual units contained. Subtree Sentence EDU 15.29 18.96 9.98 4.2.3 Fragmentation of Information Many studies that have utilized RST have simply adopted EDUs as textual units (Mann and Thompson, 1988; Daum´e III and Marcu, 2002; Hirao et al., 2013; Knight and Marcu, 2000). While EDUs are textual units for RST, they are too fine grained as textual units for methods of extractive summarization. Therefore, the models have tended to select small fragments from many sentences to maximize objective functions and have led to fragmented summaries being generated. Figure 2 has an example of EDUs. A fragmented summary is generated when small fragments are selected from many sentences. Hence, the number of sentences in the source document included in the resulting summary can be an indicator to measure the fragmentation of information. We counted the number of sentences </context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization - step one: Sentence compression. In National Conference on Artificial Intelligence (AAAI), pages 703–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proc. ACL workshop on Text Summarization Branches Out,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="13097" citStr="Lin, 2004" startWordPosition="2200" endWordPosition="2201">ned in the RST Discourse Treebank (RST-DTB) (Carlson et al., 2001) distributed by the Linguistic Data Consortium (LDC) 1. The RST-DTB Corpus includes 385 Wall Street Journal articles with RST annotations, and 30 of these documents also have one manually prepared reference summary. We set the length constraint, L, as the number of words in each reference summary. The average length of the reference summaries corresponded to approximately 10% of the length of the source document. This dataset was first used by Marcu et al. for evaluating a text summarization system (Marcu, 1998). We used ROUGE (Lin, 2004) as an evaluation criterion. We compared our method (sentence subtree) with that of EDU selection (Hirao et al., 2013). We examined two other methods, i.e., rooted sentence subtree and sentence selection. These two are different from our method in the way that they select a sentence subtree. Rooted sentence subtree only selects rooted sentence subtrees 2. Sentence selection does not trim sentence trees. It simply selects full sentences from a document tree3. We built all document trees from the RST-DTs that were annotated in the corpus. We set the term weight, wij, for our model as: where tfij</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proc. ACL workshop on Text Summarization Branches Out, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<journal>Text,</journal>
<pages>243--281</pages>
<contexts>
<context position="2457" citStr="Mann and Thompson, 1988" startWordPosition="350" endWordPosition="353">ntion focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013). Hirao et al. recently transformed RST trees into dependency trees and used them for single document summarization (Hirao et al., 2013). They formulated the summarization problem as a tree knapsack problem with constraints represented by the dependency trees. We propose a method of summarizing a single document that utilizes dependency between sentences obtained from rhetorical structures and dependency between words obtained from a dependency p</context>
<context position="17265" citStr="Mann and Thompson, 1988" startWordPosition="2868" endWordPosition="2872">gh time attracting shoppers because its hasn’t yet done enough to improve service or its selection of merchandise. Rooted subtree selection surveys [{suggest}] that Sears is having a time oks Subtree selection Sears [is] having a tough time attracting shoppers he Figure 4: Example sentences and subtrees selected by each method. Te rac is held on next month. The Table 2: Average number of words that individual extracted textual units contained. Subtree Sentence EDU 15.29 18.96 9.98 4.2.3 Fragmentation of Information Many studies that have utilized RST have simply adopted EDUs as textual units (Mann and Thompson, 1988; Daum´e III and Marcu, 2002; Hirao et al., 2013; Knight and Marcu, 2000). While EDUs are textual units for RST, they are too fine grained as textual units for methods of extractive summarization. Therefore, the models have tended to select small fragments from many sentences to maximize objective functions and have led to fragmented summaries being generated. Figure 2 has an example of EDUs. A fragmented summary is generated when small fragments are selected from many sentences. Hence, the number of sentences in the source document included in the resulting summary can be an indicator to meas</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, pages 243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>Improving summarization through rhetorical parsing tuning. In</title>
<date>1998</date>
<booktitle>In Proc. of the 6th Workshop on Very Large Corpora,</booktitle>
<pages>206--215</pages>
<contexts>
<context position="2558" citStr="Marcu, 1998" startWordPosition="370" endWordPosition="371">9; Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013). Hirao et al. recently transformed RST trees into dependency trees and used them for single document summarization (Hirao et al., 2013). They formulated the summarization problem as a tree knapsack problem with constraints represented by the dependency trees. We propose a method of summarizing a single document that utilizes dependency between sentences obtained from rhetorical structures and dependency between words obtained from a dependency parser. We have explained our method with an example in Figure 1. First, we represent a document as a </context>
<context position="13070" citStr="Marcu, 1998" startWordPosition="2195" endWordPosition="2196">document summarization contained in the RST Discourse Treebank (RST-DTB) (Carlson et al., 2001) distributed by the Linguistic Data Consortium (LDC) 1. The RST-DTB Corpus includes 385 Wall Street Journal articles with RST annotations, and 30 of these documents also have one manually prepared reference summary. We set the length constraint, L, as the number of words in each reference summary. The average length of the reference summaries corresponded to approximately 10% of the length of the source document. This dataset was first used by Marcu et al. for evaluating a text summarization system (Marcu, 1998). We used ROUGE (Lin, 2004) as an evaluation criterion. We compared our method (sentence subtree) with that of EDU selection (Hirao et al., 2013). We examined two other methods, i.e., rooted sentence subtree and sentence selection. These two are different from our method in the way that they select a sentence subtree. Rooted sentence subtree only selects rooted sentence subtrees 2. Sentence selection does not trim sentence trees. It simply selects full sentences from a document tree3. We built all document trees from the RST-DTs that were annotated in the corpus. We set the term weight, wij, f</context>
</contexts>
<marker>Marcu, 1998</marker>
<rawString>Daniel Marcu. 1998. Improving summarization through rhetorical parsing tuning. In In Proc. of the 6th Workshop on Very Large Corpora, pages 206– 215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In ECIR,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="1726" citStr="McDonald, 2007" startWordPosition="240" endWordPosition="242">hout losing important content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts. 1 Introduction Extractive summarization is one well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to hav</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan T. McDonald. 2007. A study of global inference algorithms in multi-document summarization. In ECIR, pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hajime Morita</author>
<author>Ryohei Sasano</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Subtree extractive summarization via submodular maximization.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>1023--1032</pages>
<contexts>
<context position="1989" citStr="Morita et al., 2013" startWordPosition="278" endWordPosition="281"> well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (Marcu, 1998; Daum´e III and Marcu, 2002; H</context>
</contexts>
<marker>Morita, Sasano, Takamura, Okumura, 2013</marker>
<rawString>Hajime Morita, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura. 2013. Subtree extractive summarization via submodular maximization. In ACL, pages 1023–1032.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hitoshi Nishikawa</author>
<author>Takaaki Hasegawa</author>
<author>Yoshihiro Matsuo</author>
<author>Genichiro Kikui</author>
</authors>
<title>Opinion summarization with integer linear programming formulation for sentence extraction and ordering.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>910--918</pages>
<contexts>
<context position="6741" citStr="Nishikawa et al., 2010" startWordPosition="1069" endWordPosition="1072">verage was guaranteed and their method could not accept large texts because of the high computational cost. In addil - The scare over Alar, a growth regulator - that makes apples redder and crunchier - but may be carcinogenic, - made consumers shy away from the Delicious, - though they were less affected than the McIntosh. Figure 2: Example of one sentence. Each line corresponds to one EDU. � tion, their method required large sets of data to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). �� 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations to build an RST-Discourse Tree (RST-DT) that has a hierarchical structure of the relations. There are 78 types of rhetorical relations between two spans, and each span has one of two aspects of a nucleus and a satellite. The nucleus is more salient to the discourse structure, while the other span, the tree whose terminal nodes correspond selection summa to EDUs and whose nonterminal nodes indicate</context>
</contexts>
<marker>Nishikawa, Hasegawa, Matsuo, Kikui, 2010</marker>
<rawString>Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo, and Genichiro Kikui. 2010. Opinion summarization with integer linear programming formulation for sentence extraction and ordering. In COLING, pages 910–918.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Fast joint compression and summarization via graph cuts.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1492--1502</pages>
<contexts>
<context position="1968" citStr="Qian and Liu, 2013" startWordPosition="274" endWordPosition="277">summarization is one well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (Marcu, 1998; Daum´e I</context>
<context position="5354" citStr="Qian and Liu, 2013" startWordPosition="837" endWordPosition="840">e sentence tree. Elementary Discourse Units (EDUs) in RST are defined as the minimal building blocks of discourse. EDUs roughly correspond to clauses. Most methods of summarization based on RST use EDUs as extraction textual units. We converted the rhetorical relations between EDUs to the relations between sentences to build the nested tree structure. Weucould thus take into account both relations between sentences and relations between words. � ou 2 Related work 6 Number frm Extracting a subtree from the dependency tree of 4 words is one approach to sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita 2 et al., 2013; Gillick and Favre, 2009). However, these studies have only extracted rooted subtrees 0 from sentences. We allowed our model to extract a subtree that did not inc the sentence with an asterisk * intFigure 1). T method of Filippova and Strube (2008) allows the model to extract non-rooted subtrees in sentence compression tasks that compress a single sentence with a given compression ratio. However, it is not trivial to apply their method to text summarization because no compression ratio is given to sentences. None of these methods use the discourse structures of document</context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>Xian Qian and Yang Liu. 2013. Fast joint compression and summarization via graph cuts. In EMNLP, pages 1492–1502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Text summarization model based on the budgeted median problem.</title>
<date>2009</date>
<booktitle>In CIKM,</booktitle>
<pages>1589--1592</pages>
<contexts>
<context position="1792" citStr="Takamura and Okumura, 2009" startWordPosition="247" endWordPosition="250"> The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts. 1 Introduction Extractive summarization is one well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source docu</context>
</contexts>
<marker>Takamura, Okumura, 2009</marker>
<rawString>Hiroya Takamura and Manabu Okumura. 2009. Text summarization model based on the budgeted median problem. In CIKM, pages 1589–1592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kohei Tomita</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>A new approach of extractive summarization combining sentence selection and compression.</title>
<date>2009</date>
<journal>IPSJ SIG Notes,</journal>
<pages>13--20</pages>
<contexts>
<context position="1948" citStr="Tomita et al., 2009" startWordPosition="270" endWordPosition="273">roduction Extractive summarization is one well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (M</context>
<context position="5334" citStr="Tomita et al., 2009" startWordPosition="833" endWordPosition="836">itrary subtrees of the sentence tree. Elementary Discourse Units (EDUs) in RST are defined as the minimal building blocks of discourse. EDUs roughly correspond to clauses. Most methods of summarization based on RST use EDUs as extraction textual units. We converted the rhetorical relations between EDUs to the relations between sentences to build the nested tree structure. Weucould thus take into account both relations between sentences and relations between words. � ou 2 Related work 6 Number frm Extracting a subtree from the dependency tree of 4 words is one approach to sentence compression (Tomita et al., 2009; Qian and Liu, 2013; Morita 2 et al., 2013; Gillick and Favre, 2009). However, these studies have only extracted rooted subtrees 0 from sentences. We allowed our model to extract a subtree that did not inc the sentence with an asterisk * intFigure 1). T method of Filippova and Strube (2008) allows the model to extract non-rooted subtrees in sentence compression tasks that compress a single sentence with a given compression ratio. However, it is not trivial to apply their method to text summarization because no compression ratio is given to sentences. None of these methods use the discourse st</context>
</contexts>
<marker>Tomita, Takamura, Okumura, 2009</marker>
<rawString>Kohei Tomita, Hiroya Takamura, and Manabu Okumura. 2009. A new approach of extractive summarization combining sentence selection and compression. IPSJ SIG Notes, pages 13–20.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>