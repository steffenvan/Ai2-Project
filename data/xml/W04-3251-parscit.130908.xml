<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000033">
<title confidence="0.997694">
Instance-Based Question Answering:
A Data-Driven Approach
</title>
<author confidence="0.998505">
Lucian Vlad Lita Jaime Carbonell
</author>
<affiliation confidence="0.998054">
Carnegie Mellon University Carnegie Mellon University
</affiliation>
<email confidence="0.997364">
llita@cs.cmu.edu jgc@cs.cmu.edu
</email>
<sectionHeader confidence="0.997364" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998932">
Anticipating the availability of large question-
answer datasets, we propose a principled, data-
driven Instance-Based approach to Question An-
swering. Most question answering systems incor-
porate three major steps: classify questions accord-
ing to answer types, formulate queries for document
retrieval, and extract actual answers. Under our ap-
proach, strategies for answering new questions are
directly learned from training data. We learn mod-
els of answer type, query content, and answer ex-
traction from clusters of similar questions. We view
the answer type as a distribution, rather than a class
in an ontology. In addition to query expansion, we
learn general content features from training data and
use them to enhance the queries. Finally, we treat
answer extraction as a binary classification problem
in which text snippets are labeled as correct or in-
correct answers. We present a basic implementation
of these concepts that achieves a good performance
on TREC test data.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940113636364">
Ever since Question Answering (QA) emerged as
an active research field, the community has slowly
diversified question types, increased question com-
plexity, and refined evaluation metrics - as reflected
by the TREC QA track (Voorhees, 2003). Starting
from successful pipeline architectures (Moldovan et
al., 2000; Hovy et al., 2000), QA systems have re-
sponded to changes in the nature of the QA task by
incorporating knowledge resources (Hermjakob et
al., 2000; Hovy et al., 2002), handling additional
types of questions, employing complex reasoning
mechanisms (Moldovan et al., 2003; Nyberg et al.,
2003), tapping into external data sources such as
the Web, encyclopedias, databases (Dumais et al.,
2002; Xu et al., 2003), and merging multiple agents
and strategies into meta-systems (Chu-Carroll et al.,
2003; Burger et al., 2002).
In recent years, learning components have started
to permeate Question Answering (Clarke et al.,
2003; Ravichandran et al., 2003; Echihabi and
Marcu, 2003). Although the field is still domi-
nated by knowledge-intensive approaches, compo-
nents such as question classification, answer extrac-
tion, and answer verification are beginning to be ad-
dressed through statistical methods. At the same
time, research efforts in data acquisition promise to
deliver increasingly larger question-answer datasets
(Girju et al., 2003; Fleischman et al., 2003). More-
over, Question Answering is expanding to different
languages (Magnini et al., 2003) and domains other
than news stories (Zweigenbaum, 2003). These
trends suggest the need for principled, statisti-
cally based, easily re-trainable, language indepen-
dent QA systems that take full advantage of large
amounts of training data.
We propose an instance-based, data-driven ap-
proach to Question Answering. Instead of classify-
ing questions according to limited, predefined on-
tologies, we allow training data to shape the strate-
gies for answering new questions. Answer mod-
els, query content models, and extraction models are
also learned directly from training data. We present
a basic implementation of these concepts and eval-
uate the performance.
</bodyText>
<sectionHeader confidence="0.995519" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999460833333333">
Most existing Question Answering systems classify
new questions according to static ontologies. These
ontologies incorporate human knowledge about the
expected answer (e.g. date, location, person), an-
swer type granularity (e.g. date, year, century), and
very often semantic information about the question
type (e.g. birth date, discovery date, death date).
While effective to some degree, these ontologies
are still very small, and inconsistent. Considerable
manual effort is invested into building and maintain-
ing accurate ontologies even though answer types
are arguably not always disjoint and hierarchical in
nature (e.g. “Where is the corpus callosum?” ex-
pects an answer that is both location and body part).
The most significant drawback is that ontologies
are not standard among systems, making individual
component evaluation very difficult and re-training
for new domains time-consuming.
</bodyText>
<subsectionHeader confidence="0.960163">
2.1 Answer Modeling
</subsectionHeader>
<bodyText confidence="0.999899066666667">
The task of determining the answer type of a ques-
tion is usually considered a hard 1 decision prob-
lem: questions are classified according to an an-
swer ontology. The classification (location, per-
son’s name, etc) is usually made in the beginning
of the QA process and all subsequent efforts are
focused on finding answers of that particular type.
Several existing QA systems implement feedback
loops (Harabagiu et al., 2000) or full-fledged plan-
ning (Nyberg et al., 2003) to allow for potential an-
swer type re-classification.
However, most questions can have multiple an-
swer types as well as specific answer type distribu-
tions. The following questions can accommodate
answers of types: full date, year, and decade.
</bodyText>
<table confidence="0.43965725">
Question Answer
When did Glen lift off in Friendship7? Feb. 20, 1962
When did Glen join NASA? 1959
When did Glen have long hair? the fifties
</table>
<bodyText confidence="0.999804090909091">
However, it can be argued that date is the most
likely answer type to be observed for the first ques-
tion, year the most likely type for the second ques-
tion, and decade most likely for the third ques-
tion. In fact, although the three questions can be
answered by various temporal expressions, the dis-
tributions over these expressions are quite different.
Existing answer models do not usually account for
these distributions, even though there is a clear po-
tential for better answer extraction and more refined
answer scoring.
</bodyText>
<subsectionHeader confidence="0.99967">
2.2 Document Retrieval
</subsectionHeader>
<bodyText confidence="0.9483781875">
When faced with a new question, QA systems usu-
ally generate few, carefully expanded queries which
produce ranked lists of documents. The retrieval
step, which is very critical in the QA process,
does not take full advantage of context information.
However, similar questions with known answers do
share context information in the form of lexical and
structural features present in relevant documents.
For example all questions of the type “When was
X born?” find their answers in documents which
often contain words such as “native” or “record”,
phrases such as “gave birth to X”, and sometimes
even specific parse trees.
Most IR research in Question Answering is fo-
cused on improving query expansion and structur-
&apos;the answer is classified into a single class instead of gener-
ating a probability distribution over answers
ing queries in order to take advantage of specific
document pre-processing. In addition to automatic
query expansion for QA (Yang et al., 2003), queries
are optimized to take advantage of expansion re-
sources and document sources. Very often, these
optimizations are performed offline, based on the
type of question being asked.
Several QA systems associate this type of infor-
mation with question ontologies: upon observing
questions of a certain type, specific lexical features
are sought in the retrieved documents. These fea-
tures are not always automatically learned in order
to be used in query generation. Moreover, systems
are highly dependent on specific ontologies and be-
come harder to re-train.
</bodyText>
<subsectionHeader confidence="0.99862">
2.3 Answer Extraction
</subsectionHeader>
<bodyText confidence="0.999847173913043">
Given a set of relevant documents, the answer ex-
traction step consists of identifying snippets of text
or exact phrases that answer the question. Manual
approaches to answer extraction have been mod-
erately successful in the news domain. Regular
expressions, rule and pattern-based extraction are
among the most efficient techniques for information
extraction. However, because of the difficulty in ex-
tending them to additional types of questions, learn-
ing methods are becoming more prevalent.
Current systems (Ravichandran et al., 2003) al-
ready employ traditional information extraction and
machine learning for extracting answers from rel-
evant documents. Boundary detection techniques,
finite state transducers, and text passage classifica-
tion are a few methods that are usually applied to
this task.
The drawback shared by most statistical answer
extractors is their reliance on predefined ontologies.
They are often tailored to expected answer types and
require type-specific resources. Gazetteers, ency-
clopedias, and other resources are used to generate
type specific features.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.998522">
Current efforts in data acquisition for Question An-
swering are becoming more and more common.
(Girju et al., 2003) propose a supervised algorithm
for part-whole relations extraction. (Fleischman et
al., 2003) also propose a supervised algorithm that
uses part of speech patterns and a large corpus to ex-
tract semantic relations for Who-is type questions.
Such efforts promise to provide large and dense
datasets required by instance based approaches.
Several statistical approaches have proven to be
successful in answer extraction. The statistical
agent presented in (Chu-Carroll et al., 2003) uses
</bodyText>
<figure confidence="0.968588818181818">
Test Question: When did John Glen start working at NASA?
When did &lt;NNP+&gt; &lt;VB&gt; ... at ... ?
When did Jay Leno get a job at the NBC?
When did Columbus arrive at his destination?
...
When did Beethoven die?
When did Muhammad live?
...
When did &lt;NNP&gt; &lt;VB&gt; ?
When did &lt;NNP+&gt; ... ?
When did the Raiders win their last game?
When did EMNLP celebrate its 5th anniversary?
...
When did &lt;NNP+&gt; &lt;SYNSET to_initiate&gt; ... ?
When did Sony begin its VAIO campaign?
When did Tom Ridge initiate the terror alert system?
...
When did &lt;NP&gt; ... ?
When did dinosaurs walk the earth?
When did people discoverfire?
...
When did &lt;NN&gt; &lt;VB&gt; &lt;NP&gt; ?
</figure>
<figureCaption confidence="0.999949">
Figure 1: Neighboring questions are clustered according to features they share.
</figureCaption>
<bodyText confidence="0.999908083333333">
maximum entropy and models answer correctness
by introducing a hidden variable representing the
expected answer type. Large corpora such as the
Web can be mined for simple patterns (Ravichan-
dran et al., 2003) corresponding to individual ques-
tion types. These patterns are then applied to test
questions in order to extract answers. Other meth-
ods rely solely on answer redundancy (Dumais et
al., 2002): high performance retrieval engines and
large corpora contribute to the fact that the most re-
dundant entity is very often the correct answer.
Predictive annotation (Prager et al., 1999) is one
of the techniques that bring together corpus process-
ing and smarter queries. Twenty classes of objects
are identified and annotated in the corpus, and cor-
responding labels are used to enhance IR queries.
Along the same lines, (Agichtein et al., 2001) pro-
pose a method for learning query transformations
in order to improve answer retrieval. The method
involves learning phrase features for question clas-
sification. (Wen and Zhang, 2003) address the prob-
lem of query clustering based on semantic similar-
ity and analyze several applications such as query
re-formulation and index-term selection.
</bodyText>
<sectionHeader confidence="0.926922" genericHeader="method">
4 An Instance-Based Approach
</sectionHeader>
<bodyText confidence="0.999878307692308">
This paper presents a data driven, instance-based
approach for Question Answering. We adopt the
view that strategies required in answering new ques-
tions can be directly learned from similar train-
ing examples (question-answer pairs). Consider
a multi-dimensional space, determined by features
extracted from training data. Each training question
is represented as a data point in this space. Features
can range from lexical n-grams to parse trees ele-
ments, depending on available processing.
Each test question is also projected onto the fea-
ture space. Its neighborhood consists of training
instances that share a number of features with the
new data point. Intuitively, each neighbor is similar
in some fashion to the new question. The obvious
next step would be to learn from the entire neigh-
borhood - similar to KNN classification. However,
due to the sparsity of the data and because different
groups of neighbors capture different aspects of the
test question, we choose to cluster the neighborhood
instead. Inside the neighborhood, we build individ-
ual clusters based on internal similarity. Figure 1
shows an example of neighborhood clustering. No-
tice that clusters may also have different granularity
- i.e. can share more or less features with the new
question.
</bodyText>
<figureCaption confidence="0.892788">
Figure 2: The new question is projected onto the multi-
</figureCaption>
<bodyText confidence="0.933005214285714">
dimensional feature space. A set of neighborhood clus-
ters are identified and a model is dynamically built for
each of them. Each model is applied to the test question
in order to produce its own set of candidate answers.
By clustering the neighborhood, we set the stage
for supervised methods, provided the clusters are
sufficiently dense. The goal is to learn models that
explain individual clusters. A model explains the
data if it successfully answers questions from its
corresponding cluster. For each cluster, a mod-
els is constructed and tailored to the local data.
Models generating high confidence answers are ap-
plied to the new question to produce answer candi-
dates (Figure 2) Since the test question belongs to
</bodyText>
<figure confidence="0.995907380952381">
Cluster2
Models
AnswerSet 2
AnswerSet 1
Clusterl
Models
Neighborhood
NE Tagging
POS
New Question
Clusterl
Cluster,
Cluster2
Clusterk
Parsing
Cluster3
Models
AnswerSet 3
AnswerSet k
Clusterk
Models
</figure>
<bodyText confidence="0.9975778">
multiple clusters, it benefits from different answer-
seeking strategies and different granularities.
Answering clusters of similar questions involves
several steps: learning the distribution of the
expected answer type, learning the structure and
content of queries, and learning how to extract the
answer. Although present in most systems, these
steps are often static, manually defined, or based on
limited resources (section 2). This paper proposes a
set of trainable, cluster-specific models:
</bodyText>
<listItem confidence="0.9429668">
1. the Answer Model Ai learns the cluster-specific
distribution of answer types.
2. the Query Content Model Ui is trained to enhance
the keyword-based queries with cluster-specific
content conducive to better document retrieval.
This model is orthogonal to query expansion.
3. the Extraction Model Ei is dynamically built
for answer candidate extraction, by classifying
snippets of text whether they contain a correct
answer or not.
</listItem>
<figureCaption confidence="0.800477">
Figure 3: Three cluster-specific components are learned
in order to better retrieve relevant documents, model the
expected answer, and then extract it from raw text. Local
question-answer pairs (Q,A) are used as training data.
</figureCaption>
<bodyText confidence="0.999918333333333">
These models are derived directly from cluster
data and collectively define a focused strategy for
finding answers to similar questions (Figure 3).
</bodyText>
<subsectionHeader confidence="0.987617">
4.1 The Answer Model
</subsectionHeader>
<bodyText confidence="0.99995996">
Learning cluster-specific answer type distributions
is useful not only in terms of identifying answers
in running text but also in answer ranking. A prob-
abilistic approach has the advantage of postponing
answer type decisions from early in the QA process
until answer extraction or answer ranking. It also
has the advantage of allowing training data to shape
the expected structure of answers.
The answer modeling task consists of learning
specific answer type distributions for each cluster of
questions. Provided enough data, simple techniques
such as constructing finite state machines or learn-
ing regular expressions are sufficient. The principle
can also be applied to current answer ontologies by
replacing the hard classification with a distribution
over answer types.
For high-density clusters, the problem of learn-
ing the expected answer type is reduced to learn-
ing possible answer types and performing a reliable
frequency count. However, very often clusters are
sparse (e.g. are based on rare features) and a more
reliable method is required. k-nearest training data
points Q1..Qk can be used in order to estimate the
probability that the test question q will observe an
answer type aj:
</bodyText>
<equation confidence="0.997378">
k
P(aj,q) = µ · E P(aj|Qi) · S(q, Qi) (1)
i=0
</equation>
<bodyText confidence="0.9993774">
where P(aj, Qi) is the probability of observing
an answer of type aj when asking question Qi.
S(q, Qi) represents a distance function between q
and Qi, and µ is a normalizing factor over the set
of all viable answer types in the neighborhood of q.
</bodyText>
<subsectionHeader confidence="0.954168">
4.2 The Query Content Model
</subsectionHeader>
<bodyText confidence="0.998300722222222">
Current Question Answering systems use IR in a
straight-forward fashion. Query terms are extracted
and then expanded using statistical and semantic
similarity measures. Documents are retrieved and
the top K are further processed. This approach de-
scribes the traditional IR task and does not take ad-
vantage of specific constraints, requirements, and
rich context available in the QA process.
The data-driven framework we propose takes ad-
vantage of knowledge available at retrieval time
and incorporates it to create better cluster-specific
queries. In addition to query expansion, the goal is
to learn content features: n-grams and paraphrases
(Hermjakob et al., 2002) which yield better queries
when added to simple keyword-based queries. The
Query Content Model is a cluster-specific collec-
tion of content features that generate the best docu-
ment set (Table 1).
</bodyText>
<table confidence="0.876220125">
Cluster: When did X start working for Y?
Simple Queries Query Content Model
`X joined Y in”
`X started working for Y”
`X was hired by Y”
`Y hired X”
X, Y `job interview”
...
</table>
<tableCaption confidence="0.98618575">
Table 1: Queries based only on X and Y question
terms may not be appropriate if the two entities share
a long history. A focused, cluster-specific content model
is likely to generate more precise queries.
</tableCaption>
<bodyText confidence="0.831980333333333">
For training, simple keyword-based queries are
run through a retrieval engine in order to produce
a set of potentially relevant documents. Features
</bodyText>
<figure confidence="0.46723">
Cluster Models
Training
Samples
(Q, A)
Query Content Model
Extraction Model
Answer Model
X, Y
X, Y start working
X, Y `start working”
...
</figure>
<bodyText confidence="0.9970552">
(n-grams and paraphrases) are extracted and scored
based on their co-occurrence with the correct an-
swer. More specifically, consider a positive class:
documents which contain the correct answer, and a
negative class: documents which do not contain the
answer. We compute the average mutual informa-
tion I(C; FZ) between a class of a document, and
the absence or presence of a feature fZ in the doc-
ument (McCallum and Nigam, 1998). We let C be
the class variable and FZ the feature variable:
</bodyText>
<equation confidence="0.99919025">
I(C; FZ) = H(C) − H(C|FZ)
P(c, fZ)
P(c, fZ) log
P(c)P(fZ)
</equation>
<bodyText confidence="0.99991592">
where H(C) is the entropy of the class variable and
H(C|FZ) is the entropy of the class variable condi-
tioned on the feature variable. Features that best dis-
criminate passages containing correct answers from
those that do not, are selected as potential candi-
dates for enhancing keyword-based queries.
For each question-answer pair, we generate can-
didate queries by individually adding selected fea-
tures (e.g. table 1) to the expanded word-based
query. The resulting candidate queries are subse-
quently run through a retrieval engine and scored
based on the number of passages containing cor-
rect answers (precision). The content features found
in the top u candidate queries are included in the
Query Content Model.
The Content Model is cluster specific and not in-
stance specific. It does not replace traditional query
expansion - both methods can be applied simulta-
neously to the test questions: specific keywords are
the basis for traditional query expansion and clus-
ters of similar questions are the basis for learning
additional content conducive to better document re-
trieval. Through the Query Content Model we al-
low shared context to play a more significant role in
query generation.
</bodyText>
<subsectionHeader confidence="0.994075">
4.3 The Extraction Model
</subsectionHeader>
<bodyText confidence="0.999958727272727">
During training, documents are retrieved for each
question cluster and a set of one-sentence passages
containing a minimum number of query terms is
selected. The passages are then transformed into
feature vectors to be used for classification. The
features consist of n-grams, paraphrases, distances
between keywords and potential answers, simple
statistics such as document and sentence length, part
of speech features such as required verbs etc. More
extensive sets of features can be found in informa-
tion extraction literature (Bikel et al., 1999).
Under our data-driven approach, answer extrac-
tion consists of deciding the correctness of candi-
date passages. The task is to build a model that
accepts snippets of text and decides whether they
contain a correct answer.
A classifier is trained for each question cluster.
When new question instances arrive, the already
trained cluster-specific models are applied to new,
relevant text snippets in order to test for correctness.
We will refer to the resulting classifier scores as an-
swer confidence scores.
</bodyText>
<sectionHeader confidence="0.999608" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999880142857143">
We present a basic implementation of the instance-
based approach. The resulting QA system is fully
automatically trained, without human intervention.
Instance-based approaches are known to require
large, dense training datasets which are currently
under development. Although still sparse, the
subset of all temporal questions from the TREC
9-12 (Voorhees, 2003) datasets is relatively dense
compared to the rest of the question space. This
makes it a good candidate for evaluating our
instance-based QA approach until larger and denser
datasets become available. It is also broad enough
to include different question structures and varying
degrees of difficulty and complexity such as:
</bodyText>
<listItem confidence="0.99799625">
• “When did Beethoven die?”
• “How long is a quarter in an NBA game?”
• “What year did General Montgomery lead the Allies
to a victory over the Axis troops in North Africa?”
</listItem>
<bodyText confidence="0.866120666666667">
The 296 temporal questions and their correspond-
ing answer patterns provided by NIST were used
in our experiments. The questions were processed
with a part of speech tagger (Brill, 1994) and a
parser (Collins, 1999).
The questions were clustered using template-
style frames that incorporate lexical items, parser
labels, and surface form flags (Figure 1). Consider
the following question and several of its corre-
sponding frames:
“When did Beethoven die?”
when did &lt;NNP&gt; die
when did &lt;NNP&gt; &lt;VB&gt;
when did &lt;NNP&gt; &lt;Q&gt;
when did &lt;NP&gt; &lt;Q&gt;
when did &lt;Q&gt;
where &lt;NNP&gt;,&lt;NP&gt;,&lt;VB&gt;,&lt;Q&gt; denote:
proper noun, noun phrase, verb, and generic ques-
tion term sequence, respectively. Initially, frames
are generated exhaustively for each question. Each
frame that applies to more than three questions is
then selected to represent a specific cluster.
One hundred documents were retrieved
for each query through the Google API
</bodyText>
<equation confidence="0.777381666666667">
�= E
CEC fiE0,1
(www.google.com/api). Documents containing
</equation>
<bodyText confidence="0.997751486486486">
the full question, question number, references to
TREC, NIST, AQUAINT, Question Answering and
other similar problematic content were filtered out.
When building the Query Content Model
keyword-based queries were initially formulated
and expanded. From the retrieved documents a set
of content features (n-grams and paraphrases) were
selected through average mutual information. The
features were added to the simple queries and a
new set of documents was retrieved. The enhanced
queries were scored and the corresponding top 10 n-
grams/paraphrases were included in the Query Con-
tent Model. The maximum n-gram and paraphrase
size for these features was set to 6 words.
The Extraction Model uses a support vector ma-
chine (SVM) classifier (Joachims, 2002) with a lin-
ear kernel. The task of the classifier is to decide if
text snippets contain a correct answer. The SVM
was trained on features extracted from one-sentence
passages containing at least one keyword from the
original question. The features consist of: distance
between keywords and potential answers, keyword
density in a passage, simple statistics such as doc-
ument and sentence length, query type, lexical n-
grams (up to 6-grams), and paraphrases.
We performed experiments using leave-one-out
cross validation. The system was trained and tested
without any question filtering or manual input. Each
cluster produced an answer set with correspond-
ing scores. Top 5 answers for each instance were
considered by a mean reciprocal rank (MRR) met-
ric over all N questions: MRR i i
q N = EN ranki
where ranki refers to the first correct occurrence in
the top 5 answers for question i. While not the fo-
cus of this paper, answer clustering algorithms are
likely to further improve performance.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9999514">
The most important step in our instance-based ap-
proach is identifying clusters of questions. Figure
4 shows the question distribution in terms of num-
ber of clusters. For example: 30 questions belong
to exactly 3 clusters. The number of clusters cor-
responding to a question can be seen as a measure
of how common the question is - the more clusters
a question has, the more likely it is to have a dense
neighborhood.
The resulting MRR is 0.447 and 61.5% ques-
tions have correct answers among the first five pro-
posed answers. This translates into results consis-
tently above the sixth highest score at each TREC
9-12. Our results were compared directly to the top
performing systems’ results on the same temporal
</bodyText>
<subsectionHeader confidence="0.780156">
Question Distribution With Number of Clusters
</subsectionHeader>
<figure confidence="0.581668">
# clusters (avg)
</figure>
<figureCaption confidence="0.9868675">
Figure 4: Question distribution - each bar shows the
number of questions that belong to exactly c clusters.
</figureCaption>
<figure confidence="0.90109">
# clusters
</figure>
<figureCaption confidence="0.992119666666667">
Figure 5: Number of clusters that contribute with cor-
rect answers to the final answer set - only the top 10 an-
swers were considered for each question.
</figureCaption>
<bodyText confidence="0.9984255">
question test set.
Figure 5 shows the degree to which clusters pro-
duce correct answers to test questions. Very often,
more than one cluster contributes to the final answer
set, which suggests that there is a benefit in cluster-
ing the neighborhood according to different similar-
ity features and granularity.
It is not surprising that cluster size is not cor-
related with performance (Figure 6). The overall
strategy learned from the cluster “When did &lt;NP&gt;
die?” corresponds to an MRR of 0.79, while the
strategy learned from cluster “How &lt;Q&gt;?” corre-
sponds to an MRR of 0.13. Even if the two clusters
generate strategies with radically different perfor-
mance, they have the same size - 10 questions are
covered by each cluster.
</bodyText>
<figureCaption confidence="0.899583833333333">
Figure 7 shows that performance is correlated
with answer confidence scores. The higher the con-
fidence threshold the higher the precision (MRR)
of the predicted answers. When small, unstable
clusters are ignored, the predicted MRR improves
considerably. Small clusters tend to produce unsta-
</figureCaption>
<figure confidence="0.999402083333333">
# questions
80
70
60
50
40
30
20
10
0
2 3 4 5 6 7 8 9 larger
# questions
80
70
60
50
40
30
20
10
0
1 2 3 4 5 6 7 8
Cluster Contribution to Top 10 Answers
1 Performance And Cluster Size Cluster: When did &lt;QTERM&gt; die?
</figure>
<figureCaption confidence="0.98957425">
Figure 6: Since training data is not uniformly distributed
in the feature space, cluster size is not well correlated
with performance. A specific cardinality may represent a
small and dense part cluster, or a large and sparse cluster.
</figureCaption>
<figure confidence="0.9175695">
Performance And Confidence Thresholds
confidence threshold
</figure>
<figureCaption confidence="0.994845">
Figure 7: MRR of predicted answers varies with answer
confidence thresholds. There is a tradeoffbetween confi-
dence threshold and MRR . The curves represent differ-
ent thresholds for minimum cluster size.
</figureCaption>
<bodyText confidence="0.999881789473684">
ble strategies and have extremely low performance.
Often times structurally different but semantically
equivalent clusters have a higher cardinality and
much better performance. For example, the cluster
“What year did &lt;NP&gt; die?” has cardinality 2 and
a corresponding MRR of zero. However, as seen
previously, the cluster “When did &lt;NP&gt; die?” has
cardinality 10 and a corresponding MRR of 0.79.
Table 2 presents an intuitive cluster and the top n-
grams and paraphrases with most information con-
tent. Each feature has also a corresponding average
mutual information score. These particular content
features are intuitive and highly indicative of a cor-
rect answer. However, in sparse clusters, the con-
tent features have less information content and are
more vague. For example, the very sparse cluster
“When was &lt;Q&gt;?” yields content features such as
“April”, “May”, “in the spring of”, “back in” which
only suggest broad temporal expressions.
</bodyText>
<subsubsectionHeader confidence="0.464112">
N-grams Paraphrases
</subsubsectionHeader>
<table confidence="0.982841444444444">
0.81 his death in 0.80 &lt;Q&gt; died in
0.78 died on 0.78 &lt;Q&gt; died
0.77 died in 0.68 &lt;Q&gt; died on
0.75 death in 0.58 &lt;Q&gt; died at
0.73 of death 0.38 &lt;Q&gt; , who died
0.69 to his death 0.38 &lt;Q&gt; dies
0.66 died 0.38 &lt;Q&gt; died at the age of
0.63 , born on 0.38 &lt;Q&gt; , born
0.63 date of death 0.35 &lt;Q&gt; ’s death on
</table>
<tableCaption confidence="0.984880333333333">
Table 2: Query Content Model: learning n-grams and
paraphrases for class “When did &lt;NP&gt; die?”, where
&lt;Q&gt; refers to a phrase in the original question.
</tableCaption>
<sectionHeader confidence="0.99915" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99999475">
This paper presents an principled, statistically
based, instance-based approach to Question An-
swering. Strategies and models required for answer-
ing new questions are directly learned from training
data. Since training requires very little human ef-
fort, relevant context, high information query con-
tent, and extraction are constantly improved with
the addition of more question-answer pairs.
Training data is a critical resource for this ap-
proach - clusters with very few data points are not
likely to generate accurate models. However, re-
search efforts involving data acquisition are promis-
ing to deliver larger datasets in the near future and
solve this problem. We present an implementation
of the instance-based QA approach and we eval-
uate it on temporal questions. The dataset is of
reasonable size and complexity, and is sufficiently
dense for applying instance-based methods. We per-
formed leave-one-out cross validation experiments
and obtained an overall mean reciprocal rank of
0.447. 61.5% of questions obtained correct answers
among the top five which is equivalent to a score in
the top six TREC systems on the same test set.
The experiments show that strategies derived
from very small clusters are noisy and unstable.
When larger clusters are involved, answer confi-
dence becomes correlated with higher predictive
performance. Moreover, when ignoring sparse data,
answering strategies tend to be more stable. This
supports the need for more training data as means to
improve the overall performance of the data driven,
instance based approach to question answering.
</bodyText>
<sectionHeader confidence="0.993524" genericHeader="acknowledgments">
8 Current &amp; Future Work
</sectionHeader>
<bodyText confidence="0.992368">
Data is the single most important resource for
instance-based approaches. Currently we are ex-
ploring large-scale data acquisition methods that
can provide the necessary training data density for
</bodyText>
<figure confidence="0.998362857142857">
1
0.9
0.8
0.7
Cerdinelity 2+
Cerdinelity 3+
Cerdinelity 4+
Cerdinelity S+
0.5
0.4
0.3
0.2
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
MRR
0.6
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 20 40 60 80 100 120
cluster size
MRR
</figure>
<bodyText confidence="0.9991455">
most question types, as well as the use of trivia
questions in the training process.
Our data-driven approach to Question Answering
has the advantage of incorporating learning com-
ponents. It is very easy to train and makes use of
very few resources. This property suggests that lit-
tle effort is required to re-train the system for dif-
ferent domains as well as other languages. We plan
to apply instance-based QA to European languages
and test this hypothesis using training data acquired
through unsupervised means.
More effort is required in order to better integrate
the cluster-specific models. Strategy overlap analy-
sis and refinement of local optimization criteria has
the potential to improve overall performance under
time constraints.
</bodyText>
<sectionHeader confidence="0.99967" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999589488636364">
E. Agichtein, S. Lawrence, and L. Gravano. 2001.
Learning search engine specific query transfor-
mations for question answering. WWW.
D. Bikel, R. Schwartz, and R. Weischedel. 1999.
An algorithm that learns what’s in a name. Ma-
chine Learning.
E. Brill. 1994. Some advances in rule-based part of
speech tagging. AAAI.
J. Burger, L. Ferro, W. Greiff, J. Henderson,
M. Light, and S. Mardis. 2002. Mitre’s qanda
at trec-11. TREC.
J. Chu-Carroll, K. Czuba, J. Prager, and A. Itty-
cheriah. 2003. In question answering, two heads
are better than one. HLT-NAACL.
C. Clarke, G. Cormack, G. Kemkes, M. Laszlo,
T. Lynam, E. Terra, and P. Tilker. 2003. Statis-
tical selection of exact answers. TREC.
M. Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. Disertation.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng.
2002. Web question answering: Is more always
better? SIGIR.
A. Echihabi and D. Marcu. 2003. A noisy channel
approach to question answering. ACL.
M. Fleischman, E. Hovy, and A. Echihabi. 2003.
Offline strategies for online question answering:
Answering questions before they are asked. ACL.
R. Girju, D. Moldovan, and A. Badulescu. 2003.
Learning semantic constraints for the automatic
discovery of part-whole relations. HLT-NAACL.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
R. Bunescu, R. Girju, V. Rus, and P. Morarescu.
2000. Falcon: Boosting knowledge for answer
engines. TREC.
U. Hermjakob, E. Hovy, and C. Lin. 2000.
Knowledge-based question answering. TREC.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformu-
lation resource and web exploitation for question
answering. TREC.
E. Hovy, L. Gerber, U. Hermjakob, M. Junk, and
C.Y. Lin. 2000. Question answering in webclo-
pedia. TREC.
E. Hovy, U. Hermjakob, C. Lin, and D. Ravichan-
dran. 2002. Using knowledge to facilitate factoid
answer pinpointing. COLING.
T. Joachims. 2002. Learning to classify text using
support vector machines. Disertation.
B. Magnini, S. Romagnoli, A. Vallin, J. Herrera,
A. Penas, V. Peiado, F. Verdejo, and M. de Rijke.
2003. The multiple language question answering
track at clef 2003. CLEF.
A. McCallum and K. Nigam. 1998. A comparison
of event models for naive bayes text classifica-
tion. AAAI, Workshop on Learning for Text Cate-
gorization.
D. Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea,
R. Girju, R. Goodrum, and V. Rus. 2000. The
structure and performance of an open-domain
question answering system. ACL.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maio-
rano. 2003. Cogex: A logic prover for question
answering. ACL.
E. Nyberg, T. Mitamura, J. Callan, J. Carbonell,
R. Frederking, K. Collins-Thompson, L. Hiyaku-
moto, Y. Huang, C. Huttenhower, S. Judy, J. Ko,
A. Kupsc, L.V. Lita, V. Pedro, D. Svoboda, and
B. Vand Durme. 2003. A multi strategy approach
with dynamic planning. TREC.
J. Prager, D. Radev, E. Brown, A. Coden, and
V. Samn. 1999. The use of predictive annotation
for question answering in trec8. TREC.
D. Ravichandran, A. Ittycheriah, and S. Roukos.
2003. Automatic derivation of surface text pat-
terns for a maximum entropy based question an-
swering system. HLT-NAACL.
E.M. Voorhees. 2003. Overview of the trec 2003
question answering track. TREC.
J.R. Wen and H.J. Zhang. 2003. Query clustering
in the web context. IR and Clustering.
J. Xu, A. Licuanan, and R. Weischedel. 2003. Trec
2003 qa at bbn: Answering definitional ques-
tions. TREC.
H. Yang, T.S. Chua, S. Wang, and C.K. Koh. 2003.
Structured use of external knowledge for event-
based open domain question answering. SIGIR.
P. Zweigenbaum. 2003. Question answering in
biomedicine. EACL.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984713">
<title confidence="0.9993445">Instance-Based Question A Data-Driven Approach</title>
<author confidence="0.999976">Lucian Vlad Lita Jaime Carbonell</author>
<affiliation confidence="0.999738">Carnegie Mellon University Carnegie Mellon University</affiliation>
<email confidence="0.999329">llita@cs.cmu.edujgc@cs.cmu.edu</email>
<abstract confidence="0.999368761904762">Anticipating the availability of large questionanswer datasets, we propose a principled, datadriven Instance-Based approach to Question Answering. Most question answering systems incorporate three major steps: classify questions according to answer types, formulate queries for document retrieval, and extract actual answers. Under our approach, strategies for answering new questions are directly learned from training data. We learn models of answer type, query content, and answer extraction from clusters of similar questions. We view the answer type as a distribution, rather than a class in an ontology. In addition to query expansion, we learn general content features from training data and use them to enhance the queries. Finally, we treat answer extraction as a binary classification problem in which text snippets are labeled as correct or incorrect answers. We present a basic implementation of these concepts that achieves a good performance on TREC test data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>S Lawrence</author>
<author>L Gravano</author>
</authors>
<title>Learning search engine specific query transformations for question answering.</title>
<date>2001</date>
<publisher>WWW.</publisher>
<contexts>
<context position="10412" citStr="Agichtein et al., 2001" startWordPosition="1611" endWordPosition="1614">g to individual question types. These patterns are then applied to test questions in order to extract answers. Other methods rely solely on answer redundancy (Dumais et al., 2002): high performance retrieval engines and large corpora contribute to the fact that the most redundant entity is very often the correct answer. Predictive annotation (Prager et al., 1999) is one of the techniques that bring together corpus processing and smarter queries. Twenty classes of objects are identified and annotated in the corpus, and corresponding labels are used to enhance IR queries. Along the same lines, (Agichtein et al., 2001) propose a method for learning query transformations in order to improve answer retrieval. The method involves learning phrase features for question classification. (Wen and Zhang, 2003) address the problem of query clustering based on semantic similarity and analyze several applications such as query re-formulation and index-term selection. 4 An Instance-Based Approach This paper presents a data driven, instance-based approach for Question Answering. We adopt the view that strategies required in answering new questions can be directly learned from similar training examples (question-answer pa</context>
</contexts>
<marker>Agichtein, Lawrence, Gravano, 2001</marker>
<rawString>E. Agichtein, S. Lawrence, and L. Gravano. 2001. Learning search engine specific query transformations for question answering. WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="19708" citStr="Bikel et al., 1999" startWordPosition="3085" endWordPosition="3088"> more significant role in query generation. 4.3 The Extraction Model During training, documents are retrieved for each question cluster and a set of one-sentence passages containing a minimum number of query terms is selected. The passages are then transformed into feature vectors to be used for classification. The features consist of n-grams, paraphrases, distances between keywords and potential answers, simple statistics such as document and sentence length, part of speech features such as required verbs etc. More extensive sets of features can be found in information extraction literature (Bikel et al., 1999). Under our data-driven approach, answer extraction consists of deciding the correctness of candidate passages. The task is to build a model that accepts snippets of text and decides whether they contain a correct answer. A classifier is trained for each question cluster. When new question instances arrive, the already trained cluster-specific models are applied to new, relevant text snippets in order to test for correctness. We will refer to the resulting classifier scores as answer confidence scores. 5 Experiments We present a basic implementation of the instancebased approach. The resulting</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Some advances in rule-based part of speech tagging.</title>
<date>1994</date>
<publisher>AAAI.</publisher>
<contexts>
<context position="21275" citStr="Brill, 1994" startWordPosition="3334" endWordPosition="3335">. This makes it a good candidate for evaluating our instance-based QA approach until larger and denser datasets become available. It is also broad enough to include different question structures and varying degrees of difficulty and complexity such as: • “When did Beethoven die?” • “How long is a quarter in an NBA game?” • “What year did General Montgomery lead the Allies to a victory over the Axis troops in North Africa?” The 296 temporal questions and their corresponding answer patterns provided by NIST were used in our experiments. The questions were processed with a part of speech tagger (Brill, 1994) and a parser (Collins, 1999). The questions were clustered using templatestyle frames that incorporate lexical items, parser labels, and surface form flags (Figure 1). Consider the following question and several of its corresponding frames: “When did Beethoven die?” when did &lt;NNP&gt; die when did &lt;NNP&gt; &lt;VB&gt; when did &lt;NNP&gt; &lt;Q&gt; when did &lt;NP&gt; &lt;Q&gt; when did &lt;Q&gt; where &lt;NNP&gt;,&lt;NP&gt;,&lt;VB&gt;,&lt;Q&gt; denote: proper noun, noun phrase, verb, and generic question term sequence, respectively. Initially, frames are generated exhaustively for each question. Each frame that applies to more than three questions is then se</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>E. Brill. 1994. Some advances in rule-based part of speech tagging. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Burger</author>
<author>L Ferro</author>
<author>W Greiff</author>
<author>J Henderson</author>
<author>M Light</author>
<author>S Mardis</author>
</authors>
<title>Mitre’s qanda at trec-11.</title>
<date>2002</date>
<publisher>TREC.</publisher>
<contexts>
<context position="2007" citStr="Burger et al., 2002" startWordPosition="297" endWordPosition="300">EC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003; Fleischman et al., 2003). Moreover, Question Answering is expanding to different l</context>
</contexts>
<marker>Burger, Ferro, Greiff, Henderson, Light, Mardis, 2002</marker>
<rawString>J. Burger, L. Ferro, W. Greiff, J. Henderson, M. Light, and S. Mardis. 2002. Mitre’s qanda at trec-11. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
<author>J Prager</author>
<author>A Ittycheriah</author>
</authors>
<title>In question answering, two heads are better than one.</title>
<date>2003</date>
<publisher>HLT-NAACL.</publisher>
<contexts>
<context position="1985" citStr="Chu-Carroll et al., 2003" startWordPosition="293" endWordPosition="296">s - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003; Fleischman et al., 2003). Moreover, Question Answering is ex</context>
<context position="8860" citStr="Chu-Carroll et al., 2003" startWordPosition="1346" endWordPosition="1349">cific features. 3 Related Work Current efforts in data acquisition for Question Answering are becoming more and more common. (Girju et al., 2003) propose a supervised algorithm for part-whole relations extraction. (Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. Such efforts promise to provide large and dense datasets required by instance based approaches. Several statistical approaches have proven to be successful in answer extraction. The statistical agent presented in (Chu-Carroll et al., 2003) uses Test Question: When did John Glen start working at NASA? When did &lt;NNP+&gt; &lt;VB&gt; ... at ... ? When did Jay Leno get a job at the NBC? When did Columbus arrive at his destination? ... When did Beethoven die? When did Muhammad live? ... When did &lt;NNP&gt; &lt;VB&gt; ? When did &lt;NNP+&gt; ... ? When did the Raiders win their last game? When did EMNLP celebrate its 5th anniversary? ... When did &lt;NNP+&gt; &lt;SYNSET to_initiate&gt; ... ? When did Sony begin its VAIO campaign? When did Tom Ridge initiate the terror alert system? ... When did &lt;NP&gt; ... ? When did dinosaurs walk the earth? When did people discoverfire? ..</context>
</contexts>
<marker>Chu-Carroll, Czuba, Prager, Ittycheriah, 2003</marker>
<rawString>J. Chu-Carroll, K. Czuba, J. Prager, and A. Ittycheriah. 2003. In question answering, two heads are better than one. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Clarke</author>
<author>G Cormack</author>
<author>G Kemkes</author>
<author>M Laszlo</author>
<author>T Lynam</author>
<author>E Terra</author>
<author>P Tilker</author>
</authors>
<title>Statistical selection of exact answers.</title>
<date>2003</date>
<publisher>TREC.</publisher>
<contexts>
<context position="2110" citStr="Clarke et al., 2003" startWordPosition="312" endWordPosition="315">vy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003; Fleischman et al., 2003). Moreover, Question Answering is expanding to different languages (Magnini et al., 2003) and domains other than news stories (Zweigenbaum, 2003). These trends s</context>
</contexts>
<marker>Clarke, Cormack, Kemkes, Laszlo, Lynam, Terra, Tilker, 2003</marker>
<rawString>C. Clarke, G. Cormack, G. Kemkes, M. Laszlo, T. Lynam, E. Terra, and P. Tilker. 2003. Statistical selection of exact answers. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. Disertation.</tech>
<contexts>
<context position="21304" citStr="Collins, 1999" startWordPosition="3339" endWordPosition="3340">idate for evaluating our instance-based QA approach until larger and denser datasets become available. It is also broad enough to include different question structures and varying degrees of difficulty and complexity such as: • “When did Beethoven die?” • “How long is a quarter in an NBA game?” • “What year did General Montgomery lead the Allies to a victory over the Axis troops in North Africa?” The 296 temporal questions and their corresponding answer patterns provided by NIST were used in our experiments. The questions were processed with a part of speech tagger (Brill, 1994) and a parser (Collins, 1999). The questions were clustered using templatestyle frames that incorporate lexical items, parser labels, and surface form flags (Figure 1). Consider the following question and several of its corresponding frames: “When did Beethoven die?” when did &lt;NNP&gt; die when did &lt;NNP&gt; &lt;VB&gt; when did &lt;NNP&gt; &lt;Q&gt; when did &lt;NP&gt; &lt;Q&gt; when did &lt;Q&gt; where &lt;NNP&gt;,&lt;NP&gt;,&lt;VB&gt;,&lt;Q&gt; denote: proper noun, noun phrase, verb, and generic question term sequence, respectively. Initially, frames are generated exhaustively for each question. Each frame that applies to more than three questions is then selected to represent a specifi</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. Disertation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>M Banko</author>
<author>E Brill</author>
<author>J Lin</author>
<author>A Ng</author>
</authors>
<title>Web question answering: Is more always better?</title>
<date>2002</date>
<publisher>SIGIR.</publisher>
<contexts>
<context position="1879" citStr="Dumais et al., 2002" startWordPosition="277" endWordPosition="280">y has slowly diversified question types, increased question complexity, and refined evaluation metrics - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger </context>
<context position="9968" citStr="Dumais et al., 2002" startWordPosition="1539" endWordPosition="1542">terror alert system? ... When did &lt;NP&gt; ... ? When did dinosaurs walk the earth? When did people discoverfire? ... When did &lt;NN&gt; &lt;VB&gt; &lt;NP&gt; ? Figure 1: Neighboring questions are clustered according to features they share. maximum entropy and models answer correctness by introducing a hidden variable representing the expected answer type. Large corpora such as the Web can be mined for simple patterns (Ravichandran et al., 2003) corresponding to individual question types. These patterns are then applied to test questions in order to extract answers. Other methods rely solely on answer redundancy (Dumais et al., 2002): high performance retrieval engines and large corpora contribute to the fact that the most redundant entity is very often the correct answer. Predictive annotation (Prager et al., 1999) is one of the techniques that bring together corpus processing and smarter queries. Twenty classes of objects are identified and annotated in the corpus, and corresponding labels are used to enhance IR queries. Along the same lines, (Agichtein et al., 2001) propose a method for learning query transformations in order to improve answer retrieval. The method involves learning phrase features for question classif</context>
</contexts>
<marker>Dumais, Banko, Brill, Lin, Ng, 2002</marker>
<rawString>S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002. Web question answering: Is more always better? SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Echihabi</author>
<author>D Marcu</author>
</authors>
<title>A noisy channel approach to question answering.</title>
<date>2003</date>
<publisher>ACL.</publisher>
<contexts>
<context position="2164" citStr="Echihabi and Marcu, 2003" startWordPosition="320" endWordPosition="323">hanges in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003; Fleischman et al., 2003). Moreover, Question Answering is expanding to different languages (Magnini et al., 2003) and domains other than news stories (Zweigenbaum, 2003). These trends suggest the need for principled, statistically based, e</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>A. Echihabi and D. Marcu. 2003. A noisy channel approach to question answering. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>E Hovy</author>
<author>A Echihabi</author>
</authors>
<title>Offline strategies for online question answering: Answering questions before they are asked.</title>
<date>2003</date>
<publisher>ACL.</publisher>
<contexts>
<context position="2549" citStr="Fleischman et al., 2003" startWordPosition="375" endWordPosition="378">s and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003; Fleischman et al., 2003). Moreover, Question Answering is expanding to different languages (Magnini et al., 2003) and domains other than news stories (Zweigenbaum, 2003). These trends suggest the need for principled, statistically based, easily re-trainable, language independent QA systems that take full advantage of large amounts of training data. We propose an instance-based, data-driven approach to Question Answering. Instead of classifying questions according to limited, predefined ontologies, we allow training data to shape the strategies for answering new questions. Answer models, query content models, and extr</context>
<context position="8474" citStr="Fleischman et al., 2003" startWordPosition="1288" endWordPosition="1291">ques, finite state transducers, and text passage classification are a few methods that are usually applied to this task. The drawback shared by most statistical answer extractors is their reliance on predefined ontologies. They are often tailored to expected answer types and require type-specific resources. Gazetteers, encyclopedias, and other resources are used to generate type specific features. 3 Related Work Current efforts in data acquisition for Question Answering are becoming more and more common. (Girju et al., 2003) propose a supervised algorithm for part-whole relations extraction. (Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. Such efforts promise to provide large and dense datasets required by instance based approaches. Several statistical approaches have proven to be successful in answer extraction. The statistical agent presented in (Chu-Carroll et al., 2003) uses Test Question: When did John Glen start working at NASA? When did &lt;NNP+&gt; &lt;VB&gt; ... at ... ? When did Jay Leno get a job at the NBC? When did Columbus arrive at his destination? ... When did Beethoven die? When</context>
</contexts>
<marker>Fleischman, Hovy, Echihabi, 2003</marker>
<rawString>M. Fleischman, E. Hovy, and A. Echihabi. 2003. Offline strategies for online question answering: Answering questions before they are asked. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>D Moldovan</author>
<author>A Badulescu</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of part-whole relations.</title>
<date>2003</date>
<publisher>HLT-NAACL.</publisher>
<contexts>
<context position="2523" citStr="Girju et al., 2003" startWordPosition="371" endWordPosition="374">rging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003; Fleischman et al., 2003). Moreover, Question Answering is expanding to different languages (Magnini et al., 2003) and domains other than news stories (Zweigenbaum, 2003). These trends suggest the need for principled, statistically based, easily re-trainable, language independent QA systems that take full advantage of large amounts of training data. We propose an instance-based, data-driven approach to Question Answering. Instead of classifying questions according to limited, predefined ontologies, we allow training data to shape the strategies for answering new questions. Answer models, quer</context>
<context position="8380" citStr="Girju et al., 2003" startWordPosition="1276" endWordPosition="1279">achine learning for extracting answers from relevant documents. Boundary detection techniques, finite state transducers, and text passage classification are a few methods that are usually applied to this task. The drawback shared by most statistical answer extractors is their reliance on predefined ontologies. They are often tailored to expected answer types and require type-specific resources. Gazetteers, encyclopedias, and other resources are used to generate type specific features. 3 Related Work Current efforts in data acquisition for Question Answering are becoming more and more common. (Girju et al., 2003) propose a supervised algorithm for part-whole relations extraction. (Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. Such efforts promise to provide large and dense datasets required by instance based approaches. Several statistical approaches have proven to be successful in answer extraction. The statistical agent presented in (Chu-Carroll et al., 2003) uses Test Question: When did John Glen start working at NASA? When did &lt;NNP+&gt; &lt;VB&gt; ... at ... ? When did Jay Leno get a</context>
</contexts>
<marker>Girju, Moldovan, Badulescu, 2003</marker>
<rawString>R. Girju, D. Moldovan, and A. Badulescu. 2003. Learning semantic constraints for the automatic discovery of part-whole relations. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>R Bunescu</author>
<author>R Girju</author>
<author>V Rus</author>
<author>P Morarescu</author>
</authors>
<title>Falcon: Boosting knowledge for answer engines.</title>
<date>2000</date>
<tech>TREC.</tech>
<contexts>
<context position="4643" citStr="Harabagiu et al., 2000" startWordPosition="689" endWordPosition="692">e most significant drawback is that ontologies are not standard among systems, making individual component evaluation very difficult and re-training for new domains time-consuming. 2.1 Answer Modeling The task of determining the answer type of a question is usually considered a hard 1 decision problem: questions are classified according to an answer ontology. The classification (location, person’s name, etc) is usually made in the beginning of the QA process and all subsequent efforts are focused on finding answers of that particular type. Several existing QA systems implement feedback loops (Harabagiu et al., 2000) or full-fledged planning (Nyberg et al., 2003) to allow for potential answer type re-classification. However, most questions can have multiple answer types as well as specific answer type distributions. The following questions can accommodate answers of types: full date, year, and decade. Question Answer When did Glen lift off in Friendship7? Feb. 20, 1962 When did Glen join NASA? 1959 When did Glen have long hair? the fifties However, it can be argued that date is the most likely answer type to be observed for the first question, year the most likely type for the second question, and decade </context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, Mihalcea, Bunescu, Girju, Rus, Morarescu, 2000</marker>
<rawString>S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, R. Bunescu, R. Girju, V. Rus, and P. Morarescu. 2000. Falcon: Boosting knowledge for answer engines. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hermjakob</author>
<author>E Hovy</author>
<author>C Lin</author>
</authors>
<title>Knowledge-based question answering.</title>
<date>2000</date>
<publisher>TREC.</publisher>
<contexts>
<context position="1635" citStr="Hermjakob et al., 2000" startWordPosition="241" endWordPosition="244">re labeled as correct or incorrect answers. We present a basic implementation of these concepts that achieves a good performance on TREC test data. 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approach</context>
</contexts>
<marker>Hermjakob, Hovy, Lin, 2000</marker>
<rawString>U. Hermjakob, E. Hovy, and C. Lin. 2000. Knowledge-based question answering. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
<author>Abdessamad Echihabi</author>
<author>Daniel Marcu</author>
</authors>
<title>Natural language based reformulation resource and web exploitation for question answering.</title>
<date>2002</date>
<tech>TREC.</tech>
<contexts>
<context position="16537" citStr="Hermjakob et al., 2002" startWordPosition="2566" endWordPosition="2569"> a straight-forward fashion. Query terms are extracted and then expanded using statistical and semantic similarity measures. Documents are retrieved and the top K are further processed. This approach describes the traditional IR task and does not take advantage of specific constraints, requirements, and rich context available in the QA process. The data-driven framework we propose takes advantage of knowledge available at retrieval time and incorporates it to create better cluster-specific queries. In addition to query expansion, the goal is to learn content features: n-grams and paraphrases (Hermjakob et al., 2002) which yield better queries when added to simple keyword-based queries. The Query Content Model is a cluster-specific collection of content features that generate the best document set (Table 1). Cluster: When did X start working for Y? Simple Queries Query Content Model `X joined Y in” `X started working for Y” `X was hired by Y” `Y hired X” X, Y `job interview” ... Table 1: Queries based only on X and Y question terms may not be appropriate if the two entities share a long history. A focused, cluster-specific content model is likely to generate more precise queries. For training, simple keyw</context>
</contexts>
<marker>Hermjakob, Echihabi, Marcu, 2002</marker>
<rawString>Ulf Hermjakob, Abdessamad Echihabi, and Daniel Marcu. 2002. Natural language based reformulation resource and web exploitation for question answering. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>L Gerber</author>
<author>U Hermjakob</author>
<author>M Junk</author>
<author>C Y Lin</author>
</authors>
<title>Question answering in webclopedia.</title>
<date>2000</date>
<publisher>TREC.</publisher>
<contexts>
<context position="1507" citStr="Hovy et al., 2000" startWordPosition="219" endWordPosition="222">them to enhance the queries. Finally, we treat answer extraction as a binary classification problem in which text snippets are labeled as correct or incorrect answers. We present a basic implementation of these concepts that achieves a good performance on TREC test data. 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2</context>
</contexts>
<marker>Hovy, Gerber, Hermjakob, Junk, Lin, 2000</marker>
<rawString>E. Hovy, L. Gerber, U. Hermjakob, M. Junk, and C.Y. Lin. 2000. Question answering in webclopedia. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>U Hermjakob</author>
<author>C Lin</author>
<author>D Ravichandran</author>
</authors>
<title>Using knowledge to facilitate factoid answer pinpointing.</title>
<date>2002</date>
<publisher>COLING.</publisher>
<contexts>
<context position="1655" citStr="Hovy et al., 2002" startWordPosition="245" endWordPosition="248"> incorrect answers. We present a basic implementation of these concepts that achieves a good performance on TREC test data. 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such </context>
</contexts>
<marker>Hovy, Hermjakob, Lin, Ravichandran, 2002</marker>
<rawString>E. Hovy, U. Hermjakob, C. Lin, and D. Ravichandran. 2002. Using knowledge to facilitate factoid answer pinpointing. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Learning to classify text using support vector machines.</title>
<date>2002</date>
<publisher>Disertation.</publisher>
<contexts>
<context position="22800" citStr="Joachims, 2002" startWordPosition="3566" endWordPosition="3567">tered out. When building the Query Content Model keyword-based queries were initially formulated and expanded. From the retrieved documents a set of content features (n-grams and paraphrases) were selected through average mutual information. The features were added to the simple queries and a new set of documents was retrieved. The enhanced queries were scored and the corresponding top 10 ngrams/paraphrases were included in the Query Content Model. The maximum n-gram and paraphrase size for these features was set to 6 words. The Extraction Model uses a support vector machine (SVM) classifier (Joachims, 2002) with a linear kernel. The task of the classifier is to decide if text snippets contain a correct answer. The SVM was trained on features extracted from one-sentence passages containing at least one keyword from the original question. The features consist of: distance between keywords and potential answers, keyword density in a passage, simple statistics such as document and sentence length, query type, lexical ngrams (up to 6-grams), and paraphrases. We performed experiments using leave-one-out cross validation. The system was trained and tested without any question filtering or manual input.</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Learning to classify text using support vector machines. Disertation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>S Romagnoli</author>
<author>A Vallin</author>
<author>J Herrera</author>
<author>A Penas</author>
<author>V Peiado</author>
<author>F Verdejo</author>
<author>M de Rijke</author>
</authors>
<title>The multiple language question answering track at clef</title>
<date>2003</date>
<publisher>CLEF.</publisher>
<marker>Magnini, Romagnoli, Vallin, Herrera, Penas, Peiado, Verdejo, de Rijke, 2003</marker>
<rawString>B. Magnini, S. Romagnoli, A. Vallin, J. Herrera, A. Penas, V. Peiado, F. Verdejo, and M. de Rijke. 2003. The multiple language question answering track at clef 2003. CLEF.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>A comparison of event models for naive bayes text classification. AAAI, Workshop on Learning for Text Categorization.</title>
<date>1998</date>
<contexts>
<context position="17826" citStr="McCallum and Nigam, 1998" startWordPosition="2784" endWordPosition="2787"> produce a set of potentially relevant documents. Features Cluster Models Training Samples (Q, A) Query Content Model Extraction Model Answer Model X, Y X, Y start working X, Y `start working” ... (n-grams and paraphrases) are extracted and scored based on their co-occurrence with the correct answer. More specifically, consider a positive class: documents which contain the correct answer, and a negative class: documents which do not contain the answer. We compute the average mutual information I(C; FZ) between a class of a document, and the absence or presence of a feature fZ in the document (McCallum and Nigam, 1998). We let C be the class variable and FZ the feature variable: I(C; FZ) = H(C) − H(C|FZ) P(c, fZ) P(c, fZ) log P(c)P(fZ) where H(C) is the entropy of the class variable and H(C|FZ) is the entropy of the class variable conditioned on the feature variable. Features that best discriminate passages containing correct answers from those that do not, are selected as potential candidates for enhancing keyword-based queries. For each question-answer pair, we generate candidate queries by individually adding selected features (e.g. table 1) to the expanded word-based query. The resulting candidate queri</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>A. McCallum and K. Nigam. 1998. A comparison of event models for naive bayes text classification. AAAI, Workshop on Learning for Text Categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>S Harabagiu</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>R Girju</author>
<author>R Goodrum</author>
<author>V Rus</author>
</authors>
<title>The structure and performance of an open-domain question answering system.</title>
<date>2000</date>
<publisher>ACL.</publisher>
<contexts>
<context position="1487" citStr="Moldovan et al., 2000" startWordPosition="215" endWordPosition="218"> training data and use them to enhance the queries. Finally, we treat answer extraction as a binary classification problem in which text snippets are labeled as correct or incorrect answers. We present a basic implementation of these concepts that achieves a good performance on TREC test data. 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answeri</context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Girju, Goodrum, Rus, 2000</marker>
<rawString>D. Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea, R. Girju, R. Goodrum, and V. Rus. 2000. The structure and performance of an open-domain question answering system. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>D Clark</author>
<author>S Harabagiu</author>
<author>S Maiorano</author>
</authors>
<title>Cogex: A logic prover for question answering.</title>
<date>2003</date>
<publisher>ACL.</publisher>
<contexts>
<context position="1758" citStr="Moldovan et al., 2003" startWordPosition="258" endWordPosition="261">ance on TREC test data. 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed th</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano. 2003. Cogex: A logic prover for question answering. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Nyberg</author>
<author>T Mitamura</author>
<author>J Callan</author>
<author>J Carbonell</author>
<author>R Frederking</author>
<author>K Collins-Thompson</author>
<author>L Hiyakumoto</author>
<author>Y Huang</author>
<author>C Huttenhower</author>
<author>S Judy</author>
<author>J Ko</author>
<author>A Kupsc</author>
<author>L V Lita</author>
<author>V Pedro</author>
<author>D Svoboda</author>
<author>B Vand Durme</author>
</authors>
<title>A multi strategy approach with dynamic planning.</title>
<date>2003</date>
<publisher>TREC.</publisher>
<contexts>
<context position="1780" citStr="Nyberg et al., 2003" startWordPosition="262" endWordPosition="265"> 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical meth</context>
<context position="4690" citStr="Nyberg et al., 2003" startWordPosition="697" endWordPosition="700"> not standard among systems, making individual component evaluation very difficult and re-training for new domains time-consuming. 2.1 Answer Modeling The task of determining the answer type of a question is usually considered a hard 1 decision problem: questions are classified according to an answer ontology. The classification (location, person’s name, etc) is usually made in the beginning of the QA process and all subsequent efforts are focused on finding answers of that particular type. Several existing QA systems implement feedback loops (Harabagiu et al., 2000) or full-fledged planning (Nyberg et al., 2003) to allow for potential answer type re-classification. However, most questions can have multiple answer types as well as specific answer type distributions. The following questions can accommodate answers of types: full date, year, and decade. Question Answer When did Glen lift off in Friendship7? Feb. 20, 1962 When did Glen join NASA? 1959 When did Glen have long hair? the fifties However, it can be argued that date is the most likely answer type to be observed for the first question, year the most likely type for the second question, and decade most likely for the third question. In fact, al</context>
</contexts>
<marker>Nyberg, Mitamura, Callan, Carbonell, Frederking, Collins-Thompson, Hiyakumoto, Huang, Huttenhower, Judy, Ko, Kupsc, Lita, Pedro, Svoboda, Durme, 2003</marker>
<rawString>E. Nyberg, T. Mitamura, J. Callan, J. Carbonell, R. Frederking, K. Collins-Thompson, L. Hiyakumoto, Y. Huang, C. Huttenhower, S. Judy, J. Ko, A. Kupsc, L.V. Lita, V. Pedro, D. Svoboda, and B. Vand Durme. 2003. A multi strategy approach with dynamic planning. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>D Radev</author>
<author>E Brown</author>
<author>A Coden</author>
<author>V Samn</author>
</authors>
<title>The use of predictive annotation for question answering in trec8.</title>
<date>1999</date>
<publisher>TREC.</publisher>
<contexts>
<context position="10154" citStr="Prager et al., 1999" startWordPosition="1568" endWordPosition="1571">according to features they share. maximum entropy and models answer correctness by introducing a hidden variable representing the expected answer type. Large corpora such as the Web can be mined for simple patterns (Ravichandran et al., 2003) corresponding to individual question types. These patterns are then applied to test questions in order to extract answers. Other methods rely solely on answer redundancy (Dumais et al., 2002): high performance retrieval engines and large corpora contribute to the fact that the most redundant entity is very often the correct answer. Predictive annotation (Prager et al., 1999) is one of the techniques that bring together corpus processing and smarter queries. Twenty classes of objects are identified and annotated in the corpus, and corresponding labels are used to enhance IR queries. Along the same lines, (Agichtein et al., 2001) propose a method for learning query transformations in order to improve answer retrieval. The method involves learning phrase features for question classification. (Wen and Zhang, 2003) address the problem of query clustering based on semantic similarity and analyze several applications such as query re-formulation and index-term selection</context>
</contexts>
<marker>Prager, Radev, Brown, Coden, Samn, 1999</marker>
<rawString>J. Prager, D. Radev, E. Brown, A. Coden, and V. Samn. 1999. The use of predictive annotation for question answering in trec8. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>A Ittycheriah</author>
<author>S Roukos</author>
</authors>
<title>Automatic derivation of surface text patterns for a maximum entropy based question answering system.</title>
<date>2003</date>
<publisher>HLT-NAACL.</publisher>
<contexts>
<context position="2137" citStr="Ravichandran et al., 2003" startWordPosition="316" endWordPosition="319">systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003; Fleischman et al., 2003). Moreover, Question Answering is expanding to different languages (Magnini et al., 2003) and domains other than news stories (Zweigenbaum, 2003). These trends suggest the need for princip</context>
<context position="7705" citStr="Ravichandran et al., 2003" startWordPosition="1177" endWordPosition="1180">hly dependent on specific ontologies and become harder to re-train. 2.3 Answer Extraction Given a set of relevant documents, the answer extraction step consists of identifying snippets of text or exact phrases that answer the question. Manual approaches to answer extraction have been moderately successful in the news domain. Regular expressions, rule and pattern-based extraction are among the most efficient techniques for information extraction. However, because of the difficulty in extending them to additional types of questions, learning methods are becoming more prevalent. Current systems (Ravichandran et al., 2003) already employ traditional information extraction and machine learning for extracting answers from relevant documents. Boundary detection techniques, finite state transducers, and text passage classification are a few methods that are usually applied to this task. The drawback shared by most statistical answer extractors is their reliance on predefined ontologies. They are often tailored to expected answer types and require type-specific resources. Gazetteers, encyclopedias, and other resources are used to generate type specific features. 3 Related Work Current efforts in data acquisition for</context>
<context position="9776" citStr="Ravichandran et al., 2003" startWordPosition="1507" endWordPosition="1511"> the Raiders win their last game? When did EMNLP celebrate its 5th anniversary? ... When did &lt;NNP+&gt; &lt;SYNSET to_initiate&gt; ... ? When did Sony begin its VAIO campaign? When did Tom Ridge initiate the terror alert system? ... When did &lt;NP&gt; ... ? When did dinosaurs walk the earth? When did people discoverfire? ... When did &lt;NN&gt; &lt;VB&gt; &lt;NP&gt; ? Figure 1: Neighboring questions are clustered according to features they share. maximum entropy and models answer correctness by introducing a hidden variable representing the expected answer type. Large corpora such as the Web can be mined for simple patterns (Ravichandran et al., 2003) corresponding to individual question types. These patterns are then applied to test questions in order to extract answers. Other methods rely solely on answer redundancy (Dumais et al., 2002): high performance retrieval engines and large corpora contribute to the fact that the most redundant entity is very often the correct answer. Predictive annotation (Prager et al., 1999) is one of the techniques that bring together corpus processing and smarter queries. Twenty classes of objects are identified and annotated in the corpus, and corresponding labels are used to enhance IR queries. Along the </context>
</contexts>
<marker>Ravichandran, Ittycheriah, Roukos, 2003</marker>
<rawString>D. Ravichandran, A. Ittycheriah, and S. Roukos. 2003. Automatic derivation of surface text patterns for a maximum entropy based question answering system. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the trec 2003 question answering</title>
<date>2003</date>
<contexts>
<context position="1415" citStr="Voorhees, 2003" startWordPosition="208" endWordPosition="209">ddition to query expansion, we learn general content features from training data and use them to enhance the queries. Finally, we treat answer extraction as a binary classification problem in which text snippets are labeled as correct or incorrect answers. We present a basic implementation of these concepts that achieves a good performance on TREC test data. 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In rec</context>
<context position="20591" citStr="Voorhees, 2003" startWordPosition="3220" endWordPosition="3221">ter. When new question instances arrive, the already trained cluster-specific models are applied to new, relevant text snippets in order to test for correctness. We will refer to the resulting classifier scores as answer confidence scores. 5 Experiments We present a basic implementation of the instancebased approach. The resulting QA system is fully automatically trained, without human intervention. Instance-based approaches are known to require large, dense training datasets which are currently under development. Although still sparse, the subset of all temporal questions from the TREC 9-12 (Voorhees, 2003) datasets is relatively dense compared to the rest of the question space. This makes it a good candidate for evaluating our instance-based QA approach until larger and denser datasets become available. It is also broad enough to include different question structures and varying degrees of difficulty and complexity such as: • “When did Beethoven die?” • “How long is a quarter in an NBA game?” • “What year did General Montgomery lead the Allies to a victory over the Axis troops in North Africa?” The 296 temporal questions and their corresponding answer patterns provided by NIST were used in our </context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>E.M. Voorhees. 2003. Overview of the trec 2003 question answering track. TREC. J.R. Wen and H.J. Zhang. 2003. Query clustering in the web context. IR and Clustering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>A Licuanan</author>
<author>R Weischedel</author>
</authors>
<title>Trec</title>
<date>2003</date>
<publisher>TREC.</publisher>
<contexts>
<context position="1897" citStr="Xu et al., 2003" startWordPosition="281" endWordPosition="284">ied question types, increased question complexity, and refined evaluation metrics - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer da</context>
</contexts>
<marker>Xu, Licuanan, Weischedel, 2003</marker>
<rawString>J. Xu, A. Licuanan, and R. Weischedel. 2003. Trec 2003 qa at bbn: Answering definitional questions. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yang</author>
<author>T S Chua</author>
<author>S Wang</author>
<author>C K Koh</author>
</authors>
<title>Structured use of external knowledge for eventbased open domain question</title>
<date>2003</date>
<publisher>EACL.</publisher>
<contexts>
<context position="6587" citStr="Yang et al., 2003" startWordPosition="1008" endWordPosition="1011">and structural features present in relevant documents. For example all questions of the type “When was X born?” find their answers in documents which often contain words such as “native” or “record”, phrases such as “gave birth to X”, and sometimes even specific parse trees. Most IR research in Question Answering is focused on improving query expansion and structur&apos;the answer is classified into a single class instead of generating a probability distribution over answers ing queries in order to take advantage of specific document pre-processing. In addition to automatic query expansion for QA (Yang et al., 2003), queries are optimized to take advantage of expansion resources and document sources. Very often, these optimizations are performed offline, based on the type of question being asked. Several QA systems associate this type of information with question ontologies: upon observing questions of a certain type, specific lexical features are sought in the retrieved documents. These features are not always automatically learned in order to be used in query generation. Moreover, systems are highly dependent on specific ontologies and become harder to re-train. 2.3 Answer Extraction Given a set of rel</context>
</contexts>
<marker>Yang, Chua, Wang, Koh, 2003</marker>
<rawString>H. Yang, T.S. Chua, S. Wang, and C.K. Koh. 2003. Structured use of external knowledge for eventbased open domain question answering. SIGIR. P. Zweigenbaum. 2003. Question answering in biomedicine. EACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>