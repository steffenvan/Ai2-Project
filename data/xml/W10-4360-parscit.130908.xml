<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006176">
<title confidence="0.9992595">
Assessing the effectiveness of conversational features for dialogue
segmentation in medical team meetings and in the AMI corpus
</title>
<author confidence="0.997934">
Saturnino Luz Jing Su
</author>
<affiliation confidence="0.9669665">
Department of Computer Science School of Computer Science and Statistics
Trinity College Dublinm Ireland Trinity College Dublin, Ireland
</affiliation>
<email confidence="0.993944">
luzs@cs.tcd.ie sujing@scss.tcd.ie
</email>
<sectionHeader confidence="0.997317" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999853041666667">
This paper presents a comparison of two
similar dialogue analysis tasks: segment-
ing real-life medical team meetings into
patient case discussions, and segment-
ing scenario-based meetings into topics.
In contrast to other methods which use
transcribed content and prosodic features
(such as pitch, loudness etc), the method
used in this comparison employs only the
duration of the prosodic units themselves
as the basis for dialogue representation. A
concept of Vocalisation Horizon (VH) al-
lows us to treat segmentation as a clas-
sification task where each instance to be
classified is represented by the duration
of a talk spurt, pause or speech overlap
event in the dialogue. We report on the re-
sults this method yielded in segmentation
of medical meetings, and on the implica-
tions of the results of further experiments
on a larger corpus, the Augmented Multi-
party Meeting corpus, to our ongoing ef-
forts to support data collection and infor-
mation retrieval in medical team meetings.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973592592593">
As computer mediated communication becomes
more widespread, and data gathering devices start
to make their way into the meeting rooms and the
workplace in general, the need arises for mod-
elling and analysis of dialogue and human com-
municative behaviour in general (Banerjee et al.,
2005). The focus of our interest in this area is
the study of multi-party interaction at Multidis-
ciplinary Medical Team Meeting (MDTMs), and
the eventual recording of such meetings followed
by indexing and structuring for integration into
electronic health records. MDTMs share a num-
ber of characteristics with more conventional busi-
ness meetings, and with the meeting scenarios tar-
geted in recent research projects (Renals et al.,
2007). However, MDTMs are better structured
than these meetings, and more strongly influenced
by the time pressures placed upon the medical pro-
fessionals who take part in them (Kane and Luz,
2006).
In order for meeting support and review systems
to be truly effective, they must allow users to effi-
ciently browse and retrieve information of interest
from the recorded data. Browsing in these media
can be tedious and time consuming because con-
tinuous media such as audio and video are difficult
to access since they lack natural reference points.
A good deal of research has been conducted on in-
dexing recorded meetings. From a user’s point of
view, an important aspect of indexing continuous
media, and audio in particular, is the task of struc-
turing the recorded content. Banerjee et al. (2005),
for instance, showed that users took significantly
less time to retrieve answers when they had access
to discourse structure annotation than in a control
condition in which they had access only to unan-
notated recordings.
The most salient discourse structure in a meet-
ing is the topic of conversation. The content within
a given topic is cohesive and should therefore be
viewed as a whole. In MDTMs, the meeting con-
sists basically of successive patient case discus-
sions (PCDs) in which the patient’s condition is
discussed among different medical specialists with
the objective of agreeing diagnoses, making pa-
tient management decisions etc. Thus, the individ-
ual PCDs can be regarded as the different “topics”
which make up an MDTM (Luz, 2009).
In this paper we explore the use of a content-
free approach to the representation of vocalisation
events for segmentation of MDTM dialogues. We
start by extending the work of Luz (2009) on a
small corpus of MDTM recordings, and then test
our approach on a larger dataset, the AMI (Aug-
</bodyText>
<note confidence="0.6231415">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 332–339,
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</note>
<page confidence="0.995989">
332
</page>
<bodyText confidence="0.99846725">
mented Multi-Party Interaction) corpus (Carletta,
2007). Our ultimate goal is to analyse and apply
the insights gained on the AMI corpus to our work
on data gathering and representation in MDTMs.
</bodyText>
<sectionHeader confidence="0.999729" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999954541666667">
Topic segmentation and detection, as an aid to
meeting information retrieval and meeting index-
ing, has attracted the interest of many researchers
in recent years. The objective of topic segmenta-
tion is to locate the beginning and end time of a
cohesive segment of dialogue which can be sin-
gled out as a “topic”. Meeting topic segmentation
has been strongly influenced by techniques devel-
oped for topic segmentation in text (Hearst, 1997),
and more recently in broadcast news audio, even
though it is generally acknowledged that dialogue
segmentation differs from text and scripted speech
in important respects (Gruenstein et al., 2005).
In early work (Galley et al., 2003), meeting
annotation focused on changes that produce high
inter-annotator agreement, with no further specifi-
cation of topic label or discourse structure. Cur-
rent work has paid greater attention to discourse
structure, as reflected in two major meeting cor-
pus gathering and analysis projects: the AMI
project (Renals et al., 2007) and the ICSI meet-
ing project (Morgan et al., 2001). The AMI cor-
pus distinguishes top-level and functional topics
such as “presentation”, “discussion”, “opening”,
“closing”, “agenda” which are further specified
into sub-topics (Hsueh et al., 2006). Gruenstein
et al. (2005) sought to annotated the ICSI cor-
pus hierarchically according to topic, identifying,
in addition, action items and decision points. In
contrast to these more general types of meetings,
MDTMs are segmented into better defined units
(i.e. PCDs) so that inter-annotator agreement on
topic (patient case discussion) boundaries is less of
an issue, since PCDs are collectively agreed parts
of the formal structure of the meetings.
Meeting transcripts (either done manually or
automatically) have formed the basis for a num-
ber of approaches to topic segmentation (Galley
et al., 2003; Hsueh et al., 2006; Sherman and Liu,
2008). The transcript-based meeting segmentation
described in (Galley et al., 2003) adapted the un-
supervised lexical cohesion method developed for
written text segmentation (Hearst, 1997). Other
approaches have employed supervised machine
learning methods with textual features (Hsueh et
al., 2006). Prosodic and conversational features
have also been integrated into text-based represen-
tations, often improving segmentation accuracy
(Galley et al., 2003; Hsueh and Moore, 2007).
However, approaches that rely on transcription,
and sometimes higher-level annotation on tran-
scripts, as is the case of (Sherman and Liu, 2008),
have two shortcomings which limit their applica-
bility to MDTM indexing. First, manual transcrip-
tion is unfeasible in a busy hospital setting, and
automatic speech recognition of unconstrained,
noisy dialogues falls short of the levels of accu-
racy required for good segmentation. Secondly,
the contents of MDTMs are subject to stringent
privacy and confidentiality constraints which limit
access to training data. Regardless of such appli-
cation constraints, some authors (Malioutov et al.,
2007; Shriberg et al., 2000) argue for the use of
prosodic features and other acoustic patterns di-
rectly from the audio signal for segmentation. The
approach investigated in this paper goes a step fur-
ther by representing the data solely through what
is, arguably, the simplest form of content-free rep-
resentation, namely: duration of talk spurts, si-
lences and speech overlaps, optionally comple-
mented with speaker role information (e.g. medi-
cal speciality).
</bodyText>
<sectionHeader confidence="0.996301" genericHeader="method">
3 Content-free representations
</sectionHeader>
<bodyText confidence="0.999884318181818">
There is more to the structure (and even the
semantics) of a dialogue than the textual con-
tent of the words exchanged by its participants.
The role of prosody in shaping the illocution-
ary force of vocalisations, for instance, is well
documented (Holmes, 1984), and prosodic fea-
tures have been used for automatic segmentation
of broadcast news data into sentences and topics
(Shriberg et al., 2000). Similarly, recurring audio
patterns have been employed in segmentation of
recorded lectures (Malioutov et al., 2007). Works
in the area of social psychology have used the sim-
ple conversational features of duration of vocalisa-
tions, pauses and overlaps to study the dynamics
of group interaction. Jaffe and Feldstein (1970)
characterise dialogues as Markov processes, and
Dabbs and Ruback (1987) suggest that a “content-
free” method based on the amount and structure
of vocal interactions could complement group in-
teraction frameworks such as the one proposed
by Bales (1950). Pauses and overlap statistics
alone can be used, for instance, to characterise
</bodyText>
<page confidence="0.997609">
333
</page>
<figure confidence="0.999873888888889">
Event
durations
Gaps &amp;
Overlaps
Speakers
Vocalisation
events
... ... ...
...
</figure>
<figureCaption confidence="0.999992">
Figure 1: Vocalisation Horizon for event v.
</figureCaption>
<bodyText confidence="0.999967615384615">
the differences between face-to-face and telephone
dialogue (ten Bosch et al., 2005), and a corre-
lation between the duration of pauses and topic
boundaries has been demonstrated for recordings
of spontaneous narratives (Oliveira, 2002).
These works provided the initial motivation for
our content-free representation scheme and the
topic segmentation method proposed in this paper.
It is easy to verify by inspection of both the corpus
of medical team meetings described in Section 4
and the AMI corpus that pauses and vocalisations
vary significantly in duration and position on and
around topic boundaries. Table 1 shows the mean
durations of vocalisations that initiate new topics
or PCDs in MDTMs and the scenario-based AMI
meetings, as well as the durations of pauses and
overlaps that surround it (within one vocalisation
event to the left and right). In all cases the dif-
ferences were statistically significant. These re-
sults agree with those obtained by Oliveira (2002)
for discourse topics, and suggest that an approach
based on representing the duration of vocalisa-
tions, pauses and overlaps in the immediate con-
text of a vocalisation might be effective for auto-
matic segmentation of meeting dialogues into top-
ics or PCDs.
</bodyText>
<table confidence="0.725322">
Boundary Non-boundary
AMI vocal. 5.3 (8.2) 1.6 (3.5)
AMI pauses 2.6 (4.9) 1.2 (2.8)
AMI overlaps 0.4 (0.4) 0.3 (0.6)
MDTM vocal. 12.0 (15.5) 8.1 (14.7)
MDTM pauses 9.7 (12.7) 8.2 (14.8)
</table>
<bodyText confidence="0.999948044444445">
We thus conceptualise meeting topic segmenta-
tion as a classification task approachable through
supervised machine learning. A meeting can
be pre-segmented into separate vocalisations (i.e.
talk spurts uttered by meeting participants) and si-
lences, and such basic units (henceforth referred
to as vocalisation events) can then be classified
as to whether they signal a topic transition. The
basic defining features of a vocalisation event are
the identity of the speaker who uttered the vocali-
sation (or speakers, for events containing speech
overlap) and its duration, or the duration of a
pause, for silence events. However, identity la-
bels and interval durations by themselves are not
enough to enable segmentation. As we have seen
above, some approaches to meeting segmentation
complement these basic data with text (keywords
or full transcription) uttered during vocalisation
events, and with prosodic features. Our proposal
is to retain the content-free character of the ba-
sic representation by complementing the speaker
and duration information for an event with data de-
scribing its preceding and succeeding events. We
thus aim to capture an aspect of the dynamics of
the dialogue by representing snapshots of vocali-
sation sequences. We call this general representa-
tion strategy Vocalisation Horizon (VH).
Figure 1 illustrates the basic idea. Vocalisa-
tion events are placed on a time line and com-
bine utterances produced by the speakers who took
part in the meeting. These events can be labelled
with nominal attributes (s1, s2, ...) denoting the
speaker (or some other symbolic attribute, such as
the speaker’s role in the meeting). Silences (gaps)
and group talk (overlap) can either be assigned re-
served descriptors (such as “Floor” and “Group”)
or regarded as separate annotation layers. The
general data representation scheme for, say, seg-
ment v would involve a data from its left context
(vi, v2, v3, ...) and its right context (v1, v2, v3, ...)
in addition to the data for v itself. These can be a
combination of symbolic labels (in Figure 1, for
instance, s1 for the current speaker, s3, s2, s1, .. .
for the preceding events and s3, s2, s3, ... for the
following events), durations (d, di, d2, d3, ... etc)
</bodyText>
<tableCaption confidence="0.660095333333333">
Table 1: Mean durations in seconds (and standard
deviations) of vocalisation and pauses on and near
topic boundaries in MDTM and AMI meetings.
</tableCaption>
<figure confidence="0.969264833333333">
t-test
p &lt; .01
p &lt; .01
p &lt; .01
p &lt; .05
p &lt; .05
</figure>
<page confidence="0.833379">
334
</page>
<figureCaption confidence="0.993862">
Figure 2: Meeting segmentation processing archi-
tecture.
</figureCaption>
<bodyText confidence="0.92498790625">
and gaps or overlaps gl, g2, g3, ... , g1, g2, g3, . . .
etc). Specific representations depend on the type
of annotation available on the speech data and on
the nature of the meeting. Sections 4 and 5 present
and assess various representation schemes.
The general processing architecture for meeting
segmentation assumed in this paper is shown in
Figure 2. The system will received the speech sig-
nal, possibly on a single channel, and pre-segment
it into separate channels (one per speaker) with
intervals of speech activity and silence labelled
for each stream. Depending on the quality of the
recording and the characteristics of the environ-
ment, this initial processing stage can be accom-
plished automatically through existing speaker di-
arisation methods — e.g. (Ajmera and Wooters,
2003). In the experiments reported below man-
ual annotation was employed. In the AMI cor-
pus, speaker and speech activity annotation is done
on the word level and include transcription (Car-
letta, 2007). We parsed these word-level labels,
ignoring the transcriptions, in order to build the
content-free representation described above. Once
the data representation has been created it is then
used, along with topic boundary annotations, to
train a probabilistic classifier. Finally, the topic
detection module uses the models generated in the
training phase to hypothesise boundaries in unan-
notated vocalisation event sequences and, option-
ally, performs post-processing of these sequences
before returning the final hypothesis. These mod-
ules are described in more detail below.
</bodyText>
<sectionHeader confidence="0.999271" genericHeader="method">
4 MDTM Segmentation
</sectionHeader>
<bodyText confidence="0.999981381818182">
The MDTM corpus was collected over a period of
three years as part of a detailed ethnographic study
of medical teams (Kane and Luz, 2006). The cor-
pus consists in 28 hours or meetings recorded in
a dedicated teleconferencing room at a major pri-
mary care hospital. The audio sources included a
pressure-zone microphone attached to the telecon-
ferencing system and a highly sensitive directional
microphone. Video was gathered through two sep-
arate sources: the teleconferencing system, which
showed the participants and, at times, the medi-
cal images (pathology slides, radiology) relevant
to the case under discussion, and a high-end cam-
corder mounted on a tripod. All data were im-
ported into a multimedia annotation tool and syn-
chronised. Of these, two meetings encompassing
54 PCDs were chosen an annotated for vocalisa-
tions (including speaker identity and duration) and
PCD boundaries.
Vocalisation events were encoded as vectors
v = (s, d, sl, dl,... , sn, dn, s1, d1, ... , sn, dn),
where the variables are as explained in Section 3.
The speaker labels s, si and si are replaced, for the
sake of generality, by “role” labels denoting med-
ical specialties, such as “radiologist”, “surgeon”,
“clinical oncologist”, “pathologist” etc. In addi-
tion to these roles, we reserved the special labels
“Pause” (a period of silence between two vocal-
isations by the same speaker), “SwitchingPause”
(pause between vocalisations by different speak-
ers), and “Group” (vocalisations containing over-
laps, i.e. speech by more than one speaker). We set
a minimum duration of 1s for a talk spurt to count
as a speech vocalisation event and a 0.9s minimum
duration for silence period to be a pause. Shorter
intervals (depicted in Figure 1 as the fuzzy ends of
the speech lines on the top of the chart) are incor-
porated into an adjacent vocalisation event.
The segmentation process can be defined as the
process of mapping the set of vocalisation events
V to 10, 11 where 1 represents a topic bound-
ary and 0 represents a non-boundary vocalisation
event. In order to implement this mapping we
employ a Naive Bayes classifier. The conditional
probabilities for the nominal variables (speaker
roles) are estimated on the training set by max-
imum likelihood and combined into multinomial
models, while the continuous variables are log
transformed and modelled through Gaussian ker-
nels (John and Langley, 1995).
These models are used to estimate the probabil-
ity, given by equation (1), of a vocalisation being
marked as a topic boundary given the above de-
scribed data representation, and the usual condi-
tional independence assumptions applies.
</bodyText>
<equation confidence="0.997805">
P(B = bIV = v) = P(B = b|Sf = sf, DT,, = dn, (1)
</equation>
<bodyText confidence="0.9983545">
The model can therefore be represented as a
simple Bayesian network where the only depen-
</bodyText>
<figure confidence="0.996026842105263">
Speaker diarisation
Speech signal
or
Topic boundary detection
Topic markers
...
...
...
...
... ...
Classifier
induction
word-level
transcript.
Turn segm.
by speaker
topic-level
annotation
... , S = s,..., Dn = dn)
</figure>
<page confidence="0.923956">
335
</page>
<figureCaption confidence="0.988977">
Figure 3: Bayesian model employed for dialogue
segmentation.
</figureCaption>
<bodyText confidence="0.993373127272727">
dencies are between the boundary variable and
each feature of the vocalisation event, as shown
in Figure 3.
Luz (2009) reports that, for a similar data repre-
sentation, horizons of length 2 &lt; n &lt; 6 produced
the best segmentation results. Following this find-
ing, we adopt n = 3 for all our experiments.
We tested two variants of the representation: Vpd
that discriminated between pause types (pauses,
switching pauses, group pauses, and group switch-
ing pauses), as in (Dabbs and Ruback, 1987), and
V,p which labelled all pauses equally. The evalua-
tion metrics employed include the standard classi-
fication metrics of accuracy (A), the proportion of
correctly classified segments, boundary precision
(P), the proportion of correctly assigned bound-
aries among all events marked as topic bound-
aries, boundary recall (R), the proportion of target
boundaries correctly assigned, and the F1 score,
the harmonic mean of P and R.
Although these standard metrics provide an ini-
tial approximation to segmentation effectiveness,
they have been criticised as tools for evaluating
segmentation because they are hard to interpret
and are not sensitive to near misses (Pevzner and
Hearst, 2002). Furthermore, due to the highly un-
balanced nature of the classification task (bound-
ary vocalisation events are only 3.3% of all in-
stances), accuracy scores tend to produce over-
optimistic results. Therefore, to give a fairer
picture of the effectiveness of our method, we
also report values for two error metrics proposed
specifically for segmentation: Pk (Beeferman et
al., 1999) and WindowDiff, or WD, (Pevzner and
Hearst, 2002).
The Pk metric gives the probability that two vo-
calisation events occurring k vocalisations apart
and picked otherwise randomly from the dataset
are incorrectly identified by the algorithm as be-
longing to the same or to different topics. Pk is
computed by sliding two pairs of pointers over the
reference and the hypothesis sequences and ob-
serving whether each pair of pointers rests in the
same or in different segments. An error is counted
if the pairs disagree (i.e. if they point to the same
segment in one sequence and to different segments
in the other).
WD is as an estimate of inconsistencies between
reference and hypothesis, obtained by sliding a
window of length equal k segments over the time
line and counting disagreements between true and
hypothesised boundaries. Like the standard IR
metrics, Pk and WD range over the [0, 1] interval.
Since they are error metrics, the greater the value,
the worse the segmentation.
</bodyText>
<tableCaption confidence="0.9681745">
Table 2: PCD segmentation results for 5-fold cross
validation, horizon n = 3 (mean values).
</tableCaption>
<table confidence="0.983364444444444">
Threshold Filter Data A P R Fl Pk WD
MAP no V3p 0.94 0.20 0.21 0.18 0.33 0.44
Vpd 0.95 0.17 0.20 0.16 0.30 0.38
yes V3p 0.95 0.20 0.16 0.16 0.32 0.38
Vpd 0.95 0.16 0.12 0.13 0.29 0.34
Proport. no V3p 0.95 0.28 0.28 0.28 0.26 0.36
Vpd 0.95 0.26 0.27 0.26 0.27 0.42
yes V3p 0.95 0.30 0.22 0.25 0.25 0.31
Vpd 0.95 0.22 0.14 0.17 0.27 0.33
</table>
<bodyText confidence="0.9763974">
Table 2 shows the results for segmentation of
MDTMs into PCDs under the representational
variants mentioned above and two different thresh-
olding strategies: maximum a posteriori hypothe-
sis (MAP) and proportional threshold. The latter
is a strategy that varies the threshold probability
above which an event is marked as a boundary ac-
cording to the generality of boundaries found in
the training set. The motivation for testing propor-
tional thresholds is illustrated by Figure 4, which
shows a step plot of MAP hypothesis (h) super-
imposed on the true segmentation (peaks repre-
sent boundaries) and the corresponding values for
p(blv). It is clear that a number of false positives
would be removed if the threshold were set above
the MAP level1 with no effect on the number of
false negatives.
Another possible improvement suggested by
Figure 4 is the filtering of adjacent boundary hy-
potheses. Wider peaks, such as the ones on in-
stances 14 and 172 indicate that two or more
boundaries were hypothesised in immediate suc-
cession. Since this is clearly impossible, a
straightforward improvement of the segmentation
1I.e. p(b|v) &gt; 0.5; above the horizontal line in the centre.
</bodyText>
<page confidence="0.991128">
336
</page>
<bodyText confidence="0.932856666666667">
hypothesis can be achieved by choosing a sin- Vpd yielded the best results under MAP, V�p
gle boundary marker among a cluster of adjacent worked best overall under a proportional thresh-
ones. This has been done as a post-processing old. What is the effect of encoding more detailed
step by choosing a single event with maximal es- pause and overlap information? Unfortunately, the
timated probability within a cluster of adjacent MDTM corpus has not been annotated to the level
boundary hypotheses as the new hypothesis. of detail required to allow in-depth investigation
14 85 107 142 172 198 228 250 of this question. We therefore turn to the far larger
and more detailed AMI corpus for our next exper-
iments. In addition to helping clarify the represen-
tation issue, testing our method on this corpus will
give us a better idea of how our method performs
in a more standard topic segmentation task.
</bodyText>
<figure confidence="0.612104">
5 AMI Segmentation
n
r
1 15 87 110 168 227
</figure>
<figureCaption confidence="0.890529666666667">
Figure 4: Segmentation profile showing true
boundaries (r), boundaries hypothesised by a MAP
classifier (h) and probabilities (dotted line).
</figureCaption>
<bodyText confidence="0.999981072463768">
The results suggest that both proportional
thresholding and filtering improve segmentation.
As expected, accuracy figures were generally high
(an uninformative) reflecting the great imbalance
in favour of negative instances and the conser-
vative nature of the classifier. Precision, recall
and F1 (for positive instances only) were also pre-
dictably low, with V�p under a proportional thresh-
old attaining the best results. However, in meeting
browsing marking the topic boundary precisely is
far less important than retrieving the right text is in
information retrieval or text categorisation, since
the user can easily scan the neighbouring intervals
with a slider (Banerjee et al., 2005). Therefore,
Pk and WD are the most appropriate measures of
success in this task. Here our results seem quite
encouraging, given that they all represent great
improvements over the (rather reasonable) base-
lines of Pk = .46 and WD = .51 estimated by
Monte Carlo simulation as in (Hsueh et al., 2006)
by hypothesising the same proportion of bound-
aries found in the training set. Our results also
compare favourably with some of the best results
reported in the meeting segmentation literature to
date, namely Pk = 0.32 and WD = 0.36, for a lex-
ical cohesion algorithm on the ICSI corpus (Gal-
ley et al., 2003), and Pk = 0.34 and WD = 0.36,
for a maximum entropy approach combining lexi-
cal, conversational and video features on the AMI
corpus (Hsueh et al., 2006).
Although these results are promising, they pose
a question as regards data representation. While
The AMI corpus is a collection of meetings
recorded under controlled conditions, many of
which have a fixed scenario, goals and assigned
participant roles. The corpus is manually tran-
scribed, and annotated with word-level timings
and a variety of metadata, including topics and
sub-topics (Carletta, 2007). Transcriptions in the
AMI corpus are extracted from redundant record-
ing channels (lapel, headset and array micro-
phones), and stored separately for each partici-
pant. Because timing information in AMI is so
detailed, we were able to create much richer VH
representations, including finer grained pause and
overlap information.
The original XML-encoded AMI data were
parsed and collated to produce our variants of
the VH scheme. We tested four types of VH:
V�, which includes only vocalisation events; V9,
which includes only pause and speech over-
lap events; V�, which includes all vocalisations,
pauses and overlaps; and V�, which is similar to
Vpd in that it includes speaker roles in addition to
vocalisations. Pauses and overlaps were encoded
by the same variable gz, where gz &gt; 0 indicates a
pause gz &lt; 0 an overlap, as shown in Figure 1. Un-
like MDTM, no arbitrary threshold was imposed
on the identification of pause and overlap events.
As before, we tested on a horizon n = 3, in order
to allow comparison with MDTM results.
The training and boundary inference process
also remained as in the MDTM experiment, ex-
cept that the larger amount of meeting data avail-
able enabled us to increase the number of folds for
cross validation so that the results could be tested
for statistical significance.
The error scores and the number of boundaries
predicted for the different representational vari-
</bodyText>
<page confidence="0.996042">
337
</page>
<bodyText confidence="0.99996075">
ants, filtering an thresholding strategies are shown
in Table 3. Although all methods significantly
outperformed the baseline scores of Pk = 0.473
and WD = 0.542 (paired t-tests, p &lt; 0.01, for
all conditions), there were hardly any differences
in Pk scores across the different representations,
even when conservative boundary filtering is per-
formed. Filtering, however, caused a significant
improvement for WD in all cases, though the com-
bined effects of proportional thresholding and fil-
tering caused the classifier to err on the side of
underprediction. A 3-way analysis of variance in-
cluding non-filtered scores for proportional thresh-
old resulted in F[4,235] = 31.82, p &lt; 0.01 for
WD scores. These outcomes agree with the results
of the smaller-scale MDTM segmentation exper-
iment, showing that categorisation based on con-
versational features tend to mark clusters of seg-
ments around the true topic boundary. In addition,
the trend for better performance of proportional
thresholding exhibited in the MDTM data was not
as clearly observed in the AMI data, where only
WD scores were significantly better than MAP
(p &lt; 0.01, Tukey HSD).
</bodyText>
<tableCaption confidence="0.956125666666667">
Table 3: Segmentation results for 16-fold cross
validation on AMI corpus, horizon n = 3. Cor-
rect number of boundaries in reference is 724.
</tableCaption>
<table confidence="0.999259941176471">
Threshold Filter Data Pk WD # bound.
MAP no Va 0.270 0.462 3322
Vg 0.278 0.433 1875
Vv 0.273 0.449 3075
Vr 0.271 0.448 3073
yes Va 0.272 0.362 574
Vg 0.277 0.391 851
Vv 0.275 0.358 468
Vr 0.274 0.357 469
Proport. no Va 0.289 0.398 1233
Vg 0.290 0.382 735
Vv 0.293 0.387 1002
Vr 0.293 0.387 1002
yes Va 0.293 0.353 241
Vg 0.290 0.362 383
Vv 0.297 0.350 183
Vr 0.297 0.350 182
</table>
<bodyText confidence="0.9998306">
It is noteworthy that the finer-grained represen-
tations from which speaker roles were excluded
(V�, Vg, and Va) yielded segmentation perfor-
mance comparable to the MDTM segmentation
performance under V�p and Vpd. In fact, adding
speaker role information in Vr did not result in im-
provement for AMI segmentation. Also interest-
ing is the fact that representations based solely on
pause and overlap information also produced good
performance, thus confirming our initial intuition.
</bodyText>
<subsectionHeader confidence="0.971139">
5.1 MDTM revisited
</subsectionHeader>
<bodyText confidence="0.999983708333333">
Since V�, Vg and Va seem to perform well with-
out including speaker role information (except for
the current vocalisation’s speaker role) we would
like to see how a similar representation might af-
fect segmentation performance for MDTM. We
therefore tested whether excluding preceding and
following speaker role information from Vp and
Vpd had a positive impact on PCD segmenta-
tion performance. However, contrary to our ex-
pectations neither of the modified representations
yielded better scores. The best results, achieved
for the modified Vpd under proportional thresh-
olding (PK = 0.27 and WD = 0.34), failed to
match the results obtained with the original repre-
sentation. It seems that the various and more spe-
cialised speaker roles found in medical meetings
can be good predictors of PCD boundaries. For
example: a typical pattern at the start of a PCD
is the recounting of the patient’s initial symptoms
and clinical findings by the registrar in a narrative
style. In AMI, on other hand, the roles are much
fewer, being only acted out by the participants as
part of the given scenario, which might explain the
irrelevance of these roles for segmentation.
</bodyText>
<subsectionHeader confidence="0.980659">
5.2 Conclusion
</subsectionHeader>
<bodyText confidence="0.999996208333333">
MDTM segmentation differs from topic segmen-
tation of the AMI meetings in that PCDs are more
regular in their occurrence than meeting topics
proper. Speaker role information was also found
to help MDTM segmentation, which was expected
since there are many more very distinct active
speaker roles in MDTM (10 specialties, in total).
Furthermore, V�p and Vpd represent pauses and
overlaps as reserved roles, so that the information
encoded in Vg and Va as separate variables ap-
pear in the speaker role horizon of V�p and Vpd.
It is possible that the finer-grained timing annota-
tion of the AMI corpus (including detailed overlap
and pause information unavailable in the MDTM
data) contributed to the relatively good segmen-
tation performance achieved on AMI even in the
absence of speaker role cues. It would be inter-
esting to investigate whether finer pause and over-
lap timings can also improve MDTM segmenta-
tion. This suggests some requirements for MDTM
data collection and pre-processing, such as the use
of individual close-talking and the use of a speech
recogniser to derive word-level timings. We plan
on conducting further experiments in that regard.
</bodyText>
<page confidence="0.998207">
338
</page>
<sectionHeader confidence="0.995692" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998324">
This research was funded by Science Founda-
tion Ireland under the Research Frontiers pro-
gram. The presentation was funded by Grant
07/CE/1142, Centre for Next Generation Locali-
sation (CNGL).
</bodyText>
<sectionHeader confidence="0.99896" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999946282828283">
J. Ajmera and C. Wooters. 2003. A robust speaker
clustering algorithm. In Proceedings of the IEEE
Workshop on Automatic Speech Recognition and
Understanding, ASRU’03, pages 411–416. IEEE
Press.
R. F. Bales. 1950. Interaction Process Analysis: A
Method for the Study of Small Groups. Addison-
Wesley, Cambridge, Mass.
S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The
necessity of a meeting recording and playback sys-
tem, and the benefit of topic-level annotations to
meeting browsing. In Proceedings of the 10th In-
ternational Conference on Human-Computer Inter-
action (INTERACT’05), pages 643–656.
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statis-
tical models for text segmentation. Machine Learn-
ing, 34:177–210, Feb. 10.1023/A:1007506220214.
J. Carletta. 2007. Unleashing the killer corpus: ex-
periences in creating the multi-everything ami meet-
ing corpus. Language Resources and Evaluation,
41(2):181–190.
J. M. J. Dabbs and B. Ruback. 1987. Dimensions of
group process: Amount and structure of vocal inter-
action. Advances in Experimental Social Psychol-
ogy, 20(123–169).
M. Galley, K. R. McKeown, E. Fosler-Lussier, and
H. Jing. 2003. Discourse segmentation of multi-
party conversation. In Proceedings of the 41st An-
nual Meeting of the ACL, pages 562–569.
A. Gruenstein, J. Niekrasz, and M. Purver. 2005.
Meeting structure annotation: Data and tools. In
Proceedings of the 6th SIGdial Workshop on Dis-
course and Dialogue, pages 117–127, Lisbon, Por-
tugal, September.
M. A. Hearst. 1997. Texttiling: segmenting text into
multi-paragraph subtopic passages. Comput. Lin-
guist., 23(1):33–64.
J. Holmes. 1984. Modifying illocutionary force. Jour-
nal of Pragmatics, 8(3):345 – 365.
P. Hsueh and J. D. Moore. 2007. Combining multi-
ple knowledge sources for dialogue segmentation in
multimedia archives. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics. ACL Press.
P. Hsueh, J. D. Moore, and S. Renals. 2006. Automatic
segmentation of multiparty dialogue. In Proceed-
ings of the 11th Conference of the European Chapter
of the ACL (EACL), pages 273–277. ACL Press.
J. Jaffe and S. Feldstein. 1970. Rhythms of dialogue.
Academic Press, New York.
G. H. John and P. Langley. 1995. Estimating contin-
uous distributions in Bayesian classifiers. In Pro-
ceedings of the 11th Conference on Uncertainty in
Artificial Intelligence (UAI’95), pages 338–345, San
Francisco, CA, USA, August. Morgan Kaufmann
Publishers.
B. Kane and S. Luz. 2006. Multidisciplinary med-
ical team meetings: An analysis of collaborative
working with special attention to timing and telecon-
ferencing. Computer Supported Cooperative Work
(CSCW), 15(5):501–535.
S. Luz. 2009. Locating case discussion segments
in recorded medical team meetings. In Proceed-
ings of the ACM Multimedia Workshop on Search-
ing Spontaneous Conversational Speech (SSCS’09),
pages 21–30, Beijing, China, October. ACM Press.
I. Malioutov, A. Park, R. Barzilay, and J. Glass. 2007.
Making sense of sound: Unsupervised topic seg-
mentation over acoustic input. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 504–511, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
N. Morgan, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
A. Janin, T. Pfau, E. Shriberg, and A. Stolcke. 2001.
The meeting project at ICSI. In Procs. of Human
Language Technologies Conference, San Diego.
M. Oliveira, 2002. The role of pause occurrence and
pause duration in the signaling of narrative struc-
ture, volume 2389 of LNAI, pages 43–51. Springer.
L. Pevzner and M. A. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19–36, Mar.
S. Renals, T. Hain, and H. Bourlard. 2007. Recog-
nition and interpretation of meetings: The AMI
and AMIDA projects. In Proc. IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU ’07).
M. Sherman and Y. Liu. 2008. Using hidden Markov
models for topic segmentation of meeting tran-
scripts. In Proceedings of the IEEE Spoken Lan-
guage Technology Workshop, pages 185–188.
E. Shriberg, A. Stolcke, D. Hakkani-T¨ur, and G. T¨ur.
2000. Prosody-based automatic segmentation of
speech into sentences and topics. Speech commu-
nication, 32(1-2):127–154.
L. ten Bosch, N. Oostdijk, and L. Boves. 2005. On
temporal aspects of turn taking in conversational di-
alogues. Speech Communication, 47:80–86.
</reference>
<page confidence="0.999218">
339
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.382333">
<title confidence="0.960891">Assessing the effectiveness of conversational features for segmentation in medical team meetings and in the AMI corpus</title>
<author confidence="0.999316">Saturnino Luz Jing Su</author>
<affiliation confidence="0.999388">Department of Computer Science School of Computer Science and Statistics</affiliation>
<address confidence="0.554406">Trinity College Dublinm Ireland Trinity College Dublin, Ireland</address>
<email confidence="0.685857">luzs@cs.tcd.iesujing@scss.tcd.ie</email>
<abstract confidence="0.99842236">This paper presents a comparison of two similar dialogue analysis tasks: segmenting real-life medical team meetings into patient case discussions, and segmenting scenario-based meetings into topics. In contrast to other methods which use transcribed content and prosodic features (such as pitch, loudness etc), the method used in this comparison employs only the duration of the prosodic units themselves as the basis for dialogue representation. A concept of Vocalisation Horizon (VH) allows us to treat segmentation as a classification task where each instance to be classified is represented by the duration of a talk spurt, pause or speech overlap event in the dialogue. We report on the results this method yielded in segmentation of medical meetings, and on the implications of the results of further experiments on a larger corpus, the Augmented Multiparty Meeting corpus, to our ongoing efforts to support data collection and information retrieval in medical team meetings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Ajmera</author>
<author>C Wooters</author>
</authors>
<title>A robust speaker clustering algorithm.</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU’03,</booktitle>
<pages>411--416</pages>
<publisher>IEEE Press.</publisher>
<contexts>
<context position="13656" citStr="Ajmera and Wooters, 2003" startWordPosition="2159" endWordPosition="2162">e of the meeting. Sections 4 and 5 present and assess various representation schemes. The general processing architecture for meeting segmentation assumed in this paper is shown in Figure 2. The system will received the speech signal, possibly on a single channel, and pre-segment it into separate channels (one per speaker) with intervals of speech activity and silence labelled for each stream. Depending on the quality of the recording and the characteristics of the environment, this initial processing stage can be accomplished automatically through existing speaker diarisation methods — e.g. (Ajmera and Wooters, 2003). In the experiments reported below manual annotation was employed. In the AMI corpus, speaker and speech activity annotation is done on the word level and include transcription (Carletta, 2007). We parsed these word-level labels, ignoring the transcriptions, in order to build the content-free representation described above. Once the data representation has been created it is then used, along with topic boundary annotations, to train a probabilistic classifier. Finally, the topic detection module uses the models generated in the training phase to hypothesise boundaries in unannotated vocalisat</context>
</contexts>
<marker>Ajmera, Wooters, 2003</marker>
<rawString>J. Ajmera and C. Wooters. 2003. A robust speaker clustering algorithm. In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU’03, pages 411–416. IEEE Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R F Bales</author>
</authors>
<title>Interaction Process Analysis: A Method for the Study of Small Groups.</title>
<date>1950</date>
<publisher>AddisonWesley,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="8768" citStr="Bales (1950)" startWordPosition="1365" endWordPosition="1366">es and topics (Shriberg et al., 2000). Similarly, recurring audio patterns have been employed in segmentation of recorded lectures (Malioutov et al., 2007). Works in the area of social psychology have used the simple conversational features of duration of vocalisations, pauses and overlaps to study the dynamics of group interaction. Jaffe and Feldstein (1970) characterise dialogues as Markov processes, and Dabbs and Ruback (1987) suggest that a “contentfree” method based on the amount and structure of vocal interactions could complement group interaction frameworks such as the one proposed by Bales (1950). Pauses and overlap statistics alone can be used, for instance, to characterise 333 Event durations Gaps &amp; Overlaps Speakers Vocalisation events ... ... ... ... Figure 1: Vocalisation Horizon for event v. the differences between face-to-face and telephone dialogue (ten Bosch et al., 2005), and a correlation between the duration of pauses and topic boundaries has been demonstrated for recordings of spontaneous narratives (Oliveira, 2002). These works provided the initial motivation for our content-free representation scheme and the topic segmentation method proposed in this paper. It is easy t</context>
</contexts>
<marker>Bales, 1950</marker>
<rawString>R. F. Bales. 1950. Interaction Process Analysis: A Method for the Study of Small Groups. AddisonWesley, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>C Rose</author>
<author>A I Rudnicky</author>
</authors>
<title>The necessity of a meeting recording and playback system, and the benefit of topic-level annotations to meeting browsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th International Conference on Human-Computer Interaction (INTERACT’05),</booktitle>
<pages>643--656</pages>
<contexts>
<context position="1614" citStr="Banerjee et al., 2005" startWordPosition="245" endWordPosition="248">he dialogue. We report on the results this method yielded in segmentation of medical meetings, and on the implications of the results of further experiments on a larger corpus, the Augmented Multiparty Meeting corpus, to our ongoing efforts to support data collection and information retrieval in medical team meetings. 1 Introduction As computer mediated communication becomes more widespread, and data gathering devices start to make their way into the meeting rooms and the workplace in general, the need arises for modelling and analysis of dialogue and human communicative behaviour in general (Banerjee et al., 2005). The focus of our interest in this area is the study of multi-party interaction at Multidisciplinary Medical Team Meeting (MDTMs), and the eventual recording of such meetings followed by indexing and structuring for integration into electronic health records. MDTMs share a number of characteristics with more conventional business meetings, and with the meeting scenarios targeted in recent research projects (Renals et al., 2007). However, MDTMs are better structured than these meetings, and more strongly influenced by the time pressures placed upon the medical professionals who take part in th</context>
<context position="2837" citStr="Banerjee et al. (2005)" startWordPosition="444" endWordPosition="447">m (Kane and Luz, 2006). In order for meeting support and review systems to be truly effective, they must allow users to efficiently browse and retrieve information of interest from the recorded data. Browsing in these media can be tedious and time consuming because continuous media such as audio and video are difficult to access since they lack natural reference points. A good deal of research has been conducted on indexing recorded meetings. From a user’s point of view, an important aspect of indexing continuous media, and audio in particular, is the task of structuring the recorded content. Banerjee et al. (2005), for instance, showed that users took significantly less time to retrieve answers when they had access to discourse structure annotation than in a control condition in which they had access only to unannotated recordings. The most salient discourse structure in a meeting is the topic of conversation. The content within a given topic is cohesive and should therefore be viewed as a whole. In MDTMs, the meeting consists basically of successive patient case discussions (PCDs) in which the patient’s condition is discussed among different medical specialists with the objective of agreeing diagnoses</context>
<context position="23389" citStr="Banerjee et al., 2005" startWordPosition="3750" endWordPosition="3753">ring improve segmentation. As expected, accuracy figures were generally high (an uninformative) reflecting the great imbalance in favour of negative instances and the conservative nature of the classifier. Precision, recall and F1 (for positive instances only) were also predictably low, with V�p under a proportional threshold attaining the best results. However, in meeting browsing marking the topic boundary precisely is far less important than retrieving the right text is in information retrieval or text categorisation, since the user can easily scan the neighbouring intervals with a slider (Banerjee et al., 2005). Therefore, Pk and WD are the most appropriate measures of success in this task. Here our results seem quite encouraging, given that they all represent great improvements over the (rather reasonable) baselines of Pk = .46 and WD = .51 estimated by Monte Carlo simulation as in (Hsueh et al., 2006) by hypothesising the same proportion of boundaries found in the training set. Our results also compare favourably with some of the best results reported in the meeting segmentation literature to date, namely Pk = 0.32 and WD = 0.36, for a lexical cohesion algorithm on the ICSI corpus (Galley et al., </context>
</contexts>
<marker>Banerjee, Rose, Rudnicky, 2005</marker>
<rawString>S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The necessity of a meeting recording and playback system, and the benefit of topic-level annotations to meeting browsing. In Proceedings of the 10th International Conference on Human-Computer Interaction (INTERACT’05), pages 643–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--177</pages>
<contexts>
<context position="19033" citStr="Beeferman et al., 1999" startWordPosition="3023" endWordPosition="3026"> standard metrics provide an initial approximation to segmentation effectiveness, they have been criticised as tools for evaluating segmentation because they are hard to interpret and are not sensitive to near misses (Pevzner and Hearst, 2002). Furthermore, due to the highly unbalanced nature of the classification task (boundary vocalisation events are only 3.3% of all instances), accuracy scores tend to produce overoptimistic results. Therefore, to give a fairer picture of the effectiveness of our method, we also report values for two error metrics proposed specifically for segmentation: Pk (Beeferman et al., 1999) and WindowDiff, or WD, (Pevzner and Hearst, 2002). The Pk metric gives the probability that two vocalisation events occurring k vocalisations apart and picked otherwise randomly from the dataset are incorrectly identified by the algorithm as belonging to the same or to different topics. Pk is computed by sliding two pairs of pointers over the reference and the hypothesis sequences and observing whether each pair of pointers rests in the same or in different segments. An error is counted if the pairs disagree (i.e. if they point to the same segment in one sequence and to different segments in </context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>D. Beeferman, A. Berger, and J. Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34:177–210, Feb. 10.1023/A:1007506220214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Unleashing the killer corpus: experiences in creating the multi-everything ami meeting corpus. Language Resources and Evaluation,</title>
<date>2007</date>
<pages>41--2</pages>
<contexts>
<context position="4147" citStr="Carletta, 2007" startWordPosition="656" endWordPosition="657">ferent “topics” which make up an MDTM (Luz, 2009). In this paper we explore the use of a contentfree approach to the representation of vocalisation events for segmentation of MDTM dialogues. We start by extending the work of Luz (2009) on a small corpus of MDTM recordings, and then test our approach on a larger dataset, the AMI (AugProceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 332–339, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 332 mented Multi-Party Interaction) corpus (Carletta, 2007). Our ultimate goal is to analyse and apply the insights gained on the AMI corpus to our work on data gathering and representation in MDTMs. 2 Related work Topic segmentation and detection, as an aid to meeting information retrieval and meeting indexing, has attracted the interest of many researchers in recent years. The objective of topic segmentation is to locate the beginning and end time of a cohesive segment of dialogue which can be singled out as a “topic”. Meeting topic segmentation has been strongly influenced by techniques developed for topic segmentation in text (Hearst, 1997), and m</context>
<context position="13850" citStr="Carletta, 2007" startWordPosition="2192" endWordPosition="2194">ll received the speech signal, possibly on a single channel, and pre-segment it into separate channels (one per speaker) with intervals of speech activity and silence labelled for each stream. Depending on the quality of the recording and the characteristics of the environment, this initial processing stage can be accomplished automatically through existing speaker diarisation methods — e.g. (Ajmera and Wooters, 2003). In the experiments reported below manual annotation was employed. In the AMI corpus, speaker and speech activity annotation is done on the word level and include transcription (Carletta, 2007). We parsed these word-level labels, ignoring the transcriptions, in order to build the content-free representation described above. Once the data representation has been created it is then used, along with topic boundary annotations, to train a probabilistic classifier. Finally, the topic detection module uses the models generated in the training phase to hypothesise boundaries in unannotated vocalisation event sequences and, optionally, performs post-processing of these sequences before returning the final hypothesis. These modules are described in more detail below. 4 MDTM Segmentation The </context>
<context position="24551" citStr="Carletta, 2007" startWordPosition="3947" endWordPosition="3948">ohesion algorithm on the ICSI corpus (Galley et al., 2003), and Pk = 0.34 and WD = 0.36, for a maximum entropy approach combining lexical, conversational and video features on the AMI corpus (Hsueh et al., 2006). Although these results are promising, they pose a question as regards data representation. While The AMI corpus is a collection of meetings recorded under controlled conditions, many of which have a fixed scenario, goals and assigned participant roles. The corpus is manually transcribed, and annotated with word-level timings and a variety of metadata, including topics and sub-topics (Carletta, 2007). Transcriptions in the AMI corpus are extracted from redundant recording channels (lapel, headset and array microphones), and stored separately for each participant. Because timing information in AMI is so detailed, we were able to create much richer VH representations, including finer grained pause and overlap information. The original XML-encoded AMI data were parsed and collated to produce our variants of the VH scheme. We tested four types of VH: V�, which includes only vocalisation events; V9, which includes only pause and speech overlap events; V�, which includes all vocalisations, paus</context>
</contexts>
<marker>Carletta, 2007</marker>
<rawString>J. Carletta. 2007. Unleashing the killer corpus: experiences in creating the multi-everything ami meeting corpus. Language Resources and Evaluation, 41(2):181–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M J Dabbs</author>
<author>B Ruback</author>
</authors>
<title>Dimensions of group process: Amount and structure of vocal interaction.</title>
<date>1987</date>
<booktitle>Advances in Experimental Social Psychology,</booktitle>
<pages>20--123</pages>
<contexts>
<context position="8589" citStr="Dabbs and Ruback (1987)" startWordPosition="1334" endWordPosition="1337">the illocutionary force of vocalisations, for instance, is well documented (Holmes, 1984), and prosodic features have been used for automatic segmentation of broadcast news data into sentences and topics (Shriberg et al., 2000). Similarly, recurring audio patterns have been employed in segmentation of recorded lectures (Malioutov et al., 2007). Works in the area of social psychology have used the simple conversational features of duration of vocalisations, pauses and overlaps to study the dynamics of group interaction. Jaffe and Feldstein (1970) characterise dialogues as Markov processes, and Dabbs and Ruback (1987) suggest that a “contentfree” method based on the amount and structure of vocal interactions could complement group interaction frameworks such as the one proposed by Bales (1950). Pauses and overlap statistics alone can be used, for instance, to characterise 333 Event durations Gaps &amp; Overlaps Speakers Vocalisation events ... ... ... ... Figure 1: Vocalisation Horizon for event v. the differences between face-to-face and telephone dialogue (ten Bosch et al., 2005), and a correlation between the duration of pauses and topic boundaries has been demonstrated for recordings of spontaneous narrati</context>
<context position="17967" citStr="Dabbs and Ruback, 1987" startWordPosition="2859" endWordPosition="2862">n segm. by speaker topic-level annotation ... , S = s,..., Dn = dn) 335 Figure 3: Bayesian model employed for dialogue segmentation. dencies are between the boundary variable and each feature of the vocalisation event, as shown in Figure 3. Luz (2009) reports that, for a similar data representation, horizons of length 2 &lt; n &lt; 6 produced the best segmentation results. Following this finding, we adopt n = 3 for all our experiments. We tested two variants of the representation: Vpd that discriminated between pause types (pauses, switching pauses, group pauses, and group switching pauses), as in (Dabbs and Ruback, 1987), and V,p which labelled all pauses equally. The evaluation metrics employed include the standard classification metrics of accuracy (A), the proportion of correctly classified segments, boundary precision (P), the proportion of correctly assigned boundaries among all events marked as topic boundaries, boundary recall (R), the proportion of target boundaries correctly assigned, and the F1 score, the harmonic mean of P and R. Although these standard metrics provide an initial approximation to segmentation effectiveness, they have been criticised as tools for evaluating segmentation because they</context>
</contexts>
<marker>Dabbs, Ruback, 1987</marker>
<rawString>J. M. J. Dabbs and B. Ruback. 1987. Dimensions of group process: Amount and structure of vocal interaction. Advances in Experimental Social Psychology, 20(123–169).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K R McKeown</author>
<author>E Fosler-Lussier</author>
<author>H Jing</author>
</authors>
<title>Discourse segmentation of multiparty conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>562--569</pages>
<contexts>
<context position="4975" citStr="Galley et al., 2003" startWordPosition="790" endWordPosition="793">ng information retrieval and meeting indexing, has attracted the interest of many researchers in recent years. The objective of topic segmentation is to locate the beginning and end time of a cohesive segment of dialogue which can be singled out as a “topic”. Meeting topic segmentation has been strongly influenced by techniques developed for topic segmentation in text (Hearst, 1997), and more recently in broadcast news audio, even though it is generally acknowledged that dialogue segmentation differs from text and scripted speech in important respects (Gruenstein et al., 2005). In early work (Galley et al., 2003), meeting annotation focused on changes that produce high inter-annotator agreement, with no further specification of topic label or discourse structure. Current work has paid greater attention to discourse structure, as reflected in two major meeting corpus gathering and analysis projects: the AMI project (Renals et al., 2007) and the ICSI meeting project (Morgan et al., 2001). The AMI corpus distinguishes top-level and functional topics such as “presentation”, “discussion”, “opening”, “closing”, “agenda” which are further specified into sub-topics (Hsueh et al., 2006). Gruenstein et al. (200</context>
<context position="6276" citStr="Galley et al., 2003" startWordPosition="987" endWordPosition="990">ying, in addition, action items and decision points. In contrast to these more general types of meetings, MDTMs are segmented into better defined units (i.e. PCDs) so that inter-annotator agreement on topic (patient case discussion) boundaries is less of an issue, since PCDs are collectively agreed parts of the formal structure of the meetings. Meeting transcripts (either done manually or automatically) have formed the basis for a number of approaches to topic segmentation (Galley et al., 2003; Hsueh et al., 2006; Sherman and Liu, 2008). The transcript-based meeting segmentation described in (Galley et al., 2003) adapted the unsupervised lexical cohesion method developed for written text segmentation (Hearst, 1997). Other approaches have employed supervised machine learning methods with textual features (Hsueh et al., 2006). Prosodic and conversational features have also been integrated into text-based representations, often improving segmentation accuracy (Galley et al., 2003; Hsueh and Moore, 2007). However, approaches that rely on transcription, and sometimes higher-level annotation on transcripts, as is the case of (Sherman and Liu, 2008), have two shortcomings which limit their applicability to M</context>
<context position="23994" citStr="Galley et al., 2003" startWordPosition="3856" endWordPosition="3860">e et al., 2005). Therefore, Pk and WD are the most appropriate measures of success in this task. Here our results seem quite encouraging, given that they all represent great improvements over the (rather reasonable) baselines of Pk = .46 and WD = .51 estimated by Monte Carlo simulation as in (Hsueh et al., 2006) by hypothesising the same proportion of boundaries found in the training set. Our results also compare favourably with some of the best results reported in the meeting segmentation literature to date, namely Pk = 0.32 and WD = 0.36, for a lexical cohesion algorithm on the ICSI corpus (Galley et al., 2003), and Pk = 0.34 and WD = 0.36, for a maximum entropy approach combining lexical, conversational and video features on the AMI corpus (Hsueh et al., 2006). Although these results are promising, they pose a question as regards data representation. While The AMI corpus is a collection of meetings recorded under controlled conditions, many of which have a fixed scenario, goals and assigned participant roles. The corpus is manually transcribed, and annotated with word-level timings and a variety of metadata, including topics and sub-topics (Carletta, 2007). Transcriptions in the AMI corpus are extr</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>M. Galley, K. R. McKeown, E. Fosler-Lussier, and H. Jing. 2003. Discourse segmentation of multiparty conversation. In Proceedings of the 41st Annual Meeting of the ACL, pages 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gruenstein</author>
<author>J Niekrasz</author>
<author>M Purver</author>
</authors>
<title>Meeting structure annotation: Data and tools.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>117--127</pages>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="4938" citStr="Gruenstein et al., 2005" startWordPosition="783" endWordPosition="786">ntation and detection, as an aid to meeting information retrieval and meeting indexing, has attracted the interest of many researchers in recent years. The objective of topic segmentation is to locate the beginning and end time of a cohesive segment of dialogue which can be singled out as a “topic”. Meeting topic segmentation has been strongly influenced by techniques developed for topic segmentation in text (Hearst, 1997), and more recently in broadcast news audio, even though it is generally acknowledged that dialogue segmentation differs from text and scripted speech in important respects (Gruenstein et al., 2005). In early work (Galley et al., 2003), meeting annotation focused on changes that produce high inter-annotator agreement, with no further specification of topic label or discourse structure. Current work has paid greater attention to discourse structure, as reflected in two major meeting corpus gathering and analysis projects: the AMI project (Renals et al., 2007) and the ICSI meeting project (Morgan et al., 2001). The AMI corpus distinguishes top-level and functional topics such as “presentation”, “discussion”, “opening”, “closing”, “agenda” which are further specified into sub-topics (Hsueh </context>
</contexts>
<marker>Gruenstein, Niekrasz, Purver, 2005</marker>
<rawString>A. Gruenstein, J. Niekrasz, and M. Purver. 2005. Meeting structure annotation: Data and tools. In Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue, pages 117–127, Lisbon, Portugal, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Texttiling: segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Comput. Linguist.,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="4740" citStr="Hearst, 1997" startWordPosition="756" endWordPosition="757">pus (Carletta, 2007). Our ultimate goal is to analyse and apply the insights gained on the AMI corpus to our work on data gathering and representation in MDTMs. 2 Related work Topic segmentation and detection, as an aid to meeting information retrieval and meeting indexing, has attracted the interest of many researchers in recent years. The objective of topic segmentation is to locate the beginning and end time of a cohesive segment of dialogue which can be singled out as a “topic”. Meeting topic segmentation has been strongly influenced by techniques developed for topic segmentation in text (Hearst, 1997), and more recently in broadcast news audio, even though it is generally acknowledged that dialogue segmentation differs from text and scripted speech in important respects (Gruenstein et al., 2005). In early work (Galley et al., 2003), meeting annotation focused on changes that produce high inter-annotator agreement, with no further specification of topic label or discourse structure. Current work has paid greater attention to discourse structure, as reflected in two major meeting corpus gathering and analysis projects: the AMI project (Renals et al., 2007) and the ICSI meeting project (Morga</context>
<context position="6380" citStr="Hearst, 1997" startWordPosition="1003" endWordPosition="1004"> are segmented into better defined units (i.e. PCDs) so that inter-annotator agreement on topic (patient case discussion) boundaries is less of an issue, since PCDs are collectively agreed parts of the formal structure of the meetings. Meeting transcripts (either done manually or automatically) have formed the basis for a number of approaches to topic segmentation (Galley et al., 2003; Hsueh et al., 2006; Sherman and Liu, 2008). The transcript-based meeting segmentation described in (Galley et al., 2003) adapted the unsupervised lexical cohesion method developed for written text segmentation (Hearst, 1997). Other approaches have employed supervised machine learning methods with textual features (Hsueh et al., 2006). Prosodic and conversational features have also been integrated into text-based representations, often improving segmentation accuracy (Galley et al., 2003; Hsueh and Moore, 2007). However, approaches that rely on transcription, and sometimes higher-level annotation on transcripts, as is the case of (Sherman and Liu, 2008), have two shortcomings which limit their applicability to MDTM indexing. First, manual transcription is unfeasible in a busy hospital setting, and automatic speech</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>M. A. Hearst. 1997. Texttiling: segmenting text into multi-paragraph subtopic passages. Comput. Linguist., 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Holmes</author>
</authors>
<title>Modifying illocutionary force.</title>
<date>1984</date>
<journal>Journal of Pragmatics, 8(3):345 –</journal>
<pages>365</pages>
<contexts>
<context position="8055" citStr="Holmes, 1984" startWordPosition="1255" endWordPosition="1256">entation. The approach investigated in this paper goes a step further by representing the data solely through what is, arguably, the simplest form of content-free representation, namely: duration of talk spurts, silences and speech overlaps, optionally complemented with speaker role information (e.g. medical speciality). 3 Content-free representations There is more to the structure (and even the semantics) of a dialogue than the textual content of the words exchanged by its participants. The role of prosody in shaping the illocutionary force of vocalisations, for instance, is well documented (Holmes, 1984), and prosodic features have been used for automatic segmentation of broadcast news data into sentences and topics (Shriberg et al., 2000). Similarly, recurring audio patterns have been employed in segmentation of recorded lectures (Malioutov et al., 2007). Works in the area of social psychology have used the simple conversational features of duration of vocalisations, pauses and overlaps to study the dynamics of group interaction. Jaffe and Feldstein (1970) characterise dialogues as Markov processes, and Dabbs and Ruback (1987) suggest that a “contentfree” method based on the amount and struc</context>
</contexts>
<marker>Holmes, 1984</marker>
<rawString>J. Holmes. 1984. Modifying illocutionary force. Journal of Pragmatics, 8(3):345 – 365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hsueh</author>
<author>J D Moore</author>
</authors>
<title>Combining multiple knowledge sources for dialogue segmentation in multimedia archives.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<publisher>ACL Press.</publisher>
<contexts>
<context position="6671" citStr="Hsueh and Moore, 2007" startWordPosition="1040" endWordPosition="1043">utomatically) have formed the basis for a number of approaches to topic segmentation (Galley et al., 2003; Hsueh et al., 2006; Sherman and Liu, 2008). The transcript-based meeting segmentation described in (Galley et al., 2003) adapted the unsupervised lexical cohesion method developed for written text segmentation (Hearst, 1997). Other approaches have employed supervised machine learning methods with textual features (Hsueh et al., 2006). Prosodic and conversational features have also been integrated into text-based representations, often improving segmentation accuracy (Galley et al., 2003; Hsueh and Moore, 2007). However, approaches that rely on transcription, and sometimes higher-level annotation on transcripts, as is the case of (Sherman and Liu, 2008), have two shortcomings which limit their applicability to MDTM indexing. First, manual transcription is unfeasible in a busy hospital setting, and automatic speech recognition of unconstrained, noisy dialogues falls short of the levels of accuracy required for good segmentation. Secondly, the contents of MDTMs are subject to stringent privacy and confidentiality constraints which limit access to training data. Regardless of such application constrain</context>
</contexts>
<marker>Hsueh, Moore, 2007</marker>
<rawString>P. Hsueh and J. D. Moore. 2007. Combining multiple knowledge sources for dialogue segmentation in multimedia archives. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hsueh</author>
<author>J D Moore</author>
<author>S Renals</author>
</authors>
<title>Automatic segmentation of multiparty dialogue.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the ACL (EACL),</booktitle>
<pages>273--277</pages>
<publisher>ACL Press.</publisher>
<contexts>
<context position="5551" citStr="Hsueh et al., 2006" startWordPosition="876" endWordPosition="879"> 2005). In early work (Galley et al., 2003), meeting annotation focused on changes that produce high inter-annotator agreement, with no further specification of topic label or discourse structure. Current work has paid greater attention to discourse structure, as reflected in two major meeting corpus gathering and analysis projects: the AMI project (Renals et al., 2007) and the ICSI meeting project (Morgan et al., 2001). The AMI corpus distinguishes top-level and functional topics such as “presentation”, “discussion”, “opening”, “closing”, “agenda” which are further specified into sub-topics (Hsueh et al., 2006). Gruenstein et al. (2005) sought to annotated the ICSI corpus hierarchically according to topic, identifying, in addition, action items and decision points. In contrast to these more general types of meetings, MDTMs are segmented into better defined units (i.e. PCDs) so that inter-annotator agreement on topic (patient case discussion) boundaries is less of an issue, since PCDs are collectively agreed parts of the formal structure of the meetings. Meeting transcripts (either done manually or automatically) have formed the basis for a number of approaches to topic segmentation (Galley et al., 2</context>
<context position="23687" citStr="Hsueh et al., 2006" startWordPosition="3802" endWordPosition="3805">r a proportional threshold attaining the best results. However, in meeting browsing marking the topic boundary precisely is far less important than retrieving the right text is in information retrieval or text categorisation, since the user can easily scan the neighbouring intervals with a slider (Banerjee et al., 2005). Therefore, Pk and WD are the most appropriate measures of success in this task. Here our results seem quite encouraging, given that they all represent great improvements over the (rather reasonable) baselines of Pk = .46 and WD = .51 estimated by Monte Carlo simulation as in (Hsueh et al., 2006) by hypothesising the same proportion of boundaries found in the training set. Our results also compare favourably with some of the best results reported in the meeting segmentation literature to date, namely Pk = 0.32 and WD = 0.36, for a lexical cohesion algorithm on the ICSI corpus (Galley et al., 2003), and Pk = 0.34 and WD = 0.36, for a maximum entropy approach combining lexical, conversational and video features on the AMI corpus (Hsueh et al., 2006). Although these results are promising, they pose a question as regards data representation. While The AMI corpus is a collection of meeting</context>
</contexts>
<marker>Hsueh, Moore, Renals, 2006</marker>
<rawString>P. Hsueh, J. D. Moore, and S. Renals. 2006. Automatic segmentation of multiparty dialogue. In Proceedings of the 11th Conference of the European Chapter of the ACL (EACL), pages 273–277. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jaffe</author>
<author>S Feldstein</author>
</authors>
<title>Rhythms of dialogue.</title>
<date>1970</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="8517" citStr="Jaffe and Feldstein (1970)" startWordPosition="1324" endWordPosition="1327">of the words exchanged by its participants. The role of prosody in shaping the illocutionary force of vocalisations, for instance, is well documented (Holmes, 1984), and prosodic features have been used for automatic segmentation of broadcast news data into sentences and topics (Shriberg et al., 2000). Similarly, recurring audio patterns have been employed in segmentation of recorded lectures (Malioutov et al., 2007). Works in the area of social psychology have used the simple conversational features of duration of vocalisations, pauses and overlaps to study the dynamics of group interaction. Jaffe and Feldstein (1970) characterise dialogues as Markov processes, and Dabbs and Ruback (1987) suggest that a “contentfree” method based on the amount and structure of vocal interactions could complement group interaction frameworks such as the one proposed by Bales (1950). Pauses and overlap statistics alone can be used, for instance, to characterise 333 Event durations Gaps &amp; Overlaps Speakers Vocalisation events ... ... ... ... Figure 1: Vocalisation Horizon for event v. the differences between face-to-face and telephone dialogue (ten Bosch et al., 2005), and a correlation between the duration of pauses and topi</context>
</contexts>
<marker>Jaffe, Feldstein, 1970</marker>
<rawString>J. Jaffe and S. Feldstein. 1970. Rhythms of dialogue. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H John</author>
<author>P Langley</author>
</authors>
<title>Estimating continuous distributions in Bayesian classifiers.</title>
<date>1995</date>
<booktitle>In Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence (UAI’95),</booktitle>
<pages>338--345</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Francisco, CA, USA,</location>
<contexts>
<context position="16826" citStr="John and Langley, 1995" startWordPosition="2666" endWordPosition="2669">the top of the chart) are incorporated into an adjacent vocalisation event. The segmentation process can be defined as the process of mapping the set of vocalisation events V to 10, 11 where 1 represents a topic boundary and 0 represents a non-boundary vocalisation event. In order to implement this mapping we employ a Naive Bayes classifier. The conditional probabilities for the nominal variables (speaker roles) are estimated on the training set by maximum likelihood and combined into multinomial models, while the continuous variables are log transformed and modelled through Gaussian kernels (John and Langley, 1995). These models are used to estimate the probability, given by equation (1), of a vocalisation being marked as a topic boundary given the above described data representation, and the usual conditional independence assumptions applies. P(B = bIV = v) = P(B = b|Sf = sf, DT,, = dn, (1) The model can therefore be represented as a simple Bayesian network where the only depenSpeaker diarisation Speech signal or Topic boundary detection Topic markers ... ... ... ... ... ... Classifier induction word-level transcript. Turn segm. by speaker topic-level annotation ... , S = s,..., Dn = dn) 335 Figure 3: </context>
</contexts>
<marker>John, Langley, 1995</marker>
<rawString>G. H. John and P. Langley. 1995. Estimating continuous distributions in Bayesian classifiers. In Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence (UAI’95), pages 338–345, San Francisco, CA, USA, August. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kane</author>
<author>S Luz</author>
</authors>
<title>Multidisciplinary medical team meetings: An analysis of collaborative working with special attention to timing and teleconferencing.</title>
<date>2006</date>
<booktitle>Computer Supported Cooperative Work (CSCW),</booktitle>
<pages>15--5</pages>
<contexts>
<context position="2237" citStr="Kane and Luz, 2006" startWordPosition="343" endWordPosition="346">e focus of our interest in this area is the study of multi-party interaction at Multidisciplinary Medical Team Meeting (MDTMs), and the eventual recording of such meetings followed by indexing and structuring for integration into electronic health records. MDTMs share a number of characteristics with more conventional business meetings, and with the meeting scenarios targeted in recent research projects (Renals et al., 2007). However, MDTMs are better structured than these meetings, and more strongly influenced by the time pressures placed upon the medical professionals who take part in them (Kane and Luz, 2006). In order for meeting support and review systems to be truly effective, they must allow users to efficiently browse and retrieve information of interest from the recorded data. Browsing in these media can be tedious and time consuming because continuous media such as audio and video are difficult to access since they lack natural reference points. A good deal of research has been conducted on indexing recorded meetings. From a user’s point of view, an important aspect of indexing continuous media, and audio in particular, is the task of structuring the recorded content. Banerjee et al. (2005)</context>
<context position="14583" citStr="Kane and Luz, 2006" startWordPosition="2301" endWordPosition="2304">n described above. Once the data representation has been created it is then used, along with topic boundary annotations, to train a probabilistic classifier. Finally, the topic detection module uses the models generated in the training phase to hypothesise boundaries in unannotated vocalisation event sequences and, optionally, performs post-processing of these sequences before returning the final hypothesis. These modules are described in more detail below. 4 MDTM Segmentation The MDTM corpus was collected over a period of three years as part of a detailed ethnographic study of medical teams (Kane and Luz, 2006). The corpus consists in 28 hours or meetings recorded in a dedicated teleconferencing room at a major primary care hospital. The audio sources included a pressure-zone microphone attached to the teleconferencing system and a highly sensitive directional microphone. Video was gathered through two separate sources: the teleconferencing system, which showed the participants and, at times, the medical images (pathology slides, radiology) relevant to the case under discussion, and a high-end camcorder mounted on a tripod. All data were imported into a multimedia annotation tool and synchronised. O</context>
</contexts>
<marker>Kane, Luz, 2006</marker>
<rawString>B. Kane and S. Luz. 2006. Multidisciplinary medical team meetings: An analysis of collaborative working with special attention to timing and teleconferencing. Computer Supported Cooperative Work (CSCW), 15(5):501–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Luz</author>
</authors>
<title>Locating case discussion segments in recorded medical team meetings.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACM Multimedia Workshop on Searching Spontaneous Conversational Speech (SSCS’09),</booktitle>
<pages>21--30</pages>
<publisher>ACM Press.</publisher>
<location>Beijing, China,</location>
<contexts>
<context position="3581" citStr="Luz, 2009" startWordPosition="567" endWordPosition="568">ion than in a control condition in which they had access only to unannotated recordings. The most salient discourse structure in a meeting is the topic of conversation. The content within a given topic is cohesive and should therefore be viewed as a whole. In MDTMs, the meeting consists basically of successive patient case discussions (PCDs) in which the patient’s condition is discussed among different medical specialists with the objective of agreeing diagnoses, making patient management decisions etc. Thus, the individual PCDs can be regarded as the different “topics” which make up an MDTM (Luz, 2009). In this paper we explore the use of a contentfree approach to the representation of vocalisation events for segmentation of MDTM dialogues. We start by extending the work of Luz (2009) on a small corpus of MDTM recordings, and then test our approach on a larger dataset, the AMI (AugProceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 332–339, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 332 mented Multi-Party Interaction) corpus (Carletta, 2007). Our ultimate goal is to analyse </context>
<context position="17595" citStr="Luz (2009)" startWordPosition="2798" endWordPosition="2799">epresentation, and the usual conditional independence assumptions applies. P(B = bIV = v) = P(B = b|Sf = sf, DT,, = dn, (1) The model can therefore be represented as a simple Bayesian network where the only depenSpeaker diarisation Speech signal or Topic boundary detection Topic markers ... ... ... ... ... ... Classifier induction word-level transcript. Turn segm. by speaker topic-level annotation ... , S = s,..., Dn = dn) 335 Figure 3: Bayesian model employed for dialogue segmentation. dencies are between the boundary variable and each feature of the vocalisation event, as shown in Figure 3. Luz (2009) reports that, for a similar data representation, horizons of length 2 &lt; n &lt; 6 produced the best segmentation results. Following this finding, we adopt n = 3 for all our experiments. We tested two variants of the representation: Vpd that discriminated between pause types (pauses, switching pauses, group pauses, and group switching pauses), as in (Dabbs and Ruback, 1987), and V,p which labelled all pauses equally. The evaluation metrics employed include the standard classification metrics of accuracy (A), the proportion of correctly classified segments, boundary precision (P), the proportion of</context>
</contexts>
<marker>Luz, 2009</marker>
<rawString>S. Luz. 2009. Locating case discussion segments in recorded medical team meetings. In Proceedings of the ACM Multimedia Workshop on Searching Spontaneous Conversational Speech (SSCS’09), pages 21–30, Beijing, China, October. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Malioutov</author>
<author>A Park</author>
<author>R Barzilay</author>
<author>J Glass</author>
</authors>
<title>Making sense of sound: Unsupervised topic segmentation over acoustic input.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>504--511</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7311" citStr="Malioutov et al., 2007" startWordPosition="1135" endWordPosition="1138">hes that rely on transcription, and sometimes higher-level annotation on transcripts, as is the case of (Sherman and Liu, 2008), have two shortcomings which limit their applicability to MDTM indexing. First, manual transcription is unfeasible in a busy hospital setting, and automatic speech recognition of unconstrained, noisy dialogues falls short of the levels of accuracy required for good segmentation. Secondly, the contents of MDTMs are subject to stringent privacy and confidentiality constraints which limit access to training data. Regardless of such application constraints, some authors (Malioutov et al., 2007; Shriberg et al., 2000) argue for the use of prosodic features and other acoustic patterns directly from the audio signal for segmentation. The approach investigated in this paper goes a step further by representing the data solely through what is, arguably, the simplest form of content-free representation, namely: duration of talk spurts, silences and speech overlaps, optionally complemented with speaker role information (e.g. medical speciality). 3 Content-free representations There is more to the structure (and even the semantics) of a dialogue than the textual content of the words exchang</context>
</contexts>
<marker>Malioutov, Park, Barzilay, Glass, 2007</marker>
<rawString>I. Malioutov, A. Park, R. Barzilay, and J. Glass. 2007. Making sense of sound: Unsupervised topic segmentation over acoustic input. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 504–511, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Morgan</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>A Janin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>The meeting project at ICSI.</title>
<date>2001</date>
<booktitle>In Procs. of Human Language Technologies Conference,</booktitle>
<location>San Diego.</location>
<contexts>
<context position="5355" citStr="Morgan et al., 2001" startWordPosition="850" endWordPosition="853">1997), and more recently in broadcast news audio, even though it is generally acknowledged that dialogue segmentation differs from text and scripted speech in important respects (Gruenstein et al., 2005). In early work (Galley et al., 2003), meeting annotation focused on changes that produce high inter-annotator agreement, with no further specification of topic label or discourse structure. Current work has paid greater attention to discourse structure, as reflected in two major meeting corpus gathering and analysis projects: the AMI project (Renals et al., 2007) and the ICSI meeting project (Morgan et al., 2001). The AMI corpus distinguishes top-level and functional topics such as “presentation”, “discussion”, “opening”, “closing”, “agenda” which are further specified into sub-topics (Hsueh et al., 2006). Gruenstein et al. (2005) sought to annotated the ICSI corpus hierarchically according to topic, identifying, in addition, action items and decision points. In contrast to these more general types of meetings, MDTMs are segmented into better defined units (i.e. PCDs) so that inter-annotator agreement on topic (patient case discussion) boundaries is less of an issue, since PCDs are collectively agreed</context>
</contexts>
<marker>Morgan, Baron, Edwards, Ellis, Gelbart, Janin, Pfau, Shriberg, Stolcke, 2001</marker>
<rawString>N. Morgan, D. Baron, J. Edwards, D. Ellis, D. Gelbart, A. Janin, T. Pfau, E. Shriberg, and A. Stolcke. 2001. The meeting project at ICSI. In Procs. of Human Language Technologies Conference, San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Oliveira</author>
</authors>
<title>The role of pause occurrence and pause duration in the signaling of narrative structure,</title>
<date>2002</date>
<volume>2389</volume>
<pages>43--51</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="9209" citStr="Oliveira, 2002" startWordPosition="1431" endWordPosition="1432">est that a “contentfree” method based on the amount and structure of vocal interactions could complement group interaction frameworks such as the one proposed by Bales (1950). Pauses and overlap statistics alone can be used, for instance, to characterise 333 Event durations Gaps &amp; Overlaps Speakers Vocalisation events ... ... ... ... Figure 1: Vocalisation Horizon for event v. the differences between face-to-face and telephone dialogue (ten Bosch et al., 2005), and a correlation between the duration of pauses and topic boundaries has been demonstrated for recordings of spontaneous narratives (Oliveira, 2002). These works provided the initial motivation for our content-free representation scheme and the topic segmentation method proposed in this paper. It is easy to verify by inspection of both the corpus of medical team meetings described in Section 4 and the AMI corpus that pauses and vocalisations vary significantly in duration and position on and around topic boundaries. Table 1 shows the mean durations of vocalisations that initiate new topics or PCDs in MDTMs and the scenario-based AMI meetings, as well as the durations of pauses and overlaps that surround it (within one vocalisation event t</context>
</contexts>
<marker>Oliveira, 2002</marker>
<rawString>M. Oliveira, 2002. The role of pause occurrence and pause duration in the signaling of narrative structure, volume 2389 of LNAI, pages 43–51. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pevzner</author>
<author>M A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--19</pages>
<contexts>
<context position="18653" citStr="Pevzner and Hearst, 2002" startWordPosition="2963" endWordPosition="2966">etrics employed include the standard classification metrics of accuracy (A), the proportion of correctly classified segments, boundary precision (P), the proportion of correctly assigned boundaries among all events marked as topic boundaries, boundary recall (R), the proportion of target boundaries correctly assigned, and the F1 score, the harmonic mean of P and R. Although these standard metrics provide an initial approximation to segmentation effectiveness, they have been criticised as tools for evaluating segmentation because they are hard to interpret and are not sensitive to near misses (Pevzner and Hearst, 2002). Furthermore, due to the highly unbalanced nature of the classification task (boundary vocalisation events are only 3.3% of all instances), accuracy scores tend to produce overoptimistic results. Therefore, to give a fairer picture of the effectiveness of our method, we also report values for two error metrics proposed specifically for segmentation: Pk (Beeferman et al., 1999) and WindowDiff, or WD, (Pevzner and Hearst, 2002). The Pk metric gives the probability that two vocalisation events occurring k vocalisations apart and picked otherwise randomly from the dataset are incorrectly identifi</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>L. Pevzner and M. A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28:19–36, Mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Renals</author>
<author>T Hain</author>
<author>H Bourlard</author>
</authors>
<title>Recognition and interpretation of meetings: The AMI and AMIDA projects.</title>
<date>2007</date>
<booktitle>In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU ’07).</booktitle>
<contexts>
<context position="2046" citStr="Renals et al., 2007" startWordPosition="312" endWordPosition="315">e their way into the meeting rooms and the workplace in general, the need arises for modelling and analysis of dialogue and human communicative behaviour in general (Banerjee et al., 2005). The focus of our interest in this area is the study of multi-party interaction at Multidisciplinary Medical Team Meeting (MDTMs), and the eventual recording of such meetings followed by indexing and structuring for integration into electronic health records. MDTMs share a number of characteristics with more conventional business meetings, and with the meeting scenarios targeted in recent research projects (Renals et al., 2007). However, MDTMs are better structured than these meetings, and more strongly influenced by the time pressures placed upon the medical professionals who take part in them (Kane and Luz, 2006). In order for meeting support and review systems to be truly effective, they must allow users to efficiently browse and retrieve information of interest from the recorded data. Browsing in these media can be tedious and time consuming because continuous media such as audio and video are difficult to access since they lack natural reference points. A good deal of research has been conducted on indexing rec</context>
<context position="5304" citStr="Renals et al., 2007" startWordPosition="840" endWordPosition="843"> developed for topic segmentation in text (Hearst, 1997), and more recently in broadcast news audio, even though it is generally acknowledged that dialogue segmentation differs from text and scripted speech in important respects (Gruenstein et al., 2005). In early work (Galley et al., 2003), meeting annotation focused on changes that produce high inter-annotator agreement, with no further specification of topic label or discourse structure. Current work has paid greater attention to discourse structure, as reflected in two major meeting corpus gathering and analysis projects: the AMI project (Renals et al., 2007) and the ICSI meeting project (Morgan et al., 2001). The AMI corpus distinguishes top-level and functional topics such as “presentation”, “discussion”, “opening”, “closing”, “agenda” which are further specified into sub-topics (Hsueh et al., 2006). Gruenstein et al. (2005) sought to annotated the ICSI corpus hierarchically according to topic, identifying, in addition, action items and decision points. In contrast to these more general types of meetings, MDTMs are segmented into better defined units (i.e. PCDs) so that inter-annotator agreement on topic (patient case discussion) boundaries is l</context>
</contexts>
<marker>Renals, Hain, Bourlard, 2007</marker>
<rawString>S. Renals, T. Hain, and H. Bourlard. 2007. Recognition and interpretation of meetings: The AMI and AMIDA projects. In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU ’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sherman</author>
<author>Y Liu</author>
</authors>
<title>Using hidden Markov models for topic segmentation of meeting transcripts.</title>
<date>2008</date>
<booktitle>In Proceedings of the IEEE Spoken Language Technology Workshop,</booktitle>
<pages>185--188</pages>
<contexts>
<context position="6198" citStr="Sherman and Liu, 2008" startWordPosition="977" endWordPosition="980">) sought to annotated the ICSI corpus hierarchically according to topic, identifying, in addition, action items and decision points. In contrast to these more general types of meetings, MDTMs are segmented into better defined units (i.e. PCDs) so that inter-annotator agreement on topic (patient case discussion) boundaries is less of an issue, since PCDs are collectively agreed parts of the formal structure of the meetings. Meeting transcripts (either done manually or automatically) have formed the basis for a number of approaches to topic segmentation (Galley et al., 2003; Hsueh et al., 2006; Sherman and Liu, 2008). The transcript-based meeting segmentation described in (Galley et al., 2003) adapted the unsupervised lexical cohesion method developed for written text segmentation (Hearst, 1997). Other approaches have employed supervised machine learning methods with textual features (Hsueh et al., 2006). Prosodic and conversational features have also been integrated into text-based representations, often improving segmentation accuracy (Galley et al., 2003; Hsueh and Moore, 2007). However, approaches that rely on transcription, and sometimes higher-level annotation on transcripts, as is the case of (Sher</context>
</contexts>
<marker>Sherman, Liu, 2008</marker>
<rawString>M. Sherman and Y. Liu. 2008. Using hidden Markov models for topic segmentation of meeting transcripts. In Proceedings of the IEEE Spoken Language Technology Workshop, pages 185–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>D Hakkani-T¨ur</author>
<author>G T¨ur</author>
</authors>
<title>Prosody-based automatic segmentation of speech into sentences and topics. Speech communication,</title>
<date>2000</date>
<pages>32--1</pages>
<marker>Shriberg, Stolcke, Hakkani-T¨ur, T¨ur, 2000</marker>
<rawString>E. Shriberg, A. Stolcke, D. Hakkani-T¨ur, and G. T¨ur. 2000. Prosody-based automatic segmentation of speech into sentences and topics. Speech communication, 32(1-2):127–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L ten Bosch</author>
<author>N Oostdijk</author>
<author>L Boves</author>
</authors>
<title>On temporal aspects of turn taking in conversational dialogues.</title>
<date>2005</date>
<journal>Speech Communication,</journal>
<pages>47--80</pages>
<contexts>
<context position="9058" citStr="Bosch et al., 2005" startWordPosition="1407" endWordPosition="1410">verlaps to study the dynamics of group interaction. Jaffe and Feldstein (1970) characterise dialogues as Markov processes, and Dabbs and Ruback (1987) suggest that a “contentfree” method based on the amount and structure of vocal interactions could complement group interaction frameworks such as the one proposed by Bales (1950). Pauses and overlap statistics alone can be used, for instance, to characterise 333 Event durations Gaps &amp; Overlaps Speakers Vocalisation events ... ... ... ... Figure 1: Vocalisation Horizon for event v. the differences between face-to-face and telephone dialogue (ten Bosch et al., 2005), and a correlation between the duration of pauses and topic boundaries has been demonstrated for recordings of spontaneous narratives (Oliveira, 2002). These works provided the initial motivation for our content-free representation scheme and the topic segmentation method proposed in this paper. It is easy to verify by inspection of both the corpus of medical team meetings described in Section 4 and the AMI corpus that pauses and vocalisations vary significantly in duration and position on and around topic boundaries. Table 1 shows the mean durations of vocalisations that initiate new topics </context>
</contexts>
<marker>Bosch, Oostdijk, Boves, 2005</marker>
<rawString>L. ten Bosch, N. Oostdijk, and L. Boves. 2005. On temporal aspects of turn taking in conversational dialogues. Speech Communication, 47:80–86.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>