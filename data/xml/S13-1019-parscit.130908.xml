<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001447">
<title confidence="0.982807">
KnCe2013-CORE:Semantic Text Similarity by use of Knowledge Bases
</title>
<author confidence="0.994276">
Hermann Ziak
</author>
<affiliation confidence="0.987319">
Know-Center GmbH
Graz University of Technology
</affiliation>
<address confidence="0.9394865">
Inffeldgasse 13/ 6. Stock
8010 Graz, Austria
</address>
<email confidence="0.910953">
hziak@know-center.at
</email>
<author confidence="0.989565">
Roman Kern
</author>
<affiliation confidence="0.9861475">
Know-Center GmbH
Graz University of Technology
</affiliation>
<address confidence="0.9462305">
Inffeldgasse 13/ 6. Stock
8010 Graz, Austria
</address>
<email confidence="0.940017">
rkern@know-center.at
</email>
<sectionHeader confidence="0.994673" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995659647058823">
In this paper we describe KnCe2013-CORE,
a system to compute the semantic similarity
of two short text snippets. The system com-
putes a number of features which are gath-
ered from different knowledge bases, namely
WordNet, Wikipedia and Wiktionary. The
similarity scores derived from these features
are then fed into several multilayer perceptron
neuronal networks. Depending on the size
of the text snippets different parameters for
the neural networks are used. The final out-
put of the neural networks is compared to hu-
man judged data. In the evaluation our system
performed sufficiently well for text snippets
of equal length, but the performance dropped
considerably once the pairs of text snippets
differ in size.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998925">
The task of the semantic sentence similarity is to as-
sign a score to a given pair of sentences. This score
should reflect the degree by which the two sentences
represent the same meaning. The semantic similar-
ity of two sentences could then be used in a num-
ber of different application scenarios, for example it
could help to improve the performance of informa-
tion retrieval systems.
In the past, systems based on regression mod-
els in combination with well chosen features have
demonstrated good performance on this topic[4] [6].
Therefore we took this approach as a starting point
to develop our semantic similarity system; addition-
ally, we integrated a number of existing knowledge
bases into our system. With it, trained with the data
discussed in the task specification of last year[1], we
participated in the shared task of SEM 2013.
Additionally, to the similarity based on the fea-
tures derived from the external knowledge bases, we
employ a neural network to compute the final simi-
larity score. The motivation to use a supervised ma-
chine learning algorithm has been the observation
that the semantic similarity is heavily influenced by
the context of the human evaluator. A financial ex-
pert for example would judge sentences with finan-
cial topics different to non financial experts, if oc-
curring numbers differ from each other.
The remainder of the paper is organised as fol-
lows: In Section 2 we described our system, the
main features and the neuronal network to combine
different feature sets. In Section 3 the calculation
method of our feature values is discribed. In Sec-
tion 4 we report the results of our system based on
our experiments and the submitted results of the test
data. In Section 5 and 6 we discuss the results and
the outcome of our work.
</bodyText>
<sectionHeader confidence="0.910153" genericHeader="method">
2 System Overview
</sectionHeader>
<subsectionHeader confidence="0.98894">
2.1 Processing
</subsectionHeader>
<bodyText confidence="0.998327375">
Initially the system puts the sentence pairs of the
whole training set through our annotation pipeline.
After this process the sentence pairs are compared
to each other by our different feature scoring algo-
rithms. The result is a list of scores for each of these
pairs where every score represents a feature or part
of a feature. The processed sentences are now sep-
arated by their length and used to train the neuronal
</bodyText>
<page confidence="0.948383">
138
</page>
<subsubsectionHeader confidence="0.247202">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
</subsubsectionHeader>
<bodyText confidence="0.8853586">
and the Shared Task, pages 138–142, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
network models for each length group. The testing
data is also grouped based on the sentence length
and the score for each pair is determined by a rele-
vant model.
</bodyText>
<subsectionHeader confidence="0.999077">
2.2 Token Features
</subsectionHeader>
<bodyText confidence="0.999989692307692">
The first set of features are simply the tokens from
the two respective sentences. This feature set should
perform well, if exactly the same words are used
within the pair of sentences to be compared. But
as soon as words are replaced by their synonyms or
other semantically related words, this feature set will
not be able to capture the true similarity. Used with-
out other features it could even lead to false posi-
tive matches, for example given sentences with sim-
ilar content but containing antonyms. The tokenizer
used by our system was based on the OpenNLP
maximum entropy tokenizer, which detects token
boundaries based on probability model.
</bodyText>
<subsectionHeader confidence="0.998647">
2.3 Wiktionary Features
</subsectionHeader>
<bodyText confidence="0.906537375">
While the collaboratively created encyclopedia
Wikipedia receives a lot of attention from the gen-
eral public, as well as the research community, the
free dictionary Wiktionary1 is far lesser known. The
Wiktionary dictionary stores the information in a
semi-structured way using Wikimedia syntax, where
a single page represents a single word or phrase.
Therefore we developed a parser to extract relevant
information. In our case we were especially inter-
ested in semantically related terms, where the se-
mantic relationship is:
Representations: Set of word forms for a spe-
cific term. These terms are expected to indicate the
highest semantic similarity. This includes all flex-
ions, for example the ’s’ suffix for plural forms.
Synonyms: List of synonyms for the term.
Hyponyms: List of more specific terms.
Hypernym: Terms which represent more general
terms.
Antonym: List of terms, which represent an op-
posing sense.
Related Terms: Terms, with a semantic relation-
ship, which does not fall in the aforementioned cat-
egories. For example related terms for ’bank’ are
</bodyText>
<footnote confidence="0.964229">
1http://en.wiktionary.org
</footnote>
<bodyText confidence="0.980422">
’bankrupt’. Related terms represent only a weak se-
mantic similarity.
Derived Terms: Terms, with overlapping word
forms, such as ’bank holiday’, ’bankroll’ and ’data-
bank’ for the term ’bank’. From all the semantic
relationship types, derived terms are the weakest in-
dicator for their similarities.
</bodyText>
<subsectionHeader confidence="0.997389">
2.4 WordNet Features
</subsectionHeader>
<bodyText confidence="0.99998775">
The WordNet[5][2] features were generated identi-
cally to the Wiktionary features. We used the Word-
Net off line database and the provided library to get
a broader knowledge base. Therefore we extract the
semantically related terms of each token and saved
each class of relation. Where each dependency class
produced an one value in the final feature score list
of the sentence pairs.
</bodyText>
<subsectionHeader confidence="0.989491">
2.5 Wikification Feature
</subsectionHeader>
<bodyText confidence="0.962517">
We applied a Named Entity Recognition component,
which has been trained using Wikipedia categories
as input. Given a sentence it will annotate all found
concepts that match a Wikipedia article, together
with a confidence score. So for every found entry
by the annotator there is a list of possible associ-
ated topics. The confidence score can then be used
to score the topic information, in the final step the
evaluation values where calculated as follows:
score (s1� s2) = norm(T1iT2)
where T1 and T2 are the set of topics of the two
sentences and norm is the mean of the confidence
scores of the topics.
</bodyText>
<subsectionHeader confidence="0.977496">
2.6 Other Features
</subsectionHeader>
<bodyText confidence="0.989636666666667">
Although we mainly focused our approach on the
three core features above, others seemed to be useful
to improve the performance of the system of which
some are described below.
Numbers and Financial Expression Feature:
Some sentence pairs showed particular variations
between the main features and their actual score.
Many of these sentence pairs where quite similar
in their semantic topic but contained financial ex-
pressions or numbers that differed. Therefore these
expressions where extracted and compared against
each other with a descending score.
</bodyText>
<equation confidence="0.822002">
|T1 n T2|
</equation>
<page confidence="0.986878">
139
</page>
<bodyText confidence="0.9985477">
NGrams Feature: The ngram overlapping fea-
ture is based on a noun-phrase detection which re-
turns the noun-phrases in different ngrams. This
noun-phrase detection is a pos tagger pattern which
matches multiple nouns preceding adjectives and de-
terminers. In both sentences the ngrams where ex-
tracted and compared to each other returning only
the biggest overlapping. In the end, to produce the
evaluation values, the word-count of the overlapping
ngrams were taken.
</bodyText>
<sectionHeader confidence="0.993287" genericHeader="method">
3 Distance calculation
</sectionHeader>
<bodyText confidence="0.999055333333333">
For the calculation of the distance of the different
features we chose a slightly modified version of the
Jacquard similarity coefficient.
</bodyText>
<equation confidence="0.994397">
w
Jsc(w, l) =
l
</equation>
<bodyText confidence="0.970244454545454">
Where in this case w stands for the intersection of
la�lb
the selected feature, and l for 2 where la and lb
are the length of the sentences with or without stop-
words depending on the selected feature. The as-
sumption was that for some features the gap between
sentences where one has many stop-words and sen-
tences with none would have a crucial impact but for
others it would be detrimental. In regard to this we
used, depending on the feature, the words or words
excluding stop-words.
</bodyText>
<subsectionHeader confidence="0.997574">
3.1 Scoring
</subsectionHeader>
<bodyText confidence="0.999955130434783">
One of the main issues at the beginning of our re-
search was how to signal the absence of features to
the neuronal network. As our feature scores depend
on the length of the sentence, the absence of a partic-
ular feature (e.g. financial values) and detected fea-
tures without intersections (e.g. none of the found
financial values in the sentences are intersecting) in
the sentence pairs would lead to the same result.
Therefore we applied two different similarity
scores based on the feature set. They differ in the
result they give, if there is no overlap between the
two feature sets.
For a simple term similarity we defined our simi-
larity score as
where w stands for the intersections and S for the
word-count of the sentences. The system returns the
similarity of -1 for no overlap, which signals no sim-
ilarity at all. For fully overlapping feature sets, the
score is 1.
For other features, where we did not expect them
to occur in every sentence, for example numbers or
financial terms, the similarity score was defined as
follows:
</bodyText>
<equation confidence="0.98661525">
�
1 : s = 0 or w = 0
score(w, s, l) =
Jsc(w, l) : w &gt; 0
</equation>
<bodyText confidence="0.999878666666667">
In this case the score would yield 1 decreasing for
non overlapping feature sets and will drop to -1 the
more features differentiated. This redefines the nor-
mal state as equivalent to a total similarity of all
found features and only if features differ this value
drops.
</bodyText>
<subsectionHeader confidence="0.999913">
3.2 Sentence Length Grouping
</subsectionHeader>
<bodyText confidence="0.999997111111111">
From tests with the training data we found that our
system performed very diversly with both long and
short sentences although our features where normal-
ized to the sentence length. To cover this problem
we separated the whole collection of training data
into different groups based on their length, each of
the groups were later used to train their own model.
Finally the testing data were also divided into this
groups and were applied on the group model.
</bodyText>
<subsectionHeader confidence="0.99347">
3.3 Neural Network
</subsectionHeader>
<bodyText confidence="0.999961230769231">
We applyied multilayer perceptron neuronal net-
works on the individual sentence length groups. So
for each group of sentence length we computed sep-
arately the weights of the neural network. To model
the neural networks we used the open-source library
Neuroph.2. This network was defined with a 48-
input layer, which represented the extracted feature
scores, 4 hidden layers, and a 1-output layer which
represents the similarity score of the sentences. For
the runs referenced by table 1 and 2 we used 400000
iterations, which gave us the best results in our tests,
with a maximum error of 0.001 and a learning rate
of 0.001
</bodyText>
<equation confidence="0.954296">
�
−1 : s = 0 or w = 0
score(w, s, l) = Jsc(w,l) : w &gt; 0 2http://neuroph.sourceforge.net
</equation>
<page confidence="0.996612">
140
</page>
<sectionHeader confidence="0.97494" genericHeader="method">
4 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.999475">
The following results of our system where produced
by our test-run after the challenge deadline. For
the first run we split each training set in halfe, self-
evident without the use of the datasets published af-
ter the challenge, and used the other half to validate
our system. See table 1 for result, which contain our
system.
</bodyText>
<table confidence="0.815338333333333">
MSRvid MSRpar SMTeuroparl
Grouping 0.69 0.55 0.50
Without Grouping 0.66 0.52 0.62
</table>
<tableCaption confidence="0.996074">
Table 1: Run with and without sentence length grouping
on the training set
</tableCaption>
<bodyText confidence="0.996169166666667">
For the validation the whole 2013 test set was
used as it wasnot used for training. In table 2 the
results of our system on the test-set are listed. When
using the sentence length grouping and without sen-
tence length grouping just using a single neural net-
work for all sentence similarities.
</bodyText>
<table confidence="0.907926">
FNWN headlines OnWN SMT
Grouping 0.08 0.66 0.62 0.21
Without Grouping 0.38 0.62 0.39 0.25
</table>
<tableCaption confidence="0.927482">
Table 2: Results of our system with and without sentence
length grouping on the test set
</tableCaption>
<bodyText confidence="0.996629">
Finally, we report the results from the original
evaluation of the STS-SharedTask in table 3.
</bodyText>
<table confidence="0.99805475">
FNWN headlines OnWN SMT
KnCe2013-all 0.11 0.35 0.35 0.16
KnCe2013-diff 0.13 0.40 0.35 0.18
KnCe2013-set 0.04 0.05 -0.15 -0.06
</table>
<tableCaption confidence="0.999857">
Table 3: The submission to the challenge
</tableCaption>
<sectionHeader confidence="0.999476" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999954142857143">
Based on the results we can summarize that our
submitted system, worked well for data with very
short and simple sentences, such as the MSRvid;
however for the longer the sentences the perfor-
mance declined. The grouping based on the input
length worked well for sentences of similar length
when compared, as we used the average length of
both sentences to group them, but it seamed to fail
for sentences with very diverse lengths like in the
FNWN data set as shown in table 2. Comparing the
results of the official submission to the test runs of
our system it underperformed in all datasets. We as-
sume that the poor results in the submission run were
caused by badly chosen training settings.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999988666666667">
In our system for semantic sentence similarity we
tried to integrate a number of external knowledge
bases to improve its performance. (Viz. WordNet,
Wikipedia, Wiktionary) Furthermore, we integrated
a neural network component to replicate the similar-
ity score assigned by human judges. We used dif-
ferent sets of neural networks, depending on the size
of the sentences. In the evaluation we found that
our system worked well for the most datasets. But
as soon as the pairs of sentences differed too much
in size, or the sentences were very long, the perfor-
mance decreased. In future work we will consider
to tackle this problem with partial matching[3] and
to introduces features to extract core statements of
short texts.
</bodyText>
<sectionHeader confidence="0.995644" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.852392375">
The Know-Center is funded within the Austrian
COMET Program - Competence Centers for Excel-
lent Technologies - under the auspices of the Aus-
trian Federal Ministry of Transport, Innovation and
Technology, the Austrian Federal Ministry of Econ-
omy, Family and Youth and by the State of Styria.
COMET is managed by the Austrian Research Pro-
motion Agency FFG.
</bodyText>
<sectionHeader confidence="0.991618" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999976222222222">
[1] Eneko Agirre, Daniel Cer, Mona Diab, and
Aitor Gonz´alez. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In SEM
2012: The First Joint Conference on Lexical
and Computational Semantics (SemEval 2012),
Montreal, Canada, 2012.
[2] Christiane Fellbaum, editor. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cam-
bridge, MA, 1998.
</reference>
<page confidence="0.979203">
141
</page>
<reference confidence="0.999873346153846">
[3] Prodromos Malakasiotis and Ion Androutsopou-
los. Learning textual entailment using svms and
string similarity measures.
[4] Nikos Malandrakis, Elias Iosif, and Alexan-
dros Potamianos. Deeppurple: estimating sen-
tence semantic similarity using n-gram regres-
sion models and web snippets. In Proceed-
ings of the First Joint Conference on Lexical
and Computational Semantics - Volume 1: Pro-
ceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation,
SemEval ’12, pages 565–570, Stroudsburg, PA,
USA, 2012. Association for Computational Lin-
guistics.
[5] George A. Miller. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39–41,
November 1995.
[6] Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan
ˇSnajder, and Bojana Dalbelo Baˇsi´c. Takelab:
Systems for measuring semantic text similarity.
In Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012),
pages 441–448, Montr´eal, Canada, 7-8 June
2012. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.997727">
142
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.156861">
<title confidence="0.998463">KnCe2013-CORE:Semantic Text Similarity by use of Knowledge Bases</title>
<author confidence="0.955167">Hermann</author>
<affiliation confidence="0.877279">Know-Center Graz University of</affiliation>
<address confidence="0.889912">Inffeldgasse 13/ 6. 8010 Graz,</address>
<email confidence="0.993563">hziak@know-center.at</email>
<author confidence="0.789322">Roman</author>
<email confidence="0.425533">Know-Center</email>
<affiliation confidence="0.980988">Graz University of</affiliation>
<address confidence="0.8852435">Inffeldgasse 13/ 6. 8010 Graz,</address>
<email confidence="0.997814">rkern@know-center.at</email>
<abstract confidence="0.9987015">In this paper we describe KnCe2013-CORE, a system to compute the semantic similarity of two short text snippets. The system computes a number of features which are gathered from different knowledge bases, namely WordNet, Wikipedia and Wiktionary. The similarity scores derived from these features are then fed into several multilayer perceptron neuronal networks. Depending on the size of the text snippets different parameters for the neural networks are used. The final output of the neural networks is compared to human judged data. In the evaluation our system performed sufficiently well for text snippets of equal length, but the performance dropped considerably once the pairs of text snippets differ in size.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonz´alez</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In SEM 2012: The First Joint Conference on Lexical and Computational Semantics (SemEval 2012),</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="1844" citStr="[1]" startWordPosition="288" endWordPosition="288">g. The semantic similarity of two sentences could then be used in a number of different application scenarios, for example it could help to improve the performance of information retrieval systems. In the past, systems based on regression models in combination with well chosen features have demonstrated good performance on this topic[4] [6]. Therefore we took this approach as a starting point to develop our semantic similarity system; additionally, we integrated a number of existing knowledge bases into our system. With it, trained with the data discussed in the task specification of last year[1], we participated in the shared task of SEM 2013. Additionally, to the similarity based on the features derived from the external knowledge bases, we employ a neural network to compute the final similarity score. The motivation to use a supervised machine learning algorithm has been the observation that the semantic similarity is heavily influenced by the context of the human evaluator. A financial expert for example would judge sentences with financial topics different to non financial experts, if occurring numbers differ from each other. The remainder of the paper is organised as follows: In</context>
</contexts>
<marker>[1]</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonz´alez. Semeval-2012 task 6: A pilot on semantic textual similarity. In SEM 2012: The First Joint Conference on Lexical and Computational Semantics (SemEval 2012), Montreal, Canada, 2012.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="5790" citStr="[2]" startWordPosition="931" endWordPosition="931">which represent more general terms. Antonym: List of terms, which represent an opposing sense. Related Terms: Terms, with a semantic relationship, which does not fall in the aforementioned categories. For example related terms for ’bank’ are 1http://en.wiktionary.org ’bankrupt’. Related terms represent only a weak semantic similarity. Derived Terms: Terms, with overlapping word forms, such as ’bank holiday’, ’bankroll’ and ’databank’ for the term ’bank’. From all the semantic relationship types, derived terms are the weakest indicator for their similarities. 2.4 WordNet Features The WordNet[5][2] features were generated identically to the Wiktionary features. We used the WordNet off line database and the provided library to get a broader knowledge base. Therefore we extract the semantically related terms of each token and saved each class of relation. Where each dependency class produced an one value in the final feature score list of the sentence pairs. 2.5 Wikification Feature We applied a Named Entity Recognition component, which has been trained using Wikipedia categories as input. Given a sentence it will annotate all found concepts that match a Wikipedia article, together with a</context>
</contexts>
<marker>[2]</marker>
<rawString>Christiane Fellbaum, editor. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA, 1998.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Prodromos Malakasiotis</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Learning textual entailment using svms and string similarity measures.</title>
<marker>[3]</marker>
<rawString>Prodromos Malakasiotis and Ion Androutsopoulos. Learning textual entailment using svms and string similarity measures.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nikos Malandrakis</author>
<author>Elias Iosif</author>
<author>Alexandros Potamianos</author>
</authors>
<title>Deeppurple: estimating sentence semantic similarity using n-gram regression models and web snippets.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12,</booktitle>
<pages>565--570</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="1579" citStr="[4]" startWordPosition="245" endWordPosition="245">ped considerably once the pairs of text snippets differ in size. 1 Introduction The task of the semantic sentence similarity is to assign a score to a given pair of sentences. This score should reflect the degree by which the two sentences represent the same meaning. The semantic similarity of two sentences could then be used in a number of different application scenarios, for example it could help to improve the performance of information retrieval systems. In the past, systems based on regression models in combination with well chosen features have demonstrated good performance on this topic[4] [6]. Therefore we took this approach as a starting point to develop our semantic similarity system; additionally, we integrated a number of existing knowledge bases into our system. With it, trained with the data discussed in the task specification of last year[1], we participated in the shared task of SEM 2013. Additionally, to the similarity based on the features derived from the external knowledge bases, we employ a neural network to compute the final similarity score. The motivation to use a supervised machine learning algorithm has been the observation that the semantic similarity is hea</context>
</contexts>
<marker>[4]</marker>
<rawString>Nikos Malandrakis, Elias Iosif, and Alexandros Potamianos. Deeppurple: estimating sentence semantic similarity using n-gram regression models and web snippets. In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12, pages 565–570, Stroudsburg, PA, USA, 2012. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="5787" citStr="[5]" startWordPosition="931" endWordPosition="931">ms which represent more general terms. Antonym: List of terms, which represent an opposing sense. Related Terms: Terms, with a semantic relationship, which does not fall in the aforementioned categories. For example related terms for ’bank’ are 1http://en.wiktionary.org ’bankrupt’. Related terms represent only a weak semantic similarity. Derived Terms: Terms, with overlapping word forms, such as ’bank holiday’, ’bankroll’ and ’databank’ for the term ’bank’. From all the semantic relationship types, derived terms are the weakest indicator for their similarities. 2.4 WordNet Features The WordNet[5][2] features were generated identically to the Wiktionary features. We used the WordNet off line database and the provided library to get a broader knowledge base. Therefore we extract the semantically related terms of each token and saved each class of relation. Where each dependency class produced an one value in the final feature score list of the sentence pairs. 2.5 Wikification Feature We applied a Named Entity Recognition component, which has been trained using Wikipedia categories as input. Given a sentence it will annotate all found concepts that match a Wikipedia article, together wit</context>
</contexts>
<marker>[5]</marker>
<rawString>George A. Miller. Wordnet: a lexical database for english. Commun. ACM, 38(11):39–41, November 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Baˇsi´c. Takelab: Systems for measuring semantic text similarity.</title>
<date></date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>441--448</pages>
<location>Montr´eal,</location>
<contexts>
<context position="1583" citStr="[6]" startWordPosition="246" endWordPosition="246">considerably once the pairs of text snippets differ in size. 1 Introduction The task of the semantic sentence similarity is to assign a score to a given pair of sentences. This score should reflect the degree by which the two sentences represent the same meaning. The semantic similarity of two sentences could then be used in a number of different application scenarios, for example it could help to improve the performance of information retrieval systems. In the past, systems based on regression models in combination with well chosen features have demonstrated good performance on this topic[4] [6]. Therefore we took this approach as a starting point to develop our semantic similarity system; additionally, we integrated a number of existing knowledge bases into our system. With it, trained with the data discussed in the task specification of last year[1], we participated in the shared task of SEM 2013. Additionally, to the similarity based on the features derived from the external knowledge bases, we employ a neural network to compute the final similarity score. The motivation to use a supervised machine learning algorithm has been the observation that the semantic similarity is heavily</context>
</contexts>
<marker>[6]</marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. Takelab: Systems for measuring semantic text similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 441–448, Montr´eal, Canada, 7-8 June 2012. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>