<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001083">
<title confidence="0.994073">
Experiments Using OSTIA for a Language Production Task
</title>
<author confidence="0.997808">
Dana Angluin and Leonor Becerra-Bonache
</author>
<affiliation confidence="0.997091">
Department of Computer Science, Yale University
</affiliation>
<address confidence="0.439398">
P.O.Box 208285, New Haven, CT, USA
</address>
<email confidence="0.992794">
{dana.angluin, leonor.becerra-bonache}@yale.edu
</email>
<sectionHeader confidence="0.993675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965959183674">
The phenomenon of meaning-preserving
corrections given by an adult to a child
involves several aspects: (1) the child
produces an incorrect utterance, which
the adult nevertheless understands, (2) the
adult produces a correct utterance with the
same meaning and (3) the child recognizes
the adult utterance as having the same
meaning as its previous utterance, and
takes that as a signal that its previous ut-
terance is not correct according to the adult
grammar. An adequate model of this phe-
nomenon must incorporate utterances and
meanings, account for how the child and
adult can understand each other’s mean-
ings, and model how meaning-preserving
corrections interact with the child’s in-
creasing mastery of language production.
In this paper we are concerned with how
a learner who has learned to comprehend
utterances might go about learning to pro-
duce them.
We consider a model of language com-
prehension and production based on finite
sequential and subsequential transducers.
Utterances are modeled as finite sequences
of words and meanings as finite sequences
of predicates. Comprehension is inter-
preted as a mapping of utterances to mean-
ings and production as a mapping of mean-
ings to utterances. Previous work (Castel-
lanos et al., 1993; Pieraccini et al., 1993)
has applied subsequential transducers and
the OSTIA algorithm to the problem of
learning to comprehend language; here we
apply them to the problem of learning to
produce language. For ten natural lan-
guages and a limited domain of geomet-
ric shapes and their properties and rela-
tions we define sequential transducers to
produce pairs consisting of an utterance
in that language and its meaning. Using
this data we empirically explore the prop-
erties of the OSTIA and DD-OSTIA al-
gorithms for the tasks of learning compre-
hension and production in this domain, to
assess whether they may provide a basis
for a model of meaning-preserving correc-
tions.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994775862069">
The role of corrections in language learning has
recently received substantial attention in Gram-
matical Inference. The kinds of corrections con-
sidered are mainly syntactic corrections based on
proximity between strings. For example, a cor-
rection of a string may be given by using edit
distance (Becerra-Bonache et al., 2007; Kinber,
2008) or based on the shortest extension of the
queried string (Becerra-Bonache et al., 2006),
among others. In these approaches semantic in-
formation is not used.
However, in natural situations, a child’s er-
roneous utterances are corrected by her parents
based on the meaning that the child intends to ex-
press; typically, the adult’s corrections preserve
the intended meaning of the child. Adults use cor-
rections in part as a way of making sure they have
understood the child’s intentions, in order to keep
the conversation “on track”. Thus the child’s ut-
terance and the adult’s correction have the same
meaning, but the form is different. As Chouinard
and Clark point out (2003), because children at-
tend to contrasts in form, any change in form that
does not mark a different meaning will signal to
children that they may have produced something
that is not acceptable in the target language. Re-
sults in (Chouinard and Clark, 2003) show that
adults reformulate erroneous child utterances of-
ten enough to help learning. Moreover, these re-
</bodyText>
<note confidence="0.9373085">
Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 16–23,
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998149">
16
</page>
<bodyText confidence="0.999943325">
sults show that children can not only detect differ-
ences between their own utterance and the adult
reformulation, but that they do make use of that
information.
Thus in some natural situations, corrections
have a semantic component that has not been taken
into account in previous Grammatical Inference
studies. Some interesting questions arise: What
are the effects of corrections on learning syntax?
Can corrections facilitate the language learning
process? One of our long-term goals is to find a
formal model that gives an account of this kind
of correction and in which we can address these
questions. Moreover, such a model might allow us
to show that semantic information can simplify the
problem of learning formal languages.
A simple computational model of semantics and
context for language learning incorporating se-
mantics was proposed in (Angluin and Becerra-
Bonache, 2008). This model accommodates two
different tasks: comprehension and production.
That paper focused only on the comprehension
task and formulated the learning problem as fol-
lows. The teacher provides to the learner several
example pairs consisting of a situation and an ut-
terance denoting something in the situation; the
goal of the learner is to learn the meaning func-
tion, allowing the learner to comprehend novel ut-
terances. The results in that paper show that under
certain assumptions, a simple algorithm can learn
to comprehend an adult’s utterance in the sense of
producing the same sequence of predicates, even
without mastering the adult’s grammar. For exam-
ple, receiving the utterance the blue square above
the circle, the learner would be able to produce the
sequence of predicates (bl, sq, ab, ci).
In this paper we focus on the production task,
using sequential and subsequential transducers to
model both comprehension and production. Adult
production can be modeled as converting a se-
quence of predicates into an utterance, which can
be done with access to the meaning transducer for
the adult’s language.
However, we do not assume that the child ini-
tially has access to the meaning transducer for
the adult’s language; instead we assume that the
child’s production progresses through different
stages. Initially, child production is modeled as
consisting of two different tasks: finding a correct
sequence of predicates, and inverting the meaning
function to produce a kind of “telegraphic speech”.
For example, from (gr, tr, le, sq) the child may
produce green triangle left square. Our goal is to
model how the learner might move from this tele-
graphic speech to speech that is grammatical in the
adult’s sense. Moreover, we would like to find a
formal framework in which corrections (in form of
expansions, for example, the green triangle to the
left of the square) can be given to the child dur-
ing the intermediate stages (before the learner is
able to produce grammatically correct utterances)
to study their effect on language learning.
We thus propose to model the problem of
child language production as a machine trans-
lation problem, that is, as the task of translat-
ing a sequence of predicate symbols (representing
the meaning of an utterance) into a correspond-
ing utterance in a natural language. In this pa-
per we explore the possibility of applying existing
automata-theoretic approaches to machine transla-
tion to model language production. In Section 2,
we describe the use of subsequential transducers
for machine translation tasks and review the OS-
TIA algorithm to learn them (Oncina, 1991). In
Section 3, we present our model of how the learner
can move from telegraphic to adult speech. In Sec-
tion 4, we present the results of experiments in the
model made using OSTIA. Discussion of these re-
sults is presented in Section 5 and ideas for future
work are in Section 6.
</bodyText>
<sectionHeader confidence="0.89563" genericHeader="method">
2 Learning Subsequential Transducers
</sectionHeader>
<bodyText confidence="0.9998293">
Subsequential transducers (SSTs) are a formal
model of translation widely studied in the liter-
ature. SSTs are deterministic finite state mod-
els that allow input-output mappings between lan-
guages. Each edge of an SST has an associated
input symbol and output string. When an in-
put string is accepted, an SST produces an out-
put string that consists of concatenating the out-
put substrings associated with sequence of edges
traversed, together with the substring associated
with the last state reached by the input string. Sev-
eral phenomena in natural languages can be eas-
ily represented by means of SSTs, for example,
the different orders of noun and adjective in Span-
ish and English (e.g., un cuadrado rojo - a red
square). Formal and detailed definitions can be
found in (Berstel, 1979).
For any SST it is always possible to find an
equivalent SST that has the output strings assigned
to the edges and states so that they are as close to
</bodyText>
<page confidence="0.998143">
17
</page>
<bodyText confidence="0.982769818181818">
the initial state as they can be. This is called an
Onward Subsequential Transducer (OST).
It has been proved that SSTs are learnable in
the limit from a positive presentation of sentence
pairs by an efficient algorithm called OSTIA (On-
ward Subsequential Transducer Inference Algo-
rithm) (Oncina, 1991). OSTIA takes as input a fi-
nite training set of input-output pairs of sentences,
and produces as output an OST that generalizes
the training pairs. The algorithm proceeds as fol-
lows (this description is based on (Oncina, 1998)):
</bodyText>
<listItem confidence="0.999680933333334">
• A prefix tree representation of all the input
sentences of the training set is built. Empty
strings are assigned as output strings to both
the internal nodes and the edges of this tree,
and every output sentence of the training set
is assigned to the corresponding leaf of the
tree. The result is called a tree subsequential
transducer.
• An onward tree subsequential transducer
equivalent to the tree subsequential trans-
ducer is constructed by moving the longest
common prefixes of the output strings, level
by level, from the leaves of the tree towards
the root.
• Starting from the root, all pairs of states of
</listItem>
<bodyText confidence="0.9315705625">
the onward tree subsequential transducer are
considered in order, level by level, and are
merged if possible (i.e., if the resulting trans-
ducer is subsequential and does not contra-
dict any pair in the training set).
SSTs and OSTIA have been successfully ap-
plied to different translation tasks: Roman numer-
als to their decimal representations, numbers writ-
ten in English to their Spanish spelling (Oncina,
1991) and Spanish sentences describing simple
visual scenes to corresponding English and Ger-
man sentences (Castellanos et al., 1994). They
have also been applied to language understanding
tasks (Castellanos et al., 1993; Pieraccini et al.,
1993).
Moreover, several extensions of OSTIA have
been introduced. For example, OSTIA-DR incor-
porates domain (input) and range (output) mod-
els in the learning process, allowing the algorithm
to learn SSTs that accept only sentences compat-
ible with the input model and produce only sen-
tences compatible with the output model (Oncina
and Varo, 1996). Experiments with a language un-
derstanding task gave better results with OSTIA-
DR than with OSTIA (Castellanos et al., 1993).
Another extension is DD-OSTIA (Oncina, 1998),
which instead of considering a lexicographic order
to merge states, uses a heuristic order based on a
measure of the equivalence of the states. Experi-
ments in (Oncina, 1998) show that better results
can be obtained by using DD-OSTIA in certain
translation tasks from Spanish to English.
</bodyText>
<sectionHeader confidence="0.848819" genericHeader="method">
3 From telegraphic to adult speech
</sectionHeader>
<bodyText confidence="0.999973205128205">
To model how the learner can move from tele-
graphic speech to adult speech, we reduce this
problem to a translation problem, in which the
learner has to learn a mapping from sequences of
predicates to utterances. As we have seen in the
previous section, SSTs are an interesting approach
to machine translation. Therefore, we explore the
possibility of modeling language production using
SSTs and OSTIA, to see whether they may pro-
vide a good framework to model corrections.
As described in (Angluin and Becerra-Bonache,
2008), after learning the meaning function the
learner is able to assign correct meanings to ut-
terances, and therefore, given a situation and an
utterance that denotes something in the situation,
the learner is able to point correctly to the object
denoted by the utterance. To simplify the task
we consider, we make two assumptions about the
learner at the start of the production phase: (1)
the learner’s lexicon represents a correct meaning
function and (2) the learner can generate correct
sequences of predicates.
Therefore, in the initial stage of the production
phase, the learner is able to produce a kind of
“telegraphic speech” by inverting the lexicon con-
structed during the comprehension stage. For ex-
ample, if the sequence of predicates is (bl, sq, ler,
ci), and in the lexicon blue is mapped to bl, square
to sq, right to ler and circle to ci, then by invert-
ing this mapping, the learner would produce blue
square right circle.
In order to explore the capability of SSTs and
OSTIA to model the next stage of language pro-
duction (from telegraphic to adult speech), we take
the training set to be input-output pairs each of
which contains as input a sequence of predicates
(e.g., (bl, sq, ler, ci)) and as output the correspond-
ing utterance in a natural language (e.g., the blue
square to the right of the circle). In this example,
</bodyText>
<page confidence="0.995202">
18
</page>
<bodyText confidence="0.999621">
the learner must learn to include appropriate func-
tion words. In other languages, the learner may
have to learn a correct choice of words determined
by gender, case or other factors. (Note that we are
not yet in a position to consider corrections.)
</bodyText>
<sectionHeader confidence="0.999382" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999935612903226">
Our experiments were made for a limited domain
of geometric shapes and their properties and re-
lations. This domain is a simplification of the
Miniature Language Acquisition task proposed by
Feldman et al. (Feldman et al., 1990). Previous
applications of OSTIA to language understanding
and machine translation have also used adapta-
tions and extensions of the Feldman task.
In our experiments, we have predicates for three
different shapes (circle (ci), square (sq) and tri-
angle (tr)), three different colors (blue (bl), green
(gr) and red (re)) and three different relations (to
the left of (le), to the right of (ler), and above (ab)).
We consider ten different natural languages: Ara-
bic, English, Greek, Hebrew, Hindi, Hungarian,
Mandarin, Russian, Spanish and Turkish.
We created a data sequence of input-output
pairs, each consisting of a predicate sequence and
a natural language utterance. For example, one
pair for Spanish is ((ci, re, ler, tr), el circulo rojo
a la derecha del triangulo). We ran OSTIA on ini-
tial segments of the sequence of pairs, of lengths
10, 20,30,..., to produce a sequence of subse-
quential transducers. The whole data sequence
was used to test the correctness of the transducers
generated during the process. An error is counted
whenever given a data pair (x, y), the subsequen-
tial transducer translates x to y&apos;, and y&apos; 7� y. We
say that OSTIA has converged to a correct trans-
ducer if all the transducers produced afterwards
have the same number of states and edges, and 0
errors on the whole data sequence.
To generate the sequences of input-output pairs,
for each language we constructed a meaning trans-
ducer capable of producing the 444 different pos-
sible meanings involving one or two objects. We
randomly generated 400 unique (non-repeated)
input-output pairs for each language. This process
was repeated 10 times. In addition, to investigate
the effect of the order of presentation of the input-
output pairs, we repeated the data generation pro-
cess for each language, sorting the pairs according
to a length-lex ordering of the utterances.
We give some examples to illustrate the trans-
ducers produced. Figure 1 shows an example of
a transducer produced by OSTIA after just ten
pairs of input-output examples for Spanish. This
transducer correctly translates the ten predicate se-
quences used to construct it, but the data is not
sufficient for OSTIA to generalize correctly in all
cases, and many other correct meanings are still
incorrectly translated. For example, the sequence
(ci, bl) is translated as el circulo a la izquierda del
circulo verde azul instead of el circulo azul.
The transducers produced after convergence by
OSTIA and DD-OSTIA correctly translate all 444
possible correct meanings. Examples for Spanish
are shown in Figure 2 (OSTIA) and Figure 3 (DD-
OSTIA). Note that although they correctly trans-
late all 444 correct meanings, the behavior of these
two transducers on other (incorrect) predicate se-
quences is different, for example on (tr, tr).
</bodyText>
<figure confidence="0.950151625">
bl/ azul
sq/ el cuadrado
ci/el circulo a la
izquierda del
circulo verde
ab/
re/ rojo a la derecha
del cuadrado
</figure>
<figureCaption confidence="0.99967375">
Figure 1: Production task, OSTIA. A transducer
produced using 10 random unique input-output
pairs (predicate sequence, utterance) for Spanish.
Figure 2: Production task, OSTIA. A transducer
</figureCaption>
<bodyText confidence="0.9287832">
produced (after convergence) by using random
unique input-output pairs (predicate sequence, ut-
terance) for Spanish.
Different languages required very different
numbers of data pairs to converge. Statistics on
the number of pairs needed until convergence for
OSTIA for all ten languages for both random
unique and random unique sorted data sequences
are shown in Table 1. Because better results were
reported using DD-OSTIA in machine translation
</bodyText>
<figure confidence="0.966193972972973">
le/ a la izquierda del
ler/ a la derecha del
ab/ encima del
re/ rojo
gr/ verde
ci/ el circulo
tr/ el triangulo
le/ a la izquierda del
ler/ a la derecha del
ab/ encima del
bl/ azul
re/ rojo
gr/ verde
sq/ cuadrado
ci/ circulo
tr/ triangulo
2
1
bl/ azul
sq/ el cuadrado
1
tr/ el triangulo
le/
ler/
sq/
ci/
bl/ azul
gr/
ler/ a la derecha
del cuadrado
2
tr/ encima del triangulo
ci/ verde encima del
circulo azul
bl/
re/ rojo
3
</figure>
<page confidence="0.835614">
19
</page>
<figureCaption confidence="0.9877966">
Figure 3: Production task, DD-OSTIA. A trans-
ducer produced (after convergence) using random
unique input-output pairs (predicate-sequence, ut-
terance) for Spanish.
Table 1: Production task, OSTIA. The entries give
the median number of input-output pairs until con-
vergence in 10 runs. For Greek, Hindi and Hun-
garian, the median for the unsorted case is calcu-
lated using all 444 random unique pairs, instead of
400.
</figureCaption>
<figure confidence="0.828885">
bl/ azul
re/ rojo
gr/ verde
sq/ el cuadrado
ci/ el circulo
tr/ el triangulo
1
</figure>
<table confidence="0.963448121212121">
Language # Pairs # Sorted Pairs
Arabic 150 200
English 200 235
Greek 375 400
Hebrew 195 30
Hindi 380 350
Hungarian 365 395
Mandarin 45 150
Russian 270 210
Spanish 190 150
Turkish 185 80
Language # Pairs # Sorted Pairs
Arabic 80 140
English 85 180
Greek 350 400
Hebrew 65 80
Hindi 175 120
Hungarian 245 140
Mandarin 40 150
Russian 185 210
Spanish 80 150
Turkish 50 40
Languages #states #edges
Arabic 2 20
English 2 20
Greek 9 65
Hebrew 2 20
Hindi 7 58
Hungarian 3 20
Mandarin 1 10
Russian 3 30
Spanish 2 20
Turkish 4 31
</table>
<figure confidence="0.886781428571428">
le/ a la izquierda del
ler/ a la derecha del
ab/ encima del
sq/ cuadrado
ci/ circulo
tr/ triangulo
2
</figure>
<figureCaption confidence="0.979695625">
Table 2: Production task, DD-OSTIA. The entries
give the median number of input-output pairs un-
til convergence in 10 runs. For Greek, Hindi and
Hungarian, the median for the unsorted case is cal-
culated using all 444 random unique pairs, instead
of 400.
Table 3: Production task, OSTIA. Sizes of trans-
ducers at convergence.
</figureCaption>
<bodyText confidence="0.9999618">
tasks (Oncina, 1998), we also tried using DD-
OSTIA for learning to translate a sequence of
predicates to an utterance. We used the same se-
quences of input-output pairs as in the previous
experiment. The results obtained are shown in Ta-
ble 2.
We also report the sizes of the transducers
learned by OSTIA and DD-OSTIA. Table 3 and
Table 4 show the numbers of states and edges
of the transducers after convergence for each lan-
guage. In case of disagreements, the number re-
ported is the mode.
To answer the question of whether production
is harder than comprehension in this setting, we
also considered the comprehension task, that is,
to translate an utterance in a natural language
into the corresponding sequence of predicates.
The comprehension task was studied by Oncina
et al. (Castellanos et al., 1993). They used En-
glish sentences, with a more complex version of
the Feldman task domain and more complex se-
mantic representations than we use. Our results
are presented in Table 5. The number of states
and edges of the transducers after convergence is
shown in Table 6.
</bodyText>
<sectionHeader confidence="0.998661" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.985094">
It should be noted that because the transducers
output by OSTIA and DD-OSTIA correctly repro-
duce all the pairs used to construct them, once ei-
ther algorithm has seen all 444 possible data pairs
in either the production or the comprehension task,
the resulting transducers will correctly translate all
correct inputs. However, state-merging in the al-
</bodyText>
<page confidence="0.986853">
20
</page>
<table confidence="0.999643636363636">
Languages #states #edges
Arabic 2 17
English 2 16
Greek 9 45
Hebrew 2 13
Hindi 7 40
Hungarian 3 20
Mandarin 1 10
Russian 3 23
Spanish 2 13
Turkish 3 18
</table>
<tableCaption confidence="0.9716205">
Table 4: Production task, DD-OSTIA. Sizes of
transducers at convergence.
</tableCaption>
<table confidence="0.999929818181818">
Languages OSTIA DD-OSTIA
Arabic 65 65
English 60 20
Greek 325 60
Hebrew 90 45
Hindi 60 35
Hungarian 40 45
Mandarin 60 40
Russian 280 55
Spanish 45 30
Turkish 60 35
</table>
<tableCaption confidence="0.99655">
Table 5: Comprehension task, OSTIA and DD-
</tableCaption>
<bodyText confidence="0.957705909090909">
OSTIA. Median number (in 10 runs) of input-
output pairs until convergence using a sequence of
400 random unique pairs of (utterance, predicate
sequence).
gorithms induces compression and generalization,
and the interesting questions are how much data
is required to achieve correct generalization, and
how that quantity scales with the complexity of
the task. This are very difficult questions to ap-
proach analytically, but empirical results can offer
valuable insights.
Considering the comprehension task (Tables 5
and 6), we see that OSTIA generalizes correctly
from at most 15% of all 444 possible pairs except
in the cases of Greek, Hebrew and Russian. DD-
OSTIA improves the OSTIA results, in some cases
dramatically, for all languages except Hungarian.
DD-OSTIA achieves correct generalization from
at most 15% of all possible pairs for all ten lan-
guages. Because the meaning function for all ten
language transducers is independent of the state,
in each case there is a 1-state sequential trans-
</bodyText>
<table confidence="0.999291090909091">
Languages #states #edges
Arabic 1 15
English 1 13
Greek 2 25
Hebrew 1 13
Hindi 1 13
Hungarian 1 14
Mandarin 1 17
Russian 1 24
Spanish 1 14
Turkish 1 13
</table>
<tableCaption confidence="0.860430333333333">
Table 6: Comprehension task, OSTIA and DD-
OSTIA. Sizes of transducers at convergence using
400 random unique input-output pairs (utterance,
</tableCaption>
<bodyText confidence="0.998192371428571">
predicate sequence). In cases of disagreement, the
number reported is the mode.
ducer that achieves correct translation of correct
utterances into predicate sequences. OSTIA and
DD-OSTIA converged to 1-state transducers for
all languages except Greek, for which they con-
verged to 2-state transducers. Examining one such
transducer for Greek, we found that the require-
ment that the transducer be “onward” necessitated
two states. These results are broadly compatible
with the results obtained by Oncina et al. (Castel-
lanos et al., 1993) on language understanding; the
more complex tasks they consider also give evi-
dence that this approach may scale well for the
comprehension task.
Turning to the production task (Tables 1, 2, 3
and 4), we see that providing the random samples
with a length-lex ordering of utterances has incon-
sistent effects for both OSTIA and DD-OSTIA,
sometimes dramatically increasing or decreasing
the number of samples required. We do not fur-
ther consider the sorted samples.
Comparing the production task with the com-
prehension task for OSTIA, the production task
generally requires substantially more random
unique samples than the comprehension task for
the same language. The exceptions are Mandarin
(production: 45 and comprehension: 60) and Rus-
sian (production: 270 and comprehension: 280).
For DD-OSTIA the results are similar, with the
sole exception of Mandarin (production: 40 and
comprehension: 40). For the production task DD-
OSTIA requires fewer (sometimes dramatically
fewer) samples to converge than OSTIA. How-
ever, even with DD-OSTIA the number of sam-
</bodyText>
<page confidence="0.996821">
21
</page>
<bodyText confidence="0.999940343750001">
ples is in several cases (Greek, Hindi, Hungarian
and Russian) a rather large fraction (40% or more)
of all 444 possible pairs. Further experimentation
and analysis is required to determine how these re-
sults will scale.
Looking at the sizes of the transducers learned
by OSTIA and DD-OSTIA in the production task,
we see that the numbers of states agree for all lan-
guages except Turkish. (Recall from our discus-
sion in Section 4 that there may be differences in
the behavior of the transducers learned by OSTIA
and DD-OSTIA at convergence.) For the produc-
tion task, Mandarin gives the smallest transducer;
for this fragment of the language, the translation
of correct predicate sequences into utterances can
be achieved with a 1-state transducer. In contrast,
English and Spanish both require 2 states to handle
articles correctly. For example, in the transducer
in Figure 3, the predicate for a circle (ci) is trans-
lated as el circulo if it occurs as the first object (in
state 1) and as circulo if it occurs as second ob-
ject (in state 2) because del has been supplied by
the translation of the intervening binary relation
(le, ler, or ab.) Greek gives the largest transducer
for the production task, with 9 states, and requires
the largest number of samples for DD-OSTIA to
achieve convergence, and one of the largest num-
bers of samples for OSTIA. Despite the evidence
of the extremes of Mandarin and Greek, the rela-
tion between the size of the transducer for a lan-
guage and the number of samples required to con-
verge to it is not monotonic.
In our model, one reason that learning the pro-
duction task may in general be more difficult than
learning the comprehension task is that while the
mapping of a word to a predicate does not depend
on context, the mapping of a predicate to a word
or words does (except in the case of our Mandarin
transducer.) As an example, in the comprehension
task the Russian words triugolnik, triugolnika and
triugonikom are each mapped to the predicate tr,
but the reverse mapping must be sensitive to the
context of the occurrence of tr.
These results suggest that OSTIA or DD-
OSTIA may be an effective method to learn to
translate sequences of predicates into natural lan-
guage utterances in our domain. However, some of
our objectives seem incompatible with the proper-
ties of OSTIA. In particular, it is not clear how
to incorporate the learner’s initial knowledge of
the lexicon and ability to produce “telegraphic
speech” by inverting the lexicon. Also, the in-
termediate results of the learning process do not
seem to have the properties we expect of a learner
who is progressing towards mastery of produc-
tion. That is, the intermediate transducers per-
fectly translate the predicate sequences used to
construct them, but typically produce other trans-
lations that the learner (using the lexicon) would
know to be incorrect. For example, the intermedi-
ate transducer from Figure 1 translates the predi-
cate sequence (ci) as el circulo a la izquierda del
circulo verde, which the learner’s lexicon indicates
should be translated as (ci, le, ci, gr).
</bodyText>
<sectionHeader confidence="0.999882" genericHeader="method">
6 Future work
</sectionHeader>
<bodyText confidence="0.999987027777778">
Further experiments and analysis are required to
understand how these results will scale with larger
domains and languages. In this connection, it may
be interesting to try the experiments of (Castel-
lanos et al., 1993) in the reverse (production) di-
rection. Finding a way to incorporate the learner’s
initial lexicon seems important. Perhaps by incor-
porating the learner’s knowledge of the input do-
main (the legal sequences of predicates) and using
the domain-aware version, OSTIA-D, the interme-
diate results in the learning process would be more
compatible with our modeling objectives. Coping
with errors will be necessary; perhaps an explic-
itly statistical framework for machine translation
should be considered.
If we can find an appropriate model of how
the learner’s language production process might
evolve, then we will be in a position to model
meaning-preserving corrections. That is, the
learner chooses a sequence of predicates and maps
it to a (flawed) utterance. Despite its flaws, the
learner’s utterance is understood by the teacher
(i.e., the teacher is able to map it to the sequence
of predicates chosen by the learner) and responds
with a correction, that is, a correct utterance for
that meaning. The learner, recognizing that the
teacher’s utterance has the same meaning but a
different form, then uses the correct utterance (as
well as the meaning and the incorrect utterance) to
improve the mapping of sequences of predicates to
utterances.
It is clear that in this model, corrections are not
necessary to the process of learning comprehen-
sion and production; once the learner has a correct
lexicon, the utterances of the teacher can be trans-
lated into sequences of predicates, and the pairs
</bodyText>
<page confidence="0.98318">
22
</page>
<bodyText confidence="0.999973714285714">
of (predicate sequence, utterance) can be used to
learn (via an appropriate variant of OSTIA) a per-
fect production mapping. However, it seems very
likely that corrections can make the process of
learning a production mapping easier or faster, and
finding a model in which such phenomena can be
studied remains an important goal of this work.
</bodyText>
<sectionHeader confidence="0.99918" genericHeader="conclusions">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999906142857143">
The authors sincerely thank Prof. Jose Oncina
for the use of his programs for OSTIA and DD-
OSTIA, as well as his helpful and generous ad-
vice. The research of Leonor Becerra-Bonache
was supported by a Marie Curie International
Fellowship within the 61h European Community
Framework Programme.
</bodyText>
<sectionHeader confidence="0.999432" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999807825">
Dana Angluin and Leonor Becerra-Bonache. 2008.
Learning Meaning Before Syntax. ICGI, 281–292.
Leonor Becerra-Bonache, Colin de la Higuera, J.C.
Janodet, and Frederic Tantini. 2007. Learning Balls
of Strings with Correction Queries. ECML, 18–29.
Leonor Becerra-Bonache, Adrian H. Dediu, and
Cristina Tirnauca. 2006. Learning DFA from Cor-
rection and Equivalence Queries. ICGI, 281–292.
Jean Berstel. 1979. Transductions and Context-Free
Languages. PhD Thesis, Teubner, Stuttgart, 1979.
Antonio Castellanos, Enrique Vidal, and Jose Oncina.
1993. Language Understanding and Subsequential
Transducers. ICGI, 11/1–11/10.
Antonio Castellanos, Ismael Galiano, and Enrique Vi-
dal. 1994. Applications of OSTIA to machine trans-
lation tasks. ICGI, 93–105.
Michelle M. Chouinard and Eve V. Clark. 2003. Adult
Reformulations of Child Errors as Negative Evi-
dence. Journal of Child Language, 30:637–669.
Jerome A. Feldman, George Lakoff, Andreas Stolcke,
and Susan Hollback Weber. 1990. Miniature Lan-
guage Acquisition: A touchstone for cognitive sci-
ence. Technical Report, TR-90-009. International
Computer Science Institute, Berkeley, California.
April, 1990.
Efim Kinber. 2008. On Learning Regular Expres-
sions and Patterns Via Membership and Correction
Queries. ICGI, 125–138.
Jose Oncina. 1991. Aprendizaje de lenguajes regu-
lares y transducciones subsecuenciales. PhD Thesis,
Universitat Politecnica de Valencia, Valencia, Spain,
1998.
Jose Oncina. 1998. The data driven approach applied
to the OSTIA algorithm. ICGI, 50–56.
Jose Oncina and Miguel Angel Varo. 1996. Using do-
main information during the learing of a subsequen-
tial transducer. ICGI, 301–312.
Roberto Pieraccini, Esther Levin, and Enrique Vidal.
1993. Learning how to understand language. Eu-
roSpeech’93, 448–458.
</reference>
<page confidence="0.998935">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.631418">
<title confidence="0.991994">Experiments Using OSTIA for a Language Production Task</title>
<author confidence="0.8382">Angluin</author>
<affiliation confidence="0.913287">Department of Computer Science, Yale</affiliation>
<address confidence="0.769678">P.O.Box 208285, New Haven, CT,</address>
<abstract confidence="0.99821526">The phenomenon of meaning-preserving corrections given by an adult to a child involves several aspects: (1) the child produces an incorrect utterance, which the adult nevertheless understands, (2) the adult produces a correct utterance with the same meaning and (3) the child recognizes the adult utterance as having the same meaning as its previous utterance, and takes that as a signal that its previous utterance is not correct according to the adult grammar. An adequate model of this phenomenon must incorporate utterances and meanings, account for how the child and adult can understand each other’s meanings, and model how meaning-preserving corrections interact with the child’s increasing mastery of language production. In this paper we are concerned with how a learner who has learned to comprehend utterances might go about learning to produce them. We consider a model of language comprehension and production based on finite sequential and subsequential transducers. Utterances are modeled as finite sequences of words and meanings as finite sequences of predicates. Comprehension is interpreted as a mapping of utterances to meanings and production as a mapping of meanings to utterances. Previous work (Castellanos et al., 1993; Pieraccini et al., 1993) has applied subsequential transducers and the OSTIA algorithm to the problem of learning to comprehend language; here we apply them to the problem of learning to produce language. For ten natural languages and a limited domain of geometshapes and their properties and relations we define sequential transducers to produce pairs consisting of an utterance in that language and its meaning. Using this data we empirically explore the properties of the OSTIA and DD-OSTIA algorithms for the tasks of learning comprehension and production in this domain, to assess whether they may provide a basis for a model of meaning-preserving corrections.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dana Angluin</author>
<author>Leonor Becerra-Bonache</author>
</authors>
<date>2008</date>
<booktitle>Learning Meaning Before Syntax. ICGI,</booktitle>
<pages>281--292</pages>
<contexts>
<context position="11645" citStr="Angluin and Becerra-Bonache, 2008" startWordPosition="1880" endWordPosition="1883">ed by using DD-OSTIA in certain translation tasks from Spanish to English. 3 From telegraphic to adult speech To model how the learner can move from telegraphic speech to adult speech, we reduce this problem to a translation problem, in which the learner has to learn a mapping from sequences of predicates to utterances. As we have seen in the previous section, SSTs are an interesting approach to machine translation. Therefore, we explore the possibility of modeling language production using SSTs and OSTIA, to see whether they may provide a good framework to model corrections. As described in (Angluin and Becerra-Bonache, 2008), after learning the meaning function the learner is able to assign correct meanings to utterances, and therefore, given a situation and an utterance that denotes something in the situation, the learner is able to point correctly to the object denoted by the utterance. To simplify the task we consider, we make two assumptions about the learner at the start of the production phase: (1) the learner’s lexicon represents a correct meaning function and (2) the learner can generate correct sequences of predicates. Therefore, in the initial stage of the production phase, the learner is able to produc</context>
</contexts>
<marker>Angluin, Becerra-Bonache, 2008</marker>
<rawString>Dana Angluin and Leonor Becerra-Bonache. 2008. Learning Meaning Before Syntax. ICGI, 281–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonor Becerra-Bonache</author>
<author>Colin de la Higuera</author>
<author>J C Janodet</author>
<author>Frederic Tantini</author>
</authors>
<date>2007</date>
<booktitle>Learning Balls of Strings with Correction Queries. ECML,</booktitle>
<pages>18--29</pages>
<contexts>
<context position="2488" citStr="Becerra-Bonache et al., 2007" startWordPosition="386" endWordPosition="389">nce in that language and its meaning. Using this data we empirically explore the properties of the OSTIA and DD-OSTIA algorithms for the tasks of learning comprehension and production in this domain, to assess whether they may provide a basis for a model of meaning-preserving corrections. 1 Introduction The role of corrections in language learning has recently received substantial attention in Grammatical Inference. The kinds of corrections considered are mainly syntactic corrections based on proximity between strings. For example, a correction of a string may be given by using edit distance (Becerra-Bonache et al., 2007; Kinber, 2008) or based on the shortest extension of the queried string (Becerra-Bonache et al., 2006), among others. In these approaches semantic information is not used. However, in natural situations, a child’s erroneous utterances are corrected by her parents based on the meaning that the child intends to express; typically, the adult’s corrections preserve the intended meaning of the child. Adults use corrections in part as a way of making sure they have understood the child’s intentions, in order to keep the conversation “on track”. Thus the child’s utterance and the adult’s correction </context>
</contexts>
<marker>Becerra-Bonache, Higuera, Janodet, Tantini, 2007</marker>
<rawString>Leonor Becerra-Bonache, Colin de la Higuera, J.C. Janodet, and Frederic Tantini. 2007. Learning Balls of Strings with Correction Queries. ECML, 18–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonor Becerra-Bonache</author>
<author>Adrian H Dediu</author>
<author>Cristina Tirnauca</author>
</authors>
<date>2006</date>
<booktitle>Learning DFA from Correction and Equivalence Queries. ICGI,</booktitle>
<pages>281--292</pages>
<contexts>
<context position="2591" citStr="Becerra-Bonache et al., 2006" startWordPosition="402" endWordPosition="405">IA and DD-OSTIA algorithms for the tasks of learning comprehension and production in this domain, to assess whether they may provide a basis for a model of meaning-preserving corrections. 1 Introduction The role of corrections in language learning has recently received substantial attention in Grammatical Inference. The kinds of corrections considered are mainly syntactic corrections based on proximity between strings. For example, a correction of a string may be given by using edit distance (Becerra-Bonache et al., 2007; Kinber, 2008) or based on the shortest extension of the queried string (Becerra-Bonache et al., 2006), among others. In these approaches semantic information is not used. However, in natural situations, a child’s erroneous utterances are corrected by her parents based on the meaning that the child intends to express; typically, the adult’s corrections preserve the intended meaning of the child. Adults use corrections in part as a way of making sure they have understood the child’s intentions, in order to keep the conversation “on track”. Thus the child’s utterance and the adult’s correction have the same meaning, but the form is different. As Chouinard and Clark point out (2003), because chil</context>
</contexts>
<marker>Becerra-Bonache, Dediu, Tirnauca, 2006</marker>
<rawString>Leonor Becerra-Bonache, Adrian H. Dediu, and Cristina Tirnauca. 2006. Learning DFA from Correction and Equivalence Queries. ICGI, 281–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Berstel</author>
</authors>
<title>Transductions and Context-Free Languages.</title>
<date>1979</date>
<tech>PhD Thesis,</tech>
<location>Teubner, Stuttgart,</location>
<contexts>
<context position="8334" citStr="Berstel, 1979" startWordPosition="1339" endWordPosition="1340">pings between languages. Each edge of an SST has an associated input symbol and output string. When an input string is accepted, an SST produces an output string that consists of concatenating the output substrings associated with sequence of edges traversed, together with the substring associated with the last state reached by the input string. Several phenomena in natural languages can be easily represented by means of SSTs, for example, the different orders of noun and adjective in Spanish and English (e.g., un cuadrado rojo - a red square). Formal and detailed definitions can be found in (Berstel, 1979). For any SST it is always possible to find an equivalent SST that has the output strings assigned to the edges and states so that they are as close to 17 the initial state as they can be. This is called an Onward Subsequential Transducer (OST). It has been proved that SSTs are learnable in the limit from a positive presentation of sentence pairs by an efficient algorithm called OSTIA (Onward Subsequential Transducer Inference Algorithm) (Oncina, 1991). OSTIA takes as input a finite training set of input-output pairs of sentences, and produces as output an OST that generalizes the training pai</context>
</contexts>
<marker>Berstel, 1979</marker>
<rawString>Jean Berstel. 1979. Transductions and Context-Free Languages. PhD Thesis, Teubner, Stuttgart, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Castellanos</author>
<author>Enrique Vidal</author>
<author>Jose Oncina</author>
</authors>
<title>Language Understanding and Subsequential Transducers.</title>
<date>1993</date>
<booktitle>ICGI,</booktitle>
<pages>11--1</pages>
<contexts>
<context position="1478" citStr="Castellanos et al., 1993" startWordPosition="222" endWordPosition="226"> model how meaning-preserving corrections interact with the child’s increasing mastery of language production. In this paper we are concerned with how a learner who has learned to comprehend utterances might go about learning to produce them. We consider a model of language comprehension and production based on finite sequential and subsequential transducers. Utterances are modeled as finite sequences of words and meanings as finite sequences of predicates. Comprehension is interpreted as a mapping of utterances to meanings and production as a mapping of meanings to utterances. Previous work (Castellanos et al., 1993; Pieraccini et al., 1993) has applied subsequential transducers and the OSTIA algorithm to the problem of learning to comprehend language; here we apply them to the problem of learning to produce language. For ten natural languages and a limited domain of geometric shapes and their properties and relations we define sequential transducers to produce pairs consisting of an utterance in that language and its meaning. Using this data we empirically explore the properties of the OSTIA and DD-OSTIA algorithms for the tasks of learning comprehension and production in this domain, to assess whether </context>
<context position="10259" citStr="Castellanos et al., 1993" startWordPosition="1657" endWordPosition="1660">the onward tree subsequential transducer are considered in order, level by level, and are merged if possible (i.e., if the resulting transducer is subsequential and does not contradict any pair in the training set). SSTs and OSTIA have been successfully applied to different translation tasks: Roman numerals to their decimal representations, numbers written in English to their Spanish spelling (Oncina, 1991) and Spanish sentences describing simple visual scenes to corresponding English and German sentences (Castellanos et al., 1994). They have also been applied to language understanding tasks (Castellanos et al., 1993; Pieraccini et al., 1993). Moreover, several extensions of OSTIA have been introduced. For example, OSTIA-DR incorporates domain (input) and range (output) models in the learning process, allowing the algorithm to learn SSTs that accept only sentences compatible with the input model and produce only sentences compatible with the output model (Oncina and Varo, 1996). Experiments with a language understanding task gave better results with OSTIADR than with OSTIA (Castellanos et al., 1993). Another extension is DD-OSTIA (Oncina, 1998), which instead of considering a lexicographic order to merge </context>
<context position="19663" citStr="Castellanos et al., 1993" startWordPosition="3244" endWordPosition="3247">evious experiment. The results obtained are shown in Table 2. We also report the sizes of the transducers learned by OSTIA and DD-OSTIA. Table 3 and Table 4 show the numbers of states and edges of the transducers after convergence for each language. In case of disagreements, the number reported is the mode. To answer the question of whether production is harder than comprehension in this setting, we also considered the comprehension task, that is, to translate an utterance in a natural language into the corresponding sequence of predicates. The comprehension task was studied by Oncina et al. (Castellanos et al., 1993). They used English sentences, with a more complex version of the Feldman task domain and more complex semantic representations than we use. Our results are presented in Table 5. The number of states and edges of the transducers after convergence is shown in Table 6. 5 Discussion It should be noted that because the transducers output by OSTIA and DD-OSTIA correctly reproduce all the pairs used to construct them, once either algorithm has seen all 444 possible data pairs in either the production or the comprehension task, the resulting transducers will correctly translate all correct inputs. Ho</context>
<context position="22550" citStr="Castellanos et al., 1993" startWordPosition="3723" endWordPosition="3727">izes of transducers at convergence using 400 random unique input-output pairs (utterance, predicate sequence). In cases of disagreement, the number reported is the mode. ducer that achieves correct translation of correct utterances into predicate sequences. OSTIA and DD-OSTIA converged to 1-state transducers for all languages except Greek, for which they converged to 2-state transducers. Examining one such transducer for Greek, we found that the requirement that the transducer be “onward” necessitated two states. These results are broadly compatible with the results obtained by Oncina et al. (Castellanos et al., 1993) on language understanding; the more complex tasks they consider also give evidence that this approach may scale well for the comprehension task. Turning to the production task (Tables 1, 2, 3 and 4), we see that providing the random samples with a length-lex ordering of utterances has inconsistent effects for both OSTIA and DD-OSTIA, sometimes dramatically increasing or decreasing the number of samples required. We do not further consider the sorted samples. Comparing the production task with the comprehension task for OSTIA, the production task generally requires substantially more random un</context>
<context position="26918" citStr="Castellanos et al., 1993" startWordPosition="4451" endWordPosition="4455">fectly translate the predicate sequences used to construct them, but typically produce other translations that the learner (using the lexicon) would know to be incorrect. For example, the intermediate transducer from Figure 1 translates the predicate sequence (ci) as el circulo a la izquierda del circulo verde, which the learner’s lexicon indicates should be translated as (ci, le, ci, gr). 6 Future work Further experiments and analysis are required to understand how these results will scale with larger domains and languages. In this connection, it may be interesting to try the experiments of (Castellanos et al., 1993) in the reverse (production) direction. Finding a way to incorporate the learner’s initial lexicon seems important. Perhaps by incorporating the learner’s knowledge of the input domain (the legal sequences of predicates) and using the domain-aware version, OSTIA-D, the intermediate results in the learning process would be more compatible with our modeling objectives. Coping with errors will be necessary; perhaps an explicitly statistical framework for machine translation should be considered. If we can find an appropriate model of how the learner’s language production process might evolve, the</context>
</contexts>
<marker>Castellanos, Vidal, Oncina, 1993</marker>
<rawString>Antonio Castellanos, Enrique Vidal, and Jose Oncina. 1993. Language Understanding and Subsequential Transducers. ICGI, 11/1–11/10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Castellanos</author>
<author>Ismael Galiano</author>
<author>Enrique Vidal</author>
</authors>
<title>Applications of OSTIA to machine translation tasks.</title>
<date>1994</date>
<journal>ICGI,</journal>
<pages>93--105</pages>
<contexts>
<context position="10172" citStr="Castellanos et al., 1994" startWordPosition="1644" endWordPosition="1647">e leaves of the tree towards the root. • Starting from the root, all pairs of states of the onward tree subsequential transducer are considered in order, level by level, and are merged if possible (i.e., if the resulting transducer is subsequential and does not contradict any pair in the training set). SSTs and OSTIA have been successfully applied to different translation tasks: Roman numerals to their decimal representations, numbers written in English to their Spanish spelling (Oncina, 1991) and Spanish sentences describing simple visual scenes to corresponding English and German sentences (Castellanos et al., 1994). They have also been applied to language understanding tasks (Castellanos et al., 1993; Pieraccini et al., 1993). Moreover, several extensions of OSTIA have been introduced. For example, OSTIA-DR incorporates domain (input) and range (output) models in the learning process, allowing the algorithm to learn SSTs that accept only sentences compatible with the input model and produce only sentences compatible with the output model (Oncina and Varo, 1996). Experiments with a language understanding task gave better results with OSTIADR than with OSTIA (Castellanos et al., 1993). Another extension i</context>
</contexts>
<marker>Castellanos, Galiano, Vidal, 1994</marker>
<rawString>Antonio Castellanos, Ismael Galiano, and Enrique Vidal. 1994. Applications of OSTIA to machine translation tasks. ICGI, 93–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle M Chouinard</author>
<author>Eve V Clark</author>
</authors>
<title>Adult Reformulations of Child Errors as Negative Evidence.</title>
<date>2003</date>
<journal>Journal of Child Language,</journal>
<pages>30--637</pages>
<contexts>
<context position="3430" citStr="Chouinard and Clark, 2003" startWordPosition="544" endWordPosition="547">ess; typically, the adult’s corrections preserve the intended meaning of the child. Adults use corrections in part as a way of making sure they have understood the child’s intentions, in order to keep the conversation “on track”. Thus the child’s utterance and the adult’s correction have the same meaning, but the form is different. As Chouinard and Clark point out (2003), because children attend to contrasts in form, any change in form that does not mark a different meaning will signal to children that they may have produced something that is not acceptable in the target language. Results in (Chouinard and Clark, 2003) show that adults reformulate erroneous child utterances often enough to help learning. Moreover, these reProceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 16–23, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 16 sults show that children can not only detect differences between their own utterance and the adult reformulation, but that they do make use of that information. Thus in some natural situations, corrections have a semantic component that has not been taken into account in previous Grammatical Infer</context>
</contexts>
<marker>Chouinard, Clark, 2003</marker>
<rawString>Michelle M. Chouinard and Eve V. Clark. 2003. Adult Reformulations of Child Errors as Negative Evidence. Journal of Child Language, 30:637–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome A Feldman</author>
<author>George Lakoff</author>
<author>Andreas Stolcke</author>
<author>Susan Hollback Weber</author>
</authors>
<title>Miniature Language Acquisition: A touchstone for cognitive science.</title>
<date>1990</date>
<tech>Technical Report, TR-90-009.</tech>
<institution>International Computer Science Institute, Berkeley,</institution>
<location>California.</location>
<contexts>
<context position="13477" citStr="Feldman et al., 1990" startWordPosition="2194" endWordPosition="2197">s output the corresponding utterance in a natural language (e.g., the blue square to the right of the circle). In this example, 18 the learner must learn to include appropriate function words. In other languages, the learner may have to learn a correct choice of words determined by gender, case or other factors. (Note that we are not yet in a position to consider corrections.) 4 Experiments Our experiments were made for a limited domain of geometric shapes and their properties and relations. This domain is a simplification of the Miniature Language Acquisition task proposed by Feldman et al. (Feldman et al., 1990). Previous applications of OSTIA to language understanding and machine translation have also used adaptations and extensions of the Feldman task. In our experiments, we have predicates for three different shapes (circle (ci), square (sq) and triangle (tr)), three different colors (blue (bl), green (gr) and red (re)) and three different relations (to the left of (le), to the right of (ler), and above (ab)). We consider ten different natural languages: Arabic, English, Greek, Hebrew, Hindi, Hungarian, Mandarin, Russian, Spanish and Turkish. We created a data sequence of input-output pairs, each </context>
</contexts>
<marker>Feldman, Lakoff, Stolcke, Weber, 1990</marker>
<rawString>Jerome A. Feldman, George Lakoff, Andreas Stolcke, and Susan Hollback Weber. 1990. Miniature Language Acquisition: A touchstone for cognitive science. Technical Report, TR-90-009. International Computer Science Institute, Berkeley, California. April, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efim Kinber</author>
</authors>
<title>On Learning Regular Expressions and Patterns Via Membership and Correction Queries.</title>
<date>2008</date>
<booktitle>ICGI,</booktitle>
<pages>125--138</pages>
<contexts>
<context position="2503" citStr="Kinber, 2008" startWordPosition="390" endWordPosition="391">eaning. Using this data we empirically explore the properties of the OSTIA and DD-OSTIA algorithms for the tasks of learning comprehension and production in this domain, to assess whether they may provide a basis for a model of meaning-preserving corrections. 1 Introduction The role of corrections in language learning has recently received substantial attention in Grammatical Inference. The kinds of corrections considered are mainly syntactic corrections based on proximity between strings. For example, a correction of a string may be given by using edit distance (Becerra-Bonache et al., 2007; Kinber, 2008) or based on the shortest extension of the queried string (Becerra-Bonache et al., 2006), among others. In these approaches semantic information is not used. However, in natural situations, a child’s erroneous utterances are corrected by her parents based on the meaning that the child intends to express; typically, the adult’s corrections preserve the intended meaning of the child. Adults use corrections in part as a way of making sure they have understood the child’s intentions, in order to keep the conversation “on track”. Thus the child’s utterance and the adult’s correction have the same m</context>
</contexts>
<marker>Kinber, 2008</marker>
<rawString>Efim Kinber. 2008. On Learning Regular Expressions and Patterns Via Membership and Correction Queries. ICGI, 125–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jose Oncina</author>
</authors>
<title>Aprendizaje de lenguajes regulares y transducciones subsecuenciales.</title>
<date>1991</date>
<tech>PhD Thesis,</tech>
<institution>Universitat Politecnica de Valencia,</institution>
<location>Valencia, Spain,</location>
<contexts>
<context position="7232" citStr="Oncina, 1991" startWordPosition="1151" endWordPosition="1152">ances) to study their effect on language learning. We thus propose to model the problem of child language production as a machine translation problem, that is, as the task of translating a sequence of predicate symbols (representing the meaning of an utterance) into a corresponding utterance in a natural language. In this paper we explore the possibility of applying existing automata-theoretic approaches to machine translation to model language production. In Section 2, we describe the use of subsequential transducers for machine translation tasks and review the OSTIA algorithm to learn them (Oncina, 1991). In Section 3, we present our model of how the learner can move from telegraphic to adult speech. In Section 4, we present the results of experiments in the model made using OSTIA. Discussion of these results is presented in Section 5 and ideas for future work are in Section 6. 2 Learning Subsequential Transducers Subsequential transducers (SSTs) are a formal model of translation widely studied in the literature. SSTs are deterministic finite state models that allow input-output mappings between languages. Each edge of an SST has an associated input symbol and output string. When an input str</context>
<context position="8790" citStr="Oncina, 1991" startWordPosition="1418" endWordPosition="1419"> orders of noun and adjective in Spanish and English (e.g., un cuadrado rojo - a red square). Formal and detailed definitions can be found in (Berstel, 1979). For any SST it is always possible to find an equivalent SST that has the output strings assigned to the edges and states so that they are as close to 17 the initial state as they can be. This is called an Onward Subsequential Transducer (OST). It has been proved that SSTs are learnable in the limit from a positive presentation of sentence pairs by an efficient algorithm called OSTIA (Onward Subsequential Transducer Inference Algorithm) (Oncina, 1991). OSTIA takes as input a finite training set of input-output pairs of sentences, and produces as output an OST that generalizes the training pairs. The algorithm proceeds as follows (this description is based on (Oncina, 1998)): • A prefix tree representation of all the input sentences of the training set is built. Empty strings are assigned as output strings to both the internal nodes and the edges of this tree, and every output sentence of the training set is assigned to the corresponding leaf of the tree. The result is called a tree subsequential transducer. • An onward tree subsequential t</context>
<context position="10045" citStr="Oncina, 1991" startWordPosition="1628" endWordPosition="1629">tial transducer is constructed by moving the longest common prefixes of the output strings, level by level, from the leaves of the tree towards the root. • Starting from the root, all pairs of states of the onward tree subsequential transducer are considered in order, level by level, and are merged if possible (i.e., if the resulting transducer is subsequential and does not contradict any pair in the training set). SSTs and OSTIA have been successfully applied to different translation tasks: Roman numerals to their decimal representations, numbers written in English to their Spanish spelling (Oncina, 1991) and Spanish sentences describing simple visual scenes to corresponding English and German sentences (Castellanos et al., 1994). They have also been applied to language understanding tasks (Castellanos et al., 1993; Pieraccini et al., 1993). Moreover, several extensions of OSTIA have been introduced. For example, OSTIA-DR incorporates domain (input) and range (output) models in the learning process, allowing the algorithm to learn SSTs that accept only sentences compatible with the input model and produce only sentences compatible with the output model (Oncina and Varo, 1996). Experiments with</context>
</contexts>
<marker>Oncina, 1991</marker>
<rawString>Jose Oncina. 1991. Aprendizaje de lenguajes regulares y transducciones subsecuenciales. PhD Thesis, Universitat Politecnica de Valencia, Valencia, Spain, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jose Oncina</author>
</authors>
<title>The data driven approach applied to the OSTIA algorithm.</title>
<date>1998</date>
<journal>ICGI,</journal>
<pages>50--56</pages>
<contexts>
<context position="9016" citStr="Oncina, 1998" startWordPosition="1456" endWordPosition="1457">s the output strings assigned to the edges and states so that they are as close to 17 the initial state as they can be. This is called an Onward Subsequential Transducer (OST). It has been proved that SSTs are learnable in the limit from a positive presentation of sentence pairs by an efficient algorithm called OSTIA (Onward Subsequential Transducer Inference Algorithm) (Oncina, 1991). OSTIA takes as input a finite training set of input-output pairs of sentences, and produces as output an OST that generalizes the training pairs. The algorithm proceeds as follows (this description is based on (Oncina, 1998)): • A prefix tree representation of all the input sentences of the training set is built. Empty strings are assigned as output strings to both the internal nodes and the edges of this tree, and every output sentence of the training set is assigned to the corresponding leaf of the tree. The result is called a tree subsequential transducer. • An onward tree subsequential transducer equivalent to the tree subsequential transducer is constructed by moving the longest common prefixes of the output strings, level by level, from the leaves of the tree towards the root. • Starting from the root, all </context>
<context position="10797" citStr="Oncina, 1998" startWordPosition="1743" endWordPosition="1744">also been applied to language understanding tasks (Castellanos et al., 1993; Pieraccini et al., 1993). Moreover, several extensions of OSTIA have been introduced. For example, OSTIA-DR incorporates domain (input) and range (output) models in the learning process, allowing the algorithm to learn SSTs that accept only sentences compatible with the input model and produce only sentences compatible with the output model (Oncina and Varo, 1996). Experiments with a language understanding task gave better results with OSTIADR than with OSTIA (Castellanos et al., 1993). Another extension is DD-OSTIA (Oncina, 1998), which instead of considering a lexicographic order to merge states, uses a heuristic order based on a measure of the equivalence of the states. Experiments in (Oncina, 1998) show that better results can be obtained by using DD-OSTIA in certain translation tasks from Spanish to English. 3 From telegraphic to adult speech To model how the learner can move from telegraphic speech to adult speech, we reduce this problem to a translation problem, in which the learner has to learn a mapping from sequences of predicates to utterances. As we have seen in the previous section, SSTs are an interesting</context>
<context position="18879" citStr="Oncina, 1998" startWordPosition="3112" endWordPosition="3113">50 Turkish 50 40 Languages #states #edges Arabic 2 20 English 2 20 Greek 9 65 Hebrew 2 20 Hindi 7 58 Hungarian 3 20 Mandarin 1 10 Russian 3 30 Spanish 2 20 Turkish 4 31 le/ a la izquierda del ler/ a la derecha del ab/ encima del sq/ cuadrado ci/ circulo tr/ triangulo 2 Table 2: Production task, DD-OSTIA. The entries give the median number of input-output pairs until convergence in 10 runs. For Greek, Hindi and Hungarian, the median for the unsorted case is calculated using all 444 random unique pairs, instead of 400. Table 3: Production task, OSTIA. Sizes of transducers at convergence. tasks (Oncina, 1998), we also tried using DDOSTIA for learning to translate a sequence of predicates to an utterance. We used the same sequences of input-output pairs as in the previous experiment. The results obtained are shown in Table 2. We also report the sizes of the transducers learned by OSTIA and DD-OSTIA. Table 3 and Table 4 show the numbers of states and edges of the transducers after convergence for each language. In case of disagreements, the number reported is the mode. To answer the question of whether production is harder than comprehension in this setting, we also considered the comprehension task</context>
</contexts>
<marker>Oncina, 1998</marker>
<rawString>Jose Oncina. 1998. The data driven approach applied to the OSTIA algorithm. ICGI, 50–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jose Oncina</author>
<author>Miguel Angel Varo</author>
</authors>
<title>Using domain information during the learing of a subsequential transducer.</title>
<date>1996</date>
<journal>ICGI,</journal>
<pages>301--312</pages>
<contexts>
<context position="10627" citStr="Oncina and Varo, 1996" startWordPosition="1715" endWordPosition="1718">to their Spanish spelling (Oncina, 1991) and Spanish sentences describing simple visual scenes to corresponding English and German sentences (Castellanos et al., 1994). They have also been applied to language understanding tasks (Castellanos et al., 1993; Pieraccini et al., 1993). Moreover, several extensions of OSTIA have been introduced. For example, OSTIA-DR incorporates domain (input) and range (output) models in the learning process, allowing the algorithm to learn SSTs that accept only sentences compatible with the input model and produce only sentences compatible with the output model (Oncina and Varo, 1996). Experiments with a language understanding task gave better results with OSTIADR than with OSTIA (Castellanos et al., 1993). Another extension is DD-OSTIA (Oncina, 1998), which instead of considering a lexicographic order to merge states, uses a heuristic order based on a measure of the equivalence of the states. Experiments in (Oncina, 1998) show that better results can be obtained by using DD-OSTIA in certain translation tasks from Spanish to English. 3 From telegraphic to adult speech To model how the learner can move from telegraphic speech to adult speech, we reduce this problem to a tra</context>
</contexts>
<marker>Oncina, Varo, 1996</marker>
<rawString>Jose Oncina and Miguel Angel Varo. 1996. Using domain information during the learing of a subsequential transducer. ICGI, 301–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Pieraccini</author>
<author>Esther Levin</author>
<author>Enrique Vidal</author>
</authors>
<title>Learning how to understand language.</title>
<date>1993</date>
<booktitle>EuroSpeech’93,</booktitle>
<pages>448--458</pages>
<contexts>
<context position="1504" citStr="Pieraccini et al., 1993" startWordPosition="227" endWordPosition="230">ing corrections interact with the child’s increasing mastery of language production. In this paper we are concerned with how a learner who has learned to comprehend utterances might go about learning to produce them. We consider a model of language comprehension and production based on finite sequential and subsequential transducers. Utterances are modeled as finite sequences of words and meanings as finite sequences of predicates. Comprehension is interpreted as a mapping of utterances to meanings and production as a mapping of meanings to utterances. Previous work (Castellanos et al., 1993; Pieraccini et al., 1993) has applied subsequential transducers and the OSTIA algorithm to the problem of learning to comprehend language; here we apply them to the problem of learning to produce language. For ten natural languages and a limited domain of geometric shapes and their properties and relations we define sequential transducers to produce pairs consisting of an utterance in that language and its meaning. Using this data we empirically explore the properties of the OSTIA and DD-OSTIA algorithms for the tasks of learning comprehension and production in this domain, to assess whether they may provide a basis f</context>
<context position="10285" citStr="Pieraccini et al., 1993" startWordPosition="1661" endWordPosition="1664">ial transducer are considered in order, level by level, and are merged if possible (i.e., if the resulting transducer is subsequential and does not contradict any pair in the training set). SSTs and OSTIA have been successfully applied to different translation tasks: Roman numerals to their decimal representations, numbers written in English to their Spanish spelling (Oncina, 1991) and Spanish sentences describing simple visual scenes to corresponding English and German sentences (Castellanos et al., 1994). They have also been applied to language understanding tasks (Castellanos et al., 1993; Pieraccini et al., 1993). Moreover, several extensions of OSTIA have been introduced. For example, OSTIA-DR incorporates domain (input) and range (output) models in the learning process, allowing the algorithm to learn SSTs that accept only sentences compatible with the input model and produce only sentences compatible with the output model (Oncina and Varo, 1996). Experiments with a language understanding task gave better results with OSTIADR than with OSTIA (Castellanos et al., 1993). Another extension is DD-OSTIA (Oncina, 1998), which instead of considering a lexicographic order to merge states, uses a heuristic o</context>
</contexts>
<marker>Pieraccini, Levin, Vidal, 1993</marker>
<rawString>Roberto Pieraccini, Esther Levin, and Enrique Vidal. 1993. Learning how to understand language. EuroSpeech’93, 448–458.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>