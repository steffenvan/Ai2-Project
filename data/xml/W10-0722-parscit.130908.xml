<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.103165">
<title confidence="0.978858">
Non-Expert Evaluation of Summarization Systems is Risky
</title>
<author confidence="0.999624">
Dan Gillick Yang Liu
</author>
<affiliation confidence="0.9756485">
University of California, Berkeley University of Texas, Dallas
Computer Science Division Department of Computer Science
</affiliation>
<email confidence="0.999462">
dgillick@cs.berkeley.edu yangl@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999821">
We provide evidence that intrinsic evalua-
tion of summaries using Amazon’s Mechan-
ical Turk is quite difficult. Experiments mir-
roring evaluation at the Text Analysis Con-
ference’s summarization track show that non-
expert judges are not able to recover system
rankings derived from experts.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945315789474">
Automatic summarization is a particularly difficult
task to evaluate. What makes a good summary?
What information is relevant? Is it possible to sepa-
rate information content from linguistic quality?
Besides subjectivity issues, evaluation is time-
consuming. Ideally, a judge would read the original
set of documents before deciding how well the im-
portant aspects are conveyed by a summary. A typ-
ical 10-document problem could reasonably involve
25 minutes of reading or skimming and 5 more min-
utes for assessing a 100-word summary. Since sum-
mary output can be quite variable, at least 30 top-
ics should be evaluated to get a robust estimate of
performance. Assuming a single judge evaluates all
summaries for a topic (more redundancy would be
better), we get a rough time estimate: 17.5 hours to
evaluate two systems.
Thus it is of great interest to find ways of speeding
up evaluation while minimizing subjectivity. Ama-
zon’s Mechanical Turk (MTurk) system has been
used for a variety of labeling and annotation tasks
(Snow et al., 2008), but such crowd-sourcing has not
been tested for summarization.
We describe an experiment to test whether MTurk
is able to reproduce system-level rankings that
match expert opinion. Unlike the results of other
crowd-sourcing annotations for natural language
tasks, we find that non-expert judges are unable to
provide expert-like scores and tend to disagree sig-
nificantly with each other.
This paper is organized as follows: Section 2 in-
troduces the particular summarization task and data
we use in our experiments; Section 3 describes the
design of our Human Intelligence Task (HIT). Sec-
tion 4 shows experimental results and gives some
analysis. Section 5 reviews our main findings and
provides suggestions for researchers wishing to con-
duct their own crowd-sourcing evaluations.
</bodyText>
<sectionHeader confidence="0.989758" genericHeader="method">
2 TAC Summarization Task
</sectionHeader>
<subsectionHeader confidence="0.786124">
Topic: Peter Jennings
</subsectionHeader>
<bodyText confidence="0.984228166666667">
Description: Describe Peter Jennings’ lung cancer and its
effects.
Reference: Peter Jennings’s announcement April 5, 2005,
that he had lung cancer left his colleagues at ABC News sad-
dened and dismayed. He had been ”World News Tonight”
anchorman since 1983. By the end of the week, ABC had re-
ceived 3,400 e-mails offering him prayers and good wishes.
A former heavy smoker, Jennings had not been well for some
time and was unable to travel abroad to cover foreign events.
However, his diagnosis came as a surprise to him. ABC an-
nounced that Jennings would continue to anchor the news
during chemotherapy treatment, but he was unable to do so.
</bodyText>
<tableCaption confidence="0.9684255">
Table 1: An example topic and reference summary from
the TAC 2009 summarization task.
</tableCaption>
<bodyText confidence="0.975681714285714">
Our data comes from the submissions to the Text
Analysis Conference (TAC) summarization track in
2009 (Dang, 2009). The main task involved 44
query-focused topics, each requiring a system to
produce a 100-word summary of 10 related news
documents. Experts provided four reference sum-
maries for each topic. Table 1 shows an example.
</bodyText>
<page confidence="0.948405">
148
</page>
<note confidence="0.928033">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 148–151,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<tableCaption confidence="0.98494825">
Table 2: Identical summaries often were given different
scores by the same expert human judge at TAC 2009.
Counts of absolute score differences are shown for Over-
all Quality (OQ) and Linguistic Quality (LQ).
</tableCaption>
<subsectionHeader confidence="0.998478">
2.1 Agreement and consistency
</subsectionHeader>
<bodyText confidence="0.999987272727273">
In the official TAC evaluation, each summary was
judged by one of eight experts for “Overall Quality”
and “Linguistic Quality” on a 1 (“very poor”) to 10
(“very good”) scale. Unfortunately, the lack of re-
dundant judgments means we cannot estimate inter-
annotator agreement. However, we note that out of
all 4576 submitted summaries, there are 226 pairs
that are identical, which allows us to estimate anno-
tator consistency. Table 2 shows that an expert an-
notator will give the same summary the same score
just over half the time.
</bodyText>
<subsectionHeader confidence="0.997804">
2.2 Evaluation without source documents
</subsectionHeader>
<bodyText confidence="0.999985071428571">
One way to dramatically speed up evaluation is to
use the experts’ reference summaries as a gold stan-
dard, leaving the source documents out entirely.
This is the idea behind automatic evaluation with
ROUGE (Lin, 2004), which measures ngram over-
lap with the references, and assisted evaluation with
Pyramid (Nenkova and Passonneau, 2004), which
measures overlap of facts or “Semantic Content
Units” with the references. The same idea has also
been employed in various manual evaluations, for
example by Haghighi and Vanderwende (2009), to
directly compare the summaries of two different sys-
tems. The potential bias introduced by such abbre-
viated evaluation has not been explored.
</bodyText>
<sectionHeader confidence="0.978762" genericHeader="method">
3 HIT design
</sectionHeader>
<bodyText confidence="0.999597875">
The overall structure of the HIT we designed for
summary evaluation is as follows: The worker is
asked to read the topic and description, and then
two reference summaries (there is no mention of the
source documents). The candidate summary appears
next, followed by instructions to provide scores be-
tween 1 (very poor) and 10 (very good) in each cat-
egory1. Mouse-over on the category names provides
</bodyText>
<subsectionHeader confidence="0.72263">
1Besides Overall Quality and Linguistic Quality, we include
Information Content, to encourage judges to distinguish be-
</subsectionHeader>
<bodyText confidence="0.999936210526316">
extra details, copied with slight modifications from
Dang (2007).
Our initial HIT design asked workers to perform
a head-to-head comparison of two candidate sum-
maries, but we found this unsatisfactory for a num-
ber of reasons. First, many of the resulting scores
did not obey the transitive property: given sum-
maries x, y, and z, a single worker showed a pref-
erence for y &gt; x and z &gt; y, but also x &gt; z.
Second, while this kind of head-to-head evalua-
tion may be useful for system development, we are
specifically interested here in comparing non-expert
MTurk evaluation with expert TAC evaluation.
We went through a few rounds of revisions to the
language in the HIT after observing worker feed-
back. Specifically, we found it was important to em-
phasize that a good summary not only responds to
the topic and description, but also conveys the infor-
mation in the references.
</bodyText>
<subsectionHeader confidence="0.999126">
3.1 Quality control
</subsectionHeader>
<bodyText confidence="0.9999022">
Only workers with at least a 96% HIT approval rat-
ing2 were allowed access to this task. We moni-
tored results manually and blocked workers (reject-
ing their work) if they completed a HIT in under 25
seconds. Such suspect work typically showed uni-
form scores (usually all 10s). Nearly 30% of HITs
were rejected for this reason.
To encourage careful work, we included this note
in our HITs: “High annotator consistency is impor-
tant. If the scores you provide deviate from the av-
erage scores of other annotators on the same HIT,
your work will be rejected. We will award bonuses
for particularly good work.” We gave a few small
bonuses ($0.50) to workers who left thoughtful com-
ments.
</bodyText>
<subsectionHeader confidence="0.997008">
3.2 Compensation
</subsectionHeader>
<bodyText confidence="0.967953272727273">
We experimented with a few different compensation
levels and observed a somewhat counter-intuitive re-
sult. Higher compensation ($.10 per HIT) yielded
lower quality work than lower compensation ($.07
per HIT), judging by the number of HITs we re-
jected. It seems that lower compensation attracts
workers who are less interested in making money,
and thus willing to spend more time and effort.
There is a trade-off, though, as there are fewer work-
ers willing to do the task for less money.
tween content and readability.
</bodyText>
<footnote confidence="0.9812145">
2MTurk approval ratings calculated as the fraction of HITs
approved by requesters.
</footnote>
<figure confidence="0.995140888888889">
LQ
OQ
119
117
0
Score Difference
92
82
1
20
15
2
7
3
0
mean
0.54
0.63
</figure>
<page confidence="0.980724">
149
</page>
<table confidence="0.9988112">
Sys TAC LQ OQ MTurk C
OQ LQ
A 5.16 5.64 7.03 7.27 7.27
B 4.84 5.27 6.78 6.97 6.78
C 4.50 4.93 6.51 6.85 6.49
D 4.20 4.09 6.15 6.59 6.50
E 3.91 4.70 6.19 6.54 6.58
F 3.64 6.70 7.06 7.78 6.56
G 3.57 3.43 5.82 6.33 6.28
H 3.20 5.23 5.75 6.06 5.62
</table>
<tableCaption confidence="0.990633">
Table 3: Comparison of Overall Quality (OQ) and Lin-
guistic Quality (LQ) scores between the TAC and MTurk
evaluations. Content (C) is evaluated by MTurk workers
as well. Note that system F is the lead baseline.
</tableCaption>
<sectionHeader confidence="0.991896" genericHeader="evaluation">
4 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.976703318181818">
To assess how well MTurk workers are able to em-
ulate the work of expert judges employed by TAC,
we chose a subset of systems and analyze the results
of the two evaluations. The systems were chosen to
represent the entire range of average Overall Qual-
ity scores. System F is a simple lead baseline, which
generates a summary by selecting the first sentences
up to 100 words of the most recent document. The
rest of the systems were submitted by various track
participants. The MTurk evaluation included two-
times redundancy. That is, each summary was eval-
uated by two different people. The cost for the full
evaluation, including 44 topics, 8 systems, and 2x
redundancy, at $.07 per HIT, plus 10% commission
for Amazon, was $55.
Table 3 shows average scores for the two evalu-
ations. The data suggest that the MTurk judges are
better at evaluating Linguistic Quality than Content
or Overall Quality. In particular, the MTurk judges
appear to have difficulty distinguishing Linguistic
Quality from Content. We will defend these claims
with more analysis, below.
</bodyText>
<subsectionHeader confidence="0.993859">
4.1 Worker variability
</subsectionHeader>
<bodyText confidence="0.99994465625">
The first important question to address involves the
consistency of the workers. We cannot compare
agreement between TAC and MTurk evaluations, but
the MTurk agreement statistics suggest considerable
variability. In Overall Quality, the mean score differ-
ence between two workers for the same HIT is 2.4
(the standard deviation is 2.0). The mean is 2.2 for
Linguistic Quality (the standard deviation is 1.5).
In addition, the TAC judges show more similarity
with each other—as if they are roughly in agreement
about what makes a good summary. We compute
each judge’s average score and look at the standard
deviation of these averages for the two groups. The
TAC standard deviation is 1.0 (ranging from 3.0 to
6.1), whereas the MTurk standard deviation is 2.3
(ranging from 1.0 to 9.5). Note that the average
number of HITs performed by each MTurk worker
was just over 5.
Finally, we can use regression analysis to show
what fraction of the total score variance is captured
by judges, topics, and systems. We fit linear models
in R using binary indicators for each judge, topic,
and system. Redundant evaluations in the MTurk
set are removed for unbiased comparison with the
TAC set. Table 4 shows that the differences between
the TAC and MTurk evaluations are quite striking:
Taking the TAC data alone, the topics are the major
source of variance, whereas the judges are the major
source of variance in the MTurk data. The systems
account for only a small fraction of the variance in
the MTurk evaluation, which makes system ranking
more difficult.
</bodyText>
<table confidence="0.998665">
Eval Judges Topics Systems
TAC 0.28 0.40 0.13
MTurk 0.44 0.13 0.05
</table>
<tableCaption confidence="0.8516844">
Table 4: Linear regression is used to model Overall Qual-
ity scores as a function of judges, topics, and systems, re-
spectively, for each data set. The R2 values, which give
the fraction of variance explained by each of the six mod-
els, are shown.
</tableCaption>
<subsectionHeader confidence="0.999341">
4.2 Ranking comparisons
</subsectionHeader>
<bodyText confidence="0.9999581875">
The TAC evaluation, while lacking redundant judg-
ments, was a balanced experiment. That is, each
judge scored every system for a single topic. The
same is not true for the MTurk evaluation, and as
a result, the average per-system scores shown in
Table 3 may be biased. As a result, and because
we need to test multiple system-level differences si-
multaneously, a simple t-test is not quite sufficient.
We use Tukey’s Honestly Significant Differences
(HSD), explained in detail by Yandell (1997), to as-
sess statistical significance.
Tukey’s HSD test computes significance intervals
based on the range of the sample means rather than
individual differences, and includes an adjustment to
correct for imbalanced experimental designs. The R
implementation takes as input a linear model, so we
</bodyText>
<page confidence="0.993672">
150
</page>
<table confidence="0.999397666666667">
Eval Ranking
TAC (OQ) A B C DA EB FC GC HD
MTurk (OQ) F A B C EF GF DB HB
TAC (LQ) F AF BF HF CF EA DB GE
MTurk (LQ) F A BF CF DF EF HC GC
MTurk (C) A B E F D C GA HD
</table>
<tableCaption confidence="0.913997333333333">
Table 5: Systems are shown in rank order from highest
(left) to lowest (right) for each scoring metric: Over-
all Quality (OQ), Linguistic Quality (LQ), and Content
(C). The superscripts indicate the rightmost system that
is significantly different (at 95% confidence) according
to Tukey’s HSD test.
</tableCaption>
<bodyText confidence="0.9899635">
model scores using binary indicators for (J)udges,
(T)opics, and (S)ystems (see equation 1), and mea-
sure significance in the differences between system
coefficients (Sk).
</bodyText>
<equation confidence="0.9772855">
�score = α + QiJi + � �3T3 + � skSk (1)
i 3 k
</equation>
<bodyText confidence="0.9999086875">
Table 5 shows system rankings for the two evalu-
ations. The most obvious discrepancy between the
TAC and MTurk rankings is system F, the base-
line. Both TAC and MTurk judges gave F the high-
est scores for Linguistic Quality, a reasonable result
given its construction, whereas the other summaries
tend to pull sentences out of context. But the MTurk
judges also gave F the highest scores in Overall
Quality, suggesting that readability is more impor-
tant to amateur judges than experts, or at least easier
to identify. Content appears the most difficult cate-
gory for the MTurk judges, as few significant score
differences emerge. Even with more redundancy, it
seems unlikely that MTurk judges could produce a
ranking resembling the TAC Overall Quality rank-
ing using this evaluation framework.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9999915">
Through parallel evaluations by experts at TAC and
non-experts on MTurk, we have shown two main
results. First, as expected, MTurk workers pro-
duce considerably noisier work than experts. That
is, more redundancy is required to achieve statisti-
cal significance on par with expert judgments. This
finding matches prior work with MTurk. Second,
MTurk workers are unlikely to produce a score rank-
ing that matches expert rankings for Overall Quality.
This seems to be the result of some confusion in sep-
arating content from readability.
What does this mean for future evaluations? If
we want to assess overall summary quality—that is,
balancing content and linguistic quality like expert
judges do—we will need to redesign the task for
non-experts. Perhaps MTurk workers will be bet-
ter able to understand Nenkova’s Pyramid evaluation
(2004), which is designed to isolate content. Extrin-
sic evaluation, where judges use the summary to an-
swer questions derived from the source documents
or the references, as done by Callison-Burch for
evaluation of Machine Translation systems (2009),
is another possibility.
Finally, our results suggest that anyone conduct-
ing an evaluation of summarization systems using
non-experts should calibrate their results by asking
their judges to score summaries that have already
been evaluated by experts.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999514666666667">
Thanks to Benoit Favre for discussing the evaluation
format and to the anonymous reviewers for helpful,
detailed feedback.
</bodyText>
<sectionHeader confidence="0.999093" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999767956521739">
C. Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazons Me-
chanical Turk. Proceedings of EMNLP.
H.T. Dang. 2007. Overview of DUC 2007. In Proceed-
ings of the Document Understanding Conference.
H.T. Dang. 2009. Overview of the TAC 2009 opinion
question answering and summarization tasks. In Pro-
ceedings of Text Analysis Conference (TAC 2009).
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of HLT-NAACL.
C.Y. Lin. 2004. Rouge: A package for automatic evalu-
ation of summaries. In Proceedings of the Workshop:
Text Summarization Branches Out.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: The pyramid method.
In Proceedings of HLT-NAACL.
R. Snow, B. O’Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast—but is it good?: Evaluating non-
expert annotations for natural language tasks. In Pro-
ceedings of EMNLP.
B.S. Yandell. 1997. Practical data analysis for designed
experiments. Chapman &amp; Hall/CRC.
</reference>
<page confidence="0.998351">
151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983616">
<title confidence="0.99963">Non-Expert Evaluation of Summarization Systems is Risky</title>
<author confidence="0.999317">Dan Gillick Yang Liu</author>
<affiliation confidence="0.999337">University of California, Berkeley University of Texas, Dallas Computer Science Division Department of Computer Science</affiliation>
<email confidence="0.998639">dgillick@cs.berkeley.eduyangl@hlt.utdallas.edu</email>
<abstract confidence="0.99834825">We provide evidence that intrinsic evaluation of summaries using Amazon’s Mechanical Turk is quite difficult. Experiments mirroring evaluation at the Text Analysis Conference’s summarization track show that nonexpert judges are not able to recover system rankings derived from experts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: Evaluating translation quality using Amazons Mechanical Turk.</title>
<date>2009</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<marker>Callison-Burch, 2009</marker>
<rawString>C. Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using Amazons Mechanical Turk. Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2007</date>
<booktitle>In Proceedings of the Document Understanding Conference.</booktitle>
<contexts>
<context position="5774" citStr="Dang (2007)" startWordPosition="910" endWordPosition="911">t been explored. 3 HIT design The overall structure of the HIT we designed for summary evaluation is as follows: The worker is asked to read the topic and description, and then two reference summaries (there is no mention of the source documents). The candidate summary appears next, followed by instructions to provide scores between 1 (very poor) and 10 (very good) in each category1. Mouse-over on the category names provides 1Besides Overall Quality and Linguistic Quality, we include Information Content, to encourage judges to distinguish beextra details, copied with slight modifications from Dang (2007). Our initial HIT design asked workers to perform a head-to-head comparison of two candidate summaries, but we found this unsatisfactory for a number of reasons. First, many of the resulting scores did not obey the transitive property: given summaries x, y, and z, a single worker showed a preference for y &gt; x and z &gt; y, but also x &gt; z. Second, while this kind of head-to-head evaluation may be useful for system development, we are specifically interested here in comparing non-expert MTurk evaluation with expert TAC evaluation. We went through a few rounds of revisions to the language in the HIT</context>
</contexts>
<marker>Dang, 2007</marker>
<rawString>H.T. Dang. 2007. Overview of DUC 2007. In Proceedings of the Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
</authors>
<title>Overview of the TAC</title>
<date>2009</date>
<booktitle>In Proceedings of Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="3261" citStr="Dang, 2009" startWordPosition="511" endWordPosition="512">ht” anchorman since 1983. By the end of the week, ABC had received 3,400 e-mails offering him prayers and good wishes. A former heavy smoker, Jennings had not been well for some time and was unable to travel abroad to cover foreign events. However, his diagnosis came as a surprise to him. ABC announced that Jennings would continue to anchor the news during chemotherapy treatment, but he was unable to do so. Table 1: An example topic and reference summary from the TAC 2009 summarization task. Our data comes from the submissions to the Text Analysis Conference (TAC) summarization track in 2009 (Dang, 2009). The main task involved 44 query-focused topics, each requiring a system to produce a 100-word summary of 10 related news documents. Experts provided four reference summaries for each topic. Table 1 shows an example. 148 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 148–151, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Table 2: Identical summaries often were given different scores by the same expert human judge at TAC 2009. Counts of absolute score differences are shown for Overall Q</context>
</contexts>
<marker>Dang, 2009</marker>
<rawString>H.T. Dang. 2009. Overview of the TAC 2009 opinion question answering and summarization tasks. In Proceedings of Text Analysis Conference (TAC 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>L Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="5034" citStr="Haghighi and Vanderwende (2009)" startWordPosition="790" endWordPosition="793"> the same summary the same score just over half the time. 2.2 Evaluation without source documents One way to dramatically speed up evaluation is to use the experts’ reference summaries as a gold standard, leaving the source documents out entirely. This is the idea behind automatic evaluation with ROUGE (Lin, 2004), which measures ngram overlap with the references, and assisted evaluation with Pyramid (Nenkova and Passonneau, 2004), which measures overlap of facts or “Semantic Content Units” with the references. The same idea has also been employed in various manual evaluations, for example by Haghighi and Vanderwende (2009), to directly compare the summaries of two different systems. The potential bias introduced by such abbreviated evaluation has not been explored. 3 HIT design The overall structure of the HIT we designed for summary evaluation is as follows: The worker is asked to read the topic and description, and then two reference summaries (there is no mention of the source documents). The candidate summary appears next, followed by instructions to provide scores between 1 (very poor) and 10 (very good) in each category1. Mouse-over on the category names provides 1Besides Overall Quality and Linguistic Qu</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>A. Haghighi and L. Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop: Text Summarization Branches Out.</booktitle>
<contexts>
<context position="4718" citStr="Lin, 2004" startWordPosition="745" endWordPosition="746">nfortunately, the lack of redundant judgments means we cannot estimate interannotator agreement. However, we note that out of all 4576 submitted summaries, there are 226 pairs that are identical, which allows us to estimate annotator consistency. Table 2 shows that an expert annotator will give the same summary the same score just over half the time. 2.2 Evaluation without source documents One way to dramatically speed up evaluation is to use the experts’ reference summaries as a gold standard, leaving the source documents out entirely. This is the idea behind automatic evaluation with ROUGE (Lin, 2004), which measures ngram overlap with the references, and assisted evaluation with Pyramid (Nenkova and Passonneau, 2004), which measures overlap of facts or “Semantic Content Units” with the references. The same idea has also been employed in various manual evaluations, for example by Haghighi and Vanderwende (2009), to directly compare the summaries of two different systems. The potential bias introduced by such abbreviated evaluation has not been explored. 3 HIT design The overall structure of the HIT we designed for summary evaluation is as follows: The worker is asked to read the topic and </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C.Y. Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of the Workshop: Text Summarization Branches Out.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="4837" citStr="Nenkova and Passonneau, 2004" startWordPosition="760" endWordPosition="763">ever, we note that out of all 4576 submitted summaries, there are 226 pairs that are identical, which allows us to estimate annotator consistency. Table 2 shows that an expert annotator will give the same summary the same score just over half the time. 2.2 Evaluation without source documents One way to dramatically speed up evaluation is to use the experts’ reference summaries as a gold standard, leaving the source documents out entirely. This is the idea behind automatic evaluation with ROUGE (Lin, 2004), which measures ngram overlap with the references, and assisted evaluation with Pyramid (Nenkova and Passonneau, 2004), which measures overlap of facts or “Semantic Content Units” with the references. The same idea has also been employed in various manual evaluations, for example by Haghighi and Vanderwende (2009), to directly compare the summaries of two different systems. The potential bias introduced by such abbreviated evaluation has not been explored. 3 HIT design The overall structure of the HIT we designed for summary evaluation is as follows: The worker is asked to read the topic and description, and then two reference summaries (there is no mention of the source documents). The candidate summary appe</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>A. Nenkova and R. Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>B O’Connor</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: Evaluating nonexpert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>R. Snow, B. O’Connor, D. Jurafsky, and A.Y. Ng. 2008. Cheap and fast—but is it good?: Evaluating nonexpert annotations for natural language tasks. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B S Yandell</author>
</authors>
<title>Practical data analysis for designed experiments.</title>
<date>1997</date>
<publisher>Chapman &amp; Hall/CRC.</publisher>
<contexts>
<context position="11911" citStr="Yandell (1997)" startWordPosition="1971" endWordPosition="1972">he R2 values, which give the fraction of variance explained by each of the six models, are shown. 4.2 Ranking comparisons The TAC evaluation, while lacking redundant judgments, was a balanced experiment. That is, each judge scored every system for a single topic. The same is not true for the MTurk evaluation, and as a result, the average per-system scores shown in Table 3 may be biased. As a result, and because we need to test multiple system-level differences simultaneously, a simple t-test is not quite sufficient. We use Tukey’s Honestly Significant Differences (HSD), explained in detail by Yandell (1997), to assess statistical significance. Tukey’s HSD test computes significance intervals based on the range of the sample means rather than individual differences, and includes an adjustment to correct for imbalanced experimental designs. The R implementation takes as input a linear model, so we 150 Eval Ranking TAC (OQ) A B C DA EB FC GC HD MTurk (OQ) F A B C EF GF DB HB TAC (LQ) F AF BF HF CF EA DB GE MTurk (LQ) F A BF CF DF EF HC GC MTurk (C) A B E F D C GA HD Table 5: Systems are shown in rank order from highest (left) to lowest (right) for each scoring metric: Overall Quality (OQ), Linguist</context>
</contexts>
<marker>Yandell, 1997</marker>
<rawString>B.S. Yandell. 1997. Practical data analysis for designed experiments. Chapman &amp; Hall/CRC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>