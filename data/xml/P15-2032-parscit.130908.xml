<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000129">
<title confidence="0.888359">
Pre-training of Hidden-Unit CRFs
</title>
<author confidence="0.992939">
Young-Bum Kim† Karl Stratos‡ Ruhi Sarikaya†
</author>
<affiliation confidence="0.9778265">
†Microsoft Corporation, Redmond, WA
‡Columbia University, New York, NY
</affiliation>
<email confidence="0.9542375">
{ybkim, ruhi.sarikaya}@microsoft.com
stratos@cs.columbia.edu
</email>
<sectionHeader confidence="0.997353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996819047619">
In this paper, we apply the concept of pre-
training to hidden-unit conditional ran-
dom fields (HUCRFs) to enable learning
on unlabeled data. We present a simple
yet effective pre-training technique that
learns to associate words with their clus-
ters, which are obtained in an unsuper-
vised manner. The learned parameters are
then used to initialize the supervised learn-
ing process. We also propose a word clus-
tering technique based on canonical corre-
lation analysis (CCA) that is sensitive to
multiple word senses, to further improve
the accuracy within the proposed frame-
work. We report consistent gains over
standard conditional random fields (CRFs)
and HUCRFs without pre-training in se-
mantic tagging, named entity recognition
(NER), and part-of-speech (POS) tagging
tasks, which could indicate the task inde-
pendent nature of the proposed technique.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999785719298246">
Despite the recent accuracy gains of the deep
learning techniques for sequence tagging prob-
lems (Collobert and Weston, 2008; Collobert et
al., 2011; Mohamed et al., 2010; Deoras et al.,
2012; Xu and Sarikaya, 2013; Yao et al., 2013;
Mesnil et al., 2013; Wang and Manning, 2013;
Devlin et al., 2014), conditional random fields
(CRFs) (Lafferty et al., 2001; Sutton and McCal-
lum, 2006) still have been widely used in many
research and production systems for the problems
due to the effectiveness and simplicity of train-
ing, which does not involve task specific param-
eter tuning (Collins, 2002; McCallum and Li,
2003; Sha and Pereira, 2003; Turian et al., 2010;
Kim and Snyder, 2012; Celikyilmaz et al., 2013;
Sarikaya et al., 2014; Anastasakos et al., 2014;
Kim et al., 2014; Kim et al., 2015a; Kim et al.,
2015c; Kim et al., 2015b). The objective function
for CRF training operates globally over sequence
structures and can incorporate arbitrary features.
Furthermore, this objective is convex and can be
optimized relatively efficiently using dynamic pro-
gramming.
Pre-training has been widely used in deep learn-
ing (Hinton et al., 2006) and is one of the distin-
guishing advantages of deep learning models. The
best results obtained across a wide range of tasks
involve unsupervised pre-training phase followed
by the supervised training phase. The empirical
results (Erhan et al., 2010) suggest that unsuper-
vised pre-training has the regularization effect on
the learning process and also results in a model
parameter configuration that places the model near
the basins of attraction of minima that support bet-
ter generalization.
While pre-training became a standard steps in
many deep learning model training recipes, it has
not been applied to the family of CRFs. There
were several reasons for that; (i) the shallow and
linear nature of basic CRF model topology, which
limits their expressiveness to the inner product be-
tween data and model parameters, and (ii) Lack
of a training criterion and configuration to employ
pre-training on unlabeled data in a task indepen-
dent way.
Hidden-unit CRFs (HUCRFs) of Maaten et al.
(2011) provide a deeper model topology and im-
prove the expressive power of the CRFs but it
does not address how to train them in a task inde-
pendent way using unlabeled data. In this paper,
we present an effective technique for pre-training
of HUCRFs that can potentially lead to accuracy
gains over HUCRF and basic linear chain CRF
models. We cluster words in the text and treat clus-
ters as pseudo-labels to train an HUCRF. Then we
transfer the parameters corresponding to observa-
tions to initialize the training process on labeled
</bodyText>
<page confidence="0.874697">
192
</page>
<note confidence="0.368569">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192–198,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.979963">
Figure 1: Graphical representation of hidden unit
CRFs.
</figureCaption>
<bodyText confidence="0.999805444444444">
data. The intuition behind this is that words that
are clustered together tend to assume the same la-
bels. Therefore, learning the model parameters to
assign the correct cluster ID to each word should
accrue to assigning the correct task specific label
during supervised learning.
This pre-training step significantly reduces the
challenges in training a high-performance HUCRF
by (i) acquiring a broad feature coverage from un-
labeled data and thus improving the generalization
of the model to unseen events, (ii) finding a good a
initialization point for the model parameters, and
(iii) regularizing the parameter learning by min-
imizing variance and introducing a bias towards
configurations of the parameter space that are use-
ful for unsupervised learning.
We also propose a word clustering technique
based on canonical correlation analysis (CCA)
that is sensitive to multiple word senses. For ex-
ample, the resulting clusters can differentiate the
instance of “bank” in the sense of financial insti-
tutions and the land alongside the river. This is an
important point as different senses of a word are
likely to have a different task specific tag. Putting
them in different clusters would enable the HU-
CRF model to learn the distinction in terms of la-
bel assignment.
</bodyText>
<sectionHeader confidence="0.998285" genericHeader="introduction">
2 Model
</sectionHeader>
<subsectionHeader confidence="0.99403">
2.1 HUCRF definition
</subsectionHeader>
<bodyText confidence="0.99715">
A HUCRF incorporates a layer of binary-valued
hidden units z = z1 ... zn E 10, 1} for each pair
of observation sequence x = x1 ... xn and label
sequence y = y1 ... yn. It is parameterized by
</bodyText>
<figureCaption confidence="0.940293">
Figure 2: Illustration of a pre-training scheme for
HUCRFs.
</figureCaption>
<bodyText confidence="0.68476">
θ E Rd and γ E Rd&apos; and defines a joint probability
of y and z conditioned on x as follows:
</bodyText>
<equation confidence="0.998594">
pθ,γ(y, z|x) =
exp(θ&gt;Φ(x, z) + γ&gt;Ψ(z, y))
E z&apos;∈{0,1}n exp(θ&gt;Φ(x, z0) + γ&gt;Ψ(z0, y0))
y&apos;∈Y(x,z&apos;)
</equation>
<bodyText confidence="0.99853575">
where Y(x, z) is the set of all possible label
sequences for x and z, and Φ(x, z) E Rd
and Ψ(z, y) E Rd&apos; are global feature func-
tions that decompose into local feature
</bodyText>
<equation confidence="0.6387615">
functions: Φ(x, z) = En j=1 φ(x, j, zj) and
Ψ(z, y) = Enj=1 ψ(zj, yj−1, yj).
</equation>
<bodyText confidence="0.9998088">
HUCRF forces the interaction between the ob-
servations and the labels at each position j to go
through a latent variable zj: see Figure 1 for illus-
tration. Then the probability of labels y is given
by marginalizing over the hidden units,
</bodyText>
<equation confidence="0.989946">
pθ,γ(y|x) = � pθ,γ(y, z|x)
z∈{0,1}n
</equation>
<bodyText confidence="0.999972857142857">
As in restricted Boltzmann machines (Larochelle
and Bengio, 2008), hidden units are conditionally
independent given observations and labels. This
allows for efficient inference with HUCRFs de-
spite their richness (see Maaten et al. (2011) for
details). We use a perceptron-style algorithm of
Maaten et al. (2011) for training HUCRFs.
</bodyText>
<subsectionHeader confidence="0.998776">
2.2 Pre-training HUCRFs
</subsectionHeader>
<bodyText confidence="0.9999605">
How parameters are initialized for training is im-
portant for HUCRFs because the objective func-
tion is non-convex. Instead of random initializa-
tion, we use a simple and effective initialization
scheme (in a similar spirit to the pre-training meth-
ods in neural networks) that can leverage a large
</bodyText>
<page confidence="0.995662">
193
</page>
<bodyText confidence="0.999469571428571">
body of unlabeled data. This scheme is a simple
two-step approach.
In the first step, we cluster observed tokens in
M unlabeled sequences and treat the clusters as la-
bels to train an intermediate HUCRF. Let C(u(i))
be the “cluster sequence” of the i-th unlabeled se-
quence u(i). We compute:
</bodyText>
<equation confidence="0.719900714285714">
log pθ,γ(C(u(i))|u(i)))
In the second step, we train a final model on the
labeled data {(x(i), y(i))}Ni=1 using 01 as an ini-
tialization point:
(02,γ2) N arg max
θ,γ:
init(θ,θ1)
</equation>
<bodyText confidence="0.999887333333333">
While we can use γ1 for initialization as well, we
choose to only use 01 since the label space is task-
specific. This process is illustrated in Figure 2.
In summary, the first step is used to find
generic parameters between observations and hid-
den states; the second step is used to specialize the
parameters to a particular task. Note that the first
step also generates additional feature types absent
in the labeled data which can be useful at test time.
</bodyText>
<sectionHeader confidence="0.981452" genericHeader="method">
3 Multi-Sense Clustering via CCA
</sectionHeader>
<bodyText confidence="0.999648818181818">
The proposed pre-training method requires assign-
ing a cluster to each word in unlabeled text. Since
it learns to associate the words to their clusters, the
quality of clusters becomes important. A straight-
forward approach would be to perform Brown
clustering (Brown et al., 1992), which has been
very effective in a variety of NLP tasks (Miller et
al., 2004; Koo et al., 2008).
However, Brown clustering has some undesir-
able aspects for our purpose. First, it assigns a
single cluster to each word type. Thus a word that
can be used very differently depending on its con-
text (e.g., “bank”) is treated the same across the
corpus. Second, the Brown model uses only un-
igram and bigram statistics; this can be an issue
if we wish to capture semantics in larger contexts.
Finally, the algorithm is rather slow in practice for
large vocabulary size.
To mitigate these limitations, we propose multi-
sense clustering via canonical correlation analy-
sis (CCA). While there are previous work on in-
ducing multi-sense representations (Reisinger and
</bodyText>
<figureCaption confidence="0.7875775">
Figure 3: Algorithm for deriving CCA projections
from samples of two variables.
</figureCaption>
<bodyText confidence="0.998928333333333">
Mooney, 2010; Huang et al., 2012; Neelakantan et
al., 2014), our proposed method is simpler and is
shown to perform better in experiments.
</bodyText>
<subsectionHeader confidence="0.999775">
3.1 Review of CCA
</subsectionHeader>
<bodyText confidence="0.999987125">
CCA is a general technique that operates on a
pair of multi-dimensional variables. CCA finds
k dimensions (k is a parameter to be specified)
in which these variables are maximally correlated.
Let x(1) ... x(n) E Rd and y(1) ... y(n) E Rd&apos; be
n samples of the two variables. For simplicity, as-
sume that these variables have zero mean. Then
CCA computes the following for i = 1... k:
</bodyText>
<equation confidence="0.943891333333333">
Pnl=1(as x(l))(bi y(l))
qPnqPn
l=1(a� i x(l))2 l=1(b�i y(l))2
</equation>
<bodyText confidence="0.999727727272727">
In other words, each (ai, bi) is a pair of pro-
jection vectors such that the correlation between
the projected variables az x(l) and bz y(l) (now
scalars) is maximized, under the constraint that
this projection is uncorrelated with the previous
i − 1 projections. A method based on singu-
lar value decomposition (SVD) provides an effi-
cient and exact solution to this problem (Hotelling,
1936). The resulting solution A E Rdxk (whose
i-th column is ai) and B E Rd&apos;xk (whose i-th col-
umn is bi) can be used to project the variables from
</bodyText>
<figure confidence="0.523997709677419">
CCA-PROD
Input: samples (x(1), y(1)) ... (x(n), y(n)) E {0, 1}d x
{0, 1}d&apos;, dimension k
Output: projections A E Rdxk and B E Rd&apos;xk
• Calculate B E Rdxd&apos;, u E Rd, and v E Rd&apos;:
[[x(`) i= 1]][[y(`)
j = 1]]
n
Bi,j =
`=1
ui = n [[x(`) n [[y(`)
`=1 i = 1]] vi = `=1 i = 1]]
• Define Ωˆ = diag(u)−1/2Bdiag(v)−1/2.
• Calculate rank-k SVD ˆΩ. Let U E Rdxk (V E Rd&apos;xk)
be a matrix of the left (right) singular vector corre-
sponding to the largest k singular values.
• Let A = diag(u)−1/2U and B = diag(v)−1/2V .
(01, γ1) N arg max
θ,γ
XM
i=1
XN log pθ,γ(y(i)|x(i))
i=1
arg max
aiERd, biERd&apos;:
ai ai&apos;=0 bi&apos;&lt;i
bi bi&apos;=0 bi&apos;&lt;i
194
Input: word-context pairs from a corpus of length n:
D = {(w(l), c(l))}nl=1, dimension k
Output: cluster C(l) ≤ k for l = 1 ... n
</figure>
<listItem confidence="0.996616181818182">
• Use the algorithm in Figure 3 to compute projection
matrices (11W, 11C) = CCA-PROJ(D, k).
• For each word type w, perform k-means clustering on
Cw = {11TCc(l) ∈ Rk : w(l) = w} to partition occur-
rences of w in the corpus into at most k clusters.
• Label each word w(l) with the cluster obtained from
the previous step. Let D¯ = {( ¯w(l), ¯c(l))}nl=1 denote
this new dataset.
• (11 W¯, 11 ¯C) = CCA-PROJ( D, k)
• Perform k-means clustering on {11TW¯ ¯w(l) ∈ Rk}.
• Let C(l) be the cluster corresponding to PiTW¯v(l).
</listItem>
<figureCaption confidence="0.9834495">
Figure 4: Algorithm for clustering of words in a
corpus sensitive to multiple word senses.
</figureCaption>
<bodyText confidence="0.819287">
the original d- and d&apos;-dimensional spaces to a k-
dimensional space:
</bodyText>
<equation confidence="0.5850275">
x E Rd � ATx E Rk
yERd&apos;�BTyERk
</equation>
<bodyText confidence="0.999866545454545">
The new k-dimensional representation of each
variable now contains information about the other
variable. The value of k is usually selected to be
much smaller than d or d&apos;, so the representation
is typically also low-dimensional. The CCA algo-
rithm is given in Figure 3: we assume that samples
are 0-1 indicator vectors. In practice, calculating
the CCA projections is fast since there are many
efficient SVD implantations available. Also, CCA
can incorporate arbitrary context definitions unlike
the Brown algorithm.
</bodyText>
<subsectionHeader confidence="0.998891">
3.2 Multi-sense clustering
</subsectionHeader>
<bodyText confidence="0.999990551724138">
CCA projections can be used to obtain vector
representations for both words and contexts. If
we wished for only single-sense clusters (akin
to Brown clusters), we could simply perform k-
means on word embeddings.
However, we can exploit context embeddings to
infer word senses. For each word type, we create
a set of context embeddings corresponding to all
occurrences of that word type. Then we cluster
these embeddings; we use an implementation of
k-means which automatically determines the num-
ber of clusters upper bounded by k. The number
of word senses, k, is set to be the number of la-
bel types occurring in labeled data (for each task-
specific training set).
We use the resulting context clusters to deter-
mine the sense of each occurrence of that word
type. For instance, an occurrence of “bank” might
be labeled as “bank1” near “financial” or “Chase”
and “bank2” near “shore” or “edge”.
This step is for disambiguating word senses, but
what we need for our pre-training method is the
partition of words in the corpus. Thus we perform
a second round of CCA on these disambiguated
words to obtain corresponding word embeddings.
As a final step, we perform k-means clustering on
the disambiguated word embeddings to obtain the
partition of words in the corpus. The algorithm is
shown in Table 4.
</bodyText>
<sectionHeader confidence="0.999845" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999950571428571">
To validate the effectiveness of our pre-training
method, we experiment on three sequence label-
ing tasks: semantic tagging, named entity recogni-
tion (NER), and part-of-speech (POS) tagging. We
used L-BFGS for training CRFs 1 and the averaged
perceptron for training HUCRFs. The number of
hidden variables was set to 500.
</bodyText>
<subsectionHeader confidence="0.99898">
4.1 Semantic tagging
</subsectionHeader>
<bodyText confidence="0.984182571428571">
The goal of semantic tagging is to assign the cor-
rect semantic tag to a words in a given utter-
ance. We use a training set of 50-100k queries
across domains and the test set of 5-10k queries.
For pre-training, we collected 100-200k unlabeled
text from search log data and performed a stan-
dard preprocessing step. We use n-gram features
up to n = 3, regular expression features, do-
main specific lexicon features and Brown clus-
ters. We present the results for various config-
urations in Table 1. HUCRF with random ini-
tialization from Gaussian distribution (HUCRFG)
boosts the average performance up to 90.52%
(from 90.39% of CRF). HUCRF with pre-training
with Brown clusters (HUCRFB) and CCA-based
clusters (HUCRFC) further improves performance
to 91.36% and 91.37%, respectively.
Finally, when we use multi-sense cluster
(HUCRFC+), we obtain an F1-score of 92.01%.
We also compare other alternative pre-training
methods. HUCRF with pre-training RBM
</bodyText>
<footnote confidence="0.990187">
1For CRFs, we found that L-BFGS had higher perfor-
mance than SGD and the average percetpron.
</footnote>
<page confidence="0.993765">
195
</page>
<table confidence="0.999443666666667">
alarm calendar comm. note ondevice places reminder weather home avg
CRF 92.8 89.59 92.13 88.02 88.21 89.64 87.72 96.93 88.51 90.39
HUCRFG 91.79 89.56 92.08 88.42 88.64 90.99 89.21 96.38 87.63 90.52
HUCRFR 91.64 89.6 91.77 88.64 87.43 88.54 88.83 95.88 88.17 90.06
HUCRFB 92.86 90.58 92.8 88.72 89.37 91.14 90.05 97.63 89.08 91.36
HUCRFC 92.82 90.61 92.84 88.69 88.94 91.45 90.31 97.62 89.04 91.37
HUCRFS 91.2 90.53 92.43 88.7 88.09 90.91 89.54 97.24 88.91 90.84
HUCRFNS 90.8 89.88 91.54 87.83 88.15 91.02 88.2 96.77 89.02 90.36
HUCRFC+ 92.86 91.94 93.72 89.18 89.97 93.22 91.51 97.95 89.66 92.22
</table>
<tableCaption confidence="0.998277">
Table 1: Comparison of slot F1 scores on nine personal assistant domains. The numbers in boldface
</tableCaption>
<bodyText confidence="0.924292666666667">
are the best performing method. Subscripts mean the following: G = random initialization from a
Gaussian distribution with variance 10−4, R = pre-training with Restricted Boltzmann Machine (RBM)
using contrastive divergence of (Hinton, 2002), C = pre-training with CCA-based clusters, B = pre-
training with Brown clusters, S = pre-training with skip-ngram multi-sense clusters with fixed cluster
size 5, NS = pre-training with non-parametric skip-ngram multi-sense clusters, C+ = pre-training with
CCA-based multi-sense clusters.
(HUCRFR) does not perform better than with
random initialization. The skip-gram clusters
(HUCRFS, HUCRFSN) do not perform well ei-
ther. Some examples of disambiguated word oc-
currences are shown below, demonstrating that the
algorithm in Figure 3 yields intuitive clusters.
</bodyText>
<table confidence="0.9996399">
NER POS
Test-A Test-B Test-A Test-B
CRF 90.75 86.37 95.51 94.99
HUCRFG 89.99 86.72 95.14 95.08
HUCRFR 90.12 86.43 95.42 94.14
HUCRFB 90.27 87.24 95.55 95.33
HUCRFC 90.9 86.89 95.67 95.23
HUCRFS 90.18 86.84 95.48 95.07
HUCRFNS 90.14 85.66 95.35 94.82
HUCRFC+ 92.04 88.41 95.88 95.48
</table>
<tableCaption confidence="0.9399535">
Table 2: F1 Score for NER task and Accuracy for
POS task.
</tableCaption>
<table confidence="0.504355">
word context
Book a book(1) store within 5 miles of my address
find comic book(1) stores in novi michigan
book(2) restaurant for tomorrow
book(2) taxi to pizza hut
look for book(3) chang dong tofu house in pocono
find book(3) bindery seattle
High restaurant nearby with high(1) ratings
show me high(1) credit restaurant nearby
the address for shelley high(2) school
directions to leota junior high(2) school
what’s the distance to kilburn high(3) road
domino’s pizza in high(3) ridge missouri
</table>
<tableCaption confidence="0.929472">
Table 3: Examples of disambiguated word occur-
rences.
</tableCaption>
<subsectionHeader confidence="0.913966">
4.2 NER &amp; POS tagging
</subsectionHeader>
<bodyText confidence="0.999961333333333">
We use CoNLL 2003 dataset for NER and POS
with the standard train/dev/test split. For pre-
training, we used the Reuters-RCV1 corpus. It
contains 205 millions tokens with 1.6 million
types. We follow same preprocessing steps as in
semantic tagging. Also, we use the NER features
used in Turian et al. (2010) and POS features used
in Maaten et al. (2011).
We present the results for both tasks in Table 2.
In both tasks, the HUCRFC+ yields the best per-
formance, achieving error reduction of 20% (Test-
A) and 13% (Test-B) for NER as well as 15%
(Test-A) and 8% (Test-B) for POS over HUCRFR.
Note that HUCRF does not always perform bet-
ter than CRF when initialized randomly. How-
ever, However, HUCRF consistently outperforms
CRF with the pre-training methods proposed in
this work.
</bodyText>
<sectionHeader confidence="0.996463" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999934125">
We presented an effective technique for pre-
training HUCRFs. Our method transfers observa-
tion parameters trained on clustered text to initial-
ize the training process. We also proposed a word
clustering scheme based on CCA that is sensitive
to multiple word senses. Using our pre-training
method, we reported significant improvement over
several baselines in three sequence labeling tasks.
</bodyText>
<sectionHeader confidence="0.9872" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.849674166666667">
Tasos Anastasakos, Young-Bum Kim, and Anoop Deo-
ras. 2014. Task specific continuous word represen-
tations for mono and multi-lingual spoken language
understanding. In ICASSP, pages 3246–3250. IEEE.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
</reference>
<page confidence="0.997465">
196
</page>
<reference confidence="0.99831120754717">
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
Asli Celikyilmaz, Dilek Z Hakkani-T¨ur, G¨okhan T¨ur,
and Ruhi Sarikaya. 2013. Semi-supervised seman-
tic tagging of conversational understanding using
markov topic regression. In ACL, pages 914–923.
Association for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1–8.
Association for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In ICML,
pages 160–167. ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Anoop Deoras, Ruhi Sarikaya, G¨okhan T¨ur, and
Dilek Z Hakkani-T¨ur. 2012. Joint decoding for
speech recognition and semantic tagging. In INTER-
SPEECH.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In ACL, volume 1,
pages 1370–1380.
Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? The Journal of Machine Learn-
ing Research, 11:625–660.
Geoffrey Hinton, Simon Osindero, and Yee-Whye Teh.
2006. A fast learning algorithm for deep belief nets.
Neural computation, 18(7):1527–1554.
Geoffrey Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural com-
putation, 14(8):1771–1800.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321–377.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In ACL. Association for Compu-
tational Linguistics.
Young-Bum Kim and Benjamin Snyder. 2012. Univer-
sal grapheme-to-phoneme prediction over latin al-
phabets. In EMNLP, pages 332–343. Association
for Computational Linguistics.
Young-Bum Kim, Heemoon Chae, Benjamin Snyder,
and Yu-Seop Kim. 2014. Training a korean srl
system with rich morphological features. In ACL,
pages 637–642. Association for Computational Lin-
guistics.
Young-Bum Kim, Minwoo Jeong, Karl Stratos, and
Ruhi Sarikaya. 2015a. Weakly supervised slot
tagging with partially labeled sequences from web
search click logs. In HLT-NAACL, pages 84–92. As-
sociation for Computational Linguistics.
Young-Bum Kim, Karl Stratos, Xiaohu Liu, and Ruhi
Sarikaya. 2015b. Compact lexicon selection with
spectral methods. In ACL. Association for Compu-
tational Linguistics.
Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and
Minwoo Jeong. 2015c. New transfer learning tech-
niques for disparate label sets. In ACL. Association
for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML, pages 282–289.
Hugo Larochelle and Yoshua Bengio. 2008. Classifi-
cation using discriminative restricted boltzmann ma-
chines. In ICML.
Laurens van der Maaten, Max Welling, and
Lawrence K Saul. 2011. Hidden-unit condi-
tional random fields. In AISTAT.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional ran-
dom fields, feature induction and web-enhanced lex-
icons. In HLT-NAACL, pages 188–191. Association
for Computational Linguistics.
Gr´egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In INTERSPEECH,
pages 3771–3775.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL, volume 4, pages
337–342. Citeseer.
Abdel-rahman Mohamed, Dong Yu, and Li Deng.
2010. Investigation of full-sequence training of deep
belief networks for speech recognition. In INTER-
SPEECH, pages 2846–2849.
Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In EMNLP. Association for
Computational Linguistics.
</reference>
<page confidence="0.979838">
197
</page>
<reference confidence="0.999496975">
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109–117. Association for Computational Lin-
guistics.
Ruhi Sarikaya, Asli Celikyilmaz, Anoop Deoras, and
Minwoo Jeong. 2014. Shrinkage based features for
slot tagging with conditional random fields. In Proc.
of Interspeech.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 134–
141. Association for Computational Linguistics.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. Introduction to statistical relational learn-
ing, pages 93–128.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.
Mengqiu Wang and Christopher D Manning. 2013. Ef-
fect of non-linear deep architecture in sequence la-
beling. In ICML Workshop on Deep Learning for
Audio, Speech and Language Processing.
Puyang Xu and Ruhi Sarikaya. 2013. Convolutional
neural network based triangular crf for joint intent
detection and slot filling. In IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU), pages 78–83. IEEE.
Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang,
Yangyang Shi, and Dong Yu. 2013. Recurrent neu-
ral networks for language understanding. In INTER-
SPEECH, pages 2524–2528.
</reference>
<page confidence="0.997714">
198
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.542343">
<title confidence="0.99978">Pre-training of Hidden-Unit CRFs</title>
<author confidence="0.999981">Karl Ruhi</author>
<affiliation confidence="0.77043">Corporation, Redmond, University, New York,</affiliation>
<email confidence="0.999937">stratos@cs.columbia.edu</email>
<abstract confidence="0.999204181818182">In this paper, we apply the concept of pretraining to hidden-unit conditional random fields (HUCRFs) to enable learning on unlabeled data. We present a simple yet effective pre-training technique that learns to associate words with their clusters, which are obtained in an unsupervised manner. The learned parameters are then used to initialize the supervised learning process. We also propose a word clustering technique based on canonical correlation analysis (CCA) that is sensitive to multiple word senses, to further improve the accuracy within the proposed framework. We report consistent gains over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tasos Anastasakos</author>
<author>Young-Bum Kim</author>
<author>Anoop Deoras</author>
</authors>
<title>Task specific continuous word representations for mono and multi-lingual spoken language understanding.</title>
<date>2014</date>
<booktitle>In ICASSP,</booktitle>
<pages>3246--3250</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1836" citStr="Anastasakos et al., 2014" startWordPosition="279" endWordPosition="282">bert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep learning models. The best results obtained across a wide range of tasks involve unsupervised pre-training phase followed by the supervised training phase. The empiri</context>
</contexts>
<marker>Anastasakos, Kim, Deoras, 2014</marker>
<rawString>Tasos Anastasakos, Young-Bum Kim, and Anoop Deoras. 2014. Task specific continuous word representations for mono and multi-lingual spoken language understanding. In ICASSP, pages 3246–3250. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="8215" citStr="Brown et al., 1992" startWordPosition="1340" endWordPosition="1343"> 2. In summary, the first step is used to find generic parameters between observations and hidden states; the second step is used to specialize the parameters to a particular task. Note that the first step also generates additional feature types absent in the labeled data which can be useful at test time. 3 Multi-Sense Clustering via CCA The proposed pre-training method requires assigning a cluster to each word in unlabeled text. Since it learns to associate the words to their clusters, the quality of clusters becomes important. A straightforward approach would be to perform Brown clustering (Brown et al., 1992), which has been very effective in a variety of NLP tasks (Miller et al., 2004; Koo et al., 2008). However, Brown clustering has some undesirable aspects for our purpose. First, it assigns a single cluster to each word type. Thus a word that can be used very differently depending on its context (e.g., “bank”) is treated the same across the corpus. Second, the Brown model uses only unigram and bigram statistics; this can be an issue if we wish to capture semantics in larger contexts. Finally, the algorithm is rather slow in practice for large vocabulary size. To mitigate these limitations, we p</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Z Hakkani-T¨ur</author>
<author>G¨okhan T¨ur</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Semi-supervised semantic tagging of conversational understanding using markov topic regression.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>914--923</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Celikyilmaz, Hakkani-T¨ur, T¨ur, Sarikaya, 2013</marker>
<rawString>Asli Celikyilmaz, Dilek Z Hakkani-T¨ur, G¨okhan T¨ur, and Ruhi Sarikaya. 2013. Semi-supervised semantic tagging of conversational understanding using markov topic regression. In ACL, pages 914–923. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1672" citStr="Collins, 2002" startWordPosition="253" endWordPosition="254">hnique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep l</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In ICML,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1205" citStr="Collobert and Weston, 2008" startWordPosition="172" endWordPosition="175">earning process. We also propose a word clustering technique based on canonical correlation analysis (CCA) that is sensitive to multiple word senses, to further improve the accuracy within the proposed framework. We report consistent gains over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al.,</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1229" citStr="Collobert et al., 2011" startWordPosition="176" endWordPosition="179">pose a word clustering technique based on canonical correlation analysis (CCA) that is sensitive to multiple word senses, to further improve the accuracy within the proposed framework. We report consistent gains over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Deoras</author>
<author>Ruhi Sarikaya</author>
<author>G¨okhan T¨ur</author>
<author>Dilek Z Hakkani-T¨ur</author>
</authors>
<title>Joint decoding for speech recognition and semantic tagging.</title>
<date>2012</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Deoras, Sarikaya, T¨ur, Hakkani-T¨ur, 2012</marker>
<rawString>Anoop Deoras, Ruhi Sarikaya, G¨okhan T¨ur, and Dilek Z Hakkani-T¨ur. 2012. Joint decoding for speech recognition and semantic tagging. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<volume>1</volume>
<pages>1370--1380</pages>
<contexts>
<context position="1380" citStr="Devlin et al., 2014" startWordPosition="204" endWordPosition="207"> within the proposed framework. We report consistent gains over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over seq</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In ACL, volume 1, pages 1370–1380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dumitru Erhan</author>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pierre-Antoine Manzagol</author>
<author>Pascal Vincent</author>
<author>Samy Bengio</author>
</authors>
<title>Why does unsupervised pre-training help deep learning?</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--625</pages>
<contexts>
<context position="2468" citStr="Erhan et al., 2010" startWordPosition="378" endWordPosition="381"> 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep learning models. The best results obtained across a wide range of tasks involve unsupervised pre-training phase followed by the supervised training phase. The empirical results (Erhan et al., 2010) suggest that unsupervised pre-training has the regularization effect on the learning process and also results in a model parameter configuration that places the model near the basins of attraction of minima that support better generalization. While pre-training became a standard steps in many deep learning model training recipes, it has not been applied to the family of CRFs. There were several reasons for that; (i) the shallow and linear nature of basic CRF model topology, which limits their expressiveness to the inner product between data and model parameters, and (ii) Lack of a training cr</context>
</contexts>
<marker>Erhan, Bengio, Courville, Manzagol, Vincent, Bengio, 2010</marker>
<rawString>Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. 2010. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625–660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
<author>Simon Osindero</author>
<author>Yee-Whye Teh</author>
</authors>
<title>A fast learning algorithm for deep belief nets.</title>
<date>2006</date>
<booktitle>Neural computation,</booktitle>
<pages>18--7</pages>
<contexts>
<context position="2218" citStr="Hinton et al., 2006" startWordPosition="339" endWordPosition="342">ning, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep learning models. The best results obtained across a wide range of tasks involve unsupervised pre-training phase followed by the supervised training phase. The empirical results (Erhan et al., 2010) suggest that unsupervised pre-training has the regularization effect on the learning process and also results in a model parameter configuration that places the model near the basins of attraction of minima that support better generalization. While pre-training became a standard steps in many deep learning model training recipes, it has not been a</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>Geoffrey Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<booktitle>Neural computation,</booktitle>
<pages>14--8</pages>
<contexts>
<context position="15810" citStr="Hinton, 2002" startWordPosition="2645" endWordPosition="2646">8 91.36 HUCRFC 92.82 90.61 92.84 88.69 88.94 91.45 90.31 97.62 89.04 91.37 HUCRFS 91.2 90.53 92.43 88.7 88.09 90.91 89.54 97.24 88.91 90.84 HUCRFNS 90.8 89.88 91.54 87.83 88.15 91.02 88.2 96.77 89.02 90.36 HUCRFC+ 92.86 91.94 93.72 89.18 89.97 93.22 91.51 97.95 89.66 92.22 Table 1: Comparison of slot F1 scores on nine personal assistant domains. The numbers in boldface are the best performing method. Subscripts mean the following: G = random initialization from a Gaussian distribution with variance 10−4, R = pre-training with Restricted Boltzmann Machine (RBM) using contrastive divergence of (Hinton, 2002), C = pre-training with CCA-based clusters, B = pretraining with Brown clusters, S = pre-training with skip-ngram multi-sense clusters with fixed cluster size 5, NS = pre-training with non-parametric skip-ngram multi-sense clusters, C+ = pre-training with CCA-based multi-sense clusters. (HUCRFR) does not perform better than with random initialization. The skip-gram clusters (HUCRFS, HUCRFSN) do not perform well either. Some examples of disambiguated word occurrences are shown below, demonstrating that the algorithm in Figure 3 yields intuitive clusters. NER POS Test-A Test-B Test-A Test-B CRF </context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Geoffrey Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Hotelling</author>
</authors>
<title>Relations between two sets of variates.</title>
<date>1936</date>
<journal>Biometrika,</journal>
<pages>28--3</pages>
<contexts>
<context position="10042" citStr="Hotelling, 1936" startWordPosition="1653" endWordPosition="1654"> y(1) ... y(n) E Rd&apos; be n samples of the two variables. For simplicity, assume that these variables have zero mean. Then CCA computes the following for i = 1... k: Pnl=1(as x(l))(bi y(l)) qPnqPn l=1(a� i x(l))2 l=1(b�i y(l))2 In other words, each (ai, bi) is a pair of projection vectors such that the correlation between the projected variables az x(l) and bz y(l) (now scalars) is maximized, under the constraint that this projection is uncorrelated with the previous i − 1 projections. A method based on singular value decomposition (SVD) provides an efficient and exact solution to this problem (Hotelling, 1936). The resulting solution A E Rdxk (whose i-th column is ai) and B E Rd&apos;xk (whose i-th column is bi) can be used to project the variables from CCA-PROD Input: samples (x(1), y(1)) ... (x(n), y(n)) E {0, 1}d x {0, 1}d&apos;, dimension k Output: projections A E Rdxk and B E Rd&apos;xk • Calculate B E Rdxd&apos;, u E Rd, and v E Rd&apos;: [[x(`) i= 1]][[y(`) j = 1]] n Bi,j = `=1 ui = n [[x(`) n [[y(`) `=1 i = 1]] vi = `=1 i = 1]] • Define Ωˆ = diag(u)−1/2Bdiag(v)−1/2. • Calculate rank-k SVD ˆΩ. Let U E Rdxk (V E Rd&apos;xk) be a matrix of the left (right) singular vector corresponding to the largest k singular values. • L</context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>Harold Hotelling. 1936. Relations between two sets of variates. Biometrika, 28(3/4):321–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes. In ACL. Association for Computational Linguistics.</title>
<date>2012</date>
<contexts>
<context position="9083" citStr="Huang et al., 2012" startWordPosition="1485" endWordPosition="1488">sed very differently depending on its context (e.g., “bank”) is treated the same across the corpus. Second, the Brown model uses only unigram and bigram statistics; this can be an issue if we wish to capture semantics in larger contexts. Finally, the algorithm is rather slow in practice for large vocabulary size. To mitigate these limitations, we propose multisense clustering via canonical correlation analysis (CCA). While there are previous work on inducing multi-sense representations (Reisinger and Figure 3: Algorithm for deriving CCA projections from samples of two variables. Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014), our proposed method is simpler and is shown to perform better in experiments. 3.1 Review of CCA CCA is a general technique that operates on a pair of multi-dimensional variables. CCA finds k dimensions (k is a parameter to be specified) in which these variables are maximally correlated. Let x(1) ... x(n) E Rd and y(1) ... y(n) E Rd&apos; be n samples of the two variables. For simplicity, assume that these variables have zero mean. Then CCA computes the following for i = 1... k: Pnl=1(as x(l))(bi y(l)) qPnqPn l=1(a� i x(l))2 l=1(b�i y(l))2 In other words, each (ai, bi) i</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Benjamin Snyder</author>
</authors>
<title>Universal grapheme-to-phoneme prediction over latin alphabets. In</title>
<date>2012</date>
<booktitle>EMNLP,</booktitle>
<pages>332--343</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1761" citStr="Kim and Snyder, 2012" startWordPosition="267" endWordPosition="270">niques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep learning models. The best results obtained across a wide range of tasks involve unsupervis</context>
</contexts>
<marker>Kim, Snyder, 2012</marker>
<rawString>Young-Bum Kim and Benjamin Snyder. 2012. Universal grapheme-to-phoneme prediction over latin alphabets. In EMNLP, pages 332–343. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Heemoon Chae</author>
<author>Benjamin Snyder</author>
<author>Yu-Seop Kim</author>
</authors>
<title>Training a korean srl system with rich morphological features.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>637--642</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1854" citStr="Kim et al., 2014" startWordPosition="283" endWordPosition="286"> et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep learning models. The best results obtained across a wide range of tasks involve unsupervised pre-training phase followed by the supervised training phase. The empirical results (Erhan</context>
</contexts>
<marker>Kim, Chae, Snyder, Kim, 2014</marker>
<rawString>Young-Bum Kim, Heemoon Chae, Benjamin Snyder, and Yu-Seop Kim. 2014. Training a korean srl system with rich morphological features. In ACL, pages 637–642. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Minwoo Jeong</author>
<author>Karl Stratos</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Weakly supervised slot tagging with partially labeled sequences from web search click logs.</title>
<date>2015</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>84--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1872" citStr="Kim et al., 2015" startWordPosition="287" endWordPosition="290">ras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep learning models. The best results obtained across a wide range of tasks involve unsupervised pre-training phase followed by the supervised training phase. The empirical results (Erhan et al., 2010) sug</context>
</contexts>
<marker>Kim, Jeong, Stratos, Sarikaya, 2015</marker>
<rawString>Young-Bum Kim, Minwoo Jeong, Karl Stratos, and Ruhi Sarikaya. 2015a. Weakly supervised slot tagging with partially labeled sequences from web search click logs. In HLT-NAACL, pages 84–92. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Karl Stratos</author>
<author>Xiaohu Liu</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Compact lexicon selection with spectral methods.</title>
<date>2015</date>
<booktitle>In ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1872" citStr="Kim et al., 2015" startWordPosition="287" endWordPosition="290">ras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep learning models. The best results obtained across a wide range of tasks involve unsupervised pre-training phase followed by the supervised training phase. The empirical results (Erhan et al., 2010) sug</context>
</contexts>
<marker>Kim, Stratos, Liu, Sarikaya, 2015</marker>
<rawString>Young-Bum Kim, Karl Stratos, Xiaohu Liu, and Ruhi Sarikaya. 2015b. Compact lexicon selection with spectral methods. In ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Karl Stratos</author>
<author>Ruhi Sarikaya</author>
<author>Minwoo Jeong</author>
</authors>
<title>New transfer learning techniques for disparate label sets.</title>
<date>2015</date>
<booktitle>In ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1872" citStr="Kim et al., 2015" startWordPosition="287" endWordPosition="290">ras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep learning models. The best results obtained across a wide range of tasks involve unsupervised pre-training phase followed by the supervised training phase. The empirical results (Erhan et al., 2010) sug</context>
</contexts>
<marker>Kim, Stratos, Sarikaya, Jeong, 2015</marker>
<rawString>Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and Minwoo Jeong. 2015c. New transfer learning techniques for disparate label sets. In ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<date>2008</date>
<note>Simple semi-supervised dependency parsing.</note>
<contexts>
<context position="8312" citStr="Koo et al., 2008" startWordPosition="1359" endWordPosition="1362">tates; the second step is used to specialize the parameters to a particular task. Note that the first step also generates additional feature types absent in the labeled data which can be useful at test time. 3 Multi-Sense Clustering via CCA The proposed pre-training method requires assigning a cluster to each word in unlabeled text. Since it learns to associate the words to their clusters, the quality of clusters becomes important. A straightforward approach would be to perform Brown clustering (Brown et al., 1992), which has been very effective in a variety of NLP tasks (Miller et al., 2004; Koo et al., 2008). However, Brown clustering has some undesirable aspects for our purpose. First, it assigns a single cluster to each word type. Thus a word that can be used very differently depending on its context (e.g., “bank”) is treated the same across the corpus. Second, the Brown model uses only unigram and bigram statistics; this can be an issue if we wish to capture semantics in larger contexts. Finally, the algorithm is rather slow in practice for large vocabulary size. To mitigate these limitations, we propose multisense clustering via canonical correlation analysis (CCA). While there are previous w</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1437" citStr="Lafferty et al., 2001" startWordPosition="212" endWordPosition="215">ns over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Larochelle</author>
<author>Yoshua Bengio</author>
</authors>
<title>Classification using discriminative restricted boltzmann machines.</title>
<date>2008</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="6389" citStr="Larochelle and Bengio, 2008" startWordPosition="1037" endWordPosition="1040">1}n exp(θ&gt;Φ(x, z0) + γ&gt;Ψ(z0, y0)) y&apos;∈Y(x,z&apos;) where Y(x, z) is the set of all possible label sequences for x and z, and Φ(x, z) E Rd and Ψ(z, y) E Rd&apos; are global feature functions that decompose into local feature functions: Φ(x, z) = En j=1 φ(x, j, zj) and Ψ(z, y) = Enj=1 ψ(zj, yj−1, yj). HUCRF forces the interaction between the observations and the labels at each position j to go through a latent variable zj: see Figure 1 for illustration. Then the probability of labels y is given by marginalizing over the hidden units, pθ,γ(y|x) = � pθ,γ(y, z|x) z∈{0,1}n As in restricted Boltzmann machines (Larochelle and Bengio, 2008), hidden units are conditionally independent given observations and labels. This allows for efficient inference with HUCRFs despite their richness (see Maaten et al. (2011) for details). We use a perceptron-style algorithm of Maaten et al. (2011) for training HUCRFs. 2.2 Pre-training HUCRFs How parameters are initialized for training is important for HUCRFs because the objective function is non-convex. Instead of random initialization, we use a simple and effective initialization scheme (in a similar spirit to the pre-training methods in neural networks) that can leverage a large 193 body of u</context>
</contexts>
<marker>Larochelle, Bengio, 2008</marker>
<rawString>Hugo Larochelle and Yoshua Bengio. 2008. Classification using discriminative restricted boltzmann machines. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens van der Maaten</author>
<author>Max Welling</author>
<author>Lawrence K Saul</author>
</authors>
<title>Hidden-unit conditional random fields.</title>
<date>2011</date>
<booktitle>In AISTAT.</booktitle>
<marker>van der Maaten, Welling, Saul, 2011</marker>
<rawString>Laurens van der Maaten, Max Welling, and Lawrence K Saul. 2011. Hidden-unit conditional random fields. In AISTAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>188--191</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1695" citStr="McCallum and Li, 2003" startWordPosition="255" endWordPosition="258">duction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep learning models. The bes</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In HLT-NAACL, pages 188–191. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gr´egoire Mesnil</author>
<author>Xiaodong He</author>
<author>Li Deng</author>
<author>Yoshua Bengio</author>
</authors>
<title>Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding.</title>
<date>2013</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>3771--3775</pages>
<contexts>
<context position="1334" citStr="Mesnil et al., 2013" startWordPosition="196" endWordPosition="199"> word senses, to further improve the accuracy within the proposed framework. We report consistent gains over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective functi</context>
</contexts>
<marker>Mesnil, He, Deng, Bengio, 2013</marker>
<rawString>Gr´egoire Mesnil, Xiaodong He, Li Deng, and Yoshua Bengio. 2013. Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding. In INTERSPEECH, pages 3771–3775.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<volume>4</volume>
<pages>337--342</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="8293" citStr="Miller et al., 2004" startWordPosition="1355" endWordPosition="1358">rvations and hidden states; the second step is used to specialize the parameters to a particular task. Note that the first step also generates additional feature types absent in the labeled data which can be useful at test time. 3 Multi-Sense Clustering via CCA The proposed pre-training method requires assigning a cluster to each word in unlabeled text. Since it learns to associate the words to their clusters, the quality of clusters becomes important. A straightforward approach would be to perform Brown clustering (Brown et al., 1992), which has been very effective in a variety of NLP tasks (Miller et al., 2004; Koo et al., 2008). However, Brown clustering has some undesirable aspects for our purpose. First, it assigns a single cluster to each word type. Thus a word that can be used very differently depending on its context (e.g., “bank”) is treated the same across the corpus. Second, the Brown model uses only unigram and bigram statistics; this can be an issue if we wish to capture semantics in larger contexts. Finally, the algorithm is rather slow in practice for large vocabulary size. To mitigate these limitations, we propose multisense clustering via canonical correlation analysis (CCA). While t</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In HLT-NAACL, volume 4, pages 337–342. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdel-rahman Mohamed</author>
<author>Dong Yu</author>
<author>Li Deng</author>
</authors>
<title>Investigation of full-sequence training of deep belief networks for speech recognition. In</title>
<date>2010</date>
<booktitle>INTERSPEECH,</booktitle>
<pages>2846--2849</pages>
<contexts>
<context position="1251" citStr="Mohamed et al., 2010" startWordPosition="180" endWordPosition="183">echnique based on canonical correlation analysis (CCA) that is sensitive to multiple word senses, to further improve the accuracy within the proposed framework. We report consistent gains over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2</context>
</contexts>
<marker>Mohamed, Yu, Deng, 2010</marker>
<rawString>Abdel-rahman Mohamed, Dong Yu, and Li Deng. 2010. Investigation of full-sequence training of deep belief networks for speech recognition. In INTERSPEECH, pages 2846–2849.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Jeevan Shankar</author>
<author>Alexandre Passos</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient nonparametric estimation of multiple embeddings per word in vector space.</title>
<date>2014</date>
<booktitle>In EMNLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9110" citStr="Neelakantan et al., 2014" startWordPosition="1489" endWordPosition="1492"> depending on its context (e.g., “bank”) is treated the same across the corpus. Second, the Brown model uses only unigram and bigram statistics; this can be an issue if we wish to capture semantics in larger contexts. Finally, the algorithm is rather slow in practice for large vocabulary size. To mitigate these limitations, we propose multisense clustering via canonical correlation analysis (CCA). While there are previous work on inducing multi-sense representations (Reisinger and Figure 3: Algorithm for deriving CCA projections from samples of two variables. Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014), our proposed method is simpler and is shown to perform better in experiments. 3.1 Review of CCA CCA is a general technique that operates on a pair of multi-dimensional variables. CCA finds k dimensions (k is a parameter to be specified) in which these variables are maximally correlated. Let x(1) ... x(n) E Rd and y(1) ... y(n) E Rd&apos; be n samples of the two variables. For simplicity, assume that these variables have zero mean. Then CCA computes the following for i = 1... k: Pnl=1(as x(l))(bi y(l)) qPnqPn l=1(a� i x(l))2 l=1(b�i y(l))2 In other words, each (ai, bi) is a pair of projection vect</context>
</contexts>
<marker>Neelakantan, Shankar, Passos, McCallum, 2014</marker>
<rawString>Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient nonparametric estimation of multiple embeddings per word in vector space. In EMNLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>109--117</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 109–117. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruhi Sarikaya</author>
<author>Asli Celikyilmaz</author>
<author>Anoop Deoras</author>
<author>Minwoo Jeong</author>
</authors>
<title>Shrinkage based features for slot tagging with conditional random fields.</title>
<date>2014</date>
<booktitle>In Proc. of Interspeech.</booktitle>
<contexts>
<context position="1810" citStr="Sarikaya et al., 2014" startWordPosition="275" endWordPosition="278">and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep learning models. The best results obtained across a wide range of tasks involve unsupervised pre-training phase followed by the supervised </context>
</contexts>
<marker>Sarikaya, Celikyilmaz, Deoras, Jeong, 2014</marker>
<rawString>Ruhi Sarikaya, Asli Celikyilmaz, Anoop Deoras, and Minwoo Jeong. 2014. Shrinkage based features for slot tagging with conditional random fields. In Proc. of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>134--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1718" citStr="Sha and Pereira, 2003" startWordPosition="259" endWordPosition="262">ent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep learning models. The best results obtained acro</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 134– 141. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>An introduction to conditional random fields for relational learning. Introduction to statistical relational learning,</title>
<date>2006</date>
<pages>93--128</pages>
<contexts>
<context position="1465" citStr="Sutton and McCallum, 2006" startWordPosition="216" endWordPosition="220">ional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective </context>
</contexts>
<marker>Sutton, McCallum, 2006</marker>
<rawString>Charles Sutton and Andrew McCallum. 2006. An introduction to conditional random fields for relational learning. Introduction to statistical relational learning, pages 93–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1739" citStr="Turian et al., 2010" startWordPosition="263" endWordPosition="266">he deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training operates globally over sequence structures and can incorporate arbitrary features. Furthermore, this objective is convex and can be optimized relatively efficiently using dynamic programming. Pre-training has been widely used in deep learning (Hinton et al., 2006) and is one of the distinguishing advantages of deep learning models. The best results obtained across a wide range of ta</context>
<context position="17583" citStr="Turian et al. (2010)" startWordPosition="2924" endWordPosition="2927">rant nearby with high(1) ratings show me high(1) credit restaurant nearby the address for shelley high(2) school directions to leota junior high(2) school what’s the distance to kilburn high(3) road domino’s pizza in high(3) ridge missouri Table 3: Examples of disambiguated word occurrences. 4.2 NER &amp; POS tagging We use CoNLL 2003 dataset for NER and POS with the standard train/dev/test split. For pretraining, we used the Reuters-RCV1 corpus. It contains 205 millions tokens with 1.6 million types. We follow same preprocessing steps as in semantic tagging. Also, we use the NER features used in Turian et al. (2010) and POS features used in Maaten et al. (2011). We present the results for both tasks in Table 2. In both tasks, the HUCRFC+ yields the best performance, achieving error reduction of 20% (TestA) and 13% (Test-B) for NER as well as 15% (Test-A) and 8% (Test-B) for POS over HUCRFR. Note that HUCRF does not always perform better than CRF when initialized randomly. However, However, HUCRF consistently outperforms CRF with the pre-training methods proposed in this work. 5 Conclusion We presented an effective technique for pretraining HUCRFs. Our method transfers observation parameters trained on cl</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Effect of non-linear deep architecture in sequence labeling.</title>
<date>2013</date>
<booktitle>In ICML Workshop on Deep Learning for Audio, Speech and Language Processing.</booktitle>
<contexts>
<context position="1358" citStr="Wang and Manning, 2013" startWordPosition="200" endWordPosition="203">her improve the accuracy within the proposed framework. We report consistent gains over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). The objective function for CRF training oper</context>
</contexts>
<marker>Wang, Manning, 2013</marker>
<rawString>Mengqiu Wang and Christopher D Manning. 2013. Effect of non-linear deep architecture in sequence labeling. In ICML Workshop on Deep Learning for Audio, Speech and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Puyang Xu</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Convolutional neural network based triangular crf for joint intent detection and slot filling.</title>
<date>2013</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),</booktitle>
<pages>78--83</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1295" citStr="Xu and Sarikaya, 2013" startWordPosition="188" endWordPosition="191">lysis (CCA) that is sensitive to multiple word senses, to further improve the accuracy within the proposed framework. We report consistent gains over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; K</context>
</contexts>
<marker>Xu, Sarikaya, 2013</marker>
<rawString>Puyang Xu and Ruhi Sarikaya. 2013. Convolutional neural network based triangular crf for joint intent detection and slot filling. In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 78–83. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaisheng Yao</author>
<author>Geoffrey Zweig</author>
<author>Mei-Yuh Hwang</author>
<author>Yangyang Shi</author>
<author>Dong Yu</author>
</authors>
<title>Recurrent neural networks for language understanding. In</title>
<date>2013</date>
<booktitle>INTERSPEECH,</booktitle>
<pages>2524--2528</pages>
<contexts>
<context position="1313" citStr="Yao et al., 2013" startWordPosition="192" endWordPosition="195">sitive to multiple word senses, to further improve the accuracy within the proposed framework. We report consistent gains over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; Kim et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b).</context>
</contexts>
<marker>Yao, Zweig, Hwang, Shi, Yu, 2013</marker>
<rawString>Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang, Yangyang Shi, and Dong Yu. 2013. Recurrent neural networks for language understanding. In INTERSPEECH, pages 2524–2528.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>