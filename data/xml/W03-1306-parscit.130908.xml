<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008070">
<title confidence="0.996302">
Boosting Precision and Recall of Dictionary-Based Protein Name
Recognition
</title>
<author confidence="0.994912">
Yoshimasa Tsuruokatt and Jun’ichi Tsujiitt
</author>
<affiliation confidence="0.999703">
tDepartment of Computer Science, University of Tokyo
</affiliation>
<address confidence="0.854386333333333">
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan
$CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 Japan
</address>
<email confidence="0.999745">
{tsuruoka,tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.996675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999729">
Dictionary-based protein name recogni-
tion is the first step for practical infor-
mation extraction from biomedical doc-
uments because it provides ID informa-
tion of recognized terms unlike machine
learning based approaches. However, dic-
tionary based approaches have two se-
rious problems: (1) a large number of
false recognitions mainly caused by short
names. (2) low recall due to spelling vari-
ation. In this paper, we tackle the for-
mer problem by using a machine learning
method to filter out false positives. We
also present an approximate string search-
ing method to alleviate the latter prob-
lem. Experimental results using the GE-
NIA corpus show that the filtering using
a naive Bayes classifier greatly improves
precision with slight loss of recall, result-
ing in a much better F-score.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991069047619">
The rapid increase of machine readable biomedical
texts (e.g. MEDLINE) makes automatic information
extraction from those texts much more attractive.
Especially extracting information of protein-protein
interactions from MEDLINE abstracts is regarded as
one of the most important tasks today (Marcotte et
al., 2001; Thomas et al., 2000; Ono et al., 2001).
To extract information of proteins, one has to first
recognize protein names in a text. This kind of prob-
lem has been studied in the field of natural language
processing as named entity recognition tasks. Ohta
et al. (2002) provided the GENIA corpus, an an-
notated corpus of MEDLINE abstracts, which can
be used as a gold-standard for evaluating and train-
ing named entity recognition algorithms. There
are some research efforts using machine learning
techniques to recognize biological entities in texts
(Takeuchi and Collier, 2002; Kim and Tsujii, 2002;
Kazama et al., 2002).
One drawback of these machine learning based
approaches is that they do not provide identification
information of recognized terms. For the purpose
of information extraction of protein-protein interac-
tion, the ID information of recognized proteins, such
as GenBank 1 ID or SwissProt 2 ID, is indispensable
to integrate the extracted information with the data
in other information sources.
Dictionary-based approaches, on the other hand,
intrinsically provide ID information because they
recognize a term by searching the most similar
(or identical) one in the dictionary to the target
term. This advantage currently makes dictionary-
based approaches particularly useful as the first step
for practical information extraction from biomedical
documents (Ono et al., 2001).
However, dictionary-based approaches have two
serious problems. One is a large number of false
positives mainly caused by short names, which sig-
nificantly degrade overall precision. Although this
problem can be avoided by excluding short names
from the dictionary, such a solution makes it impos-
sible to recognize short protein names. We tackle
</bodyText>
<footnote confidence="0.9999785">
1GenBank is one of the largest genetic sequence databases.
2The Swiss-Prot is an annotated protein sequence database.
</footnote>
<bodyText confidence="0.999908090909091">
this problem by using a machine learning technique.
Each recognized candidate is checked if it is really
protein name or not by a classifier trained on an an-
notated corpus.
The other problem of dictionary based approaches
is spelling variation. For example, the protein name
“NF-Kappa B” has many spelling variants such as
“NF Kappa B,” “NF kappa B,” “NF kappaB,” and
“NFkappaB.” Exact matching techniques, however,
regard these terms as completely different terms.
We alleviate this problem by using an approximate
string matching method in which surface-level sim-
ilarities between terms are considered.
This paper is organized as follows. Section 2
describes the overview of our method. Section 3
presents the approximate string searching algorithm
for candidate recognition. Section 3 describes how
to filter out false recognitions by a machine learning
method. Section 5 presents the experimental results
using the GENIA corpus. Some related work is de-
scribed in Section 6. Finally, Section 7 offers some
concluding remarks.
</bodyText>
<sectionHeader confidence="0.711932" genericHeader="method">
2 Method Overview
</sectionHeader>
<bodyText confidence="0.999939">
Our protein name recognition method consists of
two phases. In the first phase, we scan the text for
protein name candidates using a dictionary. In the
second phase, we check each candidate whether it is
really protein name or not using a machine learning
method. We call these two phases recognition phase
and filtering phase respectively. The overview of the
method is given below.
</bodyText>
<listItem confidence="0.633096666666667">
• Recognition phase
Protein name candidates are identified using a
protein name dictionary. To alleviate the prob-
lem of spelling variation, we use an approxi-
mate string matching technique.
• Filtering phase
</listItem>
<bodyText confidence="0.998377875">
Every protein name candidates is classified into
“accepted” or “rejected” by a classifier. The
classifier uses the context of the term and the
term itself as the features for the classification.
Only “accepted” candidates are recognized as
protein names.
In the following sections, we describe the details
of each phase.
</bodyText>
<table confidence="0.819058857142857">
G R - 2
0 1 2 3 4
E 1 1 2 3 4
G 2 1 2 3 4
R 3 2 1 2 3
- 4 3 2 1 2
1 5 4 3 2 2
</table>
<figureCaption confidence="0.981149">
Figure 1: Dynamic Programming Matrix
</figureCaption>
<sectionHeader confidence="0.983295" genericHeader="method">
3 Candidate Recognition
</sectionHeader>
<bodyText confidence="0.993364142857143">
The most straightforward way to exploit a dictio-
nary for candidate recognition is the exact (longest)
match algorithm. For exact match, many fast match-
ing algorithms (e.g. Boyer-Moore algorithm (1977))
have been proposed. However, the existence of
many spelling variations for the same protein name
makes the exact matching less attractive. For exam-
ple, even a short protein name “EGR-1” has at least
the six following variations:
EGR-1, EGR 1, Egr-1, Egr 1, egr-1, egr 1.
Since longer protein names have a huge number
of possible variations, it is impossible to enrich the
dictionary by expanding each protein name as de-
scribed above.
</bodyText>
<subsectionHeader confidence="0.999743">
3.1 Approximate String Searching
</subsectionHeader>
<bodyText confidence="0.99963995">
To deal with the problem of spelling variation, we
need a kind of ‘elastic’ matching algorithm, by
which a recognition system scan a text to find a sim-
ilar term to (if any) a protein name in the dictio-
nary. We need a similarity measure to do such a task.
The most popular measure of similarity between
two strings is edit distance, which is the minimum
number of operations on individual characters (e.g.
substitutions, insertions, and deletions) required to
transform one string of symbols into another. For ex-
ample, the edit distance between “EGR-1” and “GR-
2” is two, because one substitution (1 for 2) and one
deletion (E) are required.
To calculate the edit distance between two strings,
we can use a dynamic programming technique. Fig-
ure 1 illustrates an example. For clarity of presen-
tation, all costs are assumed to be 1. The matrix
C0..|x|,0..|y |is filled, where Ci,j represents the mini-
mum number of operations needed to match x1..i to
y1..j. This is computed as follows (Navarro, 1998)
</bodyText>
<equation confidence="0.99622375">
Ci,0 = i (1)
C0,j = j
Ci,j = if (xi = yj) then Ci−1,j−1
else 1 + min(Ci−1,j, Ci,j−1, Ci−1,j−1)
</equation>
<bodyText confidence="0.999947833333333">
The calculation can be done by either a row-
wise left-to-right traversal or a column-wise top-to-
bottom traversal.
There are many fast algorithms other than the dy-
namic programming for uniform-cost edit distance,
where the weight of each edit operation is constant
within the same type (Navarro, 2001). However,
what we expect is that the distance between “EGR-
1” and “EGR 1” will be smaller than that between
“EGR-1” and “FGR-1”, while the uniform-cost edit
distances of them are equal.
The dynamic programming based method is flex-
ible enough to allow us to define arbitrary costs for
individual operations depending on a letter being op-
erated. For example, we can make the cost of the
substitution between a space and a hyphen much
lower than that of the substitution between ‘E’ and
‘F.’ Therefore, we use the dynamic programming
based method for our task.
Table 1 shows the cost function used in our ex-
periments. Both insertion and deletion costs are 100
except for spaces and hyphens. Substitution costs
for similar letters are 10. Substitution costs for the
other different letters are 50.
</bodyText>
<subsectionHeader confidence="0.999962">
3.2 String Searching
</subsectionHeader>
<bodyText confidence="0.999662416666667">
We have described a method for calculating the
similarity between two strings in the previous sec-
tion. However, what we need is approximate string
searching in which the recognizer scans a text to
find a similar term to (if any) a term in the dictio-
nary. The dynamic programming based method can
be easily extended for approximate string searching.
The method is illustrated in Figure 2. The pro-
tein name to be matched is “EGR-1” and the text
to be scanned is “encoded by EGR include.” String
searching can be done by just setting the elements
corresponding separators (e.g. space) in the first row
</bodyText>
<tableCaption confidence="0.994036">
Table 1: Cost Function
</tableCaption>
<table confidence="0.992192142857143">
Operation Letter Cost
Insertion space or hyphen 10
Other letters 100
Deletion space or hyphen 10
Other letters 100
Substitution A letter for the same letter 0
A numeral for a numeral 10
space for hyphen 10
hyphen for space 10
A capital letter for the 10
corresponding small letter
A small letter for the 10
corresponding capital letter
Other letters 50
</table>
<bodyText confidence="0.999362714285714">
to zero. After filling the whole matrix, one can find
that “EGR-1” can be matched to this text at the place
of “EGR 1” with cost 1 by searching for the lowest
value in the bottom row.
To take into account the length of a term, we adopt
a normalized cost, which is calculated by dividing
the cost by the length of the term:
</bodyText>
<equation confidence="0.982062">
(cost) + �
(nomalized cost) = (4)
</equation>
<bodyText confidence="0.986505545454545">
(length of the term)
where a is a constant value 3. When the costs of two
terms are the same, the longer one is preferred due
to this constant.
To recognize a protein name in a given text, we
perform the above calculation for every term con-
tained in the dictionary and select the term that has
the lowest normalized cost.
If the normalized cost is lower than the predefined
threshold. The corresponding range in the text is
recognized as a protein name candidate.
</bodyText>
<subsectionHeader confidence="0.995829">
3.3 Implementation Issues for String Searching
</subsectionHeader>
<bodyText confidence="0.9923655">
A naive way for string searching using a dictionary
is to conduct the procedure described in the previ-
ous section one by one for every term in the dictio-
nary. However, since the size of the dictionary is
very large, this naive method takes too much time to
perform a large scale experiment.
</bodyText>
<footnote confidence="0.956069">
3a was set to 0.4 in our experiments.
</footnote>
<table confidence="0.932486909090909">
e n c o d e d b y E G R 1 i n c l u d e
E 0 1 2 3 4 5 6 7 0 1 2 0 1 2 3 0 1 0 1 2 3 4 5 6 7
G
R
-
1
1 1 2 3 4 5 6 7 1 1 2 1 0 1 2 1 1 1 1 2 3 4 5 6 7
2 2 2 3 4 5 6 7 2 2 2 2 1 0 1 2 2 2 2 2 3 4 5 6 7
3 3 3 3 4 5 6 7 3 3 3 3 2 1 0 1 2 3 3 3 3 4 5 6 7
4 4 4 4 4 5 6 7 4 4 4 4 3 2 1 1 2 3 4 4 4 4 5 6 7
5 5 5 5 5 5 6 7 5 5 5 5 4 3 2 2 1 2 3 4 5 5 5 6 7
</table>
<figureCaption confidence="0.998768">
Figure 2: Example of String Searching using Dynamic Programming Matrix
</figureCaption>
<bodyText confidence="0.9999742">
Navarro (2001) have presented a way to reduce
redundant calculations by constructing a trie of the
dictionary. The trie is used as a device to avoid
repeating the computation of the cost against same
prefix of many patterns. Suppose that we have just
calculated the cost of the term “EGR-1” and next we
have to calculate the cost of the term “EGR-2,” it is
clear that we do not have to re-calculated the first
four rows in the matrix (see Figure 2). They also
pointed out that it is possible to determine, prior to
reaching the bottom of the matrix, that the current
term cannot produce any relevant match: if all the
values of the current row are larger than the thresh-
old, then a match cannot occur since we can only
increase the cost or at best keep it the same.
</bodyText>
<sectionHeader confidence="0.969051" genericHeader="method">
4 Filtering Candidates by a Naive Bayes
Classifier
</sectionHeader>
<bodyText confidence="0.999939238095238">
One of the serious problems of dictionary-based
recognition is a large number of false recognitions
mainly caused by short entries in the dictionary. For
example, the dictionary constructed from GenBank
contains an entry “NK.” However, the word “NK”
is frequently used as a part of the term “NK cells.”
In this case, “NK” is an abbreviation of “natural
killer” and is not a protein name. Therefore this en-
try makes a large number of false recognitions lead-
ing to low precision performance.
In the filtering phase, we use a classifier trained on
an annotated corpus to suppress such kind of false
recognition. The objective of this phase is to im-
prove precision without the loss of recall.
We conduct binary classification (“accept” or “re-
ject”) on each candidate. The candidates that are
classified into “rejected” are filtered out. In other
words, only the candidates that are classified into
“accepted” are recognized as protein names.
In this paper, we use a naive Bayes classifier for
this classification task.
</bodyText>
<subsectionHeader confidence="0.985794">
4.1 Naive Bayes classifier
</subsectionHeader>
<bodyText confidence="0.999831916666667">
The naive Bayes classifier is a simple but effective
classifier which has been used in numerous applica-
tions of information processing such as image recog-
nition, natural language processing and information
retrieval (Lewis, 1998; Escudero et al., 2000; Peder-
sen, 2000; Nigam and Ghani, 2000).
Here we briefly review the naive Bayes model.
Let x be a vector we want to classify, and ck be a
possible class. What we want to know is the prob-
ability that the vector x belongs to the class ck. We
first transform the probability P(ck|x) using Bayes’
rule,
</bodyText>
<equation confidence="0.887695">
P(ck|x) = P(ck) × Pp(I) (5)
�
</equation>
<bodyText confidence="0.98343175">
Class probability P(ck) can be estimated from train-
ing data. However, direct estimation of P(ck|x) is
impossible in most cases because of the sparseness
of training data.
By assuming the conditional independence
among the elements of a vector, P (x|ck) is
decomposed as follows,
d
</bodyText>
<equation confidence="0.960232">
P(x|ck) = H P(xj|ck), (6)
j=1
</equation>
<bodyText confidence="0.951915">
where xj is the jth element of vector x. Then Equa-
tion 5 becomes
</bodyText>
<equation confidence="0.999576">
P(ck|y) = P(ck) x Hdj= p(()j  |ck) (7)
</equation>
<bodyText confidence="0.9993895">
By this equation, we can calculate P(ck|x) and clas-
sify x� into the class with the highest P(ck|x).
There are some implementation variants of the
naive Bayes classifier depending on their event mod-
els (McCallum and Nigam, 1998). In this paper, we
adopt the multi-variate Bernoulli event model.
</bodyText>
<subsectionHeader confidence="0.591704">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.9997211">
As the input of the classifier, the features of the tar-
get must be represented in the form of a vector. We
use a binary feature vector which contains only the
values of 0 or 1 for each element.
In this paper, we use the local context surround-
ing a candidate term and the words contained in the
term as the features. We call the former contextual
features and the latter term features.
The features used in our experiments are given be-
low.
</bodyText>
<listItem confidence="0.99991625">
• Contextual Features
W_1 : the preceding word.
W+1 : the following word.
• Term Features
</listItem>
<bodyText confidence="0.96787775">
Wbegin : the first word of the term.
Wend : the last word of the term.
Wmiddle : the other words of the term without
positional information (bag-of-words).
Suppose the candidate term is “putative zinc fin-
ger protein, ” and the sentence is:
... encoding a putative zinc finger protein was
found to derepress beta- galactosidase ...
We obtain the following active features for this
example.
{W_1 a}, {W+1 was}, {Wbegin putative},
protein}, {Wmiddle zinc}, {Wmiddle finger}.
</bodyText>
<subsectionHeader confidence="0.994571">
4.3 Training
</subsectionHeader>
<bodyText confidence="0.999977428571428">
The training of the classifier is done with an anno-
tated corpus. We first scan the corpus for protein
name candidates by dictionary matching. If a recog-
nized candidate is annotated as a protein name, this
candidate and its context are used as a positive (“ac-
cepted”) example for training. Otherwise, it is used
as a negative (“rejected”) example.
</bodyText>
<sectionHeader confidence="0.999583" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.998173">
5.1 Corpus and Dictionary
</subsectionHeader>
<bodyText confidence="0.9999725">
We conducted experiments of protein name recogni-
tion using the GENIA corpus version 3.01 (Ohta et
al., 2002). The GENIA corpus is an annotated cor-
pus, which contains 2000 abstracts extracted from
MEDLINE database. These abstracts are selected
from the search results with MeSH terms Human,
Blood Cells, and Transcription Factors.
The biological entities in the corpus are annotated
according to the GENIA ontology. Although the
corpus has many categories such as protein, DNA,
RNA, cell line and tissue, we used only the protein
category. When a term was recursively annotated,
only the innermost (shortest) annotation was consid-
ered.
The test data was created by randomly selecting
200 abstracts from the corpus. The remaining 1800
abstracts were used as the training data. The protein
name dictionary was constructed from the training
data by gathering all the terms that were annotated
as proteins.
Each recognition was counted as correct if the
both boundaries of the recognized term exactly
matched the boundaries of an annotation in the cor-
pus.
</bodyText>
<subsectionHeader confidence="0.99838">
5.2 Improving Precision by Filtering
</subsectionHeader>
<bodyText confidence="0.999854714285714">
We first conducted experiments to evaluate how
much precision is improved by the filtering process.
In the recognition phase, the longest matching algo-
rithm was used for candidate recognition.
The results are shown in Table 2. F-measure is de-
fined as the harmonic mean for precision and recall
as follows:
</bodyText>
<equation confidence="0.871148">
2 x precision x recall (8)
precision + recall
{Wend F=
</equation>
<tableCaption confidence="0.993308">
Table 2: Precision Improvement by Filtering
</tableCaption>
<table confidence="0.999955">
Precision Recall F-measure
w/o filtering 48.6 70.7 57.6
with filtering 74.3 65.3 69.5
</table>
<tableCaption confidence="0.875941">
Table 3: Recall Improvement by Approximate
String Search
</tableCaption>
<table confidence="0.999876727272727">
Threshold Precision Recall F-measure
1.0 72.6 39.5 51.2
2.0 73.7 63.7 68.3
3.0 74.0 66.5 70.1
4.0 73.9 66.8 70.2
5.0 73.4 67.1 70.1
6.0 73.6 67.1 70.2
7.0 73.5 67.2 70.2
8.0 73.1 67.4 70.2
9.0 72.9 67.8 70.2
10.0 72.6 67.7 70.0
</table>
<bodyText confidence="0.999593090909091">
The first row shows the performances achieved
without filtering. In this case, all the candidates
identified in the recognition phase are regarded as
protein names. The second row shows the perfor-
mance achieved with filtering by the naive Bayes
classifier. In this case, only the candidates that are
classified into “accepted” are regarded as protein
names. Notice that the filtering significantly im-
proved the precision (from 48.6% to 74.3%) with
slight loss of the recall. The F-measure was also
greatly improved (from 57.6% to 69.5%).
</bodyText>
<subsectionHeader confidence="0.9914635">
5.3 Improving Recall by Approximate String
Search
</subsectionHeader>
<bodyText confidence="0.9999697">
We also conducted experiments to evaluate how
much we can further improve the recognition per-
formance by using the approximate string search-
ing method described in Section 3. Table 3 shows
the results. The leftmost columns show the thresh-
olds of the normalized costs for approximate string
searching. As the threshold increased, the preci-
sion degraded while the recall improved. The best
F-measure was 70.2%, which is better than that of
exact matching by 0.7% (see Table 2).
</bodyText>
<tableCaption confidence="0.998253">
Table 4: Performance using Different Feature Set
</tableCaption>
<table confidence="0.9997145">
Feature Set Precision Recall F-measure
Contextual 61.0 62.6 61.8
features
Term 71.3 67.9 69.5
features
All features 73.5 67.2 70.2
</table>
<subsectionHeader confidence="0.992998">
5.4 Efficacy of Contextual Features
</subsectionHeader>
<bodyText confidence="0.999970294117647">
The advantage of using a machine learning tech-
nique is that we can exploit the context of a candi-
date for deciding whether it is really protein name or
not. In order to evaluate the efficacy of contexts, we
conducted experiments using different feature sets.
The threshold of normalized cost was set to 7.0.
Table 4 shows the results. The first row shows the
performances achieved by using only contextual fea-
tures. The second row shows those achieved by us-
ing only term features. The performances achieved
by using both feature sets are shown in the third row.
The results indicate that candidate terms them-
selves are strong cues for classification. However,
the fact that the best performance was achieved
when both feature sets were used suggests that the
context of a candidate conveys useful information
about the semantic class of the candidate.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9999696875">
Kazama et al. (2002) reported an F-measure of
56.5% on the GENIA corpus (Version 1.1) using
Support Vector Machines. Collier et al. (2001)
reported an F-measure of 75.9% evaluated on 100
MEDLINE abstracts using a Hidden Markov Model.
These research efforts are machine learning based
and do not provide ID information of recognized
terms.
Krauthammer et al. (2000) proposed a dictionary-
based gene/protein name recognition method. They
used BLAST for approximate string matching by
mapping sequences of text characters into sequences
of nucleotides that can be processed by BLAST.
They achieved a recall of 78.8% and a precision of
71.1% by a partial match criterion, which is less
strict than our exact match criterion.
</bodyText>
<sectionHeader confidence="0.996441" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999746">
In this paper we propose a two-phase protein name
recognition method. In the first phase, we scan texts
for protein name candidates using a protein name
dictionary and an approximate string searching tech-
nique. In the second phase, we filter the candidates
using a machine learning technique.
Since our method is dictionary-based, it can pro-
vide ID information of recognized terms unlike ma-
chine learning based approaches. False recognition,
which is a common problem of dictionary-based ap-
proaches, is suppressed by a classifier trained on an
annotated corpus.
Experimental results using the GENIA corpus
show that the filtering using a naive Bayes classi-
fier greatly improves precision with slight loss of re-
call. We achieved an F-measure of 70.2% for protein
name recognition on the GENIA corpus.
The future direction of this research involves:
</bodyText>
<listItem confidence="0.941406">
• Use of state-of-the-art classifiers
</listItem>
<bodyText confidence="0.999931428571428">
We have used a naive Bayes classifier in our
experiments because it requires a small com-
putational resource and exhibits good perfor-
mance. There is a chance, however, to improve
performance by using state-of-the-art machine
learning techniques including maximum en-
tropy models and support vector machines.
</bodyText>
<listItem confidence="0.970862">
• Use of other elastic matching algorithms
</listItem>
<bodyText confidence="0.918837125">
We have restricted the computation of similar-
ity to edit distance. However, it is not uncom-
mon that the order of the words in a protein
name is altered, for example,
“beta-1 integrin”
“integrin beta-1”
The character-level edit distance cannot capture
this -kind of similarities.
</bodyText>
<sectionHeader confidence="0.998772" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999737569444444">
Robert S. Boyer and J. Strother Moore. 1977. A fast
string searching algorithm. Communications of the
ACM, 20(10):762–772.
Nigel Collier, Chikashi Nobata, and Junichi Tsujii. 2001.
Automatic acquisition and classification of molecular
biology terminology using a tagged corpus. Journal of
Terminology, 7(2):239–258.
G. Escudero, L. arquez, and G. Rigau. 2000. Naive bayes
and exemplar-based approaches to word sense disam-
biguation revisited. In Proceedings of the 14th Euro-
pean Conference on Artificial Intelligence.
Jun’ichi Kazama, Takaki Makino, Yoshihiro Ohta, and
Jun’ichi Tsujii. 2002. Tuning support vector machines
for biomedical named entity recognition. In Proceed-
ings of the ACL-02 Workshop on Natural Language
Processing in the Biomedical Domain.
Jin Dong Kim and Jun’ichi Tsujii. 2002. Corpus-based
approach to biological entity recognition. In Text Data
Mining SIG (ISMB2002).
Michael Krauthammer, Andrey Rzhetsky, Pavel Moro-
zov, and Carol Friedman. 2000. Using BLAST for
identifying gene and protein names in journal articles.
Gene, 259:245–252.
David D. Lewis. 1998. Naive Bayes at forty: The inde-
pendence assumption in information retrieval. In Pro-
ceedings of ECML-98, 10th European Conference on
Machine Learning, number 1398, pages 4–15.
Edward M. Marcotte, Ioannis Xenarios, and David Eisen-
berg. 2001. Mining literature for protein-protein inter-
actions. BIOINFORMATICS, 17(4):359–363.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive bayes text classifi-
cation. In AAAI-98 Workshop on Learning for Text
Categorization.
G. Navarro, R. Baeza-Yates, and J.M. Arcoverde. 2001.
Matchsimile: A flexible approximate matching tool for
personal names searching. In Proceedings of the XVI
Brazilian Symposium on Databases (SBBD’2001),
pages 228–242.
Gonzalo Navarro. 1998. Approximate Text Searching.
Ph.D. thesis, Dept. of Computer Science, Univ. of
Chile.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Computing Surveys, 33(1):31–
88.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the ef-
fectiveness and applicability of co-training. In CIKM,
pages 86–93.
Tomoko Ohta, Yuka Tateishi, Hideki Mima, and Jun’ichi
Tsujii. 2002. Genia corpus: an annotated research
abstract corpus in molecular biology domain. In Pro-
ceedings of the Human Language Technology Confer-
ence.
Toshihide Ono, Haretsugu Hishigaki, Akira Tanigami,
and Toshihisa Takagi. 2001. Automated extraction
of information on protein-protein interactions from the
biological literature. BIOINFORMATICS, 17(2):155–
161.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive bayesian classifiers for word sense
disambiguation. In Proceedings of the First Annual
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 63–69.
K. Takeuchi and N. Collier. 2002. Use of support vec-
tor machines in extended named entity recognition. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning 2002 (CoNLL-2002), pages 119–125.
James Thomas, David Milward, Christos Ouzounis,
Stephen Pulman, and Mark Carroll. 2000. Automatic
extraction of protein interactions from scientific ab-
stracts. In Proceedings of the Pacific Symposium on
Biocomputing (PSB2000), volume 5, pages 502–513.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.643009">
<title confidence="0.9992475">Boosting Precision and Recall of Dictionary-Based Protein Name Recognition</title>
<author confidence="0.990697">Jun’ichi</author>
<affiliation confidence="0.976806">of Computer Science, University of</affiliation>
<address confidence="0.79849">Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033</address>
<affiliation confidence="0.983565">JST (Japan Science and Technology</affiliation>
<address confidence="0.95323">Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012</address>
<abstract confidence="0.992983857142857">Dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches. However, dictionary based approaches have two serious problems: (1) a large number of false recognitions mainly caused by short names. (2) low recall due to spelling variation. In this paper, we tackle the former problem by using a machine learning method to filter out false positives. We also present an approximate string searching method to alleviate the latter problem. Experimental results using the GE- NIA corpus show that the filtering using a naive Bayes classifier greatly improves precision with slight loss of recall, resulting in a much better F-score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robert S Boyer</author>
<author>J Strother Moore</author>
</authors>
<title>A fast string searching algorithm.</title>
<date>1977</date>
<journal>Communications of the ACM,</journal>
<volume>20</volume>
<issue>10</issue>
<marker>Boyer, Moore, 1977</marker>
<rawString>Robert S. Boyer and J. Strother Moore. 1977. A fast string searching algorithm. Communications of the ACM, 20(10):762–772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Collier</author>
<author>Chikashi Nobata</author>
<author>Junichi Tsujii</author>
</authors>
<title>Automatic acquisition and classification of molecular biology terminology using a tagged corpus.</title>
<date>2001</date>
<journal>Journal of Terminology,</journal>
<volume>7</volume>
<issue>2</issue>
<contexts>
<context position="19616" citStr="Collier et al. (2001)" startWordPosition="3375" endWordPosition="3378">using only contextual features. The second row shows those achieved by using only term features. The performances achieved by using both feature sets are shown in the third row. The results indicate that candidate terms themselves are strong cues for classification. However, the fact that the best performance was achieved when both feature sets were used suggests that the context of a candidate conveys useful information about the semantic class of the candidate. 6 Related Work Kazama et al. (2002) reported an F-measure of 56.5% on the GENIA corpus (Version 1.1) using Support Vector Machines. Collier et al. (2001) reported an F-measure of 75.9% evaluated on 100 MEDLINE abstracts using a Hidden Markov Model. These research efforts are machine learning based and do not provide ID information of recognized terms. Krauthammer et al. (2000) proposed a dictionarybased gene/protein name recognition method. They used BLAST for approximate string matching by mapping sequences of text characters into sequences of nucleotides that can be processed by BLAST. They achieved a recall of 78.8% and a precision of 71.1% by a partial match criterion, which is less strict than our exact match criterion. 7 Conclusion In th</context>
</contexts>
<marker>Collier, Nobata, Tsujii, 2001</marker>
<rawString>Nigel Collier, Chikashi Nobata, and Junichi Tsujii. 2001. Automatic acquisition and classification of molecular biology terminology using a tagged corpus. Journal of Terminology, 7(2):239–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Escudero</author>
<author>L arquez</author>
<author>G Rigau</author>
</authors>
<title>Naive bayes and exemplar-based approaches to word sense disambiguation revisited.</title>
<date>2000</date>
<booktitle>In Proceedings of the 14th European Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="13009" citStr="Escudero et al., 2000" startWordPosition="2270" endWordPosition="2273"> loss of recall. We conduct binary classification (“accept” or “reject”) on each candidate. The candidates that are classified into “rejected” are filtered out. In other words, only the candidates that are classified into “accepted” are recognized as protein names. In this paper, we use a naive Bayes classifier for this classification task. 4.1 Naive Bayes classifier The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing and information retrieval (Lewis, 1998; Escudero et al., 2000; Pedersen, 2000; Nigam and Ghani, 2000). Here we briefly review the naive Bayes model. Let x be a vector we want to classify, and ck be a possible class. What we want to know is the probability that the vector x belongs to the class ck. We first transform the probability P(ck|x) using Bayes’ rule, P(ck|x) = P(ck) × Pp(I) (5) � Class probability P(ck) can be estimated from training data. However, direct estimation of P(ck|x) is impossible in most cases because of the sparseness of training data. By assuming the conditional independence among the elements of a vector, P (x|ck) is decompos</context>
</contexts>
<marker>Escudero, arquez, Rigau, 2000</marker>
<rawString>G. Escudero, L. arquez, and G. Rigau. 2000. Naive bayes and exemplar-based approaches to word sense disambiguation revisited. In Proceedings of the 14th European Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Takaki Makino</author>
<author>Yoshihiro Ohta</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Tuning support vector machines for biomedical named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Natural Language Processing in the Biomedical Domain.</booktitle>
<contexts>
<context position="2098" citStr="Kazama et al., 2002" startWordPosition="312" endWordPosition="315">2001; Thomas et al., 2000; Ono et al., 2001). To extract information of proteins, one has to first recognize protein names in a text. This kind of problem has been studied in the field of natural language processing as named entity recognition tasks. Ohta et al. (2002) provided the GENIA corpus, an annotated corpus of MEDLINE abstracts, which can be used as a gold-standard for evaluating and training named entity recognition algorithms. There are some research efforts using machine learning techniques to recognize biological entities in texts (Takeuchi and Collier, 2002; Kim and Tsujii, 2002; Kazama et al., 2002). One drawback of these machine learning based approaches is that they do not provide identification information of recognized terms. For the purpose of information extraction of protein-protein interaction, the ID information of recognized proteins, such as GenBank 1 ID or SwissProt 2 ID, is indispensable to integrate the extracted information with the data in other information sources. Dictionary-based approaches, on the other hand, intrinsically provide ID information because they recognize a term by searching the most similar (or identical) one in the dictionary to the target term. This ad</context>
<context position="19498" citStr="Kazama et al. (2002)" startWordPosition="3356" endWordPosition="3359">shold of normalized cost was set to 7.0. Table 4 shows the results. The first row shows the performances achieved by using only contextual features. The second row shows those achieved by using only term features. The performances achieved by using both feature sets are shown in the third row. The results indicate that candidate terms themselves are strong cues for classification. However, the fact that the best performance was achieved when both feature sets were used suggests that the context of a candidate conveys useful information about the semantic class of the candidate. 6 Related Work Kazama et al. (2002) reported an F-measure of 56.5% on the GENIA corpus (Version 1.1) using Support Vector Machines. Collier et al. (2001) reported an F-measure of 75.9% evaluated on 100 MEDLINE abstracts using a Hidden Markov Model. These research efforts are machine learning based and do not provide ID information of recognized terms. Krauthammer et al. (2000) proposed a dictionarybased gene/protein name recognition method. They used BLAST for approximate string matching by mapping sequences of text characters into sequences of nucleotides that can be processed by BLAST. They achieved a recall of 78.8% and a pr</context>
</contexts>
<marker>Kazama, Makino, Ohta, Tsujii, 2002</marker>
<rawString>Jun’ichi Kazama, Takaki Makino, Yoshihiro Ohta, and Jun’ichi Tsujii. 2002. Tuning support vector machines for biomedical named entity recognition. In Proceedings of the ACL-02 Workshop on Natural Language Processing in the Biomedical Domain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Dong Kim</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus-based approach to biological entity recognition.</title>
<date>2002</date>
<booktitle>In Text Data Mining SIG (ISMB2002).</booktitle>
<contexts>
<context position="2076" citStr="Kim and Tsujii, 2002" startWordPosition="308" endWordPosition="311">day (Marcotte et al., 2001; Thomas et al., 2000; Ono et al., 2001). To extract information of proteins, one has to first recognize protein names in a text. This kind of problem has been studied in the field of natural language processing as named entity recognition tasks. Ohta et al. (2002) provided the GENIA corpus, an annotated corpus of MEDLINE abstracts, which can be used as a gold-standard for evaluating and training named entity recognition algorithms. There are some research efforts using machine learning techniques to recognize biological entities in texts (Takeuchi and Collier, 2002; Kim and Tsujii, 2002; Kazama et al., 2002). One drawback of these machine learning based approaches is that they do not provide identification information of recognized terms. For the purpose of information extraction of protein-protein interaction, the ID information of recognized proteins, such as GenBank 1 ID or SwissProt 2 ID, is indispensable to integrate the extracted information with the data in other information sources. Dictionary-based approaches, on the other hand, intrinsically provide ID information because they recognize a term by searching the most similar (or identical) one in the dictionary to th</context>
</contexts>
<marker>Kim, Tsujii, 2002</marker>
<rawString>Jin Dong Kim and Jun’ichi Tsujii. 2002. Corpus-based approach to biological entity recognition. In Text Data Mining SIG (ISMB2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Krauthammer</author>
<author>Andrey Rzhetsky</author>
<author>Pavel Morozov</author>
<author>Carol Friedman</author>
</authors>
<title>Using BLAST for identifying gene and protein names in journal articles.</title>
<date>2000</date>
<journal>Gene,</journal>
<pages>259--245</pages>
<contexts>
<context position="19842" citStr="Krauthammer et al. (2000)" startWordPosition="3410" endWordPosition="3413">emselves are strong cues for classification. However, the fact that the best performance was achieved when both feature sets were used suggests that the context of a candidate conveys useful information about the semantic class of the candidate. 6 Related Work Kazama et al. (2002) reported an F-measure of 56.5% on the GENIA corpus (Version 1.1) using Support Vector Machines. Collier et al. (2001) reported an F-measure of 75.9% evaluated on 100 MEDLINE abstracts using a Hidden Markov Model. These research efforts are machine learning based and do not provide ID information of recognized terms. Krauthammer et al. (2000) proposed a dictionarybased gene/protein name recognition method. They used BLAST for approximate string matching by mapping sequences of text characters into sequences of nucleotides that can be processed by BLAST. They achieved a recall of 78.8% and a precision of 71.1% by a partial match criterion, which is less strict than our exact match criterion. 7 Conclusion In this paper we propose a two-phase protein name recognition method. In the first phase, we scan texts for protein name candidates using a protein name dictionary and an approximate string searching technique. In the second phase,</context>
</contexts>
<marker>Krauthammer, Rzhetsky, Morozov, Friedman, 2000</marker>
<rawString>Michael Krauthammer, Andrey Rzhetsky, Pavel Morozov, and Carol Friedman. 2000. Using BLAST for identifying gene and protein names in journal articles. Gene, 259:245–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Naive Bayes at forty: The independence assumption in information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of ECML-98, 10th European Conference on Machine Learning, number 1398,</booktitle>
<pages>4--15</pages>
<contexts>
<context position="12986" citStr="Lewis, 1998" startWordPosition="2268" endWordPosition="2269">n without the loss of recall. We conduct binary classification (“accept” or “reject”) on each candidate. The candidates that are classified into “rejected” are filtered out. In other words, only the candidates that are classified into “accepted” are recognized as protein names. In this paper, we use a naive Bayes classifier for this classification task. 4.1 Naive Bayes classifier The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing and information retrieval (Lewis, 1998; Escudero et al., 2000; Pedersen, 2000; Nigam and Ghani, 2000). Here we briefly review the naive Bayes model. Let x be a vector we want to classify, and ck be a possible class. What we want to know is the probability that the vector x belongs to the class ck. We first transform the probability P(ck|x) using Bayes’ rule, P(ck|x) = P(ck) × Pp(I) (5) � Class probability P(ck) can be estimated from training data. However, direct estimation of P(ck|x) is impossible in most cases because of the sparseness of training data. By assuming the conditional independence among the elements of a vector</context>
</contexts>
<marker>Lewis, 1998</marker>
<rawString>David D. Lewis. 1998. Naive Bayes at forty: The independence assumption in information retrieval. In Proceedings of ECML-98, 10th European Conference on Machine Learning, number 1398, pages 4–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward M Marcotte</author>
<author>Ioannis Xenarios</author>
<author>David Eisenberg</author>
</authors>
<title>Mining literature for protein-protein interactions.</title>
<date>2001</date>
<journal>BIOINFORMATICS,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="1482" citStr="Marcotte et al., 2001" startWordPosition="211" endWordPosition="214">lse positives. We also present an approximate string searching method to alleviate the latter problem. Experimental results using the GENIA corpus show that the filtering using a naive Bayes classifier greatly improves precision with slight loss of recall, resulting in a much better F-score. 1 Introduction The rapid increase of machine readable biomedical texts (e.g. MEDLINE) makes automatic information extraction from those texts much more attractive. Especially extracting information of protein-protein interactions from MEDLINE abstracts is regarded as one of the most important tasks today (Marcotte et al., 2001; Thomas et al., 2000; Ono et al., 2001). To extract information of proteins, one has to first recognize protein names in a text. This kind of problem has been studied in the field of natural language processing as named entity recognition tasks. Ohta et al. (2002) provided the GENIA corpus, an annotated corpus of MEDLINE abstracts, which can be used as a gold-standard for evaluating and training named entity recognition algorithms. There are some research efforts using machine learning techniques to recognize biological entities in texts (Takeuchi and Collier, 2002; Kim and Tsujii, 2002; Kaza</context>
</contexts>
<marker>Marcotte, Xenarios, Eisenberg, 2001</marker>
<rawString>Edward M. Marcotte, Ioannis Xenarios, and David Eisenberg. 2001. Mining literature for protein-protein interactions. BIOINFORMATICS, 17(4):359–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for naive bayes text classification.</title>
<date>1998</date>
<booktitle>In AAAI-98 Workshop on Learning for Text Categorization.</booktitle>
<contexts>
<context position="13987" citStr="McCallum and Nigam, 1998" startWordPosition="2445" endWordPosition="2448">(ck) can be estimated from training data. However, direct estimation of P(ck|x) is impossible in most cases because of the sparseness of training data. By assuming the conditional independence among the elements of a vector, P (x|ck) is decomposed as follows, d P(x|ck) = H P(xj|ck), (6) j=1 where xj is the jth element of vector x. Then Equation 5 becomes P(ck|y) = P(ck) x Hdj= p(()j |ck) (7) By this equation, we can calculate P(ck|x) and classify x� into the class with the highest P(ck|x). There are some implementation variants of the naive Bayes classifier depending on their event models (McCallum and Nigam, 1998). In this paper, we adopt the multi-variate Bernoulli event model. 4.2 Features As the input of the classifier, the features of the target must be represented in the form of a vector. We use a binary feature vector which contains only the values of 0 or 1 for each element. In this paper, we use the local context surrounding a candidate term and the words contained in the term as the features. We call the former contextual features and the latter term features. The features used in our experiments are given below. • Contextual Features W_1 : the preceding word. W+1 : the following word. • Term </context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for naive bayes text classification. In AAAI-98 Workshop on Learning for Text Categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Navarro</author>
<author>R Baeza-Yates</author>
<author>J M Arcoverde</author>
</authors>
<title>Matchsimile: A flexible approximate matching tool for personal names searching.</title>
<date>2001</date>
<booktitle>In Proceedings of the XVI Brazilian Symposium on Databases (SBBD’2001),</booktitle>
<pages>228--242</pages>
<marker>Navarro, Baeza-Yates, Arcoverde, 2001</marker>
<rawString>G. Navarro, R. Baeza-Yates, and J.M. Arcoverde. 2001. Matchsimile: A flexible approximate matching tool for personal names searching. In Proceedings of the XVI Brazilian Symposium on Databases (SBBD’2001), pages 228–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Navarro</author>
</authors>
<title>Approximate Text Searching.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Dept. of Computer Science, Univ. of Chile.</institution>
<contexts>
<context position="7096" citStr="Navarro, 1998" startWordPosition="1129" endWordPosition="1130">idual characters (e.g. substitutions, insertions, and deletions) required to transform one string of symbols into another. For example, the edit distance between “EGR-1” and “GR2” is two, because one substitution (1 for 2) and one deletion (E) are required. To calculate the edit distance between two strings, we can use a dynamic programming technique. Figure 1 illustrates an example. For clarity of presentation, all costs are assumed to be 1. The matrix C0..|x|,0..|y |is filled, where Ci,j represents the minimum number of operations needed to match x1..i to y1..j. This is computed as follows (Navarro, 1998) Ci,0 = i (1) C0,j = j Ci,j = if (xi = yj) then Ci−1,j−1 else 1 + min(Ci−1,j, Ci,j−1, Ci−1,j−1) The calculation can be done by either a rowwise left-to-right traversal or a column-wise top-tobottom traversal. There are many fast algorithms other than the dynamic programming for uniform-cost edit distance, where the weight of each edit operation is constant within the same type (Navarro, 2001). However, what we expect is that the distance between “EGR1” and “EGR 1” will be smaller than that between “EGR-1” and “FGR-1”, while the uniform-cost edit distances of them are equal. The dynamic program</context>
</contexts>
<marker>Navarro, 1998</marker>
<rawString>Gonzalo Navarro. 1998. Approximate Text Searching. Ph.D. thesis, Dept. of Computer Science, Univ. of Chile.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Navarro</author>
</authors>
<title>A guided tour to approximate string matching.</title>
<date>2001</date>
<journal>ACM Computing Surveys,</journal>
<volume>33</volume>
<issue>1</issue>
<pages>88</pages>
<contexts>
<context position="7491" citStr="Navarro, 2001" startWordPosition="1198" endWordPosition="1199">ity of presentation, all costs are assumed to be 1. The matrix C0..|x|,0..|y |is filled, where Ci,j represents the minimum number of operations needed to match x1..i to y1..j. This is computed as follows (Navarro, 1998) Ci,0 = i (1) C0,j = j Ci,j = if (xi = yj) then Ci−1,j−1 else 1 + min(Ci−1,j, Ci,j−1, Ci−1,j−1) The calculation can be done by either a rowwise left-to-right traversal or a column-wise top-tobottom traversal. There are many fast algorithms other than the dynamic programming for uniform-cost edit distance, where the weight of each edit operation is constant within the same type (Navarro, 2001). However, what we expect is that the distance between “EGR1” and “EGR 1” will be smaller than that between “EGR-1” and “FGR-1”, while the uniform-cost edit distances of them are equal. The dynamic programming based method is flexible enough to allow us to define arbitrary costs for individual operations depending on a letter being operated. For example, we can make the cost of the substitution between a space and a hyphen much lower than that of the substitution between ‘E’ and ‘F.’ Therefore, we use the dynamic programming based method for our task. Table 1 shows the cost function used in ou</context>
<context position="10913" citStr="Navarro (2001)" startWordPosition="1915" endWordPosition="1916">tionary. However, since the size of the dictionary is very large, this naive method takes too much time to perform a large scale experiment. 3a was set to 0.4 in our experiments. e n c o d e d b y E G R 1 i n c l u d e E 0 1 2 3 4 5 6 7 0 1 2 0 1 2 3 0 1 0 1 2 3 4 5 6 7 G R - 1 1 1 2 3 4 5 6 7 1 1 2 1 0 1 2 1 1 1 1 2 3 4 5 6 7 2 2 2 3 4 5 6 7 2 2 2 2 1 0 1 2 2 2 2 2 3 4 5 6 7 3 3 3 3 4 5 6 7 3 3 3 3 2 1 0 1 2 3 3 3 3 4 5 6 7 4 4 4 4 4 5 6 7 4 4 4 4 3 2 1 1 2 3 4 4 4 4 5 6 7 5 5 5 5 5 5 6 7 5 5 5 5 4 3 2 2 1 2 3 4 5 5 5 6 7 Figure 2: Example of String Searching using Dynamic Programming Matrix Navarro (2001) have presented a way to reduce redundant calculations by constructing a trie of the dictionary. The trie is used as a device to avoid repeating the computation of the cost against same prefix of many patterns. Suppose that we have just calculated the cost of the term “EGR-1” and next we have to calculate the cost of the term “EGR-2,” it is clear that we do not have to re-calculated the first four rows in the matrix (see Figure 2). They also pointed out that it is possible to determine, prior to reaching the bottom of the matrix, that the current term cannot produce any relevant match: if all </context>
</contexts>
<marker>Navarro, 2001</marker>
<rawString>Gonzalo Navarro. 2001. A guided tour to approximate string matching. ACM Computing Surveys, 33(1):31– 88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Rayid Ghani</author>
</authors>
<title>Analyzing the effectiveness and applicability of co-training.</title>
<date>2000</date>
<booktitle>In CIKM,</booktitle>
<pages>86--93</pages>
<contexts>
<context position="13049" citStr="Nigam and Ghani, 2000" startWordPosition="2277" endWordPosition="2280">sification (“accept” or “reject”) on each candidate. The candidates that are classified into “rejected” are filtered out. In other words, only the candidates that are classified into “accepted” are recognized as protein names. In this paper, we use a naive Bayes classifier for this classification task. 4.1 Naive Bayes classifier The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing and information retrieval (Lewis, 1998; Escudero et al., 2000; Pedersen, 2000; Nigam and Ghani, 2000). Here we briefly review the naive Bayes model. Let x be a vector we want to classify, and ck be a possible class. What we want to know is the probability that the vector x belongs to the class ck. We first transform the probability P(ck|x) using Bayes’ rule, P(ck|x) = P(ck) × Pp(I) (5) � Class probability P(ck) can be estimated from training data. However, direct estimation of P(ck|x) is impossible in most cases because of the sparseness of training data. By assuming the conditional independence among the elements of a vector, P (x|ck) is decomposed as follows, d P(x|ck) = H P(xj|ck), </context>
</contexts>
<marker>Nigam, Ghani, 2000</marker>
<rawString>Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In CIKM, pages 86–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoko Ohta</author>
<author>Yuka Tateishi</author>
<author>Hideki Mima</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Genia corpus: an annotated research abstract corpus in molecular biology domain.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference.</booktitle>
<contexts>
<context position="1747" citStr="Ohta et al. (2002)" startWordPosition="258" endWordPosition="261">n a much better F-score. 1 Introduction The rapid increase of machine readable biomedical texts (e.g. MEDLINE) makes automatic information extraction from those texts much more attractive. Especially extracting information of protein-protein interactions from MEDLINE abstracts is regarded as one of the most important tasks today (Marcotte et al., 2001; Thomas et al., 2000; Ono et al., 2001). To extract information of proteins, one has to first recognize protein names in a text. This kind of problem has been studied in the field of natural language processing as named entity recognition tasks. Ohta et al. (2002) provided the GENIA corpus, an annotated corpus of MEDLINE abstracts, which can be used as a gold-standard for evaluating and training named entity recognition algorithms. There are some research efforts using machine learning techniques to recognize biological entities in texts (Takeuchi and Collier, 2002; Kim and Tsujii, 2002; Kazama et al., 2002). One drawback of these machine learning based approaches is that they do not provide identification information of recognized terms. For the purpose of information extraction of protein-protein interaction, the ID information of recognized proteins</context>
<context position="15575" citStr="Ohta et al., 2002" startWordPosition="2718" endWordPosition="2721"> features for this example. {W_1 a}, {W+1 was}, {Wbegin putative}, protein}, {Wmiddle zinc}, {Wmiddle finger}. 4.3 Training The training of the classifier is done with an annotated corpus. We first scan the corpus for protein name candidates by dictionary matching. If a recognized candidate is annotated as a protein name, this candidate and its context are used as a positive (“accepted”) example for training. Otherwise, it is used as a negative (“rejected”) example. 5 Experiment 5.1 Corpus and Dictionary We conducted experiments of protein name recognition using the GENIA corpus version 3.01 (Ohta et al., 2002). The GENIA corpus is an annotated corpus, which contains 2000 abstracts extracted from MEDLINE database. These abstracts are selected from the search results with MeSH terms Human, Blood Cells, and Transcription Factors. The biological entities in the corpus are annotated according to the GENIA ontology. Although the corpus has many categories such as protein, DNA, RNA, cell line and tissue, we used only the protein category. When a term was recursively annotated, only the innermost (shortest) annotation was considered. The test data was created by randomly selecting 200 abstracts from the co</context>
</contexts>
<marker>Ohta, Tateishi, Mima, Tsujii, 2002</marker>
<rawString>Tomoko Ohta, Yuka Tateishi, Hideki Mima, and Jun’ichi Tsujii. 2002. Genia corpus: an annotated research abstract corpus in molecular biology domain. In Proceedings of the Human Language Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshihide Ono</author>
</authors>
<title>Haretsugu Hishigaki, Akira Tanigami, and Toshihisa Takagi.</title>
<date>2001</date>
<journal>BIOINFORMATICS,</journal>
<volume>17</volume>
<issue>2</issue>
<pages>161</pages>
<marker>Ono, 2001</marker>
<rawString>Toshihide Ono, Haretsugu Hishigaki, Akira Tanigami, and Toshihisa Takagi. 2001. Automated extraction of information on protein-protein interactions from the biological literature. BIOINFORMATICS, 17(2):155– 161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>63--69</pages>
<contexts>
<context position="13025" citStr="Pedersen, 2000" startWordPosition="2274" endWordPosition="2276">duct binary classification (“accept” or “reject”) on each candidate. The candidates that are classified into “rejected” are filtered out. In other words, only the candidates that are classified into “accepted” are recognized as protein names. In this paper, we use a naive Bayes classifier for this classification task. 4.1 Naive Bayes classifier The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing and information retrieval (Lewis, 1998; Escudero et al., 2000; Pedersen, 2000; Nigam and Ghani, 2000). Here we briefly review the naive Bayes model. Let x be a vector we want to classify, and ck be a possible class. What we want to know is the probability that the vector x belongs to the class ck. We first transform the probability P(ck|x) using Bayes’ rule, P(ck|x) = P(ck) × Pp(I) (5) � Class probability P(ck) can be estimated from training data. However, direct estimation of P(ck|x) is impossible in most cases because of the sparseness of training data. By assuming the conditional independence among the elements of a vector, P (x|ck) is decomposed as follows, d</context>
</contexts>
<marker>Pedersen, 2000</marker>
<rawString>Ted Pedersen. 2000. A simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation. In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Takeuchi</author>
<author>N Collier</author>
</authors>
<title>Use of support vector machines in extended named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning</booktitle>
<pages>119--125</pages>
<contexts>
<context position="2054" citStr="Takeuchi and Collier, 2002" startWordPosition="304" endWordPosition="307"> the most important tasks today (Marcotte et al., 2001; Thomas et al., 2000; Ono et al., 2001). To extract information of proteins, one has to first recognize protein names in a text. This kind of problem has been studied in the field of natural language processing as named entity recognition tasks. Ohta et al. (2002) provided the GENIA corpus, an annotated corpus of MEDLINE abstracts, which can be used as a gold-standard for evaluating and training named entity recognition algorithms. There are some research efforts using machine learning techniques to recognize biological entities in texts (Takeuchi and Collier, 2002; Kim and Tsujii, 2002; Kazama et al., 2002). One drawback of these machine learning based approaches is that they do not provide identification information of recognized terms. For the purpose of information extraction of protein-protein interaction, the ID information of recognized proteins, such as GenBank 1 ID or SwissProt 2 ID, is indispensable to integrate the extracted information with the data in other information sources. Dictionary-based approaches, on the other hand, intrinsically provide ID information because they recognize a term by searching the most similar (or identical) one i</context>
</contexts>
<marker>Takeuchi, Collier, 2002</marker>
<rawString>K. Takeuchi and N. Collier. 2002. Use of support vector machines in extended named entity recognition. In Proceedings of the 6th Conference on Natural Language Learning 2002 (CoNLL-2002), pages 119–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Thomas</author>
<author>David Milward</author>
<author>Christos Ouzounis</author>
<author>Stephen Pulman</author>
<author>Mark Carroll</author>
</authors>
<title>Automatic extraction of protein interactions from scientific abstracts.</title>
<date>2000</date>
<booktitle>In Proceedings of the Pacific Symposium on Biocomputing (PSB2000),</booktitle>
<volume>5</volume>
<pages>502--513</pages>
<contexts>
<context position="1503" citStr="Thomas et al., 2000" startWordPosition="215" endWordPosition="218">present an approximate string searching method to alleviate the latter problem. Experimental results using the GENIA corpus show that the filtering using a naive Bayes classifier greatly improves precision with slight loss of recall, resulting in a much better F-score. 1 Introduction The rapid increase of machine readable biomedical texts (e.g. MEDLINE) makes automatic information extraction from those texts much more attractive. Especially extracting information of protein-protein interactions from MEDLINE abstracts is regarded as one of the most important tasks today (Marcotte et al., 2001; Thomas et al., 2000; Ono et al., 2001). To extract information of proteins, one has to first recognize protein names in a text. This kind of problem has been studied in the field of natural language processing as named entity recognition tasks. Ohta et al. (2002) provided the GENIA corpus, an annotated corpus of MEDLINE abstracts, which can be used as a gold-standard for evaluating and training named entity recognition algorithms. There are some research efforts using machine learning techniques to recognize biological entities in texts (Takeuchi and Collier, 2002; Kim and Tsujii, 2002; Kazama et al., 2002). One</context>
</contexts>
<marker>Thomas, Milward, Ouzounis, Pulman, Carroll, 2000</marker>
<rawString>James Thomas, David Milward, Christos Ouzounis, Stephen Pulman, and Mark Carroll. 2000. Automatic extraction of protein interactions from scientific abstracts. In Proceedings of the Pacific Symposium on Biocomputing (PSB2000), volume 5, pages 502–513.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>