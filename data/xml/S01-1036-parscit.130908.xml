<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000043">
<title confidence="0.9735505">
KUNLP system using Classification Information Model
at SENSEVAL-2
</title>
<author confidence="0.997184">
Hee-Cheol Seo, Sang-Zoo Lee, Hae-Chang Rim Ho Lee
</author>
<affiliation confidence="0.9988735">
Dept. of Computer Science and Engineering,
Korea University
</affiliation>
<address confidence="0.787865285714286">
1, 5-ka, Anam-dong
Seongbuk-Gu, Seoul, 136-701, Korea
{hcseo,zoo,rim}Onlp.korea.ac.kr
Astronest Inc.
135-090 3rd floor, Hanam BD
157-18 Samsung-Dong
Kangnam-Gu, Seoul, Korea
</address>
<email confidence="0.820674">
leeho©astronest.com
</email>
<sectionHeader confidence="0.991003" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965882352941">
The classification information model or CIM classi-
fies instances by considering the discrimination abil-
ity of their features, which was proven to be useful
for word sense disambiguation at SENSEVAL-1. But
the CIM has a problem of information loss. KUNLP
system at SENSEVAL-2 uses a modified version of the
CIM for word sense disambiguation.
We used three types of features for word sense
disambiguation: local, topical, and bigram context.
Local and topical context are similar to Chodorow&apos;s
context and refer to only unigram information. The
window of a bigram context is similar to that of a
local context but a bigram context refers to only
bigram information.
We participated in the English lexical sample task
and the Korean lexical sample task, where our sys-
tems ranked high.
</bodyText>
<sectionHeader confidence="0.997909" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.978438592592592">
The classification information model(Ho, 1997) is
the model that classifies instances by considering
the discrimination ability of their features. In the
CIM, a feature with high discrimination ability con-
tributes to the classification more than one with low
discrimination ability. Hence, we can omit the fea-
ture selection procedure.
The CIM has a kind of information loss problem
due to the assumption that a feature contributes to
only one class. We devised a modified version of the
CIM where a feature can contribute to all classes.
Word sense disambiguation task can be treated as
a kind of classification process(Ho, 2000). When a
classification technique is applied to word sense dis-
ambiguation, an instance corresponds to a context
containing a polysemous word and its class to the
proper sense of the word, and one of its features
to a piece of context information. As a classifica-
tion problem, word sense disambiguation task can
be solved by the CIM.
We used three types of features for word sense
disambiguation: local, topical, and bigram context.
Local and topical context are similar to Chodorow&apos;s
context(Chodorow, 2000) and consist of only uni-
gram information. A bigram context has a similar
window to a local context but consists of only bigram
information.
</bodyText>
<sectionHeader confidence="0.964385" genericHeader="method">
2 KUNLP system
</sectionHeader>
<bodyText confidence="0.999547666666667">
To disambiguate senses, we did two phases: corpus
preprocessing and sense disambiguation. Figure 1
shows the flow chart of our system.
</bodyText>
<table confidence="0.9121312">
Corpus
1
Tokenizer
POS-Tagger
Corpus
Preprocessing
Phrase Filter
Jr
Sense Tagger
using
Modified CIM
Sense
Disambiguation
&apos;Jr
Sense-Tagged Corpus
</table>
<figureCaption confidence="0.974133">
Figure 1: Flow chart of KUNLP system
</figureCaption>
<subsectionHeader confidence="0.995611">
2.1 Corpus preprocessing
</subsectionHeader>
<bodyText confidence="0.997003">
At the corpus preprocessing phase, we tokenized a
corpus and then tagged it with parts-of-speech using
Brill&apos;s Tagger(Brill, 1994). The tokenizer just sepa-
rates symbols from a word. For example, a sentence
&amp;quot;I&apos;m straight, white, no longer middle class, anti-
IRA, have ...&amp;quot; is tokenized to &amp;quot;I &apos;m stright , white ,
no longer middle class , anti - IRA , have ...&amp;quot;. Un-
like other symbols, an apostrophe is not separated
from the following characters.
</bodyText>
<page confidence="0.989644">
147
</page>
<subsectionHeader confidence="0.994939">
2.2 Phrase filtering
</subsectionHeader>
<bodyText confidence="0.9685014">
At the phrase filtering phase, we filtered senses using
the satellite feature, which is marked with sat tag in
training and test corpus given by the task organizer.
For example, in a sentence This air of disengagement
&lt;head sats=&amp;quot;carry_over.067:0&amp;quot;&gt;carried&lt;/head&gt;
&lt;sat id=&amp;quot;carry_over.067:0&amp;quot;&gt;over&lt;/sat&gt; to his
apparent attitude toward his things, carried over is
a phrase and also a satellite feature.
Phrase filtering is applied to sense disambiguation
as in Table 1
</bodyText>
<tableCaption confidence="0.96693">
Table 1: phrase filtering and sense disambiguation
</tableCaption>
<bodyText confidence="0.9934392">
if the number of filtered senses = 1 then
determine sense
else if the number of filtered senses &gt; 1 then
execute sense-tagger with the filtered senses
else if the number of filtered senses = 0 then
execute sense-tagger with all senses
There are satellite features in the English lexical
sample, but not in the Korean lexical sample. Hence,
phrase filtering was applied only in the English lex-
ical sample task.
</bodyText>
<listItem confidence="0.5041225">
2.3 Classification Information Model
(CIM)
</listItem>
<bodyText confidence="0.999039">
The CIM is a kind of classification model based on
the entropy theory. Given an input instance, the
CIM decides the proper class of the instance by con-
sidering individual decisions made by each feature
of the instance. In the model, the proper class of an
instance,X, is determined by Equation 1.
</bodyText>
<equation confidence="0.751306">
def
Class(X) = arg max Rel(classy, ) (1)
class,
</equation>
<bodyText confidence="0.9895866">
where classy is the j-th class and Rel(classy, X) is
the relevance between the j-th class and the instance
X. Here, if we assume that features are independent
of each other, the relevance can be defined as in
Equation 2.
</bodyText>
<equation confidence="0.9982125">
Rel(classy, X) = (2)
i=1
</equation>
<bodyText confidence="0.955712952380953">
where in is the size of the feature set, xi is the value
of the i-th feature and wij is the weight of the i-
th feature for the j-th class. In Equation 2, xi has
a binary value (1 if the feature occurs within the
window, 0 otherwise) and wij is defined in terms of
classification information.
The classification information of a feature is com-
posed of two components. One is the discrimination
score (DS), which represents the discrimination abil-
ity of classifying instances. The other is the most
probable class (MPC), which represents the most
closely related class to the feature. wij is defined
by using these two components as follows:
clef { DS i if classy = MPG,
0 otherwise
In Equation 3, DS, and MPG, represent the DS
and MPC of the i-th feature, respectively. In the
CIM, DS and MPC are defined in terms of the con-
ditional probability of a class given a feature, which
is normalized by the corpus size. The normalized
conditional probability is defined as follows:
</bodyText>
<equation confidence="0.9986175">
p(classylfs) N(class)
N(class,)
pe l as s i) N(elass)
N(classk)
p(fiiclassy)
p(fdclassk)
</equation>
<bodyText confidence="0.999945571428571">
In Equation 4, fiji is a normalized conditional
probability, N(classy) is the number of instances
belonging to the j-th class in the training data,
N(class) is the average number of instances for each
class and n is the number of classes. Given the
normalized conditional probability distribution, DSs
and MPCs are defined as follows:
</bodyText>
<equation confidence="0.984611222222222">
clef
DS i = log2 n — H(j5i)
= log2 n + 1°g2 (5)
i=
clef
MPG,= arg max 15 ji
class,
arg max p( filclassj) (6)
classi
</equation>
<bodyText confidence="0.999779333333333">
In Equation 5, H(A) is the entropy of the i-th
feature over the normalized conditional probability
distribution.
</bodyText>
<subsectionHeader confidence="0.996431">
2.4 Modifying CIM
</subsectionHeader>
<bodyText confidence="0.947840727272727">
The CIM has a problem caused by using MPCs,
which is information loss. For example, let us con-
sider the situation in Table 2 and Table 3. Table 2
shows the normalized conditional probability distri-
bution, DSs and MPCs of features in an instance.
Table 3 shows the weights and the relevance values
at the CIM using wij and at the modified CIM us-
ing &apos;d for the instance of Table 2. The feature fi
co-occurred with clas.s, and class2 and the MPC of
f is class, at Table 2. In the CIM, this feature
P.i def
</bodyText>
<page confidence="0.99592">
148
</page>
<tableCaption confidence="0.99683">
Table 2: A normalized conditional probability, DSs and MPCs of features of an instance
</tableCaption>
<table confidence="0.985644666666667">
fi 0.7 0.3 0 0 1.1187 class)
12 0 0.4 0.6 0 1.0290 class3
h 0 0.4 0.1 0.5 0.6390 class4
</table>
<tableCaption confidence="0.7663025">
Table 3: The weights and the relevance values at the CIM using wi • and at the modified CIM using for
the instance of Table 2
</tableCaption>
<equation confidence="0.6649792">
weight(w) weight(evij)
feature classi class2 class3 class4 class, class2 class3
Ii 1.1187 0 0 0 0.7831 0.3356 0 0
12 0 0 1.0290 0 0 0.4116 0.6174 0
/3 0 0 0 0.6390 0 0.2556 0.0639 0.3195
Rel(classi, X) 1.1187 0 1.0290 0.6390 0.7831 1.0028 0.6813 0.3195
normalized conditional probability(P32)
classi class2 class3 class4
feature
DS MP C
</equation>
<bodyText confidence="0.947560481481482">
contributes to only class,. Actually the feature 11
can contribute to distinguishing class2 from class3
if it consults the normalized conditional probability
distribution. In the CIM, however, the feature can
not distinguish them because their weights have the
same value.
Another aspect of the problem is that the CIM
fails to capture the minor contribution of features,
which is crucial in the case where the sum of the
minor contribution of features to a non-MPC class
dominates that of the major contribution of fea-
tures to MPC classes. For example, at Table 2,
all features, Ii, 12, and h, have different MPCs:
class&apos;, class3 and class4, respectively, it is also ob-
vious that they have some minor contribution to the
class2. The CIM will classify the instance as class,
because Rel(classi, X) = 1.1187 is the maximum
number among the Rel(classj, X). However, if we
consider the minor contribution of all the features,
we prefer class2 to classi because class2 intuitively
gains the total contribution more than c/assi.
A solution to the problem may be not to use
MPCs, but to use a measure of contribution of a
feature to a class which is proportional to the dis-
crimination score of the feaure and the normalized
conditional probability of the class given the feature.
The modified CIM can be defined as follows:
</bodyText>
<equation confidence="0.995721">
Rel(class , X) = xiwij (7)
D Si x 25ji (8)
</equation>
<bodyText confidence="0.7506674">
As shown in Table 3, the 117)12 is larger than
W13(0.3356 &gt; 0) and the instance is classified not
as class, but as class2 because Rel(class2, X) =
1.0028 &gt; Rel(classi, X) = 0.7831, which is based
on the modified CIM.
</bodyText>
<subsectionHeader confidence="0.985216">
2.5 Feature Space
</subsectionHeader>
<bodyText confidence="0.9998834">
We used three types of features for word sense dis-
ambiguation: local, topical and bigram context. In
the preliminary experiment, we have observed that,
when the CIM considered all these three types of
features, it mostly achieved the best result.
</bodyText>
<subsectionHeader confidence="0.657099">
2.5.1 Local context
</subsectionHeader>
<bodyText confidence="0.9940915">
In a local context, there can be features of the fol-
lowing templates for all words within its window:
</bodyText>
<listItem confidence="0.847749833333333">
• in the English lexical sample task
- word_position : a word and its position
- word_POS : a word and its part-of-speech
- POS_position : the part-of-speech and po-
sition of a word
• in the Korean lexical sample task
morpherne_position : a morpheme&apos; and its
position.
morphente_POS : a morpheme and its part-
of-speech.
- POS_position : the part-of-speech and po-
sition of a morpheme
</listItem>
<bodyText confidence="0.997270285714286">
In the English lexical sample task, word is a sur-
face form and can be either one of open-class words
whose POS is one of the noun, verb, adjective, and
adverb; or one of closed-class words whose POS is
A Korean sentence is composed of one or more eojeols,
which are separated by spaces, and an eojeol consists of one
or more morphemes.
</bodyText>
<page confidence="0.997003">
149
</page>
<bodyText confidence="0.9999921">
one of the determiner, preposition, pronoun, and
punctuation. The window size of ±3 words in the
English lexical sample task and the window size from
—2 to +3 word in the Korean lexical sample task
were empirically chosen.
In the first phase of the experiments, we used just
one complicated template, word_position_POS(in
Korean morpheme_position_POS), which brought
about data sparseness problem. So we split the tem-
plate into three simpler templates.
</bodyText>
<subsubsectionHeader confidence="0.695731">
2.5.2 Topical context
</subsubsectionHeader>
<bodyText confidence="0.9877905">
A topical context includes features of the following
templates for all open-class words within its window:
</bodyText>
<listItem confidence="0.983273">
• in the English lexical sample task
— word : an open-class word.
• in the Korean lexical sample task
— morpheme : an open-class morpheme.
</listItem>
<bodyText confidence="0.99939825">
The window size of ±1 sentences in the English
lexical sample task and the window size of all sen-
tences in the Korean lexical sample task were em-
pirically chosen.
</bodyText>
<subsubsectionHeader confidence="0.679576">
2.5.3 Bigram context
</subsubsectionHeader>
<bodyText confidence="0.997737666666667">
In a bigram context, there can be features of the fol-
lowing templates for all word-pairs within its win-
dow:
</bodyText>
<listItem confidence="0.982443375">
• in the English lexical sample task
(wordi,wordj) : the i-th word and th
word (i&gt;j)
(wordi,POSj) : the i-th word and j-th
part-of-speech (i&gt;j)
• in the Korean lexical sample task
— (eojeoli,eojeolj) : the i-th eojeol and j-th
eojeol (i&gt;j)
</listItem>
<bodyText confidence="0.994815">
Unlike local and topical contexts, bigram contexts
are composed of only bigram information surround-
ing the polysemous word. The window size of ±2
words in the English lexical sample task and the win-
dow size from —2 to +3 word in the Korean lexical
sample task were empirically chosen.
</bodyText>
<sectionHeader confidence="0.999493" genericHeader="method">
3 Experimental Result
</sectionHeader>
<bodyText confidence="0.99932075">
The following tables show the results of our systems
at SENSE&apos;AL-2 (Table 4). For the Korean lexical
sample task at SENSEVAL-2, only fine-grained sense
distinction was made.
</bodyText>
<tableCaption confidence="0.7668025">
Table 4: Results of KUNLP systems at SENSEVAL-2
task prec. recall
</tableCaption>
<table confidence="0.600369">
English Lexical Sample (fine g.) 0.629 0.629
English Lexical Sample (coarse g.) 0.697 0.697
Korean Lexical Sample (fine g.) 0.698 0.74
</table>
<sectionHeader confidence="0.995905" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999942">
We have described the modified CIM used for word
sense disambiguation at SENSEVAL-2. In the exper-
iments, three types of features; local, topical, and
bigram context, are used. Our system ranked as
the highest at the Korean lexical sample task and
as the topmost group at the English lexical sample
task among the supervised models at SENSEVAL-2.
Consequently, the results back up the fact that the
modified CIM and three types of features are useful
for discriminating word senses.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991433125">
Eric Brill 1994. Some advances in rule-based
part of speech tagging. In Proceedings of the
Twelfth National Conference on Artificial Intel-
ligence (AAAI-9.4).
Martin Chodorow, Claudia Leacock and George A.
Miller 2000. A Topical/Local Classifier for Word
Sense Identification. In Computers and the Hu-
manities 34: 115-120.
Ho Lee, Dae-Ho Back and Hae-Chang Rim 1997.
Word Sense Disambiguation Based on The In-
formation Theory. In Proceedings of Research on
Computational Linguisitcs Conference.
Ho Lee, Hae-Chang Rim and JungYun Seo 2000.
Word Sense Disambiguation Using the Classifica-
tion Information Model. In Computers and the
Humanities 3.4: 1.41-146.
</reference>
<page confidence="0.998311">
150
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.025702">
<title confidence="0.748446">KUNLP system using Classification Information Model at SENSEVAL-2</title>
<author confidence="0.811573">Hee-Cheol Seo</author>
<author confidence="0.811573">Sang-Zoo Lee</author>
<author confidence="0.811573">Hae-Chang Rim Ho Lee</author>
<affiliation confidence="0.735056">Dept. of Computer Science and Korea</affiliation>
<address confidence="0.540115">1, 5-ka, Seongbuk-Gu, Seoul, 136-701,</address>
<email confidence="0.885856">hcseoOnlp.korea.ac.kr</email>
<email confidence="0.885856">zooOnlp.korea.ac.kr</email>
<email confidence="0.885856">rimOnlp.korea.ac.kr</email>
<affiliation confidence="0.480372">Astronest</affiliation>
<address confidence="0.58623">135-090 3rd floor, Hanam 157-18 Kangnam-Gu, Seoul,</address>
<email confidence="0.999443">leeho©astronest.com</email>
<abstract confidence="0.996834888888889">The classification information model or CIM classifies instances by considering the discrimination ability of their features, which was proven to be useful for word sense disambiguation at SENSEVAL-1. But the CIM has a problem of information loss. KUNLP at a modified version of the CIM for word sense disambiguation. We used three types of features for word sense disambiguation: local, topical, and bigram context. Local and topical context are similar to Chodorow&apos;s context and refer to only unigram information. The window of a bigram context is similar to that of a local context but a bigram context refers to only bigram information. We participated in the English lexical sample task and the Korean lexical sample task, where our systems ranked high.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Some advances in rule-based part of speech tagging.</title>
<date>1994</date>
<booktitle>In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-9.4).</booktitle>
<contexts>
<context position="2939" citStr="Brill, 1994" startWordPosition="451" endWordPosition="452"> only unigram information. A bigram context has a similar window to a local context but consists of only bigram information. 2 KUNLP system To disambiguate senses, we did two phases: corpus preprocessing and sense disambiguation. Figure 1 shows the flow chart of our system. Corpus 1 Tokenizer POS-Tagger Corpus Preprocessing Phrase Filter Jr Sense Tagger using Modified CIM Sense Disambiguation &apos;Jr Sense-Tagged Corpus Figure 1: Flow chart of KUNLP system 2.1 Corpus preprocessing At the corpus preprocessing phase, we tokenized a corpus and then tagged it with parts-of-speech using Brill&apos;s Tagger(Brill, 1994). The tokenizer just separates symbols from a word. For example, a sentence &amp;quot;I&apos;m straight, white, no longer middle class, antiIRA, have ...&amp;quot; is tokenized to &amp;quot;I &apos;m stright , white , no longer middle class , anti - IRA , have ...&amp;quot;. Unlike other symbols, an apostrophe is not separated from the following characters. 147 2.2 Phrase filtering At the phrase filtering phase, we filtered senses using the satellite feature, which is marked with sat tag in training and test corpus given by the task organizer. For example, in a sentence This air of disengagement &lt;head sats=&amp;quot;carry_over.067:0&amp;quot;&gt;carried&lt;/head</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>Eric Brill 1994. Some advances in rule-based part of speech tagging. In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-9.4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
<author>George A Miller</author>
</authors>
<title>A Topical/Local Classifier for Word Sense Identification.</title>
<date>2000</date>
<booktitle>In Computers and the Humanities</booktitle>
<volume>34</volume>
<pages>115--120</pages>
<marker>Chodorow, Leacock, Miller, 2000</marker>
<rawString>Martin Chodorow, Claudia Leacock and George A. Miller 2000. A Topical/Local Classifier for Word Sense Identification. In Computers and the Humanities 34: 115-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ho Lee</author>
</authors>
<title>Dae-Ho Back and Hae-Chang Rim</title>
<date>1997</date>
<booktitle>In Proceedings of Research on Computational Linguisitcs Conference.</booktitle>
<marker>Lee, 1997</marker>
<rawString>Ho Lee, Dae-Ho Back and Hae-Chang Rim 1997. Word Sense Disambiguation Based on The Information Theory. In Proceedings of Research on Computational Linguisitcs Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ho Lee</author>
</authors>
<title>Hae-Chang Rim and JungYun Seo</title>
<date>2000</date>
<booktitle>In Computers and the Humanities</booktitle>
<volume>3</volume>
<pages>1--41</pages>
<marker>Lee, 2000</marker>
<rawString>Ho Lee, Hae-Chang Rim and JungYun Seo 2000. Word Sense Disambiguation Using the Classification Information Model. In Computers and the Humanities 3.4: 1.41-146.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>