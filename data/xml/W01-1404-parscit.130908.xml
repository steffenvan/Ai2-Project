<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000069">
<title confidence="0.881902333333333">
Approximating Context-Free by Rational Transduction for
Example-Based MT
Mark-Jan Nederhof
</title>
<author confidence="0.817121">
AT&amp;T Labs-Research, 180 Park Avenue, Florham Park, NJ 07932
</author>
<affiliation confidence="0.541354">
and
</affiliation>
<author confidence="0.40716">
Alfa Informatica (RUG), P.O. Box 716, NL-9700 AS Groningen, The Netherlands
</author>
<sectionHeader confidence="0.96911" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996187">
Existing studies show that a weighted
context-free transduction of reasonable
quality can be effectively learned from
examples. This paper investigates the
approximation of such transduction by
means of weighted rational transduc-
tion. The advantage is increased pro-
cessing speed, which benefits real-
time applications involving spoken lan-
guage.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.959208017857143">
Several studies have investigated automatic or
partly automatic learning of transductions for ma-
chine translation. Some of these studies have con-
centrated on finite-state or extended finite-state
machinery, such as (Vilar and others, 1999), oth-
ers have chosen models closer to context-free
grammars and context-free transduction, such as
(Alshawi et al., 2000; Watanabe et al., 2000; Ya-
mamoto and Matsumoto, 2000), and yet other
studies cannot be comfortably assigned to either
of these two frameworks, such as (Brown and oth-
ers, 1990) and (Tillmann and Ney, 2000).
In this paper we will investigate both context-
free and finite-state models. The basis for our
study is context-free transduction since that is a
powerful model of translation, which can in many
cases adequately describe the changes of word
✁The second address is the current contact address; sup-
ported by the Royal Netherlands Academy of Arts and Sci-
ences; current secondary affiliation is the German Research
Center for Artificial Intelligence (DFKI).
order between two languages, and the selection
of appropriate lexical items. Furthermore, for
limited domains, automatic learning of weighted
context-free transductions from examples seems
to be reasonably successful.
However, practical algorithms for computing
the most likely context-free derivation have a cu-
bic time complexity, in terms of the length of
the input string, or in the case of a graph out-
put by a speech recognizer, in terms of the num-
ber of nodes in the graph. For certain lexicalized
context-free models we even obtain higher time
complexities when the size of the grammar is not
to be considered as a parameter (Eisner and Satta,
1999). This may pose problems, especially for
real-time speech systems.
Therefore, we have investigated approximation
of weighted context-free transduction by means
of weighted rational transduction. The finite-state
machinery for implementing the latter kind of
transduction in general allows faster processing.
We can also more easily obtain robustness. We
hope the approximating model is able to preserve
some of the accuracy of the context-free model.
In the next section, we discuss preliminary def-
initions, adapted from existing literature, mak-
ing no more than small changes in presentation.
In Section 3 we explain how context-free trans-
duction grammars can be represented by ordinary
context-free grammars, plus a phase of postpro-
cessing. The approximation is discussed in Sec-
tion 4. As shown in Section 5, we may easily
process input in a robust way, ensuring we always
obtain output. Section 6 discusses empirical re-
sults, and we end the paper with conclusions.
</bodyText>
<sectionHeader confidence="0.99231" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.9948">
2.1 hierarchical alignment
</subsectionHeader>
<bodyText confidence="0.977498677966102">
The input to our algorithm is a corpus consisting
of pairs of sentences related by an hierarchical
alignment (Alshawi et al., 2000). In what follows,
the formalization of this concept has been slightly
changed with respect to the above reference, to
suit our purposes in the remainder of this article.
The hierarchically aligned sentence pairs in the
corpus are 5-tuples satisfying
the following. The first two components, and
, are strings, called the source string and the
target string, respectively, the lengths of which
are denoted by and . We
let and denote the sets of string positions
and respectively.
Further, (resp. ) is a mapping from posi-
tions in (resp. ) to pairs of
lists of positions from (resp. ), satisfying
the following: if a position is mapped to a pair
, then the positions in the list are
in strictly increasing order; we let “” denote list-
concatenation, and represents a list consisting
of a single element .
Each position in (resp. ) should occur
at most once in the image of (resp. ). This
means that and assign dependency struc-
tures to the source and target strings.
A further restriction on and requi
some auxiliary definitions. Let be either
or . We define as the function that maps
each position to the list of positions
when
. If is a
string , and is a list of string
positions in , then represents the string
. If is a single position, then rep-
resents the symbol .
We now say that is projective if maps each
position to some interval of positions
. We will assume that both
and are projective. (Strictly speaking, our al-
gorithm would still be applicable if they were
not projective, but it would treat the hierarchical
alignment as if the symbols in the source and tar-
get strings had been reordered to make and
projective.) Furthermore, a reasonable hier-
archical alignment satisfies ,
imply
and imply ; in other
words, a position in one string is related to at most
one position in the other. Furthermore, for each
there is a pair
such that occurs in one of the two lists of
and occurs in one of the two lists of ; this
means that positions can only be related if their
respective “mother” positions are related.
Note that this paper does not discuss how hi-
erarchical alignments can be obtained from unan-
notated corpora of bitexts. This is the subject of
existing studies, such as (Alshawi et al., 2000).
</bodyText>
<subsectionHeader confidence="0.993611">
2.2 context-free transduction
</subsectionHeader>
<bodyText confidence="0.999389833333333">
Context-free transduction was originally called
syntax-directed transduction in (Lewis II and
Stearns, 1968), but since in modern formal lan-
guage theory and computational linguistics the
term “syntax” has a much wider range of mean-
ings than just “context-free syntax”, we will not
use the original term here.
A (context-free) transduction grammar is a 5-
tuple , where is a finite set of
nonterminals, is the start symbol, and
are the source and target alphabets, and is a
finite set of productions of the form
</bodyText>
<footnote confidence="0.7684865">
1Note that we ignore the case that a single nonterminal
occurs twice or more in or ; if we were to include this
case, some tedious complications of notation would result,
without any theoretical gain such as an increase of genera-
tive power. We refer to (Lewis II and Stearns, 1968) for the
general case.
</footnote>
<bodyText confidence="0.99918256097561">
where or when or ,
respectively, which means that all symbols in the
string are indirectly linked to the ‘dummy’ posi-
tion 0.
Lastly, is the union of and a subset of
that relates positions in the two strings.
It is such that
,
where ,and ,
such that each nonterminal in occurs exactly
once in and each nonterminal in occurs ex-
actly once in .1
If we were to replace each RHS pair by only
its first part , we would obtain a context-free
grammar for the source language, and if we were
to replace each RHS pair by its second part ,
we would obtain a context-free grammar for the
target language. The combination of the two
halves of such a RHS indicates how a parse for
the source language can be related to a parse for
the target language, and this defines a transduc-
tion between the languages in an obvious way.
An example of a transduction grammar is:
This transduction defines that a sentence “I like
him” can be translated by “il me plait”.
We can reduce the generative power of context-
free transduction grammars by a syntactic restric-
tion that corresponds to the bilexical context-free
grammars (Eisner and Satta, 1999). Let us define
a bilexical transduction grammar as a transduc-
tion grammar which is such that:
there is a mapping from the set of nontermi-
nals to . Due to this property, we may
write each nonterminal as to indicate
that it is mapped to the pair , where
and , where is a so called
delexicalized nonterminal. We may write
as , where is a dummy symbol at
the dummy string position . Further,
each production is of one of the following
five forms:
</bodyText>
<subsectionHeader confidence="0.770091">
2.3 obtaining a context-free transduction
from the corpus
</subsectionHeader>
<bodyText confidence="0.943138811594203">
We extract a context-free transduction grammar
from a corpus of hierarchical alignments, by lo-
cally translating each hierarchical alignment into
a set of productions. The union of all these sets for
the whole corpus is then the transduction gram-
mar. Counting the number of times that identi-
cal productions are generated allows us to assign
probabilities to the productions by maximum like-
lihood estimation.
We will consider a method that uses only one
delexicalized nonterminal . For a pair
, we have a nonterminal or a
nonterminal , depending on whether non-
terminals are lexicalized by both source and target
alphabets, or by just the source alphabet. Let us
call that nonterminal .
Each pair of positions gives rise to
one production. Suppose that
and each position in this pair is related by
to some position from , which we will call
, respectively, and simi-
and each position in this pair is related by
to some position from , which we will call
. Then the production
Subj-IObj “like” Obj-Subj
Obj-Subj Subj-IObj “plait”
Subj-IObj “I” “me”
Obj-Subj “him” “il”
larly, suppose that
is given by
For convenience, we also allow productions of
the form:
where and .
In the experiments in Section 6, we also con-
sider nonterminals that are lexicalized only by the
source alphabet, which means that these nonter-
minals can be written as , where . The
motivation is to restrict the grammar size and to
increase the coverage.
Bilexical transduction grammars are equivalent
to the dependency transduction model from (Al-
shawi et al., 2000).
Note that both halves of the RHS contain the same
nonterminals but possibly in a different order.
However, if any position in or is
not related to some other position by , then the
production above contains, instead of a nontermi-
nal, a substring on which that position is projected
by or , respectively. E.g. if there is no po-
sition such that , then instead of
we have the string .
In general, we cannot adapt the above algo-
rithm to produce transduction grammars that are
bilexical. For example, a production of the form:
cannot be broken up into smaller, bilexical pro-
ductions.2 However, the hierarchical alignments
that we work with were produced by an algorithm
that ensures that bilexical grammars suffice. For-
mally, this applies when the following cannot oc-
cur: there are and
such that ,and occur in ,
and occur in and , and
either and , or
and , or and ,
or and .
For example, if the non-bilexical production we
would obtain is:
then the bilexical transduction grammar that our
algorithm produces contains:
</bodyText>
<sectionHeader confidence="0.961172" genericHeader="method">
3 Reordering as postprocessing
</sectionHeader>
<bodyText confidence="0.998409153846154">
In the following section we will discuss an algo-
rithm that was devised for context-free grammars.
To make it applicable to transduction, we propose
a way to represent bilexical transduction gram-
mars as ordinary context-free grammars. In the
new productions, symbols from the source and
target alphabets occur side by side, but whereas
source symbols are matched by the parser to the
input, the target symbols are gathered into output
strings. In our case, the unique output string the
parser eventually produces from an input string
is obtained from the most likely derivation that
matches that input string.
</bodyText>
<footnote confidence="0.5616265">
2That bilexical transduction grammars are less power-
ful than arbitrary context-free transduction grammars can
be shown formally; cf. Section 3.2.3 of (Aho and Ullman,
1972).
</footnote>
<bodyText confidence="0.99367235483871">
That the nonterminals in both halves of a RHS
in the transduction grammar may occur in a dif-
ferent order is solved by introducing three special
symbols, the reorder operators, which are inter-
preted after the parsing phase. These three opera-
tors will be written as “”, “” and “✶”. In a given
string, there should be matching triples of these
operators, in such a way that if there are two such
triples, then they either occur in two isolated sub-
strings, or one occurs nested between the “” and
the “” or nested between the “” and the “” of the
other triple. The interpretation of an occurrence
of a triple, say in an output string ,
is that the two enclosed substrings should be re-
ordered, so that we obtain .
Both the reorder operators and the symbols of
the target alphabet will here be marked by a hor-
izontal line to distinguish them from the source
alphabet. For example, the two productions
from the transduction grammar are represented by
the following two context-free productions:
In the first production, the RHS nonterminals oc-
cur in the same order as in the left half of the orig-
inal production, but reorder operators have been
added to indicate that, after parsing, some sub-
strings of the output string are to be reordered.
Our reorder operators are similar to the two op-
erators and from (Vilar and others, 1999),
but the former are more powerful, since the latter
allow only single words to be moved instead of
whole phrases.
</bodyText>
<sectionHeader confidence="0.998771" genericHeader="method">
4 Finite-state approximation
</sectionHeader>
<bodyText confidence="0.940106016666667">
There are several methods to approximate
context-free grammars by regular languages
(Nederhof, 2000). We will consider here only the
so called RTN method, which is applied in a sim-
plified form.3
3As opposed to (Nederhof, 2000), we assume here that
all nonterminals are mutually recursive, and the grammar
contains self-embedding. We have observed that typical
grammars that we obtain in the context of this article indeed
have the property that almost all nonterminals belong to the
same mutually recursive set.
A finite automaton is constructed as follows.
For each nonterminal from the grammar we in-
troduce two states and . For each produc-
tion we introduce states
, and we add epsilon transitions from
to and from to . The initial state of
the automaton is and the only final state is ,
where is the start symbol of the grammar.
If a symbol in the RHS of a production is
a terminal, then we add a transition from to
labelled by . If a symbol in the RHS is
a nonterminal , then we add epsilon transitions
from to and from to .
The resulting automaton is determinized and
minimized to allow fast processing of input. Note
that if we apply the approximation to the type of
context-free grammar discussed in Section 3, the
transitions include symbols from both source and
target alphabets, but we treat both uniformly as in-
put symbols for the purpose of determinizing and
minimizing. This means that the driver for the
finite automaton still encounters nondeterminism
while processing an input string, since a state may
have several outgoing transitions for different out-
put symbols.
Furthermore, we ignore any weights that might
be attached to the context-free productions, since
determinization is problematic for weighted au-
tomata in general and in particular for the type
of automaton that we would obtain when carry-
ing over the weights from the context-free gram-
mar onto the approximating language following
(Mohri and Nederhof, 2001).
Instead, weights for the transitions of the fi-
nite automaton are obtained by training, using
strings that are produced as a side effect of the
computation of the grammar from the corpus.
These strings contain the symbols from both the
source and target strings mixed together, plus oc-
currences of the reorder operators where needed.
A English/French example might be:
I me like plait him il
The way these strings were obtained ensures that
they are included in the language generated by
the context-free grammar, and they are therefore
also accepted by the approximating automaton
due to properties of the RTN approximation. The
weights are the negative log of the probabilities
obtained by maximum likelihood estimation.
</bodyText>
<sectionHeader confidence="0.995251" genericHeader="method">
5 Robustness
</sectionHeader>
<bodyText confidence="0.999769148148148">
The approximating finite automaton cannot en-
sure that the reorder operators “”, “” and “ ” oc-
cur in matching triples in output strings. There
are two possible ways to deal with this problem.
First, we could extend the driver of the finite au-
tomaton to only consider derivations in which the
operators are matched. This is however counter
to our need for very efficient processing, since we
are not aware of any practical algorithms for find-
ing matching brackets in paths in a graph of which
the complexity is less than cubic.
Therefore, we have chosen a second approach,
viz. to make the postprocessing robust, by in-
serting missing occurrences of “” or “ ” and re-
moving redundant occurrences of brackets. This
means that any string containing symbols from
the target alphabet and occurrences of the reorder
operators is turned into a string without reorder
operators, with a change of word order where nec-
essary.
Both the transduction grammar and, to a lesser
extent, the approximating finite automaton suffer
from not being able to handle all strings of sym-
bols from the source alphabet. With finite-state
processing however, it is rather easy to obtain ro-
bustness, by making the following three provi-
sions:
</bodyText>
<listItem confidence="0.941378125">
1. To the nondeterministic finite automaton we
add one epsilon transition from the initial
state to , for each nonterminal . This
means that from the initial state we may
recognize an arbitrary phrase generated by
some nonterminal from the grammar.
2. After the training phase of the weighted
(minimal deterministic) automaton, all tran-
sitions that have not been visited obtain a
fixed high (but finite) weight. This means
that such transitions are only applied if all
others fail.
3. The driver of the automaton is changed so
that it restarts at the initial state when it gets
stuck at some input word, and when neces-
sary, that input word is deleted. The out-
</listItem>
<bodyText confidence="0.9989085">
put string with the lowest weight obtained
so far (preferably attached to final states, or
to other states with outgoing transitions la-
belled by input symbols) is then concate-
nated with the output string resulting from
processing subsequent input.
</bodyText>
<sectionHeader confidence="0.999503" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999968090909091">
We have investigated a corpus of En-
glish/Japanese sentence pairs, related by
hierarchical alignment (see also (Bangalore and
Riccardi, 2001)). We have taken the first 500,
1000, 1500, ... aligned sentence pairs from this
corpus to act as training corpora of varying sizes;
we have taken 300 other sentence pairs to act as
test corpus.
We have constructed a bilexical transduction
grammar from each training corpus, in the form
of a context-free grammar, and this grammar was
approximated by a finite automaton. The input
sentences from the test corpus were then pro-
cessed by context-free and finite-state machin-
ery (in the sequel referred to by cfg and fa, re-
spectively). We have also carried out experi-
ments with robust finite-state processing, as dis-
cussed in Section 5, which is referred to by ro-
bust fa. If we append 2 after a tag, this mean
that , otherwise
(see Section 2.3).
The reorder operators from the resulting out-
put strings were applied in a robust way as ex-
plained in Section 5. The output strings were
then compared to the reference output from the
corpus, resulting in Figure 1. Our metric is word
accuracy, which is based on edit distance. For a
pair of strings, the edit distance is defined as the
minimum number of substitutions, insertions and
deletions needed to turn one string into the other.
The word accuracy of a string with regard to a
string is defined to be , where is the edit
distance between and and is the length of
.
To allow a comparison with more established
techniques (see e.g. (Bangalore and Riccardi,
2001)), we also take into consideration a simple
bigram model, trained on the strings comprising
both source and target sentences and reorder oper-
ators, as explained in Section 4. For the purposes
of predicting output symbols, a series of consecu-
tive target symbols and reorder operators follow-
ing a source symbol in the training sentences are
treated as a single symbol by the bigram model,
and only those may be output after that source
symbol. Since our construction is such that target
symbols always follow source symbols they are a
translation of (according to the automatically ob-
tained hierarchical alignment), this modification
to the bigram model prevents output of totally un-
related target symbols that could otherwise result
from a standard bigram model. It also ensures that
a bounded number of output symbols per input
symbol are produced.
The fraction of sentences that were transduced
(i.e. that were accepted by the grammar or the
automaton), is indicated in Figure 2. Since ro-
bust fa(2) and bigram are able to transduce all
input, they are not represented here. Note that the
average word accuracy is computed only with re-
spect to the sentences that could be transduced,
which explains the high accuracy for small train-
ing corpora in the cases of cfg(2) and fa(2),
where the few sentences that can be transduced
are mostly short and simple.
Figure 3 presents the time consumption of
transduction for the entire test corpus. These
data support our concerns about the high costs of
context-free processing, even though our parser
relies heavily on lexicalization.4
Figure 4 shows the sizes of the automata after
determinization and minimization. Determiniza-
tion for the largest automata indicated in the Fig-
ure took more than 24 hours for both fa(2) and
robust fa(2) , which suggests these methods be-
come unrealistic for training corpus sizes consid-
erably larger than 10,000 bitexts.
</bodyText>
<sectionHeader confidence="0.996066" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999882428571429">
For our application, context-free transduction has
a relatively high accuracy, but it also has a high
time consumption, and it may be difficult to ob-
tain robustness without further increasing the time
costs. These are two major obstacles for use in
spoken language systems. We have tried to ob-
tain a rational transduction that approximates a
</bodyText>
<footnote confidence="0.88675">
4It uses a trie to represent productions (similar to ELR
parsing (Nederhof, 1994)), postponing generation of output
for a production until all nonterminals and all input symbols
from the right-hand side have been found.
training corpus size
</footnote>
<figureCaption confidence="0.960165">
Figure 1: Average word accuracy for transduced sentences.
training corpus size
Figure 2: Fraction of the sentences that were transduced.
</figureCaption>
<figure confidence="0.996856527777778">
1
➦
word accuracy
c
➦
0.6
0.5
cfg2
cfg
fa2
fa
bigram
robust_fa2
robust_fa
0.4
0.3
0.2
0 1000 2000 3000 4000 5000 6000 7000 8000
0.9
0.8
0.7
0 1000 2000 3000 4000 5000 6000 7000 8000
➧
accepted
e
➧
0.8
0.6
0.4
0.2
0
1
fa
fa2
cfg
cfg2
</figure>
<bodyText confidence="0.996281972222222">
context-free transduction, preserving some of its
accuracy.
Our experiments show that the automata we ob-
tain become very large for training corpora of in-
creasing sizes. This poses a problem for deter-
minization. We conjecture that the main source of
the excessive growth of the automata lies in noise
in the bitexts and their hierarchical alignments. It
is a subject for further study whether we can re-
duce the impact of this noise, e.g. by clustering of
source symbols, or by removing some infrequent,
idiosyncratic rules from the obtained transduction
grammar. Also, other methods of regular approx-
imation of context-free grammars may be consid-
ered.
In comparison to a simpler model, viz. bi-
grams, our approximating transductions do not
have a very high accuracy, which is especially
worrying since the off-line costs of computation
are much higher than in the case of bigrams. The
relatively low accuracy may be due to sparse-
ness of data when attaching weights to transitions:
the size of the minimal deterministic automaton
grows much faster than the size of the training
corpus it is constructed from, and the same train-
ing corpus is used to train the weights of the tran-
sitions of the automaton. Thereby, many transi-
tions do not obtain accurate weights, and unseen
input sentences are not translated accurately.
The problems described here may be avoided
by leaving out the determinization of the automa-
ton. This however leads to two new problems:
training of the weights requires more sophisti-
cated algorithms, and we may expect an increase
in the time needed to transduce input sentences,
since now both source and target symbols give
</bodyText>
<figure confidence="0.989834025">
robust_fa2
fa2
robust_fa
fa
0 1000 2000 3000 4000 5000 6000 7000 8000
# transitions
➩
s
4.5e+06
2.5e+06
3.5e+06
1.5e+06
500000
4e+06
2e+06
3e+06
1e+06
0
cfg2
cfg
robust_fa2
robust_fa
fa2
fa
1e+06
time (msec)
m
➨
800000
600000
400000
200000
0
➨
➩
0 1000 2000 3000 4000 5000 6000 7000 8000
1.6e+06
1.4e+06
1.2e+06
training corpus size training corpus size
</figure>
<figureCaption confidence="0.999951">
Figure 3: Time consumption of transduction. Figure 4: Sizes of the automata.
</figureCaption>
<bodyText confidence="0.995792">
rise to nondeterminism. Whether these problems
can be overcome requires further study.
</bodyText>
<sectionHeader confidence="0.996034" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999936125">
This work is a continuation of partly unpub-
lished experiments by Srinivas Bangalore, which
includes regular approximation of grammars ob-
tained from hierarchical alignments. Many ideas
in this paper originate from frequent discus-
sions with Hiyan Alshawi, Srinivas Bangalore
and Mehryar Mohri, for which I am very grate-
ful.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999872352941176">
A.V. Aho and J.D. Ullman. 1972. Parsing, volume 1
of The Theory of Parsing, Translation and Compil-
ing. Prentice-Hall.
H. Alshawi, S. Bangalore, and S. Douglas. 2000.
Learning dependency translation models as collec-
tions of finite-state head transducers. Computa-
tional Linguistics, 26(1):45–60.
S. Bangalore and G. Riccardi. 2001. A finite-state
approach to machine translation. In 2nd Meeting of
the North American Chapter of the ACL, Pittsburgh,
PA, June.
P.F. Brown et al. 1990. A statistical approach to
machine translation. Computational Linguistics,
16(2):79–85.
J. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automa-
ton grammars. In 37th Annual Meeting of the ACL,
pages 457–464, Maryland, June.
P.M. Lewis II and R.E. Stearns. 1968. Syntax-directed
transduction. Journal of the ACM, 15(3):465–488.
M. Mohri and M.-J. Nederhof. 2001. Regular approx-
imation of context-free grammars through transfor-
mation. In J.-C. Junqua and G. van Noord, editors,
Robustness in Language and Speech Technology,
pages 153–163. Kluwer Academic Publishers.
M.-J. Nederhof. 1994. An optimal tabular parsing al-
gorithm. In 32nd Annual Meeting of the ACL, pages
117–124, Las Cruces, New Mexico, June.
M.-J. Nederhof. 2000. Practical experiments with
regular approximation of context-free languages.
Computational Linguistics, 26(1):17–44.
C. Tillmann and H. Ney. 2000. Word re-ordering and
DP-based search in statistical machine translation.
In The 18th International Conference on Compu-
tational Linguistics, pages 850–856, Saarbr¨ucken,
July–August.
J.M. Vilar et al. 1999. Text and speech translation by
means of subsequential transducers. In A. Kornai,
editor, Extended finite state models of language,
pages 121–139. Cambridge University Press.
H. Watanabe, S. Kurohashi, and E. Aramaki. 2000.
Finding structural correspondences from bilingual
parsed corpus for corpus-based translation. In The
18th International Conference on Computational
Linguistics, pages 906–912, Saarbr¨ucken, July–
August.
K. Yamamoto and Y. Matsumoto. 2000. Acquisition
of phrase-level bilingual correspondence using de-
pendency structure. In The 18th International Con-
ference on Computational Linguistics, pages 933–
939, Saarbr¨ucken, July–August.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.310322">
<title confidence="0.9479945">Approximating Context-Free by Rational Transduction Example-Based MT</title>
<author confidence="0.565377">Mark-Jan</author>
<note confidence="0.784518">AT&amp;T Labs-Research, 180 Park Avenue, Florham Park, NJ Alfa Informatica (RUG), P.O. Box 716, NL-9700 AS Groningen, The Netherlands</note>
<abstract confidence="0.981888818181818">Existing studies show that a weighted context-free transduction of reasonable quality can be effectively learned from examples. This paper investigates the approximation of such transduction by of weighted transduction. The advantage is increased processing speed, which benefits realtime applications involving spoken language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<date>1972</date>
<journal>Parsing,</journal>
<booktitle>of The Theory of Parsing, Translation and Compiling.</booktitle>
<volume>1</volume>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="11444" citStr="Aho and Ullman, 1972" startWordPosition="1919" endWordPosition="1922">resent bilexical transduction grammars as ordinary context-free grammars. In the new productions, symbols from the source and target alphabets occur side by side, but whereas source symbols are matched by the parser to the input, the target symbols are gathered into output strings. In our case, the unique output string the parser eventually produces from an input string is obtained from the most likely derivation that matches that input string. 2That bilexical transduction grammars are less powerful than arbitrary context-free transduction grammars can be shown formally; cf. Section 3.2.3 of (Aho and Ullman, 1972). That the nonterminals in both halves of a RHS in the transduction grammar may occur in a different order is solved by introducing three special symbols, the reorder operators, which are interpreted after the parsing phase. These three operators will be written as “”, “” and “✶”. In a given string, there should be matching triples of these operators, in such a way that if there are two such triples, then they either occur in two isolated substrings, or one occurs nested between the “” and the “” or nested between the “” and the “” of the other triple. The interpretation of an occurrence of a </context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>A.V. Aho and J.D. Ullman. 1972. Parsing, volume 1 of The Theory of Parsing, Translation and Compiling. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Bangalore</author>
<author>S Douglas</author>
</authors>
<title>Learning dependency translation models as collections of finite-state head transducers.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="958" citStr="Alshawi et al., 2000" startWordPosition="130" endWordPosition="133">tively learned from examples. This paper investigates the approximation of such transduction by means of weighted rational transduction. The advantage is increased processing speed, which benefits realtime applications involving spoken language. 1 Introduction Several studies have investigated automatic or partly automatic learning of transductions for machine translation. Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000). In this paper we will investigate both contextfree and finite-state models. The basis for our study is context-free transduction since that is a powerful model of translation, which can in many cases adequately describe the changes of word ✁The second address is the current contact address; supported by the Royal Netherlands Academy of Arts and Sciences; current secondary affiliation is the</context>
<context position="3414" citStr="Alshawi et al., 2000" startWordPosition="517" endWordPosition="520"> existing literature, making no more than small changes in presentation. In Section 3 we explain how context-free transduction grammars can be represented by ordinary context-free grammars, plus a phase of postprocessing. The approximation is discussed in Section 4. As shown in Section 5, we may easily process input in a robust way, ensuring we always obtain output. Section 6 discusses empirical results, and we end the paper with conclusions. 2 Preliminaries 2.1 hierarchical alignment The input to our algorithm is a corpus consisting of pairs of sentences related by an hierarchical alignment (Alshawi et al., 2000). In what follows, the formalization of this concept has been slightly changed with respect to the above reference, to suit our purposes in the remainder of this article. The hierarchically aligned sentence pairs in the corpus are 5-tuples satisfying the following. The first two components, and , are strings, called the source string and the target string, respectively, the lengths of which are denoted by and . We let and denote the sets of string positions and respectively. Further, (resp. ) is a mapping from positions in (resp. ) to pairs of lists of positions from (resp. ), satisfying the f</context>
<context position="5618" citStr="Alshawi et al., 2000" startWordPosition="913" endWordPosition="916">d been reordered to make and projective.) Furthermore, a reasonable hierarchical alignment satisfies , imply and imply ; in other words, a position in one string is related to at most one position in the other. Furthermore, for each there is a pair such that occurs in one of the two lists of and occurs in one of the two lists of ; this means that positions can only be related if their respective “mother” positions are related. Note that this paper does not discuss how hierarchical alignments can be obtained from unannotated corpora of bitexts. This is the subject of existing studies, such as (Alshawi et al., 2000). 2.2 context-free transduction Context-free transduction was originally called syntax-directed transduction in (Lewis II and Stearns, 1968), but since in modern formal language theory and computational linguistics the term “syntax” has a much wider range of meanings than just “context-free syntax”, we will not use the original term here. A (context-free) transduction grammar is a 5- tuple , where is a finite set of nonterminals, is the start symbol, and are the source and target alphabets, and is a finite set of productions of the form 1Note that we ignore the case that a single nonterminal o</context>
<context position="9617" citStr="Alshawi et al., 2000" startWordPosition="1611" endWordPosition="1615">o some position from , which we will call . Then the production Subj-IObj “like” Obj-Subj Obj-Subj Subj-IObj “plait” Subj-IObj “I” “me” Obj-Subj “him” “il” larly, suppose that is given by For convenience, we also allow productions of the form: where and . In the experiments in Section 6, we also consider nonterminals that are lexicalized only by the source alphabet, which means that these nonterminals can be written as , where . The motivation is to restrict the grammar size and to increase the coverage. Bilexical transduction grammars are equivalent to the dependency transduction model from (Alshawi et al., 2000). Note that both halves of the RHS contain the same nonterminals but possibly in a different order. However, if any position in or is not related to some other position by , then the production above contains, instead of a nonterminal, a substring on which that position is projected by or , respectively. E.g. if there is no position such that , then instead of we have the string . In general, we cannot adapt the above algorithm to produce transduction grammars that are bilexical. For example, a production of the form: cannot be broken up into smaller, bilexical productions.2 However, the hiera</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning dependency translation models as collections of finite-state head transducers. Computational Linguistics, 26(1):45–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>G Riccardi</author>
</authors>
<title>A finite-state approach to machine translation.</title>
<date>2001</date>
<booktitle>In 2nd Meeting of the North American Chapter of the ACL,</booktitle>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="17857" citStr="Bangalore and Riccardi, 2001" startWordPosition="3003" endWordPosition="3006"> transitions are only applied if all others fail. 3. The driver of the automaton is changed so that it restarts at the initial state when it gets stuck at some input word, and when necessary, that input word is deleted. The output string with the lowest weight obtained so far (preferably attached to final states, or to other states with outgoing transitions labelled by input symbols) is then concatenated with the output string resulting from processing subsequent input. 6 Experiments We have investigated a corpus of English/Japanese sentence pairs, related by hierarchical alignment (see also (Bangalore and Riccardi, 2001)). We have taken the first 500, 1000, 1500, ... aligned sentence pairs from this corpus to act as training corpora of varying sizes; we have taken 300 other sentence pairs to act as test corpus. We have constructed a bilexical transduction grammar from each training corpus, in the form of a context-free grammar, and this grammar was approximated by a finite automaton. The input sentences from the test corpus were then processed by context-free and finite-state machinery (in the sequel referred to by cfg and fa, respectively). We have also carried out experiments with robust finite-state proces</context>
<context position="19264" citStr="Bangalore and Riccardi, 2001" startWordPosition="3252" endWordPosition="3255">ting output strings were applied in a robust way as explained in Section 5. The output strings were then compared to the reference output from the corpus, resulting in Figure 1. Our metric is word accuracy, which is based on edit distance. For a pair of strings, the edit distance is defined as the minimum number of substitutions, insertions and deletions needed to turn one string into the other. The word accuracy of a string with regard to a string is defined to be , where is the edit distance between and and is the length of . To allow a comparison with more established techniques (see e.g. (Bangalore and Riccardi, 2001)), we also take into consideration a simple bigram model, trained on the strings comprising both source and target sentences and reorder operators, as explained in Section 4. For the purposes of predicting output symbols, a series of consecutive target symbols and reorder operators following a source symbol in the training sentences are treated as a single symbol by the bigram model, and only those may be output after that source symbol. Since our construction is such that target symbols always follow source symbols they are a translation of (according to the automatically obtained hierarchica</context>
</contexts>
<marker>Bangalore, Riccardi, 2001</marker>
<rawString>S. Bangalore and G. Riccardi. 2001. A finite-state approach to machine translation. In 2nd Meeting of the North American Chapter of the ACL, Pittsburgh, PA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<marker>Brown, 1990</marker>
<rawString>P.F. Brown et al. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the ACL,</booktitle>
<pages>457--464</pages>
<location>Maryland,</location>
<contexts>
<context position="2273" citStr="Eisner and Satta, 1999" startWordPosition="342" endWordPosition="345"> the selection of appropriate lexical items. Furthermore, for limited domains, automatic learning of weighted context-free transductions from examples seems to be reasonably successful. However, practical algorithms for computing the most likely context-free derivation have a cubic time complexity, in terms of the length of the input string, or in the case of a graph output by a speech recognizer, in terms of the number of nodes in the graph. For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999). This may pose problems, especially for real-time speech systems. Therefore, we have investigated approximation of weighted context-free transduction by means of weighted rational transduction. The finite-state machinery for implementing the latter kind of transduction in general allows faster processing. We can also more easily obtain robustness. We hope the approximating model is able to preserve some of the accuracy of the context-free model. In the next section, we discuss preliminary definitions, adapted from existing literature, making no more than small changes in presentation. In Sect</context>
<context position="7586" citStr="Eisner and Satta, 1999" startWordPosition="1260" endWordPosition="1263">RHS pair by its second part , we would obtain a context-free grammar for the target language. The combination of the two halves of such a RHS indicates how a parse for the source language can be related to a parse for the target language, and this defines a transduction between the languages in an obvious way. An example of a transduction grammar is: This transduction defines that a sentence “I like him” can be translated by “il me plait”. We can reduce the generative power of contextfree transduction grammars by a syntactic restriction that corresponds to the bilexical context-free grammars (Eisner and Satta, 1999). Let us define a bilexical transduction grammar as a transduction grammar which is such that: there is a mapping from the set of nonterminals to . Due to this property, we may write each nonterminal as to indicate that it is mapped to the pair , where and , where is a so called delexicalized nonterminal. We may write as , where is a dummy symbol at the dummy string position . Further, each production is of one of the following five forms: 2.3 obtaining a context-free transduction from the corpus We extract a context-free transduction grammar from a corpus of hierarchical alignments, by locall</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In 37th Annual Meeting of the ACL, pages 457–464, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Lewis</author>
<author>R E Stearns</author>
</authors>
<title>Syntax-directed transduction.</title>
<date>1968</date>
<journal>Journal of the ACM,</journal>
<volume>15</volume>
<issue>3</issue>
<marker>Lewis, Stearns, 1968</marker>
<rawString>P.M. Lewis II and R.E. Stearns. 1968. Syntax-directed transduction. Journal of the ACM, 15(3):465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>M-J Nederhof</author>
</authors>
<title>Regular approximation of context-free grammars through transformation.</title>
<date>2001</date>
<booktitle>Robustness in Language and Speech Technology,</booktitle>
<pages>153--163</pages>
<editor>In J.-C. Junqua and G. van Noord, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="14850" citStr="Mohri and Nederhof, 2001" startWordPosition="2505" endWordPosition="2508">mly as input symbols for the purpose of determinizing and minimizing. This means that the driver for the finite automaton still encounters nondeterminism while processing an input string, since a state may have several outgoing transitions for different output symbols. Furthermore, we ignore any weights that might be attached to the context-free productions, since determinization is problematic for weighted automata in general and in particular for the type of automaton that we would obtain when carrying over the weights from the context-free grammar onto the approximating language following (Mohri and Nederhof, 2001). Instead, weights for the transitions of the finite automaton are obtained by training, using strings that are produced as a side effect of the computation of the grammar from the corpus. These strings contain the symbols from both the source and target strings mixed together, plus occurrences of the reorder operators where needed. A English/French example might be: I me like plait him il The way these strings were obtained ensures that they are included in the language generated by the context-free grammar, and they are therefore also accepted by the approximating automaton due to properties</context>
</contexts>
<marker>Mohri, Nederhof, 2001</marker>
<rawString>M. Mohri and M.-J. Nederhof. 2001. Regular approximation of context-free grammars through transformation. In J.-C. Junqua and G. van Noord, editors, Robustness in Language and Speech Technology, pages 153–163. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>An optimal tabular parsing algorithm.</title>
<date>1994</date>
<booktitle>In 32nd Annual Meeting of the ACL,</booktitle>
<pages>117--124</pages>
<location>Las Cruces, New Mexico,</location>
<contexts>
<context position="21596" citStr="Nederhof, 1994" startWordPosition="3635" endWordPosition="3636">gure took more than 24 hours for both fa(2) and robust fa(2) , which suggests these methods become unrealistic for training corpus sizes considerably larger than 10,000 bitexts. 7 Conclusions For our application, context-free transduction has a relatively high accuracy, but it also has a high time consumption, and it may be difficult to obtain robustness without further increasing the time costs. These are two major obstacles for use in spoken language systems. We have tried to obtain a rational transduction that approximates a 4It uses a trie to represent productions (similar to ELR parsing (Nederhof, 1994)), postponing generation of output for a production until all nonterminals and all input symbols from the right-hand side have been found. training corpus size Figure 1: Average word accuracy for transduced sentences. training corpus size Figure 2: Fraction of the sentences that were transduced. 1 ➦ word accuracy c ➦ 0.6 0.5 cfg2 cfg fa2 fa bigram robust_fa2 robust_fa 0.4 0.3 0.2 0 1000 2000 3000 4000 5000 6000 7000 8000 0.9 0.8 0.7 0 1000 2000 3000 4000 5000 6000 7000 8000 ➧ accepted e ➧ 0.8 0.6 0.4 0.2 0 1 fa fa2 cfg cfg2 context-free transduction, preserving some of its accuracy. Our experi</context>
</contexts>
<marker>Nederhof, 1994</marker>
<rawString>M.-J. Nederhof. 1994. An optimal tabular parsing algorithm. In 32nd Annual Meeting of the ACL, pages 117–124, Las Cruces, New Mexico, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>Practical experiments with regular approximation of context-free languages.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="13013" citStr="Nederhof, 2000" startWordPosition="2195" endWordPosition="2196">owing two context-free productions: In the first production, the RHS nonterminals occur in the same order as in the left half of the original production, but reorder operators have been added to indicate that, after parsing, some substrings of the output string are to be reordered. Our reorder operators are similar to the two operators and from (Vilar and others, 1999), but the former are more powerful, since the latter allow only single words to be moved instead of whole phrases. 4 Finite-state approximation There are several methods to approximate context-free grammars by regular languages (Nederhof, 2000). We will consider here only the so called RTN method, which is applied in a simplified form.3 3As opposed to (Nederhof, 2000), we assume here that all nonterminals are mutually recursive, and the grammar contains self-embedding. We have observed that typical grammars that we obtain in the context of this article indeed have the property that almost all nonterminals belong to the same mutually recursive set. A finite automaton is constructed as follows. For each nonterminal from the grammar we introduce two states and . For each production we introduce states , and we add epsilon transitions f</context>
</contexts>
<marker>Nederhof, 2000</marker>
<rawString>M.-J. Nederhof. 2000. Practical experiments with regular approximation of context-free languages. Computational Linguistics, 26(1):17–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Word re-ordering and DP-based search in statistical machine translation.</title>
<date>2000</date>
<booktitle>In The 18th International Conference on Computational Linguistics,</booktitle>
<pages>850--856</pages>
<location>Saarbr¨ucken, July–August.</location>
<contexts>
<context position="1163" citStr="Tillmann and Ney, 2000" startWordPosition="165" endWordPosition="168">ime applications involving spoken language. 1 Introduction Several studies have investigated automatic or partly automatic learning of transductions for machine translation. Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000). In this paper we will investigate both contextfree and finite-state models. The basis for our study is context-free transduction since that is a powerful model of translation, which can in many cases adequately describe the changes of word ✁The second address is the current contact address; supported by the Royal Netherlands Academy of Arts and Sciences; current secondary affiliation is the German Research Center for Artificial Intelligence (DFKI). order between two languages, and the selection of appropriate lexical items. Furthermore, for limited domains, automatic learning of weighted con</context>
</contexts>
<marker>Tillmann, Ney, 2000</marker>
<rawString>C. Tillmann and H. Ney. 2000. Word re-ordering and DP-based search in statistical machine translation. In The 18th International Conference on Computational Linguistics, pages 850–856, Saarbr¨ucken, July–August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Vilar</author>
</authors>
<title>Text and speech translation by means of subsequential transducers.</title>
<date>1999</date>
<booktitle>Extended finite state models of language,</booktitle>
<pages>121--139</pages>
<editor>In A. Kornai, editor,</editor>
<publisher>Cambridge University Press.</publisher>
<marker>Vilar, 1999</marker>
<rawString>J.M. Vilar et al. 1999. Text and speech translation by means of subsequential transducers. In A. Kornai, editor, Extended finite state models of language, pages 121–139. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Watanabe</author>
<author>S Kurohashi</author>
<author>E Aramaki</author>
</authors>
<title>Finding structural correspondences from bilingual parsed corpus for corpus-based translation.</title>
<date>2000</date>
<booktitle>In The 18th International Conference on Computational Linguistics,</booktitle>
<pages>906--912</pages>
<location>Saarbr¨ucken,</location>
<contexts>
<context position="981" citStr="Watanabe et al., 2000" startWordPosition="134" endWordPosition="137">amples. This paper investigates the approximation of such transduction by means of weighted rational transduction. The advantage is increased processing speed, which benefits realtime applications involving spoken language. 1 Introduction Several studies have investigated automatic or partly automatic learning of transductions for machine translation. Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000). In this paper we will investigate both contextfree and finite-state models. The basis for our study is context-free transduction since that is a powerful model of translation, which can in many cases adequately describe the changes of word ✁The second address is the current contact address; supported by the Royal Netherlands Academy of Arts and Sciences; current secondary affiliation is the German Research Center</context>
</contexts>
<marker>Watanabe, Kurohashi, Aramaki, 2000</marker>
<rawString>H. Watanabe, S. Kurohashi, and E. Aramaki. 2000. Finding structural correspondences from bilingual parsed corpus for corpus-based translation. In The 18th International Conference on Computational Linguistics, pages 906–912, Saarbr¨ucken, July– August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamamoto</author>
<author>Y Matsumoto</author>
</authors>
<title>Acquisition of phrase-level bilingual correspondence using dependency structure.</title>
<date>2000</date>
<booktitle>In The 18th International Conference on Computational Linguistics,</booktitle>
<pages>933--939</pages>
<location>Saarbr¨ucken, July–August.</location>
<contexts>
<context position="1012" citStr="Yamamoto and Matsumoto, 2000" startWordPosition="138" endWordPosition="142">stigates the approximation of such transduction by means of weighted rational transduction. The advantage is increased processing speed, which benefits realtime applications involving spoken language. 1 Introduction Several studies have investigated automatic or partly automatic learning of transductions for machine translation. Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000). In this paper we will investigate both contextfree and finite-state models. The basis for our study is context-free transduction since that is a powerful model of translation, which can in many cases adequately describe the changes of word ✁The second address is the current contact address; supported by the Royal Netherlands Academy of Arts and Sciences; current secondary affiliation is the German Research Center for Artificial Intelligence (D</context>
</contexts>
<marker>Yamamoto, Matsumoto, 2000</marker>
<rawString>K. Yamamoto and Y. Matsumoto. 2000. Acquisition of phrase-level bilingual correspondence using dependency structure. In The 18th International Conference on Computational Linguistics, pages 933– 939, Saarbr¨ucken, July–August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>