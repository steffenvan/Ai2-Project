<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006185">
<title confidence="0.986456">
Using Lexical Expansion to Learn Inference Rules from Sparse Data
</title>
<author confidence="0.961648">
Oren Melamud§, Ido Dagan§, Jacob Goldberger†, Idan Szpektor‡
</author>
<affiliation confidence="0.602998">
§ Computer Science Department, Bar-Ilan University
† Faculty of Engineering, Bar-Ilan University
‡ Yahoo! Research Israel
</affiliation>
<email confidence="0.4818415">
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
idan@yahoo-inc.com
</email>
<sectionHeader confidence="0.991674" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999894733333333">
Automatic acquisition of inference rules
for predicates is widely addressed by com-
puting distributional similarity scores be-
tween vectors of argument words. In
this scheme, prior work typically refrained
from learning rules for low frequency
predicates associated with very sparse ar-
gument vectors due to expected low reli-
ability. To improve the learning of such
rules in an unsupervised way, we propose
to lexically expand sparse argument word
vectors with semantically similar words.
Our evaluation shows that lexical expan-
sion significantly improves performance
in comparison to state-of-the-art baselines.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996152507936508">
The benefit of utilizing template-based inference
rules between predicates was demonstrated in
NLP tasks such as Question Answering (QA)
(Ravichandran and Hovy, 2002) and Information
Extraction (IE) (Shinyama and Sekine, 2006). For
example, the inference rule ‘X treat Y → X relieve
Y’, between the templates ‘X treat Y’ and ‘X re-
lieve Y’ may be useful to identify the answer to
“Which drugs relieve stomach ache?”.
The predominant unsupervised approach for
learning inference rules between templates is via
distributional similarity (Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Szpektor and Da-
gan, 2008). Specifically, each argument slot in
a template is represented by an argument vector,
containing the words (or terms) that instantiate this
slot in all of the occurrences of the template in a
learning corpus. Two templates are then deemed
semantically similar if the argument vectors of
their corresponding slots are similar.
Ideally, inference rules should be learned for
all templates that occur in the learning corpus.
However, many templates are rare and occur only
few times in the corpus. This is a typical NLP
phenomenon that can be associated with either a
small learning corpus, as in the cases of domain
specific corpora and resource-scarce languages, or
with templates with rare terms or long multi-word
expressions such as ‘X be also a risk factor to Y’
or ‘X finish second in Y’, which capture very spe-
cific meanings. Due to few occurrences, the slots
of rare templates are represented with very sparse
argument vectors, which in turn lead to low relia-
bility in distributional similarity scores.
A common practice in prior work for learn-
ing predicate inference rules is to simply disre-
gard templates below a minimal frequency thresh-
old (Lin and Pantel, 2001; Kotlerman et al., 2010;
Dinu and Lapata, 2010; Ritter et al., 2010). Yet,
acquiring rules for rare templates may be benefi-
cial both in terms of coverage, but also in terms
of more accurate rule application, since rare tem-
plates are less ambiguous than frequent ones.
We propose to improve the learning of rules be-
tween infrequent templates by expanding their ar-
gument vectors. This is done via a “dual” distribu-
tional similarity approach, in which we consider
two words to be similar if they instantiate similar
sets of templates. We then use these similarities
to expand the argument vector of each slot with
words that were identified as similar to the original
arguments in the vector. Finally, similarities be-
tween templates are computed using the expanded
vectors, resulting in a ‘smoothed’ version of the
original similarity measure.
Evaluations on a rule application task show
that our lexical expansion approach significantly
improves the performance of the state-of-the-art
DIRT algorithm (Lin and Pantel, 2001). In addi-
tion, our approach outperforms a similarity mea-
sure based on vectors of latent topics instead of
word vectors, a common way to avoid sparseness
issues by means of dimensionality reduction.
</bodyText>
<page confidence="0.979463">
283
</page>
<note confidence="0.528636">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 283–288,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.962417" genericHeader="method">
2 Technical Background
</sectionHeader>
<bodyText confidence="0.999985322580645">
The distributional similarity score for an inference
rule between two predicate templates, e.g. ‘X re-
sign Y → X quit Y’, is typically computed by mea-
suring the similarity between the argument vec-
tors of the corresponding X slots and Y slots of
the two templates. To this end, first the argument
vectors should be constructed and then a similarity
measure between two vectors should be provided.
We note that we focus here on binary templates
with two slots each, but this approach can be ap-
plied to any template.
A common starting point is to compute a
co-occurrence matrix M from a learning cor-
pus. M’s rows correspond to the template slots
and the columns correspond to the various terms
that instantiate the slots. Each entry Mi,j, e.g.
Mx quit,John, contains a count of the number of
times the term j instantiated the template slot i in
the corpus. Thus, each row Mi,* corresponds to
an argument vector for slot i. Next, some func-
tion of the counts is used to assign weights to all
Mi,j entries. In this paper we use pointwise mu-
tual information (PMI), which is common in prior
work (Lin and Pantel, 2001; Szpektor and Dagan,
2008).
Finally, rules are assessed using some similar-
ity measure between corresponding argument vec-
tors. The state-of-the-art DIRT algorithm (Lin and
Pantel, 2001) uses the highly cited Lin similarity
measures (Lin, 1998) to score rules between bi-
nary templates as follows:
</bodyText>
<equation confidence="0.99850075">
Lin(v, v&apos;) = EwEvnvl [v(w) + v&apos;(w)] (1)
EwEvUvl [v (w) + v&apos;(w)]
DIRT(l → r)
�= Lin(vl:x, vr:x) · Lin(vl:y, vr:y)
</equation>
<bodyText confidence="0.998800266666667">
where v and v&apos; are two argument vectors, l and
r are the templates participating in the inference
rule and vl:x corresponds to the argument vector
of slot X of template l, etc. While the original
DIRT algorithm utilizes the Lin measure, one can
replace it with any other vector similarity measure.
A separate line of research for word simi-
larity introduced directional similarity measures
that have a bias for identifying generaliza-
tion/specification relations, i.e. relations be-
tween predicates with narrow (or specific) seman-
tic meanings to predicates with broader meanings
inferred by them (unlike the symmetric Lin). One
such example is the Cover measure (Weeds and
Weir, 2003):
</bodyText>
<equation confidence="0.71321">
Cover (v,v&apos;) = EwEvnvl[v(w)] (3)
EwEvUvl [v(w)]
</equation>
<bodyText confidence="0.9998718">
As can be seen, in the core of the Lin and Cover
measures, as well as in many other well known
distributional similarity measures such as Jaccard,
Dice and Cosine, stand the number of shared ar-
guments vs. the total number of arguments in the
two vectors. Therefore, when the argument vec-
tors are sparse, containing very few non-zero fea-
tures, these scores become unreliable and volatile,
changing greatly with every inclusion or exclusion
of a single shared argument.
</bodyText>
<sectionHeader confidence="0.994174" genericHeader="method">
3 Lexical Expansion Scheme
</sectionHeader>
<bodyText confidence="0.980817382352941">
We wish to overcome the sparseness issues in rare
feature vectors, especially in cases where argu-
ment vectors of semantically similar predicates
comprise similar but not exactly identical argu-
ments. To this end, we propose a three step
scheme. First, we learn lexical expansion sets for
argument words, such as the set {euros, money}
for the word dollars. Then we use these sets to ex-
pand the argument word vectors of predicate tem-
plates. For example, given the template ‘X can
be exchanged for Y’, with the following argument
words instantiating slot X {dollars, gold}, and
the expansion set above, we would expand the ar-
gument word vector to include all the following
words {dollars, euros, money, gold}. Finally, we
use the expanded argument word vectors to com-
pute the scores for predicate inference rules with a
given similarity measure.
When a template is instantiated with an ob-
served word, we expect it to also be instantiated
with semantically similar words such as the ones
in the expansion set of the observed word. We
“blame” the lack of such template occurrences
only on the size of the corpus and the sparseness
phenomenon in natural languages. Thus, we uti-
lize our lexical expansion scheme to synthetically
add these expected but missing occurrences, ef-
fectively smoothing or generalizing over the ex-
plicitly observed argument occurrences. Our ap-
proach is inspired by query expansion (Voorhees,
1994) in Information Retrieval (IR), as well as by
the recent lexical expansion framework proposed
in (Biemann and Riedl, 2013), and the work by
(2)
</bodyText>
<page confidence="0.986444">
284
</page>
<bodyText confidence="0.9998242">
Miller et al. (2012) on word sense disambigua-
tion. Yet, to the best of our knowledge, this is the
first work that applies lexical expansion to distri-
butional similarity feature vectors. We next de-
scribe our scheme in detail.
</bodyText>
<subsectionHeader confidence="0.99979">
3.1 Learning Lexical Expansions
</subsectionHeader>
<bodyText confidence="0.996616153846154">
We start by constructing the co-occurrence matrix
M (Section 2), where each entry Mt:s,w indicates
the number of times that word w instantiates slot
s of template t in the learning corpus, denoted by
’t:s’, where s can be either X or Y.
In traditional distributional similarity, the rows
Mt:s,∗ serve as argument vectors of template slots.
However, to learn expansion sets we take a “dual”
view and consider each matrix column M∗:∗,w (de-
noted vw) as a feature vector for the argument
word w. Under this view, templates (or more
specifically, template slots) are the features. For
instance, for the word dollars the respective fea-
ture vector may include entries such as ‘X can be
exchanged for’, ‘can be exchanged for Y’, ‘pur-
chase Y’ and ‘sell Y’.
We next learn an expansion set per each word
w by computing the distributional similarity be-
tween the vectors of w and any other argument
word w0, sim(vw, vw,). Then we take the N most
similar words as w’s expansion set with degree
N, denoted by LNw = {w01, ..., w0 N1. Any simi-
larity measure could be used, but as our experi-
ments show, different measures generate sets with
different properties, and some may be fitter for ar-
gument vector expansion than others.
</bodyText>
<subsectionHeader confidence="0.998207">
3.2 Expanding Argument Vectors
</subsectionHeader>
<bodyText confidence="0.999926875">
Given a row count vector Mt:s,∗ for slot s of tem-
plate t, we enrich it with expansion sets as fol-
lows. For each w in Mt:s,∗, the original count in
vt:s(w) is redistributed equally between itself and
all words in w’s expansion set, i.e. all w0 E LNw ,
(possibly yielding fractional counts) where N is a
global parameter of the model. Specifically, the
new count that is assigned to each word w is its
remaining original count after it has been redis-
tributed (or zero if no original count), plus all the
counts that were distributed to it from other words.
Next, PMI weights are recomputed according to
the new counts, and the resulting expanded vector
is denoted by v+t:s. Similarity between template
slots is now computed over the expanded vectors
instead of the original ones, e.g. Lin(v+ l:x, v+ r:x).
</bodyText>
<sectionHeader confidence="0.998386" genericHeader="method">
4 Experimental Settings
</sectionHeader>
<bodyText confidence="0.999102347826087">
We constructed a relatively small learning corpus
for investigating the sparseness issues of such cor-
pora. To this end, we used a random sample from
the large scale web-based ReVerb corpus1 (Fader
et al., 2011), comprising tuple extractions of pred-
icate templates with their argument instantiations.
We applied some clean-up preprocessing to these
extractions, discarding stop words, rare words and
non-alphabetical words that instantiated either the
X or the Y argument slots. In addition, we dis-
carded templates that co-occur with less than 5
unique argument words in either of their slots, as-
suming that such few arguments cannot convey re-
liable semantic information, even with expansion.
Our final corpus consists of around 350,000 ex-
tractions and 14,000 unique templates. In this cor-
pus around one third of the extractions refer to
templates that co-occur with at most 35 unique ar-
guments in both their slots.
We evaluated the quality of inference
rules using the dataset constructed by Zeich-
ner et al. (2012)2, which contains about 6,500
manually annotated template rule applications,
each labeled as correct or not. For example,
‘The game develop eye-hand coordination --/+ The
game launch eye-hand coordination’ is a rule
application in this dataset of the rule ‘X develop
Y -+ X launch Y’, labeled as incorrect, and
‘Captain Cook sail to Australia -+ Captain Cook
depart for Australia’ is a rule application of the
rule ‘X sail to Y -+ X depart for Y’, labeled as
correct. Specifically, we induced two datasets
from Zeichner et al.’s dataset, denoted DS-5-35
and DS-5-50, which consist of all rule applica-
tions whose templates are present in our learning
corpus and co-occurred with at least 5 and at
most 35 and 50 unique argument words in both
their slots, respectively. DS-5-35 includes 311
rule applications (104 correct and 207 incorrect)
and DS-5-50 includes 502 rule applications (190
correct and 312 incorrect).
Our evaluation task is to rank all rule applica-
tions in each test set based on the similarity scores
of the applied rules. Optimal performance would
rank all correct rule applications above the in-
correct ones. As a baseline for rule scoring we
</bodyText>
<footnote confidence="0.9969225">
1http://reverb.cs.washington.edu/
2http://www.cs.biu.ac.il/nlp/
downloads/annotation-rule-application.
htm
</footnote>
<page confidence="0.998454">
285
</page>
<bodyText confidence="0.9999696875">
used the DIRT algorithm scheme, denoted DIRT-
LE-None. We then compared between the perfor-
mance of this baseline and its expanded versions,
testing two similarity measures for generating the
expansion sets of arguments: Lin and Cover. We
denote these expanded methods DIRT-LE-SIM-N,
where SIM is the similarity measure used to gen-
erate the expansion sets and N is the lexical expan-
sion degree, e.g. DIRT-LE-Lin-2.
We remind the reader that our scheme utilizes
two similarity measures. The first measure as-
sesses the similarity between the argument vectors
of the two templates in the rule. This measure
is kept constant in our experiments and is iden-
tical to DIRT’s similarity measure (Lin). 3 The
second measure assesses the similarity between
words and is used for the lexical expansion of ar-
gument vectors. Since this is the research goal
of this paper, we experimented with two different
measures for lexical expansion: a symmetric mea-
sure (Lin) and an asymmetric measure (Cover).
To this end we evaluated their effect on DIRT’s
rule ranking performance and compared them to a
vanilla version of DIRT without lexical expansion.
As another baseline, we follow Dinu and La-
pata (2010) inducing LDA topic vectors for tem-
plate slots and computing predicate template infer-
ence rule scores based on similarity between these
vectors. We use standard hyperparameters for
learning the LDA model (Griffiths and Steyvers,
2004). This method is denoted LDA-K, where K is
the number of topics in the model.
</bodyText>
<sectionHeader confidence="0.999921" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999981285714286">
We evaluated the performance of each tested
method by measuring Mean Average Precision
(MAP) (Manning et al., 2008) of the rule applica-
tion ranking computed by this method. In order
to compute MAP values and corresponding sta-
tistical significance, we randomly split each test
set into 30 subsets. For each method we com-
puted Average Precision on every subset and then
took the average as the MAP value. We varied
the degree of the lexical expansion in our model
and the number of topics in the topic model base-
line to analyze their effect on the performance of
these methods on our datasets. We note that in our
model a greater degree of lexical expansion cor-
</bodyText>
<footnote confidence="0.957069666666667">
3Experiments with Cosine as the template similarity mea-
sure instead of Lin for both DIRT and its expanded versions
yielded similar results. We omit those for brevity.
</footnote>
<bodyText confidence="0.998848058823529">
responds to more aggressive smoothing (or gen-
eralization) of the explicitly observed data, while
the same goes for a lower number of topics in the
topic model. The results on DS-5-35 and DS-5-50
are illustrated in Figure 1.
The most dramatic improvement over the base-
lines is evident in DS-5-35, where DIRT-LE-
Cover-2 achieves a MAP score of 0.577 in com-
parison to 0.459 achieved by its DIRT-LE-None
baseline. This is indeed the dataset where we ex-
pected expansion to affect most due the extreme
sparseness of argument vectors. Both DIRT-LE-
Cover-N and DIRT-LE-Lin-N outperform DIRT-
LE-None for all tested values of N, with statisti-
cal significance via a paired t-test at p &lt; 0.05 for
DIRT-LE-Cover-N where 1 &lt; N &lt; 5, and p &lt;
0.01 for DIRT-LE-Cover-2. On DS-5-50, improve-
ment over the DIRT-LE-None baseline is still sig-
nificant with both DIRT-LE-Cover-N and DIRT-
LE-Lin-N outperforming DIRT-LE-None. DIRT-
LE-Cover-N again performs best and achieves a
relative improvement of over 10% with statistical
significance at p &lt; 0.05 for 2 &lt; N &lt; 3.
The above shows that expansion is effective for
improving rule learning between infrequent tem-
plates. Furthermore, the fact that DIRT-LE-Cover-
N outperforms DIRT-LE-Lin-N suggests that us-
ing directional expansions, which are biased to
generalizations of the observed argument words,
e.g. vehicle as an expansion for car, is more ef-
fective than using symmetrically related words,
such as bicycle or automobile. This conclusion
appears also to be valid from a semantic reason-
ing perspective, as given an observed predicate-
argument occurrence, such as ‘drive car’ we can
more likely infer that a presumed occurrence of
the same predicate with a generalization of the ar-
gument, such as ‘drive vehicle’, is valid, i.e. ‘drive
car -+ drive vehicle’. On the other hand while
‘drive car -+ drive automobile’ is likely to be
valid, ‘drive car -+ drive bicycle’ and ‘drive ve-
hicle -+ drive bicycle’ are not.
Figure 1 also depicts the performance of LDA
as a vector smoothing approach. LDA-K out-
performs the DIRT-LE-None baseline under DS-
5-35 but with no statistical significance. Under
DS-5-50 LDA-K performs worst, slightly outper-
forming DIRT-LE-None only for K=450. Further-
more, under both datasets, LDA-K is outperformed
by DIRT-LE-Cover-N. These results indicate that
LDA is less effective than our expansion approach.
</bodyText>
<page confidence="0.99742">
286
</page>
<figureCaption confidence="0.988533">
Figure 1: MAP scores on DS-5-35 and DS-5-50 for the original DIRT scheme, denoted DIRT-LE-None,
</figureCaption>
<bodyText confidence="0.999092095238096">
and for the compared smoothing methods as follows. DIRT with varied degrees of lexical expansion
is denoted as DIRT-LE-Lin-N and DIRT-LE-Cover-N. The topic model with varied number of topics is
denoted as LDA-K. Data labels indicate the expansion degree (N) or the number of LDA topics (K),
depending on the tested method.
One reason may be that in our model, every expan-
sion set may be viewed as a cluster around a spe-
cific word, an outstanding difference in compari-
son to topics, which provide a global partition over
all words. We note that performance improve-
ment of singleton document clusters over global
partitions was also shown in IR (Kurland and Lee,
2009).
In order to further illustrate our lexical expan-
sion scheme we focus on the rule application
‘Captain Cook sail to Australia → Captain Cook
depart for Australia’, which is labeled as correct
in our test set and corresponds to the rule ‘X sail
to Y → X depart for Y’. There are 30 words in-
stantiating the X slot of the predicate ‘sail to’
in our learning corpus including {Columbus, em-
peror, James, John, trader}. On the other hand,
there are 18 words instantiating the X slot of the
predicate ‘depart for’ including {Amanda, Jerry,
Michael, mother, queen}. While semantic simi-
larity between these two sets of words is evident,
they share no words in common, and therefore the
original DIRT algorithm, DIRT-LE-None, wrongly
assigns a zero score to the rule.
The following are descriptions of some of the
argument word expansions performed by DIRT-
LE-Cover-2 (using the notation LNw defined in Sec-
tion 3.1) for the X slot of ‘sail to’ L2John = {mr.,
dr.}, L2 trader = {people, man}, and for the X slot
of ‘depart for’, L2Michael = {John, mr.}, L2mother =
{people, woman}. Given these expansions the two
slots now share the following words {mr. ,people,
John} and the rule score becomes positive.
It is also interesting to compare the expansions
performed by DIRT-LE-Lin-2 to the above. For
instance in this case L2mother = {father, sarah},
which does not identify people as a shared argu-
ment for the rule.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999749">
We propose to improve the learning of infer-
ence rules between infrequent predicate templates
with sparse argument vectors by utilizing a novel
scheme that lexically expands argument vectors
with semantically similar words. Similarities be-
tween argument words are discovered using a dual
distributional representation, in which templates
are the features.
We tested the performance of our expansion
approach on rule application datasets that were
biased towards rare templates. Our evaluation
showed that rule learning with expanded vectors
outperformed the baseline learning with original
vectors. It also outperformed an LDA-based simi-
larity model that overcomes sparseness via dimen-
sionality reduction.
In future work we plan to investigate how our
scheme performs when integrated with manually
constructed resources for lexical expansion, such
as WordNet (Fellbaum, 1998).
</bodyText>
<sectionHeader confidence="0.998887" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.918375833333334">
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, and
the European Community’s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
</bodyText>
<page confidence="0.984486">
287
</page>
<bodyText confidence="0.9981386">
Ellen M Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SIGIR.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
</bodyText>
<sectionHeader confidence="0.889702" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.97560275">
Chris Biemann and Martin Riedl. 2013. Text: Now
in 2d! a framework for lexical expansion with con-
textual similarity. Journal of Language Modeling,
1(1).
</bodyText>
<reference confidence="0.998244326530612">
Georgiana Dinu and Mirella Lapata. 2010. Topic mod-
els for meaning similarity in context. In Proceedings
of COLING: Posters.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228–5235.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359–389.
Oren Kurland and Lillian Lee. 2009. Clusters, lan-
guage models, and ad hoc information retrieval.
ACM Transactions on Information Systems (TOIS),
27(3):13.
Dekang Lin and Patrick Pantel. 2001. DIRT – discov-
ery of inference rules from text. In Proceedings of
KDD.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. Proceedings of COLING,
Mumbai, India.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Alan Ritter, Oren Etzioni, et al. 2010. A latent dirich-
let allocation method for selectional preferences. In
Proceedings of ACL.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of NAACL.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
Proceedings ofACL (short papers).
</reference>
<page confidence="0.997103">
288
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.223632">
<title confidence="0.999977">Using Lexical Expansion to Learn Inference Rules from Sparse Data</title>
<author confidence="0.640606">Ido Jacob Idan Science Department</author>
<author confidence="0.640606">Bar-Ilan of Engineering</author>
<author confidence="0.640606">Bar-Ilan</author>
<affiliation confidence="0.468169">Research Israel</affiliation>
<email confidence="0.999795">idan@yahoo-inc.com</email>
<abstract confidence="0.9997549375">Automatic acquisition of inference rules for predicates is widely addressed by computing distributional similarity scores between vectors of argument words. In this scheme, prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability. To improve the learning of such rules in an unsupervised way, we propose to lexically expand sparse argument word vectors with semantically similar words. Our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for meaning similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING:</booktitle>
<publisher>Posters.</publisher>
<contexts>
<context position="2777" citStr="Dinu and Lapata, 2010" startWordPosition="419" endWordPosition="422">e cases of domain specific corpora and resource-scarce languages, or with templates with rare terms or long multi-word expressions such as ‘X be also a risk factor to Y’ or ‘X finish second in Y’, which capture very specific meanings. Due to few occurrences, the slots of rare templates are represented with very sparse argument vectors, which in turn lead to low reliability in distributional similarity scores. A common practice in prior work for learning predicate inference rules is to simply disregard templates below a minimal frequency threshold (Lin and Pantel, 2001; Kotlerman et al., 2010; Dinu and Lapata, 2010; Ritter et al., 2010). Yet, acquiring rules for rare templates may be beneficial both in terms of coverage, but also in terms of more accurate rule application, since rare templates are less ambiguous than frequent ones. We propose to improve the learning of rules between infrequent templates by expanding their argument vectors. This is done via a “dual” distributional similarity approach, in which we consider two words to be similar if they instantiate similar sets of templates. We then use these similarities to expand the argument vector of each slot with words that were identified as simil</context>
<context position="14264" citStr="Dinu and Lapata (2010)" startWordPosition="2313" endWordPosition="2317">two templates in the rule. This measure is kept constant in our experiments and is identical to DIRT’s similarity measure (Lin). 3 The second measure assesses the similarity between words and is used for the lexical expansion of argument vectors. Since this is the research goal of this paper, we experimented with two different measures for lexical expansion: a symmetric measure (Lin) and an asymmetric measure (Cover). To this end we evaluated their effect on DIRT’s rule ranking performance and compared them to a vanilla version of DIRT without lexical expansion. As another baseline, we follow Dinu and Lapata (2010) inducing LDA topic vectors for template slots and computing predicate template inference rule scores based on similarity between these vectors. We use standard hyperparameters for learning the LDA model (Griffiths and Steyvers, 2004). This method is denoted LDA-K, where K is the number of topics in the model. 5 Results We evaluated the performance of each tested method by measuring Mean Average Precision (MAP) (Manning et al., 2008) of the rule application ranking computed by this method. In order to compute MAP values and corresponding statistical significance, we randomly split each test se</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Topic models for meaning similarity in context. In Proceedings of COLING: Posters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="11007" citStr="Fader et al., 2011" startWordPosition="1796" endWordPosition="1799">count after it has been redistributed (or zero if no original count), plus all the counts that were distributed to it from other words. Next, PMI weights are recomputed according to the new counts, and the resulting expanded vector is denoted by v+t:s. Similarity between template slots is now computed over the expanded vectors instead of the original ones, e.g. Lin(v+ l:x, v+ r:x). 4 Experimental Settings We constructed a relatively small learning corpus for investigating the sparseness issues of such corpora. To this end, we used a random sample from the large scale web-based ReVerb corpus1 (Fader et al., 2011), comprising tuple extractions of predicate templates with their argument instantiations. We applied some clean-up preprocessing to these extractions, discarding stop words, rare words and non-alphabetical words that instantiated either the X or the Y argument slots. In addition, we discarded templates that co-occur with less than 5 unique argument words in either of their slots, assuming that such few arguments cannot convey reliable semantic information, even with expansion. Our final corpus consists of around 350,000 extractions and 14,000 unique templates. In this corpus around one third o</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="20819" citStr="Fellbaum, 1998" startWordPosition="3390" endWordPosition="3391">are discovered using a dual distributional representation, in which templates are the features. We tested the performance of our expansion approach on rule application datasets that were biased towards rare templates. Our evaluation showed that rule learning with expanded vectors outperformed the baseline learning with original vectors. It also outperformed an LDA-based similarity model that overcomes sparseness via dimensionality reduction. In future work we plan to investigate how our scheme performs when integrated with manually constructed resources for lexical expansion, such as WordNet (Fellbaum, 1998). Acknowledgments This work was partially supported by the Israeli Ministry of Science and Technology grant 3-8705, the Israel Science Foundation grant 880/12, and the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). 287 Ellen M Voorhees. 1994. Query expansion using lexical-semantic relations. In Proceedings of SIGIR. Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Proceedings of EMNLP. References Chris Biemann and Martin Riedl. 2013. Text: Now in 2d! a framework for lexical expansion with co</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National academy of Sciences of the United States of America, 101(Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="14498" citStr="Griffiths and Steyvers, 2004" startWordPosition="2349" endWordPosition="2352">f argument vectors. Since this is the research goal of this paper, we experimented with two different measures for lexical expansion: a symmetric measure (Lin) and an asymmetric measure (Cover). To this end we evaluated their effect on DIRT’s rule ranking performance and compared them to a vanilla version of DIRT without lexical expansion. As another baseline, we follow Dinu and Lapata (2010) inducing LDA topic vectors for template slots and computing predicate template inference rule scores based on similarity between these vectors. We use standard hyperparameters for learning the LDA model (Griffiths and Steyvers, 2004). This method is denoted LDA-K, where K is the number of topics in the model. 5 Results We evaluated the performance of each tested method by measuring Mean Average Precision (MAP) (Manning et al., 2008) of the rule application ranking computed by this method. In order to compute MAP values and corresponding statistical significance, we randomly split each test set into 30 subsets. For each method we computed Average Precision on every subset and then took the average as the MAP value. We varied the degree of the lexical expansion in our model and the number of topics in the topic model baseli</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National academy of Sciences of the United States of America, 101(Suppl 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="2754" citStr="Kotlerman et al., 2010" startWordPosition="415" endWordPosition="418">earning corpus, as in the cases of domain specific corpora and resource-scarce languages, or with templates with rare terms or long multi-word expressions such as ‘X be also a risk factor to Y’ or ‘X finish second in Y’, which capture very specific meanings. Due to few occurrences, the slots of rare templates are represented with very sparse argument vectors, which in turn lead to low reliability in distributional similarity scores. A common practice in prior work for learning predicate inference rules is to simply disregard templates below a minimal frequency threshold (Lin and Pantel, 2001; Kotlerman et al., 2010; Dinu and Lapata, 2010; Ritter et al., 2010). Yet, acquiring rules for rare templates may be beneficial both in terms of coverage, but also in terms of more accurate rule application, since rare templates are less ambiguous than frequent ones. We propose to improve the learning of rules between infrequent templates by expanding their argument vectors. This is done via a “dual” distributional similarity approach, in which we consider two words to be similar if they instantiate similar sets of templates. We then use these similarities to expand the argument vector of each slot with words that w</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Kurland</author>
<author>Lillian Lee</author>
</authors>
<title>Clusters, language models, and ad hoc information retrieval.</title>
<date>2009</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="18526" citStr="Kurland and Lee, 2009" startWordPosition="3019" endWordPosition="3022">llows. DIRT with varied degrees of lexical expansion is denoted as DIRT-LE-Lin-N and DIRT-LE-Cover-N. The topic model with varied number of topics is denoted as LDA-K. Data labels indicate the expansion degree (N) or the number of LDA topics (K), depending on the tested method. One reason may be that in our model, every expansion set may be viewed as a cluster around a specific word, an outstanding difference in comparison to topics, which provide a global partition over all words. We note that performance improvement of singleton document clusters over global partitions was also shown in IR (Kurland and Lee, 2009). In order to further illustrate our lexical expansion scheme we focus on the rule application ‘Captain Cook sail to Australia → Captain Cook depart for Australia’, which is labeled as correct in our test set and corresponds to the rule ‘X sail to Y → X depart for Y’. There are 30 words instantiating the X slot of the predicate ‘sail to’ in our learning corpus including {Columbus, emperor, James, John, trader}. On the other hand, there are 18 words instantiating the X slot of the predicate ‘depart for’ including {Amanda, Jerry, Michael, mother, queen}. While semantic similarity between these t</context>
</contexts>
<marker>Kurland, Lee, 2009</marker>
<rawString>Oren Kurland and Lillian Lee. 2009. Clusters, language models, and ad hoc information retrieval. ACM Transactions on Information Systems (TOIS), 27(3):13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT – discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="1503" citStr="Lin and Pantel, 2001" startWordPosition="208" endWordPosition="211">ance in comparison to state-of-the-art baselines. 1 Introduction The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’, between the templates ‘X treat Y’ and ‘X relieve Y’ may be useful to identify the answer to “Which drugs relieve stomach ache?”. The predominant unsupervised approach for learning inference rules between templates is via distributional similarity (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Szpektor and Dagan, 2008). Specifically, each argument slot in a template is represented by an argument vector, containing the words (or terms) that instantiate this slot in all of the occurrences of the template in a learning corpus. Two templates are then deemed semantically similar if the argument vectors of their corresponding slots are similar. Ideally, inference rules should be learned for all templates that occur in the learning corpus. However, many templates are rare and occur only few times in the corpus. This is a typical NLP phenomenon that can be ass</context>
<context position="2730" citStr="Lin and Pantel, 2001" startWordPosition="411" endWordPosition="414"> with either a small learning corpus, as in the cases of domain specific corpora and resource-scarce languages, or with templates with rare terms or long multi-word expressions such as ‘X be also a risk factor to Y’ or ‘X finish second in Y’, which capture very specific meanings. Due to few occurrences, the slots of rare templates are represented with very sparse argument vectors, which in turn lead to low reliability in distributional similarity scores. A common practice in prior work for learning predicate inference rules is to simply disregard templates below a minimal frequency threshold (Lin and Pantel, 2001; Kotlerman et al., 2010; Dinu and Lapata, 2010; Ritter et al., 2010). Yet, acquiring rules for rare templates may be beneficial both in terms of coverage, but also in terms of more accurate rule application, since rare templates are less ambiguous than frequent ones. We propose to improve the learning of rules between infrequent templates by expanding their argument vectors. This is done via a “dual” distributional similarity approach, in which we consider two words to be similar if they instantiate similar sets of templates. We then use these similarities to expand the argument vector of eac</context>
<context position="5272" citStr="Lin and Pantel, 2001" startWordPosition="832" endWordPosition="835">applied to any template. A common starting point is to compute a co-occurrence matrix M from a learning corpus. M’s rows correspond to the template slots and the columns correspond to the various terms that instantiate the slots. Each entry Mi,j, e.g. Mx quit,John, contains a count of the number of times the term j instantiated the template slot i in the corpus. Thus, each row Mi,* corresponds to an argument vector for slot i. Next, some function of the counts is used to assign weights to all Mi,j entries. In this paper we use pointwise mutual information (PMI), which is common in prior work (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Finally, rules are assessed using some similarity measure between corresponding argument vectors. The state-of-the-art DIRT algorithm (Lin and Pantel, 2001) uses the highly cited Lin similarity measures (Lin, 1998) to score rules between binary templates as follows: Lin(v, v&apos;) = EwEvnvl [v(w) + v&apos;(w)] (1) EwEvUvl [v (w) + v&apos;(w)] DIRT(l → r) �= Lin(vl:x, vr:x) · Lin(vl:y, vr:y) where v and v&apos; are two argument vectors, l and r are the templates participating in the inference rule and vl:x corresponds to the argument vector of slot X of template l, etc. While the orig</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT – discovery of inference rules from text. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="5515" citStr="Lin, 1998" startWordPosition="869" endWordPosition="870">x quit,John, contains a count of the number of times the term j instantiated the template slot i in the corpus. Thus, each row Mi,* corresponds to an argument vector for slot i. Next, some function of the counts is used to assign weights to all Mi,j entries. In this paper we use pointwise mutual information (PMI), which is common in prior work (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Finally, rules are assessed using some similarity measure between corresponding argument vectors. The state-of-the-art DIRT algorithm (Lin and Pantel, 2001) uses the highly cited Lin similarity measures (Lin, 1998) to score rules between binary templates as follows: Lin(v, v&apos;) = EwEvnvl [v(w) + v&apos;(w)] (1) EwEvUvl [v (w) + v&apos;(w)] DIRT(l → r) �= Lin(vl:x, vr:x) · Lin(vl:y, vr:y) where v and v&apos; are two argument vectors, l and r are the templates participating in the inference rule and vl:x corresponds to the argument vector of slot X of template l, etc. While the original DIRT algorithm utilizes the Lin measure, one can replace it with any other vector similarity measure. A separate line of research for word similarity introduced directional similarity measures that have a bias for identifying generalizati</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to information retrieval, volume 1.</title>
<date>2008</date>
<publisher>Cambridge University Press Cambridge.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to information retrieval, volume 1. Cambridge University Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Chris Biemann</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation.</title>
<date>2012</date>
<booktitle>Proceedings of COLING,</booktitle>
<location>Mumbai, India.</location>
<contexts>
<context position="8490" citStr="Miller et al. (2012)" startWordPosition="1359" endWordPosition="1362">rds such as the ones in the expansion set of the observed word. We “blame” the lack of such template occurrences only on the size of the corpus and the sparseness phenomenon in natural languages. Thus, we utilize our lexical expansion scheme to synthetically add these expected but missing occurrences, effectively smoothing or generalizing over the explicitly observed argument occurrences. Our approach is inspired by query expansion (Voorhees, 1994) in Information Retrieval (IR), as well as by the recent lexical expansion framework proposed in (Biemann and Riedl, 2013), and the work by (2) 284 Miller et al. (2012) on word sense disambiguation. Yet, to the best of our knowledge, this is the first work that applies lexical expansion to distributional similarity feature vectors. We next describe our scheme in detail. 3.1 Learning Lexical Expansions We start by constructing the co-occurrence matrix M (Section 2), where each entry Mt:s,w indicates the number of times that word w instantiates slot s of template t in the learning corpus, denoted by ’t:s’, where s can be either X or Y. In traditional distributional similarity, the rows Mt:s,∗ serve as argument vectors of template slots. However, to learn expan</context>
</contexts>
<marker>Miller, Biemann, Zesch, Gurevych, 2012</marker>
<rawString>Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation. Proceedings of COLING, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1114" citStr="Ravichandran and Hovy, 2002" startWordPosition="146" endWordPosition="149">s scheme, prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability. To improve the learning of such rules in an unsupervised way, we propose to lexically expand sparse argument word vectors with semantically similar words. Our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines. 1 Introduction The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’, between the templates ‘X treat Y’ and ‘X relieve Y’ may be useful to identify the answer to “Which drugs relieve stomach ache?”. The predominant unsupervised approach for learning inference rules between templates is via distributional similarity (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Szpektor and Dagan, 2008). Specifically, each argument slot in a template is represented by an argument vector, containing the words (or terms) that instantiate this slot in all of</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Ritter, Etzioni, 2010</marker>
<rawString>Alan Ritter, Oren Etzioni, et al. 2010. A latent dirichlet allocation method for selectional preferences. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1174" citStr="Shinyama and Sekine, 2006" startWordPosition="154" endWordPosition="157">or low frequency predicates associated with very sparse argument vectors due to expected low reliability. To improve the learning of such rules in an unsupervised way, we propose to lexically expand sparse argument word vectors with semantically similar words. Our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines. 1 Introduction The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’, between the templates ‘X treat Y’ and ‘X relieve Y’ may be useful to identify the answer to “Which drugs relieve stomach ache?”. The predominant unsupervised approach for learning inference rules between templates is via distributional similarity (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Szpektor and Dagan, 2008). Specifically, each argument slot in a template is represented by an argument vector, containing the words (or terms) that instantiate this slot in all of the occurrences of the template in a learning corpus. Two t</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1559" citStr="Szpektor and Dagan, 2008" startWordPosition="216" endWordPosition="220"> Introduction The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’, between the templates ‘X treat Y’ and ‘X relieve Y’ may be useful to identify the answer to “Which drugs relieve stomach ache?”. The predominant unsupervised approach for learning inference rules between templates is via distributional similarity (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Szpektor and Dagan, 2008). Specifically, each argument slot in a template is represented by an argument vector, containing the words (or terms) that instantiate this slot in all of the occurrences of the template in a learning corpus. Two templates are then deemed semantically similar if the argument vectors of their corresponding slots are similar. Ideally, inference rules should be learned for all templates that occur in the learning corpus. However, many templates are rare and occur only few times in the corpus. This is a typical NLP phenomenon that can be associated with either a small learning corpus, as in the c</context>
<context position="5299" citStr="Szpektor and Dagan, 2008" startWordPosition="836" endWordPosition="839">e. A common starting point is to compute a co-occurrence matrix M from a learning corpus. M’s rows correspond to the template slots and the columns correspond to the various terms that instantiate the slots. Each entry Mi,j, e.g. Mx quit,John, contains a count of the number of times the term j instantiated the template slot i in the corpus. Thus, each row Mi,* corresponds to an argument vector for slot i. Next, some function of the counts is used to assign weights to all Mi,j entries. In this paper we use pointwise mutual information (PMI), which is common in prior work (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Finally, rules are assessed using some similarity measure between corresponding argument vectors. The state-of-the-art DIRT algorithm (Lin and Pantel, 2001) uses the highly cited Lin similarity measures (Lin, 1998) to score rules between binary templates as follows: Lin(v, v&apos;) = EwEvnvl [v(w) + v&apos;(w)] (1) EwEvUvl [v (w) + v&apos;(w)] DIRT(l → r) �= Lin(vl:x, vr:x) · Lin(vl:y, vr:y) where v and v&apos; are two argument vectors, l and r are the templates participating in the inference rule and vl:x corresponds to the argument vector of slot X of template l, etc. While the original DIRT algorithm utilize</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Zeichner</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
</authors>
<title>Crowdsourcing inference-rule evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="11811" citStr="Zeichner et al. (2012)" startWordPosition="1923" endWordPosition="1927">ds and non-alphabetical words that instantiated either the X or the Y argument slots. In addition, we discarded templates that co-occur with less than 5 unique argument words in either of their slots, assuming that such few arguments cannot convey reliable semantic information, even with expansion. Our final corpus consists of around 350,000 extractions and 14,000 unique templates. In this corpus around one third of the extractions refer to templates that co-occur with at most 35 unique arguments in both their slots. We evaluated the quality of inference rules using the dataset constructed by Zeichner et al. (2012)2, which contains about 6,500 manually annotated template rule applications, each labeled as correct or not. For example, ‘The game develop eye-hand coordination --/+ The game launch eye-hand coordination’ is a rule application in this dataset of the rule ‘X develop Y -+ X launch Y’, labeled as incorrect, and ‘Captain Cook sail to Australia -+ Captain Cook depart for Australia’ is a rule application of the rule ‘X sail to Y -+ X depart for Y’, labeled as correct. Specifically, we induced two datasets from Zeichner et al.’s dataset, denoted DS-5-35 and DS-5-50, which consist of all rule applica</context>
</contexts>
<marker>Zeichner, Berant, Dagan, 2012</marker>
<rawString>Naomi Zeichner, Jonathan Berant, and Ido Dagan. 2012. Crowdsourcing inference-rule evaluation. In Proceedings ofACL (short papers).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>