<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998721">
A Latent Variable Model of Synchronous Parsing
for Syntactic and Semantic Dependencies
</title>
<author confidence="0.86698">
James Henderson
</author>
<affiliation confidence="0.599522">
Dept Computer Science
Univ Geneva
</affiliation>
<figure confidence="0.218522076923077">
james.henderson@
cui.unige.ch
Paola Merlo
Dept Linguistics
Univ Geneva
merlo@
lettres.unige.ch
Gabriele Musillo
Depts Linguistics
and Computer Science
Univ Geneva
musillo@
lettres.unige.ch
</figure>
<note confidence="0.655898666666667">
Ivan Titov∗
Dept Computer Science
Univ Illinois at U-C
</note>
<email confidence="0.985875">
titov@uiuc.edu
</email>
<sectionHeader confidence="0.998505" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977357142857">
We propose a solution to the challenge
of the CoNLL 2008 shared task that uses
a generative history-based latent variable
model to predict the most likely derivation
of a synchronous dependency parser for
both syntactic and semantic dependencies.
The submitted model yields 79.1% macro-
average F1 performance, for the joint task,
86.9% syntactic dependencies LAS and
71.0% semantic dependencies F1. A larger
model trained after the deadline achieves
80.5% macro-average F1, 87.6% syntac-
tic dependencies LAS, and 73.1% seman-
tic dependencies F1.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998897666666667">
Successes in syntactic tasks, such as statistical
parsing and tagging, have recently paved the way
to statistical learning techniques for levels of se-
mantic representation, such as recovering the log-
ical form of a sentence for information extraction
and question-answering applications (e.g. (Wong
and Mooney, 2007)) or jointly learning the syntac-
tic structure of the sentence and the propositional
argument-structure of its main predicates (Musillo
and Merlo, 2006; Merlo and Musillo, 2008). In
this vein, the CoNLL 2008 shared task sets the
challenge of learning jointly both syntactic depen-
dencies (extracted from the Penn Treebank (Mar-
cus et al., 1993) ) and semantic dependencies (ex-
tracted both from PropBank (Palmer et al., 2005)
</bodyText>
<note confidence="0.335676">
�
</note>
<footnote confidence="0.8396434">
c 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
0Authors in alphabetical order.
</footnote>
<bodyText confidence="0.999716677419355">
and NomBank (Meyers et al., 2004) under a uni-
fied representation.
We propose a solution that uses a generative
history-based model to predict the most likely
derivation of a synchronous dependency parser for
both syntactic and semantic dependencies. Our
probabilistic model is based on Incremental Sig-
moid Belief Networks (ISBNs), a recently pro-
posed latent variable model for syntactic struc-
ture prediction, which has shown very good be-
haviour for both constituency (Titov and Hender-
son, 2007a) and dependency parsing (Titov and
Henderson, 2007b). The ability of ISBNs to in-
duce their features automatically enables us to ex-
tend this architecture to learning a synchronous
parse of syntax and semantics without modifica-
tion of the main architecture. By solving the
problem with synchronous parsing, a probabilistic
model is learnt which maximises the joint proba-
bility of the syntactic and semantic dependencies
and thereby guarantees that the output structure is
globally coherent, while at the same time building
the two structures separately. This extension of the
ISBN architecture is therefore applicable to other
problems where two independent, but related, lev-
els of representation are being learnt, such as sta-
tistical machine translation.
Currently the largest model we have trained
achieves 80.5% macro-average F1 performance for
the joint task, 87.6% syntactic dependencies LAS,
and 73.1% semantic dependencies F1.
</bodyText>
<sectionHeader confidence="0.99578" genericHeader="method">
2 The Probability Model
</sectionHeader>
<bodyText confidence="0.9987256">
Our probability model is a joint generative model
of syntactic and semantic dependencies. The
two dependency structures are specified as the se-
quence of actions for a synchronous parser, which
requires each dependency structure to be projec-
</bodyText>
<page confidence="0.945314">
178
</page>
<reference confidence="0.3355085">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 178–182
Manchester, August 2008
</reference>
<bodyText confidence="0.707138">
tivised separately.
</bodyText>
<subsectionHeader confidence="0.997544">
2.1 Synchronous derivations
</subsectionHeader>
<bodyText confidence="0.99675268292683">
The derivations for syntactic dependency trees are
the same as specified in (Titov and Henderson,
2007b), which are based on the shift-reduce style
parser of (Nivre et al., 2006). The derivations use a
stack and an input queue. There are actions for cre-
ating a leftward or rightward arc between the top of
the stack and the front of the queue, for popping a
word from the stack, and for shifting a word from
the queue to the stack. The derivations for seman-
tic dependency graphs use virtually the same set
of actions, but impose fewer constraints on when
they can be applied, due to the fact that a word in
a semantic dependency graph can have more than
one parent. An additional action predicates was
introduced to label a predicate with sense s.
Let Td be a syntactic dependency tree with
derivation D1d, ..., Dmd
d , and Ts be a semantic de-
pendency graph with derivation D1 s, ..., Dms
s . To
define derivations for the joint structure Td, Ts,
we need to specify how the two derivations are
synchronised, and in particular make the impor-
tant choice of the granularity of the synchronisa-
tion step. Linguistic intuition would perhaps sug-
gest that syntax and semantics are connected at the
clause level – a big step size – while a fully in-
tegrated system would synchronise at each pars-
ing decision, thereby providing the most commu-
nication between these two levels. We choose to
synchronise the construction of the two structures
at every word – an intermediate step size. This
choice is simpler, as it is based on the natural to-
tal order of the input, and it avoids the problems
of the more linguistically motivated choice, where
chunks corresponding to different semantic propo-
sitions would be overlapping.
We divide the two derivations into the chunks
between shifting each word onto the stack,
ctd = Dbt d , ..., Det d
d and ct s = Dbts
</bodyText>
<equation confidence="0.6274158">
s , ..., Dets
s ,
d
Dbt d = Dbts−1
d−1
</equation>
<bodyText confidence="0.964530125">
where s = shiftt−1 and
Det
d = Dets+1
d+1 s = shiftt. Then the actions of
the synchronous derivations consist of quadruples
Ct = (ctd, switch, cts, shiftt), where switch means
switching from syntactic to semantic mode. This
gives us the following joint probability model,
where n is the number of words in the input.
P(Td, Ts) = P(C1, ... , Cn)
= Fit P�C1,..., C&apos;t-1) (1 )
(Ct
The probability of each synchronous derivation
chunk Ct is the product of four factors, related to
the syntactic level, the semantic level and the two
synchronising steps.
</bodyText>
<equation confidence="0.9856396">
P(Ct|C1,...,Ct−1) =
P(ctd|C1, ... , Ct−1)×
P(switch|ctd, C1, ... , Ct−1)× (2)
P(cts|switch, ctd, C1, ..., Ct−1)×
P(shiftt|ct d, ct s, C1,... , Ct−1)
</equation>
<bodyText confidence="0.999968142857143">
These synchronous derivations C1, ... , Cn only
require a single input queue, since the shift opera-
tions are synchronised, but they require two sepa-
rate stacks, one for the syntactic derivation and one
for the semantic derivation.
The probability of ctd is decomposed into deriva-
tion action Di probabilities, and likewise for cts.
</bodyText>
<equation confidence="0.988849">
P(ctd|C1, ... ,Ct−1)
=�iP(Di d|D d ,..., Di−1
bt (3)
</equation>
<bodyText confidence="0.994245">
d d , C1,..., Ct−1)
The actions are also sometimes split into a se-
quence of elementary decisions Di = di1, ... , din,
as discussed in (Titov and Henderson, 2007a).
</bodyText>
<subsectionHeader confidence="0.999963">
2.2 Projectivisation of dependencies
</subsectionHeader>
<bodyText confidence="0.999879708333333">
These derivations can only specify projective
syntactic or semantic dependency graphs. Ex-
ploratory data analysis indicates that many in-
stances of non-projectivity in the complete graph
are due to crossings of the syntactic and seman-
tic graphs. The amount of non-projectivity of the
joint syntactic-semantic graph is approximately
7.5% non-projective arcs, while summing the non-
projectivity within the two separate graphs results
in only roughly 3% non-projective arcs.
Because our synchronous derivations use two
different stacks for the syntactic and semantic de-
pendencies, respectively, we only require each in-
dividual graph to be projective. As with many de-
pendency parsers (Nivre et al., 2006; Titov and
Henderson, 2007b), we handle non-projective (i.e.
crossing) arcs by transforming them into non-
crossing arcs with augmented labels.1 Because
our syntactic derivations are equivalent to those of
(Nivre et al., 2006), we use their HEAD methods
to projectivise the syntactic dependencies.
Although our semantic derivations use the same
set of actions as the syntactic derivations, they dif-
fer in that the graph of semantic dependencies need
</bodyText>
<footnote confidence="0.988939">
1During testing, these projectivised structures are then
transformed back to the original format for evaluation.
</footnote>
<page confidence="0.996877">
179
</page>
<bodyText confidence="0.999904142857143">
not form a tree. The only constraints we place on
the set of semantic dependencies are imposed by
the use of a stack, which excludes crossing arcs.
Given two crossing arcs, we try to uncross them
by changing an endpoint of one of the arcs. The
arc (p, a), where p is a predicate and a is an argu-
ment, is changed to (p, h), where h is the syntactic
head of argument a. Its label r is then changed to
r/d where d is the syntactic dependency of a on
h. This transformation may need to be repeated
before the arcs become uncrossed. The choice of
which arc to transform is done using a greedy al-
gorithm and a number of heuristics, without doing
any global optimisation across the data.
This projectivisation method is similar to the
HEAD method of (Nivre et al., 2006), but has two
interesting new characteristics. First, syntactic de-
pendencies are used to projectivise the semantic
dependencies. Because the graph of semantic roles
is disconnected, moving across semantic arcs is of-
ten not possible. This would cause a large number
of roles to be moved to ROOT. Second, our method
changes the semantic argument of a given pred-
icate, whereas syntactic dependency projectivisa-
tion changes the head of a given dependent. This
difference is motivated by a predicate-centred view
of semantic dependencies, as it avoids changing a
predicate to a node which is not a predicate.
</bodyText>
<sectionHeader confidence="0.996834" genericHeader="method">
3 The Learning Architecture
</sectionHeader>
<bodyText confidence="0.999993833333333">
The synchronous derivations described above are
modelled with an Incremental Sigmoid Belief Net-
work (ISBN) (Titov and Henderson, 2007a). IS-
BNs are dynamic Bayesian Networks which incre-
mentally specify their model structure based on the
partial structure being built by a derivation. They
have previously been applied to constituency and
dependency parsing. In both cases the derivations
were based on a push-down automaton, but ISBNs
can be directly applied to any automaton. We suc-
cessfully apply ISBNs to a two-stack automaton,
without changing the machine learning methods.
</bodyText>
<subsectionHeader confidence="0.99701">
3.1 The Incremental Sigmoid Belief Networks
</subsectionHeader>
<bodyText confidence="0.999952857142857">
ISBNs use vectors of latent variables to represent
properties of parsing history relevant to the next
decisions. Latent variables do not need to be anno-
tated in the training data, but instead get induced
during learning. As illustrated by the vectors Si
in figure 1, the latent feature vectors are used to
estimate the probabilities of derivation actions Di.
</bodyText>
<figure confidence="0.935033">
i−c Si−1
si
j
Di dk
</figure>
<figureCaption confidence="0.9948684">
Figure 1: An ISBN for estimating
P(dik|history(i,k)) – one of the elementary
decisions. Variables whose values are given in
history(i, k) are shaded, and latent and current
decision variables are unshaded.
</figureCaption>
<bodyText confidence="0.999715">
Latent variable vectors are connected to variables
from previous positions via a pattern of edges de-
termined by the previous decisions. Our ISBN
model distinguishes two types of latent states: syn-
tactic states, when syntactic decisions are consid-
ered, and semantic states, when semantic decision
are made. Different patterns of interconnections
are used for different types of states. We use the
neural network approximation (Titov and Hender-
son, 2007a) to perform inference in our model.
As also illustrated in figure 1, the induced latent
variables Si at state i are statistically dependent on
both pre-defined features of the derivation history
Di, ... , Di−&apos; and the latent variables for a finite
set of relevant previous states Si�, i&apos; &lt; i. Choos-
ing this set of relevant previous states is one of the
main design decisions in building an ISBN model.
By connecting to a previous state, we place that
state in the local context of the current decision.
This specification of the domain of locality deter-
mines the inductive bias of learning with ISBNs.
Thus, we need to choose the set of local (i.e. con-
nected) states in accordance with our prior knowl-
edge about which previous decisions are likely to
be particularly relevant to the current decision.
</bodyText>
<subsectionHeader confidence="0.999627">
3.2 Layers and features
</subsectionHeader>
<bodyText confidence="0.999918090909091">
To choose previous relevant decisions, we make
use of the partial syntactic and semantic depen-
dency structures which have been decided so far
in the parse. Specifically, the current latent state
vector is connected to the most recent previous la-
tent state vectors (if they exist) whose configura-
tion shares a node with the current configuration,
as specified in Table 1. The nodes are chosen be-
cause their properties are thought to be relevant to
the current decision. Each row of the table indi-
cates which nodes need to be identical, while each
</bodyText>
<figure confidence="0.832105714285714">
S
S
i
D
i−c
D
i−1
</figure>
<page confidence="0.987762">
180
</page>
<tableCaption confidence="0.976936">
Table 1: Latent-to-latent variable connections. In-
</tableCaption>
<bodyText confidence="0.994721407407407">
put= input queue; Top= top of stack; RDT= right-
most right dependent of top; LDT= leftmost left
dependent of top; HT= Head of top; LDN= left-
most dependent of next (front of input).
column indicates whether the latent state vectors
are for the syntactic or semantic derivations. For
example, the first row indicates edges between the
current state and a state which had the same in-
put as the current state. The three columns indi-
cate that this edge holds within syntactic states,
within semantic states, and from syntactic to se-
mantic states. The fourth cell of the third row, for
example, indicates that there is an edge between
the current semantic state on top of the stack and
the most recent semantic state where the rightmost
dependent of the current top of the semantic stack
was at the top of the semantic stack.
Each of these relations has a distinct weight ma-
trix for the resulting edges in the ISBN, but the
same weight matrix is used at each position where
the relation applies. Training and testing times
scale linearly with the number of relations.
The pre-defined features of the parse history
which also influence the current decision are spec-
ified in table 2. The model distinguishes argument
roles of nominal predicates from argument roles of
verbal predicates.
</bodyText>
<subsectionHeader confidence="0.999253">
3.3 Decoding
</subsectionHeader>
<bodyText confidence="0.999963666666666">
Given a trained ISBN as our probability esti-
mator, we search for the most probable joint
syntactic-semantic dependency structure using a
beam search. Most pruning is done just after each
shift operation (when the next word is predicted).
Global constraints (such as label uniqueness) are
not enforced by decoding, but can be learnt.
For the system whose results we submitted, we
then do a second step to improve on the choice
of syntactic dependency structure. Because of the
lack of edges in the graphical model from seman-
tic to syntactic states, it is easy to marginalise out
the semantic structure, giving us the most proba-
ble syntactic dependency structure. This syntactic
structure is then combined with the semantic struc-
</bodyText>
<table confidence="0.99971495">
State Stack Syntactic step features
LEX POS DEP
Input syn + +
Top syn + +
Top - 1 syn +
HT syn +
RDT syn +
LDT syn +
LDN +
State Stack Semantic step features
LEX POS DEP SENSE
Input sem + + +
Top sem + + +
Top - 1 sem + +
HT sem + +
RDT sem +
LDT sem +
LDN sem +
A0-A5 of Top sem +
A0-A5 of Input +
</table>
<tableCaption confidence="0.995165">
Table 2: Pre-defined features. syn=syntactic stack;
</tableCaption>
<bodyText confidence="0.973892727272727">
sem=semantic stack. Input= input queue; Top=
top of stack; RDT= rightmost dependent of top;
LDT= leftmost dependent of Top; HT= Head of
top; LDN= leftmost dependent of next (front of
input); A0-A5 of Top/Input= arguments of top of
stack / input.
ture from the first stage, to get our submitted re-
sults. This second stage does not maximise perfor-
mance on the joint syntactic-semantic dependency
structure, but it better fits the evaluation measure
used to rank systems.
</bodyText>
<sectionHeader confidence="0.996737" genericHeader="evaluation">
4 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.999978157894737">
The experimental set-up common for all the teams
is described in the introduction (Surdeanu et al.,
2008). The submitted model has latent variable
vectors of 60 units, and a word frequency cut-off
of 100, resulting in a small vocabulary of 1083
words. We used a beam of size 15 to prune deriva-
tions after each shift operation to obtain the joint
structure, and a beam of size 40 when perform-
ing the marginalisation. Training took approxi-
mately 2.5 days on a standard PC with 3.0 GHz
Pentium4 CPU. It took approximately 2 hours to
parse the entire testing set (2,824 sentences) and
an additional 3 hours to perform syntactic parsing
when marginalising out the semantic structures.2
Shortly after the submission deadline, we trained a
‘large’ model with a latent variable vector of size
80, a word frequency cut-off of 20, and additional
latent-to-latent connections from semantics to syn-
tax of the same configuration as the last column
</bodyText>
<footnote confidence="0.7123815">
2A multifold speed-up with a small decrease in accuracy
can be achieved by using a small beam.
</footnote>
<table confidence="0.944204">
Closest Current
Syn-Syn Srl-Srl Syn-Srl
Input Input
Top Top
+ + +
+ + +
RDT Top
LDT Top
HT Top
LDN Top
Input Top
+ +
+ +
+ +
+ +
+
181
Syn Semantic P Overall
LAS P R F1 R F1
Submitted
D 86.1 78.8 64.7 71.1 82.5 75.4 78.8
W 87.8 79.6 66.2 72.3 83.7 77.0 80.2
B 80.0 66.6 55.3 60.4 73.3 67.6 70.3
WB 86.9 78.2 65.0 71.0 82.5 76.0 79.1
Joint inference
D 85.5 78.8 64.7 71.1 82.2 75.1 78.5
Large, joint inference
D 86.5 79.9 67.5 73.2 83.2 77.0 80.0
W 88.5 80.4 69.2 74.4 84.4 78.8 81.5
B 81.0 68.3 57.7 62.6 74.7 69.4 71.9
WB 87.6 79.1 67.9 73.1 83.4 77.8 80.5
</table>
<tableCaption confidence="0.820404">
Table 3: Scores on the development set and the
final testing sets (percentages). D= development
set; W=WSJ; B=Brown; WB=WSJ+Brown;
</tableCaption>
<bodyText confidence="0.999855822222223">
of table 1. This model took about 50% longer in
training and testing.
In table 3, we report results for the marginalised
inference (‘submitted’) and joint inference for the
submitted model, and the results for joint inference
with the ‘large’ model. The larger model improves
on the submitted results by almost 1.5%, a signifi-
cant improvement. If completed earlier, this model
would have been fifth overall, second for syntactic
LAS, and fifth for semantic F1.
To explore the relationship between the two
components of the model, we removed the edges
between the syntax and the semantics in the sub-
mitted model. This model’s performance drops by
about 3.5% for semantic role labelling, thereby in-
dicating that the latent annotation of parsing states
helps semantic role labelling. However, it also
indicates that there is much room for improve-
ment in developing useful semantic-specific fea-
tures, which was not done for these experiments
simply due to constraints on development time.
To test whether joint learning degrades the ac-
curacy of the syntactic parsing model, we trained a
syntactic parsing model with the same features and
the same pattern of interconnections as used for the
syntactic states in our joint model. The resulting
labelled attachment score was non-significantly
lower (0.2%) than the score for the marginalised
inference with the joint model. This result sug-
gests that, though the latent variables associated
with syntactic states in the joint model were trained
to be useful in semantic role labelling, this did not
have a negative effect on syntactic parsing accu-
racy, and may even have helped.
Finally, an analysis of the errors on the develop-
ment set for the submitted model paints a coherent
picture. We find attachment of adjuncts particu-
larly hard. For dependency labels, we make the
most mistakes on modification labels, while for se-
mantic labels, we find TMP, ADV, LOC, and PRN
particularly hard. NomBank arcs are not learnt as
well as PropBank arcs: we identify PropBank SRL
arguments at F1 70.8% while Nombank arguments
reach 58.1%, and predicates at accuracy 87.9% for
PropBank and 74.9% for NomBank.
</bodyText>
<sectionHeader confidence="0.999721" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999984333333333">
While still preliminary, these results indicate that
synchronous parsing is an effective way of build-
ing joint models on separate structures. The gen-
erality of the ISBN design used so far suggests
that ISBN’s latent feature induction extends well to
estimating very complex probability models, with
little need for feature engineering. Nonetheless,
performance could be improved by task-specific
features, which we plan for future work.
</bodyText>
<sectionHeader confidence="0.999042" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9778005">
This work was partly funded by European Community FP7
grant 216594 (CLASSiC, www.classic-project.org), Swiss
NSF grant 114044, and Swiss NSF fellowships PBGE2-
117146 and PBGE22-119276. Part of this work was done
when G. Musillo was visiting MIT/CSAIL, hosted by Prof.
Michael Collins.
</bodyText>
<sectionHeader confidence="0.998568" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999865484848485">
Marcus, M., B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19:313–330.
Merlo, P. and G. Musillo. 2008. Semantic parsing for high-
precision semantic role labelling. In Procs of CoNLL
2008, Manchester, UK.
Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The nombank
project: An interim report. In Meyers, A., editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Annotation,
24–31, Boston, MA.
Musillo, G. and P. Merlo. 2006. Accurate semantic parsing
of the Proposition Bank. In Procs of NAACL 2006, New
York, NY.
Nivre, J., J. Hall, J. Nilsson, G. Eryigit, and S. Marinov. 2006.
Pseudo-projective dependency parsing with support vector
machines. In Proc. of CoNNL, 221–225, New York, USA.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The Propo-
sition Bank: An annotated corpus of semantic roles. Com-
putational Linguistics, 31:71–105.
Surdeanu, M., R. Johansson, A. Meyers, L. M`arquez, and J.
Nivre. 2008. The CoNLL-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In Procs of
CoNLL-2008, Manchester,UK.
Titov, I. and J. Henderson. 2007a. Constituent parsing with
incremental sigmoid belief networks. In Procs of ACL’07,
pages 632–639, Prague, Czech Republic.
Titov, I. and J. Henderson. 2007b. A latent variable model
for generative dependency parsing. In Procs of IWPT’07,
Prague, Czech Republic.
Wong, Y.W. and R. Mooney. 2007. Learning synchronous
grammars for semantic parsing with lambda calculus. In
Procs of ACL’07, 960–967, Prague, Czech Republic.
</reference>
<page confidence="0.998014">
182
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.084259">
<title confidence="0.9996695">A Latent Variable Model of Synchronous for Syntactic and Semantic Dependencies</title>
<author confidence="0.992268">James</author>
<affiliation confidence="0.9948475">Dept Computer Univ Geneva</affiliation>
<email confidence="0.7300215">james.henderson@cui.unige.ch</email>
<author confidence="0.823881">Paola</author>
<affiliation confidence="0.9871215">Dept Univ Geneva</affiliation>
<address confidence="0.369956">merlo@</address>
<email confidence="0.716094">lettres.unige.ch</email>
<author confidence="0.887302">Gabriele</author>
<affiliation confidence="0.977676333333333">Depts and Computer Univ Geneva</affiliation>
<email confidence="0.8778485">musillo@lettres.unige.ch</email>
<affiliation confidence="0.943571">Dept Computer Univ Illinois at</affiliation>
<email confidence="0.997456">titov@uiuc.edu</email>
<abstract confidence="0.994372466666667">We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. The submitted model yields 79.1% macroaverage F1 performance, for the joint task, 86.9% syntactic dependencies LAS and 71.0% semantic dependencies F1. A larger model trained after the deadline achieves 80.5% macro-average F1, 87.6% syntactic dependencies LAS, and 73.1% semantic dependencies F1.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>CoNLL</author>
</authors>
<date>2008</date>
<booktitle>Proceedings of the 12th Conference on Computational Natural Language Learning,</booktitle>
<pages>178--182</pages>
<location>Manchester,</location>
<contexts>
<context position="1484" citStr="CoNLL 2008" startWordPosition="211" endWordPosition="212">F1, 87.6% syntactic dependencies LAS, and 73.1% semantic dependencies F1. 1 Introduction Successes in syntactic tasks, such as statistical parsing and tagging, have recently paved the way to statistical learning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. (Wong and Mooney, 2007)) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) � c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of </context>
</contexts>
<marker>CoNLL, 2008</marker>
<rawString>CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 178–182 Manchester, August 2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1619" citStr="Marcus et al., 1993" startWordPosition="230" endWordPosition="234">s statistical parsing and tagging, have recently paved the way to statistical learning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. (Wong and Mooney, 2007)) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) � c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid B</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M., B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Merlo</author>
<author>G Musillo</author>
</authors>
<title>Semantic parsing for highprecision semantic role labelling.</title>
<date>2008</date>
<booktitle>In Procs of CoNLL</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="1454" citStr="Merlo and Musillo, 2008" startWordPosition="203" endWordPosition="206">r the deadline achieves 80.5% macro-average F1, 87.6% syntactic dependencies LAS, and 73.1% semantic dependencies F1. 1 Introduction Successes in syntactic tasks, such as statistical parsing and tagging, have recently paved the way to statistical learning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. (Wong and Mooney, 2007)) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) � c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict </context>
</contexts>
<marker>Merlo, Musillo, 2008</marker>
<rawString>Merlo, P. and G. Musillo. 2008. Semantic parsing for highprecision semantic role labelling. In Procs of CoNLL 2008, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>The nombank project: An interim report.</title>
<date>2004</date>
<booktitle>HLTNAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<pages>24--31</pages>
<editor>In Meyers, A., editor,</editor>
<location>Boston, MA.</location>
<contexts>
<context position="1945" citStr="Meyers et al., 2004" startWordPosition="272" endWordPosition="275">f the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) � c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic structure prediction, which has shown very good behaviour for both constituency (Titov and Henderson, 2007a) and dependency parsing (Titov and Henderson, 2007b). The ability of ISBNs to induce their features automatically enables us to extend thi</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The nombank project: An interim report. In Meyers, A., editor, HLTNAACL 2004 Workshop: Frontiers in Corpus Annotation, 24–31, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Musillo</author>
<author>P Merlo</author>
</authors>
<title>Accurate semantic parsing of the Proposition Bank.</title>
<date>2006</date>
<booktitle>In Procs of NAACL</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="1428" citStr="Musillo and Merlo, 2006" startWordPosition="199" endWordPosition="202">larger model trained after the deadline achieves 80.5% macro-average F1, 87.6% syntactic dependencies LAS, and 73.1% semantic dependencies F1. 1 Introduction Successes in syntactic tasks, such as statistical parsing and tagging, have recently paved the way to statistical learning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. (Wong and Mooney, 2007)) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) � c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative histo</context>
</contexts>
<marker>Musillo, Merlo, 2006</marker>
<rawString>Musillo, G. and P. Merlo. 2006. Accurate semantic parsing of the Proposition Bank. In Procs of NAACL 2006, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>G Eryigit</author>
<author>S Marinov</author>
</authors>
<title>Pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proc. of CoNNL,</booktitle>
<pages>221--225</pages>
<location>New York, USA.</location>
<marker>Nivre, Hall, Nilsson, Eryigit, Marinov, 2006</marker>
<rawString>Nivre, J., J. Hall, J. Nilsson, G. Eryigit, and S. Marinov. 2006. Pseudo-projective dependency parsing with support vector machines. In Proc. of CoNNL, 221–225, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--71</pages>
<contexts>
<context position="1699" citStr="Palmer et al., 2005" startWordPosition="244" endWordPosition="247">arning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. (Wong and Mooney, 2007)) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) � c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M`arquez</author>
<author>J Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Procs of CoNLL-2008,</booktitle>
<location>Manchester,UK.</location>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Surdeanu, M., R. Johansson, A. Meyers, L. M`arquez, and J. Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Procs of CoNLL-2008, Manchester,UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>Constituent parsing with incremental sigmoid belief networks.</title>
<date>2007</date>
<booktitle>In Procs of ACL’07,</booktitle>
<pages>632--639</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2405" citStr="Titov and Henderson, 2007" startWordPosition="342" endWordPosition="346">ike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic structure prediction, which has shown very good behaviour for both constituency (Titov and Henderson, 2007a) and dependency parsing (Titov and Henderson, 2007b). The ability of ISBNs to induce their features automatically enables us to extend this architecture to learning a synchronous parse of syntax and semantics without modification of the main architecture. By solving the problem with synchronous parsing, a probabilistic model is learnt which maximises the joint probability of the syntactic and semantic dependencies and thereby guarantees that the output structure is globally coherent, while at the same time building the two structures separately. This extension of the ISBN architecture is the</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Titov, I. and J. Henderson. 2007a. Constituent parsing with incremental sigmoid belief networks. In Procs of ACL’07, pages 632–639, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>A latent variable model for generative dependency parsing.</title>
<date>2007</date>
<booktitle>In Procs of IWPT’07,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2405" citStr="Titov and Henderson, 2007" startWordPosition="342" endWordPosition="346">ike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic structure prediction, which has shown very good behaviour for both constituency (Titov and Henderson, 2007a) and dependency parsing (Titov and Henderson, 2007b). The ability of ISBNs to induce their features automatically enables us to extend this architecture to learning a synchronous parse of syntax and semantics without modification of the main architecture. By solving the problem with synchronous parsing, a probabilistic model is learnt which maximises the joint probability of the syntactic and semantic dependencies and thereby guarantees that the output structure is globally coherent, while at the same time building the two structures separately. This extension of the ISBN architecture is the</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Titov, I. and J. Henderson. 2007b. A latent variable model for generative dependency parsing. In Procs of IWPT’07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Procs of ACL’07, 960–967,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1278" citStr="Wong and Mooney, 2007" startWordPosition="177" endWordPosition="180">submitted model yields 79.1% macroaverage F1 performance, for the joint task, 86.9% syntactic dependencies LAS and 71.0% semantic dependencies F1. A larger model trained after the deadline achieves 80.5% macro-average F1, 87.6% syntactic dependencies LAS, and 73.1% semantic dependencies F1. 1 Introduction Successes in syntactic tasks, such as statistical parsing and tagging, have recently paved the way to statistical learning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. (Wong and Mooney, 2007)) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) � c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Wong, Y.W. and R. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Procs of ACL’07, 960–967, Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>