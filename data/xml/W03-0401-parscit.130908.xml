<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000595">
<title confidence="0.993387">
A model of syntactic disambiguation based on lexicalized grammars
</title>
<author confidence="0.990786">
Yusuke Miyao Jun’ichi Tsujii
</author>
<affiliation confidence="0.875204">
Department of Computer Science, Department of Computer Science,
University of Tokyo University of Tokyo
yusuke@is.s.u-tokyo.ac.jp CREST, JST
(Japan Science and Technology Corporation)
</affiliation>
<email confidence="0.990609">
tsujii@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.995474" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999870625">
This paper presents a new approach to syntac-
tic disambiguation based on lexicalized gram-
mars. While existing disambiguation mod-
els decompose the probability of parsing re-
sults into that of primitive dependencies of two
words, our model selects the most probable
parsing result from a set of candidates allowed
by a lexicalized grammar. Since parsing re-
sults given by the lexicalized grammar cannot
be decomposed into independent sub-events,
we apply a maximum entropy model for fea-
ture forests, which allows probabilistic model-
ing without the independence assumption. Our
approach provides a general method of produc-
ing a consistent probabilistic model of parsing
results given by lexicalized grammars.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99925974489796">
Recent studies on the automatic extraction of lexicalized
grammars (Xia, 1999; Chen and Vijay-Shanker, 2000;
Hockenmaier and Steedman, 2002a) allow the modeling
of syntactic disambiguation based on linguistically moti-
vated grammar theories including LTAG (Chiang, 2000)
and CCG (Clark et al., 2002; Hockenmaier and Steed-
man, 2002b). However, existing models of disambigua-
tion with lexicalized grammars are a mere extension of
lexicalized probabilistic context-free grammars (LPCFG)
(Collins, 1996; Collins, 1997; Charniak, 1997), which
are based on the decomposition ofparsing results into the
syntactic/semantic dependencies of two words in a sen-
tence under the assumption of independence of the de-
pendencies. While LPCFG models have proved that the
incorporation of lexical associations (i.e., dependencies
of words) significantly improves the accuracy of parsing,
this idea has been naively inherited in the recent studies
on disambiguation models of lexicalized grammars.
However, the disambiguation models of lexicalized
grammars should be totally different from that of LPCFG,
because the grammars define the relation of syntax and
semantics, and can restrict the possible structure of pars-
ing results. Parsing results cannot simply be decomposed
into primitive dependencies, because the complete struc-
ture is determined by solving the syntactic constraints
of a complete sentence. For example, when we apply
a unification-based grammar, LPCFG-like modeling re-
sults in an inconsistent probability model because the
model assigns probabilities to parsing results not allowed
by the grammar (Abney, 1997). We have only two ways
of adhering to LPCFG models: preserve the consistency
of probability models by abandoning improvements to
the lexicalized grammars using complex constraints (Chi-
ang, 2000), or ignore the inconsistency in probability
models (Clark et al., 2002).
This paper provides a new model of syntactic disam-
biguation in which lexicalized grammars can restrict the
possible structures of parsing results. Our modeling aims
at providing grounds for i) producing a consistent proba-
bilistic model of lexicalized grammars, as well as ii) eval-
uating the contributions of syntactic and semantic prefer-
ences to syntactic disambiguation. The model is com-
posed of the syntax and semantics probabilities, which
represent syntactic and semantic preferences respectively.
The syntax probability is responsible for determining the
syntactic categories chosen by words in a sentence, and
the semantics probability selects the most plausible de-
pendencies of words from candidates allowed by the syn-
tactic categories yielded by the syntax probability. Since
the sequence of syntactic categories restricts the possi-
ble structure of parsing results, the semantics probabil-
ity is a conditional probability without decomposition
into the primitive dependencies of words. Recently used
machine learning methods including maximum entropy
models (Berger et al., 1996) and support vector machines
(Vapnik, 1995) provide grounds for this type of model-
ing, because it allows various dependent features to be
incorporated into the model without the independence as-
sumption.
The above approach, however, has a serious deficiency:
a lexicalized grammar assigns exponentially many pars-
ing results because of local ambiguities in a sentence,
which is problematic in estimating the parameters of a
probability model. To cope with this, we adopted an
algorithm of maximum entropy estimation for feature
forests (Miyao and Tsujii, 2002; Geman and Johnson,
2002), which allows parameters to be efficiently esti-
mated. The algorithm enables probabilistic modeling
of complete structures, such as transition sequences in
Markov models and parse trees, without dividing them
into independent sub-events. The algorithm avoids expo-
nential explosion by representing a probabilistic event by
a packed representation of a feature space. If a complete
structure is represented with a feature forest of a tractable
size, the parameters can be efficiently estimated by dy-
namic programming.
A series of studies on parsing with wide-coverage LFG
(Johnson et al., 1999; Riezler et al., 2000; Riezler et al.,
2002) have had a similar motivation to ours. Their mod-
els have also been based on a discriminative model to
select a parsing result from all candidates given by the
grammar. A significant difference is that we apply max-
imum entropy estimation for feature forests to avoid the
inherent problem with estimation: the exponential explo-
sion of parsing results given by the grammar. They as-
sumed that parsing results would be suppressed to a rea-
sonable number through using heuristic rules, or by care-
fully implementing a fully restrictive and wide-coverage
grammar, which requires a considerable amount of effort
to develop. Our contention is that this problem can be
solved in a more sophisticated way as is discussed in this
paper. Another difference is that our model is separated
into syntax and semantics probabilities, which will ben-
efit computational/linguistic investigations into the rela-
tion between syntax and semantics, and allow separate
improvements to both models.
Overall, the approach taken in this paper is different
from existing models in the following respects.
</bodyText>
<listItem confidence="0.977703666666667">
• Since it does not require the assumption of inde-
pendence, the probability model is consistent with
lexicalized grammars with complex constraints in-
cluding unification-based grammar formalism. Our
model can assign consistent probabilities to parsing
results of lexicalized grammars, while the traditional
models assign probabilities to parsing results not al-
lowed by the grammar.
• Since the syntax and semantics probabilities are sep-
</listItem>
<bodyText confidence="0.995560709677419">
arate, we can improve them individually. For exam-
ple, the syntax model can be improved by smooth-
ing using the syntactic classes of words, while the
semantics model should be able to be improved by
using semantic classes. In addition, the model can
be a starting point that allows the theory of syntax
and semantics to be evaluated through consulting an
extensive corpus.
We evaluated the validity of our model through experi-
ments on a disambiguation task of parsing the Penn Tree-
bank (Marcus et al., 1994) with an automatically acquired
LTAG grammar. To assess the contribution of the syntax
and semantics probabilities to the accuracy ofparsing and
to evaluate the validity of applying maximum entropy es-
timation for feature forests, we compared three models
trained with the same training set and the same set of fea-
tures. Following the experimental results, we concluded
that i) a parser with the syntax probability only achieved
high accuracy with the lexicalized grammar, ii) the in-
corporation of preferences for lexical association through
the semantics probability resulted in significant improve-
ments, and iii) our model recorded an accuracy that was
quite close to the traditional model, which indicated the
validity of applying maximum entropy estimation for fea-
ture forests.
In what follows, we first describe the existing models
for syntactic disambiguation, and discuss problems with
them in Section 2. We then define the general form for
parsing results of lexicalized grammars, and introduce
our model in Section 3. We prove the validity of our ap-
proach through a series of experiments in Section 4.
</bodyText>
<sectionHeader confidence="0.839153" genericHeader="method">
2 Traditional models for syntactic
disambiguation
</sectionHeader>
<bodyText confidence="0.999947916666667">
This section reviews the existing models for syntactic dis-
ambiguation from the viewpoint of representing parsing
results of lexicalized grammars. In particular, we dis-
cuss how the models incorporate syntactic/semantic pref-
erences for syntactic disambiguation. The existing stud-
ies are based on the decomposition of parsing results into
primitive lexical dependencies where syntactic/semantic
preferences are combined. This traditional scheme of
syntactic disambiguation can be problematic with lexi-
calized grammars. Throughout the discussion, we refer
to the example sentence “What does your student want to
write?”, whose parse tree is in Figure 1.
</bodyText>
<subsectionHeader confidence="0.997165">
2.1 Lexicalized parse trees
</subsectionHeader>
<bodyText confidence="0.999860857142857">
The first successful work on syntactic disambiguation
was based on lexicalized probabilistic context-free gram-
mar (LPCFG) (Collins, 1997; Charniak, 1997). Although
LPCFG is not exactly classified into lexicalized grammar
formalism, we should mention these studies since they
demonstrated that lexical dependencies were essential to
improving the accuracy of parsing.
</bodyText>
<figure confidence="0.997786714285714">
S
what
does
S
S
NP VP
to write
</figure>
<figureCaption confidence="0.9533575">
Figure 1: A parse tree for “What does your student want
to write?”
</figureCaption>
<figure confidence="0.87783">
Swant
</figure>
<figureCaption confidence="0.999589">
Figure 2: A lexicalized parse tree
</figureCaption>
<bodyText confidence="0.999979545454545">
A lexicalized parse tree is an extension of a parse tree
that is achieved by augmenting each non-terminal with its
lexical head. There is an example of a lexicalized parse
tree in Figure 2, which is a lexicalized version of the one
in Figure 1. A lexicalized parse tree is represented by
a set of branchings in the tree1: T = {(whi, wui, ri)},
where whi is a head word, wui the head word of a
non-head, and ri a grammar rule corresponding to each
branching. LPCFG models yield a probability of the
complete parse tree T = {(whi, wui, ri)} by the prod-
uct of probabilities of branchings in it.
</bodyText>
<equation confidence="0.840981">
p(T ) = � p(whi,wui,ri|η),
i
</equation>
<bodyText confidence="0.999964666666667">
where η is a condition of the probability, which is usually
the nonterminal symbol of the mother node. Since each
branching is augmented with the lexical heads of non-
terminals in the rule, the model can capture lexical de-
pendencies, which increase the accuracy. This is because
lexical dependencies approximately represent the seman-
tic preference of a sentence. As is well known, a syntactic
structure is not accurately disambiguated only with syn-
tactic preferences, and the incorporation of approximate
</bodyText>
<footnote confidence="0.773114">
1For simplicity, we have assumed parse trees are only com-
posed of binary branchings.
</footnote>
<bodyText confidence="0.99767775">
semantic preferences was the key to improving the accu-
racy of syntactic disambiguation.
We should note that this model has the following three
disadvantages.
</bodyText>
<listItem confidence="0.89090675">
1. The model fails to represent some linguistic depen-
dencies, including long-distance dependencies and
argument/modifier distinctions. Since an existing
study incorporates these relations ad hoc (Collins,
1997), they are apparently crucial in accurate dis-
ambiguation. This is also problematic for providing
a sufficient representation of semantics.
2. The model assumes the statistical independence of
branchings, which is apparently not preserved. For
example, the ambiguity of PP-attachments should be
resolved by considering three words: the modifiee of
the PP, its preposition, and the object of the PP.
3. The preferences of syntax and semantics are com-
bined in the lexical dependencies of two words,
i.e., features for syntactic preference and those for
semantic preference are not distinguished in the
</listItem>
<bodyText confidence="0.871114142857143">
model. Lexicalized grammars formalize the con-
straints of the relations between syntax and seman-
tics, but the model does not assume the existence
of such constraints. The model prevents further im-
provements to the syntax/semantics models; in addi-
tion to the linguistic analysis of the relation between
syntax and semantics.
</bodyText>
<subsectionHeader confidence="0.99828">
2.2 Derivation trees
</subsectionHeader>
<bodyText confidence="0.994848636363637">
Recent work on the automatic extraction of LTAG (Xia,
1999; Chen and Vijay-Shanker, 2000) and disambigua-
tion models (Chiang, 2000) has been the first on the sta-
tistical model for syntactic disambiguation based on lexi-
calized grammars. However, the models are based on the
lexical dependencies of elementary trees, which is a sim-
ple extension of the LPCFG. That is, the models are still
based on decomposition into primitive lexical dependen-
cies.
Derivation trees, the structural description in LTAG
(Schabes et al., 1988), represent the association of lex-
ical items i.e., elementary trees. In LTAG, all syntactic
constraints of words are described in an elementary tree,
and the dependencies of elementary trees, i.e., a deriva-
tion tree, describe the semantic relations of words more
directly than lexicalized parse trees. For example, Fig-
ure 3 has a derivation tree corresponding to the parse
tree in Figure 12. The dotted lines represent substitu-
tion while the solid lines represent adjunction. We should
note that the relations captured by ad-hoc augmentation
2The nodes in a derivation tree are denoted with the names
of the elementary trees, while we have omitted details.
</bodyText>
<figure confidence="0.985512714285714">
your student want
VP
NPstudent
your student want
VPwant
VPwrite
Swant
Swant
what
does
to write
write
what does student want to
your
</figure>
<figureCaption confidence="0.999879">
Figure 3: A derivation tree
</figureCaption>
<bodyText confidence="0.999268">
of lexicalized parse trees, such as the distinction of argu-
ments/modifiers and unbounded dependencies (Collins,
1997), are elegantly represented in derivation trees. For-
mally, a derivation tree is represented as a set of depen-
dencies: D = {(αi, ηαj, ri)}, where αi is an elemen-
tary tree, ηαi represents a node in αj where substitu-
tion/adjunction has occurred, and ri is a label of the ap-
plied rule, i.e., adjunction or substitution.
A probability of derivation tree D = {(αi, ηαj, ri)} is
generally defined as follows (Schabes et al., 1988; Chi-
ang, 2000).
</bodyText>
<equation confidence="0.9958675">
p(D) = � p(αi|ηαj , ri)
i
</equation>
<bodyText confidence="0.999968">
Note that each probability on the right represents the syn-
tactic/semantic preference of a dependency of two lexical
items. We can readily see that the model is very similar
to LPCFG models.
The first problem with LPCFG is partially solved
by this model, since the dependencies not represented
in LPCFG (e.g., long-distance dependencies and ar-
gument/modifier distinctions) are elegantly represented,
while some relations (e.g., the control relation between
“want” and “student”) are not yet represented. However,
the other two problems remain unsolved in this model.
In particular, when we apply Feature-Based LTAG (FB-
LTAG), the above probability is no longer consistent be-
cause of the non-local constraints caused by feature uni-
fication (Abney, 1997).
</bodyText>
<subsectionHeader confidence="0.997708">
2.3 Dependency structures
</subsectionHeader>
<bodyText confidence="0.999913384615385">
A disambiguation model for wide-coverage CCG (Clark
et al., 2002) aims at representing deep linguistic depen-
dencies including long-distance dependencies and con-
trol relations. This model can represent all the syntac-
tic/semantic dependencies of words in a sentence. How-
ever, the statistical model is still a mere extension of
LPCFG, i.e., it is based on decomposition into primitive
lexical dependencies.
In this model, a lexicalized grammar defines the map-
ping from a sentence into dependency structures, which
represent all the necessary dependencies of words in a
sentence, including long-distance dependencies and con-
trol relations. There is an example in Figure 4, which
</bodyText>
<figure confidence="0.9196605">
write
your
</figure>
<figureCaption confidence="0.999827">
Figure 4: A dependency structure
</figureCaption>
<bodyText confidence="0.9972467">
corresponds to the parse tree in Figure 1. Note that this
representation includes a dependency not represented in
the derivation tree (the control relation between “want”
and “student”). A dependency structure is formally de-
fined as a set of dependencies: S = {(whi, wni,ηi)},
where whi and wni are a head and argument word of the
dependency, and ηi is an argument position of the head
word filled by the argument word.
An existing model assigns a probability value to de-
pendency structure S = {(whi, wni, ηi)} as follows.
</bodyText>
<equation confidence="0.9930365">
�p = p(wni|whi,ηi)
i
</equation>
<bodyText confidence="0.999465076923077">
Primitive probability is approximated by the relative fre-
quency of lexical dependencies of two words in a training
corpus.
Since dependency structures include all necessary de-
pendency relations, the first problem with LPCFG is now
completely solved. However, the third problem still re-
mains unsolved. The probability of a complete parse tree
is defined as the product of probabilities of primitive de-
pendencies of two words. In addition, the second prob-
lem is getting worse; the independence assumption is ap-
parently violated in this model, since the possible depen-
dency structures are restricted by the grammar. The prob-
ability model is no longer consistent.
</bodyText>
<sectionHeader confidence="0.9856445" genericHeader="method">
3 Probability Model based on Lexicalized
Grammars
</sectionHeader>
<bodyText confidence="0.999642266666666">
This section introduces our model of syntactic disam-
biguation, which is based on the decomposition of the
parsing model into the syntax and semantics models. The
concept behind it is that the plausibility of a parsing re-
sult is determined by i) the plausibility of syntax, and ii)
selecting the most probable semantics from the structures
allowed by the given syntax. This section formalizes the
general form of statistical models for disambiguation of
parsing including lexicalized parse trees, derivation trees,
and dependency structures. Problems with the existing
models are then discussed, and our model is introduced.
Suppose that a set W of words and a set C of syn-
tactic categories (e.g., nonterminal symbols of CFG, ele-
mentary trees of LTAG, feature structures of HPSG (Sag
and Wasow, 1999)) are given. A lexicalized grammar is
</bodyText>
<table confidence="0.9339015">
ARG1
MODIFY
what student ARG1 want does to
MODIFY
ARG2
Lexicalized parse tree
(write, what, SS write S),
(write, does, SS does S),
(write, student, SS NP VP),
(student, your, NPS your student),
(write, want, VPS want VP),
(write, to, VPS to write)
Derivation tree
(write, what, SUBST),
(write, does, ADJ),
(write, student, SUBST),
(student, your, ADJ),
(write, want, ADJ),
(write, to, ADJ)
Dependency structure
</table>
<figureCaption confidence="0.84004125">
(write, what, ARG2),
(write, does, MODIFY),
(write, student, ARG1),
(student, your, MODIFY),
(write, want, MODIFY),
(want, student, ARG1),
(write, to, MODIFY)
Figure 5: Parsing results of lexicalized grammars
</figureCaption>
<bodyText confidence="0.99941225">
then defined as a tuple G = (L, R), where L = {l =
(w, c)|w E W, c E C} is a lexicon and R is a set of
grammar rules. A parsing result of lexicalized gram-
mars is defined as a labeled graph structure A = {a|a =
(lh,ln, d)}, where a is an edge representing the depen-
dency of head lh and argument ln labeled with d. For
example, the lexicalized parse tree in Figure 2 is repre-
sented in this form as in Figure 5, as well as the derivation
tree and the dependency structure.
Given the above definition, the existing models dis-
cussed in Section 2 yield a probability P(A|w) for given
sentence w as in the following general form.
</bodyText>
<equation confidence="0.989736">
P(A|w) = � p(a|η),
a∈A
</equation>
<bodyText confidence="0.999991541666667">
In short, the probability of the complete structure is de-
fined as the product of probabilities of lexical depen-
dencies. For example, p(a|η) corresponds to the prob-
ability of branchings in LPCFG models, that of substi-
tution/adjunction in derivation tree models, and that of
primitive dependencies in dependency structure models.
The models, however, have a crucial weakness with
lexicalized grammar formalism; probability values are
assigned to parsing results not allowed by the grammar,
i.e., the model is no longer consistent. Hence, the disam-
biguation model of lexicalized grammars should not be
decomposed into primitive lexical dependencies.
A possible solution to this problem is to directly es-
timate p(A|w) by applying a maximum entropy model
(Berger et al., 1996). However, such modeling will lead
us to extensive tweaking of features that is theoretically
unjustifiable, and will not contribute to the theoretical
investigation of the relations of syntax and semantics.
Since lexicalized grammars express all syntactic con-
straints by syntactic categories of words, we have as-
sumed that we first determine which syntactic category c
should be chosen, and then determine which argument re-
lations are likely to appear under the constraints imposed
by the syntactic categories. Formally,
</bodyText>
<equation confidence="0.980913">
p(A|w) = p(c|w)p(A|c).
</equation>
<bodyText confidence="0.9999266875">
The first probability in the above formula is the prob-
ability of syntactic categories, i.e., the probability of se-
lecting a sequence of syntactic categories in a sentence.
Since syntactic categories in lexicalized grammars deter-
mine the syntactic constraints of words, this expresses the
syntactic preference of each word in a sentence. Note that
our objective is not only to improve parsing accuracy but
also to investigate the relation between syntax and seman-
tics. We have not adopted the local contexts of words as
in the supertaggers in LTAG (Joshi and Srinivas, 1994)
because they partially include the semantic preferences
of a sentence. The probability is purely unigram to se-
lect the probable syntactic category for each word. The
probability is then given by the product of probabilities
to select a syntactic category for each word from a set of
candidate categories allowed by the lexicon.
</bodyText>
<equation confidence="0.9754465">
p(c|w) = � p(ci|wi)
i
</equation>
<bodyText confidence="0.999814045454546">
The second describes the probability of semantics,
which expresses the semantic preferences of relating the
words in a sentence. Note that the semantics probabil-
ity is dependent on the syntactic categories determined
by the syntax probability, because in lexicalized grammar
formalism, a series of syntactic categories determines the
possible structures of parsing results. Parsing results are
obtained by solving the constraints given by the grammar.
Hence, we cannot simply decompose semantics probabil-
ity into the dependency probabilities of two words. We
define semantics probability as a discriminative model
that selects the most probable parsing result from a set
of candidates given by parsing.
Since semantics probability cannot be decomposed
into independent sub-events, we applied a maximum en-
tropy model, which allowed probabilistic modeling with-
out the independence assumption. Using this model, we
can assign consistent probabilities to parsing results with
complex structures, such as ones represented with feature
structures (Abney, 1997; Johnson et al., 1999). Given
parsing result A, semantics probability is defined as fol-
lows:
</bodyText>
<equation confidence="0.926598">
�1 �p(A|c) = Z exp E λ(s)
s∈S(A)
exp E λ(s&apos;) ,
(s&apos;ES(A&apos;) )
</equation>
<bodyText confidence="0.9998420625">
where S(A) is a set of connected subgraphs of A, λ(s)
is a weight of subgraph s, and A(c) is a set of parsing
results allowed by the sequence of syntactic categories c.
Since we aim at separating syntactic and semantic pref-
erences, feature functions for semantic probability distin-
guish only words, not syntactic categories. We should
note that subgraphs should not be limited to an edge, i.e.,
the lexical dependency of two words. By taking more
than one edge as a subgraph, we can represent the depen-
dency of more than two words, although existing mod-
els do not adopt such dependencies. Various ambigui-
ties should be resolved by considering the dependency
of more than two words; e.g. PP-attachment ambiguity
should be resolved by the dependency of three words.
Consequently, the probability model takes the follow-
ing form.
</bodyText>
<equation confidence="0.954573666666667">
� 
p(A |w) = S
np(ci|
i
wi) Z exp
c
</equation>
<bodyText confidence="0.9999694">
However, this model has a crucial flaw: the maxi-
mum likelihood estimation of semantics probability is
intractable. This is because the estimation requires Zc
to be computed, which requires summation over A(c),
exponentially many parsing results. To cope with this
problem, we applied an efficient algorithm of maximum
entropy estimation for feature forests (Miyao and Tsu-
jii, 2002; Geman and Johnson, 2002). This enabled
the tractable estimation of the above probability, when
a set of candidates are represented in a feature forest of a
tractable size.
Here, we should mention that the disadvantages of the
traditional models discussed in Section 2 have been com-
pletely solved by this model. It can be applied to any
parsing results given by a lexicalized grammar, does not
require the independence assumption, and is defined as a
combination of syntax and semantics probabilities, where
the semantics probability is a discriminative model that
selects a parsing result from the set of candidates given
by the syntax probability.
</bodyText>
<sectionHeader confidence="0.999299" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999968766666667">
The model proposed in Section 3 is generally applica-
ble to any lexicalized grammars, and this section reports
the evaluation of our model with a wide-coverage LTAG
grammar, which is automatically acquired from the Penn
Treebank (Marcus et al., 1994) Sections 02–21. The
grammar was acquired by an algorithm similar to (Xia,
1999), and consisted of 2,105 elementary trees, where
1,010 were initial trees and 1,095 were auxiliary ones.
The coverage of the grammar against Section 22 (1,700
sentences) was 92.6% (1,575 sentences) in a weak sense
(i.e., the grammar could output a structure consistent with
the bracketing in the test corpus), and 68.0% (1,156 sen-
tences) in a strong sense (i.e., the grammar could output
exactly the correct derivation).
Since the grammar acquisition algorithm could output
derivation trees for the sentences in the training corpus
(Section 02–21), we used them as a training set of the
probability model. The model of syntax probability was
estimated with syntactic categories appearing in the train-
ing set. For estimating the semantics probability, a parser
produced all possible derivation trees for each sequence
of syntactic categories (corresponding to each sentence)
in the training set, and the obtained derivation trees, i.e.,
A(c), are passed to a maximum entropy estimator. By ap-
plying the grammar acquisition algorithm to Section 22,
we obtained the derivation trees of the sentences in this
section, and from this set we prepared a test set by elim-
inating non-sententials, long sentences (including more
than 40 words), sentences not covered by the grammar,
and sentences that caused time-outs in parsing. The re-
sulting set consisted of 917 derivation trees.
The following three disambiguation models were pre-
pared using the training set.
syntax Only composed of the syntax probability, i.e.,
p(c|w)
traditional Similar to our model, but semantics proba-
bility p(A|c) was decomposed into the probabilities
of the primitive dependencies of two words as in the
traditional modeling, i.e., this model is an inconsis-
tent probability model
our model The model by maximum entropy estimation
for feature forests
The syntax probability was a unigram model, and con-
texts around the word such as previous words/categories
were not used. Hence, it includes only syntactic prefer-
ences of words. The semantics parts of traditional and
our model were maximum entropy models, where ex-
actly the same set of features were used, i.e., the differ-
ence between the two models was only in an event repre-
sentation: derivation trees were decomposed into primi-
tive dependencies in traditional, while in our model they
were represented by a feature forest without decompo-
sition. Hence, we can evaluate the effects of applying
maximum entropy estimation for feature forests by com-
paring our model with traditional. While our model al-
lowed features to be incorporated that were not limited
to the dependencies of two words (Section 3), the models
used throughout the experiments only included features
of the dependencies of two words. The semantics proba-
bilities were developed with two sets of features includ-
</bodyText>
<equation confidence="0.7352554">
�Zc =
A&apos;EA(c)
1
(sES(A)
λ (s) ) 
</equation>
<bodyText confidence="0.34544325">
exact partial
syntax
traditional
our model
</bodyText>
<tableCaption confidence="0.668275666666667">
Table 1: Accuracy of dependencies (1)
exact partial
syntax
traditional
our model
Table 2: Accuracy of dependencies (2)
</tableCaption>
<bodyText confidence="0.999960361111111">
ing surface forms/POSs of words, the labels of dependen-
cies (substitution/adjunction), and the distance between
two words. The first feature set had 283,755 features
and the other had 150,156 features excluding fine-grained
features of the first set. There were 701,819 events for
traditional, and 32,371 for our model. The difference in
the number of events was caused by the difference in the
units of events, i.e., an event corresponded to a depen-
dency in traditional, while it corresponded to a sentence
in our model.
The parameters of the models were estimated by the
limited-memory BFGS algorithm (Nocedal, 1980) with
a Gaussian distribution as the prior probability distri-
bution for smoothing (Chen and Rosenfeld, 1999) im-
plemented in a maximum entropy estimator for feature
forests (Miyao, 2002). The estimation for traditional was
converged in 67 iterations in 127 seconds, and our model
in 29 iterations in 111 seconds on a Pentium III 1.26-GHz
CPU with 4 GB of memory. These results reveal that the
estimation with our model is comparatively efficient with
traditional. The parsing algorithm was CKY-style pars-
ing with beam thresholding, which was similar to ones
used in (Collins, 1996; Clark et al., 2002). Although
we needed to compute normalizing factor Z. to obtain
probability values, we used unnormalized products as the
preference score for beam thresholding, following (Clark
et al., 2002). We did not use any preprocessing such as
supertagging (Joshi and Srinivas, 1994) and the parser
searched for the most plausible derivation tree from the
derivation forest in terms of the probability given by the
combination of syntax and semantics probabilities.
Tables 1 and 2 list the accuracy of dependencies, i.e.,
edges in derivation trees, for each model with two sets
of features for the semantics model3. Since in derivation
trees each word in a sentence depends on one and only
one word (see Figure 3), the accuracy is the number of
</bodyText>
<footnote confidence="0.739705">
3Since the features of the syntax part were not changed, the
results for syntax are exactly the same.
</footnote>
<bodyText confidence="0.999963923076923">
correct edges divided by the number of all edges in the
tree. The exact column indicates the ratio of dependen-
cies where the syntactic category, the argument position,
and the dependee head word of the argument word are
correctly output. The partial column shows the ratio of
dependencies where the words are related regardless of
the label. We should note that the exact measure is a very
stringent because the model must select the correct syn-
tactic category from 2,105 categories.
First, we can see that syntax achieved a high level of
accuracy although it was not quite sufficient yet. We
think this was because the grammar could adequately re-
strict the possible structure of parsing results, and the dis-
ambiguation model tried to search for the most probable
structure from the candidates allowed by the grammar.
Second, traditional and our model recorded significantly
higher accuracy than syntax. The accuracy of our model
was almost matched traditional, which proved the valid-
ity of probabilistic modeling with maximum entropy es-
timation for feature forests. The differences between tra-
ditional and our model were insignificant and the results
proved that a consistent probability model of parsing can
be built without the independence assumption, and attains
performance that rivals the traditional models in terms of
parsing accuracy.
We should note that accuracy can further be improved
with our model because it allows other features to be in-
corporated that were not used in these experiments be-
cause the model is not rely on the decomposition into
the dependencies of two words. Another possibility to
increase the accuracy is to refine the LTAG grammar. Al-
though we assumed that all syntactic constraints were
expressed with syntactic categories (Section 3), i.e., el-
ementary trees, the grammar used in the experiments
were not augmented with feature structures and not suffi-
ciently restrictive to eliminate syntactically invalid struc-
tures. Since our model did not include the preferences of
syntactic relations of words, we expect the refinement of
the grammar will greatly improve the accuracy.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999964615384615">
This paper described a novel model for syntactic dis-
ambiguation based on lexicalized grammars, where the
model selects the most probable parsing result from the
candidates allowed by a lexicalized grammar. Since lex-
icalized grammars can restrict the possible structure of
parsing results, the probabilistic model cannot simply
be decomposed into independent events as in the ex-
isting disambiguation models for parsing. By apply-
ing a maximum entropy model for feature forests, we
achieved probabilistic modeling without decomposition.
Through experiments, we proved the syntax-only model
could record with high level of accuracy with a lexical-
ized grammar, and maximum entropy estimation for fea-
</bodyText>
<figure confidence="0.977967666666667">
73.4 77.3
79.2 83.4
79.6 83.6
73.4 77.3
79.6 83.6
78.9 82.8
</figure>
<bodyText confidence="0.9998388">
ture forests could attain competitive accuracy compared
to the traditional model. We see this work as the first step
in the application of linguistically motivated grammars to
the parsing of real-world texts as well as the evaluation of
linguistic theories by consulting extensive corpora.
Future work should include the application of our
model to other lexicalized grammars including HPSG.
The development of sophisticated parsing strategies is
also required to improve the accuracy and efficiency of
parsing. Since parsing results of lexicalized grammars
such as HPSG and CCG can include non-local dependen-
cies, we cannot simply apply well-known parsing strate-
gies, such as beam thresholding, which assume the local
computation of preference scores. Further investigations
must be left for future research.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999614951219512">
Steven P. Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4).
Adam L. Berger, Stephen A. Della Pietra, and Vincent.
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39–71.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of 14th National Conference on Artificial Intelli-
gence, pages 598–603.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian
prior for smoothing maximum entropy models. Tech-
nical Report CMUCS-99-108, Carnegie Mellon Uni-
versity.
John Chen and K. Vijay-Shanker. 2000. Automated ex-
traction of TAGs from the Penn Treebank. In Proceed-
ings of 6th IWPT.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings ofACL 2000, pages 456–463.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building deep dependency structures with a
wide-coverage CCG parser. In Proceedings of 40th
ACL.
Michael Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proceedings of 34th
ACL, pages 184–191.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of 35th
ACL.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochastic
unification-based grammars. In Proceedings of 40th
ACL, pages 279–286.
Julia Hockenmaier and Mark Steedman. 2002a. Acquir-
ing compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of 3rd LREC.
Julia Hockenmaier and Mark Steedman. 2002b. Gen-
erative models for statistical parsing with Combina-
tory Categorial Grammar. In Proceedings of 40th ACL,
pages 335–342.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
“unification-based” grammars. In Proceedings of 37th
ACL, pages 535–541.
Aravind K. Joshi and B. Srinivas. 1994. Disambiguation
of super parts of speech (or supertags): Almost pars-
ing. In Proceedings of 17th COLING, pages 161–165.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop.
Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proceedings of
HLT 2002.
Yusuke Miyao. 2002. Amis – a maximum entropy es-
timator for feature forests. Available via http://www-
tsujii.is.s.u-tokyo.ac.jp/%7Eyusuke/amis/.
Jorge Nocedal. 1980. Updating quasi-Newton matrices
with limited storage. Mathematics of Computation,
35:773–783.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proceedings of 38th ACL.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of 40th ACL.
Ivan A. Sag and Thomas Wasow. 1999. Syntactic Theory
– A Formal Introduction. CSLI Lecture Notes no. 92.
CSLI Publications.
Yves Schabes, Anne Abeill´e, and Aravind K. Joshi.
1988. Parsing strategies with ‘lexicalized grammars’:
Application to tree adjoining grammars. In Proceed-
ings of 12th COLING, pages 578–583.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of 5th NLPRS.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.920023">
<title confidence="0.998056">A model of syntactic disambiguation based on lexicalized grammars</title>
<author confidence="0.999144">Yusuke Jun’ichi</author>
<affiliation confidence="0.99709375">Department of Computer Department of Computer University of University of yusuke@is.s.u-tokyo.ac.jp CREST, (Japan Science and Technology</affiliation>
<email confidence="0.936466">tsujii@is.s.u-tokyo.ac.jp</email>
<abstract confidence="0.999779">This paper presents a new approach to syntactic disambiguation based on lexicalized grammars. While existing disambiguation models decompose the probability of parsing results into that of primitive dependencies of two words, our model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="2640" citStr="Abney, 1997" startWordPosition="373" endWordPosition="374">guation models of lexicalized grammars should be totally different from that of LPCFG, because the grammars define the relation of syntax and semantics, and can restrict the possible structure of parsing results. Parsing results cannot simply be decomposed into primitive dependencies, because the complete structure is determined by solving the syntactic constraints of a complete sentence. For example, when we apply a unification-based grammar, LPCFG-like modeling results in an inconsistent probability model because the model assigns probabilities to parsing results not allowed by the grammar (Abney, 1997). We have only two ways of adhering to LPCFG models: preserve the consistency of probability models by abandoning improvements to the lexicalized grammars using complex constraints (Chiang, 2000), or ignore the inconsistency in probability models (Clark et al., 2002). This paper provides a new model of syntactic disambiguation in which lexicalized grammars can restrict the possible structures of parsing results. Our modeling aims at providing grounds for i) producing a consistent probabilistic model of lexicalized grammars, as well as ii) evaluating the contributions of syntactic and semantic </context>
<context position="14762" citStr="Abney, 1997" startWordPosition="2266" endWordPosition="2267">see that the model is very similar to LPCFG models. The first problem with LPCFG is partially solved by this model, since the dependencies not represented in LPCFG (e.g., long-distance dependencies and argument/modifier distinctions) are elegantly represented, while some relations (e.g., the control relation between “want” and “student”) are not yet represented. However, the other two problems remain unsolved in this model. In particular, when we apply Feature-Based LTAG (FBLTAG), the above probability is no longer consistent because of the non-local constraints caused by feature unification (Abney, 1997). 2.3 Dependency structures A disambiguation model for wide-coverage CCG (Clark et al., 2002) aims at representing deep linguistic dependencies including long-distance dependencies and control relations. This model can represent all the syntactic/semantic dependencies of words in a sentence. However, the statistical model is still a mere extension of LPCFG, i.e., it is based on decomposition into primitive lexical dependencies. In this model, a lexicalized grammar defines the mapping from a sentence into dependency structures, which represent all the necessary dependencies of words in a senten</context>
<context position="22159" citStr="Abney, 1997" startWordPosition="3442" endWordPosition="3443"> grammar. Hence, we cannot simply decompose semantics probability into the dependency probabilities of two words. We define semantics probability as a discriminative model that selects the most probable parsing result from a set of candidates given by parsing. Since semantics probability cannot be decomposed into independent sub-events, we applied a maximum entropy model, which allowed probabilistic modeling without the independence assumption. Using this model, we can assign consistent probabilities to parsing results with complex structures, such as ones represented with feature structures (Abney, 1997; Johnson et al., 1999). Given parsing result A, semantics probability is defined as follows: �1 �p(A|c) = Z exp E λ(s) s∈S(A) exp E λ(s&apos;) , (s&apos;ES(A&apos;) ) where S(A) is a set of connected subgraphs of A, λ(s) is a weight of subgraph s, and A(c) is a set of parsing results allowed by the sequence of syntactic categories c. Since we aim at separating syntactic and semantic preferences, feature functions for semantic probability distinguish only words, not syntactic categories. We should note that subgraphs should not be limited to an edge, i.e., the lexical dependency of two words. By taking more </context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven P. Abney. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of 14th National Conference on Artificial Intelligence,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="1560" citStr="Charniak, 1997" startWordPosition="216" endWordPosition="217">t probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition ofparsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, this idea has been naively inherited in the recent studies on disambiguation models of lexicalized grammars. However, the disambiguation models of lexicalized grammars should be totally different from that of LPCFG, because the grammars define the relation of s</context>
<context position="9218" citStr="Charniak, 1997" startWordPosition="1372" endWordPosition="1373">c preferences for syntactic disambiguation. The existing studies are based on the decomposition of parsing results into primitive lexical dependencies where syntactic/semantic preferences are combined. This traditional scheme of syntactic disambiguation can be problematic with lexicalized grammars. Throughout the discussion, we refer to the example sentence “What does your student want to write?”, whose parse tree is in Figure 1. 2.1 Lexicalized parse trees The first successful work on syntactic disambiguation was based on lexicalized probabilistic context-free grammar (LPCFG) (Collins, 1997; Charniak, 1997). Although LPCFG is not exactly classified into lexicalized grammar formalism, we should mention these studies since they demonstrated that lexical dependencies were essential to improving the accuracy of parsing. S what does S S NP VP to write Figure 1: A parse tree for “What does your student want to write?” Swant Figure 2: A lexicalized parse tree A lexicalized parse tree is an extension of a parse tree that is achieved by augmenting each non-terminal with its lexical head. There is an example of a lexicalized parse tree in Figure 2, which is a lexicalized version of the one in Figure 1. A </context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of 14th National Conference on Artificial Intelligence, pages 598–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMUCS-99-108,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="28266" citStr="Chen and Rosenfeld, 1999" startWordPosition="4426" endWordPosition="4429">ce between two words. The first feature set had 283,755 features and the other had 150,156 features excluding fine-grained features of the first set. There were 701,819 events for traditional, and 32,371 for our model. The difference in the number of events was caused by the difference in the units of events, i.e., an event corresponded to a dependency in traditional, while it corresponded to a sentence in our model. The parameters of the models were estimated by the limited-memory BFGS algorithm (Nocedal, 1980) with a Gaussian distribution as the prior probability distribution for smoothing (Chen and Rosenfeld, 1999) implemented in a maximum entropy estimator for feature forests (Miyao, 2002). The estimation for traditional was converged in 67 iterations in 127 seconds, and our model in 29 iterations in 111 seconds on a Pentium III 1.26-GHz CPU with 4 GB of memory. These results reveal that the estimation with our model is comparatively efficient with traditional. The parsing algorithm was CKY-style parsing with beam thresholding, which was similar to ones used in (Collins, 1996; Clark et al., 2002). Although we needed to compute normalizing factor Z. to obtain probability values, we used unnormalized pro</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical Report CMUCS-99-108, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Automated extraction of TAGs from the Penn Treebank.</title>
<date>2000</date>
<booktitle>In Proceedings of 6th IWPT.</booktitle>
<contexts>
<context position="1139" citStr="Chen and Vijay-Shanker, 2000" startWordPosition="157" endWordPosition="160">e dependencies of two words, our model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition ofparsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependenci</context>
<context position="12181" citStr="Chen and Vijay-Shanker, 2000" startWordPosition="1847" endWordPosition="1850">e preferences of syntax and semantics are combined in the lexical dependencies of two words, i.e., features for syntactic preference and those for semantic preference are not distinguished in the model. Lexicalized grammars formalize the constraints of the relations between syntax and semantics, but the model does not assume the existence of such constraints. The model prevents further improvements to the syntax/semantics models; in addition to the linguistic analysis of the relation between syntax and semantics. 2.2 Derivation trees Recent work on the automatic extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000) and disambiguation models (Chiang, 2000) has been the first on the statistical model for syntactic disambiguation based on lexicalized grammars. However, the models are based on the lexical dependencies of elementary trees, which is a simple extension of the LPCFG. That is, the models are still based on decomposition into primitive lexical dependencies. Derivation trees, the structural description in LTAG (Schabes et al., 1988), represent the association of lexical items i.e., elementary trees. In LTAG, all syntactic constraints of words are described in an elementary tree, and the dependenci</context>
</contexts>
<marker>Chen, Vijay-Shanker, 2000</marker>
<rawString>John Chen and K. Vijay-Shanker. 2000. Automated extraction of TAGs from the Penn Treebank. In Proceedings of 6th IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>456--463</pages>
<contexts>
<context position="1301" citStr="Chiang, 2000" startWordPosition="180" endWordPosition="181">icalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition ofparsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, th</context>
<context position="2835" citStr="Chiang, 2000" startWordPosition="401" endWordPosition="403">f parsing results. Parsing results cannot simply be decomposed into primitive dependencies, because the complete structure is determined by solving the syntactic constraints of a complete sentence. For example, when we apply a unification-based grammar, LPCFG-like modeling results in an inconsistent probability model because the model assigns probabilities to parsing results not allowed by the grammar (Abney, 1997). We have only two ways of adhering to LPCFG models: preserve the consistency of probability models by abandoning improvements to the lexicalized grammars using complex constraints (Chiang, 2000), or ignore the inconsistency in probability models (Clark et al., 2002). This paper provides a new model of syntactic disambiguation in which lexicalized grammars can restrict the possible structures of parsing results. Our modeling aims at providing grounds for i) producing a consistent probabilistic model of lexicalized grammars, as well as ii) evaluating the contributions of syntactic and semantic preferences to syntactic disambiguation. The model is composed of the syntax and semantics probabilities, which represent syntactic and semantic preferences respectively. The syntax probability i</context>
<context position="12222" citStr="Chiang, 2000" startWordPosition="1855" endWordPosition="1856"> lexical dependencies of two words, i.e., features for syntactic preference and those for semantic preference are not distinguished in the model. Lexicalized grammars formalize the constraints of the relations between syntax and semantics, but the model does not assume the existence of such constraints. The model prevents further improvements to the syntax/semantics models; in addition to the linguistic analysis of the relation between syntax and semantics. 2.2 Derivation trees Recent work on the automatic extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000) and disambiguation models (Chiang, 2000) has been the first on the statistical model for syntactic disambiguation based on lexicalized grammars. However, the models are based on the lexical dependencies of elementary trees, which is a simple extension of the LPCFG. That is, the models are still based on decomposition into primitive lexical dependencies. Derivation trees, the structural description in LTAG (Schabes et al., 1988), represent the association of lexical items i.e., elementary trees. In LTAG, all syntactic constraints of words are described in an elementary tree, and the dependencies of elementary trees, i.e., a derivatio</context>
<context position="13984" citStr="Chiang, 2000" startWordPosition="2144" endWordPosition="2146">what does student want to your Figure 3: A derivation tree of lexicalized parse trees, such as the distinction of arguments/modifiers and unbounded dependencies (Collins, 1997), are elegantly represented in derivation trees. Formally, a derivation tree is represented as a set of dependencies: D = {(αi, ηαj, ri)}, where αi is an elementary tree, ηαi represents a node in αj where substitution/adjunction has occurred, and ri is a label of the applied rule, i.e., adjunction or substitution. A probability of derivation tree D = {(αi, ηαj, ri)} is generally defined as follows (Schabes et al., 1988; Chiang, 2000). p(D) = � p(αi|ηαj , ri) i Note that each probability on the right represents the syntactic/semantic preference of a dependency of two lexical items. We can readily see that the model is very similar to LPCFG models. The first problem with LPCFG is partially solved by this model, since the dependencies not represented in LPCFG (e.g., long-distance dependencies and argument/modifier distinctions) are elegantly represented, while some relations (e.g., the control relation between “want” and “student”) are not yet represented. However, the other two problems remain unsolved in this model. In par</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In Proceedings ofACL 2000, pages 456–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Building deep dependency structures with a wide-coverage CCG parser.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th ACL.</booktitle>
<contexts>
<context position="1329" citStr="Clark et al., 2002" startWordPosition="184" endWordPosition="187"> be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition ofparsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, this idea has been naively inh</context>
<context position="2907" citStr="Clark et al., 2002" startWordPosition="411" endWordPosition="414">o primitive dependencies, because the complete structure is determined by solving the syntactic constraints of a complete sentence. For example, when we apply a unification-based grammar, LPCFG-like modeling results in an inconsistent probability model because the model assigns probabilities to parsing results not allowed by the grammar (Abney, 1997). We have only two ways of adhering to LPCFG models: preserve the consistency of probability models by abandoning improvements to the lexicalized grammars using complex constraints (Chiang, 2000), or ignore the inconsistency in probability models (Clark et al., 2002). This paper provides a new model of syntactic disambiguation in which lexicalized grammars can restrict the possible structures of parsing results. Our modeling aims at providing grounds for i) producing a consistent probabilistic model of lexicalized grammars, as well as ii) evaluating the contributions of syntactic and semantic preferences to syntactic disambiguation. The model is composed of the syntax and semantics probabilities, which represent syntactic and semantic preferences respectively. The syntax probability is responsible for determining the syntactic categories chosen by words i</context>
<context position="14855" citStr="Clark et al., 2002" startWordPosition="2277" endWordPosition="2280">rtially solved by this model, since the dependencies not represented in LPCFG (e.g., long-distance dependencies and argument/modifier distinctions) are elegantly represented, while some relations (e.g., the control relation between “want” and “student”) are not yet represented. However, the other two problems remain unsolved in this model. In particular, when we apply Feature-Based LTAG (FBLTAG), the above probability is no longer consistent because of the non-local constraints caused by feature unification (Abney, 1997). 2.3 Dependency structures A disambiguation model for wide-coverage CCG (Clark et al., 2002) aims at representing deep linguistic dependencies including long-distance dependencies and control relations. This model can represent all the syntactic/semantic dependencies of words in a sentence. However, the statistical model is still a mere extension of LPCFG, i.e., it is based on decomposition into primitive lexical dependencies. In this model, a lexicalized grammar defines the mapping from a sentence into dependency structures, which represent all the necessary dependencies of words in a sentence, including long-distance dependencies and control relations. There is an example in Figure</context>
<context position="28758" citStr="Clark et al., 2002" startWordPosition="4507" endWordPosition="4510">thm (Nocedal, 1980) with a Gaussian distribution as the prior probability distribution for smoothing (Chen and Rosenfeld, 1999) implemented in a maximum entropy estimator for feature forests (Miyao, 2002). The estimation for traditional was converged in 67 iterations in 127 seconds, and our model in 29 iterations in 111 seconds on a Pentium III 1.26-GHz CPU with 4 GB of memory. These results reveal that the estimation with our model is comparatively efficient with traditional. The parsing algorithm was CKY-style parsing with beam thresholding, which was similar to ones used in (Collins, 1996; Clark et al., 2002). Although we needed to compute normalizing factor Z. to obtain probability values, we used unnormalized products as the preference score for beam thresholding, following (Clark et al., 2002). We did not use any preprocessing such as supertagging (Joshi and Srinivas, 1994) and the parser searched for the most plausible derivation tree from the derivation forest in terms of the probability given by the combination of syntax and semantics probabilities. Tables 1 and 2 list the accuracy of dependencies, i.e., edges in derivation trees, for each model with two sets of features for the semantics mo</context>
</contexts>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>Stephen Clark, Julia Hockenmaier, and Mark Steedman. 2002. Building deep dependency structures with a wide-coverage CCG parser. In Proceedings of 40th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of 34th ACL,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="1528" citStr="Collins, 1996" startWordPosition="212" endWordPosition="213">ethod of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition ofparsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, this idea has been naively inherited in the recent studies on disambiguation models of lexicalized grammars. However, the disambiguation models of lexicalized grammars should be totally different from that of LPCFG, because the g</context>
<context position="28737" citStr="Collins, 1996" startWordPosition="4505" endWordPosition="4506">ory BFGS algorithm (Nocedal, 1980) with a Gaussian distribution as the prior probability distribution for smoothing (Chen and Rosenfeld, 1999) implemented in a maximum entropy estimator for feature forests (Miyao, 2002). The estimation for traditional was converged in 67 iterations in 127 seconds, and our model in 29 iterations in 111 seconds on a Pentium III 1.26-GHz CPU with 4 GB of memory. These results reveal that the estimation with our model is comparatively efficient with traditional. The parsing algorithm was CKY-style parsing with beam thresholding, which was similar to ones used in (Collins, 1996; Clark et al., 2002). Although we needed to compute normalizing factor Z. to obtain probability values, we used unnormalized products as the preference score for beam thresholding, following (Clark et al., 2002). We did not use any preprocessing such as supertagging (Joshi and Srinivas, 1994) and the parser searched for the most plausible derivation tree from the derivation forest in terms of the probability given by the combination of syntax and semantics probabilities. Tables 1 and 2 list the accuracy of dependencies, i.e., edges in derivation trees, for each model with two sets of features</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of 34th ACL, pages 184–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of 35th ACL.</booktitle>
<contexts>
<context position="1543" citStr="Collins, 1997" startWordPosition="214" endWordPosition="215">ing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition ofparsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, this idea has been naively inherited in the recent studies on disambiguation models of lexicalized grammars. However, the disambiguation models of lexicalized grammars should be totally different from that of LPCFG, because the grammars define </context>
<context position="9201" citStr="Collins, 1997" startWordPosition="1370" endWordPosition="1371">ntactic/semantic preferences for syntactic disambiguation. The existing studies are based on the decomposition of parsing results into primitive lexical dependencies where syntactic/semantic preferences are combined. This traditional scheme of syntactic disambiguation can be problematic with lexicalized grammars. Throughout the discussion, we refer to the example sentence “What does your student want to write?”, whose parse tree is in Figure 1. 2.1 Lexicalized parse trees The first successful work on syntactic disambiguation was based on lexicalized probabilistic context-free grammar (LPCFG) (Collins, 1997; Charniak, 1997). Although LPCFG is not exactly classified into lexicalized grammar formalism, we should mention these studies since they demonstrated that lexical dependencies were essential to improving the accuracy of parsing. S what does S S NP VP to write Figure 1: A parse tree for “What does your student want to write?” Swant Figure 2: A lexicalized parse tree A lexicalized parse tree is an extension of a parse tree that is achieved by augmenting each non-terminal with its lexical head. There is an example of a lexicalized parse tree in Figure 2, which is a lexicalized version of the on</context>
<context position="11149" citStr="Collins, 1997" startWordPosition="1691" endWordPosition="1692">nce of a sentence. As is well known, a syntactic structure is not accurately disambiguated only with syntactic preferences, and the incorporation of approximate 1For simplicity, we have assumed parse trees are only composed of binary branchings. semantic preferences was the key to improving the accuracy of syntactic disambiguation. We should note that this model has the following three disadvantages. 1. The model fails to represent some linguistic dependencies, including long-distance dependencies and argument/modifier distinctions. Since an existing study incorporates these relations ad hoc (Collins, 1997), they are apparently crucial in accurate disambiguation. This is also problematic for providing a sufficient representation of semantics. 2. The model assumes the statistical independence of branchings, which is apparently not preserved. For example, the ambiguity of PP-attachments should be resolved by considering three words: the modifiee of the PP, its preposition, and the object of the PP. 3. The preferences of syntax and semantics are combined in the lexical dependencies of two words, i.e., features for syntactic preference and those for semantic preference are not distinguished in the m</context>
<context position="13547" citStr="Collins, 1997" startWordPosition="2067" endWordPosition="2068">e 3 has a derivation tree corresponding to the parse tree in Figure 12. The dotted lines represent substitution while the solid lines represent adjunction. We should note that the relations captured by ad-hoc augmentation 2The nodes in a derivation tree are denoted with the names of the elementary trees, while we have omitted details. your student want VP NPstudent your student want VPwant VPwrite Swant Swant what does to write write what does student want to your Figure 3: A derivation tree of lexicalized parse trees, such as the distinction of arguments/modifiers and unbounded dependencies (Collins, 1997), are elegantly represented in derivation trees. Formally, a derivation tree is represented as a set of dependencies: D = {(αi, ηαj, ri)}, where αi is an elementary tree, ηαi represents a node in αj where substitution/adjunction has occurred, and ri is a label of the applied rule, i.e., adjunction or substitution. A probability of derivation tree D = {(αi, ηαj, ri)} is generally defined as follows (Schabes et al., 1988; Chiang, 2000). p(D) = � p(αi|ηαj , ri) i Note that each probability on the right represents the syntactic/semantic preference of a dependency of two lexical items. We can readi</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of 35th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Mark Johnson</author>
</authors>
<title>Dynamic programming for parsing and estimation of stochastic unification-based grammars.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th ACL,</booktitle>
<pages>279--286</pages>
<contexts>
<context position="4578" citStr="Geman and Johnson, 2002" startWordPosition="658" endWordPosition="661">imum entropy models (Berger et al., 1996) and support vector machines (Vapnik, 1995) provide grounds for this type of modeling, because it allows various dependent features to be incorporated into the model without the independence assumption. The above approach, however, has a serious deficiency: a lexicalized grammar assigns exponentially many parsing results because of local ambiguities in a sentence, which is problematic in estimating the parameters of a probability model. To cope with this, we adopted an algorithm of maximum entropy estimation for feature forests (Miyao and Tsujii, 2002; Geman and Johnson, 2002), which allows parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler</context>
<context position="23581" citStr="Geman and Johnson, 2002" startWordPosition="3682" endWordPosition="3685">e dependency of more than two words; e.g. PP-attachment ambiguity should be resolved by the dependency of three words. Consequently, the probability model takes the following form. �  p(A |w) = S np(ci| i wi) Z exp c However, this model has a crucial flaw: the maximum likelihood estimation of semantics probability is intractable. This is because the estimation requires Zc to be computed, which requires summation over A(c), exponentially many parsing results. To cope with this problem, we applied an efficient algorithm of maximum entropy estimation for feature forests (Miyao and Tsujii, 2002; Geman and Johnson, 2002). This enabled the tractable estimation of the above probability, when a set of candidates are represented in a feature forest of a tractable size. Here, we should mention that the disadvantages of the traditional models discussed in Section 2 have been completely solved by this model. It can be applied to any parsing results given by a lexicalized grammar, does not require the independence assumption, and is defined as a combination of syntax and semantics probabilities, where the semantics probability is a discriminative model that selects a parsing result from the set of candidates given by</context>
</contexts>
<marker>Geman, Johnson, 2002</marker>
<rawString>Stuart Geman and Mark Johnson. 2002. Dynamic programming for parsing and estimation of stochastic unification-based grammars. In Proceedings of 40th ACL, pages 279–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Acquiring compact lexicalized grammars from a cleaner treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of 3rd LREC.</booktitle>
<contexts>
<context position="1171" citStr="Hockenmaier and Steedman, 2002" startWordPosition="161" endWordPosition="164">ur model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition ofparsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have prov</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002a. Acquiring compact lexicalized grammars from a cleaner treebank. In Proceedings of 3rd LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th ACL,</booktitle>
<pages>335--342</pages>
<contexts>
<context position="1171" citStr="Hockenmaier and Steedman, 2002" startWordPosition="161" endWordPosition="164">ur model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition ofparsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have prov</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002b. Generative models for statistical parsing with Combinatory Categorial Grammar. In Proceedings of 40th ACL, pages 335–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of 37th ACL,</booktitle>
<pages>535--541</pages>
<contexts>
<context position="5169" citStr="Johnson et al., 1999" startWordPosition="746" endWordPosition="749">002; Geman and Johnson, 2002), which allows parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002) have had a similar motivation to ours. Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar. A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar. They assumed that parsing results would be suppressed to a reasonable number through using heuristic rules, or by carefully implementing a fully restrictive and wide-coverage grammar,</context>
<context position="22182" citStr="Johnson et al., 1999" startWordPosition="3444" endWordPosition="3447">ce, we cannot simply decompose semantics probability into the dependency probabilities of two words. We define semantics probability as a discriminative model that selects the most probable parsing result from a set of candidates given by parsing. Since semantics probability cannot be decomposed into independent sub-events, we applied a maximum entropy model, which allowed probabilistic modeling without the independence assumption. Using this model, we can assign consistent probabilities to parsing results with complex structures, such as ones represented with feature structures (Abney, 1997; Johnson et al., 1999). Given parsing result A, semantics probability is defined as follows: �1 �p(A|c) = Z exp E λ(s) s∈S(A) exp E λ(s&apos;) , (s&apos;ES(A&apos;) ) where S(A) is a set of connected subgraphs of A, λ(s) is a weight of subgraph s, and A(c) is a set of parsing results allowed by the sequence of syntactic categories c. Since we aim at separating syntactic and semantic preferences, feature functions for semantic probability distinguish only words, not syntactic categories. We should note that subgraphs should not be limited to an edge, i.e., the lexical dependency of two words. By taking more than one edge as a subg</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based” grammars. In Proceedings of 37th ACL, pages 535–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>B Srinivas</author>
</authors>
<title>Disambiguation of super parts of speech (or supertags): Almost parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of 17th COLING,</booktitle>
<pages>161--165</pages>
<contexts>
<context position="20755" citStr="Joshi and Srinivas, 1994" startWordPosition="3231" endWordPosition="3234">tactic categories. Formally, p(A|w) = p(c|w)p(A|c). The first probability in the above formula is the probability of syntactic categories, i.e., the probability of selecting a sequence of syntactic categories in a sentence. Since syntactic categories in lexicalized grammars determine the syntactic constraints of words, this expresses the syntactic preference of each word in a sentence. Note that our objective is not only to improve parsing accuracy but also to investigate the relation between syntax and semantics. We have not adopted the local contexts of words as in the supertaggers in LTAG (Joshi and Srinivas, 1994) because they partially include the semantic preferences of a sentence. The probability is purely unigram to select the probable syntactic category for each word. The probability is then given by the product of probabilities to select a syntactic category for each word from a set of candidate categories allowed by the lexicon. p(c|w) = � p(ci|wi) i The second describes the probability of semantics, which expresses the semantic preferences of relating the words in a sentence. Note that the semantics probability is dependent on the syntactic categories determined by the syntax probability, becau</context>
<context position="29031" citStr="Joshi and Srinivas, 1994" startWordPosition="4548" endWordPosition="4551"> in 127 seconds, and our model in 29 iterations in 111 seconds on a Pentium III 1.26-GHz CPU with 4 GB of memory. These results reveal that the estimation with our model is comparatively efficient with traditional. The parsing algorithm was CKY-style parsing with beam thresholding, which was similar to ones used in (Collins, 1996; Clark et al., 2002). Although we needed to compute normalizing factor Z. to obtain probability values, we used unnormalized products as the preference score for beam thresholding, following (Clark et al., 2002). We did not use any preprocessing such as supertagging (Joshi and Srinivas, 1994) and the parser searched for the most plausible derivation tree from the derivation forest in terms of the probability given by the combination of syntax and semantics probabilities. Tables 1 and 2 list the accuracy of dependencies, i.e., edges in derivation trees, for each model with two sets of features for the semantics model3. Since in derivation trees each word in a sentence depends on one and only one word (see Figure 3), the accuracy is the number of 3Since the features of the syntax part were not changed, the results for syntax are exactly the same. correct edges divided by the number </context>
</contexts>
<marker>Joshi, Srinivas, 1994</marker>
<rawString>Aravind K. Joshi and B. Srinivas. 1994. Disambiguation of super parts of speech (or supertags): Almost parsing. In Proceedings of 17th COLING, pages 161–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="7227" citStr="Marcus et al., 1994" startWordPosition="1074" endWordPosition="1077">probabilities to parsing results not allowed by the grammar. • Since the syntax and semantics probabilities are separate, we can improve them individually. For example, the syntax model can be improved by smoothing using the syntactic classes of words, while the semantics model should be able to be improved by using semantic classes. In addition, the model can be a starting point that allows the theory of syntax and semantics to be evaluated through consulting an extensive corpus. We evaluated the validity of our model through experiments on a disambiguation task of parsing the Penn Treebank (Marcus et al., 1994) with an automatically acquired LTAG grammar. To assess the contribution of the syntax and semantics probabilities to the accuracy ofparsing and to evaluate the validity of applying maximum entropy estimation for feature forests, we compared three models trained with the same training set and the same set of features. Following the experimental results, we concluded that i) a parser with the syntax probability only achieved high accuracy with the lexicalized grammar, ii) the incorporation of preferences for lexical association through the semantics probability resulted in significant improveme</context>
<context position="24469" citStr="Marcus et al., 1994" startWordPosition="3824" endWordPosition="3827">ved by this model. It can be applied to any parsing results given by a lexicalized grammar, does not require the independence assumption, and is defined as a combination of syntax and semantics probabilities, where the semantics probability is a discriminative model that selects a parsing result from the set of candidates given by the syntax probability. 4 Experiments The model proposed in Section 3 is generally applicable to any lexicalized grammars, and this section reports the evaluation of our model with a wide-coverage LTAG grammar, which is automatically acquired from the Penn Treebank (Marcus et al., 1994) Sections 02–21. The grammar was acquired by an algorithm similar to (Xia, 1999), and consisted of 2,105 elementary trees, where 1,010 were initial trees and 1,095 were auxiliary ones. The coverage of the grammar against Section 22 (1,700 sentences) was 92.6% (1,575 sentences) in a weak sense (i.e., the grammar could output a structure consistent with the bracketing in the test corpus), and 68.0% (1,156 sentences) in a strong sense (i.e., the grammar could output exactly the correct derivation). Since the grammar acquisition algorithm could output derivation trees for the sentences in the trai</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT</booktitle>
<contexts>
<context position="4552" citStr="Miyao and Tsujii, 2002" startWordPosition="654" endWordPosition="657">ng methods including maximum entropy models (Berger et al., 1996) and support vector machines (Vapnik, 1995) provide grounds for this type of modeling, because it allows various dependent features to be incorporated into the model without the independence assumption. The above approach, however, has a serious deficiency: a lexicalized grammar assigns exponentially many parsing results because of local ambiguities in a sentence, which is problematic in estimating the parameters of a probability model. To cope with this, we adopted an algorithm of maximum entropy estimation for feature forests (Miyao and Tsujii, 2002; Geman and Johnson, 2002), which allows parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Joh</context>
<context position="23555" citStr="Miyao and Tsujii, 2002" startWordPosition="3677" endWordPosition="3681">solved by considering the dependency of more than two words; e.g. PP-attachment ambiguity should be resolved by the dependency of three words. Consequently, the probability model takes the following form. �  p(A |w) = S np(ci| i wi) Z exp c However, this model has a crucial flaw: the maximum likelihood estimation of semantics probability is intractable. This is because the estimation requires Zc to be computed, which requires summation over A(c), exponentially many parsing results. To cope with this problem, we applied an efficient algorithm of maximum entropy estimation for feature forests (Miyao and Tsujii, 2002; Geman and Johnson, 2002). This enabled the tractable estimation of the above probability, when a set of candidates are represented in a feature forest of a tractable size. Here, we should mention that the disadvantages of the traditional models discussed in Section 2 have been completely solved by this model. It can be applied to any parsing results given by a lexicalized grammar, does not require the independence assumption, and is defined as a combination of syntax and semantics probabilities, where the semantics probability is a discriminative model that selects a parsing result from the </context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proceedings of HLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
</authors>
<title>Amis – a maximum entropy estimator for feature forests. Available via http://wwwtsujii.is.s.u-tokyo.ac.jp/%7Eyusuke/amis/.</title>
<date>2002</date>
<contexts>
<context position="28343" citStr="Miyao, 2002" startWordPosition="4440" endWordPosition="4441"> features excluding fine-grained features of the first set. There were 701,819 events for traditional, and 32,371 for our model. The difference in the number of events was caused by the difference in the units of events, i.e., an event corresponded to a dependency in traditional, while it corresponded to a sentence in our model. The parameters of the models were estimated by the limited-memory BFGS algorithm (Nocedal, 1980) with a Gaussian distribution as the prior probability distribution for smoothing (Chen and Rosenfeld, 1999) implemented in a maximum entropy estimator for feature forests (Miyao, 2002). The estimation for traditional was converged in 67 iterations in 127 seconds, and our model in 29 iterations in 111 seconds on a Pentium III 1.26-GHz CPU with 4 GB of memory. These results reveal that the estimation with our model is comparatively efficient with traditional. The parsing algorithm was CKY-style parsing with beam thresholding, which was similar to ones used in (Collins, 1996; Clark et al., 2002). Although we needed to compute normalizing factor Z. to obtain probability values, we used unnormalized products as the preference score for beam thresholding, following (Clark et al.,</context>
</contexts>
<marker>Miyao, 2002</marker>
<rawString>Yusuke Miyao. 2002. Amis – a maximum entropy estimator for feature forests. Available via http://wwwtsujii.is.s.u-tokyo.ac.jp/%7Eyusuke/amis/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
</authors>
<title>Updating quasi-Newton matrices with limited storage.</title>
<date>1980</date>
<journal>Mathematics of Computation,</journal>
<pages>35--773</pages>
<contexts>
<context position="28158" citStr="Nocedal, 1980" startWordPosition="4412" endWordPosition="4413">surface forms/POSs of words, the labels of dependencies (substitution/adjunction), and the distance between two words. The first feature set had 283,755 features and the other had 150,156 features excluding fine-grained features of the first set. There were 701,819 events for traditional, and 32,371 for our model. The difference in the number of events was caused by the difference in the units of events, i.e., an event corresponded to a dependency in traditional, while it corresponded to a sentence in our model. The parameters of the models were estimated by the limited-memory BFGS algorithm (Nocedal, 1980) with a Gaussian distribution as the prior probability distribution for smoothing (Chen and Rosenfeld, 1999) implemented in a maximum entropy estimator for feature forests (Miyao, 2002). The estimation for traditional was converged in 67 iterations in 127 seconds, and our model in 29 iterations in 111 seconds on a Pentium III 1.26-GHz CPU with 4 GB of memory. These results reveal that the estimation with our model is comparatively efficient with traditional. The parsing algorithm was CKY-style parsing with beam thresholding, which was similar to ones used in (Collins, 1996; Clark et al., 2002)</context>
</contexts>
<marker>Nocedal, 1980</marker>
<rawString>Jorge Nocedal. 1980. Updating quasi-Newton matrices with limited storage. Mathematics of Computation, 35:773–783.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Jonas Kuhn</author>
<author>Mark Johnson</author>
</authors>
<title>Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training.</title>
<date>2000</date>
<booktitle>In Proceedings of 38th ACL.</booktitle>
<contexts>
<context position="5191" citStr="Riezler et al., 2000" startWordPosition="750" endWordPosition="753">, 2002), which allows parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002) have had a similar motivation to ours. Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar. A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar. They assumed that parsing results would be suppressed to a reasonable number through using heuristic rules, or by carefully implementing a fully restrictive and wide-coverage grammar, which requires a cons</context>
</contexts>
<marker>Riezler, Prescher, Kuhn, Johnson, 2000</marker>
<rawString>Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark Johnson. 2000. Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training. In Proceedings of 38th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th ACL.</booktitle>
<contexts>
<context position="5214" citStr="Riezler et al., 2002" startWordPosition="754" endWordPosition="757">parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002) have had a similar motivation to ours. Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar. A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar. They assumed that parsing results would be suppressed to a reasonable number through using heuristic rules, or by carefully implementing a fully restrictive and wide-coverage grammar, which requires a considerable amount of effo</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell III, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proceedings of 40th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Thomas Wasow</author>
</authors>
<title>Syntactic Theory – A Formal Introduction.</title>
<date>1999</date>
<booktitle>CSLI Lecture Notes no. 92.</booktitle>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="17562" citStr="Sag and Wasow, 1999" startWordPosition="2710" endWordPosition="2713"> is that the plausibility of a parsing result is determined by i) the plausibility of syntax, and ii) selecting the most probable semantics from the structures allowed by the given syntax. This section formalizes the general form of statistical models for disambiguation of parsing including lexicalized parse trees, derivation trees, and dependency structures. Problems with the existing models are then discussed, and our model is introduced. Suppose that a set W of words and a set C of syntactic categories (e.g., nonterminal symbols of CFG, elementary trees of LTAG, feature structures of HPSG (Sag and Wasow, 1999)) are given. A lexicalized grammar is ARG1 MODIFY what student ARG1 want does to MODIFY ARG2 Lexicalized parse tree (write, what, SS write S), (write, does, SS does S), (write, student, SS NP VP), (student, your, NPS your student), (write, want, VPS want VP), (write, to, VPS to write) Derivation tree (write, what, SUBST), (write, does, ADJ), (write, student, SUBST), (student, your, ADJ), (write, want, ADJ), (write, to, ADJ) Dependency structure (write, what, ARG2), (write, does, MODIFY), (write, student, ARG1), (student, your, MODIFY), (write, want, MODIFY), (want, student, ARG1), (write, to, </context>
</contexts>
<marker>Sag, Wasow, 1999</marker>
<rawString>Ivan A. Sag and Thomas Wasow. 1999. Syntactic Theory – A Formal Introduction. CSLI Lecture Notes no. 92. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Anne Abeill´e</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing strategies with ‘lexicalized grammars’: Application to tree adjoining grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of 12th COLING,</booktitle>
<pages>578--583</pages>
<marker>Schabes, Abeill´e, Joshi, 1988</marker>
<rawString>Yves Schabes, Anne Abeill´e, and Aravind K. Joshi. 1988. Parsing strategies with ‘lexicalized grammars’: Application to tree adjoining grammars. In Proceedings of 12th COLING, pages 578–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="4038" citStr="Vapnik, 1995" startWordPosition="577" endWordPosition="578">bability is responsible for determining the syntactic categories chosen by words in a sentence, and the semantics probability selects the most plausible dependencies of words from candidates allowed by the syntactic categories yielded by the syntax probability. Since the sequence of syntactic categories restricts the possible structure of parsing results, the semantics probability is a conditional probability without decomposition into the primitive dependencies of words. Recently used machine learning methods including maximum entropy models (Berger et al., 1996) and support vector machines (Vapnik, 1995) provide grounds for this type of modeling, because it allows various dependent features to be incorporated into the model without the independence assumption. The above approach, however, has a serious deficiency: a lexicalized grammar assigns exponentially many parsing results because of local ambiguities in a sentence, which is problematic in estimating the parameters of a probability model. To cope with this, we adopted an algorithm of maximum entropy estimation for feature forests (Miyao and Tsujii, 2002; Geman and Johnson, 2002), which allows parameters to be efficiently estimated. The a</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Extracting tree adjoining grammars from bracketed corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of 5th NLPRS.</booktitle>
<contexts>
<context position="1109" citStr="Xia, 1999" startWordPosition="155" endWordPosition="156">of primitive dependencies of two words, our model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition ofparsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of </context>
<context position="12150" citStr="Xia, 1999" startWordPosition="1845" endWordPosition="1846">e PP. 3. The preferences of syntax and semantics are combined in the lexical dependencies of two words, i.e., features for syntactic preference and those for semantic preference are not distinguished in the model. Lexicalized grammars formalize the constraints of the relations between syntax and semantics, but the model does not assume the existence of such constraints. The model prevents further improvements to the syntax/semantics models; in addition to the linguistic analysis of the relation between syntax and semantics. 2.2 Derivation trees Recent work on the automatic extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000) and disambiguation models (Chiang, 2000) has been the first on the statistical model for syntactic disambiguation based on lexicalized grammars. However, the models are based on the lexical dependencies of elementary trees, which is a simple extension of the LPCFG. That is, the models are still based on decomposition into primitive lexical dependencies. Derivation trees, the structural description in LTAG (Schabes et al., 1988), represent the association of lexical items i.e., elementary trees. In LTAG, all syntactic constraints of words are described in an elem</context>
<context position="24549" citStr="Xia, 1999" startWordPosition="3839" endWordPosition="3840">, does not require the independence assumption, and is defined as a combination of syntax and semantics probabilities, where the semantics probability is a discriminative model that selects a parsing result from the set of candidates given by the syntax probability. 4 Experiments The model proposed in Section 3 is generally applicable to any lexicalized grammars, and this section reports the evaluation of our model with a wide-coverage LTAG grammar, which is automatically acquired from the Penn Treebank (Marcus et al., 1994) Sections 02–21. The grammar was acquired by an algorithm similar to (Xia, 1999), and consisted of 2,105 elementary trees, where 1,010 were initial trees and 1,095 were auxiliary ones. The coverage of the grammar against Section 22 (1,700 sentences) was 92.6% (1,575 sentences) in a weak sense (i.e., the grammar could output a structure consistent with the bracketing in the test corpus), and 68.0% (1,156 sentences) in a strong sense (i.e., the grammar could output exactly the correct derivation). Since the grammar acquisition algorithm could output derivation trees for the sentences in the training corpus (Section 02–21), we used them as a training set of the probability m</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fei Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proceedings of 5th NLPRS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>