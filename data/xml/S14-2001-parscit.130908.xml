<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010066">
<title confidence="0.993935666666667">
SemEval-2014 Task 1: Evaluation of Compositional Distributional
Semantic Models on Full Sentences through Semantic Relatedness and
Textual Entailment
</title>
<author confidence="0.985355">
Marco Marelli(1) Luisa Bentivogli(2) Marco Baroni(1)
Raffaella Bernardi(1) Stefano Menini(1,2) Roberto Zamparelli(1)
</author>
<affiliation confidence="0.962588">
(1) University of Trento, Italy
</affiliation>
<address confidence="0.739041">
(2) FBK - Fondazione Bruno Kessler, Trento, Italy
</address>
<email confidence="0.997881">
{name.surname}@unitn.it, {bentivo,menini}@fbk.eu
</email>
<sectionHeader confidence="0.993857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999886333333333">
This paper presents the task on the evalu-
ation of Compositional Distributional Se-
mantics Models on full sentences orga-
nized for the first time within SemEval-
2014. Participation was open to systems
based on any approach. Systems were pre-
sented with pairs of sentences and were
evaluated on their ability to predict hu-
man judgments on (i) semantic relatedness
and (ii) entailment. The task attracted 21
teams, most of which participated in both
subtasks. We received 17 submissions in
the relatedness subtask (for a total of 66
runs) and 18 in the entailment subtask (65
runs).
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.960710701754386">
Distributional Semantic Models (DSMs) approx-
imate the meaning of words with vectors sum-
marizing their patterns of co-occurrence in cor-
pora. Recently, several compositional extensions
of DSMs (CDSMs) have been proposed, with the
purpose of representing the meaning of phrases
and sentences by composing the distributional rep-
resentations of the words they contain (Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011; Mitchell and Lapata, 2010; Socher et al.,
2012). Despite the ever increasing interest in the
field, the development of adequate benchmarks for
CDSMs, especially at the sentence level, is still
lagging. Existing data sets, such as those intro-
duced by Mitchell and Lapata (2008) and Grefen-
stette and Sadrzadeh (2011), are limited to a few
hundred instances of very short sentences with a
fixed structure. In the last ten years, several large
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
data sets have been developed for various com-
putational semantics tasks, such as Semantic Text
Similarity (STS)(Agirre et al., 2012) or Recogniz-
ing Textual Entailment (RTE) (Dagan et al., 2006).
Working with such data sets, however, requires
dealing with issues, such as identifying multiword
expressions, recognizing named entities or access-
ing encyclopedic knowledge, which have little to
do with compositionality per se. CDSMs should
instead be evaluated on data that are challenging
for reasons due to semantic compositionality (e.g.
context-cued synonymy resolution and other lexi-
cal variation phenomena, active/passive and other
syntactic alternations, impact of negation at vari-
ous levels, operator scope, and other effects linked
to the functional lexicon). These issues do not oc-
cur frequently in, e.g., the STS and RTE data sets.
With these considerations in mind, we devel-
oped SICK (Sentences Involving Compositional
Knowledge), a data set aimed at filling the void,
including a large number of sentence pairs that
are rich in the lexical, syntactic and semantic phe-
nomena that CDSMs are expected to account for,
but do not require dealing with other aspects of
existing sentential data sets that are not within
the scope of compositional distributional seman-
tics. Moreover, we distinguished between generic
semantic knowledge about general concept cate-
gories (such as knowledge that a couple is formed
by a bride and a groom) and encyclopedic knowl-
edge about specific instances of concepts (e.g.,
knowing the fact that the current president of the
US is Barack Obama). The SICK data set contains
many examples of the former, but none of the lat-
ter.
</bodyText>
<sectionHeader confidence="0.985348" genericHeader="method">
2 The Task
</sectionHeader>
<bodyText confidence="0.99989025">
The Task involved two subtasks. (i) Relatedness:
predicting the degree of semantic similarity be-
tween two sentences, and (ii) Entailment: detect-
ing the entailment relation holding between them
</bodyText>
<page confidence="0.802281">
1
</page>
<note confidence="0.7251875">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 1–8,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999731621621621">
(see below for the exact definition). Sentence re-
latedness scores provide a direct way to evalu-
ate CDSMs, insofar as their outputs are able to
quantify the degree of semantic similarity between
sentences. On the other hand, starting from the
assumption that understanding a sentence means
knowing when it is true, being able to verify
whether an entailment is valid is a crucial chal-
lenge for semantic systems.
In the semantic relatedness subtask, given two
sentences, systems were required to produce a re-
latedness score (on a continuous scale) indicating
the extent to which the sentences were expressing
a related meaning. Table 1 shows examples of sen-
tence pairs with different degrees of semantic re-
latedness; gold relatedness scores are expressed on
a 5-point rating scale.
In the entailment subtask, given two sentences
A and B, systems had to determine whether the
meaning of B was entailed by A. In particular, sys-
tems were required to assign to each pair either
the ENTAILMENT label (when A entails B, viz.,
B cannot be false when A is true), the CONTRA-
DICTION label (when A contradicted B, viz. B is
false whenever A is true), or the NEUTRAL label
(when the truth of B could not be determined on
the basis of A). Table 2 shows examples of sen-
tence pairs holding different entailment relations.
Participants were invited to submit up to five
system runs for one or both subtasks. Developers
of CDSMs were especially encouraged to partic-
ipate, but developers of other systems that could
tackle sentence relatedness or entailment tasks
were also welcome. Besides being of intrinsic in-
terest, the latter systems’ performance will serve
to situate CDSM performance within the broader
landscape of computational semantics.
</bodyText>
<sectionHeader confidence="0.992693" genericHeader="method">
3 The SICK Data Set
</sectionHeader>
<bodyText confidence="0.999814555555556">
The SICK data set, consisting of about 10,000 En-
glish sentence pairs annotated for relatedness in
meaning and entailment, was used to evaluate the
systems participating in the task. The data set
creation methodology is outlined in the following
subsections, while all the details about data gen-
eration and annotation, quality control, and inter-
annotator agreement can be found in Marelli et al.
(2014).
</bodyText>
<subsectionHeader confidence="0.999702">
3.1 Data Set Creation
</subsectionHeader>
<bodyText confidence="0.999981340425532">
SICK was built starting from two existing data
sets: the 8K ImageFlickr data set1 and the
SemEval-2012 STS MSR-Video Descriptions data
set.2 The 8K ImageFlickr dataset is a dataset of
images, where each image is associated with five
descriptions. To derive SICK sentence pairs we
randomly chose 750 images and we sampled two
descriptions from each of them. The SemEval-
2012 STS MSR-Video Descriptions data set is a
collection of sentence pairs sampled from the short
video snippets which compose the Microsoft Re-
search Video Description Corpus. A subset of 750
sentence pairs were randomly chosen from this
data set to be used in SICK.
In order to generate SICK data from the 1,500
sentence pairs taken from the source data sets, a 3-
step process was applied to each sentence compos-
ing the pair, namely (i) normalization, (ii) expan-
sion and (iii) pairing. Table 3 presents an example
of the output of each step in the process.
The normalization step was carried out on the
original sentences (S0) to exclude or simplify in-
stances that contained lexical, syntactic or seman-
tic phenomena (e.g., named entities, dates, num-
bers, multiword expressions) that CDSMs are cur-
rently not expected to account for.
The expansion step was applied to each of the
normalized sentences (S1) in order to create up to
three new sentences with specific characteristics
suitable to CDSM evaluation. In this step syntac-
tic and lexical transformations with predictable ef-
fects were applied to each normalized sentence, in
order to obtain (i) a sentence with a similar mean-
ing (S2), (ii) a sentence with a logically contradic-
tory or at least highly contrasting meaning (S3),
and (iii) a sentence that contains most of the same
lexical items, but has a different meaning (S4) (this
last step was carried out only where it could yield
a meaningful sentence; as a result, not all normal-
ized sentences have an (S4) expansion).
Finally, in the pairing step each normalized
sentence in the pair was combined with all the
sentences resulting from the expansion phase and
with the other normalized sentence in the pair.
Considering the example in Table 3, S1a and S1b
were paired. Then, S1a and S1b were each com-
bined with S2a, S2b,S3a, S3b, S4a, and S4b, lead-
</bodyText>
<footnote confidence="0.994506">
1http://nlp.cs.illinois.edu/HockenmaierGroup/data.html
2http://www.cs.york.ac.uk/semeval-
2012/task6/index.php?id=data
</footnote>
<page confidence="0.996477">
2
</page>
<figure confidence="0.933380846153846">
Relatedness score Example
A: “A man is jumping into an empty pool”
1.6
B: “There is no biker jumping in the air”
A: “Two children are lying in the snow and are making snow angels”
2.9
B: “Two angels are making snow on the lying children”
A: “The young boys are playing outdoors and the man is smiling nearby”
3.6
B: “There is no boy playing outdoors and there is no man smiling”
A: “A person in a black jacket is doing tricks on a motorbike”
4.9
B: “A man in a black jacket is doing tricks on a motorbike”
</figure>
<tableCaption confidence="0.952964">
Table 1: Examples of sentence pairs with their gold relatedness scores (on a 5-point rating scale).
</tableCaption>
<figure confidence="0.962724">
Entailment label Example
A: “Two teams are competing in a football match”
ENTAILMENT
B: “Two groups ofpeople are playing football”
CONTRADICTION A: “The brown horse is near a red barrel at the rodeo”
B: “The brown horse is far from a red barrel at the rodeo”
A: “A man in a black jacket is doing tricks on a motorbike”
NEUTRAL
B: “A person is riding the bicycle on one wheel”
</figure>
<tableCaption confidence="0.931783">
Table 2: Examples of sentence pairs with their gold entailment labels.
</tableCaption>
<bodyText confidence="0.999850833333333">
ing to a total of 13 different sentence pairs.
Furthermore, a number of pairs composed of
completely unrelated sentences were added to the
data set by randomly taking two sentences from
two different pairs.
The result is a set of about 10,000 new sen-
tence pairs, in which each sentence is contrasted
with either a (near) paraphrase, a contradictory or
strongly contrasting statement, another sentence
with very high lexical overlap but different mean-
ing, or a completely unrelated sentence. The ra-
tionale behind this approach was that of building
a data set which encouraged the use of a com-
positional semantics step in understanding when
two sentences have close meanings or entail each
other, hindering methods based on individual lex-
ical items, on the syntactic complexity of the two
sentences or on pure world knowledge.
</bodyText>
<subsectionHeader confidence="0.997135">
3.2 Relatedness and Entailment Annotation
</subsectionHeader>
<bodyText confidence="0.999989578947369">
Each pair in the SICK dataset was annotated to
mark (i) the degree to which the two sentence
meanings are related (on a 5-point scale), and (ii)
whether one entails or contradicts the other (con-
sidering both directions). The ratings were col-
lected through a large crowdsourcing study, where
each pair was evaluated by 10 different subjects,
and the order of presentation of the sentences was
counterbalanced (i.e., 5 judgments were collected
for each presentation order). Swapping the order
of the sentences within each pair served a two-
fold purpose: (i) evaluating the entailment rela-
tion in both directions and (ii) controlling pos-
sible bias due to priming effects in the related-
ness task. Once all the annotations were collected,
the relatedness gold score was computed for each
pair as the average of the ten ratings assigned by
participants, whereas a majority vote scheme was
adopted for the entailment gold labels.
</bodyText>
<subsectionHeader confidence="0.999533">
3.3 Data Set Statistics
</subsectionHeader>
<bodyText confidence="0.999953142857143">
For the purpose of the task, the data set was ran-
domly split into training and test set (50% and
50%), ensuring that each relatedness range and en-
tailment category was equally represented in both
sets. Table 4 shows the distribution of sentence
pairs considering the combination of relatedness
ranges and entailment labels. The “total” column
</bodyText>
<page confidence="0.968989">
3
</page>
<figure confidence="0.957182625">
Original pair
S0a: A sea turtle is hunting for fish S0b: The turtle followed the fish
Normalized pair
S1a: A sea turtle is hunting for fish S1b: The turtle is following the fish
Expanded pairs
S2a: A sea turtle is hunting for food S2b: The turtle is following the red fish
S3a: A sea turtle is not hunting for fish S3b: The turtle isn’t following the fish
S4a: A fish is hunting for a turtle in the sea S4b: The fish is following the turtle
</figure>
<tableCaption confidence="0.991842">
Table 3: Data set creation process.
</tableCaption>
<bodyText confidence="0.907613666666667">
indicates the total number of pairs in each range
of relatedness, while the “total” row contains the
total number of pairs in each entailment class.
</bodyText>
<table confidence="0.980273642857143">
SICK Training Set
relatedness CONTRADICT ENTAIL NEUTRAL TOTAL
1-2 range 0 (0%) 0 (0%) 471 (10%) 471
2-3 range 59 (1%) 2 (0%) 638 (13%) 699
3-4 range 498 (10%) 71 (1%) 1344 (27%) 1913
4-5 range 155 (3%) 1344 (27%) 352 (7%) 1851
TOTAL 712 1417 2805 4934
SICK Test Set
relatedness CONTRADICT ENTAIL NEUTRAL TOTAL
1-2 range 0 (0%) 1 (0%) 451 (9%) 452
2-3 range 59 (1%) 0 (0%) 615(13%) 674
3-4 range 496 (10%) 65 (1%) 1398 (28%) 1959
4-5 range 157 (3%) 1338 (27%) 326 (7%) 1821
TOTAL 712 1404 2790 4906
</table>
<tableCaption confidence="0.962051">
Table 4: Distribution of sentence pairs across the
Training and Test Sets.
</tableCaption>
<sectionHeader confidence="0.981209" genericHeader="method">
4 Evaluation Metrics and Baselines
</sectionHeader>
<bodyText confidence="0.996914555555556">
Both subtasks were evaluated using standard met-
rics. In particular, the results on entailment were
evaluated using accuracy, whereas the outputs on
relatedness were evaluated using Pearson correla-
tion, Spearman correlation, and Mean Squared Er-
ror (MSE). Pearson correlation was chosen as the
official measure to rank the participating systems.
Table 5 presents the performance of 4 base-
lines. The Majority baseline always assigns
the most common label in the training data
(NEUTRAL), whereas the Probability baseline
assigns labels randomly according to their rela-
tive frequency in the training set. The Overlap
baseline measures word overlap, again with
parameters (number of stop words and EN-
TAILMENT/NEUTRAL/CONTRADICTION
thresholds) estimated on the training part of the
data.
</bodyText>
<table confidence="0.969084">
Baseline Relatedness Entailment
Chance 0 33.3%
Majority NA 56.7%
Probability NA 41.8%
Overlap 0.63 56.2%
</table>
<tableCaption confidence="0.985974">
Table 5: Performance of baselines. Figure of merit
</tableCaption>
<bodyText confidence="0.767749">
is Pearson correlation for relatedness and accuracy
for entailment. NA = Not Applicable
</bodyText>
<sectionHeader confidence="0.95623" genericHeader="method">
5 Submitted Runs and Results
</sectionHeader>
<bodyText confidence="0.99994984">
Overall, 21 teams participated in the task. Partici-
pants were allowed to submit up to 5 runs for each
subtask and had to choose the primary run to be in-
cluded in the comparative evaluation. We received
17 submissions to the relatedness subtask (for a
total of 66 runs) and 18 for the entailment subtask
(65 runs).
We asked participants to pre-specify a pri-
mary run to encourage commitment to a
theoretically-motivated approach, rather than
post-hoc performance-based assessment. Inter-
estingly, some participants used the non-primary
runs to explore the performance one could reach
by exploiting weaknesses in the data that are not
likely to hold in future tasks of the same kind
(for instance, run 3 submitted by The Meaning
Factory exploited sentence ID ordering informa-
tion, but it was not presented as a primary run).
Participants could also use non-primary runs to
test smart baselines. In the relatedness subtask
six non-primary runs slightly outperformed the
official winning primary entry,3 while in the
entailment task all ECNU’s runs but run 4 were
better than ECNU’s primary run. Interestingly,
the differences between the ECNU’s runs were
</bodyText>
<footnote confidence="0.950786666666667">
3They were: The Meaning Factory’s run3 (Pearson
0.84170) ECNU’s runs2 (0.83893) run5 (0.83500) and Stan-
fordNLP’s run4 (0.83462) and run2 (0.83103).
</footnote>
<page confidence="0.993778">
4
</page>
<bodyText confidence="0.999864230769231">
due to the learning methods used.
We present the results achieved by primary runs
against the Entailment and Relatedness subtasks in
Table 6 and Table 7, respectively.4 We witnessed
a very close finish in both subtasks, with 4 more
systems within 3 percentage points of the winner
in both cases. 4 of these 5 top systems were the
same across the two subtasks. Most systems per-
formed well above the best baselines from Table
5.
The overall performance pattern suggests that,
owing perhaps to the more controlled nature of
the sentences, as well as to the purely linguistic
nature of the challenges it presents, SICK entail-
ment is “easier” than RTE. Considering the first
five RTE challenges (Bentivogli et al., 2009), the
median values ranged from 56.20% to 61.75%,
whereas the average values ranged from 56.45%
to 61.97%. The entailment scores obtained on
the SICK data set are considerably higher, being
77.06% for the median system and 75.36% for
the average system. On the other hand, the re-
latedness task is more challenging than the one
run on MSRvid (one of our data sources) at STS
2012, where the top Pearson correlation was 0.88
(Agirre et al., 2012).
</bodyText>
<sectionHeader confidence="0.998179" genericHeader="method">
6 Approaches
</sectionHeader>
<bodyText confidence="0.9999866">
A summary of the approaches used by the sys-
tems to address the task is presented in Table 8.
In the table, systems in bold are those for which
the authors submitted a paper (Ferrone and Zan-
zotto, 2014; Bjerva et al., 2014; Beltagy et al.,
2014; Lai and Hockenmaier, 2014; Alves et al.,
2014; Le´on et al., 2014; Bestgen, 2014; Zhao et
al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014;
Lien and Kouylekov, 2014; Jimenez et al., 2014;
Proisl and Evert, 2014; Gupta et al., 2014). For the
others, we used the brief description sent with the
system’s results, double-checking the information
with the authors. In the table, “E” and “R” refer
to the entailment and relatedness task respectively,
and “B” to both.
Almost all systems combine several kinds of
features. To highlight the role played by com-
position, we draw a distinction between compo-
sitional and non-compositional features, and di-
vide the former into ‘fully compositional’ (sys-
</bodyText>
<footnote confidence="0.99360675">
4ITTK’s primary run could not be evaluated due to tech-
nical problems with the submission. The best ITTK’s non-
primary run scored 78,2% accuracy in the entailment task and
0.76 r in the relatedness task.
</footnote>
<table confidence="0.999015105263158">
ID Compose ACCURACY
Illinois-LH run1 P/S 84.6
ECNU run1 S 83.6
UNAL-NLP run1 83.1
SemantiKLUE run1 82.3
The Meaning Factory run1 S 81.6
CECL ALL run1 80.0
BUAP run1 P 79.7
UoW run1 78.5
Uedinburgh run1 S 77.1
UIO-Lien run1 77.0
FBK-TR run3 P 75.4
StanfordNLP run5 S 74.5
UTexas run1 P/S 73.2
Yamraj run1 70.7
asjai run5 S 69.8
haLF run2 S 69.4
RTM-DCU run1 67.2
UANLPCourse run2 S 48.7
</table>
<tableCaption confidence="0.965518">
Table 6: Primary run results for the entailment
</tableCaption>
<bodyText confidence="0.991431416666667">
subtask. The table also shows whether a sys-
tem exploits composition information at either the
phrase (P) or sentence (S) level.
tems that compositionally computed the meaning
of the full sentences, though not necessarily by as-
signing meanings to intermediate syntactic con-
stituents) and ‘partially compositional’ (systems
that stop the composition at the level of phrases).
As the table shows, thirteen systems used compo-
sition in at least one of the tasks; ten used compo-
sition for full sentences and six for phrases, only.
The best systems are among these thirteen sys-
tems.
Let us focus on such compositional methods.
Concerning the relatedness task, the fine-grained
analyses reported for several systems (Illinois-
LH, The Meaning Factory and ECNU) shows that
purely compositional systems currently reach per-
formance above 0.7 r. In particular, ECNU’s
compositional feature gives 0.75 r, The Meaning
Factory’s logic-based composition model 0.73 r,
and Illinois-LH compositional features combined
with Word Overlap 0.75 r. While competitive,
these scores are lower than the one of the best
</bodyText>
<page confidence="0.991223">
5
</page>
<table confidence="0.999941166666667">
ID Compose r p MSE
ECNU run1 S 0.828 0.769 0.325
StanfordNLP run5 S 0.827 0.756 0.323
The Meaning Factory run1 S 0.827 0.772 0.322
UNAL-NLP run1 0.804 0.746 0.359
Illinois-LH run1 P/S 0.799 0.754 0.369
CECL ALL run1 0.780 0.732 0.398
SemantiKLUE run1 0.780 0.736 0.403
RTM-DCU run1 0.764 0.688 0.429
UTexas run1 P/S 0.714 0.674 0.499
UoW run1 0.711 0.679 0.511
FBK-TR run3 P 0.709 0.644 0.591
BUAP run1 P 0.697 0.645 0.528
UANLPCourse run2 S 0.693 0.603 0.542
UQeResearch run1 0.642 0.626 0.822
ASAP run1 P 0.628 0.597 0.662
Yamraj run1 0.535 0.536 2.665
asjai run5 S 0.479 0.461 1.104
</table>
<tableCaption confidence="0.990872">
Table 7: Primary run results for the relatedness
</tableCaption>
<bodyText confidence="0.997240852459017">
subtask (r for Pearson and p for Spearman corre-
lation). The table also shows whether a system ex-
ploits composition information at either the phrase
(P) or sentence (S) level.
purely non-compositional system (UNAL-NLP)
which reaches the 4th position (0.80 r UNAL-NLP
vs. 0.82 r obtained by the best system). UNAL-
NLP however exploits an ad-hoc “negation” fea-
ture discussed below.
In the entailment task, the best non-
compositional model (again UNAL-NLP)
reaches the 3rd position, within close reach of the
best system (83% UNAL-NLP vs. 84.5% obtained
by the best system). Again, purely compositional
models have lower performance. haLF CDSM
reaches 69.42% accuracy, Illinois-LH Word
Overlap combined with a compositional feature
reaches 71.8%. The fine-grained analysis reported
by Illinois-LH (Lai and Hockenmaier, 2014)
shows that a full compositional system (based
on point-wise multiplication) fails to capture
contradiction. It is better than partial phrase-based
compositional models in recognizing entailment
pairs, but worse than them on recognizing neutral
pairs.
Given our more general interest in the distri-
butional approaches, in Table 8 we also classify
the different DSMs used as ‘Vector Space Mod-
els’, ‘Topic Models’ and ‘Neural Language Mod-
els’. Due to the impact shown by learning methods
(see ECNU’s results), we also report the different
learning approaches used.
Several participating systems deliberately ex-
ploit ad-hoc features that, while not helping a true
understanding of sentence meaning, exploit some
systematic characteristics of SICK that should be
controlled for in future releases of the data set.
In particular, the Textual Entailment subtask has
been shown to rely too much on negative words
and antonyms. The Illinois-LH team reports that,
just by checking the presence of negative words
(the Negation Feature in the table), one can detect
86.4% of the contradiction pairs, and by combin-
ing Word Overlap and antonyms one can detect
83.6% of neutral pairs and 82.6% of entailment
pairs. This approach, however, is obviously very
brittle (it would not have been successful, for in-
stance, if negation had been optionally combined
with word-rearranging in the creation of S4 sen-
tences, see Section 3.1 above).
Finally, Table 8 reports about the use of external
resources in the task. One of the reasons we cre-
ated SICK was to have a compositional semantics
benchmark that would not require too many ex-
ternal tools and resources (e.g., named-entity rec-
ognizers, gazetteers, ontologies). By looking at
what the participants chose to use, we think we
succeeded, as only standard NLP pre-processing
tools (tokenizers, PoS taggers and parsers) and rel-
atively few knowledge resources (mostly, Word-
Net and paraphrase corpora) were used.
</bodyText>
<sectionHeader confidence="0.99216" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999893125">
We presented the results of the first task on the
evaluation of compositional distributional seman-
tic models and other semantic systems on full sen-
tences, organized within SemEval-2014. Two sub-
tasks were offered: (i) predicting the degree of re-
latedness between two sentences, and (ii) detect-
ing the entailment relation holding between them.
The task has raised noticeable attention in the
community: 17 and 18 submissions for the relat-
edness and entailment subtasks, respectively, for a
total of 21 participating teams. Participation was
not limited to compositional models but the major-
ity of systems (13/21) used composition in at least
one of the subtasks. Moreover, the top-ranking
systems in both tasks use compositional features.
However, it must be noted that all systems also ex-
</bodyText>
<page confidence="0.998526">
6
</page>
<table confidence="0.999954125">
Participant ID Non composition features fea Comp Learning Methods External Resources
Vector Semantics Model Topic Model Neural Language Model Denotational Model Word Overlap Word Similarity Syntactic Features Sentence difference Negation Features Sentence Composition Phrase composition SVM and Kernel methods K-Nearest Neighbours Classifier Combination Random Forest FoL/Probabilistic FoL Curriculum based learning Other WordNet Paraphrases DB Other Corpora ImageFlicker STS MSR-Video
Description
ASAP R R R R R R R R R
ASJAI B B B B B B B B E B R B
BUAP B B B B E B E B
UEdinburgh B B B B B E R B
CECL B B B B B B
ECNU B B B B B B B B B B B B B
FBK-TR R R R E B E E B R E R R E
haLF E E E E
IITK B B B B B B B B B
Illinois-LH B B B B B B B B B B B B
RTM-DCU B B B B B
SemantiKLUE B B B B B B B B
StandfordNLP B B R R R B E
The Meaning Factory R R R R R R B E R E B B R
UANLPCourse B B B B B
UIO-Lien E E
UNAL-NLP B B B B R B B
UoW B B B B B B
UQeRsearch R R R R R R R
UTexas B B B B B B B
Yamarj B B B B
</table>
<tableCaption confidence="0.9553295">
Table 8: Summary of the main characteristics of the participating systems on R(elatedness), E(ntailment)
or B(oth)
</tableCaption>
<bodyText confidence="0.999556666666667">
ploit non-compositional features and most of them
use external resources, especially WordNet. Al-
most all the participating systems outperformed
the proposed baselines in both tasks. Further anal-
yses carried out by some participants in the task
show that purely compositional approaches reach
accuracy above 70% in entailment and 0.70 r for
relatedness. These scores are comparable with the
average results obtained in the task.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998691166666667">
We thank the creators of the ImageFlickr, MSR-
Video, and SemEval-2012 STS data sets for grant-
ing us permission to use their data for the task. The
University of Trento authors were supported by
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
</bodyText>
<sectionHeader confidence="0.998382" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994730952380953">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), volume 2.
Ana O. Alves, Adirana Ferrugento, Mariana Lorenc¸o,
and Filipe Rodrigues. 2014. ASAP: Automatica se-
mantic alignment for phrases. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193, Boston,
MA.
Islam Beltagy, Stephen Roller, Gemma Boleda, Katrin
Erk, and Raymon J. Mooney. 2014. UTexas: Nat-
ural language semantics using distributional seman-
tics and probablisitc logic. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
</reference>
<page confidence="0.996328">
7
</page>
<reference confidence="0.99608832967033">
Luisa Bentivogli, Ido Dagan, Hoa T. Dang, Danilo Gi-
ampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge.
In The Text Analysis Conference (TAC 2009).
Yves Bestgen. 2014. CECL: a new baseline and a non-
compositional approach for the Sick benchmark. In
Proceedings of SemEval 2014: International Work-
shop on Semantic Evaluation.
Ergun Bic¸ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similar-
ity. In Proceedings of SemEval 2014: International
Workshop on Semantic Evaluation.
Johannes Bjerva, Johan Bos, Rob van der Goot, and
Malvina Nissim. 2014. The Meaning Factory: For-
mal Semantics for Recognizing Textual Entailment
and Determining Semantic Similarity. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine learning challenges. Evalu-
ating predictive uncertainty, visual object classifica-
tion, and recognising textual entailment, pages 177–
190. Springer.
Lorenzo Ferrone and Fabio Massimo Zanzotto. 2014.
haLF:comparing a pure CDSM approach and a stan-
dard ML system for RTE. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394–1404, Edinburgh, UK.
Rohit Gupta, Ismail El Maarouf Hannah Bechara, and
Costantin Orasˇan. 2014. UoW: NLP techniques de-
veloped at the University of Wolverhampton for Se-
mantic Similarity and Textual Entailment. In Pro-
ceedings of SemEval 2014: International Workshop
on Semantic Evaluation.
Sergio Jimenez, George Duenas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of SemEval 2014: International Workshop on Se-
mantic Evaluation.
Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A
denotational and distributional approach to seman-
tics. In Proceedings of SemEval 2014: International
Workshop on Semantic Evaluation.
Sa´ul Le´on, Darnes Vilarino, David Pinto, Mireya To-
var, and Beatrice Beltr´an. 2014. BUAP:evaluating
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of SemEval 2014: Inter-
national Workshop on Semantic Evaluation.
Elisabeth Lien and Milen Kouylekov. 2014. UIO-
Lien: Entailment recognition using minimal recur-
sion semantics. In Proceedings of SemEval 2014:
International Workshop on Semantic Evaluation.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC, Reykjavik.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236–244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Thomas Proisl and Stefan Evert. 2014. SemantiK-
LUE: Robust semantic similarity at multiple levels
using maximum weight matching. In Proceedings of
SemEval 2014: International Workshop on Semantic
Evaluation.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201–1211, Jeju Island, Ko-
rea.
An N. P. Vo, Octavian Popescu, and Tommaso Caselli.
2014. FBK-TR: SVM for Semantic Relatedness and
Corpus Patterns for RTE. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014.
ECNU: One Stone Two Birds: Ensemble of Het-
erogenous Measures for Semantic Relatedness and
Textual Entailment. In Proceedings of SemEval
2014: International Workshop on Semantic Evalu-
ation.
</reference>
<page confidence="0.99845">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.582346">
<title confidence="0.941721333333333">SemEval-2014 Task 1: Evaluation of Compositional Semantic Models on Full Sentences through Semantic Relatedness Textual Entailment</title>
<author confidence="0.989906">Luisa Marco Stefano Roberto</author>
<affiliation confidence="0.999188">(1)University of Trento,</affiliation>
<address confidence="0.961919">(2)FBK - Fondazione Bruno Kessler, Trento,</address>
<abstract confidence="0.98168775">This paper presents the task on the evaluation of Compositional Distributional Semantics Models on full sentences organized for the first time within SemEval- 2014. Participation was open to systems based on any approach. Systems were presented with pairs of sentences and were evaluated on their ability to predict hujudgments on relatedness The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<volume>2</volume>
<contexts>
<context position="2209" citStr="Agirre et al., 2012" startWordPosition="320" endWordPosition="323">the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ data sets have been developed for various computational semantics tasks, such as Semantic Text Similarity (STS)(Agirre et al., 2012) or Recognizing Textual Entailment (RTE) (Dagan et al., 2006). Working with such data sets, however, requires dealing with issues, such as identifying multiword expressions, recognizing named entities or accessing encyclopedic knowledge, which have little to do with compositionality per se. CDSMs should instead be evaluated on data that are challenging for reasons due to semantic compositionality (e.g. context-cued synonymy resolution and other lexical variation phenomena, active/passive and other syntactic alternations, impact of negation at various levels, operator scope, and other effects l</context>
<context position="16555" citStr="Agirre et al., 2012" startWordPosition="2690" endWordPosition="2693">well as to the purely linguistic nature of the challenges it presents, SICK entailment is “easier” than RTE. Considering the first five RTE challenges (Bentivogli et al., 2009), the median values ranged from 56.20% to 61.75%, whereas the average values ranged from 56.45% to 61.97%. The entailment scores obtained on the SICK data set are considerably higher, being 77.06% for the median system and 75.36% for the average system. On the other hand, the relatedness task is more challenging than the one run on MSRvid (one of our data sources) at STS 2012, where the top Pearson correlation was 0.88 (Agirre et al., 2012). 6 Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the informat</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), volume 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana O Alves</author>
<author>Adirana Ferrugento</author>
<author>Mariana Lorenc¸o</author>
<author>Filipe Rodrigues</author>
</authors>
<title>ASAP: Automatica semantic alignment for phrases.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<marker>Alves, Ferrugento, Lorenc¸o, Rodrigues, 2014</marker>
<rawString>Ana O. Alves, Adirana Ferrugento, Mariana Lorenc¸o, and Filipe Rodrigues. 2014. ASAP: Automatica semantic alignment for phrases. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<location>Boston, MA.</location>
<contexts>
<context position="1389" citStr="Baroni and Zamparelli, 2010" startWordPosition="197" endWordPosition="200">ss and (ii) entailment. The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs). 1 Introduction Distributional Semantic Models (DSMs) approximate the meaning of words with vectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer ar</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, pages 1183–1193, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Stephen Roller</author>
<author>Gemma Boleda</author>
<author>Katrin Erk</author>
<author>Raymon J Mooney</author>
</authors>
<title>UTexas: Natural language semantics using distributional semantics and probablisitc logic.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="16813" citStr="Beltagy et al., 2014" startWordPosition="2739" endWordPosition="2742">d from 56.45% to 61.97%. The entailment scores obtained on the SICK data set are considerably higher, being 77.06% for the median system and 75.36% for the average system. On the other hand, the relatedness task is more challenging than the one run on MSRvid (one of our data sources) at STS 2012, where the top Pearson correlation was 0.88 (Agirre et al., 2012). 6 Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compo</context>
</contexts>
<marker>Beltagy, Roller, Boleda, Erk, Mooney, 2014</marker>
<rawString>Islam Beltagy, Stephen Roller, Gemma Boleda, Katrin Erk, and Raymon J. Mooney. 2014. UTexas: Natural language semantics using distributional semantics and probablisitc logic. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Ido Dagan</author>
<author>Hoa T Dang</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
</authors>
<title>The fifth PASCAL recognizing textual entailment challenge.</title>
<date>2009</date>
<booktitle>In The Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="16111" citStr="Bentivogli et al., 2009" startWordPosition="2613" endWordPosition="2616">he Entailment and Relatedness subtasks in Table 6 and Table 7, respectively.4 We witnessed a very close finish in both subtasks, with 4 more systems within 3 percentage points of the winner in both cases. 4 of these 5 top systems were the same across the two subtasks. Most systems performed well above the best baselines from Table 5. The overall performance pattern suggests that, owing perhaps to the more controlled nature of the sentences, as well as to the purely linguistic nature of the challenges it presents, SICK entailment is “easier” than RTE. Considering the first five RTE challenges (Bentivogli et al., 2009), the median values ranged from 56.20% to 61.75%, whereas the average values ranged from 56.45% to 61.97%. The entailment scores obtained on the SICK data set are considerably higher, being 77.06% for the median system and 75.36% for the average system. On the other hand, the relatedness task is more challenging than the one run on MSRvid (one of our data sources) at STS 2012, where the top Pearson correlation was 0.88 (Agirre et al., 2012). 6 Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for whic</context>
</contexts>
<marker>Bentivogli, Dagan, Dang, Giampiccolo, Magnini, 2009</marker>
<rawString>Luisa Bentivogli, Ido Dagan, Hoa T. Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth PASCAL recognizing textual entailment challenge. In The Text Analysis Conference (TAC 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Bestgen</author>
</authors>
<title>CECL: a new baseline and a noncompositional approach for the Sick benchmark.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="16895" citStr="Bestgen, 2014" startWordPosition="2755" endWordPosition="2756">ably higher, being 77.06% for the median system and 75.36% for the average system. On the other hand, the relatedness task is more challenging than the one run on MSRvid (one of our data sources) at STS 2012, where the top Pearson correlation was 0.88 (Agirre et al., 2012). 6 Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully composi</context>
</contexts>
<marker>Bestgen, 2014</marker>
<rawString>Yves Bestgen. 2014. CECL: a new baseline and a noncompositional approach for the Sick benchmark. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Andy Way</author>
</authors>
<title>RTM-DCU: Referential translation machines for semantic similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<marker>Bic¸ici, Way, 2014</marker>
<rawString>Ergun Bic¸ici and Andy Way. 2014. RTM-DCU: Referential translation machines for semantic similarity. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Bjerva</author>
<author>Johan Bos</author>
<author>Rob van der Goot</author>
<author>Malvina Nissim</author>
</authors>
<title>The Meaning Factory: Formal Semantics for Recognizing Textual Entailment and Determining Semantic Similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<marker>Bjerva, Bos, van der Goot, Nissim, 2014</marker>
<rawString>Johannes Bjerva, Johan Bos, Rob van der Goot, and Malvina Nissim. 2014. The Meaning Factory: Formal Semantics for Recognizing Textual Entailment and Determining Semantic Similarity. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge. In Machine learning challenges. Evaluating predictive uncertainty, visual object classification, and recognising textual entailment,</title>
<date>2006</date>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2270" citStr="Dagan et al., 2006" startWordPosition="330" endWordPosition="333"> as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ data sets have been developed for various computational semantics tasks, such as Semantic Text Similarity (STS)(Agirre et al., 2012) or Recognizing Textual Entailment (RTE) (Dagan et al., 2006). Working with such data sets, however, requires dealing with issues, such as identifying multiword expressions, recognizing named entities or accessing encyclopedic knowledge, which have little to do with compositionality per se. CDSMs should instead be evaluated on data that are challenging for reasons due to semantic compositionality (e.g. context-cued synonymy resolution and other lexical variation phenomena, active/passive and other syntactic alternations, impact of negation at various levels, operator scope, and other effects linked to the functional lexicon). These issues do not occur f</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Machine learning challenges. Evaluating predictive uncertainty, visual object classification, and recognising textual entailment, pages 177– 190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorenzo Ferrone</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>haLF:comparing a pure CDSM approach and a standard ML system for RTE.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="16770" citStr="Ferrone and Zanzotto, 2014" startWordPosition="2730" endWordPosition="2734">6.20% to 61.75%, whereas the average values ranged from 56.45% to 61.97%. The entailment scores obtained on the SICK data set are considerably higher, being 77.06% for the median system and 75.36% for the average system. On the other hand, the relatedness task is more challenging than the one run on MSRvid (one of our data sources) at STS 2012, where the top Pearson correlation was 0.88 (Agirre et al., 2012). 6 Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by compo</context>
</contexts>
<marker>Ferrone, Zanzotto, 2014</marker>
<rawString>Lorenzo Ferrone and Fabio Massimo Zanzotto. 2014. haLF:comparing a pure CDSM approach and a standard ML system for RTE. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1394--1404</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="1423" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="201" endWordPosition="204">ask attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs). 1 Introduction Distributional Semantic Models (DSMs) approximate the meaning of words with vectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of EMNLP, pages 1394–1404, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit Gupta</author>
<author>Ismail El Maarouf Hannah Bechara</author>
<author>Costantin Orasˇan</author>
</authors>
<title>UoW: NLP techniques developed at the University of Wolverhampton for Semantic Similarity and Textual Entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<marker>Gupta, Bechara, Orasˇan, 2014</marker>
<rawString>Rohit Gupta, Ismail El Maarouf Hannah Bechara, and Costantin Orasˇan. 2014. UoW: NLP techniques developed at the University of Wolverhampton for Semantic Similarity and Textual Entailment. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>George Duenas</author>
<author>Julia Baquero</author>
<author>Alexander Gelbukh</author>
</authors>
<title>UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="17002" citStr="Jimenez et al., 2014" startWordPosition="2773" endWordPosition="2776">, the relatedness task is more challenging than the one run on MSRvid (one of our data sources) at STS 2012, where the top Pearson correlation was 0.88 (Agirre et al., 2012). 6 Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (sys4ITTK’s primary run could not be evaluated due to technical problems with the submission. The b</context>
</contexts>
<marker>Jimenez, Duenas, Baquero, Gelbukh, 2014</marker>
<rawString>Sergio Jimenez, George Duenas, Julia Baquero, and Alexander Gelbukh. 2014. UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Lai</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Illinois-lh: A denotational and distributional approach to semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="16840" citStr="Lai and Hockenmaier, 2014" startWordPosition="2743" endWordPosition="2746">%. The entailment scores obtained on the SICK data set are considerably higher, being 77.06% for the median system and 75.36% for the average system. On the other hand, the relatedness task is more challenging than the one run on MSRvid (one of our data sources) at STS 2012, where the top Pearson correlation was 0.88 (Agirre et al., 2012). 6 Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositio</context>
<context position="20689" citStr="Lai and Hockenmaier, 2014" startWordPosition="3371" endWordPosition="3374">itional system (UNAL-NLP) which reaches the 4th position (0.80 r UNAL-NLP vs. 0.82 r obtained by the best system). UNALNLP however exploits an ad-hoc “negation” feature discussed below. In the entailment task, the best noncompositional model (again UNAL-NLP) reaches the 3rd position, within close reach of the best system (83% UNAL-NLP vs. 84.5% obtained by the best system). Again, purely compositional models have lower performance. haLF CDSM reaches 69.42% accuracy, Illinois-LH Word Overlap combined with a compositional feature reaches 71.8%. The fine-grained analysis reported by Illinois-LH (Lai and Hockenmaier, 2014) shows that a full compositional system (based on point-wise multiplication) fails to capture contradiction. It is better than partial phrase-based compositional models in recognizing entailment pairs, but worse than them on recognizing neutral pairs. Given our more general interest in the distributional approaches, in Table 8 we also classify the different DSMs used as ‘Vector Space Models’, ‘Topic Models’ and ‘Neural Language Models’. Due to the impact shown by learning methods (see ECNU’s results), we also report the different learning approaches used. Several participating systems delibera</context>
</contexts>
<marker>Lai, Hockenmaier, 2014</marker>
<rawString>Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A denotational and distributional approach to semantics. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sa´ul Le´on</author>
<author>Darnes Vilarino</author>
<author>David Pinto</author>
<author>Mireya Tovar</author>
<author>Beatrice Beltr´an</author>
</authors>
<title>BUAP:evaluating compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<marker>Le´on, Vilarino, Pinto, Tovar, Beltr´an, 2014</marker>
<rawString>Sa´ul Le´on, Darnes Vilarino, David Pinto, Mireya Tovar, and Beatrice Beltr´an. 2014. BUAP:evaluating compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elisabeth Lien</author>
<author>Milen Kouylekov</author>
</authors>
<title>UIOLien: Entailment recognition using minimal recursion semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="16980" citStr="Lien and Kouylekov, 2014" startWordPosition="2769" endWordPosition="2772"> system. On the other hand, the relatedness task is more challenging than the one run on MSRvid (one of our data sources) at STS 2012, where the top Pearson correlation was 0.88 (Agirre et al., 2012). 6 Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (sys4ITTK’s primary run could not be evaluated due to technical problems with</context>
</contexts>
<marker>Lien, Kouylekov, 2014</marker>
<rawString>Elisabeth Lien and Milen Kouylekov. 2014. UIOLien: Entailment recognition using minimal recursion semantics. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Luisa Bentivogli</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>A SICK cure for the evaluation of compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC, Reykjavik.</booktitle>
<contexts>
<context position="6226" citStr="Marelli et al. (2014)" startWordPosition="965" endWordPosition="968">ntailment tasks were also welcome. Besides being of intrinsic interest, the latter systems’ performance will serve to situate CDSM performance within the broader landscape of computational semantics. 3 The SICK Data Set The SICK data set, consisting of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment, was used to evaluate the systems participating in the task. The data set creation methodology is outlined in the following subsections, while all the details about data generation and annotation, quality control, and interannotator agreement can be found in Marelli et al. (2014). 3.1 Data Set Creation SICK was built starting from two existing data sets: the 8K ImageFlickr data set1 and the SemEval-2012 STS MSR-Video Descriptions data set.2 The 8K ImageFlickr dataset is a dataset of images, where each image is associated with five descriptions. To derive SICK sentence pairs we randomly chose 750 images and we sampled two descriptions from each of them. The SemEval2012 STS MSR-Video Descriptions data set is a collection of sentence pairs sampled from the short video snippets which compose the Microsoft Research Video Description Corpus. A subset of 750 sentence pairs w</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of LREC, Reykjavik.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="1701" citStr="Mitchell and Lapata (2008)" startWordPosition="245" endWordPosition="248">ectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ data sets have been developed for various computational semantics tasks, such as Semantic Text Similarity (STS)(Agirre et al., 2012) or Recognizing Textual Entailment (RTE) (Dagan et al., 2006). Working with such data sets, </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, pages 236–244, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="1450" citStr="Mitchell and Lapata, 2010" startWordPosition="205" endWordPosition="208">ich participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs). 1 Introduction Distributional Semantic Models (DSMs) approximate the meaning of words with vectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativeco</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Proisl</author>
<author>Stefan Evert</author>
</authors>
<title>SemantiKLUE: Robust semantic similarity at multiple levels using maximum weight matching.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="17026" citStr="Proisl and Evert, 2014" startWordPosition="2777" endWordPosition="2780"> is more challenging than the one run on MSRvid (one of our data sources) at STS 2012, where the top Pearson correlation was 0.88 (Agirre et al., 2012). 6 Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (sys4ITTK’s primary run could not be evaluated due to technical problems with the submission. The best ITTK’s nonprimary ru</context>
</contexts>
<marker>Proisl, Evert, 2014</marker>
<rawString>Thomas Proisl and Stefan Evert. 2014. SemantiKLUE: Robust semantic similarity at multiple levels using maximum weight matching. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1201--1211</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1472" citStr="Socher et al., 2012" startWordPosition="209" endWordPosition="212">btasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs). 1 Introduction Distributional Semantic Models (DSMs) approximate the meaning of words with vectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher Manning, and Andrew Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of EMNLP, pages 1201–1211, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>An N P Vo</author>
<author>Octavian Popescu</author>
<author>Tommaso Caselli</author>
</authors>
<title>FBK-TR: SVM for Semantic Relatedness and Corpus Patterns for RTE.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="16931" citStr="Vo et al., 2014" startWordPosition="2761" endWordPosition="2764">median system and 75.36% for the average system. On the other hand, the relatedness task is more challenging than the one run on MSRvid (one of our data sources) at STS 2012, where the top Pearson correlation was 0.88 (Agirre et al., 2012). 6 Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (sys4ITTK’s primary run coul</context>
</contexts>
<marker>Vo, Popescu, Caselli, 2014</marker>
<rawString>An N. P. Vo, Octavian Popescu, and Tommaso Caselli. 2014. FBK-TR: SVM for Semantic Relatedness and Corpus Patterns for RTE. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Zhao</author>
<author>Tian Tian Zhu</author>
<author>Man Lan</author>
</authors>
<title>ECNU: One Stone Two Birds: Ensemble of Heterogenous Measures for Semantic Relatedness and Textual Entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="16914" citStr="Zhao et al., 2014" startWordPosition="2757" endWordPosition="2760">ing 77.06% for the median system and 75.36% for the average system. On the other hand, the relatedness task is more challenging than the one run on MSRvid (one of our data sources) at STS 2012, where the top Pearson correlation was 0.88 (Agirre et al., 2012). 6 Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (sys4ITTK’s</context>
</contexts>
<marker>Zhao, Zhu, Lan, 2014</marker>
<rawString>Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014. ECNU: One Stone Two Birds: Ensemble of Heterogenous Measures for Semantic Relatedness and Textual Entailment. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>