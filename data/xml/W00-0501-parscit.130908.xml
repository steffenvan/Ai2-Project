<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<bodyText confidence="0.4444144">
When is an Embedded MT System &amp;quot;Good Enough&amp;quot; for Filtering?
Clare R. Voss Carol Van Ess-Dykema
Army Research Laboratory Department of Defense
Adelphi, MD 20783 Ft. Meade, MD
voss@arl.mil cjv anes@ afterlife.ncsc.mil
</bodyText>
<sectionHeader confidence="0.963015" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965">
This paper proposes an end-to-end process
analysis template with replicable measures
to evaluate the filtering performance of a
Scan-OCR-MT system. Preliminary results&apos;
across three language-specific FALCon2
systems show that, with one exception, the
derived measures consistently yield the
same performance ranking: Haitian Creole
at the low end, Arabic in the middle, and
Spanish at the high end.
</bodyText>
<sectionHeader confidence="0.936833" genericHeader="method">
1 The Filtering Problem
</sectionHeader>
<bodyText confidence="0.9999553125">
How do people quickly determine whether a
particular foreign language text document is
relevant to their interest when they do not
understand that foreign language? FALCon, our
embedded MT system, has been designed to
assist an English-speaking person in filtering,
i.e., deciding which foreign language documents
are worth having an expert translator process
further. In this paper, we seek to determine when
such systems are &amp;quot;good enough&amp;quot; for filtering.
We define &amp;quot;filtering&amp;quot; to be a forced-choice
decision-making process on individual
documents, where each document is assigned a
single value, either a &amp;quot;yes, relevant&amp;quot; or a &amp;quot;no,
irrelevant&amp;quot; by the system user.3 The single-
document relevance assessment is performed
</bodyText>
<footnote confidence="0.8661216">
I For a more extensive report of our work, see Voss
and Van Ess-Dykema (2000).
2 FALCon (Forward Area Language CONverter) is a
laptop-based embedded MT system integrated at the
Army Research Laboratory for field use. (Fisher and
Voss, 1997)
3 See the report entitled &amp;quot;Multilingual Information
Management: Current Levels and Future Abilities&amp;quot;
for other defmitions of filtering, available at
http://www.cs.cmu.edu/People/ref/mlim/.
</footnote>
<bodyText confidence="0.999949243902439">
independent of the content of other documents in
the processing collection.
When Church and Hovy (1993) introduced the
notion that &amp;quot;crummy&amp;quot; MT engines could be put
to good use on tasks less-demanding than
publication-quality translation, MT research
efforts did not typically evaluate system
performance in the context of specific tasks.
(Sparck Jones and Galliers, 1996). In the last
few years, however, the Church and Hovy
insight has led to innovative experiments, like
those reported by Resnik (1997), Pomarede et al.
(1998), and Taylor and White (1998), using
task-based evaluation methods. Most recently,
research on task-based evaluation has been.
proposed within TIDES, a recent DARPA
initiative whose goals include enabling English-
speaking individuals to access, correlate, and
interpret multilingual sources of information
(DARPA, 1999; Harmon, 1999).
This paper introduces a method of assessing
when an embedded MT system is &amp;quot;good
enough&amp;quot; for the filtering of hard-copy foreign
language (FL) documents by individuals with no
knowledge of that language. We describe
preliminary work developing measures on
system-internal components that assess: (i) the
flow of words relevant to the filtering task and
domain through the steps of document
processing in our embedded MT system, and (ii)
the level of &amp;quot;noise,&amp;quot; i.e., processing errors,
passing through the system. We present an
analysis template that displays the processing
steps, the sequence of document versions, and
the basic measures of our evaluation method.
After tracing the processing of Spanish, Arabic,
and Haitian Creole parallel texts that is recorded
in the analysis templates, we discuss our
preliminary results on the filtering performance
of the three language-specific embedded MT
systems from this process flow.
</bodyText>
<page confidence="0.976329">
1
</page>
<figureCaption confidence="0.997295">
Figure 1 Analysis Template
</figureCaption>
<sectionHeader confidence="0.776863" genericHeader="method">
2 An Embedded MT System Design&apos;s
</sectionHeader>
<bodyText confidence="0.999133411764706">
Our three systems process documents using a
sequence of three software modules. First, the
Scan software module creates an online bitmap
image in real-time as the user feeds the
document into the page-feed scanner.5 Second,
the optical character recognition (OCR) software
converts that image to character text and, third,
the machine translation (MT) software converts
the foreign language character text to English,
where it may be stored to disk or displayed on
screen directly to the user. The user interface
only requires that the user push one or two
buttons to carry out all of the system&apos;s
processing on an individual document.
We tested three separate language-specific
embedded MT systems for Spanish, Arabic and
Haitian Creole. These systems differ in their
</bodyText>
<footnote confidence="0.9906618">
4 We use &amp;quot;embedded MT systems&amp;quot; as defined in Voss
and Reeder (1998).
5 We chose a small scanner for portability of the
system. Substituting in a flatbed scanner would riot
affect performance.
</footnote>
<bodyText confidence="0.998367">
OCR and MT components, but otherwise they
share the same software, Omnipage&apos;s Paperport
for scaning and Windows95 as the operating
system.6
</bodyText>
<sectionHeader confidence="0.988765" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.995455266666667">
As we sought to measure the performance of
each component in the systems, it quickly
became apparent that not all available measures
may be equally applicable for our filtering task.
For example, counting the number of source
language (SL) characters correctly OCR-ed may
be overly specific: as discussed below, we only
need to make use of the number of SL words
that are correctly OCR-ed. In the sections to
follow, we describe those measures that have
been most informative for the task of filtering.
Analysis Template
We use three types of information in our
evaluation of the end-to-end embedded MT
systems that we have available to us:
transformation processes, document versions,
and basic count measures. The transformation
processes are listed vertically in the diamonds on
the left side of figure 1. Starting with the
hardcopy original document, each process
transforms its input text and creates a new
version. These document versions are listed
vertically in the boxes in the second column of
the figure. For each version, we compute one or
more basic count measures on the words in that
version&apos;s text. That is, for each process, there is
an associated document version and for each
document version, there are associated basic
count measures. These count measures shown
as A. through M. are defined in figure 2 below.
</bodyText>
<subsectionHeader confidence="0.903689">
Two-Pass Evaluation
</subsectionHeader>
<bodyText confidence="0.998134222222222">
For each end-to-end system and language pair,
we follow two separate passes in creating
analysis files from scanned-in bitmap images.
The first pass is for end-to-end Scan-OCR-MT
evaluation: &amp;quot;OCR&amp;quot; the original document, then
MT the resulting OCR-output file. The second
pass is for Ground Truth-MT evaluation:
&amp;quot;ground-truth&amp;quot; (GT) the original document, then
MT the resulting GT-ed output file.
</bodyText>
<footnote confidence="0.4290425">
6 See Voss and Van Ess-Dykema (2000) for a
description of the products used.
</footnote>
<figure confidence="0.9929573">
Processes Document Versions
Measures
tagged TL doe:
open/closed words
tagged TL doe:
cern. related words
image doe in SL
post-OCR doc in SL
post-MT doe in TL
ID
closed class
rds
ID
wrong TL
words
ID
domsdn
words
tagged TL doe.
domain words
</figure>
<page confidence="0.544534">
2
</page>
<figure confidence="0.987523444444444">
H.
TL open cl
words
J.
cern. relate
words in TL
L.
4 domain-rel
words in T
</figure>
<figureCaption confidence="0.8752085">
&apos;ensures 0 SCAN/ -It&apos; ( 5) (1) 0 SCAN/ GT/ M T HAITIAN SCAN/ CREOLE •19
words in 0 OCR/ 1239 3 410 OCR/ MT le 1110 OCR/ GT/ 4(0)
!age doc el SPANISH GT/ re 031 040 CO 4(2) 0 •(27) MT 16
.,c. 4, MT MT la 0 ARABIC 0 28 0 4 all re
</figureCaption>
<table confidence="0.984052818181818">
, CO % co illl 0 4 •ri 37 •8 4(0) CO ,.
I &amp;quot;words&amp;quot; lost/added 411) +4 CIO 0 (+4) 411, 6 0 29 0 I)
n OCR process (0) GI 21 4(23) 0 17 r 0 33
( # &amp;quot;wont &amp;quot;lost 49 CI MT Jo 410 6 8
in MT process r 0 re 12
) la SO 2
&apos; * la
not found &amp;quot;words,, • 1 20
(NFWs) in SL 9 3
G.
# TL words
generated
.. ...1.
s s #wcol rods se di ncTI as,: I
) .
van . # incon-ect I
words in TL 1
M.
•
# not domain- 1
relevant I
words in TL 1
</table>
<figureCaption confidence="0.996555">
Figure 2 Comparison of Language-Specific System Results
</figureCaption>
<bodyText confidence="0.998901633333333">
The two passes represent the &amp;quot;worst&amp;quot; and
&amp;quot;best&amp;quot; cases respectively for filtering within
each of the three embedded MT systems. By
&amp;quot;ground truth&amp;quot; versions of the document, we
mean online duplicated versions that match,
character-for-character, the input text.
We intentionally chose low-performance
OCR software (for each language) to simulate a
&amp;quot;worst case&amp;quot; performance by our systems,
enabling us to compare them with the ideal high-
performance ground-truth input to simulate a
&amp;quot;best case&amp;quot; performance.
Texts from the Center for Disease Control
In order to compare the three language-specific
systems, we had to fmd a corpus in a domain
well-defmed for filtering7 that included parallel
texts in Spanish, Arabic, and Haitian Creole. We
found parallel corpora for these and many other
languages at a website of the Center for Disease
Control (CDC).8
We chose a paragraph from the chicken
pox/varicella bulletin, page 2, for each of our
three languages. This passage contains narrative
full-length sentences and minimizes the OCR
complications that arise with variable layouts.
Our objective for selecting this input paragraph
was to illustrate our methodology in a tractable
way for multiple languages. Our next step will
be to increase the amount of data analyzed for
each language.
</bodyText>
<sectionHeader confidence="0.991632" genericHeader="method">
4 Analyses
</sectionHeader>
<bodyText confidence="0.991322">
We fill out one analysis template for each
document tested in a language-specific system.
Example templates with the basic count
</bodyText>
<tableCaption confidence="0.784014142857143">
7 Filtering judgments are &amp;quot;well-defined&amp;quot; when
multiple readers of a text in a domain agree on the
&amp;quot;yes, relevant&amp;quot; status of the text.
8 See http://www.iinmunize.orgivis/index.htrn. The
texts are &apos;Vaccine Information Statements&amp;quot;
describing basic medical symptoms that individuals
should know about in advance of being vaccinated.
</tableCaption>
<page confidence="0.996697">
3
</page>
<bodyText confidence="0.999759878787879">
measures9 are presented in figure 2 for each of
the three embedded MT systems that we tested.
Notice that in figure 2 we distinguish valid
words of a language from OCR-generated
strings of characters that we identify as &amp;quot;words.&amp;quot;
The latter &amp;quot;words&amp;quot; may include any of the
following: wordstrings with OCR-induced
spelling changes (valid or invalid for the specific
language), wordstrings duplicating misspellings
in the source document, and words accurately
OCR-ed. &amp;quot;Words&amp;quot; may also be lost in the MT
process (see F.).1°
The wide, block arrow in figure 2 connects E.
and G. because they are both based on the MT
output document. (We do not compute a sum for
these counts because the E &amp;quot;words&amp;quot; are in the SL
and the G words are in the TL.) The open class
words (see H.) are nouns, verbs, adjectives, and
adverbs. Closed class words (see I.) include all
parts of speech not listed as open class
categories.
In this methodology, we track the content
words that ultimately contribute to the final
filtering decision. Clearly for other tasks, such
as summarization or information extraction,
other measures may be more appropriate. The
basic count measures A. through M. are
preliminary and will require refinement as more
data sets are tested. From these basic count
measures, we define four derived percentage
measures in section 5 and summarize these cases
across our three systems in figure 3 of that
section.
</bodyText>
<subsectionHeader confidence="0.988558">
4.1 Embedded Spanish MT System Test
</subsectionHeader>
<bodyText confidence="0.950862482142857">
&amp;quot;Worst&amp;quot; case (Scan-OCR-MT pass)
As can be seen in figure 2, not all of the original
80 Spanish words in the source document retain
their correct spelling after being OCR-ed. Only
26 OCR-ed &amp;quot;words&amp;quot; are found in the MT
lexicon, i.e., recognized as valid Spanish words.
Forty-nine of the OCR-ed &amp;quot;words&amp;quot; are treated as
&amp;quot;not found words&amp;quot; (NFWs) by the MT engine,
even though they may in fact be actual Spanish
words. Five other OCR-ed &amp;quot;words&amp;quot; are lost in
9 The following formulas summarize the relations
among the count measures: A = B+C; B = D+E+F;
G H+I; H = J+K; J = L+M.
10 For example, we found that the word la in the
Spanish text was not present in the TL output, i.e.,
the English equivalent the did not appear in the
English translation.
the MT process. Thus, the OCR process reduced
the number of Spanish words that the MT engine
could accept as input by more than 60%.
Of the remaining 40% that generated 29
English words, we found that 5 were &amp;quot;filter-
relevant&amp;quot; as follows. The MT engine ignored 49
post-OCR Spanish &amp;quot;words&amp;quot; and working from
the remaining 26 Spanish words, generated 29
English words.&amp;quot; Seventeen were open class
words and 12 were closed class words. Nearly
all of the open class words were translated
correctly or were semantically appropriate for
the domain (16 out of 17). From this correct set
of 16 open class words, 5 were domain-relevant
and 9 were not. That is, 5 of the 29 generated
English words, or 17%, were semantically
related and domain relevant words, i.e., triggers
for filtering judgments.
&amp;quot;Best&amp;quot; case (GT-MT pass)
The MT engine generated 77 English words
from the 80 original Spanish words. Thirty-
eight, or half of the 77, were open class words;
39 were closed class words. All of the 38 open
class words were correctly translated or
semantically related to the preferred translation.
And half of those, 17, were domain-relevant.
Thus, the 77 English words generated by the MT
engine contained 17 &amp;quot;filter-relevant&amp;quot; words, or
22%.
Comparing the Two Passes
Surprisingly the GT-MT pass only yields a 5%
improvement in filtering judgments over the
Scan-OCR-MT pass, even though the OCR itself
reduced the number of Spanish words that the
MT engine could accept as input by more than
60%. We must be cautious in interpreting the
significance of this comparison, given the single,
short paragraph used only for illustrating our
methodology.
</bodyText>
<subsectionHeader confidence="0.979449">
4.2 Embedded Arabic MT System Test
</subsectionHeader>
<bodyText confidence="0.947939111111111">
&amp;quot;Worst&amp;quot; case (Scan-OCR-MT pass)
The OCR process converted the original 84
Arabic words into 88 &amp;quot;words&amp;quot;. Of the original
84 Arabic words in the source document, only
11 This occurred because the MT engine was not
using a word-for-word scheme. The Spanish verb for
debo is translated into 2 English words, I must. As we
will note further on, different languages have
different expansion rates into English.
</bodyText>
<page confidence="0.995182">
4
</page>
<bodyText confidence="0.99985487804878">
55 retain their correct spelling after being OCR-
ed and are found in the MT lexicon, i.e.,
recognized as valid Arabic words. Ten of the
other OCR-ed &amp;quot;words&amp;quot; are treated as NFWs by
the MT engine. The remaining 23 OCR-ed
mixture of original words and OCR-induced
&amp;quot;words&amp;quot; are not found in the Arabic MT lexicon.
Thus, the OCR process reduced the number of
original Arabic words that the MT engine could
accept as input by slightly more than 65%.
Of the remaining 35% that generated 70
English words, we found that 7 were &amp;quot;filter-
relevant&amp;quot; as follows. The MT lexicon did not
contain 10 post-OCR Arabic &amp;quot;words&amp;quot; and
working from the remaining 55 Arabic words,
the MT engine generated 70 English words.12
Thirty of the 70 were open class words and 40
were closed class words. Only one-third of the
open class words were translated correctly or
were semantically appropriate for the domain
(10 out of 30). From this correct set of 10 open
class words, 7 were domain-relevant and 3 were
not. Thus, this pass yields 7 words for filtering
judgments from the 70 generated English words,
or 10%, were semantically related and domain
relevant words.
&amp;quot;Best&amp;quot; case (GT-MT pass)
Of the 84 original Arabic words, even with the
GT as input, 28 were not found in the MT
lexicon, reflecting the engine&apos;s emerging status
and the need for further development. Two
others were not found in the Arabic MT lexicon,
leaving 54 remaining words as input to the MT
engine. The MT engine generated 68 English
words from these 54 words. Thirty-one of the
68 were open class words; 37 were closed class
words. Of the open class words, 25 were
translated correctly or semantically related. And
8 of those 25 were domain-relevant. Thus, the
68 English words generated by the MT engine
contained 8 &amp;quot;filter-relevant&amp;quot; words, or 12%.
</bodyText>
<subsectionHeader confidence="0.804223">
Comparing the Two Passes
</subsectionHeader>
<bodyText confidence="0.91912625">
The GT-MT pass yields a 2% improvement in
filtering judgments over the Scan-OCR-MT
pass, even though the OCR itself reduced the
12 This expansion rate is consistent with the rule-of-
thumb that Arabic linguists have for every one
Arabic word yielding on average 1.3 words in
English.
number of Arabic words that the MT engine
could accept as input by about 65%.
One of the interesting findings about OCR-ed
Arabic &amp;quot;words&amp;quot; was the presence of &amp;quot;false
positives,&amp;quot; inaccurately OCR-ed source
document words that were nonetheless valid in
Arabic. That is, we found instances of valid
Arabic words in the OCR output that appeared
as different words in the original document.13
</bodyText>
<subsectionHeader confidence="0.995908">
4.3 Embedded Haitian MT System Test
</subsectionHeader>
<bodyText confidence="0.99973104">
&amp;quot;Worst&amp;quot; case (Scan-OCR-MT pass)
In the template for the 76-word Haitian Creole
source document, we see that 27 words were lost
in the OCR process, leaving only 49 in the post-
OCR document. Of those 49, only 20 exhibit
their correct spelling after being OCR-ed and are
found in the MT lexicon. Twenty-nine of the 49
OCR-ed &amp;quot;words&amp;quot; are not found (NFWs) by the
MT engine. The OCR process reduced the
number of original Haitian Creole words
acceptable by the MT engine from 76 to 20, or
74%.
Of the remaining 26% that generated 22
English words, we found that none were &amp;quot;filter
relevant,&amp;quot; i.e., 0%, as follows. The MT engine
ignored 29 post-OCR &amp;quot;words&amp;quot; and working from
the remaining 20 Haitian words, generated 22
English words. Ten were open class words and
12 were closed class words. Only 2 out of the 10
open class words were translated correctly or
were semantically appropriate for the domain.
From this correct set of 2 open class words,
none were domain-relevant. The human would
be unable to use this fmal document version to
make his or her filtering relevance judgments.
</bodyText>
<subsectionHeader confidence="0.513205">
&amp;quot;Best&amp;quot; case (GT-MT pass)
</subsectionHeader>
<bodyText confidence="0.999947">
The MT engine generated 63 English words
from the 76 original Haitian Creole words.
Thirty of the 63 were open class words; 33 were
closed class words. Only 11 of the 30 open class
words were correctly translated or semantically
related. Of those 11 words, 3 were domain-
relevant. So, from the 63 generated English
words, only 3 were &amp;quot;filter-relevant&amp;quot;, or 5%.
</bodyText>
<footnote confidence="0.997368">
13 As a result, the number of words in the two passes
can differ. As we see in figure 2 in the Scan-OCR-
MT pass, there were 55 SL words translated but, in
the GT-MT pass, only 54 SL words in the original
text.
</footnote>
<page confidence="0.980566">
5
</page>
<table confidence="0.963847">
Derived Spanish Arabic Haitian Creole
Meas. OCR GT OCR GT OCR GT
40 95 35 64 26 79
55 49 14 37 9 17
17 22 10 12 0 5
94 100 33 67 20 33
</table>
<figureCaption confidence="0.8033745">
Figure 3 Summary of Language-Specific Resull s
(percentages)
</figureCaption>
<subsectionHeader confidence="0.985751">
Comparing the Two Passes
</subsectionHeader>
<bodyText confidence="0.99972675">
With an OCR package not trained for this
specific language and an MT engine from a
research effort, the embedded MT system with
these components does not assist the human on
the filtering task. And even with the ground-
truth input, the MT engine is not sufficiently
robust to produce useful translations of valid
Haitian Creole words.
</bodyText>
<sectionHeader confidence="0.990167" genericHeader="method">
5 Cross-System Results
</sectionHeader>
<bodyText confidence="0.99997425">
In figure 3 we compare the three language-
specific systems, we make use of four measures
derived from the basic counts, A. through M., as
defmed in figure 2.
</bodyText>
<subsectionHeader confidence="0.812456">
W. Original Document-MT Word Recall
</subsectionHeader>
<bodyText confidence="0.999268625">
% of original SL document words translatable
by the MT engine after being OCR-ed. (DIA)
This measure on the GT pass in all 3 systems
gives us the proportion of words in the original
SL document that are in the individual MT
lexicons. The Spanish lexicon is strong for the
domain of our document (W = 95%). The
measures for Arabic and Haitian Creole reflect
the fact that their MT lexicons are still under
development (W = 64% and 79%, respectively).
This measure on the OCR pass, given the
corresponding measure on the GT pass as a
baseline, captures the degradation introduced by
the Scan-OCR processing of the document.
From figure 3 we see that the Spanish system
loses approximately 55% of its original
document words going into the MT engine (95%
minus 40%), the Haitian Creole 53% (79%
minus 26%), and the Arabic 29% (64% minus
35%). Recall that the Spanish and Haitian
Creole systems included the same OCR
software, which may account for the similar
level of performance here. This software was not
available to us for Arabic.
</bodyText>
<sectionHeader confidence="0.450398" genericHeader="method">
X. MT Semantic Adequacy
</sectionHeader>
<construct confidence="0.416631666666667">
% of TL words generated by MT engine that are
open class &amp; semantically adequate in their
translation (JIG)
</construct>
<bodyText confidence="0.999852538461538">
This measure is intended to assess whether a
system can be used for filtering broad-level
topics (in contrast to domains with specialized
vocabulary that we discuss below). Here we see
evidence for two patterns that recur in the two
measures below. First, the GT pass---with one
exception---exhibits better performance than the
OCR pass. Second, there is a ranking of the
systems with Haitian Creole at the low end,
Arabic in the middle, and Spanish at the high
end. We will need more data to determine the
significance of the one exception (55% versus
49%).
</bodyText>
<sectionHeader confidence="0.877916" genericHeader="method">
Y. MT Domain-Relevant Adequacy
</sectionHeader>
<bodyText confidence="0.972571">
% of M words generated by MT engine that are
open class, semantically adequate in their
translation, and domain-relevant (LIG)
In all of the systems there was a slight gain in
domain-relevant filtering performance from the
OCR pass to the GT pass. We can rank the
systems with the Haitian Creole at the low end,
the Arabic in the middle, and the Spanish at the
high end: the measures in both the OCR and GT
passes in Haitian Creole are lower than in the
Arabic, which are lower than in the Spanish.
Only the Spanish documents, but not the Arabic
or Haitian Creole ones, when machine translated
in either pass were judged domain-relevant by
five people during an informal test.14 Thus, our
data suggests that the Spanish system&apos;s lower
bound (OCR pass) of 17% on this measure is
needed for filtering.
</bodyText>
<listItem confidence="0.9333635">
Z. MT Open Class Semantic Adequacy
% of open class IL words generated by MT
engine that are semantically adequate in their
translation (JIH)
</listItem>
<footnote confidence="0.982487">
14 We are in the process of running an experiment to
validate the protocol for establishing domain-relevant
judgments as part of our research in measures of
effectiveness (MOEs) for task-based evaluation.
</footnote>
<page confidence="0.998222">
6
</page>
<bodyText confidence="0.999918857142857">
The same pattern emerges with this measure. In
each system there is an improvement in
performance stepping from the OCR pass to the
GT pass. Across systems we see the same
ranking, with the OCR and GT passes of the
Haitian Creole falling below the Arabic which
falls below the Spanish.
</bodyText>
<sectionHeader confidence="0.818052" genericHeader="conclusions">
Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.995404784313726">
Our main contribution has been the proposal of
an end-to-end process analysis template and a
replicable evaluation methodology. We present
measures to evaluate filtering performance and
preliminary results on Spanish, Arabic and
Haitian Creole FALCon systems.
The cross-system comparisons using the
measures presented, with one exception, yielded
the following expected rankings: (i) the GT-MT
pass exhibits better performance than the Scan-
OCR-MT pass and (ii) the Haitian Creole
system is at the low end, Arabic is in the middle,
and Spanish is at the high end.
Our long-term objective is to compare the
results of the system-internal &amp;quot;measures of
performance&amp;quot; (MOPs) presented here with
results we still need from system-external
&amp;quot;measures of effectiveness&amp;quot; (MOEs).&apos;5 MOE-
based methods evaluate (i) baseline unaided
human performance, (ii) human performance
using a new system and (iii) human expert
performance. From this comparison we will be
able to determine whether these two
independently derived sets of measures are
replicable and validate each other. So far, we
have only addressed our original question,
&amp;quot;when is an embedded MT system good enough
for filtering?&amp;quot; in terms of MOPs. We found that,
for our particular passage in the medical domain,
documents need to reach at least 17% on our
derived measure Y., MT domain-relevant
adequacy (recall discussion of derived measure
Y, in section 5).
Given that all but one process step (&amp;quot;ID wrong
TL words&amp;quot; as shown in figure 1 where a human
stick figure appears) in filling the template can
be automated, the next phase of this work will
be to create a software tool to speed up and
systematize this process, improving our system
evaluation by increasing the number of
15 See Roche and Watts (1991) for definitions of
these terms.
documents that can be regularly used to test each
new system and reducing the burden on the
operational linguists who assist us for the one
critical step. Currently available tools for
parallel text processing, including text alignment
software, may provide new user interface
options as well, improving the interactive
assessment process and possibly extending the
input set to include transcribed speech.
</bodyText>
<sectionHeader confidence="0.996364" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997392">
We would like to acknowledge Lisa Decrozant
(Army Research Laboratory) and Brian
Branagan (Department of Defense) for language
expertise and Francis Fisher (Army Research
Laboratory) for systems engineering expertise.
</bodyText>
<sectionHeader confidence="0.999415" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999486888888889">
Church, K. and Hovy, E. 1993. Good Applications
for Crummy Machine Translation. Machine
Translation, Volume 8, pages 239 - 258.
DARPA 1999. Translingual Information Detection,
Extraction, and Summarization (TIDES) Initiative.
http://www.darpa.mil/ito/researchitides/index.html
Fisher, F. and Voss, C. R. 1997. &amp;quot;FALCon, an MT
System Support Tool for Non-linguists.&amp;quot; In
Proceedings of the Advanced Information
Processing and Analysis Conference. McLean,VA.
Harmon, D. 1999. &amp;quot;A Framework for Evaluation in
TIDES.&amp;quot; Presentation at TIDES Planning
Workshop, with link at http://www.dyncorp-
is.com/darpa/meetings/tides99jultagenda.html, July
28-30, Leesburg, VA.
Pomarede, J.-M., Taylor, K., and Van Ess-Dykema,
C. 1998. Sparse Training Data and EBMT. In
Proceedings of the Workshop on Embedded MT
Systems: Design, Construction, and Evaluation of
Systems with an MT Component held in
conjunction with the Association for Machine
Translation in the Americas (AMTA&apos;98),
Langhorne, PA, October.
Resnik, P. 1997. Evaluating Multilingual Gisting of
Web Pages. In Working Notes of the AAAI Spring
Symposium on Natural Language Processing for
the World Wide Web, Palo Alto, CA.
Roche, J. G. and Watts, B. D. 1991. Choosing
Analytic Measures. The Journal of Strategic
Studies, Volume 14, pages 165-209, June.
Sparck Jones, K. and Galliers, J. 1996. Evaluating
Natural Language Processing Systems. Springer-
Verlag Publishers, Berlin, Germany.
Taylor, K. and White, J. 1998. Predicting What MT
is Good for: User Judgments and Task
Performance. In Proceedings of the Third
</reference>
<page confidence="0.99026">
7
</page>
<reference confidence="0.999375714285714">
Conference of the Association for Machine
Translation in the Americas (AMTA&apos;98), pages
364 -373, Langhorne, PA, October.
Voss, C. R. and Reeder, F. (eds.). 1998.
Proceedings of the Workshop on Embedded MT
Systems: Design, Construction, and Evaluation of
Systems with an MT Component held in
conjunction with the Association for Machine
Translation in the Americas (AMTA&apos;98),
Langhorne, PA, October.
Voss, C. R. and Van Ess-Dykema, C. 2000.
Evaluating Scan-OCR-MT Processing for the
Filtering Task. Army Research Laboratory
Technical Report, Adelphi, MD.
</reference>
<page confidence="0.998495">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.996983">When is an Embedded MT System &amp;quot;Good Enough&amp;quot; for Filtering?</title>
<author confidence="0.99868">Clare R Voss Carol Van_Ess-Dykema</author>
<affiliation confidence="0.833026">Army Research Laboratory Department of Defense</affiliation>
<address confidence="0.826204">Adelphi, MD 20783 Ft. Meade, MD</address>
<email confidence="0.651363">voss@arl.milcjvanes@afterlife.ncsc.mil</email>
<abstract confidence="0.979894916666667">This paper proposes an end-to-end process analysis template with replicable measures to evaluate the filtering performance of a Scan-OCR-MT system. Preliminary results&apos; three language-specific systems show that, with one exception, the derived measures consistently yield the same performance ranking: Haitian Creole at the low end, Arabic in the middle, and Spanish at the high end. 1 The Filtering Problem How do people quickly determine whether a particular foreign language text document is relevant to their interest when they do not understand that foreign language? FALCon, our embedded MT system, has been designed to assist an English-speaking person in filtering, i.e., deciding which foreign language documents are worth having an expert translator process further. In this paper, we seek to determine when such systems are &amp;quot;good enough&amp;quot; for filtering. We define &amp;quot;filtering&amp;quot; to be a forced-choice decision-making process on individual documents, where each document is assigned a single value, either a &amp;quot;yes, relevant&amp;quot; or a &amp;quot;no, by the system The singledocument relevance assessment is performed I For a more extensive report of our work, see Voss and Van Ess-Dykema (2000). 2FALCon (Forward Area Language CONverter) is a laptop-based embedded MT system integrated at the Army Research Laboratory for field use. (Fisher and Voss, 1997) 3See the report entitled &amp;quot;Multilingual Information Management: Current Levels and Future Abilities&amp;quot; for other defmitions of filtering, available at http://www.cs.cmu.edu/People/ref/mlim/. independent of the content of other documents in the processing collection. When Church and Hovy (1993) introduced the notion that &amp;quot;crummy&amp;quot; MT engines could be put to good use on tasks less-demanding than publication-quality translation, MT research efforts did not typically evaluate system performance in the context of specific tasks. (Sparck Jones and Galliers, 1996). In the last few years, however, the Church and Hovy insight has led to innovative experiments, like those reported by Resnik (1997), Pomarede et al. (1998), and Taylor and White (1998), using task-based evaluation methods. Most recently, research on task-based evaluation has been. proposed within TIDES, a recent DARPA initiative whose goals include enabling Englishspeaking individuals to access, correlate, and interpret multilingual sources of information (DARPA, 1999; Harmon, 1999). This paper introduces a method of assessing when an embedded MT system is &amp;quot;good enough&amp;quot; for the filtering of hard-copy foreign language (FL) documents by individuals with no knowledge of that language. We describe preliminary work developing measures on that assess: (i) the flow of words relevant to the filtering task and domain through the steps of document processing in our embedded MT system, and (ii) the level of &amp;quot;noise,&amp;quot; i.e., processing errors, passing through the system. We present an template displays the processing steps, the sequence of document versions, and the basic measures of our evaluation method. After tracing the processing of Spanish, Arabic, and Haitian Creole parallel texts that is recorded in the analysis templates, we discuss our preliminary results on the filtering performance of the three language-specific embedded MT systems from this process flow. 1 Figure 1 Analysis Template 2 An Embedded MT System Design&apos;s Our three systems process documents using a sequence of three software modules. First, the Scan software module creates an online bitmap image in real-time as the user feeds the into the page-feed Second, the optical character recognition (OCR) software converts that image to character text and, third, the machine translation (MT) software converts the foreign language character text to English, where it may be stored to disk or displayed on screen directly to the user. The user interface only requires that the user push one or two buttons to carry out all of the system&apos;s processing on an individual document. tested three separate embedded MT systems for Spanish, Arabic and Haitian Creole. These systems differ in their 4We use &amp;quot;embedded MT systems&amp;quot; as defined in Voss and Reeder (1998). 5We chose a small scanner for portability of the system. Substituting in a flatbed scanner would riot affect performance. OCR and MT components, but otherwise they share the same software, Omnipage&apos;s Paperport for scaning and Windows95 as the operating As we sought to measure the performance of each component in the systems, it quickly became apparent that not all available measures may be equally applicable for our filtering task. For example, counting the number of source (SL) OCR-ed may be overly specific: as discussed below, we only to make use of the number of SL that are correctly OCR-ed. In the sections to follow, we describe those measures that have been most informative for the task of filtering. Analysis Template We use three types of information in our evaluation of the end-to-end embedded MT systems that we have available to us: transformation processes, document versions, and basic count measures. The transformation processes are listed vertically in the diamonds on the left side of figure 1. Starting with the hardcopy original document, each process transforms its input text and creates a new version. These document versions are listed vertically in the boxes in the second column of the figure. For each version, we compute one or more basic count measures on the words in that version&apos;s text. That is, for each process, there is an associated document version and for each version, are associated basic count measures. These count measures shown A. through M. are defined figure 2 below. Two-Pass Evaluation For each end-to-end system and language pair, we follow two separate passes in creating analysis files from scanned-in bitmap images. The first pass is for end-to-end Scan-OCR-MT evaluation: &amp;quot;OCR&amp;quot; the original document, then MT the resulting OCR-output file. The second pass is for Ground Truth-MT evaluation: &amp;quot;ground-truth&amp;quot; (GT) the original document, then MT the resulting GT-ed output file. 6See Voss Van Ess-Dykema (2000) for a description of the products used. Processes Document Versions Measures tagged TL doe: open/closed words tagged TL doe: cern. related words image doe in SL post-OCR doc in SL doe in TL ID closed class rds wrong words words TL domain words 2 H. TL open cl words J. cern. relate words in TL L. words in T &apos;ensures 0 SCAN/ OCR/ ( 5) 3 0 SCAN/ OCR/ MT M T HAITIAN SCAN/ OCR/ CREOLE •1916 words in !age doc 0 SPANISH GT/ re 031 040 CO le 1110 •(27) 4 4(0) 29 MT re , el MT MT la illl 0 ARABIC 0 4(2) 0 r all ,. I &amp;quot;words&amp;quot; lost/added n OCR process 4, % co 0 0 4 •ri 28 0 6 CO I) ( # &amp;quot;wont &amp;quot;lost in MT process CO +4 CIO 21 (+4) 411, 37 •80 12 0 33 ) 411) (0) GI MT Jo 0 6 2 0 8 &apos; 49 CI re 410 17 found r 0 SO la (NFWs) in SL la * 20 G. 1 3 9 TL generated .. ...1. s s I ) # I van words in TL 1 M. • relevant I words in TL 1 Figure 2 Comparison of Language-Specific System Results The two passes represent the &amp;quot;worst&amp;quot; and &amp;quot;best&amp;quot; cases respectively for filtering within each of the three embedded MT systems. By &amp;quot;ground truth&amp;quot; versions of the document, we mean online duplicated versions that match, character-for-character, the input text. We intentionally chose low-performance OCR software (for each language) to simulate a &amp;quot;worst case&amp;quot; performance by our systems, enabling us to compare them with the ideal highperformance ground-truth input to simulate a &amp;quot;best case&amp;quot; performance. Texts from the Center for Disease Control In order to compare the three language-specific systems, we had to fmd a corpus in a domain for that included parallel texts in Spanish, Arabic, and Haitian Creole. We found parallel corpora for these and many other languages at a website of the Center for Disease We chose a paragraph from the chicken pox/varicella bulletin, page 2, for each of our three languages. This passage contains narrative full-length sentences and minimizes the OCR complications that arise with variable layouts. Our objective for selecting this input paragraph was to illustrate our methodology in a tractable way for multiple languages. Our next step will be to increase the amount of data analyzed for each language. We fill out one analysis template for each document tested in a language-specific system. Example templates with the basic count 7Filtering judgments are &amp;quot;well-defined&amp;quot; when multiple readers of a text in a domain agree on the &amp;quot;yes, relevant&amp;quot; status of the text. 8See http://www.iinmunize.orgivis/index.htrn. The texts are &apos;Vaccine Information Statements&amp;quot; describing basic medical symptoms that individuals should know about in advance of being vaccinated. 3 are presented in figure 2 for each of the three embedded MT systems that we tested. Notice that in figure 2 we distinguish valid words of a language from OCR-generated strings of characters that we identify as &amp;quot;words.&amp;quot; The latter &amp;quot;words&amp;quot; may include any of the following: wordstrings with OCR-induced spelling changes (valid or invalid for the specific language), wordstrings duplicating misspellings in the source document, and words accurately OCR-ed. &amp;quot;Words&amp;quot; may also be lost in the MT (see The wide, block arrow in figure 2 connects E. and G. because they are both based on the MT output document. (We do not compute a sum for these counts because the E &amp;quot;words&amp;quot; are in the SL and the G words are in the TL.) The open class words (see H.) are nouns, verbs, adjectives, and adverbs. Closed class words (see I.) include all parts of speech not listed as open class categories. In this methodology, we track the content words that ultimately contribute to the final filtering decision. Clearly for other tasks, such as summarization or information extraction, other measures may be more appropriate. The basic count measures A. through M. are preliminary and will require refinement as more data sets are tested. From these basic count measures, we define four derived percentage measures in section 5 and summarize these cases across our three systems in figure 3 of that section. 4.1 Embedded Spanish MT System Test case pass) As can be seen in figure 2, not all of the original 80 Spanish words in the source document retain their correct spelling after being OCR-ed. Only 26 OCR-ed &amp;quot;words&amp;quot; are found in the MT lexicon, i.e., recognized as valid Spanish words. Forty-nine of the OCR-ed &amp;quot;words&amp;quot; are treated as &amp;quot;not found words&amp;quot; (NFWs) by the MT engine, even though they may in fact be actual Spanish words. Five other OCR-ed &amp;quot;words&amp;quot; are lost in 9The following formulas summarize the relations among the count measures: A = B+C; B = D+E+F; G H+I; H = J+K; J = L+M. For example, we found that the word the Spanish text was not present in the TL output, i.e., English equivalent not appear in the English translation. the MT process. Thus, the OCR process reduced the number of Spanish words that the MT engine could accept as input by more than 60%. Of the remaining 40% that generated 29 English words, we found that 5 were &amp;quot;filterrelevant&amp;quot; as follows. The MT engine ignored 49 post-OCR Spanish &amp;quot;words&amp;quot; and working from the remaining 26 Spanish words, generated 29 English words.&amp;quot; Seventeen were open class and 12 were closed class words. all of the open class words were translated correctly or were semantically appropriate for the domain (16 out of 17). From this correct set of 16 open class words, 5 were domain-relevant and 9 were not. That is, 5 of the 29 generated English words, or 17%, were semantically related and domain relevant words, i.e., triggers for filtering judgments. case pass) The MT engine generated 77 English words from the 80 original Spanish words. Thirtyeight, or half of the 77, were open class words; 39 were closed class words. All of the 38 open class words were correctly translated or semantically related to the preferred translation. And half of those, 17, were domain-relevant. Thus, the 77 English words generated by the MT engine contained 17 &amp;quot;filter-relevant&amp;quot; words, or 22%. Two Passes Surprisingly the GT-MT pass only yields a 5% improvement in filtering judgments over the Scan-OCR-MT pass, even though the OCR itself reduced the number of Spanish words that the MT engine could accept as input by more than 60%. We must be cautious in interpreting the significance of this comparison, given the single, short paragraph used only for illustrating our methodology. 4.2 Embedded Arabic MT System Test case pass) The OCR process converted the original 84 Arabic words into 88 &amp;quot;words&amp;quot;. Of the original 84 Arabic words in the source document, only 11This occurred because the MT engine was not using a word-for-word scheme. The Spanish verb for translated into 2 English words, must. we will note further on, different languages have different expansion rates into English. 4 55 retain their correct spelling after being OCRed and are found in the MT lexicon, i.e., recognized as valid Arabic words. Ten of the other OCR-ed &amp;quot;words&amp;quot; are treated as NFWs by the MT engine. The remaining 23 OCR-ed mixture of original words and OCR-induced &amp;quot;words&amp;quot; are not found in the Arabic MT lexicon. Thus, the OCR process reduced the number of original Arabic words that the MT engine could accept as input by slightly more than 65%. Of the remaining 35% that generated 70 English words, we found that 7 were &amp;quot;filterrelevant&amp;quot; as follows. The MT lexicon did not contain 10 post-OCR Arabic &amp;quot;words&amp;quot; and working from the remaining 55 Arabic words, MT engine generated 70 English Thirty of the 70 were open class words and 40 were closed class words. Only one-third of the open class words were translated correctly or were semantically appropriate for the domain (10 out of 30). From this correct set of 10 open class words, 7 were domain-relevant and 3 were not. Thus, this pass yields 7 words for filtering judgments from the 70 generated English words, or 10%, were semantically related and domain relevant words. case pass) Of the 84 original Arabic words, even with the GT as input, 28 were not found in the MT lexicon, reflecting the engine&apos;s emerging status and the need for further development. Two others were not found in the Arabic MT lexicon, leaving 54 remaining words as input to the MT engine. The MT engine generated 68 English words from these 54 words. Thirty-one of the 68 were open class words; 37 were closed class words. Of the open class words, 25 were translated correctly or semantically related. And 8 of those 25 were domain-relevant. Thus, the 68 English words generated by the MT engine contained 8 &amp;quot;filter-relevant&amp;quot; words, or 12%. Comparing the Two Passes GT-MT yields a 2% improvement in filtering judgments over the Scan-OCR-MT even though the reduced the 12This expansion rate is consistent with the rule-ofthumb that Arabic linguists have for every one Arabic word yielding on average 1.3 words in English. number of Arabic words that the MT engine could accept as input by about 65%. One of the interesting findings about OCR-ed Arabic &amp;quot;words&amp;quot; was the presence of &amp;quot;false positives,&amp;quot; inaccurately OCR-ed source document words that were nonetheless valid in Arabic. That is, we found instances of valid Arabic words in the OCR output that appeared different words in the original 4.3 Embedded Haitian MT System Test case pass) In the template for the 76-word Haitian Creole source document, we see that 27 words were lost in the OCR process, leaving only 49 in the post- OCR document. Of those 49, only 20 exhibit their correct spelling after being OCR-ed and are found in the MT lexicon. Twenty-nine of the 49 OCR-ed &amp;quot;words&amp;quot; are not found (NFWs) by the MT engine. The OCR process reduced the number of original Haitian Creole words acceptable by the MT engine from 76 to 20, or 74%. Of the remaining 26% that generated 22 English words, we found that none were &amp;quot;filter relevant,&amp;quot; i.e., 0%, as follows. The MT engine ignored 29 post-OCR &amp;quot;words&amp;quot; and working from the remaining 20 Haitian words, generated 22 English words. Ten were open class words and 12 were closed class words. Only 2 out of the 10 open class words were translated correctly or were semantically appropriate for the domain. From this correct set of 2 open class words, none were domain-relevant. The human would be unable to use this fmal document version to make his or her filtering relevance judgments. case pass) The MT engine generated 63 English words from the 76 original Haitian Creole words. Thirty of the 63 were open class words; 33 were closed class words. Only 11 of the 30 open class words were correctly translated or semantically related. Of those 11 words, 3 were domainrelevant. So, from the 63 generated English words, only 3 were &amp;quot;filter-relevant&amp;quot;, or 5%. 13As a result, the number of words in the two passes can differ. As we see in figure 2 in the Scan-OCR-</abstract>
<note confidence="0.3216215">MT pass, there were 55 SL words translated but, in the GT-MT pass, only 54 SL words in the original text. 5 Derived Spanish Arabic Haitian Creole Meas. OCR GT OCR GT OCR GT</note>
<phone confidence="0.75589225">40 95 35 64 26 79 55 49 14 37 9 17 17 22 10 12 0 5 94 100 33 67 20 33</phone>
<abstract confidence="0.985603873333333">Figure 3 Summary of Language-Specific Resull (percentages) Comparing the Two Passes an not trained for this specific language and an MT engine from a research effort, the embedded MT system with these components does not assist the human on the filtering task. And even with the groundtruth input, the MT engine is not sufficiently robust to produce useful translations of valid Haitian Creole words. Results In figure 3 we compare the three languagespecific systems, we make use of four measures derived from the basic counts, A. through M., as defmed in figure 2. W. Original Document-MT Word Recall % of original SL document words translatable the MT engine after being OCR-ed. This measure on the GT pass in all 3 systems gives us the proportion of words in the original SL document that are in the individual MT lexicons. The Spanish lexicon is strong for the domain of our document (W = 95%). The measures for Arabic and Haitian Creole reflect the fact that their MT lexicons are still under development (W = 64% and 79%, respectively). This measure on the OCR pass, given the corresponding measure on the GT pass as a baseline, captures the degradation introduced by the Scan-OCR processing of the document. From figure 3 we see that the Spanish system loses approximately 55% of its original document words going into the MT engine (95% minus 40%), the Haitian Creole 53% (79% minus 26%), and the Arabic 29% (64% minus 35%). Recall that the Spanish and Haitian Creole systems included the same OCR software, which may account for the similar level of performance here. This software was not available to us for Arabic. X. MT Semantic Adequacy % of TL words generated by MT engine that are open class &amp; semantically adequate in their translation (JIG) This measure is intended to assess whether a system can be used for filtering broad-level topics (in contrast to domains with specialized vocabulary that we discuss below). Here we see evidence for two patterns that recur in the two measures below. First, the GT pass---with one exception---exhibits better performance than the OCR pass. Second, there is a ranking of the systems with Haitian Creole at the low end, Arabic in the middle, and Spanish at the high end. We will need more data to determine the significance of the one exception (55% versus 49%). Y. MT Domain-Relevant Adequacy % of M words generated by MT engine that are open class, semantically adequate in their translation, and domain-relevant (LIG) In all of the systems there was a slight gain in domain-relevant filtering performance from the OCR pass to the GT pass. We can rank the systems with the Haitian Creole at the low end, the Arabic in the middle, and the Spanish at the high end: the measures in both the OCR and GT passes in Haitian Creole are lower than in the Arabic, which are lower than in the Spanish. Only the Spanish documents, but not the Arabic or Haitian Creole ones, when machine translated in either pass were judged domain-relevant by people during an informal Thus, our data suggests that the Spanish system&apos;s lower bound (OCR pass) of 17% on this measure is needed for filtering. Z. MT Open Class Semantic Adequacy of open class generated by MT engine that are semantically adequate in their 14We are the process of running an experiment to validate the protocol for establishing domain-relevant judgments as part of our research in measures of effectiveness (MOEs) for task-based evaluation. 6 The same pattern emerges with this measure. In each system there is an improvement in performance stepping from the OCR pass to the GT pass. Across systems we see the same ranking, with the OCR and GT passes of the Haitian Creole falling below the Arabic which falls below the Spanish. Conclusion and Future Work Our main contribution has been the proposal of an end-to-end process analysis template and a replicable evaluation methodology. We present measures to evaluate filtering performance and preliminary results on Spanish, Arabic and Haitian Creole FALCon systems. The cross-system comparisons using the measures presented, with one exception, yielded the following expected rankings: (i) the GT-MT pass exhibits better performance than the Scan- OCR-MT pass and (ii) the Haitian Creole system is at the low end, Arabic is in the middle, and Spanish is at the high end. Our long-term objective is to compare the of the of performance&amp;quot; (MOPs) presented here with we still need from of effectiveness&amp;quot; MOEbased methods evaluate (i) baseline unaided human performance, (ii) human performance using a new system and (iii) human expert performance. From this comparison we will be able to determine whether these two independently derived sets of measures are replicable and validate each other. So far, we have only addressed our original question, is an embedded MT system enough for filtering?&amp;quot; in terms of MOPs. We found that, for our particular passage in the medical domain, documents need to reach at least 17% on our derived measure Y., MT domain-relevant adequacy (recall discussion of derived measure Y, in section 5). Given that all but one process step (&amp;quot;ID wrong TL words&amp;quot; as shown in figure 1 where a human stick figure appears) in filling the template can be automated, the next phase of this work will be to create a software tool to speed up and systematize this process, improving our system evaluation by increasing the number of 15See Roche and Watts (1991) for definitions of these terms. documents that can be regularly used to test each new system and reducing the burden on the operational linguists who assist us for the one critical step. Currently available tools for parallel text processing, including text alignment software, may provide new user interface options as well, improving the interactive assessment process and possibly extending the input set to include transcribed speech. Acknowledgements We would like to acknowledge Lisa Decrozant (Army Research Laboratory) and Brian Branagan (Department of Defense) for language expertise and Francis Fisher (Army Research Laboratory) for systems engineering expertise.</abstract>
<note confidence="0.8402915">References Church, K. and Hovy, E. 1993. Good Applications Crummy Machine Translation. 8, pages 239 - 258. DARPA 1999. Translingual Information Detection, Extraction, and Summarization (TIDES) Initiative.</note>
<web confidence="0.984684">http://www.darpa.mil/ito/researchitides/index.html</web>
<note confidence="0.879464388888889">Fisher, F. and Voss, C. R. 1997. &amp;quot;FALCon, an MT System Support Tool for Non-linguists.&amp;quot; In Proceedings of the Advanced Information and Analysis Conference. Harmon, D. 1999. &amp;quot;A Framework for Evaluation in TIDES.&amp;quot; Presentation at TIDES Planning Workshop, with link at http://www.dyncorpis.com/darpa/meetings/tides99jultagenda.html, July 28-30, Leesburg, VA. Pomarede, J.-M., Taylor, K., and Van Ess-Dykema, C. 1998. Sparse Training Data and EBMT. In Proceedings of the Workshop on Embedded MT Systems: Design, Construction, and Evaluation of with an MT Component in conjunction with the Association for Machine Translation in the Americas (AMTA&apos;98), Langhorne, PA, October. Resnik, P. 1997. Evaluating Multilingual Gisting of</note>
<title confidence="0.854091">Pages. In Notes of the AAAI Spring Symposium on Natural Language Processing for</title>
<note confidence="0.924803363636364">World Wide Web, Alto, CA. Roche, J. G. and Watts, B. D. 1991. Choosing Measures. Journal of Strategic 14, pages 165-209, June. Jones, K. and Galliers, J. 1996. Language Processing Systems. Springer- Verlag Publishers, Berlin, Germany. Taylor, K. and White, J. 1998. Predicting What MT is Good for: User Judgments and Task In of the Third 7</note>
<title confidence="0.62679">Conference of the Association for Machine in the Americas pages</title>
<address confidence="0.45863">364 -373, Langhorne, PA, October.</address>
<note confidence="0.937433375">Voss, C. R. and Reeder, F. (eds.). Proceedings of the Workshop on Embedded MT Systems: Design, Construction, and Evaluation of with an MT Component in conjunction with the Association for Machine Translation in the Americas (AMTA&apos;98), Langhorne, PA, October. Voss, C. R. and Van Ess-Dykema, C. 2000.</note>
<title confidence="0.631348">Evaluating Scan-OCR-MT Processing for the</title>
<affiliation confidence="0.6991805">Filtering Task. Army Research Laboratory Technical Report, Adelphi, MD.</affiliation>
<intro confidence="0.569725">8</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Church</author>
<author>E Hovy</author>
</authors>
<title>Good Applications for Crummy Machine Translation.</title>
<date>1993</date>
<journal>Machine Translation,</journal>
<volume>8</volume>
<pages>239--258</pages>
<contexts>
<context position="1898" citStr="Church and Hovy (1993)" startWordPosition="275" endWordPosition="278">o, irrelevant&amp;quot; by the system user.3 The singledocument relevance assessment is performed I For a more extensive report of our work, see Voss and Van Ess-Dykema (2000). 2 FALCon (Forward Area Language CONverter) is a laptop-based embedded MT system integrated at the Army Research Laboratory for field use. (Fisher and Voss, 1997) 3 See the report entitled &amp;quot;Multilingual Information Management: Current Levels and Future Abilities&amp;quot; for other defmitions of filtering, available at http://www.cs.cmu.edu/People/ref/mlim/. independent of the content of other documents in the processing collection. When Church and Hovy (1993) introduced the notion that &amp;quot;crummy&amp;quot; MT engines could be put to good use on tasks less-demanding than publication-quality translation, MT research efforts did not typically evaluate system performance in the context of specific tasks. (Sparck Jones and Galliers, 1996). In the last few years, however, the Church and Hovy insight has led to innovative experiments, like those reported by Resnik (1997), Pomarede et al. (1998), and Taylor and White (1998), using task-based evaluation methods. Most recently, research on task-based evaluation has been. proposed within TIDES, a recent DARPA initiative</context>
</contexts>
<marker>Church, Hovy, 1993</marker>
<rawString>Church, K. and Hovy, E. 1993. Good Applications for Crummy Machine Translation. Machine Translation, Volume 8, pages 239 - 258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<title>Translingual Information Detection, Extraction, and Summarization (TIDES) Initiative.</title>
<date>1999</date>
<note>http://www.darpa.mil/ito/researchitides/index.html</note>
<contexts>
<context position="2640" citStr="DARPA, 1999" startWordPosition="384" endWordPosition="385">on, MT research efforts did not typically evaluate system performance in the context of specific tasks. (Sparck Jones and Galliers, 1996). In the last few years, however, the Church and Hovy insight has led to innovative experiments, like those reported by Resnik (1997), Pomarede et al. (1998), and Taylor and White (1998), using task-based evaluation methods. Most recently, research on task-based evaluation has been. proposed within TIDES, a recent DARPA initiative whose goals include enabling Englishspeaking individuals to access, correlate, and interpret multilingual sources of information (DARPA, 1999; Harmon, 1999). This paper introduces a method of assessing when an embedded MT system is &amp;quot;good enough&amp;quot; for the filtering of hard-copy foreign language (FL) documents by individuals with no knowledge of that language. We describe preliminary work developing measures on system-internal components that assess: (i) the flow of words relevant to the filtering task and domain through the steps of document processing in our embedded MT system, and (ii) the level of &amp;quot;noise,&amp;quot; i.e., processing errors, passing through the system. We present an analysis template that displays the processing steps, the s</context>
</contexts>
<marker>DARPA, 1999</marker>
<rawString>DARPA 1999. Translingual Information Detection, Extraction, and Summarization (TIDES) Initiative. http://www.darpa.mil/ito/researchitides/index.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Fisher</author>
<author>C R Voss</author>
</authors>
<title>FALCon, an MT System Support Tool for Non-linguists.&amp;quot;</title>
<date>1997</date>
<booktitle>In Proceedings of the Advanced Information Processing and Analysis Conference. McLean,VA.</booktitle>
<contexts>
<context position="1605" citStr="Fisher and Voss, 1997" startWordPosition="238" endWordPosition="241">pert translator process further. In this paper, we seek to determine when such systems are &amp;quot;good enough&amp;quot; for filtering. We define &amp;quot;filtering&amp;quot; to be a forced-choice decision-making process on individual documents, where each document is assigned a single value, either a &amp;quot;yes, relevant&amp;quot; or a &amp;quot;no, irrelevant&amp;quot; by the system user.3 The singledocument relevance assessment is performed I For a more extensive report of our work, see Voss and Van Ess-Dykema (2000). 2 FALCon (Forward Area Language CONverter) is a laptop-based embedded MT system integrated at the Army Research Laboratory for field use. (Fisher and Voss, 1997) 3 See the report entitled &amp;quot;Multilingual Information Management: Current Levels and Future Abilities&amp;quot; for other defmitions of filtering, available at http://www.cs.cmu.edu/People/ref/mlim/. independent of the content of other documents in the processing collection. When Church and Hovy (1993) introduced the notion that &amp;quot;crummy&amp;quot; MT engines could be put to good use on tasks less-demanding than publication-quality translation, MT research efforts did not typically evaluate system performance in the context of specific tasks. (Sparck Jones and Galliers, 1996). In the last few years, however, the C</context>
</contexts>
<marker>Fisher, Voss, 1997</marker>
<rawString>Fisher, F. and Voss, C. R. 1997. &amp;quot;FALCon, an MT System Support Tool for Non-linguists.&amp;quot; In Proceedings of the Advanced Information Processing and Analysis Conference. McLean,VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harmon</author>
</authors>
<title>A Framework for Evaluation in TIDES.&amp;quot; Presentation at TIDES Planning Workshop, with link at http://www.dyncorpis.com/darpa/meetings/tides99jultagenda.html,</title>
<date>1999</date>
<location>Leesburg, VA.</location>
<contexts>
<context position="2655" citStr="Harmon, 1999" startWordPosition="386" endWordPosition="387">ch efforts did not typically evaluate system performance in the context of specific tasks. (Sparck Jones and Galliers, 1996). In the last few years, however, the Church and Hovy insight has led to innovative experiments, like those reported by Resnik (1997), Pomarede et al. (1998), and Taylor and White (1998), using task-based evaluation methods. Most recently, research on task-based evaluation has been. proposed within TIDES, a recent DARPA initiative whose goals include enabling Englishspeaking individuals to access, correlate, and interpret multilingual sources of information (DARPA, 1999; Harmon, 1999). This paper introduces a method of assessing when an embedded MT system is &amp;quot;good enough&amp;quot; for the filtering of hard-copy foreign language (FL) documents by individuals with no knowledge of that language. We describe preliminary work developing measures on system-internal components that assess: (i) the flow of words relevant to the filtering task and domain through the steps of document processing in our embedded MT system, and (ii) the level of &amp;quot;noise,&amp;quot; i.e., processing errors, passing through the system. We present an analysis template that displays the processing steps, the sequence of docu</context>
</contexts>
<marker>Harmon, 1999</marker>
<rawString>Harmon, D. 1999. &amp;quot;A Framework for Evaluation in TIDES.&amp;quot; Presentation at TIDES Planning Workshop, with link at http://www.dyncorpis.com/darpa/meetings/tides99jultagenda.html, July 28-30, Leesburg, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-M Pomarede</author>
<author>K Taylor</author>
<author>C Van Ess-Dykema</author>
</authors>
<title>Sparse Training Data and EBMT.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Embedded MT Systems: Design, Construction, and Evaluation of Systems with an MT Component held in conjunction with the Association for Machine Translation in the Americas (AMTA&apos;98),</booktitle>
<location>Langhorne, PA,</location>
<marker>Pomarede, Taylor, Van Ess-Dykema, 1998</marker>
<rawString>Pomarede, J.-M., Taylor, K., and Van Ess-Dykema, C. 1998. Sparse Training Data and EBMT. In Proceedings of the Workshop on Embedded MT Systems: Design, Construction, and Evaluation of Systems with an MT Component held in conjunction with the Association for Machine Translation in the Americas (AMTA&apos;98), Langhorne, PA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Evaluating Multilingual Gisting of Web Pages.</title>
<date>1997</date>
<booktitle>In Working Notes of the AAAI Spring Symposium on Natural Language Processing for the World Wide Web,</booktitle>
<location>Palo Alto, CA.</location>
<contexts>
<context position="2299" citStr="Resnik (1997)" startWordPosition="338" endWordPosition="339">nd Future Abilities&amp;quot; for other defmitions of filtering, available at http://www.cs.cmu.edu/People/ref/mlim/. independent of the content of other documents in the processing collection. When Church and Hovy (1993) introduced the notion that &amp;quot;crummy&amp;quot; MT engines could be put to good use on tasks less-demanding than publication-quality translation, MT research efforts did not typically evaluate system performance in the context of specific tasks. (Sparck Jones and Galliers, 1996). In the last few years, however, the Church and Hovy insight has led to innovative experiments, like those reported by Resnik (1997), Pomarede et al. (1998), and Taylor and White (1998), using task-based evaluation methods. Most recently, research on task-based evaluation has been. proposed within TIDES, a recent DARPA initiative whose goals include enabling Englishspeaking individuals to access, correlate, and interpret multilingual sources of information (DARPA, 1999; Harmon, 1999). This paper introduces a method of assessing when an embedded MT system is &amp;quot;good enough&amp;quot; for the filtering of hard-copy foreign language (FL) documents by individuals with no knowledge of that language. We describe preliminary work developing </context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Resnik, P. 1997. Evaluating Multilingual Gisting of Web Pages. In Working Notes of the AAAI Spring Symposium on Natural Language Processing for the World Wide Web, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Roche</author>
<author>B D Watts</author>
</authors>
<title>Choosing Analytic Measures.</title>
<date>1991</date>
<journal>The Journal of Strategic Studies,</journal>
<volume>14</volume>
<pages>165--209</pages>
<contexts>
<context position="23541" citStr="Roche and Watts (1991)" startWordPosition="3939" endWordPosition="3942">ded MT system good enough for filtering?&amp;quot; in terms of MOPs. We found that, for our particular passage in the medical domain, documents need to reach at least 17% on our derived measure Y., MT domain-relevant adequacy (recall discussion of derived measure Y, in section 5). Given that all but one process step (&amp;quot;ID wrong TL words&amp;quot; as shown in figure 1 where a human stick figure appears) in filling the template can be automated, the next phase of this work will be to create a software tool to speed up and systematize this process, improving our system evaluation by increasing the number of 15 See Roche and Watts (1991) for definitions of these terms. documents that can be regularly used to test each new system and reducing the burden on the operational linguists who assist us for the one critical step. Currently available tools for parallel text processing, including text alignment software, may provide new user interface options as well, improving the interactive assessment process and possibly extending the input set to include transcribed speech. Acknowledgements We would like to acknowledge Lisa Decrozant (Army Research Laboratory) and Brian Branagan (Department of Defense) for language expertise and Fr</context>
</contexts>
<marker>Roche, Watts, 1991</marker>
<rawString>Roche, J. G. and Watts, B. D. 1991. Choosing Analytic Measures. The Journal of Strategic Studies, Volume 14, pages 165-209, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>K</author>
<author>J Galliers</author>
</authors>
<title>Evaluating Natural Language Processing Systems.</title>
<date>1996</date>
<publisher>SpringerVerlag Publishers,</publisher>
<location>Berlin, Germany.</location>
<marker>Jones, K, Galliers, 1996</marker>
<rawString>Sparck Jones, K. and Galliers, J. 1996. Evaluating Natural Language Processing Systems. SpringerVerlag Publishers, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Taylor</author>
<author>J White</author>
</authors>
<title>Predicting What MT is Good for: User Judgments and Task Performance.</title>
<date>1998</date>
<booktitle>In Proceedings of the Third Conference of the Association for Machine Translation in the Americas (AMTA&apos;98),</booktitle>
<pages>364--373</pages>
<location>Langhorne, PA,</location>
<contexts>
<context position="2352" citStr="Taylor and White (1998)" startWordPosition="345" endWordPosition="348">f filtering, available at http://www.cs.cmu.edu/People/ref/mlim/. independent of the content of other documents in the processing collection. When Church and Hovy (1993) introduced the notion that &amp;quot;crummy&amp;quot; MT engines could be put to good use on tasks less-demanding than publication-quality translation, MT research efforts did not typically evaluate system performance in the context of specific tasks. (Sparck Jones and Galliers, 1996). In the last few years, however, the Church and Hovy insight has led to innovative experiments, like those reported by Resnik (1997), Pomarede et al. (1998), and Taylor and White (1998), using task-based evaluation methods. Most recently, research on task-based evaluation has been. proposed within TIDES, a recent DARPA initiative whose goals include enabling Englishspeaking individuals to access, correlate, and interpret multilingual sources of information (DARPA, 1999; Harmon, 1999). This paper introduces a method of assessing when an embedded MT system is &amp;quot;good enough&amp;quot; for the filtering of hard-copy foreign language (FL) documents by individuals with no knowledge of that language. We describe preliminary work developing measures on system-internal components that assess: (</context>
</contexts>
<marker>Taylor, White, 1998</marker>
<rawString>Taylor, K. and White, J. 1998. Predicting What MT is Good for: User Judgments and Task Performance. In Proceedings of the Third Conference of the Association for Machine Translation in the Americas (AMTA&apos;98), pages 364 -373, Langhorne, PA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Voss</author>
<author>F Reeder</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of the Workshop on Embedded MT Systems: Design, Construction, and Evaluation of Systems with an MT Component held in conjunction with the Association for Machine Translation in the Americas (AMTA&apos;98),</booktitle>
<location>Langhorne, PA,</location>
<contexts>
<context position="4483" citStr="Voss and Reeder (1998)" startWordPosition="670" endWordPosition="673">5 Second, the optical character recognition (OCR) software converts that image to character text and, third, the machine translation (MT) software converts the foreign language character text to English, where it may be stored to disk or displayed on screen directly to the user. The user interface only requires that the user push one or two buttons to carry out all of the system&apos;s processing on an individual document. We tested three separate language-specific embedded MT systems for Spanish, Arabic and Haitian Creole. These systems differ in their 4 We use &amp;quot;embedded MT systems&amp;quot; as defined in Voss and Reeder (1998). 5 We chose a small scanner for portability of the system. Substituting in a flatbed scanner would riot affect performance. OCR and MT components, but otherwise they share the same software, Omnipage&apos;s Paperport for scaning and Windows95 as the operating system.6 3 Approach As we sought to measure the performance of each component in the systems, it quickly became apparent that not all available measures may be equally applicable for our filtering task. For example, counting the number of source language (SL) characters correctly OCR-ed may be overly specific: as discussed below, we only need</context>
</contexts>
<marker>Voss, Reeder, 1998</marker>
<rawString>Voss, C. R. and Reeder, F. (eds.). 1998. Proceedings of the Workshop on Embedded MT Systems: Design, Construction, and Evaluation of Systems with an MT Component held in conjunction with the Association for Machine Translation in the Americas (AMTA&apos;98), Langhorne, PA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Voss</author>
<author>C Van Ess-Dykema</author>
</authors>
<title>Evaluating Scan-OCR-MT Processing for the Filtering Task. Army Research Laboratory</title>
<date>2000</date>
<tech>Technical Report,</tech>
<location>Adelphi, MD.</location>
<marker>Voss, Van Ess-Dykema, 2000</marker>
<rawString>Voss, C. R. and Van Ess-Dykema, C. 2000. Evaluating Scan-OCR-MT Processing for the Filtering Task. Army Research Laboratory Technical Report, Adelphi, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>