<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012371">
<title confidence="0.993605">
Reducing Annotation Effort for Quality Estimation via Active Learning
</title>
<author confidence="0.997722">
Daniel Beck and Lucia Specia and Trevor Cohn
</author>
<affiliation confidence="0.888852666666667">
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
</affiliation>
<email confidence="0.998699">
{debeck1,l.specia,t.cohn}@sheffield.ac.uk
</email>
<sectionHeader confidence="0.9948" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998301875">
Quality estimation models provide feed-
back on the quality of machine translated
texts. They are usually trained on human-
annotated datasets, which are very costly
due to its task-specific nature. We in-
vestigate active learning techniques to re-
duce the size of these datasets and thus
annotation effort. Experiments on a num-
ber of datasets show that with as little as
25% of the training instances it is possible
to obtain similar or superior performance
compared to that of the complete datasets.
In other words, our active learning query
strategies can not only reduce annotation
effort but can also result in better quality
predictors.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978243902439">
The purpose of machine translation (MT) qual-
ity estimation (QE) is to provide a quality pre-
diction for new, unseen machine translated texts,
without relying on reference translations (Blatz et
al., 2004; Specia et al., 2009; Callison-Burch et
al., 2012). This task is usually addressed with
machine learning models trained on datasets com-
posed of source sentences, their machine transla-
tions, and a quality label assigned by humans. A
common use of quality predictions is the decision
between post-editing a given machine translated
sentence and translating its source from scratch,
based on whether its post-editing effort is esti-
mated to be lower than the effort of translating the
source sentence.
Since quality scores for the training of QE mod-
els are given by human experts, the annotation pro-
cess is costly and subject to inconsistencies due to
the subjectivity of the task. To avoid inconsisten-
cies because of disagreements among annotators,
it is often recommended that a QE model is trained
for each translator, based on labels given by such
a translator (Specia, 2011). This further increases
the annotation costs because different datasets are
needed for different tasks. Therefore, strategies to
reduce the demand for annotated data are needed.
Such strategies can also bring the possibility of se-
lecting data that is less prone to inconsistent anno-
tations, resulting in more robust and accurate pre-
dictions.
In this paper we investigate Active Learning
(AL) techniques to reduce the size of the dataset
while keeping the performance of the resulting
QE models. AL provides methods to select in-
formative data points from a large pool which,
if labelled, can potentially improve the perfor-
mance of a machine learning algorithm (Settles,
2010). The rationale behind these methods is to
help the learning algorithm achieve satisfactory re-
sults from only on a subset of the available data,
thus incurring less annotation effort.
</bodyText>
<sectionHeader confidence="0.999731" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999852157894737">
Most research work on QE for machine transla-
tion is focused on feature engineering and feature
selection, with some recent work on devising more
reliable and less subjective quality labels. Blatz et
al. (2004) present the first comprehensive study on
QE for MT: 91 features were proposed and used
to train predictors based on an automatic metric
(e.g. NIST (Doddington, 2002)) as the quality la-
bel. Quirk (2004) showed that small datasets man-
ually annotated by humans for quality can result
in models that outperform those trained on much
larger, automatically labelled sets.
Since quality labels are subjective to the anno-
tators’ judgements, Specia and Farzindar (2010)
evaluated the performance of QE models using
HTER (Snover et al., 2006) as the quality score,
i.e., the edit distance between the MT output and
its post-edited version. Specia (2011) compared
the performance of models based on labels for
</bodyText>
<page confidence="0.98121">
543
</page>
<bodyText confidence="0.9294451">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543–548,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
post-editing effort, post-editing time, and HTER.
In terms of learning algorithms, by and large
most approaches use Support Vector Machines,
particularly regression-based approaches. For an
overview on various feature sets and machine
learning algorithms, we refer the reader to a re-
cent shared task on the topic (Callison-Burch et
al., 2012).
Previous work use supervised learning methods
(“passive learning” following the AL terminol-
ogy) to train QE models. On the other hand, AL
has been successfully used in a number of natural
language applications such as text classification
(Lewis and Gale, 1994), named entity recognition
(Vlachos, 2006) and parsing (Baldridge and Os-
borne, 2004). See Olsson (2009) for an overview
on AL for natural language processing as well as
a comprehensive list of previous work.
</bodyText>
<sectionHeader confidence="0.994551" genericHeader="method">
3 Experimental Settings
</sectionHeader>
<subsectionHeader confidence="0.959109">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999050928571429">
We perform experiments using four MT datasets
manually annotated for quality:
English-Spanish (en-es): 2, 254 sentences
translated by Moses (Koehn et al., 2007), as pro-
vided by the WMT12 Quality Estimation shared
task (Callison-Burch et al., 2012). Effort scores
range from 1 (too bad to be post-edited) to 5 (no
post-editing needed). Three expert post-editors
evaluated each sentence and the final score was
obtained by a weighted average between the three
scores. We use the default split given in the shared
task: 1, 832 sentences for training and 432 for
test.
French-English (fr-en): 2,525 sentences trans-
lated by Moses as provided in Specia (2011), an-
notated by a single translator. Human labels in-
dicate post-editing effort ranging from 1 (too bad
to be post-edited) to 4 (little or no post-editing
needed). We use a random split of 90% sentences
for training and 10% for test.
Arabic-English (ar-en): 2, 585 sentences trans-
lated by two state-of-the-art SMT systems (de-
noted ar-en-1 and ar-en-2), as provided in (Specia
et al., 2011). A random split of 90% sentences for
training and 10% for test is used. Human labels in-
dicate the adequacy of the translation ranging from
1 (completely inadequate) to 4 (adequate). These
datasets were annotated by two expert translators.
</bodyText>
<subsectionHeader confidence="0.983524">
3.2 Query Methods
</subsectionHeader>
<bodyText confidence="0.999975388888889">
The core of an AL setting is how the learner will
gather new instances to add to its training data. In
our setting, we use a pool-based strategy, where
the learner queries an instance pool and selects
the best instance according to an informativeness
measure. The learner then asks an “oracle” (in this
case, the human expert) for the true label of the in-
stance and adds it to the training data.
Query methods use different criteria to predict
how informative an instance is. We experiment
with two of them: Uncertainty Sampling (US)
(Lewis and Gale, 1994) and Information Density
(ID) (Settles and Craven, 2008). In the following,
we denote M(x) the query score with respect to
method M.
According to the US method, the learner selects
the instance that has the highest labelling variance
according to its model:
</bodyText>
<equation confidence="0.998424">
US(x) = V ar(y|x)
</equation>
<bodyText confidence="0.99954425">
The ID method considers that more dense regions
of the query space bring more useful information,
leveraging the instance uncertainty and its similar-
ity to all the other instances in the pool:
</bodyText>
<equation confidence="0.996772">
1 �U
ID(x) = V ar(y|x) × U
u=1
</equation>
<bodyText confidence="0.99984275">
The β parameter controls the relative importance
of the density term. In our experiments, we set it
to 1, giving equal weights to variance and density.
The U term is the number of instances in the query
pool. As similarity measure sim(x, x(u)), we use
the cosine distance between the feature vectors.
With each method, we choose the instance that
maximises its respective equation.
</bodyText>
<subsectionHeader confidence="0.985069">
3.3 Experiments
</subsectionHeader>
<bodyText confidence="0.999972333333333">
To build our QE models, we extracted the 17 fea-
tures used by the baseline approach in the WMT12
QE shared task.1 These features were used with a
Support Vector Regressor (SVR) with radial basis
function and fixed hyperparameters (C=5, γ=0.01,
c=0.5), using the Scikit-learn toolkit (Pedregosa
et al., 2011). For each dataset and each query
method, we performed 20 active learning simu-
lation experiments and averaged the results. We
</bodyText>
<footnote confidence="0.9784035">
1We refer the reader to (Callison-Burch et al., 2012) for
a detailed description of the feature set, but this was a very
strong baseline, with only five out of 19 participating systems
outperforming it.
</footnote>
<equation confidence="0.566121">
Q
sim(x, x(u))
</equation>
<page confidence="0.979842">
544
</page>
<bodyText confidence="0.999920954545454">
started with 50 randomly selected sentences from
the training set and used all the remaining train-
ing sentences as our query pool, adding one new
sentence to the training set at each iteration.
Results were evaluated by measuring Mean Ab-
solute Error (MAE) scores on the test set. We
also performed an “oracle” experiment: at each it-
eration, it selects the instance that minimises the
MAE on the test set. The oracle results give an
upper bound in performance for each test set.
Since an SVR does not supply variance values
for its predictions, we employ a technique known
as query-by-bagging (Abe and Mamitsuka, 1998).
The idea is to build an ensemble of N SVRs
trained on sub-samples of the training data. When
selecting a new query, the ensemble is able to re-
turn N predictions for each instance, from where a
variance value can be inferred. We used 20 SVRs
as our ensemble and 20 as the size of each training
sub-sample.2 The variance values are then used
as-is in the case of US strategy and combined with
query densities in case of the ID strategy.
</bodyText>
<sectionHeader confidence="0.999495" genericHeader="method">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.99738952173913">
Figure 1 shows the learning curves for all query
methods and all datasets. The “random” curves
are our baseline since they are equivalent to pas-
sive learning (with various numbers of instances).
We first evaluated our methods in terms of how
many instances they needed to achieve 99% of the
MAE score on the full dataset. For three datasets,
the AL methods significantly outperformed the
random selection baseline, while no improvement
was observed on the ar-en-1 dataset. Results are
summarised in Table 1.
The learning curves in Figure 1 show an inter-
esting behaviour for most AL methods: some of
them were able to yield lower MAE scores than
models trained on the full dataset. This is par-
ticularly interesting in the fr-en case, where both
methods were able to obtain better scores using
only ∼25% of the available instances, with the
US method resulting in 0.03 improvement. The
random selection strategy performs surprisingly
well (for some datasets it is better than the AL
strategies with certain number of instances), pro-
viding extra evidence that much smaller annotated
</bodyText>
<footnote confidence="0.99233175">
2We also tried sub-samples with the same size of the cur-
rent training data but this had a large impact in the query
methods running time while not yielding significantly better
results.
</footnote>
<figureCaption confidence="0.993294">
Figure 1: Learning curves for different query se-
</figureCaption>
<bodyText confidence="0.960377333333333">
lection strategies in the four datasets. The horizon-
tal axis shows the number of instances in the train-
ing set and the vertical axis shows MAE scores.
</bodyText>
<page confidence="0.996315">
545
</page>
<table confidence="0.996787166666667">
US ID Random Full dataset
#instances MAE #instances MAE #instances MAE
en-es 959 (52%) 0.6818 549 (30%) 0.6816 1079 (59%) 0.6818 0.6750
fr-en 79 (3%) 0.5072 134 (6%) 0.5077 325 (14%) 0.5070 0.5027
ar-en-1 51 (2%) 0.6067 51 (2%) 0.6052 51 (2%) 0.6061 0.6058
ar-en-2 209 (9%) 0.6288 148 (6%) 0.6289 532 (23%) 0.6288 0.6290
</table>
<tableCaption confidence="0.9875035">
Table 1: Number (proportion) of instances needed to achieve 99% of the performance of the full dataset.
Bold-faced values indicate the best performing datasets.
</tableCaption>
<table confidence="0.999161666666667">
Best MAE US Best MAE ID Full dataset
#instances MAE US MAE Random #instances MAE ID MAE Random
en-es 1832 (100%) 0.6750 0.6750 1122 (61%) 0.6722 0.6807 0.6750
fr-en 559 (25%) 0.4708 0.5010 582 (26%) 0.4843 0.5008 0.5027
ar-en-1 610 (26%) 0.5956 0.6042 351 (15%) 0.5987 0.6102 0.6058
ar-en-2 1782 (77%) 0.6212 0.6242 190 (8%) 0.6170 0.6357 0.6227
</table>
<tableCaption confidence="0.978286">
Table 2: Best MAE scores obtained in the AL experiments. For each method, the first column shows the
</tableCaption>
<bodyText confidence="0.995933148148148">
number (proportion) of instances used to obtain the best MAE, the second column shows the MAE score
obtained and the third column shows the MAE score for random instance selection at the same number
of instances. The last column shows the MAE obtained using the full dataset. Best scores are shown in
bold and are significantly better (paired t-test, p &lt; 0.05) than both their randomly selected counterparts
and the full dataset MAE.
datasets than those used currently can be sufficient
for machine translation QE.
The best MAE scores achieved for each dataset
are shown in Table 2. The figures were tested for
significance using pairwise t-test with 95% confi-
dence,3 with bold-faced values in the table indicat-
ing significantly better results.
The lower bounds in MAE given by the ora-
cle curves show that AL methods can indeed im-
prove the performance of QE models: an ideal
query method would achieve a very large improve-
ment in MAE using fewer than 200 instances in all
datasets. The fact that different datasets present
similar oracle curves suggests that this is not re-
lated for a specific dataset but actually a common
behaviour in QE. Although some of this gain in
MAE may be due to overfitting to the test set, the
results obtained with the fr-en and ar-en-2 datasets
are very promising, and therefore we believe that
it is possible to use AL to improve QE results in
other cases, as long as more effective query tech-
niques are designed.
</bodyText>
<sectionHeader confidence="0.9800305" genericHeader="method">
5 Further analysis on the oracle
behaviour
</sectionHeader>
<bodyText confidence="0.991199837837838">
By analysing the oracle curves we can observe an-
other interesting phenomenon which is the rapid
increase in error when reaching the last ∼200 in-
stances of the training data. A possible explana-
3We took the average of the MAE scores obtained from
the 20 runs with each query method for that.
tion for this behaviour is the existence of erro-
neous, inconsistent or contradictory labels in the
datasets. Quality annotation is a subjective task by
nature, and it is thus subject to noise, e.g., due to
misinterpretations or disagreements. Our hypothe-
sis is that these last sentences are the most difficult
to annotate and therefore more prone to disagree-
ments.
To investigate this phenomenon, we performed
an additional experiment with the en-es dataset,
the only dataset for which multiple annotations
are available (from three judges). We measure the
Kappa agreement index (Cohen, 1960) between all
pairs of judges in the subset containing the first
300 instances (the 50 initial random instances plus
250 instances chosen by the oracle). We then mea-
sured Kappa in windows of 300 instances until the
last instance of the training set is selected by the
oracle method. We also measure variances in sen-
tence length using windows of 300 instances. The
idea of this experiment is to test whether sentences
that are more difficult to annotate (because of their
length or subjectivity, generating more disagree-
ment between the judges) add noise to the dataset.
The resulting Kappa curves are shown in Fig-
ure 2: the agreement between judges is high for
the initial set of sentences selected, tends to de-
crease until it reaches ∼1000 instances, and then
starts to increase again. Figure 3 shows the results
for source sentence length, which follow the same
trend (in a reversed manner). Contrary to our hy-
</bodyText>
<page confidence="0.997259">
546
</page>
<figureCaption confidence="0.749192">
Figure 2: Kappa curves for the en-es dataset. The
</figureCaption>
<bodyText confidence="0.99859252">
horizontal axis shows the number of instances and
the vertical axis shows the kappa values. Each
point in the curves shows the kappa index for a
window containing the last 300 sentences chosen
by the oracle.
pothesis, these results suggest that the most diffi-
cult sentences chosen by the oracle are those in the
middle range instead of the last ones. If we com-
pare this trend against the oracle curve in Figure 1,
we can see that those middle instances are the ones
that do not change the performance of the oracle.
The resulting trends are interesting because they
give evidence that sentences that are difficult to an-
notate do not contribute much to QE performance
(although not hurting it either). However, they do
not confirm our hypothesis about the oracle be-
haviour. Another possible source of disagreement
is the feature set: the features may not be discrim-
inative enough to distinguish among different in-
stances, i.e., instances with very similar features
but different labels might be genuinely different,
but the current features are not sufficient to indi-
cate that. In future work we plan to further inves-
tigate this by hypothesis by using other feature sets
and analysing their behaviour.
</bodyText>
<sectionHeader confidence="0.991389" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.996252464285714">
We have presented the first known experiments us-
ing active learning for the task of estimating ma-
chine translation quality. The results are promis-
ing: we were able to reduce the number of in-
stances needed to train the models in three of the
four datasets. In addition, in some of the datasets
active learning yielded significantly better models
using only a small subset of the training instances.
Figure 3: Average source and target sentence
lengths for the en-es dataset. The horizontal axis
shows the number of instances and the vertical
axis shows the length values. Each point in the
curves shows the average length for a window con-
taining the last 300 sentences chosen by the oracle.
The oracle results give evidence that it is possi-
ble to go beyond these encouraging results by em-
ploying better selection strategies in active learn-
ing. In future work we will investigate more
advanced query techniques that consider features
other than variance and density of the data points.
We also plan to further investigate the behaviour
of the oracle curves using not only different fea-
ture sets but also different quality scores such as
HTER and post-editing time. We believe that a
better understanding of this behaviour can guide
further developments not only for instance selec-
tion techniques but also for the design of better
quality features and quality annotation schemes.
</bodyText>
<sectionHeader confidence="0.996981" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999336">
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9, Daniel Beck)
and from the EU FP7-ICT QTLaunchPad project
(No. 296347, Lucia Specia).
</bodyText>
<sectionHeader confidence="0.985107" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.530535">
Naoki Abe and Hiroshi Mamitsuka. 1998. Query
learning strategies using boosting and bagging. In
Proceedings of the Fifteenth International Confer-
ence on Machine Learning, pages 1–9.
Jason Baldridge and Miles Osborne. 2004. Active
learning and the total cost of annotation. In Pro-
ceedings of EMNLP, pages 9–16.
</bodyText>
<page confidence="0.992822">
547
</page>
<reference confidence="0.996431413333333">
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315–321.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of 7th Workshop
on Statistical Machine Translation.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and psychological
measurement, 20(1):37–46.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, pages 128–132.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180.
David D. Lewis and Willian A. Gale. 1994. A sequen-
tial algorithm for training text classifiers. In Pro-
ceedings of the ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 1–
10.
Fredrik Olsson. 2009. A literature survey of active
machine learning in the context of natural language
processing. Technical report.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Duborg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ´Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.
Chris Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of
LREC, pages 825–828.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1070–1079.
Burr Settles. 2010. Active learning literature survey.
Technical report.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Lucia Specia and Atefeh Farzindar. 2010. Estimating
machine translation post-editing effort with HTER.
In Proceedings of AMTA Workshop Bringing MT to
the User: MT Research and the Translation Indus-
try.
Lucia Specia, M Turchi, Zhuoran Wang, and J Shawe-
Taylor. 2009. Improving the confidence of machine
translation quality estimates. In Proceedings of MT
Summit XII.
Lucia Specia, Najeh Hajlaoui, Catalina Hallett, and
Wilker Aziz. 2011. Predicting machine translation
adequacy. In Proceedings of MT Summit XIII.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of EAMT.
Andreas Vlachos. 2006. Active annotation. In Pro-
ceedings of the Workshop on Adaptive Text Extrac-
tion and Mining at EACL.
</reference>
<page confidence="0.996224">
548
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.402332">
<title confidence="0.999968">Reducing Annotation Effort for Quality Estimation via Active Learning</title>
<author confidence="0.999862">Beck Specia</author>
<affiliation confidence="0.9931795">Department of Computer University of</affiliation>
<address confidence="0.433928">Sheffield, United</address>
<abstract confidence="0.995667294117647">Quality estimation models provide feedback on the quality of machine translated texts. They are usually trained on humanannotated datasets, which are very costly due to its task-specific nature. We investigate active learning techniques to reduce the size of these datasets and thus annotation effort. Experiments on a number of datasets show that with as little as 25% of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets. In other words, our active learning query strategies can not only reduce annotation effort but can also result in better quality predictors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th Conference on Computational Linguistics,</booktitle>
<pages>315--321</pages>
<contexts>
<context position="1101" citStr="Blatz et al., 2004" startWordPosition="163" endWordPosition="166"> to reduce the size of these datasets and thus annotation effort. Experiments on a number of datasets show that with as little as 25% of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets. In other words, our active learning query strategies can not only reduce annotation effort but can also result in better quality predictors. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Callison-Burch et al., 2012). This task is usually addressed with machine learning models trained on datasets composed of source sentences, their machine translations, and a quality label assigned by humans. A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. Since quality scores for the training of QE models are given by human experts, the annotation process</context>
<context position="3060" citStr="Blatz et al. (2004)" startWordPosition="477" endWordPosition="480">nce of the resulting QE models. AL provides methods to select informative data points from a large pool which, if labelled, can potentially improve the performance of a machine learning algorithm (Settles, 2010). The rationale behind these methods is to help the learning algorithm achieve satisfactory results from only on a subset of the available data, thus incurring less annotation effort. 2 Related Work Most research work on QE for machine translation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels. Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT outpu</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, and George Foster. 2004. Confidence estimation for machine translation. In Proceedings of the 20th Conference on Computational Linguistics, pages 315–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of 7th Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1152" citStr="Callison-Burch et al., 2012" startWordPosition="171" endWordPosition="174"> thus annotation effort. Experiments on a number of datasets show that with as little as 25% of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets. In other words, our active learning query strategies can not only reduce annotation effort but can also result in better quality predictors. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Callison-Burch et al., 2012). This task is usually addressed with machine learning models trained on datasets composed of source sentences, their machine translations, and a quality label assigned by humans. A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. Since quality scores for the training of QE models are given by human experts, the annotation process is costly and subject to inconsistencies due to th</context>
<context position="4293" citStr="Callison-Burch et al., 2012" startWordPosition="665" endWordPosition="668"> its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543–548, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics post-editing effort, post-editing time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-based approaches. For an overview on various feature sets and machine learning algorithms, we refer the reader to a recent shared task on the topic (Callison-Burch et al., 2012). Previous work use supervised learning methods (“passive learning” following the AL terminology) to train QE models. On the other hand, AL has been successfully used in a number of natural language applications such as text classification (Lewis and Gale, 1994), named entity recognition (Vlachos, 2006) and parsing (Baldridge and Osborne, 2004). See Olsson (2009) for an overview on AL for natural language processing as well as a comprehensive list of previous work. 3 Experimental Settings 3.1 Datasets We perform experiments using four MT datasets manually annotated for quality: English-Spanish</context>
<context position="8037" citStr="Callison-Burch et al., 2012" startWordPosition="1284" endWordPosition="1287"> cosine distance between the feature vectors. With each method, we choose the instance that maximises its respective equation. 3.3 Experiments To build our QE models, we extracted the 17 features used by the baseline approach in the WMT12 QE shared task.1 These features were used with a Support Vector Regressor (SVR) with radial basis function and fixed hyperparameters (C=5, γ=0.01, c=0.5), using the Scikit-learn toolkit (Pedregosa et al., 2011). For each dataset and each query method, we performed 20 active learning simulation experiments and averaged the results. We 1We refer the reader to (Callison-Burch et al., 2012) for a detailed description of the feature set, but this was a very strong baseline, with only five out of 19 participating systems outperforming it. Q sim(x, x(u)) 544 started with 50 randomly selected sentences from the training set and used all the remaining training sentences as our query pool, adding one new sentence to the training set at each iteration. Results were evaluated by measuring Mean Absolute Error (MAE) scores on the test set. We also performed an “oracle” experiment: at each iteration, it selects the instance that minimises the MAE on the test set. The oracle results give an</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of 7th Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and psychological measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="14053" citStr="Cohen, 1960" startWordPosition="2308" endWordPosition="2309">ry method for that. tion for this behaviour is the existence of erroneous, inconsistent or contradictory labels in the datasets. Quality annotation is a subjective task by nature, and it is thus subject to noise, e.g., due to misinterpretations or disagreements. Our hypothesis is that these last sentences are the most difficult to annotate and therefore more prone to disagreements. To investigate this phenomenon, we performed an additional experiment with the en-es dataset, the only dataset for which multiple annotations are available (from three judges). We measure the Kappa agreement index (Cohen, 1960) between all pairs of judges in the subset containing the first 300 instances (the 50 initial random instances plus 250 instances chosen by the oracle). We then measured Kappa in windows of 300 instances until the last instance of the training set is selected by the oracle method. We also measure variances in sentence length using windows of 300 instances. The idea of this experiment is to test whether sentences that are more difficult to annotate (because of their length or subjectivity, generating more disagreement between the judges) add noise to the dataset. The resulting Kappa curves are </context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology Research,</booktitle>
<pages>128--132</pages>
<contexts>
<context position="3226" citStr="Doddington, 2002" startWordPosition="506" endWordPosition="507"> machine learning algorithm (Settles, 2010). The rationale behind these methods is to help the learning algorithm achieve satisfactory results from only on a subset of the available data, thus incurring less annotation effort. 2 Related Work Most research work on QE for machine translation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels. Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for </context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the Second International Conference on Human Language Technology Research, pages 128–132.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="4960" citStr="Koehn et al., 2007" startWordPosition="767" endWordPosition="770">(“passive learning” following the AL terminology) to train QE models. On the other hand, AL has been successfully used in a number of natural language applications such as text classification (Lewis and Gale, 1994), named entity recognition (Vlachos, 2006) and parsing (Baldridge and Osborne, 2004). See Olsson (2009) for an overview on AL for natural language processing as well as a comprehensive list of previous work. 3 Experimental Settings 3.1 Datasets We perform experiments using four MT datasets manually annotated for quality: English-Spanish (en-es): 2, 254 sentences translated by Moses (Koehn et al., 2007), as provided by the WMT12 Quality Estimation shared task (Callison-Burch et al., 2012). Effort scores range from 1 (too bad to be post-edited) to 5 (no post-editing needed). Three expert post-editors evaluated each sentence and the final score was obtained by a weighted average between the three scores. We use the default split given in the shared task: 1, 832 sentences for training and 432 for test. French-English (fr-en): 2,525 sentences translated by Moses as provided in Specia (2011), annotated by a single translator. Human labels indicate post-editing effort ranging from 1 (too bad to be</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Willian A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="4555" citStr="Lewis and Gale, 1994" startWordPosition="706" endWordPosition="709">onal Linguistics post-editing effort, post-editing time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-based approaches. For an overview on various feature sets and machine learning algorithms, we refer the reader to a recent shared task on the topic (Callison-Burch et al., 2012). Previous work use supervised learning methods (“passive learning” following the AL terminology) to train QE models. On the other hand, AL has been successfully used in a number of natural language applications such as text classification (Lewis and Gale, 1994), named entity recognition (Vlachos, 2006) and parsing (Baldridge and Osborne, 2004). See Olsson (2009) for an overview on AL for natural language processing as well as a comprehensive list of previous work. 3 Experimental Settings 3.1 Datasets We perform experiments using four MT datasets manually annotated for quality: English-Spanish (en-es): 2, 254 sentences translated by Moses (Koehn et al., 2007), as provided by the WMT12 Quality Estimation shared task (Callison-Burch et al., 2012). Effort scores range from 1 (too bad to be post-edited) to 5 (no post-editing needed). Three expert post-ed</context>
<context position="6654" citStr="Lewis and Gale, 1994" startWordPosition="1052" endWordPosition="1055">sets were annotated by two expert translators. 3.2 Query Methods The core of an AL setting is how the learner will gather new instances to add to its training data. In our setting, we use a pool-based strategy, where the learner queries an instance pool and selects the best instance according to an informativeness measure. The learner then asks an “oracle” (in this case, the human expert) for the true label of the instance and adds it to the training data. Query methods use different criteria to predict how informative an instance is. We experiment with two of them: Uncertainty Sampling (US) (Lewis and Gale, 1994) and Information Density (ID) (Settles and Craven, 2008). In the following, we denote M(x) the query score with respect to method M. According to the US method, the learner selects the instance that has the highest labelling variance according to its model: US(x) = V ar(y|x) The ID method considers that more dense regions of the query space bring more useful information, leveraging the instance uncertainty and its similarity to all the other instances in the pool: 1 �U ID(x) = V ar(y|x) × U u=1 The β parameter controls the relative importance of the density term. In our experiments, we set it </context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and Willian A. Gale. 1994. A sequential algorithm for training text classifiers. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1– 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fredrik Olsson</author>
</authors>
<title>A literature survey of active machine learning in the context of natural language processing.</title>
<date>2009</date>
<tech>Technical report.</tech>
<contexts>
<context position="4658" citStr="Olsson (2009)" startWordPosition="723" endWordPosition="724">e most approaches use Support Vector Machines, particularly regression-based approaches. For an overview on various feature sets and machine learning algorithms, we refer the reader to a recent shared task on the topic (Callison-Burch et al., 2012). Previous work use supervised learning methods (“passive learning” following the AL terminology) to train QE models. On the other hand, AL has been successfully used in a number of natural language applications such as text classification (Lewis and Gale, 1994), named entity recognition (Vlachos, 2006) and parsing (Baldridge and Osborne, 2004). See Olsson (2009) for an overview on AL for natural language processing as well as a comprehensive list of previous work. 3 Experimental Settings 3.1 Datasets We perform experiments using four MT datasets manually annotated for quality: English-Spanish (en-es): 2, 254 sentences translated by Moses (Koehn et al., 2007), as provided by the WMT12 Quality Estimation shared task (Callison-Burch et al., 2012). Effort scores range from 1 (too bad to be post-edited) to 5 (no post-editing needed). Three expert post-editors evaluated each sentence and the final score was obtained by a weighted average between the three </context>
</contexts>
<marker>Olsson, 2009</marker>
<rawString>Fredrik Olsson. 2009. A literature survey of active machine learning in the context of natural language processing. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Duborg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Duborg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
</authors>
<title>Training a sentence-level machine translation confidence measure.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>825--828</pages>
<contexts>
<context position="3262" citStr="Quirk (2004)" startWordPosition="513" endWordPosition="514">0). The rationale behind these methods is to help the learning algorithm achieve satisfactory results from only on a subset of the available data, thus incurring less annotation effort. 2 Related Work Most research work on QE for machine translation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels. Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543</context>
</contexts>
<marker>Quirk, 2004</marker>
<rawString>Chris Quirk. 2004. Training a sentence-level machine translation confidence measure. In Proceedings of LREC, pages 825–828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
<author>Mark Craven</author>
</authors>
<title>An analysis of active learning strategies for sequence labeling tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1070--1079</pages>
<contexts>
<context position="6710" citStr="Settles and Craven, 2008" startWordPosition="1060" endWordPosition="1063">uery Methods The core of an AL setting is how the learner will gather new instances to add to its training data. In our setting, we use a pool-based strategy, where the learner queries an instance pool and selects the best instance according to an informativeness measure. The learner then asks an “oracle” (in this case, the human expert) for the true label of the instance and adds it to the training data. Query methods use different criteria to predict how informative an instance is. We experiment with two of them: Uncertainty Sampling (US) (Lewis and Gale, 1994) and Information Density (ID) (Settles and Craven, 2008). In the following, we denote M(x) the query score with respect to method M. According to the US method, the learner selects the instance that has the highest labelling variance according to its model: US(x) = V ar(y|x) The ID method considers that more dense regions of the query space bring more useful information, leveraging the instance uncertainty and its similarity to all the other instances in the pool: 1 �U ID(x) = V ar(y|x) × U u=1 The β parameter controls the relative importance of the density term. In our experiments, we set it to 1, giving equal weights to variance and density. The </context>
</contexts>
<marker>Settles, Craven, 2008</marker>
<rawString>Burr Settles and Mark Craven. 2008. An analysis of active learning strategies for sequence labeling tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1070–1079.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Active learning literature survey.</title>
<date>2010</date>
<tech>Technical report.</tech>
<contexts>
<context position="2652" citStr="Settles, 2010" startWordPosition="413" endWordPosition="414">nt datasets are needed for different tasks. Therefore, strategies to reduce the demand for annotated data are needed. Such strategies can also bring the possibility of selecting data that is less prone to inconsistent annotations, resulting in more robust and accurate predictions. In this paper we investigate Active Learning (AL) techniques to reduce the size of the dataset while keeping the performance of the resulting QE models. AL provides methods to select informative data points from a large pool which, if labelled, can potentially improve the performance of a machine learning algorithm (Settles, 2010). The rationale behind these methods is to help the learning algorithm achieve satisfactory results from only on a subset of the available data, thus incurring less annotation effort. 2 Related Work Most research work on QE for machine translation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels. Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Qu</context>
</contexts>
<marker>Settles, 2010</marker>
<rawString>Burr Settles. 2010. Active learning literature survey. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="3593" citStr="Snover et al., 2006" startWordPosition="562" endWordPosition="565"> work on devising more reliable and less subjective quality labels. Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543–548, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics post-editing effort, post-editing time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-based approaches. For an overview on various feature sets and machine learnin</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Atefeh Farzindar</author>
</authors>
<title>Estimating machine translation post-editing effort with HTER.</title>
<date>2010</date>
<booktitle>In Proceedings of AMTA Workshop Bringing MT to the User: MT Research and the Translation Industry.</booktitle>
<contexts>
<context position="3521" citStr="Specia and Farzindar (2010)" startWordPosition="550" endWordPosition="553">ation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels. Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543–548, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics post-editing effort, post-editing time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-based</context>
</contexts>
<marker>Specia, Farzindar, 2010</marker>
<rawString>Lucia Specia and Atefeh Farzindar. 2010. Estimating machine translation post-editing effort with HTER. In Proceedings of AMTA Workshop Bringing MT to the User: MT Research and the Translation Industry.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>M Turchi</author>
<author>Zhuoran Wang</author>
<author>J ShaweTaylor</author>
</authors>
<title>Improving the confidence of machine translation quality estimates.</title>
<date>2009</date>
<booktitle>In Proceedings of MT Summit XII.</booktitle>
<contexts>
<context position="1122" citStr="Specia et al., 2009" startWordPosition="167" endWordPosition="170">of these datasets and thus annotation effort. Experiments on a number of datasets show that with as little as 25% of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets. In other words, our active learning query strategies can not only reduce annotation effort but can also result in better quality predictors. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Callison-Burch et al., 2012). This task is usually addressed with machine learning models trained on datasets composed of source sentences, their machine translations, and a quality label assigned by humans. A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. Since quality scores for the training of QE models are given by human experts, the annotation process is costly and subjec</context>
</contexts>
<marker>Specia, Turchi, Wang, ShaweTaylor, 2009</marker>
<rawString>Lucia Specia, M Turchi, Zhuoran Wang, and J ShaweTaylor. 2009. Improving the confidence of machine translation quality estimates. In Proceedings of MT Summit XII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Najeh Hajlaoui</author>
<author>Catalina Hallett</author>
<author>Wilker Aziz</author>
</authors>
<title>Predicting machine translation adequacy.</title>
<date>2011</date>
<booktitle>In Proceedings of MT Summit XIII.</booktitle>
<contexts>
<context position="5840" citStr="Specia et al., 2011" startWordPosition="913" endWordPosition="916"> a weighted average between the three scores. We use the default split given in the shared task: 1, 832 sentences for training and 432 for test. French-English (fr-en): 2,525 sentences translated by Moses as provided in Specia (2011), annotated by a single translator. Human labels indicate post-editing effort ranging from 1 (too bad to be post-edited) to 4 (little or no post-editing needed). We use a random split of 90% sentences for training and 10% for test. Arabic-English (ar-en): 2, 585 sentences translated by two state-of-the-art SMT systems (denoted ar-en-1 and ar-en-2), as provided in (Specia et al., 2011). A random split of 90% sentences for training and 10% for test is used. Human labels indicate the adequacy of the translation ranging from 1 (completely inadequate) to 4 (adequate). These datasets were annotated by two expert translators. 3.2 Query Methods The core of an AL setting is how the learner will gather new instances to add to its training data. In our setting, we use a pool-based strategy, where the learner queries an instance pool and selects the best instance according to an informativeness measure. The learner then asks an “oracle” (in this case, the human expert) for the true la</context>
</contexts>
<marker>Specia, Hajlaoui, Hallett, Aziz, 2011</marker>
<rawString>Lucia Specia, Najeh Hajlaoui, Catalina Hallett, and Wilker Aziz. 2011. Predicting machine translation adequacy. In Proceedings of MT Summit XIII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
</authors>
<title>Exploiting objective annotations for measuring translation post-editing effort.</title>
<date>2011</date>
<booktitle>In Proceedings of EAMT.</booktitle>
<contexts>
<context position="1977" citStr="Specia, 2011" startWordPosition="307" endWordPosition="308">ons is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. Since quality scores for the training of QE models are given by human experts, the annotation process is costly and subject to inconsistencies due to the subjectivity of the task. To avoid inconsistencies because of disagreements among annotators, it is often recommended that a QE model is trained for each translator, based on labels given by such a translator (Specia, 2011). This further increases the annotation costs because different datasets are needed for different tasks. Therefore, strategies to reduce the demand for annotated data are needed. Such strategies can also bring the possibility of selecting data that is less prone to inconsistent annotations, resulting in more robust and accurate predictions. In this paper we investigate Active Learning (AL) techniques to reduce the size of the dataset while keeping the performance of the resulting QE models. AL provides methods to select informative data points from a large pool which, if labelled, can potentia</context>
<context position="3704" citStr="Specia (2011)" startWordPosition="582" endWordPosition="583">e study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543–548, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics post-editing effort, post-editing time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-based approaches. For an overview on various feature sets and machine learning algorithms, we refer the reader to a recent shared task on the topic (Callison-Burch et al., 2012). Previous </context>
<context position="5453" citStr="Specia (2011)" startWordPosition="850" endWordPosition="851">asets manually annotated for quality: English-Spanish (en-es): 2, 254 sentences translated by Moses (Koehn et al., 2007), as provided by the WMT12 Quality Estimation shared task (Callison-Burch et al., 2012). Effort scores range from 1 (too bad to be post-edited) to 5 (no post-editing needed). Three expert post-editors evaluated each sentence and the final score was obtained by a weighted average between the three scores. We use the default split given in the shared task: 1, 832 sentences for training and 432 for test. French-English (fr-en): 2,525 sentences translated by Moses as provided in Specia (2011), annotated by a single translator. Human labels indicate post-editing effort ranging from 1 (too bad to be post-edited) to 4 (little or no post-editing needed). We use a random split of 90% sentences for training and 10% for test. Arabic-English (ar-en): 2, 585 sentences translated by two state-of-the-art SMT systems (denoted ar-en-1 and ar-en-2), as provided in (Specia et al., 2011). A random split of 90% sentences for training and 10% for test is used. Human labels indicate the adequacy of the translation ranging from 1 (completely inadequate) to 4 (adequate). These datasets were annotated </context>
</contexts>
<marker>Specia, 2011</marker>
<rawString>Lucia Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In Proceedings of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
</authors>
<title>Active annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Adaptive Text Extraction and Mining at EACL.</booktitle>
<contexts>
<context position="4597" citStr="Vlachos, 2006" startWordPosition="713" endWordPosition="714">g time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-based approaches. For an overview on various feature sets and machine learning algorithms, we refer the reader to a recent shared task on the topic (Callison-Burch et al., 2012). Previous work use supervised learning methods (“passive learning” following the AL terminology) to train QE models. On the other hand, AL has been successfully used in a number of natural language applications such as text classification (Lewis and Gale, 1994), named entity recognition (Vlachos, 2006) and parsing (Baldridge and Osborne, 2004). See Olsson (2009) for an overview on AL for natural language processing as well as a comprehensive list of previous work. 3 Experimental Settings 3.1 Datasets We perform experiments using four MT datasets manually annotated for quality: English-Spanish (en-es): 2, 254 sentences translated by Moses (Koehn et al., 2007), as provided by the WMT12 Quality Estimation shared task (Callison-Burch et al., 2012). Effort scores range from 1 (too bad to be post-edited) to 5 (no post-editing needed). Three expert post-editors evaluated each sentence and the fina</context>
</contexts>
<marker>Vlachos, 2006</marker>
<rawString>Andreas Vlachos. 2006. Active annotation. In Proceedings of the Workshop on Adaptive Text Extraction and Mining at EACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>