<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010761">
<title confidence="0.993964">
A New Dataset and Evaluation for Belief/Factuality
</title>
<author confidence="0.99622525">
Vinodkumar Prabhakaran1, Tomas By2, Julia Hirschberg1, Owen Rambow3*,
Samira Shaikh4, Tomek Strzalkowski4, Jennifer Tracey5, Michael Arrigo5,
Rupayan Basu6, Micah Clark2, Adam Dalton2, Mona Diab&apos;, Louise Guthrie2,
Anna Prokofieva1, Stephanie Strassel5, Gregory Werner&apos;, Janyce Wiebe8, Yorick Wilks2
</author>
<affiliation confidence="0.99848225">
1Department of Computer Science, Columbia University, New York, NY, USA
2Florida Institute for Human and Machine Cognition (IHMC), FL, USA
3Center for Computational Learning Systems, Columbia University, New York, NY, USA
4State University of New York - University of Albany, NY, USA
5Linguistic Data Consortium (LDC), University of Pennsylvania, PA, USA
6Amazon.com Inc., CA, USA
7George Washington University, DC, USA
8University of Pittsburgh, PA, USA
</affiliation>
<email confidence="0.953519">
*corresponding author; email address: rambow@ccls.columbia.edu
</email>
<sectionHeader confidence="0.992574" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99854">
The terms “belief” and “factuality” both re-
fer to the intention of the writer to present the
propositional content of an utterance as firmly
believed by the writer, not firmly believed, or
having some other status. This paper presents
an ongoing annotation effort and an associated
evaluation.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980266666667">
This paper presents an ongoing project aimed at de-
veloping a community-wide evaluation of expressed
belief, also known as “factuality”. Belief and fac-
tuality are closely related to hedging, veridical-
ity, and modality. The project has grown out of
the DARPA DEFT project; participants include the
Linguistic Data Consortium (LDC) and three per-
former sites: Columbia University/George Wash-
ington University, the Florida Institute for Human
and Machine Cognition, and the University of Al-
bany. The goal of our research project is not lin-
guistic annotation, but the identification of meaning
which is expressed in a non-linguistic manner. Such
a meaning representation is useful for many applica-
tions; in our project we are specifically interested in
</bodyText>
<page confidence="0.980508">
82
</page>
<bodyText confidence="0.999865846153846">
knowledge base population. A different part of the
DEFT program is concerned with the representation
of propositional meaning, following the tradition of
the ACE program in representing entities, relations
and events (ERE) (Doddington et al., 2004). The
work presented here is concerned with the attitude of
agents towards propositional content: do the agents
express a committed belief or a non-committed be-
lief in the propositional content? Our work has sev-
eral characteristics that set it apart from other work:
we are interested in annotation which can be done
fairly quickly; we are not interested in annotating
linguistic elements (such as trigger words); and we
are planning an integration with sentiment annota-
tion.
The structure of the paper is as follows: we start
out by situating our notion of “belief” with respect to
other notions of extra-propositional meaning (Sec-
tion 2); we then present our annotation in some de-
tail, with a special comparison to FactBank (Saur´ı
and Pustejovsky, 2012). While the goal of this paper
is not to talk about computational systems that were
run as part of the evaluation (different publications
will be available for that purpose), we quickly sum-
marize their main characteristics so that the evalu-
ation results can be interpreted. We then turn to
</bodyText>
<note confidence="0.932106">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 82–91,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.9948208">
the pilot evaluation we have performed, presenting
first the evaluation with respect to propositions (Sec-
tion 5) and then a qualitative evaluation. We con-
clude with a discussion of plans for the upcoming
open evaluation, scheduled for December 2015.
</bodyText>
<sectionHeader confidence="0.936167" genericHeader="introduction">
2 Terminology and Related Work
</sectionHeader>
<bodyText confidence="0.999952583333333">
In this section, we identify how we define differ-
ent terms. Different papers may have different and
conflicting definitions of these terms, but for lack of
space we do not provide an overview over all defini-
tions.
While at first the terms “belief” and “factuality”
appear to relate to rather different things (a subjec-
tive state versus truth), in the NLP community they
in fact refer to the same phenomenon, while having
rather different connotations. The phenomenon is
the communicative intention of a writer1 to present
propositional content as something that she firmly
believes is true, weakly believes is true, or has some
other attitude towards, namely a wish or a reported
belief. The term “belief” here describes the cogni-
tive state of the writer (Diab et al., 2009), and comes
from artificial intelligence and cognitive science,
as in the Belief-Desire-Intention model of Bratman
(1999 1987). The term “factuality” describes the
communicative intention of the writer (Sauriand
Pustejovsky, 2012, p. 263) (our emphasis):
The fact that an eventuality is depicted
as holding or not does not mean that this
is the case in the world, but that this is
how it is characterized by its informant.
Similarly, it does not mean that this is
the real knowledge that informant has (his
true cognitive state regarding that event)
but what he wants us to believe it is.
We would like to emphasize that the terms “be-
lief” and “factuality” do not refer to the underly-
ing truth of propositions, only to the intention of the
writer to present them as, in her view, true. Thus, we
as researchers cannot determine what is true from an
analysis of factuality (or of belief). The term “factu-
ality” is often misunderstood in this respect, which
</bodyText>
<footnote confidence="0.903992">
1For brevity, we will assume a female writer as the source
of utterances in this paper. Everything we say applies equally
to spoken and written communication, and equally to male and
female communicators.
</footnote>
<bodyText confidence="0.999894536585366">
is one of the reasons we prefer not to use it. In order
to understand the relation between belief/factuality
and truth, we need to distinguish two assumptions.
First, we may assume that the writer is not lying (as-
sumption of truthfulness). In this paper, we make
this first assumption. Second, we could assume that
the writer knows what is true (assumption of truth).
In this paper, we do not make this second assump-
tion. We discuss these two assumptions in turn.
We start with the assumption of truthfulness. In
the quote above, Sauriand Pustejovsky (2012) (apart
from distinguishing factuality from truth) also make
the point that the writer’s communicative intention
of making the reader believe she has a specific belief
state does not mean that she actually has that cogni-
tive state, since she may be lying. Lying is clearly an
important phenomenon that researchers have looked
into (Mihalcea and Strapparava, 2009; Ott et al.,
2011).2 However, we (as linguists interested in un-
derstanding how language enables communication)
feel that assuming the writer is truthful is a standard
assumption about communication which we should
in general make. This is because if we do not make
this assumption, we cannot explain why communi-
cation is possible at all, since discourse participants
would have no motivation to ever adopt another dis-
course participant’s belief as their own. We there-
fore do claim that we can infer belief from utter-
ances, while assuming that the writer is not lying,
and knowing that this assumption may be false in
certain cases.
We now turn to the second assumption, the as-
sumption of truth. Even if we assume that the
writer is not lying, the assumption of truth is not
required for communication to succeed; this is be-
cause the writer may be wrong, and this has no ef-
fect on the communication. For example, Ptolemy
successfully made many people believe that the sun
rotates around the earth, as was his (presumably)
honest communicative intention. Therefore, to us
as researchers interested in describing how language
</bodyText>
<footnote confidence="0.998759">
2Sarcasm and irony differ from lying in that the communica-
tive intention and the cognitive state are aligned, but they do not
align with the standard interpretation of the utterance. Here, the
intention is that the reader recognizes that the form of the ut-
terance does not literally express the cognitive state. We leave
aside sarcasm and irony in this paper; for current computational
work on sarcasm detection, see for example (Gonz´alez-Ib´a˜nez
et al., 2011).
</footnote>
<page confidence="0.999497">
83
</page>
<bodyText confidence="0.999884647058824">
is used to communicate, it does not matter that as-
tronomers now believe that Ptolemy was wrong, it
does not change our account of communication and
it does not change the communication that happened
two millennia ago. And since we do not need to
make the assumption that the writer knows what she
is talking about, we choose not to make this assump-
tion. In the case of Ptolemy, we leave this determi-
nation – what is actually true – to astronomers. In
other cases, we typically have models of trustwor-
thiness: if a writer sends her spouse a text message
saying she is hungry, the spouse has no reason to be-
lieve she is wrong. We leave this issue aside in this
paper.
The term “hedge” refers to words or phrases that
add ambiguity or uncertainty (Propositional Hedges)
or show the speakers lack of commitment to a propo-
sition (Relational Hedges). For example, The ball
is sort of blue contains a Relational Hedge (sort
of) and I think the ball is blue includes a proposi-
tional hedge (think). Propositional hedges indicate
non-committed belief. There has been a major effort
to annotate texts with hedging information (Farkas
et al., 2010), with an open evaluation. While be-
lief and hedging are closely related, we see the be-
lief/factuality annotation as more general than hedg-
ing (since it does not only include non-committed
belief), and also more semantic (since we are not
identifying language use but underlying meaning).
The term “modality” is used in formal seman-
tics as well as in descriptive linguistics. Many se-
manticists (e.g. (Kratzer, 1991; Kaufmann et al.,
2006)) define modality as quantification over pos-
sible worlds. Modality can be of two types: epis-
temic, which qualifies the speaker’s commitment,
and deontic, which concerns freedom to act. Be-
lief/factuality falls under epistemic modality. An-
other view of modality relates more to a speaker’s
attitude toward a proposition (e.g. (McShane et al.,
2004; Baker et al., 2010; Prabhakaran et al., 2012)),
which is closer to the way we model belief.
We interpret the term “veridical” as referring to
a property of certain words (usually verbs), namely
to mark the proposition expressed by their syntac-
tic complement clause as firmly believed (commit-
ted belief) by the writer (Kiparsky and Kiparsky,
1970). Veridicality as a property of lexical or
lexico-syntactic elements is thus a way of relating
belief/factuality to linguistic means of expressing
them, but we take the notion of belief/factuality as
being the underlying notion.
</bodyText>
<sectionHeader confidence="0.998518" genericHeader="method">
3 Annotation
</sectionHeader>
<subsectionHeader confidence="0.999946">
3.1 Annotation Manual
</subsectionHeader>
<bodyText confidence="0.993765333333333">
The purpose of this annotation is to capture the com-
mitment of the writer’s belief in the propositions ex-
pressed in the text. The annotation for this project
marks beliefs held by the writer only. We exhaus-
tively annotate all (clausal) propositions in each doc-
ument with a four-way belief type distinction, with
the following categories.
Committed belief (CB) – the writer strongly be-
lieves that the proposition is true. Examples:
</bodyText>
<listItem confidence="0.989168470588235">
(1) a. The sun will rise tomorrow.
b. I know John and Katie went to Paris last
year.
Non-committed belief (NCB) – the writer believes
that the proposition is possibly or probably true, but
is not certain. Examples:
(2) a. It could rain tomorrow.
b. I think John and Katie went to Paris last
year.
Reported belief (ROB) – the writer attributes be-
lief (either committed or non-committed) to another
person or group. Note that this label is only applied
when the writer’s own belief in the proposition is
unclear. Examples:
(3) a. Channel 6 said it could rain tomorrow.
b. Sarah said that John and Katie went to Paris
last year.
</listItem>
<bodyText confidence="0.9978366">
Non-belief propositions (NA) – the writer ex-
pressed some other cognitive attitude toward the
proposition, such as desire or intention, or expressly
states that s/he has no belief about the proposition
(e.g., by asking a question). Examples:
</bodyText>
<listItem confidence="0.758437">
(4) a. Is it going to rain tomorrow?
b. I hope John and Katie went to Paris last
year.
</listItem>
<bodyText confidence="0.99982625">
We do not make any effort to evaluate the truth
value of the propositions, only the expressed level
of belief in them held by the writer. Thus a strongly
held false belief would not appear any different from
</bodyText>
<page confidence="0.989384">
84
</page>
<bodyText confidence="0.999953">
a strongly held true belief. Similarly, lying, sarcasm,
irony, and other cases where the writer’s internal be-
lief may differ from the expressed belief are not cap-
tured. That is, we take all expressed beliefs at “face
value”. We also do not capture any cognitive atti-
tudes expressed about a proposition other than be-
lief. An NA tag signifies just that there is no be-
lief expressed about the proposition; it does not sig-
nify that there is another cognitive attitude expressed
(e.g., 4a). Similarly, a proposition tagged as CB
may also have other cognitive attitudes expressed
about them (e.g., in “John managed to go to Paris
last week”, the author is expressing CB towards the
proposition go, but also the success modality (Prab-
hakaran et al., 2012)); we do not capture them.
Annotators are not required to identify the full
text span of the proposition. Instead, we take ad-
vantage of the close relationship between the seman-
tics of the proposition and the syntactic structure of
the clause by marking only the head of the structural
unit containing the proposition (propositional head).
For each proposition, annotators mark a head word
and tag it with one of the four belief types. Note
that the syntactic head word (perhaps lemmatized)
can serve as a convenient name for the proposition,
so for the examples above, we can talk about the be-
lief in the ‘rain’ proposition and in the ‘go’ propo-
sition. When a sentence has a single clause con-
taining only one proposition, there will be only one
head word to identify (usually a verb, but see details
below on identifying heads of propositions). Many
sentences contain multiple propositions, and the an-
notation guidelines provide detailed instructions on
identifying head words. Note that the (b) examples
above contain an additional proposition which is not
marked; a full markup for example (3b) is below.
</bodyText>
<listItem confidence="0.984816">
(5) Sarah said/CB that John and Katie went/ROB
to Paris last year.
</listItem>
<bodyText confidence="0.6368675">
This is equivalent to the following span-based anno-
tation:
</bodyText>
<listItem confidence="0.992118">
(6) [CB Sarah said [ROB that John and Katie went
to Paris last year.]]
</listItem>
<bodyText confidence="0.9342285">
The general principles of head word selection for
each proposition can be summarized as follows:
</bodyText>
<listItem confidence="0.94676875">
1. Annotate the lexical verb of the clause express-
ing the proposition, if there is one.
2. If the verb of the clause is a copula, annotate
the head of the predicate that follows the copula
(noun for NP, preposition for PP, etc.).
3. Deontic modal auxiliaries, which signal a com-
plex proposition, are annotated in addition to
the lexical verb, as a separate belief.
</listItem>
<bodyText confidence="0.999587833333333">
All annotations are applied to a single whitespace-
delimited word. In cases where the head of a propo-
sition is a multiword expression (MWE), the head
of the MWE is selected. In cases of noun phrases
where no head is apparent (e.g. bok choy), the last
word of the MWE is selected.
</bodyText>
<subsectionHeader confidence="0.999886">
3.2 Comparison with FactBank
</subsectionHeader>
<bodyText confidence="0.998574625">
As already explained (Section 2), we take the terms
“belief” and “factuality” to refer to the same phe-
nomenon underlyingly (with perhaps different em-
phases). Therefore, the FactBank annotation is basi-
cally compatible with ours. Our annotation is much
simpler than that of FactBank in order to allow for a
quicker annotation. We summarize the main points
of simplification here.
</bodyText>
<listItem confidence="0.9887381">
• We have taken the source always to be the
writer. As we will discuss in Section 7.1, we
will adopt the FactBank annotation in the next
iteration of our annotation.
• We do not distinguish between possible and
probable; this distinction may be hard to anno-
tate and not too valuable.
• We ignore negation. If present, we simply as-
sume it is part of the proposition which is the
target.
</listItem>
<bodyText confidence="0.9987796">
Werner et al. (2015) study the relation between
belief and factuality in more detail. They provide an
automatic way of mapping the annotations in Fact-
Bank to the 4-way distinction of speaker/writer’s be-
lief that we present in this paper.
</bodyText>
<subsectionHeader confidence="0.998474">
3.3 Corpus and Annotation Results
</subsectionHeader>
<bodyText confidence="0.9996312">
The annotation effort for this phase of belief annota-
tion for DEFT produced a training corpus of 852,836
words and an evaluation corpus of 100,037 words.
All annotated data consisted of English text from
discussion forum threads. The discussion forum
</bodyText>
<page confidence="0.998836">
85
</page>
<bodyText confidence="0.993435625">
threads were originally collected for the DARPA
BOLT program, and were harvested from a wide va-
riety of sites. Discussion forum sites were chosen
for harvesting in BOLT based on human judgement
that the site was likely to contain many threads dis-
cussing either current events or personal anecdotes.
For details on the BOLT collection, see Garland et
al. (2012). Threads longer than 1000 words were
truncated to produce documents consisting of one or
more consecutive posts from a single thread. Long
threads may generate multiple documents consisting
of non-overlapping sections of the same thread (e.g.,
document 1 contains posts 1-5, while document 2
contains posts 6-12, etc.). The distribution of the
four belief types in the training and evaluation cor-
pora can be seen in Table 1.
</bodyText>
<table confidence="0.998300142857143">
Annotations CB NCB ROB NA
Training Corpus
143240 79995 3890 7150 52205
(56%) (3%) (5%) (36%)
Evaluation Corpus
17553 8730 583 941 7299
(50%) (3%) (5%) (42%)
</table>
<tableCaption confidence="0.999726">
Table 1: Annotation Statistics
</tableCaption>
<bodyText confidence="0.999988791666667">
The source data pool, annotation procedures, and
annotators were the same for both the training and
evaluation datasets, with the exception of the fact
that the evaluation annotations received a full second
pass over the annotation by a senior annotator (not
the same as the first pass annotator) to increase con-
sistency and reduce annotator errors. The training
annotations were produced with a single annotation
pass, and quality control was conducted through a
second pass by a senior annotator on a sample of ap-
proximately 15% of the data. Inter-annotator agree-
ment on headword selection was 93% and agree-
ment on belief type labeling was 84%. Overall
observed agreement, combining headword selection
and belief type label, was 78% (Kappa score .60).
Agreement on belief type was least reliable on the
categories of ROB and NCB, both of which were
sometime erroneously marked as CB. Both of these
categories, in addition to being less frequent in the
corpus, have difficult edge cases in which the an-
notator must make a judgment based on the context
of the document (for example, deciding whether the
writer clearly shares a belief attributed to another
person for ROB).
</bodyText>
<sectionHeader confidence="0.99274" genericHeader="method">
4 Evaluation Systems
</sectionHeader>
<bodyText confidence="0.999976555555556">
We conducted a multi-site pilot evaluation for the
task of identifying beliefs expressed in text. Three
performer sites took part in this evaluation. In this
section, we briefly describe the systems built at these
performer sites. The first two systems are rule-based
systems, whereas the third system is a supervised
learning system. We limit the discussion of these
systems to a high level, postponing the detailed sys-
tem descriptions to separate future publications.
</bodyText>
<subsectionHeader confidence="0.992995">
4.1 System A
</subsectionHeader>
<bodyText confidence="0.999964210526316">
System A is adapted from a Sentiment Slot Filling
system which participated in the 2014 TAC KBP
SSF Evaluation (Shaikh et al., 2014). This system
uses the Stanford Parser to create a syntactic depen-
dency structure for every sentence in a given doc-
ument. Using the dependency tree, it extracts the
belief targets, which are usually the subjects of the
sentence. In addition, the system extracts belief re-
lations – a unary or binary predicate – typically a
verb, an adjective or a noun. The focus of this ver-
sion of System A is to identify propositional heads
that express belief of any type. Each relation so ex-
tracted was initially marked as CB. A few heuristics
were then applied to distinguish CBs from NCBs -
such as presence of hedge words (maybe, guess). In
addition, a few heuristics were added to tag relations
as NAs, for example when the predicates appear in a
question. The current version of System A does not
account for ROB tags.
</bodyText>
<subsectionHeader confidence="0.991757">
4.2 System B
</subsectionHeader>
<bodyText confidence="0.9999552">
System B uses the dependency tree and part-of-
speech tags from the Stanford NLP tools, together
with a custom verb lexicon to recognize belief ex-
pressions. The tree is processed to convert ob-
jects and complements to a single format, and then
transformed into one or more belief triples (subject,
verb, object). The system maintains a database of
nested belief context, as in ‘X believes Y believes
Z’, but we did not notice many instances of this phe-
nomenon in the data. Partly because our System B
</bodyText>
<page confidence="0.994846">
86
</page>
<table confidence="0.998699571428571">
System A System B System C
Prec. Rec. F-meas. Prec. Rec. F-meas. Prec. Rec. F-meas.
CB 35.9 39.9 37.8 42.1 36.8 39.3 68.9 77.9 73.1
NCB 13.3 8.8 10.6 4.6 7.4 5.7 52.9 29.7 38.0
ROB 0.0 0.0 0.0 1.3 0.9 1.0 43.8 15.6 23.0
NA 40.3 5.9 10.2 35.8 4.8 8.4 80.1 62.0 69.9
Overall 35.5 22.5 27.6 34.4 20.6 25.8 72.0 66.4 69.1
</table>
<tableCaption confidence="0.999873">
Table 2: Results obtained for System A, System B, and System C on the final Evaluation dataset.
</tableCaption>
<bodyText confidence="0.999660166666667">
recognizes reported beliefs (ROB) independently of
the distinction between committed/non-committed
belief in the annotations, the heuristic rules (mainly
based on the presence of modal auxiliaries) that we
added for the purpose of classifying the beliefs (CB,
NCB, ROB, NA) did not work reliably in all cases.
</bodyText>
<subsectionHeader confidence="0.998207">
4.3 System C
</subsectionHeader>
<bodyText confidence="0.999994181818182">
System C uses a supervised learning approach to
identify tokens denoting the heads of propositions
that denote author’s expressed beliefs. It approaches
this problem as a 5-way (CB, NCB, ROB, NA, nil)
multi-class classification task at the word level. Sys-
tem C is adapted from a previous system which uses
an earlier, simpler definition and annotation of be-
lief (Prabhakaran et al., 2010). The system uses lex-
ical and syntactic features for this task, which are
extracted using the part-of-speech tags and depen-
dency parses obtained from the Stanford CoreNLP
system. In addition to the features described in
(Prabhakaran et al., 2010), System C uses a set of
new features including features based on a dictio-
nary of hedge-words (Prokofieva and Hirschberg,
2014). The hedge features improved the NCB F-
measure by around 2.2 percentage points (an overall
F-measure improvement of 0.25 percentage points)
in experiments conducted on a separate development
set. It uses a quadratic kernel SVM to build the
model, which outperformed the linear kernel in ex-
periments conducted on the development set.
</bodyText>
<sectionHeader confidence="0.998251" genericHeader="method">
5 Proposition-Oriented Evaluation
</sectionHeader>
<bodyText confidence="0.99994505">
We now describe the results obtained on a
proposition-oriented quantitative evaluation of these
systems. We focus on a system’s ability to correctly
identify the propositional heads of each type of be-
lief (CB, NCB, ROB, NA). Only the words denoting
heads of propositions will get one of these tags, and
hence the majority of words in our data will not have
any tags. We expect the system to find the proposi-
tional heads and to correctly assign their belief tags.
We use the entire Evaluation dataset described in
Section 3 for this evaluation (entirely unseen during
the development of the systems). We report preci-
sion, recall and F-measure for each belief type. We
also report their micro-averages as the overall result.
We compute F-measure as the harmonic mean be-
tween precision and recall. The best results obtained
by each system described in Section 4 are presented
in Table 2.
For System A, four different configurations were
run for the evaluation, in which the NCB and NA
tagging was either enabled or disabled. (The cur-
rent version does not account for ROB tags.) In Ta-
ble 2, Columns 2-4, we show the performance of
System A while all 3 tags (CB, NCB and NA) are
enabled. The results of other three configurations
are comparable. Any sentence where the belief tar-
get could not be located, either due to parsing error
or due to missing coreference (as supplied by ERE),
was discarded. This resulted in a relatively lower re-
call in the evaluation, but produced high precision
in a target-driven pilot evaluation (Section 6). The
results obtained by System B in the evaluation are
shown in Table 2: Columns 5-7. The results of Sys-
tem B, when ignoring the belief categories (i.e., on
identifying heads of propositions), were 83.6% pre-
cision and 50% recall. Table 2: Columns 8-10 shows
results obtained by System C trained on 80% of the
training dataset (the rest of the corpus was used as a
development set).
The supervised learning approach obtained over-
</bodyText>
<page confidence="0.997634">
87
</page>
<bodyText confidence="0.99983575">
all better performance than rule based approach in
our evaluation. ROB and NCB were the most dif-
ficult classes to predict for all three systems (e.g.,
highest recall posted for ROB is only 15.6%). CB
was relatively easier to predict. NA was difficult to
predict using the rule based approach, but supervised
learning approach obtained reasonable performance
of 69.9 F-measure.
</bodyText>
<sectionHeader confidence="0.938145" genericHeader="method">
6 An Entity-Focused Evaluation:
Preliminary Investigation
</sectionHeader>
<bodyText confidence="0.999834185185185">
In this section, we describe an initial investigation
towards an entity-focused evaluation. An entity-
focused evaluation tests a different kind of question
about beliefs: given an entity e, what beliefs does the
writer have about e? This entity-focused evaluation
draws its parallels from TAC KBP Sentiment Slot
Filling Evaluation (SSF) task. In the SSF, the task is
to determine a target entity given a source entity and
a sentiment between them. The goal is to populate
a knowledge base with information regarding enti-
ties and the sentiment relations between them. In
the same vein, an entity-focused belief task would
provide knowledge about the salient belief relations
between entities. For this purpose, we needed to de-
fine what is meant by “having a belief about an en-
tity” and agreed on the following preliminary rules.
The rules are entirely syntactic. In the following ex-
amples, the target entity is Mary, and the statement
after the arrow shows what the beliefs are about her
(and what the level of commitment by the writer is).
Adjunct clause case 1. If the target entity is con-
tained in a clause (lets call it the “core clause”) but
NOT in an adjunct clause which modifies the core
clause, we omit the adjunct clause (even though the
adjunct clause in some sense pertains to the core
clause but by virtue of being an adjunct, it is omiss-
able).
</bodyText>
<listItem confidence="0.995603">
(7) While John was in/CB Paris, Mary left/CB Paul
−→ CB: Mary left Paul
</listItem>
<bodyText confidence="0.997747">
Adjunct clause case 2. If the target is in an ad-
junct clause to a core clause where the target is not
mentioned, we retain both the adjunct clause as a
standalone belief, and the combination of the ad-
junct and core (i.e., we have two beliefs about the
entity).
</bodyText>
<listItem confidence="0.999839">
(8) John was happy/CB when Mary left/CB Paul
−→ CB: John was happy when Mary left Paul
; CB: Mary left Paul
</listItem>
<bodyText confidence="0.999877481481482">
We devised similar rules for complement clauses,
we omit them here.
For the actual evaluation, we used files which also
had been hand-annotated for ACE entities. How-
ever, we did not have a gold annotation for entity-
focused belief, as this study is still contributing to-
wards a definition of this notion. Only two systems
participated, System A and System C. System A as
described in Section 4.1 already takes the notion of
entity into account. For System C, we used the parse
to determine the span associated with the annotated
headword, and counted a proposition whose span in-
cluded an entity to be about that entity. In order
to understand how these two ways of determining
entity-focused belief relate to each other, we com-
pared the two systems to each other. We obtained
an F-measure of 52%. We also hand evaluated the
positive claims of System C, obtaining an accuracy
of 48% on the positive claims. The errors are due
to parse errors, the presence of the entity in adjuncts
which do not appear germane (contradicting adjunct
clause case 2), the presence of irrelevant adjunct
clauses (counter to adjunct clause case 1), and to a
lack of clarity in the annotation standard. As an ex-
ample of the lack of clarity, consider the following
sentence from our evaluation corpus, with two kids
as target entity:
</bodyText>
<listItem confidence="0.828283">
(9) I didn’t see these two kids (sic) names on the
news
</listItem>
<bodyText confidence="0.999821857142857">
two kids is a possessor of the direct object, and fell
into the span of the annotated see for System C, but
System A deemed the ‘see’ belief not to be about it.
We conclude that this purely syntactic definition of
“belief about an entity” is not satisfactory. The def-
inition of “belief about an entity” remains an open
question and we return to it in Section 7.3.
</bodyText>
<sectionHeader confidence="0.996336" genericHeader="method">
7 Plans for Next Round
</sectionHeader>
<subsectionHeader confidence="0.999579">
7.1 Adding the Source
</subsectionHeader>
<bodyText confidence="0.999883">
Currently, we are only annotating and evaluating the
writer’s beliefs. Beliefs attributed by the writer to
other sources are marked ROB. We intend to anno-
tate the source for all beliefs, using the method of
</bodyText>
<page confidence="0.997333">
88
</page>
<bodyText confidence="0.984910125">
nested attribution pioneered by MPQA (Wiebe et al.,
2005) and adopted by FactBank (Saur´ı and Puste-
jovsky, 2012). Consider the following sentence.
(10) John believes Mary knows that the clock was
stolen
In the nested attribution approach, according to
the writer, according to John, Mary firmly believes
(CB) the ‘steal’ proposition. According to the writer,
John firmly believes the ‘know’ proposition and the
‘steal’ proposition (as indicated by the veridical verb
know). The writer herself firmly believes (CB) the
‘believe’ proposition, does not express an opinion on
the ‘know’ proposition (ROB), and also firmly be-
lieves (CB) the ‘steal’ proposition (again, the reader
infers this from the use of know). We intend to an-
notate all these levels of belief.
</bodyText>
<subsectionHeader confidence="0.998992">
7.2 Defining the Target Proposition
</subsectionHeader>
<bodyText confidence="0.999964695652174">
In our work to date, we have assumed that the tar-
get of a belief is a proposition, and we have repre-
sented the proposition by the syntactic head word of
the clause which describes the proposition (which
is equivalent to a text span under syntactic projec-
tion). We are investigating extending this in sev-
eral manners. First, we are considering including
the heads of event noun phrases (the sudden collapse
of the building). Second, we are looking at using a
semantic representation for the proposition (as op-
posed the syntactic head of the text passage describ-
ing the proposition). We do not propose to develop
our own semantic representation, but instead we will
look to using existing relation and event representa-
tions based on the ACE program (Doddington et al.,
2004). These have the advantage that there are off-
the-shelf computational tools available for detecting
ACE relations and events; they have the disadvan-
tage that they do not cover all propositions we may
be interested in. An alternative would be the use of
a shallower semantic representation such as Prop-
Bank (Kingsbury et al., 2002), FrameNet (Baker et
al., 1998), or AMR (Banarescu et al., 2013).
</bodyText>
<subsectionHeader confidence="0.918099">
7.3 Entities as Targets
</subsectionHeader>
<bodyText confidence="0.998233555555555">
In Section 6, we discussed an initial evaluation of a
belief being about an entity. In this section we dis-
cuss further guidelines for identifying belief targets,
i.e., when one can say that someone’s belief is about
a certain entity.
In general, the notion of belief “aboutness” is
fairly fuzzy and it may be difficult to circumscribe
precisely without some additional constraints. Sup-
pose then that one of the ultimate objectives of belief
extraction is to populate a knowledge base with be-
liefs held about specific entities: individuals, groups,
artifacts, etc., which adds this constraint that the
extracted belief is knowledge-base-worthy, or re-
portable. Some initial guidelines may go as sug-
gested below. The objective is to provide guidance
for a human assessor — not to propose a solution.
We should note that these guidelines generally tran-
scend any syntactic or structural considerations and
appeal directly to the annotators’ judgment. Further-
more, we note that these guidelines are not about ef-
fects relating to information structure – in one sense
of “being about”, the same sentence may be referred
to as being “about” different things in different con-
texts. We are aiming for a lexical-semantic, not a
pragmatic notion of aboutness.
A belief whose target is proposition p is about an
entity T if one of the following clauses holds:
</bodyText>
<listItem confidence="0.533769916666667">
1. p describes a property of T, where the prop-
erty is considered semi-permanent but not nec-
essarily limited to physical or mental charac-
teristics (e.g., red, long, brainy) and may also
include behavioral properties (smart, slow) as
well as characteristics bestowed on by others
(beloved).
2. T is an agent of p, i.e., T is said to be perform-
ing some activity, physical or mental: drive a
car, send a letter, etc.
3. T is directly involved in (or affected by) p but is
not an agent: this includes situations where T’s
</listItem>
<bodyText confidence="0.9846612">
involvement may be passive but is nonetheless
required for p to be performed, e.g., receive a
letter, win a prize, etc.
We make no claim that the above list is exhaustive
or that there would not be exceptions to these rules.
For this reason we may also attempt to describe con-
ditions under which a belief is not about T. For ex-
ample: a belief target p is not about entity T even
though T may be mentioned within the scope of p
if:
</bodyText>
<page confidence="0.999277">
89
</page>
<bodyText confidence="0.9876052">
4. T appears uninvolved in p and is apparently un-
affected by its execution, e.g., reading about,
waiting for, etc.
those of the authors and do not reflect the official
policy or position of the Department of Defense or
the U.S. Government. We thank several anonymous
reviewers for their constructive feedback.
We intend to explore whether we can define this
notion of belief aboutness sufficiently well to obtain
good inter-annotator agreement.
</bodyText>
<subsectionHeader confidence="0.998466">
7.4 Combining with Sentiment
</subsectionHeader>
<bodyText confidence="0.973077666666667">
We are planning on working on an annotation and an
evaluation that combines belief with sentiment. The
motivation for this is that belief and sentiment are
similar types of meaning: they are attitudes towards
propositions or entities which are expressed directly
or indirectly. The similarity can also be seen in the
fact that FactBank took the notion of nested source
from MPQA, which is a sentiment-annotated cor-
pus. Furthermore, many lexical items express both a
belief and a sentiment at once:
(11) I hope Bertha enjoys the oysters
Here, the writer expresses a positive sentiment to-
wards the ‘enjoy’ proposition, and at the same time
she is expressing a lack of certainty (NCB) in the
‘enjoy’ proposition.
</bodyText>
<subsectionHeader confidence="0.999728">
7.5 Adding Spanish and Chinese
</subsectionHeader>
<bodyText confidence="0.999707333333333">
We will be extending our annotation (including
some of the extensions mentioned above) to Span-
ish and Chinese.
</bodyText>
<sectionHeader confidence="0.997373" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999954916666667">
We have presented an ongoing annotation effort re-
lated to belief/factuality and an initial evaluation
based on that annotation effort. To our knowledge,
the annotated corpus is by far the largest corpus an-
notated in terms of belief/factuality. We have pre-
sented several proposed extensions to the annota-
tion. The linguistic resources described in this paper
will be published in the LDC catalog, making them
available to the broader research community. The
materials will be used in an open evaluation in late
2015 or early 2016. The evaluation will cover both
belief/factuality and sentiment.
</bodyText>
<sectionHeader confidence="0.998324" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.942251">
This paper is based upon work supported by the
DARPA DEFT Program. The views expressed are
</bodyText>
<sectionHeader confidence="0.987763" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999808913043478">
Collin F. Baker, J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet project. In 36th Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics (COLING-ACL’98), pages 86–90, Montr´eal.
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Nathaniel W. Filardo, Lori S. Levin, and Christine D.
Piatko. 2010. A modality lexicon and its use in auto-
matic tagging. In LREC.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop and Interoperability with Dis-
course, pages 178–186, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Michael E. Bratman. 1999 [1987]. Intention, Plans, and
Practical Reason. CSLI Publications.
Mona Diab, Lori Levin, Teruko Mitamura, Owen Ram-
bow, Vinodkumar Prabhakaran, and Weiwei Guo.
2009. Committed belief annotation and tagging. In
Proceedings of the Third Linguistic Annotation Work-
shop, pages 68–73, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) program–tasks, data, and evaluation. In Proceed-
ings of the Fourth International Conference on Lan-
guage Resources and Evaluation (LREC’04), pages
837–840.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
Csirik, and Gy¨orgy Szarvas. 2010. The conll-2010
shared task: Learning to detect hedges and their scope
in natural language text. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 1–12, Uppsala, Sweden, July.
Association for Computational Linguistics.
Roberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 581–586, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.977236">
90
</page>
<reference confidence="0.999870246376812">
Stefan Kaufmann, Cleo Condoravdi, and Valentina
Harizanov, 2006. Formal Approaches to Modality,
pages 72–106. Mouton de Gruyter.
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceedings of the Human Language Tech-
nology Conference, San Diego, CA.
Paul Kiparsky and Carol Kiparsky. 1970. Facts. In
Manfred Bierwisch and Karl Erich Heidolph, editors,
Progress in Linguistics, pages 143–173. Mouton, The
Hague, Paris.
Angelika Kratzer. 1991. Modality. In Arnim von Ste-
chow and Dieter Wunderlich, editors, Semantics: An
International Handbook of Contemporary Research.
Walter de Gruyter, Berlin.
Marjorie McShane, Sergei Nirenburg, and Ron
Zacharsky. 2004. Mood and modality: Out of
the theory and into the fray. Natural Language
Engineering, 19(1):57–89.
Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 309–
312, Suntec, Singapore, August. Association for Com-
putational Linguistics.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
309–319, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2010. Automatic committed belief tagging.
In Coling 2010: Posters, pages 1014–1022, Beijing,
China, August. Coling 2010 Organizing Committee.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko,
Owen Rambow, and Benjamin Van Durme. 2012. Sta-
tistical modality tagging from rule-based annotations
and crowdsourcing. In Proceedings of the Workshop
on Extra-Propositional Aspects of Meaning in Com-
putational Linguistics, pages 57–64, Jeju, Republic of
Korea, July. Association for Computational Linguis-
tics.
Anna Prokofieva and Julia Hirschberg. 2014. Hedg-
ing and speaker commitment. In 5th Intl. Workshop
on Emotion, Social Signals, Sentiment &amp; Linked Open
Data, Reykjavik, Iceland.
Roser Saur´ı and James Pustejovsky. 2012. Are you sure
that this happened? assessing the factuality degree of
events in text. Computational Linguistics, 38(2):261–
299.
Samira Shaikh, Rob Giarrusso, Veena Ravishankar, and
Tomek Strzalkowski. 2014. The SUNY Albany Senti-
ment Slot Filling System. In Proceedings of the 2014
TAC KBP Sentiment Slot Filling Evaluation, Gaithers-
burg, Maryland, USA. NIST.
Gregory J. Werner, Vinodkumar Prabhakaran, Mona
Diab, and Owen Rambow. 2015. Committed belief
tagging on the factbank and lu corpora: A compara-
tive study. In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, Denver, USA, June. Association for Com-
putational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language ann. Language Resources and Evaluation,
39(2/3):164–210.
</reference>
<page confidence="0.999171">
91
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.448865">
<title confidence="0.999975">A New Dataset and Evaluation for Belief/Factuality</title>
<author confidence="0.9980295">Tomas Julia Owen Tomek Jennifer Michael Micah Adam Mona Louise Stephanie Gregory Janyce Yorick</author>
<affiliation confidence="0.914233">of Computer Science, Columbia University, New York, NY, Institute for Human and Machine Cognition (IHMC), FL, for Computational Learning Systems, Columbia University, New York, NY, University of New York - University of Albany, NY, Data Consortium (LDC), University of Pennsylvania, PA, Inc., CA, Washington University, DC,</affiliation>
<address confidence="0.999344">of Pittsburgh, PA, USA</address>
<email confidence="0.998045">*correspondingauthor;emailaddress:rambow@ccls.columbia.edu</email>
<abstract confidence="0.983511875">The terms “belief” and “factuality” both refer to the intention of the writer to present the propositional content of an utterance as firmly believed by the writer, not firmly believed, or having some other status. This paper presents an ongoing annotation effort and an associated evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL’98),</booktitle>
<pages>86--90</pages>
<location>Montr´eal.</location>
<contexts>
<context position="30418" citStr="Baker et al., 1998" startWordPosition="5072" endWordPosition="5075"> the syntactic head of the text passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals, g</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL’98), pages 86–90, Montr´eal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathryn Baker</author>
<author>Michael Bloodgood</author>
<author>Bonnie J Dorr</author>
<author>Nathaniel W Filardo</author>
<author>Lori S Levin</author>
<author>Christine D Piatko</author>
</authors>
<title>A modality lexicon and its use in automatic tagging.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="10035" citStr="Baker et al., 2010" startWordPosition="1615" endWordPosition="1618">ted belief), and also more semantic (since we are not identifying language use but underlying meaning). The term “modality” is used in formal semantics as well as in descriptive linguistics. Many semanticists (e.g. (Kratzer, 1991; Kaufmann et al., 2006)) define modality as quantification over possible worlds. Modality can be of two types: epistemic, which qualifies the speaker’s commitment, and deontic, which concerns freedom to act. Belief/factuality falls under epistemic modality. Another view of modality relates more to a speaker’s attitude toward a proposition (e.g. (McShane et al., 2004; Baker et al., 2010; Prabhakaran et al., 2012)), which is closer to the way we model belief. We interpret the term “veridical” as referring to a property of certain words (usually verbs), namely to mark the proposition expressed by their syntactic complement clause as firmly believed (committed belief) by the writer (Kiparsky and Kiparsky, 1970). Veridicality as a property of lexical or lexico-syntactic elements is thus a way of relating belief/factuality to linguistic means of expressing them, but we take the notion of belief/factuality as being the underlying notion. 3 Annotation 3.1 Annotation Manual The purp</context>
</contexts>
<marker>Baker, Bloodgood, Dorr, Filardo, Levin, Piatko, 2010</marker>
<rawString>Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Nathaniel W. Filardo, Lori S. Levin, and Christine D. Piatko. 2010. A modality lexicon and its use in automatic tagging. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<title>Abstract meaning representation for sembanking.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>178--186</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="30451" citStr="Banarescu et al., 2013" startWordPosition="5078" endWordPosition="5081">xt passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals, groups, artifacts, etc., which add</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178–186, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Bratman</author>
</authors>
<title>Intention, Plans, and Practical Reason.</title>
<date>1999</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="4547" citStr="Bratman (1999" startWordPosition="694" endWordPosition="695"> to rather different things (a subjective state versus truth), in the NLP community they in fact refer to the same phenomenon, while having rather different connotations. The phenomenon is the communicative intention of a writer1 to present propositional content as something that she firmly believes is true, weakly believes is true, or has some other attitude towards, namely a wish or a reported belief. The term “belief” here describes the cognitive state of the writer (Diab et al., 2009), and comes from artificial intelligence and cognitive science, as in the Belief-Desire-Intention model of Bratman (1999 1987). The term “factuality” describes the communicative intention of the writer (Sauriand Pustejovsky, 2012, p. 263) (our emphasis): The fact that an eventuality is depicted as holding or not does not mean that this is the case in the world, but that this is how it is characterized by its informant. Similarly, it does not mean that this is the real knowledge that informant has (his true cognitive state regarding that event) but what he wants us to believe it is. We would like to emphasize that the terms “belief” and “factuality” do not refer to the underlying truth of propositions, only to t</context>
</contexts>
<marker>Bratman, 1999</marker>
<rawString>Michael E. Bratman. 1999 [1987]. Intention, Plans, and Practical Reason. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Lori Levin</author>
<author>Teruko Mitamura</author>
<author>Owen Rambow</author>
<author>Vinodkumar Prabhakaran</author>
<author>Weiwei Guo</author>
</authors>
<title>Committed belief annotation and tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Linguistic Annotation Workshop,</booktitle>
<pages>68--73</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="4427" citStr="Diab et al., 2009" startWordPosition="676" endWordPosition="679">space we do not provide an overview over all definitions. While at first the terms “belief” and “factuality” appear to relate to rather different things (a subjective state versus truth), in the NLP community they in fact refer to the same phenomenon, while having rather different connotations. The phenomenon is the communicative intention of a writer1 to present propositional content as something that she firmly believes is true, weakly believes is true, or has some other attitude towards, namely a wish or a reported belief. The term “belief” here describes the cognitive state of the writer (Diab et al., 2009), and comes from artificial intelligence and cognitive science, as in the Belief-Desire-Intention model of Bratman (1999 1987). The term “factuality” describes the communicative intention of the writer (Sauriand Pustejovsky, 2012, p. 263) (our emphasis): The fact that an eventuality is depicted as holding or not does not mean that this is the case in the world, but that this is how it is characterized by its informant. Similarly, it does not mean that this is the real knowledge that informant has (his true cognitive state regarding that event) but what he wants us to believe it is. We would li</context>
</contexts>
<marker>Diab, Levin, Mitamura, Rambow, Prabhakaran, Guo, 2009</marker>
<rawString>Mona Diab, Lori Levin, Teruko Mitamura, Owen Rambow, Vinodkumar Prabhakaran, and Weiwei Guo. 2009. Committed belief annotation and tagging. In Proceedings of the Third Linguistic Annotation Workshop, pages 68–73, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lance Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<title>The automatic content extraction (ace) program–tasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04),</booktitle>
<pages>837--840</pages>
<contexts>
<context position="2184" citStr="Doddington et al., 2004" startWordPosition="311" endWordPosition="314">y/George Washington University, the Florida Institute for Human and Machine Cognition, and the University of Albany. The goal of our research project is not linguistic annotation, but the identification of meaning which is expressed in a non-linguistic manner. Such a meaning representation is useful for many applications; in our project we are specifically interested in 82 knowledge base population. A different part of the DEFT program is concerned with the representation of propositional meaning, following the tradition of the ACE program in representing entities, relations and events (ERE) (Doddington et al., 2004). The work presented here is concerned with the attitude of agents towards propositional content: do the agents express a committed belief or a non-committed belief in the propositional content? Our work has several characteristics that set it apart from other work: we are interested in annotation which can be done fairly quickly; we are not interested in annotating linguistic elements (such as trigger words); and we are planning an integration with sentiment annotation. The structure of the paper is as follows: we start out by situating our notion of “belief” with respect to other notions of </context>
<context position="30058" citStr="Doddington et al., 2004" startWordPosition="5013" endWordPosition="5016">d of the clause which describes the proposition (which is equivalent to a text span under syntactic projection). We are investigating extending this in several manners. First, we are considering including the heads of event noun phrases (the sudden collapse of the building). Second, we are looking at using a semantic representation for the proposition (as opposed the syntactic head of the text passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can </context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extraction (ace) program–tasks, data, and evaluation. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04), pages 837–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The conll-2010 shared task: Learning to detect hedges and their scope in natural language text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Farkas, Vincze, M´ora, Csirik, Szarvas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The conll-2010 shared task: Learning to detect hedges and their scope in natural language text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 1–12, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Gonz´alez-Ib´a˜nez</author>
<author>Smaranda Muresan</author>
<author>Nina Wacholder</author>
</authors>
<title>Identifying sarcasm in twitter: A closer look.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>581--586</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>Gonz´alez-Ib´a˜nez, Muresan, Wacholder, 2011</marker>
<rawString>Roberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and Nina Wacholder. 2011. Identifying sarcasm in twitter: A closer look. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 581–586, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Kaufmann</author>
<author>Cleo Condoravdi</author>
<author>Valentina Harizanov</author>
</authors>
<title>Formal Approaches to Modality,</title>
<date>2006</date>
<pages>72--106</pages>
<note>Mouton de Gruyter.</note>
<contexts>
<context position="9670" citStr="Kaufmann et al., 2006" startWordPosition="1558" endWordPosition="1561">ncludes a propositional hedge (think). Propositional hedges indicate non-committed belief. There has been a major effort to annotate texts with hedging information (Farkas et al., 2010), with an open evaluation. While belief and hedging are closely related, we see the belief/factuality annotation as more general than hedging (since it does not only include non-committed belief), and also more semantic (since we are not identifying language use but underlying meaning). The term “modality” is used in formal semantics as well as in descriptive linguistics. Many semanticists (e.g. (Kratzer, 1991; Kaufmann et al., 2006)) define modality as quantification over possible worlds. Modality can be of two types: epistemic, which qualifies the speaker’s commitment, and deontic, which concerns freedom to act. Belief/factuality falls under epistemic modality. Another view of modality relates more to a speaker’s attitude toward a proposition (e.g. (McShane et al., 2004; Baker et al., 2010; Prabhakaran et al., 2012)), which is closer to the way we model belief. We interpret the term “veridical” as referring to a property of certain words (usually verbs), namely to mark the proposition expressed by their syntactic comple</context>
</contexts>
<marker>Kaufmann, Condoravdi, Harizanov, 2006</marker>
<rawString>Stefan Kaufmann, Cleo Condoravdi, and Valentina Harizanov, 2006. Formal Approaches to Modality, pages 72–106. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
<author>Mitch Marcus</author>
</authors>
<title>Adding semantic annotation to the Penn TreeBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="30387" citStr="Kingsbury et al., 2002" startWordPosition="5067" endWordPosition="5070">ion for the proposition (as opposed the syntactic head of the text passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about sp</context>
</contexts>
<marker>Kingsbury, Palmer, Marcus, 2002</marker>
<rawString>Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002. Adding semantic annotation to the Penn TreeBank. In Proceedings of the Human Language Technology Conference, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kiparsky</author>
<author>Carol Kiparsky</author>
</authors>
<date>1970</date>
<booktitle>Progress in Linguistics,</booktitle>
<pages>143--173</pages>
<editor>Facts. In Manfred Bierwisch and Karl Erich Heidolph, editors,</editor>
<location>Mouton, The Hague, Paris.</location>
<contexts>
<context position="10363" citStr="Kiparsky and Kiparsky, 1970" startWordPosition="1668" endWordPosition="1671">y can be of two types: epistemic, which qualifies the speaker’s commitment, and deontic, which concerns freedom to act. Belief/factuality falls under epistemic modality. Another view of modality relates more to a speaker’s attitude toward a proposition (e.g. (McShane et al., 2004; Baker et al., 2010; Prabhakaran et al., 2012)), which is closer to the way we model belief. We interpret the term “veridical” as referring to a property of certain words (usually verbs), namely to mark the proposition expressed by their syntactic complement clause as firmly believed (committed belief) by the writer (Kiparsky and Kiparsky, 1970). Veridicality as a property of lexical or lexico-syntactic elements is thus a way of relating belief/factuality to linguistic means of expressing them, but we take the notion of belief/factuality as being the underlying notion. 3 Annotation 3.1 Annotation Manual The purpose of this annotation is to capture the commitment of the writer’s belief in the propositions expressed in the text. The annotation for this project marks beliefs held by the writer only. We exhaustively annotate all (clausal) propositions in each document with a four-way belief type distinction, with the following categories</context>
</contexts>
<marker>Kiparsky, Kiparsky, 1970</marker>
<rawString>Paul Kiparsky and Carol Kiparsky. 1970. Facts. In Manfred Bierwisch and Karl Erich Heidolph, editors, Progress in Linguistics, pages 143–173. Mouton, The Hague, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelika Kratzer</author>
</authors>
<date>1991</date>
<booktitle>Modality. In Arnim von Stechow</booktitle>
<editor>and Dieter Wunderlich, editors,</editor>
<location>Berlin.</location>
<contexts>
<context position="9646" citStr="Kratzer, 1991" startWordPosition="1556" endWordPosition="1557"> ball is blue includes a propositional hedge (think). Propositional hedges indicate non-committed belief. There has been a major effort to annotate texts with hedging information (Farkas et al., 2010), with an open evaluation. While belief and hedging are closely related, we see the belief/factuality annotation as more general than hedging (since it does not only include non-committed belief), and also more semantic (since we are not identifying language use but underlying meaning). The term “modality” is used in formal semantics as well as in descriptive linguistics. Many semanticists (e.g. (Kratzer, 1991; Kaufmann et al., 2006)) define modality as quantification over possible worlds. Modality can be of two types: epistemic, which qualifies the speaker’s commitment, and deontic, which concerns freedom to act. Belief/factuality falls under epistemic modality. Another view of modality relates more to a speaker’s attitude toward a proposition (e.g. (McShane et al., 2004; Baker et al., 2010; Prabhakaran et al., 2012)), which is closer to the way we model belief. We interpret the term “veridical” as referring to a property of certain words (usually verbs), namely to mark the proposition expressed b</context>
</contexts>
<marker>Kratzer, 1991</marker>
<rawString>Angelika Kratzer. 1991. Modality. In Arnim von Stechow and Dieter Wunderlich, editors, Semantics: An International Handbook of Contemporary Research. Walter de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marjorie McShane</author>
<author>Sergei Nirenburg</author>
<author>Ron Zacharsky</author>
</authors>
<title>Mood and modality: Out of the theory and into the fray.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="10015" citStr="McShane et al., 2004" startWordPosition="1611" endWordPosition="1614">nly include non-committed belief), and also more semantic (since we are not identifying language use but underlying meaning). The term “modality” is used in formal semantics as well as in descriptive linguistics. Many semanticists (e.g. (Kratzer, 1991; Kaufmann et al., 2006)) define modality as quantification over possible worlds. Modality can be of two types: epistemic, which qualifies the speaker’s commitment, and deontic, which concerns freedom to act. Belief/factuality falls under epistemic modality. Another view of modality relates more to a speaker’s attitude toward a proposition (e.g. (McShane et al., 2004; Baker et al., 2010; Prabhakaran et al., 2012)), which is closer to the way we model belief. We interpret the term “veridical” as referring to a property of certain words (usually verbs), namely to mark the proposition expressed by their syntactic complement clause as firmly believed (committed belief) by the writer (Kiparsky and Kiparsky, 1970). Veridicality as a property of lexical or lexico-syntactic elements is thus a way of relating belief/factuality to linguistic means of expressing them, but we take the notion of belief/factuality as being the underlying notion. 3 Annotation 3.1 Annota</context>
</contexts>
<marker>McShane, Nirenburg, Zacharsky, 2004</marker>
<rawString>Marjorie McShane, Sergei Nirenburg, and Ron Zacharsky. 2004. Mood and modality: Out of the theory and into the fray. Natural Language Engineering, 19(1):57–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carlo Strapparava</author>
</authors>
<title>The lie detector: Explorations in the automatic recognition of deceptive language.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACLIJCNLP 2009 Conference Short Papers,</booktitle>
<pages>309--312</pages>
<institution>Suntec, Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="6506" citStr="Mihalcea and Strapparava, 2009" startWordPosition="1023" endWordPosition="1026">ond, we could assume that the writer knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Sauriand Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumptio</context>
</contexts>
<marker>Mihalcea, Strapparava, 2009</marker>
<rawString>Rada Mihalcea and Carlo Strapparava. 2009. The lie detector: Explorations in the automatic recognition of deceptive language. In Proceedings of the ACLIJCNLP 2009 Conference Short Papers, pages 309– 312, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Jeffrey T Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>309--319</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6525" citStr="Ott et al., 2011" startWordPosition="1027" endWordPosition="1030">iter knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Sauriand Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumption may be false in c</context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Hancock. 2011. Finding deceptive opinion spam by any stretch of the imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 309–319, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
<author>Mona Diab</author>
</authors>
<title>Automatic committed belief tagging.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>1014--1022</pages>
<location>Beijing, China,</location>
<contexts>
<context position="21569" citStr="Prabhakaran et al., 2010" startWordPosition="3569" endWordPosition="3572">on-committed belief in the annotations, the heuristic rules (mainly based on the presence of modal auxiliaries) that we added for the purpose of classifying the beliefs (CB, NCB, ROB, NA) did not work reliably in all cases. 4.3 System C System C uses a supervised learning approach to identify tokens denoting the heads of propositions that denote author’s expressed beliefs. It approaches this problem as a 5-way (CB, NCB, ROB, NA, nil) multi-class classification task at the word level. System C is adapted from a previous system which uses an earlier, simpler definition and annotation of belief (Prabhakaran et al., 2010). The system uses lexical and syntactic features for this task, which are extracted using the part-of-speech tags and dependency parses obtained from the Stanford CoreNLP system. In addition to the features described in (Prabhakaran et al., 2010), System C uses a set of new features including features based on a dictionary of hedge-words (Prokofieva and Hirschberg, 2014). The hedge features improved the NCB Fmeasure by around 2.2 percentage points (an overall F-measure improvement of 0.25 percentage points) in experiments conducted on a separate development set. It uses a quadratic kernel SVM </context>
</contexts>
<marker>Prabhakaran, Rambow, Diab, 2010</marker>
<rawString>Vinodkumar Prabhakaran, Owen Rambow, and Mona Diab. 2010. Automatic committed belief tagging. In Coling 2010: Posters, pages 1014–1022, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Michael Bloodgood</author>
<author>Mona Diab</author>
<author>Bonnie Dorr</author>
<author>Lori Levin</author>
<author>Christine D Piatko</author>
<author>Owen Rambow</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Statistical modality tagging from rule-based annotations and crowdsourcing.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,</booktitle>
<pages>57--64</pages>
<institution>of Korea, July. Association for Computational Linguistics.</institution>
<location>Jeju, Republic</location>
<marker>Prabhakaran, Bloodgood, Diab, Dorr, Levin, Piatko, Rambow, Van Durme, 2012</marker>
<rawString>Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen Rambow, and Benjamin Van Durme. 2012. Statistical modality tagging from rule-based annotations and crowdsourcing. In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics, pages 57–64, Jeju, Republic of Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Prokofieva</author>
<author>Julia Hirschberg</author>
</authors>
<title>Hedging and speaker commitment.</title>
<date>2014</date>
<booktitle>In 5th Intl. Workshop on Emotion, Social Signals, Sentiment &amp; Linked Open Data,</booktitle>
<location>Reykjavik, Iceland.</location>
<contexts>
<context position="21942" citStr="Prokofieva and Hirschberg, 2014" startWordPosition="3629" endWordPosition="3632"> beliefs. It approaches this problem as a 5-way (CB, NCB, ROB, NA, nil) multi-class classification task at the word level. System C is adapted from a previous system which uses an earlier, simpler definition and annotation of belief (Prabhakaran et al., 2010). The system uses lexical and syntactic features for this task, which are extracted using the part-of-speech tags and dependency parses obtained from the Stanford CoreNLP system. In addition to the features described in (Prabhakaran et al., 2010), System C uses a set of new features including features based on a dictionary of hedge-words (Prokofieva and Hirschberg, 2014). The hedge features improved the NCB Fmeasure by around 2.2 percentage points (an overall F-measure improvement of 0.25 percentage points) in experiments conducted on a separate development set. It uses a quadratic kernel SVM to build the model, which outperformed the linear kernel in experiments conducted on the development set. 5 Proposition-Oriented Evaluation We now describe the results obtained on a proposition-oriented quantitative evaluation of these systems. We focus on a system’s ability to correctly identify the propositional heads of each type of belief (CB, NCB, ROB, NA). Only the</context>
</contexts>
<marker>Prokofieva, Hirschberg, 2014</marker>
<rawString>Anna Prokofieva and Julia Hirschberg. 2014. Hedging and speaker commitment. In 5th Intl. Workshop on Emotion, Social Signals, Sentiment &amp; Linked Open Data, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur´ı</author>
<author>James Pustejovsky</author>
</authors>
<title>Are you sure that this happened? assessing the factuality degree of events in text.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>2</issue>
<pages>299</pages>
<marker>Saur´ı, Pustejovsky, 2012</marker>
<rawString>Roser Saur´ı and James Pustejovsky. 2012. Are you sure that this happened? assessing the factuality degree of events in text. Computational Linguistics, 38(2):261– 299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samira Shaikh</author>
<author>Rob Giarrusso</author>
<author>Veena Ravishankar</author>
<author>Tomek Strzalkowski</author>
</authors>
<title>The SUNY Albany Sentiment Slot Filling System.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 TAC KBP Sentiment Slot Filling Evaluation,</booktitle>
<location>Gaithersburg, Maryland, USA. NIST.</location>
<contexts>
<context position="19119" citStr="Shaikh et al., 2014" startWordPosition="3140" endWordPosition="3143">tems We conducted a multi-site pilot evaluation for the task of identifying beliefs expressed in text. Three performer sites took part in this evaluation. In this section, we briefly describe the systems built at these performer sites. The first two systems are rule-based systems, whereas the third system is a supervised learning system. We limit the discussion of these systems to a high level, postponing the detailed system descriptions to separate future publications. 4.1 System A System A is adapted from a Sentiment Slot Filling system which participated in the 2014 TAC KBP SSF Evaluation (Shaikh et al., 2014). This system uses the Stanford Parser to create a syntactic dependency structure for every sentence in a given document. Using the dependency tree, it extracts the belief targets, which are usually the subjects of the sentence. In addition, the system extracts belief relations – a unary or binary predicate – typically a verb, an adjective or a noun. The focus of this version of System A is to identify propositional heads that express belief of any type. Each relation so extracted was initially marked as CB. A few heuristics were then applied to distinguish CBs from NCBs - such as presence of </context>
</contexts>
<marker>Shaikh, Giarrusso, Ravishankar, Strzalkowski, 2014</marker>
<rawString>Samira Shaikh, Rob Giarrusso, Veena Ravishankar, and Tomek Strzalkowski. 2014. The SUNY Albany Sentiment Slot Filling System. In Proceedings of the 2014 TAC KBP Sentiment Slot Filling Evaluation, Gaithersburg, Maryland, USA. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory J Werner</author>
<author>Vinodkumar Prabhakaran</author>
<author>Mona Diab</author>
<author>Owen Rambow</author>
</authors>
<title>Committed belief tagging on the factbank and lu corpora: A comparative study.</title>
<date>2015</date>
<booktitle>In Proceedings of the Workshop on ExtraPropositional Aspects of Meaning in Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, USA,</location>
<contexts>
<context position="15829" citStr="Werner et al. (2015)" startWordPosition="2604" endWordPosition="2607">ore, the FactBank annotation is basically compatible with ours. Our annotation is much simpler than that of FactBank in order to allow for a quicker annotation. We summarize the main points of simplification here. • We have taken the source always to be the writer. As we will discuss in Section 7.1, we will adopt the FactBank annotation in the next iteration of our annotation. • We do not distinguish between possible and probable; this distinction may be hard to annotate and not too valuable. • We ignore negation. If present, we simply assume it is part of the proposition which is the target. Werner et al. (2015) study the relation between belief and factuality in more detail. They provide an automatic way of mapping the annotations in FactBank to the 4-way distinction of speaker/writer’s belief that we present in this paper. 3.3 Corpus and Annotation Results The annotation effort for this phase of belief annotation for DEFT produced a training corpus of 852,836 words and an evaluation corpus of 100,037 words. All annotated data consisted of English text from discussion forum threads. The discussion forum 85 threads were originally collected for the DARPA BOLT program, and were harvested from a wide v</context>
</contexts>
<marker>Werner, Prabhakaran, Diab, Rambow, 2015</marker>
<rawString>Gregory J. Werner, Vinodkumar Prabhakaran, Mona Diab, and Owen Rambow. 2015. Committed belief tagging on the factbank and lu corpora: A comparative study. In Proceedings of the Workshop on ExtraPropositional Aspects of Meaning in Computational Linguistics, Denver, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language ann. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="28547" citStr="Wiebe et al., 2005" startWordPosition="4765" endWordPosition="4768">to the span of the annotated see for System C, but System A deemed the ‘see’ belief not to be about it. We conclude that this purely syntactic definition of “belief about an entity” is not satisfactory. The definition of “belief about an entity” remains an open question and we return to it in Section 7.3. 7 Plans for Next Round 7.1 Adding the Source Currently, we are only annotating and evaluating the writer’s beliefs. Beliefs attributed by the writer to other sources are marked ROB. We intend to annotate the source for all beliefs, using the method of 88 nested attribution pioneered by MPQA (Wiebe et al., 2005) and adopted by FactBank (Saur´ı and Pustejovsky, 2012). Consider the following sentence. (10) John believes Mary knows that the clock was stolen In the nested attribution approach, according to the writer, according to John, Mary firmly believes (CB) the ‘steal’ proposition. According to the writer, John firmly believes the ‘know’ proposition and the ‘steal’ proposition (as indicated by the veridical verb know). The writer herself firmly believes (CB) the ‘believe’ proposition, does not express an opinion on the ‘know’ proposition (ROB), and also firmly believes (CB) the ‘steal’ proposition (</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language ann. Language Resources and Evaluation, 39(2/3):164–210.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>