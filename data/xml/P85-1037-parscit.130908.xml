<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000341">
<note confidence="0.4702004">
EXTRACTING SEMANTIC HIERARCHIES FROM A LARGE ON-LINE DICTIONARY
Martin S. Chodorow
Department of Psychology, Hunter College of CUNY
and
I.B.M. Thomas J. Watson Research Center
</note>
<author confidence="0.709749">
Yorktown Heights, New York 10598
Roy J. Byrd
George E. Heidorn
I.B.M. Thomas J. Watson Research Center
Yorktown Heights, New York 10598
</author>
<sectionHeader confidence="0.947435" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999532125">
Dictionaries are rich sources of detailed semantic infor-
mation, but in order to use the information for natural
language processing, it must be organized systematically.
This paper describes automatic and semi-automatic
procedures for extracting and organizing semantic fea-
ture information implicit in dictionary definitions. Two
head-finding heuristics are described for locating the
genus terms in noun and verb definitions. The assump-
tion is that the genus term represents inherent features
of the word it defines. The two heuristics have been
used to process definitions of 40,000 nouns and 8,000
verbs, producing indexes in which each genus term is
associated with the words it defined. The Sprout pro-
gram interactively grows a taxonomic &amp;quot;tree&amp;quot; from any
specified root feature by consulting the genus index. Its
output is a tree in which all of the nodes have the root
feature for at least one of their senses. The Filter pro-
gram uses an inverted form of the genus index. Filtering
begins with an initial filter file consisting of words that
have a given feature (e.g. [+ human)) in all of their
senses. The program then locates, in the index, words
whose genus terms all appear in the filter file. The out-
put is a list of new words that have the given feature in
all of their senses.
</bodyText>
<sectionHeader confidence="0.993758" genericHeader="keywords">
1. Introduction.
</sectionHeader>
<bodyText confidence="0.999881115384615">
The goal of this research is to extract semantic informa-
tion from standard dictionary definitions, for use in
constructing lexicons for natural language processing
systems. Although dictionaries contain finely detailed
semantic knowledge, the systematic organization of that
knowledge has not heretofore been exploited in such a
way as to make the information available for computer
applications.
Amsler(1980) demonstrates that additional structure
can be imposed upon a dictionary by making certain as-
sumptions about the ways in which definitions are con-
structed. Foremost among these assumptions is that
definitions consist of a &amp;quot;genus&amp;quot; term, which identifies
the superordinate concept of the defined word, and
&amp;quot;differentia&amp;quot; which distinguish this instance of the
superordinate category from other instances. By manu-
ally extracting and disambiguating genus terms for a
pocket dictionary, Amster demonstrated the feasibility
of generating semantic hierarchies.
It was our goal to automate the genus extraction and
disambiguation processes so that semantic hierarchies
could be generated from full-sized dictionaries. The
fully automatic genus extraction process is described in
Section 2. Sections 3 and 4 describe two different
disambiguation and hierarchy-extraction techniques that
rely on the genus information. Both of these techniques
</bodyText>
<page confidence="0.996861">
299
</page>
<bodyText confidence="0.996829214285714">
are semi-automatic, since they crucially require decisions
. to be made by a human user during processing. Never-
theless, significant savings occur when the system or-
ganizes the presentation of material to the user. Further
economy results from the automatic access to word de-
finitions contained in the on-line dictionary from which
the genus terms were extracted.
The information extracted using the techniques we have
developed will initially be used to add semantic infor-
mation to entries in the lexicons accessed by various
natural language processing programs developed as part
of the EPISTLE project at IBM. Descriptions of some
of these programs may be found in Heidorn, et al.
(1982), and Byrd and McCord( 1985).
</bodyText>
<sectionHeader confidence="0.94634" genericHeader="introduction">
2. Head finding.
</sectionHeader>
<bodyText confidence="0.91316925">
In the definition of car given in Figure 1, and repeated
here:
car: a vehicle moving on wheels.
the word vehicle serves as the genus term, while moving
on wheels differentiates cars from some other types of
vehicles. Taken as an ensemble, all of the word/genus
pairs contained in a normal dictionary for words of a
given part-of-speech form what Amsler(1980) calls a
&amp;quot;tangled hierarchy&amp;quot;. In this hierarchy, each word would
constitute a node whose subordinate nodes are words
for which it serves as a genus term. The words at those
subordinate nodes are called the Word&apos;s &amp;quot;hyponyms&amp;quot;.
Similarly, the words at the superordinate nodes for a
given word are the genus terms for the various sense
definitions of that word. These are called the given
word&apos;s &amp;quot;hypernyms&amp;quot;. Because words are ambiguous
(i.e.. have multiple senses), any word may have multiple
hypernyms; hence the hierarchy is &amp;quot;tangled&amp;quot;.
vehicle: (n) (often attrib) an inert medium in
which a medicinally active agent is ad-
ministered
vehicle: (n) any of various other media acting usu.
as solvents, carriers, or binders for ac-
tive ingredients or pigments
</bodyText>
<listItem confidence="0.748206666666667">
vehicle: (n) an agent of transmission: CARRIER
vehicle: (n) a medium through which something
is expressed, achieved, or displayed
vehicle: (n) a means of carrying or transporting
something: CONVEYANCE
vehicle: (n) a piece of mechanized equipment
</listItem>
<bodyText confidence="0.746902583333333">
ambulance: (n) a vehicle equipped for transport-
ing wounded, injured, or sick persons
or animals
bicycle: (n) a vehicle with two wheels tandem, a
steering handle, a saddle seat, and
pedals by which it is propelled
car: (n) a vehicle moving on wheels
tanker: (n) a cargo boat fitted with tanks for car-
rying liquid in bulk
tanker: (n) a vehicle on which a tank is mounted
to carry liquids; also: a cargo airplane
for transporting fuel
</bodyText>
<figureCaption confidence="0.989559">
Figure I. Selected dictionary definitions.
</figureCaption>
<bodyText confidence="0.939479">
Figure I shows selected definitions from Webster&apos;s Sev-
enth New Collegiate Dictionary for vehicle and a few re-
lated words. In each definition, the genus term has been
italicized. Figure 2 shows the small segment of the tan-
gled hierarchy based on those definitions, with the
hyponyms and hypernyms of vehicle labelled.
Our automated mechanism for finding the genus terms
is based on the observation that the genus term for verb
and noun definitions is typically the head of the defining
phrase. This reduces the task to that of finding the
heads of verb phrases and noun phrases.
</bodyText>
<page confidence="0.982432">
300
</page>
<figure confidence="0.3725985">
medium means agent equipment
HYPERNYMS
</figure>
<figureCaption confidence="0.997542">
Figure 2. The tangled hierarchy around &amp;quot;vehicle&amp;quot;.
</figureCaption>
<bodyText confidence="0.997720352941176">
The syntax of the verb phrase used in verb definitions
makes it possible to locate its head with a simple
heuristic: the head is the single verb following the word
to. If there is a conjunction of verbs following to, then
they are all heads. Thus, given the following two defi-
nitions for winter,
winter: (v) to pass the winter
winter: (v) to keep, feed, or manage during the winter
the heuristic would find four heads: pass, keep, feed, and
manage.
Applying this heuristic to the definitions for the 8,000
verbs that have definitions in Webster&apos;s Seventh showed
that 2225 distinct verbs were used as heads of defi-
nitions and that they were used 24,000 times. In other
words, each genus term served as the hypernym for ten
other verbs, on average. The accuracy of head finding
for verbs was virtually 100 percent.
Head finding is much more complex for noun definitions
because of their greater variety. At the same time, the
magnitude of the task (over 80,000 defining noun
phrases) demanded that we use a heuristic procedure,
rather than a full parser, which would have been pro-
hibitively expensive. We were able to take advantage
of the fact that dictionary definitions are written in a
special and predictable style, and that their analysis does
not require the full power of an analyzer for general
English.
The procedure used may be briefly described as follows.
First the substring of the definition which must contain
the head is found. This substring is bounded on the left
by a word which obligatorily appears in prenominal po-
sition: a, an, the, its, two, three, ..., twelve, first, second,
... It is bounded on the right by a word or sequence that
can only appear in postnominal position:
</bodyText>
<listItem confidence="0.990144714285714">
• a relative pronoun (introducing a relative clause)
• a preposition not followed by a conjunction (thus.
introducing a complement to the head noun)
• a preposition-conjunction-preposition configuration
(also introducing a complement)
• a present participle following a noun (thus, intro-
ducing a reduced relative clause)
</listItem>
<bodyText confidence="0.999879533333333">
The heuristic for finding the boundary on the right
works because of certain restrictions on constituents
appearing within a noun phrase. Emonds (1976, pp.
167-172) notes that an adjective phrase or a verb phrase
must end with its phrasal head if it appears to the left of
the head noun in a noun phrase. For example, in the very
old man, the adjective phrase very old has its head ad-
jective in final position: in the quietly sleeping children,
the verb phrase quietly sleeping ends in its head verb.
Another constraint, the Surface Recursion Restriction
(Emonds, 1976, p. 19), prohibits free recursion of a
node appearing within a phrase, to the left of the phrase
head. This prevents prenominal modifying phrases from
containing S and PP nodes. Taken together, the two
restrictions specify that S, PP, and any other constituent
which does not end in its head-of-phrase element cannot
appear as a prenominal modifier and must, therefore, be
postnominal. Lexical items or sequences that mark the
beginnings of these constituents are used by the heuristic
to establish the right boundary of the substring which
must contain the head of the noun definition.
Once the substring is isolated, the search for the head
begins. Typically, but not always, it is the rightmost
noun in the substring. If however, the substring contains
a conjunction, each conjunct is processed separately,
and multiple heads may result. If the word found be-
longs to a small class of &amp;quot;empty heads&amp;quot; (words like one.
any, kind, class, manner, family, race, group, complex,
etc.) and is followed by of, then the string following of
is reprocessed in an effort to locate additional heads.
</bodyText>
<figure confidence="0.97841225">
vehicle boat airplane
\
ambulance bicycle car tanker
HYPONYMS:
</figure>
<page confidence="0.996509">
301
</page>
<bodyText confidence="0.99996975">
Applying this procedure to the definitions for the 40,000
defined nouns in Webster&apos;s Seventh showed that 10,000
distinct nouns were used as heads of definitions and that
they were used 85,000 times. In other words, each
genus term served as the hypernym for 8.5 other verbs,
on average. The accuracy of head-finding for nouns was
approximately 98 percent, based on a random sample
of the output.
</bodyText>
<sectionHeader confidence="0.963957" genericHeader="method">
3. Sprouting
</sectionHeader>
<bodyText confidence="0.999262543478261">
Sprouting, which derives its name from the action of
growing a semantic tree from a specified root, uses the
results of head-finding as its raw material. This infor-
mation is organized into a &amp;quot;hyponym index&amp;quot;, in which
each word that was used as a genus term is associated
with all of its hyponyms. Thus. &amp;quot;vehicle&amp;quot; would have
an entry which reads (in part):
vehicle: ambulance ... bicycle ... car ... tanker ...
For a given part-of-speech, the hyponym index needs to
be built only once.
When invoking the sprouting process, the user selects a
root from which a semantic tree is to be grown. The
system then computes the transitive closure over the
hyponym index, beginning at the chosen root. In effect,
for each new word (including the root), all of its
hyponyms are added to the tree. This operation is ap-
plied recursively, until no further new words are found.
The interactiveness of the sprouting process results from
the fact that the user is consulted for each new word.
If he decides that that word does not belong to the tree
being grown, he may prune it (and the branches that
would emerge from it). These pruning decisions result
in the disambiguation of the tree. The user is assisted in
making such decisions by having available an on-line
version of Webster&apos;s Seventh, in which he may review
the definitions, usage notes, etc. for any words of which
he is unsure.
The output of a sprouting session, then, is a
disambiguated tree extracted from the tangled hierarchy
represented by the hyponym index. Actually, the output
more nearly resembles a bush since it is usually shallow
(typically only 3 to 4 levels deep) and very wide. For
example, a tree grown from vehicle had 75 direct de-
scendants from the root, and contained over 250 nodes
in its first two levels, alone. The important aspect of the
output, therefore, is not its structure, but rather the fact
that the words it contains all have at least one sense
which bears the property for which the root was ori-
ginally selected. It is important to note that any serious
use of sprouting to locate all words bearing a particular
semantic feature must involve the careful selection and
use of several roots, because of the variety of genus
terms employed by the Webster&apos;s lexicographers. For
example, if it were desired to find all nouns which bear
the k-female1 inherent feature, sprouts should at least
be begun from female, woman, girl, and even wife.
</bodyText>
<sectionHeader confidence="0.963282" genericHeader="method">
4. Filtering
</sectionHeader>
<bodyText confidence="0.999377043478261">
Filtering, like sprouting, results in lists of words bearing
a certain property (e.g., [+human1). Unlike sprouting,
however, filtering only picks up words all of whose
senses have the property.
It is based on a &amp;quot;hypernym index&amp;quot; (the inversion of the
hyponym index), in which each word is listed with its
hypernyms. as in the example given here:
vehicle: agent equipment means medium
The filtering process begins with a &amp;quot;seed filter&amp;quot; consist-
ing of an initial set of words all of whose senses bear
some required property. The seed filter may be obtained
in any manner that is convenient. In our work, this may
be either from the semantic codes assigned to words by
the Longman Dictionary of Contemporary English, or
from morphological processing of word lists, as de-
scribed in Byrd and McCord (1985). For example,
morphological analysis of words ending in -man, -sman.
-cc, -er, and -ist constitute a rich source of [ +human]
nouns. Given the filter, the system uses it to evaluate
all of the words in the hypernym index. Any words, all
of whose hypernyms are already in the filter, become
candidates for inclusion in the filter during the next pass.
The user is consulted for each candidate, and may accept
</bodyText>
<page confidence="0.993599">
302
</page>
<table confidence="0.9879173">
Pass# Filter Size New Words
1 2539* 1091
2 4113** 234
3 4347 43
4 4390 0
5 4661*** 49
Total 4710
* Obtained from Longman Dictionary of Contempory English
** Includes 483 new words from morphological analysis
•** Includes 271 new words from morphological analysis
</table>
<figureCaption confidence="0.996773">
Figure 3. A Filtering of [+ human] nouns.
</figureCaption>
<bodyText confidence="0.99889528">
or reject it. Finally, all accepted words are added to the
filter, and the process is repeated until it converges.
An example of the filtering procedure applied to nouns
bearing the [+ human] inherent feature is given in Figure
3. It can be seen that the process converges fairly
quickly, and that it is fairly productive, yielding, in this
case, an almost two-for-one return on the size of the in-
itial filter. For nouns with the (-human] inherent fea-
ture, an initial filter of 22,000 words yielded 11,600 new
words on the first pass, with a false alarm rate of less
than 196 based on a random sample of the output. From
an initial filter of 15 [+ time] nouns, 300 additional ones
were obtained after three passes through the filter.
These examples demonstrate another important fact
about filtering: that it can be used to project the seman-
tic information available from a smaller, more manage-
able source such as a learner&apos;s dictionary onto the larger
set of words obtained from a collegiate sized dictionary.
• As does sprouting, filtering also produces a list of words
having some desired property. In this case, however, the
resulting words have the property in all of their senses.
This type of result is useful in a parsing system, such as
the one described in Heidorn, et al. (1982), in which it
may be necessary to know whether a noun must refer to
a human being, not merely that it may refer to one.
</bodyText>
<sectionHeader confidence="0.998748" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.999646142857143">
This work receives its primary motivation from the de-
sire to build natural language processing systems capable
of processing unrestricted English input. As we emerge
from the days when hand-built lexicons of several hun-
dred to a few thousand entries were sufficient, we need
to explore ways of easing the lexicon construction task.
Fortunately, the tools required to do the job are becom-
ing available at the same time. Primary among them are
machine readable dictionaries, such as Webster&apos;s and
Longman, which contain the raw material for analysis.
Software tools for dictionary analysis, such as those de-
scribed here and in Calzolari (1982), are also gradually
emerging. With experience, and enhanced understand-
ing of the information structure in published diction-
</bodyText>
<page confidence="0.997606">
303
</page>
<bodyText confidence="0.729056">
Calzolari, N. (1984), &amp;quot;Detecting Patterns in a Lexical
Data Base,&amp;quot; Proceedings of COLING/ACL-1984
aries, we expect to achieve some success in the
automated construction of lexicons for natural language
processing systems.
References Emonds, J.E. (1976), A Transformational Approach to
Amsler, R. A. (1980), The Structure of the Merriam- English Syntax. New York: Academic Press.
Webster Pocket Dictionary, Doctoral Dissertation, Heidom, G. E., K. Jensen, L. A. Miller, R. J. Byrd, and
TR-164, University of Texas, Austin. M. S. Chodorow (1982), &amp;quot;The EPISTLE Text-
Byrd, R. J. and M. C. McCord (1985), &amp;quot;The lexical base Critiquing System,&amp;quot; IBM Systems Journal, 21, 305-326.
for semantic interpretation in a Prolog parser&amp;quot; presented Longman Dictionary of Contemporary English (1978),
at the CUNY Workshop on the Lexicon, Parsing, and Longman Group Limited, London.
Semantic Interpretation, 18 January 1985. Webster&apos;s Seventh New Collegiate Dictionary (1963), G.
&amp; C. Merriam, Springfield, Massachusetts.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938189">
<title confidence="0.997494">EXTRACTING SEMANTIC HIERARCHIES FROM A LARGE ON-LINE DICTIONARY</title>
<author confidence="0.999977">Martin S Chodorow</author>
<affiliation confidence="0.996139">Department of Psychology, Hunter College of CUNY and I.B.M. Thomas J. Watson Research Center</affiliation>
<address confidence="0.977525">Yorktown Heights, New York 10598</address>
<author confidence="0.999836">Roy J Byrd George E Heidorn</author>
<affiliation confidence="0.999771">I.B.M. Thomas J. Watson Research Center</affiliation>
<address confidence="0.97942">Yorktown Heights, New York 10598</address>
<abstract confidence="0.99975256">Dictionaries are rich sources of detailed semantic information, but in order to use the information for natural language processing, it must be organized systematically. This paper describes automatic and semi-automatic procedures for extracting and organizing semantic feature information implicit in dictionary definitions. Two head-finding heuristics are described for locating the genus terms in noun and verb definitions. The assumption is that the genus term represents inherent features of the word it defines. The two heuristics have been used to process definitions of 40,000 nouns and 8,000 verbs, producing indexes in which each genus term is associated with the words it defined. The Sprout program interactively grows a taxonomic &amp;quot;tree&amp;quot; from any specified root feature by consulting the genus index. Its output is a tree in which all of the nodes have the root feature for at least one of their senses. The Filter program uses an inverted form of the genus index. Filtering begins with an initial filter file consisting of words that have a given feature (e.g. [+ human)) in all of their senses. The program then locates, in the index, words whose genus terms all appear in the filter file. The output is a list of new words that have the given feature in all of their senses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>