<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.980339">
Identifying Segment Topics in Medical Dictations
</title>
<author confidence="0.9854795">
Johannes Matiasek, Jeremy Jancsary
Alexandra Klein
</author>
<affiliation confidence="0.785224">
Austrian Research Institute for
Artificial Intelligence
</affiliation>
<address confidence="0.931794">
Freyung 6, Wien, Austria
</address>
<email confidence="0.997597">
firstname.lastname@ofai.at
</email>
<sectionHeader confidence="0.99386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929">
In this paper, we describe the use of lexi-
cal and semantic features for topic classi-
fication in dictated medical reports. First,
we employ SVM classification to assign
whole reports to coarse work-type cate-
gories. Afterwards, text segments and
their topic are identified in the output
of automatic speech recognition. This
is done by assigning work-type-specific
topic labels to each word based on fea-
tures extracted from a sliding context win-
dow, again using SVM classification uti-
lizing semantic features. Classifier stack-
ing is then used for a posteriori error cor-
rection, yielding a further improvement in
classification accuracy.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988834">
The use of automatic speech recognition (ASR) is
quite common in the medical domain, where for
every consultation or medical treatment a written
report has to be produced. Usually, these reports
are dictated and transcribed afterwards. The use of
ASR can, thereby, significantly reduce the typing
efforts, but, as can be seen in figure 1, quite some
work is left.
complaint dehydration weakness and diarrhea full
stop Mr. Will Shawn is a 81-year-old cold Asian
gentleman who came in with fever and Persian
diaper was sent to the emergency department by his
primary care physician due him being dehydrated
period ... neck physical exam general alert and
oriented times three known acute distress vital
signs are stable ... diagnosis is one chronic
diarrhea with hydration he also has hypokalemia
neck number thromboctopenia probably duty liver
cirrhosis ... a plan was discussed with patient in
detail will transfer him to a nurse and facility
for further care ... end of dictation
</bodyText>
<figureCaption confidence="0.995767">
Figure 1: Raw output of speech recognition
</figureCaption>
<bodyText confidence="0.999559333333333">
When properly edited and formatted, the same
dictation appears significantly more comprehensi-
ble, as can be seen in figure 2.
</bodyText>
<author confidence="0.869265">
Harald Trost
</author>
<affiliation confidence="0.98393175">
Department of Medical Cybernetics
and Artificial Intelligence
of the Center for Brain Research,
Medical University Vienna, Austria
</affiliation>
<email confidence="0.985378">
harald.trost@meduniwien.ac.at
</email>
<sectionHeader confidence="0.962651" genericHeader="introduction">
CHIEF COMPLAINT
</sectionHeader>
<keyword confidence="0.55109">
Dehydration, weakness and diarrhea.
</keyword>
<sectionHeader confidence="0.984102" genericHeader="method">
HISTORY OF PRESENT ILLNESS
</sectionHeader>
<bodyText confidence="0.924907666666667">
Mr. Wilson is a 81-year-old Caucasian gentleman
who came in here with fever and persistent
diarrhea. He was sent to the emergency department
by his primary care physician due to him being
dehydrated.
. . .
</bodyText>
<sectionHeader confidence="0.693126" genericHeader="method">
PHYSICAL EXAMINATION
</sectionHeader>
<bodyText confidence="0.417636">
GENERAL: He is alert and oriented times three,
not in acute distress.
</bodyText>
<note confidence="0.695764">
VITAL SIGNS: Stable.
</note>
<bodyText confidence="0.261473">
. . .
</bodyText>
<sectionHeader confidence="0.494598" genericHeader="method">
DIAGNOSIS
</sectionHeader>
<listItem confidence="0.8983406">
1. Chronic diarrhea with dehydration. He also
has hypokalemia.
2. Thromboctopenia, probably due to liver
cirrhosis.
. . .
</listItem>
<sectionHeader confidence="0.968508" genericHeader="method">
PLAN AND DISCUSSION
</sectionHeader>
<bodyText confidence="0.83335225">
The plan was discussed with the patient in detail.
Will transfer him to a nursing facility for
further care.
. . .
</bodyText>
<figureCaption confidence="0.998639">
Figure 2: A typical medical report
</figureCaption>
<bodyText confidence="0.9998578">
Besides the usual problem with recognition er-
rors, section headers are often not dictated or hard
to recognize as such. One task that has to be per-
formed in order to arrive at the structured report
shown in figure 2 is therefore to identify topical
sections in the text and to classify them accord-
ingly.
In the following, we first describe the problem
setup, the steps needed for data preparation, and
the division of the classification task into subprob-
lems. We then describe the experiments performed
and their results.
In the outlook we hint at ways to integrate this
approach with another, multilevel, segmentation
framework.
</bodyText>
<sectionHeader confidence="0.758343" genericHeader="method">
2 Data Description and Problem Setup
</sectionHeader>
<bodyText confidence="0.991106666666667">
Available corpus data consists of raw recognition
results and manually formatted and corrected re-
ports of medical dictations. 11462 reports were
</bodyText>
<subsubsectionHeader confidence="0.568434">
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 19–25,
</subsubsectionHeader>
<bodyText confidence="0.256569">
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.999277">
19
</page>
<bodyText confidence="0.99922575">
available in both forms, 51382 reports only as cor-
rected transcripts. When analysing the data, it
became clear that the structure of segment topics
varied strongly across different work-types. Thus
we decided to pursue a two-step approach: firstly
classify reports according to their work-type and,
secondly, train and apply work-type specific clas-
sification models for segment topic classification.
</bodyText>
<subsectionHeader confidence="0.97687">
2.1 Classification framework
</subsectionHeader>
<bodyText confidence="0.9981848">
For all classification tasks discussed here, we em-
ployed support-vector machines (SVM, Vapnik
(1995)) as the statistical framework, though in dif-
ferent incarnations and setups. SVMs have proven
to be an effective means for text categorization
(Joachims, 1998) as they are capable to robustly
deal with high-dimensional, sparse feature spaces.
Depending on the task, we experimented with dif-
ferent feature weighting schemes and SVM kernel
functions as will be described in section 3.
</bodyText>
<subsectionHeader confidence="0.997225">
2.2 Features used for classification
</subsectionHeader>
<bodyText confidence="0.83930937037037">
The usual approach in text categorization is to use
bag-of-word features, i.e. the words occuring in a
document are collected disregarding the order of
their appearance. In the domain of medical dic-
tation, however, often abbreviations or different
medical terms may be used to refer to the same se-
mantic concept. In addition, medical terms often
are multi-word expressions, e.g., “coronary heart
disease”. Therefore, a better approach for feature
mapping is needed to arrive at features at an ap-
propriate generalization level:
• Tokenization is performed using a large
finite-state lexicon including multi-word
medical concepts extracted from the UMLS
medical metathesaurus (Lindberg et al.,
1993). Thus, multi-word terms remain intact.
In addition, numeric quantities in special
(spoken or written) formats or together with a
dimension are mapped to semantic types (e.g.
“blood pressure” or “physical quantity”), also
using a finite-state transducer.
• The tokens are lemmatized and, if possi-
ble, replaced by the UMLS semantic con-
cept identifier(s) they map to. Thus,
“CHD”, “coronary disease” and “coronary
heart disease” all map to the same concept
“C0010068”.
</bodyText>
<listItem confidence="0.970143">
• In addition, also the UMLS semantic type, if
available, is used as a feature, so, in the ex-
ample above, “B2.2.1.2.1” (Disease or Syn-
drome) is added.
• Since topics in a medical report roughly fol-
low an order, for the segment topic identifica-
tion task also the relative position of a word
in the report (ranging from -1 to +1) is used.
We also explored different weighting schemes:
• binary: only the presence of a feature is in-
dicated.
• term frequency: the number of occurences
of a feature in the segment to be classified is
used as weight.
• TFIDF: a measure popular from information
retrieval, where tfidfi,j of term ti in docu-
ment dj E D is usually defined as
</listItem>
<equation confidence="0.7570615">
cti,j .log |D|
Ei cti,j |{dj : ti E dj}|
</equation>
<bodyText confidence="0.838502">
An example of how this feature extraction pro-
cess works is given below:
</bodyText>
<figure confidence="0.932949916666667">
token(s) feature(s) comment
...
an stop word
78 year old QH OLD pattern-based type
female C0085287 UMLS concept
A2.9.2 UMLS semtype
intubated intubate lemmatized (no concept)
with stop word
lung cancer C0242379 UMLS concept
C0684249 UMLS concept
B2.2.1.2.1.2 UMLS semtype
...
</figure>
<subsectionHeader confidence="0.996457">
2.3 Data Annotation
</subsectionHeader>
<bodyText confidence="0.999868">
For the first classification task, i.e. work-type clas-
sification, no further annotation is necessary, ev-
ery report in our data corpus had a label indicating
the work-type. For the segment topic classification
task, however, every token of the report had to be
assigned a topic label.
</bodyText>
<subsectionHeader confidence="0.995574">
2.3.1 Analysis of Corrected Transcripts
</subsectionHeader>
<bodyText confidence="0.999983571428571">
For the experiments described here, we con-
centrated on the “Consultations” work-type, for
which clear structuring recommendations, such
as E2184-02 (ASTM International, 2002), exist.
However, in practice the structure of medical re-
ports shows high variation and deviations from
the guidelines, making it harder to come up with
</bodyText>
<page confidence="0.967655">
20
</page>
<bodyText confidence="0.999981">
an appropriate set of class labels. Therefore, us-
ing the aforementioned standard, we assigned the
headings that actually appeared in the data to the
closest type, introducing new types only when ab-
solutely necessary. Thus we arrived at 23 heading
classes. Every (possibly multi-word) token was
then labeled with the heading class of the last sec-
tion heading occurring before it in the text using a
simple parser.
</bodyText>
<subsectionHeader confidence="0.91618">
2.3.2 Aligment and Label Transfer
</subsectionHeader>
<bodyText confidence="0.999105916666667">
When inspecting manually corrected reports (cf.
fig. 2), one can easily identify a heading and clas-
sify the topic of the text below it accordingly.
However, our goal is to develop a model for iden-
tifying and classifying segments in the dictation,
thus we have to map the annotation of corrected
reports onto the corresponding ASR output. The
basic idea here is to align the tokens of the cor-
rected report with the tokens in ASR output and to
copy the annotations (cf. figure 3). There are some
problems we have to take care of during align-
ment:
</bodyText>
<listItem confidence="0.937978">
1. non-dictated items in the corrected test (e.g.
punctuation, headings)
2. dictated words that do not occur in the cor-
rected text (meta instructions, repetitions)
3. non-identical but corresponding items
(recognition errors, reformulations)
</listItem>
<bodyText confidence="0.999822538461538">
For this alignment task, a standard string-edit
distance based method is not sufficient. There-
fore, we augment it with a more sophisticated cost
function. It assigns tokens that are similar (ei-
ther from a semantic or from a phonetic point of
view) a low cost for substitution, whereas dissimi-
lar tokens receive a prohibitively expensive score.
Costs for deletion and insertion are assigned in-
versely. Semantic similarity is computed using
Wordnet (Fellbaum, 1998) and UMLS. For pho-
netic matching, the Metaphone algorithm (Philips,
1990) was used (for details see Huber et al. (2006)
and Jancsary et al. (2007)).
</bodyText>
<sectionHeader confidence="0.999895" genericHeader="evaluation">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998459">
3.1 Work-Type Categorization
</subsectionHeader>
<bodyText confidence="0.999914">
In total we had 62844 written medical reports
with assigned work-type information from differ-
ent hospitals, 7 work-types are distinguished. We
randomly selected approximately a quarter of the
</bodyText>
<table confidence="0.9700787">
corrected report OP ASR output
... ... ... ... ...
ChiefCompl CHIEF del
ChiefCompl COMPLAINT sub complaint ChiefCompl
ChiefCompl Dehydration sub dehydration ChiefCompl
ChiefCompl , del weakness ChiefCompl
ChiefCompl weakness sub
ChiefCompl and sub and ChiefCompl
ChiefCompl diarrhea sub diarrhea ChiefCompl
ChiefCompl . sub fullstop ChiefCompl
HistoryOfP Mr. sub Mr. HistoryOfP
HistoryOfP Wilson sub Will HistoryOfP
ins Shawn HistoryOfP
HistoryOfP is sub is HistoryOfP
HistoryOfP a sub a HistoryOfP
HistoryOfP 81-year-old sub 81-year-old HistoryOfP
HistoryOfP Caucasian sub cold HistoryOfP
HistoryOfP ins Asian HistoryOfP
HistoryOfP gentleman sub gentleman HistoryOfP
HistoryOfP who sub who HistoryOfP
HistoryOfP came sub came HistoryOfP
HistoryOfP in del
HistoryOfP here sub here HistoryOfP
HistoryOfP with sub with HistoryOfP
HistoryOfP fever sub fever HistoryOfP
HistoryOfP and sub and HistoryOfP
HistoryOfP persistent sub Persian HistoryOfP
HistoryOfP diarrhea sub diaper HistoryOfP
HistoryOfP . del
... ... ... ... ...
</table>
<figureCaption confidence="0.978835">
Figure 3: Mapping labels via alignment
</figureCaption>
<bodyText confidence="0.706929333333333">
reports as the training set, the rest was used for
testing. The distribution of the data can be seen in
table 1.
</bodyText>
<table confidence="0.999845111111111">
Trainingset Testset Work-Type
649 4.1 1966 4.2 CA Cardiology
7965 51.0 24151 51.1 CL ClinicalReports
1867 11.9 5590 11.8 CN Consultations
1120 7.2 3319 7.0 DS DischargeSummaries
335 2.1 878 1.8 ER EmergencyMedicine
2185 14.0 6789 14.4 HP HistoryAndPhysicals
1496 9.6 4534 9.6 OR OperativeReports
15617 47227 Total
</table>
<tableCaption confidence="0.998526">
Table 1: Distribution of Work-types
</tableCaption>
<bodyText confidence="0.999858235294118">
As features for categorization, we used a bag-
of-words approach, but instead of the surface form
of every token of a report, we used its semantic
features as described in section 2.2. As a catego-
rization engine, we used LIBSVM (Chang&amp;Lin,
2001) with an RBF kernel. The features where
weighted with TFIDF. In order to compensate for
different document length, each feature vector was
normalized to unit length. After some param-
eter tuning iterations, the SVM model performs
really well with a microaveraged F11 value of
0.9437. This indicates high overall accuracy, and
the macroaveraged F1 value of 0.9341 shows, that
also lower frequency categories are predicted quite
reliably. The detailed results are shown in table 2.
Thus the first step in the cascaded model, i.e.
the selection of the work-type specific segment
</bodyText>
<footnote confidence="0.7536735">
1F1 = 2XprecisionXrecall
precision+recall
</footnote>
<page confidence="0.980618">
21
</page>
<table confidence="0.999417272727273">
predicted rec. prec. F1
true CA CL CN DS ER HP OR
CA 1966 1882 53 5 6 0 9 11 0.9573 0.9787 0.9679
CL 24151 25 23675 217 13 18 155 48 0.9803 0.9529 0.9664
CN 5590 1 447 4695 7 17 413 10 0.8399 0.8814 0.8601
DS 3319 1 37 8 3241 2 27 3 0.9765 0.9818 0.9792
ER 878 0 90 7 10 754 13 4 0.8588 0.9425 0.8987
HP 6789 4 512 393 22 7 5838 13 0.8599 0.9040 0.8814
OR 4534 10 31 2 2 2 3 4484 0.9890 0.9805 0.9847
microaveraged 0.9437
macroaveraged 0.9341
</table>
<tableCaption confidence="0.9625">
Table 2: Work-Type categorization results
</tableCaption>
<bodyText confidence="0.667983">
topic model, yields reliable performance.
</bodyText>
<subsectionHeader confidence="0.998174">
3.2 Segment Topic Classification
</subsectionHeader>
<bodyText confidence="0.999994909090909">
In contrast to work-type categorization, where
whole reports need to be categorized, the identifi-
cation of segment topics requires a different setup.
Since not only the topic labels are to be deter-
mined, but also segment boundaries are unknown
in the classification task, each token constitutes
an example under this setting. Segments are then
contiguous text regions with the same topic label.
It is clearly not enough to consider only features
of the token to be classified, thus we include also
contextual and positional features.
</bodyText>
<subsectionHeader confidence="0.563735">
3.2.1 Feature and Kernel Selection
</subsectionHeader>
<bodyText confidence="0.9998715">
In particular, we employ a sliding window ap-
proach, i.e. for each data set not only the token to
be classified, but also the 10 preceding and the 10
following tokens are considered (at the beginning
or towards the end of a report, context is reduced
appropriately). This window defines the text frag-
ment to be used for classifying the center token,
and features are collected from this window again
as described in section 2.2. Additionaly, the rela-
tive position (ranging from -1 to +1) of the center
token is used as a feature.
The rationale behind this setup is that
</bodyText>
<listItem confidence="0.984502714285714">
1. usually topics in medical reports follow an or-
dering, thus relative position may help.
2. holding features also from adjacent segments
might also be helpful since topic succession
also follows typical patterns.
3. a sufficiently sized context might also smooth
label assignment and prevent label oscilla-
</listItem>
<bodyText confidence="0.999112222222222">
tion, since the classification features for ad-
jacent words overlap to a great deal.
A second choice to be made was the selection
of the kernel best suited for this particular classifi-
cation problem. In order to get an impression, we
made a preliminary mini-experiment with just 5
reports each for training (4341 datasets) and test-
ing (3382 datasets), the results of which are re-
ported in table 3.
</bodyText>
<table confidence="0.993714">
Accuracy
Feature Weight linear RBF
TFIDF 0.4977 0.3131
TFIDF normalized 0.5544 0.6199
Binary 0.6417 0.6562
</table>
<tableCaption confidence="0.999623">
Table 3: Preliminary Kernel Comparison
</tableCaption>
<bodyText confidence="0.991989333333333">
While these results are of course not significant,
two things could be learned from the preliminary
experiment:
</bodyText>
<listItem confidence="0.873854333333333">
1. linear kernels may have similar or even better
performance,
2. training times with LIBSVM with a large
</listItem>
<bodyText confidence="0.956830090909091">
number of examples may soon get infeasible
(we were not able to repeat this experiment
with 50 reports due to excessive runtime).
Since LibSVM solves linear and nonlinear
SVMs in the same way, LibSVM is not particu-
larly efficient for linear SVMs. Therefore we de-
cided to switch to Liblinear (Fan et al., 2008), a
linear classifier optimized for handling data with
millions of instances and features2.
2Indeed, training a model from 669 reports (463994 ex-
amples) could be done in less then 5 minutes!
</bodyText>
<page confidence="0.997349">
22
</page>
<table confidence="0.9543355">
# True Label Total F1 predicted class label (#)
... 3 4 ... 14 ...
... ... ... ... ... ... ... ... ...
3 Diagnosis 40871 0.603 ... 24391 2864 ... 8691 ...
4 DiagAndPlan 21762 0.365 ... 5479 6477 ... 7950 ...
... ... ... ... ... ... ... ... ...
14 Plan 31729 0.598 ... 5714 3419 ... 21034 ...
... ... ... ... ... ... ... ... ...
</table>
<tableCaption confidence="0.999713">
Table 4: Confusion matrix (part of)
</tableCaption>
<subsectionHeader confidence="0.841131">
3.2.2 Segment Topic Classification Results
</subsectionHeader>
<bodyText confidence="0.999904461538461">
Experiments were performed on a randomly se-
lected subset of reports from the “Consultations”
work-type (1338) that were available both in cor-
rected form and in raw ASR output form. An-
notations were constructed for the corrected tran-
scripts, as described in section 2.3, transfer of la-
bels to the ASR output was performed as shown in
section 2.3.2.
Both data sets were split into training and test
sets of equal size (669 reports each), experiments
with different feature weighting schemes have
been performed on both corrected data and ASR
output. The overall results are shown in table 5.
</bodyText>
<table confidence="0.947292666666667">
Feature weights corrected reports ASR output
micro- macro- micro- macro-
avg.F1 avg.F1 avg.F1 avg.F1
TFIDF 0.7553 0.5178 0.7136 0.4440
TFIDF norm. 0.7632 0.3470 0.7268 0.3131
Binary 0.7693 0.4636 0.7413 0.3953
</table>
<tableCaption confidence="0.995896">
Table 5: Segment topic classification results
</tableCaption>
<bodyText confidence="0.974268714285714">
Consistently, macroaveraged F1 values are
much lower than their microaveraged counterparts
indicating that low-frequency topic labels are pre-
dicted with less accuracy.
Also, segment classification works better with
corrected reports than with raw ASR output. The
reason for that behaviour is
</bodyText>
<listItem confidence="0.934875">
1. ASR data are more noisy due to recognition
errors, and
2. while in corrected reports appropriate section
headers are available (not as header, but the
words) this is not necessarily the case in ASR
output (also the wording of dictated headers
and written headers may be different).
</listItem>
<bodyText confidence="0.999177424242424">
A general note on the used topic labels must
also be made: Due to the nature of our data it
was inevitable to use topic labels that overlap in
some cases. The most prominent example here is
“Diagnosis”, “Plan”, and “Diagnosis and Plan”.
The third label clearly subsumes the other two, but
in the data available the physicians often decided
to dictate diagnoses and the respective treatment
in an alternating way, associating each diagnosis
with the appropriate plan. This made it necessary
to include all three labels, with obvious effects that
could easily seen when inspecting the confusion
matrix, a part of which is shown in table 4.
When looking at the misclassifications in these
3 categories it can easily be seen, that they are pre-
dominantly due to overlapping categories.
Another source of problems in the data is the
skewed distribution of segment types in the re-
ports. Sections labelled with one of the four la-
bel categories that weren’t predicted at all (Chief-
Complaints, Course, Procedure, and Time, cf. ta-
ble 6) occur in less than 2% of the reports or are
infrequent and extremely short. This fact had, of
course, undesirable effects on the macroavered F1
scores. Additional difficulties that are similar to
the overlap problem discussed above are strong
thematic similarities between some section types
(e.g., Findings and Diagnosis, or ReasonForEn-
counter and HistoryOfPresentIllness) that result in
a very similar vocabulary used.
Given these difficulties due to the data, the re-
sults are encouraging. There is, however, still
plenty of room left for improvement.
</bodyText>
<subsectionHeader confidence="0.999813">
3.3 Improving Topic Classification
</subsectionHeader>
<bodyText confidence="0.99989575">
Liblinear does not only provide class label pre-
dictions, it is also possible to obtain class proba-
bilities. The usual way then to predict the label
is to choose the one with the highest probability.
When analysing the errors made by the segment
topic classification task described above, it turned
out that often the correct label was ranked second
or third (cf. table 6). Thus, the idea of just taking
</bodyText>
<page confidence="0.993694">
23
</page>
<table confidence="0.999960173913043">
Label count correct prediction in
best best 2 best 3
Allergies 3456 29.72 71.64 85.21
ChiefComplai 697
Course 30
Diagnosis 43565 64.69 83.29 91.37
DiagAndPlan 19409 35.24 70.45 86.81
DiagnosticSt 35554 82.47 91.34 93.05
Findings 791 0.38 1.26
Habits 2735 7.31 32.69 41.76
HistoryOfPre 122735 92.26 97.55 98.20
Medication 14553 85.87 93.38 95.22
Neurologic 5226 54.08 86.93 89.19
PastHistory 43775 71.13 86.26 88.82
PastSurgical 5752 49.32 78.88 84.47
PhysicalExam 86031 93.56 97.01 97.57
Plan 36476 62.57 84.63 94.65
Practitioner 1262 55.07 76.78 82.73
Procedures 109
ReasonForEnc 15819 25.42 42.35 43.47
ReviewOfSyst 29316 79.81 89.90 91.87
Time 58
Total 467349 76.93 88.65 92.00
</table>
<tableCaption confidence="0.998225">
Table 6: Ranked predictions
</tableCaption>
<bodyText confidence="0.997315566666667">
the highest ranked class label could be possibly
improved by a more informed choice.
While the segment topic classifier already takes
contextual features into account, it has still no in-
formation on the classification results of the neigh-
boring text segments. However, there are con-
straints on the length of text segments, thus, e.g.
a text segment of length 1 with a different topic la-
bel than the surrounding text is highly implausible.
Furthermore, there are also regularities in the suc-
cession of topic labels, which can be captured by
the monostratal local classification only indirectly
– if at all.
A look at figure 4 exemplifies how a bet-
ter informed choice of the label could result in
higher prediction accuracy. The segment labelled
“PastHistory” correctly ends 4 tokens earlier than
predicted, and, additionally, this label erroneously
is predicted again for the phrase “progressive
weight loss”. The correct label, however, has still
a rather high probability in the predicted label
distribution. By means of stacking an additional
classier onto the first one we hope to be able to
correct some of the locally made errors a posteri-
ori.
The setup for the error correction classifier
we experimented with was as follows (it was
performed only for the segment topic classi-
fier trained on ASR output with binary feature
weights):
</bodyText>
<footnote confidence="0.610609">
1. The training set of the classifier was clas-
</footnote>
<table confidence="0.988367555555555">
True Label Label probabilities (%)
Predicted ... 10 11 12 ... 17 18
. . .
=PastHistory[11] age PastHistory 0 95 0 0 0
=PastHistory[11] 63 PastHistory 0 95 0 0 0
=PastHistory[11] and PastHistory 0 95 0 0 1
=PastHistory[11] his PastHistory 0 95 0 0 1
= PastHistory [11] father PastHistory 0 88 0 0 9
=PastHistory[11] died PastHistory 0 90 0 0 8
= PastHistory [11] from PastHistory 0 84 0 0 14
= PastHistory [11] myocardial infa PastHistory 0 81 0 0 17
= PastHistory [11] at PastHistory 0 77 0 0 20
=PastHistory[11] age PastHistory 0 78 0 119
=PastHistory[11] 57 PastHistory 0 78 0 119
=PastHistory[11] period PastHistory 0 78 0 1 19
- ReviewOfSyst[18] review PastHistory 0 76 0 1 20
- ReviewOfSyst[18] of PastHistory 0 76 0 1 21
- ReviewOfSyst[18] systems PastHistory 0 78 0 0 19
- ReviewOfSyst[18] he PastHistory 1 57 0 1 37
= ReviewOfSyst[18] has ReviewOfSyst 1 32 0 1 58
= ReviewOfSyst[18] had ReviewOfSyst 1 32 0 1 58
- ReviewOfSyst[18] progressive PastHistory 1 49 0 1 42
- ReviewOfSyst[18] weight loss PastHistory 1 60 0 1 32
= ReviewOfSyst[18] period ReviewOfSyst 1 31 0 0 62
= ReviewOfSyst[18] his ReviewOfSyst 1 13 0 1 81
= ReviewOfSyst[18] appetite ReviewOfSyst 1 13 0 1 81
. . .
</table>
<figureCaption confidence="0.99895">
Figure 4: predicted label probabilites
</figureCaption>
<bodyText confidence="0.988205">
sified, and the predicted label probabilities
were collected as features.
2. Again, a sliding window (with different
sizes) was used for feature construction. Fea-
tures were set up for each label at each win-
dow position and the respective predicted la-
bel probability was used as its value.
</bodyText>
<listItem confidence="0.8379264">
3. A linear classifier was trained on these fea-
tures of the training set
4. This classifier was applied to the results of
classifying the test set with the original seg-
ment topic classifier.
</listItem>
<bodyText confidence="0.995231">
Three different window sizes were used on the
corrected reports, only one window was applied
on ASR output (cf. table 7). As can be seen, each
</bodyText>
<table confidence="0.990923714285714">
corrected reports ASR output
micro- macro- micro- macro-
context window avg.F1 avg.F1 avg.F1 avg.F1
No correction 0.7693 0.4636 0.7413 0.3953
[−3, +3] 0.7782 0.4773 - -
[−6, +0] 0.7798 0.4754 - -
[−3, +4] 0.7788 0.4769 0.7520 0.4055
</table>
<tableCaption confidence="0.999346">
Table 7: A posteriori correction results
</tableCaption>
<bodyText confidence="0.99947">
context variant improved on both microaveraged
and macroaveraged F1 in a range of 0,9 to 1.4 per-
cent points. Thus, stacked error correction indeed
is possible and able to improve classification re-
sults.
</bodyText>
<page confidence="0.99834">
24
</page>
<sectionHeader confidence="0.983853" genericHeader="conclusions">
4 Conclusion and Outlook
</sectionHeader>
<bodyText confidence="0.999990897959184">
We have presented a 3 step approach to seg-
ment topic identification in dictations of medi-
cal reports. In the first step, a categorization of
work-type is performed on the whole report us-
ing SVM classification employing semantic fea-
tures. The categorization model yields good per-
formance (over 94% accuracy) and is a prerequi-
site for subsequent application of work-type spe-
cific segment classification models.
For segment topic detection, every word was as-
signed a class label based on contextual features
in a sliding window approach. Here also semantic
features were used as a means for feature gener-
alisation. In various experiments, linear models
using binary feature weights had the best perfor-
mance. A posteriori error correction via classifier
stacking additionally improved the results.
When comparing our results to the results of
Jancsary et al. (2008), who pursue a multi-level
segmentation aproach using conditional random
fields optimizing over the whole report, the locally
obtained SVM results cannot compete fully. On
label chain 2, which is equivalent to segment top-
ics as investigated here, Jancsary et al. (2008) re-
port an estimated accuracy of 81.45 f 2.14 % on
ASR output (after some postprocessing), whereas
our results, even with a posteriori error correction,
are at least 4 percent points behind. This is prob-
ably due to the fact that the multi-level annotation
employed in Jancsary et al. (2008) contains addi-
tional information useful for the learning task, and
constraints between the levels improve segmenta-
tion behavior at the segment boundaries. Never-
theless, our approach has the merit of employing a
framework that can be trained in a fraction of the
time needed for CRF training, and classification
works locally.
An investigation on how to combine these two
complementary approaches is planned for the fu-
ture. The idea here is to use the probability distri-
butions on labels returned by our approach as (ad-
ditional) features in the CRF model. It might be
possible to leave out some other features currently
employed in return, thereby reducing model com-
plexity. The benefit we hope to get by doing so are
shorter training time for CRF training, and, since,
contrary to CRFs, SVMs are a large margin classi-
fication method, hopefully the CRF model can be
improved by the present approach.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997871">
The work presented here has been carried out in
the context of the Austrian KNet competence net-
work COAST. We gratefully acknowledge fund-
ing by the Austrian Federal Ministry of Economics
and Labour, and ZIT Zentrum fuer Innovation und
Technologie, Vienna. The Austrian Research In-
stitute for Artificial Intelligence is supported by
the Austrian Federal Ministry for Transport, Inno-
vation, and Technology and by the Austrian Fed-
eral Ministry for Science and Research.
</bodyText>
<sectionHeader confidence="0.999162" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999707837837838">
ASTM International. 2002. ASTM E2184-02: Stan-
dard specification for healthcare document formats.
C.-C. Chang and C.-J. Lin. 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classification. Journal of Machine Learning
Research, 9(2008):1871–1874.
C. Fellbaum. 1998. WordNet: an electronic lexical
database. MIT Press, Cambridge, MA.
M. Huber, J. Jancsary, A. Klein, J. Matiasek, H. Trost.
2006. Mismatch interpretation by semantics-driven
alignment. Proceedings of Konvens 2006.
J. Jancsary, A. Klein, J. Matiasek, H. Trost. 2007.
Semantics-based Automatic Literal Reconstruction
Of Dictations. In Alcantara M. and Declerck
T.(eds.), Semantic Representation of Spoken Lan-
guage 2007 (SRSL7) Universidad de Salamanca,
Spain, pp. 67-74.
J. Jancsary, J. Matiasek, H. Trost. 2008. Reveal-
ing the Structure of Medical Dictations with Con-
ditional Random Fields. Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, pp. 1–10.
T. Joachims. 1998. Text Categorization with Sup-
port Vector Machines: Learning with Many Rele-
vant Features. Proceedings of the European Confer-
ence on Machine Learning. Springer, pp. 137–142.
D.A.B. Lindberg, B.L. Humphreys, A.T. McCray.
1993. The Unified Medical Language System.
Methods of Information in Medicine, (32):281-291.
Lawrence Philips. 1990. Hanging on the metaphone.
Computer Language, 7(12).
V.N. Vapnik 1995. The Nature of Statistical Learning
Theory. Springer.
</reference>
<page confidence="0.998734">
25
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.447847">
<title confidence="0.999881">Identifying Segment Topics in Medical Dictations</title>
<author confidence="0.741003">Johannes Matiasek</author>
<author confidence="0.741003">Jeremy Alexandra</author>
<affiliation confidence="0.981076">Austrian Research Institute Artificial</affiliation>
<address confidence="0.992535">Freyung 6, Wien, Austria</address>
<email confidence="0.998137">firstname.lastname@ofai.at</email>
<abstract confidence="0.997450470588235">In this paper, we describe the use of lexical and semantic features for topic classification in dictated medical reports. First, we employ SVM classification to assign whole reports to coarse work-type categories. Afterwards, text segments and their topic are identified in the output of automatic speech recognition. This is done by assigning work-type-specific topic labels to each word based on features extracted from a sliding context window, again using SVM classification utilizing semantic features. Classifier stacking is then used for a posteriori error correction, yielding a further improvement in classification accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ASTM International</author>
</authors>
<title>ASTM E2184-02: Standard specification for healthcare document formats.</title>
<date>2002</date>
<contexts>
<context position="7584" citStr="International, 2002" startWordPosition="1186" endWordPosition="1187"> with stop word lung cancer C0242379 UMLS concept C0684249 UMLS concept B2.2.1.2.1.2 UMLS semtype ... 2.3 Data Annotation For the first classification task, i.e. work-type classification, no further annotation is necessary, every report in our data corpus had a label indicating the work-type. For the segment topic classification task, however, every token of the report had to be assigned a topic label. 2.3.1 Analysis of Corrected Transcripts For the experiments described here, we concentrated on the “Consultations” work-type, for which clear structuring recommendations, such as E2184-02 (ASTM International, 2002), exist. However, in practice the structure of medical reports shows high variation and deviations from the guidelines, making it harder to come up with 20 an appropriate set of class labels. Therefore, using the aforementioned standard, we assigned the headings that actually appeared in the data to the closest type, introducing new types only when absolutely necessary. Thus we arrived at 23 heading classes. Every (possibly multi-word) token was then labeled with the heading class of the last section heading occurring before it in the text using a simple parser. 2.3.2 Aligment and Label Transf</context>
</contexts>
<marker>International, 2002</marker>
<rawString>ASTM International. 2002. ASTM E2184-02: Standard specification for healthcare document formats.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm</title>
<date>2001</date>
<marker>Chang, Lin, 2001</marker>
<rawString>C.-C. Chang and C.-J. Lin. 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<issue>2008</issue>
<contexts>
<context position="15284" citStr="Fan et al., 2008" startWordPosition="2433" endWordPosition="2436">FIDF normalized 0.5544 0.6199 Binary 0.6417 0.6562 Table 3: Preliminary Kernel Comparison While these results are of course not significant, two things could be learned from the preliminary experiment: 1. linear kernels may have similar or even better performance, 2. training times with LIBSVM with a large number of examples may soon get infeasible (we were not able to repeat this experiment with 50 reports due to excessive runtime). Since LibSVM solves linear and nonlinear SVMs in the same way, LibSVM is not particularly efficient for linear SVMs. Therefore we decided to switch to Liblinear (Fan et al., 2008), a linear classifier optimized for handling data with millions of instances and features2. 2Indeed, training a model from 669 reports (463994 examples) could be done in less then 5 minutes! 22 # True Label Total F1 predicted class label (#) ... 3 4 ... 14 ... ... ... ... ... ... ... ... ... ... 3 Diagnosis 40871 0.603 ... 24391 2864 ... 8691 ... 4 DiagAndPlan 21762 0.365 ... 5479 6477 ... 7950 ... ... ... ... ... ... ... ... ... ... 14 Plan 31729 0.598 ... 5714 3419 ... 21034 ... ... ... ... ... ... ... ... ... ... Table 4: Confusion matrix (part of) 3.2.2 Segment Topic Classification Results</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9(2008):1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9437" citStr="Fellbaum, 1998" startWordPosition="1489" endWordPosition="1490"> that do not occur in the corrected text (meta instructions, repetitions) 3. non-identical but corresponding items (recognition errors, reformulations) For this alignment task, a standard string-edit distance based method is not sufficient. Therefore, we augment it with a more sophisticated cost function. It assigns tokens that are similar (either from a semantic or from a phonetic point of view) a low cost for substitution, whereas dissimilar tokens receive a prohibitively expensive score. Costs for deletion and insertion are assigned inversely. Semantic similarity is computed using Wordnet (Fellbaum, 1998) and UMLS. For phonetic matching, the Metaphone algorithm (Philips, 1990) was used (for details see Huber et al. (2006) and Jancsary et al. (2007)). 3 Experiments 3.1 Work-Type Categorization In total we had 62844 written medical reports with assigned work-type information from different hospitals, 7 work-types are distinguished. We randomly selected approximately a quarter of the corrected report OP ASR output ... ... ... ... ... ChiefCompl CHIEF del ChiefCompl COMPLAINT sub complaint ChiefCompl ChiefCompl Dehydration sub dehydration ChiefCompl ChiefCompl , del weakness ChiefCompl ChiefCompl </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: an electronic lexical database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Huber</author>
<author>J Jancsary</author>
<author>A Klein</author>
<author>J Matiasek</author>
<author>H Trost</author>
</authors>
<title>Mismatch interpretation by semantics-driven alignment.</title>
<date>2006</date>
<booktitle>Proceedings of Konvens</booktitle>
<contexts>
<context position="9556" citStr="Huber et al. (2006)" startWordPosition="1507" endWordPosition="1510">(recognition errors, reformulations) For this alignment task, a standard string-edit distance based method is not sufficient. Therefore, we augment it with a more sophisticated cost function. It assigns tokens that are similar (either from a semantic or from a phonetic point of view) a low cost for substitution, whereas dissimilar tokens receive a prohibitively expensive score. Costs for deletion and insertion are assigned inversely. Semantic similarity is computed using Wordnet (Fellbaum, 1998) and UMLS. For phonetic matching, the Metaphone algorithm (Philips, 1990) was used (for details see Huber et al. (2006) and Jancsary et al. (2007)). 3 Experiments 3.1 Work-Type Categorization In total we had 62844 written medical reports with assigned work-type information from different hospitals, 7 work-types are distinguished. We randomly selected approximately a quarter of the corrected report OP ASR output ... ... ... ... ... ChiefCompl CHIEF del ChiefCompl COMPLAINT sub complaint ChiefCompl ChiefCompl Dehydration sub dehydration ChiefCompl ChiefCompl , del weakness ChiefCompl ChiefCompl weakness sub ChiefCompl and sub and ChiefCompl ChiefCompl diarrhea sub diarrhea ChiefCompl ChiefCompl . sub fullstop Ch</context>
</contexts>
<marker>Huber, Jancsary, Klein, Matiasek, Trost, 2006</marker>
<rawString>M. Huber, J. Jancsary, A. Klein, J. Matiasek, H. Trost. 2006. Mismatch interpretation by semantics-driven alignment. Proceedings of Konvens 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jancsary</author>
<author>A Klein</author>
<author>J Matiasek</author>
<author>H Trost</author>
</authors>
<title>Semantics-based Automatic Literal Reconstruction Of Dictations.</title>
<date>2007</date>
<booktitle>In Alcantara M. and Declerck T.(eds.), Semantic Representation of Spoken Language 2007 (SRSL7) Universidad de Salamanca, Spain,</booktitle>
<pages>67--74</pages>
<contexts>
<context position="9583" citStr="Jancsary et al. (2007)" startWordPosition="1512" endWordPosition="1515">ormulations) For this alignment task, a standard string-edit distance based method is not sufficient. Therefore, we augment it with a more sophisticated cost function. It assigns tokens that are similar (either from a semantic or from a phonetic point of view) a low cost for substitution, whereas dissimilar tokens receive a prohibitively expensive score. Costs for deletion and insertion are assigned inversely. Semantic similarity is computed using Wordnet (Fellbaum, 1998) and UMLS. For phonetic matching, the Metaphone algorithm (Philips, 1990) was used (for details see Huber et al. (2006) and Jancsary et al. (2007)). 3 Experiments 3.1 Work-Type Categorization In total we had 62844 written medical reports with assigned work-type information from different hospitals, 7 work-types are distinguished. We randomly selected approximately a quarter of the corrected report OP ASR output ... ... ... ... ... ChiefCompl CHIEF del ChiefCompl COMPLAINT sub complaint ChiefCompl ChiefCompl Dehydration sub dehydration ChiefCompl ChiefCompl , del weakness ChiefCompl ChiefCompl weakness sub ChiefCompl and sub and ChiefCompl ChiefCompl diarrhea sub diarrhea ChiefCompl ChiefCompl . sub fullstop ChiefCompl HistoryOfP Mr. sub</context>
</contexts>
<marker>Jancsary, Klein, Matiasek, Trost, 2007</marker>
<rawString>J. Jancsary, A. Klein, J. Matiasek, H. Trost. 2007. Semantics-based Automatic Literal Reconstruction Of Dictations. In Alcantara M. and Declerck T.(eds.), Semantic Representation of Spoken Language 2007 (SRSL7) Universidad de Salamanca, Spain, pp. 67-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jancsary</author>
<author>J Matiasek</author>
<author>H Trost</author>
</authors>
<title>Revealing the Structure of Medical Dictations with Conditional Random Fields.</title>
<date>2008</date>
<booktitle>Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="24635" citStr="Jancsary et al. (2008)" startWordPosition="3992" endWordPosition="3995">he categorization model yields good performance (over 94% accuracy) and is a prerequisite for subsequent application of work-type specific segment classification models. For segment topic detection, every word was assigned a class label based on contextual features in a sliding window approach. Here also semantic features were used as a means for feature generalisation. In various experiments, linear models using binary feature weights had the best performance. A posteriori error correction via classifier stacking additionally improved the results. When comparing our results to the results of Jancsary et al. (2008), who pursue a multi-level segmentation aproach using conditional random fields optimizing over the whole report, the locally obtained SVM results cannot compete fully. On label chain 2, which is equivalent to segment topics as investigated here, Jancsary et al. (2008) report an estimated accuracy of 81.45 f 2.14 % on ASR output (after some postprocessing), whereas our results, even with a posteriori error correction, are at least 4 percent points behind. This is probably due to the fact that the multi-level annotation employed in Jancsary et al. (2008) contains additional information useful f</context>
</contexts>
<marker>Jancsary, Matiasek, Trost, 2008</marker>
<rawString>J. Jancsary, J. Matiasek, H. Trost. 2008. Revealing the Structure of Medical Dictations with Conditional Random Fields. Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pp. 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>Proceedings of the European Conference on Machine Learning.</booktitle>
<pages>137--142</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="4585" citStr="Joachims, 1998" startWordPosition="705" endWordPosition="706">n analysing the data, it became clear that the structure of segment topics varied strongly across different work-types. Thus we decided to pursue a two-step approach: firstly classify reports according to their work-type and, secondly, train and apply work-type specific classification models for segment topic classification. 2.1 Classification framework For all classification tasks discussed here, we employed support-vector machines (SVM, Vapnik (1995)) as the statistical framework, though in different incarnations and setups. SVMs have proven to be an effective means for text categorization (Joachims, 1998) as they are capable to robustly deal with high-dimensional, sparse feature spaces. Depending on the task, we experimented with different feature weighting schemes and SVM kernel functions as will be described in section 3. 2.2 Features used for classification The usual approach in text categorization is to use bag-of-word features, i.e. the words occuring in a document are collected disregarding the order of their appearance. In the domain of medical dictation, however, often abbreviations or different medical terms may be used to refer to the same semantic concept. In addition, medical terms</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. Proceedings of the European Conference on Machine Learning. Springer, pp. 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A B Lindberg</author>
<author>B L Humphreys</author>
<author>A T McCray</author>
</authors>
<title>The Unified Medical Language System.</title>
<date>1993</date>
<journal>Methods of Information in Medicine,</journal>
<pages>32--281</pages>
<contexts>
<context position="5543" citStr="Lindberg et al., 1993" startWordPosition="848" endWordPosition="851">.e. the words occuring in a document are collected disregarding the order of their appearance. In the domain of medical dictation, however, often abbreviations or different medical terms may be used to refer to the same semantic concept. In addition, medical terms often are multi-word expressions, e.g., “coronary heart disease”. Therefore, a better approach for feature mapping is needed to arrive at features at an appropriate generalization level: • Tokenization is performed using a large finite-state lexicon including multi-word medical concepts extracted from the UMLS medical metathesaurus (Lindberg et al., 1993). Thus, multi-word terms remain intact. In addition, numeric quantities in special (spoken or written) formats or together with a dimension are mapped to semantic types (e.g. “blood pressure” or “physical quantity”), also using a finite-state transducer. • The tokens are lemmatized and, if possible, replaced by the UMLS semantic concept identifier(s) they map to. Thus, “CHD”, “coronary disease” and “coronary heart disease” all map to the same concept “C0010068”. • In addition, also the UMLS semantic type, if available, is used as a feature, so, in the example above, “B2.2.1.2.1” (Disease or Sy</context>
</contexts>
<marker>Lindberg, Humphreys, McCray, 1993</marker>
<rawString>D.A.B. Lindberg, B.L. Humphreys, A.T. McCray. 1993. The Unified Medical Language System. Methods of Information in Medicine, (32):281-291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Philips</author>
</authors>
<title>Hanging on the metaphone.</title>
<date>1990</date>
<journal>Computer Language,</journal>
<volume>7</volume>
<issue>12</issue>
<contexts>
<context position="9510" citStr="Philips, 1990" startWordPosition="1500" endWordPosition="1501">3. non-identical but corresponding items (recognition errors, reformulations) For this alignment task, a standard string-edit distance based method is not sufficient. Therefore, we augment it with a more sophisticated cost function. It assigns tokens that are similar (either from a semantic or from a phonetic point of view) a low cost for substitution, whereas dissimilar tokens receive a prohibitively expensive score. Costs for deletion and insertion are assigned inversely. Semantic similarity is computed using Wordnet (Fellbaum, 1998) and UMLS. For phonetic matching, the Metaphone algorithm (Philips, 1990) was used (for details see Huber et al. (2006) and Jancsary et al. (2007)). 3 Experiments 3.1 Work-Type Categorization In total we had 62844 written medical reports with assigned work-type information from different hospitals, 7 work-types are distinguished. We randomly selected approximately a quarter of the corrected report OP ASR output ... ... ... ... ... ChiefCompl CHIEF del ChiefCompl COMPLAINT sub complaint ChiefCompl ChiefCompl Dehydration sub dehydration ChiefCompl ChiefCompl , del weakness ChiefCompl ChiefCompl weakness sub ChiefCompl and sub and ChiefCompl ChiefCompl diarrhea sub di</context>
</contexts>
<marker>Philips, 1990</marker>
<rawString>Lawrence Philips. 1990. Hanging on the metaphone. Computer Language, 7(12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="4426" citStr="Vapnik (1995)" startWordPosition="681" endWordPosition="682"> Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 19 available in both forms, 51382 reports only as corrected transcripts. When analysing the data, it became clear that the structure of segment topics varied strongly across different work-types. Thus we decided to pursue a two-step approach: firstly classify reports according to their work-type and, secondly, train and apply work-type specific classification models for segment topic classification. 2.1 Classification framework For all classification tasks discussed here, we employed support-vector machines (SVM, Vapnik (1995)) as the statistical framework, though in different incarnations and setups. SVMs have proven to be an effective means for text categorization (Joachims, 1998) as they are capable to robustly deal with high-dimensional, sparse feature spaces. Depending on the task, we experimented with different feature weighting schemes and SVM kernel functions as will be described in section 3. 2.2 Features used for classification The usual approach in text categorization is to use bag-of-word features, i.e. the words occuring in a document are collected disregarding the order of their appearance. In the dom</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V.N. Vapnik 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>