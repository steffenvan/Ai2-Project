<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.212650">
<title confidence="0.996291">
ECNU: Leveraging Word Embeddings to Boost Performance for Paraphrase
in Twitter
</title>
<author confidence="0.999882">
Jiang Zhao, Man Lan*
</author>
<affiliation confidence="0.982463333333333">
Shanghai Key Laboratory of Multidimensional Information Processing
Department of Computer Science and Technology,
East China Normal University Shanghai 200241, P. R. China
</affiliation>
<email confidence="0.996454">
51121201042@ecnu.cn; mlan@cs.ecnu.edu.cn*
</email>
<sectionHeader confidence="0.995597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920157894737">
This paper describes our approaches to para-
phrase recognition in Twitter organized as task
1 in Semantic Evaluation 2015. Lots of ap-
proaches have been proposed to address the
paraphrasing task on conventional texts ( sur-
veyed in (Madnani and Dorr, 2010)). In this
work we examined the effectiveness of vari-
ous linguistic features proposed in tradition-
al paraphrasing task on informal texts, (i.e.,
Twitter), for example, string based, corpus
based, and syntactic features, which served as
input of a classification algorithm. Besides,
we also proposed novel features based on
distributed word representations, which were
learned using deep learning paradigms. Re-
sults on test dataset show that our proposed
features improve the performance by a mar-
gin of 1.9% in terms of F1-score and our team
ranks third among 10 teams with 38 systems.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931923076923">
Generally, a paraphrase is an alternative surface
form in the same language expressing the same se-
mantic content as the original form and it can appear
at different levels, e.g., lexical, phrasal, sentential
(Madnani and Dorr, 2010). Identifying paraphrase
can improve the performance of several natural lan-
guage processing (NLP) applications, such as query
and pattern expansion (Metzler et al., 2007), ma-
chine translation (Mirkin et al., 2009), question an-
swering (Duboue and Chu-Carroll, 2006), see sur-
vey (Androutsopoulos and Malakasiotis, 2010) for
completion. Most of previous work of paraphrase
are on formal text. Recently with the rapidly growth
</bodyText>
<page confidence="0.985438">
34
</page>
<bodyText confidence="0.98511931372549">
of microblogs and social media services, the compu-
tational linguistic community is moving its attention
to informal genre of text (Java et al., 2007; Ritter et
al., 2010). For example, (Zanzotto et al., 2011) de-
fined the problem of redundancy detection in Twitter
and proposed SVM models based on bag-of-word,
syntactic content features to detect paraphrase.
To provide a benchmark so as to compare and de-
velop different paraphrasing techniques in Twitter,
the paraphrase and semantic similarity task in Se-
mEval 2015 (Xu et al., 2015) requires the partici-
pants to determine whether two tweets express the
same meaning or not and optionally a degree score
between 0 and 1, which can be regarded as a bina-
ry classification problem. Paraphrasing task is very
close to semantic textual similarity and textual en-
tailment task (Marelli et al., 2014) since substan-
tially these tasks all concentrated on modeling the
underlying similarity between two sentences. The
commonly-used features in these tasks can be cat-
egorized into several following groups: (1) string
based which measures the sequence similarities of
original strings with others, e.g., n-gram Overlap,
cosine similarity; (2) corpus based which measures
word or sentence similarities using word distribu-
tional vectors learned from large corpora using dis-
tributional models, like Latent Semantic Analysis
(LSA), etc. (3) knowledge based which estimates
similarities with the aid of external resources, such
as WordNet; (4) syntactic based which utilizes syn-
tax information to measure similarities; (5) other
features such as using Named Entity similarity.
In this work, we built a supervised binary clas-
sifier for paraphrase judgment and adopted multi-
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 34–39,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
ple features used in conventional texts to recognize
paraphrase in Twitter, which includes string based
features, corpus based features, etc. Besides, we
also proposed a novel feature based on distribut-
ed word representations (i.e., word embeddings)
learned over a large raw corpus using neural lan-
guage models. The results on test dataset demon-
strate that linguistic features are effective for para-
phrase in Twitter task and proposed word embed-
ding features further improve the performance.
The rest of this paper is organized as follows. Sec-
tion 2 describes the features used in our systems.
System setups and experimental results on training
and test datasets are presented in Section 3. Finally,
conclusions and future work are given in Section 4.
</bodyText>
<sectionHeader confidence="0.963376" genericHeader="method">
2 Feature Engineering
</sectionHeader>
<bodyText confidence="0.999776">
In this section, we describe the our preprocessing
step and the traditional NLP linguistic features, as
well as the word embedding features used in our sys-
tems.
</bodyText>
<subsectionHeader confidence="0.994023">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999997529411765">
We conducted following text preprocessing opera-
tions before we extracted features: (1) we recov-
ered the elongated words to their normal forms,
e.g., “goooooood” to “good”; (2) about 5,000 slangs
or abbreviations collected from Internet were used
to convert these informal texts into their complete
forms, e.g., “1dering” to “wondering”, “2g2b4g”
to “to good to be forgotten”; (3) the WordNet-
based Lemmatizer implemented in Natural Lan-
guage Toolkit1 was used to lemmatize all words to
their nearest base forms in WordNet, for example,
was is lemmatized to be. (4) we replaced a word
from one sentence with another word from the other
sentence if the two words share the same meaning,
where WordNet was used to look up synonyms. No
word sense disambiguation was performed and all
synsets for a particular lemma were considered.
</bodyText>
<subsectionHeader confidence="0.999918">
2.2 String Based Features
</subsectionHeader>
<bodyText confidence="0.9997745">
We firstly recorded length information of given sen-
tences pairs using following eight measure function-
</bodyText>
<equation confidence="0.825531">
s: |A|, |B|, |A−B|, |B−A|, |AUB|, |AnB|, function-
(|A|−|B|) |B |,(|B|−|A|)
|A|
</equation>
<bodyText confidence="0.91919">
where |A |stands for the number of non-repeated
</bodyText>
<footnote confidence="0.914481">
1http://nltk.org/
</footnote>
<bodyText confidence="0.999936192307692">
words in sentence A , |A − B |means the number of
unmatched words found in A but not in B , |A u B |s-
tands for the set size of non-repeated words found in
either A or B and |A n B |means the set size of shared
words found in both A and B .
Motivated by the hypothesis that two texts are
considered to be more similar if they share more
strings, we adopted the following five types of mea-
surements: (1) longest common sequence similar-
ity on the original and lemmatized sentences; (2)
Jaccard, Dice, Overlap coefficient on origi-
nal word sequences; (3) Jaccard similarity using
n-grams, where n-grams were obtained at three dif-
ferent levels, i.e., the original word level (n=1,2,3),
the lemmatized word level (n=1,2,3) and the char-
acter level (n=2,3,4); (4) weighted word overlap
feature (ˇSari´c et al., 2012) that takes the impor-
tance of words into consideration, where Web 1T
5-gram Corpus2 was used to estimate the impor-
tance of words. (5) sentences were represented as
vectors in tf*idf schema based on their lemmatized
forms and then these vectors were used to calcu-
late cosine, Manhattan, Euclidean distance
and Pearson, Spearmanr, Kendalltau cor-
relation coefficients based on different perspectives.
Totally, we got thirty-one string based features.
</bodyText>
<subsectionHeader confidence="0.999716">
2.3 Corpus Based Features
</subsectionHeader>
<bodyText confidence="0.999877941176471">
Corpus based features aim to capture the semantic
similarities using distributional meanings of words
and Latent Semantic Analysis (LSA) (Landauer and
Dumais, 1997) is widely used to estimate the dis-
tributional vectors of words. Hence, we adopted t-
wo distributional sets released in TakeLab (ˇSari´c et
al., 2012), where LSA is performed over the New Y-
ork Times Annotated Corpus (NYT)3 and Wikipedi-
a. Then two strategies were used to convert the
distributional meanings of words to sentence level:
(i) simply summing up the distributional vectors of
words in the sentence, (ii) using the information con-
tent (ˇSari´c et al., 2012) to weigh the LSA vector of
each word w and summing them up. At last we used
cosine similarity to measure the similarity of two
sentences based on these vectors. Besides, we used
the Co-occurrence Retrieval Model (CRM) (Weeds,
</bodyText>
<footnote confidence="0.998583">
2https://catalog.ldc.upenn.edu/LDC2006T13
3https://catalog.ldc.upenn.edu/LDC2008T19
</footnote>
<page confidence="0.999175">
35
</page>
<bodyText confidence="0.99983296">
2003) as another type of corpus based feature. The
CRM was calculated based on a notion of substi-
tutability, that is, the more appropriate it was to sub-
stitute word w1 in place of word w2 in a suitable
natural language task, the more semantically similar
they were.
Besides, the extraction of aforementioned fea-
tures rely on large external corpora, while (Guo
and Diab, 2012) proposed a novel latent model,
i.e., weighted textual matrix factorization (WTM-
F), to capture the contextual meanings of words
in sentences based on internal term-sentence ma-
trix. WTMF factorizes the original term-sentence
matrix X into two matrices such that Xi,j �
PT*,iQ*,j, where P*,i is a latent semantics vec-
tor profile for word wi and Q*,j is the vec-
tor profile that represents the sentence sj. The
weight matrix W is introduced in the optimiza-
tion process in order to model the missing word-
s at the right level of emphasis. Then, we used
cosine, Manhattan, Euclidean functions
and Pearson, Spearmanr, Kendalltau corre-
lation coefficients to calculate the similarities based
on sentence representations. At last, we obtained
twelve corpus based features.
</bodyText>
<subsectionHeader confidence="0.993724">
2.4 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999933857142857">
We estimated the similarities of sentence pairs at
syntactic level. Stanford CoreNLP toolkit (Manning
and Surdeanu, 2014) was used to obtain POS tag
sequences. Afterwards, we performed eight mea-
sure functions described in Section 2.2 over these
sequences, which resulted in eight syntactic based
features.
</bodyText>
<subsectionHeader confidence="0.98023">
2.5 Other Features
</subsectionHeader>
<bodyText confidence="0.99998">
We built a binary feature to indicate whether two
sentences in a pair have the same polarity (affirma-
tive or negative) by looking up a manually-collected
negation list with 29 negation words (e.g., scarcely,
no, little). Also, we checked whether one sentence
entails the other only using the named entity infor-
mation which was provided in the dataset. Finally,
we obtained nineteen other features.
</bodyText>
<subsectionHeader confidence="0.995043">
2.6 Word Embedding Features
</subsectionHeader>
<bodyText confidence="0.999981444444444">
Recently, deep learning has achieved a great suc-
cess in the fields of computer vision, automatic
speech recognition and natural language processing.
As a consequence of its application in NLP, word
embeddings have been building blocks in many
tasks, e.g., named entity recognition and chunk-
ing (Turian et al., 2010), semantic word similari-
ties (Mikolov et al., 2013a), etc. Being distribut-
ed representation of words, word embeddings usu-
ally are learned using neural networks over a large
raw corpus and has outperformed LSA for pre-
serving linear regularities among words (Mikolov
et al., 2013a). Due to its superior performance,
we adopted word embeddings to estimate the sim-
ilarities of sentence pairs. In our experiments, we
used seven different word embeddings with differ-
ent dimensions: word2vec (Mikolov et al., 2013b),
Collobert and Weston embeddings (Collobert and
Weston, 2008) and HLBL embeddings (Mnih and
Hinton, 2007). Word2vec embeddings are dis-
tributed within the word2vec toolkit4 and they are
300-dimensional vectors learned from Google News
Corpus which consists of over a 100 billion word-
s. Collobert and Weston and HLBL embeddings are
learned over a part of RCV1 corpus which consist-
s of 63 millions words, with 25, 50, 100, or 200
dimensions and 50, 100 dimensions over 5-gram
windows respectively. To obtain sentence repre-
sentations, we simply summed up embedding vec-
tors corresponding to the non-stopwords tokens in
bag of words (BOW) of sentences. After that, we
used cosine, Manhattan, Euclidean func-
tions and Pearson, Spearmanr, Kendalltau
correlation coefficients to calculate the similarities
based on these synthetic sentence representations.
We got ninety word embedding features.
</bodyText>
<sectionHeader confidence="0.997197" genericHeader="evaluation">
3 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.99813">
3.1 System Setups
</subsectionHeader>
<bodyText confidence="0.999983777777778">
The organizers provided 13,063 training pairs to-
gether with 4,727 development pairs in development
phase and 972 test pairs in test phase. We removed
the debatable instances (i.e., two annotators vote for
yes and the other three for no) existing in the dataset,
which resulted in 11,530 training pairs and 4,142 de-
velopment pairs. We built two supervised classifica-
tion systems over these datasets. One is mlfeats
which only uses the traditional linguistic features
</bodyText>
<footnote confidence="0.962984">
4https://code.google.com/p/word2vec
</footnote>
<page confidence="0.992548">
36
</page>
<table confidence="0.9994285">
Algorithm mlfeats nnfeats
Precision Recall F1 Precision Recall F1
SVC(0.1) 0.756 0.942 0.839 0.756 0.942 0.839
GB(140) 0.756 0.939 0.838 0.754 0.940 0.837
GB(150) 0.755 0.939 0.837 0.753 0.939 0.836
RF(45) 0.754 0.937 0.835 0.749 0.936 0.832
</table>
<tableCaption confidence="0.9869205">
Table 1: Top results of different classification algorithms in systems mlfeats and nnfeats on development dataset
together with parameter values in brackets.
</tableCaption>
<table confidence="0.999866444444444">
System F1-Rank Precision Recall F1
ECNU nnfeats 4 0.767 0.583 0.662
ECNU mlfeats 10 0.754 0.560 0.643
BASELINE logistic 21 0.679 0.520 0.589
BASELINE WTMF 28 0.450 0.663 0.536
BASELINE random 38 0.192 0.434 0.266
ASOBEK svckernel 1 0.680 0.669 0.674
ASOBEK linearsvm 2 0.682 0.663 0.672
MITRE ikr 3 0.569 0.806 0.667
</table>
<tableCaption confidence="0.980292">
Table 2: Performance and rankings of systems mlfeats, nnfeats and baseline systems on test dataset officially
released by the organizers, as well as top ranking systems.
</tableCaption>
<bodyText confidence="0.999884">
(i.e., features described in Section 2.2-2.5, 64 fea-
tures in total) and the other is nnfeats which com-
bines the traditional linguistic features with the word
embedding features (148 features in total). Sever-
al classification algorithms were explored on devel-
opment dataset including Support Vector Classifi-
cation (SVC, linear), Random Forest (RF), Gradi-
ent Boosting (GB) implemented in the scikit-learn
toolkit (Pedregosa et al., 2011) and a large scale
of parameter values in these algorithms were tuned,
i.e., the trade-off parameter c in SVR, the number of
trees n in RF, the number of boosting stages n in G-
B. F-score was used to evaluate the performance of
systems.
</bodyText>
<subsectionHeader confidence="0.990148">
3.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999945205882353">
Table 1 presents the best four F1 results achieved by
different algorithms together with their parameters
in system mlfeats and nnfeats on developmen-
t dataset. The results show that these two system-
s consistently yield comparable performance, which
means that our proposed features based on word em-
beddings have little help to detect paraphrase on de-
velopment set. And we also find that SVC performs
slightly better than GB and RF algorithm. There-
fore, we adopted a major voting schema based on
SVC (c=0.1) and GB (n=140,150) in test period.
Table 2 summarizes the performance and ranks of
our systems on test dataset, along with the baseline
systems provided by the organizers and the top three
systems. From this table, we observe following find-
ings. Firstly, nnfeats using word embedding fea-
tures outperforms the system mlfeats only using
traditional linguistic features by 1.9%, which is in-
consistent with the findings on development set. The
possible reason may be that test data is collected
from a different time period while train and devel-
opment data is from the same time period while the
word embedding features might more or less cap-
ture this differences. Secondly, our results are sig-
nificantly better than the three baseline systems s-
ince our systems incorporate the features used in
baseline systems and other effective features. Third-
ly, the top 1 system (i.e., ASOBEK svckernel)
yields 3.1% and 1.2% improvement over our system
mlfeats and nnfeats respectively, which indi-
cates that word embedding features and traditional
linguistic features are effective in resolving Twitter
paraphrase problem.
To explore the influence of different feature type-
</bodyText>
<page confidence="0.998606">
37
</page>
<bodyText confidence="0.999114454545455">
s, we conducted feature ablation experiments where
we removed one feature group from all feature set
every time and then executed the same classifica-
tion procedure. Table 3 shows the results of fea-
ture ablation experiments. From this table, we can
see that the most influential features for recognizing
tweet paraphrase is corpus based features and the
second most important feature group is word em-
bedding features, which are within our expectation
since these two kinds of feature take advantage of
the semantic meaning of words.
</bodyText>
<table confidence="0.974384571428571">
Feature Precision Recall F1
All 0.767 0.583 0.662
-string 0.717 0.594 0.650 (-0.012)
-corpus 0.772 0.543 0.638 (-0.024)
-syntactic 0.797 0.560 0.658 (-0.004)
-other 0.784 0.560 0.653 (-0.009)
-embedding 0.823 0.531 0.646 (-0.016)
</table>
<tableCaption confidence="0.999769">
Table 3: The results of feature ablation experiments.
</tableCaption>
<sectionHeader confidence="0.997524" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9992341">
In this paper we address paraphrase in Twitter task
by building a supervised classification model. Many
linguistic features used in traditional paraphrase task
and newly proposed features based on word embed-
dings were extracted. The results on test dataset
demonstrate that (1) our proposed word embedding
features improve the performance by a value of
1.9%; (2) the linguistic features used in paraphrase
on conventional texts task are also useful and effec-
tive in Twitter domain.
</bodyText>
<sectionHeader confidence="0.994953" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999153833333333">
This research is supported by grants from Science
and Technology Commission of Shanghai Munici-
pality under research grant no. (14DZ2260800 and
15ZR1410700) and Shanghai Collaborative Innova-
tion Center of Trustworthy Software for Internet of
Things (ZF1213).
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998934">
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entailment
methods. J. Artif. Int. Res., pages 135–187.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceedings
of the 25th international conference on Machine learn-
ing, pages 160–167.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006. An-
swering the question you wish they had asked: The im-
pact of paraphrasing for question answering. In NAA-
CL, Companion Volume: Short Papers, pages 33–36.
Weiwei Guo and Mona Diab. 2012. Modeling sentences
in the latent space. In ACL, pages 864–872.
Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng.
2007. Why we Twitter: understanding microblogging
usage and communities. In Proceedings of the 9th We-
bKDD and 1st SNA-KDD 2007 workshop on Web min-
ing and social network analysis, pages 56–65.
Thomas K Landauer and Susan T Dumais. 1997. A so-
lution to Plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, page 211.
Nitin Madnani and Bonnie J Dorr. 2010. Gener-
ating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341–387.
Christopher D. Manning and Mihai et al. Surdeanu.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In 52nd ACL : System Demonstra-
tions.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffael-
la Bernardi, Stefano Menini, and Roberto Zamparelli.
2014. Semeval-2014 task 1: Evaluation of composi-
tional distributional semantic models on full sentences
through semantic relatedness and textual entailment.
In SemEval, pages 1–8.
Donald Metzler, Susan Dumais, and Christopher Meek.
2007. Similarity measures for short segments of text.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corra-
do, and Jeff Dean. 2013b. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111–3119.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-language entailment modeling for translating
unknown terms. In ACL, pages 791–799.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proceedings of the 24th international conference on
Machine learning, pages 641–648.
</reference>
<page confidence="0.985431">
38
</page>
<reference confidence="0.999662777777778">
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort,
et al. 2011. Scikit-learn: Machine learning in Python.
The Journal of Machine Learning Research, 12:2825–
2830.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of Twitter conversations. pages
172–180.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In the 48th ACL, pages
384–394.
Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder,
and Bojana Dalbelo Ba&amp;quot;si´c. 2012. TakeLab: Systems
for measuring semantic text similarity. In *SEM 2012
and (SemEval 2012), pages 441–448, Montr´eal, Cana-
da.
Julie Elizabeth Weeds. 2003. Measures and applications
of lexical distributional similarity. Ph.D. thesis, Uni-
versity of Sussex.
Wei Xu, Chris Callison-Burch, and William B. Dolan.
2015. SemEval-2015 Task 1: Paraphrase and semantic
similarity in Twitter (PIT). In Proceedings of the 9th
International Workshop on Semantic Evaluation (Se-
mEval), Denver, CO.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Kostas Tsioutsiouliklis. 2011. Linguistic redundan-
cy in Twitter. In EMNLP, pages 659–669.
</reference>
<page confidence="0.999532">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.399473">
<title confidence="0.9970925">ECNU: Leveraging Word Embeddings to Boost Performance for Paraphrase in Twitter</title>
<author confidence="0.950627">Man Shanghai Key Laboratory of Multidimensional Information Zhao</author>
<affiliation confidence="0.988918">Department of Computer Science and</affiliation>
<note confidence="0.458742">East China Normal University Shanghai 200241, P. R.</note>
<abstract confidence="0.9993912">This paper describes our approaches to paraphrase recognition in Twitter organized as task 1 in Semantic Evaluation 2015. Lots of approaches have been proposed to address the paraphrasing task on conventional texts ( surveyed in (Madnani and Dorr, 2010)). In this work we examined the effectiveness of various linguistic features proposed in traditional paraphrasing task on informal texts, (i.e., Twitter), for example, string based, corpus based, and syntactic features, which served as input of a classification algorithm. Besides, we also proposed novel features based on distributed word representations, which were learned using deep learning paradigms. Results on test dataset show that our proposed features improve the performance by a margin of 1.9% in terms of F1-score and our team ranks third among 10 teams with 38 systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>Prodromos Malakasiotis</author>
</authors>
<title>A survey of paraphrasing and textual entailment methods.</title>
<date>2010</date>
<journal>J. Artif. Int. Res.,</journal>
<pages>135--187</pages>
<contexts>
<context position="1726" citStr="Androutsopoulos and Malakasiotis, 2010" startWordPosition="254" endWordPosition="257">terms of F1-score and our team ranks third among 10 teams with 38 systems. 1 Introduction Generally, a paraphrase is an alternative surface form in the same language expressing the same semantic content as the original form and it can appear at different levels, e.g., lexical, phrasal, sentential (Madnani and Dorr, 2010). Identifying paraphrase can improve the performance of several natural language processing (NLP) applications, such as query and pattern expansion (Metzler et al., 2007), machine translation (Mirkin et al., 2009), question answering (Duboue and Chu-Carroll, 2006), see survey (Androutsopoulos and Malakasiotis, 2010) for completion. Most of previous work of paraphrase are on formal text. Recently with the rapidly growth 34 of microblogs and social media services, the computational linguistic community is moving its attention to informal genre of text (Java et al., 2007; Ritter et al., 2010). For example, (Zanzotto et al., 2011) defined the problem of redundancy detection in Twitter and proposed SVM models based on bag-of-word, syntactic content features to detect paraphrase. To provide a benchmark so as to compare and develop different paraphrasing techniques in Twitter, the paraphrase and semantic simila</context>
</contexts>
<marker>Androutsopoulos, Malakasiotis, 2010</marker>
<rawString>Ion Androutsopoulos and Prodromos Malakasiotis. 2010. A survey of paraphrasing and textual entailment methods. J. Artif. Int. Res., pages 135–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="10794" citStr="Collobert and Weston, 2008" startWordPosition="1688" endWordPosition="1691">med entity recognition and chunking (Turian et al., 2010), semantic word similarities (Mikolov et al., 2013a), etc. Being distributed representation of words, word embeddings usually are learned using neural networks over a large raw corpus and has outperformed LSA for preserving linear regularities among words (Mikolov et al., 2013a). Due to its superior performance, we adopted word embeddings to estimate the similarities of sentence pairs. In our experiments, we used seven different word embeddings with different dimensions: word2vec (Mikolov et al., 2013b), Collobert and Weston embeddings (Collobert and Weston, 2008) and HLBL embeddings (Mnih and Hinton, 2007). Word2vec embeddings are distributed within the word2vec toolkit4 and they are 300-dimensional vectors learned from Google News Corpus which consists of over a 100 billion words. Collobert and Weston and HLBL embeddings are learned over a part of RCV1 corpus which consists of 63 millions words, with 25, 50, 100, or 200 dimensions and 50, 100 dimensions over 5-gram windows respectively. To obtain sentence representations, we simply summed up embedding vectors corresponding to the non-stopwords tokens in bag of words (BOW) of sentences. After that, we</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Ariel Duboue</author>
<author>Jennifer Chu-Carroll</author>
</authors>
<title>Answering the question you wish they had asked: The impact of paraphrasing for question answering.</title>
<date>2006</date>
<booktitle>In NAACL, Companion Volume: Short Papers,</booktitle>
<pages>33--36</pages>
<contexts>
<context position="1673" citStr="Duboue and Chu-Carroll, 2006" startWordPosition="247" endWordPosition="250">ove the performance by a margin of 1.9% in terms of F1-score and our team ranks third among 10 teams with 38 systems. 1 Introduction Generally, a paraphrase is an alternative surface form in the same language expressing the same semantic content as the original form and it can appear at different levels, e.g., lexical, phrasal, sentential (Madnani and Dorr, 2010). Identifying paraphrase can improve the performance of several natural language processing (NLP) applications, such as query and pattern expansion (Metzler et al., 2007), machine translation (Mirkin et al., 2009), question answering (Duboue and Chu-Carroll, 2006), see survey (Androutsopoulos and Malakasiotis, 2010) for completion. Most of previous work of paraphrase are on formal text. Recently with the rapidly growth 34 of microblogs and social media services, the computational linguistic community is moving its attention to informal genre of text (Java et al., 2007; Ritter et al., 2010). For example, (Zanzotto et al., 2011) defined the problem of redundancy detection in Twitter and proposed SVM models based on bag-of-word, syntactic content features to detect paraphrase. To provide a benchmark so as to compare and develop different paraphrasing tech</context>
</contexts>
<marker>Duboue, Chu-Carroll, 2006</marker>
<rawString>Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006. Answering the question you wish they had asked: The impact of paraphrasing for question answering. In NAACL, Companion Volume: Short Papers, pages 33–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>864--872</pages>
<contexts>
<context position="8373" citStr="Guo and Diab, 2012" startWordPosition="1311" endWordPosition="1314">e used cosine similarity to measure the similarity of two sentences based on these vectors. Besides, we used the Co-occurrence Retrieval Model (CRM) (Weeds, 2https://catalog.ldc.upenn.edu/LDC2006T13 3https://catalog.ldc.upenn.edu/LDC2008T19 35 2003) as another type of corpus based feature. The CRM was calculated based on a notion of substitutability, that is, the more appropriate it was to substitute word w1 in place of word w2 in a suitable natural language task, the more semantically similar they were. Besides, the extraction of aforementioned features rely on large external corpora, while (Guo and Diab, 2012) proposed a novel latent model, i.e., weighted textual matrix factorization (WTMF), to capture the contextual meanings of words in sentences based on internal term-sentence matrix. WTMF factorizes the original term-sentence matrix X into two matrices such that Xi,j � PT*,iQ*,j, where P*,i is a latent semantics vector profile for word wi and Q*,j is the vector profile that represents the sentence sj. The weight matrix W is introduced in the optimization process in order to model the missing words at the right level of emphasis. Then, we used cosine, Manhattan, Euclidean functions and Pearson, S</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. Modeling sentences in the latent space. In ACL, pages 864–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshay Java</author>
<author>Xiaodan Song</author>
<author>Tim Finin</author>
<author>Belle Tseng</author>
</authors>
<title>Why we Twitter: understanding microblogging usage and communities.</title>
<date>2007</date>
<booktitle>In Proceedings of the 9th WebKDD and 1st SNA-KDD</booktitle>
<pages>56--65</pages>
<contexts>
<context position="1983" citStr="Java et al., 2007" startWordPosition="296" endWordPosition="299">hrasal, sentential (Madnani and Dorr, 2010). Identifying paraphrase can improve the performance of several natural language processing (NLP) applications, such as query and pattern expansion (Metzler et al., 2007), machine translation (Mirkin et al., 2009), question answering (Duboue and Chu-Carroll, 2006), see survey (Androutsopoulos and Malakasiotis, 2010) for completion. Most of previous work of paraphrase are on formal text. Recently with the rapidly growth 34 of microblogs and social media services, the computational linguistic community is moving its attention to informal genre of text (Java et al., 2007; Ritter et al., 2010). For example, (Zanzotto et al., 2011) defined the problem of redundancy detection in Twitter and proposed SVM models based on bag-of-word, syntactic content features to detect paraphrase. To provide a benchmark so as to compare and develop different paraphrasing techniques in Twitter, the paraphrase and semantic similarity task in SemEval 2015 (Xu et al., 2015) requires the participants to determine whether two tweets express the same meaning or not and optionally a degree score between 0 and 1, which can be regarded as a binary classification problem. Paraphrasing task </context>
</contexts>
<marker>Java, Song, Finin, Tseng, 2007</marker>
<rawString>Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng. 2007. Why we Twitter: understanding microblogging usage and communities. In Proceedings of the 9th WebKDD and 1st SNA-KDD 2007 workshop on Web mining and social network analysis, pages 56–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review,</title>
<date>1997</date>
<pages>211</pages>
<contexts>
<context position="7218" citStr="Landauer and Dumais, 1997" startWordPosition="1126" endWordPosition="1129">the importance of words into consideration, where Web 1T 5-gram Corpus2 was used to estimate the importance of words. (5) sentences were represented as vectors in tf*idf schema based on their lemmatized forms and then these vectors were used to calculate cosine, Manhattan, Euclidean distance and Pearson, Spearmanr, Kendalltau correlation coefficients based on different perspectives. Totally, we got thirty-one string based features. 2.3 Corpus Based Features Corpus based features aim to capture the semantic similarities using distributional meanings of words and Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997) is widely used to estimate the distributional vectors of words. Hence, we adopted two distributional sets released in TakeLab (ˇSari´c et al., 2012), where LSA is performed over the New York Times Annotated Corpus (NYT)3 and Wikipedia. Then two strategies were used to convert the distributional meanings of words to sentence level: (i) simply summing up the distributional vectors of words in the sentence, (ii) using the information content (ˇSari´c et al., 2012) to weigh the LSA vector of each word w and summing them up. At last we used cosine similarity to measure the similarity of two senten</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, page 211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of data-driven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="1409" citStr="Madnani and Dorr, 2010" startWordPosition="209" endWordPosition="212"> features, which served as input of a classification algorithm. Besides, we also proposed novel features based on distributed word representations, which were learned using deep learning paradigms. Results on test dataset show that our proposed features improve the performance by a margin of 1.9% in terms of F1-score and our team ranks third among 10 teams with 38 systems. 1 Introduction Generally, a paraphrase is an alternative surface form in the same language expressing the same semantic content as the original form and it can appear at different levels, e.g., lexical, phrasal, sentential (Madnani and Dorr, 2010). Identifying paraphrase can improve the performance of several natural language processing (NLP) applications, such as query and pattern expansion (Metzler et al., 2007), machine translation (Mirkin et al., 2009), question answering (Duboue and Chu-Carroll, 2006), see survey (Androutsopoulos and Malakasiotis, 2010) for completion. Most of previous work of paraphrase are on formal text. Recently with the rapidly growth 34 of microblogs and social media services, the computational linguistic community is moving its attention to informal genre of text (Java et al., 2007; Ritter et al., 2010). Fo</context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie J Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics, 36(3):341–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In 52nd ACL : System Demonstrations.</booktitle>
<marker>Manning, Mihai, 2014</marker>
<rawString>Christopher D. Manning and Mihai et al. Surdeanu. 2014. The Stanford CoreNLP natural language processing toolkit. In 52nd ACL : System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In SemEval,</title>
<date>2014</date>
<pages>1--8</pages>
<contexts>
<context position="2678" citStr="Marelli et al., 2014" startWordPosition="410" endWordPosition="413">roblem of redundancy detection in Twitter and proposed SVM models based on bag-of-word, syntactic content features to detect paraphrase. To provide a benchmark so as to compare and develop different paraphrasing techniques in Twitter, the paraphrase and semantic similarity task in SemEval 2015 (Xu et al., 2015) requires the participants to determine whether two tweets express the same meaning or not and optionally a degree score between 0 and 1, which can be regarded as a binary classification problem. Paraphrasing task is very close to semantic textual similarity and textual entailment task (Marelli et al., 2014) since substantially these tasks all concentrated on modeling the underlying similarity between two sentences. The commonly-used features in these tasks can be categorized into several following groups: (1) string based which measures the sequence similarities of original strings with others, e.g., n-gram Overlap, cosine similarity; (2) corpus based which measures word or sentence similarities using word distributional vectors learned from large corpora using distributional models, like Latent Semantic Analysis (LSA), etc. (3) knowledge based which estimates similarities with the aid of extern</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In SemEval, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>Susan Dumais</author>
<author>Christopher Meek</author>
</authors>
<title>Similarity measures for short segments of text.</title>
<date>2007</date>
<contexts>
<context position="1579" citStr="Metzler et al., 2007" startWordPosition="233" endWordPosition="236"> deep learning paradigms. Results on test dataset show that our proposed features improve the performance by a margin of 1.9% in terms of F1-score and our team ranks third among 10 teams with 38 systems. 1 Introduction Generally, a paraphrase is an alternative surface form in the same language expressing the same semantic content as the original form and it can appear at different levels, e.g., lexical, phrasal, sentential (Madnani and Dorr, 2010). Identifying paraphrase can improve the performance of several natural language processing (NLP) applications, such as query and pattern expansion (Metzler et al., 2007), machine translation (Mirkin et al., 2009), question answering (Duboue and Chu-Carroll, 2006), see survey (Androutsopoulos and Malakasiotis, 2010) for completion. Most of previous work of paraphrase are on formal text. Recently with the rapidly growth 34 of microblogs and social media services, the computational linguistic community is moving its attention to informal genre of text (Java et al., 2007; Ritter et al., 2010). For example, (Zanzotto et al., 2011) defined the problem of redundancy detection in Twitter and proposed SVM models based on bag-of-word, syntactic content features to dete</context>
</contexts>
<marker>Metzler, Dumais, Meek, 2007</marker>
<rawString>Donald Metzler, Susan Dumais, and Christopher Meek. 2007. Similarity measures for short segments of text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="10274" citStr="Mikolov et al., 2013" startWordPosition="1609" endWordPosition="1612">h 29 negation words (e.g., scarcely, no, little). Also, we checked whether one sentence entails the other only using the named entity information which was provided in the dataset. Finally, we obtained nineteen other features. 2.6 Word Embedding Features Recently, deep learning has achieved a great success in the fields of computer vision, automatic speech recognition and natural language processing. As a consequence of its application in NLP, word embeddings have been building blocks in many tasks, e.g., named entity recognition and chunking (Turian et al., 2010), semantic word similarities (Mikolov et al., 2013a), etc. Being distributed representation of words, word embeddings usually are learned using neural networks over a large raw corpus and has outperformed LSA for preserving linear regularities among words (Mikolov et al., 2013a). Due to its superior performance, we adopted word embeddings to estimate the similarities of sentence pairs. In our experiments, we used seven different word embeddings with different dimensions: word2vec (Mikolov et al., 2013b), Collobert and Weston embeddings (Collobert and Weston, 2008) and HLBL embeddings (Mnih and Hinton, 2007). Word2vec embeddings are distribute</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="10274" citStr="Mikolov et al., 2013" startWordPosition="1609" endWordPosition="1612">h 29 negation words (e.g., scarcely, no, little). Also, we checked whether one sentence entails the other only using the named entity information which was provided in the dataset. Finally, we obtained nineteen other features. 2.6 Word Embedding Features Recently, deep learning has achieved a great success in the fields of computer vision, automatic speech recognition and natural language processing. As a consequence of its application in NLP, word embeddings have been building blocks in many tasks, e.g., named entity recognition and chunking (Turian et al., 2010), semantic word similarities (Mikolov et al., 2013a), etc. Being distributed representation of words, word embeddings usually are learned using neural networks over a large raw corpus and has outperformed LSA for preserving linear regularities among words (Mikolov et al., 2013a). Due to its superior performance, we adopted word embeddings to estimate the similarities of sentence pairs. In our experiments, we used seven different word embeddings with different dimensions: word2vec (Mikolov et al., 2013b), Collobert and Weston embeddings (Collobert and Weston, 2008) and HLBL embeddings (Mnih and Hinton, 2007). Word2vec embeddings are distribute</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Ido Dagan</author>
<author>Marc Dymetman</author>
<author>Idan Szpektor</author>
</authors>
<title>Source-language entailment modeling for translating unknown terms.</title>
<date>2009</date>
<booktitle>In ACL,</booktitle>
<pages>791--799</pages>
<contexts>
<context position="1622" citStr="Mirkin et al., 2009" startWordPosition="240" endWordPosition="243">taset show that our proposed features improve the performance by a margin of 1.9% in terms of F1-score and our team ranks third among 10 teams with 38 systems. 1 Introduction Generally, a paraphrase is an alternative surface form in the same language expressing the same semantic content as the original form and it can appear at different levels, e.g., lexical, phrasal, sentential (Madnani and Dorr, 2010). Identifying paraphrase can improve the performance of several natural language processing (NLP) applications, such as query and pattern expansion (Metzler et al., 2007), machine translation (Mirkin et al., 2009), question answering (Duboue and Chu-Carroll, 2006), see survey (Androutsopoulos and Malakasiotis, 2010) for completion. Most of previous work of paraphrase are on formal text. Recently with the rapidly growth 34 of microblogs and social media services, the computational linguistic community is moving its attention to informal genre of text (Java et al., 2007; Ritter et al., 2010). For example, (Zanzotto et al., 2011) defined the problem of redundancy detection in Twitter and proposed SVM models based on bag-of-word, syntactic content features to detect paraphrase. To provide a benchmark so as</context>
</contexts>
<marker>Mirkin, Specia, Cancedda, Dagan, Dymetman, Szpektor, 2009</marker>
<rawString>Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido Dagan, Marc Dymetman, and Idan Szpektor. 2009. Source-language entailment modeling for translating unknown terms. In ACL, pages 791–799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>641--648</pages>
<contexts>
<context position="10838" citStr="Mnih and Hinton, 2007" startWordPosition="1695" endWordPosition="1698">., 2010), semantic word similarities (Mikolov et al., 2013a), etc. Being distributed representation of words, word embeddings usually are learned using neural networks over a large raw corpus and has outperformed LSA for preserving linear regularities among words (Mikolov et al., 2013a). Due to its superior performance, we adopted word embeddings to estimate the similarities of sentence pairs. In our experiments, we used seven different word embeddings with different dimensions: word2vec (Mikolov et al., 2013b), Collobert and Weston embeddings (Collobert and Weston, 2008) and HLBL embeddings (Mnih and Hinton, 2007). Word2vec embeddings are distributed within the word2vec toolkit4 and they are 300-dimensional vectors learned from Google News Corpus which consists of over a 100 billion words. Collobert and Weston and HLBL embeddings are learned over a part of RCV1 corpus which consists of 63 millions words, with 25, 50, 100, or 200 dimensions and 50, 100 dimensions over 5-gram windows respectively. To obtain sentence representations, we simply summed up embedding vectors corresponding to the non-stopwords tokens in bag of words (BOW) of sentences. After that, we used cosine, Manhattan, Euclidean functions</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pages 641–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2830</pages>
<marker>Pedregosa, Ga¨el Varoquaux, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, et al. 2011. Scikit-learn: Machine learning in Python. The Journal of Machine Learning Research, 12:2825– 2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of Twitter conversations.</title>
<date>2010</date>
<pages>172--180</pages>
<contexts>
<context position="2005" citStr="Ritter et al., 2010" startWordPosition="300" endWordPosition="303">(Madnani and Dorr, 2010). Identifying paraphrase can improve the performance of several natural language processing (NLP) applications, such as query and pattern expansion (Metzler et al., 2007), machine translation (Mirkin et al., 2009), question answering (Duboue and Chu-Carroll, 2006), see survey (Androutsopoulos and Malakasiotis, 2010) for completion. Most of previous work of paraphrase are on formal text. Recently with the rapidly growth 34 of microblogs and social media services, the computational linguistic community is moving its attention to informal genre of text (Java et al., 2007; Ritter et al., 2010). For example, (Zanzotto et al., 2011) defined the problem of redundancy detection in Twitter and proposed SVM models based on bag-of-word, syntactic content features to detect paraphrase. To provide a benchmark so as to compare and develop different paraphrasing techniques in Twitter, the paraphrase and semantic similarity task in SemEval 2015 (Xu et al., 2015) requires the participants to determine whether two tweets express the same meaning or not and optionally a degree score between 0 and 1, which can be regarded as a binary classification problem. Paraphrasing task is very close to seman</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of Twitter conversations. pages 172–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In the 48th ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="10224" citStr="Turian et al., 2010" startWordPosition="1601" endWordPosition="1604"> looking up a manually-collected negation list with 29 negation words (e.g., scarcely, no, little). Also, we checked whether one sentence entails the other only using the named entity information which was provided in the dataset. Finally, we obtained nineteen other features. 2.6 Word Embedding Features Recently, deep learning has achieved a great success in the fields of computer vision, automatic speech recognition and natural language processing. As a consequence of its application in NLP, word embeddings have been building blocks in many tasks, e.g., named entity recognition and chunking (Turian et al., 2010), semantic word similarities (Mikolov et al., 2013a), etc. Being distributed representation of words, word embeddings usually are learned using neural networks over a large raw corpus and has outperformed LSA for preserving linear regularities among words (Mikolov et al., 2013a). Due to its superior performance, we adopted word embeddings to estimate the similarities of sentence pairs. In our experiments, we used seven different word embeddings with different dimensions: word2vec (Mikolov et al., 2013b), Collobert and Weston embeddings (Collobert and Weston, 2008) and HLBL embeddings (Mnih and</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In the 48th ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane Sari´c</author>
<author>Goran Glavas</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Ba&amp;quot;si´c.</title>
<date></date>
<booktitle>In *SEM 2012 and (SemEval</booktitle>
<pages>441--448</pages>
<location>Montr´eal, Canada.</location>
<marker>Sari´c, Glavas, Karan, </marker>
<rawString>Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder, and Bojana Dalbelo Ba&amp;quot;si´c. 2012. TakeLab: Systems for measuring semantic text similarity. In *SEM 2012 and (SemEval 2012), pages 441–448, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Elizabeth Weeds</author>
</authors>
<title>Measures and applications of lexical distributional similarity.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sussex.</institution>
<marker>Weeds, 2003</marker>
<rawString>Julie Elizabeth Weeds. 2003. Measures and applications of lexical distributional similarity. Ph.D. thesis, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
</authors>
<title>SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT).</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval),</booktitle>
<location>Denver, CO.</location>
<contexts>
<context position="2369" citStr="Xu et al., 2015" startWordPosition="358" endWordPosition="361">evious work of paraphrase are on formal text. Recently with the rapidly growth 34 of microblogs and social media services, the computational linguistic community is moving its attention to informal genre of text (Java et al., 2007; Ritter et al., 2010). For example, (Zanzotto et al., 2011) defined the problem of redundancy detection in Twitter and proposed SVM models based on bag-of-word, syntactic content features to detect paraphrase. To provide a benchmark so as to compare and develop different paraphrasing techniques in Twitter, the paraphrase and semantic similarity task in SemEval 2015 (Xu et al., 2015) requires the participants to determine whether two tweets express the same meaning or not and optionally a degree score between 0 and 1, which can be regarded as a binary classification problem. Paraphrasing task is very close to semantic textual similarity and textual entailment task (Marelli et al., 2014) since substantially these tasks all concentrated on modeling the underlying similarity between two sentences. The commonly-used features in these tasks can be categorized into several following groups: (1) string based which measures the sequence similarities of original strings with other</context>
</contexts>
<marker>Xu, Callison-Burch, Dolan, 2015</marker>
<rawString>Wei Xu, Chris Callison-Burch, and William B. Dolan. 2015. SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval), Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Kostas Tsioutsiouliklis</author>
</authors>
<title>Linguistic redundancy in Twitter. In</title>
<date>2011</date>
<booktitle>EMNLP,</booktitle>
<pages>659--669</pages>
<contexts>
<context position="2043" citStr="Zanzotto et al., 2011" startWordPosition="306" endWordPosition="309">g paraphrase can improve the performance of several natural language processing (NLP) applications, such as query and pattern expansion (Metzler et al., 2007), machine translation (Mirkin et al., 2009), question answering (Duboue and Chu-Carroll, 2006), see survey (Androutsopoulos and Malakasiotis, 2010) for completion. Most of previous work of paraphrase are on formal text. Recently with the rapidly growth 34 of microblogs and social media services, the computational linguistic community is moving its attention to informal genre of text (Java et al., 2007; Ritter et al., 2010). For example, (Zanzotto et al., 2011) defined the problem of redundancy detection in Twitter and proposed SVM models based on bag-of-word, syntactic content features to detect paraphrase. To provide a benchmark so as to compare and develop different paraphrasing techniques in Twitter, the paraphrase and semantic similarity task in SemEval 2015 (Xu et al., 2015) requires the participants to determine whether two tweets express the same meaning or not and optionally a degree score between 0 and 1, which can be regarded as a binary classification problem. Paraphrasing task is very close to semantic textual similarity and textual ent</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Tsioutsiouliklis, 2011</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Kostas Tsioutsiouliklis. 2011. Linguistic redundancy in Twitter. In EMNLP, pages 659–669.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>