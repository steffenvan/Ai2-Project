<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.89079">
MODULAR LOGIC GRAMMARS
Michael C. McCord
IBM Thomas J. Watson Research Center
P. 0. Box 218
</note>
<author confidence="0.413464">
Yorktown Heights, NY 10598
</author>
<sectionHeader confidence="0.726291" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999516">
This report describes a logic grammar formalism,
Modular Logic Grammars, exhibiting a high degree
of modularity between syntax and semantics. There
is a syntax rule compiler (compiling into Prolog)
which takes care of the building of analysis
structures and the interface to a clearly separated
semantic interpretation component dealing with
scoping and the construction of logical forms. The
whole system can work in either a one-pass mode or
a two-pass mode. In the one-pass mode, logical
forms are built directly during parsing through
interleaved calls to semantics, added automatically
by the rule compiler. In the two-pass mode, syn-
tactic analysis trees are built automatically in
the first pass, and then given to the (one-pass)
semantic component. The grammar formalism includes
two devices which cause the automatIcally built
syntactic structures to differ from derivation trees
in two ways: (1) There is a shift operator, for
dealing with left-embedding constructions such as
English possessive noun phrases while using right-
recursive rules (which are appropriate for Prolog
parsing). (2) There is a distinction in the syn-
tactic formalism between strong non-terminals and
weak non-terminals, which is important for distin-
guishing major levels of grammar.
</bodyText>
<sectionHeader confidence="0.99829" genericHeader="introduction">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999215549295775">
The term logic grammar will be used here, in
the context of natural language processing, to mean
a logic programming system (implemented normally
in Prolog), which associates semantic represent-
ations (normally in some version of predicate logic)
with natural language text. Logic grammars may have
varying degrees on modularity in their treatments
of syntax and semantics. There may or may not be
an isolatable syntactic component.
In writing metamorphosis grammars (Colmerauer,
1973), or definite clause grammars, DCG&apos;s, (a spe-
cial case of metamorphosis grammars, Pereira and
Warren. 1980), it is possible to build logical forms
directly in the syntax rules by letting non-
terminals have arguments that represent partial
logical forms being manipulated. Some of the ear-
liest logic grammars (e.g., Dahl, 1977) used this
approach. There is certainly an appeal in being
direct, but there are some disadvantages in this
lack of modularity. One disadvantage is that it
seems difficult to get an adequate treatment of the
scoping of quantifiers (and more generally
focalizers, McCord, 1981) when the building of log-
ical forms is too closely bonded to syntax. Another
disadvantage is just a general result of lack of
modularity: it can be harder to develop and un-
derstand syntax rules when too much is going on in
them.
The logic grammars described in McCord (1982,
1981) were three-pass systems, where one of the main
points of the modularity was a good treatment of
scoping. The first pass was the syntactic compo-
nent, written as a definite clause grammar, where
syntactic structures were explicitly built up in
the arguments of the non-terminals. Word sense
selection and slot-filling were done in this first
pass, so that the output analysis trees were actu-
ally partially semantic. The second pass was a
preliminary stage of semantic interpretation in
which the syntactic analysis tree was reshaped to
reflect proper scoping of modifiers. The third pass
took the reshaped tree and produced logical forms
in a straightforward way by carrying out modification
of nodes by their daughters using a modular system
of rules that manipulate semantic items -- consist-
ing of logical forms together with terms that de-
termine how they can combine.
The CHAT-80 system (Pereira and Warren, 1982,
Pereira, 1983) is a three-pass system. The first
pass is a purely syntactic comporent using an
extraposition grammar (Pereira, 1981) and producing
syntactic analyses in rightmost normal form. The
second pass handles word sense selection and slot-
and the third pass handles some scoping
phenomena and the final semantic interpretation.
One gets a great deal of modularity between syntax
and semantics in that the first component has no
elements of semantic interpretation at all.
In McCord (1984) a one-pass semantic inter-
pretation component, SEM, for the EPISTLE system
(Miller, Heidorn and Jensen, 1981) was described.
SEM has been interfaced both to the EPISTLE NLP
grammar (Heidorn, 1972, Jensen and Heidorn, 1983),
as well as to a logic grammar, SYNT, written as a
DCG by the author. These grammars are purely syn-
tactic and use the EPISTLE notion (op. cit.) of
approximate parse, which is similar to Pereira&apos;s
notion of rightmost normal form, but was developed
independently. Thus SYNT/SEM is a two-pass system
with a clear modularity between syntax and seman-
tics.
</bodyText>
<page confidence="0.998176">
104
</page>
<bodyText confidence="0.99995548">
In DCG&apos;s and extraposition grammars, the
building of analysis structures .(either logical
forms or syntactic trees) must be specified ex-
plicitly in the syntax rules. A certain amount of
modularity is then lost, because the grammar writer
must be aware of manipulating these structures, and
the possibility of using the grammar in different
ways is reduced. In Dahl and McCord (1983), a logic
grammar formalism was described, modifier structure
grammars (MSG&apos;s), in which structure-building (of
annotated derivation trees) is implicit in the
formalism. MSG&apos;s look formally like extraposition
grammars, with the additional ingredient that se-
mantic items (of the type used in McCord (1981))
can be indicated on the left-hand sides of rules,
and contribute automatically to the construction
of a syntactico-semantic tree much like that in
McCord (1981). These MSG&apos;s were used interpretively
in parsing, and then (essentially) the two-pass
semantic interpretation system of McCord (1981) was
used to get logical forms. So, totally there were
three passes in this system.
In this report, I wish to describe a logic
grammar system, modular logic grammars (MLG&apos;s),
with the following features:
</bodyText>
<listItem confidence="0.964661">
• There is a syntax rule compiler whIch takes care
</listItem>
<bodyText confidence="0.697531">
of the building of analysis structures and the
interface to semantic interpretation.
</bodyText>
<listItem confidence="0.942108875">
• There is a clearly separated semantic inter-
pretation component dealing with scoping and
the construction of logical forms.
• The whole system (syntax and semantics) can work
optionally in either a one-pass mode or a two-
pass mode.
• In the one-pass mode, no syntactic structures
are built, but logical forms are built directly
</listItem>
<bodyText confidence="0.893541375">
during parsing through interleaved calls to the
semantic interpretation component, added auto-
matically by the rule compiler.
matically produced) syntactic analysis trees
much more readable and natural linguistically.
In the absence of shift constructions, these
trees are like derivation trees, but only with
nodes corresponding to strong non-terminals.
</bodyText>
<listItem confidence="0.888759333333333">
• In an experimental MLG, the semantic component
handles all the scoping phenomena handled by
that in McCord (1981) and more than the semantic
component in McCord (1984). The logical form
language is improved over that in the previous
systems.
</listItem>
<bodyText confidence="0.999938636363636">
The MLG formalism allows for a great deal of modu-
larity in natural language grammars, because the
syntax rules can be written with very little
awareness of semantics or the building of analysis
structures, and the very same syntactic component
can be used in either the one-pass or the two-pass
mode described above.
Three other logic grammar systems designed with
modularity in mind are Hirschman and Puder (1982),
Abramson (1984) and Porto and Filgueiras (1984).
These will be compared with MLG&apos;s in Section 6.
</bodyText>
<sectionHeader confidence="0.994186" genericHeader="method">
2. THE MLG SYNTACTIC FORMALISM
</sectionHeader>
<bodyText confidence="0.980493647058824">
The syntactic component for an MLG consists
of a declaration of the strong non-terminals, fol-
lowed by a sequence of MLG syntax rules. The dec-
laration of strong non-terminals is of the form
strongnonterminals(NT1.NT2. .NTn.ni1).
where the NTi are the desired strong non-terminals
(only their principal functors are indicated).
Non-terminals that are not declared strong are
called weak. The significance of the strong/weak
distinction will be explained below.
MLG syntax rules are of the form
A &gt; B
In the two-pass mode, the calls to the semantic
interpretation component are not interleaved,
but are made in a second pass, operating on
syntactic analysis trees produced (automat-
ically) in the first pass.
</bodyText>
<listItem confidence="0.515224583333333">
• The syntactic formalism includes a, device,
called the shift operator, for dealing with
left-embedding constructions such as English
possessive noun phrases (&amp;quot;my wife&apos;s brother&apos;s
friend&apos;s car&amp;quot;) and Japanese relative clauses.
The shift operator instructs the rule compiler
to build the structures appropriate for left-
embedding. These structures are not derivation
trees, because the syntax rules are right-re-
cursive, because of the top-down parsing asso-
ciated with Prolog.
• There is a distinction in the syntactic
</listItem>
<bodyText confidence="0.99192225">
formalism between strong non-terminals and weak
non-terminals, which is important for distin-
guishing major levels of grammar and which
simplifies the. working of semantic interpreta-
tion. This distinction also makes the (auto-
where A is a non-terminal and B is a rule body. A
rule body is any combination of surface terminals,
logical terminals, goals, shifted non-terminals,
non-terminals, the symbol &apos;nil&apos;, and the cut symbol
&apos;/&apos;, using the sequencing operator &apos;:&apos; and the &apos;or&apos;
symbol &apos;1&apos;. (We represent left-to-right sequencing
with a colon instead of a comma, as is often done
in logic grammars.) These rule body elements are
Prolog terms (normally with arguments), and they
are distinguished formally as follows.
A surface terminal is of the form +A, where A
is any Prolog term. Surface terminals corre-
spond to ordinary terminals in DCG&apos;s (they match
elements of the surface word string), and the
notation is often (Al in DCG&apos;s.
</bodyText>
<listItem confidence="0.9179016">
• A logical terminal is of the form Op-LF, where
Op is a modification operator and LF is a logical
form. Logical terminals are special cases of
semantic Rmins, the significance of which will
be explained below. Formally, the rule compiler
</listItem>
<page confidence="0.998378">
105
</page>
<bodyText confidence="0.8558895">
recognizes them as being terms of the form A-8.
There can be any number of them in a rule body.
</bodyText>
<listItem confidence="0.987282">
• A goal is of the form $A, where A is a term re-
</listItem>
<bodyText confidence="0.726463">
presenting a Prolog goal. (This is the usual
provision for Prolog procedure calls, which are
often indicated by enclosure in braces in
DCG&apos;s.)
</bodyText>
<listItem confidence="0.8415618">
• A shifted non-terminal is either of the form %A,
or of the form FA, where A is a weak non-
terminal and F is any term. (In practice, F
will be a list of features.) As indicated in
the Introduction, the shift operator &apos;%&apos; is used
to handle left-embedding constructions in a
right-recursive rule system.
• Any rule body element not of the above four
forms and not &apos;nil&apos; or the cut symbol is taken
to be a non-terminal.
</listItem>
<bodyText confidence="0.986349655172414">
A terminal is either a surface terminal or a
logical terminal. Surface terminals are building
blocks for the word string being analyzed, and
logical terminals are building blocks for the
analysis structures.
A syntax rule is called strong or weak,
cording as the non-terminal on its left-hand side
is strong or weak.
It can be seen that on a purely formal level,
the only differences between MLG syntax rules and
DCG&apos;s are (1) the appearance of logical terminals
in rule bodies of MLG&apos;s, (2) the use of the shift
operator, and (3) the distinction between strong
and weak non-terminals. However, for a given lin-
guistic coverage, the syntactic component of an MLG
will normally be more compact than the corresponding
DCG because structure-building must be explicit in
DCG&apos;s. In this report, the arrow .--&gt;&apos; (as opposed
to &apos;==&gt;&apos;) will be used for for DCG rules, and the
same notation for sequencing, tsrminals, etc.. will
be used for DCG&apos;s as for MLG&apos;s.
What is the significance of the strong/weak
distinction for non-terminals and rules? Roughly,
a strong rule should be thought of as introducing
a new level of grammar, whereas a weak rule defines
analysis within a level. Major categories like
sentence and noun phrase are expanded by strong
rules, but auxiliary rules like the recursive rules
that find the postmodifiers of a verb are weak
rules. An analogy with ATN&apos;s (Woods, 197)) is teat
strong non-terminals are like the start categories
of subnetworks (with structure-building POP arcs
for termination), whereas weak non-terminals are
like internal nodes.
In the one-pass mode, the MLG rule compiler
makes the following distinction for strong and weak
rules. In the Horn clause translation of a strong
rile, a call to the semantic interpretation compo-
nent is compiled in at the end of the clause. The
non-terminals appearing in rules (both strong and
weak) are given extra arguments which manipulate
semantic structures used in the call to semantic
interpretation. No such call to semantics is com-
piled in for weak rules. Weak rules only gather
information to be used in the call to semantics made
by the next higher strong rule. (Also, a shift
generates a call to semantics.)
In the two-pass mode, where syntactic analysis
trees are built during the first pass, the rule
compiler builds in the construction of a tree node
corresponding to every strong rule. The node is
labeled essentially by the non-terminal appearing
on the left-hand side of the strong rule. (A shift
also generates the construction of a tree node.)
Details of rule compilation will be given in the
next section.
As indicated above, logical terminals, and more
generally semantic items, are of the form
</bodyText>
<equation confidence="0.559238333333333">
Operator-LogicalForm.
The Operator is a term which determines how the
semantic item can combine with ocher semantic items
</equation>
<bodyText confidence="0.867919714285714">
during semantic interpretation. In this combina-
tion, new semantic items are formed which are no
longer logical terminals.) Logical terminals are
most typically associated with lexical items, al-
though they are also used to produce certain non-
lexical ingredients in logical form analysis. An
example for the lexical item &amp;quot;each&amp;quot; might be
</bodyText>
<equation confidence="0.400246">
Q/P - each(P,Q).
</equation>
<bodyText confidence="0.9983065">
Here the operator Q/P is such that when the &amp;quot;each&amp;quot;
item modifies, say, an item having logical form
man(X), P gets unified with man(X), and the re-
sulting semantic item is
</bodyText>
<equation confidence="0.771472">
@Q - each(rtan(X),Q)
</equation>
<bodyText confidence="0.970701909090909">
where Q is an operator which causes Q to get uni-
fied wit4 the logical form of a further modificand.
Details sf the use of semantic items will be given
in Section 4.
Now let us look at the syntactic component of
a sample MLG which covers the same ground as a
well-known DCG. The following DCG is taken essen-
tially from Pereira and Warren (1980). It is the
sort of DCG that builds logical forms directly oy
manipulating partial logical forms in arguments of
the grammar symbols.
</bodyText>
<equation confidence="0.950974571428572">
sent(?) --&gt; np(X,P1,P): vp(X,P1).
np(X,P:,P) det(P2,F1,P): noun(X,P3):
reiclause(X,P3,P2).
np(X,?,P) --&gt; name(X).
vp(X,P) transverb(X,Y,P1): np(Y,P1,P).
vp(X,P) --&gt; intransverb(X,P).
relclause(X,PI,P1E.P2) --&gt; +that: vp(X,P2).
reiclause(*,P,P) --&gt; nil.
det(Pl,P2,P) --&gt; +D: $dt(D,P1,P2,P).
noun(X,P) --&gt; +N: Sn(N,X,P).
name(X) --&gt; +X: $nm(X).
transverb(X,Y,P) --&gt; +V: $tv(V,X,Y,P).
intrausverb(X,P) --&gt; +V: $iv(V,X,P).
/* Lexicon */
</equation>
<bodyText confidence="0.8703515">
n(man,X,man(X)). n(woman,X,woman(X)).
nm(john). nm(mary).
</bodyText>
<page confidence="0.99752">
106
</page>
<bodyText confidence="0.99972625">
dt(every,PI,P2,all(P1,P2)).
dt(a,PI,P2,ex(P1,P2)).
tv(loves,X,Y,love(X,Y)).
iv(lives,X,live(X)).
The syntactic component of an analogous MLG is as
follows. The lexicon is exactly the same as that
of the preceding DCG. For reference below, this
grammar will be called MLGRAM.
</bodyText>
<equation confidence="0.957382846153846">
strongnonterminals(sent.np.relclause.det.ni1).
sent ==&gt; np(X): vp(X).
np(X) ==&gt; det: noun(X): relclause(X).
np(X) ==&gt; name(X).
vp(X) ==&gt; transverb(X,Y): np(Y).
vp(X) ==&gt; intransverb(X).
relclause(X) ==&gt; 4-that: vp(X).
relclause(*) ==&gt; nil.
det ==&gt; +D: Sdt(D,P1,P2,P): P2/P1-P.
noun(X) ==&gt; +N: Sn(N,X,P): 1-P.
name(X) ==&gt; +X: Snm(X).
transverb(X,Y) ==&gt; +V: Stv(V,X,Y,P): 1-P.
intransverb(X) ==&gt; +V: Siv(V,X,P): I-P
</equation>
<bodyText confidence="0.953913388888889">
This small grammar illustrates all the ingredients
of MLG syntax rules except the shift operator. The
shift will be illustrated below. Note that &apos;sent&apos;
and &apos;np&apos; are strong categories but &apos;vp&apos; is weak.
A result is that there will be no call to semantics
at the end of the &apos;vp&apos; rule. Instead, the semantic
structures associated with the verb and object are
passed up to the &apos;sent&apos; level, so that the subject
and object are &amp;quot;thrown into the same pot&amp;quot; for se-
mantic combination. (However, their surface order
is not forgotten.)
There are only two types of modification op-
erators appearing in the semantic items of this MLG:
&apos;1&apos; and P2/Pl. The operator &apos;I&apos; means &apos;left-
conjoin&apos;. Its effect is to left-conjoin its asso-
ciated logical form to the logical form of the
modificand (although its use in this small grammar
is almost trivial). The operator P2/P1 is associ-
ated with determiners, and its effect has been il-
lustrated above.
The semantic component will be given below in
Section 4. A sartple semantic analysis for the
sentence &amp;quot;Every man that lives loves a woman&amp;quot; is
all(man(X1)&amp;live(X1),ex(woman(X2),love(Xl.X2))).
This is the same as for the above DCG. We will also
show a sample parse in the next section.
A fragment of an MLG illustrating the use of
the shift in the treatment of possessive noun
phrases is as follows:
np ==&gt; det: npl.
npl ==&gt; premods: noun: np2.
np2 ==&gt; postmods.
np2 ==&gt; poss:
The idea of this fragment can be described in a
rough procedural way, as follows. In parsing an
np, one reads an ordinary determiner (det), then
goes to npl. In npl, one reads several premodifiers
(premods), say adjectives, then a head noun, then
goes to np2. In np2, one may either finish by
reading postmodifiers (postmods), OR one may read
an apostrophe-s (poss) and then SHIFT back to npl.
Illustration for the noun phrase, &amp;quot;the old man&apos;s
dusty hat&amp;quot;:
the old man &apos;s
np det npl premods noun np2 poss
dusty hat (nil)
premods noun np2 postmods
When the shift is encountered, the syntactic
structures (in the two-pass mode) are manipulated
(in the compiled rules) so that the initial np (&amp;quot;the
old man&amp;quot;) becomes a left-embedded sub-structure of
the larger np (whose head is &amp;quot;hat&amp;quot;). But if no
apostrophe-s is encountered, then the structure for
&amp;quot;the old man&amp;quot; remains on the top level.
</bodyText>
<sectionHeader confidence="0.960621" genericHeader="method">
3. COMPILATION OF MLG SYNTAX RULES
</sectionHeader>
<bodyText confidence="0.995401054054054">
In describing rule compilation, we will first
look at the two-pass mode, where syntactic struc-
tures are built in the first pass, because the re-
lationship of the analysis structures to the syntax
rules is more direct in this case.
The syntactic structures manipulated by the
compiled rules are represented as syntactic items,
which are terms of the form
syn(Features,Daughters)
where Features is a feature list (to be defined), and
Daughters is a list consisting of syntactic items
and terminals. Both types of terminal (surface and
logical) are included in Daughters, but the dis-
playing procedures for syntactic structures can
optionally filter out one or the other of the two
types. A feature list is of the form nt:Argl, where
nt is the principal functor of a strong non-terminal
and Argl is its first argument. (If nt has no ar-
guments, we take Argl=nil.) It is convenient, in
large grammars, to use this first argument Argl to
hold a list (based on the operator &apos;:&apos;) of gram-
matical features of the phrase analyzed by the
non-terminal (like number and person for noun
phrases).
In compiling DCG rules into Prolog clauses,
each non-terminal gets two extra arguments treated
as a difference list representing the word string
analyzed by the non-terminal. In compiling MLG
rules, exactly the same thing is done to handle word
strings. For handling syntactic structures, the
MLG rule compiler adds additional arguments which
manipulate &apos;syn&apos; structures. The number of addi-
tional arguments and the way they are used depend
on whether the non-terminal is strong or weak. If
the original non-terminal is strong and has the form
nt(Xl, Xn)
then in the compiled version we will have
</bodyText>
<page confidence="0.993814">
107
</page>
<bodyText confidence="0.995855317073171">
nt(Xl, Xn, Syn, Strl,Str2).
Here there is a single syntactic structure argument,
Syn, representing the syntactic structure of the
phrase associated by at with the word string given
by the difference list (Strl, Str2).
On the other hand, when the non-terminal at
is weak, four syntactic structure arguments are
added, producing a compiled predication of the form
nt(Xl, Xn, SynO,Syn, Modsl,Mods2, Strl,Str2).
Here the pair (Modsl, Mods2) holds a difference list
for the sequence of structures analyzed by the weak
non-terminal nt. These structures could be &apos;syn&apos;
structures or terminals, and they will be daughters
(modifiers) for a &apos;syn&apos; structure associated with
the closest higher call to a strong non-terminal
-- let us call this higher &apos;syn&apos; structure the ma-
trix &apos;syn&apos; structure. The other pair (SynO, Syn)
represents the changing view of what the matrix
&apos;syn&apos; structure actually should be, a view that may
change because a shift is encountered while satis-
fying at. SynO represents the version before sat-
isfying nt, and Syn represents the version after
satisfying at. If no shift is encountered while
satisfying at, then Syn will just equal SynO. But
if a shift is encountered, the old version SynO will
become a daughter node in the new version Syn.
In compiling a rule with several non-terminals
in the rule body, linked by the sequencing operator
the argument pairs (SynO, Syn) and (Modsl,
Mods2) for weak non-terminals are linked, respec-
tively, across adjacent non-terminals in a manner
similar to the linking of the difference lists for
word-string arguments. Calls to strong non-
terminals associate &apos;syn&apos; structure elements with
the modifier &apos;lists, just as surface terminals are
associated with elements of the word-string lists.
Let us look now at the compilation of a set
of rules. We will take the noun phrase grammar
fragment illustrating the shift and shown above in
Section 2, and repeated for convenience here, to-
gether with declarations of strong non-terminals.
</bodyText>
<equation confidence="0.92562715">
strongnonterminals(np.det.noun.poss.ni1).
np ==&gt; det: npl.
npl ==&gt; premods: noun: np2.
np2 ==&gt; postmods.
np2 ==&gt; pass: Upl.
The compiled rules are as follows:
np(Syn, Str1,Str3) &lt;-
det(Mod, Str1,Str2) &amp;
npl(syn(np:nil,Mod:Mods),Syn,
Mods,nil, Str2,Str3).
npl(Synl,Syn3, Modsl,Mods3, Str1,Str4) &lt;-
premods(SynI,Syn2, Modsl,Mod:Mods2,
Strl,Str2) &amp;
noun(Mod, Str2,Str3) &amp;
np2(Syn2,Syn3, Mods2,Mods3, Str3,Str4).
np2(Syn1,Syn2, Modsl,Mods2, Strl,Str2) &lt;-
postmods(Syn1,Syn2, Modsl,Mods2, Strl,Str2).
np2(syn(Feas,Mods0),Syn, Mod:Modsl,Modsl,
Str1,Str3) &lt;-
poss(Mod, Strl,Str2) &amp;
</equation>
<bodyText confidence="0.969919142857143">
npl(syn(Feas,syn(Feas,Mods0):Mods2),Syn,
Mods2,nil, Str2,Str3).
In the first compiled rule, the structure Syn
to be associated with the call to &apos;np&apos; appears again
in the second matrix structure argument of &apos;npl&apos;.
The first matrix structure argument of &apos;npl&apos; is
syn(np:nil,Mod:Mods).
and this will turn out to be the value of Syn if
no shifts are encountered. Here Mod is the &apos;syn&apos;
structure associated with the determiner &apos;&lt;let&apos;, and
Mods is the list of modifiers determined further
by &apos;npl&apos;. The feature list np:nil is constructed
from the leading non-terminal &apos;np&apos; of this strong
rule. (It would have been np:Argl if np had a
(first) argument Argl.)
In the second and third compiled rules, the
matrix structure pairs (first two arguments) and
the modifier difference list pairs are linked in a
straightforward way to reflect sequencing.
The fourth rule shows the effect of the shift.
Here syn(Feas,Mods0), the previous &amp;quot;conjecture&amp;quot; for
the matrix structure, is now made simply the first
modifier in the larger structure
syn(Feas.syn(Feas,Mods0):Mods2)
which becomes the new &amp;quot;conjecture&amp;quot; by being placed
in the first argument of the further call to &apos;npl&apos;.
If the shift operator had been used in its binary
form FOUpl, then the new conjecture would be
syn(NT:F,syn(NT:FO,Mods0):Mods2)
where the old conjecture was syn(NT:F,Mods0). In
larger grammars, this allows one to. have a com-
pletely correct feature list NT:F0 for the left-
embedded modifier.
To illustrate the compilation of terminal
symbols, let us look at the rule
det ==&gt; +D: Sdt(D,P1,P2,P): P2/P1-P.
from the grammar MLGRAM in Section 2. The compiled
rule is
det(syn(detinil,*D:P2/P1-P:nil), D.Str,Str) &lt;-
dt(D,PI,P2,P).
Note that both the surface terminal +D and the
logical terminal P2/P1-P are entered as modifiers
of the &apos;dee node. The semantic interpretation
component looks only at the logical terminals, but
in certain applications it is useful to be able to
see the surface terminals in the syntactic struc-
tures. As mentioned above, the display procedures
for syntactic structures can optionally show only
one type of terminal.
</bodyText>
<page confidence="0.995948">
108
</page>
<bodyText confidence="0.998556">
The display of the syntactic structure of the
sentence &amp;quot;Every man loves a woman&amp;quot; produced by
MLGRAH is as follows.
now we need a list of structures because of a
raising phenomenon necessary for proper scoping,
which we will discuss in Sections 4 and 5.
</bodyText>
<figure confidence="0.904094">
sentence: nil
np:Xl
det:nil
X2/X3-all(X3,X2)
1-man(X1)
1-love(X1,X4)
np:X4
det:nil
X5/X6-ex(X6,X5)
1-woman(X4)
</figure>
<bodyText confidence="0.977602448717949">
Note that no &apos;vp&apos; node is shown in the parse tree;
&apos;vp&apos; is a weak non-terminal. The logical form
produced for this tree by the semantic component
given in the next section is
all(man(X1), ex(woman(X2),love(X1,X2))).
Now let us look at the compilation of syntax
rules for the one-pass mode. In this mode, syn-
tactic structures are not built, but semantic
structures are built up directly. The rule compiler
adds extra arguments to non-terminals for manipu-
lation of semantic structures, and adds calls to
the top-level semantic interpretation procedure,
&apos;semant&apos;.
The procedure &apos;semant&apos; builds complex semantic
structures out of simpler ones, where the original
building blocks are the logical terminals appearing
in the MLG syntax rules. In this process of con-
struction, it would be possible to work with se-
mantic items (and in fact a subsystem of the rules
do work directly with semantic items), but it ap-
pears to be more efficient to work with slightly
more elaborate structures which we call augmented
semantic items. These&apos; are terms of the form
sem(Feas,0p,LF),
where Op and LF are such that Op-LF is an ordinary
semantic item, and Feas is either a feature list
or the list terminal:nil. The latter form is used
for the initial augmented semantic items associated
with logical terminals.
As in the two-pass mode, the number of analysis
structure arguments added to a non-terminal by the
compiler depends on whether the non-terminal is
strong or weak. If the original non-terminal is
strong and has the form
nt(Xl, Xn)
then in the compiled version we will have
nt(Xl, Xn, Semsl,Sems2, Strl,Str2).
Here (Semsl, Sems2) is a difference list of aug-
mented semantic items representing the list of se-
mantic structures for the phrase associated by at
with the word string given by the difference list
(Strl, Str2). In the syntactic (two-pass) mode,
only one argument (for a &apos;syn&apos;) is needed here, but
When the non-terminal nt is weak, five extra
arguments are added, producing a compiled predi-
cation of the form
nt(Xl, Xn, Feas, Sems0,Sems, Semsl,Sems2,
Strl,Str2).
Here Feas is the feature list for the matrix strong
non-terminal. The pair (Sems0, Sems) represents
the changing &amp;quot;conjecture&amp;quot; for the complete list of.
daughter (augmented) semantic items for the matrix
node, and is analogous to first extra argument pair
in the two-pass mode. The pair (Semsl, Sems2) holds
a difference list for the sequence of semantic items
analyzed by the weak non-terminal at. Semsl will
be a final sublist of Sems0, and Sems2 will of
course be a final sublist of Semsl.
For each strong rule, a call to &apos;semant&apos; is
added at the end of the compiled form of the rule.
The form of the call is
semant(Feas, Sems, Semsl,Sems2).
Here ....eas is the feature list for the non-terminal
on the left-hand side of the rule. Sems is the final
version of the list of daughter semantic items
(after all adjustments for shifts) and (Semsl,
Sems2) is the difference list of semantic items
resulting from the semantic interpretation for this
level. (Think of Feas and Sems as input to
&apos;semant&apos;, and (Semsl, Sems2) as output.) (Semsl,
Sems2) will be the structure arguments for the
non-terminal on the left-hand side of the strong
rule. A call to &apos;semant&apos; is also generated when a
shift is encountered, as we will see below. The
actual working of .semant. is the topic of the next
section.
For the shift grammar fragment shown above,
the compiled rules are as follows.
</bodyText>
<table confidence="0.678857">
np(Sems,Sems0, Strl,Str3) &lt;-
det(Semsl,Sems2, Strl,Str2)
Semsl,Sems3, Sems2,nil, Str2,Str3)
semant(np:nil, Sems3, Sems,Sems0).
npl(Feas, Semsl,Sems3, Sems4,Sems7, Strl,Str4) &lt;-
premods(Feas, Semsl,Sems2, Sems4,Sems5,
Strl,Str2)
noun(Sems5,Sems6, Str2,Str3)
np2(Feas, Sems2,Sems3, Sems6,Sems7, Str3,Str4).
np2(Feas, Semsl,Sems2, Sems3,Sems4, Strl,Str2) &lt;-
postmods(Feas, Semsl,Sems2, Sems3,Sems4,
Strl,Str2).
np2(Feas, Semsl.Sems4, Sems5,Sems6, Strl,Str3) &lt;-
poss(Sems5,Sems6, Strl,Str2)
semant(Feas, Semsl, Sems2,Sems3) &amp;
npl(Feas, Sems2,Sems4, Sems3,nil, Str2,Str3).
</table>
<bodyText confidence="0.990709666666667">
In the first compiled rule (a strong rule), the pair
(Sems, Sems0) is a difference list of the semantic
items analyzing the noun phrase. (Typically there
</bodyText>
<page confidence="0.996491">
109
</page>
<bodyText confidence="0.999848666666667">
will just be one element in this list, but there
can be more when modifiers of the noun phrases
contain quantifiers that cause the modifiers to get
promoted semantically to be sisters of the noun
phrase.) This difference list is the output of the
call to &apos;semant&apos; compiled in at the end of the first
rule. The input to this call is the list Sems3
(along with the feature list np:nil). We arrive
at Sems3 as follows. The list Semsl is started by
the call to &apos;det&apos;; its first element is the
determiner (if there is one), and the list is con-
tinued in the list Sems2 of modifiers determined
further by the call to &apos;npl&apos;. In this call to &apos;npl&apos;,
the initial list Semsl is given in the second ar-
gument of &apos;npl&apos; as the &amp;quot;initial version&amp;quot; for the
final list of modifiers of the noun phrase. Sems3,
being in the next argument of &apos;npl&apos;, is the &amp;quot;final
version&amp;quot; of the np modifier list, and this is the
list given as input to &apos;secant&apos;. If the processing
of &apos;npl&apos; encounters no shifts, then Sems3 will just
equal Semsl.
In the second compiled rule (for &apos;npl&apos;), the
&amp;quot;versions&amp;quot; of the total list of modifiers are linked
in a chain
</bodyText>
<sectionHeader confidence="0.430155" genericHeader="method">
(Semsl, Sems2, Sems3)
</sectionHeader>
<bodyText confidence="0.998142333333333">
in the second and third arguments of the weak non-
terminals. The actual modifiers produced by this
rule are linked in a chain
</bodyText>
<equation confidence="0.50847">
(Sems4, Sems5, Sems6, Sems7)
</equation>
<bodyText confidence="0.993633404761905">
in the fourth and fifth arguments of the weak non-
terminals and the first and second arguments of the
strong non-terminals. A similar situation holds
for the first of the &apos;np2&apos; rules.
In the second &apos;np2&apos; rule, a shift is encount-
ered, so a call to &apos;semant&apos; is generated. This is
necessary because of the shift of levels; the mod-
ifiers produced so far represent all the modifiers
in an np, and these must be combined by &apos;semant&apos;
to get the analysis of this np. As input to this
call to &apos;secant&apos;, we take the list Semsl, which is
the current version of the modifiers of the matrix
np. The output is the difference list Sems2,
Sems3). Sems2 is given to the succeeding call to
&apos;npl&apos; as the new current version of the matrix
modifier list. The tail Sems3 of the difference
list output by &apos;semant&apos; is given to &apos;npl&apos; in its
fourth argument to receive further modifiers. Sems4
is the final version of the matrix modifier list,
determined by &apos;npl&apos;, and this information is also
put in the third argument of &apos;np2&apos;. The difference
list (Sems5, Sems6) contains the single element
produced by &apos;poss., and this list tails off the list
Semsl.
When a semantic item Op-LF occurs in a rule
body, the rule compiler inserts the augmented se-
mantic item sem(terminal:ni1,0p,LF). As an example,
the weak rule
transverb(X,Y) ==&gt; +V: $tv(V,X,Y,P): 1-P.
compiles into the clause
transverb(X,Y, Feas, Semsl ,Semsl,
sem(terminal:ni1,1,P):Sems2,Sems2,
V.Str,Str) &lt;-
tv(V,X,Y,P).
The strong rule
det ==&gt; +D: $dt(D,P1,P2,P): P2/P1-P.
compiles into the clause
det(Semsl,Sems2, D.Sems4,Sems4)&lt;-
dt(D,PI,P2,P) &amp;
semant(det:nil,
sem(terminal:nil,P2/P1,P):nil,
Semsl ,Sems2).
</bodyText>
<sectionHeader confidence="0.977978" genericHeader="method">
4. SEMANTIC INTERPRETATION FOR MLC&apos;S
</sectionHeader>
<bodyText confidence="0.9998615">
The semantic interpretation schemes for both
the one-pass mode and the two-pass mode share a
large core of common procedures; they differ only
at the top level. In both schemes, augmented se-
mantic items are combined with one another, forming
more and more complex items, until a single item
is constructed which represents the structure of
the whole sentence. In this final structure, only
the logical form component is of interest; the other
two components are discarded. We will describe the
top levels for both modes, then describe the common
core.
The top level for the one-pass mode is simpler,
because semantic interpretation works in tandem with
the parser, and does not itself have to go through
the parse tree. The procedure &apos;semant&apos;, which has
interleaved calls in the compiled syntax rules,
essentially is the top-level procedure, but there
is some minor cleaning up that has to be done. If
the top-level non-terminal is &apos;sentence&apos; (with no
arguments), then the top-level analysis procedure
for the one-pass mode can be
</bodyText>
<equation confidence="0.98956775">
analyze(Sent) &lt;-
sentence(Sems,nil,Sent,nil) &amp;
semant(top:nil,Sems,sem(*,*,LF):nil,nil) &amp;
outlogform(LF).
</equation>
<bodyText confidence="0.990626">
Normally, the first argument, Sems, of &apos;sentence&apos;,
will be a list containing a single augmented se-
mantic item, and its logical form component will
he the desired logical form. However, for some
grammars, the additional call to &apos;semant&apos; is needed
to complete the modification process. The procedure
&apos;outlogform&apos; simplifies the logical form and outputs
it.
The definition of &apos;semant&apos; itself is given in
a single clause:
</bodyText>
<equation confidence="0.9676665">
semant(Feas,Sems,Sems2,Sems3) &lt;-
reorder(Sems,Sems1) &amp;
</equation>
<bodyText confidence="0.96665875">
modlist(Semsl,sem(Feas,id,t),
Sem,Sems2,Sem:Sems3).
Here, the procedure &apos;reorder&apos; takes the list Sees
of augmented semantic items to be combined and re-
</bodyText>
<page confidence="0.990517">
110
</page>
<bodyText confidence="0.94217365">
orders it (permutes it), to obtain proper (or most
likely) scoping. This procedure belongs to the
common core of the two methods of semantic inter-
pretation, and will be discussed further below.
The procedure Imodlise does the following. A call
modlist(Sems,SemO,Sem,Semsl,Sems2)
takes a list Sems of (augmented) semantic items and
combines them with (lets them modify) the item Sem0,
producing an item Sem (as the combination), along
with a difference list (Semsl, Sems2) of items which
are promoted to be sisters of Sem. The leftmost
member of Sees acts as the outermost modifier.
Thus, in the definition of &apos;semane, the result list
Semsl of reordering acts on the trivial item
sem(Feas,id,t) to form a difference list (Sems2,
Sem:Sems3) where the result Sem is right-appended
to its sisters. &apos;modlist&apos; also belongs to the
common core, and will be defined below.
The top level for the two-pass system can be
defined as follows.
</bodyText>
<figure confidence="0.8958518">
analyze2(Sent) &lt;-
• sentence(Syn,Sent,nil) &amp;
synsem(Syn,Sems,nil) &amp;
semant(top:nil,Sems,sem(*,*,LF):nil,nil) &amp;
outlogform(LF).
</figure>
<bodyText confidence="0.811644615384615">
The only difference between this and &apos;analyze&apos; above
is that the call to &apos;sentence&apos; produces a syntactic
item Syn, and this is given to the procedure
&apos;synsem&apos;. The latter is the main recursive proce-
dure of the two-pass system. A call
synsem(Syn,Semsl,Sems2)
takes a syntactic item Syn and produces a difference
list (Semsl, Sems2) of augmented semantic items
representing the semantic structure of Syn. (Typ-
ically, this list will just have one element, but
it can have more if modifiers get promoted to sis-
ters of the node.)
The definition of &apos;synsem&apos; is as follows.
</bodyText>
<equation confidence="0.964906">
synsem(syn(Feas,Mods),Sems2,Sems3) &lt;-
synsemlist(Mods,Sems) &amp;
reorder(Sems,Sems1) &amp;
</equation>
<bodyText confidence="0.737385260869565">
modlist(Semsl,sem(Feas,id,t),
Sem,Sems2,Sem:Sems3).
Note that this differs from the definition of
&apos;semant&apos; only in that &apos;synsem&apos; must first
recursively process the daughters Mods of its input
syntactic item before calling &apos;reorder&apos; and
modlist&apos;. The procedure &apos;synsemlist&apos; that proc-
esses the daughters is defined as follows.
synsemlist(syn(Feas,Mods0):Mods,Sems1) &lt;-
synsem(syn(Feas,Mods0),Semsl,Sems2) &amp;
synsemlist(Mods,Sems2).
synsemlist((0p-LF):Mods,
sem(terminal:ni1,0p,LF):Sems) &lt;-
synsemlist(Mods,Sems).
synsemlist(Mod:Mods,Sems) &lt;-
synsemlist(Mods,Sems).
synsemlist(nil,ni1).
The first clause calls &apos;synsem&apos; recursively when
the daughter is another &apos;syn&apos; structure. The second
clause replaces a logical terminal by an augmented
semantic item whose feature list is terminal:nil.
The next clause ignores any other type of daughter
(this would normally be a surface terminal).
</bodyText>
<figureCaption confidence="0.35451875">
Now we can proceed to the common core of the
two semantic interpretation systems. The procedure
modlist&apos; is defined recursively in a straightfor-
ward way:
</figureCaption>
<table confidence="0.852852">
modlist(Sem:Sems, Sem0, Sem2, Semsl,Sems3) &lt;-
modlist(Sems, Sem0, Semi, Sems2,Sems3) &amp;
modify(Sem, Semi, Sem2, Semsl,Sems2).
modlist(nil, Sem, Sem, Sems,Sems).
Here &apos;modify&apos; takes a single item Sem and lets it
operate on Semi, giving Sem2 and a difference list
(Semsl, Sems2) of sister items. Its definition is
modify(Sem, Semi, Semi, Sem2:Sems,Sems) &lt;-
</table>
<equation confidence="0.90786">
raise(Sem,Seml,Sem2)
modify(sem(*,0p,LF),
sem(Feas,Opl,LF1),
sem(Feas,0p2,LF2), Sems,Sems) &apos;-
mod(Op-LF, Opl-LFI, 0p2-LF2).
</equation>
<bodyText confidence="0.96860475862069">
Here &apos;raise&apos; is responsible for raising the
item Semi so that it becomes a sister of the item
Semi; Sem2 is a new version of Semi after the
raising, although in most cases, Sem2 equals Semi_
Raising occurs for a noun phrase like &amp;quot;a chicken
in every pot&amp;quot;, where the quantifier &amp;quot;every&amp;quot; has
higher scope than the quantifier &amp;quot;a&amp;quot;. The semantic
item for &amp;quot;every pot&amp;quot; gets promoted to a left sister
of that for &amp;quot;a chicken&amp;quot;. &apos;raise&apos; is defined bas-
ically by a system of unit clauses which look at
specific types of phrases. For the small grammar
MLGRAM of Section 2, no raising is necessary, and
the definition of &apos;raise&apos; can just be omitted.
The procedures &apos;raise&apos; and &apos;reorder&apos; are two
key ingredients of reshaping (the movement of se-
mantic items to handle scoping problems), which was
discussed extensively in McCord (1982, 1981). In
those two systems, reshaping was a separate pass
of semantic interpretation, but here, as in McCord
(1984), reshaping is interleaved with the rest of
semantic interpretation. In spite of the new top-
level organization for semantic interpretation of
MLG&apos;s, the low-level procedures for raising and
reordering are basically the same as in the previous
systems, and we refer to the previous reports for
further discussion.
The procedure &apos;mod&apos;, used in the second clause
for &apos;modify&apos;, is the heart of semantic interpreta-
tion.
</bodyText>
<sectionHeader confidence="0.535069" genericHeader="method">
mod(Sem, Seml, Sem2)
</sectionHeader>
<bodyText confidence="0.9848892">
means that the (non-augmented) semantic item Sem
modifies (combines with) the item Semi to give the
item Sem2. &apos;mod&apos; is defined by a system consisting
basically of unit clauses which key off the mod-
ification operators appearing in the semantic items.
</bodyText>
<page confidence="0.997622">
111
</page>
<bodyText confidence="0.922922078947368">
In the experimental MLG described in the next sec-
tion, there are 22 such clauses. For the grammar
MLGRAM of Section 2, the following set of clauses
suffices.
mod(id-*, Sem, Sem) &lt;- /.
mod(Sem, id-*, Sem) &lt;- /.
mod(1-P, Op-Q, Op-R) &lt;- and(P,Q,R).
mod(P/Q-R, Op-Q, @P-R).
mod(@P-Q, Op-P, Op-Q).
The first two clauses say that the operator &apos;id&apos;
acts like an identity. The second clause defines
&apos;1&apos; as a left-conjoining operator (its corresponding
logical form gets left-conjoined to that of the
modificand). The call and(P,Q,R) makes R=P&amp;Q, ex-
cept that it treats &apos;t&apos; (&apos;true&apos;) as an identity.
The next clause for &apos;mod&apos; allows a quantifier se-
mantic item like P/Q-each(Q,P) to operate on an item
like 1-man(X) to give the item @P-each(man(X),P).
The final clause then allows this item to operate
on 1-live(X) to give 1-each(man(X),live(X)).
The low-level procedure &apos;mod&apos; is the same (in
purpose) as the procedure &apos;trans in McCord (1981),
and has close similarities to &apos;trans&apos; in McCord
(1982) and &apos;mod&apos; in McCord (1984), so we refer to
this previous work for more illustrations of this
approach to modification.
For MLGRAM, the only ingredient of semantic
interpretation remaining to be defined is &apos;reorder&apos;.
We can define it in a way that is somewhat more
general than is necessary for this small grammar,
but which employs a technique useful for larger
grammars. Each augmented semantic item is assigned
a precedence number, and the reordering (sorting)
is done so that when item B has higher precedence
number than item A, then B is ordered to the left
of A; otherwise items are kept in their original
order. The following clauses then define &apos;reorder&apos;
in a way suitable for MLGRAM.
</bodyText>
<equation confidence="0.7872064">
reorder(A:L,M) &lt;-
reorder(L,L1) &amp; insert(A,L1,M).
reorder(nil,ni1).
insert(A,3:L,9:L1) &lt;-
prec(A,PA) &amp; prec(B,PB) &amp; gt(PB,PA) &amp;/&amp;
insert(A,L,L1).
insert(A,L,A;L).
prec(sem(terwnal:*,*,*),2) &lt;- I.
pruc(sem(re1c1ause:*,*,*),1) &lt;- /.
prec(*,3).
</equation>
<bodyText confidence="0.999944789473684">
Thus terminals are ordered to the end, except not
after relative clauses. In particular, the subject
and object of a sentence are ordered before the verb
Is terminal in the sentence), and this allows the
straightforward process of modification in &apos;mod&apos;
to scope the quantifiers of the subject and object
over the material of the verb. One can alter the
definition of &apos;prec&apos; to get finer distinctions in
scoping, and for this we refer to McCord (1982,
1981).
For a grammar as small as MLGRAM, which has
no treatment of scoping phenomena, the total corn-
plexity of the MLG, including the semantic inter-
pretation component we have given in this Section,
is certainly greater than that of the comparable
DCG in Section 2. However, for larger grammars,
the modularity is definitely worthwhile -- concep-
tually, and probably in the total size of the sys-
tem.
</bodyText>
<sectionHeader confidence="0.982634" genericHeader="evaluation">
5. AN EXPERIMENTAL MLG
</sectionHeader>
<bodyText confidence="0.986111363636364">
This section describes briefly an experimental
MLG, called MODL, which covers the same linguistic
ground as the grammar (called MOD) in McCord (1981).
The syntactic component of MOD, a DCG, is essen-
tially the same as that in McCord (1982). One
feature of these syntactic components is a system-
atic use of slot-filling to treat complements of
verbs and nouns. This method increases modularity
between syntax and lexicon, and is described in
detail in McCord (1982).
One purpose of MOD, which is carried over to
MODL, is a good treatment of scoping of modifiers
and a good specification of logical form. The
logical form language used by MODL as the target
of semantic interpretation has been improved some-
what over that used for MOD. We describe here some
of the characteristics of the new logical form
language, called LFL, and give sample LFL analyses
obtained by MODL, but we defer a more detailed de-
scription of LFL to a later report.
The main predicates of LFL are word-senses for.
words in the natural language being analyzed, for
example, believel(X,Y) in the sense &amp;quot;X believes that
Y holds&amp;quot;. Quantifiers, like &apos;each&apos;, are special
cases of word-senses. There are also a small number
of non-lexical predicates in LFL, some of which are
associated with inflections of words, like &apos;past&apos;
for past tense, or syntactic constructions, like
yesno&apos; for yes-no questions, or have significance
at discourse level, dealing for instance with
topic/comment. The arguments for predicates of LFL
can be conscants, variables, or other logical forms
(expressions of LFL).
Expressions of LFL are either predications (in
the sense just indicated) or combinations of LFL
expressions using the conjunction &apos;&amp;&apos; and the in-
dexing operator &apos;:&apos;. Specifically, if P is a log-
ical form and E is a variable, then P:E (read &amp;quot;P
indexed by E&amp;quot;) is also a logical form. When an
indexed logical form P:E appears as part of a larger
logical form Q, and the index variable E is used
elsewhere in Q. then E can be thought of roughly
as standing for P together with its &amp;quot;context&amp;quot;.
Contexts include references to time and place which
are normally left implicit in natural language.
When P specifies an event, as in see(john,mary),
writing P:E and subsequently using E will guarantee
that E refers to the same event. In the logical
form language used in McCord (1981), event variables
(as arguments of verb and noun senses) were used
for indexing. But the indexing operator is more
powerful because it can index complex logical forms.
For some applications, it is sufficient to ignore
contexts, and in such cases we just think of P:E
as verifying P and binding E to an instantiation
</bodyText>
<page confidence="0.995685">
112
</page>
<bodyText confidence="0.926588783783784">
of P. In fact, for PROLOG execution of logical
forms without contexts, &apos;:&apos; can be defined by the
single clause: P:P &lt;- P.
A specific purpose of the MOD system in McCord
(1981) was to point out the importance of a class
of predicates called focalizers, and to offer a
method for dealing with them in semantic interpre-
tation. Focalizers include many determiners,
adverbs, and adjectives (or their word-senses), as
well as certain non-lexical predicates like &apos;yesno&apos;.
Focalizers take two logical form arguments called
the base and the focus:
focalizer(Base,Focus).
The Focus is often associated with sentence stress,
hence the name. The pair (Base. Focus) is called
the scope of the focalizer.
The adverbs &apos;only&apos; and &apos;even&apos; are focalizers
which most clearly exhibit the connection with
stress. The predication only(P,Q) reads &amp;quot;the only
case where P holds is when Q also holds&amp;quot;. We get
different analyses depending on focus.
John only buys books at Smith&apos;s.
only(at(smith,buy(john,X1)), book(X1)).
John only buys books at Smith&apos;s.
only(book(X1)&amp;at(X2,buy(john,X1)), X2=smith).
Quantificational adverbs like &apos;always&apos; and
&apos;seldom&apos;, studied by David Lewis (1975), are also
focalizers. Lewis made the point that these
quantifiers are properly considered unselective, in
the sense that they quantify over all the free
variables in (what we call) their bases. For ex-
ample, in
John always buys books at Smith&apos;s.
always(book(X1)&amp;at(X2,buy(john,X1)), X2=smith).
the quantification is over both X1 and X2. (A
paraphrase is &amp;quot;Always, if X1 is a book and John buys
X1 at X2, then X2 is Smith&apos;s&amp;quot;.)
</bodyText>
<subsectionHeader confidence="0.684184">
Quantificational determiners are also
</subsectionHeader>
<bodyText confidence="0.893975368421053">
focalizers (and are unselective quantifiers); they
correspond closely in meaning to the
quantificational adverbs (&apos;all&apos; - &apos;always&apos;, &apos;many&apos;
- &apos;often&apos;, &apos;few&apos; - &apos;seldom&apos;, etc.). We have the
paraphrases:
Leopards often attack monkeys in trees.
often(leopard(X1)&amp;tree(X2)&amp;in(X2,attack(X1,X3)),
monkey(X3)).
Many leopard attacks in trees are (attacks)
on monkeys.
many(leopard(X1)Estree(X2)&amp;in(X2,attack(X1,X3)),
monkey(X3)).
Adverbs and adjectives involving comparison
or degree along some scale of evaluation (a wide
class) are also focalizers. The base specifies the
base of comparison, and the focus singles out what
is being compared to the base. This shows up most
clearly in the superlative forms. Consider the
adverb &amp;quot;fastest&amp;quot;:
</bodyText>
<tableCaption confidence="0.5596965">
John ran fastest yesterday.
fastest(run(john):E, yesterday(E)).
John ran fastest yesterday.
fastest(yesterday(run(X)), X=john).
</tableCaption>
<bodyText confidence="0.984344452830189">
In the first sentence, with focus on &amp;quot;yesterday&amp;quot;,
the meaning is that, among all the events of John&apos;s
running (this is the base), John&apos;s running yesterday
was fastest. The logical form illustrates the in-
dexing operator. In the second sentence, with focus
on &amp;quot;John&amp;quot;, the meaning is that among all the events
of running yesterday (there is an implicit location
for these events), John&apos;s running was fastest.
As an example of a non-lexical focalizer, we
have yesno(P,Q), which presupposes that a case of
P holds, and asks whether P &amp; Q holds. (The pair
(P. Q) is like Topic/Comment for yes-no questions.)
Example:
Did John see Mary yesterday?
yesno(yesterday(see(john,X)), X=mary)
It is possible to give Prolog definitions for
most of the focalizers discussed above which are
suitable for extensional evaluation and which amount
to model-theoretic definitions of them. This will
be discussed in a later report on LFL.
A point of the grammar MODL is to be able to
produce LFL analyses of sentences using the modular
semantic interpretation system outlined in the
preceding section, and to arrive at the right (or
most likely) scopes for focalizers and other modi-
fiers. The decision on scoping can depend on
heuristics involving precedences, on very reliable
cues from the syntactic position, and even on the
specification of foci by explicit underlining in
the input string (which is most relevant for
adverbial focalizers). Although written text does
not often use such explicit specification of
adverbial foci, it is important that the system can
get the right logical form after having some spec-
ification of the adverbial focus, because this
specification might be obtained from prosody in
spoken language, or might come from the use of
discourse information. It also is an indication
of the modularity of the system that it can use the
same syntactic rules and parse path no matter where
the adverbial focus happens to lie. •
Most of the specific linguistic information
for semantic interpretation is encoded in the
procedures &apos;mod&apos;, &apos;reorder&apos;, and &apos;raise&apos;, which
manipulate semantic items. In MODL there are 22
clauses for the procedure &apos;mod&apos;, most of which are
unit clauses. These involve ten different modifi-
cation operators, four of which were illustrated
in the preceding section. The definition of &apos;mod&apos;
in MODL is taken fairly directly from the corre-
sponding procedure &apos;trans&apos; in MOD (McCord, 1981),
although there are some changes involved in handling
the new version of the logical form language (LFL),
</bodyText>
<page confidence="0.998233">
113
</page>
<bodyText confidence="0.990953888888889">
especially the indexing operator. The definitions
of &apos;reorder&apos; and &apos;raise&apos; are essentially the same
as for procedures in MOD.
attack(X,Y). The combination of items on the left
of &apos;only&apos; is
An illustration of analysis in the two-pass
mode in MODL is now given. For the sentence
&amp;quot;Leopards only attack monkeys in trees&amp;quot;, the syn-
tactic analysis tree is as follows.
</bodyText>
<figure confidence="0.547368916666667">
sent
nounph
1-leopard(X)
avp
(P&lt;Q)-only(P,Q)
1-attack(X,Y)
nounph
1-monkey(Y)
prepph
@@R-in(Z,R)
nounph
1-tree(Z)
</figure>
<bodyText confidence="0.959056461538461">
Here we display complete logical terminals in the
leaf nodes of the tree. An indication of the
meanings of the operators (P&lt;Q) and 9@it will be
given below.
In the semantic interpretation of the prepo-
sitional phrase, the &apos;tree&apos; item gets promoted (by
&apos;raise&apos;) to be a left-sister of the the &apos;in&apos; item,
and the list of daughter items (augmented semantic
items) of the &apos;sent&apos; node is the following.
nounph 1 leopard(X)
avp P&lt;Q only(P.Q)
terminal 1 attack(X,Y)
nounph 1 monkey(Y)
nounph 1 tree(Z)
prepph @@R in(Z,R).
Here we diplay each augmented semantic item
sem(nt:Feas,0p,LF) simply in the form nt Op IS.
The material in the first field of the &apos;monkey&apos; item
actually shows that it is stressed. The reshaping
vocedure &apos;reorder&apos; rearranges these items into the
order:
nounph 1 leopard(X)
nounph 1 tree(Z)
prepph @@R in(Z,R)
terminal 1 attack(X,Y)
avp P&lt;Q only(P,Q)
nounph 1 monkey(Y)
Next, these items successively modify (according
to the rules for &apos;mod&apos;) the matrix item, sent id
t, with the rightmost daughter acting as innermost
modifier. The rules for &apos;mod&apos; involving the oper-
ator (P&lt;Q) associated with only(P,Q) are designed
so that the logical form material to the right of
&apos;only&apos; goes into the focus Q of &apos;only&apos; and the ma-
terial to the left goes into the base P. The ma-
terial to the right is just monkey(Y). The items
on the left (&apos;leopard&apos;, &apos;tree&apos;, &apos;in&apos;, &apos;attack&apos;) are
allowed to combine (through &apos;mod&apos;) in an independent
way before being put Into the base of &apos;only&apos;. The
operator CO@R associated with in(Z,R) causes R to
be bound to the logical form of the modificand
leopard(X)&amp;tree(Z)&amp;in(Z,attack(X,Y))
This goes into the base, so the whole logical form
is
only(leopard(X)&amp;tree(Z)&amp;in(Z,attack(X,Y)),
monkey(Y)).
For detailed traces of logical form construction
by this method, see McCord (1981).
An illustration of the treatment of left-
embedding in MODL in a two-pass analysis of the
sentence &amp;quot;John sees each boy&apos;s brother&apos;s teacher&amp;quot;
is as follows.
</bodyText>
<figure confidence="0.894739882352941">
sent
nounph
l-(X=john)
1-see(X,W)
nounph
nounph
nounph
determiner
Q/P-each(P,Q)
1-boy(Y)
1-poss
1-brother(Z.Y)
1-poss
1-teacher(W,Z)
Logical form...
each(boy(Y),the(brother(Z,Y),
the(teacher(W,Z),see(john,W)))).
</figure>
<bodyText confidence="0.973040111111111">
The MODL noun phrase rules include the shift (in a
way that is an elaboration of the shift grammar
fragment in Section 2), as well as rules for slot-
filling for nouns like &apos;brother&apos; and &apos;teacher&apos; which
have more than one argument in logical form. Ex-
actly the same logical form is obtained by MODL for
the sentence &amp;quot;John sees the teacher of the brother
of each boy&amp;quot;. Both of these analyses involve
raising. ;n the first, the &apos;poss. node resulting
from the apostrophe-s is raised to become a definite
article. In the second, the prepositional phrases
(their semantic structures) are promoted to be
sisters of the &amp;quot;teacher&amp;quot; node, and the order of the
quantifiers is (correctly) reversed.
The syntactic component of MODL was adapted
as closely as possible from that of MOD (a DCG) in
order to get an idea of the efficiency of MLG&apos;s.
The fact that the MLG rule compiler produces more
structure-building arguments than are in the DCG
would tend to lengthen analysis times, but it is
hard to predict the effect of the different organ-
ization of the semantic interpreter (from a three-
pass system to a one-pass and a two-pass version
of MOW. The following five sentences were used
for timing tests.
Who did John say that the man introduced Mary to?
Each book Mary said was given to Bill
</bodyText>
<page confidence="0.996578">
114
</page>
<bodyText confidence="0.999663315789474">
was written by a woman.
Leopards only attack monkeys in trees.
John saw each boy&apos;s brother&apos;s teacher.
Does anyone wanting to see the teacher know
whether there are any books left in this room?
Using Waterloo Prolog (an interpreter) on an IBM
3081, the following average times to get the logical
forms for the five sentences were obtained (not
including time for I/O and initial word separation):
MODL, one-pass mode - 40 milliseconds.
MODL, two-pass mode - 42 milliseconds.
MOD - 35 milliseconds.
So there was a loss of speed, but not a significant
one. MODL has also been implemented in PSC Prolog
(on a 3081). Here the average one-pass analysis
time for the five sentences was improved to 30
milliseconds per sentence.
On the other hand, the MLG grammar (in source
form) is more compact and easier to understand.
The syntactic components for MOD and MODL were
compared numerically by a Prolog program that totals
up the Sizes of all the grammar rules, where the size
of a compound term is defined to be 1 plus the sum
of the sizes of its arguments, and the size of any
other term is 1. The total for MODL was 1433, and
for MOD was 1807, for a ratio of 79%.
So far, nothing has been said in this report
about semantic constraints in MODL. Currently, MODL
exercises constraints by unification of semantic
types. Prolog terms representing type requirements
on slot-fillers must be unified with types of actual
fillers. The types used in MODL are type trees.
A type tree is either a variable (unspecified type)
or a term whose principal functor is an atomic type
(like &apos;human&apos;), and whose arguments are subordinate
type trees. A type tree Ti is subordinate to a type
tree T2 if either Ti is a variable or the principal
functor of Ti is a subtype (ako) of the principal
functor of T2. Type trees are a generalization of
the type lists used by Dahl (1981), which are lists
of the form T1:T2:T3:..., where Tl is a supertype
of T2, T2 is a supertype of T3, ..., and the tail
of the list may be a variable. The point of the
generalization is to allow cross-classification.
Multiple daughters of a type node cross-classify
it. The lexicon in MODL includes a preprocessor
for lexical entries which allows the original lex-
ical entries to specify type constraints in a com-
pact, non-redundant way. There is a Prolog
representation for type-hierarchies, and the lexi-
cal preprocessor manufactures full type trees from
a specification of their leaf nodes.
In the one-pass mode for analysis with MLG&apos;s,
logical forms get built up during parsing, so log-
ical forms are available for examination by semantic
checking procedures of the sort outlined in McCord
(1984). If such methods are arguably best, then
there may be more argument for a one-pass system
(with interleaving of semantics). The general
question of the number of passes in a natural lan-
guage understander is an interesting one. The MLG
formalism makes this easier to investigate, because
the same syntactic component can be used with one-
pass or two-pass interpretation.
In MODL, there is a small dictionary stored
directly in Prolog, but MODL is also interfaced to
a large dictionary/morphology system (Byrd, 1983,
1984) which produces syntactic and morphological
information for words based on over 70,000 lemmata.
There are plans to include enough semantic infor-
mation in this dictionary to provide semantic con-
straints for a large MLG.
Alexa McCray is working on the syntactic com-
ponent for an MLG with very wide coverage. I wish
to thank her for useful conversations about the
nature of the system. ,
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="conclusions">
6. COMPARISON WITH OTHER SYSTEMS
</sectionHeader>
<bodyText confidence="0.999937212765957">
The Restriction Grammars (RG&apos;s) of Hirschman
and Puder (1982) are logic grammars that were de-
signed with modularity in mind. Restriction Gram-
mars derive from the Linguistic String Project
(Sager, 1981). An RG consists of context-free
phrase structure rules to which restrictions are
appended. The rule compiler (written in Prolog and
compiling into Prolog), sees to it that derivation
trees are constructed automatically during parsing.
The restrictions appended to the rules are basically
Prolog procedures which can walk around, during the
parse, in the partially constructed parse tree, and
can look at the words remaining in the input stream.
Thus there is a modularity between the phrase-
structure parts of the syntax rules and the re-
strictions. The paper contains an interesting
discussion of Prolog representations of parse trees
that make it easy to walk around in them.
A disadvantage of RG&apos;s is that the automat-
ically constructed analysis tree is just a deriva-
tion tree. With MLG&apos;s, the shift operator and the
declaration of strong non-terminals produce analy-
sis structures which are more appropriate seman-
tically and are easier to read for large grammars.
In addition, MLG analysis trees contain logical
terminals as building blocks for a modular semantic
interpretation system. The method of walking lbout
in the partially constructed parse tree is powerful
and is worth exploring further; but the more common
way of exercising constraints in logic grammars by
parameter passing and unification seems to be ade-
quate linguistically and notationally more compact,
as well as more efficient for the compiled Prolog
program.
Another type of logic grammar developed with
modularity in mind is the Definite Clause Trans-
lation Grammars (DCTG&apos;s) of Abramson (1984). These
were inspired partially by RG&apos;s (Hirschman and
Puder, 1982), by MSG&apos;s (Dahl and McCord, 1983), and
by Attribute Grammars (Knuth, 1968). A DCTG rule
is like a DCG rule with an appended list of clauses
which compute the semantics of the node resulting
from use of the rule. The non-terminals on the
right-hand side of the syntactic portion of the rule
can be indexed by variables, and these index vari-
ables can be used in the semantic portion to link
to the syntactic portion. For example, the DCG rule
</bodyText>
<page confidence="0.995597">
115
</page>
<bodyText confidence="0.985169393442623">
sent(P) --&gt; np(X,P1,P): vp(X,P1).
from the DCG in Section 2 has the DCTG equivalent:
sent ::= np@N: vp@V &lt;:&gt;
logic(P) N@1ogic(X,P1,P) &amp; V@Iogic(X,P1).
(Our notation is slightly different from Abramson&apos;s
and is designed to fit the Prolog syntax of this
report.) Here the indexing operator is &apos;@&apos;. The
syntactic portion is separated from the semantic
portion by the operator &apos;&lt;:&gt;&apos;. The non-terminals
in DCTG&apos;s can have arguments, as in DCG&apos;s, which
could be used to exercise constraints (re-
strictions), but it is possible to do everything
by referring to the indexing variables. The DCTG
rule compiler sees to the automatic construction
of a derivation tree, where each node is labeled
not only by the expanded non-terminal but also by
the list of clauses in the semantic portion of the
expanding rule. These clauses can then be used in
computing the semantics of the node. When an in-
dexed non-terminal NT@X appears on the right-hand
side of a rule, the indexing variable X gets
instantiated to the tree node corresponding to the
expansion of NT.
There is a definite separation of DCTG rules
into a syntactic portion and a semantic portion,
with a resulting increase of modularity. Procedures
involving different sorts of constraints can be
separated from one another, because of the device
of referring to the indexing variables. However,
it seems that once the reader (or writer) knows that
certain variables in the DCG rule deal with the
construction of logical forms, the original DCG rule
is just as easy (if not easier) to read. The DCTG
rule is definitely longer than the DCG rule. The
corresponding MLG rule:
sent ==&gt; np(X): vp(X).
is shorter, and does. not need to mention logical
forms at all. Of course, there are relevant
portions of the semantic component that are applied
in connection with this rule, but many parts of the
semantic component are relevant to several syntax
rules, thus reducing the total size of the system.
A claimed advantage for DCTG&apos;s is that the
semantics for each rule is listed locally with each
rule. There is certainly an appeal in that, because
with MLG&apos;s (as well as the methods in McCord (1982,
1;81)), the semantics seems to float off more on
its own. Semantic items do have a life of their
own, and they can move about in the tree (implic-
itly, in some versions of the semantic interpreter)
because of raising and reordering. This is not as
neat theoretically, but it seems more appropriate
for capturing actual natural language.
Another disadvantage of DCTG&apos;s (as with RG!s)
is that the analysis trees that are constructed
automatically are derivation trees.
The last system to be discussed here, that in
Porto and Filgueiras (1984), does not involve a new
grammar formalism, but a methodology for writing
DCG&apos;s. The authors define a notion of intermediate
semantic representation (ISA) including entities and
predications, where the predications can be viewed
as logical forms. In writing DCG rules, one sys-
tematically includes at the end of the rule a call
to a semantic procedure (specific to the given rule)
which combines ISR&apos;s obtained in arguments of the
non-terminals on the right-hand side of the rule.
Two DCG rules in this style (given by the authors)
are as follows:
sent(S) --&gt; np(N): vp(V): $ssv(N,V,S).
vp(S) --&gt; verb(V,trans): np(N): $svo(V,N,S).
Here &apos;ssv&apos; and &apos;svo&apos; are semantic procedures that
are specific to the &apos;sent&apos; rule and the &apos;vp&apos; rule,
respectively. The rules that define &apos;ssv&apos; and &apos;svo&apos;
can include some general rules, but also a mass of
very specific rules tied to specific words. Two
specific rules given by the authors for analyzing
&amp;quot;All Viennese composers wrote e waltz&amp;quot; are as fol-
lows.
svo(wrote,M:X,wrote(X)) &lt;- is_a(M,music).
ssv(P:X,wrote(Y),author_of(Y,X)) ‹-
is_a(P,person).
Note that the verb &apos;wrote&apos; changes from the surface
form &apos;wrote&apos;, to the intermediate form wrote(X),
then to the form author_of(Y,X). In most logic
grammar systems (including MOD and MOM.), some form
of argument filling is done for predicates; infor-
mation is added by binding argument variables,
rather than changing the whole form of the predi-
cation. The authors claim that it is less efficient
to do argument filling, because one can make an
early choice of a word sense which may lead to
failure and backtracking. An intermediate form like
wrote(X) above may only make a partial decision
about the sense.
The value of the &amp;quot;changing&amp;quot; method over the
&amp;quot;adding&amp;quot; method would appear to hinge a lot on the
question of parse-time efficiency, because the
&amp;quot;changing&apos; method seems more complicated conceptu-
ally. It. seems simpler to have the notion that
there are word-senses which are predicates with a
certain number of arguments, and to deal only with
these, rather than inventing intermediate forms that
help in discrimination during the parse. So it is
partly an empirical question which would be decided
after logic grammars dealing semantically with
massive dictionaries are developed.
There is modularity in rules written in the
style of Porto and Filgueiras, because all the se-
mantic structure-building is concentrated in the
semantic procedures added (by the grammar writer)
at the ends of the rules. In MLG&apos;s, in the one-pass
mode, the same semantic procedure call, to &apos;semant&apos;,
is added at the ends of strong rules, automatically
by the compiler. The diversity comes in the an-
cillary procedures for &apos;semant&apos;, especially &apos;mod&apos;.
In fact, &apos;mod&apos; (or &apos;trans&apos; in McCord, 1981) has
something in common with the Porto-Filgueiras pro-
cedures in that it takes two intermediate repres-
entations (semantic items) in its first two
arguments and produces a new intermediate repre-
sentation in its third argument. However, the
</bodyText>
<page confidence="0.996277">
116
</page>
<bodyText confidence="0.999806083333333">
changes that &apos;mod&apos; makes all involve the
modification-operator components of semantic items,
rather than the logical-form components. It might
be interesting and worthwhile to look at a combi-
nation of the two approaches.
Both a strength and a weakness of the Porto-
Filgueiras semantic procedures (compared with
&apos;mod&apos;) is that there are many of them, associated
with specific syntactic rules. The strength is that
a specific procedure knows that it is looking at
the &amp;quot;results&amp;quot; of a specific rule. But a weakness
is that generalizations are missed. For example,
modification by a quantified noun phrase (after
slot-filling or the equivalent) is often the same,
no matter where it comes from. The method in MLO&apos;s
allows semantic items to move about and then act
by one &apos;mod&apos; rule. The reshaping procedures are
free to look at specific syntactic information, even
specific words when necessary, because they work
with augmented semantic items. Of course, another
disadvantage of the diversity of the Porto-
Filgueiras procedures is that they must be explic-
itly added by the writer of syntax rules, so that
there is not as much modularity as in MLG&apos;s.
</bodyText>
<sectionHeader confidence="0.999502" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.996056171052632">
Abramson, H. (1984) &amp;quot;Definite clause translation
grammars,&amp;quot; Proc. 1984 International Symposium on
Logic Programming, pp. 233-240, Atlantic City.
Byrd, R. J. (1983) &amp;quot;Word formation in natural lan-
guage processing systems,&amp;quot; Proc. 8th International
Joint Conference on Artificial Intelligence, pp.
704-706, Karlsruhe.
Byrd, R. J. (1984) &amp;quot;The Ultimate Dictionary Users&apos;
Guide,&amp;quot; IBM Research Internal Report.
Colmerauer, A. (1978) &amp;quot;Metamorphosis grammars,&amp;quot; in
L. Bolc (Ed.), Natural Language Communication with
Computers, Springer-Verlag.
Dahl, V. (1977) &amp;quot;Un systeme deductif d&apos;interrogation
de banques de donnees en espagnol,&amp;quot; Groupe
d&apos;Intelligence Artificielle, Univ. d&apos;Aix-Marseille.
Dahl, V. (1981) &amp;quot;Translating Spanish into logic
through logic,&amp;quot; American Journal of Computational
Linguistics, vol. 7, pp. 149-164.
Dahl, V, and McCord, M. C. (1983) &amp;quot;Treating coor-
dination in logic grammars,&amp;quot; American Journal of
Computational Linguistics, vol. 9, pp. 69-91.
Heidorn, G. E. (1972) Natural Language Inputs to a
Simulation Programming System, Naval Postgraduate
School Technical Report No. NPS-55HD72101A.
Hirschman, 1i. and Puder, K. (1982) &amp;quot;Restriction
grammar in Prolog,&amp;quot; Proc. First International Logic
Programming Conference, pp. 85-90, Marseille.
Jensen, K. and Heidorn, G. E. (1983) &amp;quot;The fitted
parse: 100% parsing capability in a syntactic
grammar of English,&amp;quot; IBM Research Report RC 9729.
Knuth, D. E. (1968) &amp;quot;Semantics of context-free
languages,&amp;quot; Mathematical Systems Theory, vol. 2,
pp. 127-145.
Lewis, D. (1975) &amp;quot;Adverbs of quantification,&amp;quot; In
E.L. Keenan (Ed.), Formal Semantics of Natural
Language, pp. 3-15, Cambridge University Press.
McCord, M. C. (1982) &amp;quot;Using slots and modifiers in
logic grammars for natural language,&amp;quot; Artificial
Intelligence, vol 18, pp. 327-367. (Appeared first
as 1980 Technical Report, University of Kentucky.)
McCord, M. C. (1981) &amp;quot;Focalizers, the scoping
problem, and semantic interpretation rules in logic
grammars,&amp;quot; Technical Report, University of
Kentucky. To appear in Logic Programming and its
Applications, D. Warren and M: van Caneghem, Eds.
McCord, M. C. (1984) &amp;quot;Semantic interpretation for
the EPISTLE system,&amp;quot; Proc. Second International
Logic Programming Conference, pp. 65-76, Uppsala.
Miller, L. A., Heidorn, G. E., and Jensen, K.
(1981) &amp;quot;Text-critiquing with the EPISTLE system:
an author&apos;s aid to better syntax,&amp;quot; AFIPS Conference
Proceedings, vol. 50, pp. 649-655.
Pereira, F. (1981) &amp;quot;Extraposition grammars,&amp;quot; Amer-
ican Journal of Computational Linguistics, vol. 7,
pp. 243-256.
Pereira, F. (1983) &amp;quot;Logic for natural language
analysis,&amp;quot; SRI International, Technical Note 275.
Pereira, F. and Warren, D. (1980) &amp;quot;Definite clause
grammars for language analysis - a survey of the
formalism and a comparison with transition net-
works,&amp;quot; Artificial Intelligence, vol. 13, pp.
231-278.
Pereira, F. and Warren, D. (1982) &amp;quot;An efficient
easily adaptable system for interpreting natural
language queries,&amp;quot; American Journal of Computa-
tional Linguistics, vol. 8, pp. 110-119.
Porto, A. and Filgueiras, M. (1984) &amp;quot;Natural lan-
guage semantics: A logic programming approach,&amp;quot;
Proc. 1984 International Symposium on Logic Pro-
gramming, pp. 228-232, Atlantic City.
Sager, N. (1981) Natural Language Information
Processing: A Computer Grammar of English and Its
Applications , Addison-Wesley.
Woods, W. A. (1970) &amp;quot;Transition network grammars
for natural language analysis,&amp;quot; C. ACM, vol. 13,
pp. 591-606.
</reference>
<page confidence="0.998054">
117
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.904917">
<title confidence="0.999384">MODULAR LOGIC GRAMMARS</title>
<author confidence="0.999998">Michael C McCord</author>
<affiliation confidence="0.999891">IBM Thomas J. Watson Research Center</affiliation>
<address confidence="0.9915855">P. 0. Box 218 Yorktown Heights, NY 10598</address>
<abstract confidence="0.997001">This report describes a logic grammar formalism, Logic Grammars, a high degree of modularity between syntax and semantics. There is a syntax rule compiler (compiling into Prolog) which takes care of the building of analysis structures and the interface to a clearly separated semantic interpretation component dealing with scoping and the construction of logical forms. The whole system can work in either a one-pass mode or a two-pass mode. In the one-pass mode, logical forms are built directly during parsing through interleaved calls to semantics, added automatically the rule compiler. In the two-pass mode, tactic analysis trees are built automatically in the first pass, and then given to the (one-pass) semantic component. The grammar formalism includes two devices which cause the automatIcally built syntactic structures to differ from derivation trees in two ways: (1) There is a shift operator, for dealing with left-embedding constructions such as English possessive noun phrases while using rightrecursive rules (which are appropriate for Prolog (2) There is a distinction in the formalism between and which is important for distinguishing major levels of grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Abramson</author>
</authors>
<title>Definite clause translation grammars,&amp;quot;</title>
<date>1984</date>
<booktitle>Proc. 1984 International Symposium on Logic Programming,</booktitle>
<pages>233--240</pages>
<location>Atlantic City.</location>
<contexts>
<context position="7440" citStr="Abramson (1984)" startWordPosition="1166" endWordPosition="1167"> the scoping phenomena handled by that in McCord (1981) and more than the semantic component in McCord (1984). The logical form language is improved over that in the previous systems. The MLG formalism allows for a great deal of modularity in natural language grammars, because the syntax rules can be written with very little awareness of semantics or the building of analysis structures, and the very same syntactic component can be used in either the one-pass or the two-pass mode described above. Three other logic grammar systems designed with modularity in mind are Hirschman and Puder (1982), Abramson (1984) and Porto and Filgueiras (1984). These will be compared with MLG&apos;s in Section 6. 2. THE MLG SYNTACTIC FORMALISM The syntactic component for an MLG consists of a declaration of the strong non-terminals, followed by a sequence of MLG syntax rules. The declaration of strong non-terminals is of the form strongnonterminals(NT1.NT2. .NTn.ni1). where the NTi are the desired strong non-terminals (only their principal functors are indicated). Non-terminals that are not declared strong are called weak. The significance of the strong/weak distinction will be explained below. MLG syntax rules are of the </context>
<context position="58119" citStr="Abramson (1984)" startWordPosition="9270" endWordPosition="9271">rge grammars. In addition, MLG analysis trees contain logical terminals as building blocks for a modular semantic interpretation system. The method of walking lbout in the partially constructed parse tree is powerful and is worth exploring further; but the more common way of exercising constraints in logic grammars by parameter passing and unification seems to be adequate linguistically and notationally more compact, as well as more efficient for the compiled Prolog program. Another type of logic grammar developed with modularity in mind is the Definite Clause Translation Grammars (DCTG&apos;s) of Abramson (1984). These were inspired partially by RG&apos;s (Hirschman and Puder, 1982), by MSG&apos;s (Dahl and McCord, 1983), and by Attribute Grammars (Knuth, 1968). A DCTG rule is like a DCG rule with an appended list of clauses which compute the semantics of the node resulting from use of the rule. The non-terminals on the right-hand side of the syntactic portion of the rule can be indexed by variables, and these index variables can be used in the semantic portion to link to the syntactic portion. For example, the DCG rule 115 sent(P) --&gt; np(X,P1,P): vp(X,P1). from the DCG in Section 2 has the DCTG equivalent: se</context>
</contexts>
<marker>Abramson, 1984</marker>
<rawString>Abramson, H. (1984) &amp;quot;Definite clause translation grammars,&amp;quot; Proc. 1984 International Symposium on Logic Programming, pp. 233-240, Atlantic City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Byrd</author>
</authors>
<title>Word formation in natural language processing systems,&amp;quot;</title>
<date>1983</date>
<booktitle>Proc. 8th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>704--706</pages>
<location>Karlsruhe.</location>
<contexts>
<context position="55927" citStr="Byrd, 1983" startWordPosition="8922" endWordPosition="8923">e available for examination by semantic checking procedures of the sort outlined in McCord (1984). If such methods are arguably best, then there may be more argument for a one-pass system (with interleaving of semantics). The general question of the number of passes in a natural language understander is an interesting one. The MLG formalism makes this easier to investigate, because the same syntactic component can be used with onepass or two-pass interpretation. In MODL, there is a small dictionary stored directly in Prolog, but MODL is also interfaced to a large dictionary/morphology system (Byrd, 1983, 1984) which produces syntactic and morphological information for words based on over 70,000 lemmata. There are plans to include enough semantic information in this dictionary to provide semantic constraints for a large MLG. Alexa McCray is working on the syntactic component for an MLG with very wide coverage. I wish to thank her for useful conversations about the nature of the system. , 6. COMPARISON WITH OTHER SYSTEMS The Restriction Grammars (RG&apos;s) of Hirschman and Puder (1982) are logic grammars that were designed with modularity in mind. Restriction Grammars derive from the Linguistic St</context>
</contexts>
<marker>Byrd, 1983</marker>
<rawString>Byrd, R. J. (1983) &amp;quot;Word formation in natural language processing systems,&amp;quot; Proc. 8th International Joint Conference on Artificial Intelligence, pp. 704-706, Karlsruhe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Byrd</author>
</authors>
<title>The Ultimate Dictionary Users&apos; Guide,&amp;quot;</title>
<date>1984</date>
<journal>IBM Research Internal Report.</journal>
<marker>Byrd, 1984</marker>
<rawString>Byrd, R. J. (1984) &amp;quot;The Ultimate Dictionary Users&apos; Guide,&amp;quot; IBM Research Internal Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Colmerauer</author>
</authors>
<title>Metamorphosis grammars,&amp;quot; in L.</title>
<date>1978</date>
<journal>Bolc (Ed.), Natural Language Communication with Computers, Springer-Verlag.</journal>
<marker>Colmerauer, 1978</marker>
<rawString>Colmerauer, A. (1978) &amp;quot;Metamorphosis grammars,&amp;quot; in L. Bolc (Ed.), Natural Language Communication with Computers, Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Dahl</author>
</authors>
<title>Un systeme deductif d&apos;interrogation de banques de donnees en espagnol,&amp;quot; Groupe d&apos;Intelligence Artificielle,</title>
<date>1977</date>
<tech>Univ. d&apos;Aix-Marseille.</tech>
<contexts>
<context position="2217" citStr="Dahl, 1977" startWordPosition="335" endWordPosition="336">ns (normally in some version of predicate logic) with natural language text. Logic grammars may have varying degrees on modularity in their treatments of syntax and semantics. There may or may not be an isolatable syntactic component. In writing metamorphosis grammars (Colmerauer, 1973), or definite clause grammars, DCG&apos;s, (a special case of metamorphosis grammars, Pereira and Warren. 1980), it is possible to build logical forms directly in the syntax rules by letting nonterminals have arguments that represent partial logical forms being manipulated. Some of the earliest logic grammars (e.g., Dahl, 1977) used this approach. There is certainly an appeal in being direct, but there are some disadvantages in this lack of modularity. One disadvantage is that it seems difficult to get an adequate treatment of the scoping of quantifiers (and more generally focalizers, McCord, 1981) when the building of logical forms is too closely bonded to syntax. Another disadvantage is just a general result of lack of modularity: it can be harder to develop and understand syntax rules when too much is going on in them. The logic grammars described in McCord (1982, 1981) were three-pass systems, where one of the m</context>
</contexts>
<marker>Dahl, 1977</marker>
<rawString>Dahl, V. (1977) &amp;quot;Un systeme deductif d&apos;interrogation de banques de donnees en espagnol,&amp;quot; Groupe d&apos;Intelligence Artificielle, Univ. d&apos;Aix-Marseille.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Dahl</author>
</authors>
<title>Translating Spanish into logic through logic,&amp;quot;</title>
<date>1981</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>7</volume>
<pages>149--164</pages>
<contexts>
<context position="54619" citStr="Dahl (1981)" startWordPosition="8707" endWordPosition="8708">Currently, MODL exercises constraints by unification of semantic types. Prolog terms representing type requirements on slot-fillers must be unified with types of actual fillers. The types used in MODL are type trees. A type tree is either a variable (unspecified type) or a term whose principal functor is an atomic type (like &apos;human&apos;), and whose arguments are subordinate type trees. A type tree Ti is subordinate to a type tree T2 if either Ti is a variable or the principal functor of Ti is a subtype (ako) of the principal functor of T2. Type trees are a generalization of the type lists used by Dahl (1981), which are lists of the form T1:T2:T3:..., where Tl is a supertype of T2, T2 is a supertype of T3, ..., and the tail of the list may be a variable. The point of the generalization is to allow cross-classification. Multiple daughters of a type node cross-classify it. The lexicon in MODL includes a preprocessor for lexical entries which allows the original lexical entries to specify type constraints in a compact, non-redundant way. There is a Prolog representation for type-hierarchies, and the lexical preprocessor manufactures full type trees from a specification of their leaf nodes. In the one</context>
</contexts>
<marker>Dahl, 1981</marker>
<rawString>Dahl, V. (1981) &amp;quot;Translating Spanish into logic through logic,&amp;quot; American Journal of Computational Linguistics, vol. 7, pp. 149-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Dahl</author>
<author>M C McCord</author>
</authors>
<title>Treating coordination in logic grammars,&amp;quot;</title>
<date>1983</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>9</volume>
<pages>69--91</pages>
<contexts>
<context position="5143" citStr="Dahl and McCord (1983)" startWordPosition="809" endWordPosition="812">he EPISTLE notion (op. cit.) of approximate parse, which is similar to Pereira&apos;s notion of rightmost normal form, but was developed independently. Thus SYNT/SEM is a two-pass system with a clear modularity between syntax and semantics. 104 In DCG&apos;s and extraposition grammars, the building of analysis structures .(either logical forms or syntactic trees) must be specified explicitly in the syntax rules. A certain amount of modularity is then lost, because the grammar writer must be aware of manipulating these structures, and the possibility of using the grammar in different ways is reduced. In Dahl and McCord (1983), a logic grammar formalism was described, modifier structure grammars (MSG&apos;s), in which structure-building (of annotated derivation trees) is implicit in the formalism. MSG&apos;s look formally like extraposition grammars, with the additional ingredient that semantic items (of the type used in McCord (1981)) can be indicated on the left-hand sides of rules, and contribute automatically to the construction of a syntactico-semantic tree much like that in McCord (1981). These MSG&apos;s were used interpretively in parsing, and then (essentially) the two-pass semantic interpretation system of McCord (1981)</context>
<context position="58220" citStr="Dahl and McCord, 1983" startWordPosition="9284" endWordPosition="9287"> a modular semantic interpretation system. The method of walking lbout in the partially constructed parse tree is powerful and is worth exploring further; but the more common way of exercising constraints in logic grammars by parameter passing and unification seems to be adequate linguistically and notationally more compact, as well as more efficient for the compiled Prolog program. Another type of logic grammar developed with modularity in mind is the Definite Clause Translation Grammars (DCTG&apos;s) of Abramson (1984). These were inspired partially by RG&apos;s (Hirschman and Puder, 1982), by MSG&apos;s (Dahl and McCord, 1983), and by Attribute Grammars (Knuth, 1968). A DCTG rule is like a DCG rule with an appended list of clauses which compute the semantics of the node resulting from use of the rule. The non-terminals on the right-hand side of the syntactic portion of the rule can be indexed by variables, and these index variables can be used in the semantic portion to link to the syntactic portion. For example, the DCG rule 115 sent(P) --&gt; np(X,P1,P): vp(X,P1). from the DCG in Section 2 has the DCTG equivalent: sent ::= np@N: vp@V &lt;:&gt; logic(P) N@1ogic(X,P1,P) &amp; V@Iogic(X,P1). (Our notation is slightly different f</context>
</contexts>
<marker>Dahl, McCord, 1983</marker>
<rawString>Dahl, V, and McCord, M. C. (1983) &amp;quot;Treating coordination in logic grammars,&amp;quot; American Journal of Computational Linguistics, vol. 9, pp. 69-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
</authors>
<title>Natural Language Inputs to a Simulation Programming System, Naval Postgraduate School</title>
<date>1972</date>
<tech>Technical Report No. NPS-55HD72101A.</tech>
<contexts>
<context position="4378" citStr="Heidorn, 1972" startWordPosition="685" endWordPosition="686">comporent using an extraposition grammar (Pereira, 1981) and producing syntactic analyses in rightmost normal form. The second pass handles word sense selection and slotand the third pass handles some scoping phenomena and the final semantic interpretation. One gets a great deal of modularity between syntax and semantics in that the first component has no elements of semantic interpretation at all. In McCord (1984) a one-pass semantic interpretation component, SEM, for the EPISTLE system (Miller, Heidorn and Jensen, 1981) was described. SEM has been interfaced both to the EPISTLE NLP grammar (Heidorn, 1972, Jensen and Heidorn, 1983), as well as to a logic grammar, SYNT, written as a DCG by the author. These grammars are purely syntactic and use the EPISTLE notion (op. cit.) of approximate parse, which is similar to Pereira&apos;s notion of rightmost normal form, but was developed independently. Thus SYNT/SEM is a two-pass system with a clear modularity between syntax and semantics. 104 In DCG&apos;s and extraposition grammars, the building of analysis structures .(either logical forms or syntactic trees) must be specified explicitly in the syntax rules. A certain amount of modularity is then lost, becaus</context>
</contexts>
<marker>Heidorn, 1972</marker>
<rawString>Heidorn, G. E. (1972) Natural Language Inputs to a Simulation Programming System, Naval Postgraduate School Technical Report No. NPS-55HD72101A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>1i Hirschman</author>
<author>K Puder</author>
</authors>
<title>Restriction grammar in Prolog,&amp;quot;</title>
<date>1982</date>
<booktitle>Proc. First International Logic Programming Conference,</booktitle>
<pages>85--90</pages>
<location>Marseille.</location>
<contexts>
<context position="7423" citStr="Hirschman and Puder (1982)" startWordPosition="1162" endWordPosition="1165">mantic component handles all the scoping phenomena handled by that in McCord (1981) and more than the semantic component in McCord (1984). The logical form language is improved over that in the previous systems. The MLG formalism allows for a great deal of modularity in natural language grammars, because the syntax rules can be written with very little awareness of semantics or the building of analysis structures, and the very same syntactic component can be used in either the one-pass or the two-pass mode described above. Three other logic grammar systems designed with modularity in mind are Hirschman and Puder (1982), Abramson (1984) and Porto and Filgueiras (1984). These will be compared with MLG&apos;s in Section 6. 2. THE MLG SYNTACTIC FORMALISM The syntactic component for an MLG consists of a declaration of the strong non-terminals, followed by a sequence of MLG syntax rules. The declaration of strong non-terminals is of the form strongnonterminals(NT1.NT2. .NTn.ni1). where the NTi are the desired strong non-terminals (only their principal functors are indicated). Non-terminals that are not declared strong are called weak. The significance of the strong/weak distinction will be explained below. MLG syntax </context>
<context position="56413" citStr="Hirschman and Puder (1982)" startWordPosition="9000" endWordPosition="9003">MODL, there is a small dictionary stored directly in Prolog, but MODL is also interfaced to a large dictionary/morphology system (Byrd, 1983, 1984) which produces syntactic and morphological information for words based on over 70,000 lemmata. There are plans to include enough semantic information in this dictionary to provide semantic constraints for a large MLG. Alexa McCray is working on the syntactic component for an MLG with very wide coverage. I wish to thank her for useful conversations about the nature of the system. , 6. COMPARISON WITH OTHER SYSTEMS The Restriction Grammars (RG&apos;s) of Hirschman and Puder (1982) are logic grammars that were designed with modularity in mind. Restriction Grammars derive from the Linguistic String Project (Sager, 1981). An RG consists of context-free phrase structure rules to which restrictions are appended. The rule compiler (written in Prolog and compiling into Prolog), sees to it that derivation trees are constructed automatically during parsing. The restrictions appended to the rules are basically Prolog procedures which can walk around, during the parse, in the partially constructed parse tree, and can look at the words remaining in the input stream. Thus there is </context>
<context position="58186" citStr="Hirschman and Puder, 1982" startWordPosition="9278" endWordPosition="9281">gical terminals as building blocks for a modular semantic interpretation system. The method of walking lbout in the partially constructed parse tree is powerful and is worth exploring further; but the more common way of exercising constraints in logic grammars by parameter passing and unification seems to be adequate linguistically and notationally more compact, as well as more efficient for the compiled Prolog program. Another type of logic grammar developed with modularity in mind is the Definite Clause Translation Grammars (DCTG&apos;s) of Abramson (1984). These were inspired partially by RG&apos;s (Hirschman and Puder, 1982), by MSG&apos;s (Dahl and McCord, 1983), and by Attribute Grammars (Knuth, 1968). A DCTG rule is like a DCG rule with an appended list of clauses which compute the semantics of the node resulting from use of the rule. The non-terminals on the right-hand side of the syntactic portion of the rule can be indexed by variables, and these index variables can be used in the semantic portion to link to the syntactic portion. For example, the DCG rule 115 sent(P) --&gt; np(X,P1,P): vp(X,P1). from the DCG in Section 2 has the DCTG equivalent: sent ::= np@N: vp@V &lt;:&gt; logic(P) N@1ogic(X,P1,P) &amp; V@Iogic(X,P1). (Ou</context>
</contexts>
<marker>Hirschman, Puder, 1982</marker>
<rawString>Hirschman, 1i. and Puder, K. (1982) &amp;quot;Restriction grammar in Prolog,&amp;quot; Proc. First International Logic Programming Conference, pp. 85-90, Marseille.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jensen</author>
<author>G E Heidorn</author>
</authors>
<title>The fitted parse: 100% parsing capability in a syntactic grammar of English,&amp;quot;</title>
<date>1983</date>
<journal>IBM Research</journal>
<tech>Report RC 9729.</tech>
<contexts>
<context position="4405" citStr="Jensen and Heidorn, 1983" startWordPosition="687" endWordPosition="690"> an extraposition grammar (Pereira, 1981) and producing syntactic analyses in rightmost normal form. The second pass handles word sense selection and slotand the third pass handles some scoping phenomena and the final semantic interpretation. One gets a great deal of modularity between syntax and semantics in that the first component has no elements of semantic interpretation at all. In McCord (1984) a one-pass semantic interpretation component, SEM, for the EPISTLE system (Miller, Heidorn and Jensen, 1981) was described. SEM has been interfaced both to the EPISTLE NLP grammar (Heidorn, 1972, Jensen and Heidorn, 1983), as well as to a logic grammar, SYNT, written as a DCG by the author. These grammars are purely syntactic and use the EPISTLE notion (op. cit.) of approximate parse, which is similar to Pereira&apos;s notion of rightmost normal form, but was developed independently. Thus SYNT/SEM is a two-pass system with a clear modularity between syntax and semantics. 104 In DCG&apos;s and extraposition grammars, the building of analysis structures .(either logical forms or syntactic trees) must be specified explicitly in the syntax rules. A certain amount of modularity is then lost, because the grammar writer must b</context>
</contexts>
<marker>Jensen, Heidorn, 1983</marker>
<rawString>Jensen, K. and Heidorn, G. E. (1983) &amp;quot;The fitted parse: 100% parsing capability in a syntactic grammar of English,&amp;quot; IBM Research Report RC 9729.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>Semantics of context-free languages,&amp;quot;</title>
<date>1968</date>
<journal>Mathematical Systems Theory,</journal>
<volume>2</volume>
<pages>127--145</pages>
<contexts>
<context position="58261" citStr="Knuth, 1968" startWordPosition="9292" endWordPosition="9293">od of walking lbout in the partially constructed parse tree is powerful and is worth exploring further; but the more common way of exercising constraints in logic grammars by parameter passing and unification seems to be adequate linguistically and notationally more compact, as well as more efficient for the compiled Prolog program. Another type of logic grammar developed with modularity in mind is the Definite Clause Translation Grammars (DCTG&apos;s) of Abramson (1984). These were inspired partially by RG&apos;s (Hirschman and Puder, 1982), by MSG&apos;s (Dahl and McCord, 1983), and by Attribute Grammars (Knuth, 1968). A DCTG rule is like a DCG rule with an appended list of clauses which compute the semantics of the node resulting from use of the rule. The non-terminals on the right-hand side of the syntactic portion of the rule can be indexed by variables, and these index variables can be used in the semantic portion to link to the syntactic portion. For example, the DCG rule 115 sent(P) --&gt; np(X,P1,P): vp(X,P1). from the DCG in Section 2 has the DCTG equivalent: sent ::= np@N: vp@V &lt;:&gt; logic(P) N@1ogic(X,P1,P) &amp; V@Iogic(X,P1). (Our notation is slightly different from Abramson&apos;s and is designed to fit the</context>
</contexts>
<marker>Knuth, 1968</marker>
<rawString>Knuth, D. E. (1968) &amp;quot;Semantics of context-free languages,&amp;quot; Mathematical Systems Theory, vol. 2, pp. 127-145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>Adverbs of quantification,&amp;quot; In</title>
<date>1975</date>
<booktitle>Formal Semantics of Natural Language,</booktitle>
<pages>3--15</pages>
<editor>E.L. Keenan (Ed.),</editor>
<publisher>University Press.</publisher>
<location>Cambridge</location>
<contexts>
<context position="45004" citStr="Lewis (1975)" startWordPosition="7147" endWordPosition="7148">lizer(Base,Focus). The Focus is often associated with sentence stress, hence the name. The pair (Base. Focus) is called the scope of the focalizer. The adverbs &apos;only&apos; and &apos;even&apos; are focalizers which most clearly exhibit the connection with stress. The predication only(P,Q) reads &amp;quot;the only case where P holds is when Q also holds&amp;quot;. We get different analyses depending on focus. John only buys books at Smith&apos;s. only(at(smith,buy(john,X1)), book(X1)). John only buys books at Smith&apos;s. only(book(X1)&amp;at(X2,buy(john,X1)), X2=smith). Quantificational adverbs like &apos;always&apos; and &apos;seldom&apos;, studied by David Lewis (1975), are also focalizers. Lewis made the point that these quantifiers are properly considered unselective, in the sense that they quantify over all the free variables in (what we call) their bases. For example, in John always buys books at Smith&apos;s. always(book(X1)&amp;at(X2,buy(john,X1)), X2=smith). the quantification is over both X1 and X2. (A paraphrase is &amp;quot;Always, if X1 is a book and John buys X1 at X2, then X2 is Smith&apos;s&amp;quot;.) Quantificational determiners are also focalizers (and are unselective quantifiers); they correspond closely in meaning to the quantificational adverbs (&apos;all&apos; - &apos;always&apos;, &apos;many</context>
</contexts>
<marker>Lewis, 1975</marker>
<rawString>Lewis, D. (1975) &amp;quot;Adverbs of quantification,&amp;quot; In E.L. Keenan (Ed.), Formal Semantics of Natural Language, pp. 3-15, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C McCord</author>
</authors>
<title>Using slots and modifiers in logic grammars for natural language,&amp;quot;</title>
<date>1982</date>
<journal>Artificial Intelligence,</journal>
<tech>Technical Report,</tech>
<volume>18</volume>
<pages>327--367</pages>
<institution>University of Kentucky.</institution>
<note>Appeared first as</note>
<contexts>
<context position="2766" citStr="McCord (1982" startWordPosition="429" endWordPosition="430">pulated. Some of the earliest logic grammars (e.g., Dahl, 1977) used this approach. There is certainly an appeal in being direct, but there are some disadvantages in this lack of modularity. One disadvantage is that it seems difficult to get an adequate treatment of the scoping of quantifiers (and more generally focalizers, McCord, 1981) when the building of logical forms is too closely bonded to syntax. Another disadvantage is just a general result of lack of modularity: it can be harder to develop and understand syntax rules when too much is going on in them. The logic grammars described in McCord (1982, 1981) were three-pass systems, where one of the main points of the modularity was a good treatment of scoping. The first pass was the syntactic component, written as a definite clause grammar, where syntactic structures were explicitly built up in the arguments of the non-terminals. Word sense selection and slot-filling were done in this first pass, so that the output analysis trees were actually partially semantic. The second pass was a preliminary stage of semantic interpretation in which the syntactic analysis tree was reshaped to reflect proper scoping of modifiers. The third pass took t</context>
<context position="37594" citStr="McCord (1982" startWordPosition="5942" endWordPosition="5943">rs for a noun phrase like &amp;quot;a chicken in every pot&amp;quot;, where the quantifier &amp;quot;every&amp;quot; has higher scope than the quantifier &amp;quot;a&amp;quot;. The semantic item for &amp;quot;every pot&amp;quot; gets promoted to a left sister of that for &amp;quot;a chicken&amp;quot;. &apos;raise&apos; is defined basically by a system of unit clauses which look at specific types of phrases. For the small grammar MLGRAM of Section 2, no raising is necessary, and the definition of &apos;raise&apos; can just be omitted. The procedures &apos;raise&apos; and &apos;reorder&apos; are two key ingredients of reshaping (the movement of semantic items to handle scoping problems), which was discussed extensively in McCord (1982, 1981). In those two systems, reshaping was a separate pass of semantic interpretation, but here, as in McCord (1984), reshaping is interleaved with the rest of semantic interpretation. In spite of the new toplevel organization for semantic interpretation of MLG&apos;s, the low-level procedures for raising and reordering are basically the same as in the previous systems, and we refer to the previous reports for further discussion. The procedure &apos;mod&apos;, used in the second clause for &apos;modify&apos;, is the heart of semantic interpretation. mod(Sem, Seml, Sem2) means that the (non-augmented) semantic item S</context>
<context position="39389" citStr="McCord (1982)" startWordPosition="6232" endWordPosition="6233">nd clause defines &apos;1&apos; as a left-conjoining operator (its corresponding logical form gets left-conjoined to that of the modificand). The call and(P,Q,R) makes R=P&amp;Q, except that it treats &apos;t&apos; (&apos;true&apos;) as an identity. The next clause for &apos;mod&apos; allows a quantifier semantic item like P/Q-each(Q,P) to operate on an item like 1-man(X) to give the item @P-each(man(X),P). The final clause then allows this item to operate on 1-live(X) to give 1-each(man(X),live(X)). The low-level procedure &apos;mod&apos; is the same (in purpose) as the procedure &apos;trans in McCord (1981), and has close similarities to &apos;trans&apos; in McCord (1982) and &apos;mod&apos; in McCord (1984), so we refer to this previous work for more illustrations of this approach to modification. For MLGRAM, the only ingredient of semantic interpretation remaining to be defined is &apos;reorder&apos;. We can define it in a way that is somewhat more general than is necessary for this small grammar, but which employs a technique useful for larger grammars. Each augmented semantic item is assigned a precedence number, and the reordering (sorting) is done so that when item B has higher precedence number than item A, then B is ordered to the left of A; otherwise items are kept in th</context>
<context position="40774" citStr="McCord (1982" startWordPosition="6450" endWordPosition="6451">L,9:L1) &lt;- prec(A,PA) &amp; prec(B,PB) &amp; gt(PB,PA) &amp;/&amp; insert(A,L,L1). insert(A,L,A;L). prec(sem(terwnal:*,*,*),2) &lt;- I. pruc(sem(re1c1ause:*,*,*),1) &lt;- /. prec(*,3). Thus terminals are ordered to the end, except not after relative clauses. In particular, the subject and object of a sentence are ordered before the verb Is terminal in the sentence), and this allows the straightforward process of modification in &apos;mod&apos; to scope the quantifiers of the subject and object over the material of the verb. One can alter the definition of &apos;prec&apos; to get finer distinctions in scoping, and for this we refer to McCord (1982, 1981). For a grammar as small as MLGRAM, which has no treatment of scoping phenomena, the total cornplexity of the MLG, including the semantic interpretation component we have given in this Section, is certainly greater than that of the comparable DCG in Section 2. However, for larger grammars, the modularity is definitely worthwhile -- conceptually, and probably in the total size of the system. 5. AN EXPERIMENTAL MLG This section describes briefly an experimental MLG, called MODL, which covers the same linguistic ground as the grammar (called MOD) in McCord (1981). The syntactic component o</context>
<context position="60766" citStr="McCord (1982" startWordPosition="9721" endWordPosition="9722">) to read. The DCTG rule is definitely longer than the DCG rule. The corresponding MLG rule: sent ==&gt; np(X): vp(X). is shorter, and does. not need to mention logical forms at all. Of course, there are relevant portions of the semantic component that are applied in connection with this rule, but many parts of the semantic component are relevant to several syntax rules, thus reducing the total size of the system. A claimed advantage for DCTG&apos;s is that the semantics for each rule is listed locally with each rule. There is certainly an appeal in that, because with MLG&apos;s (as well as the methods in McCord (1982, 1;81)), the semantics seems to float off more on its own. Semantic items do have a life of their own, and they can move about in the tree (implicitly, in some versions of the semantic interpreter) because of raising and reordering. This is not as neat theoretically, but it seems more appropriate for capturing actual natural language. Another disadvantage of DCTG&apos;s (as with RG!s) is that the analysis trees that are constructed automatically are derivation trees. The last system to be discussed here, that in Porto and Filgueiras (1984), does not involve a new grammar formalism, but a methodolo</context>
</contexts>
<marker>McCord, 1982</marker>
<rawString>McCord, M. C. (1982) &amp;quot;Using slots and modifiers in logic grammars for natural language,&amp;quot; Artificial Intelligence, vol 18, pp. 327-367. (Appeared first as 1980 Technical Report, University of Kentucky.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C McCord</author>
</authors>
<title>Focalizers, the scoping problem, and semantic interpretation rules in logic grammars,&amp;quot;</title>
<date>1981</date>
<tech>Technical Report,</tech>
<institution>University of Kentucky.</institution>
<location>Eds.</location>
<note>To appear in Logic Programming and its</note>
<contexts>
<context position="2493" citStr="McCord, 1981" startWordPosition="379" endWordPosition="380">erauer, 1973), or definite clause grammars, DCG&apos;s, (a special case of metamorphosis grammars, Pereira and Warren. 1980), it is possible to build logical forms directly in the syntax rules by letting nonterminals have arguments that represent partial logical forms being manipulated. Some of the earliest logic grammars (e.g., Dahl, 1977) used this approach. There is certainly an appeal in being direct, but there are some disadvantages in this lack of modularity. One disadvantage is that it seems difficult to get an adequate treatment of the scoping of quantifiers (and more generally focalizers, McCord, 1981) when the building of logical forms is too closely bonded to syntax. Another disadvantage is just a general result of lack of modularity: it can be harder to develop and understand syntax rules when too much is going on in them. The logic grammars described in McCord (1982, 1981) were three-pass systems, where one of the main points of the modularity was a good treatment of scoping. The first pass was the syntactic component, written as a definite clause grammar, where syntactic structures were explicitly built up in the arguments of the non-terminals. Word sense selection and slot-filling wer</context>
<context position="5447" citStr="McCord (1981)" startWordPosition="854" endWordPosition="855">.(either logical forms or syntactic trees) must be specified explicitly in the syntax rules. A certain amount of modularity is then lost, because the grammar writer must be aware of manipulating these structures, and the possibility of using the grammar in different ways is reduced. In Dahl and McCord (1983), a logic grammar formalism was described, modifier structure grammars (MSG&apos;s), in which structure-building (of annotated derivation trees) is implicit in the formalism. MSG&apos;s look formally like extraposition grammars, with the additional ingredient that semantic items (of the type used in McCord (1981)) can be indicated on the left-hand sides of rules, and contribute automatically to the construction of a syntactico-semantic tree much like that in McCord (1981). These MSG&apos;s were used interpretively in parsing, and then (essentially) the two-pass semantic interpretation system of McCord (1981) was used to get logical forms. So, totally there were three passes in this system. In this report, I wish to describe a logic grammar system, modular logic grammars (MLG&apos;s), with the following features: • There is a syntax rule compiler whIch takes care of the building of analysis structures and the in</context>
<context position="6880" citStr="McCord (1981)" startWordPosition="1075" endWordPosition="1076">ly in either a one-pass mode or a twopass mode. • In the one-pass mode, no syntactic structures are built, but logical forms are built directly during parsing through interleaved calls to the semantic interpretation component, added automatically by the rule compiler. matically produced) syntactic analysis trees much more readable and natural linguistically. In the absence of shift constructions, these trees are like derivation trees, but only with nodes corresponding to strong non-terminals. • In an experimental MLG, the semantic component handles all the scoping phenomena handled by that in McCord (1981) and more than the semantic component in McCord (1984). The logical form language is improved over that in the previous systems. The MLG formalism allows for a great deal of modularity in natural language grammars, because the syntax rules can be written with very little awareness of semantics or the building of analysis structures, and the very same syntactic component can be used in either the one-pass or the two-pass mode described above. Three other logic grammar systems designed with modularity in mind are Hirschman and Puder (1982), Abramson (1984) and Porto and Filgueiras (1984). These </context>
<context position="39333" citStr="McCord (1981)" startWordPosition="6223" endWordPosition="6224">y that the operator &apos;id&apos; acts like an identity. The second clause defines &apos;1&apos; as a left-conjoining operator (its corresponding logical form gets left-conjoined to that of the modificand). The call and(P,Q,R) makes R=P&amp;Q, except that it treats &apos;t&apos; (&apos;true&apos;) as an identity. The next clause for &apos;mod&apos; allows a quantifier semantic item like P/Q-each(Q,P) to operate on an item like 1-man(X) to give the item @P-each(man(X),P). The final clause then allows this item to operate on 1-live(X) to give 1-each(man(X),live(X)). The low-level procedure &apos;mod&apos; is the same (in purpose) as the procedure &apos;trans in McCord (1981), and has close similarities to &apos;trans&apos; in McCord (1982) and &apos;mod&apos; in McCord (1984), so we refer to this previous work for more illustrations of this approach to modification. For MLGRAM, the only ingredient of semantic interpretation remaining to be defined is &apos;reorder&apos;. We can define it in a way that is somewhat more general than is necessary for this small grammar, but which employs a technique useful for larger grammars. Each augmented semantic item is assigned a precedence number, and the reordering (sorting) is done so that when item B has higher precedence number than item A, then B is </context>
<context position="41347" citStr="McCord (1981)" startWordPosition="6545" endWordPosition="6546">g, and for this we refer to McCord (1982, 1981). For a grammar as small as MLGRAM, which has no treatment of scoping phenomena, the total cornplexity of the MLG, including the semantic interpretation component we have given in this Section, is certainly greater than that of the comparable DCG in Section 2. However, for larger grammars, the modularity is definitely worthwhile -- conceptually, and probably in the total size of the system. 5. AN EXPERIMENTAL MLG This section describes briefly an experimental MLG, called MODL, which covers the same linguistic ground as the grammar (called MOD) in McCord (1981). The syntactic component of MOD, a DCG, is essentially the same as that in McCord (1982). One feature of these syntactic components is a systematic use of slot-filling to treat complements of verbs and nouns. This method increases modularity between syntax and lexicon, and is described in detail in McCord (1982). One purpose of MOD, which is carried over to MODL, is a good treatment of scoping of modifiers and a good specification of logical form. The logical form language used by MODL as the target of semantic interpretation has been improved somewhat over that used for MOD. We describe here</context>
<context position="43529" citStr="McCord (1981)" startWordPosition="6916" endWordPosition="6917">or &apos;:&apos;. Specifically, if P is a logical form and E is a variable, then P:E (read &amp;quot;P indexed by E&amp;quot;) is also a logical form. When an indexed logical form P:E appears as part of a larger logical form Q, and the index variable E is used elsewhere in Q. then E can be thought of roughly as standing for P together with its &amp;quot;context&amp;quot;. Contexts include references to time and place which are normally left implicit in natural language. When P specifies an event, as in see(john,mary), writing P:E and subsequently using E will guarantee that E refers to the same event. In the logical form language used in McCord (1981), event variables (as arguments of verb and noun senses) were used for indexing. But the indexing operator is more powerful because it can index complex logical forms. For some applications, it is sufficient to ignore contexts, and in such cases we just think of P:E as verifying P and binding E to an instantiation 112 of P. In fact, for PROLOG execution of logical forms without contexts, &apos;:&apos; can be defined by the single clause: P:P &lt;- P. A specific purpose of the MOD system in McCord (1981) was to point out the importance of a class of predicates called focalizers, and to offer a method for de</context>
<context position="48730" citStr="McCord, 1981" startWordPosition="7720" endWordPosition="7721">odularity of the system that it can use the same syntactic rules and parse path no matter where the adverbial focus happens to lie. • Most of the specific linguistic information for semantic interpretation is encoded in the procedures &apos;mod&apos;, &apos;reorder&apos;, and &apos;raise&apos;, which manipulate semantic items. In MODL there are 22 clauses for the procedure &apos;mod&apos;, most of which are unit clauses. These involve ten different modification operators, four of which were illustrated in the preceding section. The definition of &apos;mod&apos; in MODL is taken fairly directly from the corresponding procedure &apos;trans&apos; in MOD (McCord, 1981), although there are some changes involved in handling the new version of the logical form language (LFL), 113 especially the indexing operator. The definitions of &apos;reorder&apos; and &apos;raise&apos; are essentially the same as for procedures in MOD. attack(X,Y). The combination of items on the left of &apos;only&apos; is An illustration of analysis in the two-pass mode in MODL is now given. For the sentence &amp;quot;Leopards only attack monkeys in trees&amp;quot;, the syntactic analysis tree is as follows. sent nounph 1-leopard(X) avp (P&lt;Q)-only(P,Q) 1-attack(X,Y) nounph 1-monkey(Y) prepph @@R-in(Z,R) nounph 1-tree(Z) Here we displa</context>
<context position="51121" citStr="McCord (1981)" startWordPosition="8106" endWordPosition="8107">into the focus Q of &apos;only&apos; and the material to the left goes into the base P. The material to the right is just monkey(Y). The items on the left (&apos;leopard&apos;, &apos;tree&apos;, &apos;in&apos;, &apos;attack&apos;) are allowed to combine (through &apos;mod&apos;) in an independent way before being put Into the base of &apos;only&apos;. The operator CO@R associated with in(Z,R) causes R to be bound to the logical form of the modificand leopard(X)&amp;tree(Z)&amp;in(Z,attack(X,Y)) This goes into the base, so the whole logical form is only(leopard(X)&amp;tree(Z)&amp;in(Z,attack(X,Y)), monkey(Y)). For detailed traces of logical form construction by this method, see McCord (1981). An illustration of the treatment of leftembedding in MODL in a two-pass analysis of the sentence &amp;quot;John sees each boy&apos;s brother&apos;s teacher&amp;quot; is as follows. sent nounph l-(X=john) 1-see(X,W) nounph nounph nounph determiner Q/P-each(P,Q) 1-boy(Y) 1-poss 1-brother(Z.Y) 1-poss 1-teacher(W,Z) Logical form... each(boy(Y),the(brother(Z,Y), the(teacher(W,Z),see(john,W)))). The MODL noun phrase rules include the shift (in a way that is an elaboration of the shift grammar fragment in Section 2), as well as rules for slotfilling for nouns like &apos;brother&apos; and &apos;teacher&apos; which have more than one argument in l</context>
<context position="64064" citStr="McCord, 1981" startWordPosition="10252" endWordPosition="10253">mpirical question which would be decided after logic grammars dealing semantically with massive dictionaries are developed. There is modularity in rules written in the style of Porto and Filgueiras, because all the semantic structure-building is concentrated in the semantic procedures added (by the grammar writer) at the ends of the rules. In MLG&apos;s, in the one-pass mode, the same semantic procedure call, to &apos;semant&apos;, is added at the ends of strong rules, automatically by the compiler. The diversity comes in the ancillary procedures for &apos;semant&apos;, especially &apos;mod&apos;. In fact, &apos;mod&apos; (or &apos;trans&apos; in McCord, 1981) has something in common with the Porto-Filgueiras procedures in that it takes two intermediate representations (semantic items) in its first two arguments and produces a new intermediate representation in its third argument. However, the 116 changes that &apos;mod&apos; makes all involve the modification-operator components of semantic items, rather than the logical-form components. It might be interesting and worthwhile to look at a combination of the two approaches. Both a strength and a weakness of the PortoFilgueiras semantic procedures (compared with &apos;mod&apos;) is that there are many of them, associat</context>
</contexts>
<marker>McCord, 1981</marker>
<rawString>McCord, M. C. (1981) &amp;quot;Focalizers, the scoping problem, and semantic interpretation rules in logic grammars,&amp;quot; Technical Report, University of Kentucky. To appear in Logic Programming and its Applications, D. Warren and M: van Caneghem, Eds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C McCord</author>
</authors>
<title>Semantic interpretation for the EPISTLE system,&amp;quot;</title>
<date>1984</date>
<booktitle>Proc. Second International Logic Programming Conference,</booktitle>
<pages>65--76</pages>
<location>Uppsala.</location>
<contexts>
<context position="4183" citStr="McCord (1984)" startWordPosition="655" endWordPosition="656">f logical forms together with terms that determine how they can combine. The CHAT-80 system (Pereira and Warren, 1982, Pereira, 1983) is a three-pass system. The first pass is a purely syntactic comporent using an extraposition grammar (Pereira, 1981) and producing syntactic analyses in rightmost normal form. The second pass handles word sense selection and slotand the third pass handles some scoping phenomena and the final semantic interpretation. One gets a great deal of modularity between syntax and semantics in that the first component has no elements of semantic interpretation at all. In McCord (1984) a one-pass semantic interpretation component, SEM, for the EPISTLE system (Miller, Heidorn and Jensen, 1981) was described. SEM has been interfaced both to the EPISTLE NLP grammar (Heidorn, 1972, Jensen and Heidorn, 1983), as well as to a logic grammar, SYNT, written as a DCG by the author. These grammars are purely syntactic and use the EPISTLE notion (op. cit.) of approximate parse, which is similar to Pereira&apos;s notion of rightmost normal form, but was developed independently. Thus SYNT/SEM is a two-pass system with a clear modularity between syntax and semantics. 104 In DCG&apos;s and extraposi</context>
<context position="6934" citStr="McCord (1984)" startWordPosition="1084" endWordPosition="1085">he one-pass mode, no syntactic structures are built, but logical forms are built directly during parsing through interleaved calls to the semantic interpretation component, added automatically by the rule compiler. matically produced) syntactic analysis trees much more readable and natural linguistically. In the absence of shift constructions, these trees are like derivation trees, but only with nodes corresponding to strong non-terminals. • In an experimental MLG, the semantic component handles all the scoping phenomena handled by that in McCord (1981) and more than the semantic component in McCord (1984). The logical form language is improved over that in the previous systems. The MLG formalism allows for a great deal of modularity in natural language grammars, because the syntax rules can be written with very little awareness of semantics or the building of analysis structures, and the very same syntactic component can be used in either the one-pass or the two-pass mode described above. Three other logic grammar systems designed with modularity in mind are Hirschman and Puder (1982), Abramson (1984) and Porto and Filgueiras (1984). These will be compared with MLG&apos;s in Section 6. 2. THE MLG S</context>
<context position="37712" citStr="McCord (1984)" startWordPosition="5961" endWordPosition="5962"> &amp;quot;a&amp;quot;. The semantic item for &amp;quot;every pot&amp;quot; gets promoted to a left sister of that for &amp;quot;a chicken&amp;quot;. &apos;raise&apos; is defined basically by a system of unit clauses which look at specific types of phrases. For the small grammar MLGRAM of Section 2, no raising is necessary, and the definition of &apos;raise&apos; can just be omitted. The procedures &apos;raise&apos; and &apos;reorder&apos; are two key ingredients of reshaping (the movement of semantic items to handle scoping problems), which was discussed extensively in McCord (1982, 1981). In those two systems, reshaping was a separate pass of semantic interpretation, but here, as in McCord (1984), reshaping is interleaved with the rest of semantic interpretation. In spite of the new toplevel organization for semantic interpretation of MLG&apos;s, the low-level procedures for raising and reordering are basically the same as in the previous systems, and we refer to the previous reports for further discussion. The procedure &apos;mod&apos;, used in the second clause for &apos;modify&apos;, is the heart of semantic interpretation. mod(Sem, Seml, Sem2) means that the (non-augmented) semantic item Sem modifies (combines with) the item Semi to give the item Sem2. &apos;mod&apos; is defined by a system consisting basically of </context>
<context position="39416" citStr="McCord (1984)" startWordPosition="6237" endWordPosition="6238">left-conjoining operator (its corresponding logical form gets left-conjoined to that of the modificand). The call and(P,Q,R) makes R=P&amp;Q, except that it treats &apos;t&apos; (&apos;true&apos;) as an identity. The next clause for &apos;mod&apos; allows a quantifier semantic item like P/Q-each(Q,P) to operate on an item like 1-man(X) to give the item @P-each(man(X),P). The final clause then allows this item to operate on 1-live(X) to give 1-each(man(X),live(X)). The low-level procedure &apos;mod&apos; is the same (in purpose) as the procedure &apos;trans in McCord (1981), and has close similarities to &apos;trans&apos; in McCord (1982) and &apos;mod&apos; in McCord (1984), so we refer to this previous work for more illustrations of this approach to modification. For MLGRAM, the only ingredient of semantic interpretation remaining to be defined is &apos;reorder&apos;. We can define it in a way that is somewhat more general than is necessary for this small grammar, but which employs a technique useful for larger grammars. Each augmented semantic item is assigned a precedence number, and the reordering (sorting) is done so that when item B has higher precedence number than item A, then B is ordered to the left of A; otherwise items are kept in their original order. The fol</context>
<context position="55414" citStr="McCord (1984)" startWordPosition="8839" endWordPosition="8840">s to allow cross-classification. Multiple daughters of a type node cross-classify it. The lexicon in MODL includes a preprocessor for lexical entries which allows the original lexical entries to specify type constraints in a compact, non-redundant way. There is a Prolog representation for type-hierarchies, and the lexical preprocessor manufactures full type trees from a specification of their leaf nodes. In the one-pass mode for analysis with MLG&apos;s, logical forms get built up during parsing, so logical forms are available for examination by semantic checking procedures of the sort outlined in McCord (1984). If such methods are arguably best, then there may be more argument for a one-pass system (with interleaving of semantics). The general question of the number of passes in a natural language understander is an interesting one. The MLG formalism makes this easier to investigate, because the same syntactic component can be used with onepass or two-pass interpretation. In MODL, there is a small dictionary stored directly in Prolog, but MODL is also interfaced to a large dictionary/morphology system (Byrd, 1983, 1984) which produces syntactic and morphological information for words based on over </context>
</contexts>
<marker>McCord, 1984</marker>
<rawString>McCord, M. C. (1984) &amp;quot;Semantic interpretation for the EPISTLE system,&amp;quot; Proc. Second International Logic Programming Conference, pp. 65-76, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Miller</author>
<author>G E Heidorn</author>
<author>K Jensen</author>
</authors>
<title>Text-critiquing with the EPISTLE system: an author&apos;s aid to better syntax,&amp;quot;</title>
<date>1981</date>
<booktitle>AFIPS Conference Proceedings,</booktitle>
<volume>50</volume>
<pages>649--655</pages>
<marker>Miller, Heidorn, Jensen, 1981</marker>
<rawString>Miller, L. A., Heidorn, G. E., and Jensen, K. (1981) &amp;quot;Text-critiquing with the EPISTLE system: an author&apos;s aid to better syntax,&amp;quot; AFIPS Conference Proceedings, vol. 50, pp. 649-655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Extraposition grammars,&amp;quot;</title>
<date>1981</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>7</volume>
<pages>243--256</pages>
<contexts>
<context position="3821" citStr="Pereira, 1981" startWordPosition="598" endWordPosition="599"> a preliminary stage of semantic interpretation in which the syntactic analysis tree was reshaped to reflect proper scoping of modifiers. The third pass took the reshaped tree and produced logical forms in a straightforward way by carrying out modification of nodes by their daughters using a modular system of rules that manipulate semantic items -- consisting of logical forms together with terms that determine how they can combine. The CHAT-80 system (Pereira and Warren, 1982, Pereira, 1983) is a three-pass system. The first pass is a purely syntactic comporent using an extraposition grammar (Pereira, 1981) and producing syntactic analyses in rightmost normal form. The second pass handles word sense selection and slotand the third pass handles some scoping phenomena and the final semantic interpretation. One gets a great deal of modularity between syntax and semantics in that the first component has no elements of semantic interpretation at all. In McCord (1984) a one-pass semantic interpretation component, SEM, for the EPISTLE system (Miller, Heidorn and Jensen, 1981) was described. SEM has been interfaced both to the EPISTLE NLP grammar (Heidorn, 1972, Jensen and Heidorn, 1983), as well as to </context>
</contexts>
<marker>Pereira, 1981</marker>
<rawString>Pereira, F. (1981) &amp;quot;Extraposition grammars,&amp;quot; American Journal of Computational Linguistics, vol. 7, pp. 243-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Logic for natural language analysis,&amp;quot;</title>
<date>1983</date>
<journal>SRI International, Technical Note</journal>
<volume>275</volume>
<contexts>
<context position="3703" citStr="Pereira, 1983" startWordPosition="580" endWordPosition="581"> were done in this first pass, so that the output analysis trees were actually partially semantic. The second pass was a preliminary stage of semantic interpretation in which the syntactic analysis tree was reshaped to reflect proper scoping of modifiers. The third pass took the reshaped tree and produced logical forms in a straightforward way by carrying out modification of nodes by their daughters using a modular system of rules that manipulate semantic items -- consisting of logical forms together with terms that determine how they can combine. The CHAT-80 system (Pereira and Warren, 1982, Pereira, 1983) is a three-pass system. The first pass is a purely syntactic comporent using an extraposition grammar (Pereira, 1981) and producing syntactic analyses in rightmost normal form. The second pass handles word sense selection and slotand the third pass handles some scoping phenomena and the final semantic interpretation. One gets a great deal of modularity between syntax and semantics in that the first component has no elements of semantic interpretation at all. In McCord (1984) a one-pass semantic interpretation component, SEM, for the EPISTLE system (Miller, Heidorn and Jensen, 1981) was descri</context>
</contexts>
<marker>Pereira, 1983</marker>
<rawString>Pereira, F. (1983) &amp;quot;Logic for natural language analysis,&amp;quot; SRI International, Technical Note 275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>D Warren</author>
</authors>
<title>Definite clause grammars for language analysis - a survey of the formalism and a comparison with transition networks,&amp;quot;</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<pages>231--278</pages>
<contexts>
<context position="14341" citStr="Pereira and Warren (1980)" startWordPosition="2317" endWordPosition="2320">in logical form analysis. An example for the lexical item &amp;quot;each&amp;quot; might be Q/P - each(P,Q). Here the operator Q/P is such that when the &amp;quot;each&amp;quot; item modifies, say, an item having logical form man(X), P gets unified with man(X), and the resulting semantic item is @Q - each(rtan(X),Q) where Q is an operator which causes Q to get unified wit4 the logical form of a further modificand. Details sf the use of semantic items will be given in Section 4. Now let us look at the syntactic component of a sample MLG which covers the same ground as a well-known DCG. The following DCG is taken essentially from Pereira and Warren (1980). It is the sort of DCG that builds logical forms directly oy manipulating partial logical forms in arguments of the grammar symbols. sent(?) --&gt; np(X,P1,P): vp(X,P1). np(X,P:,P) det(P2,F1,P): noun(X,P3): reiclause(X,P3,P2). np(X,?,P) --&gt; name(X). vp(X,P) transverb(X,Y,P1): np(Y,P1,P). vp(X,P) --&gt; intransverb(X,P). relclause(X,PI,P1E.P2) --&gt; +that: vp(X,P2). reiclause(*,P,P) --&gt; nil. det(Pl,P2,P) --&gt; +D: $dt(D,P1,P2,P). noun(X,P) --&gt; +N: Sn(N,X,P). name(X) --&gt; +X: $nm(X). transverb(X,Y,P) --&gt; +V: $tv(V,X,Y,P). intrausverb(X,P) --&gt; +V: $iv(V,X,P). /* Lexicon */ n(man,X,man(X)). n(woman,X,woman(</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, F. and Warren, D. (1980) &amp;quot;Definite clause grammars for language analysis - a survey of the formalism and a comparison with transition networks,&amp;quot; Artificial Intelligence, vol. 13, pp. 231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>D Warren</author>
</authors>
<title>An efficient easily adaptable system for interpreting natural language queries,&amp;quot;</title>
<date>1982</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>8</volume>
<pages>110--119</pages>
<contexts>
<context position="3687" citStr="Pereira and Warren, 1982" startWordPosition="576" endWordPosition="579">selection and slot-filling were done in this first pass, so that the output analysis trees were actually partially semantic. The second pass was a preliminary stage of semantic interpretation in which the syntactic analysis tree was reshaped to reflect proper scoping of modifiers. The third pass took the reshaped tree and produced logical forms in a straightforward way by carrying out modification of nodes by their daughters using a modular system of rules that manipulate semantic items -- consisting of logical forms together with terms that determine how they can combine. The CHAT-80 system (Pereira and Warren, 1982, Pereira, 1983) is a three-pass system. The first pass is a purely syntactic comporent using an extraposition grammar (Pereira, 1981) and producing syntactic analyses in rightmost normal form. The second pass handles word sense selection and slotand the third pass handles some scoping phenomena and the final semantic interpretation. One gets a great deal of modularity between syntax and semantics in that the first component has no elements of semantic interpretation at all. In McCord (1984) a one-pass semantic interpretation component, SEM, for the EPISTLE system (Miller, Heidorn and Jensen, </context>
</contexts>
<marker>Pereira, Warren, 1982</marker>
<rawString>Pereira, F. and Warren, D. (1982) &amp;quot;An efficient easily adaptable system for interpreting natural language queries,&amp;quot; American Journal of Computational Linguistics, vol. 8, pp. 110-119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Porto</author>
<author>M Filgueiras</author>
</authors>
<title>Natural language semantics: A logic programming approach,&amp;quot;</title>
<date>1984</date>
<booktitle>Proc. 1984 International Symposium on Logic Programming,</booktitle>
<pages>228--232</pages>
<location>Atlantic City.</location>
<contexts>
<context position="7472" citStr="Porto and Filgueiras (1984)" startWordPosition="1169" endWordPosition="1172">na handled by that in McCord (1981) and more than the semantic component in McCord (1984). The logical form language is improved over that in the previous systems. The MLG formalism allows for a great deal of modularity in natural language grammars, because the syntax rules can be written with very little awareness of semantics or the building of analysis structures, and the very same syntactic component can be used in either the one-pass or the two-pass mode described above. Three other logic grammar systems designed with modularity in mind are Hirschman and Puder (1982), Abramson (1984) and Porto and Filgueiras (1984). These will be compared with MLG&apos;s in Section 6. 2. THE MLG SYNTACTIC FORMALISM The syntactic component for an MLG consists of a declaration of the strong non-terminals, followed by a sequence of MLG syntax rules. The declaration of strong non-terminals is of the form strongnonterminals(NT1.NT2. .NTn.ni1). where the NTi are the desired strong non-terminals (only their principal functors are indicated). Non-terminals that are not declared strong are called weak. The significance of the strong/weak distinction will be explained below. MLG syntax rules are of the form A &gt; B In the two-pass mode,</context>
<context position="61307" citStr="Porto and Filgueiras (1984)" startWordPosition="9809" endWordPosition="9812">ertainly an appeal in that, because with MLG&apos;s (as well as the methods in McCord (1982, 1;81)), the semantics seems to float off more on its own. Semantic items do have a life of their own, and they can move about in the tree (implicitly, in some versions of the semantic interpreter) because of raising and reordering. This is not as neat theoretically, but it seems more appropriate for capturing actual natural language. Another disadvantage of DCTG&apos;s (as with RG!s) is that the analysis trees that are constructed automatically are derivation trees. The last system to be discussed here, that in Porto and Filgueiras (1984), does not involve a new grammar formalism, but a methodology for writing DCG&apos;s. The authors define a notion of intermediate semantic representation (ISA) including entities and predications, where the predications can be viewed as logical forms. In writing DCG rules, one systematically includes at the end of the rule a call to a semantic procedure (specific to the given rule) which combines ISR&apos;s obtained in arguments of the non-terminals on the right-hand side of the rule. Two DCG rules in this style (given by the authors) are as follows: sent(S) --&gt; np(N): vp(V): $ssv(N,V,S). vp(S) --&gt; verb</context>
</contexts>
<marker>Porto, Filgueiras, 1984</marker>
<rawString>Porto, A. and Filgueiras, M. (1984) &amp;quot;Natural language semantics: A logic programming approach,&amp;quot; Proc. 1984 International Symposium on Logic Programming, pp. 228-232, Atlantic City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sager</author>
</authors>
<title>Natural Language Information Processing: A Computer Grammar of English and Its Applications ,</title>
<date>1981</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="56553" citStr="Sager, 1981" startWordPosition="9024" endWordPosition="9025">produces syntactic and morphological information for words based on over 70,000 lemmata. There are plans to include enough semantic information in this dictionary to provide semantic constraints for a large MLG. Alexa McCray is working on the syntactic component for an MLG with very wide coverage. I wish to thank her for useful conversations about the nature of the system. , 6. COMPARISON WITH OTHER SYSTEMS The Restriction Grammars (RG&apos;s) of Hirschman and Puder (1982) are logic grammars that were designed with modularity in mind. Restriction Grammars derive from the Linguistic String Project (Sager, 1981). An RG consists of context-free phrase structure rules to which restrictions are appended. The rule compiler (written in Prolog and compiling into Prolog), sees to it that derivation trees are constructed automatically during parsing. The restrictions appended to the rules are basically Prolog procedures which can walk around, during the parse, in the partially constructed parse tree, and can look at the words remaining in the input stream. Thus there is a modularity between the phrasestructure parts of the syntax rules and the restrictions. The paper contains an interesting discussion of Pro</context>
</contexts>
<marker>Sager, 1981</marker>
<rawString>Sager, N. (1981) Natural Language Information Processing: A Computer Grammar of English and Its Applications , Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Transition network grammars for natural language analysis,&amp;quot;</title>
<date>1970</date>
<journal>C. ACM,</journal>
<volume>13</volume>
<pages>591--606</pages>
<marker>Woods, 1970</marker>
<rawString>Woods, W. A. (1970) &amp;quot;Transition network grammars for natural language analysis,&amp;quot; C. ACM, vol. 13, pp. 591-606.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>