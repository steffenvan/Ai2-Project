<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001449">
<title confidence="0.998821">
An Efficient Parallel Substrate for Typed Feature Structures on
Shared Memory Parallel Machines
</title>
<author confidence="0.997518">
NINOMIYA Takashi, TORISAWA Kentaro t and TSUJII Jun&apos;ichitI
</author>
<affiliation confidence="0.9921015">
tDepartment of Information Science
Graduate School of Science, University of Tokyo*
</affiliation>
<keyword confidence="0.259494">
1CCL, UMIST, U.K.
</keyword>
<sectionHeader confidence="0.92303" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892222222222">
This paper describes an efficient parallel system
for processing Typed Feature Structures (TFSs)
on shared-memory parallel machines. We call
the system Parallel Substrate for TFS (PSTFS).
PSTFS is designed for parallel computing envi-
ronments where a large number of agents are
working and communicating with each other.
Such agents use PSTFS as their low-level mod-
ule for solving constraints on TFSs and send-
ing/receiving TFSs to/from other agents in an
efficient manner. From a programmers&apos; point
of view, PSTFS provides a simple and unified
mechanism for building high-level parallel NLP
systems. The performance and the flexibility of
our PSTFS are shown through the experiments
on two different types of parallel HPSG parsers.
The speed-up was more than 10 times on both
parsers.
</bodyText>
<sectionHeader confidence="0.996306" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964363636364">
The need for real-time NLP systems has been
discussed for the last decade. The difficulty in
implementing such a system is that people can
not use sophisticated but computationally ex-
pensive methodologies. However, if we could
provide an efficient tool/environment for de-
veloping parallel NLP systems, programmers
would have to be less concerned about the issues
related to efficiency of the system. This became
possible due to recent developments of parallel
machines with shared-memory architecture.
We propose an efficient programming envi-
ronment for developing parallel NLP systems
on shared-memory parallel machines, called the
Parallel Substrate for Typed Feature Structures
(PSTFS). The environment is based on agent-
based/object-oriented architecture. In other
words, a system based on PSTFS has many
computational agents running on different pro-
cessors in parallel; those agents communicate
with each other by using messages including
TFSs. Tasks of the whole system, such as pars-
</bodyText>
<footnote confidence="0.7192335">
* This research is partially founded by the project of
JSPS(JSPS-RFTF96P00502).
</footnote>
<note confidence="0.542123">
Order
</note>
<figureCaption confidence="0.999953">
Figure 1: Agent-based System with the PSTFS
</figureCaption>
<bodyText confidence="0.987498875">
ing or semantic processing, are divided into sev-
eral pieces which can be simultaneously com-
puted by several agents.
Several parallel NLP systems have been de-
veloped previously. But most of them have been
neither efficient nor practical enough (Adriaens
and Hahn, 1994). On the other hand, our
PSTFS provides the following features.
</bodyText>
<listItem confidence="0.9987252">
• An efficient communication scheme for
messages including Typed Feature Struc-
tures (TFSs) (Carpenter, 1992).
• Efficient treatment of TFSs by an abstract
machine (Makino et al., 1998).
</listItem>
<bodyText confidence="0.9998278">
Another possible way to develop parallel NLP
systems with TFSs is to use a full concurrent
logic programming language (Clark and Gre-
gory, 1986; Ueda, 1985). However, we have ob-
served that it is necessary to control parallelism
in a flexible way to achieve high-performance.
(Fixed concurrency in a logic programming lan-
guage does not provide sufficient flexibility.)
Our agent-based architecture is suitable for ac-
complishing such flexibility in parallelism.
The next section discusses PSTFS from a pro-
grammers&apos; point of view. Section 3 describes
the PSTFS architecture in detail. Section 4 de-
scribes the performance of PSTFS on our HPSG
parsers.
</bodyText>
<page confidence="0.989952">
968
</page>
<figure confidence="0.907823093023256">
Franz 1
Schubert
Johann 1
Bach J
(Franz, Schubert)
Fran:
Schubert
(Johann, Bach)
Johann
Bach
F = {
[
IRST
AST
IRST
AST
...
:R= {
ULL
IRST
AST
FULL
IRST
AST
&apos;•&apos;
(A) Description of CSAs
define Control Agent name-eoacatenator-sub
When a message selv*(s) arrives, do the followings,
S := CSA selve-constraint(concatenate_name(x,?));
return S;
define Control Agent name-concatenator
When a message stave arrives, do the followings,
R
F := (CSA selve-ceuttraist(name(?)));
forall0;E F do
create name-concatenator-sub agent .Vi;
./if, solvo(x); i := i + 1;
foreillend
forj = 0 to i do
R := RU (Wait- for-result(Arj));
forend
return 12;
B) Description of CAs
</figure>
<figureCaption confidence="0.9105205">
Figure 2: Example concatenate_name
2 Programmers&apos; View
</figureCaption>
<bodyText confidence="0.975374">
From a programmers&apos; point of view, the PSTFS
mechanism is quite simple and natural, which
is due to careful design for accomplishing high-
performance and ease of programming.
Systems to be constructed on our PSTFS will
include two different types of agents:
</bodyText>
<listItem confidence="0.9999225">
• Control Agents (CAs)
• Constraint Solver Agents (CSAs)
</listItem>
<bodyText confidence="0.997785414634146">
As illustrated in Figure 1, CAs have overall
control of a system, including control of par-
allelism, and they behave as masters of CSAs.
CSAs modify TFSs according to the orders from
CAs. Note that CAs can neither modify nor
generate TFSs by themselves.
PSTFS has been implemented by combin-
ing two existing programming languages: the
concurrent object-oriented programming lan-
guage ABCL/f (Taura, 1997) and the sequential
programming language LiLFeS (Makino et al.,
1998). CAs can be written in ABCL/f, while
description of CSAs can be mainly written in
LiLFeS.
Figure 2 shows an example of a part of the
PSTFS code. The task of this code is to con-
catenate the first and the second name in a
given list. One of the CAs is called name-
concatenator. This specific CA gathers pairs of
the first and last name by asking a CSA with the
message solve-constraint(&apos;name(?)&apos;). When
the CSA receives this message, the argument
&apos;name(?)&apos; is treated as a Prolog query in
LiLFeS1, according to the program of a CSA
((A) of Figure 2). There are several facts with
the predicate &apos;name&apos;. When the goal &apos;name(?)&apos;
is processed by a CSA, all the possible answers
defined by these facts are returned. The ob-
tained pairs are stored in the variable F in the
name-concatenator ((C) in Figure 2).
The next behavior of the name-concatenator
agent is to create CAs (name-concatenator-
subs) and to send the message solve with a
TFS to each created CA running in parallel.
The message contains one of the TFSs in F.
Each name-concatenator-sub asks a CSA to con-
catenate FIRST and LAST in a TFS. Then
each CSA concatenates them using the defi-
nite clause concatenate_name given in (A) of
Figure 2. The result is returned to the name-
concatenator-sub which had asked to do the job.
Note that the name-concatenator-sub can ask
any of the existing CSAs. All CSAs can basi-
cally perform concatenation in parallel and in-
dependent way. Then, the name-concatenator
waits for the name-concatenator-sub to return
concatenated names, and puts the return val-
ues into the variable R.
The CA name-concatcnator controls the over-
all process. It controls parallelism by creating
CAs and sending messages to them. On the
other hand. all the operations on TFSs are per-
formed by CSAs when they are asked by CAs.
Suppose that one is trying to implement a
parsing system based on PSTFS. The distinc-
tion between CAs and CSAs roughly corre-
sponds to the distinction between an abstract
parsing schema and application of phrase struc-
ture rules. Here, a parsing schema means a
high-level description of a parsing algorithm in
which the application of phrase structure rules
is regarded as an atomic operation or a sub-
routine. This distinction is a minor factor in
writing a sequential parser, but it has a major
impact on a parallel environment.
For instance, suppose that several distinct
agents evoke applications of phrase structure
rules against the same data simultaneously, and
the applications are accompanied with destruc-
tive operations on the data. This can cause an
anomaly, since the agents will modify the orig-
inal data in unpredictable order and there is
no way to keep consistency. In order to avoid
this anomaly, one has to determine what is an
atomic operation and provide a method to pre-
vent the anomaly when atomic operations are
evoked by several agents. In our framework,
any action taken by CSAs is viewed as such
an atomic operation and it is guaranteed that
no anomaly occurs even if CSAs concurrently
LiLFeS supports definite clause programs, a TFS
version of Horn clauses.
</bodyText>
<figure confidence="0.982493368421053">
name(
begin-definitions
Constraint Solver Agent
IRST Franz 1 \
AST Schubert
&apos;FIRST Johann 1 \
name( AST Bach JP
cortcatenate-name(X, Y)
Fl
Ei
FULL ID Fl
Y= FIRST 1
LAST 7
end-de nitions
X FIRST
LAST
(C) Values of F and R
969
PSTFS
</figure>
<figureCaption confidence="0.999955">
Figure 3: Inside of the PSTFS
</figureCaption>
<bodyText confidence="0.999946214285714">
perform operations on the same data. This
can be done by introducing copying of TFSs,
which does not require any destructive opera-
tions. The details are described in the next sec-
tion.
The other implication of the distinction be-
tween CAs and CSAs is that this enables effi-
cient communication between agents in a natu-
ral way. During parsing in HPSG, it is possible
that TFSs with hundreds of nodes can be gen-
erated. Encoding such TFSs in a message and
sending them in an efficient way are not triv-
ial. PSTFS provides a communication scheme
that enables efficient sending/receiving of such
TFSs. This becomes possible because of the
distinction of agents. In other words, since CAs
cannot modify a TFS, CAs do not have to have
a real image of TFSs. When CSAs return the
results of computations to CAs, the CSAs send
only an ID of a TFS. Only when the ID is passed
to other CSAs and they try to modify a TFS
with the ID, the actual transfer of the TFS&apos;s
real image occurs. Since the transfer is car-
ried out only between CSAs, it can be directly
performed using a low level representation of
TFSs used in CSAs in an efficient manner. Note
that if CAs were to modify TFSs directly, this
scheme could not have been used.
</bodyText>
<sectionHeader confidence="0.989436" genericHeader="introduction">
3 Architecture
</sectionHeader>
<bodyText confidence="0.9999086">
This section explains the inner structure of
PSTFS focusing on the execution mechanism of
CSAs (See (Taura, 1997) for further detail on
CAs). A CSA is implemented by modifying the
abstract machine for TFSs (i.e., LiAM), origi-
nally designed for executing LiLFeS (Makino et
al., 1998).
The important constraint in designing the ex-
ecution mechanism for CSAs is that TFSs gen-
erated by CSAs must be kept unmodified. This
is because the TFSs must be used with several
agents in parallel. If the TFS had been modi-
fied by a CSA and if other agents did not know
the fact, the expected results could not have
been obtained. Note that unification, which is
</bodyText>
<figure confidence="0.99425875">
(0 Copying from shared heap
(ii) Computation on local heap
(iii) Write resulting TFSs to shared heap
Res • TFS
</figure>
<figureCaption confidence="0.999962">
Figure 4: Operation steps on PSTFS
</figureCaption>
<bodyText confidence="0.999139454545455">
a major operation on TFSs, is a destructive op-
eration, and modifications are likely to occur
while executing CSAs. Our execution mecha-
nism handles this problem by letting CSAs copy
TFSs generated by other CSAs at each time.
Though this may not look like an efficient way
at first glance, it has been performed efficiently
by shared memory mechanisms and our copying
methods.
A CSA uses two different types of memory
areas as its heap:
</bodyText>
<listItem confidence="0.999586">
• shared heap
• local heap
</listItem>
<bodyText confidence="0.999852866666667">
A local heap is used for temporary operations
during the computation inside a CSA. A CSA
cannot read/write local heap of other CSAs. A
shared heap is used as a medium of commu-
nication between CSAs, and it is realized on
a shared memory. When a CSA completes a
computation on TFSs, it writes the result on
a shared heap. Since the shared heap can be
read by any CSAs, each CSA can read the re-
sult performed by any other CSAs. However,
the portion of a shared heap that the CSA can
write to is limited. Any other CSA cannot write
on that portion.
Next, we look at the steps performed by a
CSA when it is asked by CAs with a message.
</bodyText>
<figure confidence="0.83175475">
Local Heap
Shared Heap
Local Heap
Shared Heap
</figure>
<page confidence="0.980521">
970
</page>
<bodyText confidence="0.999732">
Note that the message only contains the IDs of
the TFSs as described in the previous section.
The IDs are realized as pointers on the shared
heap.
</bodyText>
<listItem confidence="0.991280785714286">
1. Copy TFSs pointed at by the IDs in the
message from the shared heap to the local
heap of the CSA. ((i) in Figure 4.)
2. Process a query using LiAM and the local
heap. ((ii) in Figure 4.)
3. If a query has an answer, the result is
copied to the portion of the shared heap
writable by the CSA. Keep IDs on the
copied TFSs. If there is no answer for the
query, go to Step 5. ((iii) in Figure 4.)
4. Evoke backtracking in LiAM and go to Step
2.
5. Send the message, including the kept IDs,
back to the CA that had asked the task.
</listItem>
<bodyText confidence="0.999349344827586">
Note that, in step 3, the results of the compu-
tation becomes readable by other CSAs. This
procedure has the following desirable features.
Simultaneous Copying An identical TFS on
a shared heap can be copied by several
CSAs simultaneously. This is due to our
shared memory mechanism and the prop-
erty of LiAM that copying does not have
any side-effect on TFSs2.
Simultaneous/Safe Writing CSAs can
write on their own shared heap without the
danger of accidental modification by other
CSAs.
Demand Driven Copying As described in
the previous section, the transfer of real
images of TFSs is performed only after the
IDs of the TFSs reach to the CSAs requir-
ing the TFSs. Redundant copying/sending
of the TFSs&apos; real image is reduced, and the
transfer is performed efficiently by mecha-
nisms originally provided by LiAM.
With efficient data transfer in shared-memory
machines, these features reduce the overhead of
parallefization.
Note that copying in the procedures makes
it possible to support non-determinism in NLP
systems. For instance, during parsing, interme-
diate parse trees must be kept. In a chart pars-
ing for a unification-based grammar, generated
</bodyText>
<footnote confidence="0.8328">
2Actually, this is not trivial. Copying in Step 3 nor-
malizes TFSs and stores the TFSs into a continuous re-
gion on a shared heap. TFSs stored in such a way can
be copied without any side-effect.
</footnote>
<bodyText confidence="0.999815571428571">
edges are kept untouched, and destructive oper-
ations on the results must be done after copying
them. The copying of TFSs in the above steps
realizes such mechanisms in a natural way, as it
is designed for efficient support for data sharing
and destructive operations on shared heaps by
parallel agents.
</bodyText>
<sectionHeader confidence="0.9938615" genericHeader="method">
4 Application and Performance
Evaluation
</sectionHeader>
<bodyText confidence="0.999232857142857">
This section describes two different types of
HPSG parsers implemented on PSTFS. One is
designed for our Japanese grammar and the al-
gorithm is a parallel version of the CKY algo-
rithm (Kasami, 1965). The other is a parser for
an ALE-style Grammar (Carpenter and Penn,
1994). The algorithms of both parsers are based
on parallel parsing algorithms for CFG (Ni-
nomiya et al., 1997; Nijholt, 1994; Grishman
and Chitrao, 1988; Thompson, 1994). Descrip-
tions of both parsers are concise. Both of them
are written in less than 1,000 lines. This shows
that our PSTFS can be easily used. With the
high performance of the parsers, this shows the
feasibility and flexibility of our PSTFS.
For simplicity of discussion, we assume that
HPSG consists of lexical entries and rule
schemata. Lexical entries can be regarded as
11 Ss assigned to each word. A rule schema is
a rule in the form of z — abc • • • where z. a. b. c
are TFSs.
</bodyText>
<subsectionHeader confidence="0.746936">
4.1 Parallel CKY-style HPSG Parsing
Algorithm
</subsectionHeader>
<bodyText confidence="0.9885706">
A sequential CKY parser for CFG uses a data
structure called a triangular table. Let Fi j de-
note a cell in the triangular table. Each cell Fij
has a set of the non-terminal symbols in CFG
that can generate the word sequence from the
i 1-th word to the j-th word in an input sen-
tence. The sequential CKY algorithm computes
each Fio according to a certain order.
Our algorithm for a parallel CKY-style parser
for HPSG computes each Fi,j in parallel. Note
that F1,3 contains TFSs covering the word se-
quence from the i 1-th word to the j-th
word, not non-terminals. We consider only the
rule schemata with a form of z ab where
z, a, b are TFSs. Parsing is started by a CA
called PARSER- PA7ZSE7Z creates cell-agents
&lt; i &lt;j &lt; n) and distributes them to pro-
cessors on a parallel machine (Figure 5). Each
Cij computes Fij in parallel. More precisely,
Ci,j(j – i = 1) looks up a dictionary and obtains
lexical entries. Ci,j(j – i&gt; 1) waits for the mes-
sages including Fi,k and Fk j for all k(i &lt; k &lt;j)
from other cell-agents. When C2,3 receives Fi k
and Fk 3 for an arbitrary k, C2,3 computes TF&apos;s
by applying rule schemata to each members of
</bodyText>
<page confidence="0.997633">
971
</page>
<figureCaption confidence="0.99809">
Figure 5: Correspondence between CKY matrix
and agents: Ci,j correspond to the element of a
CKY triangular matrix
</figureCaption>
<bodyText confidence="0.99321525">
Fi,k and Fk,j. The computed TFSs are consid-
ered to be mothers of members of Fi,k and Fkd
and they are added to Fij. Note that these ap-
plications of rule schemata are done in parallel
in several C&apos;SAs3. Finally, when computation of
(using Ft.k and Fk,j for all k(i &lt; k &lt; j)) is
completed, distributes Fio to other agents
waiting for F. Parsing is completed when the
computation of F0,„ is completed.
We have done a series of experiments on a
shared-memory parallel machine, SUN Ultra
Enterprise 10000 consisting of 64 nodes (each
node is a 250 MHz UltraSparc) and 6 GByte
shared memory. The corpus consists of 879
random sentences from the EDR Japanese cor-
pus written in Japanese (average length of sen-
tences is 20.8)4. The grammar we used is an
underspecified Japanese HPSG grammar (Mit-
suishi et al., 1998) consisting of 6 ID-schemata
and 39 lexical entries (assigned to functional
words) and 41 lexical-entry-templates (assigned
to parts of speech). This grammar has wide cov-
erage and high accuracy for real-world texts&apos;.
Table 1 shows the result and comparison with
a parser written in LiLFeS. Figure 6 shows
its speed-up. From the Figure 6, we observe
that the maximum speedup reaches up to 12.4
times. The average parsing time is 85 msec per
</bodyText>
<footnote confidence="0.9951515">
3CSAs cannot be added dynamically in our imple-
mentation. So, to gain the maximum parallelism, we
assigned a CSA to each processor. Each C, asks the
CSA on the same processor to apply rule schemata.
&apos;We chose 1000 random sentences from the EDR
Japanese corpus, and the used 897 sentences are all the
parsable sentences by the grammar.
&apos;This grammar can generate parse trees for 82% of
10000 sentences from the EDR Japanese corpus and the
dependency accuracy is 78%.
</footnote>
<table confidence="0.998999">
Number of Avg. of Parsing Time(rnsec)
Processors PSTFS laLFeS
1 1057 991 &apos;
10 248
20 138
30 106
40 93
50 85
60 135
</table>
<tableCaption confidence="0.999803">
Table 1: Average parsing time per sentence
</tableCaption>
<figureCaption confidence="0.987489">
Figure 6: Speed-up of parsing time on parallel
CKY-style HPSG parser
</figureCaption>
<bodyText confidence="0.846479">
sentence6.
</bodyText>
<subsectionHeader confidence="0.955235">
4.2 Chart-based Parallel HPSG
Parsing Algorithm for ALE
Grammar
</subsectionHeader>
<bodyText confidence="0.999623529411765">
Next, we developed a parallel chart-based
HPSG parser for an ALE-style grammar. The
algorithm is based on a chart schema on which
each agent throws active edges and inactive
edges containing a TFS. When we regard the
rule schemata as a set of rewriting rules in
CFG, this algorithm is exactly the same as
the Thompson&apos;s algorithm (Thompson, 1994)
and similar to FAX (Matsumoto, 1987). The
main difference between the chart-based parser
and our CKY-style parser is that the ALE-style
parser supports a n-branching tree.
A parsing process is started by a CA called
PARSER- It .creates word-position agents
Pk(0 &lt; k &lt; n), distributes them to parallel
processors and waits for them to complete their
tasks. The role of the word-position agent Pk
</bodyText>
<footnote confidence="0.537619">
&apos;Using 60 processors is worse than with 50 proces-
sors. In general, when the number of processes increases
to near or more than the number of existing processors,
context switch between processes occurs frequently on
shared-memory parallel machines (many people can use
the machines simultaneously). We believe the cause for
the inefficiency when using 60 processors lies in such con-
text switches.
</footnote>
<figure confidence="0.9929088">
Speed-up
14
12
10
a
4
2
0
0 10 20 30 40 50
* of processors
</figure>
<page confidence="0.99322">
972
</page>
<tableCaption confidence="0.996478">
Table 2: Test corpus for parallel ALE-style
HPSG parser
</tableCaption>
<bodyText confidence="0.980671857142857">
is to collect edges adjacent to the position k.
A word-position agent has its own active edges
and inactive edges. An active edge is in the form
(i,z —&gt; AoxB), where A is a set of TFSs which
have already been unified with an existing con-
stituents, B is a set of TFSs which have not
been unified yet, and x is the TFS which can be
unified with the constituent in an inactive edge
whose left-side is in position k. Inactive edges
are in the form (k,x,j), where kis the left-side
position of the constituent x and j is the right-
side position of the constituent x. That is, the
set of all inactive edges whose left-side position
is k are collected by Pk.
In our algorithm, Pk is always waiting for ei-
ther an active edge or an inactive edge, and per-
forms the following procedure when receiving an
edge.
• When Pk receives an active edge (i. z ---
A o B), Pk preserve the edge and tries to
find the unifiable constituent with x from
the set of inactive edges that Pk has already
received. lithe unification succeeds, a new
active edge (i, z Ax o B) is created. If
the dot in the new active edge reaches to
the end of RHS (i.e. B = 0), a new inactive
edge is created and is sent to pi. Otherwise
the new active edge is sent to Pj.
</bodyText>
<listItem confidence="0.924015454545455">
• When Pk receives an inactive edge (k, x, j),
Pk preserves the edge and tries to find the
unifiable constituent on the right side of
the dot from the set of active edges that
Pk has already received. If the unification
succeeds, a new active edge (i, z —&gt; Ax o B)
is created. If the dot in the new active edge
reaches to the end of RHS (i.e. B = 0), a
new inactive edge is created and is sent to
Pi. Otherwise the new active edge is sent
to P.
</listItem>
<bodyText confidence="0.9997346">
As long as word-position-agents follow these
behavior, they can run in parallel without any
other restriction.
We have done a series of experiments in the
same machine settings as the experiments with
</bodyText>
<table confidence="0.9992275">
Short Length sentences
Number of Avg. of Parsing Time(msec) ALE
Processors PSTFS LiLFeS
1 b2h 125 1590
10 160
20 156
30 127
40 205
50 142
60 170
Lon • Length Sentences
Number of Avg. of Farsing Time(msec)
Processors PSTFS LiLFeS ALE
1 1930( 30557 359370
10 3208
20 2139
30 1776
40 1841
50 1902
60 2052
</table>
<tableCaption confidence="0.999726">
Table 3: Average parsing time per sentence
</tableCaption>
<figureCaption confidence="0.9777725">
Figure 7: Speed-up of parsing time on chart-
based parallel HPSG parser
</figureCaption>
<bodyText confidence="0.999340642857143">
the CKY-style HPSG parser. We measured
both its speed up and real parsing time, and
we compared our parallel parser with the ALE
system and a sequential parser on LiLFeS. The
grammar we used is a sample HPSG grammar
attached to ALE system&amp;quot;, which has 7 schemata
and 62 lexical entries. The test corpus we
used in this experiment is shown in the Table
2. Results and comparison with other sequen-
tial parsing systems are given in Table 3. Its
speedup is shown in Figure 7. From the figure,
we observe that the maximum speedup reaches
up to 10.9 times and its parsing time is 1776
msec per sentence.
</bodyText>
<subsectionHeader confidence="0.944386">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.972491714285714">
In both parsers, parsing time reaches a level
required by real-time applications, though we
used computationally expensive grammar for-
malisms, i.e. HPSG with reasonable coverage
and accuracy. This shows the feasibility of our
&apos;This sample grammar is converted to LiLFeS style
half automatically.
</bodyText>
<figure confidence="0.989747">
Short Length Sentences
kiln believes sandy to walk
a person whom he sees walks
he is seen
he persuades her to walk
Long Length Sentences
a person who sees kim who sees sandy whom he tries to
see walks
a person who sees kim who sees sandy who sees kim whom
he tries to see walks
a person who sees kim who sees sandy who sees kim who
believes her to tend to walk walks
4
Speed-up
12
10
8
4
2
0
0 10 20 30 40 50 80
S of Processors
</figure>
<page confidence="0.956549">
973
</page>
<note confidence="0.873949">
Processor 10 Processor Status
</note>
<figureCaption confidence="0.999576">
Figure 8: Processors status
</figureCaption>
<bodyText confidence="0.999989208333333">
framework for the goal to provide a parallel pro-
gramming environment for real-time NLP. In
addition, our parallel HPSG parsers are con-
siderably more efficient than other sequential
HPSG parsers.
However, the speed-up is not proportional to
the number of processors. We think that this is
because the parallelism extracted in our parsing
algorithm is not enough. Figure 8 shows the log
of parsing Japanese sentences by the CKY-style
parser. The black lines indicate when a proces-
sor is busy. One can see that many processors
are frequently idle.
We think that this idle time does not sug-
gest that parallel NLP systems are useless. On
the contrary, this suggest that parallel NLP sys-
tems have many possibilities. If we introduce
semantic processing for instance, overall pro-
cessing time may not change because the idle
time is used for semantic processing. Another
possibility is the use of parallel NLP systems as
a server. Even if we feed several sentences at a
time, throughput will not change, because the
idle time is used for parsing different sentences.
</bodyText>
<sectionHeader confidence="0.998404" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999532090909091">
We described PSTFS, a substrate for parallel
processing of typed feature structures. PSTFS
serves as an efficient programming environment
for implementing parallel NLP systems. We
have shown the feasibility and flexibility of
our PSTFS through the implementation of two
HPSG parsers.
For the future, we are considering the use of
our HPSG parser on PSTFS for a speech recog-
nition system, a Natural Language Interface or
Speech Machine Translation applications.
</bodyText>
<sectionHeader confidence="0.999599" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999356596491228">
Adriaens and Hahn, editors. 1994. Parallel
Natural Language Processing. Ablex Publish-
ing Corporation, New Jersey.
Bob Carpenter and Gerald Penn. 1994. ALE
2.0 user&apos;s guide. Technical report, Carnegie
Mellon University Laboratory for Computa-
tional Linguistics, Pittsburgh, PA.
Bob Carpenter. 1992. The Logic of Typed Fea-
ture Structures. Cambridge University Press,
Cambridge, England.
K. Clark and S. Gregory. 1986. Parlog: Parallel
programming in logic. Journal of the ACM
Transaction on Programming Languages and
Systems, 8(1):1-49.
Ralph Grishman and Mehesh Chitrao. 1988.
Evaluation of a parallel chart parser. In Pro-
ceedings of the second Conference on Applied
Natural Language Processing, pages 71-76.
Association for Computational Linguistics.
T. Kasami. 1965. An efficient recognition and
syntax algorithm for context-free languages.
Technical Report AFCRL-65-758, Air Force
Cambrige Research Lab., Bedford, Mass.
Takaki Makino, Minoru Yoshida, Kentaro Tori-
sawa, and Jun&apos;ichi Tsujii. 1998. LiLFeS
— towards a practical HPSG parser. In
COLING-ACL&apos;98 Proceedings, August.
Yuji Matsumoto. 1987. A parallel parsing sys-
tem for natural language analysis. In Proceed-
ings of 3rd International Conference on Logic
Programming, pages 396-409.
Yutaka Mitsuishi, Kentaro Torisavva, and
Jun&apos;ichi Tsujii. 1998. HPSG-style underspec-
ified Japanese grammar with wide coverage.
In COL ING-ACP98 Proceedings, August.
Anton Nijholt, 1994. Parallel Natural Language
Processing, chapter Parallel Approaches to
Context-Free Language Parsing, pages 135-
167. Ablex Publishing Corporation.
Takashi Ninomiya, Kentaro Torisawa, Kenjiro
Taura, and Jun&apos;ichi Tsujii. 1997. A par-
allel cky parsing algorithm on large-scale
distributed-memory parallel machines. In
PACLING &apos;97, pages 223-231, September.
Kenjiro Taura. 1997. Efficient and Reusable
Implementation of Fine-Grain Multithread-
ing and Garbage Collection on Distributed-
Memory Parallel Computers. Ph.D. thesis,
Department of Information Sciencethe, Uni-
versity of Tokyo.
Henry S. Thompson, 1994. Parallel Natural
Language Processing, chapter Parallel Parsers
for Context-Free Grammars–Two Actual Im-
plementations Comparesd, pages 168-187.
Ablex Publishing Corporation.
Kazunori Ueda. 1985. Guarded horn clauses.
Technical Report TR-103, ICOT.
</reference>
<figure confidence="0.999004083333333">
I , I
r.---r-. ff--- •=_.- - -
- -
-
- -r.-. -
616,12 616.14 616.16
40
30
20 —
10
0
616.18 (sec)
</figure>
<page confidence="0.97563">
974
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.511880">
<title confidence="0.995828">An Efficient Parallel Substrate for Typed Feature Structures on Shared Memory Parallel Machines</title>
<author confidence="0.936628">TORISAWA Kentaro t Jun&apos;ichitI Takashi</author>
<affiliation confidence="0.879455">tDepartment of Information Science Graduate School of Science, University of Tokyo* UMIST, U.K.</affiliation>
<abstract confidence="0.988635736842105">This paper describes an efficient parallel system for processing Typed Feature Structures (TFSs) shared-memory parallel machines. We Parallel Substrate for TFS (PSTFS). is designed parallel computing environments where a large number of agents are working and communicating with each other. agents use PSTFS as their low-level modsolving constraints on TFSs and sending/receiving TFSs to/from other agents in an efficient manner. From a programmers&apos; point of view, PSTFS provides a simple and unified mechanism for building high-level parallel NLP systems. The performance and the flexibility of our PSTFS are shown through the experiments on two different types of parallel HPSG parsers. The speed-up was more than 10 times on both parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adriaens</author>
<author>editors Hahn</author>
</authors>
<date>1994</date>
<booktitle>Parallel Natural Language Processing.</booktitle>
<publisher>Ablex Publishing Corporation,</publisher>
<location>New Jersey.</location>
<contexts>
<context position="2451" citStr="Adriaens and Hahn, 1994" startWordPosition="364" endWordPosition="367">other words, a system based on PSTFS has many computational agents running on different processors in parallel; those agents communicate with each other by using messages including TFSs. Tasks of the whole system, such as pars* This research is partially founded by the project of JSPS(JSPS-RFTF96P00502). Order Figure 1: Agent-based System with the PSTFS ing or semantic processing, are divided into several pieces which can be simultaneously computed by several agents. Several parallel NLP systems have been developed previously. But most of them have been neither efficient nor practical enough (Adriaens and Hahn, 1994). On the other hand, our PSTFS provides the following features. • An efficient communication scheme for messages including Typed Feature Structures (TFSs) (Carpenter, 1992). • Efficient treatment of TFSs by an abstract machine (Makino et al., 1998). Another possible way to develop parallel NLP systems with TFSs is to use a full concurrent logic programming language (Clark and Gregory, 1986; Ueda, 1985). However, we have observed that it is necessary to control parallelism in a flexible way to achieve high-performance. (Fixed concurrency in a logic programming language does not provide sufficie</context>
</contexts>
<marker>Adriaens, Hahn, 1994</marker>
<rawString>Adriaens and Hahn, editors. 1994. Parallel Natural Language Processing. Ablex Publishing Corporation, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
<author>Gerald Penn</author>
</authors>
<title>ALE 2.0 user&apos;s guide.</title>
<date>1994</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University Laboratory for Computational Linguistics,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="13970" citStr="Carpenter and Penn, 1994" startWordPosition="2362" endWordPosition="2365">-effect. edges are kept untouched, and destructive operations on the results must be done after copying them. The copying of TFSs in the above steps realizes such mechanisms in a natural way, as it is designed for efficient support for data sharing and destructive operations on shared heaps by parallel agents. 4 Application and Performance Evaluation This section describes two different types of HPSG parsers implemented on PSTFS. One is designed for our Japanese grammar and the algorithm is a parallel version of the CKY algorithm (Kasami, 1965). The other is a parser for an ALE-style Grammar (Carpenter and Penn, 1994). The algorithms of both parsers are based on parallel parsing algorithms for CFG (Ninomiya et al., 1997; Nijholt, 1994; Grishman and Chitrao, 1988; Thompson, 1994). Descriptions of both parsers are concise. Both of them are written in less than 1,000 lines. This shows that our PSTFS can be easily used. With the high performance of the parsers, this shows the feasibility and flexibility of our PSTFS. For simplicity of discussion, we assume that HPSG consists of lexical entries and rule schemata. Lexical entries can be regarded as 11 Ss assigned to each word. A rule schema is a rule in the form</context>
</contexts>
<marker>Carpenter, Penn, 1994</marker>
<rawString>Bob Carpenter and Gerald Penn. 1994. ALE 2.0 user&apos;s guide. Technical report, Carnegie Mellon University Laboratory for Computational Linguistics, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>The Logic of Typed Feature Structures.</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="2623" citStr="Carpenter, 1992" startWordPosition="391" endWordPosition="392"> TFSs. Tasks of the whole system, such as pars* This research is partially founded by the project of JSPS(JSPS-RFTF96P00502). Order Figure 1: Agent-based System with the PSTFS ing or semantic processing, are divided into several pieces which can be simultaneously computed by several agents. Several parallel NLP systems have been developed previously. But most of them have been neither efficient nor practical enough (Adriaens and Hahn, 1994). On the other hand, our PSTFS provides the following features. • An efficient communication scheme for messages including Typed Feature Structures (TFSs) (Carpenter, 1992). • Efficient treatment of TFSs by an abstract machine (Makino et al., 1998). Another possible way to develop parallel NLP systems with TFSs is to use a full concurrent logic programming language (Clark and Gregory, 1986; Ueda, 1985). However, we have observed that it is necessary to control parallelism in a flexible way to achieve high-performance. (Fixed concurrency in a logic programming language does not provide sufficient flexibility.) Our agent-based architecture is suitable for accomplishing such flexibility in parallelism. The next section discusses PSTFS from a programmers&apos; point of v</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Bob Carpenter. 1992. The Logic of Typed Feature Structures. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Clark</author>
<author>S Gregory</author>
</authors>
<title>Parlog: Parallel programming in logic.</title>
<date>1986</date>
<journal>Journal of the ACM Transaction on Programming Languages and Systems,</journal>
<pages>8--1</pages>
<contexts>
<context position="2843" citStr="Clark and Gregory, 1986" startWordPosition="425" endWordPosition="429">ed into several pieces which can be simultaneously computed by several agents. Several parallel NLP systems have been developed previously. But most of them have been neither efficient nor practical enough (Adriaens and Hahn, 1994). On the other hand, our PSTFS provides the following features. • An efficient communication scheme for messages including Typed Feature Structures (TFSs) (Carpenter, 1992). • Efficient treatment of TFSs by an abstract machine (Makino et al., 1998). Another possible way to develop parallel NLP systems with TFSs is to use a full concurrent logic programming language (Clark and Gregory, 1986; Ueda, 1985). However, we have observed that it is necessary to control parallelism in a flexible way to achieve high-performance. (Fixed concurrency in a logic programming language does not provide sufficient flexibility.) Our agent-based architecture is suitable for accomplishing such flexibility in parallelism. The next section discusses PSTFS from a programmers&apos; point of view. Section 3 describes the PSTFS architecture in detail. Section 4 describes the performance of PSTFS on our HPSG parsers. 968 Franz 1 Schubert Johann 1 Bach J (Franz, Schubert) Fran: Schubert (Johann, Bach) Johann Bac</context>
</contexts>
<marker>Clark, Gregory, 1986</marker>
<rawString>K. Clark and S. Gregory. 1986. Parlog: Parallel programming in logic. Journal of the ACM Transaction on Programming Languages and Systems, 8(1):1-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Mehesh Chitrao</author>
</authors>
<title>Evaluation of a parallel chart parser.</title>
<date>1988</date>
<booktitle>In Proceedings of the second Conference on Applied Natural Language Processing,</booktitle>
<pages>71--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14117" citStr="Grishman and Chitrao, 1988" startWordPosition="2386" endWordPosition="2389">ps realizes such mechanisms in a natural way, as it is designed for efficient support for data sharing and destructive operations on shared heaps by parallel agents. 4 Application and Performance Evaluation This section describes two different types of HPSG parsers implemented on PSTFS. One is designed for our Japanese grammar and the algorithm is a parallel version of the CKY algorithm (Kasami, 1965). The other is a parser for an ALE-style Grammar (Carpenter and Penn, 1994). The algorithms of both parsers are based on parallel parsing algorithms for CFG (Ninomiya et al., 1997; Nijholt, 1994; Grishman and Chitrao, 1988; Thompson, 1994). Descriptions of both parsers are concise. Both of them are written in less than 1,000 lines. This shows that our PSTFS can be easily used. With the high performance of the parsers, this shows the feasibility and flexibility of our PSTFS. For simplicity of discussion, we assume that HPSG consists of lexical entries and rule schemata. Lexical entries can be regarded as 11 Ss assigned to each word. A rule schema is a rule in the form of z — abc • • • where z. a. b. c are TFSs. 4.1 Parallel CKY-style HPSG Parsing Algorithm A sequential CKY parser for CFG uses a data structure ca</context>
</contexts>
<marker>Grishman, Chitrao, 1988</marker>
<rawString>Ralph Grishman and Mehesh Chitrao. 1988. Evaluation of a parallel chart parser. In Proceedings of the second Conference on Applied Natural Language Processing, pages 71-76. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An efficient recognition and syntax algorithm for context-free languages.</title>
<date>1965</date>
<tech>Technical Report AFCRL-65-758,</tech>
<institution>Air Force Cambrige Research Lab.,</institution>
<location>Bedford, Mass.</location>
<contexts>
<context position="13895" citStr="Kasami, 1965" startWordPosition="2351" endWordPosition="2352"> heap. TFSs stored in such a way can be copied without any side-effect. edges are kept untouched, and destructive operations on the results must be done after copying them. The copying of TFSs in the above steps realizes such mechanisms in a natural way, as it is designed for efficient support for data sharing and destructive operations on shared heaps by parallel agents. 4 Application and Performance Evaluation This section describes two different types of HPSG parsers implemented on PSTFS. One is designed for our Japanese grammar and the algorithm is a parallel version of the CKY algorithm (Kasami, 1965). The other is a parser for an ALE-style Grammar (Carpenter and Penn, 1994). The algorithms of both parsers are based on parallel parsing algorithms for CFG (Ninomiya et al., 1997; Nijholt, 1994; Grishman and Chitrao, 1988; Thompson, 1994). Descriptions of both parsers are concise. Both of them are written in less than 1,000 lines. This shows that our PSTFS can be easily used. With the high performance of the parsers, this shows the feasibility and flexibility of our PSTFS. For simplicity of discussion, we assume that HPSG consists of lexical entries and rule schemata. Lexical entries can be r</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>T. Kasami. 1965. An efficient recognition and syntax algorithm for context-free languages. Technical Report AFCRL-65-758, Air Force Cambrige Research Lab., Bedford, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaki Makino</author>
<author>Minoru Yoshida</author>
<author>Kentaro Torisawa</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>LiLFeS — towards a practical HPSG parser.</title>
<date>1998</date>
<booktitle>In COLING-ACL&apos;98 Proceedings,</booktitle>
<contexts>
<context position="2699" citStr="Makino et al., 1998" startWordPosition="402" endWordPosition="405">ly founded by the project of JSPS(JSPS-RFTF96P00502). Order Figure 1: Agent-based System with the PSTFS ing or semantic processing, are divided into several pieces which can be simultaneously computed by several agents. Several parallel NLP systems have been developed previously. But most of them have been neither efficient nor practical enough (Adriaens and Hahn, 1994). On the other hand, our PSTFS provides the following features. • An efficient communication scheme for messages including Typed Feature Structures (TFSs) (Carpenter, 1992). • Efficient treatment of TFSs by an abstract machine (Makino et al., 1998). Another possible way to develop parallel NLP systems with TFSs is to use a full concurrent logic programming language (Clark and Gregory, 1986; Ueda, 1985). However, we have observed that it is necessary to control parallelism in a flexible way to achieve high-performance. (Fixed concurrency in a logic programming language does not provide sufficient flexibility.) Our agent-based architecture is suitable for accomplishing such flexibility in parallelism. The next section discusses PSTFS from a programmers&apos; point of view. Section 3 describes the PSTFS architecture in detail. Section 4 describ</context>
<context position="4842" citStr="Makino et al., 1998" startWordPosition="742" endWordPosition="745">gramming. Systems to be constructed on our PSTFS will include two different types of agents: • Control Agents (CAs) • Constraint Solver Agents (CSAs) As illustrated in Figure 1, CAs have overall control of a system, including control of parallelism, and they behave as masters of CSAs. CSAs modify TFSs according to the orders from CAs. Note that CAs can neither modify nor generate TFSs by themselves. PSTFS has been implemented by combining two existing programming languages: the concurrent object-oriented programming language ABCL/f (Taura, 1997) and the sequential programming language LiLFeS (Makino et al., 1998). CAs can be written in ABCL/f, while description of CSAs can be mainly written in LiLFeS. Figure 2 shows an example of a part of the PSTFS code. The task of this code is to concatenate the first and the second name in a given list. One of the CAs is called nameconcatenator. This specific CA gathers pairs of the first and last name by asking a CSA with the message solve-constraint(&apos;name(?)&apos;). When the CSA receives this message, the argument &apos;name(?)&apos; is treated as a Prolog query in LiLFeS1, according to the program of a CSA ((A) of Figure 2). There are several facts with the predicate &apos;name&apos;. </context>
<context position="9680" citStr="Makino et al., 1998" startWordPosition="1591" endWordPosition="1594">modify a TFS with the ID, the actual transfer of the TFS&apos;s real image occurs. Since the transfer is carried out only between CSAs, it can be directly performed using a low level representation of TFSs used in CSAs in an efficient manner. Note that if CAs were to modify TFSs directly, this scheme could not have been used. 3 Architecture This section explains the inner structure of PSTFS focusing on the execution mechanism of CSAs (See (Taura, 1997) for further detail on CAs). A CSA is implemented by modifying the abstract machine for TFSs (i.e., LiAM), originally designed for executing LiLFeS (Makino et al., 1998). The important constraint in designing the execution mechanism for CSAs is that TFSs generated by CSAs must be kept unmodified. This is because the TFSs must be used with several agents in parallel. If the TFS had been modified by a CSA and if other agents did not know the fact, the expected results could not have been obtained. Note that unification, which is (0 Copying from shared heap (ii) Computation on local heap (iii) Write resulting TFSs to shared heap Res • TFS Figure 4: Operation steps on PSTFS a major operation on TFSs, is a destructive operation, and modifications are likely to occ</context>
</contexts>
<marker>Makino, Yoshida, Torisawa, Tsujii, 1998</marker>
<rawString>Takaki Makino, Minoru Yoshida, Kentaro Torisawa, and Jun&apos;ichi Tsujii. 1998. LiLFeS — towards a practical HPSG parser. In COLING-ACL&apos;98 Proceedings, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
</authors>
<title>A parallel parsing system for natural language analysis.</title>
<date>1987</date>
<booktitle>In Proceedings of 3rd International Conference on Logic Programming,</booktitle>
<pages>396--409</pages>
<contexts>
<context position="18309" citStr="Matsumoto, 1987" startWordPosition="3139" endWordPosition="3140">991 &apos; 10 248 20 138 30 106 40 93 50 85 60 135 Table 1: Average parsing time per sentence Figure 6: Speed-up of parsing time on parallel CKY-style HPSG parser sentence6. 4.2 Chart-based Parallel HPSG Parsing Algorithm for ALE Grammar Next, we developed a parallel chart-based HPSG parser for an ALE-style grammar. The algorithm is based on a chart schema on which each agent throws active edges and inactive edges containing a TFS. When we regard the rule schemata as a set of rewriting rules in CFG, this algorithm is exactly the same as the Thompson&apos;s algorithm (Thompson, 1994) and similar to FAX (Matsumoto, 1987). The main difference between the chart-based parser and our CKY-style parser is that the ALE-style parser supports a n-branching tree. A parsing process is started by a CA called PARSER- It .creates word-position agents Pk(0 &lt; k &lt; n), distributes them to parallel processors and waits for them to complete their tasks. The role of the word-position agent Pk &apos;Using 60 processors is worse than with 50 processors. In general, when the number of processes increases to near or more than the number of existing processors, context switch between processes occurs frequently on shared-memory parallel ma</context>
</contexts>
<marker>Matsumoto, 1987</marker>
<rawString>Yuji Matsumoto. 1987. A parallel parsing system for natural language analysis. In Proceedings of 3rd International Conference on Logic Programming, pages 396-409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yutaka Mitsuishi</author>
<author>Kentaro Torisavva</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>HPSG-style underspecified Japanese grammar with wide coverage.</title>
<date>1998</date>
<booktitle>In COL ING-ACP98 Proceedings,</booktitle>
<contexts>
<context position="16719" citStr="Mitsuishi et al., 1998" startWordPosition="2865" endWordPosition="2869">in several C&apos;SAs3. Finally, when computation of (using Ft.k and Fk,j for all k(i &lt; k &lt; j)) is completed, distributes Fio to other agents waiting for F. Parsing is completed when the computation of F0,„ is completed. We have done a series of experiments on a shared-memory parallel machine, SUN Ultra Enterprise 10000 consisting of 64 nodes (each node is a 250 MHz UltraSparc) and 6 GByte shared memory. The corpus consists of 879 random sentences from the EDR Japanese corpus written in Japanese (average length of sentences is 20.8)4. The grammar we used is an underspecified Japanese HPSG grammar (Mitsuishi et al., 1998) consisting of 6 ID-schemata and 39 lexical entries (assigned to functional words) and 41 lexical-entry-templates (assigned to parts of speech). This grammar has wide coverage and high accuracy for real-world texts&apos;. Table 1 shows the result and comparison with a parser written in LiLFeS. Figure 6 shows its speed-up. From the Figure 6, we observe that the maximum speedup reaches up to 12.4 times. The average parsing time is 85 msec per 3CSAs cannot be added dynamically in our implementation. So, to gain the maximum parallelism, we assigned a CSA to each processor. Each C, asks the CSA on the s</context>
</contexts>
<marker>Mitsuishi, Torisavva, Tsujii, 1998</marker>
<rawString>Yutaka Mitsuishi, Kentaro Torisavva, and Jun&apos;ichi Tsujii. 1998. HPSG-style underspecified Japanese grammar with wide coverage. In COL ING-ACP98 Proceedings, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Nijholt</author>
</authors>
<title>Parallel Natural Language Processing, chapter Parallel Approaches to Context-Free Language Parsing,</title>
<date>1994</date>
<pages>135--167</pages>
<publisher>Ablex Publishing Corporation.</publisher>
<contexts>
<context position="14089" citStr="Nijholt, 1994" startWordPosition="2384" endWordPosition="2385">n the above steps realizes such mechanisms in a natural way, as it is designed for efficient support for data sharing and destructive operations on shared heaps by parallel agents. 4 Application and Performance Evaluation This section describes two different types of HPSG parsers implemented on PSTFS. One is designed for our Japanese grammar and the algorithm is a parallel version of the CKY algorithm (Kasami, 1965). The other is a parser for an ALE-style Grammar (Carpenter and Penn, 1994). The algorithms of both parsers are based on parallel parsing algorithms for CFG (Ninomiya et al., 1997; Nijholt, 1994; Grishman and Chitrao, 1988; Thompson, 1994). Descriptions of both parsers are concise. Both of them are written in less than 1,000 lines. This shows that our PSTFS can be easily used. With the high performance of the parsers, this shows the feasibility and flexibility of our PSTFS. For simplicity of discussion, we assume that HPSG consists of lexical entries and rule schemata. Lexical entries can be regarded as 11 Ss assigned to each word. A rule schema is a rule in the form of z — abc • • • where z. a. b. c are TFSs. 4.1 Parallel CKY-style HPSG Parsing Algorithm A sequential CKY parser for </context>
</contexts>
<marker>Nijholt, 1994</marker>
<rawString>Anton Nijholt, 1994. Parallel Natural Language Processing, chapter Parallel Approaches to Context-Free Language Parsing, pages 135-167. Ablex Publishing Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Kentaro Torisawa</author>
<author>Kenjiro Taura</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>A parallel cky parsing algorithm on large-scale distributed-memory parallel machines.</title>
<date>1997</date>
<booktitle>In PACLING &apos;97,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="14074" citStr="Ninomiya et al., 1997" startWordPosition="2379" endWordPosition="2383">. The copying of TFSs in the above steps realizes such mechanisms in a natural way, as it is designed for efficient support for data sharing and destructive operations on shared heaps by parallel agents. 4 Application and Performance Evaluation This section describes two different types of HPSG parsers implemented on PSTFS. One is designed for our Japanese grammar and the algorithm is a parallel version of the CKY algorithm (Kasami, 1965). The other is a parser for an ALE-style Grammar (Carpenter and Penn, 1994). The algorithms of both parsers are based on parallel parsing algorithms for CFG (Ninomiya et al., 1997; Nijholt, 1994; Grishman and Chitrao, 1988; Thompson, 1994). Descriptions of both parsers are concise. Both of them are written in less than 1,000 lines. This shows that our PSTFS can be easily used. With the high performance of the parsers, this shows the feasibility and flexibility of our PSTFS. For simplicity of discussion, we assume that HPSG consists of lexical entries and rule schemata. Lexical entries can be regarded as 11 Ss assigned to each word. A rule schema is a rule in the form of z — abc • • • where z. a. b. c are TFSs. 4.1 Parallel CKY-style HPSG Parsing Algorithm A sequential </context>
</contexts>
<marker>Ninomiya, Torisawa, Taura, Tsujii, 1997</marker>
<rawString>Takashi Ninomiya, Kentaro Torisawa, Kenjiro Taura, and Jun&apos;ichi Tsujii. 1997. A parallel cky parsing algorithm on large-scale distributed-memory parallel machines. In PACLING &apos;97, pages 223-231, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenjiro Taura</author>
</authors>
<title>Efficient and Reusable Implementation of Fine-Grain Multithreading and Garbage Collection on DistributedMemory Parallel Computers.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Information Sciencethe, University of Tokyo.</institution>
<contexts>
<context position="4773" citStr="Taura, 1997" startWordPosition="734" endWordPosition="735">eful design for accomplishing highperformance and ease of programming. Systems to be constructed on our PSTFS will include two different types of agents: • Control Agents (CAs) • Constraint Solver Agents (CSAs) As illustrated in Figure 1, CAs have overall control of a system, including control of parallelism, and they behave as masters of CSAs. CSAs modify TFSs according to the orders from CAs. Note that CAs can neither modify nor generate TFSs by themselves. PSTFS has been implemented by combining two existing programming languages: the concurrent object-oriented programming language ABCL/f (Taura, 1997) and the sequential programming language LiLFeS (Makino et al., 1998). CAs can be written in ABCL/f, while description of CSAs can be mainly written in LiLFeS. Figure 2 shows an example of a part of the PSTFS code. The task of this code is to concatenate the first and the second name in a given list. One of the CAs is called nameconcatenator. This specific CA gathers pairs of the first and last name by asking a CSA with the message solve-constraint(&apos;name(?)&apos;). When the CSA receives this message, the argument &apos;name(?)&apos; is treated as a Prolog query in LiLFeS1, according to the program of a CSA (</context>
<context position="9511" citStr="Taura, 1997" startWordPosition="1565" endWordPosition="1566"> image of TFSs. When CSAs return the results of computations to CAs, the CSAs send only an ID of a TFS. Only when the ID is passed to other CSAs and they try to modify a TFS with the ID, the actual transfer of the TFS&apos;s real image occurs. Since the transfer is carried out only between CSAs, it can be directly performed using a low level representation of TFSs used in CSAs in an efficient manner. Note that if CAs were to modify TFSs directly, this scheme could not have been used. 3 Architecture This section explains the inner structure of PSTFS focusing on the execution mechanism of CSAs (See (Taura, 1997) for further detail on CAs). A CSA is implemented by modifying the abstract machine for TFSs (i.e., LiAM), originally designed for executing LiLFeS (Makino et al., 1998). The important constraint in designing the execution mechanism for CSAs is that TFSs generated by CSAs must be kept unmodified. This is because the TFSs must be used with several agents in parallel. If the TFS had been modified by a CSA and if other agents did not know the fact, the expected results could not have been obtained. Note that unification, which is (0 Copying from shared heap (ii) Computation on local heap (iii) Wr</context>
</contexts>
<marker>Taura, 1997</marker>
<rawString>Kenjiro Taura. 1997. Efficient and Reusable Implementation of Fine-Grain Multithreading and Garbage Collection on DistributedMemory Parallel Computers. Ph.D. thesis, Department of Information Sciencethe, University of Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry S Thompson</author>
</authors>
<title>Parallel Natural Language Processing, chapter Parallel Parsers for Context-Free Grammars–Two Actual Implementations Comparesd,</title>
<date>1994</date>
<pages>168--187</pages>
<publisher>Ablex Publishing Corporation.</publisher>
<contexts>
<context position="14134" citStr="Thompson, 1994" startWordPosition="2390" endWordPosition="2391">in a natural way, as it is designed for efficient support for data sharing and destructive operations on shared heaps by parallel agents. 4 Application and Performance Evaluation This section describes two different types of HPSG parsers implemented on PSTFS. One is designed for our Japanese grammar and the algorithm is a parallel version of the CKY algorithm (Kasami, 1965). The other is a parser for an ALE-style Grammar (Carpenter and Penn, 1994). The algorithms of both parsers are based on parallel parsing algorithms for CFG (Ninomiya et al., 1997; Nijholt, 1994; Grishman and Chitrao, 1988; Thompson, 1994). Descriptions of both parsers are concise. Both of them are written in less than 1,000 lines. This shows that our PSTFS can be easily used. With the high performance of the parsers, this shows the feasibility and flexibility of our PSTFS. For simplicity of discussion, we assume that HPSG consists of lexical entries and rule schemata. Lexical entries can be regarded as 11 Ss assigned to each word. A rule schema is a rule in the form of z — abc • • • where z. a. b. c are TFSs. 4.1 Parallel CKY-style HPSG Parsing Algorithm A sequential CKY parser for CFG uses a data structure called a triangular</context>
<context position="18272" citStr="Thompson, 1994" startWordPosition="3133" endWordPosition="3134">sec) Processors PSTFS laLFeS 1 1057 991 &apos; 10 248 20 138 30 106 40 93 50 85 60 135 Table 1: Average parsing time per sentence Figure 6: Speed-up of parsing time on parallel CKY-style HPSG parser sentence6. 4.2 Chart-based Parallel HPSG Parsing Algorithm for ALE Grammar Next, we developed a parallel chart-based HPSG parser for an ALE-style grammar. The algorithm is based on a chart schema on which each agent throws active edges and inactive edges containing a TFS. When we regard the rule schemata as a set of rewriting rules in CFG, this algorithm is exactly the same as the Thompson&apos;s algorithm (Thompson, 1994) and similar to FAX (Matsumoto, 1987). The main difference between the chart-based parser and our CKY-style parser is that the ALE-style parser supports a n-branching tree. A parsing process is started by a CA called PARSER- It .creates word-position agents Pk(0 &lt; k &lt; n), distributes them to parallel processors and waits for them to complete their tasks. The role of the word-position agent Pk &apos;Using 60 processors is worse than with 50 processors. In general, when the number of processes increases to near or more than the number of existing processors, context switch between processes occurs fr</context>
</contexts>
<marker>Thompson, 1994</marker>
<rawString>Henry S. Thompson, 1994. Parallel Natural Language Processing, chapter Parallel Parsers for Context-Free Grammars–Two Actual Implementations Comparesd, pages 168-187. Ablex Publishing Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Ueda</author>
</authors>
<title>Guarded horn clauses.</title>
<date>1985</date>
<tech>Technical Report TR-103, ICOT.</tech>
<contexts>
<context position="2856" citStr="Ueda, 1985" startWordPosition="430" endWordPosition="431">ich can be simultaneously computed by several agents. Several parallel NLP systems have been developed previously. But most of them have been neither efficient nor practical enough (Adriaens and Hahn, 1994). On the other hand, our PSTFS provides the following features. • An efficient communication scheme for messages including Typed Feature Structures (TFSs) (Carpenter, 1992). • Efficient treatment of TFSs by an abstract machine (Makino et al., 1998). Another possible way to develop parallel NLP systems with TFSs is to use a full concurrent logic programming language (Clark and Gregory, 1986; Ueda, 1985). However, we have observed that it is necessary to control parallelism in a flexible way to achieve high-performance. (Fixed concurrency in a logic programming language does not provide sufficient flexibility.) Our agent-based architecture is suitable for accomplishing such flexibility in parallelism. The next section discusses PSTFS from a programmers&apos; point of view. Section 3 describes the PSTFS architecture in detail. Section 4 describes the performance of PSTFS on our HPSG parsers. 968 Franz 1 Schubert Johann 1 Bach J (Franz, Schubert) Fran: Schubert (Johann, Bach) Johann Bach F = { [ IRS</context>
</contexts>
<marker>Ueda, 1985</marker>
<rawString>Kazunori Ueda. 1985. Guarded horn clauses. Technical Report TR-103, ICOT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>