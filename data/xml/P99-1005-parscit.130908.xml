<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000111">
<title confidence="0.9941625">
Distributional Similarity Models: Clustering vs. Nearest
Neighbors
</title>
<author confidence="0.997336">
Lillian Lee
</author>
<affiliation confidence="0.996864">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.753725">
Ithaca, NY 14853-7501
</address>
<email confidence="0.997527">
llee@cs.cornell.edu
</email>
<note confidence="0.773672">
Fernando Pereira
A247, AT&amp;T Labs — Research
180 Park Avenue
</note>
<address confidence="0.853997">
Florham Park, NJ 07932-0971
</address>
<email confidence="0.996431">
pereira@research.att.com
</email>
<sectionHeader confidence="0.994232" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999886111111111">
Distributional similarity is a useful notion in es-
timating the probabilities of rare joint events.
It has been employed both to cluster events ac-
cording to their distributions, and to directly
compute averages of estimates for distributional
neighbors of a target event. Here, we examine
the tradeoffs between model size and prediction
accuracy for cluster-based and nearest neigh-
bors distributional models of unseen events.
</bodyText>
<sectionHeader confidence="0.998427" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999674476923077">
In many statistical language-processing prob-
lems, it is necessary to estimate the joint proba-
bility or cooccurrence probability of events drawn
from two prescribed sets. Data sparseness can
make such estimates difficult when the events
under consideration are sufficiently fine-grained,
for instance, when they correspond to occur-
rences of specific words in given configurations.
In particular, in many practical modeling tasks,
a substantial fraction of the cooccurrences of in-
terest have never been seen in training data. In
most previous work (Jelinek and Mercer, 1980;
Katz, 1987; Church and Gale, 1991; Ney and
Essen, 1993), this lack of information is ad-
dressed by reserving some mass in the proba-
bility model for unseen joint events, and then
assigning that mass to those events as a func-
tion of their marginal frequencies.
An intuitively appealing alternative to relying
on marginal frequencies alone is to combine es-
timates of the probabilities of &amp;quot;similar&amp;quot; events.
More specifically, a joint event (x, y) would be
considered similar to another (x&apos;, y) if the distri-
butions of Y given x and Y given x&apos; (the cooc-
currence distributions of x and x&apos;) meet an ap-
propriate definition of distributional similarity.
For example, one can infer that the bigram &amp;quot;af-
ter ACL-99&amp;quot; is plausible — even if it has never
occurred before — from the fact that the bigram
&amp;quot;after ACL-95&amp;quot; has occurred, if &amp;quot;ACL-99&amp;quot; and
&amp;quot;ACL-95&amp;quot; have similar cooccurrence distribu-
tions.
For concreteness and experimental evalua-
tion, we focus in this paper on a particular type
of cooccurrence, that of a main verb and the
head noun of its direct object in English text.
Our main goal is to obtain estimates 13(v In) of
the conditional probability of a main verb v
given a direct object head noun n, which can
then be used in particular prediction tasks.
In previous work, we and our co-authors have
proposed two different probability estimation
methods that incorporate word similarity infor-
mation: distributional clustering and nearest-
neighbors averaging. Distributional clustering
(Pereira et al., 1993) assigns to each word a
probability distribution over clusters to which
it may belong, and characterizes each cluster
by a centroid, which is an average of cooccur-
rence distributions of words weighted according
to cluster membership probabilities. Cooccur-
rence probabilities can then be derived from ei-
ther a membership-weighted average of the clus-
ters to which the words in the cooccurrence be-
long, or just from the highest-probability clus-
ter.
In contrast, nearest-neighbors averaging&apos;
(Dagan et al., 1999) does not explicitly clus-
ter words. Rather, a given cooccurrence prob-
ability is estimated by averaging probabilities
for the set of cooccurrences most similar to the
target cooccurrence. That is, while both meth-
ods involve appealing to similar &amp;quot;witnesses&amp;quot; (in
the clustering case, these witnesses are the cen-
troids; for nearest-neighbors averaging, they are
</bodyText>
<footnote confidence="0.748821333333333">
&apos;In previous papers, we have used the term
&amp;quot;similarity-based&amp;quot;, but this term would cause confusion
in the present article.
</footnote>
<page confidence="0.998577">
33
</page>
<bodyText confidence="0.999908394366198">
the most similar words), in nearest-neighbors
averaging the witnesses vary for different cooc-
currences, whereas in distributional clustering
the same set of witnesses is used for every cooc-
currence (see Figure 1).
We thus see that distributional clustering and
nearest-neighbors averaging are complementary
approaches. Distributional clustering gener-
ally creates a compact representation of the
data, namely, the cluster membership probabil-
ity tables and the cluster centroids. Nearest-
neighbors averaging, on the other hand, asso-
ciates a specific set of similar words to each word
and thus typically increases the amount of stor-
age required. In a way, it is clustering taken to
the limit - each word forms its own cluster.
In previous work, we have shown that both
distributional clustering and nearest-neighbors
averaging can yield improvements of up to 40%
with respect to Katz&apos;s (1987) state-of-the-art
backoff method in the prediction of unseen cooc-
currences. In the case of nearest-neighbors aver-
aging, we have also demonstrated perplexity re-
ductions of 20% and statistically significant im-
provement in speech recognition error rate. Fur-
thermore, each method has generated some dis-
cussion in the literature (Hofmann et al., 1999;
Baker and McCallum, 1998; Ide and Veronis,
1998). Given the relative success of these meth-
ods and their complementarity, it is natural to
wonder how they compare in practice.
Several authors (Schiitze, 1993; Dagan et al.,
1995; Ide and Veronis, 1998) have suggested
that clustering methods, by reducing data to
a small set of representatives, might perform
less well than nearest-neighbors averaging-type
methods. For instance, Dagan et al. (1995,
p. 124) argue:
This [class-based] approach, which fol-
lows long traditions in semantic clas-
sification, is very appealing, as it at-
tempts to capture &amp;quot;typical&amp;quot; properties
of classes of words. However .... it is
not clear that word co-occurrence pat-
terns can be generalized to class co-
occurrence parameters without losing
too much information.
Furthermore, early work on class-based lan-
guage models was inconclusive (Brown et al.,
1992).
In this paper, we present a detailed com-
parison of distributional clustering and nearest-
neighbors averaging on several large datasets,
exploring the tradeoff in similarity-based mod-
eling between memory usage on the one hand
and estimation accuracy on the other. We find
that the performances of the two methods are
in general very similar: with respect to Katz&apos;s
back-off, they both provide average error reduc-
tions of up to 40% on one task and up to 7%
on a related, but somewhat more difficult, task.
Only in a fairly unrealistic setting did nearest-
neighbors averaging clearly beat distributional
clustering, but even in this case, both meth-
ods were able to achieve average error reduc-
tions of at least 18% in comparison to back-
off. Therefore, previous claims that clustering
methods are necessarily inferior are not strongly
supported by the evidence of these experiments,
although it is of course possible that the situa-
tion may be different for other tasks.
</bodyText>
<sectionHeader confidence="0.939129" genericHeader="method">
2 Two models
</sectionHeader>
<bodyText confidence="0.99995175">
We now survey the distributional clustering
(section 2.1) and nearest-neighbors averaging
(section 2.2) models. Section 2.3 examines the
relationships between these two methods.
</bodyText>
<subsectionHeader confidence="0.989351">
2.1 Clustering
</subsectionHeader>
<bodyText confidence="0.99992980952381">
The distributional clustering model that we
evaluate in this paper is a refinement of our ear-
lier model (Pereira et al., 1993). The new model
has important theoretical advantages over the
earlier one and interesting mathematical prop-
erties, which will be discussed elsewhere. Here,
we will outline the main motivation for the
model, the iterative equations that implement
it, and their practical use in clustering.
The model involves two discrete random vari-
ables N (nouns) and V (verbs) whose joint dis-
tribution we have sampled, and a new unob-
served discrete random variable C representing
probabilistic clusters of elements of N. The
role of the hidden variable C is specified by
the conditional distribution p(cln), which can
be thought of as the probability that n belongs
to cluster c. We want to preserve in C as much
as possible of the information that N has about
V, that is, maximize the mutual information2
/(V, C). On the other hand, we would also
</bodyText>
<page confidence="0.759091">
2 I (X ,Y) = E. Ey P(x, y) log (P(x,y)1P(x)P(y)).
34
</page>
<figure confidence="0.998931416666667">
.o•
••S.
••
S.
S.
/0 0 0 0
t. OA OB
0 0
•
•
0
•••■ _
</figure>
<figureCaption confidence="0.857086666666667">
Figure 1: Difference between clustering and nearest neighbors. Although A and B belong mostly to
the same cluster (dotted ellipse), the two nearest neighbors to A are not the nearest two neighbors
to B.
</figureCaption>
<bodyText confidence="0.99848425">
like to control the degree of compression of C
relative to N, that is, the mutual information
/(C, N). Furthermore, since C is intended to
summarize N in its role as a predictor of V, it
should carry no information about V that N
does not already have. That is, V should be
conditionally independent of C given N, which
allows us to write
</bodyText>
<equation confidence="0.981713">
p(vIc)=EP(vIn)P(nle) • (1)
</equation>
<bodyText confidence="0.9935572">
The distribution p(VI c) is the centroid for clus-
ter c.
It can be shown that /(V, C) is maximized
subject to fixed /(C, N) and the above condi-
tional independence assumption when
</bodyText>
<equation confidence="0.987946">
(
p(c1n) = pc) zn exp [--.3D(p(Vin) I Ip(V1c))] , (2)
</equation>
<bodyText confidence="0.999809">
where 3 is the Lagrange multiplier associated
with fixed /(C, N), Zn is the normalization
</bodyText>
<equation confidence="0.840567">
Zn = E p(c) exp [-3D(p( V In) I jp(VI ,
</equation>
<bodyText confidence="0.994532">
and D is the Kullback-Leiber (KL) divergence,
which measures the distance, in an information-
theoretic sense, between two distributions q and
r:
</bodyText>
<equation confidence="0.96958">
D(q11r) =E q(v) log
</equation>
<bodyText confidence="0.989645630434783">
r(v).
The main behavioral difference between this
model and our previous one is the p(c) factor in
(2), which tends to sharpen cluster membership
distributions. In addition, our earlier experi-
ments used a uniform marginal distribution for
the nouns instead of the marginal distribution
in the actual data, in order to make clustering
more sensitive to informative but relatively rare
nouns. While neither difference leads to major
changes in clustering results, we prefer the cur-
rent model for its better theoretical foundation.
For fixed 3, equations (2) and (1) together
with Bayes rule and marginalization can be used
in a provably convergent iterative reestimation
process for p(NIC), p(VC) and p(C). These
distributions form the model for the given 3.
It is easy to see that for 3 = 0, p(nl c) does not
depend on the cluster distribution p(Vic), so the
natural number of clusters (distinct values of
C) is one. At the other extreme, for very large
3 the natural number of clusters is the same
as the number of nouns. In general, a higher
value of 3 corresponds to a larger number of
clusters. The natural number of clusters k and
the probabilistic model for different values of 13
are estimated as follows. We specify an increas-
ing sequence {A} of 13 values (the &amp;quot;annealing&amp;quot;
schedule), starting with a very low value /30 and
increasing slowly (in our experiments, /30 = 1
and 31 = 1.13i). Assuming that the natural
number of clusters and model for 3i have been
computed, we set = 3i+1 and split each clus-
ter into two twins by taking small random per-
turbations of the original cluster centroids. We
then apply the iterative reestimation procedure
until convergence. If two twins end up with sig-
nificantly different centroids, we conclude that
they are now separate clusters. Thus, for each
i we have a number of clusters lc, and a model
relating those clusters to the data variables N
and V.
A cluster model can be used to estimate
p(v In) when v and n have not occurred together
in training. We consider two heuristic ways of
doing this estimation:
</bodyText>
<page confidence="0.989961">
35
</page>
<listItem confidence="0.989829">
• all-cluster weighted average:
/5(vIn) = EP(v14(cin)
• nearest-cluster estimate:
</listItem>
<equation confidence="0.96412">
/5(vin) = p(vIc*) ,
</equation>
<bodyText confidence="0.997204">
where c* maximizes p(c* in) .
</bodyText>
<subsectionHeader confidence="0.996899">
2.2 Nearest-neighbors averaging
</subsectionHeader>
<bodyText confidence="0.999896857142857">
As noted earlier, the nearest-neighbors averag-
ing method is an alternative to clustering for
estimating the probabilities of unseen cooccur-
rences. Given an unseen pair (n, v), we calcu-
late an estimate 23(v in) as an appropriate aver-
age of p(vIn&apos;) where n&apos; is distributionally sim-
ilar to n. Many distributional similarity mea-
sures can be considered (Lee, 1999). In this
paper, we focus on the one that gave the best
results in our earlier work (Dagan et al., 1999),
the Jensen-Shannon divergence (Rao, 1982; Lin,
1991). The Jensen-Shannon divergence of two
discrete distributions p and q over the same do-
main is defined as
</bodyText>
<equation confidence="0.992404">
J S q) =1 [D P q) D P q)] .
2 2 2
</equation>
<bodyText confidence="0.998105">
It is easy to see that JS(p, q) is always defined.
In previous work, we used the estimate
</bodyText>
<equation confidence="0.8217745">
13(70) = —1 E p(vin&apos;) exp(-13J(n, n&apos;)) ,
an n&apos;ES(n,k)
</equation>
<bodyText confidence="0.998947166666667">
where J(n,n&apos;) = JS (p(Vin),p(Vin&apos;)), and
k are tunable parameters, S(n, k) is the set of
k nouns with the smallest Jensen-Shannon di-
vergence to 11, and an is a normalization term.
However, in the present work we use the simpler
unweighted average
</bodyText>
<equation confidence="0.435427">
n&apos;ES(n,k)
</equation>
<bodyText confidence="0.9999681">
and examine the effect of the choice of k on
modeling performance. By eliminating extra
parameters, this restricted formulation allows a
more direct comparison of nearest-neighbors av-
eraging to distributional clustering, as discussed
in the next section. Furthermore, our earlier
experiments showed that an exponentially de-
creasing weight has much the same effect on per-
formance as a bound on the number of nearest
neighbors participating in the estimate.
</bodyText>
<subsectionHeader confidence="0.979551">
2.3 Discussion
</subsectionHeader>
<bodyText confidence="0.99999494">
In the previous two sections, we presented
two complementary paradigms for incorporat-
ing distributional similarity information into
cooccurrence probability estimates. Now, one
cannot always draw conclusions about the rel-
ative fitness of two methods simply from head-
to-head performance comparisons; for instance,
one method might actually make use of inher-
ently more informative statistics but produce
worse results because the authors chose a sub-
optimal weighting scheme. In the present case,
however, we are working with two models which,
while representing opposite extremes in terms of
generalization, share enough features to make
the comparison meaningful.
First, both models use linear combinations
of cooccurrence probabilities for similar enti-
ties. Second, each has a single free param-
eter k, and the two k&apos;s enjoy a natural in-
verse correspondence: a large number of clus-
ters in the distributional clustering case results
in only the closest centroids contributing sig-
nificantly to the cooccurrence probability esti-
mate, whereas a large number of neighbors in
the nearest-neighbors averaging case means that
relatively distant words are consulted. And fi-
nally, the two distance functions are similar in
spirit: both are based on the KL divergence to
some type of averaged distribution. We have
thus attempted to eliminate functional form,
number and type of parameters, and choice of
distance function from playing a role in the com-
parison, increasing our confidence that we are
truly comparing paradigms and not implemen-
tation details.
What are the fundamental differences be-
tween the two methods? From the foregoing
discussion it is clear that distributional clus-
tering is theoretically more satisfying and de-
pends on a single model complexity parameter.
On the other hand, nearest-neighbors averaging
in its most general form offers more flexibility
in defining the set of most similar words and
their relative weights (Dagan et al., 1999). Also,
the training phase requires little computation,
as opposed to the iterative re-estimation proce-
dure employed to build the cluster model. But
the key difference is the amount of data com-
pression, or equivalently the amount of general-
ization, produced by the two models. Cluster-
</bodyText>
<equation confidence="0.986921">
1
25(vin) = –k E P(vIni) (3)
</equation>
<page confidence="0.977387">
36
</page>
<bodyText confidence="0.9998854">
ing yields a far more compact representation of
the data when k, the model size parameter, is
smaller than IN I. As noted above, various au-
thors have conjectured that this data reduction
must inevitably result in lower performance in
comparison to nearest-neighbor methods, which
store the most specific information for each in-
dividual word. Our experiments aim to ex-
plore this hypothesized generalization-accuracy
tradeoff.
</bodyText>
<sectionHeader confidence="0.998395" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.535649">
3.1 Methodology
</subsectionHeader>
<bodyText confidence="0.991098454545455">
We compared the two similarity-based esti-
mation techniques at the following decision
task, which evaluates their ability to choose
the more likely of two unseen cooccurrences.
Test instances consist of noun-verb-verb triples
(n, v1, v2), where both (n, vi) and (n, v2) are un-
seen cooccurrences, but (n, vi) is more likely
(how this is determined is discussed below). For
each test instance, the language model prob-
abilities p def
i_ 15(vi In) and /32 de f= 13(v2In) are
computed; the result of the test is either cor-
rect (Pi &gt; p2), incorrect (251 &lt; p2,) or a tie
= 132). Overall performance is measured by
the error rate on the entire test set, defined as
—1(# of incorrect choices + (# of ties)/2) ,
where T is the number of test triples, not count-
ing multiplicities.
Our global experimental design was to run
ten-fold cross-validation experiments comparing
distributional clustering, nearest-neighbors av-
eraging, and Katz&apos;s backoff (the baseline) on the
decision task just outlined. All results we report
below are averages over the ten train-test splits.
For each split, test triples were created from the
held-out test set. Each model used the training
set to calculate all basic quantities (e.g., p(v In)
for each verb and noun), but not to train k.
Then, the performance of each similarity-based
model was evaluated on the test triples for a
sequence of settings for k.
We expected that clustering performance
with respect to the baseline would initially im-
prove and then decline. That is, we conjec-
tured that the model would overgeneralize at
small k but overfit the training data at large
k. In contrast, for nearest-neighbors averag-
ing, we hypothesized monotonically decreasing
performance curves: using only the very most
similar words would yield high performance,
whereas including more distant, uninformative
words would result in lower accuracy. From pre-
vious experience, we believed that both meth-
ods would do well with respect to backoff.
</bodyText>
<subsectionHeader confidence="0.994447">
3.2 Data
</subsectionHeader>
<bodyText confidence="0.99952">
In order to implement the experimental
methodology just described, we employed the
follow data preparation method:
</bodyText>
<listItem confidence="0.984379666666667">
1. Gather verb-object pairs using the CASS
partial parser (Abney, 1996)
2. Partition set of pairs into ten folds
3. For each test fold,
(a) discard seen pairs and duplicates
(b) discard pairs with unseen nouns or un-
seen verbs
(c) for each remaining (n, v1), create
(n, v1, v2) such that (n, v2) is less likely
</listItem>
<bodyText confidence="0.998682944444444">
Step 3b is necessary because neither the
similarity-based methods nor backoff handle
novel unigrams gracefully.
We instantiated this schema in three ways:
AP89 We retrieved 1,577,582 verb-object
pairs from 1989 Associated Press (AP)
newswire, discarding singletons (pairs occurring
only once) as is commonly done in language
modeling. We split this set by type3, which
does not realistically model how new data oc-
curs in real life, but does conveniently guaran-
tee that the entire test set is unseen. In step
3c all (n, v2) were found such that (n, vi) oc-
curred at least twice as often as (n, v2) in the
test fold; this gives reasonable reassurance that
n is indeed more likely to cooccur with v1, even
though (n, v2) is plausible (since it did in fact
occur).
</bodyText>
<footnote confidence="0.964126666666667">
3When a corpus is split by type, all instances of a
given type must end up in the same partition. If the
split is by token, then instances of the same type may
end up in different partitions. For example, for corpus
&amp;quot;a b a c&amp;quot;, &amp;quot;a b&amp;quot;+&amp;quot;a c&amp;quot; is a valid split by token, but not
by type.
</footnote>
<page confidence="0.995562">
37
</page>
<table confidence="0.999249">
Test type split singletons? # training % of test # test baseline
pairs unseen triples error
AP89 type no 1033870 100 42795 28.3%
AP9Ounseen token yes 1123686 14 4019 39.6%
AP9Ofake 77 14479 79.9%
</table>
<tableCaption confidence="0.996843">
Table 1: Data for the three types of experiments. All numbers are averages over the ten splits.
</tableCaption>
<bodyText confidence="0.9997846875">
AP9Ounseen 1,483,728 pairs were extracted
from 1990 AP newswire and split by token. Al-
though splitting by token is undoubtedly a bet-
ter way to generate train-test splits than split-
ting by type, it had the unfortunate side effect
of diminishing the average percentage of unseen
cooccurrences in the test sets to 14%. While
this is still a substantial fraction of the data
(demonstrating the seriousness of the sparse
data problem), it caused difficulties in creat-
ing test triples: after applying filtering step 3b,
there were relatively few candidate nouns and
verbs satisfying the fairly stringent condition 3c.
Therefore, singletons were retained in the AP90
data. Step 3c was carried out as for AP89.
AP9Ofake The procedure for creating the
AP90unseen data resulted in much smaller test
sets than in the AP89 case (see Table 1). To
generate larger test sets, we used the same folds
as in AP90unseen, but implemented step 3c dif-
ferently. Instead of selecting v2 from cooccur-
rences (n, v2) in the held-out set, test triples
were constructed using v2 that never cooccurred
with n in either the training or the test data.
That is, each test triple represented a choice
between a plausible cooccurrence (n, v1) and an
implausible ( &amp;quot;fake&amp;quot;) cooccurrence (n, v2). To
ensure a large differential between the two al-
ternatives, we further restricted (n, v1) to occur
at least twice (in the test fold). We also chose v2
from the set of 50 most frequent verbs, resulting
in much higher error rates for backoff.
</bodyText>
<subsectionHeader confidence="0.892688">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.999922133333333">
We now present evaluation results ordered by
relative difficulty of the decision task.
Figure 2 shows the performance of distribu-
tional clustering and nearest-neighbors averag-
ing on the AP9Ofake data (in all plots, error bars
represent one standard deviation). Recall that
the task here was to distinguish between plau-
sible and implausible cooccurrences, making it
a somewhat easier problem than that posed in
the AP89 and AP90unseen experiments. Both
similarity-based methods improved on the base-
line error (which, by construction of the test
triples, was guaranteed to be high) by as much
as 40%. Also, the curves have the shapes pre-
dicted in section 3.1.
</bodyText>
<figure confidence="0.9716115">
SO
40
error rate reduction 20
10
</figure>
<figureCaption confidence="0.9876825">
Figure 2: Average error reduction with respect
to backoff on AP9Ofake test sets.
</figureCaption>
<bodyText confidence="0.999973055555556">
We next examine our AP89 experiment re-
sults, shown in Figure 3. The similarity-based
methods clearly outperform backoff, with the
best error reductions occurring at small k for
both types of models. Nearest-neighbors aver-
aging appears to have the advantage over dis-
tributional clustering, and the nearest cluster
method yields lower error rates than the aver-
aged cluster method (the differences are statisti-
cally significant according to the paired t-test).
We might hypothesize that nearest-neighbors
averaging is better in situations of extreme spar-
sity of data. However, these results must be
taken with some caution given their unrealistic
type-based train-test split.
A striking feature of Figure 3 is that all the
curves have the same shape, which is not at all
what we predicted in section 3.1. The reason
</bodyText>
<figure confidence="0.8616385">
all clusters .4-1
nearest cluster
nearest neighbors HEI-1
50 100 150 220 250 300 350 400
</figure>
<page confidence="0.889937">
38
</page>
<figureCaption confidence="0.9874645">
Figure 3: Average error reduction with respect
to backoff on AP89 test sets.
</figureCaption>
<bodyText confidence="0.927137882352941">
that the very most similar words are appar-
ently not as informative as slightly more dis-
tant words is due to recall errors. Observe that
if (n, vi) and (n, v2) are unseen in the train-
ing data, and if word n&apos; has very small Jensen-
Shannon divergence to n, then chances are that
711 also does not occur with either v1 or v2, re-
sulting in an estimate of zero probability for
both test cooccurrences. Figure 4 proves that
this is the case: if zero-ties are ignored, then the
error rate curve for nearest-neighbors averaging
has the expected shape. Of course, clustering is
not prone to this problem because it automati-
cally smoothes its probability estimates.
average error over AP89, normal vs. precision results
nearest neighbors
nearest neighbors. Ignoring recall errors
</bodyText>
<figure confidence="0.994485">
0
0.19
50 100 150 210,0 250 300 350 400
</figure>
<figureCaption confidence="0.982334333333333">
Figure 4: Average error (not error reduction)
using nearest-neighbors averaging on AP89,
showing the effect of ignoring recall mistakes.
</figureCaption>
<bodyText confidence="0.979282916666667">
Finally, Figure 5 presents the results of
our AP9Ounseen experiments. Again, the use
of similarity information provides better-than-
baseline performance, but, due to the relative
difficulty of the decision task in these exper-
iments (indicated by the higher baseline er-
ror rate with respect to AP89), the maximum
average improvements are in the 6-8% range.
The error rate reductions posted by weighted-
average clustering, nearest-centroid clustering,
and nearest-neighbors averaging are all well
within the standard deviations of each other.
</bodyText>
<figure confidence="0.429633">
50 200 250 300 350 400
</figure>
<figureCaption confidence="0.70744975">
Figure 5: Average error reduction with respect
to backoff on AP9Ounseen test sets. As in the
AP89 case, the nonmonotonicity of the nearest-
neighbors averaging curve is due to recall errors.
</figureCaption>
<sectionHeader confidence="0.998409" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999949578947369">
In our experiments, the performances of distri-
butional clustering and nearest-neighbors aver-
aging proved to be in general very similar: only
in the unorthodox AP89 setting did nearest-
neighbors averaging clearly yield better error
rates. Overall, both methods achieved peak per-
formances at relatively small values of k, which
is gratifying from a computational point of view.
Some questions remain. We observe that
distributional clustering seems to suffer higher
variance. It is not clear whether this is due
to poor estimates of the KL divergence to cen-
troids, and thus cluster membership, for rare
nouns, or to noise sensitivity in the search for
cluster splits. Also, weighted-average clustering
never seems to outperform the nearest-centroid
method, suggesting that the advantages of prob-
abilistic clustering over &amp;quot;hard&amp;quot; clustering may
be computational rather than in modeling ef-
</bodyText>
<figure confidence="0.997678961538462">
20
15
10
,tt
SO
100
150
200
250
300
350
400
25
all clusters 4+-4
nearest cluster
nearest neighbors 1.13-4
all clusters
nearest cluster •-•—■
nearest neighbors Hs--4
0.26
0.25
0.24
0.23
0.22
0.21
12
</figure>
<page confidence="0.997336">
39
</page>
<bodyText confidence="0.999737444444444">
fectiveness (Boolean clustering is NP-complete
(Brucker, 1978)). Last but not least, we do not
yet have a principled explanation for the similar
performance of nearest-neighbors averaging and
distributional clustering. Further experiments,
especially in other tasks such as language mod-
eling, might help tease apart the two methods
or better understand the reasons for their simi-
larity.
</bodyText>
<sectionHeader confidence="0.998331" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99951575">
We thank the anonymous reviewers for their
helpful comments and Steve Abney for help
with extracting verb-object pairs with his parser
CASS.
</bodyText>
<sectionHeader confidence="0.998461" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999836565789474">
Steven Abney. 1996. Partial parsing via finite-state
cascades. In Proceedings of the ESSLLI &apos;96 Ro-
bust Parsing Workshop.
L. Douglas Baker and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In 21st Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval (SIGIR &apos;98), pages 96-
103.
Peter F. Brown, Vincent J. DellaPietra, Peter V.
deSouza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467-479,
December.
Peter Brucker. 1978. On the complexity of clus-
tering problems. In Rudolf Henn, Bernhard H.
Korte, and Werner Oettli, editors, Optimization
and Operations Research, number 157 in Lecture
Notes in Economics and Mathematical Systems.
Springer-Verlag, Berlin.
Kenneth W. Church and William A. Gale. 1991.
A comparison of the enhanced Good-Turing and
deleted estimation methods for estimating proba-
bilities of English bigrams. Computer Speech and
Language, 5:19-54.
Ido Dagan, Shaul Marcus, and Shaul Markovitch.
1995. Contextual word similarity and estimation
from sparse data. Computer Speech and Lan-
guage, 9:123-152.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43-69.
Thomas Hofmann, Jan Puzicha, and Michael I. Jor-
dan. 1999. Learning from dyadic data. In Ad-
vances in Neural Information Processing Systems
11. MIT Press. To appear.
Nancy Ide and Jean Veronis. 1998. Introduction to
the special issue on word sense disambiguation:
The state of the art. Computational Linguistics,
24(1):1-40, March.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters
from sparse data. In Proceedings of the Workshop
on Pattern Recognition in Practice, Amsterdam,
May. North Holland.
Slava M. Katz. 1987. Estimation of probabilities
from sparse data for the language model com-
ponent of a speech recognizer. IEEE Transac-
tions on Acoustics, Speech and Signal Processing,
ASSP-35(3):400-401, March.
Lillian Lee. 1999. Measures of distributional simi-
larity. In 37th Annual Meeting of the ACL, Som-
erset, New Jersey. Distributed by Morgan Kauf-
mann, San Francisco.
Jianhua Lin. 1991. Divergence measures based on
the Shannon entropy. IEEE Transactions on In-
formation Theory, 37(1):145-151.
Hermann Ney and Ute Essen. 1993. Estimating
&apos;small&apos; probabilities by leaving-one-out. In Third
European Conference On Speech Communication
and Technology, pages 2239-2242, Berlin, Ger-
many.
Fernando C. N. Pereira, Naftali Tishby, and Lillian
Lee. 1993. Distributional clustering of English
words. In 31st Annual Meeting of the ACL, pages
183-190, Somerset, New Jersey. Association for
Computational Linguistics. Distributed by Mor-
gan Kaufmann, San Francisco.
C. Radhakrishna Rao. 1982. Diversity: Its measure-
ment, decomposition, apportionment and analy-
sis. Sankyha: The Indian Journal of Statistics,
44(A):1-22.
Hinrich Schiitze. 1993. Word space. In S. J. Hanson,
J. D. Cowan, and C. L. Giles, editors, Advances in
Neural Information Processing Systems 5, pages
895-902. Morgan Kaufmann, San Francisco.
</reference>
<page confidence="0.99863">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.526540">
<title confidence="0.9869185">Distributional Similarity Models: Clustering vs. Nearest Neighbors</title>
<author confidence="0.999968">Lillian Lee</author>
<affiliation confidence="0.999451">Department of Computer Science Cornell University</affiliation>
<address confidence="0.99975">Ithaca, NY 14853-7501</address>
<email confidence="0.999568">llee@cs.cornell.edu</email>
<author confidence="0.988349">Fernando Pereira</author>
<affiliation confidence="0.550602">A247, AT&amp;T Labs — Research</affiliation>
<address confidence="0.9946895">180 Park Avenue Florham Park, NJ 07932-0971</address>
<email confidence="0.999849">pereira@research.att.com</email>
<abstract confidence="0.9987242">Distributional similarity is a useful notion in estimating the probabilities of rare joint events. It has been employed both to cluster events according to their distributions, and to directly compute averages of estimates for distributional neighbors of a target event. Here, we examine the tradeoffs between model size and prediction accuracy for cluster-based and nearest neighbors distributional models of unseen events.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Partial parsing via finite-state cascades.</title>
<date>1996</date>
<booktitle>In Proceedings of the ESSLLI &apos;96 Robust Parsing Workshop.</booktitle>
<contexts>
<context position="17863" citStr="Abney, 1996" startWordPosition="2900" endWordPosition="2901">neralize at small k but overfit the training data at large k. In contrast, for nearest-neighbors averaging, we hypothesized monotonically decreasing performance curves: using only the very most similar words would yield high performance, whereas including more distant, uninformative words would result in lower accuracy. From previous experience, we believed that both methods would do well with respect to backoff. 3.2 Data In order to implement the experimental methodology just described, we employed the follow data preparation method: 1. Gather verb-object pairs using the CASS partial parser (Abney, 1996) 2. Partition set of pairs into ten folds 3. For each test fold, (a) discard seen pairs and duplicates (b) discard pairs with unseen nouns or unseen verbs (c) for each remaining (n, v1), create (n, v1, v2) such that (n, v2) is less likely Step 3b is necessary because neither the similarity-based methods nor backoff handle novel unigrams gracefully. We instantiated this schema in three ways: AP89 We retrieved 1,577,582 verb-object pairs from 1989 Associated Press (AP) newswire, discarding singletons (pairs occurring only once) as is commonly done in language modeling. We split this set by type3</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Partial parsing via finite-state cascades. In Proceedings of the ESSLLI &apos;96 Robust Parsing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Douglas Baker</author>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Distributional clustering of words for text classification.</title>
<date>1998</date>
<booktitle>In 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;98),</booktitle>
<pages>96--103</pages>
<contexts>
<context position="5056" citStr="Baker and McCallum, 1998" startWordPosition="776" endWordPosition="779">uired. In a way, it is clustering taken to the limit - each word forms its own cluster. In previous work, we have shown that both distributional clustering and nearest-neighbors averaging can yield improvements of up to 40% with respect to Katz&apos;s (1987) state-of-the-art backoff method in the prediction of unseen cooccurrences. In the case of nearest-neighbors averaging, we have also demonstrated perplexity reductions of 20% and statistically significant improvement in speech recognition error rate. Furthermore, each method has generated some discussion in the literature (Hofmann et al., 1999; Baker and McCallum, 1998; Ide and Veronis, 1998). Given the relative success of these methods and their complementarity, it is natural to wonder how they compare in practice. Several authors (Schiitze, 1993; Dagan et al., 1995; Ide and Veronis, 1998) have suggested that clustering methods, by reducing data to a small set of representatives, might perform less well than nearest-neighbors averaging-type methods. For instance, Dagan et al. (1995, p. 124) argue: This [class-based] approach, which follows long traditions in semantic classification, is very appealing, as it attempts to capture &amp;quot;typical&amp;quot; properties of class</context>
</contexts>
<marker>Baker, McCallum, 1998</marker>
<rawString>L. Douglas Baker and Andrew Kachites McCallum. 1998. Distributional clustering of words for text classification. In 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;98), pages 96-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J DellaPietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<contexts>
<context position="5912" citStr="Brown et al., 1992" startWordPosition="909" endWordPosition="912">sted that clustering methods, by reducing data to a small set of representatives, might perform less well than nearest-neighbors averaging-type methods. For instance, Dagan et al. (1995, p. 124) argue: This [class-based] approach, which follows long traditions in semantic classification, is very appealing, as it attempts to capture &amp;quot;typical&amp;quot; properties of classes of words. However .... it is not clear that word co-occurrence patterns can be generalized to class cooccurrence parameters without losing too much information. Furthermore, early work on class-based language models was inconclusive (Brown et al., 1992). In this paper, we present a detailed comparison of distributional clustering and nearestneighbors averaging on several large datasets, exploring the tradeoff in similarity-based modeling between memory usage on the one hand and estimation accuracy on the other. We find that the performances of the two methods are in general very similar: with respect to Katz&apos;s back-off, they both provide average error reductions of up to 40% on one task and up to 7% on a related, but somewhat more difficult, task. Only in a fairly unrealistic setting did nearestneighbors averaging clearly beat distributional</context>
</contexts>
<marker>Brown, DellaPietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. DellaPietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brucker</author>
</authors>
<title>On the complexity of clustering problems.</title>
<date>1978</date>
<booktitle>Optimization and Operations Research, number 157 in Lecture Notes in Economics and Mathematical Systems.</booktitle>
<editor>In Rudolf Henn, Bernhard H. Korte, and Werner Oettli, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<marker>Brucker, 1978</marker>
<rawString>Peter Brucker. 1978. On the complexity of clustering problems. In Rudolf Henn, Bernhard H. Korte, and Werner Oettli, editors, Optimization and Operations Research, number 157 in Lecture Notes in Economics and Mathematical Systems. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--19</pages>
<contexts>
<context position="1339" citStr="Church and Gale, 1991" startWordPosition="188" endWordPosition="191">1 Introduction In many statistical language-processing problems, it is necessary to estimate the joint probability or cooccurrence probability of events drawn from two prescribed sets. Data sparseness can make such estimates difficult when the events under consideration are sufficiently fine-grained, for instance, when they correspond to occurrences of specific words in given configurations. In particular, in many practical modeling tasks, a substantial fraction of the cooccurrences of interest have never been seen in training data. In most previous work (Jelinek and Mercer, 1980; Katz, 1987; Church and Gale, 1991; Ney and Essen, 1993), this lack of information is addressed by reserving some mass in the probability model for unseen joint events, and then assigning that mass to those events as a function of their marginal frequencies. An intuitively appealing alternative to relying on marginal frequencies alone is to combine estimates of the probabilities of &amp;quot;similar&amp;quot; events. More specifically, a joint event (x, y) would be considered similar to another (x&apos;, y) if the distributions of Y given x and Y given x&apos; (the cooccurrence distributions of x and x&apos;) meet an appropriate definition of distributional s</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Kenneth W. Church and William A. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Shaul Marcus</author>
<author>Shaul Markovitch</author>
</authors>
<title>Contextual word similarity and estimation from sparse data. Computer Speech and Language,</title>
<date>1995</date>
<pages>9--123</pages>
<contexts>
<context position="5258" citStr="Dagan et al., 1995" startWordPosition="809" endWordPosition="812">ts of up to 40% with respect to Katz&apos;s (1987) state-of-the-art backoff method in the prediction of unseen cooccurrences. In the case of nearest-neighbors averaging, we have also demonstrated perplexity reductions of 20% and statistically significant improvement in speech recognition error rate. Furthermore, each method has generated some discussion in the literature (Hofmann et al., 1999; Baker and McCallum, 1998; Ide and Veronis, 1998). Given the relative success of these methods and their complementarity, it is natural to wonder how they compare in practice. Several authors (Schiitze, 1993; Dagan et al., 1995; Ide and Veronis, 1998) have suggested that clustering methods, by reducing data to a small set of representatives, might perform less well than nearest-neighbors averaging-type methods. For instance, Dagan et al. (1995, p. 124) argue: This [class-based] approach, which follows long traditions in semantic classification, is very appealing, as it attempts to capture &amp;quot;typical&amp;quot; properties of classes of words. However .... it is not clear that word co-occurrence patterns can be generalized to class cooccurrence parameters without losing too much information. Furthermore, early work on class-based</context>
</contexts>
<marker>Dagan, Marcus, Markovitch, 1995</marker>
<rawString>Ido Dagan, Shaul Marcus, and Shaul Markovitch. 1995. Contextual word similarity and estimation from sparse data. Computer Speech and Language, 9:123-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="3313" citStr="Dagan et al., 1999" startWordPosition="510" endWordPosition="513">ion: distributional clustering and nearestneighbors averaging. Distributional clustering (Pereira et al., 1993) assigns to each word a probability distribution over clusters to which it may belong, and characterizes each cluster by a centroid, which is an average of cooccurrence distributions of words weighted according to cluster membership probabilities. Cooccurrence probabilities can then be derived from either a membership-weighted average of the clusters to which the words in the cooccurrence belong, or just from the highest-probability cluster. In contrast, nearest-neighbors averaging&apos; (Dagan et al., 1999) does not explicitly cluster words. Rather, a given cooccurrence probability is estimated by averaging probabilities for the set of cooccurrences most similar to the target cooccurrence. That is, while both methods involve appealing to similar &amp;quot;witnesses&amp;quot; (in the clustering case, these witnesses are the centroids; for nearest-neighbors averaging, they are &apos;In previous papers, we have used the term &amp;quot;similarity-based&amp;quot;, but this term would cause confusion in the present article. 33 the most similar words), in nearest-neighbors averaging the witnesses vary for different cooccurrences, whereas in d</context>
<context position="11927" citStr="Dagan et al., 1999" startWordPosition="1945" endWordPosition="1948">uster weighted average: /5(vIn) = EP(v14(cin) • nearest-cluster estimate: /5(vin) = p(vIc*) , where c* maximizes p(c* in) . 2.2 Nearest-neighbors averaging As noted earlier, the nearest-neighbors averaging method is an alternative to clustering for estimating the probabilities of unseen cooccurrences. Given an unseen pair (n, v), we calculate an estimate 23(v in) as an appropriate average of p(vIn&apos;) where n&apos; is distributionally similar to n. Many distributional similarity measures can be considered (Lee, 1999). In this paper, we focus on the one that gave the best results in our earlier work (Dagan et al., 1999), the Jensen-Shannon divergence (Rao, 1982; Lin, 1991). The Jensen-Shannon divergence of two discrete distributions p and q over the same domain is defined as J S q) =1 [D P q) D P q)] . 2 2 2 It is easy to see that JS(p, q) is always defined. In previous work, we used the estimate 13(70) = —1 E p(vin&apos;) exp(-13J(n, n&apos;)) , an n&apos;ES(n,k) where J(n,n&apos;) = JS (p(Vin),p(Vin&apos;)), and k are tunable parameters, S(n, k) is the set of k nouns with the smallest Jensen-Shannon divergence to 11, and an is a normalization term. However, in the present work we use the simpler unweighted average n&apos;ES(n,k) and ex</context>
<context position="14943" citStr="Dagan et al., 1999" startWordPosition="2426" endWordPosition="2429">unctional form, number and type of parameters, and choice of distance function from playing a role in the comparison, increasing our confidence that we are truly comparing paradigms and not implementation details. What are the fundamental differences between the two methods? From the foregoing discussion it is clear that distributional clustering is theoretically more satisfying and depends on a single model complexity parameter. On the other hand, nearest-neighbors averaging in its most general form offers more flexibility in defining the set of most similar words and their relative weights (Dagan et al., 1999). Also, the training phase requires little computation, as opposed to the iterative re-estimation procedure employed to build the cluster model. But the key difference is the amount of data compression, or equivalently the amount of generalization, produced by the two models. Cluster1 25(vin) = –k E P(vIni) (3) 36 ing yields a far more compact representation of the data when k, the model size parameter, is smaller than IN I. As noted above, various authors have conjectured that this data reduction must inevitably result in lower performance in comparison to nearest-neighbor methods, which stor</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando Pereira. 1999. Similarity-based models of word cooccurrence probabilities. Machine Learning, 34(1-3):43-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
<author>Jan Puzicha</author>
<author>Michael I Jordan</author>
</authors>
<title>Learning from dyadic data.</title>
<date>1999</date>
<booktitle>In Advances in Neural Information Processing Systems 11.</booktitle>
<publisher>MIT Press.</publisher>
<note>To appear.</note>
<contexts>
<context position="5030" citStr="Hofmann et al., 1999" startWordPosition="772" endWordPosition="775"> amount of storage required. In a way, it is clustering taken to the limit - each word forms its own cluster. In previous work, we have shown that both distributional clustering and nearest-neighbors averaging can yield improvements of up to 40% with respect to Katz&apos;s (1987) state-of-the-art backoff method in the prediction of unseen cooccurrences. In the case of nearest-neighbors averaging, we have also demonstrated perplexity reductions of 20% and statistically significant improvement in speech recognition error rate. Furthermore, each method has generated some discussion in the literature (Hofmann et al., 1999; Baker and McCallum, 1998; Ide and Veronis, 1998). Given the relative success of these methods and their complementarity, it is natural to wonder how they compare in practice. Several authors (Schiitze, 1993; Dagan et al., 1995; Ide and Veronis, 1998) have suggested that clustering methods, by reducing data to a small set of representatives, might perform less well than nearest-neighbors averaging-type methods. For instance, Dagan et al. (1995, p. 124) argue: This [class-based] approach, which follows long traditions in semantic classification, is very appealing, as it attempts to capture &amp;quot;ty</context>
</contexts>
<marker>Hofmann, Puzicha, Jordan, 1999</marker>
<rawString>Thomas Hofmann, Jan Puzicha, and Michael I. Jordan. 1999. Learning from dyadic data. In Advances in Neural Information Processing Systems 11. MIT Press. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Jean Veronis</author>
</authors>
<title>Introduction to the special issue on word sense disambiguation: The state of the art.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--1</pages>
<contexts>
<context position="5080" citStr="Ide and Veronis, 1998" startWordPosition="780" endWordPosition="783">stering taken to the limit - each word forms its own cluster. In previous work, we have shown that both distributional clustering and nearest-neighbors averaging can yield improvements of up to 40% with respect to Katz&apos;s (1987) state-of-the-art backoff method in the prediction of unseen cooccurrences. In the case of nearest-neighbors averaging, we have also demonstrated perplexity reductions of 20% and statistically significant improvement in speech recognition error rate. Furthermore, each method has generated some discussion in the literature (Hofmann et al., 1999; Baker and McCallum, 1998; Ide and Veronis, 1998). Given the relative success of these methods and their complementarity, it is natural to wonder how they compare in practice. Several authors (Schiitze, 1993; Dagan et al., 1995; Ide and Veronis, 1998) have suggested that clustering methods, by reducing data to a small set of representatives, might perform less well than nearest-neighbors averaging-type methods. For instance, Dagan et al. (1995, p. 124) argue: This [class-based] approach, which follows long traditions in semantic classification, is very appealing, as it attempts to capture &amp;quot;typical&amp;quot; properties of classes of words. However ...</context>
</contexts>
<marker>Ide, Veronis, 1998</marker>
<rawString>Nancy Ide and Jean Veronis. 1998. Introduction to the special issue on word sense disambiguation: The state of the art. Computational Linguistics, 24(1):1-40, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Proceedings of the Workshop on Pattern Recognition in Practice,</booktitle>
<publisher>North Holland.</publisher>
<location>Amsterdam,</location>
<contexts>
<context position="1304" citStr="Jelinek and Mercer, 1980" startWordPosition="182" endWordPosition="185">stributional models of unseen events. 1 Introduction In many statistical language-processing problems, it is necessary to estimate the joint probability or cooccurrence probability of events drawn from two prescribed sets. Data sparseness can make such estimates difficult when the events under consideration are sufficiently fine-grained, for instance, when they correspond to occurrences of specific words in given configurations. In particular, in many practical modeling tasks, a substantial fraction of the cooccurrences of interest have never been seen in training data. In most previous work (Jelinek and Mercer, 1980; Katz, 1987; Church and Gale, 1991; Ney and Essen, 1993), this lack of information is addressed by reserving some mass in the probability model for unseen joint events, and then assigning that mass to those events as a function of their marginal frequencies. An intuitively appealing alternative to relying on marginal frequencies alone is to combine estimates of the probabilities of &amp;quot;similar&amp;quot; events. More specifically, a joint event (x, y) would be considered similar to another (x&apos;, y) if the distributions of Y given x and Y given x&apos; (the cooccurrence distributions of x and x&apos;) meet an appropr</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Frederick Jelinek and Robert L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, Amsterdam, May. North Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<pages>35--3</pages>
<contexts>
<context position="1316" citStr="Katz, 1987" startWordPosition="186" endWordPosition="187">een events. 1 Introduction In many statistical language-processing problems, it is necessary to estimate the joint probability or cooccurrence probability of events drawn from two prescribed sets. Data sparseness can make such estimates difficult when the events under consideration are sufficiently fine-grained, for instance, when they correspond to occurrences of specific words in given configurations. In particular, in many practical modeling tasks, a substantial fraction of the cooccurrences of interest have never been seen in training data. In most previous work (Jelinek and Mercer, 1980; Katz, 1987; Church and Gale, 1991; Ney and Essen, 1993), this lack of information is addressed by reserving some mass in the probability model for unseen joint events, and then assigning that mass to those events as a function of their marginal frequencies. An intuitively appealing alternative to relying on marginal frequencies alone is to combine estimates of the probabilities of &amp;quot;similar&amp;quot; events. More specifically, a joint event (x, y) would be considered similar to another (x&apos;, y) if the distributions of Y given x and Y given x&apos; (the cooccurrence distributions of x and x&apos;) meet an appropriate definit</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing, ASSP-35(3):400-401, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the ACL,</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>Somerset, New Jersey.</location>
<note>Distributed by</note>
<contexts>
<context position="11823" citStr="Lee, 1999" startWordPosition="1926" endWordPosition="1927">rred together in training. We consider two heuristic ways of doing this estimation: 35 • all-cluster weighted average: /5(vIn) = EP(v14(cin) • nearest-cluster estimate: /5(vin) = p(vIc*) , where c* maximizes p(c* in) . 2.2 Nearest-neighbors averaging As noted earlier, the nearest-neighbors averaging method is an alternative to clustering for estimating the probabilities of unseen cooccurrences. Given an unseen pair (n, v), we calculate an estimate 23(v in) as an appropriate average of p(vIn&apos;) where n&apos; is distributionally similar to n. Many distributional similarity measures can be considered (Lee, 1999). In this paper, we focus on the one that gave the best results in our earlier work (Dagan et al., 1999), the Jensen-Shannon divergence (Rao, 1982; Lin, 1991). The Jensen-Shannon divergence of two discrete distributions p and q over the same domain is defined as J S q) =1 [D P q) D P q)] . 2 2 2 It is easy to see that JS(p, q) is always defined. In previous work, we used the estimate 13(70) = —1 E p(vin&apos;) exp(-13J(n, n&apos;)) , an n&apos;ES(n,k) where J(n,n&apos;) = JS (p(Vin),p(Vin&apos;)), and k are tunable parameters, S(n, k) is the set of k nouns with the smallest Jensen-Shannon divergence to 11, and an is a</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In 37th Annual Meeting of the ACL, Somerset, New Jersey. Distributed by Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on the Shannon entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>37--1</pages>
<contexts>
<context position="11981" citStr="Lin, 1991" startWordPosition="1954" endWordPosition="1955"> estimate: /5(vin) = p(vIc*) , where c* maximizes p(c* in) . 2.2 Nearest-neighbors averaging As noted earlier, the nearest-neighbors averaging method is an alternative to clustering for estimating the probabilities of unseen cooccurrences. Given an unseen pair (n, v), we calculate an estimate 23(v in) as an appropriate average of p(vIn&apos;) where n&apos; is distributionally similar to n. Many distributional similarity measures can be considered (Lee, 1999). In this paper, we focus on the one that gave the best results in our earlier work (Dagan et al., 1999), the Jensen-Shannon divergence (Rao, 1982; Lin, 1991). The Jensen-Shannon divergence of two discrete distributions p and q over the same domain is defined as J S q) =1 [D P q) D P q)] . 2 2 2 It is easy to see that JS(p, q) is always defined. In previous work, we used the estimate 13(70) = —1 E p(vin&apos;) exp(-13J(n, n&apos;)) , an n&apos;ES(n,k) where J(n,n&apos;) = JS (p(Vin),p(Vin&apos;)), and k are tunable parameters, S(n, k) is the set of k nouns with the smallest Jensen-Shannon divergence to 11, and an is a normalization term. However, in the present work we use the simpler unweighted average n&apos;ES(n,k) and examine the effect of the choice of k on modeling perfor</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Jianhua Lin. 1991. Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1):145-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Ute Essen</author>
</authors>
<title>Estimating &apos;small&apos; probabilities by leaving-one-out. In</title>
<date>1993</date>
<booktitle>Third European Conference On Speech Communication and Technology,</booktitle>
<pages>2239--2242</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="1361" citStr="Ney and Essen, 1993" startWordPosition="192" endWordPosition="195">statistical language-processing problems, it is necessary to estimate the joint probability or cooccurrence probability of events drawn from two prescribed sets. Data sparseness can make such estimates difficult when the events under consideration are sufficiently fine-grained, for instance, when they correspond to occurrences of specific words in given configurations. In particular, in many practical modeling tasks, a substantial fraction of the cooccurrences of interest have never been seen in training data. In most previous work (Jelinek and Mercer, 1980; Katz, 1987; Church and Gale, 1991; Ney and Essen, 1993), this lack of information is addressed by reserving some mass in the probability model for unseen joint events, and then assigning that mass to those events as a function of their marginal frequencies. An intuitively appealing alternative to relying on marginal frequencies alone is to combine estimates of the probabilities of &amp;quot;similar&amp;quot; events. More specifically, a joint event (x, y) would be considered similar to another (x&apos;, y) if the distributions of Y given x and Y given x&apos; (the cooccurrence distributions of x and x&apos;) meet an appropriate definition of distributional similarity. For example</context>
</contexts>
<marker>Ney, Essen, 1993</marker>
<rawString>Hermann Ney and Ute Essen. 1993. Estimating &apos;small&apos; probabilities by leaving-one-out. In Third European Conference On Speech Communication and Technology, pages 2239-2242, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In 31st Annual Meeting of the ACL,</booktitle>
<pages>183--190</pages>
<publisher>Association</publisher>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="2805" citStr="Pereira et al., 1993" startWordPosition="432" endWordPosition="435">s. For concreteness and experimental evaluation, we focus in this paper on a particular type of cooccurrence, that of a main verb and the head noun of its direct object in English text. Our main goal is to obtain estimates 13(v In) of the conditional probability of a main verb v given a direct object head noun n, which can then be used in particular prediction tasks. In previous work, we and our co-authors have proposed two different probability estimation methods that incorporate word similarity information: distributional clustering and nearestneighbors averaging. Distributional clustering (Pereira et al., 1993) assigns to each word a probability distribution over clusters to which it may belong, and characterizes each cluster by a centroid, which is an average of cooccurrence distributions of words weighted according to cluster membership probabilities. Cooccurrence probabilities can then be derived from either a membership-weighted average of the clusters to which the words in the cooccurrence belong, or just from the highest-probability cluster. In contrast, nearest-neighbors averaging&apos; (Dagan et al., 1999) does not explicitly cluster words. Rather, a given cooccurrence probability is estimated by</context>
<context position="7209" citStr="Pereira et al., 1993" startWordPosition="1118" endWordPosition="1121">e error reductions of at least 18% in comparison to backoff. Therefore, previous claims that clustering methods are necessarily inferior are not strongly supported by the evidence of these experiments, although it is of course possible that the situation may be different for other tasks. 2 Two models We now survey the distributional clustering (section 2.1) and nearest-neighbors averaging (section 2.2) models. Section 2.3 examines the relationships between these two methods. 2.1 Clustering The distributional clustering model that we evaluate in this paper is a refinement of our earlier model (Pereira et al., 1993). The new model has important theoretical advantages over the earlier one and interesting mathematical properties, which will be discussed elsewhere. Here, we will outline the main motivation for the model, the iterative equations that implement it, and their practical use in clustering. The model involves two discrete random variables N (nouns) and V (verbs) whose joint distribution we have sampled, and a new unobserved discrete random variable C representing probabilistic clusters of elements of N. The role of the hidden variable C is specified by the conditional distribution p(cln), which c</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando C. N. Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In 31st Annual Meeting of the ACL, pages 183-190, Somerset, New Jersey. Association for Computational Linguistics. Distributed by Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Radhakrishna Rao</author>
</authors>
<title>Diversity: Its measurement, decomposition, apportionment and analysis.</title>
<date>1982</date>
<journal>Sankyha: The Indian Journal of Statistics,</journal>
<pages>44--1</pages>
<contexts>
<context position="11969" citStr="Rao, 1982" startWordPosition="1952" endWordPosition="1953">est-cluster estimate: /5(vin) = p(vIc*) , where c* maximizes p(c* in) . 2.2 Nearest-neighbors averaging As noted earlier, the nearest-neighbors averaging method is an alternative to clustering for estimating the probabilities of unseen cooccurrences. Given an unseen pair (n, v), we calculate an estimate 23(v in) as an appropriate average of p(vIn&apos;) where n&apos; is distributionally similar to n. Many distributional similarity measures can be considered (Lee, 1999). In this paper, we focus on the one that gave the best results in our earlier work (Dagan et al., 1999), the Jensen-Shannon divergence (Rao, 1982; Lin, 1991). The Jensen-Shannon divergence of two discrete distributions p and q over the same domain is defined as J S q) =1 [D P q) D P q)] . 2 2 2 It is easy to see that JS(p, q) is always defined. In previous work, we used the estimate 13(70) = —1 E p(vin&apos;) exp(-13J(n, n&apos;)) , an n&apos;ES(n,k) where J(n,n&apos;) = JS (p(Vin),p(Vin&apos;)), and k are tunable parameters, S(n, k) is the set of k nouns with the smallest Jensen-Shannon divergence to 11, and an is a normalization term. However, in the present work we use the simpler unweighted average n&apos;ES(n,k) and examine the effect of the choice of k on mod</context>
</contexts>
<marker>Rao, 1982</marker>
<rawString>C. Radhakrishna Rao. 1982. Diversity: Its measurement, decomposition, apportionment and analysis. Sankyha: The Indian Journal of Statistics, 44(A):1-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schiitze</author>
</authors>
<title>Word space.</title>
<date>1993</date>
<booktitle>Advances in Neural Information Processing Systems 5,</booktitle>
<pages>895--902</pages>
<editor>In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors,</editor>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="5238" citStr="Schiitze, 1993" startWordPosition="807" endWordPosition="808">yield improvements of up to 40% with respect to Katz&apos;s (1987) state-of-the-art backoff method in the prediction of unseen cooccurrences. In the case of nearest-neighbors averaging, we have also demonstrated perplexity reductions of 20% and statistically significant improvement in speech recognition error rate. Furthermore, each method has generated some discussion in the literature (Hofmann et al., 1999; Baker and McCallum, 1998; Ide and Veronis, 1998). Given the relative success of these methods and their complementarity, it is natural to wonder how they compare in practice. Several authors (Schiitze, 1993; Dagan et al., 1995; Ide and Veronis, 1998) have suggested that clustering methods, by reducing data to a small set of representatives, might perform less well than nearest-neighbors averaging-type methods. For instance, Dagan et al. (1995, p. 124) argue: This [class-based] approach, which follows long traditions in semantic classification, is very appealing, as it attempts to capture &amp;quot;typical&amp;quot; properties of classes of words. However .... it is not clear that word co-occurrence patterns can be generalized to class cooccurrence parameters without losing too much information. Furthermore, early</context>
</contexts>
<marker>Schiitze, 1993</marker>
<rawString>Hinrich Schiitze. 1993. Word space. In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, Advances in Neural Information Processing Systems 5, pages 895-902. Morgan Kaufmann, San Francisco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>