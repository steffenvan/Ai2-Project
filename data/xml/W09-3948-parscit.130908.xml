<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011670">
<title confidence="0.9988285">
A Comparison between Dialog Corpora Acquired
with Real and Simulated Users
</title>
<author confidence="0.992433">
David Griol Zoraida Callejas, Ram´on L´opez-C´ozar
</author>
<affiliation confidence="0.778664">
Departamento de Informdtica Dpto. Lenguajes y Sistemas Informdticos
Universidad Carlos III de Madrid Universidad de Granada
</affiliation>
<email confidence="0.991088">
dgriol@inf.uc3m.es {zoraida, rlopezc}@ugr.es
</email>
<sectionHeader confidence="0.993605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996466">
In this paper, we test the applicability
of a stochastic user simulation technique
to generate dialogs which are similar to
real human-machine spoken interactions.
To do so, we present a comparison be-
tween two corpora employing a compre-
hensive set of evaluation measures. The
first corpus was acquired from real inter-
actions of users with a spoken dialog sys-
tem, whereas the second was generated by
means of the simulation technique, which
decides the next user answer taking into
account the previous user turns, the last
system answer and the objective of the di-
alog.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999908672413793">
During the last decade, there has been a grow-
ing interest in learning corpus-based approaches
for the different components of spoken dialog sys-
tems (Minker, 1999), (Young, 2002), (Esteve et al.,
2003), (He and Young, 2003), (Torres et al., 2005),
(Georgila et al., 2006), (Williams and Young,
2007). One of the most relevant areas of study
has been the automatic generation of dialogs be-
tween the dialog manager and an additional mod-
ule, called the user simulator, which generates au-
tomatic interactions with the dialog system.
A considerable effort is necessary to acquire
and label a corpus with the data necessary to train
good models. User simulators make it possible to
generate a large number of dialogs in a very simple
way, reducing the time and effort needed for the
evaluation of a dialog system each time the sys-
tem is modified.
The construction of user models based on sta-
tistical methods has provided interesting and well-
founded results in recent years and is currently a
growing research area. A probabilistic user model
can be trained from a corpus of human-computer
dialogs to simulate user answers. Therefore, it can
be used to learn a dialog strategy by means of its
interaction with the dialog manager. In the liter-
ature, there are several corpus-based approaches
for developing user simulators, learning optimal
management strategies, and evaluating the dialog
system (Scheffler and Young, 2001) (Pietquin and
Dutoit, 2005) (Georgila et al., 2006) (Cuaydhuitl
et al., 2006) (L´opez-C´ozar et al., 2006). A sum-
mary of user simulation techniques for reinforce-
ment learning of the dialog strategy can be found
in (Schatzmann et al., 2006). In this paper, we
propose a statistical approach to acquire a labeled
dialog corpus from the interaction of a user simu-
lator and a dialog manager. In our methodology,
the new user turn is selected using the probabil-
ity distribution provided by a neural network. By
means of the interaction of the dialog manager and
the user simulator, an initial dialog corpus can be
extended by increasing its variability and detect-
ing dialog situations in which the dialog manager
does not provide an appropriate answer. We pro-
pose the use of this corpus for evaluating both our
user simulation technique and our dialog system
performance.
Different studies have been carried out to com-
pare corpora acquired by means of different tech-
niques and to define the most suitable measures to
carry out this evaluation (Schatzmann et al., 2005),
(Turunen et al., 2006), (Ai et al., 2007b), (Ai and
Litman, 2006), (Ai and Litman, 2007), (Ai et al.,
2007a). In this work, we have applied our dia-
log simulation technique to acquire a corpus in the
academic domain, and compared it with a corpus
recorded from real users interactions with a spo-
</bodyText>
<subsubsectionHeader confidence="0.642016">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 326–332,
</subsubsectionHeader>
<affiliation confidence="0.938219">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.999749">
326
</page>
<bodyText confidence="0.999409933333334">
ken dialog system
The results of this comparison show that the
simulated corpus obtained is very similar to the
corpus recorded from real user interactions in
terms of number of turns, confirmations and dia-
log acts among other evaluation measures.
The rest of the paper is organized as follows.
Section 2 summarizes the main characteristics of
the UAH system. Section 3 describes our statis-
tical methodology for user simulation. Section 4
describes the set of measures used to compare the
corpus acquired with real users and the simulated
corpus. Section 5 presents the results of this eval-
uation, and finally, the conclusions are presented
in Section 6.
</bodyText>
<sectionHeader confidence="0.976726" genericHeader="method">
2 The UAH System
</sectionHeader>
<bodyText confidence="0.953683862068966">
Universidad Al Habla (UAH - University on the
Line) is a spoken dialog system developed to pro-
vide spoken access to academic information about
our department (Callejas and L´opez-C´ozar, 2005)
(Callejas and L´opez-C´ozar, 2008). The system is
comprised of the five typical modules of current
spoken dialog systems, concerned with automatic
speech recognition (ASR), dialog management
(DM), database access (DB Access), data storage
(DB) and oral response generation (RG). In ad-
dition, we implemented a module called GAG to
automatically create ASR grammars (Callejas and
L´opez-C´ozar, 2007).
The dialog manager of the UAH system was im-
plemented using VoiceXML documents that are
dynamically created using PHP. This way, it can
adapt the system responses to the context of the
conversation and the dialog state, which improves
the naturalness of the interaction. For example, the
help messages provided by the system take into
account the topic that the user and the system are
addressing at a particular moment. The context is
used as well to decide the confirmation strategy to
use.
The information that the system provides can be
classified in four main groups: subjects, profes-
sors, doctoral studies and registration, as shown in
Table 1. As can be observed, the system must ask
the user for different pieces of information before
producing a response.
A set of 100 dialogs was acquired with the UAH
system by means of its interaction with real users.
The acquisition process resulted in a spontaneous
Spanish speech dialog corpus with 60 different
speakers. The total number of user turns was 422
and the recorded material has a duration of 150
minutes. Figure 1 shows an example of a dialog
extracted from this corpus. Turns tagged with an
‘S’ refer to system turns, and turns tagged with ‘U’
refer to user turns.
S1: Welcome to the UAH system. How can I
help you?
U1: I want to know information about sub-
jects in Computer Science
S2: Which subject do you want to consult?
U2: Language Processors I
S3: What would you like to know?
U3: Can you inform me about the schedules
of Group D?
S4: The schedule of Group D is: Mondays
from 9 to 10 and Wednesdays from 9 to 11
and from 11 to 13. Anything else?
U4: Can you tell me the name of the lecturer?
S5: The name of the lecturer of the Group
D of Language Processors I is David Griol.
Anything else?
U5: No, thank you.
S6: Thank you for using the UAH system.
</bodyText>
<figureCaption confidence="0.9947375">
Figure 1: An example of a dialog acquired with
the interaction of the UAH system with real users
</figureCaption>
<sectionHeader confidence="0.8769195" genericHeader="method">
3 The Statistical User Simulation
Technique
</sectionHeader>
<bodyText confidence="0.998135388888889">
In the UAH spoken dialogue system, the user sim-
ulator developed replaces the functions performed
by the ASR and the NLU modules.
The methodology that we have developed for
user simulation extends our work for developing
a statistical methodology for dialog management
(Griol et al., 2008). The user answers are gener-
ated taking into account the information provided
by the simulator throughout the history of the dia-
log, the last system turn, and the objective(s) pre-
defined for the dialog. A labeled corpus of dialogs
is used to estimate the user model. The formal de-
scription of the proposed model is as follows:
Let AZ be the output of the dialog system (the
system answer) at time i, expressed in terms of di-
alog acts. Let UZ be the semantic representation of
the user turn. We represent a dialog as a sequence
of pairs (system-turn, user-turn):
</bodyText>
<page confidence="0.986613">
327
</page>
<table confidence="0.877233608695652">
Category Information provided by the user (including examples) Information provided by the
system
Subject Name Compilers Degree, lecturers, responsible
lecturer, semester, credits, web
page
Degree, in case that there are Computer science
several subjects with the same
name
Group name and optionally type, A Timetable, lecturer
in case he asks for information Theory A
about a specific group
Lecturers Any combination of name and Zoraida Office location, contact infor-
surnames Zoraida Callejas mation (phone, fax, email),
Ms. Callejas groups and subjects, doctoral
courses
Optionally semester, in case he First semester Tutoring timetable
asks for the tutoring hours Second semester
Doctoral studies Name of a doctoral program Software development Department, responsible
Name of a course if he asks Object-oriented program- Type, credits
for information about a specific ming
course
Registration Name of the deadline Provisional registration Initial time, final time, de-
confirmation scription
</table>
<tableCaption confidence="0.999439">
Table 1: Information provided by the UAH system
</tableCaption>
<bodyText confidence="0.984172714285714">
(A1, U1), ··· , (Ai, Ui), ··· , (An, Un)
where A1 is the greeting turn of the system (the
first turn of the dialog), and Un is the last user turn.
We refer to a pair (Ai, Ui) as Si, the state of the
dialog sequence at time i.
Given this representation, the objective of the
user simulator at time i is to find an appropriate
user answer Ui. This selection, which is a local
process for each time i, takes into account the se-
quence of dialog states that precede time i, the sys-
tem answer at time i, and the objective of the di-
alog O. If the most probable user answer Ui is
selected at each time i, the selection is made using
the following maximization:
</bodyText>
<equation confidence="0.681292">
P(Ui|S1,··· ,Si_1, Ai, O)
</equation>
<bodyText confidence="0.999796857142857">
where set U contains all the possible user answers.
As the number of possible sequences of states
is very large, we establish a partition in this space
(i.e., in the history of the dialog preceding time i).
Let URi be the user register at time i. The user
register is defined as a data structure that contains
the information provided by the user throughout
the previous history of the dialog.The partition
that we establish in this space is based on the as-
sumption that two different sequences of states are
equivalent if they lead to the same UR. After ap-
plying the above considerations and establishing
the equivalence relations in the histories of the di-
alogs, the selection of the best Ui is given by:
</bodyText>
<equation confidence="0.951755">
P (Ui|URi_1, Ai, O) (1)
</equation>
<bodyText confidence="0.999996869565218">
We propose the use of a multilayer percep-
tron (MLP) to make the assignation of a user
turn. The input layer receives the current situa-
tion of the dialog, which is represented by the term
(URi_1, Ai, O) in Equation 1. The values of the
output layer can be viewed as the a posteriori prob-
ability of selecting the different user answers de-
fined for the simulator given the current situation
of the dialog. The choice of the most probable
user answer of this probability distribution leads
to Equation 1. In this case, the user simulator will
always generate the same answer for the same sit-
uation of the dialog. Since we want to provide the
user simulator with a richer variability of behav-
iors, we base our choice on the probability distri-
bution supplied by the MLP on all the feasible user
answers.
For the UAH task, the variable O is modeled
taking into account the different types of scenarios
defined for the acquisition of the original corpus
with real users (33).
The corpus acquired with real users includes in-
formation about the errors that were introduced by
</bodyText>
<equation confidence="0.9948855">
Ui = argmax
UiEU
Ui = argmax
UiEU
</equation>
<page confidence="0.971916">
328
</page>
<bodyText confidence="0.99981608">
the ASR and the NLU modules during this acqui-
sition. This information also includes confidence
measures, which are used by the DM to evaluate
the reliability of the concepts and attributes gener-
ated by the NLU module.
An error simulator module has been designed
to perform error generation. The error simulator
modifies the frames generated by the user simula-
tor once the UR is updated. In addition, the error
simulator adds a confidence score to each concept
and attribute in the frames. Experimentally, we
have detected 2.3 errors per dialog in our initial
corpus. This value can be modified to adapt the er-
ror simulator module to the operation of any ASR
and NLU modules.
A maximum number of twelve user turns per di-
alog was defined for acquiring a corpus using our
user simulator. A user request for closing the di-
alog is selected once the system has provided the
information defined in the objective(s) of the dia-
log. The dialogs that fulfill this condition before
the maximum number of turns are considered suc-
cessful. The dialog manager considers that the di-
alog is unsuccessful and decides to abort it when
the following conditions take place:
</bodyText>
<listItem confidence="0.980003416666667">
• The dialog exceeds the maximum number of
user turns.
• The answer selected by the dialog manager
corresponds with a query not required by the
user simulator.
• The database query module provides an error
warning because the user simulator has not
provided the mandatory information needed
to carry out the query.
• The answer generator provides a error warn-
ing when the selected answer involves the use
of a data not provided by the user simulator.
</listItem>
<bodyText confidence="0.999146944444445">
The semantics selected for the dialog manager
is represented through the 27 possible system an-
swers defined for the UAH task. The selection of
the possible user answers is carried out using the
semantics defined for the user in the NLU module.
A dialog corpus of 3000 successful dialogs was
acquired using the user simulation technique fol-
lowing a set of scenarios covering all the possible
types of queries regarding the information sought.
Figure 2 shows an example of a dialog cor-
responding to a scenario with two objectives (to
know timetables of a given lecturer and subject,
and information about the registration in its spe-
cific degree). The confidence scores generated by
the error simulator are shown between brackets.
Sentences in natural language, which are equiv-
alent to the labeled system and user turns, have
been incorporated to clarify the explanation.
</bodyText>
<figure confidence="0.973158677419355">
S1: Opening
Welcome to the UAH system. How may I help
you?
U1: Lecturers [0.1]
I want to know information about lecturers.
S2: (Confirmation:Lecturers)
Do you want to know information about lec-
turers?
U2: Affirmation [0.9]
Yes.
S3: (Question:Lecturer Name)
Tell me the name of the lecturer.
U3: Lecturer Name: Ram´on L´opez-C´ozar
[0.8]
Ram´on L´opez-C´ozar.
S4: (Question:Lecturer Information)
What would you like to know about Mr.
L´opez-C´ozar?
U4: (Question:Office) [0.9]
The location of his office please.
S5: (Answer:Office) (New-Query)
{The office of Ram´on L´opez-C´ozar is located
in the third floor of ...} Anything else?
U5: (Question:Registration) [0.9]
Degree: Computer Science[0.9]
How to register in Computer Science.
S6: (Answer:Registration) (New-Query)
{Registration information} Anything else?
U6: Negation [0.9] No.
S7: (Closing)
Thank you for using the UAH system.
</figure>
<figureCaption confidence="0.9859615">
Figure 2: An example of a dialog acquired by
means of the simulation technique
</figureCaption>
<sectionHeader confidence="0.849599" genericHeader="method">
4 Evaluation of the Corpora
</sectionHeader>
<bodyText confidence="0.999976714285714">
We used a set of measures to carry out the evalu-
ation of the acquired corpora based on prior work
in the dialog literature. (Schatzmann et al., 2005)
proposed a comprehensive set of quantitative eval-
uation measures to compare two dialog corpora.
These measures were adapted for our purpose and
can be divided into three types:
</bodyText>
<page confidence="0.997678">
329
</page>
<table confidence="0.997503833333333">
High-level dialog features
Average number of turns per dialog
Percentage of different dialogs
Number of repetitions of the most seen dialog
Number of turns of the most seen dialog
Number of turns of the shortest dialog
Number of turns of the longest dialog
Dialog style/cooperativeness measures
System dialog acts: Confirmation of concepts and attributes, Questions to require information, and
Answers generated after a database query.
User dialog acts: Request to the system, Provide information, Confirmation, Yes/No answers, and
Other answers.
</table>
<figureCaption confidence="0.9679">
Figure 3: Evaluation measures used to compare the acquired corpora
</figureCaption>
<listItem confidence="0.986466692307692">
• High-level dialog features: These features
evaluate the duration of the dialogs, the
amount of information transmitted in the in-
dividual turns, and how active the dialog par-
ticipants are.
• Dialog style/cooperativeness measures:
These measures analyze the frequency of
the different speech acts and study, for
example, the proportion of actions which are
goal-directed vs. dialog formalities.
• Task success/efficiency measures: These are
computations of the goal achievement rates
and goal completion times.
</listItem>
<bodyText confidence="0.999968">
We have defined six high-level dialog features
for the evaluation of the dialogs: the average num-
ber of turns per dialog, the percentage of differ-
ent dialogs without considering the attribute val-
ues, the number of repetitions of the most seen di-
alog, the number of turns of the most seen dialog,
the number of turns of the shortest dialog, and the
number of turns of the longest dialog. Using these
measures, we tried to evaluate the success of the
simulated dialogs as well as their efficiency and
variability with regard to the different objectives.
For dialog style features, we have defined a set
of system/user dialog acts. On the system side,
we have measured the frequency of confirmations,
questions that require information, and system an-
swers generated after a database query. We have
not taken into account the opening and closing sys-
tem turns. On the user side, we have measured the
percentage of turns in which the user carries out
a request to the system, provide information, con-
firms a concept or attribute, Yes/No answers, and
other answers not included in the previous cate-
gories.
We have not considered task success/efficiency
measures in our evaluation, since only the dialogs
that fulfill the objectives predefined in the scenar-
ios have been incorporated into our corpora. We
have considered successful dialogs those that ful-
fill the complete list of objectives defined in the
corresponding scenario. Figure 3 summarizes the
complete set of measures used in the evaluation.
</bodyText>
<sectionHeader confidence="0.974399" genericHeader="evaluation">
5 Evaluation Results
</sectionHeader>
<bodyText confidence="0.999960444444444">
To compare the two corpora, we have computed
the mean value for each corpus with respect to
each of the evaluation measures shown in the pre-
vious section. Then two-tailed t-tests have been
employed to compare the means across the two
corpora as described in (Ai et al., 2007a). All dif-
ferences reported as statistically significant have
p-values less than 0.05 after Bonferroni correc-
tions.
</bodyText>
<subsectionHeader confidence="0.99842">
5.1 High-level Dialog Features
</subsectionHeader>
<bodyText confidence="0.99984925">
As stated in the previous section, the first group of
experiments covers the following statistical prop-
erties: i) Dialog length in terms of the average
number of turns per dialog, number of turns of the
shortest dialog, number of turns of the longest di-
alog, and number of turns of the most seen dialog;
ii) Number of different dialogs in each corpus in
terms of the percentage of different dialogs and the
number of repetitions of the most seen dialog; iii)
Turn length in terms of actions per turn; iv) Partic-
ipant activity as a ratio of system and user actions
per dialog.
</bodyText>
<page confidence="0.992595">
330
</page>
<table confidence="0.998031714285714">
Initial Corpus Simulated Corpus
Average number of user turns per dialog 4.99 3.75
Percentage of different dialogs 85.71% 77.42%
Number of repetitions of the most seen dialog 5 27
Number of turns of the most seen dialog 2 2
Number of turns of the shortest dialog 2 2
Number of turns of the longest dialog 14 12
</table>
<tableCaption confidence="0.999768">
Table 2: Results of the high-level dialog features defined for the comparison of the three corpora
</tableCaption>
<bodyText confidence="0.998209769230769">
Table 2 shows the results of the comparison of
the high-level dialog features. It can be observed
that all measures have similar values in both cor-
pora. The more significant difference is the aver-
age number of user turns. In the four types of sce-
narios, the dialogs acquired using the simulation
technique were shorter than the dialogs acquired
with real users. This can be explained by the fact
that there was a number of dialogs acquired with
real users in which the user asked for additional
information not included in the definition of the
corresponding scenario once the dialog objectives
had been achieved.
</bodyText>
<subsectionHeader confidence="0.999686">
5.2 Dialog Style and Cooperativeness
</subsectionHeader>
<bodyText confidence="0.999938222222222">
Tables 3 and 4 respectively show the frequency of
the most dominant user and system dialog acts.
Table 3 shows the results of this comparison for
the system dialog acts. It can be observed that
there are also only slight differences between the
values obtained for both corpora. There is a higher
percentage of confirmations and questions in the
corpus acquired with real users due to its higher
average number of turns per dialog.
Table 4 shows the results of this comparison for
the user dialog acts. The most significant differ-
ence between both corpora is the percentage of
turns in which the user makes a request to the sys-
tem, which is lower in the corpus acquired with
real users. This is possibly because it is less prob-
able that simulated users provide useless informa-
tion, as it is shown in the lower percentage of the
users turns classified as Other answers.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999968294117647">
In this paper, we have presented a comparison be-
tween two corpora acquired using two different
techniques. Firstly, we gathered an initial dialog
corpus from real user-system interactions. Sec-
ondly, we have employed a statistical user simu-
lation technique based on a classification process
to automatically obtain a corpus of simulated di-
alogs. Our results show that it is feasible to acquire
a realistic corpus by means of the simulation tech-
nique. The experimental results reported indicate
that the simulated and real interactions corpora are
very similar in terms of number of user turns, user
and system dialog style and cooperativeness, and
most frequent dialogs statistics. As future work,
we plan to employ the simulated dialogs for eval-
uation purposes and for extracting valuable infor-
mation to optimize the current dialog strategy.
</bodyText>
<sectionHeader confidence="0.998108" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998256827586207">
H. Ai and D. Litman. 2006. Comparing Real-Real,
Simulated-Simulated, and Simulated-Real Spoken
Dialogue Corpora. In Procs. ofAAAI Workshop Sta-
tistical and Empirical Approaches for Spoken Dia-
logue Systems, Boston, USA.
H. Ai and D.J. Litman. 2007. Knowledge Consistent
User Simulations for Dialog Systems. In Proc. of In-
terspeech’07, pages 2697–2700, Antwerp, Belgium.
H. Ai, A. Raux, D. Bohus, M. Eskenazi, and D. Litman.
2007a. Comparing Spoken Dialog Corpora Col-
lected with Recruited Subjects versus Real Users. In
Proc. of the SIGdial’07, pages 124–131, Antwerp,
Belgium.
H. Ai, J.R. Tetreault, and D.J. Litman. 2007b. Com-
paring User Simulation Models For Dialog Strategy
Learning. In Proc. of NAACL HLT’07, pages 1–4,
Rochester, NY, USA.
Z. Callejas and R. L´opez-C´ozar. 2005. Implementing
modular dialogue systems: a case study. In Proc. of
Applied Spoken Language Interaction in Distributed
Environments (ASIDE’05), Aalborg, Denmark.
Z. Callejas and R. L´opez-C´ozar. 2007. Automatic
creation of ASR grammar rules for unknown vo-
cabulary applications. In Proc. of the 8th Interna-
tional workshop on Electronics, Control, Modelling,
Measurement and Signals (ECMS’07), pages 50–55,
Liberec, Czech Republic.
Z. Callejas and R. L´opez-C´ozar. 2008. Relations be-
tween de-facto criteria in the evaluation of a spoken
</reference>
<page confidence="0.995045">
331
</page>
<table confidence="0.99263625">
Initial Corpus Simulated Corpus
Confirmation of concepts and attributes 13.51% 12.23%
Questions to require information 18.44% 16.57%
Answers generated after a database query 68.05% 71.20%
</table>
<tableCaption confidence="0.987876">
Table 3: Percentages of the different types of system dialog acts in both corpora
</tableCaption>
<table confidence="0.999612">
Initial Corpus Simulated Corpus
Request to the system 31.74% 35.43%
Provide information 21.72% 20.98%
Confirmation 10.81% 9.34%
Yes/No answers 33.47% 32.77%
Other answers 2.26% 1.48%
</table>
<tableCaption confidence="0.996112">
Table 4: Percentages of the different types of user dialog acts in both corpora
</tableCaption>
<reference confidence="0.99983195890411">
dialogue system. Speech Communication, 50(8–
9):646–665.
H. Cuay´ahuitl, S. Renals, O. Lemon, and H. Shi-
modaira. 2006. Learning Multi-Goal Dialogue
Strategies Using Reinforcement Learning with Re-
duced State-Action Spaces. In Proc. of the 9th Inter-
national Conference on Spoken Language Process-
ing (Interspeech/ICSLP), pages 469–472, Pittsburgh
(USA).
Y. Esteve, C. Raymond, F. Bechet, and R. De Mori.
2003. Conceptual Decoding for Spoken Dialog sys-
tems. In Proc. of European Conference on Speech
Communications and Technology (Eurospeech’03),
volume 1, pages 617–620, Geneva (Switzerland).
K. Georgila, J. Henderson, and O. Lemon. 2006. User
Simulation for Spoken Dialogue Systems: Learn-
ing and Evaluation. In Proc. of the 9th Interna-
tional Conference on Spoken Language Processing
(Interspeech/ICSLP), pages 1065–1068, Pittsburgh
(USA).
D. Griol, L.F. Hurtado, E. Segarra, and E. Sanchis.
2008. A Statistical Approach to Spoken Dialog Sys-
tems Design and Evaluation. Speech Communica-
tion, 50(8–9):666–682.
Y. He and S. Young. 2003. A data-driven spoken lan-
guage understanding system. In Proc. of IEEEAuto-
matic Speech Recognition and Understanding Work-
shop (ASRU’03), pages 583–588, St. Thomas (U.S.
Virgin Islands).
R. L´opez-C´ozar, Z. Callejas, and M. McTear. 2006.
Testing the performance of spoken dialogue systems
by means of an artificially simulated user. Artificial
Intelligence Review, 26:291–323.
W. Minker. 1999. Stocastically-based semantic analy-
sis. In Kluwer Academic Publishers, Boston (USA).
O. Pietquin and T. Dutoit. 2005. A probabilistic
framework for dialog simulation and optimal strat-
egy learning. In IEEE Transactions on Speech and
Audio Processing, Special Issue on Data Mining of
Speech, Audio and Dialog, volume 14, pages 589–
599.
J. Schatzmann, K. Georgila, and S. Young. 2005.
Quantitative Evaluation of User Simulation Tech-
niques for Spoken Dialogue Systems. In Proc. of
SIGdial’05, pages 45–54, Lisbon (Portugal).
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. In Knowledge
Engineering Review, volume 21(2), pages 97–126.
K. Scheffler and S. Young. 2001. Automatic learning
of dialogue strategy using dialogue simulation and
reinforcement learning. In Proc. of HLT’02, pages
12–18, San Diego (USA).
F. Torres, L.F. Hurtado, F. Garc´ıa, E. Sanchis, and
E. Segarra. 2005. Error handling in a stochastic dia-
log system through confidence measures. In Speech
Communication, pages (45):211–229.
M. Turunen, J. Hakulinen, and A. Kainulainen. 2006.
Evaluation of a Spoken Dialogue System with Us-
ability Tests and Long-term Pilot Studies: Similar-
ities and Differences. In Proc. of the 9th Interna-
tional Conference on Spoken Language Processing
(Interspeech/ICSLP), pages 1057–1060, Pittsburgh,
USA.
J. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. In Computer Speech and Language, volume
21(2), pages 393–422.
S. Young. 2002. The Statistical Approach to the De-
sign of Spoken Dialogue Systems. Technical re-
port, CUED/F-INFENG/TR.433, Cambridge Uni-
versity Engineering Department, Cambridge (UK).
</reference>
<page confidence="0.998259">
332
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.609009">
<title confidence="0.9996335">A Comparison between Dialog Corpora with Real and Simulated Users</title>
<author confidence="0.999802">David Griol Zoraida Callejas</author>
<author confidence="0.999802">Ram´on L´opez-C´ozar</author>
<affiliation confidence="0.9306145">Departamento de Informdtica Dpto. Lenguajes y Sistemas Informdticos Universidad Carlos III de Madrid Universidad de Granada</affiliation>
<abstract confidence="0.9812380625">In this paper, we test the applicability of a stochastic user simulation technique to generate dialogs which are similar to real human-machine spoken interactions. To do so, we present a comparison between two corpora employing a comprehensive set of evaluation measures. The first corpus was acquired from real interactions of users with a spoken dialog system, whereas the second was generated by means of the simulation technique, which decides the next user answer taking into account the previous user turns, the last system answer and the objective of the dialog.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Ai</author>
<author>D Litman</author>
</authors>
<title>Comparing Real-Real, Simulated-Simulated, and Simulated-Real Spoken Dialogue Corpora.</title>
<date>2006</date>
<booktitle>In Procs. ofAAAI Workshop Statistical and Empirical Approaches for Spoken Dialogue Systems,</booktitle>
<location>Boston, USA.</location>
<contexts>
<context position="3435" citStr="Ai and Litman, 2006" startWordPosition="550" endWordPosition="553">s of the interaction of the dialog manager and the user simulator, an initial dialog corpus can be extended by increasing its variability and detecting dialog situations in which the dialog manager does not provide an appropriate answer. We propose the use of this corpus for evaluating both our user simulation technique and our dialog system performance. Different studies have been carried out to compare corpora acquired by means of different techniques and to define the most suitable measures to carry out this evaluation (Schatzmann et al., 2005), (Turunen et al., 2006), (Ai et al., 2007b), (Ai and Litman, 2006), (Ai and Litman, 2007), (Ai et al., 2007a). In this work, we have applied our dialog simulation technique to acquire a corpus in the academic domain, and compared it with a corpus recorded from real users interactions with a spoProceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 326–332, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 326 ken dialog system The results of this comparison show that the simulated corpus obtained is very similar to the corpus recorded from real user i</context>
</contexts>
<marker>Ai, Litman, 2006</marker>
<rawString>H. Ai and D. Litman. 2006. Comparing Real-Real, Simulated-Simulated, and Simulated-Real Spoken Dialogue Corpora. In Procs. ofAAAI Workshop Statistical and Empirical Approaches for Spoken Dialogue Systems, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ai</author>
<author>D J Litman</author>
</authors>
<title>Knowledge Consistent User Simulations for Dialog Systems.</title>
<date>2007</date>
<booktitle>In Proc. of Interspeech’07,</booktitle>
<pages>2697--2700</pages>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="3458" citStr="Ai and Litman, 2007" startWordPosition="554" endWordPosition="557"> the dialog manager and the user simulator, an initial dialog corpus can be extended by increasing its variability and detecting dialog situations in which the dialog manager does not provide an appropriate answer. We propose the use of this corpus for evaluating both our user simulation technique and our dialog system performance. Different studies have been carried out to compare corpora acquired by means of different techniques and to define the most suitable measures to carry out this evaluation (Schatzmann et al., 2005), (Turunen et al., 2006), (Ai et al., 2007b), (Ai and Litman, 2006), (Ai and Litman, 2007), (Ai et al., 2007a). In this work, we have applied our dialog simulation technique to acquire a corpus in the academic domain, and compared it with a corpus recorded from real users interactions with a spoProceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 326–332, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 326 ken dialog system The results of this comparison show that the simulated corpus obtained is very similar to the corpus recorded from real user interactions in terms of</context>
</contexts>
<marker>Ai, Litman, 2007</marker>
<rawString>H. Ai and D.J. Litman. 2007. Knowledge Consistent User Simulations for Dialog Systems. In Proc. of Interspeech’07, pages 2697–2700, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ai</author>
<author>A Raux</author>
<author>D Bohus</author>
<author>M Eskenazi</author>
<author>D Litman</author>
</authors>
<title>Comparing Spoken Dialog Corpora Collected with Recruited Subjects versus Real Users.</title>
<date>2007</date>
<booktitle>In Proc. of the SIGdial’07,</booktitle>
<pages>124--131</pages>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="3410" citStr="Ai et al., 2007" startWordPosition="546" endWordPosition="549">ral network. By means of the interaction of the dialog manager and the user simulator, an initial dialog corpus can be extended by increasing its variability and detecting dialog situations in which the dialog manager does not provide an appropriate answer. We propose the use of this corpus for evaluating both our user simulation technique and our dialog system performance. Different studies have been carried out to compare corpora acquired by means of different techniques and to define the most suitable measures to carry out this evaluation (Schatzmann et al., 2005), (Turunen et al., 2006), (Ai et al., 2007b), (Ai and Litman, 2006), (Ai and Litman, 2007), (Ai et al., 2007a). In this work, we have applied our dialog simulation technique to acquire a corpus in the academic domain, and compared it with a corpus recorded from real users interactions with a spoProceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 326–332, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 326 ken dialog system The results of this comparison show that the simulated corpus obtained is very similar to the corpus </context>
<context position="18237" citStr="Ai et al., 2007" startWordPosition="3014" endWordPosition="3017">ince only the dialogs that fulfill the objectives predefined in the scenarios have been incorporated into our corpora. We have considered successful dialogs those that fulfill the complete list of objectives defined in the corresponding scenario. Figure 3 summarizes the complete set of measures used in the evaluation. 5 Evaluation Results To compare the two corpora, we have computed the mean value for each corpus with respect to each of the evaluation measures shown in the previous section. Then two-tailed t-tests have been employed to compare the means across the two corpora as described in (Ai et al., 2007a). All differences reported as statistically significant have p-values less than 0.05 after Bonferroni corrections. 5.1 High-level Dialog Features As stated in the previous section, the first group of experiments covers the following statistical properties: i) Dialog length in terms of the average number of turns per dialog, number of turns of the shortest dialog, number of turns of the longest dialog, and number of turns of the most seen dialog; ii) Number of different dialogs in each corpus in terms of the percentage of different dialogs and the number of repetitions of the most seen dialog</context>
</contexts>
<marker>Ai, Raux, Bohus, Eskenazi, Litman, 2007</marker>
<rawString>H. Ai, A. Raux, D. Bohus, M. Eskenazi, and D. Litman. 2007a. Comparing Spoken Dialog Corpora Collected with Recruited Subjects versus Real Users. In Proc. of the SIGdial’07, pages 124–131, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ai</author>
<author>J R Tetreault</author>
<author>D J Litman</author>
</authors>
<title>Comparing User Simulation Models For Dialog Strategy Learning.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL HLT’07,</booktitle>
<pages>1--4</pages>
<location>Rochester, NY, USA.</location>
<contexts>
<context position="3410" citStr="Ai et al., 2007" startWordPosition="546" endWordPosition="549">ral network. By means of the interaction of the dialog manager and the user simulator, an initial dialog corpus can be extended by increasing its variability and detecting dialog situations in which the dialog manager does not provide an appropriate answer. We propose the use of this corpus for evaluating both our user simulation technique and our dialog system performance. Different studies have been carried out to compare corpora acquired by means of different techniques and to define the most suitable measures to carry out this evaluation (Schatzmann et al., 2005), (Turunen et al., 2006), (Ai et al., 2007b), (Ai and Litman, 2006), (Ai and Litman, 2007), (Ai et al., 2007a). In this work, we have applied our dialog simulation technique to acquire a corpus in the academic domain, and compared it with a corpus recorded from real users interactions with a spoProceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 326–332, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 326 ken dialog system The results of this comparison show that the simulated corpus obtained is very similar to the corpus </context>
<context position="18237" citStr="Ai et al., 2007" startWordPosition="3014" endWordPosition="3017">ince only the dialogs that fulfill the objectives predefined in the scenarios have been incorporated into our corpora. We have considered successful dialogs those that fulfill the complete list of objectives defined in the corresponding scenario. Figure 3 summarizes the complete set of measures used in the evaluation. 5 Evaluation Results To compare the two corpora, we have computed the mean value for each corpus with respect to each of the evaluation measures shown in the previous section. Then two-tailed t-tests have been employed to compare the means across the two corpora as described in (Ai et al., 2007a). All differences reported as statistically significant have p-values less than 0.05 after Bonferroni corrections. 5.1 High-level Dialog Features As stated in the previous section, the first group of experiments covers the following statistical properties: i) Dialog length in terms of the average number of turns per dialog, number of turns of the shortest dialog, number of turns of the longest dialog, and number of turns of the most seen dialog; ii) Number of different dialogs in each corpus in terms of the percentage of different dialogs and the number of repetitions of the most seen dialog</context>
</contexts>
<marker>Ai, Tetreault, Litman, 2007</marker>
<rawString>H. Ai, J.R. Tetreault, and D.J. Litman. 2007b. Comparing User Simulation Models For Dialog Strategy Learning. In Proc. of NAACL HLT’07, pages 1–4, Rochester, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Callejas</author>
<author>R L´opez-C´ozar</author>
</authors>
<title>Implementing modular dialogue systems: a case study.</title>
<date>2005</date>
<booktitle>In Proc. of Applied Spoken Language Interaction in Distributed Environments (ASIDE’05),</booktitle>
<location>Aalborg, Denmark.</location>
<marker>Callejas, L´opez-C´ozar, 2005</marker>
<rawString>Z. Callejas and R. L´opez-C´ozar. 2005. Implementing modular dialogue systems: a case study. In Proc. of Applied Spoken Language Interaction in Distributed Environments (ASIDE’05), Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Callejas</author>
<author>R L´opez-C´ozar</author>
</authors>
<title>Automatic creation of ASR grammar rules for unknown vocabulary applications.</title>
<date>2007</date>
<booktitle>In Proc. of the 8th International workshop on Electronics, Control, Modelling, Measurement and Signals (ECMS’07),</booktitle>
<pages>50--55</pages>
<location>Liberec, Czech Republic.</location>
<marker>Callejas, L´opez-C´ozar, 2007</marker>
<rawString>Z. Callejas and R. L´opez-C´ozar. 2007. Automatic creation of ASR grammar rules for unknown vocabulary applications. In Proc. of the 8th International workshop on Electronics, Control, Modelling, Measurement and Signals (ECMS’07), pages 50–55, Liberec, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Callejas</author>
<author>R L´opez-C´ozar</author>
</authors>
<title>Relations between de-facto criteria in the evaluation of a spoken dialogue system.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>8</issue>
<pages>9--646</pages>
<marker>Callejas, L´opez-C´ozar, 2008</marker>
<rawString>Z. Callejas and R. L´opez-C´ozar. 2008. Relations between de-facto criteria in the evaluation of a spoken dialogue system. Speech Communication, 50(8– 9):646–665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cuay´ahuitl</author>
<author>S Renals</author>
<author>O Lemon</author>
<author>H Shimodaira</author>
</authors>
<title>Learning Multi-Goal Dialogue Strategies Using Reinforcement Learning with Reduced State-Action Spaces.</title>
<date>2006</date>
<booktitle>In Proc. of the 9th International Conference on Spoken Language Processing (Interspeech/ICSLP),</booktitle>
<pages>469--472</pages>
<location>Pittsburgh (USA).</location>
<marker>Cuay´ahuitl, Renals, Lemon, Shimodaira, 2006</marker>
<rawString>H. Cuay´ahuitl, S. Renals, O. Lemon, and H. Shimodaira. 2006. Learning Multi-Goal Dialogue Strategies Using Reinforcement Learning with Reduced State-Action Spaces. In Proc. of the 9th International Conference on Spoken Language Processing (Interspeech/ICSLP), pages 469–472, Pittsburgh (USA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Esteve</author>
<author>C Raymond</author>
<author>F Bechet</author>
<author>R De Mori</author>
</authors>
<title>Conceptual Decoding for Spoken Dialog systems.</title>
<date>2003</date>
<booktitle>In Proc. of European Conference on Speech Communications and Technology (Eurospeech’03),</booktitle>
<volume>1</volume>
<pages>617--620</pages>
<location>Geneva</location>
<marker>Esteve, Raymond, Bechet, De Mori, 2003</marker>
<rawString>Y. Esteve, C. Raymond, F. Bechet, and R. De Mori. 2003. Conceptual Decoding for Spoken Dialog systems. In Proc. of European Conference on Speech Communications and Technology (Eurospeech’03), volume 1, pages 617–620, Geneva (Switzerland).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Georgila</author>
<author>J Henderson</author>
<author>O Lemon</author>
</authors>
<title>User Simulation for Spoken Dialogue Systems: Learning and Evaluation.</title>
<date>2006</date>
<booktitle>In Proc. of the 9th International Conference on Spoken Language Processing (Interspeech/ICSLP),</booktitle>
<pages>1065--1068</pages>
<location>Pittsburgh (USA).</location>
<contexts>
<context position="1159" citStr="Georgila et al., 2006" startWordPosition="174" endWordPosition="177">prehensive set of evaluation measures. The first corpus was acquired from real interactions of users with a spoken dialog system, whereas the second was generated by means of the simulation technique, which decides the next user answer taking into account the previous user turns, the last system answer and the objective of the dialog. 1 Introduction During the last decade, there has been a growing interest in learning corpus-based approaches for the different components of spoken dialog systems (Minker, 1999), (Young, 2002), (Esteve et al., 2003), (He and Young, 2003), (Torres et al., 2005), (Georgila et al., 2006), (Williams and Young, 2007). One of the most relevant areas of study has been the automatic generation of dialogs between the dialog manager and an additional module, called the user simulator, which generates automatic interactions with the dialog system. A considerable effort is necessary to acquire and label a corpus with the data necessary to train good models. User simulators make it possible to generate a large number of dialogs in a very simple way, reducing the time and effort needed for the evaluation of a dialog system each time the system is modified. The construction of user model</context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2006</marker>
<rawString>K. Georgila, J. Henderson, and O. Lemon. 2006. User Simulation for Spoken Dialogue Systems: Learning and Evaluation. In Proc. of the 9th International Conference on Spoken Language Processing (Interspeech/ICSLP), pages 1065–1068, Pittsburgh (USA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Griol</author>
<author>L F Hurtado</author>
<author>E Segarra</author>
<author>E Sanchis</author>
</authors>
<title>A Statistical Approach to Spoken Dialog Systems Design and Evaluation. Speech Communication,</title>
<date>2008</date>
<pages>50--8</pages>
<contexts>
<context position="7404" citStr="Griol et al., 2008" startWordPosition="1219" endWordPosition="1222">l me the name of the lecturer? S5: The name of the lecturer of the Group D of Language Processors I is David Griol. Anything else? U5: No, thank you. S6: Thank you for using the UAH system. Figure 1: An example of a dialog acquired with the interaction of the UAH system with real users 3 The Statistical User Simulation Technique In the UAH spoken dialogue system, the user simulator developed replaces the functions performed by the ASR and the NLU modules. The methodology that we have developed for user simulation extends our work for developing a statistical methodology for dialog management (Griol et al., 2008). The user answers are generated taking into account the information provided by the simulator throughout the history of the dialog, the last system turn, and the objective(s) predefined for the dialog. A labeled corpus of dialogs is used to estimate the user model. The formal description of the proposed model is as follows: Let AZ be the output of the dialog system (the system answer) at time i, expressed in terms of dialog acts. Let UZ be the semantic representation of the user turn. We represent a dialog as a sequence of pairs (system-turn, user-turn): 327 Category Information provided by t</context>
</contexts>
<marker>Griol, Hurtado, Segarra, Sanchis, 2008</marker>
<rawString>D. Griol, L.F. Hurtado, E. Segarra, and E. Sanchis. 2008. A Statistical Approach to Spoken Dialog Systems Design and Evaluation. Speech Communication, 50(8–9):666–682.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>S Young</author>
</authors>
<title>A data-driven spoken language understanding system.</title>
<date>2003</date>
<booktitle>In Proc. of IEEEAutomatic Speech Recognition and Understanding Workshop (ASRU’03),</booktitle>
<pages>583--588</pages>
<publisher>St. Thomas (U.S. Virgin Islands).</publisher>
<contexts>
<context position="1111" citStr="He and Young, 2003" startWordPosition="166" endWordPosition="169">omparison between two corpora employing a comprehensive set of evaluation measures. The first corpus was acquired from real interactions of users with a spoken dialog system, whereas the second was generated by means of the simulation technique, which decides the next user answer taking into account the previous user turns, the last system answer and the objective of the dialog. 1 Introduction During the last decade, there has been a growing interest in learning corpus-based approaches for the different components of spoken dialog systems (Minker, 1999), (Young, 2002), (Esteve et al., 2003), (He and Young, 2003), (Torres et al., 2005), (Georgila et al., 2006), (Williams and Young, 2007). One of the most relevant areas of study has been the automatic generation of dialogs between the dialog manager and an additional module, called the user simulator, which generates automatic interactions with the dialog system. A considerable effort is necessary to acquire and label a corpus with the data necessary to train good models. User simulators make it possible to generate a large number of dialogs in a very simple way, reducing the time and effort needed for the evaluation of a dialog system each time the sy</context>
</contexts>
<marker>He, Young, 2003</marker>
<rawString>Y. He and S. Young. 2003. A data-driven spoken language understanding system. In Proc. of IEEEAutomatic Speech Recognition and Understanding Workshop (ASRU’03), pages 583–588, St. Thomas (U.S. Virgin Islands).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L´opez-C´ozar</author>
<author>Z Callejas</author>
<author>M McTear</author>
</authors>
<title>Testing the performance of spoken dialogue systems by means of an artificially simulated user.</title>
<date>2006</date>
<journal>Artificial Intelligence Review,</journal>
<pages>26--291</pages>
<marker>L´opez-C´ozar, Callejas, McTear, 2006</marker>
<rawString>R. L´opez-C´ozar, Z. Callejas, and M. McTear. 2006. Testing the performance of spoken dialogue systems by means of an artificially simulated user. Artificial Intelligence Review, 26:291–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Minker</author>
</authors>
<title>Stocastically-based semantic analysis. In</title>
<date>1999</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston (USA).</location>
<contexts>
<context position="1051" citStr="Minker, 1999" startWordPosition="158" endWordPosition="159">-machine spoken interactions. To do so, we present a comparison between two corpora employing a comprehensive set of evaluation measures. The first corpus was acquired from real interactions of users with a spoken dialog system, whereas the second was generated by means of the simulation technique, which decides the next user answer taking into account the previous user turns, the last system answer and the objective of the dialog. 1 Introduction During the last decade, there has been a growing interest in learning corpus-based approaches for the different components of spoken dialog systems (Minker, 1999), (Young, 2002), (Esteve et al., 2003), (He and Young, 2003), (Torres et al., 2005), (Georgila et al., 2006), (Williams and Young, 2007). One of the most relevant areas of study has been the automatic generation of dialogs between the dialog manager and an additional module, called the user simulator, which generates automatic interactions with the dialog system. A considerable effort is necessary to acquire and label a corpus with the data necessary to train good models. User simulators make it possible to generate a large number of dialogs in a very simple way, reducing the time and effort n</context>
</contexts>
<marker>Minker, 1999</marker>
<rawString>W. Minker. 1999. Stocastically-based semantic analysis. In Kluwer Academic Publishers, Boston (USA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Pietquin</author>
<author>T Dutoit</author>
</authors>
<title>A probabilistic framework for dialog simulation and optimal strategy learning.</title>
<date>2005</date>
<booktitle>In IEEE Transactions on Speech and Audio Processing, Special Issue on Data Mining of Speech, Audio and Dialog,</booktitle>
<volume>14</volume>
<pages>589--599</pages>
<contexts>
<context position="2332" citStr="Pietquin and Dutoit, 2005" startWordPosition="365" endWordPosition="368">he system is modified. The construction of user models based on statistical methods has provided interesting and wellfounded results in recent years and is currently a growing research area. A probabilistic user model can be trained from a corpus of human-computer dialogs to simulate user answers. Therefore, it can be used to learn a dialog strategy by means of its interaction with the dialog manager. In the literature, there are several corpus-based approaches for developing user simulators, learning optimal management strategies, and evaluating the dialog system (Scheffler and Young, 2001) (Pietquin and Dutoit, 2005) (Georgila et al., 2006) (Cuaydhuitl et al., 2006) (L´opez-C´ozar et al., 2006). A summary of user simulation techniques for reinforcement learning of the dialog strategy can be found in (Schatzmann et al., 2006). In this paper, we propose a statistical approach to acquire a labeled dialog corpus from the interaction of a user simulator and a dialog manager. In our methodology, the new user turn is selected using the probability distribution provided by a neural network. By means of the interaction of the dialog manager and the user simulator, an initial dialog corpus can be extended by increa</context>
</contexts>
<marker>Pietquin, Dutoit, 2005</marker>
<rawString>O. Pietquin and T. Dutoit. 2005. A probabilistic framework for dialog simulation and optimal strategy learning. In IEEE Transactions on Speech and Audio Processing, Special Issue on Data Mining of Speech, Audio and Dialog, volume 14, pages 589– 599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K Georgila</author>
<author>S Young</author>
</authors>
<title>Quantitative Evaluation of User Simulation Techniques for Spoken Dialogue Systems.</title>
<date>2005</date>
<booktitle>In Proc. of SIGdial’05,</booktitle>
<pages>45--54</pages>
<location>Lisbon</location>
<contexts>
<context position="3368" citStr="Schatzmann et al., 2005" startWordPosition="538" endWordPosition="541">sing the probability distribution provided by a neural network. By means of the interaction of the dialog manager and the user simulator, an initial dialog corpus can be extended by increasing its variability and detecting dialog situations in which the dialog manager does not provide an appropriate answer. We propose the use of this corpus for evaluating both our user simulation technique and our dialog system performance. Different studies have been carried out to compare corpora acquired by means of different techniques and to define the most suitable measures to carry out this evaluation (Schatzmann et al., 2005), (Turunen et al., 2006), (Ai et al., 2007b), (Ai and Litman, 2006), (Ai and Litman, 2007), (Ai et al., 2007a). In this work, we have applied our dialog simulation technique to acquire a corpus in the academic domain, and compared it with a corpus recorded from real users interactions with a spoProceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 326–332, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 326 ken dialog system The results of this comparison show that the simulated corp</context>
<context position="15140" citStr="Schatzmann et al., 2005" startWordPosition="2522" endWordPosition="2525">swer:Office) (New-Query) {The office of Ram´on L´opez-C´ozar is located in the third floor of ...} Anything else? U5: (Question:Registration) [0.9] Degree: Computer Science[0.9] How to register in Computer Science. S6: (Answer:Registration) (New-Query) {Registration information} Anything else? U6: Negation [0.9] No. S7: (Closing) Thank you for using the UAH system. Figure 2: An example of a dialog acquired by means of the simulation technique 4 Evaluation of the Corpora We used a set of measures to carry out the evaluation of the acquired corpora based on prior work in the dialog literature. (Schatzmann et al., 2005) proposed a comprehensive set of quantitative evaluation measures to compare two dialog corpora. These measures were adapted for our purpose and can be divided into three types: 329 High-level dialog features Average number of turns per dialog Percentage of different dialogs Number of repetitions of the most seen dialog Number of turns of the most seen dialog Number of turns of the shortest dialog Number of turns of the longest dialog Dialog style/cooperativeness measures System dialog acts: Confirmation of concepts and attributes, Questions to require information, and Answers generated after </context>
</contexts>
<marker>Schatzmann, Georgila, Young, 2005</marker>
<rawString>J. Schatzmann, K. Georgila, and S. Young. 2005. Quantitative Evaluation of User Simulation Techniques for Spoken Dialogue Systems. In Proc. of SIGdial’05, pages 45–54, Lisbon (Portugal).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>M Stuttle</author>
<author>S Young</author>
</authors>
<title>A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Dialogue Management Strategies.</title>
<date>2006</date>
<journal>In Knowledge Engineering Review,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>97--126</pages>
<contexts>
<context position="2544" citStr="Schatzmann et al., 2006" startWordPosition="400" endWordPosition="403">del can be trained from a corpus of human-computer dialogs to simulate user answers. Therefore, it can be used to learn a dialog strategy by means of its interaction with the dialog manager. In the literature, there are several corpus-based approaches for developing user simulators, learning optimal management strategies, and evaluating the dialog system (Scheffler and Young, 2001) (Pietquin and Dutoit, 2005) (Georgila et al., 2006) (Cuaydhuitl et al., 2006) (L´opez-C´ozar et al., 2006). A summary of user simulation techniques for reinforcement learning of the dialog strategy can be found in (Schatzmann et al., 2006). In this paper, we propose a statistical approach to acquire a labeled dialog corpus from the interaction of a user simulator and a dialog manager. In our methodology, the new user turn is selected using the probability distribution provided by a neural network. By means of the interaction of the dialog manager and the user simulator, an initial dialog corpus can be extended by increasing its variability and detecting dialog situations in which the dialog manager does not provide an appropriate answer. We propose the use of this corpus for evaluating both our user simulation technique and our</context>
</contexts>
<marker>Schatzmann, Weilhammer, Stuttle, Young, 2006</marker>
<rawString>J. Schatzmann, K. Weilhammer, M. Stuttle, and S. Young. 2006. A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Dialogue Management Strategies. In Knowledge Engineering Review, volume 21(2), pages 97–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Scheffler</author>
<author>S Young</author>
</authors>
<title>Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning.</title>
<date>2001</date>
<booktitle>In Proc. of HLT’02,</booktitle>
<pages>12--18</pages>
<location>San Diego (USA).</location>
<contexts>
<context position="2304" citStr="Scheffler and Young, 2001" startWordPosition="361" endWordPosition="364"> a dialog system each time the system is modified. The construction of user models based on statistical methods has provided interesting and wellfounded results in recent years and is currently a growing research area. A probabilistic user model can be trained from a corpus of human-computer dialogs to simulate user answers. Therefore, it can be used to learn a dialog strategy by means of its interaction with the dialog manager. In the literature, there are several corpus-based approaches for developing user simulators, learning optimal management strategies, and evaluating the dialog system (Scheffler and Young, 2001) (Pietquin and Dutoit, 2005) (Georgila et al., 2006) (Cuaydhuitl et al., 2006) (L´opez-C´ozar et al., 2006). A summary of user simulation techniques for reinforcement learning of the dialog strategy can be found in (Schatzmann et al., 2006). In this paper, we propose a statistical approach to acquire a labeled dialog corpus from the interaction of a user simulator and a dialog manager. In our methodology, the new user turn is selected using the probability distribution provided by a neural network. By means of the interaction of the dialog manager and the user simulator, an initial dialog corp</context>
</contexts>
<marker>Scheffler, Young, 2001</marker>
<rawString>K. Scheffler and S. Young. 2001. Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning. In Proc. of HLT’02, pages 12–18, San Diego (USA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Torres</author>
<author>L F Hurtado</author>
<author>F Garc´ıa</author>
<author>E Sanchis</author>
<author>E Segarra</author>
</authors>
<title>Error handling in a stochastic dialog system through confidence measures. In Speech Communication,</title>
<date>2005</date>
<pages>45--211</pages>
<marker>Torres, Hurtado, Garc´ıa, Sanchis, Segarra, 2005</marker>
<rawString>F. Torres, L.F. Hurtado, F. Garc´ıa, E. Sanchis, and E. Segarra. 2005. Error handling in a stochastic dialog system through confidence measures. In Speech Communication, pages (45):211–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Turunen</author>
<author>J Hakulinen</author>
<author>A Kainulainen</author>
</authors>
<title>Evaluation of a Spoken Dialogue System with Usability Tests and Long-term Pilot Studies: Similarities and Differences.</title>
<date>2006</date>
<booktitle>In Proc. of the 9th International Conference on Spoken Language Processing (Interspeech/ICSLP),</booktitle>
<pages>1057--1060</pages>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="3392" citStr="Turunen et al., 2006" startWordPosition="542" endWordPosition="545">bution provided by a neural network. By means of the interaction of the dialog manager and the user simulator, an initial dialog corpus can be extended by increasing its variability and detecting dialog situations in which the dialog manager does not provide an appropriate answer. We propose the use of this corpus for evaluating both our user simulation technique and our dialog system performance. Different studies have been carried out to compare corpora acquired by means of different techniques and to define the most suitable measures to carry out this evaluation (Schatzmann et al., 2005), (Turunen et al., 2006), (Ai et al., 2007b), (Ai and Litman, 2006), (Ai and Litman, 2007), (Ai et al., 2007a). In this work, we have applied our dialog simulation technique to acquire a corpus in the academic domain, and compared it with a corpus recorded from real users interactions with a spoProceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 326–332, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 326 ken dialog system The results of this comparison show that the simulated corpus obtained is very simi</context>
</contexts>
<marker>Turunen, Hakulinen, Kainulainen, 2006</marker>
<rawString>M. Turunen, J. Hakulinen, and A. Kainulainen. 2006. Evaluation of a Spoken Dialogue System with Usability Tests and Long-term Pilot Studies: Similarities and Differences. In Proc. of the 9th International Conference on Spoken Language Processing (Interspeech/ICSLP), pages 1057–1060, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
<author>S Young</author>
</authors>
<title>Partially Observable Markov Decision Processes for Spoken Dialog Systems.</title>
<date>2007</date>
<booktitle>In Computer Speech and Language,</booktitle>
<volume>21</volume>
<issue>2</issue>
<pages>393--422</pages>
<contexts>
<context position="1187" citStr="Williams and Young, 2007" startWordPosition="178" endWordPosition="181">ion measures. The first corpus was acquired from real interactions of users with a spoken dialog system, whereas the second was generated by means of the simulation technique, which decides the next user answer taking into account the previous user turns, the last system answer and the objective of the dialog. 1 Introduction During the last decade, there has been a growing interest in learning corpus-based approaches for the different components of spoken dialog systems (Minker, 1999), (Young, 2002), (Esteve et al., 2003), (He and Young, 2003), (Torres et al., 2005), (Georgila et al., 2006), (Williams and Young, 2007). One of the most relevant areas of study has been the automatic generation of dialogs between the dialog manager and an additional module, called the user simulator, which generates automatic interactions with the dialog system. A considerable effort is necessary to acquire and label a corpus with the data necessary to train good models. User simulators make it possible to generate a large number of dialogs in a very simple way, reducing the time and effort needed for the evaluation of a dialog system each time the system is modified. The construction of user models based on statistical metho</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>J. Williams and S. Young. 2007. Partially Observable Markov Decision Processes for Spoken Dialog Systems. In Computer Speech and Language, volume 21(2), pages 393–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
</authors>
<title>The Statistical Approach to the Design of Spoken Dialogue Systems.</title>
<date>2002</date>
<tech>Technical report, CUED/F-INFENG/TR.433,</tech>
<institution>Cambridge University Engineering Department,</institution>
<location>Cambridge (UK).</location>
<contexts>
<context position="1066" citStr="Young, 2002" startWordPosition="160" endWordPosition="161">interactions. To do so, we present a comparison between two corpora employing a comprehensive set of evaluation measures. The first corpus was acquired from real interactions of users with a spoken dialog system, whereas the second was generated by means of the simulation technique, which decides the next user answer taking into account the previous user turns, the last system answer and the objective of the dialog. 1 Introduction During the last decade, there has been a growing interest in learning corpus-based approaches for the different components of spoken dialog systems (Minker, 1999), (Young, 2002), (Esteve et al., 2003), (He and Young, 2003), (Torres et al., 2005), (Georgila et al., 2006), (Williams and Young, 2007). One of the most relevant areas of study has been the automatic generation of dialogs between the dialog manager and an additional module, called the user simulator, which generates automatic interactions with the dialog system. A considerable effort is necessary to acquire and label a corpus with the data necessary to train good models. User simulators make it possible to generate a large number of dialogs in a very simple way, reducing the time and effort needed for the e</context>
</contexts>
<marker>Young, 2002</marker>
<rawString>S. Young. 2002. The Statistical Approach to the Design of Spoken Dialogue Systems. Technical report, CUED/F-INFENG/TR.433, Cambridge University Engineering Department, Cambridge (UK).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>