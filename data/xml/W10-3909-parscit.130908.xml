<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000033">
<title confidence="0.9991655">
Large Corpus-based Semantic Feature Extraction
for Pronoun Coreference
</title>
<author confidence="0.997665">
Shasha Liao Ralph Grishman
</author>
<affiliation confidence="0.76147">
Dept. of Computer Science Dept. of Computer Science
New York University New York University
</affiliation>
<email confidence="0.999665">
liaoss@cs.nyu.edu grishman@cs.nyu.edu
</email>
<sectionHeader confidence="0.995652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972766666667">
Semantic information is a very important
factor in coreference resolution. The
combination of large corpora and ‘deep’
analysis procedures has made it possible
to acquire a range of semantic informa-
tion and apply it to this task. In this pa-
per, we generate two statistically-based
semantic features from a large corpus and
measure their influence on pronoun
coreference. One is contextual compati-
bility, which decides if the antecedent
can be used in the anaphor’s context; the
other is role pair, which decides if the ac-
tions asserted of the antecedent and the
anaphor are likely to apply to the same
entity. We apply a semantic labeling sys-
tem and a baseline coreference system to
a large corpus to generate semantic pat-
terns and convert them into features in a
MaxEnt model. These features produce
an absolute gain of 1.5% to 1.7% in reso-
lution accuracy (a 6% reduction in er-
rors). To understand the limitations of
these features, we also extract patterns
from the test corpus, use these patterns to
train a coreference model, and examine
some of the cases where coreference still
fails. We also compare the performance
of patterns extracted from semantic role
labeling and syntax.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972">
Coreference resolution is the task of determining
whether two phrases refer to the same entity.
Coreference is critical to most NLP tasks, yet
even the sub-problem of pronoun coreference
remains very challenging. In principle, we need
several types of information to identify the right
antecedent. First, number and gender agreement
constraints can narrow the candidate set. If mul-
tiple candidates remain, we would next use some
sequence or syntactic features, like position,
word, word salience and discourse focus. For
example, whether an antecedent is in subject po-
sition might be helpful because the subject is
more likely to be referred to; or an entity that has
been referred to repeatedly is more likely to be
referred to again. However, these features do not
suffice to pick the correct antecedent, and some-
times similar syntactic structures might have
quite different coreference solutions. For exam-
ple, for the following two sentences:
</bodyText>
<listItem confidence="0.9880265">
(1) The terrorist shot a 13-year-old boy; he was
arrested after the attack.
(2) The terrorist shot a 13-year-old boy; he was
fatally wounded in the attack.
</listItem>
<bodyText confidence="0.999309615384615">
it is likely that “he” refers to “terrorist” in (1)
and “boy” in (2). However, we cannot get the
right antecedent using the features we mentioned
above because the examples share the same ante-
cedent words and syntactic structure. People can
still resolve these correctly because “terrorist” is
more likely to be arrested than “boy”, and be-
cause the one shooting is more likely to be ar-
rested than the one being shot.
In such cases, semantic constraints and prefer-
ences are required for correct coreference resolu-
tion. Methods for acquiring and using such
knowledge are receiving increasing attention in
</bodyText>
<page confidence="0.983257">
60
</page>
<note confidence="0.6176">
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60–68,
Beijing, August 2010
</note>
<bodyText confidence="0.999908515151515">
recent work on anaphora resolution. Dagan and
Itai (1990), Bean and Riloff (2004), Yang and Su
(2007), and Ponzetto and Strube (2006) all ex-
plored this task.
However, this task is difficult because it re-
quires the acquisition of a large amount of se-
mantic information. Furthermore, there is not
universal agreement on the value of these seman-
tic preferences for pronoun coreference. Kehler
et al. (2004) reported that such information did
not produce apparent improvement in overall
pronoun resolution.
In this paper, we will extract semantic features
from a semantic role labeling system instead of a
parse tree, and explore whether pronoun corefer-
ence resolution can benefit from such knowledge,
which is automatically extracted from a large
corpus. We studied two features: the contextual
compatibility feature which has been demon-
strated to work at the syntactic level by previous
work; and the role pair feature, which has not
previously been applied to general domain pro-
noun co-reference. In addition, to obtain a rough
upper bound on the benefits of our approach and
understand its limitations, we conducted a second
experiment in which the semantic knowledge is
extracted from the evaluation corpus.
We will use the term mention to describe an
individual referring phrase. For most studies of
coreference, mentions are noun phrases and may
be headed by a name, a common noun, or a pro-
noun. We will use the term entity to refer to a set
of coreferential mentions.
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999791448275862">
Contextual compatibility features have long been
studied for pronoun coreference: Dagan and Itai
(1990) proposed a heuristics-based approach to
pronoun resolution. It determined the preference
of candidates based on predicate-argument fre-
quencies.
Bean and Riloff (2004) present a system,
which uses contextual role knowledge to aid
coreference resolution. They used lexical and
syntactic heuristics to identify high-confidence
coreference relations and used them as training
data for learning contextual role knowledge.
They got substantial gains on articles in two spe-
cific domains, terrorism and natural disasters.
Yang et al. (2005) use statistically-based se-
mantic compatibility information to improve
pronoun resolution. They use corpus-based and
web-based extraction strategies, and their work
shows that statistically-based semantic compati-
bility information can improve coreference reso-
lution.
In contrast, Kehler et al. (2004) claimed that
the contextual compatibility feature does not help
much for pronoun coreference: existing learning-
based approaches already performed well; such
statistics are simply not good predictors for pro-
noun interpretation; data is sparse in the collected
predicate-argument statistics.
The role pair feature has not been studied for
general, broad-domain pronoun co-reference, but
it has been used for other tasks: Pekar (2006)
built pairs of &apos;templates&apos; which share an &apos;anchor&apos;
argument; these correspond closely to our role
pairs. Association statistics of the template pairs
were used to acquire verb entailments. Abe et al.
(2008) looked for pairs appearing in specific syn-
tactic patterns in order to acquire finer-grained
event relations. Chambers and Jurafsky (2008)
built narrative event chains, which are partially
ordered sets of events related by a common pro-
tagonist. They use high-precision hand-coded
rules to get coreference information, extract
predicate arguments that link the mentions to
verbs, and link the arguments of the coreferred
mentions to build a verb entailment model.
Bean and Riloff (2004) used high-precision
hand-coded rules to identify coreferent mention
pairs, which are then used to acquire role pairs
that they refer to as Caseframe Network features.
They use these features to improve coreference
resolution for two domain-specific corpora in-
volving terrorism and natural disasters. Their
result raises the natural question as to whether
the approach (which may capture domain-
specific pairs such as “kidnap—release” in the
terrorism domain) can be successfully extended
to a general news corpus. We address this ques-
tion in the experiments reported here.
</bodyText>
<sectionHeader confidence="0.961675" genericHeader="method">
3 Corpus Analysis
</sectionHeader>
<bodyText confidence="0.999793714285714">
In order to extract semantic features from our
large training corpus, we apply a sequence of
analyzers. These include name tagging, parsing,
a baseline coreference analyzer, and, most im-
portant, a semantic labeling system that can gen-
erate the logical grammatical and predicate-
argument representation automatically from a
</bodyText>
<page confidence="0.998642">
61
</page>
<bodyText confidence="0.99989424">
parse tree (Meyers et al. 2009). We use semantic
labeling because it provides more general and
meaningful patterns, with a “deeper” analysis
than parsed text. The output of the semantic la-
beling is the dependency representation of the
text, where each sentence is a graph consisting of
nodes (corresponding to words) and arcs. Each
arc captures up to three relations between two
words: (1) a SURFACE relation, the relation be-
tween a predicate and an argument in the parse of
a sentence; (2) a LOGIC1 (grammatical logical)
relation which regularizes for lexical and syntac-
tic phenomena like passive, relative clauses, and
deleted subjects; and (3) a LOGIC2 (predicate-
argument) relation corresponding to relations in
PropBank and NomBank. It is designed to be
compatible with the Penn TreeBank (Marcus et
al., 1994) framework and therefore, Penn Tree-
Bank-based parsers, while incorporating Named
Entities, PropBank, and NomBank.
Because nouns and verbs provide the most
relevant contexts and capture the events in which
the entities participate, we generate semantic pat-
terns (triples) only for those arcs with verb or
noun heads. We use the following relations:
</bodyText>
<listItem confidence="0.930157">
• Logic2 relations: We use in particular the
Arg0 relation (which corresponds roughly to
agent) and Arg1 relation (which corresponds
roughly to patient).
• Logic1 relations: We use in particular the Sbj
and Obj relations, representing the logical
subject and object of a verb (regularizing
passive, relative clauses, deleted subjects)
• Surface relations: T-pos relation is particu-
larly used, which captures the head noun –
determiner relation for possessive constructs
such as “bomber’s attack” and “his responsi-
bility”.
</listItem>
<bodyText confidence="0.971319666666667">
For example, for the sentence:
John is hit by Tom’s brother.
we generate the semantic patterns
</bodyText>
<subsubsectionHeader confidence="0.891337333333333">
&lt;Arg1 hit John&gt;
&lt;Arg0 hit brother&gt;
&lt;T-pos brother Tom&gt;
</subsubsectionHeader>
<bodyText confidence="0.999911083333333">
We apply this labeling system to all the data
we use, and to generate the semantic pattern, we
take first its predicate-argument role; if that is
null, we take its logical grammatical role; if both
are null, we take its surface role.
To reduce data sparseness, all inflected words
are changed to their base form (e.g. “attack-
ers”→“attacker”). All names are replaced by
their ACE types (person, organization, location,
etc.). Only patterns with noun arguments are ex-
tracted because we only consider noun phrases as
possible antecedents.
</bodyText>
<sectionHeader confidence="0.997154" genericHeader="method">
4 Semantic Features
</sectionHeader>
<subsectionHeader confidence="0.999769">
4.1 Contextual Compatibility Patterns
</subsectionHeader>
<bodyText confidence="0.989392333333333">
Pronouns, especially neutral pronouns (“it”,
“they”), carry little semantics of their own, so
examining the compatibility of the context of a
pronoun and its candidate antecedents is a good
way to improve antecedent selection. Specifi-
cally, we want to determine whether the predi-
cate, which is applied to the anaphor, can be ap-
plied to the antecedents. We take the semantic
pattern with the anaphor in third position. Then,
each candidate antecedent is substituted for the
anaphor to see if it is suitable for the context. For
example, consider the sentence
The company issued a statement that it
bought G.M.
which would generate the semantic patterns
</bodyText>
<equation confidence="0.50091925">
&lt;Arg0 issue company&gt;
&lt;Arg1 issue statement&gt;
&lt;Arg0 buy it&gt;
&lt;Arg1 buy Organization&gt;
</equation>
<bodyText confidence="0.9498610625">
(here “G.M” is a name of type organization and
so is replaced by the token Organization). The
relevant context of the anaphor is the semantic
pattern &lt;Arg0 buy it&gt;. Suppose there are two
candidate antecedents for “it”: “company” and
“statement”. We would generate the two seman-
tic patterns &lt;Arg0 buy company&gt; and &lt;Arg0 buy
statement&gt;. Assuming &lt;Arg0 buy company&gt; is
more highly ranked than &lt;Arg0 buy statement&gt;,
we can infer that the anaphor is more likely to
refer to “company”. (We describe the specific
metric we use for ranking below, in section 4.3.)
As further examples consider:
(3) The suspect&apos;s lawyer, Chifumu Banda, told
the court he had advised Chiluba not to ap-
pear in court Friday.
</bodyText>
<page confidence="0.94645">
62
</page>
<listItem confidence="0.97917075">
(4) Foreign military analysts said it would be
highly unusual for an accident to kill a
whole submarine crew and they suggested
possible causes to a disaster...
</listItem>
<bodyText confidence="0.999600166666667">
For (3), if we know that a lawyer is more
likely to give advice than a suspect, we could
link “he” to “lawyer” instead of “suspect” in the
first sentence. For (4), if we know that analysts
are more likely to “suggest” than crew, we can
link “they” to “analysts” in the second sentence.
</bodyText>
<subsectionHeader confidence="0.993734">
4.2 Role Pair Patterns
</subsectionHeader>
<bodyText confidence="0.999574">
The role pair pattern is a new feature in general
pronoun co-reference. The original intuition for
introducing it into coreference is that there are
pairs of actions involving the same entity that are
much more likely to occur together than would
be true if one assumed statistical independence.
The second action may be a rephrasing or elabo-
ration of the first, or the two might be actions
that are part of a common ‘script’. For example:
</bodyText>
<listItem confidence="0.97929525">
(5) Prime Minister Mahathir Mohamad sacked
the former deputy premier in 1998, who was
sentenced to a total of 15 years in jail after
being convicted of corruption and sodomy.
He was released after four years because....
(6) The robber attacked the boy with a knife; he
was bleeding heavily and died in the hospital
the next day.
</listItem>
<bodyText confidence="0.998869461538462">
For (5), if we know that the person who was
sentenced is more likely to be released than the
person who sacked others, we would know “he”
refers to “deputy premier” instead of “prime min-
ister”. And in (6), because someone being at-
tacked is more likely to die than the attacker, we
can infer that “he” refers to “boy”.
To acquire such information, we need to iden-€
tify those pairs of predicates which are likely to
apply to the same entity. We collect this data
from a large corpus. The basic process is: apply a
baseline coreference system to produce mentions
and entities for a large corpus. For every entity,
record the predicates for every mention, and then
the pairs of predicates for successive mentions
within each entity.
Although the performance of the baseline
coreference is not very high, and individual
documents may yield many idiosyncratic pairs,
we can gather many significant role pairs by col-
lecting statistics from a large corpus and filtering
out the low frequency patterns; this process can
eliminate much of the noise due to coreference
errors.
Here is an example of the extracted role pairs
involving “attack”:
</bodyText>
<table confidence="0.935292">
Obj volley x
Arg0 bombard x
Obj barrage x
Arg0 snatch x
attack x Sbj attack x
Arg0 ↔Arg0
pound x
Obj reoccupy x
Arg1 halt x
Arg0 assault x
Arg1 bombard x
Table1. Top 10 role pairs associated with
“Arg0 attack x”
</table>
<subsectionHeader confidence="0.989334">
4.3 Contextual Compatibility Scores
</subsectionHeader>
<bodyText confidence="0.999960571428571">
To properly compare the patterns involving al-
ternative candidate antecedents, we need to nor-
malize the raw frequencies first. We followed
Yang et al. (2005)’s idea, which normalizes the
pattern frequency by the frequency of the candi-
dates, and use a relative score that is normalized
by the maximum score of all its candidates:
</bodyText>
<equation confidence="0.935525">
CompScore(Pcontext,Cand )
CompFreq(Pcontext,Cand)
MaxCi∈Set(cands)CompFreq(Pcontext,Ci)
Com Fre P .req(Pcontext,Cand)
P R( context,Cand) _ freq(Cand)
</equation>
<bodyText confidence="0.984800181818182">
where Pcontezt,Canut is the contextual compatibility
pattern built from the context of the pronoun and
€
the base form of the candidate.
In contrast to Yang’s work, which used con-
textual compatibility on the mention level, we
consider the contextual compatibility of an entity
to an anaphor: we calculate the contextual in-
formation of all the mentions and choose the one
with highest score as the contextual compatibility
score for this entity1:
</bodyText>
<footnote confidence="0.839076">
1 Note that all the mentions in the entity are generated by
the overall coreference system. Also, the ACE entity type of
names is determined by the system. No key annotations are
considered in the entire coreference phase.
and
</footnote>
<page confidence="0.952809">
63
</page>
<equation confidence="0.472751">
freq(context,entity) �y
= Maxmentioni ∈entity ✓&apos; eq(Pcontext,mentioni )
</equation>
<subsectionHeader confidence="0.980489">
4.4 Role Pair Scores
</subsectionHeader>
<bodyText confidence="0.9961569">
Unlike the contextual compatibility feature, we
only take the role pair of the successive mentions
in the candidate entity and the anaphor, because
they are more reliably coreferential than arbitrary
pairs of mentions within an entity:
where and are the contextual pat-
terns of the anaphor and the last mention in the
candidate entity.
For a set of possible candidates, we compute a
relative score:
</bodyText>
<equation confidence="0.968500666666667">
PairScore(pana, pcand )
PairFreq(pana,pcand )
Maxpi∈Set(cands)PairFreq(pana, pi)
</equation>
<bodyText confidence="0.960317875">
Both scores are quantized (binned) in intervals
of 0.1 for use as MaxEnt features.
names or proper nouns, and generates entities
when it is finished. The whole is a one-pass
process, resolving coreference in the order in
which mentions appear in the document. In the
pronoun coreference process, every pronoun
mention is assigned to one of the candidate enti-
ties.
Features Description
Hobbs_ Hobbs distance between the
Distance last mention in the entity
and the anaphor
Head_Pro Combined word features of
the head of the last mention
in the entity and anaphor
Is_Subject True if the last mention in
the entity is a subject of the
sentence
Last_Cat Whether the last mention in
the entity is a noun phrase,
a pronoun or a name
Co_Prior Number of prior references
to this entity
</bodyText>
<tableCaption confidence="0.996344">
Table 2. Features used in baseline system
</tableCaption>
<sectionHeader confidence="0.996523" genericHeader="method">
5 Experiment
</sectionHeader>
<bodyText confidence="0.999963076923077">
Our coreference solution system uses ACE anno-
tated data and follows the ACE 2005 English
entity guidelines.2 The baseline coreference sys-
tem to compare with is the same one used for
extracting semantic features from the large cor-
pus. It employs an entity-mention (rather than a
mention-pair) model.
Besides entity and mention information, which
(as mentioned above) is system output, the se-
mantic information is also automatically ex-
tracted by a semantic labeling system. As a result,
we report results in section 5.4 which involve no
information from the reference (key) annotation.
</bodyText>
<subsectionHeader confidence="0.986297">
5.1 Baseline System Description
</subsectionHeader>
<bodyText confidence="0.999836666666667">
The baseline system first applies processes like
parsing, semantic labeling, name tagging, and
entity mention tagging, producing a set of men-
tions to which coreference analysis is then ap-
plied. The coreference phase deals with corefer-
ence among mentions that might be pronouns,
</bodyText>
<footnote confidence="0.5461835">
2 Automatic Content Extraction evaluation,
http://projects.ldc.upenn.edu/ace/
</footnote>
<bodyText confidence="0.999617125">
The baseline co-reference system has separate,
quite elaborate, primarily rule-based systems to
handle names, nominals, headless NP&apos;s, and ad-
verbs (&amp;quot;here&amp;quot;, &amp;quot;there&amp;quot;) which may be anaphoric,
as well as first- and second-person pronouns. The
MaxEnt model under study in this paper is only
responsible for third-person pronouns. Also,
gender, number, and human/non-human are han-
dled separately outside of the MaxEnt model,
and the model only resolves mentions that satisfy
these constraints.3 In the MaxEnt model, 5 basic
features (described in table 2) are used. Thus,
while the set of features used in the model is
relatively small in comparison to many current
statistically based reference resolvers, these are
the primary features relevant to the limited task
</bodyText>
<tableCaption confidence="0.754955571428572">
3 Gender information is obtained from a dictionary of gen-
der-specific nouns and from first-name lists from the US
Census. Number information comes from large syntactic
dictionaries, corpus annotation of collective nouns (syntac-
tically singular nouns which may take plural anaphors), and
name tagger information (some organizations and political
entities may take plural anaphors).
</tableCaption>
<page confidence="0.999235">
64
</page>
<bodyText confidence="0.998847">
of the MaxEnt model, and its performance is still
competitive4.
</bodyText>
<subsectionHeader confidence="0.997097">
5.2 Corpus Description
</subsectionHeader>
<bodyText confidence="0.983530777777778">
There are two kinds of corpora used in our ex-
periment, a small coreference-annotated corpus
used for training and evaluating (in cross-
validation) the pronoun coreference model, and a
large raw-text corpus for extracting semantic in-
formation.
For model training and evaluation, we assem-
bled two small corpora from the available ACE
data. One consists of news articles (460 docu-
ments) from ACE 2005 (330 documents) and
ACE 2003 (130 documents), which together con-
tain 3934 pronouns. The other is the full ACE
2005 training set (592 documents), which in-
cludes newswire, broadcast news, broadcast con-
versations (interviews and discussions), web logs,
web forums, and Fisher telephone transcripts,
and contains 5659 pronouns.
In evaluation, we consider a pronoun to be
correctly resolved if its antecedent in the system
output (the most recent prior mention in the en-
tity to which the pronoun is assigned) matches
the antecedent in the key. We report accuracy
(percentage of pronouns which are correctly re-
solved).
We used a large corpus to extract semantic in-
formation, consisting of five years of AFP
newswire from the LDC English Gigaword cor-
pus (1996, 2002, 2004, 2005 and 2006), a total of
907,368 documents. We omit news articles writ-
ten in 1998, 2000 and 2003 to insure there is no
overlap between the ACE data and Gigaword
data. We pre-processed each document (parsing,
name identification, and semantic labeling) and
ran the baseline coreference system, which
automatically identified all the mentions (includ-
ing name mentions and nominal mentions) and
built a set of entities for each document.
4For example, among papers reporting a pronoun accuracy
metric, Kehler et al. (2004), testing on a 2002 ACE news
corpus, get a pronoun accuracy (without semantic features)
of 75.7%; (Yang et al. 2005), testing on the MUC corefer-
ence corpora (also news) get for their single-candidate
baseline (without semantic features) 75.1% pronoun accu-
racy. Although the testing conditions in each case are dif-
ferent, these are comparable to our baseline performance.
</bodyText>
<subsectionHeader confidence="0.9953445">
5.3 Semantic Information Extraction from
Large Corpus
</subsectionHeader>
<bodyText confidence="0.999950823529412">
In order to remove noise, we only keep contex-
tual compatibility patterns that appear more than
5 times; and only keep role pair patterns which
appear more than 15 times, and appear in more
than three different years to avoid random pairs
extracted from repeated stories. We automati-
cally extracted 626,008 contextual compatibility
patterns and 4,736,359 role pairs. Note that we
extract fewer patterns than Yang (2005), who
extracted in total 2,203,203 contextual compati-
bility patterns, from a much smaller corpus
(173,252 Wall Street Journal articles). This
might be for two reasons: first, we pruned low
frequency patterns; second, we used a semantic
labeling system instead of shallow parsing. Sec-
tion 5.6 gives a comparison of pattern extraction
based on different levels of analysis.
</bodyText>
<sectionHeader confidence="0.821837" genericHeader="evaluation">
5.4 Results
</sectionHeader>
<table confidence="0.999428571428571">
News Corpus 2005 Corpus
Accu SignTest Accu SignTest
(p &lt;=) (p &lt;=)
baseline 75.54 72.04
context 76.59 0.025 73.35 0.002
role pair 76.28 0.031 73.03 0.003
combine 77.02 0.0005 73.72 0.0015
</table>
<tableCaption confidence="0.9889615">
Table 3. Accuracy of 5-fold cross-validation with sta-
tistics-based semantic features
</tableCaption>
<bodyText confidence="0.999956625">
We did a 5-fold cross validation to test the con-
tribution from statistically-based semantic fea-
tures, and report an average accuracy. All the
mentions and their features are obtained from
system output; as a result, if the correct antece-
dent is not correctly discovered and analyzed
from the previous phases, we will not be able to
co-refer the pronoun correctly. Experiments on
the news articles show that each feature provides
approximately 1% gain by itself, and contributes
to a substantial overall gain of 1.45%. For the
2005 corpus, the baseline is lower because the
documents come from different genres, and we
get more gain from each semantic feature. We
also computed the significance over the baseline
using the sign test5.
</bodyText>
<footnote confidence="0.889682">
5 In applying the sign test, we treated each pronoun as an
independent sample, which is either correctly resolved or
incorrectly resolved. Where the individual observations are
</footnote>
<page confidence="0.999">
65
</page>
<subsectionHeader confidence="0.927952">
5.5 Self-Extracted Bound
</subsectionHeader>
<bodyText confidence="0.999726041666667">
To better understand the potential maximum con-
tribution of our semantic features, we constructed
an approximation to the most favorable possible
semantic features for each test set. We did this by
using perfect coreference knowledge and by col-
lecting patterns for each test set from the test set
itself. For each corpus used for cross-validation,
we first collect all the contextual compatibility
and role pair patterns corresponding to the cor-
rect antecedents (we ignore the patterns corre-
sponding to the wrong antecedents, because we
can not get this negative information when we
extract them from a large corpus), and score
these patterns to produce semantic features for
the MaxEnt Model, both training and testing.
We then use these features in the model and do a
cross-validation as before. Also, as before, we
rely on system output to identify and analyze
potential antecedents; if the prior phases do not
do so correctly, coreference analysis may well
fail. This experiment shows that we can get
about 3 to 4% gain from each feature type sepa-
rately; 4.5 to 5.5% gain is achieved from the two
features together.
</bodyText>
<table confidence="0.999522857142857">
News Corpus 2005 Corpus
Accu SignTest Accu SignTest
(p &lt;_) (p &lt;_)
baseline 75.54 72.04
context 79.23 7e-14 76.04 9e-27
role pair 78.85 6e-13 75.95 1e-26
combine 79.97 4e-16 77.50 2e-38
</table>
<tableCaption confidence="0.9934925">
Table 4. Accuracy of 5-fold cross-validation with self-
extracted semantic features
</tableCaption>
<subsectionHeader confidence="0.9992125">
5.6 Comparison between Semantic and
Syntax Patterns
</subsectionHeader>
<bodyText confidence="0.997886888888889">
To better understand the difference between se-
mantic role labeling and syntactic relations, we
did a comparison between patterns extracted
from the syntax level and those extracted from
semantic role labeling:
Experiments show that using semantic roles
(such as Arg0 and Arg1) works better. This may
(changes in) binary outcomes, the sign test provides a suita-
bly sensitive significance test. (In particular, it is compara-
ble to performing a paired t-test over counts of correct reso-
lutions, aggregated over documents.)
be because the &amp;quot;deeper&amp;quot; representation provides
more generalization of relations. For example,
the phrases “weapon’s use” and “use weapon”
share the same semantic relation &lt;Arg1 use
weapon&gt;, while they yield different grammatical
relations: &lt;T-pos use weapon&gt; and &lt;Obj use
weapon&gt;.
</bodyText>
<tableCaption confidence="0.987804333333333">
Table 5. Accuracy of 5-fold cross-validation with self-
extracted semantic features based on different levels
of syntactic/semantic relations
</tableCaption>
<subsectionHeader confidence="0.593496">
5.7 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9995813125">
We analyzed the errors in the self-extracted re-
sults, to see why such corpus-specific semantic
features do not produce an even greater reduction
in errors. For the contextual compatibility feature,
we find cases where an incorrect candidate is
equally compatible with the context of the ana-
phor; for example, if all the candidates are person
names, they will share the same context feature
because they generate the same ACE type. In
other cases, the context does not provide enough
information. For example, in a context tuple
&lt;Arg0 get x&gt;, x can be almost any noun, because
“get” is too vague to predicate the compatible
subjects. There are similar limitations with the
role pair feature; for example, &lt;Arg0 get they&gt;
can be associated with a lot of other actions.
To quantify this problem, we counted the pat-
terns that appear in both positive examples (cor-
rect antecedents) and negative examples (incor-
rect antecedents). For contextual compatibility
patterns, 39.5% of the patterns which appear with
positive examples also appear in the negative
sample, while for role pair patterns, 19% of the
patterns which appear with positive examples
also appear in the negative sample. So we see
that, even with a pattern set highly tuned to the
test set, many patterns do not by themselves
serve to distinguish correct from incorrect
coreference.
We analyzed some of the cases where the se-
mantic information does not help, or even harms
the analysis. In some cases all the antecedent
</bodyText>
<table confidence="0.968175">
News Corpus 2005 Corpus
semantic syntax semantic syntax
baseline 75.54 72.04
context 79.23 77.73 76.04 75.83
role pair 78.85 76.87 75.95 74.17
combine 79.97 78.42 77.50 76.76
</table>
<page confidence="0.976388">
66
</page>
<bodyText confidence="0.979638">
scores are very low, either because the patterns
are very rare or the antecedent is a common word
that appears in a lot of patterns. In other cases,
several antecedents have a high compatibility
score but the correct one does not have the top
score. In these cases, the contextual compatibility
is not reliable, as was pointed out by Kehler et al.
(2004):
(7) The model for a republic, adopted over bitter
objections from those advocating direct elec-
tion of a president, is for presidential nomi-
nations to be made with public input and the
winning candidate decided by a two-thirds
majority of Parliament. Former prime minis-
ter Paul Keating, who put the republic issue
in the spotlight in his unsuccessful 1996
campaign for re-election, welcomed the re-
sult.
Here adding semantic features leads “his” to
be incorrectly resolved to “president” rather than
the entity with mentions “prime minister” and
“Paul Keating”; all the relevant patterns are
common, but the score for &lt;Arg0 campaign
president&gt; is higher (around 0.0012) than for
&lt;Arg0 campaign minister&gt; (0.0004) or &lt;Arg0
campaign Person&gt; (0.0006).
Another problem is that the patterns do not
capture enough context information, for example:
(8) The U.S. administration has been pressing
the Security Council to adopt a statement
condemning Pyongyang for failing to meet
its obligations.
If we can get the semantic context of “fail to
meet its obligations” instead of “its obligations”,
we might get better solutions for (8).
The role pair information raises similar prob-
lems. Some verbs are very vague, like “get”,
“take”, “have”, and role pairs with these verbs
might not be very useful. Here is an example:
</bodyText>
<listItem confidence="0.91121725">
(9) The retired Greek officer tried to get Ocalan
to the Netherlands, home to a large Kurdish
community. He claimed he had been ma-
nipulated by the government.
</listItem>
<bodyText confidence="0.9998605">
In this sentence, the role pair information is
very vague and it is hard to select a proper ante-
cedent by connecting the subject of “try” or “get”
or the object of “get” to the subject of “claim”.
</bodyText>
<subsectionHeader confidence="0.997988">
5.8 Limitations of Semantic Features
</subsectionHeader>
<bodyText confidence="0.999366875">
The availability of very large corpora coupled
with improved pre-processing (e.g., faster pars-
ers, accurate semantic labelers) is making it eas-
ier to extract large sets of semantic patterns.
However, results on “perfect” semantic informa-
tion show that even if we can get very good se-
mantic features, there are at least two concerns to
address:
</bodyText>
<listItem confidence="0.895204">
• How to best capture the context information:
larger context patterns may suffer from data
sparseness; simple patterns may be insuffi-
ciently selective, appearing in both positive
and negative samples.
• In some cases, the baseline features are suffi-
</listItem>
<bodyText confidence="0.9718365">
cient to select the antecedent and the semantic
features only do harm. If we are able to better
gauge our confidence in the decisions based
on the baseline features and on the semantic
features, we may be able to combine these
two sources more effectively.
</bodyText>
<sectionHeader confidence="0.999056" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999925411764706">
We have presented two ways to incorporate se-
mantic features into a MaxEnt model-based pro-
noun coreference system, where these features
have been extracted from a large corpus using a
baseline IE (Information Extraction) system and
a semantic labeling system, with no specific do-
main information.
We also estimated the maximal benefit of
these features and did some error analysis to
identify cases where this semantic knowledge did
not suffice. Our experiments show the value of
these semantic features for pronoun coreference,
but also the limitations of our current context
representation and reference resolution models.
Last, we compared the features extracted from
different levels of analysis, and showed that
&apos;deeper&apos; representations worked better.
</bodyText>
<sectionHeader confidence="0.999466" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997544857142857">
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Two-phased event relation acquisition: coupling
the relation-oriented and argument-oriented ap-
proaches. Proc. 22nd Int&apos;l Conf. on Computational
Linguistics (COLING 2008).
David Bean and Ellen Riloff. 2004. Unsupervised
Learning of Contextual Role Knowledge for
</reference>
<page confidence="0.988465">
67
</page>
<reference confidence="0.999753475">
Coreference Resolution. Proc. HLT-NAACL
2004.
Nathanael Chambers and Dan Jurafsky. 2008.
Unsupervised Learning of Narrative Event
Chains. Proc. ACL-08: HLT.
I. Dagan and A. Itai. 1990. Automatic processing of
large corpora for the resolution of anaphora
references. Proc. 13th International Conference on
Computational Linguistics (COLING 1990).
J. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:339–352.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument fre-
quencies for pronoun interpretation. Proc.
HLT-NAACL 2004.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao
and W. Xu. 2009. Automatic Recognition of Logi-
cal Relations for English, Chinese and Japanese in
the GLARF Framework. In SEW-2009 (Semantic
Evaluations Workshop) at NAACL HLT-2009
Viktor Pekar. 2006. Acquisition of verb entailment
from text. Proc. HLT-NAACL 2006.
Simone Paolo Ponzetto and Michael Strube. 2006
Exploiting semantic Role Labeling, WordNet
and Wikipedia for Coreference Resolution.
Proc. HLT-NAACL 2006.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004. An NP-
cluster approach to coreference resolution.
Proc. 20th International Conference on Computa-
tional Linguistics (COLING 2004).
Xiaofeng Yang, Jian Su, Chew Lim Tan.2005. Im-
proving Pronoun Resolution Using Statistics-
Based Semantic Compatibility Information.
Proc. 43rd Annual Meeting of the Assn. for Com-
putational Linguistics.
Xiaofeng Yang and Jian Su. 2007. Coreference
Resolution Using Semantic Relatedness In-
formation from Automatically Discovered Pat-
terns. Proc. 45th Annual Meeting of the Assn. for
Computational Linguistics.
</reference>
<page confidence="0.999446">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956189">
<title confidence="0.9894495">Large Corpus-based Semantic Feature Extraction for Pronoun Coreference</title>
<author confidence="0.999952">Shasha Liao Ralph Grishman</author>
<affiliation confidence="0.998875">Dept. of Computer Science Dept. of Computer Science New York University New York University</affiliation>
<email confidence="0.99937">liaoss@cs.nyu.edugrishman@cs.nyu.edu</email>
<abstract confidence="0.999338161290322">Semantic information is a very important factor in coreference resolution. The combination of large corpora and ‘deep’ analysis procedures has made it possible to acquire a range of semantic information and apply it to this task. In this paper, we generate two statistically-based semantic features from a large corpus and measure their influence on pronoun coreference. One is contextual compatibility, which decides if the antecedent can be used in the anaphor’s context; the other is role pair, which decides if the actions asserted of the antecedent and the anaphor are likely to apply to the same entity. We apply a semantic labeling system and a baseline coreference system to a large corpus to generate semantic patterns and convert them into features in a MaxEnt model. These features produce an absolute gain of 1.5% to 1.7% in resolution accuracy (a 6% reduction in errors). To understand the limitations of these features, we also extract patterns from the test corpus, use these patterns to train a coreference model, and examine some of the cases where coreference still fails. We also compare the performance of patterns extracted from semantic role labeling and syntax.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shuya Abe</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Two-phased event relation acquisition: coupling the relation-oriented and argument-oriented approaches.</title>
<date>2008</date>
<booktitle>Proc. 22nd Int&apos;l Conf. on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="6341" citStr="Abe et al. (2008)" startWordPosition="987" endWordPosition="990">extual compatibility feature does not help much for pronoun coreference: existing learningbased approaches already performed well; such statistics are simply not good predictors for pronoun interpretation; data is sparse in the collected predicate-argument statistics. The role pair feature has not been studied for general, broad-domain pronoun co-reference, but it has been used for other tasks: Pekar (2006) built pairs of &apos;templates&apos; which share an &apos;anchor&apos; argument; these correspond closely to our role pairs. Association statistics of the template pairs were used to acquire verb entailments. Abe et al. (2008) looked for pairs appearing in specific syntactic patterns in order to acquire finer-grained event relations. Chambers and Jurafsky (2008) built narrative event chains, which are partially ordered sets of events related by a common protagonist. They use high-precision hand-coded rules to get coreference information, extract predicate arguments that link the mentions to verbs, and link the arguments of the coreferred mentions to build a verb entailment model. Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pai</context>
</contexts>
<marker>Abe, Inui, Matsumoto, 2008</marker>
<rawString>Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008. Two-phased event relation acquisition: coupling the relation-oriented and argument-oriented approaches. Proc. 22nd Int&apos;l Conf. on Computational Linguistics (COLING 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bean</author>
<author>Ellen Riloff</author>
</authors>
<title>Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution.</title>
<date>2004</date>
<booktitle>Proc. HLT-NAACL</booktitle>
<contexts>
<context position="3360" citStr="Bean and Riloff (2004)" startWordPosition="535" endWordPosition="538">words and syntactic structure. People can still resolve these correctly because “terrorist” is more likely to be arrested than “boy”, and because the one shooting is more likely to be arrested than the one being shot. In such cases, semantic constraints and preferences are required for correct coreference resolution. Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60–68, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task. However, this task is difficult because it requires the acquisition of a large amount of semantic information. Furthermore, there is not universal agreement on the value of these semantic preferences for pronoun coreference. Kehler et al. (2004) reported that such information did not produce apparent improvement in overall pronoun resolution. In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun coreference resolution can benefit from</context>
<context position="5036" citStr="Bean and Riloff (2004)" startWordPosition="802" endWordPosition="805"> in which the semantic knowledge is extracted from the evaluation corpus. We will use the term mention to describe an individual referring phrase. For most studies of coreference, mentions are noun phrases and may be headed by a name, a common noun, or a pronoun. We will use the term entity to refer to a set of coreferential mentions. 2 Related Work Contextual compatibility features have long been studied for pronoun coreference: Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It determined the preference of candidates based on predicate-argument frequencies. Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two specific domains, terrorism and natural disasters. Yang et al. (2005) use statistically-based semantic compatibility information to improve pronoun resolution. They use corpus-based and web-based extraction strategies, and their work shows that statistically-based semantic compatibility information can</context>
<context position="6826" citStr="Bean and Riloff (2004)" startWordPosition="1060" endWordPosition="1063">rrespond closely to our role pairs. Association statistics of the template pairs were used to acquire verb entailments. Abe et al. (2008) looked for pairs appearing in specific syntactic patterns in order to acquire finer-grained event relations. Chambers and Jurafsky (2008) built narrative event chains, which are partially ordered sets of events related by a common protagonist. They use high-precision hand-coded rules to get coreference information, extract predicate arguments that link the mentions to verbs, and link the arguments of the coreferred mentions to build a verb entailment model. Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters. Their result raises the natural question as to whether the approach (which may capture domainspecific pairs such as “kidnap—release” in the terrorism domain) can be successfully extended to a general news corpus. We address this question in the experiments reported here. 3 Corpus Analysis In order </context>
</contexts>
<marker>Bean, Riloff, 2004</marker>
<rawString>David Bean and Ellen Riloff. 2004. Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution. Proc. HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised Learning of Narrative Event Chains.</title>
<date>2008</date>
<booktitle>Proc. ACL-08: HLT.</booktitle>
<contexts>
<context position="6479" citStr="Chambers and Jurafsky (2008)" startWordPosition="1007" endWordPosition="1010">ll; such statistics are simply not good predictors for pronoun interpretation; data is sparse in the collected predicate-argument statistics. The role pair feature has not been studied for general, broad-domain pronoun co-reference, but it has been used for other tasks: Pekar (2006) built pairs of &apos;templates&apos; which share an &apos;anchor&apos; argument; these correspond closely to our role pairs. Association statistics of the template pairs were used to acquire verb entailments. Abe et al. (2008) looked for pairs appearing in specific syntactic patterns in order to acquire finer-grained event relations. Chambers and Jurafsky (2008) built narrative event chains, which are partially ordered sets of events related by a common protagonist. They use high-precision hand-coded rules to get coreference information, extract predicate arguments that link the mentions to verbs, and link the arguments of the coreferred mentions to build a verb entailment model. Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific cor</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised Learning of Narrative Event Chains. Proc. ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>A Itai</author>
</authors>
<title>Automatic processing of large corpora for the resolution of anaphora references.</title>
<date>1990</date>
<booktitle>Proc. 13th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="3336" citStr="Dagan and Itai (1990)" startWordPosition="531" endWordPosition="534">re the same antecedent words and syntactic structure. People can still resolve these correctly because “terrorist” is more likely to be arrested than “boy”, and because the one shooting is more likely to be arrested than the one being shot. In such cases, semantic constraints and preferences are required for correct coreference resolution. Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60–68, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task. However, this task is difficult because it requires the acquisition of a large amount of semantic information. Furthermore, there is not universal agreement on the value of these semantic preferences for pronoun coreference. Kehler et al. (2004) reported that such information did not produce apparent improvement in overall pronoun resolution. In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun coreference res</context>
<context position="4869" citStr="Dagan and Itai (1990)" startWordPosition="780" endWordPosition="783">main pronoun co-reference. In addition, to obtain a rough upper bound on the benefits of our approach and understand its limitations, we conducted a second experiment in which the semantic knowledge is extracted from the evaluation corpus. We will use the term mention to describe an individual referring phrase. For most studies of coreference, mentions are noun phrases and may be headed by a name, a common noun, or a pronoun. We will use the term entity to refer to a set of coreferential mentions. 2 Related Work Contextual compatibility features have long been studied for pronoun coreference: Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It determined the preference of candidates based on predicate-argument frequencies. Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two specific domains, terrorism and natural disasters. Yang et al. (2005) use statistically-based semantic compatibility information to impr</context>
</contexts>
<marker>Dagan, Itai, 1990</marker>
<rawString>I. Dagan and A. Itai. 1990. Automatic processing of large corpora for the resolution of anaphora references. Proc. 13th International Conference on Computational Linguistics (COLING 1990).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>Resolving pronoun references.</title>
<date>1978</date>
<journal>Lingua,</journal>
<pages>44--339</pages>
<marker>Hobbs, 1978</marker>
<rawString>J. Hobbs. 1978. Resolving pronoun references. Lingua, 44:339–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
<author>D Appelt</author>
<author>L Taylor</author>
<author>A Simma</author>
</authors>
<title>The (non)utility of predicate-argument frequencies for pronoun interpretation.</title>
<date>2004</date>
<booktitle>Proc. HLT-NAACL</booktitle>
<contexts>
<context position="3682" citStr="Kehler et al. (2004)" startWordPosition="589" endWordPosition="592">ds for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60–68, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task. However, this task is difficult because it requires the acquisition of a large amount of semantic information. Furthermore, there is not universal agreement on the value of these semantic preferences for pronoun coreference. Kehler et al. (2004) reported that such information did not produce apparent improvement in overall pronoun resolution. In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun coreference resolution can benefit from such knowledge, which is automatically extracted from a large corpus. We studied two features: the contextual compatibility feature which has been demonstrated to work at the syntactic level by previous work; and the role pair feature, which has not previously been applied to general domain pronoun co-reference. In addi</context>
<context position="5702" citStr="Kehler et al. (2004)" startWordPosition="893" endWordPosition="896">knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two specific domains, terrorism and natural disasters. Yang et al. (2005) use statistically-based semantic compatibility information to improve pronoun resolution. They use corpus-based and web-based extraction strategies, and their work shows that statistically-based semantic compatibility information can improve coreference resolution. In contrast, Kehler et al. (2004) claimed that the contextual compatibility feature does not help much for pronoun coreference: existing learningbased approaches already performed well; such statistics are simply not good predictors for pronoun interpretation; data is sparse in the collected predicate-argument statistics. The role pair feature has not been studied for general, broad-domain pronoun co-reference, but it has been used for other tasks: Pekar (2006) built pairs of &apos;templates&apos; which share an &apos;anchor&apos; argument; these correspond closely to our role pairs. Association statistics of the template pairs were used to acqu</context>
<context position="20670" citStr="Kehler et al. (2004)" startWordPosition="3277" endWordPosition="3280">, consisting of five years of AFP newswire from the LDC English Gigaword corpus (1996, 2002, 2004, 2005 and 2006), a total of 907,368 documents. We omit news articles written in 1998, 2000 and 2003 to insure there is no overlap between the ACE data and Gigaword data. We pre-processed each document (parsing, name identification, and semantic labeling) and ran the baseline coreference system, which automatically identified all the mentions (including name mentions and nominal mentions) and built a set of entities for each document. 4For example, among papers reporting a pronoun accuracy metric, Kehler et al. (2004), testing on a 2002 ACE news corpus, get a pronoun accuracy (without semantic features) of 75.7%; (Yang et al. 2005), testing on the MUC coreference corpora (also news) get for their single-candidate baseline (without semantic features) 75.1% pronoun accuracy. Although the testing conditions in each case are different, these are comparable to our baseline performance. 5.3 Semantic Information Extraction from Large Corpus In order to remove noise, we only keep contextual compatibility patterns that appear more than 5 times; and only keep role pair patterns which appear more than 15 times, and a</context>
<context position="27517" citStr="Kehler et al. (2004)" startWordPosition="4371" endWordPosition="4374">information does not help, or even harms the analysis. In some cases all the antecedent News Corpus 2005 Corpus semantic syntax semantic syntax baseline 75.54 72.04 context 79.23 77.73 76.04 75.83 role pair 78.85 76.87 75.95 74.17 combine 79.97 78.42 77.50 76.76 66 scores are very low, either because the patterns are very rare or the antecedent is a common word that appears in a lot of patterns. In other cases, several antecedents have a high compatibility score but the correct one does not have the top score. In these cases, the contextual compatibility is not reliable, as was pointed out by Kehler et al. (2004): (7) The model for a republic, adopted over bitter objections from those advocating direct election of a president, is for presidential nominations to be made with public input and the winning candidate decided by a two-thirds majority of Parliament. Former prime minister Paul Keating, who put the republic issue in the spotlight in his unsuccessful 1996 campaign for re-election, welcomed the result. Here adding semantic features leads “his” to be incorrectly resolved to “president” rather than the entity with mentions “prime minister” and “Paul Keating”; all the relevant patterns are common, </context>
</contexts>
<marker>Kehler, Appelt, Taylor, Simma, 2004</marker>
<rawString>A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The (non)utility of predicate-argument frequencies for pronoun interpretation. Proc. HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>M Kosaka</author>
<author>N Xue</author>
<author>H Ji</author>
<author>A Sun</author>
<author>S Liao</author>
<author>W Xu</author>
</authors>
<date>2009</date>
<booktitle>Automatic Recognition of Logical Relations for English, Chinese and Japanese in the GLARF Framework. In SEW-2009 (Semantic Evaluations Workshop) at NAACL HLT-2009</booktitle>
<contexts>
<context position="7773" citStr="Meyers et al. 2009" startWordPosition="1204" endWordPosition="1207">ural question as to whether the approach (which may capture domainspecific pairs such as “kidnap—release” in the terrorism domain) can be successfully extended to a general news corpus. We address this question in the experiments reported here. 3 Corpus Analysis In order to extract semantic features from our large training corpus, we apply a sequence of analyzers. These include name tagging, parsing, a baseline coreference analyzer, and, most important, a semantic labeling system that can generate the logical grammatical and predicateargument representation automatically from a 61 parse tree (Meyers et al. 2009). We use semantic labeling because it provides more general and meaningful patterns, with a “deeper” analysis than parsed text. The output of the semantic labeling is the dependency representation of the text, where each sentence is a graph consisting of nodes (corresponding to words) and arcs. Each arc captures up to three relations between two words: (1) a SURFACE relation, the relation between a predicate and an argument in the parse of a sentence; (2) a LOGIC1 (grammatical logical) relation which regularizes for lexical and syntactic phenomena like passive, relative clauses, and deleted su</context>
</contexts>
<marker>Meyers, Kosaka, Xue, Ji, Sun, Liao, Xu, 2009</marker>
<rawString>A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao and W. Xu. 2009. Automatic Recognition of Logical Relations for English, Chinese and Japanese in the GLARF Framework. In SEW-2009 (Semantic Evaluations Workshop) at NAACL HLT-2009</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viktor Pekar</author>
</authors>
<title>Acquisition of verb entailment from text.</title>
<date>2006</date>
<booktitle>Proc. HLT-NAACL</booktitle>
<contexts>
<context position="6134" citStr="Pekar (2006)" startWordPosition="957" endWordPosition="958">-based extraction strategies, and their work shows that statistically-based semantic compatibility information can improve coreference resolution. In contrast, Kehler et al. (2004) claimed that the contextual compatibility feature does not help much for pronoun coreference: existing learningbased approaches already performed well; such statistics are simply not good predictors for pronoun interpretation; data is sparse in the collected predicate-argument statistics. The role pair feature has not been studied for general, broad-domain pronoun co-reference, but it has been used for other tasks: Pekar (2006) built pairs of &apos;templates&apos; which share an &apos;anchor&apos; argument; these correspond closely to our role pairs. Association statistics of the template pairs were used to acquire verb entailments. Abe et al. (2008) looked for pairs appearing in specific syntactic patterns in order to acquire finer-grained event relations. Chambers and Jurafsky (2008) built narrative event chains, which are partially ordered sets of events related by a common protagonist. They use high-precision hand-coded rules to get coreference information, extract predicate arguments that link the mentions to verbs, and link the a</context>
</contexts>
<marker>Pekar, 2006</marker>
<rawString>Viktor Pekar. 2006. Acquisition of verb entailment from text. Proc. HLT-NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic Role Labeling, WordNet and Wikipedia for Coreference Resolution.</title>
<date>2006</date>
<booktitle>Proc. HLT-NAACL</booktitle>
<contexts>
<context position="3412" citStr="Ponzetto and Strube (2006)" startWordPosition="544" endWordPosition="547">resolve these correctly because “terrorist” is more likely to be arrested than “boy”, and because the one shooting is more likely to be arrested than the one being shot. In such cases, semantic constraints and preferences are required for correct coreference resolution. Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60–68, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task. However, this task is difficult because it requires the acquisition of a large amount of semantic information. Furthermore, there is not universal agreement on the value of these semantic preferences for pronoun coreference. Kehler et al. (2004) reported that such information did not produce apparent improvement in overall pronoun resolution. In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun coreference resolution can benefit from such knowledge, which is automatically extracted fr</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006 Exploiting semantic Role Labeling, WordNet and Wikipedia for Coreference Resolution. Proc. HLT-NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>J Su</author>
<author>G Zhou</author>
<author>C Tan</author>
</authors>
<title>An NPcluster approach to coreference resolution.</title>
<date>2004</date>
<booktitle>Proc. 20th International Conference on Computational Linguistics (COLING</booktitle>
<marker>Yang, Su, Zhou, Tan, 2004</marker>
<rawString>X. Yang, J. Su, G. Zhou, and C. Tan. 2004. An NPcluster approach to coreference resolution. Proc. 20th International Conference on Computational Linguistics (COLING 2004).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
</authors>
<title>Chew Lim Tan.2005. Improving Pronoun Resolution Using StatisticsBased Semantic Compatibility Information.</title>
<booktitle>Proc. 43rd Annual Meeting of the Assn. for Computational Linguistics.</booktitle>
<marker>Yang, Su, </marker>
<rawString>Xiaofeng Yang, Jian Su, Chew Lim Tan.2005. Improving Pronoun Resolution Using StatisticsBased Semantic Compatibility Information. Proc. 43rd Annual Meeting of the Assn. for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
</authors>
<title>Coreference Resolution Using Semantic Relatedness Information from Automatically Discovered Patterns.</title>
<date>2007</date>
<booktitle>Proc. 45th Annual Meeting of the Assn. for Computational Linguistics.</booktitle>
<contexts>
<context position="3380" citStr="Yang and Su (2007)" startWordPosition="539" endWordPosition="542">cture. People can still resolve these correctly because “terrorist” is more likely to be arrested than “boy”, and because the one shooting is more likely to be arrested than the one being shot. In such cases, semantic constraints and preferences are required for correct coreference resolution. Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60–68, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task. However, this task is difficult because it requires the acquisition of a large amount of semantic information. Furthermore, there is not universal agreement on the value of these semantic preferences for pronoun coreference. Kehler et al. (2004) reported that such information did not produce apparent improvement in overall pronoun resolution. In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun coreference resolution can benefit from such knowledge, whi</context>
</contexts>
<marker>Yang, Su, 2007</marker>
<rawString>Xiaofeng Yang and Jian Su. 2007. Coreference Resolution Using Semantic Relatedness Information from Automatically Discovered Patterns. Proc. 45th Annual Meeting of the Assn. for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>