<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.77884">
Mix and Match Replacement Rules
</title>
<author confidence="0.858665">
Dale Gerdemann
</author>
<affiliation confidence="0.978347">
Department of Linguistics
</affiliation>
<address confidence="0.680661">
Universit¨at T¨ubingen
72074 T¨ubingen, Germany
</address>
<email confidence="0.995437">
dg@sfs.uni-tuebingen.de
</email>
<sectionHeader confidence="0.987407" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998300714285714">
A flexible construction kit is presented compiling
various forms of finite state replacement rules.
The approach is simpler and more declarative
than algorithms in the tradition of Kaplan &amp;
Kay. Simple constraints can be combined to
achieve complex effects, including effects based
on Optimality Theory.
</bodyText>
<sectionHeader confidence="0.996864" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999232">
Traditional finite state algorithms for compiling re-
placement rules have a very procedural flavor. The
general approach originated by Kaplan &amp; Kay [8] in-
volves steps for introducing markers, constraining the
markers in various ways, making the actual replace-
ment, constraining the markers again after the replace-
ment and finally removing the markers. Several vari-
ants of Kaplan &amp; Kay’s algorithm have been presented
([9], [12], [4]) with the goals being either to improve
efficiency or to provide slightly different semantics. In
this paper, I present an alternative, more declarative
approach. Rather than being based on a sequence
of steps that are composed together, this approach is
based on intersection. A basic, unconstrained replace-
ment is represented by a transducer, which can then
be constrained by intersecting this transducer with au-
tomata representing a variety of constraints. The ob-
vious objection to this approach is that transducers
are not closed under intersection.1 It turns out, how-
ever, that a rather limited form of intersection is suf-
ficient for implementing the various forms of replace-
ment rules that have appeared in the literature, along
with a wide variety of replacement rules that have not
appeared in the literature.
One of the advantages of finite state technology
is reusability. An implementation designed for one
finite state toolbox can easily be redeployed with
a different toolbox. At least that is the theory. In
practice, toolboxes tend to provide a number of
incompatible features. Replacement rules, in partic-
ular, occur in many variations: optional/obligatory,
left-to-right/right-to-left/simultaneous, longest-
matching/shortest-matching, etc. Sometimes a rather
small variation has required introducing some new
syntax. The widely used Xfst system [1], for example,
</bodyText>
<footnote confidence="0.7372736">
1 More precisely, regular relations are not closed under inter-
section. It is convenient, however, to systematically confuse
finite state transducers (respectively, finite state acceptors)
with the regular relations (respectively, regular languages)
that they encode.
</footnote>
<bodyText confidence="0.999711916666666">
has special syntax (dotted brackets) used to enforce a
constraint against repeated epenthesis. While this is
certainly a useful option, it is also an impediment to
users who wish to transfer their Xfst implementation
to another toolbox. Clearly such users would benefit
more from a kind of replacement rule construction kit,
allowing such variant replacement rules to be put to-
gether as needed. But the procedural style of Kaplan
&amp; Kay-inspired algorithms makes the development
of such a construction kit very difficult. Given the
declarative approach developed in this paper, the
construction kit approach can be implemented in any
standard finite-state toolkit.
The declarative approach to replacement rules is
very flexible, allowing for example, variants of finite-
state Optimality Theory ([2] [5]) to be incorporated
into the rules. In some sense, this is not new, as
Karttunen’s approach to longest match replacement
[9], was already a kind of optimization. The declara-
tive, mix-and-match approach, however, allows for the
easy incorporation of various optimality constraints.
For example, a constraint against repeated epenthe-
sis could be loosened to allow the minimal amount of
epenthesis consistent with some other constraints.2
</bodyText>
<sectionHeader confidence="0.713411" genericHeader="introduction">
2 Flat Representation for
Transducers using Types
</sectionHeader>
<bodyText confidence="0.971145294117647">
The general form for a finite-state rewrite rule is as in
1:
x → T(x)/a p (1)
This is understood as a rule for replacing the string
x with the transduction T (x) in the specified context,
with some unspecified mode of operation such as oblig-
atory left-to-right. Traditionally T is specified as a
cross-product of regular languages, but as discussed
in Gerdemann &amp; van Noord [4], this is by no means
necessary.
In order to develop an approach based on intersec-
tion, a rule such as 1 (ignoring context, and other con-
straints) will first be transformed into a finite state
acceptor (FSA). There is of course a standard trick of
using same-length transducers, which goes back to the
early days of finite-state morphology [11]. A same-
length transducer can be encoded as an FSA with
</bodyText>
<footnote confidence="0.988255666666667">
2 This paper concentrates on some fairly simple examples that
are good for expository purposes. For further examples, see:
www.sfs.uni-tuebingen.de/∼dg/mixmatch.
</footnote>
<page confidence="0.988837">
39
</page>
<bodyText confidence="0.948878769230769">
Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 39–47
transitions labeled with pairs of symbols, or equiva-
lently by an FSA in which odd-numbered transitions
(from the start state) count as inputs, and even num-
bered transitions count as outputs. This is, however,
an inconvenient representation for two reasons. First,
we often work with transducers that do not have the
same-length property. And second, we would like our
flattened transducer representations to contain other
symbols for markup which count as neither input nor
output. So rather than force things into a same-length
mold, we use the following very general definition for
a flat representation of a transducer:3
</bodyText>
<construct confidence="0.981632">
Definition 2.1 A finite state automaton A is a flat
representation of a transducer T with respect to
transducers extract input and extract output if:
</construct>
<equation confidence="0.843684">
T = extract input−1 o identity(A) o extract output
</equation>
<bodyText confidence="0.999784444444444">
It should be understood here that the equals sign
means that the transducers defined on either side rep-
resent the same regular relation,
This flat representation is the central feature of the
approach. This approach allows extract input and ex-
tract output to be defined in many different ways. To
instantiate these extraction transducers in a useful and
flexible way, I start by introducing a type system with
types that these transducers can refer to.
</bodyText>
<subsectionHeader confidence="0.947841">
2.1 Types
</subsectionHeader>
<bodyText confidence="0.995074409090909">
The goal of introducing types is to make it easy to
define extract input and extract output. If some sym-
bols are of the input type, for example, then these are
the ones that should be extracted by extract input. A
simple idea would be to use different alphabets for
these two types. In some cases, this is a convenient
idea. For example, if the input alphabet is Cyrillic
and the output alphabet is Latin, then it is easy to
distinguish input and output symbols. In most cases,
however, this is not practical. Therefore an alternative
approach is adopted: every untyped symbol is followed
by a symbol type indicator, which specifies the type of
its predecessor.
At a minimum, there need to be two symbol type
indicators: one for input symbols and one for output
symbols. More usefully, there should also be indicators
for symbols that are neither input nor output (marker
symbols) and also for symbols that are both input
and output (identity symbols). Possibly, for some al-
gorithms, it would be useful to have a several types
of marker symbols, but for now four symbols will be
enough.
</bodyText>
<listItem confidence="0.9992546">
• define input a;
• define output b;
• define identity c;
• define marker x;
• define symbolTypeIndicator
</listItem>
<bodyText confidence="0.701197">
[input U output U identity U marker];
</bodyText>
<footnote confidence="0.9889755">
3 It is nevertheless possible using a two-level approach to work
out ideas similar to those in this paper. See Yli-Jyr¨a [15].
</footnote>
<bodyText confidence="0.9996088">
Note that the self-explanatory syntax here is as in
the Foma system (Hulden [6]). The particular choice
of symbols used here is arbitrary and can easily be
changed,
Extended alphabet symbols thus consist of a se-
quence of two symbols, where the second symbol deter-
mines the type of the first symbol. With the preceding
conventions, for example, the sequence [z a] (= [z in-
put]) represents ‘z’ as an input symbol, and [&lt; x] (=
[&lt; marker]) represents the angle bracket ‘&lt;’ used as
a marker symbol. For the purpose of illustration with
a running example, a small test alphabet is defined
here (even though Foma does not require the alphabet
to be specified). In these definitions, xsig should be
understood as “extended sigma.”
</bodyText>
<listItem confidence="0.998062">
• define sigma [a U b U c U x]; # small test alphabet
• define xsig [sigma symbolTypeIndicator];
</listItem>
<bodyText confidence="0.98872525">
An approach using sequences of two symbols as the
basic unit can be awkward to work with and painful
to debug. Thus it is essential to provide a set of tools
that are specialized to work with this approach.
</bodyText>
<subsubsectionHeader confidence="0.705375">
2.1.1 Matchers, Constructors and Accessors
</subsubsectionHeader>
<bodyText confidence="0.972220166666667">
Three basic tools for working with an extended alpha-
bet are matchers, constructors and accessors. Gener-
ally, these tools are designed to hide the symbol type
indicators from the user.
Matchers Matchers are a set of defined regular ex-
pressions for matching the various types of symbols.
</bodyText>
<listItem confidence="0.9988958">
• define in [sigma [input U identity]];
• define inx [sigma input];
• define out [sigma [output U identity]];
• define outx [sigma output];
• define ident [sigma identity];
</listItem>
<bodyText confidence="0.998252666666667">
Note that the ‘x’ in inx and outx should be read as
“except identity.” Also note that these definitions use
the toy alphabet sigma. For general use, one should
replace this with Σ, which is the Foma symbol for an
open alphabet.
Constructors Constructors are used to turn an or-
dinary alphabet symbols into typed symbols of the
extended alphabet. For this purpose, parameterized
definitions are used.
</bodyText>
<listItem confidence="0.9997632">
• define in(x) [x [input U identity]];
• define inx(x) [x input];
• define out(x) [x [output U identity]];
• define outx(x) [x output];
• define ident(x) [x [identity]];
</listItem>
<page confidence="0.996974">
40
</page>
<bodyText confidence="0.999252">
Accessors Accessors are used to extract either the
input side or the output side. By extracting both sides,
a definition can be given for unflatten. The definitions
rely upon a definition of term complement which is
specialized for use with the extended alphabet.&apos; The
first step in extractInput eliminates all non-input sym-
bols. This is composed with a transducer that elimi-
nates the symbol type indicators.
</bodyText>
<listItem confidence="0.938631142857143">
• define tcomp(e) xsig-e; # term complement
• define extractInput
[tcomp(in):ǫ U in]* o [Σ Σ:ǫ]*;
• define extractOutput
[tcomp(out):ǫ U out]* o [Σ Σ:ǫ]*;
• define unflatten(φ)
extractInput−1 o φ o extractOutput;
</listItem>
<subsubsectionHeader confidence="0.935031">
2.1.2 Pretty Printers
</subsubsectionHeader>
<bodyText confidence="0.99944912">
Just as in any programming language, it is important
to have concise, understandable print representations
of complex data structures. Most finite state toolboxes
include a graphical interface, which is very helpful for
debugging. It is certainly helpful to have such an inter-
face, but experience has shown that networks quickly
become way too complicated to be understood visu-
ally. Effective debugging is an acquired skill that re-
quires a great deal of ingenuity in breaking problems
into units that can be understood as networks. Since
the approach of using a typed alphabet doubles the
size of the network, it must also at least double the
cognitive complexity of understanding the network.&apos;
What is needed is a graphical interface that shows
the types of symbols in a concise way without using se-
quences of two symbols, and without using the symbol
type indicator. An implementation might, for exam-
ple, use different colors for the different types. But
since such an implementation does not exist, I simply
define here an ad hoc pretty printing approach, spe-
cialized for use with the small toy alphabet. The idea
is that an input ‘a’ is transduced to ‘a:’, an output
‘a’ is transduced to ‘:a’, and an identity ‘a’ is trans-
duced to ‘a’. In all cases, the symbol type indicator is
removed.&apos;
</bodyText>
<listItem confidence="0.936108666666667">
• define ppInput
[inx(a):a%: U inx(b):b%: U inx(c):c%:
U inx(x):x%:];
</listItem>
<bodyText confidence="0.954343882352941">
4 The expression tcomp(in):ǫ U in could be written more ex-
plicitly as tcomp(in):ǫ U identity(in). It is, however, stan-
dard in the literature to let language expressions be coerced
to identity relation expressions when the context requires this
interpretation.
5 Besides raising the cognitive complexity, the typed approach
also clearly increases the computational complexity. But this
is unlikely to be a practical concern. The complexity of fi-
nite state implementations does not, in any case, derive from
the complexity of individual replacement rules. Normally,
complexity derives from composing a long sequence of such
replacement rules.
6 Note that the percent sign is used in Foma to escape special
characters. So ‘a%:’ is the two-character symbol consisting
of an ‘a’ followed by a colon. Note also that the subscript 2
in the last definition is used to extract the second projection
(also known as “range” or “lower language”).
</bodyText>
<listItem confidence="0.993305625">
• define ppOutput
[outx(a):%:a U outx(b):%:b U outx(c):%:c
U outx(x):%:x];
• define ppIdentity
[ident(a):a U ident(b):b U ident(c):c
U ident(x):x];
• ppIO [ppInput U ppOutput U ppIdentity];
• define pp(X) [X o ppIO*]2;
</listItem>
<bodyText confidence="0.99966975">
Further utilities and pretty printers will be intro-
duced as needed. First, however, we should start to
see how the type system can be useful for our compi-
lation.
</bodyText>
<subsubsectionHeader confidence="0.498394">
2.1.3 Assertions and Boolean Tests
</subsubsectionHeader>
<bodyText confidence="0.999647428571429">
An important invariant of the typed approach is that
every sequence is of even length and every symbol in
an even position is a symbol type indicator. Consis-
tent use of matchers, constructors and accessors will
help to ensure that this invariant holds. Neverthe-
less, things can go wrong, so it is important to use
boolean tests as assertions. Boolean types were intro-
duced into the regular expression calculus in van No-
ord &amp; Gerdemann [14], though the details are worked
out differently here. The central idea is similar to con-
ventions in a variety of programming languages, where
certain designated values are understood as, or coerced
to boolean values. In the finite state domain, it is con-
venient let ǫ be true, and 0 be false.
</bodyText>
<listItem confidence="0.9974855">
• define true ǫ;
• define false 0;
</listItem>
<bodyText confidence="0.999462428571429">
Note that with these conventions, true is a single-
state FSA where the one state is both initial and final,
and false is a single-state FSA where the one state is
non-final.&apos;
Given these conventions, the boolean connectives
and, or and not can be defined as concatenation, union
and complementation, respectively.
</bodyText>
<listItem confidence="0.997575">
• and(B1, B2) B1 B2;
• or(B1, B2) B1 U B2;
• not(B) �B fl true;
</listItem>
<bodyText confidence="0.65610475">
7 In Foma, it is easy to see if assertions have succeeded or not
from the compiler output. For example, with a source file
(assert.fom) containing the following two lines
define assertion1 ǫ;
define assertion2 ∅;
the compiler produces the following output
foma[0]: source assert.fom
Opening file ’assert.fom’.
defined assertion1: 136 bytes. 1 states, 0 arcs, 1 path.
defined assertion2: 136 bytes. 1 states, 0 arcs, 0 paths.
The information that the compiled version of assertion1 has
1 path is sufficient to see that this assertion succeded.
For an example using a lot of assertions, see www.sfs.uni-
tuebingen.de/∼dg/mixmatch/FinnOTMatching.fom. The
FinnOTMatching program is also a good illustration of fi-
nite state OT in general.
</bodyText>
<page confidence="0.997463">
41
</page>
<bodyText confidence="0.997352">
With these connectives, basic decidable properties
of FSA’s can be defined.8
</bodyText>
<listItem confidence="0.8516794">
• empty(L) not([L:true]2);
• subset(A1,A2) empty(A1 n HA2);
• equal(A1, A2)
and(subset(A1,A2), subset(A2,A1));
Using these tools, the following assertions can be
defined.
• define evenLength(L) empty(L n [[E E]* E]]);
• define indicatorsInEvenPositions(L)
subset([[[E E]* E]:ǫ E E*:ǫ]2,
symbolTypeIndicator);
</listItem>
<sectionHeader confidence="0.8173465" genericHeader="method">
3 Basic Unconstrained Replace-
ment
</sectionHeader>
<bodyText confidence="0.9972948">
To compile a replacement rule as in (1), we must start
by flattening the transducer T. We have already seen
the unflatten definition, and it is clear that flatten
should be the inverse of this, so that for transducer
T:
</bodyText>
<equation confidence="0.969718">
T ≡ unflatten(flatten(T)) (2)
If T is a cross-product transducer, then this is easy
</equation>
<listItem confidence="0.758702">
to define:
• define flattenCross(φ, ψ )
</listItem>
<bodyText confidence="0.983820625">
[φ o [E ǫ:input]*]2 [ψ o [E ǫ:output]*]2;
Flattening for non-cross product transducers, al-
though not hard, requires access to states and transi-
tions and therefore cannot be done in a portable way.9
In general, the flattening step may arbitrarily inter-
sperse input and output symbols, making it impossi-
ble to write constraints that refer at the same time
to input and output symbols. The general problem
is that transducers are not closed under intersection.
The good news is that constraints on replacement rules
rarely need to refer simultaneously to inputs and out-
puts.
Given a flattened transducer T, we can define a flat-
tened unconstrained replacement rule with T as its
center. By analogy with Optimality Theory, this ba-
sic replacement rule is called replaceGen.
</bodyText>
<listItem confidence="0.849657">
• define lb [a marker];
• define rb [b marker];
• define bracket lb U rb;
• define replaceGen(φ) [ident U [lb φ rb]]*;
Since these rules have introduced a couple of marker
symbols, it is important to define corresponding pretty
printing definitions.
• define ppBrackets [lb:%&lt; U rb:%&gt;];
• define pp1(X) [X o [ppIO U ppBrackets]*]2;
</listItem>
<figureCaption confidence="0.999954">
Fig. 1: regex pp1(replaceGen(flattenCross(ǫ, x)))
Fig. 2: Non-iterated Epenthesis, flattened version
</figureCaption>
<bodyText confidence="0.999684111111111">
Putting these pieces together, we see in Figure 1 the
pretty-printed flattened automaton for free epenthesis
of ‘x’. This automaton looks superficially like an inter-
mediate step in a traditional replacement rule compila-
tion procedure. But in fact, it is considerably different.
The brackets, for example, are not inserted as steps in
a procedure. Constraints may refer to the brackets,
but the brackets are neither in the input nor in the
output.
</bodyText>
<sectionHeader confidence="0.995602" genericHeader="method">
4 Constraints
</sectionHeader>
<bodyText confidence="0.999872833333333">
Constraints are of two basic types: strict and opti-
mizing. Contextual constraints are, for example, tra-
ditionally treated as strict constraints, and optimiz-
ing constraints are typically limited to constraints for
leftmost and longest matching (Karttunen [9]). Strict
constraints, being easier, are a natural starting point.
</bodyText>
<subsectionHeader confidence="0.977208">
4.1 Strict Constraints
</subsectionHeader>
<bodyText confidence="0.999927285714286">
As a simple example of a strict constraint, consider a
constraint against iterated epenthesis. This may seem
like a minor constraint, but nevertheless it was consid-
ered important enough to merit some special syntax in
Xfst.10 This is easy to define in the mix-and-match ap-
proach, using auxiliary definitions for complement and
containment. The automaton is shown in Figure 2.
</bodyText>
<listItem confidence="0.979467181818182">
• define comp(E) xsig* - E;
• define contain(E) [xsig* E xsig*];
• define noIterEpenStrict
comp(contain([lb outx* rb lb outx* rb]));
• regex pp1(replaceGen(flattenCross(ǫ, x))
n
noIterEpenStrict);
8 For transducers, Foma provides the boolean tests:
isfunctional and isidentity.
9 See: www.sfs.uni-tuebingen.de/∼dg/mixmatch/flatten for a
mix of Foma code and Perl code.
</listItem>
<figure confidence="0.991615375">
10 See Beesley &amp; Karttunen [1], p. 67.
abcx
:x
�
s1
so
�
s�
abcx
�x
s1 s2
�
�
so
abcx
s�
</figure>
<page confidence="0.816223">
42
</page>
<figureCaption confidence="0.998694">
Fig. 3: Non-iterated Epenthesis, unflattened version
</figureCaption>
<bodyText confidence="0.9939235">
The unflattened version, compiled from the follow-
ing code, is shown in Figure 3.
</bodyText>
<listItem confidence="0.918522">
• regex unflatten(
</listItem>
<equation confidence="0.656540333333333">
replaceGen(flattenCross(ǫ, x))
n
noIterEpenStrict);
</equation>
<sectionHeader confidence="0.986536" genericHeader="method">
5 Defeasible Constraints
</sectionHeader>
<bodyText confidence="0.999344071428571">
Now let us relax the constraint against iterated
epenthesis and allow iterated epenthesis only when
there is no other alternative. Suppose that there is
a higher ranking constraint requiring the output to
have a sequence of two x’s. If the input already has a
sequence of two x’s, then the output constraint will in
any case be satisfied. If the input only has singleton
x’s, then epenthesis is needed. But iterated epenthesis
is not needed. If, however, the input has no x’s, then
iterated epenthesis is needed. But it is still desirable
to do no more iterated epenthesis than necessary. We
start, as in OT, by putting a star after every constraint
violation. To do this, we need some standard utilities
from Kaplan &amp; Kay [8].11
</bodyText>
<listItem confidence="0.977488428571429">
• define intro(S) [[E E] U ǫ:S]*;
• define introx(S) [[E E] U ǫ:S]* [E E];
• define xintro(S) [E E] [[E E] U ǫ:S]*;
• define ign(L, S) [L o intro(S)]2;
• define ignx(L, S) [L o introx(S)]2;
• define xign(L, S) [L o xintro(S)]2;
• define ifPThenS(P,S) comp([P comp(S)]);
• define ifSThenP(P,S) comp([comp(P) S]);
• define PIffS(P,S) ifPThenS(P,S) n ifSThenP(P,S);
Using these definitions, we can introduce a star, as
in OT, and constrain it to occur after constraint vio-
lations.
• define star [c marker];
• define ppStar star:%*;
</listItem>
<footnote confidence="0.623041">
11 As a useful convention, ‘x’ at the beginning means “except
at the beginning,” and similarly for ‘x’ at the end. The ab-
breviation ign is used for ignore.
</footnote>
<listItem confidence="0.784753333333333">
• define pp2(X)
[X o [ppIO U ppBrackets U ppStar]*]2;
• define markViolation(T, π)
</listItem>
<equation confidence="0.836818">
ign(T, star)
n
PIffS([xsig* ignx(π,star)], [star xsig*]);
</equation>
<bodyText confidence="0.999572">
Then we can mark the violation. The result is shown
in Figure 4.
</bodyText>
<listItem confidence="0.892573">
• regex pp2(markViolation(
</listItem>
<bodyText confidence="0.984197515151515">
replaceGen(flattenCross(ǫ, x)),
[lb outx* rb lb outx* rb]));
The next step involves minimizing the number of
stars. We want to rule out the case that an input would
be mapped to different outputs, b and w (for “better”
and “worse”), where output b contains n stars, output
w contains m stars, and m &gt; n. The general idea is
to construct a worsening transducer that turns better
candidates into worse ones. Then, given a set of candi-
dates c, the optimal candidates will be c − worsen(c).
This assumes, of course, that the worsening of a can-
didate is complete, all worse candidates are included
and no non-worse candidates are included. In some
cases, this relation can only be approximated.
There are two kinds of worsening: star-based wors-
ening, which is the main focus of this paper, and
generalized worsening. The difference is that star-
based worsening works by adding stars to a candi-
date, whereas generalized worsening directly manip-
ulates the output for a given input to make it worse.
In either case, worsening is used to filter out bad can-
didates by comparison with alternative candidates for
the same input. This means that GEN cannot actu-
ally change the input, GEN can only add markup sym-
bols into the input, where markup symbols are disjoint
from input symbols.12
For replaceGen, the obvious markup symbols are the
brackets and the output symbols. But what about the
identity symbols? Here, an input symbol [E, input] =
[E, a] is “marked up” as [E, identity] = [E, c]. To deal
with this problem, the changeMarkup step is written
to allow identity symbols into non-identity input sym-
bols, and vice versa.
</bodyText>
<listItem confidence="0.968816090909091">
• define changeIdentities
[[E identity:input] U [E input:identity] U xsig]*;
• define changeOutputs
[outx:ǫ U ǫ:outx U xsig]*;
• define changeBrackets
[bracket:ǫ U ǫ:bracket U xsig]*;
• define changeMarkup(X) [
X
o changeIdentities
o changeOutputs
o changeBrackets]2;
</listItem>
<footnote confidence="0.798857833333333">
12 So GEN can only change the input indirectly by adding
markup symbols that are interpreted as editing instructions.
For example, if GEN brackets syllables, then unsyllabified
parts of the input may be deleted in a later (phonetic) mod-
ule. Other kinds of editing interpretations for markup can
easily be invented.
</footnote>
<figure confidence="0.9928612">
a b c x
SO
a b c x
&lt;O:x&gt;
S1
</figure>
<page confidence="0.989954">
43
</page>
<figureCaption confidence="0.899514">
Fig. 4: No iterated Epenthesis with violations starred
</figureCaption>
<figure confidence="0.993348285714286">
abcx
�x
s1 s�
�
�
�x
s� s5
�
�
so
abcx
s�
�
s6
</figure>
<bodyText confidence="0.997623714285714">
Returning now to our running example, suppose
that we add a higher ranked constraint which in some
cases can only be satisfied by use of iterated epenthe-
sis. Specifically, let us require that the output must
contain a sequence of two instances of ‘x’. Since this
is a strict constraint, it can be applied by intersection.
The result is shown in Figure 5.
</bodyText>
<listItem confidence="0.952612">
• define containsOutputXXGen
replaceGen(flattenCross(e, x))
n
</listItem>
<equation confidence="0.774177">
contain([out(x) tcomp(out(Σ))* out(x)]);
</equation>
<bodyText confidence="0.99833">
We then mark violations of the no iterated epenthe-
sis constraint, with the result shown in Figure 6.
</bodyText>
<listItem confidence="0.945732">
• regex pp2(markViolation(
containsOutputXXGen,
[lb outx* rb lb outx* rb]));
</listItem>
<bodyText confidence="0.99958125">
The automaton in Figure 6 is fairly complicated.
After changing markup, however, it is greatly simpli-
fied, since constraints on correct ordering of output are
abstracted away. The result is shown in Figure 7.
</bodyText>
<listItem confidence="0.979641666666667">
• regex pp2(changeMarkup(markViolation(
containsOutputXXGen,
[lb outx* rb lb outx* rb])));
</listItem>
<figureCaption confidence="0.961936">
Figure 7 shows that every candidate has one of three
things.
</figureCaption>
<listItem confidence="0.997724">
1. an identity x, or
2. an output-only x, or
3. a star.
</listItem>
<bodyText confidence="0.999571666666667">
If we now add in a positive number of stars, we
obtain an automaton that matches every non-optimal
candidate as seen in Figure 8.
</bodyText>
<listItem confidence="0.950718">
• regex
</listItem>
<bodyText confidence="0.981294">
pp2(addStars(changeMarkup(markViolation(
containsOutputXXGen,
[lb outx* rb lb outx* rb])))) ;
In general, however, just adding stars is not suffi-
cient to obtain an automaton that matches every non-
optimal candidate. A simple example suffices to illus-
trate the problem. Suppose that there are two candi-
dates, where the better one is ab*c and the worse one
is a*bc*. Here it is not possible to add a star to the
better candidate to match the worse candidate. The
issue is that stars need to be not only added but also
moved around in order to match up. Here it is in gen-
eral not possible using finite state methods, to obtain
all possible placements of the stars. So various levels
of approximation must be defined.
</bodyText>
<listItem confidence="0.988139272727273">
• define permuteStarLeft
[e:star tcomp(star)* star:e];
• define permuteStarRight
[star:e tcomp(star)* e:star];
• define permuteStars
[xsig* [permuteLeft U permuteRight]]* xsig*;
• define permute0(X) X2;
• define permute1(X) [X o permuteStars]2;
• define permute2(X)
[X o permuteStars o permuteStars]2;
• etc
</listItem>
<bodyText confidence="0.9996845">
In theory, an unbounded amount of permutation
may be necessary in order to optimize all possible in-
puts. In practice, however, this is only the case in
highly artificial examples such as the one constructed
by Frank and Satta [2]. The normal case is that only
permute0 and permute1 are needed. To test how much
permutation is needed, one can use the isFunctional13
test described in Gerdemann &amp; van Noord [5].
</bodyText>
<listItem confidence="0.982806333333333">
• define isOptimized(Starred)
isfunctional([Unflatten(Starred)
o
</listItem>
<equation confidence="0.295056">
Σ-star -&gt; e]);
</equation>
<bodyText confidence="0.99994175">
The idea here can be illustrated quite simply. Sup-
pose the input abc results in two outputs xa*ybc* and
axb*z where {x, y, z} is markup. If we delete every-
thing but the stars, then abc is mapped to ** and *.
The isFunctional test is designed to rule out cases like
this. Each input should be mapped to a set of candi-
dates with a unique number of stars. If this condition
holds, then the set of candidates has been optimized.
By using the isFunctional test, it can be determined
that no permutation of stars is necessary for this case.
So we can optimize in the following way, where the
result is shown in Figure 9.
</bodyText>
<listItem confidence="0.907669">
• define optimizeStars0(StarredCandidates) [
StarredCandidates
</listItem>
<bodyText confidence="0.430417">
-
permute0(
addStars(
changeMarkup(StarredCandidates)))];
</bodyText>
<footnote confidence="0.763154">
13 In Foma, this is called isfunctional. The return values are
true and false as described in section 2.1.3,
</footnote>
<page confidence="0.998666">
44
</page>
<figureCaption confidence="0.9994815">
Fig. 5: containsOutputXXGen
Fig. 6: Starred violations of no iterated epenthesis
Fig. 7: After changing the markup
Fig. 8: After adding stars
</figureCaption>
<figure confidence="0.998057071428571">
abc sl
�
x
s2
�
abcx
so
abc
x
0
&lt;
x
s6
&lt;
s3
01
&gt;
S5
ebc
�
s2
�b—
�
�
ebc
��
sJ
.0
�
sl
ebc
��
s3
�
.6
�
.10
�� �
�
�
.8
abcx
6
�
�
sa
�
.9 yll
a::aa&lt;b::bb&gt;c:
a::aa&lt;b::bb&gt;c
:c c * x::x x
SI
* x: x
:c c :x
S0
a::aa&lt;b::bb&gt;c:
:c c :x
a::aa&lt;b::bb&gt;c:
:c c :x s2
*
a::aa&lt;b::bb&gt;c:
:c c * x::x x
* x: x
s0
a::aa&lt;b::bb&gt;c:
s1
x: x
:c c x::x x
*
s3
</figure>
<page confidence="0.895564">
45
</page>
<figureCaption confidence="0.981517">
Fig. 9: After optimizing
</figureCaption>
<figure confidence="0.958654875">
&lt;Ox&gt;
a b c X
X &lt;Ox&gt; s5
abc sl
X
abc
X
a b c X
X
abc
s7
s4
sO
abc
abc
s3
&lt;Ox&gt;
X
&lt;Ox&gt;
s�
&lt;Ox&gt;
abc
s6
abc
&lt;Ox&gt;
s8
• define opimizedRewriteRule
unflatten(
optimizeStars0(
markViolation(
containsOutputXXGen,
[lb outx* rb lb outx* rb])));
</figure>
<bodyText confidence="0.99961475">
The transducer in Figure 9 is fairly complex, so it
is not obvious that it is correct. The skeptical reader
should study some sample input-output such as the
following.
</bodyText>
<listItem confidence="0.8895442">
• ab H {abxx,axbxx,axxb,axxbx,xabxx,xaxbxx,
xaxxb,xaxxbx,xxab,xxabx,xxaxb,xxaxbx}
• ax H {axx,axxx,xaxx,xaxxx}
• axx H {axx,axxx,axxxx,axxxxx,xaxx,xaxxx,
xaxxxx,xaxxxxx}
</listItem>
<sectionHeader confidence="0.971168" genericHeader="conclusions">
6 Conclusion and further direc-
tions
</sectionHeader>
<bodyText confidence="0.99953864">
It may seem like a lot of work has been expended in
order to define a fairly simple replacement rule. But
most of this work has gone into preparatory ground
work. Further constraints can now be added in a fairly
routine way. Certainly, constraints referring to the left
and right context are necessary, and now are easy to
define. And other, more exotic constraints are not
difficult. The user of a toolbox is thus not bound by
the particular flavors of replacement rules provided by
the toolbox.
For optimizing constraints, I have introduced two
kinds of worsening: star-based and generalized.
Though this paper has concentrated on star-based
worsening, it may well be the case that generalized
worsening is even more important. The idea of gener-
alized worsening is that markup can be directly ma-
nipulated to make candidates worse. For example, for
a longest match constraint, the worsening transducer
could manipulate the marked-up matches to make the
matches shorter. Or a worsener for a leftmost con-
straint could manipulate markup to move matches fur-
ther to the right. It is clear that a worsening trans-
ducer should encode an irreflexive, asymmetrical, tran-
sitive relation. Although these properties are in gen-
eral undecidable [7], in practical cases it is usually clear
enough that a worsening transducer is well formed.
Generalized worsening is particularly useful for de-
scribing prosodic constraints. For example, medieval
Indian prosodists introduced following hierarchy of
long-short syllable patterns (Singh, [13]), where long
syllables count as two positions and short syllables
count as one: SSSSS &lt; LSSS &lt; SLSS &lt; SSLS &lt; LLS &lt;
SSSL &lt; LSL &lt; SLL.14 This looks remarkably like some
versions of generalized alignment in Optimality The-
ory. For star-based worsening, the relevant constraint
assigns stars to each long syllable, where the number
of stars for a long syllable starting in position i is Fi+1,
the i+1st Fibonacci number, and short syllables do not
get stars. So with stars added, the hierarchy becomes:
SSSSS &lt; L*SSS &lt; SL**SS &lt; SSL***S &lt; L*L***S &lt;
SSSL***** &lt; L*SL***** &lt; SL**L*****. Clearly it
is not possible to define a transducer to add such a
pattern of stars, so the only hope here is to use gen-
eralized worsening. The details are a little tricky, but
the basic principle is to treat L as a bracketed pair of
S’s and let worsening move the bracketing to the right
or introduce additional bracketing.15
Although there may be tricky issues in defining a
generalized worsening transducer, the optimization it-
self is very straightforward.
</bodyText>
<listItem confidence="0.9756115">
• define optimize(G, Worsener)
G - [G o Worsener]2;
</listItem>
<bodyText confidence="0.999873916666667">
To summarize, then, replacement rules have been
encoded in a declarative, one-level way, allowing both
optimizing and non-optimizing constraints to be mixed
and combined in a natural and easy way. In combin-
ing OT-style constraints with traditional Generative
Phonology-style replacement rules, one may see Kisse-
berth [10] as a source of inspiration. Long ago, before
OT was invented, Kisseberth spoke of rules conspir-
ing to create certain surface effects. The approach
presented in this paper is intended to allow such con-
spiratorial effects to be compiled directly into the re-
placement rules.
</bodyText>
<footnote confidence="0.682002333333333">
14 For some issues concerning this pattern, see Gerdemann [3].
15 See www.sfs.uni-tuebingen.de/∼dg/mixmatch/zeck.fom for
details.
</footnote>
<page confidence="0.999669">
46
</page>
<sectionHeader confidence="0.99539" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998204595744681">
[1] K. R. Beesley and L. Karttunen. Finite State Morphology.
CSLI, Stanford, 2003.
[2] R. Frank and G. Satta. Optimality theory and the compu-
tational complexity of constraint violability. Computational
Linguistics, 24:307–315, 1998.
[3] D. Gerdemann. Combinatorial proofs of Zeckendorf fam-
ily identities. Fibonacci Quarterly, pages 249–262, August,
2008/2009.
[4] D. Gerdemann and G. van Noord. Transducers from rewrite
rules with backreferences. In Ninth Conference of the Euro-
pean Chapter of the Association for Computational Linguis-
tics, Bergen Norway, 1999.
[5] D. Gerdemann and G. van Noord. Approximation and exact-
ness in finite state optimality theory. In J. Eisner, L. Kart-
tunen, and A. Th´eriault, editors, SIGPHON 2000, Finite
State Phonology. Proceedings of the Fifth Workshop of the
ACL Special Interest Group in Computational Phonology,
Luxembourg, 2000.
[6] M. Hulden. Foma: a finite-state compiler and library. In Pro-
ceedings of the EACL Demonstrations Session, pages 29–32,
2009.
[7] J. H. Johnson. Rational equivalence relations. Theoretical
Computer Science, 47(1):39–60, 1986.
[8] R. Kaplan and M. Kay. Regular models of phonological rule
systems. Computational Linguistics, 20(3):331–379, 1994.
[9] L. Karttunen. Directed replacement. In 34th Annual Meet-
ing of the Association for Computational Linguistics, Santa
Cruz, 1996.
[10] C. Kisseberth. On the functional unity of phonological rules.
Linguistic Inquiry, 1:291–306, 1970.
[11] K. Koskenniemi. Two level morphology: A general computa-
tional model for word-form recognition and production, 1983.
Publication No. 11, Department of General Linguistics, Uni-
versity of Helsinki.
[12] M. Mohri and R. Sproat. An efficient compiler for weighted
rewrite rules. In 34th Annual Meeting of the Association for
Computational Linguistics, Santa Cruz, 1996.
[13] P. Singh. The so-called Fibonacci numbers in ancient and me-
dieval India. Historia Mathematica, 12(3):229–244, 1985.
[14] G. van Noord and D. Gerdemann. An extendible regular ex-
pression compiler for finite-state approaches in natural lan-
guage processing. In O. Boldt, H. Juergensen, and L. Rob-
bins, editors, Workshop on Implementing Automata; WIA99
Pre-Proceedings, Potsdam, Germany, 1999.
[15] A. Yli-Jyr¨a. Transducers from parallel replace rules and modes
with generalized lenient composition. In Finite-state methods
and natural language processing, 2007.
</reference>
<page confidence="0.999462">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.732713">
<title confidence="0.999888">Mix and Match Replacement Rules</title>
<author confidence="0.998889">Dale</author>
<affiliation confidence="0.98715">Department of Universit¨at</affiliation>
<address confidence="0.923557">72074 T¨ubingen,</address>
<abstract confidence="0.97471925">A flexible construction kit is presented compiling various forms of finite state replacement rules. The approach is simpler and more declarative than algorithms in the tradition of Kaplan &amp; Kay. Simple constraints can be combined to achieve complex effects, including effects based on Optimality Theory.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K R Beesley</author>
<author>L Karttunen</author>
</authors>
<title>Finite State Morphology.</title>
<date>2003</date>
<location>CSLI, Stanford,</location>
<contexts>
<context position="2269" citStr="[1]" startWordPosition="334" endWordPosition="334">lacement rules that have not appeared in the literature. One of the advantages of finite state technology is reusability. An implementation designed for one finite state toolbox can easily be redeployed with a different toolbox. At least that is the theory. In practice, toolboxes tend to provide a number of incompatible features. Replacement rules, in particular, occur in many variations: optional/obligatory, left-to-right/right-to-left/simultaneous, longestmatching/shortest-matching, etc. Sometimes a rather small variation has required introducing some new syntax. The widely used Xfst system [1], for example, 1 More precisely, regular relations are not closed under intersection. It is convenient, however, to systematically confuse finite state transducers (respectively, finite state acceptors) with the regular relations (respectively, regular languages) that they encode. has special syntax (dotted brackets) used to enforce a constraint against repeated epenthesis. While this is certainly a useful option, it is also an impediment to users who wish to transfer their Xfst implementation to another toolbox. Clearly such users would benefit more from a kind of replacement rule constructio</context>
<context position="18424" citStr="[1]" startWordPosition="2957" endWordPosition="2957">ortant enough to merit some special syntax in Xfst.10 This is easy to define in the mix-and-match approach, using auxiliary definitions for complement and containment. The automaton is shown in Figure 2. • define comp(E) xsig* - E; • define contain(E) [xsig* E xsig*]; • define noIterEpenStrict comp(contain([lb outx* rb lb outx* rb])); • regex pp1(replaceGen(flattenCross(ǫ, x)) n noIterEpenStrict); 8 For transducers, Foma provides the boolean tests: isfunctional and isidentity. 9 See: www.sfs.uni-tuebingen.de/∼dg/mixmatch/flatten for a mix of Foma code and Perl code. 10 See Beesley &amp; Karttunen [1], p. 67. abcx :x � s1 so � s� abcx �x s1 s2 � � so abcx s� 42 Fig. 3: Non-iterated Epenthesis, unflattened version The unflattened version, compiled from the following code, is shown in Figure 3. • regex unflatten( replaceGen(flattenCross(ǫ, x)) n noIterEpenStrict); 5 Defeasible Constraints Now let us relax the constraint against iterated epenthesis and allow iterated epenthesis only when there is no other alternative. Suppose that there is a higher ranking constraint requiring the output to have a sequence of two x’s. If the input already has a sequence of two x’s, then the output constraint </context>
</contexts>
<marker>[1]</marker>
<rawString>K. R. Beesley and L. Karttunen. Finite State Morphology. CSLI, Stanford, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Frank</author>
<author>G Satta</author>
</authors>
<title>Optimality theory and the computational complexity of constraint violability.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<contexts>
<context position="3347" citStr="[2]" startWordPosition="492" endWordPosition="492">eir Xfst implementation to another toolbox. Clearly such users would benefit more from a kind of replacement rule construction kit, allowing such variant replacement rules to be put together as needed. But the procedural style of Kaplan &amp; Kay-inspired algorithms makes the development of such a construction kit very difficult. Given the declarative approach developed in this paper, the construction kit approach can be implemented in any standard finite-state toolkit. The declarative approach to replacement rules is very flexible, allowing for example, variants of finitestate Optimality Theory ([2] [5]) to be incorporated into the rules. In some sense, this is not new, as Karttunen’s approach to longest match replacement [9], was already a kind of optimization. The declarative, mix-and-match approach, however, allows for the easy incorporation of various optimality constraints. For example, a constraint against repeated epenthesis could be loosened to allow the minimal amount of epenthesis consistent with some other constraints.2 2 Flat Representation for Transducers using Types The general form for a finite-state rewrite rule is as in 1: x → T(x)/a p (1) This is understood as a rule fo</context>
<context position="25135" citStr="[2]" startWordPosition="4089" endWordPosition="4089">of the stars. So various levels of approximation must be defined. • define permuteStarLeft [e:star tcomp(star)* star:e]; • define permuteStarRight [star:e tcomp(star)* e:star]; • define permuteStars [xsig* [permuteLeft U permuteRight]]* xsig*; • define permute0(X) X2; • define permute1(X) [X o permuteStars]2; • define permute2(X) [X o permuteStars o permuteStars]2; • etc In theory, an unbounded amount of permutation may be necessary in order to optimize all possible inputs. In practice, however, this is only the case in highly artificial examples such as the one constructed by Frank and Satta [2]. The normal case is that only permute0 and permute1 are needed. To test how much permutation is needed, one can use the isFunctional13 test described in Gerdemann &amp; van Noord [5]. • define isOptimized(Starred) isfunctional([Unflatten(Starred) o Σ-star -&gt; e]); The idea here can be illustrated quite simply. Suppose the input abc results in two outputs xa*ybc* and axb*z where {x, y, z} is markup. If we delete everything but the stars, then abc is mapped to ** and *. The isFunctional test is designed to rule out cases like this. Each input should be mapped to a set of candidates with a unique num</context>
</contexts>
<marker>[2]</marker>
<rawString>R. Frank and G. Satta. Optimality theory and the computational complexity of constraint violability. Computational Linguistics, 24:307–315, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gerdemann</author>
</authors>
<title>Combinatorial proofs of Zeckendorf family identities. Fibonacci Quarterly,</title>
<date>2008</date>
<pages>249--262</pages>
<marker>[3]</marker>
<rawString>D. Gerdemann. Combinatorial proofs of Zeckendorf family identities. Fibonacci Quarterly, pages 249–262, August, 2008/2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gerdemann</author>
<author>G van Noord</author>
</authors>
<title>Transducers from rewrite rules with backreferences.</title>
<date>1999</date>
<booktitle>In Ninth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Bergen</location>
<contexts>
<context position="904" citStr="[4]" startWordPosition="130" endWordPosition="130">an algorithms in the tradition of Kaplan &amp; Kay. Simple constraints can be combined to achieve complex effects, including effects based on Optimality Theory. 1 Introduction Traditional finite state algorithms for compiling replacement rules have a very procedural flavor. The general approach originated by Kaplan &amp; Kay [8] involves steps for introducing markers, constraining the markers in various ways, making the actual replacement, constraining the markers again after the replacement and finally removing the markers. Several variants of Kaplan &amp; Kay’s algorithm have been presented ([9], [12], [4]) with the goals being either to improve efficiency or to provide slightly different semantics. In this paper, I present an alternative, more declarative approach. Rather than being based on a sequence of steps that are composed together, this approach is based on intersection. A basic, unconstrained replacement is represented by a transducer, which can then be constrained by intersecting this transducer with automata representing a variety of constraints. The obvious objection to this approach is that transducers are not closed under intersection.1 It turns out, however, that a rather limited</context>
<context position="4215" citStr="[4]" startWordPosition="632" endWordPosition="632">imality constraints. For example, a constraint against repeated epenthesis could be loosened to allow the minimal amount of epenthesis consistent with some other constraints.2 2 Flat Representation for Transducers using Types The general form for a finite-state rewrite rule is as in 1: x → T(x)/a p (1) This is understood as a rule for replacing the string x with the transduction T (x) in the specified context, with some unspecified mode of operation such as obligatory left-to-right. Traditionally T is specified as a cross-product of regular languages, but as discussed in Gerdemann &amp; van Noord [4], this is by no means necessary. In order to develop an approach based on intersection, a rule such as 1 (ignoring context, and other constraints) will first be transformed into a finite state acceptor (FSA). There is of course a standard trick of using same-length transducers, which goes back to the early days of finite-state morphology [11]. A samelength transducer can be encoded as an FSA with 2 This paper concentrates on some fairly simple examples that are good for expository purposes. For further examples, see: www.sfs.uni-tuebingen.de/∼dg/mixmatch. 39 Workshop Adaptation of Language Res</context>
</contexts>
<marker>[4]</marker>
<rawString>D. Gerdemann and G. van Noord. Transducers from rewrite rules with backreferences. In Ninth Conference of the European Chapter of the Association for Computational Linguistics, Bergen Norway, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gerdemann</author>
<author>G van Noord</author>
</authors>
<title>Approximation and exactness in finite state optimality theory.</title>
<date>2000</date>
<booktitle>SIGPHON 2000, Finite State Phonology. Proceedings of the Fifth Workshop of the ACL Special Interest Group in Computational Phonology,</booktitle>
<editor>In J. Eisner, L. Karttunen, and A. Th´eriault, editors,</editor>
<location>Luxembourg,</location>
<contexts>
<context position="3351" citStr="[5]" startWordPosition="493" endWordPosition="493">Xfst implementation to another toolbox. Clearly such users would benefit more from a kind of replacement rule construction kit, allowing such variant replacement rules to be put together as needed. But the procedural style of Kaplan &amp; Kay-inspired algorithms makes the development of such a construction kit very difficult. Given the declarative approach developed in this paper, the construction kit approach can be implemented in any standard finite-state toolkit. The declarative approach to replacement rules is very flexible, allowing for example, variants of finitestate Optimality Theory ([2] [5]) to be incorporated into the rules. In some sense, this is not new, as Karttunen’s approach to longest match replacement [9], was already a kind of optimization. The declarative, mix-and-match approach, however, allows for the easy incorporation of various optimality constraints. For example, a constraint against repeated epenthesis could be loosened to allow the minimal amount of epenthesis consistent with some other constraints.2 2 Flat Representation for Transducers using Types The general form for a finite-state rewrite rule is as in 1: x → T(x)/a p (1) This is understood as a rule for re</context>
<context position="25314" citStr="[5]" startWordPosition="4120" endWordPosition="4120">define permuteStars [xsig* [permuteLeft U permuteRight]]* xsig*; • define permute0(X) X2; • define permute1(X) [X o permuteStars]2; • define permute2(X) [X o permuteStars o permuteStars]2; • etc In theory, an unbounded amount of permutation may be necessary in order to optimize all possible inputs. In practice, however, this is only the case in highly artificial examples such as the one constructed by Frank and Satta [2]. The normal case is that only permute0 and permute1 are needed. To test how much permutation is needed, one can use the isFunctional13 test described in Gerdemann &amp; van Noord [5]. • define isOptimized(Starred) isfunctional([Unflatten(Starred) o Σ-star -&gt; e]); The idea here can be illustrated quite simply. Suppose the input abc results in two outputs xa*ybc* and axb*z where {x, y, z} is markup. If we delete everything but the stars, then abc is mapped to ** and *. The isFunctional test is designed to rule out cases like this. Each input should be mapped to a set of candidates with a unique number of stars. If this condition holds, then the set of candidates has been optimized. By using the isFunctional test, it can be determined that no permutation of stars is necessar</context>
</contexts>
<marker>[5]</marker>
<rawString>D. Gerdemann and G. van Noord. Approximation and exactness in finite state optimality theory. In J. Eisner, L. Karttunen, and A. Th´eriault, editors, SIGPHON 2000, Finite State Phonology. Proceedings of the Fifth Workshop of the ACL Special Interest Group in Computational Phonology, Luxembourg, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hulden</author>
</authors>
<title>Foma: a finite-state compiler and library.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL Demonstrations Session,</booktitle>
<pages>29--32</pages>
<contexts>
<context position="7616" citStr="[6]" startWordPosition="1200" endWordPosition="1200"> are neither input nor output (marker symbols) and also for symbols that are both input and output (identity symbols). Possibly, for some algorithms, it would be useful to have a several types of marker symbols, but for now four symbols will be enough. • define input a; • define output b; • define identity c; • define marker x; • define symbolTypeIndicator [input U output U identity U marker]; 3 It is nevertheless possible using a two-level approach to work out ideas similar to those in this paper. See Yli-Jyr¨a [15]. Note that the self-explanatory syntax here is as in the Foma system (Hulden [6]). The particular choice of symbols used here is arbitrary and can easily be changed, Extended alphabet symbols thus consist of a sequence of two symbols, where the second symbol determines the type of the first symbol. With the preceding conventions, for example, the sequence [z a] (= [z input]) represents ‘z’ as an input symbol, and [&lt; x] (= [&lt; marker]) represents the angle bracket ‘&lt;’ used as a marker symbol. For the purpose of illustration with a running example, a small test alphabet is defined here (even though Foma does not require the alphabet to be specified). In these definitions, xs</context>
</contexts>
<marker>[6]</marker>
<rawString>M. Hulden. Foma: a finite-state compiler and library. In Proceedings of the EACL Demonstrations Session, pages 29–32, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Johnson</author>
</authors>
<title>Rational equivalence relations.</title>
<date>1986</date>
<journal>Theoretical Computer Science,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="28593" citStr="[7]" startWordPosition="4697" endWordPosition="4697">n star-based worsening, it may well be the case that generalized worsening is even more important. The idea of generalized worsening is that markup can be directly manipulated to make candidates worse. For example, for a longest match constraint, the worsening transducer could manipulate the marked-up matches to make the matches shorter. Or a worsener for a leftmost constraint could manipulate markup to move matches further to the right. It is clear that a worsening transducer should encode an irreflexive, asymmetrical, transitive relation. Although these properties are in general undecidable [7], in practical cases it is usually clear enough that a worsening transducer is well formed. Generalized worsening is particularly useful for describing prosodic constraints. For example, medieval Indian prosodists introduced following hierarchy of long-short syllable patterns (Singh, [13]), where long syllables count as two positions and short syllables count as one: SSSSS &lt; LSSS &lt; SLSS &lt; SSLS &lt; LLS &lt; SSSL &lt; LSL &lt; SLL.14 This looks remarkably like some versions of generalized alignment in Optimality Theory. For star-based worsening, the relevant constraint assigns stars to each long syllable, </context>
</contexts>
<marker>[7]</marker>
<rawString>J. H. Johnson. Rational equivalence relations. Theoretical Computer Science, 47(1):39–60, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
<author>M Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="623" citStr="[8]" startWordPosition="86" endWordPosition="86">ement Rules Dale Gerdemann Department of Linguistics Universit¨at T¨ubingen 72074 T¨ubingen, Germany dg@sfs.uni-tuebingen.de Abstract A flexible construction kit is presented compiling various forms of finite state replacement rules. The approach is simpler and more declarative than algorithms in the tradition of Kaplan &amp; Kay. Simple constraints can be combined to achieve complex effects, including effects based on Optimality Theory. 1 Introduction Traditional finite state algorithms for compiling replacement rules have a very procedural flavor. The general approach originated by Kaplan &amp; Kay [8] involves steps for introducing markers, constraining the markers in various ways, making the actual replacement, constraining the markers again after the replacement and finally removing the markers. Several variants of Kaplan &amp; Kay’s algorithm have been presented ([9], [12], [4]) with the goals being either to improve efficiency or to provide slightly different semantics. In this paper, I present an alternative, more declarative approach. Rather than being based on a sequence of steps that are composed together, this approach is based on intersection. A basic, unconstrained replacement is re</context>
<context position="19442" citStr="[8]" startWordPosition="3132" endWordPosition="3132">native. Suppose that there is a higher ranking constraint requiring the output to have a sequence of two x’s. If the input already has a sequence of two x’s, then the output constraint will in any case be satisfied. If the input only has singleton x’s, then epenthesis is needed. But iterated epenthesis is not needed. If, however, the input has no x’s, then iterated epenthesis is needed. But it is still desirable to do no more iterated epenthesis than necessary. We start, as in OT, by putting a star after every constraint violation. To do this, we need some standard utilities from Kaplan &amp; Kay [8].11 • define intro(S) [[E E] U ǫ:S]*; • define introx(S) [[E E] U ǫ:S]* [E E]; • define xintro(S) [E E] [[E E] U ǫ:S]*; • define ign(L, S) [L o intro(S)]2; • define ignx(L, S) [L o introx(S)]2; • define xign(L, S) [L o xintro(S)]2; • define ifPThenS(P,S) comp([P comp(S)]); • define ifSThenP(P,S) comp([comp(P) S]); • define PIffS(P,S) ifPThenS(P,S) n ifSThenP(P,S); Using these definitions, we can introduce a star, as in OT, and constrain it to occur after constraint violations. • define star [c marker]; • define ppStar star:%*; 11 As a useful convention, ‘x’ at the beginning means “except at th</context>
</contexts>
<marker>[8]</marker>
<rawString>R. Kaplan and M. Kay. Regular models of phonological rule systems. Computational Linguistics, 20(3):331–379, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
</authors>
<title>Directed replacement.</title>
<date>1996</date>
<booktitle>In 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Santa Cruz,</location>
<contexts>
<context position="893" citStr="[9]" startWordPosition="128" endWordPosition="128">larative than algorithms in the tradition of Kaplan &amp; Kay. Simple constraints can be combined to achieve complex effects, including effects based on Optimality Theory. 1 Introduction Traditional finite state algorithms for compiling replacement rules have a very procedural flavor. The general approach originated by Kaplan &amp; Kay [8] involves steps for introducing markers, constraining the markers in various ways, making the actual replacement, constraining the markers again after the replacement and finally removing the markers. Several variants of Kaplan &amp; Kay’s algorithm have been presented ([9], [12], [4]) with the goals being either to improve efficiency or to provide slightly different semantics. In this paper, I present an alternative, more declarative approach. Rather than being based on a sequence of steps that are composed together, this approach is based on intersection. A basic, unconstrained replacement is represented by a transducer, which can then be constrained by intersecting this transducer with automata representing a variety of constraints. The obvious objection to this approach is that transducers are not closed under intersection.1 It turns out, however, that a rat</context>
<context position="3476" citStr="[9]" startWordPosition="514" endWordPosition="514">t, allowing such variant replacement rules to be put together as needed. But the procedural style of Kaplan &amp; Kay-inspired algorithms makes the development of such a construction kit very difficult. Given the declarative approach developed in this paper, the construction kit approach can be implemented in any standard finite-state toolkit. The declarative approach to replacement rules is very flexible, allowing for example, variants of finitestate Optimality Theory ([2] [5]) to be incorporated into the rules. In some sense, this is not new, as Karttunen’s approach to longest match replacement [9], was already a kind of optimization. The declarative, mix-and-match approach, however, allows for the easy incorporation of various optimality constraints. For example, a constraint against repeated epenthesis could be loosened to allow the minimal amount of epenthesis consistent with some other constraints.2 2 Flat Representation for Transducers using Types The general form for a finite-state rewrite rule is as in 1: x → T(x)/a p (1) This is understood as a rule for replacing the string x with the transduction T (x) in the specified context, with some unspecified mode of operation such as ob</context>
<context position="17559" citStr="[9]" startWordPosition="2829" endWordPosition="2829">. This automaton looks superficially like an intermediate step in a traditional replacement rule compilation procedure. But in fact, it is considerably different. The brackets, for example, are not inserted as steps in a procedure. Constraints may refer to the brackets, but the brackets are neither in the input nor in the output. 4 Constraints Constraints are of two basic types: strict and optimizing. Contextual constraints are, for example, traditionally treated as strict constraints, and optimizing constraints are typically limited to constraints for leftmost and longest matching (Karttunen [9]). Strict constraints, being easier, are a natural starting point. 4.1 Strict Constraints As a simple example of a strict constraint, consider a constraint against iterated epenthesis. This may seem like a minor constraint, but nevertheless it was considered important enough to merit some special syntax in Xfst.10 This is easy to define in the mix-and-match approach, using auxiliary definitions for complement and containment. The automaton is shown in Figure 2. • define comp(E) xsig* - E; • define contain(E) [xsig* E xsig*]; • define noIterEpenStrict comp(contain([lb outx* rb lb outx* rb])); •</context>
</contexts>
<marker>[9]</marker>
<rawString>L. Karttunen. Directed replacement. In 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kisseberth</author>
</authors>
<title>On the functional unity of phonological rules.</title>
<date>1970</date>
<journal>Linguistic Inquiry,</journal>
<volume>1</volume>
<marker>[10]</marker>
<rawString>C. Kisseberth. On the functional unity of phonological rules. Linguistic Inquiry, 1:291–306, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Koskenniemi</author>
</authors>
<title>Two level morphology: A general computational model for word-form recognition and production,</title>
<date>1983</date>
<journal>Publication</journal>
<volume>11</volume>
<institution>Department of General Linguistics, University of Helsinki.</institution>
<contexts>
<context position="4559" citStr="[11]" startWordPosition="691" endWordPosition="691">ing the string x with the transduction T (x) in the specified context, with some unspecified mode of operation such as obligatory left-to-right. Traditionally T is specified as a cross-product of regular languages, but as discussed in Gerdemann &amp; van Noord [4], this is by no means necessary. In order to develop an approach based on intersection, a rule such as 1 (ignoring context, and other constraints) will first be transformed into a finite state acceptor (FSA). There is of course a standard trick of using same-length transducers, which goes back to the early days of finite-state morphology [11]. A samelength transducer can be encoded as an FSA with 2 This paper concentrates on some fairly simple examples that are good for expository purposes. For further examples, see: www.sfs.uni-tuebingen.de/∼dg/mixmatch. 39 Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 39–47 transitions labeled with pairs of symbols, or equivalently by an FSA in which odd-numbered transitions (from the start state) count as inputs, and even numbered transitions count as outputs. This is, however, an inconvenient representation for two reasons. First, we o</context>
</contexts>
<marker>[11]</marker>
<rawString>K. Koskenniemi. Two level morphology: A general computational model for word-form recognition and production, 1983. Publication No. 11, Department of General Linguistics, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>R Sproat</author>
</authors>
<title>An efficient compiler for weighted rewrite rules.</title>
<date>1996</date>
<booktitle>In 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Santa Cruz,</location>
<contexts>
<context position="899" citStr="[12]" startWordPosition="129" endWordPosition="129">ive than algorithms in the tradition of Kaplan &amp; Kay. Simple constraints can be combined to achieve complex effects, including effects based on Optimality Theory. 1 Introduction Traditional finite state algorithms for compiling replacement rules have a very procedural flavor. The general approach originated by Kaplan &amp; Kay [8] involves steps for introducing markers, constraining the markers in various ways, making the actual replacement, constraining the markers again after the replacement and finally removing the markers. Several variants of Kaplan &amp; Kay’s algorithm have been presented ([9], [12], [4]) with the goals being either to improve efficiency or to provide slightly different semantics. In this paper, I present an alternative, more declarative approach. Rather than being based on a sequence of steps that are composed together, this approach is based on intersection. A basic, unconstrained replacement is represented by a transducer, which can then be constrained by intersecting this transducer with automata representing a variety of constraints. The obvious objection to this approach is that transducers are not closed under intersection.1 It turns out, however, that a rather li</context>
</contexts>
<marker>[12]</marker>
<rawString>M. Mohri and R. Sproat. An efficient compiler for weighted rewrite rules. In 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Singh</author>
</authors>
<title>The so-called Fibonacci numbers in ancient and medieval India.</title>
<date>1985</date>
<journal>Historia Mathematica,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="28882" citStr="[13]" startWordPosition="4736" endWordPosition="4736"> the marked-up matches to make the matches shorter. Or a worsener for a leftmost constraint could manipulate markup to move matches further to the right. It is clear that a worsening transducer should encode an irreflexive, asymmetrical, transitive relation. Although these properties are in general undecidable [7], in practical cases it is usually clear enough that a worsening transducer is well formed. Generalized worsening is particularly useful for describing prosodic constraints. For example, medieval Indian prosodists introduced following hierarchy of long-short syllable patterns (Singh, [13]), where long syllables count as two positions and short syllables count as one: SSSSS &lt; LSSS &lt; SLSS &lt; SSLS &lt; LLS &lt; SSSL &lt; LSL &lt; SLL.14 This looks remarkably like some versions of generalized alignment in Optimality Theory. For star-based worsening, the relevant constraint assigns stars to each long syllable, where the number of stars for a long syllable starting in position i is Fi+1, the i+1st Fibonacci number, and short syllables do not get stars. So with stars added, the hierarchy becomes: SSSSS &lt; L*SSS &lt; SL**SS &lt; SSL***S &lt; L*L***S &lt; SSSL***** &lt; L*SL***** &lt; SL**L*****. Clearly it is not po</context>
</contexts>
<marker>[13]</marker>
<rawString>P. Singh. The so-called Fibonacci numbers in ancient and medieval India. Historia Mathematica, 12(3):229–244, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
<author>D Gerdemann</author>
</authors>
<title>An extendible regular expression compiler for finite-state approaches in natural language processing. In</title>
<date>1999</date>
<booktitle>Workshop on Implementing Automata; WIA99 Pre-Proceedings,</booktitle>
<editor>O. Boldt, H. Juergensen, and L. Robbins, editors,</editor>
<location>Potsdam, Germany,</location>
<contexts>
<context position="13436" citStr="[14]" startWordPosition="2168" endWordPosition="2168">nters will be introduced as needed. First, however, we should start to see how the type system can be useful for our compilation. 2.1.3 Assertions and Boolean Tests An important invariant of the typed approach is that every sequence is of even length and every symbol in an even position is a symbol type indicator. Consistent use of matchers, constructors and accessors will help to ensure that this invariant holds. Nevertheless, things can go wrong, so it is important to use boolean tests as assertions. Boolean types were introduced into the regular expression calculus in van Noord &amp; Gerdemann [14], though the details are worked out differently here. The central idea is similar to conventions in a variety of programming languages, where certain designated values are understood as, or coerced to boolean values. In the finite state domain, it is convenient let ǫ be true, and 0 be false. • define true ǫ; • define false 0; Note that with these conventions, true is a singlestate FSA where the one state is both initial and final, and false is a single-state FSA where the one state is non-final.&apos; Given these conventions, the boolean connectives and, or and not can be defined as concatenation, </context>
</contexts>
<marker>[14]</marker>
<rawString>G. van Noord and D. Gerdemann. An extendible regular expression compiler for finite-state approaches in natural language processing. In O. Boldt, H. Juergensen, and L. Robbins, editors, Workshop on Implementing Automata; WIA99 Pre-Proceedings, Potsdam, Germany, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yli-Jyr¨a</author>
</authors>
<title>Transducers from parallel replace rules and modes with generalized lenient composition.</title>
<date>2007</date>
<booktitle>In Finite-state methods and natural language processing,</booktitle>
<contexts>
<context position="7535" citStr="[15]" startWordPosition="1186" endWordPosition="1186">or output symbols. More usefully, there should also be indicators for symbols that are neither input nor output (marker symbols) and also for symbols that are both input and output (identity symbols). Possibly, for some algorithms, it would be useful to have a several types of marker symbols, but for now four symbols will be enough. • define input a; • define output b; • define identity c; • define marker x; • define symbolTypeIndicator [input U output U identity U marker]; 3 It is nevertheless possible using a two-level approach to work out ideas similar to those in this paper. See Yli-Jyr¨a [15]. Note that the self-explanatory syntax here is as in the Foma system (Hulden [6]). The particular choice of symbols used here is arbitrary and can easily be changed, Extended alphabet symbols thus consist of a sequence of two symbols, where the second symbol determines the type of the first symbol. With the preceding conventions, for example, the sequence [z a] (= [z input]) represents ‘z’ as an input symbol, and [&lt; x] (= [&lt; marker]) represents the angle bracket ‘&lt;’ used as a marker symbol. For the purpose of illustration with a running example, a small test alphabet is defined here (even tho</context>
</contexts>
<marker>[15]</marker>
<rawString>A. Yli-Jyr¨a. Transducers from parallel replace rules and modes with generalized lenient composition. In Finite-state methods and natural language processing, 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>