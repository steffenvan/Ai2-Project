<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.972191">
Bilingual Active Learning for Relation Classification via Pseudo Paral-
lel Corpora
</title>
<author confidence="0.893511">
Longhua Qian Haotian Hui Ya’nan Hu Guodong Zhou* Qiaoming Zhu
</author>
<affiliation confidence="0.795995666666667">
Natural Language Processing Lab
School of Computer Science and Technology, Soochow University
1 Shizi Street, Suzhou, China 215006
</affiliation>
<email confidence="0.997557">
{qianlonghua,20134227019,20114227025,gdzhou,qmzhu}@suda.edu.cn
</email>
<sectionHeader confidence="0.998585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885772727273">
Active learning (AL) has been proven ef-
fective to reduce human annotation ef-
forts in NLP. However, previous studies
on AL are limited to applications in a
single language. This paper proposes a
bilingual active learning paradigm for re-
lation classification, where the unlabeled
instances are first jointly chosen in terms
of their prediction uncertainty scores in
two languages and then manually labeled
by an oracle. Instead of using a parallel
corpus, labeled and unlabeled instances
in one language are translated into ones
in the other language and all instances in
both languages are then fed into a bilin-
gual active learning engine as pseudo
parallel corpora. Experimental results on
the ACE RDC 2005 Chinese and English
corpora show that bilingual active learn-
ing for relation classification signifi-
cantly outperforms monolingual active
learning.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999839090909091">
Semantic relation extraction between named en-
tities (aka. entity relation extraction or more con-
cisely relation extraction) is an important subtask
of Information Extraction (IE) as well as Natural
Language Processing (NLP). With its aim to
identify and classify the semantic relationship
between two entities (ACE 2002-2007), relation
extraction is of great significance to many NLP
applications, such as question answering, infor-
mation fusion, social network construction, and
knowledge mining and population etc.
</bodyText>
<note confidence="0.476108">
* Corresponding author
</note>
<bodyText confidence="0.999704853658537">
In the literature, the mainstream research on
relation extraction adopts statistical machine
learning methods, which can be grouped into
supervised learning (Zelenko et al., 2003; Culotta
and Soresen, 2004; Zhou et al., 2005; Zhang et
al., 2006; Qian et al., 2008; Chan and Roth,
2011), semi-supervised learning (Zhang et al.,
2004; Chen et al., 2006; Zhou et al., 2008; Qian
et al., 2010) and unsupervised learning (Hase-
gawa et al., 2004; Zhang et al., 2005) in terms of
the amount of labeled training data they need.
Usually the extraction performance depends
heavily on the quality and quantity of the labeled
data, however, the manual annotation of a large-
scale corpus is labor-intensive and time-
consuming. In the last decade researchers have
turned to another effective learning paradigm--
active learning (AL), which, given a small num-
ber of labeled instances and a large number of
unlabeled instances, selects the most informative
unlabeled instances to be manually annotated and
add them into the training data in an iterative
fashion. Essentially active learning attempts to
decrease the quantity of labeled instances by en-
hancing their quality, gauged by their informa-
tiveness to the learner. Since its emergence, ac-
tive learning has been successfully applied to
many tasks in NLP (Engelson and Dagan, 1996;
Hwa, 2004; Tomanek et al., 2007; Settles and
Craven, 2008).
It is trivial to validate, as we will do later in
this paper, that active learning can also alleviate
the annotation burden for relation extraction in
one language while retaining the extraction per-
formance. However, there are cases when we
may exploit relation extraction in multiple lan-
guages and there are corpora with relation in-
stances annotated for more than one language,
such as the ACE RDC 2005 English and Chinese
corpora. Hu et al. (2013) shows that supervised
relation extraction in one language (e.g. Chinese)
</bodyText>
<page confidence="0.959196">
582
</page>
<note confidence="0.8314675">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 582–592,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999884681818182">
can be enhanced by relation instances translated
from another language (e.g. English). This dem-
onstrates that there is some complementariness
between relation instances in two languages, par-
ticularly when the training data is scarce. One
natural question is: Can this characteristic be
made full use of so that active learning can
maximally benefit relation extraction in two lan-
guages? To the best of our knowledge, so far the
issue of joint active learning in two languages
has yet been addressed. Moreover, the success of
joint bilingual learning may lend itself to many
inherent multilingual NLP tasks such as POS
tagging (Yarowsky and Ngai, 2001), name entity
recognition (Yarowsky et al., 2001), sentiment
analysis (Wan, 2009), and semantic role labeling
(Sebastian and Lapata, 2009) etc.
This paper proposes a bilingual active learn-
ing (BAL) paradigm to relation classification
with a small number of labeled relation instances
and a large number of unlabeled instances in two
languages (non-parallel). Instead of using a par-
allel corpus which should have entity/relation
alignment information and is thus difficult to
obtain, this paper employs an off-the-shelf ma-
chine translator to translate both labeled and
unlabeled instances from one language into the
other language, forming pseudo parallel corpora.
These translated instances along with the original
instances are then fed into a bilingual active
learning engine. Findings obtained from experi-
ments with relation classification on the ACE
2005 corpora show that this kind of pseudo-
parallel corpora can significantly improve the
classification performance for both languages in
a BAL framework.
The rest of the paper is organized as follows.
Section 2 reviews the previous work on relation
extraction while Section 3 describes our baseline
systems. Section 4 elaborates on the bilingual
active learning paradigm and Section 5 discusses
the experimental results. Finally conclusions and
directions for future work are presented in Sec-
tion 6.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997243126984127">
While there are many studies in monolingual
relation extraction, there are only a few on multi-
lingual relation extraction in the literature.
Monolingual relation extraction: A wide
range of studies on relation extraction focus on
monolingual resources. As far as representation
of relation instances is concerned, there are fea-
ture-based methods (Zhao et al., 2004; Zhou et
al., 2005; Chan and Roth, 2011) and kernel-
based methods (Zelenko et al., 2003; Zhang et al.,
2006; Qian et al., 2008), mainly for the English
language. Both methods are also widely used in
relation extraction in other languages, such as
those in Chinese relation extraction (Che et al.,
2005; Li et al., 2008; Yu et al., 2010).
Multilingual relation extraction: There are
only two studies related to multilingual relation
extraction. Kim et al. (2010) propose a cross-
lingual annotation projection approach which
uses parallel corpora to acquire a relation detec-
tor on the target language. However, the map-
ping of two entities involved in a relation in-
stance may leads to errors. Therefore, Kim and
Lee (2012) further employ a graph-based semi-
supervised learning method, namely Label
Propagation (LP), to indirectly propagate labels
from the source language to the target language
in an iterative fashion. Both studies transfer rela-
tion annotations via parallel corpora from the
resource-rich language (English) to the resource-
poor language (Korean), but not vice versa.
Based on a small number of labeled instances
and a large number of unlabeled instances in
both languages, our method differs from theirs in
that we adopt a bilingual active learning para-
digm via machine translation and improve the
performance for both languages simultaneously.
Active Learning in NLP: Active learning
has become an active research topic due to its
potential to significantly reduce the amount of
labeled training data while achieving comparable
performance with supervised learning. It has
been successfully applied to many NLP applica-
tions, such as POS tagging (Engelson and Dagan,
1996; Ringger et al., 2007), word sense disam-
biguation (Chan and Ng, 2007; Zhu and Hovy,
2007), sentiment detection (Brew et al., 2010; Li
et al., 2012), syntactical parsing (Hwa, 2004;
Osborne and Baldridge, 2004), and named entity
recognition (Shen et al., 2004; Tomanek et al.,
2007; Tomanek and Hahn, 2009) etc.
Different from these AL studies on a single
task, Reichart et al. (2008) introduce a multi-task
active learning (MTAL) paradigm, where unla-
beled instances are selected for two annotation
tasks (i.e. named entity and syntactic parse tree).
They demonstrate that MTAL in the same lan-
guage outperforms one-sided and random selec-
tion AL. From a different perspective, we pro-
pose an active learning framework for the same
task, but across two different languages.
Another related study (Haffari and Sarkar,
2009) deals with active learning for multilingual
</bodyText>
<page confidence="0.997981">
583
</page>
<bodyText confidence="0.99996825">
machine translation, which make use of multilin-
gual corpora to decrease human annotation ef-
forts by selecting highly informative sentences
for a newly added language in multilingual paral-
lel corpora. While machine translation inherently
deals with multilingual parallel corpora, our task
focuses on relation extraction by pseudo parallel
corpora in two languages.
</bodyText>
<sectionHeader confidence="0.986035" genericHeader="method">
3 Baseline Systems
</sectionHeader>
<bodyText confidence="0.999588666666667">
This section first introduces the fundamental su-
pervised learning method, and then describes a
baseline active learning algorithm.
</bodyText>
<subsectionHeader confidence="0.99954">
3.1 Supervised Learning
</subsectionHeader>
<bodyText confidence="0.966047625">
We adopt the feature-based method for funda-
mental supervised relation classification, rather
than the tree kernel-based method, since active
learning needs a large number of iterations and
the kernel-based method usually performs much
slower than the feature-based one. Following is a
list of our used features, much similar to Zhou et
al. (2005):
</bodyText>
<listItem confidence="0.59072725">
a) Lexical features of entities and their contexts
WM1: bag-of-words in the 1st entity mention
HM1: headword of M1
WM2: bag-of-words in the 2nd entity mention
</listItem>
<bodyText confidence="0.976875545454545">
HM2: headword of M2
HM12: combination of HM1 and HM2
WBNULL: when no word in between
WBFL: the only one word in between
WBF: the first word in between when at least
two words in between
WBL: the last word in between when at least
two words in between
WBO: other words in between except the first
and last words when at least three words in
between
</bodyText>
<listItem confidence="0.916600333333334">
b) Entity type
ET12: combination of entity types
EST12: combination of entity subtypes
EC12: combination of entity classes
c) Mention level
ML12: combination of entity mention levels
MT12: combination of LDC mention types
d) Overlap
#WB: number of other mentions in between
#MB: number of words in between
M1&gt;M2 or M1&lt;M2: flag indicating whether
M2/M1 is included in M1/M2.
</listItem>
<subsectionHeader confidence="0.999489">
3.2 Active Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.999822866666667">
We use a pool-based active learning procedure
with uncertainty sampling (Scheffer et al., 2001;
Culotta and McCallum, 2005; Kim et al., 2006)
for both Chinese and English relation classifica-
tion as illustrated in Fig. 1. During iterations a
batch of unlabeled instances are chosen in terms
of their informativeness to the current classifier,
labeled by an oracle and in turn added into the
labeled data to retrain the classifier. Due to our
focus on the effectiveness of bilingual active
learning on relation classification, we only use
uncertainty sampling without incorporating more
complex measures, such as diversity and repre-
sentativeness (Settles and Craven, 2008), and
leave them for future work.
</bodyText>
<table confidence="0.87495275">
Algorithm uncertainty-based active learning
Input:
- L, labeled data set
- U, unlabeled data set
- n, batch size
Output:
- SVM, classifier
Repeat:
</table>
<listItem confidence="0.992416833333333">
1. Train a single classifier SVM on L
2. Run the classifier on U
3. Find at most n instances in U that the classifier
has the highest prediction uncertainty
4. Have these instances labeled by an oracle
5. Add them into L
</listItem>
<figureCaption confidence="0.73313875">
Until: certain number of instances are labeled or
certain performance is reached
Figure 1. Pool-based active learning with uncer-
tainty sampling
</figureCaption>
<bodyText confidence="0.999927375">
Since the SVMLIB package used in this paper
can output probabilities assigned to the class la-
bels on an instance, we have three uncertainty
metrics readily available, i.e., least confidence
(LC), margin (M) and entropy (E). The NER
experimental results on multiple corpora (Settles
and Craven, 2008) show that there is no single
clear winner among these three metrics. This
conclusion is also validated by our preliminary
experiments on the task of active learning rela-
tion extraction, thus we adopt the LC metric for
simplicity. Specifically, with a sequence of K
probabilities for a relation instance at some itera-
tion, denoted as {p1,p2,...pK} in the descending
order, the LC metric of the relation instance can
be simply picked as the first one, i.e.
</bodyText>
<equation confidence="0.867357">
HLC(1)
= p1
</equation>
<bodyText confidence="0.999917">
Where K denotes the total number of relation
classes. Note that this metric actually reflects
prediction reliability (i.e. reverse uncertainty)
rather than uncertainty in order to facilitate joint
</bodyText>
<page confidence="0.990526">
584
</page>
<bodyText confidence="0.998402">
confidence calculation for two languages (cf.
§4.4). Intuitively, the smaller the HLC is, the less
confident the prediction is.
</bodyText>
<sectionHeader confidence="0.9114025" genericHeader="method">
4 Bilingual Active Learning for Rela-
tion Classification
</sectionHeader>
<bodyText confidence="0.9999035">
In this section, we elaborate on the bilingual ac-
tive learning for relation extraction.
</bodyText>
<subsectionHeader confidence="0.996301">
4.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.999903066666667">
With Chinese and English (designated as c and e)
as two languages used in our study, this paper
intends to address the task of bilingual relation
classification, i.e., assigning relation labels to
candidate instances that have semantic relation-
ships. Suppose we have a small number of la-
beled instances in both languages, denoted as Lc
and Le (non-parallel) respectively, and a large
number of unlabeled instances in both languages,
denoted as Uc and Ue (non-parallel). The test in-
stances in both languages are represented as Tc
and Te. In order to take full advantage of bilin-
gual resources, we translate both labeled and
unlabeled instances in one language to ones in
the other language as follows:
</bodyText>
<construct confidence="0.91737925">
Lc 4 Let
Uc 4 Uet
Le 4 Lct
Ue 4 Lct
</construct>
<bodyText confidence="0.999764">
The objective is to learn SVM classifiers in
both languages, denoted as SVMc and SVMe re-
spectively, in a BAL fashion to improve their
classification performance.
</bodyText>
<subsectionHeader confidence="0.997775">
4.2 Bilingual Active Learning Framework
</subsectionHeader>
<bodyText confidence="0.99997821875">
Currently, AL is widely used in NLP tasks in a
single language, i.e., during iterations unlabeled
instances least confident only in one language
are picked and manually labeled to augment the
training data. The only exception is AL for ma-
chine translation (Haffari et al., 2009; Haffari
and Sarkar, 2009), whose purpose is to select the
most informative sentences in the source lan-
guage to be manually translated into the target
language. Previous studies (Reichart et al., 2008;
Haffari and Sarkar, 2009) show that multi-task
active learning (MTAL) can yield promising
overall results, no matter whether they are two
different tasks or the task of machine translation
on multiple language pairs. If a specific NLP
task on two languages, such as relation classifi-
cation, can be regarded as two tasks, it is reason-
able to argue that these two tasks can benefit
each other when jointly performed in the BAL
framework. Yet, to our knowledge, this issue
remains unexplored.
An important issue for bilingual learning is
how to obtain two language views for relation
instances from multilingual resources. There are
three solutions to this problem, i.e. parallel cor-
pora (Lu et al., 2011), translated corpora (aka.
pseudo parallel corpora) (Wan 2009), and bilin-
gual lexicons (Oh et al., 2009). We adopt the one
with pseudo parallel corpora, using the machine
translation method to generate instances from
one language to the other in the BAL paradigm,
as depicted in Fig. 2.
</bodyText>
<figureCaption confidence="0.996473">
Figure 2. Framework of bilingual active learning
</figureCaption>
<bodyText confidence="0.998542333333333">
In order to make full use of pseudo parallel
corpora, translated labeled and unlabeled in-
stances are augmented in the following two ways:
</bodyText>
<listItem confidence="0.999110333333333">
• For labeled Chinese instances (Lc) and Eng-
lish instances (Le), their translated counter-
parts (Let and Lct), along with their labels, are
directly added into the labeled instances in the
other language;
• For unlabeled Chinese instances (Uc) and
English instances (Ue), during an active learn-
ing iteration the top n unlabeled instances in
Uc and Uet which are least confidently jointly
</listItem>
<figure confidence="0.999632375">
Test
Chinese Instances
(Tc)
Labeled Translated
Chinese Instances
(Lct)
Unlabeled
Translated Chinese
Instances (Uct)
Labeled
Chinese Instances
(Lc)
Unlabeled
Chinese Instances
(Uc)
Chinese View
Bilingual
active learning
Machine
Translation
Machine
Translation
Machine
Translation
Machine
Translation
Unlabeled Translated
English Instances (Uet)
Labeled
English Instances (Le)
Labeled Translated
English Instances
(Let)
Unlabeled
English Instances
(Ue)
Test
English Instances
(Te)
English View
</figure>
<page confidence="0.993869">
585
</page>
<bodyText confidence="0.999209666666667">
predicted by SVMc and SVMe are labeled by
an oracle and added to Lc and Le respectively.
(cf. §4.4)
</bodyText>
<subsectionHeader confidence="0.972018">
4.3 Instance Projection via MT
</subsectionHeader>
<bodyText confidence="0.999944">
Among the several off-the-shelf machine transla-
tion services, we select the Google Translator1
because of its high quality and easy accessibility.
Both the mentions of relation instances and the
mentions of two involved entities are first trans-
lated into the other language via machine transla-
tion. Then, two entities in the original instance
are aligned with their counterparts in the trans-
lated instance in order to form an aligned bilin-
gual relation instance pair.
</bodyText>
<subsectionHeader confidence="0.360472">
Instance translation
</subsectionHeader>
<bodyText confidence="0.999972">
All the positive instances in the ACE 2005 Chi-
nese and English corpora are translated to an-
other language respectively, i.e. Chinese to Eng-
lish and vice versa. The relation instance is rep-
resented as the word sequence between two enti-
ties. This word sequence, rather than the whole
sentence, is then translated to another language
by the Google Translator. The reason is that, al-
though this sequence loses partial contextual in-
formation of the relation instance, its translation
quality is supposed to be better. Our preliminary
experiments indicate that the addition of contex-
tual information fail to benefit the task. After
translation, word segmentation is performed on
Chinese instances translated from English while
tokenization is needed for translated English in-
stances.
</bodyText>
<subsectionHeader confidence="0.444984">
Entity alignment
</subsectionHeader>
<bodyText confidence="0.9998741875">
The objective of entity alignment is to build a
mapping from the entities in the original in-
stances to the entities in the translated instances.
Put in another way, entity alignment automati-
cally marks the entity mentions in the translated
instance, thereby the feature vector correspond-
ing to the translated instance can be constructed.
Entity alignment is vital in cross-language rela-
tion extraction whose difficulty lies in the fact
that the same entity mention as an isolated phrase
and as an integral phrase in the relation instance
can be translated to different phrases. For exam-
ple, the Chinese entity mention “官员” (officer)
is translated to “officer” in isolation, it is, how-
ever, translated to “officials” when in the relation
instance “叙利亚 官员” (Syrian officials).
</bodyText>
<footnote confidence="0.870317">
1 http://translate.google.com
</footnote>
<table confidence="0.952472">
Algorithm entity alignment
Input:
- Me, entity mention in English
- Re, relation instance in English
- Mct, translation of Me in Chinese
- Rc, translation of Re in Chinese
- L, a lexicon consisting of entries like (ei, ci, pi),
where pi is the translation probability from ei to ci
- α, probability threshold
Output:
</table>
<tableCaption confidence="0.239421">
- Mc, the counterpart of Me in Rc
</tableCaption>
<figure confidence="0.965179909090909">
Steps:
1. If Mct can be exactly found in Rc, then return
Mct
2. If the rightmost part of Mct can be found in Rc,
then this part can be returned
3. For very word we in Me,
a) If there exists a word wc in Rc and (we, wc, p)
in L and p&gt;α, then (we, wc) is a match of two words
b) Return a successive sequence of matching
words wc
4. Return null
</figure>
<figureCaption confidence="0.999897">
Figure 3. Entity alignment algorithm
</figureCaption>
<bodyText confidence="0.996818454545455">
Therefore, we devise some heuristics to align
entity mentions between Chinese and English.
The basic idea is that the word sequence in one
mention successively matches the word sequence
in the other mention. Take entity alignment from
English to Chinese as an example, given entity
mention Me in relation instance Re in English and
their respective translations Mct and Rc in Chi-
nese, the objective of entity alignment is to find
Mc, the counterpart of Me in Rc. The procedure of
entity alignment algorithm can be described in
Fig. 3.
In the algorithm, the probability threshold α is
empirically set to 0.002 where the precision and
recall of entity alignment are balanced. Our lexi-
con is derived from the FBIS parallel corpus
(#LDC2003E14), which is widely used in ma-
chine translation between English and Chinese. It
should be noted that the process of relation trans-
lation and entity alignment are far from perfec-
tion, leading to reduction in the number of in-
stances being mapping to the other language, i.e.
</bodyText>
<equation confidence="0.93441425">
|Lc |&gt; |Let|
|Uc |&gt; |Uet|
|Le |&gt; |Lct|
|Ue |&gt; |Lct|
</equation>
<subsectionHeader confidence="0.994706">
4.4 Bilingual Active Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.9990045">
The basic idea of our BAL paradigm is that,
while unlabeled instances uncertain in one lan-
</bodyText>
<page confidence="0.992789">
586
</page>
<bodyText confidence="0.9969403">
guage are informative to the learner in that lan-
guage, unlabeled instances jointly uncertain in
both languages are informative to the learners in
both languages, thus potentially improving clas-
sification performance for both languages more
than their individual active learners do. This
idea is embodied in the BAL algorithm in Fig. 4,
where n is the batch size, i.e., the number of in-
stances selected, labeled and augmented at each
iteration.
</bodyText>
<figureCaption confidence="0.925175">
Figure 4. Bilingual active learning algorithm
</figureCaption>
<bodyText confidence="0.9999435">
The key point of this algorithm lies in Step 5
and Step 6, where unlabeled instances from Uc
and Ue are selected and labeled respectively.
Take Chinese for an example, when gauging the
prediction uncertainty for an unlabeled instance
in Uc, not only its own uncertainty measure Hc
predicted by SVMc is considered, but also the
uncertainty measure Het for its translation coun-
terpart in Uet, which is predicted by SVMe, is con-
sidered. Generally, in order to jointly consider
these two measures, there are three methods to
compute their means, namely, arithmetic mean,
geometric mean and harmonic mean. Preliminary
experiments show that among these three means,
there is no single winner, so we simply take the
geometric mean defined as follows:
</bodyText>
<equation confidence="0.994558">
Hg = Hc * Het (2)
</equation>
<bodyText confidence="0.999924555555556">
Considering that we adopt the LC measure as
the uncertainty score, when an instance in Uc
can’t find its translation counterpart in Uet due to
translation error or entity alignment failure, Het is
set to 1, i.e. the maximum. Since the bigger H is,
the more confident the prediction is, the less
likely the instance will be chosen, in this way we
discourage the unlabeled instances without trans-
lation counterparts.
</bodyText>
<sectionHeader confidence="0.999368" genericHeader="method">
5 Experimentation
</sectionHeader>
<bodyText confidence="0.99984925">
We have systematically evaluated our BAL para-
digm on the relation classification task using
ACE RDC 2005 RDC Chinese and English cor-
pora.
</bodyText>
<subsectionHeader confidence="0.8061495">
5.1 Experimental Settings
Corpora and Preprocessing
</subsectionHeader>
<bodyText confidence="0.99995384">
We use the ACE 2005 RDC Chinese and English
corpora as the benchmark data (hereafter we re-
fer to them as the Chinese corpus (ACE2005c)
and the English corpus (ACE2005e) respec-
tively). Both corpora have the same en-
tity/relation hierarchies, which define 7 entity
types, 6 major relation types. However, the Chi-
nese corpus contains 633 documents and 9,147
positive relation instances while the English cor-
pus only contains 498 files and 6,253 positive
instances. Therefore, in order to balance the cor-
pus scale to fairly evaluate bilingual active learn-
ing impact on relation classification, we ran-
domly select 458 Chinese files and thus get
6,268 positive instances, comparable to the Eng-
lish corpus.
Preprocessing steps for both corpora include
sentence splitting and tokenization (word seg-
mentation for Chinese using ICTCLAS2). Then,
positive relation mentions with word sequences
between two entities and their feature vectors are
extracted from sentences while negative relation
mentions are simply discarded because we focus
on the task of relation classification. After entity
and relation mentions in one language are trans-
</bodyText>
<footnote confidence="0.944848">
2 http://ictclas.org/
</footnote>
<page confidence="0.997167">
587
</page>
<bodyText confidence="0.999788454545455">
lated into the other language using the Google
translator, entity alignment is performed between
relation mentions and their translations. Finally
4,747 Chinese relation mentions are successfully
translated and aligned from English and vice
versa, 4,936 English relation mentions are trans-
lated and aligned from Chinese.
SVMLIB (Chang and Lin, 2011) is selected as
our classifier since it supports multi-class classi-
fication. The training parameters C (SVM) is set
to 2.4 according to our previous work on relation
extraction (Qian et al., 2010). Relation classifica-
tion performance is evaluated using the standard
Precision (P), Recall (R) and their harmonic av-
erage (F1) as well as deficiency measure (cf. lat-
ter in this section.). Overall performance scores
are averaged over 10 runs. For each run, 1/40
and 1/5 randomly selected instances are used as
the training and test set respectively while the
remaining instances are used as the unlabeled set
for further labeling during active learning itera-
tions.
</bodyText>
<sectionHeader confidence="0.910336" genericHeader="method">
Methods for Comparison
</sectionHeader>
<bodyText confidence="0.99980585106383">
For fair comparison, two baseline methods of
supervised learning are included to augment their
training sets with labeled instances during itera-
tions. However, these labeled instances are cho-
sen randomly from the corpus.
SL-MO (Supervised Learning with monolin-
gual labeled instances): only the monolingual
labeled instances are fed to the SVM classifiers
for both Chinese and English relation classifica-
tion respectively. The initial training data only
contain Lc and Le for Chinese and English respec-
tively.
SL-CR (Supervised Learning with cross-
lingual labeled instances): in addition to mono-
lingual labeled instances (SL-MO), the training
data for supervised learning contain labeled in-
stances translated from the other language. That
is, the initial training data contain Lc and Lct for
Chinese, or Le and Let for English. More impor-
tant, at each iteration not only the labeled in-
stances are added to the training data of its own
language, but their translated instances are also
added to the training data of the other language.
AL-MO (Active Learning with monolingual
instances): labeled and unlabeled data for active
learning only contain monolingual instances. No
translated instances are involved. That is, the
data contain Lc and Uc for Chinese, or Le and Ue
for English respectively. This is the normal ac-
tive learning method applied to a single language.
AL-CR (Active Learning with cross-lingual
instances): both the manually labeled instances
and their translated ones are added to the respec-
tive training data. The initial training data con-
tain Lc and Lct for Chinese, or Le and Let for Eng-
lish. At each iteration, the n least confidently
classified instances in Uc and Ue are labeled and
added to the Chinese/English training data re-
spectively. Their translated instances in Uet and
Uct are also added to the English/Chinese training
data respectively.
AL-BI (Active Learning with bilingual la-
beled and unlabeled instances): similar to AL-
CR with the exception that the unlabeled in-
stances are chosen not by uncertainty scores in
one language, but by the joint uncertainty scores
in two languages. (cf. §4.4)
</bodyText>
<sectionHeader confidence="0.467637" genericHeader="evaluation">
Evaluation Metric
</sectionHeader>
<bodyText confidence="0.9999438">
Although learning curves are often used to evalu-
ate the performance for active learning, it is pref-
erable to quantitatively compare various active
learning methods using a statistical metric defi-
ciency (Schein and Ungar, 2007) defined as:
</bodyText>
<equation confidence="0.9993378">
∑n ( ( ) ( ))
F REF F AL
−
i = 1 n i def AL REF =
n ( , ) (3)
∑ n
( ( ) ( ))
F REF F REF
−
i = 1 n i
</equation>
<bodyText confidence="0.9993207">
Where n is the number of iterations involved in
active learning and Fi is the F1-score of relation
classification at the ith iteration. REF is the base-
line active learning method and AL is an im-
proved variant of REF, such as AL-CR or AL-
BI. Essentially this deficiency metric measures
the degree to which REF outperforms AL. Thus,
smaller deficiency value (i.e. &lt;1.0) indicates AL
outperforms REF while a larger value (i.e. &gt;1.0)
indicates AL underperforms REF.
</bodyText>
<subsectionHeader confidence="0.9922605">
5.2 Experimental Results and Analysis
Comparison of overall deficiency
</subsectionHeader>
<bodyText confidence="0.999888583333333">
Table 1 compares the deficiency scores of rela-
tion classification on the Chinese (ACE2005c)
and English corpora (ACE2005e) for various
learning methods, i.e., SL-CR, AL-MO, AL-CR
and AL-BI. Particularly, SL-MO is used as the
baseline system against which deficiency scores
for other methods are computed. The batch size n
is set to 100 and iterations stop after all the unla-
beled instances have run out of. Deficiency
scores are averaged over 10 runs and the best
ones are highlighted in bold font. Each run has a
different test set and a different seed set.
</bodyText>
<page confidence="0.987303">
588
</page>
<figure confidence="0.999234133333333">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
1.0
C-SL-CR
C-AL-MO
C-AL-CR
C-AL-BI
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
E-SL-CR
E-AL-MO
E-AL-CR
E-AL-BI
100 200 300 400 500 600 700 800 900 1000
100 200 300 400 500 600 700 800 900 1000
(a) Chinese (b) English
</figure>
<figureCaption confidence="0.991384">
Figure 5. Deficiency comparison for different batch sizes
</figureCaption>
<figure confidence="0.997505">
(a) Chinese (b) English
</figure>
<figureCaption confidence="0.958568">
Figure 6. Learning curves for different methods
</figureCaption>
<figure confidence="0.922318181818182">
0
2
4
6
8
10
12
14
16
18
20
</figure>
<page confidence="0.913385357142857">
22
24
26
28
30
32
34
36
38
40
42
44
46
48
</page>
<figure confidence="0.884053454545454">
0
2
4
6
8
10
12
14
16
18
20
</figure>
<page confidence="0.920128071428571">
22
24
26
28
30
32
34
36
38
40
42
44
46
48
</page>
<figure confidence="0.97986870967742">
95
93
91
89
87
85
83
81
79
77
75
93
91
89
87
85
83
81
79
77
75
E-SL-MO
E-SL-CR
E-AL-MO
E-AL-CR
E-AL-BI
C-SL-MO
C-SL-CR
C-AL-MO
C-AL-CR
C-AL-BI
</figure>
<bodyText confidence="0.999601181818182">
The table shows that among the three active
learning methods, bilingual active learning (AL-
BI) achieves the best performance for both Chi-
nese and English relation classification. This
demonstrates that, bilingual active learning with
jointly selecting the unlabeled instances can not
only enhance relation classification for its own
language, but also help relation classification for
the other language due to the complementary
nature of relation instances between Chinese and
English.
</bodyText>
<table confidence="0.997594">
Corpora SL-CR AL-MO AL-CR AL-BI
ACE2005c 0.934 0.383 0.323 0.254
ACE2005e 0.779 0.405 0.298 0.160
</table>
<tableCaption confidence="0.9133855">
Table 1. Deficiency comparison of different
methods
</tableCaption>
<bodyText confidence="0.992844956521739">
The table also shows the consistent utility of
cross-lingual information for relation classifica-
tion for both languages. When cross-lingual in-
formation is augmented, SL-CR outperforms
SL-MO and AL-CR outperforms AL-MO.
Comparison of different batch sizes
Figure 5(a) and 5(b) illustrate the deficiency
scores for four learning methods (SL-CR, AL-
MO, AL-CR and AL-BI) against the SL-MO
method with different batch sizes (n), where pre-
fixes “C” and “E” denote Chinese and English
respectively. The horizontal axes denote the
range of n (&lt;=1000) while the vertical ones de-
note the deficiency scores.
The figures show that the deficiency scores
for three active learning methods run virtually
parallel with each other while they increase mo-
notonously with the batch size n. This suggests
that for both Chinese and English AL-BI consis-
tently performs best against other methods across
a wide range of batch sizes, though the overall
advantage of three active learning methods gen-
erally diminish.
</bodyText>
<subsectionHeader confidence="0.85913">
Comparison of learning curves
</subsectionHeader>
<bodyText confidence="0.9886711">
In order to gain an intuition into how the per-
formance evolves when the labeled instances are
added into the training data during iterations, we
depict the learning curves for various learning
methods on the Chinese and English corpora in
Fig. 6(a) and 6(b) respectively. The horizontal
axes denote learning iterations while the vertical
ones denote F1-scores. For simplicity of illustra-
tion the F1-scores are collected from one of the
10 runs.
</bodyText>
<page confidence="0.995377">
589
</page>
<bodyText confidence="0.999937583333333">
The figures clearly demonstrate the perform-
ance difference for both languages among five
methods at the beginning of iterations while F1-
scores converge at the end of iterations. Particu-
larly at the very outset, AL-BI outperforms other
methods, quickly jumps to a very high point
comparable to its best performance. However,
after the 10th iteration the performance scores for
the three AL variants tend to show trivial differ-
ence probably because most highly informative
instances have already been added to the training
data.
</bodyText>
<subsectionHeader confidence="0.769128">
Comparison of annotation scale
</subsectionHeader>
<bodyText confidence="0.999481076923077">
In order to better compare BAL with other AL
methods Figure 7 zooms out partial data on three
AL methods in Fig. 6 and rescale the data for
AL-MO, where “C” and “E” denote Chinese and
English respectively. Likewise, the vertical axis
denotes F1-scores while the horizontal axis de-
notes the number of instances labeled for AL-
CR and AL-BI. However, for AL-MO that num-
ber is doubled. This figure tries to answer the
question: to label n respective instances in both
languages for BAL or to labeled 2n instances in
just one language for monolingual AL, can the
former rival the latter?
</bodyText>
<figureCaption confidence="0.964789">
Figure 7. Comparison of annotation scale among
three AL methods
</figureCaption>
<bodyText confidence="0.999968416666667">
The figure shows that for both Chinese and
English, when the number of instances (n) to be
labeled is no greater than 400, AL-BI with n in-
stances can achieve comparable performance
with AL-MO with 2n instances. It implies that
when the labeled instances are limited, labeling
instances, half in one language and half in the
other for BAL, is competitive against labeling
the same total number of instances in just one
language for monolingual AL, not to mention
that the former can generate two relation extrac-
tors on two languages.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999912">
This paper proposes a bilingual active learning
paradigm for Chinese and English relation classi-
fication. Given a small number of relation in-
stances and a large number of unlabeled relation
instances in both languages, we translate both the
labeled and unlabeled instances in one language
to the other as pseudo parallel corpora. After en-
tity alignment, these labeled and unlabeled in-
stances in both languages are fed into a bilingual
active learning engine. Experiments with the task
of relation classification on the ACE RDC 2005
Chinese and English corpora show that bilingual
active learning can significantly outperforms
monolingual active learning for both Chinese and
English simultaneously. Moreover, we demon-
strate that BAL across two languages can com-
pete against monolingual AL when the annota-
tion scale is limited, though the overall number
of labeled instances remains the same.
For future work, on one hand, we plan to
combine uncertainty sampling with diversity and
informativeness measures; on the other hand, we
intend to combine BAL with semi-supervised
learning to further reduce human annotation ef-
forts.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999787909090909">
This research is supported by Grants 61373096,
61305088, 61273320, and 61331011 under the
National Natural Science Foundation of China;
Project 2012AA011102 under the “863” Na-
tional High-Tech Research and Development of
China; Grant 11KJA520003 under the Education
Bureau of Jiangsu, China. We would like to
thank the excellent and insightful comments
from the three anonymous reviewers. Thanks
also go to my colleague Dr. Shoushan Li for his
helpful suggestions.
</bodyText>
<sectionHeader confidence="0.995069" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.979550909090909">
ACE. 2002-2007. Automatic Content Extraction.
http://www.ldc.upenn.edu/Projects/ACE/
A. Brew, D. Greene, and P. Cunningham. 2010. Using
crowdsourcing and active learning to track senti-
ment in online media. ECAI’2010: 145–150.
Y.S. Chan and D. Roth. 2011. Exploiting Syntactico-
Semantic Structures for Relation Extraction.
ACL’2011: 551-560
Y.S. Chan and H.T. Ng. 2007. Domain adaptation
with active learning for word sense disambiguation.
ACL’2007.
</reference>
<figure confidence="0.997672133333333">
100 200 300 400 500 600 700 800 900 1000
94
92
90
88
86
84
82
80
C-AL-MO (2n)
C-AL-CR
C-AL-BI
E-AL-MO (2n)
E-AL-CR
E-AL-BI
</figure>
<page confidence="0.961761">
590
</page>
<reference confidence="0.999668970873787">
C.C. Chang and C.J. Lin. 2011. LIBSVM: a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2(27):1-27.
W.X. Che, T. Liu, and S. Li. 2005. Automatic Extrac-
tion of Entity Relation (in Chinese). Journal of
Chinese Information Processing, 19(2): 1-6.
J.X. Chen, D.H. Ji, and C. L. Tan. 2006. Relation Ex-
traction using Label Propagation-based Semi-
supervised Learning. ACL/COLING’2006: 129-136.
A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. ACL’2004: 423-439.
A. Culotta and A. McCallum. 2005. Reducing label-
ing effort for stuctured prediction tasks. AAAI’2005:
746–751.
S. P. Engelson and I. Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. ACL’1996: 319–326.
G. Haffari, M. Roy, and A. Sarkar. 2009. Active
learning for statistical phrase-based machine trans-
lation. NAACL’2009: 415–423.
G. Haffari and A. Sarkar. 2009. Active learning for
multilingual statistical machine translation.
ACL/IJCNLP’2009: 181–189.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Dis-
covering Relations among Named Entities from
Large Corpora. ACL’2004.
Y.N. Hu, J.G. Shu, L.H. Qian, and Q.M. Qiao. 2013.
Cross-lingual Relation Extraction based on Ma-
chine Translation (in Chinese). Journal of Chinese
Information Processing, 27(5): 191-197.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3): 253–276.
S. Kim, M. Jeong, J. Lee, and G.G. Lee. 2010. A
Cross-lingual Annotation Projection Approach for
Relation Detection. COLING’2010: 564-571.
S. Kim and G.G. Lee. 2012. A Graph-based Cross-
lingual Projection Approach for Weakly Super-
vised Relation Extraction. ACL’2012: 48-53.
S. Kim, Y. Song, K. Kim, J.W. Cha, and G.G. Lee.
2006. MMR-based active machine learning for bio
named entity recognition. HLT-NAACL’2006: 69–
72.
W.J. Li, P. Zhang, F.R. Wei, Y.X. Hou, and Q. Lu.
2008. A Novel Feature-based Approach to Chinese
Entity Relation Extraction. ACL’2008: 89-92.
S.S. Li, S.F. Ju, G.D. Zhou, and X.J. Li. 2012. Active
learning for imbalanced sentiment classifica-
tion. EMNLP-CoNLL’2012: 139-148.
B. Lu, C.H. Tan, C. Cardie, and B.K. Tsou. 2011.
Joint Bilingual Sentiment Classification with
Unlabeled Parallel Corpora. ACL’2011: 320-330.
J. Oh, K. Uchimoto, and K. Torisawa. 2009. Bilin-
gual Co-Training for Monolingual Hyponymy-
Relation Acquisition. ACL’2009: 432-440.
M. Osborne and J. Baldridge. 2004. Ensemble based
active learning for parse selection. HLT-NAACL’
2004: 89–96.
L.H. Qian, G.D. Zhou, F. Kong, and Q.M. Zhu. 2010.
Clustering-based Stratified Seed Sampling for
Semi-Supervised Relation Classification.
EMNLP2010: 346-355.
L.H. Qian, G.D. Zhou, Q.M. Zhu, and P.D. Qian.
2008. Exploiting constituent dependencies for tree
kernel-based semantic relation extraction. COL-
ING’2008: 697-704.
R. Reichart, K. Tomanek, U. Hahn, and A. Rappoport.
2008. Multi-task active learning for linguistic an-
notations. ACL’2008: 861-869.
E. Ringger, P. McClanahan, R. Haertel, G. Busby, M.
Carmen, J. Carroll, K. Seppi, and D. Lonsdale.
2007. Active learning for part-of-speech tagging:
Accelerating corpus annotation. In Proceedings of
the Linguistic Annotation Workshop at ACL’2007:
101–108.
T. Scheffer, C. Decomain, and S. Wrobel. 2001. Ac-
tive hidden Markov models for information extrac-
tion. In Proceedings of the International Confer-
ence on Advances in Intelligent Data Analysis
(CAIDA), pages 309–318.
A. I. Schein and L. H. Ungar. 2007. Active learning
for logistic regression: an evaluation. Machine
Learning, 68(3): 235-265.
P. Sebastian and M. Lapata. 2009. Cross-lingual an-
notation projection of semantic roles. Journal of
ArtiÞcial Intelligence Research, 36(1): 307-340.
B. Settles and M. Craven. 2008. An Analysis of Ac-
tive Learning Strategies for Sequence Labeling
Tasks. EMNLP’2008: 1070–1079.
D. Shen, J. Zhang, J. Su, G.D. Zhou and C.-L. Tan.
2004. Multi-criteria-based active learning for
named entity recognition. ACL’2004.
K. Tomanek and U. Hahn. 2009. Semi-Supervised
Active Learning for Sequence Labeling. ACL-
IJCNLP’2009: 1039-1047.
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts an-
notation costs and maintains reusability of anno-
tated data. EMNLP-CoNLL’2007: 486–495.
X.J. Wan. 2009. Co-Training for Cross-Lingual Sen-
timent Classification. ACL-AFNLP’2009: 235-243.
D. Yarowsky and G. Ngai. 2001. Inducing multilin-
gual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. NAACL’2001: 1-8.
</reference>
<page confidence="0.976468">
591
</page>
<reference confidence="0.999836228571429">
D. Yarowsky, G. Ngai, and R. Wicentorski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. HLT’2001:1-8.
H.H. Yu, L.H. Qian, G.D. Zhou, and Q.M. Zhu. 2010.
Chinese Semantic Relation Extraction based on
Unified Syntactic and Entity Semantic Tree (in
Chinese). Journal of Chinese Information Process-
ing, 24(5): 17-23.
D. Zelenko, C. Aone, and A. Richardella. 2003. Ker-
nel Methods for Relation Extraction. Journal of
Machine Learning Research, 3: 1083-1106.
Z. Zhang. 2004. Weakly-supervised relation classifi-
cation for Information Extraction. CIKM’2004.
M. Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L.
Tan. 2005. Discovering Relations between Named
Entities from a Large Raw Corpus Using Tree
Similarity-Based Clustering. IJCNLP’2005: 378-
389.
M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A
Composite Kernel to Extract Relations between
Entities with both Flat and Structured Features.
ACL/COLING’2006: 825-832.
S.B. Zhao and R. Grishman. 2005. Extracting rela-
tions with integrated information using kernel
methods. ACL’2005: 419-426.
G.D. Zhou, J.H. Li, L.H. Qian, and Q.M. Zhu. 2008.
Semi-Supervised Learning for Relation Extraction.
IJCNLP’2008: 32-38.
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. Ex-
ploring various knowledge in relation extraction.
ACL’2005: 427-434.
J.B. Zhu and E. Hovy. 2007. Active learning for word
sense disambiguation with methods for addressing
the class imbalance problem. EMNLP-
CoNLL’2007: 783-790.
</reference>
<page confidence="0.997718">
592
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.399582">
<title confidence="0.85210375">Active Learning for Relation Classification via Pseudo Parallel Corpora Longhua Qian Haotian Hui Ya’nan Hu Guodong Zhou* Qiaoming Natural Language Processing</title>
<note confidence="0.723945">School of Computer Science and Technology, Soochow 1 Shizi Street, Suzhou, China 215006 {qianlonghua,20134227019,20114227025,gdzhou,qmzhu}@suda.edu.cn</note>
<abstract confidence="0.998281478260869">Active learning (AL) has been proven effective to reduce human annotation efforts in NLP. However, previous studies on AL are limited to applications in a single language. This paper proposes a bilingual active learning paradigm for relation classification, where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle. Instead of using a parallel corpus, labeled and unlabeled instances in one language are translated into ones in the other language and all instances in both languages are then fed into a bilingual active learning engine as pseudo parallel corpora. Experimental results on the ACE RDC 2005 Chinese and English corpora show that bilingual active learning for relation classification significantly outperforms monolingual active learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Automatic Content Extraction.</title>
<note>http://www.ldc.upenn.edu/Projects/ACE/</note>
<marker></marker>
<rawString>ACE. 2002-2007. Automatic Content Extraction. http://www.ldc.upenn.edu/Projects/ACE/</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Brew</author>
<author>D Greene</author>
<author>P Cunningham</author>
</authors>
<title>Using crowdsourcing and active learning to track sentiment in online media.</title>
<date>2010</date>
<pages>2010--145</pages>
<contexts>
<context position="8055" citStr="Brew et al., 2010" startWordPosition="1232" endWordPosition="1235">differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same t</context>
</contexts>
<marker>Brew, Greene, Cunningham, 2010</marker>
<rawString>A. Brew, D. Greene, and P. Cunningham. 2010. Using crowdsourcing and active learning to track sentiment in online media. ECAI’2010: 145–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>D Roth</author>
</authors>
<title>Exploiting SyntacticoSemantic Structures for Relation Extraction.</title>
<date>2011</date>
<volume>2011</volume>
<pages>551--560</pages>
<contexts>
<context position="2040" citStr="Chan and Roth, 2011" startWordPosition="293" endWordPosition="296">ge Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-- active learning (AL), which, given a small number of labeled instances and a large number </context>
<context position="6282" citStr="Chan and Roth, 2011" startWordPosition="952" endWordPosition="955">ystems. Section 4 elaborates on the bilingual active learning paradigm and Section 5 discusses the experimental results. Finally conclusions and directions for future work are presented in Section 6. 2 Related Work While there are many studies in monolingual relation extraction, there are only a few on multilingual relation extraction in the literature. Monolingual relation extraction: A wide range of studies on relation extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities invol</context>
</contexts>
<marker>Chan, Roth, 2011</marker>
<rawString>Y.S. Chan and D. Roth. 2011. Exploiting SyntacticoSemantic Structures for Relation Extraction. ACL’2011: 551-560</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
</authors>
<title>Domain adaptation with active learning for word sense disambiguation.</title>
<date>2007</date>
<pages>2007</pages>
<contexts>
<context position="7994" citStr="Chan and Ng, 2007" startWordPosition="1222" endWordPosition="1225"> number of unlabeled instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspe</context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Y.S. Chan and H.T. Ng. 2007. Domain adaptation with active learning for word sense disambiguation. ACL’2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Chang</author>
<author>C J Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="24072" citStr="Chang and Lin, 2011" startWordPosition="3814" endWordPosition="3817">tween two entities and their feature vectors are extracted from sentences while negative relation mentions are simply discarded because we focus on the task of relation classification. After entity and relation mentions in one language are trans2 http://ictclas.org/ 587 lated into the other language using the Google translator, entity alignment is performed between relation mentions and their translations. Finally 4,747 Chinese relation mentions are successfully translated and aligned from English and vice versa, 4,936 English relation mentions are translated and aligned from Chinese. SVMLIB (Chang and Lin, 2011) is selected as our classifier since it supports multi-class classification. The training parameters C (SVM) is set to 2.4 according to our previous work on relation extraction (Qian et al., 2010). Relation classification performance is evaluated using the standard Precision (P), Recall (R) and their harmonic average (F1) as well as deficiency measure (cf. latter in this section.). Overall performance scores are averaged over 10 runs. For each run, 1/40 and 1/5 randomly selected instances are used as the training and test set respectively while the remaining instances are used as the unlabeled</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>C.C. Chang and C.J. Lin. 2011. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(27):1-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W X Che</author>
<author>T Liu</author>
<author>S Li</author>
</authors>
<title>Automatic Extraction of Entity Relation (in Chinese).</title>
<date>2005</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>1--6</pages>
<contexts>
<context position="6542" citStr="Che et al., 2005" startWordPosition="996" endWordPosition="999">ion extraction, there are only a few on multilingual relation extraction in the literature. Monolingual relation extraction: A wide range of studies on relation extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and Lee (2012) further employ a graph-based semisupervised learning method, namely Label Propagation (LP), to indirectly propagate labels from the source language to the target language in an itera</context>
</contexts>
<marker>Che, Liu, Li, 2005</marker>
<rawString>W.X. Che, T. Liu, and S. Li. 2005. Automatic Extraction of Entity Relation (in Chinese). Journal of Chinese Information Processing, 19(2): 1-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J X Chen</author>
<author>D H Ji</author>
<author>C L Tan</author>
</authors>
<title>Relation Extraction using Label Propagation-based Semisupervised Learning.</title>
<date>2006</date>
<volume>2006</volume>
<pages>129--136</pages>
<contexts>
<context position="2105" citStr="Chen et al., 2006" startWordPosition="303" endWordPosition="306">tic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-- active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled in</context>
</contexts>
<marker>Chen, Ji, Tan, 2006</marker>
<rawString>J.X. Chen, D.H. Ji, and C. L. Tan. 2006. Relation Extraction using Label Propagation-based Semisupervised Learning. ACL/COLING’2006: 129-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>J Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<volume>2004</volume>
<pages>423--439</pages>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>A. Culotta and J. Sorensen. 2004. Dependency tree kernels for relation extraction. ACL’2004: 423-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>A McCallum</author>
</authors>
<title>Reducing labeling effort for stuctured prediction tasks.</title>
<date>2005</date>
<pages>2005--746</pages>
<contexts>
<context position="10712" citStr="Culotta and McCallum, 2005" startWordPosition="1651" endWordPosition="1654">en WBO: other words in between except the first and last words when at least three words in between b) Entity type ET12: combination of entity types EST12: combination of entity subtypes EC12: combination of entity classes c) Mention level ML12: combination of entity mention levels MT12: combination of LDC mention types d) Overlap #WB: number of other mentions in between #MB: number of words in between M1&gt;M2 or M1&lt;M2: flag indicating whether M2/M1 is included in M1/M2. 3.2 Active Learning Algorithm We use a pool-based active learning procedure with uncertainty sampling (Scheffer et al., 2001; Culotta and McCallum, 2005; Kim et al., 2006) for both Chinese and English relation classification as illustrated in Fig. 1. During iterations a batch of unlabeled instances are chosen in terms of their informativeness to the current classifier, labeled by an oracle and in turn added into the labeled data to retrain the classifier. Due to our focus on the effectiveness of bilingual active learning on relation classification, we only use uncertainty sampling without incorporating more complex measures, such as diversity and representativeness (Settles and Craven, 2008), and leave them for future work. Algorithm uncertai</context>
</contexts>
<marker>Culotta, McCallum, 2005</marker>
<rawString>A. Culotta and A. McCallum. 2005. Reducing labeling effort for stuctured prediction tasks. AAAI’2005: 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Engelson</author>
<author>I Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from corpora.</title>
<date>1996</date>
<booktitle>ACL’1996:</booktitle>
<pages>319--326</pages>
<contexts>
<context position="3071" citStr="Engelson and Dagan, 1996" startWordPosition="458" endWordPosition="461">and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-- active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially active learning attempts to decrease the quantity of labeled instances by enhancing their quality, gauged by their informativeness to the learner. Since its emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings</context>
<context position="7925" citStr="Engelson and Dagan, 1996" startWordPosition="1210" endWordPosition="1213">but not vice versa. Based on a small number of labeled instances and a large number of unlabeled instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language o</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>S. P. Engelson and I. Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. ACL’1996: 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Haffari</author>
<author>M Roy</author>
<author>A Sarkar</author>
</authors>
<title>Active learning for statistical phrase-based machine translation.</title>
<date>2009</date>
<volume>2009</volume>
<pages>415--423</pages>
<contexts>
<context position="14285" citStr="Haffari et al., 2009" startWordPosition="2234" endWordPosition="2237">translate both labeled and unlabeled instances in one language to ones in the other language as follows: Lc 4 Let Uc 4 Uet Le 4 Lct Ue 4 Lct The objective is to learn SVM classifiers in both languages, denoted as SVMc and SVMe respectively, in a BAL fashion to improve their classification performance. 4.2 Bilingual Active Learning Framework Currently, AL is widely used in NLP tasks in a single language, i.e., during iterations unlabeled instances least confident only in one language are picked and manually labeled to augment the training data. The only exception is AL for machine translation (Haffari et al., 2009; Haffari and Sarkar, 2009), whose purpose is to select the most informative sentences in the source language to be manually translated into the target language. Previous studies (Reichart et al., 2008; Haffari and Sarkar, 2009) show that multi-task active learning (MTAL) can yield promising overall results, no matter whether they are two different tasks or the task of machine translation on multiple language pairs. If a specific NLP task on two languages, such as relation classification, can be regarded as two tasks, it is reasonable to argue that these two tasks can benefit each other when j</context>
</contexts>
<marker>Haffari, Roy, Sarkar, 2009</marker>
<rawString>G. Haffari, M. Roy, and A. Sarkar. 2009. Active learning for statistical phrase-based machine translation. NAACL’2009: 415–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Haffari</author>
<author>A Sarkar</author>
</authors>
<title>Active learning for multilingual statistical machine translation.</title>
<date>2009</date>
<pages>2009--181</pages>
<contexts>
<context position="8744" citStr="Haffari and Sarkar, 2009" startWordPosition="1341" endWordPosition="1344">Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilingual 583 machine translation, which make use of multilingual corpora to decrease human annotation efforts by selecting highly informative sentences for a newly added language in multilingual parallel corpora. While machine translation inherently deals with multilingual parallel corpora, our task focuses on relation extraction by pseudo parallel corpora in two languages. 3 Baseline Systems This section first introduces the fundamental supervised learning method, and then describes a baseline active learning algorithm. 3.1 Supervised Learning We adopt the </context>
<context position="14312" citStr="Haffari and Sarkar, 2009" startWordPosition="2238" endWordPosition="2241"> and unlabeled instances in one language to ones in the other language as follows: Lc 4 Let Uc 4 Uet Le 4 Lct Ue 4 Lct The objective is to learn SVM classifiers in both languages, denoted as SVMc and SVMe respectively, in a BAL fashion to improve their classification performance. 4.2 Bilingual Active Learning Framework Currently, AL is widely used in NLP tasks in a single language, i.e., during iterations unlabeled instances least confident only in one language are picked and manually labeled to augment the training data. The only exception is AL for machine translation (Haffari et al., 2009; Haffari and Sarkar, 2009), whose purpose is to select the most informative sentences in the source language to be manually translated into the target language. Previous studies (Reichart et al., 2008; Haffari and Sarkar, 2009) show that multi-task active learning (MTAL) can yield promising overall results, no matter whether they are two different tasks or the task of machine translation on multiple language pairs. If a specific NLP task on two languages, such as relation classification, can be regarded as two tasks, it is reasonable to argue that these two tasks can benefit each other when jointly performed in the BAL</context>
</contexts>
<marker>Haffari, Sarkar, 2009</marker>
<rawString>G. Haffari and A. Sarkar. 2009. Active learning for multilingual statistical machine translation. ACL/IJCNLP’2009: 181–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hasegawa</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<date>2004</date>
<booktitle>Discovering Relations among Named Entities from Large Corpora. ACL’2004.</booktitle>
<contexts>
<context position="2193" citStr="Hasegawa et al., 2004" startWordPosition="318" endWordPosition="322">at significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-- active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fas</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discovering Relations among Named Entities from Large Corpora. ACL’2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y N Hu</author>
<author>J G Shu</author>
<author>L H Qian</author>
<author>Q M Qiao</author>
</authors>
<title>Cross-lingual Relation Extraction based on Machine Translation (in Chinese).</title>
<date>2013</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>27</volume>
<issue>5</issue>
<pages>191--197</pages>
<contexts>
<context position="3582" citStr="Hu et al. (2013)" startWordPosition="544" endWordPosition="547">s emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 582–592, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics can be enhanced by relation instances translated from another language (e.g. English). This demonstrates that there is some complementariness between relation instances in two languages, particularly when the training data is scarce. One natural question is: Can this characteristic be made full use of so that active learning</context>
</contexts>
<marker>Hu, Shu, Qian, Qiao, 2013</marker>
<rawString>Y.N. Hu, J.G. Shu, L.H. Qian, and Q.M. Qiao. 2013. Cross-lingual Relation Extraction based on Machine Translation (in Chinese). Journal of Chinese Information Processing, 27(5): 191-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
</authors>
<title>Sample selection for statistical parsing.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<pages>253--276</pages>
<contexts>
<context position="3082" citStr="Hwa, 2004" startWordPosition="462" endWordPosition="463">last decade researchers have turned to another effective learning paradigm-- active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially active learning attempts to decrease the quantity of labeled instances by enhancing their quality, gauged by their informativeness to the learner. Since its emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings of the 52n</context>
<context position="8105" citStr="Hwa, 2004" startWordPosition="1242" endWordPosition="1243">arning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another r</context>
</contexts>
<marker>Hwa, 2004</marker>
<rawString>R. Hwa. 2004. Sample selection for statistical parsing. Computational Linguistics, 30(3): 253–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>M Jeong</author>
<author>J Lee</author>
<author>G G Lee</author>
</authors>
<title>A Cross-lingual Annotation Projection Approach for Relation Detection.</title>
<date>2010</date>
<volume>2010</volume>
<pages>564--571</pages>
<contexts>
<context position="6702" citStr="Kim et al. (2010)" startWordPosition="1021" endWordPosition="1024">n extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and Lee (2012) further employ a graph-based semisupervised learning method, namely Label Propagation (LP), to indirectly propagate labels from the source language to the target language in an iterative fashion. Both studies transfer relation annotations via parallel corpora from the resource-rich language (English) to the resourcepoor language (Korean), b</context>
</contexts>
<marker>Kim, Jeong, Lee, Lee, 2010</marker>
<rawString>S. Kim, M. Jeong, J. Lee, and G.G. Lee. 2010. A Cross-lingual Annotation Projection Approach for Relation Detection. COLING’2010: 564-571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>G G Lee</author>
</authors>
<title>A Graph-based Crosslingual Projection Approach for Weakly Supervised Relation Extraction.</title>
<date>2012</date>
<volume>2012</volume>
<pages>48--53</pages>
<contexts>
<context position="6959" citStr="Kim and Lee (2012)" startWordPosition="1064" endWordPosition="1067">., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and Lee (2012) further employ a graph-based semisupervised learning method, namely Label Propagation (LP), to indirectly propagate labels from the source language to the target language in an iterative fashion. Both studies transfer relation annotations via parallel corpora from the resource-rich language (English) to the resourcepoor language (Korean), but not vice versa. Based on a small number of labeled instances and a large number of unlabeled instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the perform</context>
</contexts>
<marker>Kim, Lee, 2012</marker>
<rawString>S. Kim and G.G. Lee. 2012. A Graph-based Crosslingual Projection Approach for Weakly Supervised Relation Extraction. ACL’2012: 48-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>Y Song</author>
<author>K Kim</author>
<author>J W Cha</author>
<author>G G Lee</author>
</authors>
<title>MMR-based active machine learning for bio named entity recognition. HLT-NAACL’2006:</title>
<date>2006</date>
<pages>69--72</pages>
<contexts>
<context position="10731" citStr="Kim et al., 2006" startWordPosition="1655" endWordPosition="1658">en except the first and last words when at least three words in between b) Entity type ET12: combination of entity types EST12: combination of entity subtypes EC12: combination of entity classes c) Mention level ML12: combination of entity mention levels MT12: combination of LDC mention types d) Overlap #WB: number of other mentions in between #MB: number of words in between M1&gt;M2 or M1&lt;M2: flag indicating whether M2/M1 is included in M1/M2. 3.2 Active Learning Algorithm We use a pool-based active learning procedure with uncertainty sampling (Scheffer et al., 2001; Culotta and McCallum, 2005; Kim et al., 2006) for both Chinese and English relation classification as illustrated in Fig. 1. During iterations a batch of unlabeled instances are chosen in terms of their informativeness to the current classifier, labeled by an oracle and in turn added into the labeled data to retrain the classifier. Due to our focus on the effectiveness of bilingual active learning on relation classification, we only use uncertainty sampling without incorporating more complex measures, such as diversity and representativeness (Settles and Craven, 2008), and leave them for future work. Algorithm uncertainty-based active le</context>
</contexts>
<marker>Kim, Song, Kim, Cha, Lee, 2006</marker>
<rawString>S. Kim, Y. Song, K. Kim, J.W. Cha, and G.G. Lee. 2006. MMR-based active machine learning for bio named entity recognition. HLT-NAACL’2006: 69– 72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Li</author>
<author>P Zhang</author>
<author>F R Wei</author>
<author>Y X Hou</author>
<author>Q Lu</author>
</authors>
<title>A Novel Feature-based Approach to Chinese Entity Relation Extraction.</title>
<date>2008</date>
<volume>2008</volume>
<pages>89--92</pages>
<contexts>
<context position="6559" citStr="Li et al., 2008" startWordPosition="1000" endWordPosition="1003">ere are only a few on multilingual relation extraction in the literature. Monolingual relation extraction: A wide range of studies on relation extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and Lee (2012) further employ a graph-based semisupervised learning method, namely Label Propagation (LP), to indirectly propagate labels from the source language to the target language in an iterative fashion. Bot</context>
</contexts>
<marker>Li, Zhang, Wei, Hou, Lu, 2008</marker>
<rawString>W.J. Li, P. Zhang, F.R. Wei, Y.X. Hou, and Q. Lu. 2008. A Novel Feature-based Approach to Chinese Entity Relation Extraction. ACL’2008: 89-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Li</author>
<author>S F Ju</author>
<author>G D Zhou</author>
<author>X J Li</author>
</authors>
<title>Active learning for imbalanced sentiment classification.</title>
<date>2012</date>
<volume>2012</volume>
<pages>139--148</pages>
<contexts>
<context position="8073" citStr="Li et al., 2012" startWordPosition="1236" endWordPosition="1239"> in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across tw</context>
</contexts>
<marker>Li, Ju, Zhou, Li, 2012</marker>
<rawString>S.S. Li, S.F. Ju, G.D. Zhou, and X.J. Li. 2012. Active learning for imbalanced sentiment classification. EMNLP-CoNLL’2012: 139-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lu</author>
<author>C H Tan</author>
<author>C Cardie</author>
<author>B K Tsou</author>
</authors>
<date>2011</date>
<booktitle>Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora. ACL’2011:</booktitle>
<pages>320--330</pages>
<contexts>
<context position="15190" citStr="Lu et al., 2011" startWordPosition="2381" endWordPosition="2384">ing overall results, no matter whether they are two different tasks or the task of machine translation on multiple language pairs. If a specific NLP task on two languages, such as relation classification, can be regarded as two tasks, it is reasonable to argue that these two tasks can benefit each other when jointly performed in the BAL framework. Yet, to our knowledge, this issue remains unexplored. An important issue for bilingual learning is how to obtain two language views for relation instances from multilingual resources. There are three solutions to this problem, i.e. parallel corpora (Lu et al., 2011), translated corpora (aka. pseudo parallel corpora) (Wan 2009), and bilingual lexicons (Oh et al., 2009). We adopt the one with pseudo parallel corpora, using the machine translation method to generate instances from one language to the other in the BAL paradigm, as depicted in Fig. 2. Figure 2. Framework of bilingual active learning In order to make full use of pseudo parallel corpora, translated labeled and unlabeled instances are augmented in the following two ways: • For labeled Chinese instances (Lc) and English instances (Le), their translated counterparts (Let and Lct), along with their</context>
</contexts>
<marker>Lu, Tan, Cardie, Tsou, 2011</marker>
<rawString>B. Lu, C.H. Tan, C. Cardie, and B.K. Tsou. 2011. Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora. ACL’2011: 320-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Oh</author>
<author>K Uchimoto</author>
<author>K Torisawa</author>
</authors>
<title>Bilingual Co-Training for Monolingual HyponymyRelation Acquisition.</title>
<date>2009</date>
<volume>2009</volume>
<pages>432--440</pages>
<contexts>
<context position="15294" citStr="Oh et al., 2009" startWordPosition="2397" endWordPosition="2400">n multiple language pairs. If a specific NLP task on two languages, such as relation classification, can be regarded as two tasks, it is reasonable to argue that these two tasks can benefit each other when jointly performed in the BAL framework. Yet, to our knowledge, this issue remains unexplored. An important issue for bilingual learning is how to obtain two language views for relation instances from multilingual resources. There are three solutions to this problem, i.e. parallel corpora (Lu et al., 2011), translated corpora (aka. pseudo parallel corpora) (Wan 2009), and bilingual lexicons (Oh et al., 2009). We adopt the one with pseudo parallel corpora, using the machine translation method to generate instances from one language to the other in the BAL paradigm, as depicted in Fig. 2. Figure 2. Framework of bilingual active learning In order to make full use of pseudo parallel corpora, translated labeled and unlabeled instances are augmented in the following two ways: • For labeled Chinese instances (Lc) and English instances (Le), their translated counterparts (Let and Lct), along with their labels, are directly added into the labeled instances in the other language; • For unlabeled Chinese in</context>
</contexts>
<marker>Oh, Uchimoto, Torisawa, 2009</marker>
<rawString>J. Oh, K. Uchimoto, and K. Torisawa. 2009. Bilingual Co-Training for Monolingual HyponymyRelation Acquisition. ACL’2009: 432-440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Osborne</author>
<author>J Baldridge</author>
</authors>
<title>Ensemble based active learning for parse selection. HLT-NAACL’</title>
<date>2004</date>
<pages>89--96</pages>
<contexts>
<context position="8135" citStr="Osborne and Baldridge, 2004" startWordPosition="1244" endWordPosition="1247">digm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sark</context>
</contexts>
<marker>Osborne, Baldridge, 2004</marker>
<rawString>M. Osborne and J. Baldridge. 2004. Ensemble based active learning for parse selection. HLT-NAACL’ 2004: 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L H Qian</author>
<author>G D Zhou</author>
<author>F Kong</author>
<author>Q M Zhu</author>
</authors>
<title>Clustering-based Stratified Seed Sampling for Semi-Supervised Relation Classification.</title>
<date>2010</date>
<volume>2010</volume>
<pages>346--355</pages>
<contexts>
<context position="2144" citStr="Qian et al., 2010" startWordPosition="311" endWordPosition="314">(ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-- active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and ad</context>
<context position="24268" citStr="Qian et al., 2010" startWordPosition="3846" endWordPosition="3849">and relation mentions in one language are trans2 http://ictclas.org/ 587 lated into the other language using the Google translator, entity alignment is performed between relation mentions and their translations. Finally 4,747 Chinese relation mentions are successfully translated and aligned from English and vice versa, 4,936 English relation mentions are translated and aligned from Chinese. SVMLIB (Chang and Lin, 2011) is selected as our classifier since it supports multi-class classification. The training parameters C (SVM) is set to 2.4 according to our previous work on relation extraction (Qian et al., 2010). Relation classification performance is evaluated using the standard Precision (P), Recall (R) and their harmonic average (F1) as well as deficiency measure (cf. latter in this section.). Overall performance scores are averaged over 10 runs. For each run, 1/40 and 1/5 randomly selected instances are used as the training and test set respectively while the remaining instances are used as the unlabeled set for further labeling during active learning iterations. Methods for Comparison For fair comparison, two baseline methods of supervised learning are included to augment their training sets wit</context>
</contexts>
<marker>Qian, Zhou, Kong, Zhu, 2010</marker>
<rawString>L.H. Qian, G.D. Zhou, F. Kong, and Q.M. Zhu. 2010. Clustering-based Stratified Seed Sampling for Semi-Supervised Relation Classification. EMNLP2010: 346-355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L H Qian</author>
<author>G D Zhou</author>
<author>Q M Zhu</author>
<author>P D Qian</author>
</authors>
<title>Exploiting constituent dependencies for tree kernel-based semantic relation extraction.</title>
<date>2008</date>
<volume>2008</volume>
<pages>697--704</pages>
<contexts>
<context position="2018" citStr="Qian et al., 2008" startWordPosition="289" endWordPosition="292">l as Natural Language Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-- active learning (AL), which, given a small number of labeled instanc</context>
<context position="6368" citStr="Qian et al., 2008" startWordPosition="968" endWordPosition="971">cusses the experimental results. Finally conclusions and directions for future work are presented in Section 6. 2 Related Work While there are many studies in monolingual relation extraction, there are only a few on multilingual relation extraction in the literature. Monolingual relation extraction: A wide range of studies on relation extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and Lee (2012) further </context>
</contexts>
<marker>Qian, Zhou, Zhu, Qian, 2008</marker>
<rawString>L.H. Qian, G.D. Zhou, Q.M. Zhu, and P.D. Qian. 2008. Exploiting constituent dependencies for tree kernel-based semantic relation extraction. COLING’2008: 697-704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reichart</author>
<author>K Tomanek</author>
<author>U Hahn</author>
<author>A Rappoport</author>
</authors>
<title>Multi-task active learning for linguistic annotations.</title>
<date>2008</date>
<volume>2008</volume>
<pages>861--869</pages>
<contexts>
<context position="8309" citStr="Reichart et al. (2008)" startWordPosition="1274" endWordPosition="1277">otential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilingual 583 machine translation, which make use of multilingual corpora to decrease human annotation efforts by selecting highly</context>
<context position="14486" citStr="Reichart et al., 2008" startWordPosition="2266" endWordPosition="2269">, denoted as SVMc and SVMe respectively, in a BAL fashion to improve their classification performance. 4.2 Bilingual Active Learning Framework Currently, AL is widely used in NLP tasks in a single language, i.e., during iterations unlabeled instances least confident only in one language are picked and manually labeled to augment the training data. The only exception is AL for machine translation (Haffari et al., 2009; Haffari and Sarkar, 2009), whose purpose is to select the most informative sentences in the source language to be manually translated into the target language. Previous studies (Reichart et al., 2008; Haffari and Sarkar, 2009) show that multi-task active learning (MTAL) can yield promising overall results, no matter whether they are two different tasks or the task of machine translation on multiple language pairs. If a specific NLP task on two languages, such as relation classification, can be regarded as two tasks, it is reasonable to argue that these two tasks can benefit each other when jointly performed in the BAL framework. Yet, to our knowledge, this issue remains unexplored. An important issue for bilingual learning is how to obtain two language views for relation instances from mu</context>
</contexts>
<marker>Reichart, Tomanek, Hahn, Rappoport, 2008</marker>
<rawString>R. Reichart, K. Tomanek, U. Hahn, and A. Rappoport. 2008. Multi-task active learning for linguistic annotations. ACL’2008: 861-869.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ringger</author>
<author>P McClanahan</author>
<author>R Haertel</author>
<author>G Busby</author>
<author>M Carmen</author>
<author>J Carroll</author>
<author>K Seppi</author>
<author>D Lonsdale</author>
</authors>
<title>Active learning for part-of-speech tagging: Accelerating corpus annotation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop at ACL’2007:</booktitle>
<pages>101--108</pages>
<contexts>
<context position="7948" citStr="Ringger et al., 2007" startWordPosition="1214" endWordPosition="1217">on a small number of labeled instances and a large number of unlabeled instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided an</context>
</contexts>
<marker>Ringger, McClanahan, Haertel, Busby, Carmen, Carroll, Seppi, Lonsdale, 2007</marker>
<rawString>E. Ringger, P. McClanahan, R. Haertel, G. Busby, M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale. 2007. Active learning for part-of-speech tagging: Accelerating corpus annotation. In Proceedings of the Linguistic Annotation Workshop at ACL’2007: 101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Scheffer</author>
<author>C Decomain</author>
<author>S Wrobel</author>
</authors>
<title>Active hidden Markov models for information extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Advances in Intelligent Data Analysis (CAIDA),</booktitle>
<pages>309--318</pages>
<contexts>
<context position="10684" citStr="Scheffer et al., 2001" startWordPosition="1647" endWordPosition="1650">east two words in between WBO: other words in between except the first and last words when at least three words in between b) Entity type ET12: combination of entity types EST12: combination of entity subtypes EC12: combination of entity classes c) Mention level ML12: combination of entity mention levels MT12: combination of LDC mention types d) Overlap #WB: number of other mentions in between #MB: number of words in between M1&gt;M2 or M1&lt;M2: flag indicating whether M2/M1 is included in M1/M2. 3.2 Active Learning Algorithm We use a pool-based active learning procedure with uncertainty sampling (Scheffer et al., 2001; Culotta and McCallum, 2005; Kim et al., 2006) for both Chinese and English relation classification as illustrated in Fig. 1. During iterations a batch of unlabeled instances are chosen in terms of their informativeness to the current classifier, labeled by an oracle and in turn added into the labeled data to retrain the classifier. Due to our focus on the effectiveness of bilingual active learning on relation classification, we only use uncertainty sampling without incorporating more complex measures, such as diversity and representativeness (Settles and Craven, 2008), and leave them for fut</context>
</contexts>
<marker>Scheffer, Decomain, Wrobel, 2001</marker>
<rawString>T. Scheffer, C. Decomain, and S. Wrobel. 2001. Active hidden Markov models for information extraction. In Proceedings of the International Conference on Advances in Intelligent Data Analysis (CAIDA), pages 309–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A I Schein</author>
<author>L H Ungar</author>
</authors>
<title>Active learning for logistic regression: an evaluation.</title>
<date>2007</date>
<booktitle>Machine Learning,</booktitle>
<volume>68</volume>
<issue>3</issue>
<pages>235--265</pages>
<contexts>
<context position="27123" citStr="Schein and Ungar, 2007" startWordPosition="4297" endWordPosition="4300">a respectively. Their translated instances in Uet and Uct are also added to the English/Chinese training data respectively. AL-BI (Active Learning with bilingual labeled and unlabeled instances): similar to ALCR with the exception that the unlabeled instances are chosen not by uncertainty scores in one language, but by the joint uncertainty scores in two languages. (cf. §4.4) Evaluation Metric Although learning curves are often used to evaluate the performance for active learning, it is preferable to quantitatively compare various active learning methods using a statistical metric deficiency (Schein and Ungar, 2007) defined as: ∑n ( ( ) ( )) F REF F AL − i = 1 n i def AL REF = n ( , ) (3) ∑ n ( ( ) ( )) F REF F REF − i = 1 n i Where n is the number of iterations involved in active learning and Fi is the F1-score of relation classification at the ith iteration. REF is the baseline active learning method and AL is an improved variant of REF, such as AL-CR or ALBI. Essentially this deficiency metric measures the degree to which REF outperforms AL. Thus, smaller deficiency value (i.e. &lt;1.0) indicates AL outperforms REF while a larger value (i.e. &gt;1.0) indicates AL underperforms REF. 5.2 Experimental Results </context>
</contexts>
<marker>Schein, Ungar, 2007</marker>
<rawString>A. I. Schein and L. H. Ungar. 2007. Active learning for logistic regression: an evaluation. Machine Learning, 68(3): 235-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sebastian</author>
<author>M Lapata</author>
</authors>
<title>Cross-lingual annotation projection of semantic roles.</title>
<date>2009</date>
<journal>Journal of ArtiÞcial Intelligence Research,</journal>
<volume>36</volume>
<issue>1</issue>
<pages>307--340</pages>
<contexts>
<context position="4645" citStr="Sebastian and Lapata, 2009" startWordPosition="703" endWordPosition="706">tances in two languages, particularly when the training data is scarce. One natural question is: Can this characteristic be made full use of so that active learning can maximally benefit relation extraction in two languages? To the best of our knowledge, so far the issue of joint active learning in two languages has yet been addressed. Moreover, the success of joint bilingual learning may lend itself to many inherent multilingual NLP tasks such as POS tagging (Yarowsky and Ngai, 2001), name entity recognition (Yarowsky et al., 2001), sentiment analysis (Wan, 2009), and semantic role labeling (Sebastian and Lapata, 2009) etc. This paper proposes a bilingual active learning (BAL) paradigm to relation classification with a small number of labeled relation instances and a large number of unlabeled instances in two languages (non-parallel). Instead of using a parallel corpus which should have entity/relation alignment information and is thus difficult to obtain, this paper employs an off-the-shelf machine translator to translate both labeled and unlabeled instances from one language into the other language, forming pseudo parallel corpora. These translated instances along with the original instances are then fed </context>
</contexts>
<marker>Sebastian, Lapata, 2009</marker>
<rawString>P. Sebastian and M. Lapata. 2009. Cross-lingual annotation projection of semantic roles. Journal of ArtiÞcial Intelligence Research, 36(1): 307-340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
<author>M Craven</author>
</authors>
<title>An Analysis of Active Learning Strategies for Sequence Labeling Tasks.</title>
<date>2008</date>
<volume>2008</volume>
<pages>1070--1079</pages>
<contexts>
<context position="3131" citStr="Settles and Craven, 2008" startWordPosition="468" endWordPosition="471">ed to another effective learning paradigm-- active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially active learning attempts to decrease the quantity of labeled instances by enhancing their quality, gauged by their informativeness to the learner. Since its emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings of the 52nd Annual Meeting of the Association for Computati</context>
<context position="11260" citStr="Settles and Craven, 2008" startWordPosition="1736" endWordPosition="1739"> with uncertainty sampling (Scheffer et al., 2001; Culotta and McCallum, 2005; Kim et al., 2006) for both Chinese and English relation classification as illustrated in Fig. 1. During iterations a batch of unlabeled instances are chosen in terms of their informativeness to the current classifier, labeled by an oracle and in turn added into the labeled data to retrain the classifier. Due to our focus on the effectiveness of bilingual active learning on relation classification, we only use uncertainty sampling without incorporating more complex measures, such as diversity and representativeness (Settles and Craven, 2008), and leave them for future work. Algorithm uncertainty-based active learning Input: - L, labeled data set - U, unlabeled data set - n, batch size Output: - SVM, classifier Repeat: 1. Train a single classifier SVM on L 2. Run the classifier on U 3. Find at most n instances in U that the classifier has the highest prediction uncertainty 4. Have these instances labeled by an oracle 5. Add them into L Until: certain number of instances are labeled or certain performance is reached Figure 1. Pool-based active learning with uncertainty sampling Since the SVMLIB package used in this paper can output</context>
</contexts>
<marker>Settles, Craven, 2008</marker>
<rawString>B. Settles and M. Craven. 2008. An Analysis of Active Learning Strategies for Sequence Labeling Tasks. EMNLP’2008: 1070–1079.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>J Zhang</author>
<author>J Su</author>
<author>G D Zhou</author>
<author>C-L Tan</author>
</authors>
<title>Multi-criteria-based active learning for named entity recognition.</title>
<date>2004</date>
<pages>2004</pages>
<contexts>
<context position="8184" citStr="Shen et al., 2004" startWordPosition="1252" endWordPosition="1255"> both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilin</context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>D. Shen, J. Zhang, J. Su, G.D. Zhou and C.-L. Tan. 2004. Multi-criteria-based active learning for named entity recognition. ACL’2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tomanek</author>
<author>U Hahn</author>
</authors>
<title>Semi-Supervised Active Learning for Sequence Labeling.</title>
<date>2009</date>
<volume>2009</volume>
<pages>1039--1047</pages>
<contexts>
<context position="8231" citStr="Tomanek and Hahn, 2009" startWordPosition="1260" endWordPosition="1263">arning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilingual 583 machine translation, which make use of</context>
</contexts>
<marker>Tomanek, Hahn, 2009</marker>
<rawString>K. Tomanek and U. Hahn. 2009. Semi-Supervised Active Learning for Sequence Labeling. ACLIJCNLP’2009: 1039-1047.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tomanek</author>
<author>J Wermter</author>
<author>U Hahn</author>
</authors>
<title>An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data.</title>
<date>2007</date>
<pages>2007--486</pages>
<contexts>
<context position="3104" citStr="Tomanek et al., 2007" startWordPosition="464" endWordPosition="467"> researchers have turned to another effective learning paradigm-- active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially active learning attempts to decrease the quantity of labeled instances by enhancing their quality, gauged by their informativeness to the learner. Since its emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings of the 52nd Annual Meeting of th</context>
<context position="8206" citStr="Tomanek et al., 2007" startWordPosition="1256" endWordPosition="1259">ultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilingual 583 machine trans</context>
</contexts>
<marker>Tomanek, Wermter, Hahn, 2007</marker>
<rawString>K. Tomanek, J. Wermter, and U. Hahn. 2007. An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data. EMNLP-CoNLL’2007: 486–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X J Wan</author>
</authors>
<title>Co-Training for Cross-Lingual Sentiment Classification.</title>
<date>2009</date>
<volume>2009</volume>
<pages>235--243</pages>
<contexts>
<context position="4588" citStr="Wan, 2009" startWordPosition="697" endWordPosition="698">e complementariness between relation instances in two languages, particularly when the training data is scarce. One natural question is: Can this characteristic be made full use of so that active learning can maximally benefit relation extraction in two languages? To the best of our knowledge, so far the issue of joint active learning in two languages has yet been addressed. Moreover, the success of joint bilingual learning may lend itself to many inherent multilingual NLP tasks such as POS tagging (Yarowsky and Ngai, 2001), name entity recognition (Yarowsky et al., 2001), sentiment analysis (Wan, 2009), and semantic role labeling (Sebastian and Lapata, 2009) etc. This paper proposes a bilingual active learning (BAL) paradigm to relation classification with a small number of labeled relation instances and a large number of unlabeled instances in two languages (non-parallel). Instead of using a parallel corpus which should have entity/relation alignment information and is thus difficult to obtain, this paper employs an off-the-shelf machine translator to translate both labeled and unlabeled instances from one language into the other language, forming pseudo parallel corpora. These translated </context>
<context position="15252" citStr="Wan 2009" startWordPosition="2391" endWordPosition="2392">r the task of machine translation on multiple language pairs. If a specific NLP task on two languages, such as relation classification, can be regarded as two tasks, it is reasonable to argue that these two tasks can benefit each other when jointly performed in the BAL framework. Yet, to our knowledge, this issue remains unexplored. An important issue for bilingual learning is how to obtain two language views for relation instances from multilingual resources. There are three solutions to this problem, i.e. parallel corpora (Lu et al., 2011), translated corpora (aka. pseudo parallel corpora) (Wan 2009), and bilingual lexicons (Oh et al., 2009). We adopt the one with pseudo parallel corpora, using the machine translation method to generate instances from one language to the other in the BAL paradigm, as depicted in Fig. 2. Figure 2. Framework of bilingual active learning In order to make full use of pseudo parallel corpora, translated labeled and unlabeled instances are augmented in the following two ways: • For labeled Chinese instances (Lc) and English instances (Le), their translated counterparts (Let and Lct), along with their labels, are directly added into the labeled instances in the </context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>X.J. Wan. 2009. Co-Training for Cross-Lingual Sentiment Classification. ACL-AFNLP’2009: 235-243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>G Ngai</author>
</authors>
<title>Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora.</title>
<date>2001</date>
<volume>2001</volume>
<pages>1--8</pages>
<contexts>
<context position="4507" citStr="Yarowsky and Ngai, 2001" startWordPosition="684" endWordPosition="687"> instances translated from another language (e.g. English). This demonstrates that there is some complementariness between relation instances in two languages, particularly when the training data is scarce. One natural question is: Can this characteristic be made full use of so that active learning can maximally benefit relation extraction in two languages? To the best of our knowledge, so far the issue of joint active learning in two languages has yet been addressed. Moreover, the success of joint bilingual learning may lend itself to many inherent multilingual NLP tasks such as POS tagging (Yarowsky and Ngai, 2001), name entity recognition (Yarowsky et al., 2001), sentiment analysis (Wan, 2009), and semantic role labeling (Sebastian and Lapata, 2009) etc. This paper proposes a bilingual active learning (BAL) paradigm to relation classification with a small number of labeled relation instances and a large number of unlabeled instances in two languages (non-parallel). Instead of using a parallel corpus which should have entity/relation alignment information and is thus difficult to obtain, this paper employs an off-the-shelf machine translator to translate both labeled and unlabeled instances from one lan</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>D. Yarowsky and G. Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. NAACL’2001: 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>G Ngai</author>
<author>R Wicentorski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<pages>2001--1</pages>
<contexts>
<context position="4556" citStr="Yarowsky et al., 2001" startWordPosition="691" endWordPosition="694">nglish). This demonstrates that there is some complementariness between relation instances in two languages, particularly when the training data is scarce. One natural question is: Can this characteristic be made full use of so that active learning can maximally benefit relation extraction in two languages? To the best of our knowledge, so far the issue of joint active learning in two languages has yet been addressed. Moreover, the success of joint bilingual learning may lend itself to many inherent multilingual NLP tasks such as POS tagging (Yarowsky and Ngai, 2001), name entity recognition (Yarowsky et al., 2001), sentiment analysis (Wan, 2009), and semantic role labeling (Sebastian and Lapata, 2009) etc. This paper proposes a bilingual active learning (BAL) paradigm to relation classification with a small number of labeled relation instances and a large number of unlabeled instances in two languages (non-parallel). Instead of using a parallel corpus which should have entity/relation alignment information and is thus difficult to obtain, this paper employs an off-the-shelf machine translator to translate both labeled and unlabeled instances from one language into the other language, forming pseudo par</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentorski, 2001</marker>
<rawString>D. Yarowsky, G. Ngai, and R. Wicentorski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. HLT’2001:1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Yu</author>
<author>L H Qian</author>
<author>G D Zhou</author>
<author>Q M Zhu</author>
</authors>
<title>Chinese Semantic Relation Extraction based on Unified Syntactic and Entity Semantic Tree (in Chinese).</title>
<date>2010</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>24</volume>
<issue>5</issue>
<pages>17--23</pages>
<contexts>
<context position="6577" citStr="Yu et al., 2010" startWordPosition="1004" endWordPosition="1007">w on multilingual relation extraction in the literature. Monolingual relation extraction: A wide range of studies on relation extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and Lee (2012) further employ a graph-based semisupervised learning method, namely Label Propagation (LP), to indirectly propagate labels from the source language to the target language in an iterative fashion. Both studies transfer</context>
</contexts>
<marker>Yu, Qian, Zhou, Zhu, 2010</marker>
<rawString>H.H. Yu, L.H. Qian, G.D. Zhou, and Q.M. Zhu. 2010. Chinese Semantic Relation Extraction based on Unified Syntactic and Entity Semantic Tree (in Chinese). Journal of Chinese Information Processing, 24(5): 17-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel Methods for Relation Extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1083--1106</pages>
<contexts>
<context position="1933" citStr="Zelenko et al., 2003" startWordPosition="273" endWordPosition="276">isely relation extraction) is an important subtask of Information Extraction (IE) as well as Natural Language Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective lea</context>
<context position="6328" citStr="Zelenko et al., 2003" startWordPosition="960" endWordPosition="963">active learning paradigm and Section 5 discusses the experimental results. Finally conclusions and directions for future work are presented in Section 6. 2 Related Work While there are many studies in monolingual relation extraction, there are only a few on multilingual relation extraction in the literature. Monolingual relation extraction: A wide range of studies on relation extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel Methods for Relation Extraction. Journal of Machine Learning Research, 3: 1083-1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhang</author>
</authors>
<title>Weakly-supervised relation classification for Information Extraction.</title>
<date>2004</date>
<marker>Zhang, 2004</marker>
<rawString>Z. Zhang. 2004. Weakly-supervised relation classification for Information Extraction. CIKM’2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhang</author>
<author>J Su</author>
<author>D M Wang</author>
<author>G D Zhou</author>
<author>C L Tan</author>
</authors>
<title>Discovering Relations between Named Entities from a Large Raw Corpus Using Tree Similarity-Based Clustering.</title>
<date>2005</date>
<volume>2005</volume>
<pages>378--389</pages>
<contexts>
<context position="2214" citStr="Zhang et al., 2005" startWordPosition="323" endWordPosition="326"> NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-- active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially act</context>
</contexts>
<marker>Zhang, Su, Wang, Zhou, Tan, 2005</marker>
<rawString>M. Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L. Tan. 2005. Discovering Relations between Named Entities from a Large Raw Corpus Using Tree Similarity-Based Clustering. IJCNLP’2005: 378-389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhang</author>
<author>J Zhang</author>
<author>J Su</author>
<author>G D Zhou</author>
</authors>
<title>A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features.</title>
<date>2006</date>
<volume>2006</volume>
<pages>825--832</pages>
<contexts>
<context position="1999" citStr="Zhang et al., 2006" startWordPosition="285" endWordPosition="288">traction (IE) as well as Natural Language Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-- active learning (AL), which, given a small number</context>
<context position="6348" citStr="Zhang et al., 2006" startWordPosition="964" endWordPosition="967">gm and Section 5 discusses the experimental results. Finally conclusions and directions for future work are presented in Section 6. 2 Related Work While there are many studies in monolingual relation extraction, there are only a few on multilingual relation extraction in the literature. Monolingual relation extraction: A wide range of studies on relation extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features. ACL/COLING’2006: 825-832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Zhao</author>
<author>R Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<volume>2005</volume>
<pages>419--426</pages>
<marker>Zhao, Grishman, 2005</marker>
<rawString>S.B. Zhao and R. Grishman. 2005. Extracting relations with integrated information using kernel methods. ACL’2005: 419-426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Zhou</author>
<author>J H Li</author>
<author>L H Qian</author>
<author>Q M Zhu</author>
</authors>
<title>Semi-Supervised Learning for Relation Extraction.</title>
<date>2008</date>
<volume>2008</volume>
<pages>32--38</pages>
<contexts>
<context position="2124" citStr="Zhou et al., 2008" startWordPosition="307" endWordPosition="310">tween two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-- active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manua</context>
</contexts>
<marker>Zhou, Li, Qian, Zhu, 2008</marker>
<rawString>G.D. Zhou, J.H. Li, L.H. Qian, and Q.M. Zhu. 2008. Semi-Supervised Learning for Relation Extraction. IJCNLP’2008: 32-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Zhou</author>
<author>J Su</author>
<author>J Zhang</author>
<author>M Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<volume>2005</volume>
<pages>427--434</pages>
<contexts>
<context position="1979" citStr="Zhou et al., 2005" startWordPosition="281" endWordPosition="284">k of Information Extraction (IE) as well as Natural Language Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-- active learning (AL), which, </context>
<context position="6260" citStr="Zhou et al., 2005" startWordPosition="948" endWordPosition="951">ibes our baseline systems. Section 4 elaborates on the bilingual active learning paradigm and Section 5 discusses the experimental results. Finally conclusions and directions for future work are presented in Section 6. 2 Related Work While there are many studies in monolingual relation extraction, there are only a few on multilingual relation extraction in the literature. Monolingual relation extraction: A wide range of studies on relation extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping</context>
<context position="9677" citStr="Zhou et al. (2005)" startWordPosition="1477" endWordPosition="1480">lel corpora, our task focuses on relation extraction by pseudo parallel corpora in two languages. 3 Baseline Systems This section first introduces the fundamental supervised learning method, and then describes a baseline active learning algorithm. 3.1 Supervised Learning We adopt the feature-based method for fundamental supervised relation classification, rather than the tree kernel-based method, since active learning needs a large number of iterations and the kernel-based method usually performs much slower than the feature-based one. Following is a list of our used features, much similar to Zhou et al. (2005): a) Lexical features of entities and their contexts WM1: bag-of-words in the 1st entity mention HM1: headword of M1 WM2: bag-of-words in the 2nd entity mention HM2: headword of M2 HM12: combination of HM1 and HM2 WBNULL: when no word in between WBFL: the only one word in between WBF: the first word in between when at least two words in between WBL: the last word in between when at least two words in between WBO: other words in between except the first and last words when at least three words in between b) Entity type ET12: combination of entity types EST12: combination of entity subtypes EC12</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. Exploring various knowledge in relation extraction. ACL’2005: 427-434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Zhu</author>
<author>E Hovy</author>
</authors>
<title>Active learning for word sense disambiguation with methods for addressing the class imbalance problem.</title>
<date>2007</date>
<volume>2007</volume>
<pages>783--790</pages>
<contexts>
<context position="8015" citStr="Zhu and Hovy, 2007" startWordPosition="1226" endWordPosition="1229">d instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an </context>
</contexts>
<marker>Zhu, Hovy, 2007</marker>
<rawString>J.B. Zhu and E. Hovy. 2007. Active learning for word sense disambiguation with methods for addressing the class imbalance problem. EMNLPCoNLL’2007: 783-790.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>