<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<note confidence="0.760861333333333">
WIT: _A Toolkit for Building Robust and Real-Time Spoken Dialogue
Systems
Mikio Nakano: Noboru Miyazaki, Norihito Yasuda, Akira Sugiyama,
</note>
<author confidence="0.588406">
Jun-ichi Hirasawa, Kohji Dohsaka, Kiyoaki Aikawa
</author>
<affiliation confidence="0.575246">
NTT Corporation
</affiliation>
<address confidence="0.6546335">
3-1 Morinosato-Wakamiya
Atsugi, Kanagawa 243-0198, Japan
</address>
<email confidence="0.999721">
E-mail: nakano@atom.brl.ntt.cojp
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999772818181818">
This paper describes WIT, a toolkit
for building spoken dialogue systems.
WIT features an incremental under-
standing mechanism that enables ro-
bust utterance understanding and real-
time responses. WIT&apos;s ability to com-
pile domain-dependent system specifi-
cations into internal knowledge sources
makes building spoken dialogue sys-
tems much easier than it is from
scratch.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999527190476191">
The recent great advances in speech and language
technologies have made it possible to build fully
implemented spoken dialogue systems (Aust et
al., 1995; Allen et al., 1996; Zue et al., 2000;
Walker et al., 2000). One of the next research
goals is to make these systems task-portable, that
is, to simplify the process of porting to another
task domain.
To this end, several toolkits for building spo-
ken dialogue systems have been developed (Bar-
nett and Singh, 1997; Sasajima et al., 1999).
One is the CSLU Toolkit (Sutton et al., 1998),
which enables rapid prototyping of a spoken di-
alogue system that incorporates a finite-state dia-
logue model. It decreases the amount of the ef-
fort required in building a spoken dialogue sys-
tem in a user-defined task domain. However, it
limits system functions; it is not easy to employ
the advanced language processing techniques de-
veloped in the realm of computational linguis-
tics. Another is GALAXY-II (Seneff et al., 1998),
</bodyText>
<affiliation confidence="0.5172555">
*Mikio Nakano is currently a visiting scientist at MIT
Laboratory for Computer Science.
</affiliation>
<bodyText confidence="0.980368605263158">
which enables modules in a dialogue system to
communicate with each other. It consists of the
hub and several servers, such as the speech recog-
nition server and the natural language server, and
the hub communicates with these servers. Al-
though it requires more specifications than finite-
state-model-based toolkits, it places less limita-
tions on system functions.
Our objective is to build robust and real-time
spoken dialogue systems in different task do-
mains. By robust we mean utterance understand-
ing is robust enough to capture not only utter-
ances including grammatical errors or self-repairs
but also utterances that are not clearly segmented
into sentences by pauses. Real time means the
system can respond to the user in real time. The
reason we focus on these features is that they are
crucial to the usability of spoken dialogue sys-
tems as well as to the accuracy of understand-
ing and appropriateness of the content of the sys-
tem utterance. Robust understanding allows the
user to speak to the system in an unrestricted
way. Responding in real time is important be-
cause if a system response is delayed, the user
might think that his/her utterance was not recog-
nized by the system and make another utterance,
making the dialogue disorderly. Systems having
these features should have several modules that
work in parallel, and each module needs some
domain-dependent knowledge sources. Creat-
ing and maintaining these knowledge sources re-
quire much effort, thus a toolkit would be help-
ful. Previous toolkits, however, do not allow us to
achieve these features, or do not provide mecha-
nisms that achieve these features without requir-
ing excessive efforts by the developers.
This paper presents WIT&apos;, which is a toolkit
&apos;WIT is an acronym of Workable spoken dialogue Inter-
</bodyText>
<page confidence="0.996848">
150
</page>
<bodyText confidence="0.999647333333333">
for building spoken dialogue systems that inte-
grate speech recognition, language understanding
and generation, and speech output. WIT features
an incremental understanding method (Nakano et
al., 1999b) that makes it possible to build a robust
and real-time system. In addition, WIT compiles
domain-dependent system specifications into in-
ternal knowledge sources so that building systems
is easier. Although WIT requires more domain-
dependent specifications than finite-state-model-
based toolkits, WIT-based systems are capable
of taking full advantage of language processing
technology WIT has been implemented and used
to build several spoken dialogue systems.
In what follows, we overview WIT, explain its
architecture, domain-dependent system specifica-
tions, and implementation, and then discuss its
advantages and problems.
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="introduction">
2 Overview
</sectionHeader>
<bodyText confidence="0.9865669375">
A WIT-based spoken dialogue system has four
main modules: the speech recognition module,
the language understanding module, the lan-
guage generation module, and the speech out-
put module. These modules exploit domain-
dependent knowledge sources, which are auto-
matically generated from the domain-dependent
system specifications. The relationship among
the modules, knowledge sources, and specifica-
tions are depicted in Figure 1.
WIT can also display and move a human-face-
like animated agent, which is controlled by the
speech output module, although this paper does
not go into details because it focuses only on spo-
ken dialogue. We also omit the GUI facilities pro-
vided by WIT.
</bodyText>
<subsectionHeader confidence="0.5041095">
3 Architecture of WIT-Based Spoken
Dialogue Systems
</subsectionHeader>
<bodyText confidence="0.999946">
Here we explain how the modules in WIT work
by exploiting domain-dependent knowledge and
how they interact with each other.
</bodyText>
<subsectionHeader confidence="0.999368">
3.1 Speech Recognition
</subsectionHeader>
<bodyText confidence="0.992747433962264">
The speech recognition module is a phoneme-
HMM-based speaker-independent continuous
speech recognizer that incrementally outputs
face Toolkit.
word hypotheses. As the recognition engine,
either VoiceRex, developed by NTT (Noda et
al., 1998), or HTK from Entropic Research can
be used. Acoustic models for HTK is trained
with the continuous speech database of the
Acoustical Society of Japan (Kobayashi et al.,
1992). This recognizer incrementally outputs
word hypotheses as soon as they are found in the
best-scored path in the forward search (Hirasawa
et al., 1998) using the ISTAR (Incremental
Structure Transmitter And Receiver) protocol,
which conveys word graph information as well as
word hypotheses. This incremental output allows
the language understanding module to process
recognition results before the speech interval
ends, and thus real-time responses are possible.
This module continuously runs and outputs
recognition results when it detects a speech
interval. This enables the language generation
module to react immediately to user interruptions
while the system is speaking.
The language model for speech recognition
is a network (regular) grammar, and it allows
each speech interval to be an arbitrary number
of phrases. A phrase is a sequence of words,
which is to be defined in a domain-dependent
way. Sentences can be decomposed into a cou-
ple of phrases. The reason we use a repeti-
tion of phrases instead of a sentence grammar
for the language model is that the speech recog-
nition module of a robust spoken dialogue sys-
tem sometimes has to recognize spontaneously
spoken utterances, which include self-repairs and
repetition. In Japanese, bunsetsu is appropriate
for defining phrases. A bunsetsu consists of one
content word and a number (possibly zero) of
function words. In the meeting room reservation
system we have developed, examples of defined
phrases are bunsetsu to specify the room to be re-
served and the time of the reservation and bun-
setsu to express affirmation and negation.
When the speech recognition module finds a
phrase boundary, it sends the category of the
phrase to the language understanding module,
and this information is used in the parsing pro-
cess.
It is possible to hold multiple language mod-
els and use any one of them when recogniz-
ing a speech interval. The language models are
</bodyText>
<page confidence="0.987016">
151
</page>
<figure confidence="0.68202818">
Phase
definitions
Semantic
frame
specifications
Dialogue
state
`\.
Language
understanding
module
word
hypotheses
Speech
recognition
module
Language 4* Generation 4- Surface-
generation procedures generation
module templates
strings
•
Speech List of 4-- List of
output pre-recorded pre-recorded
module speech files speech files
\ Rule k
definitions
Feature
definitions
&apos;A 4
Phrase
Lexical
dPfinitions ss,\ 4 structure --ir
\ rules„
\
\
-
4LLexicon
Network \
definitions „ 1.
s44 A set of
language
models
Phrase
definitions
At
user utterance system utterance
knowledge source
domain-dependent
specification
module
</figure>
<figureCaption confidence="0.999996">
Figure 1: Architecture of WIT
</figureCaption>
<bodyText confidence="0.999714625">
switched according to the requests from the lan-
guage understanding module. In this way, the
speech recognition success rate is increased by
using the context of the dialogue.
Although the current version of WIT does not
exploit probabilistic language models, such mod-
els can be incorporated without changing the ba-
sic WIT architecture.
</bodyText>
<subsectionHeader confidence="0.999344">
3.2 Language Understanding
</subsectionHeader>
<bodyText confidence="0.996741833333334">
The language understanding module receives
word hypotheses from the speech recognition
module and incrementally understands the se-
quence of the word hypotheses to update the di-
alogue state, in which the result of understand-
ing and discourse information are represented
by a frame (i.e., attribute-value pairs). The un-
derstanding module utilizes ISSS (Incremental
Significant-utterance Sequence Search) (Nakano
et al., 1999b), which is an integrated parsing and
discourse processing method. ISSS enables the
incremental understanding of user utterances that
are not segmented into sentences prior to pars-
ing by incrementally finding the most plausible
sequence of sentences (or signcant utterances
in the ISSS terms) out of the possible sentence
sequences for the input word sequence. ISSS
also makes it possible for the language generation
module to respond in real time because it can out-
put a partial result of understanding at any point
in time.
The domain-dependent knowledge used in this
module consists of a unification-based lexicon
and phrase structure rules. Disjunctive feature
descriptions are also possible; WIT incorporates
an efficient method for handling disjunctions
(Nakano, 1991). When a phrase boundary is de-
tected, the feature structure for a phrase is com-
puted using some built-in rules from the feature
structure rules for the words in the phrase. The
phrase structure rules specify what kind of phrase
sequences can be considered as sentences, and
they also enable computing the semantic repre-
sentation for found sentences. Two kinds of sen-
tences can be considered; domain-related ones
that express the user&apos;s intention about the reser-
</bodyText>
<page confidence="0.992102">
152
</page>
<bodyText confidence="0.999972117647059">
vation and dialogue-related ones that express the
user&apos;s attitude with respect to the progress of the
dialogue, such as confirmation and denial. Con-
sidering the meeting room reservation system, ex-
amples of domain-related sentences are &amp;quot;I need to
book Room 2 on Wednesday&amp;quot;, &amp;quot;I need to book
Room 2&amp;quot;, and &amp;quot;Room 2&amp;quot; and dialogue-related
ones are &amp;quot;yes&amp;quot;, &amp;quot;no&amp;quot;, and &amp;quot;Okay&amp;quot;.
The semantic representation for a sentence is
a command for updating• the dialogue state. The
dialogue state is represented by a list of attribute-
value pairs. For example, attributes used in the
meeting room reservation system include task-
related attributes, such as the date and time of
the reservation, as well as attributes that represent
discourse-related information, such as confirma-
tion and grounding.
</bodyText>
<subsectionHeader confidence="0.999268">
3.3 Language Generation
</subsectionHeader>
<bodyText confidence="0.998643397058824">
How the language generation module works
varies depending on whether the user or system
has the initiative of turn taking in the dialogue2.
Precisely speaking, the participant having the ini-
tiative is the one the system assumes has it in the
dialogue.
The domain-dependent knowledge used by the
language generation module is generation proce-
dures, which consist of a set of dialogue-phase
definitions. For each dialogue phase, an initial
function, an action function, a time-out function,
and a language model are assigned. In addition,
phase definitions designate whether the user or
the system has the initiative. In the phases in
which the system has the initiative, only the ini-
tial function and the language model are assigned.
The meeting room reservation system, for exam-
ple, has three phases: the phase in which the
user tells the system his/her request, the phase in
which the system confirms it, and the phase in
which the system tells the user the result of the
database access. In the first two phases, the user
holds the initiative, and in the last phase, the sys-
tem holds the initiative.
Functions defined here decide what string
should be spoken and send that string to the
speech output module based on the current di-
alogue state. They can also shift the dialogue
2The notion of the initiative in this paper is different from
that of the dialogue initiative of Chu-Carroll (2000).
phase and change the holder of the initiative as
well as change the dialogue state. When the dia-
logue phase shifts, the language model for speech
recognition is changed to get better speech recog-
nition performance. Typically, the language gen-
eration module is responsible for database access.
The language generation module works as fol-
lows. It first checks which dialogue participant
has the initiative. If the initiative is held by the
user, it waits until the user&apos;s speech interval ends
or a duration of silence after the end of a system
utterance is detected. The action function in the
dialogue phase at that point in time is executed in
the former case; the time-out function is executed
in the latter case. Then it goes back to the initial
stage. If the system holds the initiative, the mod-
ule executes the initial function of the phase. In
typical question-answer systems, the user has the
initiative when asking questions and the system
has it when answering.
Since the language generation module works in
parallel with the language understanding module,
utterance generation is possible even while the
system is listening to user utterances and that ut-
terance understanding is possible even while it is
speaking (Nakano et al., 1999a). Thus the system
can respond immediately after user pauses when
the user has the initiative. When the system holds
the initiative, it can immediately react to an in-
terruption by the user because user utterances are
understood in an incremental way (Dohsaka and
Shimazu, 1997).
The time-out function is effective in moving
the dialogue forward when the dialogue gets
stuck for some reason. For example, the system
may be able to repeat the same question with an-
other expression and may also be able to ask the
user a more specific question.
</bodyText>
<subsectionHeader confidence="0.983141">
3.4 Speech Output
</subsectionHeader>
<bodyText confidence="0.999969555555555">
The speech output module produces speech ac-
cording to the requests from the language gener-
ation module by using the correspondence table
between strings and pre-recorded speech data. It
also notifies the language generation module that
speech output has finished so that the language
generation module can take into account the tim-
ing of the end of system utterance. The meeting
room reservation system uses speech files of short
</bodyText>
<page confidence="0.995629">
153
</page>
<bodyText confidence="0.814198">
phrases.
</bodyText>
<sectionHeader confidence="0.8746445" genericHeader="method">
4 Building Spoken Dialogue Systems
with WIT
</sectionHeader>
<subsectionHeader confidence="0.797144">
4.1 Domain-Dependent System
Specifications
</subsectionHeader>
<bodyText confidence="0.990434763157895">
Spoken dialogue systems can be built with WIT
by preparing several domain-dependent specifica-
tions. Below we explain the specifications.
Feature Definitions: Feature definitions spec-
ify the set of features used in the grammar for lan-
guage understanding. They also specify whether
each feature is a head feature or a foot feature
(Pollard and Sag, 1994). This information is used
when constructing feature structures for phrases
in a built-in process.
The following is an example of a feature defini-
tion. Here we use examples from the specification
of the meeting room reservation system.
(case head)
It means that the case feature is used and it is a
head feature3.
Lexical Descriptions: Lexical descriptions
specify both pronunciations and grammatical
features for words. Below is an example lexical
item for the word 1-gatsu (January).
(1-gatsu ichigatsu month nil 1)
The first three elements are the identifier, the pro-
nunciation, and the grammatical category of the
word. The remaining two elements are the case
and semantic feature values.
Phrase Definitions: Phrase definitions specify
what kind of word sequence can be recognized
as a phrase. Each definition is a pair compris-
ing a phrase category name and a network of
word categories. In the example below, month-
phrase is the phrase category name and the re-
maining part is the network of word categories.
opt means an option and or means a disjunc-
tion. For instance, a word sequence that con-
sists of a word in the month category, such as 1-
gatsu (January), and a word in the admoninal -
particle category, such as no (of), forms a
phrase in the month-phrase category.
</bodyText>
<footnote confidence="0.818582">
3In this section, we use examples of different description
from the actual ones for simplicity. Actual specifications are
written in part in Japanese.
</footnote>
<figure confidence="0.9099955">
(month-phrase
(month
(opt
(or
expression-following-subject
(admoninal-particle
(opt
sentence-final-particle))))))
</figure>
<bodyText confidence="0.992497681818182">
Network Definitions: Network definitions
specify what kind of phrases can be included in
each language model. Each definition is a pair
comprising a network name and a set of phrase
category names.
Semantic-Frame Specifications: The result of
understanding and dialogue history can be stored
in the dialogue state, which is represented by a
flat frame structure, i.e., a set of attribute-value
pairs. Semantic-frame specifications define the
attributes used in the frame. The meeting room
reservation system uses task-related attributes.
Two are start and end, which represent the
user&apos;s intention about the start and end times of
the reservation for some meeting room. It also
has attributes that represent discourse informa-
tion. One is confirmed, whose value indicates
whether if the system has already made an utter-
ance to confirm the content of the task-related at-
tributes.
Rule Definitions: Each rule has one of the fol-
lowing two forms.
</bodyText>
<table confidence="0.5780556">
( (rule name)
(child feature structure)
. . . (child feature structure)
=&gt; (mother feature structure)
(priority increase) )
( (rule name)
(child feature structure)
. . . (child feature structure)
=&gt; (frame operation command)
(priority increase) )
</table>
<bodyText confidence="0.939943">
These rules are similar to DCG (Pereira and War-
ren, 1980) rules; they can include logical vari-
ables and these variables can be bound when
these rules are applied. It is possible to add to the
rules constraints that stipulate relationships that
must hold among variables (Nakano, 1991), but
we do not explain these constraints in detail in this
</bodyText>
<page confidence="0.999557">
154
</page>
<bodyText confidence="0.9994864">
paper. The priorities are used for disambiguat-
ing interpretation in the incremental understand-
ing method (Nakano et al., 1999b).
When the command on the right-hand side of
the arrow is a frame operation command, phrases
to which this rule can be applied can be consid-
ered a sentence, and the sentence&apos;s semantic rep-
resentation is the command for updating the dia-
logue state. The command is one of the follow-
ing:
</bodyText>
<listItem confidence="0.9980264">
• A command to set the value of an attribute
of the frame,
• A command to increase the priority,
• Conditional commands (If-then-else type
command, the condition being whether the
value of an attribute of the frame is or is not
equal to a specified value, or a conjunction
or disjunction of the above condition), or
• A list of commands to be sequentially exe-
cuted.
</listItem>
<bodyText confidence="0.985802546875">
Thanks to conditional commands, it is possible
to represent the semantics of sentences context-
dependently.
The following rule is an example.
(start-end-times-command
(time-phrase :from *start)
(time-phrase (:or :to nil) *end)
.&gt; (command (set :start *start)
(set :end *end)))
The name of this rule is s tart -end- t imes -
command. The second and third elements
are child feature structures. In these elements,
time-phrase is a phrase category, : from and
( : or : to nil) are case feature values, and
*start and *end are semantic feature val-
ues. Here : or means a disjunction, and sym-
bols starting with an asterisk are variables. The
right-hand side of the arrow is a command to up-
date the frame. The second element of the com-
mand, (set :start *start) , changes the
: start attribute value of the frame to the in-
stance of *start, which should be bound when
applying this rule to the child feature structures.
Phase Definitions: Each phase definition con-
sists of a phase name, a network name, an ini-
tiative holder specification, an initial function, an
action function, a maximum silence duration, and
a time-out function. The network name is the
identifier of the language model for the speech
recognition. The maximum silence duration spec-
ifies how long the generation module should wait
until the time-out function is invoked.
Below is an example of a phase definition.
The first element request is the name of this
phase, &amp;quot; fmr_reques t &amp;quot; is the name of the
network, and move-to-request-phase and
request-phase--action are the names of
the initial and action functions. In this phase,
the maximum silence duration is ten seconds and
the name of the time-out function is reques t -
phase- t imeout
(request &amp;quot;fmr_request&amp;quot;
move-to-request-phase
request-phase-action
10.0
request-phase-timeout)
For the definitions of these functions, WIT pro-
vides functions for accessing the dialogue state,
sending a request to speak to the speech out-
put module, generating strings to be spoken us-
ing surface generation templates, shifting the di-
alogue phase, taking and releasing the initiative,
and so on. Functions are defined in terms of the
Common Lisp program.
Surface-generation Templates: Surface-
generation templates are used by the surface
generation library function, which converts
a list-structured semantic representation to a
sequence of strings. Each string can be spoken,
i.e., it is in the list of pre-recorded speech files.
For example, let us consider the conversion
of the semantic representation ( date ( date-
expres s on 3 15 ) ) to strings using the fol-
lowing template.
</bodyText>
<equation confidence="0.614744333333333">
( (date
(date-expression *month *day))
((*month gatsu) (*day nichi)))
</equation>
<bodyText confidence="0.999362">
The surface generation library function matches
the input semantic representation with the first el-
ement of the template and checks if a sequences
</bodyText>
<page confidence="0.997988">
155
</page>
<bodyText confidence="0.9898024">
of strings appear in the speech file list. It re-
turns ( &amp;quot; 3 gat su15nichi &amp;quot; ) (March 15th)
if the string &amp;quot;3gatsul5nichi&amp;quot; is in the list of
pre-recorded speech files, and otherwise, returns
&amp;quot; 3gatsu&amp;quot; &amp;quot; 15n.ichi &amp;quot; ) when these
strings are in the list.
List of Pre-recorded Speech Files: The list of
pre-recorded speech files should show the corre-
spondence between strings and speech files to be
played by the speech output module.
</bodyText>
<subsectionHeader confidence="0.999625">
4.2 Compiling System Specifications
</subsectionHeader>
<bodyText confidence="0.999755888888889">
From the specifications explained above, domain-
dependent knowledge sources are created as indi-
cated by the dashed arrows in Figure 1. When cre-
ating the knowledge sources, WIT checks for sev-
eral kinds of consistency. For example, the set of
word categories appearing in the lexicon and the
set of word categories appearing in phrase defi-
nitions are compared. This makes it easy to find
errors in the domain specifications.
</bodyText>
<sectionHeader confidence="0.680763" genericHeader="method">
$ Implementation
</sectionHeader>
<bodyText confidence="0.999986619047619">
WIT has been implemented in Common Lisp and
C on UNIX, and we have built several experi-
mental and demonstration dialogue systems using
it, including a meeting room reservation system
(Nakano et al., 1999b), a video-recording pro-
gramming system, a schedule management sys-
tem (Nakano et al., 1999a), and a weather in-
fomiation system (Dohsaka et al., 2000). The
meeting room reservation system has vocabulary
of about 140 words, around 40 phrase structure
rules, nine attributes in the semantic frame, and
around 100 speech files. A sample dialogue be-
tween this system and a naive user is shown
in Figure 2. This system employs H&apos;TK as the
speech recognition engine. The weather informa-
tion system can answer the user&apos;s questions about
weather forecasts in Japan. The vocabulary size
is around 500, and the number of phrase structure
rules is 31. The number of attributes in the se-
mantic frame is 11, and the number of the files of
the pre-recorded speech is about 13,000.
</bodyText>
<sectionHeader confidence="0.999022" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9674155">
As explained above, the architecture of WIT al-
lows us to develop a system that can use utter-
ances that are not clearly segmented into sen-
tences by pauses and respond in real time. Below
we discuss other advantages and remaining prob-
lems.
</bodyText>
<subsectionHeader confidence="0.998668">
6.1 Descriptive Power
</subsectionHeader>
<bodyText confidence="0.99990632">
Whereas previous finite-state-model-based tool-
kits place many severe restrictions on domain de-
scriptions, WIT has enough descriptive power to
build a variety of dialogue systems. Although the
dialogue state is represented by a simple attribute-
value matrix, since there is no limitation on the
number of attributes, it can hold more compli-
cated information. For example, it is possible to
represent a discourse stack whose depth is lim-
ited. Recording some dialogue history is also
possible. Since the language understanding mod-
ule utilizes unification, a wide variety of lin-
guistic phenomena can be covered. For exam-
ple, speech repairs, particle omission, and fillers
can be dealt with in the framework of unifica-
tion grammar (Nakano et al., 1994; Nakano and
Shimazu, 1999). The language generation mod-
ule features Common Lisp functions, so there is
no limitation on the description. Some of the
systems we have developed feature a generation
method based on hierarchical planning (Dohsaka
and Shimazu, 1997). It is also possible to build a
simple finite-state-model-based dialogue system
using WIT. States can be represented by dialogue
phases in WIT.
</bodyText>
<subsectionHeader confidence="0.998384">
6.2 Consistency
</subsectionHeader>
<bodyText confidence="0.999946636363636">
In an agglutinative language such as Japanese,
there is no established definition of words, so dia-
logue system developers must define words. This
sometimes causes a problem in that the defini-
tion of word, that is, the word boundaries, in the
speech recognition module are different from that
in the language understanding module. In WIT,
however, since the common lexicon is used in
both the speech recognition module and language
understanding module, the consistency between
them is maintained.
</bodyText>
<subsectionHeader confidence="0.999681">
6.3 Avoiding Information Loss
</subsectionHeader>
<bodyText confidence="0.999973">
In ordinary spoken language systems, the speech
recognition module sends just a word hypoth-
esis to the language processing module, which
</bodyText>
<page confidence="0.99458">
156
</page>
<table confidence="0.863851222222222">
speaker start end
time (s) time (s)
system: 614.53 615.93
user: 616.38 618.29
system: 619.97 620.13
user: 622.65 624.08
system: 625.68 625.91
user: 626.65 627.78
system: 629.25 629.55
user: 629.91 631.67
system: 633.29 633.57
user: 634.95 636.00
system: 637.50 645.43
user: 645.74 646.04
system: 647.05 648.20
utterance
donoyOna goyo ken desho ka (how may I help you?)
kaigishitsu o yoyaku shitai ndesu ga (I&apos;d like to make a reserva-
tion for a meeting room)
hai (uh-huh)
san-gatsu jani-nichi (on March 12th)
hai (uh-huh)
jayo-ji kara (from 14:00)
hai (uh-huh)
jashichi-ji sanjup-pun made (to 17:30)
hai (uh-huh)
dai-kaigishitsu (the large meeting room)
</table>
<bodyText confidence="0.897158">
san-gatsu j kara, jashichi-ji sanjup-pun made,
dai-kaigishitsu toya koto de yoroshi deshoka (on March 12th,
from 14:00 to 17:30, the large meeting room, is that right?)
hai (yes)
kashikomarimashitti (all right)
</bodyText>
<figureCaption confidence="0.993937">
Figure 2: An example dialogue of an example system
</figureCaption>
<bodyText confidence="0.998841333333333">
must disambiguate word meaning and find phrase
boundaries by parsing. In contrast, the speech
recognition module in WIT sends not only words
but also word categories, phrase boundaries, and
phrase categories. This leads to less expensive
and better language understanding.
</bodyText>
<subsectionHeader confidence="0.994994">
6.4 Problems and Limitations
</subsectionHeader>
<bodyText confidence="0.999987464285714">
Several problems remain with WIT. One of the
most significant is that the system developer must
write language generation functions. If the gen-
eration functions employ sophisticated dialogue
strategies, the system can perform complicated
dialogues that are not just question answering.
WIT, however, does not provide task-independent
facilities that make it easier to employ such dia-
logue strategies.
There have been several efforts aimed at de-
veloping a domain-independent method for gen-
erating responses from a frame representation of
user requests (Bobrow et al., 1977; Chu-Carroll,
1999). Incorporating such techniques would de-
crease the system developer workload. However,
there has been no work on domain-independent
response generation for robust spoken dialogue
systems that can deal with utterances that might
include pauses in the middle of a sentence, which
WIT handles well. Therefore incorporating those
techniques remains as a future work.
Another limitation is that WIT cannot deal with
multiple speech recognition candidates such as
those in an N-best list. Extending WIT to deal
with multiple recognition results would improve
the performance of the whole system. The ISSS
preference mechanism is expected to play a role
in choosing the best recognition result.
</bodyText>
<sectionHeader confidence="0.998451" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999873">
This paper described WIT, a toolkit for build-
ing spoken dialogue systems. Although it re-
quires more system specifications than previous
finite-state-model-based toolkits, it enables one
to easily construct real-time, robust spoken dia-
logue systems that incorporates advanced compu-
tational linguistics technologies.
</bodyText>
<sectionHeader confidence="0.99649" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99970475">
The authors thank Drs. Ken&apos;ichiro Ishii, Nori-
hiro Hagita, and Takeshi Kawabata for their sup-
port of this research. Thanks also go to Tetsuya
Kubota, Ryoko Kima, and the members of the
Dialogue Understanding Research Group. We
used the speech recognition engine VoiceRex de-
veloped by NTT Cyber Space Laboratories and
thank those who helped us use it. Comments by
</bodyText>
<page confidence="0.992866">
157
</page>
<bodyText confidence="0.985858">
the anonymous reviewers were of great help.
</bodyText>
<sectionHeader confidence="0.997222" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999558297029703">
James F. Allen, Bradford W. Miller, Eric K. Ringger,
and Teresa Sikorsld. 1996. A robust system for nat-
ural spoken dialogue. In Proceedings of the 34th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-96), pages 62-70.
Harald Aust, Martin Oerder, Frank Seide, and Volker
Steinbiss. 1995. The Philips automatic train
timetable information system. Speech Communi-
cation, 17:249-262.
James Barnett and Mona Singh. 1997. Designing
a portable spoken language system. In Elisabeth
Maier, Marion Mast, and Susann LuperFoy, editors,
Dialogue Processing in Spoken Language Systems,
pages 156-170. Springer-Verlag.
Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay,
Donald A. Norman, Henry Thompson, and Teny
Winograd. 1977. GUS, a frame driven dialog sys-
tem. Artificial Intelligence, 8:155-173.
Jennifer Chu-Carroll. 1999. Form-based reason-
ing for mixed-initiative dialogue management in
information-query systems. In Proceedings of the
Sixth European Conference on Speech Communica-
tion and Technology (Eurospeech-99), pages 1519-
1522.
Junnifer Chu-Carroll. 2000. MIMIC: An adaptive
mixed initiative spoken dialogue system for infor-
mation queries. In Proceedings of the 6th Con-
ference on Applied Natural Language Processing
(ANLP-00), pages 97-104.
Kohji Dohsaka and Akira Shimazu. 1997. System ar-
chitecture for spoken utterance production in col-
laborative dialogue. In Working Notes of IJCAI
1997 Workshop on Collaboration, Cooperation and
Conflict in Dialogue Systems.
Kohji Dohsaka, Norihito Yasuda, Noboru Miyazaki,
Mikio Nakano, and Kiyoald Aikawa. 2000. An ef-
ficient dialogue control method under system&apos;s lim-
ited knowledge. In Proceedings of the Sixth Inter-
national Conference on Spoken Language Process-
ing (ICSLP-00).
Jun-ichi Hirasawa, Noboru Miyazaki, Mikio Nakano,
and Takeshi Kawabata. 1998. Implementation
of coordinative nodding behavior on spoken dia-
logue systems. In Proceedings of the Fifth Interna-
tional Conference on Spoken Language Processing
(ICSLP-98), pages 2347-2350.
Tetsunori Kobayashi, Shuichi Itahashi, Satoru
Hayamizu, and Toshiyuld Takezawa. 1992. Asj
continuous speech corpus for research. The journal
of the Acoustical Society of Japan, 48(12):888-893.
Mikio Nakano and Akira Shimazu. 1999. Pars-
ing utterances including self-repairs. In Yorick
Wilks, editor, Machine Conversations, pages 99-
112. Kluwer Academic Publishers.
Mikio Nakano, Akira Shimazu, and Kiyoshi Kogure.
1994. A grammar and a parser for spontaneous
speech. In Proceedings of the 15th Interna-
tional Conference on Computational Linguistics
(COLING-94), pages 1014-1020.
Mikio Nakano, Kohji Dohsaka, Noboru Miyazaki,
Jun ichi Hirasawa, Masafumi Tamoto, Masahito
Kawamori, Akira Sugiyama, and Takeshi Kawa-
bata. 1999a. Handling rich turn-taking in spoken
dialogue systems. In Proceedings of the Sixth Eu-
ropean Conference on Speech Communication and
Technology (Eurospeech-99), pages 1167-1170.
Mikio Nakano, Noboru Miyazalci, Jun-ichi Hirasawa,
Kohji Dohsaka, and Takeshi Kawabata. 1999b.
Understanding unsegmented user utterances in real-
time spoken dialogue systems. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics (ACL-99), pages 200-
207.
Mikio Nakano. 1991. Constraint projection: An ef-
ficient treatment of disjunctive feature descriptions.
In Proceedings of the 29th Annual Meeting of the
Association for Computational Linguistics (ACL-
91), pages 307-314.
Yoshiaki Noda, Yoshilcazu Yamaguchi, Tomokazu
Yamada, Alcihiro Ixnamura, Satoshi Takahashi,
Tomoko Matsui, and Kiyoald Ailcawa. 1998. The
development of speech recognition engine REX. In
Proceedings of the 1998 lEICE General Confer-
ence D-14-9, page 220. (in Japanese).
Fernando C. N. Pereira and David H. D. Warren.
1980. Definite clause grammars for language
analysis-a survey of the formalism and a compar-
ison with augmented transition networks. Artificial
Intelligence, 13:231-278.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. CSLI, Stanford.
Munehiko Sasajima, Yakehide Yano, and Yasuyuld
Kono. 1999. EUROPA: A generic framework for
developing spoken dialogue systems. In Proceed-
ings of the Sixth European Conference on Speech
Communication and Technology (Eurospeech-99),
pages 1163-1166.
Stephanie Seneff, Ed Hurley, Raymond Lau, Chris-
tine Pao, Philipp Schmid, and Victor Zue. 1998.
GALAXY-II: A reference architecture for conver-
sational system development. In Proceedings of
</reference>
<page confidence="0.979102">
158
</page>
<reference confidence="0.998685416666667">
the Fifth International Conference on Spoken Lan-
guage Processing (ICSLP-98).
Stephen Sutton, Ronald A. Cole, Jacques de Villiers,
Johan Schallcwyk, Pieter Vermeulen, Michael W.
Macon, Yonghong Yan, Edward Kaiser, Brian Run-
dle, Khaldoun Shobald, Paul Hosom, Alex Kain,
Johan Wouters, Dominic W. Massaro, and Michael
Cohen. 1998. Universal speech tools: The
CSLU toolkit. In Proceedings of the Fifth Interna-
tional Conference on Spoken Language Processing
(ICSLP-98), pages 3221-3224.
Marilyn Walker, Irene Langkilde, Jerry Wright Allen
Gorin, and Diane Litman. 2000. Learning to pre-
dict problematic situations in a spoken dialogue
system: Experiments with how may I help you? In
Proceedings of the First Meeting of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL-00), pages 210-217.
Victor Zue, Stephanie Seneff, James Glass, Joseph Po-
lifroni, Christine Pao, Timothy J. Hazen, and Lee
Hetherington. 2000. Jupiter: A telephone-based
conversational interface for weather information.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):85-96.
</reference>
<page confidence="0.998834">
159
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.520178">
<title confidence="0.998652">WIT: _A Toolkit for Building Robust and Real-Time Spoken Dialogue Systems</title>
<author confidence="0.809296">Mikio Nakano Noboru Miyazaki</author>
<author confidence="0.809296">Norihito Yasuda</author>
<author confidence="0.809296">Akira Jun-ichi Hirasawa</author>
<author confidence="0.809296">Kohji Dohsaka</author>
<author confidence="0.809296">Kiyoaki</author>
<affiliation confidence="0.986389">NTT</affiliation>
<address confidence="0.9786545">3-1 Atsugi, Kanagawa 243-0198,</address>
<email confidence="0.998014">E-mail:nakano@atom.brl.ntt.cojp</email>
<abstract confidence="0.98853425">This paper describes WIT, a toolkit for building spoken dialogue systems. WIT features an incremental understanding mechanism that enables robust utterance understanding and realtime responses. WIT&apos;s ability to compile domain-dependent system specifications into internal knowledge sources makes building spoken dialogue systems much easier than it is from scratch.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
<author>Bradford W Miller</author>
<author>Eric K Ringger</author>
<author>Teresa Sikorsld</author>
</authors>
<title>A robust system for natural spoken dialogue.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL-96),</booktitle>
<pages>62--70</pages>
<contexts>
<context position="856" citStr="Allen et al., 1996" startWordPosition="116" endWordPosition="119">agawa 243-0198, Japan E-mail: nakano@atom.brl.ntt.cojp Abstract This paper describes WIT, a toolkit for building spoken dialogue systems. WIT features an incremental understanding mechanism that enables robust utterance understanding and realtime responses. WIT&apos;s ability to compile domain-dependent system specifications into internal knowledge sources makes building spoken dialogue systems much easier than it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al., 1996; Zue et al., 2000; Walker et al., 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spoken dialogue systems have been developed (Barnett and Singh, 1997; Sasajima et al., 1999). One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model. It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain. However, it</context>
</contexts>
<marker>Allen, Miller, Ringger, Sikorsld, 1996</marker>
<rawString>James F. Allen, Bradford W. Miller, Eric K. Ringger, and Teresa Sikorsld. 1996. A robust system for natural spoken dialogue. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL-96), pages 62-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Aust</author>
<author>Martin Oerder</author>
<author>Frank Seide</author>
<author>Volker Steinbiss</author>
</authors>
<title>The Philips automatic train timetable information system.</title>
<date>1995</date>
<journal>Speech Communication,</journal>
<pages>17--249</pages>
<contexts>
<context position="836" citStr="Aust et al., 1995" startWordPosition="112" endWordPosition="115">akamiya Atsugi, Kanagawa 243-0198, Japan E-mail: nakano@atom.brl.ntt.cojp Abstract This paper describes WIT, a toolkit for building spoken dialogue systems. WIT features an incremental understanding mechanism that enables robust utterance understanding and realtime responses. WIT&apos;s ability to compile domain-dependent system specifications into internal knowledge sources makes building spoken dialogue systems much easier than it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al., 1996; Zue et al., 2000; Walker et al., 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spoken dialogue systems have been developed (Barnett and Singh, 1997; Sasajima et al., 1999). One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model. It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task</context>
</contexts>
<marker>Aust, Oerder, Seide, Steinbiss, 1995</marker>
<rawString>Harald Aust, Martin Oerder, Frank Seide, and Volker Steinbiss. 1995. The Philips automatic train timetable information system. Speech Communication, 17:249-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Barnett</author>
<author>Mona Singh</author>
</authors>
<title>Designing a portable spoken language system.</title>
<date>1997</date>
<booktitle>Dialogue Processing in Spoken Language Systems,</booktitle>
<pages>156--170</pages>
<editor>In Elisabeth Maier, Marion Mast, and Susann LuperFoy, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1148" citStr="Barnett and Singh, 1997" startWordPosition="166" endWordPosition="170">in-dependent system specifications into internal knowledge sources makes building spoken dialogue systems much easier than it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al., 1996; Zue et al., 2000; Walker et al., 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spoken dialogue systems have been developed (Barnett and Singh, 1997; Sasajima et al., 1999). One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model. It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain. However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics. Another is GALAXY-II (Seneff et al., 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enable</context>
</contexts>
<marker>Barnett, Singh, 1997</marker>
<rawString>James Barnett and Mona Singh. 1997. Designing a portable spoken language system. In Elisabeth Maier, Marion Mast, and Susann LuperFoy, editors, Dialogue Processing in Spoken Language Systems, pages 156-170. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel G Bobrow</author>
<author>Ronald M Kaplan</author>
<author>Martin Kay</author>
<author>Donald A Norman</author>
<author>Henry Thompson</author>
<author>Teny Winograd</author>
</authors>
<title>GUS, a frame driven dialog system.</title>
<date>1977</date>
<journal>Artificial Intelligence,</journal>
<pages>8--155</pages>
<contexts>
<context position="27532" citStr="Bobrow et al., 1977" startWordPosition="4378" endWordPosition="4381">age understanding. 6.4 Problems and Limitations Several problems remain with WIT. One of the most significant is that the system developer must write language generation functions. If the generation functions employ sophisticated dialogue strategies, the system can perform complicated dialogues that are not just question answering. WIT, however, does not provide task-independent facilities that make it easier to employ such dialogue strategies. There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests (Bobrow et al., 1977; Chu-Carroll, 1999). Incorporating such techniques would decrease the system developer workload. However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well. Therefore incorporating those techniques remains as a future work. Another limitation is that WIT cannot deal with multiple speech recognition candidates such as those in an N-best list. Extending WIT to deal with multiple recognition results would improve the performance of the whol</context>
</contexts>
<marker>Bobrow, Kaplan, Kay, Norman, Thompson, Winograd, 1977</marker>
<rawString>Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay, Donald A. Norman, Henry Thompson, and Teny Winograd. 1977. GUS, a frame driven dialog system. Artificial Intelligence, 8:155-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
</authors>
<title>Form-based reasoning for mixed-initiative dialogue management in information-query systems.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixth European Conference on Speech Communication and Technology (Eurospeech-99),</booktitle>
<pages>1519--1522</pages>
<contexts>
<context position="27552" citStr="Chu-Carroll, 1999" startWordPosition="4382" endWordPosition="4383">4 Problems and Limitations Several problems remain with WIT. One of the most significant is that the system developer must write language generation functions. If the generation functions employ sophisticated dialogue strategies, the system can perform complicated dialogues that are not just question answering. WIT, however, does not provide task-independent facilities that make it easier to employ such dialogue strategies. There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests (Bobrow et al., 1977; Chu-Carroll, 1999). Incorporating such techniques would decrease the system developer workload. However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well. Therefore incorporating those techniques remains as a future work. Another limitation is that WIT cannot deal with multiple speech recognition candidates such as those in an N-best list. Extending WIT to deal with multiple recognition results would improve the performance of the whole system. The ISSS p</context>
</contexts>
<marker>Chu-Carroll, 1999</marker>
<rawString>Jennifer Chu-Carroll. 1999. Form-based reasoning for mixed-initiative dialogue management in information-query systems. In Proceedings of the Sixth European Conference on Speech Communication and Technology (Eurospeech-99), pages 1519-1522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junnifer Chu-Carroll</author>
</authors>
<title>MIMIC: An adaptive mixed initiative spoken dialogue system for information queries.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Conference on Applied Natural Language Processing (ANLP-00),</booktitle>
<pages>97--104</pages>
<contexts>
<context position="12419" citStr="Chu-Carroll (2000)" startWordPosition="1937" endWordPosition="1938">mple, has three phases: the phase in which the user tells the system his/her request, the phase in which the system confirms it, and the phase in which the system tells the user the result of the database access. In the first two phases, the user holds the initiative, and in the last phase, the system holds the initiative. Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state. They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of Chu-Carroll (2000). phase and change the holder of the initiative as well as change the dialogue state. When the dialogue phase shifts, the language model for speech recognition is changed to get better speech recognition performance. Typically, the language generation module is responsible for database access. The language generation module works as follows. It first checks which dialogue participant has the initiative. If the initiative is held by the user, it waits until the user&apos;s speech interval ends or a duration of silence after the end of a system utterance is detected. The action function in the dialog</context>
</contexts>
<marker>Chu-Carroll, 2000</marker>
<rawString>Junnifer Chu-Carroll. 2000. MIMIC: An adaptive mixed initiative spoken dialogue system for information queries. In Proceedings of the 6th Conference on Applied Natural Language Processing (ANLP-00), pages 97-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kohji Dohsaka</author>
<author>Akira Shimazu</author>
</authors>
<title>System architecture for spoken utterance production in collaborative dialogue.</title>
<date>1997</date>
<booktitle>In Working Notes of IJCAI</booktitle>
<contexts>
<context position="13942" citStr="Dohsaka and Shimazu, 1997" startWordPosition="2186" endWordPosition="2189">the initiative when asking questions and the system has it when answering. Since the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking (Nakano et al., 1999a). Thus the system can respond immediately after user pauses when the user has the initiative. When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997). The time-out function is effective in moving the dialogue forward when the dialogue gets stuck for some reason. For example, the system may be able to repeat the same question with another expression and may also be able to ask the user a more specific question. 3.4 Speech Output The speech output module produces speech according to the requests from the language generation module by using the correspondence table between strings and pre-recorded speech data. It also notifies the language generation module that speech output has finished so that the language generation module can take into a</context>
<context position="24915" citStr="Dohsaka and Shimazu, 1997" startWordPosition="3980" endWordPosition="3983">ble to represent a discourse stack whose depth is limited. Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar (Nakano et al., 1994; Nakano and Shimazu, 1999). The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shimazu, 1997). It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented by dialogue phases in WIT. 6.2 Consistency In an agglutinative language such as Japanese, there is no established definition of words, so dialogue system developers must define words. This sometimes causes a problem in that the definition of word, that is, the word boundaries, in the speech recognition module are different from that in the language understanding module. In WIT, however, since the common lexicon is used in both the speech recognition module and language understa</context>
</contexts>
<marker>Dohsaka, Shimazu, 1997</marker>
<rawString>Kohji Dohsaka and Akira Shimazu. 1997. System architecture for spoken utterance production in collaborative dialogue. In Working Notes of IJCAI 1997 Workshop on Collaboration, Cooperation and Conflict in Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kohji Dohsaka</author>
<author>Norihito Yasuda</author>
<author>Noboru Miyazaki</author>
<author>Mikio Nakano</author>
<author>Kiyoald Aikawa</author>
</authors>
<title>An efficient dialogue control method under system&apos;s limited knowledge.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth International Conference on Spoken Language Processing (ICSLP-00).</booktitle>
<contexts>
<context position="23018" citStr="Dohsaka et al., 2000" startWordPosition="3668" endWordPosition="3671">WIT checks for several kinds of consistency. For example, the set of word categories appearing in the lexicon and the set of word categories appearing in phrase definitions are compared. This makes it easy to find errors in the domain specifications. $ Implementation WIT has been implemented in Common Lisp and C on UNIX, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system (Nakano et al., 1999b), a video-recording programming system, a schedule management system (Nakano et al., 1999a), and a weather infomiation system (Dohsaka et al., 2000). The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2. This system employs H&apos;TK as the speech recognition engine. The weather information system can answer the user&apos;s questions about weather forecasts in Japan. The vocabulary size is around 500, and the number of phrase structure rules is 31. The number of attributes in the semantic frame is 11, and the number of the files of the pre-recorded speech </context>
</contexts>
<marker>Dohsaka, Yasuda, Miyazaki, Nakano, Aikawa, 2000</marker>
<rawString>Kohji Dohsaka, Norihito Yasuda, Noboru Miyazaki, Mikio Nakano, and Kiyoald Aikawa. 2000. An efficient dialogue control method under system&apos;s limited knowledge. In Proceedings of the Sixth International Conference on Spoken Language Processing (ICSLP-00).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-ichi Hirasawa</author>
<author>Noboru Miyazaki</author>
<author>Mikio Nakano</author>
<author>Takeshi Kawabata</author>
</authors>
<title>Implementation of coordinative nodding behavior on spoken dialogue systems.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifth International Conference on Spoken Language Processing (ICSLP-98),</booktitle>
<pages>2347--2350</pages>
<contexts>
<context position="5792" citStr="Hirasawa et al., 1998" startWordPosition="891" endWordPosition="894">ow they interact with each other. 3.1 Speech Recognition The speech recognition module is a phonemeHMM-based speaker-independent continuous speech recognizer that incrementally outputs face Toolkit. word hypotheses. As the recognition engine, either VoiceRex, developed by NTT (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval. This enables the language generation module to react immediately to user interruptions while the system is speaking. The language model for speech recognition is a network (regular) grammar,</context>
</contexts>
<marker>Hirasawa, Miyazaki, Nakano, Kawabata, 1998</marker>
<rawString>Jun-ichi Hirasawa, Noboru Miyazaki, Mikio Nakano, and Takeshi Kawabata. 1998. Implementation of coordinative nodding behavior on spoken dialogue systems. In Proceedings of the Fifth International Conference on Spoken Language Processing (ICSLP-98), pages 2347-2350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsunori Kobayashi</author>
<author>Shuichi Itahashi</author>
<author>Satoru Hayamizu</author>
<author>Toshiyuld Takezawa</author>
</authors>
<title>Asj continuous speech corpus for research.</title>
<date>1992</date>
<journal>The journal of the Acoustical Society of Japan,</journal>
<pages>48--12</pages>
<contexts>
<context position="5641" citStr="Kobayashi et al., 1992" startWordPosition="867" endWordPosition="870">d by WIT. 3 Architecture of WIT-Based Spoken Dialogue Systems Here we explain how the modules in WIT work by exploiting domain-dependent knowledge and how they interact with each other. 3.1 Speech Recognition The speech recognition module is a phonemeHMM-based speaker-independent continuous speech recognizer that incrementally outputs face Toolkit. word hypotheses. As the recognition engine, either VoiceRex, developed by NTT (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval. This enables the language generation mo</context>
</contexts>
<marker>Kobayashi, Itahashi, Hayamizu, Takezawa, 1992</marker>
<rawString>Tetsunori Kobayashi, Shuichi Itahashi, Satoru Hayamizu, and Toshiyuld Takezawa. 1992. Asj continuous speech corpus for research. The journal of the Acoustical Society of Japan, 48(12):888-893.</rawString>
</citation>
<citation valid="true">
<title>Mikio Nakano and Akira Shimazu.</title>
<date>1999</date>
<booktitle>Machine Conversations,</booktitle>
<pages>99--112</pages>
<editor>In Yorick Wilks, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>1999</marker>
<rawString>Mikio Nakano and Akira Shimazu. 1999. Parsing utterances including self-repairs. In Yorick Wilks, editor, Machine Conversations, pages 99-112. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Nakano</author>
<author>Akira Shimazu</author>
<author>Kiyoshi Kogure</author>
</authors>
<title>A grammar and a parser for spontaneous speech.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING-94),</booktitle>
<pages>1014--1020</pages>
<contexts>
<context position="24653" citStr="Nakano et al., 1994" startWordPosition="3940" endWordPosition="3943">iptive power to build a variety of dialogue systems. Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information. For example, it is possible to represent a discourse stack whose depth is limited. Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar (Nakano et al., 1994; Nakano and Shimazu, 1999). The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shimazu, 1997). It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented by dialogue phases in WIT. 6.2 Consistency In an agglutinative language such as Japanese, there is no established definition of words, so dialogue system developers must define words. This sometimes causes a problem in t</context>
</contexts>
<marker>Nakano, Shimazu, Kogure, 1994</marker>
<rawString>Mikio Nakano, Akira Shimazu, and Kiyoshi Kogure. 1994. A grammar and a parser for spontaneous speech. In Proceedings of the 15th International Conference on Computational Linguistics (COLING-94), pages 1014-1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Nakano</author>
<author>Kohji Dohsaka</author>
<author>Noboru Miyazaki</author>
</authors>
<title>ichi Hirasawa, Masafumi Tamoto, Masahito Kawamori, Akira Sugiyama, and Takeshi Kawabata. 1999a. Handling rich turn-taking in spoken dialogue systems.</title>
<date></date>
<booktitle>In Proceedings of the Sixth European Conference on Speech Communication and Technology (Eurospeech-99),</booktitle>
<pages>1167--1170</pages>
<marker>Nakano, Dohsaka, Miyazaki, </marker>
<rawString>Mikio Nakano, Kohji Dohsaka, Noboru Miyazaki, Jun ichi Hirasawa, Masafumi Tamoto, Masahito Kawamori, Akira Sugiyama, and Takeshi Kawabata. 1999a. Handling rich turn-taking in spoken dialogue systems. In Proceedings of the Sixth European Conference on Speech Communication and Technology (Eurospeech-99), pages 1167-1170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Nakano</author>
<author>Noboru Miyazalci</author>
<author>Jun-ichi Hirasawa</author>
<author>Kohji Dohsaka</author>
<author>Takeshi Kawabata</author>
</authors>
<title>Understanding unsegmented user utterances in realtime spoken dialogue systems.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99),</booktitle>
<pages>200--207</pages>
<contexts>
<context position="3714" citStr="Nakano et al., 1999" startWordPosition="583" endWordPosition="586">ain-dependent knowledge sources. Creating and maintaining these knowledge sources require much effort, thus a toolkit would be helpful. Previous toolkits, however, do not allow us to achieve these features, or do not provide mechanisms that achieve these features without requiring excessive efforts by the developers. This paper presents WIT&apos;, which is a toolkit &apos;WIT is an acronym of Workable spoken dialogue Inter150 for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output. WIT features an incremental understanding method (Nakano et al., 1999b) that makes it possible to build a robust and real-time system. In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier. Although WIT requires more domaindependent specifications than finite-state-modelbased toolkits, WIT-based systems are capable of taking full advantage of language processing technology WIT has been implemented and used to build several spoken dialogue systems. In what follows, we overview WIT, explain its architecture, domain-dependent system specifications, and implementation, and then discuss it</context>
<context position="8985" citStr="Nakano et al., 1999" startWordPosition="1382" endWordPosition="1385">ialogue. Although the current version of WIT does not exploit probabilistic language models, such models can be incorporated without changing the basic WIT architecture. 3.2 Language Understanding The language understanding module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the result of understanding and discourse information are represented by a frame (i.e., attribute-value pairs). The understanding module utilizes ISSS (Incremental Significant-utterance Sequence Search) (Nakano et al., 1999b), which is an integrated parsing and discourse processing method. ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to parsing by incrementally finding the most plausible sequence of sentences (or signcant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence. ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time. The domain-dependent knowledge used in this module consists of a unificat</context>
<context position="13664" citStr="Nakano et al., 1999" startWordPosition="2141" endWordPosition="2144">time is executed in the former case; the time-out function is executed in the latter case. Then it goes back to the initial stage. If the system holds the initiative, the module executes the initial function of the phase. In typical question-answer systems, the user has the initiative when asking questions and the system has it when answering. Since the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking (Nakano et al., 1999a). Thus the system can respond immediately after user pauses when the user has the initiative. When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997). The time-out function is effective in moving the dialogue forward when the dialogue gets stuck for some reason. For example, the system may be able to repeat the same question with another expression and may also be able to ask the user a more specific question. 3.4 Speech Output The speech output module produces speec</context>
<context position="18310" citStr="Nakano et al., 1999" startWordPosition="2883" endWordPosition="2886"> (priority increase) ) ( (rule name) (child feature structure) . . . (child feature structure) =&gt; (frame operation command) (priority increase) ) These rules are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied. It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano, 1991), but we do not explain these constraints in detail in this 154 paper. The priorities are used for disambiguating interpretation in the incremental understanding method (Nakano et al., 1999b). When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence&apos;s semantic representation is the command for updating the dialogue state. The command is one of the following: • A command to set the value of an attribute of the frame, • A command to increase the priority, • Conditional commands (If-then-else type command, the condition being whether the value of an attribute of the frame is or is not equal to a specified value, or a conjunction or disjunction of the above condition), </context>
<context position="22868" citStr="Nakano et al., 1999" startWordPosition="3644" endWordPosition="3647"> explained above, domaindependent knowledge sources are created as indicated by the dashed arrows in Figure 1. When creating the knowledge sources, WIT checks for several kinds of consistency. For example, the set of word categories appearing in the lexicon and the set of word categories appearing in phrase definitions are compared. This makes it easy to find errors in the domain specifications. $ Implementation WIT has been implemented in Common Lisp and C on UNIX, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system (Nakano et al., 1999b), a video-recording programming system, a schedule management system (Nakano et al., 1999a), and a weather infomiation system (Dohsaka et al., 2000). The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2. This system employs H&apos;TK as the speech recognition engine. The weather information system can answer the user&apos;s questions about weather forecasts in Japan. The vocabulary size is around 500, and the</context>
</contexts>
<marker>Nakano, Miyazalci, Hirasawa, Dohsaka, Kawabata, 1999</marker>
<rawString>Mikio Nakano, Noboru Miyazalci, Jun-ichi Hirasawa, Kohji Dohsaka, and Takeshi Kawabata. 1999b. Understanding unsegmented user utterances in realtime spoken dialogue systems. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99), pages 200-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Nakano</author>
</authors>
<title>Constraint projection: An efficient treatment of disjunctive feature descriptions.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (ACL91),</booktitle>
<pages>307--314</pages>
<contexts>
<context position="9760" citStr="Nakano, 1991" startWordPosition="1500" endWordPosition="1501">ences prior to parsing by incrementally finding the most plausible sequence of sentences (or signcant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence. ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time. The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules. Disjunctive feature descriptions are also possible; WIT incorporates an efficient method for handling disjunctions (Nakano, 1991). When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase. The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences. Two kinds of sentences can be considered; domain-related ones that express the user&apos;s intention about the reser152 vation and dialogue-related ones that express the user&apos;s attitude with respect to the progress of the dialogue, such as confirmation an</context>
<context position="18121" citStr="Nakano, 1991" startWordPosition="2854" endWordPosition="2855">lated attributes. Rule Definitions: Each rule has one of the following two forms. ( (rule name) (child feature structure) . . . (child feature structure) =&gt; (mother feature structure) (priority increase) ) ( (rule name) (child feature structure) . . . (child feature structure) =&gt; (frame operation command) (priority increase) ) These rules are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied. It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano, 1991), but we do not explain these constraints in detail in this 154 paper. The priorities are used for disambiguating interpretation in the incremental understanding method (Nakano et al., 1999b). When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence&apos;s semantic representation is the command for updating the dialogue state. The command is one of the following: • A command to set the value of an attribute of the frame, • A command to increase the priority, • Conditional commands (If-</context>
</contexts>
<marker>Nakano, 1991</marker>
<rawString>Mikio Nakano. 1991. Constraint projection: An efficient treatment of disjunctive feature descriptions. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (ACL91), pages 307-314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshiaki Noda</author>
<author>Yoshilcazu Yamaguchi</author>
</authors>
<title>Tomokazu Yamada, Alcihiro Ixnamura, Satoshi Takahashi, Tomoko Matsui, and Kiyoald Ailcawa.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 lEICE General Conference</booktitle>
<pages>14--9</pages>
<note>(in Japanese).</note>
<marker>Noda, Yamaguchi, 1998</marker>
<rawString>Yoshiaki Noda, Yoshilcazu Yamaguchi, Tomokazu Yamada, Alcihiro Ixnamura, Satoshi Takahashi, Tomoko Matsui, and Kiyoald Ailcawa. 1998. The development of speech recognition engine REX. In Proceedings of the 1998 lEICE General Conference D-14-9, page 220. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>David H D Warren</author>
</authors>
<title>Definite clause grammars for language analysis-a survey of the formalism and a comparison with augmented transition networks.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<pages>13--231</pages>
<contexts>
<context position="17894" citStr="Pereira and Warren, 1980" startWordPosition="2814" endWordPosition="2818"> times of the reservation for some meeting room. It also has attributes that represent discourse information. One is confirmed, whose value indicates whether if the system has already made an utterance to confirm the content of the task-related attributes. Rule Definitions: Each rule has one of the following two forms. ( (rule name) (child feature structure) . . . (child feature structure) =&gt; (mother feature structure) (priority increase) ) ( (rule name) (child feature structure) . . . (child feature structure) =&gt; (frame operation command) (priority increase) ) These rules are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied. It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano, 1991), but we do not explain these constraints in detail in this 154 paper. The priorities are used for disambiguating interpretation in the incremental understanding method (Nakano et al., 1999b). When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence&apos;s semant</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Fernando C. N. Pereira and David H. D. Warren. 1980. Definite clause grammars for language analysis-a survey of the formalism and a comparison with augmented transition networks. Artificial Intelligence, 13:231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl J Pollard</author>
<author>Ivan A Sag</author>
</authors>
<date>1994</date>
<booktitle>Head-Driven Phrase Structure Grammar. CSLI,</booktitle>
<location>Stanford.</location>
<contexts>
<context position="15107" citStr="Pollard and Sag, 1994" startWordPosition="2373" endWordPosition="2376">ed so that the language generation module can take into account the timing of the end of system utterance. The meeting room reservation system uses speech files of short 153 phrases. 4 Building Spoken Dialogue Systems with WIT 4.1 Domain-Dependent System Specifications Spoken dialogue systems can be built with WIT by preparing several domain-dependent specifications. Below we explain the specifications. Feature Definitions: Feature definitions specify the set of features used in the grammar for language understanding. They also specify whether each feature is a head feature or a foot feature (Pollard and Sag, 1994). This information is used when constructing feature structures for phrases in a built-in process. The following is an example of a feature definition. Here we use examples from the specification of the meeting room reservation system. (case head) It means that the case feature is used and it is a head feature3. Lexical Descriptions: Lexical descriptions specify both pronunciations and grammatical features for words. Below is an example lexical item for the word 1-gatsu (January). (1-gatsu ichigatsu month nil 1) The first three elements are the identifier, the pronunciation, and the grammatica</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. CSLI, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Munehiko Sasajima</author>
<author>Yakehide Yano</author>
<author>Yasuyuld Kono</author>
</authors>
<title>EUROPA: A generic framework for developing spoken dialogue systems.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixth European Conference on Speech Communication and Technology (Eurospeech-99),</booktitle>
<pages>1163--1166</pages>
<contexts>
<context position="1172" citStr="Sasajima et al., 1999" startWordPosition="171" endWordPosition="174">fications into internal knowledge sources makes building spoken dialogue systems much easier than it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al., 1996; Zue et al., 2000; Walker et al., 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spoken dialogue systems have been developed (Barnett and Singh, 1997; Sasajima et al., 1999). One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model. It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain. However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics. Another is GALAXY-II (Seneff et al., 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enables modules in a dialogue </context>
</contexts>
<marker>Sasajima, Yano, Kono, 1999</marker>
<rawString>Munehiko Sasajima, Yakehide Yano, and Yasuyuld Kono. 1999. EUROPA: A generic framework for developing spoken dialogue systems. In Proceedings of the Sixth European Conference on Speech Communication and Technology (Eurospeech-99), pages 1163-1166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Seneff</author>
<author>Ed Hurley</author>
<author>Raymond Lau</author>
<author>Christine Pao</author>
<author>Philipp Schmid</author>
<author>Victor Zue</author>
</authors>
<title>GALAXY-II: A reference architecture for conversational system development.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifth International Conference on Spoken Language Processing (ICSLP-98).</booktitle>
<contexts>
<context position="1646" citStr="Seneff et al., 1998" startWordPosition="250" endWordPosition="253">sk domain. To this end, several toolkits for building spoken dialogue systems have been developed (Barnett and Singh, 1997; Sasajima et al., 1999). One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model. It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain. However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics. Another is GALAXY-II (Seneff et al., 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enables modules in a dialogue system to communicate with each other. It consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers. Although it requires more specifications than finitestate-model-based toolkits, it places less limitations on system functions. Our objective is to build robust and real-time spoken dialogue systems in different task domains. By robust we mean utterance understanding is robu</context>
</contexts>
<marker>Seneff, Hurley, Lau, Pao, Schmid, Zue, 1998</marker>
<rawString>Stephanie Seneff, Ed Hurley, Raymond Lau, Christine Pao, Philipp Schmid, and Victor Zue. 1998. GALAXY-II: A reference architecture for conversational system development. In Proceedings of the Fifth International Conference on Spoken Language Processing (ICSLP-98).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stephen Sutton</author>
<author>Ronald A Cole</author>
<author>Jacques de Villiers</author>
<author>Johan Schallcwyk</author>
<author>Pieter Vermeulen</author>
<author>Michael W Macon</author>
<author>Yonghong Yan</author>
<author>Edward Kaiser</author>
<author>Brian Rundle</author>
<author>Khaldoun Shobald</author>
<author>Paul Hosom</author>
<author>Alex Kain</author>
<author>Johan Wouters</author>
<author>Dominic W Massaro</author>
<author>Michael Cohen</author>
</authors>
<title>Universal speech tools: The CSLU toolkit.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifth International Conference on Spoken Language Processing (ICSLP-98),</booktitle>
<pages>3221--3224</pages>
<marker>Sutton, Cole, de Villiers, Schallcwyk, Vermeulen, Macon, Yan, Kaiser, Rundle, Shobald, Hosom, Kain, Wouters, Massaro, Cohen, 1998</marker>
<rawString>Stephen Sutton, Ronald A. Cole, Jacques de Villiers, Johan Schallcwyk, Pieter Vermeulen, Michael W. Macon, Yonghong Yan, Edward Kaiser, Brian Rundle, Khaldoun Shobald, Paul Hosom, Alex Kain, Johan Wouters, Dominic W. Massaro, and Michael Cohen. 1998. Universal speech tools: The CSLU toolkit. In Proceedings of the Fifth International Conference on Spoken Language Processing (ICSLP-98), pages 3221-3224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Irene Langkilde</author>
<author>Jerry Wright Allen Gorin</author>
<author>Diane Litman</author>
</authors>
<title>Learning to predict problematic situations in a spoken dialogue system: Experiments with how may I help you?</title>
<date>2000</date>
<booktitle>In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-00),</booktitle>
<pages>210--217</pages>
<contexts>
<context position="896" citStr="Walker et al., 2000" startWordPosition="124" endWordPosition="127">tom.brl.ntt.cojp Abstract This paper describes WIT, a toolkit for building spoken dialogue systems. WIT features an incremental understanding mechanism that enables robust utterance understanding and realtime responses. WIT&apos;s ability to compile domain-dependent system specifications into internal knowledge sources makes building spoken dialogue systems much easier than it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al., 1996; Zue et al., 2000; Walker et al., 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spoken dialogue systems have been developed (Barnett and Singh, 1997; Sasajima et al., 1999). One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model. It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain. However, it limits system functions; it is not easy</context>
</contexts>
<marker>Walker, Langkilde, Gorin, Litman, 2000</marker>
<rawString>Marilyn Walker, Irene Langkilde, Jerry Wright Allen Gorin, and Diane Litman. 2000. Learning to predict problematic situations in a spoken dialogue system: Experiments with how may I help you? In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-00), pages 210-217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Zue</author>
<author>Stephanie Seneff</author>
<author>James Glass</author>
<author>Joseph Polifroni</author>
<author>Christine Pao</author>
<author>Timothy J Hazen</author>
<author>Lee Hetherington</author>
</authors>
<title>Jupiter: A telephone-based conversational interface for weather information.</title>
<date>2000</date>
<booktitle>IEEE Transactions on Speech and Audio Processing,</booktitle>
<pages>8--1</pages>
<contexts>
<context position="874" citStr="Zue et al., 2000" startWordPosition="120" endWordPosition="123">n E-mail: nakano@atom.brl.ntt.cojp Abstract This paper describes WIT, a toolkit for building spoken dialogue systems. WIT features an incremental understanding mechanism that enables robust utterance understanding and realtime responses. WIT&apos;s ability to compile domain-dependent system specifications into internal knowledge sources makes building spoken dialogue systems much easier than it is from scratch. 1 Introduction The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (Aust et al., 1995; Allen et al., 1996; Zue et al., 2000; Walker et al., 2000). One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain. To this end, several toolkits for building spoken dialogue systems have been developed (Barnett and Singh, 1997; Sasajima et al., 1999). One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model. It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain. However, it limits system fun</context>
</contexts>
<marker>Zue, Seneff, Glass, Polifroni, Pao, Hazen, Hetherington, 2000</marker>
<rawString>Victor Zue, Stephanie Seneff, James Glass, Joseph Polifroni, Christine Pao, Timothy J. Hazen, and Lee Hetherington. 2000. Jupiter: A telephone-based conversational interface for weather information. IEEE Transactions on Speech and Audio Processing, 8(1):85-96.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>