<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000116">
<title confidence="0.95562">
Smatch: an Evaluation Metric for Semantic Feature Structures
</title>
<author confidence="0.980567">
Shu Cai
</author>
<affiliation confidence="0.7996985">
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.767387">
Marina del Rey, CA 90292
</address>
<email confidence="0.998688">
shucai@isi.edu
</email>
<author confidence="0.986466">
Kevin Knight
</author>
<affiliation confidence="0.803677">
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.767828">
Marina del Rey, CA 90292
</address>
<email confidence="0.999129">
knight@isi.edu
</email>
<sectionHeader confidence="0.993904" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999815083333333">
The evaluation of whole-sentence seman-
tic structures plays an important role in
semantic parsing and large-scale seman-
tic structure annotation. However, there is
no widely-used metric to evaluate whole-
sentence semantic structures. In this pa-
per, we present smatch, a metric that cal-
culates the degree of overlap between two
semantic feature structures. We give an
efficient algorithm to compute the metric
and show the results of an inter-annotator
agreement study.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999101897435898">
The goal of semantic parsing is to generate all se-
mantic relationships in a text. Its output is of-
ten represented by whole-sentence semantic struc-
tures. Evaluating such structures is necessary for
semantic parsing tasks, as well as semantic anno-
tation tasks which create linguistic resources for
semantic parsing.
However, there is no widely-used evalua-
tion method for whole-sentence semantic struc-
tures. Current whole-sentence semantic parsing
is mainly evaluated in two ways: 1. task cor-
rectness (Tang and Mooney, 2001), which eval-
uates on an NLP task that uses the parsing re-
sults; 2. whole-sentence accuracy (Zettlemoyer
and Collins, 2005), which counts the number of
sentences parsed completely correctly.
Nevertheless, it is worthwhile to explore evalua-
tion methods that use scores which range from 0 to
1 (“partial credit”) to measure whole-sentence se-
mantic structures. By using such methods, we are
able to differentiate between two similar whole-
sentence semantic structures regardless of specific
tasks or domains. In this work, we provide an eval-
uation metric that uses the degree of overlap be-
tween two whole-sentence semantic structures as
the partial credit.
In this paper, we observe that the difficulty
of computing the degree of overlap between two
whole-sentence semantic feature structures comes
from determining an optimal variable alignment
between them, and further prove that finding such
alignment is NP-complete. We investigate how to
compute this metric and provide several practical
and replicable computing methods by using Inte-
ger Linear Programming (ILP) and hill-climbing
method. We show that our metric can be used
for measuring the annotator agreement in large-
scale linguistic annotation, and evaluating seman-
tic parsing.
</bodyText>
<sectionHeader confidence="0.96165" genericHeader="method">
2 Semantic Overlap
</sectionHeader>
<bodyText confidence="0.999950333333333">
We work on a semantic feature structure represen-
tation in a standard neo-Davidsonian (Davidson,
1969; Parsons, 1990) framework. For example,
semantics of the sentence “the boy wants to go” is
represented by the following directed graph:
In this graph, there are three concepts: want-
01, boy, and go-01. Both want-01 and go-01 are
frames from PropBank framesets (Kingsbury and
Palmer, 2002). The frame want-01 has two argu-
ments connected with ARG0 and ARG1, and go-
01 has an argument (which is also the same boy
instance) connected with ARG0.
</bodyText>
<page confidence="0.956287">
748
</page>
<bodyText confidence="0.842863888888889">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 748–752,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
Following (Langkilde and Knight, 1998) and
(Langkilde-Geary, 2002), we refer to this semantic
representation as AMR (Abstract Meaning Repre-
sentation).
Semantic relationships encoded in the AMR
graph can also be viewed as a conjunction of logi-
cal propositions, or triples:
</bodyText>
<equation confidence="0.992221333333334">
instance(a, want-01) ∧
instance(b, boy) ∧
instance(c, go-01) ∧
ARG0(a, b) ∧
ARG1(a, c) ∧
ARG0(c, b)
</equation>
<bodyText confidence="0.999312">
Each AMR triple takes one of these forms:
relation(variable, concept) (the first three triples
above), or relation(variable1, variable2) (the last
three triples above).
Suppose we take a second AMR (for “the boy
wants the football”) and its associated proposi-
tional triples:
</bodyText>
<equation confidence="0.9967578">
instance(x, want-01) ∧
instance(y, boy) ∧
instance(z, football) ∧
ARG0(x, y) ∧
ARG1(x, z)
</equation>
<bodyText confidence="0.974452029411765">
Our evaluation metric measures precision, re-
call, and f-score of the triples in the second AMR
against the triples in the first AMR, i.e., the
amount of propositional overlap.
The difficulty is that variable names are not
shared between the two AMRs, so there are mul-
tiple ways to compute the propositional overlap
based on different variable mappings. We there-
fore define the smatch score (for semantic match)
as the maximum f-score obtainable via a one-to-
one matching of variables between the two AMRs.
In the example above, there are six ways to
match up variables between the two AMRs:
M P R F
x=a, y=b, z=c: 4 4/5 4/6 0.73
x=a, y=c, z=b: 1 1/5 1/6 0.18
x=b, y=a, z=c: 0 0/5 0/6 0.00
x=b, y=c, z=a: 0 0/5 0/6 0.00
x=c, y=a, z=b: 0 0/5 0/6 0.00
x=c, y=b, z=a: 2 2/5 2/6 0.36
smatch score: 0.73
Here, M is the number of propositional triples that
agree given a variable mapping, P is the precision
of the second AMR against the first, R is its re-
call, and F is its f-score. The smatch score is the
maximum of the f-scores.
However, for AMRs that contain large number
of variables, it is not efficient to get the f-score by
simply using the method above. Exhaustively enu-
merating all variable mappings requires comput-
ing the f-score for n!/(n − m)! variable mappings
(assuming one AMR has n variables and the other
has m variables, and m &lt; n). This algorithm is
too slow for all but the shortest AMR pairs.
</bodyText>
<sectionHeader confidence="0.922409" genericHeader="method">
3 Computing the Metric
</sectionHeader>
<bodyText confidence="0.9980626">
This section describes how to compute the smatch
score. As input, we are given AMR1 (with m vari-
ables) and AMR2 (with n variables). Without loss
of generality, m &lt; n.
Baseline. Our baseline first matches variables
that share concepts. For example, it would match
a in the first AMR example with x in the second
AMR example of Section 2, because both are in-
stances of want-01. If there are two or more vari-
ables to choose from, we pick the first available
one. The rest of the variables are mapped ran-
domly.
ILP method. We can get an optimal solution
using integer linear programming (ILP). We create
two types of variables:
</bodyText>
<listItem confidence="0.9948352">
• (Variable mapping) vij = 1 iff the ith vari-
able in AMR1 is mapped to the jth variable
in AMR2 (otherwise vij = 0)
• (Triple match) tkl = 1 iff AMR1 triple
k matches AMR2 triple l, otherwise tkl
</listItem>
<bodyText confidence="0.823004857142857">
= 0. A triple relation1(xy) matches
relation2(wz) iff relation1 = relation2, vxw
= 1, and vyz = 1 or y and z are the same con-
cept.
Our constraints ensure a one-to-one mapping of
variables, and they ensure that the chosen t values
are consistent with the chosen v values:
</bodyText>
<equation confidence="0.794037">
For all i, � vij &lt; 1
j
�For all j, vij &lt; 1
i
</equation>
<bodyText confidence="0.705427">
For all triple pairs r(xy)r(wz) (r for relation),
</bodyText>
<equation confidence="0.757418666666667">
tr(xy)r(wz) &lt; vxw
749
tr(xy)r(wz) &lt; vyz
</equation>
<bodyText confidence="0.923283611111111">
when y and z are variables.
Finally, we ask the ILP solver to maximize:
� tkl
kl
which denotes the maximum number of matching
triples which lead to the smatch score.
Hill-climbing method. Finally, we develop a
portable heuristic algorithm that does not require
an ILP solver1. This method works in a greedy
style. We begin with m random one-to-one map-
pings between the m variables of AMR1 and the
n variables of AMR2. Each variable mapping is
a pair (i, map(i)) with 1 &lt; i &lt; m and 1 &lt;
map(i) &lt; n. We refer to the m mappings as a
variable mapping state.
We first generate a random initial variable map-
ping state, compute its triple match number, then
hill-climb via two types of small changes:
</bodyText>
<listItem confidence="0.996762333333333">
1. Move one of the m mappings to a currently-
unmapped variable from the n.
2. Swap two of the m mappings.
</listItem>
<bodyText confidence="0.994140809523809">
Any variable mapping state has m(n − m) +
m(m − 1) = m(n − 1) neighbors during the
hill-climbing search. We greedily choose the best
neighbor, repeating until no neighbor improves the
number of triple matches.
We experiment with two modifications to the
greedy search: (1) executing multiple random
restarts to avoid local optima, and (2) using our
Baseline concept matching (“smart initialization”)
instead of random initialization.
NP-completeness. There is unlikely to be
an exact polynomial-time algorithm for comput-
ing smatch. We can reduce the 0-1 Maximum
Quadratic Assignment Problem (0-1-Max-QAP)
(Nagarajan and Sviridenko, 2009) and the sub-
graph isomorphism problem directly to the full
smatch problem on graphs.2
We note that other widely-used metrics, such as
TER (Snover et al., 2006), are also NP-complete.
Fortunately, the next section shows that the smatch
methods above are efficient and effective.
</bodyText>
<footnote confidence="0.9956175">
1The tool can be downloaded at
http://amr.isi.edu/evaluation.html.
2Thanks to David Chiang for observing the subgraph iso-
morphism reduction.
</footnote>
<sectionHeader confidence="0.868981" genericHeader="method">
4 Using Smatch
</sectionHeader>
<bodyText confidence="0.853275545454545">
We report an AMR inter-annotator agreement
study using smatch.
1. Our study has 4 annotators (A, B, C, D), who
then converge on a consensus annotation E.
We thus have 10 pairs of annotations: A-B,
A-C, ... , D-E.
2. The study is carried out 5 times. Each
time annotators build AMRs for 4 sentences
from the Wall Street Journal corpus. Sen-
tence lengths range from 12 to 54 words, and
AMRs range from 6 to 29 variables.
</bodyText>
<listItem confidence="0.990249785714286">
3. We use 7 smatch calculation methods in our
experiments:
• Base: Baseline matching method
• ILP: Integer Linear Programming
• R: Hill-climbing with random initializa-
tion
• 10R: Hill-climbing with random initial-
ization plus 9 random restarts
• S: Hill-climbing with smart initializa-
tion
• S+4R: Hill-climbing with smart initial-
ization plus 4 random restarts
• S+9R: Hill-climbing with smart initial-
ization plus 9 random restarts
</listItem>
<bodyText confidence="0.968351944444444">
Table 1 shows smatch scores provided by the
methods. Columns labeled 1-5 indicate sen-
tence groups. Each individual smatch score is
a document-level score of 4 AMR pairs.3 ILP
scores are optimal, so lower scores (in bold) in-
dicate search errors.
Table 2 summarizes search accuracy as a per-
centage of smatch scores that equal that of ILP.
Results show that the restarts are essential for hill-
climbing, and that 9 restarts are sufficient to obtain
good quality. The table also shows total runtimes
over 200 AMR pairs (10 annotator pairs, 5 sen-
tence groups, 4 AMR pairs per group). Heuris-
tic search with smart initialization and 4 restarts
(S+4R) gives the best trade-off between accuracy
and speed, so this is the setting we use in practice.
Figure 1 shows smatch scores of each annotator
(A-D) against the consensus annotation (E). The
</bodyText>
<footnote confidence="0.99475675">
3For documents containing multiple AMRs, we use the
sum of matched triples over all AMR pairs to compute pre-
cision, recall, and f-score, much like corpus-level Bleu (Pap-
ineni et al., 2002).
</footnote>
<page confidence="0.982451">
750
</page>
<table confidence="0.947056181818182">
1 2 B 4 5 1 2 C 4 5 1 2 D 4 5 1 2 E 4 5
3 3 3 3
Base 0.68 0.74 0.84 0.71 0.83 0.69 0.70 0.80 0.69 0.78 0.77 0.72 0.75 0.68 0.63 0.79 0.86 0.92 0.85 0.89
ILP 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92
R 0.74 0.79 0.84 0.75 0.86 0.74 0.75 0.80 0.77 0.88 0.83 0.76 0.75 0.72 0.75 0.85 0.92 0.92 0.89 0.89
A 10R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92
S 0.74 0.80 0.84 0.75 0.88 0.75 0.78 0.80 0.76 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92
S+4R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92
S+9R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92
Base - - - - - 0.72 0.68 0.74 0.69 0.79 0.71 0.72 0.76 0.65 0.57 0.68 0.71 0.83 0.79 0.86
ILP - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89
</table>
<equation confidence="0.971622315789474">
R - - - - - 0.74 0.83 0.72 0.72 0.83 0.78 0.83 0.76 0.68 0.68 0.74 0.81 0.83 0.83 0.89
B 10R - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89
S - - - - - 0.73 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89
S+4R - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89
S+9R - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89
Base - - - - - - - - - - 0.68 0.68 0.74 0.69 0.65 0.64 0.64 0.87 0.79 0.83
ILP - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89
R - - - - - - - - - - 0.74 0.79 0.74 0.75 0.78 0.71 0.76 0.87 0.85 0.89
C 10R - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89
S - - - - - - - - - - 0.74 0.79 0.74 0.77 0.81 0.74 0.76 0.87 0.85 0.89
S+4R - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89
S+9R - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89
Base - - - - - - - - - - - - - - - 0.68 0.69 0.81 0.74 0.64
ILP - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79
R - - - - - - - - - - - - - - - 0.77 0.73 0.81 0.78 0.79
D 10R - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79
S - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79
S+4R - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79
S+9R - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79
</equation>
<tableCaption confidence="0.973358">
Table 1: Inter-annotator smatch agreement for 5 groups of sentences, as computed with seven different
methods (Base, ILP, R, 10R, S, S+4R, S+9R). The number 1-5 indicate the sentence group number. Bold
scores are search errors.
</tableCaption>
<table confidence="0.999923666666667">
Base ILP R 10R S S+4R S+9R
Accuracy 20% 100% 66.5% 100% 92% 100% 100%
Time (sec) 0.86 49.67 5.85 64.78 2.31 28.36 59.69
</table>
<tableCaption confidence="0.70675">
Table 2: Accuracy and running time (seconds) of
various computing methods of smatch over 200
AMR pairs.
</tableCaption>
<bodyText confidence="0.997701083333333">
plot demonstrates that, as time goes by, annotators
reach better agreement with the consensus.
We also note that smatch is used to measure
the accuracy of machine-generated AMRs. (Jones
et al., 2012) use it to evaluate automatic seman-
tic parsing in a narrow domain, while Ulf Her-
mjakob4 has developed a heuristic algorithm that
exploits and supplements Ontonotes annotations
(Pradhan et al., 2007) in order to automatically
create AMRs for Ontonotes sentences, with a
smatch score of 0.74 against human consensus
AMRs.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999926571428571">
Related work on directly measuring the seman-
tic representation includes the method in (Dri-
dan and Oepen, 2011), which evaluates semantic
parser output directly by comparing semantic sub-
structures, though they require an alignment be-
tween sentence spans and semantic sub-structures.
In contrast, our metric does not require the align-
</bodyText>
<footnote confidence="0.911232">
4personal communication
</footnote>
<figureCaption confidence="0.9837915">
Figure 1: Smatch scores of annotators (A-D)
against the consensus annotation (E) over time.
</figureCaption>
<bodyText confidence="0.999731">
ment between an input sentence and its semantic
analysis. (Allen et al., 2008) propose a metric
which computes the maximum score by any align-
ment between LF graphs, but they do not address
how to determine the alignments.
</bodyText>
<sectionHeader confidence="0.998674" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999498142857143">
We present an evaluation metric for whole-
sentence semantic analysis, and show that it can
be computed efficiently. We use the metric to
measure semantic annotation agreement rates and
parsing accuracy. In the future, we plan to investi-
gate how to adapt smatch to other semantic repre-
sentations.
</bodyText>
<figure confidence="0.997040538461538">
1 2 3 4 5
Time
Smatch scores of each annotator against consens 1
0.95
0.9
0.85
0.8
0.75
0.7
Annotator A
Annotator B
Annotator C
Annotator D
</figure>
<page confidence="0.990591">
751
</page>
<sectionHeader confidence="0.997436" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9977945">
We would like to thank David Chiang, Hui Zhang,
other ISI colleagues and our anonymous review-
ers for their thoughtful comments. This work was
supported by NSF grant IIS-0908532.
</bodyText>
<sectionHeader confidence="0.998531" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999921452830188">
J.F. Allen, M. Swift, and W. Beaumont. 2008. Deep
Semantic Analysis of Text. In Proceedings of the
2008 Conference on Semantics in Text Processing.
D. Davidson. 1969. The Individuation of Events. In
Nicholas Rescher (ed.) Essays in Honor of Carl G.
HempeL Dordrecht: D. Reidel.
R. Dridan and S. Oepen. 2011. Parser Evaluation us-
ing Elementary Dependency Matching. In Proceed-
ings of the 12th International Conference on Parsing
Technologies.
B. Jones, J. Andreas, D. Bauer, K. M. Hermann, and
K. Knight. 2012. Semantics-Based Machine Trans-
lation with Hyperedge Replacement Grammars. In
Proceedings of COLING.
P. Kingsbury and M. Palmer. 2002. From Treebank to
Propbank. In Proceedings of LREC.
I. Langkilde and K. Knight. 1998. Generation that Ex-
ploits Corpus-based Statistical Knowledge. In Pro-
ceedings of COLING-ACL.
I. Langkilde-Geary. 2002. An Empirical Verifica-
tion of Coverage and Correctness for a General-
Purpose Sentence Generator. In Proceedings of In-
ternational Natural Language Generation Confer-
ence (INLG’02).
V. Nagarajan and M. Sviridenko. 2009. On the Maxi-
mum Quadratic Assignment Problem. Mathematics
of Operations Research, 34.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics.
T. Parsons. 1990. Events in the Semantics of English.
The MIT Press.
S. S. Pradhan, E. Hovy, M. Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A Unified Relational Semantic Representation. In
Proceedings of the International Conference on Se-
mantic Computing (ICSC ’07).
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings
of the 7th Conference of the Association for Machine
Translation in the Americas (AMTA-2006).
L. R. Tang and R. J. Mooney. 2001. Using Multiple
Clause Constructors in Inductive Logic Program-
ming for Semantic Parsing. In Proceedings of the
12th European Conference on Machine Learning.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
Map Sentences to Logical Form: Structured Classi-
fication with Probabilistic Categorial Grammars. In
Proceedings of the 21st Conference in Uncertainty
in Artificial Intelligence.
</reference>
<page confidence="0.997829">
752
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.779572">
<title confidence="0.999213">Smatch: an Evaluation Metric for Semantic Feature Structures</title>
<author confidence="0.975526">Shu</author>
<affiliation confidence="0.998573">USC Information Sciences</affiliation>
<address confidence="0.992431">4676 Admiralty Way, Suite</address>
<author confidence="0.926862">Marina del Rey</author>
<author confidence="0.926862">CA</author>
<email confidence="0.999062">shucai@isi.edu</email>
<author confidence="0.965345">Kevin</author>
<affiliation confidence="0.999576">USC Information Sciences</affiliation>
<address confidence="0.992931">4676 Admiralty Way, Suite</address>
<author confidence="0.920759">Marina del Rey</author>
<author confidence="0.920759">CA</author>
<email confidence="0.99922">knight@isi.edu</email>
<abstract confidence="0.999015692307692">The evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation. However, there is no widely-used metric to evaluate wholesentence semantic structures. In this pawe present a metric that calculates the degree of overlap between two semantic feature structures. We give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>M Swift</author>
<author>W Beaumont</author>
</authors>
<title>Deep Semantic Analysis of Text.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Semantics in Text Processing.</booktitle>
<contexts>
<context position="14274" citStr="Allen et al., 2008" startWordPosition="2596" endWordPosition="2599">ntonotes sentences, with a smatch score of 0.74 against human consensus AMRs. 5 Related Work Related work on directly measuring the semantic representation includes the method in (Dridan and Oepen, 2011), which evaluates semantic parser output directly by comparing semantic substructures, though they require an alignment between sentence spans and semantic sub-structures. In contrast, our metric does not require the align4personal communication Figure 1: Smatch scores of annotators (A-D) against the consensus annotation (E) over time. ment between an input sentence and its semantic analysis. (Allen et al., 2008) propose a metric which computes the maximum score by any alignment between LF graphs, but they do not address how to determine the alignments. 6 Conclusion and Future Work We present an evaluation metric for wholesentence semantic analysis, and show that it can be computed efficiently. We use the metric to measure semantic annotation agreement rates and parsing accuracy. In the future, we plan to investigate how to adapt smatch to other semantic representations. 1 2 3 4 5 Time Smatch scores of each annotator against consens 1 0.95 0.9 0.85 0.8 0.75 0.7 Annotator A Annotator B Annotator C Anno</context>
</contexts>
<marker>Allen, Swift, Beaumont, 2008</marker>
<rawString>J.F. Allen, M. Swift, and W. Beaumont. 2008. Deep Semantic Analysis of Text. In Proceedings of the 2008 Conference on Semantics in Text Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidson</author>
</authors>
<title>The Individuation of Events.</title>
<date>1969</date>
<booktitle>In Nicholas Rescher (ed.) Essays in Honor of Carl</booktitle>
<editor>G. HempeL Dordrecht: D. Reidel.</editor>
<contexts>
<context position="2658" citStr="Davidson, 1969" startWordPosition="400" endWordPosition="401">n two whole-sentence semantic feature structures comes from determining an optimal variable alignment between them, and further prove that finding such alignment is NP-complete. We investigate how to compute this metric and provide several practical and replicable computing methods by using Integer Linear Programming (ILP) and hill-climbing method. We show that our metric can be used for measuring the annotator agreement in largescale linguistic annotation, and evaluating semantic parsing. 2 Semantic Overlap We work on a semantic feature structure representation in a standard neo-Davidsonian (Davidson, 1969; Parsons, 1990) framework. For example, semantics of the sentence “the boy wants to go” is represented by the following directed graph: In this graph, there are three concepts: want01, boy, and go-01. Both want-01 and go-01 are frames from PropBank framesets (Kingsbury and Palmer, 2002). The frame want-01 has two arguments connected with ARG0 and ARG1, and go01 has an argument (which is also the same boy instance) connected with ARG0. 748 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 748–752, Sofia, Bulgaria, August 4-9 2013. c�2013 Association</context>
</contexts>
<marker>Davidson, 1969</marker>
<rawString>D. Davidson. 1969. The Individuation of Events. In Nicholas Rescher (ed.) Essays in Honor of Carl G. HempeL Dordrecht: D. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dridan</author>
<author>S Oepen</author>
</authors>
<title>Parser Evaluation using Elementary Dependency Matching.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies.</booktitle>
<contexts>
<context position="13858" citStr="Dridan and Oepen, 2011" startWordPosition="2534" endWordPosition="2538">t, as time goes by, annotators reach better agreement with the consensus. We also note that smatch is used to measure the accuracy of machine-generated AMRs. (Jones et al., 2012) use it to evaluate automatic semantic parsing in a narrow domain, while Ulf Hermjakob4 has developed a heuristic algorithm that exploits and supplements Ontonotes annotations (Pradhan et al., 2007) in order to automatically create AMRs for Ontonotes sentences, with a smatch score of 0.74 against human consensus AMRs. 5 Related Work Related work on directly measuring the semantic representation includes the method in (Dridan and Oepen, 2011), which evaluates semantic parser output directly by comparing semantic substructures, though they require an alignment between sentence spans and semantic sub-structures. In contrast, our metric does not require the align4personal communication Figure 1: Smatch scores of annotators (A-D) against the consensus annotation (E) over time. ment between an input sentence and its semantic analysis. (Allen et al., 2008) propose a metric which computes the maximum score by any alignment between LF graphs, but they do not address how to determine the alignments. 6 Conclusion and Future Work We present </context>
</contexts>
<marker>Dridan, Oepen, 2011</marker>
<rawString>R. Dridan and S. Oepen. 2011. Parser Evaluation using Elementary Dependency Matching. In Proceedings of the 12th International Conference on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Jones</author>
<author>J Andreas</author>
<author>D Bauer</author>
<author>K M Hermann</author>
<author>K Knight</author>
</authors>
<title>Semantics-Based Machine Translation with Hyperedge Replacement Grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="13413" citStr="Jones et al., 2012" startWordPosition="2464" endWordPosition="2467">agreement for 5 groups of sentences, as computed with seven different methods (Base, ILP, R, 10R, S, S+4R, S+9R). The number 1-5 indicate the sentence group number. Bold scores are search errors. Base ILP R 10R S S+4R S+9R Accuracy 20% 100% 66.5% 100% 92% 100% 100% Time (sec) 0.86 49.67 5.85 64.78 2.31 28.36 59.69 Table 2: Accuracy and running time (seconds) of various computing methods of smatch over 200 AMR pairs. plot demonstrates that, as time goes by, annotators reach better agreement with the consensus. We also note that smatch is used to measure the accuracy of machine-generated AMRs. (Jones et al., 2012) use it to evaluate automatic semantic parsing in a narrow domain, while Ulf Hermjakob4 has developed a heuristic algorithm that exploits and supplements Ontonotes annotations (Pradhan et al., 2007) in order to automatically create AMRs for Ontonotes sentences, with a smatch score of 0.74 against human consensus AMRs. 5 Related Work Related work on directly measuring the semantic representation includes the method in (Dridan and Oepen, 2011), which evaluates semantic parser output directly by comparing semantic substructures, though they require an alignment between sentence spans and semantic</context>
</contexts>
<marker>Jones, Andreas, Bauer, Hermann, Knight, 2012</marker>
<rawString>B. Jones, J. Andreas, D. Bauer, K. M. Hermann, and K. Knight. 2012. Semantics-Based Machine Translation with Hyperedge Replacement Grammars. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
</authors>
<title>From Treebank to Propbank.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="2946" citStr="Kingsbury and Palmer, 2002" startWordPosition="444" endWordPosition="447">methods by using Integer Linear Programming (ILP) and hill-climbing method. We show that our metric can be used for measuring the annotator agreement in largescale linguistic annotation, and evaluating semantic parsing. 2 Semantic Overlap We work on a semantic feature structure representation in a standard neo-Davidsonian (Davidson, 1969; Parsons, 1990) framework. For example, semantics of the sentence “the boy wants to go” is represented by the following directed graph: In this graph, there are three concepts: want01, boy, and go-01. Both want-01 and go-01 are frames from PropBank framesets (Kingsbury and Palmer, 2002). The frame want-01 has two arguments connected with ARG0 and ARG1, and go01 has an argument (which is also the same boy instance) connected with ARG0. 748 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 748–752, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Following (Langkilde and Knight, 1998) and (Langkilde-Geary, 2002), we refer to this semantic representation as AMR (Abstract Meaning Representation). Semantic relationships encoded in the AMR graph can also be viewed as a conjunction of logical proposition</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>P. Kingsbury and M. Palmer. 2002. From Treebank to Propbank. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that Exploits Corpus-based Statistical Knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="3327" citStr="Langkilde and Knight, 1998" startWordPosition="502" endWordPosition="505">emantics of the sentence “the boy wants to go” is represented by the following directed graph: In this graph, there are three concepts: want01, boy, and go-01. Both want-01 and go-01 are frames from PropBank framesets (Kingsbury and Palmer, 2002). The frame want-01 has two arguments connected with ARG0 and ARG1, and go01 has an argument (which is also the same boy instance) connected with ARG0. 748 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 748–752, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Following (Langkilde and Knight, 1998) and (Langkilde-Geary, 2002), we refer to this semantic representation as AMR (Abstract Meaning Representation). Semantic relationships encoded in the AMR graph can also be viewed as a conjunction of logical propositions, or triples: instance(a, want-01) ∧ instance(b, boy) ∧ instance(c, go-01) ∧ ARG0(a, b) ∧ ARG1(a, c) ∧ ARG0(c, b) Each AMR triple takes one of these forms: relation(variable, concept) (the first three triples above), or relation(variable1, variable2) (the last three triples above). Suppose we take a second AMR (for “the boy wants the football”) and its associated propositional </context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. 1998. Generation that Exploits Corpus-based Statistical Knowledge. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde-Geary</author>
</authors>
<title>An Empirical Verification of Coverage and Correctness for a GeneralPurpose Sentence Generator.</title>
<date>2002</date>
<booktitle>In Proceedings of International Natural Language Generation Conference (INLG’02).</booktitle>
<contexts>
<context position="3355" citStr="Langkilde-Geary, 2002" startWordPosition="507" endWordPosition="508"> wants to go” is represented by the following directed graph: In this graph, there are three concepts: want01, boy, and go-01. Both want-01 and go-01 are frames from PropBank framesets (Kingsbury and Palmer, 2002). The frame want-01 has two arguments connected with ARG0 and ARG1, and go01 has an argument (which is also the same boy instance) connected with ARG0. 748 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 748–752, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Following (Langkilde and Knight, 1998) and (Langkilde-Geary, 2002), we refer to this semantic representation as AMR (Abstract Meaning Representation). Semantic relationships encoded in the AMR graph can also be viewed as a conjunction of logical propositions, or triples: instance(a, want-01) ∧ instance(b, boy) ∧ instance(c, go-01) ∧ ARG0(a, b) ∧ ARG1(a, c) ∧ ARG0(c, b) Each AMR triple takes one of these forms: relation(variable, concept) (the first three triples above), or relation(variable1, variable2) (the last three triples above). Suppose we take a second AMR (for “the boy wants the football”) and its associated propositional triples: instance(x, want-01</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>I. Langkilde-Geary. 2002. An Empirical Verification of Coverage and Correctness for a GeneralPurpose Sentence Generator. In Proceedings of International Natural Language Generation Conference (INLG’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Nagarajan</author>
<author>M Sviridenko</author>
</authors>
<title>On the Maximum Quadratic Assignment Problem.</title>
<date>2009</date>
<journal>Mathematics of Operations Research,</journal>
<volume>34</volume>
<contexts>
<context position="8118" citStr="Nagarajan and Sviridenko, 2009" startWordPosition="1354" endWordPosition="1357">riable mapping state has m(n − m) + m(m − 1) = m(n − 1) neighbors during the hill-climbing search. We greedily choose the best neighbor, repeating until no neighbor improves the number of triple matches. We experiment with two modifications to the greedy search: (1) executing multiple random restarts to avoid local optima, and (2) using our Baseline concept matching (“smart initialization”) instead of random initialization. NP-completeness. There is unlikely to be an exact polynomial-time algorithm for computing smatch. We can reduce the 0-1 Maximum Quadratic Assignment Problem (0-1-Max-QAP) (Nagarajan and Sviridenko, 2009) and the subgraph isomorphism problem directly to the full smatch problem on graphs.2 We note that other widely-used metrics, such as TER (Snover et al., 2006), are also NP-complete. Fortunately, the next section shows that the smatch methods above are efficient and effective. 1The tool can be downloaded at http://amr.isi.edu/evaluation.html. 2Thanks to David Chiang for observing the subgraph isomorphism reduction. 4 Using Smatch We report an AMR inter-annotator agreement study using smatch. 1. Our study has 4 annotators (A, B, C, D), who then converge on a consensus annotation E. We thus have</context>
</contexts>
<marker>Nagarajan, Sviridenko, 2009</marker>
<rawString>V. Nagarajan and M. Sviridenko. 2009. On the Maximum Quadratic Assignment Problem. Mathematics of Operations Research, 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10422" citStr="Papineni et al., 2002" startWordPosition="1737" endWordPosition="1741">and that 9 restarts are sufficient to obtain good quality. The table also shows total runtimes over 200 AMR pairs (10 annotator pairs, 5 sentence groups, 4 AMR pairs per group). Heuristic search with smart initialization and 4 restarts (S+4R) gives the best trade-off between accuracy and speed, so this is the setting we use in practice. Figure 1 shows smatch scores of each annotator (A-D) against the consensus annotation (E). The 3For documents containing multiple AMRs, we use the sum of matched triples over all AMR pairs to compute precision, recall, and f-score, much like corpus-level Bleu (Papineni et al., 2002). 750 1 2 B 4 5 1 2 C 4 5 1 2 D 4 5 1 2 E 4 5 3 3 3 3 Base 0.68 0.74 0.84 0.71 0.83 0.69 0.70 0.80 0.69 0.78 0.77 0.72 0.75 0.68 0.63 0.79 0.86 0.92 0.85 0.89 ILP 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92 R 0.74 0.79 0.84 0.75 0.86 0.74 0.75 0.80 0.77 0.88 0.83 0.76 0.75 0.72 0.75 0.85 0.92 0.92 0.89 0.89 A 10R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92 S 0.74 0.80 0.84 0.75 0.88 0.75 0.78 0.80 0.76 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92 S+4R 0.74 0.80 0.84 0.76 0.</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Parsons</author>
</authors>
<title>Events in the Semantics of English.</title>
<date>1990</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="2674" citStr="Parsons, 1990" startWordPosition="402" endWordPosition="403">ence semantic feature structures comes from determining an optimal variable alignment between them, and further prove that finding such alignment is NP-complete. We investigate how to compute this metric and provide several practical and replicable computing methods by using Integer Linear Programming (ILP) and hill-climbing method. We show that our metric can be used for measuring the annotator agreement in largescale linguistic annotation, and evaluating semantic parsing. 2 Semantic Overlap We work on a semantic feature structure representation in a standard neo-Davidsonian (Davidson, 1969; Parsons, 1990) framework. For example, semantics of the sentence “the boy wants to go” is represented by the following directed graph: In this graph, there are three concepts: want01, boy, and go-01. Both want-01 and go-01 are frames from PropBank framesets (Kingsbury and Palmer, 2002). The frame want-01 has two arguments connected with ARG0 and ARG1, and go01 has an argument (which is also the same boy instance) connected with ARG0. 748 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 748–752, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computation</context>
</contexts>
<marker>Parsons, 1990</marker>
<rawString>T. Parsons. 1990. Events in the Semantics of English. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Pradhan</author>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>Ontonotes: A Unified Relational Semantic Representation.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Semantic Computing (ICSC ’07).</booktitle>
<contexts>
<context position="13611" citStr="Pradhan et al., 2007" startWordPosition="2495" endWordPosition="2498">Base ILP R 10R S S+4R S+9R Accuracy 20% 100% 66.5% 100% 92% 100% 100% Time (sec) 0.86 49.67 5.85 64.78 2.31 28.36 59.69 Table 2: Accuracy and running time (seconds) of various computing methods of smatch over 200 AMR pairs. plot demonstrates that, as time goes by, annotators reach better agreement with the consensus. We also note that smatch is used to measure the accuracy of machine-generated AMRs. (Jones et al., 2012) use it to evaluate automatic semantic parsing in a narrow domain, while Ulf Hermjakob4 has developed a heuristic algorithm that exploits and supplements Ontonotes annotations (Pradhan et al., 2007) in order to automatically create AMRs for Ontonotes sentences, with a smatch score of 0.74 against human consensus AMRs. 5 Related Work Related work on directly measuring the semantic representation includes the method in (Dridan and Oepen, 2011), which evaluates semantic parser output directly by comparing semantic substructures, though they require an alignment between sentence spans and semantic sub-structures. In contrast, our metric does not require the align4personal communication Figure 1: Smatch scores of annotators (A-D) against the consensus annotation (E) over time. ment between an</context>
</contexts>
<marker>Pradhan, Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2007</marker>
<rawString>S. S. Pradhan, E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2007. Ontonotes: A Unified Relational Semantic Representation. In Proceedings of the International Conference on Semantic Computing (ICSC ’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-2006).</booktitle>
<contexts>
<context position="8277" citStr="Snover et al., 2006" startWordPosition="1381" endWordPosition="1384">ves the number of triple matches. We experiment with two modifications to the greedy search: (1) executing multiple random restarts to avoid local optima, and (2) using our Baseline concept matching (“smart initialization”) instead of random initialization. NP-completeness. There is unlikely to be an exact polynomial-time algorithm for computing smatch. We can reduce the 0-1 Maximum Quadratic Assignment Problem (0-1-Max-QAP) (Nagarajan and Sviridenko, 2009) and the subgraph isomorphism problem directly to the full smatch problem on graphs.2 We note that other widely-used metrics, such as TER (Snover et al., 2006), are also NP-complete. Fortunately, the next section shows that the smatch methods above are efficient and effective. 1The tool can be downloaded at http://amr.isi.edu/evaluation.html. 2Thanks to David Chiang for observing the subgraph isomorphism reduction. 4 Using Smatch We report an AMR inter-annotator agreement study using smatch. 1. Our study has 4 annotators (A, B, C, D), who then converge on a consensus annotation E. We thus have 10 pairs of annotations: A-B, A-C, ... , D-E. 2. The study is carried out 5 times. Each time annotators build AMRs for 4 sentences from the Wall Street Journa</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Tang</author>
<author>R J Mooney</author>
</authors>
<title>Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning.</booktitle>
<contexts>
<context position="1305" citStr="Tang and Mooney, 2001" startWordPosition="192" endWordPosition="195">thm to compute the metric and show the results of an inter-annotator agreement study. 1 Introduction The goal of semantic parsing is to generate all semantic relationships in a text. Its output is often represented by whole-sentence semantic structures. Evaluating such structures is necessary for semantic parsing tasks, as well as semantic annotation tasks which create linguistic resources for semantic parsing. However, there is no widely-used evaluation method for whole-sentence semantic structures. Current whole-sentence semantic parsing is mainly evaluated in two ways: 1. task correctness (Tang and Mooney, 2001), which evaluates on an NLP task that uses the parsing results; 2. whole-sentence accuracy (Zettlemoyer and Collins, 2005), which counts the number of sentences parsed completely correctly. Nevertheless, it is worthwhile to explore evaluation methods that use scores which range from 0 to 1 (“partial credit”) to measure whole-sentence semantic structures. By using such methods, we are able to differentiate between two similar wholesentence semantic structures regardless of specific tasks or domains. In this work, we provide an evaluation metric that uses the degree of overlap between two whole-</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>L. R. Tang and R. J. Mooney. 2001. Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing. In Proceedings of the 12th European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="1427" citStr="Zettlemoyer and Collins, 2005" startWordPosition="212" endWordPosition="215">emantic parsing is to generate all semantic relationships in a text. Its output is often represented by whole-sentence semantic structures. Evaluating such structures is necessary for semantic parsing tasks, as well as semantic annotation tasks which create linguistic resources for semantic parsing. However, there is no widely-used evaluation method for whole-sentence semantic structures. Current whole-sentence semantic parsing is mainly evaluated in two ways: 1. task correctness (Tang and Mooney, 2001), which evaluates on an NLP task that uses the parsing results; 2. whole-sentence accuracy (Zettlemoyer and Collins, 2005), which counts the number of sentences parsed completely correctly. Nevertheless, it is worthwhile to explore evaluation methods that use scores which range from 0 to 1 (“partial credit”) to measure whole-sentence semantic structures. By using such methods, we are able to differentiate between two similar wholesentence semantic structures regardless of specific tasks or domains. In this work, we provide an evaluation metric that uses the degree of overlap between two whole-sentence semantic structures as the partial credit. In this paper, we observe that the difficulty of computing the degree </context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2005. Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars. In Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>