<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.044254">
<title confidence="0.82159">
SZTE-NLP: Sentiment Detection on Twitter Messages
</title>
<author confidence="0.997524">
Viktor Hangya, G´abor Berend, Rich´ard Farkas
</author>
<affiliation confidence="0.9988525">
University of Szeged
Department of Informatics
</affiliation>
<email confidence="0.997607">
hangyav@gmail.com, {berendg,rfarkas}@inf.u-szeged.hu
</email>
<sectionHeader confidence="0.995846" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994637714285714">
In this paper we introduce our contribution
to the SemEval-2013 Task 2 on “Sentiment
Analysis in Twitter”. We participated in “task
B”, where the objective was to build mod-
els which classify tweets into three classes
(positive, negative or neutral) by their con-
tents. To solve this problem we basically fol-
lowed the supervised learning approach and
proposed several domain (i.e. microblog) spe-
cific improvements including text preprocess-
ing and feature engineering. Beyond the su-
pervised setting we also introduce some early
results employing a huge, automatically anno-
tated tweet dataset.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998773125">
In the past few years, the popularity of social me-
dia has increased. Many studies have been made in
the area (Jansen et al., 2009; O’Connor et al., 2010;
Bifet and Frank, 2010; Sang and Bos, 2012). Peo-
ple post messages on a variety of topics, for example
products, political issues, etc. Thus a big amount of
user generated data is created day-by-day. The man-
ual processing of this data is impossible, therefore
automatic procedures are needed.
In this paper we introduce an approach which is
able to assign sentiment labels to Twitter messages.
More precisely, it classifies tweets into positive, neg-
ative or neutral polarity classes. The system partici-
pated in the SemEval-2013 Task 2: Sentiment Anal-
ysis in Twitter, Task–B Message Polarity Classifica-
tion (Wilson et al., 2013). In our approach we used
a unigram based supervised model because it has
been shown that it works well on short messages like
tweets (Jiang et al., 2011; Barbosa and Feng, 2010;
Agarwal et al., 2011; Liu, 2010). We reduced the
size of the dictionary by normalizing the messages
and by stop word filtering. We also explored novel
features which gave us information on the polarity of
a tweet, for example we made use of the acronyms
in messages.
In the “constrained” track of Task–B we used the
given training and development data only. For the
“unconstrained” track we downloaded tweets using
the Twitter Streaming API1 and automatically anno-
tated them. We present some preliminary results on
exploiting this huge dataset for training our classi-
fier.
</bodyText>
<sectionHeader confidence="0.984022" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999785636363636">
At the beginning of our experiments we used a
unigram-based supervised model. Later on, we re-
alized that the size of our dictionary is huge, so
in the normalization phase we tried to reduce the
number of words in it. We investigated novel fea-
tures which contain information on the polarity of
the messages. Using these features we were able to
improve the precision of our classifier. For imple-
mentation we used the MALLET toolkit, which is a
Java-based package for natural language processing
(McCallum, 2002).
</bodyText>
<subsectionHeader confidence="0.94571">
2.1 Normalization
</subsectionHeader>
<bodyText confidence="0.9689805">
One reason for the unusually big dictionary size is
that it contains one word in many forms, for exam-
</bodyText>
<footnote confidence="0.988935">
1https://dev.twitter.com/docs/
streaming-apis/streams/public
</footnote>
<page confidence="0.962729">
549
</page>
<bodyText confidence="0.97449">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 549–553, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
ple in upper and lower case, in a misspelled form,
with character repetition, etc. On the other hand, it
contained numerous special annotations which are
typical for blogging, such as Twitter-specific anno-
tations, URL’s, smileys, etc. Keeping these in mind
we made the following normalization steps:
</bodyText>
<listItem confidence="0.935073235294118">
• First, in order to get rid of the multiple forms
of a single word we converted them into lower
case form then we stemmed them. For this pur-
pose we used the Porter Stemming Algorithm.
• We replaced the @ and # Twitter-specific tags
with the [USER] and [TAG] notations, respec-
tively. Besides we converted every URL in the
messages to the [URL] notation.
• Smileys in messages play an important role
in polarity classification. For this reason we
grouped them into positive and negative smi-
ley classes. We considered:), :-),: ), :D, =), ;),
; ), (: and :(, :-(, : (, ):, ) : smileys as positive
and negative, respectively.
• Since numbers do not contain information re-
garding a message polarity, we converted them
as well to the [NUMBER] form. In ad-
dition, we replaced the question and excla-
mation marks with the [QUESTION MARK]
and [EXCLAMATION MARK] notations. Af-
ter this we removed the unnecessary char-
acters ’&amp;quot;#$%&amp;()*+,./:;&lt;=&gt;\{}, with
the exception that we removed the ’ character
only if a word started or ended with it.
• In the case of words which contained character
repetitions – more precisely those which con-
tained the same character at least three times
in a row –, we reduced the length of this se-
quence to three. For instance, in the case
of the word yeeeeahhhhhhh we got the form
yeeeahhh. This way we unified these charac-
ter repetitions, but we did not loose this extra
information.
• Finally we made a stop word filtering in order
</listItem>
<bodyText confidence="0.887551181818182">
to get rid of the undesired words. To identify
these words we did not use a stop word dictio-
nary, rather we filtered out those words which
appeared too frequently in the training corpus.
We have chosen this method because we would
like to automatically detect those words which
are not relevant in the classification.
Before the normalization step, the dictionary con-
tained approximately 41, 000 words. After the above
introduced steps we managed to reduce the size of
the dictionary to 15, 000 words.
</bodyText>
<subsectionHeader confidence="0.982552">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.999846736842105">
After normalizing Twitter messages, we searched
for special features which characterize the polarity
of the tweets. One such feature is the polarity of
each word in a message. To determine the polarity
of a word, we used the SentiWordNet sentiment lex-
icon (Baccianella et al., 2010). In this lexicon, a pos-
itive, negative and an objective real value belong to
each word, which describes the polarity of the given
word. We consider a word as positive if the related
positive value is greater than 0.3, we consider it as
negative if the related negative value is greater than
0.2 and we consider it as objective if the related ob-
jective value is greater than 0.8. The threshold of the
objective value is high because most words are ob-
jective in this lexicon. After calculating the polarity
of each word we created three new features for each
tweet which are the number of positive, negative and
objective words, respectively. We also checked if a
negation word precedes a positive or negative word
and if so we inverted its polarity.
We also tried to group acronyms by their polarity.
For this purpose we used an acronym lexicon which
can be found on the www.internetslang.com
website. For each acronym we used the polarity of
each word in the acronym’s description and we de-
termined the polarity of the acronym by calculat-
ing the rate of positive and negative words in the
description. This way we created two new fea-
tures which are the number of positive and negative
acronyms in a given message.
Our intuition was that people like to use character
repetitions in their words for expressing their happi-
ness or sadness. Besides normalizing these tokens
(see Section 2.1), we created a new feature as well,
which represents the number of this kind of words
in a tweet.
Beyond character repetitions people like to write
words or a part of the text in upper case in order to
</bodyText>
<page confidence="0.978261">
550
</page>
<bodyText confidence="0.998554333333333">
call the reader’s attention. Because of this we cre-
ated another feature which is the number of upper
case words in the given text.
</bodyText>
<sectionHeader confidence="0.99193" genericHeader="method">
3 Collected Data
</sectionHeader>
<bodyText confidence="0.999978371428571">
In order to achieve an appropriate precision with su-
pervised methods we need a big amount of training
data. Creating this database manually is a hard and
time-consuming task. In many cases it is hard even
for humans to determine the polarity of a message,
for instance:
After a whole 5 hours away from work, I
get to go back again, I’m so lucky!
In the above tweet we cannot decide precisely the
polarity because the writer can be serious or just sar-
castic.
In order to increase the size of the training data
we acquired additional tweets, which we used in
the unconstrained run for Task–B. We created an ap-
plication which downloads tweets using the Twitter
Streaming API. The API supports language filter-
ing, which was used to get rid of non-English mes-
sages. Our manual investigations of the downloaded
tweets revealed, however, that this filter allows a big
amount of non-English tweets, which is probably
due to the fact that some Twitter users did not set
their language. We used Twitter4J2 API (which is
a Java library for the Twitter API) for downloading
these tweets. We automatically annotated the down-
loaded tweets using the Twitter Sentiment3 web ap-
plication, similar to Barbosa and Feng (2010) but
we used only one annotator. This web application
also supports language detection, but after this extra
filtration, our dataset still contained a considerable
amount of non-English messages. After 16 hours
of data collection we got 350, 000 annotated tweets,
where the distribution of neutral, positive and neg-
ative classes was approximately 60%, 20%, 20%,
respectively. For further testing purposes we have
chosen 10, 000 tweets from each class.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99972">
We report results on the two official test sets of the
shared task. The “twitter” test set consists of 3, 813
</bodyText>
<footnote confidence="0.999278">
2http://twitter4j.org
3http://www.sentiment140.com
</footnote>
<bodyText confidence="0.9537423125">
tweets while the “sms” set consists of 2,094 sms
messages. We evaluated both test databases in two
ways, in the so-called constrained run we only used
the official training database, while in the uncon-
strained run we also used a part of the additional
data, which was mentioned in the 3 section. The
official training database contained 4,028 positive,
1, 655 negative and 3, 821 neutral tweets while for
the unconstrained run we used an additional 10, 000
tweets from each class. This way in each phase we
got four kinds of runs, which were evaluated with
the Naive Bayes and Maximum Entropy classifiers.
In Table 1 the evaluation of the unigram-based
model with the Naive Bayes learner can be seen.
The table contains the F-scores for the positive, neg-
ative and neutral labels for each of the four runs.
The avg column contains the average F-score for the
positive and negative labels, which was the official
evaluation metric for SemEval-2013 Task 2. We got
the best scores for the neutral label whilst the worst
scores are obtained for the negative label, which is
due to the fact that there were much less negative
instances in the training database. It can be seen
that the F-scores for the unconstrained run are better
both for the tweet and sms test databases. For the
unigram-based model the F-scores are higher when
we used the Maximum Entropy model (see Table 2).
pos neg neut avg
twitter-constrained 0.59 0.09 0.65 0.34
twitter-unconstrained 0.60 0.17 0.65 0.38
sms-constrained 0.46 0.16 0.63 0.31
sms-unconstrained 0.47 0.38 0.53 0.42
</bodyText>
<tableCaption confidence="0.999782">
Table 1: Unigram-based model, Naive Bayes learner
</tableCaption>
<table confidence="0.9830758">
pos neg neut avg
twitter-constrained 0.60 0.33 0.67 0.46
twitter-unconstrained 0.60 0.40 0.66 0.50
sms-constrained 0.47 0.31 0.69 0.39
sms-unconstrained 0.52 0.47 0.66 0.49
</table>
<tableCaption confidence="0.890569">
Table 2: Unigram-based model, Maximum Entropy
learner
</tableCaption>
<bodyText confidence="0.9136565">
In Tables 3 and 4 the evaluation results can be
seen for the normalized model. The normalization
</bodyText>
<page confidence="0.995198">
551
</page>
<bodyText confidence="0.995451">
step increased the precision for both learning al-
gorithms and the Maximum Entropy learner is still
better than Naive Bayes. Besides this we noticed
that for both learners in the case of the tweet test
database, the unconstrained run had lower scores
than the constrained whilst in the case of the sms
test database this phenomenon did not appear.
</bodyText>
<table confidence="0.9970608">
pos neg neut avg
twitter-constrained 0.65 0.32 0.67 0.48
twitter-unconstrained 0.62 0.21 0.63 0.41
sms-constrained 0.56 0.27 0.72 0.41
sms-unconstrained 0.52 0.35 0.66 0.43
</table>
<tableCaption confidence="0.999148">
Table 3: Normalized model, Naive Bayes learner
</tableCaption>
<table confidence="0.9974658">
pos neg neut avg
twitter-constrained 0.66 0.40 0.68 0.53
twitter-unconstrained 0.61 0.42 0.64 0.51
sms-constrained 0.61 0.38 0.77 0.49
sms-unconstrained 0.57 0.47 0.72 0.52
</table>
<tableCaption confidence="0.865772">
Table 4: Normalized model, Maximum Entropy
learner
</tableCaption>
<bodyText confidence="0.999883730769231">
The evaluation results of the feature-based model
can be seen in Tables 5 and 6. In the case of the
Naive Bayes learner, the features did not increase the
F-scores, only for the sms-unconstrained run. For
the other runs the achieved scores decreased. In the
case of the Maximum Entropy learner the features
increased the F-scores, slightly for the constrained
runs and a bit more for the unconstrained runs.
From this analysis we can conclude that the nor-
malization of the messages yielded a considerable
increase in the F-scores. We discussed above that
this step also significantly reduced the size of the
dictionary. The features increased the precision too,
especially for the unconstrained run. This means
that these features represent properties which are
useful for those training data which are not from the
same corpus as the test messages. We compared two
machine learning algorithms and from the results we
concluded that the Maximum Entropy learner per-
forms better than the Naive Bayes on this task. Our
experiments also showed that the external, automat-
ically labeled training database helped only in the
classification of sms messages. This is due to the
fact that the smses and our external database are
from a different distribution than the official tweet
database.
</bodyText>
<table confidence="0.9842362">
pos neg neut avg
twitter-constrained 0.65 0.32 0.67 0.48
twitter-unconstrained 0.62 0.17 0.79 0.39
sms-constrained 0.56 0.38 0.74 0.47
sms-unconstrained 0.54 0.29 0.70 0.41
</table>
<tableCaption confidence="0.995632">
Table 5: Feature-based model, Naive Bayes learner
</tableCaption>
<table confidence="0.999549">
pos neg neut avg
twitter-constrained 0.66 0.41 0.69 0.54
twitter-unconstrained 0.63 0.43 0.65 0.53
sms-constrained 0.62 0.39 0.79 0.50
sms-unconstrained 0.61 0.49 0.75 0.55
</table>
<tableCaption confidence="0.923848">
Table 6: Feature-based model, Maximum Entropy
learner
</tableCaption>
<sectionHeader confidence="0.995778" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999773">
Recently, sentiment analysis on Twitter messages
has gained a lot of attention due to the huge amount
of Twitter users and their tweets. In this paper we ex-
amined different methods for classifying Twitter and
sms messages. We proposed special features which
characterize the polarity of the messages and we
concluded that due to the informality (slang, spelling
mistakes, etc.) of the messages it is crucial to nor-
malize them properly.
In the future, we plan to investigate the utility of
relations between Twitter users and between their
tweets and we are interested in topic-dependent sen-
timent analysis.
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996255">
This work was supported in part by the Euro-
pean Union and the European Social Fund through
project FuturICT.hu (grant no.: T ´AMOP-4.2.2.C-
11/1/KONV-2012-0013).
</bodyText>
<sectionHeader confidence="0.995499" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.8229335">
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment Analysis
</reference>
<page confidence="0.988909">
552
</page>
<reference confidence="0.999553529411765">
of Twitter Data. In Proceedings of the Workshop on
Language in Social Media (LSM 2011), pages 30–38,
June.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC’10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. In Poster volume, Coling 2010, pages 36–44,
August.
Albert Bifet and Eibe Frank. 2010. Sentiment Knowl-
edge Discovery in Twitter Streaming Data.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter Power: Tweets as Electronic
Word of Mouth. In Journal of the American society
for information science and technology, pages 2169–
2188.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent Twitter Sentiment
Classification. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics, pages 151–160, June.
Bing Liu. 2010. Sentiment Analysis and Subjectivity. In
N. Indurkhya and F. J. Damerau, editors, Handbook of
Natural Language Processing.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From Tweets to Polls: Linking Text Sentiment to
Public Opinion Time Series. In Proceedings of the
International AAAI Conference on Weblogs and Social
Media, May.
Erik Tjong Kim Sang and Johan Bos. 2012. Predicting
the 2011 Dutch Senate Election Results with Twitter.
In Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 53–60, April.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 Task 2: Sentiment Analysis in Twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ’13, June.
</reference>
<page confidence="0.99902">
553
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.891919">
<title confidence="0.999248">SZTE-NLP: Sentiment Detection on Twitter Messages</title>
<author confidence="0.984773">Viktor Hangya</author>
<author confidence="0.984773">G´abor Berend</author>
<author confidence="0.984773">Rich´ard</author>
<affiliation confidence="0.98439">University of Department of</affiliation>
<abstract confidence="0.9953778">In this paper we introduce our contribution to the SemEval-2013 Task 2 on “Sentiment Analysis in Twitter”. We participated in “task B”, where the objective was to build models which classify tweets into three classes (positive, negative or neutral) by their contents. To solve this problem we basically followed the supervised learning approach and proposed several domain (i.e. microblog) specific improvements including text preprocessing and feature engineering. Beyond the supervised setting we also introduce some early results employing a huge, automatically annotated tweet dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment Analysis of Twitter Data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Language in Social Media (LSM 2011),</booktitle>
<pages>30--38</pages>
<contexts>
<context position="1788" citStr="Agarwal et al., 2011" startWordPosition="276" endWordPosition="279">nual processing of this data is impossible, therefore automatic procedures are needed. In this paper we introduce an approach which is able to assign sentiment labels to Twitter messages. More precisely, it classifies tweets into positive, negative or neutral polarity classes. The system participated in the SemEval-2013 Task 2: Sentiment Analysis in Twitter, Task–B Message Polarity Classification (Wilson et al., 2013). In our approach we used a unigram based supervised model because it has been shown that it works well on short messages like tweets (Jiang et al., 2011; Barbosa and Feng, 2010; Agarwal et al., 2011; Liu, 2010). We reduced the size of the dictionary by normalizing the messages and by stop word filtering. We also explored novel features which gave us information on the polarity of a tweet, for example we made use of the acronyms in messages. In the “constrained” track of Task–B we used the given training and development data only. For the “unconstrained” track we downloaded tweets using the Twitter Streaming API1 and automatically annotated them. We present some preliminary results on exploiting this huge dataset for training our classifier. 2 Approach At the beginning of our experiments </context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment Analysis of Twitter Data. In Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 30–38, June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="5846" citStr="Baccianella et al., 2010" startWordPosition="947" endWordPosition="950">aining corpus. We have chosen this method because we would like to automatically detect those words which are not relevant in the classification. Before the normalization step, the dictionary contained approximately 41, 000 words. After the above introduced steps we managed to reduce the size of the dictionary to 15, 000 words. 2.2 Features After normalizing Twitter messages, we searched for special features which characterize the polarity of the tweets. One such feature is the polarity of each word in a message. To determine the polarity of a word, we used the SentiWordNet sentiment lexicon (Baccianella et al., 2010). In this lexicon, a positive, negative and an objective real value belong to each word, which describes the polarity of the given word. We consider a word as positive if the related positive value is greater than 0.3, we consider it as negative if the related negative value is greater than 0.2 and we consider it as objective if the related objective value is greater than 0.8. The threshold of the objective value is high because most words are objective in this lexicon. After calculating the polarity of each word we created three new features for each tweet which are the number of positive, ne</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust Sentiment Detection on Twitter from Biased and Noisy Data. In Poster volume, Coling</title>
<date>2010</date>
<pages>36--44</pages>
<contexts>
<context position="1766" citStr="Barbosa and Feng, 2010" startWordPosition="272" endWordPosition="275">eated day-by-day. The manual processing of this data is impossible, therefore automatic procedures are needed. In this paper we introduce an approach which is able to assign sentiment labels to Twitter messages. More precisely, it classifies tweets into positive, negative or neutral polarity classes. The system participated in the SemEval-2013 Task 2: Sentiment Analysis in Twitter, Task–B Message Polarity Classification (Wilson et al., 2013). In our approach we used a unigram based supervised model because it has been shown that it works well on short messages like tweets (Jiang et al., 2011; Barbosa and Feng, 2010; Agarwal et al., 2011; Liu, 2010). We reduced the size of the dictionary by normalizing the messages and by stop word filtering. We also explored novel features which gave us information on the polarity of a tweet, for example we made use of the acronyms in messages. In the “constrained” track of Task–B we used the given training and development data only. For the “unconstrained” track we downloaded tweets using the Twitter Streaming API1 and automatically annotated them. We present some preliminary results on exploiting this huge dataset for training our classifier. 2 Approach At the beginni</context>
<context position="8792" citStr="Barbosa and Feng (2010)" startWordPosition="1463" endWordPosition="1466"> Task–B. We created an application which downloads tweets using the Twitter Streaming API. The API supports language filtering, which was used to get rid of non-English messages. Our manual investigations of the downloaded tweets revealed, however, that this filter allows a big amount of non-English tweets, which is probably due to the fact that some Twitter users did not set their language. We used Twitter4J2 API (which is a Java library for the Twitter API) for downloading these tweets. We automatically annotated the downloaded tweets using the Twitter Sentiment3 web application, similar to Barbosa and Feng (2010) but we used only one annotator. This web application also supports language detection, but after this extra filtration, our dataset still contained a considerable amount of non-English messages. After 16 hours of data collection we got 350, 000 annotated tweets, where the distribution of neutral, positive and negative classes was approximately 60%, 20%, 20%, respectively. For further testing purposes we have chosen 10, 000 tweets from each class. 4 Results We report results on the two official test sets of the shared task. The “twitter” test set consists of 3, 813 2http://twitter4j.org 3http:</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust Sentiment Detection on Twitter from Biased and Noisy Data. In Poster volume, Coling 2010, pages 36–44, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Eibe Frank</author>
</authors>
<title>Sentiment Knowledge Discovery in Twitter Streaming Data.</title>
<date>2010</date>
<contexts>
<context position="985" citStr="Bifet and Frank, 2010" startWordPosition="143" endWordPosition="146"> to build models which classify tweets into three classes (positive, negative or neutral) by their contents. To solve this problem we basically followed the supervised learning approach and proposed several domain (i.e. microblog) specific improvements including text preprocessing and feature engineering. Beyond the supervised setting we also introduce some early results employing a huge, automatically annotated tweet dataset. 1 Introduction In the past few years, the popularity of social media has increased. Many studies have been made in the area (Jansen et al., 2009; O’Connor et al., 2010; Bifet and Frank, 2010; Sang and Bos, 2012). People post messages on a variety of topics, for example products, political issues, etc. Thus a big amount of user generated data is created day-by-day. The manual processing of this data is impossible, therefore automatic procedures are needed. In this paper we introduce an approach which is able to assign sentiment labels to Twitter messages. More precisely, it classifies tweets into positive, negative or neutral polarity classes. The system participated in the SemEval-2013 Task 2: Sentiment Analysis in Twitter, Task–B Message Polarity Classification (Wilson et al., 2</context>
</contexts>
<marker>Bifet, Frank, 2010</marker>
<rawString>Albert Bifet and Eibe Frank. 2010. Sentiment Knowledge Discovery in Twitter Streaming Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Mimi Zhang</author>
<author>Kate Sobel</author>
<author>Abdur Chowdury</author>
</authors>
<title>Twitter Power: Tweets as Electronic Word of Mouth.</title>
<date>2009</date>
<journal>In Journal of the American</journal>
<pages>2169--2188</pages>
<contexts>
<context position="939" citStr="Jansen et al., 2009" startWordPosition="135" endWordPosition="138">cipated in “task B”, where the objective was to build models which classify tweets into three classes (positive, negative or neutral) by their contents. To solve this problem we basically followed the supervised learning approach and proposed several domain (i.e. microblog) specific improvements including text preprocessing and feature engineering. Beyond the supervised setting we also introduce some early results employing a huge, automatically annotated tweet dataset. 1 Introduction In the past few years, the popularity of social media has increased. Many studies have been made in the area (Jansen et al., 2009; O’Connor et al., 2010; Bifet and Frank, 2010; Sang and Bos, 2012). People post messages on a variety of topics, for example products, political issues, etc. Thus a big amount of user generated data is created day-by-day. The manual processing of this data is impossible, therefore automatic procedures are needed. In this paper we introduce an approach which is able to assign sentiment labels to Twitter messages. More precisely, it classifies tweets into positive, negative or neutral polarity classes. The system participated in the SemEval-2013 Task 2: Sentiment Analysis in Twitter, Task–B Mes</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur Chowdury. 2009. Twitter Power: Tweets as Electronic Word of Mouth. In Journal of the American society for information science and technology, pages 2169– 2188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent Twitter Sentiment Classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>151--160</pages>
<contexts>
<context position="1742" citStr="Jiang et al., 2011" startWordPosition="268" endWordPosition="271">generated data is created day-by-day. The manual processing of this data is impossible, therefore automatic procedures are needed. In this paper we introduce an approach which is able to assign sentiment labels to Twitter messages. More precisely, it classifies tweets into positive, negative or neutral polarity classes. The system participated in the SemEval-2013 Task 2: Sentiment Analysis in Twitter, Task–B Message Polarity Classification (Wilson et al., 2013). In our approach we used a unigram based supervised model because it has been shown that it works well on short messages like tweets (Jiang et al., 2011; Barbosa and Feng, 2010; Agarwal et al., 2011; Liu, 2010). We reduced the size of the dictionary by normalizing the messages and by stop word filtering. We also explored novel features which gave us information on the polarity of a tweet, for example we made use of the acronyms in messages. In the “constrained” track of Task–B we used the given training and development data only. For the “unconstrained” track we downloaded tweets using the Twitter Streaming API1 and automatically annotated them. We present some preliminary results on exploiting this huge dataset for training our classifier. 2</context>
</contexts>
<marker>Jiang, Yu, Zhou, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent Twitter Sentiment Classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 151–160, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Subjectivity.</title>
<date>2010</date>
<booktitle>Handbook of Natural Language Processing.</booktitle>
<editor>In N. Indurkhya and F. J. Damerau, editors,</editor>
<contexts>
<context position="1800" citStr="Liu, 2010" startWordPosition="280" endWordPosition="281">s data is impossible, therefore automatic procedures are needed. In this paper we introduce an approach which is able to assign sentiment labels to Twitter messages. More precisely, it classifies tweets into positive, negative or neutral polarity classes. The system participated in the SemEval-2013 Task 2: Sentiment Analysis in Twitter, Task–B Message Polarity Classification (Wilson et al., 2013). In our approach we used a unigram based supervised model because it has been shown that it works well on short messages like tweets (Jiang et al., 2011; Barbosa and Feng, 2010; Agarwal et al., 2011; Liu, 2010). We reduced the size of the dictionary by normalizing the messages and by stop word filtering. We also explored novel features which gave us information on the polarity of a tweet, for example we made use of the acronyms in messages. In the “constrained” track of Task–B we used the given training and development data only. For the “unconstrained” track we downloaded tweets using the Twitter Streaming API1 and automatically annotated them. We present some preliminary results on exploiting this huge dataset for training our classifier. 2 Approach At the beginning of our experiments we used a un</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>Bing Liu. 2010. Sentiment Analysis and Subjectivity. In N. Indurkhya and F. J. Damerau, editors, Handbook of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="2862" citStr="McCallum, 2002" startWordPosition="456" endWordPosition="457">present some preliminary results on exploiting this huge dataset for training our classifier. 2 Approach At the beginning of our experiments we used a unigram-based supervised model. Later on, we realized that the size of our dictionary is huge, so in the normalization phase we tried to reduce the number of words in it. We investigated novel features which contain information on the polarity of the messages. Using these features we were able to improve the precision of our classifier. For implementation we used the MALLET toolkit, which is a Java-based package for natural language processing (McCallum, 2002). 2.1 Normalization One reason for the unusually big dictionary size is that it contains one word in many forms, for exam1https://dev.twitter.com/docs/ streaming-apis/streams/public 549 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 549–553, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics ple in upper and lower case, in a misspelled form, with character repetition, etc. On the other hand, it contained numerous special annotations which are typical f</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series.</title>
<date>2010</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social</booktitle>
<location>Media,</location>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series. In Proceedings of the International AAAI Conference on Weblogs and Social Media, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
<author>Johan Bos</author>
</authors>
<title>Predicting the 2011 Dutch Senate Election Results with Twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>53--60</pages>
<contexts>
<context position="1006" citStr="Sang and Bos, 2012" startWordPosition="147" endWordPosition="150">classify tweets into three classes (positive, negative or neutral) by their contents. To solve this problem we basically followed the supervised learning approach and proposed several domain (i.e. microblog) specific improvements including text preprocessing and feature engineering. Beyond the supervised setting we also introduce some early results employing a huge, automatically annotated tweet dataset. 1 Introduction In the past few years, the popularity of social media has increased. Many studies have been made in the area (Jansen et al., 2009; O’Connor et al., 2010; Bifet and Frank, 2010; Sang and Bos, 2012). People post messages on a variety of topics, for example products, political issues, etc. Thus a big amount of user generated data is created day-by-day. The manual processing of this data is impossible, therefore automatic procedures are needed. In this paper we introduce an approach which is able to assign sentiment labels to Twitter messages. More precisely, it classifies tweets into positive, negative or neutral polarity classes. The system participated in the SemEval-2013 Task 2: Sentiment Analysis in Twitter, Task–B Message Polarity Classification (Wilson et al., 2013). In our approach</context>
</contexts>
<marker>Sang, Bos, 2012</marker>
<rawString>Erik Tjong Kim Sang and Johan Bos. 2012. Predicting the 2011 Dutch Senate Election Results with Twitter. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 53–60, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
</authors>
<title>SemEval-2013 Task 2: Sentiment Analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<contexts>
<context position="1589" citStr="Wilson et al., 2013" startWordPosition="240" endWordPosition="243"> and Frank, 2010; Sang and Bos, 2012). People post messages on a variety of topics, for example products, political issues, etc. Thus a big amount of user generated data is created day-by-day. The manual processing of this data is impossible, therefore automatic procedures are needed. In this paper we introduce an approach which is able to assign sentiment labels to Twitter messages. More precisely, it classifies tweets into positive, negative or neutral polarity classes. The system participated in the SemEval-2013 Task 2: Sentiment Analysis in Twitter, Task–B Message Polarity Classification (Wilson et al., 2013). In our approach we used a unigram based supervised model because it has been shown that it works well on short messages like tweets (Jiang et al., 2011; Barbosa and Feng, 2010; Agarwal et al., 2011; Liu, 2010). We reduced the size of the dictionary by normalizing the messages and by stop word filtering. We also explored novel features which gave us information on the polarity of a tweet, for example we made use of the acronyms in messages. In the “constrained” track of Task–B we used the given training and development data only. For the “unconstrained” track we downloaded tweets using the Tw</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, Ritter, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013. SemEval-2013 Task 2: Sentiment Analysis in Twitter. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>