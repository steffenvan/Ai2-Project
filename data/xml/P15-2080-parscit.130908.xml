<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011017">
<title confidence="0.996313">
User Based Aggregation for Biterm Topic Model
</title>
<author confidence="0.996992">
Weizheng Chen, Jinpeng Wang, Yan Zhang , Hongfei Yan and Xiaoming Li
</author>
<affiliation confidence="0.984791">
School of Electronic Engineering and Computer Science, Peking University, China
</affiliation>
<email confidence="0.991018">
{cwz.pku,wjp.pku,yhf10291@gmail.com, zhy@cis.pku.edu.cn, lxm@pku.edu.cn
</email>
<sectionHeader confidence="0.993677" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993025">
Biterm Topic Model (BTM) is designed
to model the generative process of
the word co-occurrence patterns in
short texts such as tweets. However,
two aspects of BTM may restrict its
performance: 1) user individualities are
ignored to obtain the corpus level words
co-occurrence patterns; and 2) the strong
assumptions that two co-occurring words
will be assigned the same topic label
could not distinguish background words
from topical words. In this paper, we
propose Twitter-BTM model to address
those issues by considering user level
personalization in BTM. Firstly, we
use user based biterms aggregation to
learn user specific topic distribution.
Secondly, each user’s preference between
background words and topical words is
estimated by incorporating a background
topic. Experiments on a large-scale
real-world Twitter dataset show that
Twitter-BTM outperforms several state-
of-the-art baselines.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999621416666667">
In recent years, short texts are increasingly preva-
lent due to the explosive growth of online social
media. For example, about 500 million tweets are
published per day on Twitter1, one of the most
popular online social networking services. Proba-
bilistic topic models (Blei et al., 2003) are broadly
used to uncover the hidden topics of tweets, s-
ince the low-dimensional semantic representation
is crucial for many applications, such as prod-
uct recommendation (Zhao et al., 2014), hashtag
recommendation (Ma et al., 2014), user interest
tracking (Sasaki et al., 2014), sentiment analysis
</bodyText>
<footnote confidence="0.994632">
1See https://about.Twitter.com/company
</footnote>
<bodyText confidence="0.991364585365854">
(Si et al., 2013). However, the scarcity of context
and the noisy words restrict LDA and its variations
in topic modeling over short texts.
Previous works model topic distribution at
three different levels for tweets: 1) document,
the standard LDA assumes each document is
associated with a topic distribution (Godin et
al., 2013; Huang, 2012). LDA and its variations
suffer from context sparsity in each tweet. 2)
user, user based aggregation is utilized to alleviate
the sparsity problem in short texts (Weng et al.,
2010; Hong and Davison, 2010). In these models,
all the tweets of the same user are aggregated
together as a pseudo document based on the
observation that the tweets written by the same
user are more similar. 3) corpus, BTM (Yan et al.,
2013) assumes that all the biterms (co-occurring
word pairs) are generated by a corpus level topic
distribution to benefit from the global rich word
co-occurrence patterns.
As far as we know, how to incorporate user
factor into BTM has not been studied yet. User
based aggregation has proven effective for LDA.
But unfortunately, our preliminary experiments in-
dicate that simple user-based aggregation for BTM
will generate incoherent topics. To distinguish be-
tween commonly used words (e.g., good, people,
etc) and topical words (e.g., food, travel, etc), a
background topic is often incorporated into the
topic models. Zhao et al. (2011) use a back-
ground topic in Twitter-LDA to distill discrimi-
native words in tweets. Sasaki et al. (2014) re-
duce the perplexity of Twitter-LDA by estimating
the ratio between choosing background words and
topical words for each user. They both make a very
strong assumption that one tweet only covers one
topic. Yan et al. (2015) use a background topic to
distinguish between common biterms and bursty
biterms, which need external data to evaluate the
burstiness of each biterm as prior knowledge. Un-
like those above, we incorporate a background
</bodyText>
<page confidence="0.956176">
489
</page>
<bodyText confidence="0.932673736842105">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 489–494,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
topic to absorb non-discriminative common words
in each biterm. And we also estimate the user’s
preference between common words and topical
words. Our new model is named as Twitter-BTM,
which combines user based aggregation and the
background topic in BTM. Finally, experiments on
a Twitter dataset show that Twitter-BTM not only
can discover more coherent topics but also can
give more accurate topic representation of tweets
compared with several state-of-the-art baselines.
We organize the rest of the paper as follows.
Section 2 gives a brief review for BTM. Section
3 introduces our Twitter-BTM model and its im-
plementation. Section 4 describes experimental
results on a large-scale Twitter dataset. Finally,
Section 5 contains a conclusion and future work.
</bodyText>
<sectionHeader confidence="0.990306" genericHeader="introduction">
2 BTM
</sectionHeader>
<bodyText confidence="0.999616454545455">
There are two major differences between BTM
and LDA (Yan et al., 2013). For one thing, con-
sidering a topic is a mixture of highly correlated
words, which implies that they often occur togeth-
er in the same document, BTM models the gen-
erative process of the word co-occurrence patterns
directly. Thus a document made up of n words will
be converted to C2n biterms. For another, LDA and
its variants suffer from the severe data sparsity in
short documents. BTM uses global co-occurrence
patterns to model the topic distribution over corpus
level instead of document level.
The graphical representation of BTM (Yan et
al., 2013) is shown in Figure 1(a). It assumes
that the whole corpus is associated with a distri-
butions θ over K topics drawn from a Dirichlet
prior Dir(α). And each topic t is associated with
a multinomial distribution φt over a vocabulary
of V unique words drawn from a Dirichlet pri-
or Dir(Q). The generative process for a corpus
which consists of NB biterms B = {b1, ..., bNB},
where bi = (wi1, wi2), is as follows:
</bodyText>
<listItem confidence="0.98703">
1 For each topic t=1,...,T
(a) Draw φt — Dir(Q)
2 For the whole tweets collection
(a) Draw θ — Dir(α)
3 For each biterm b = 1,...,NB
(a) Draw zb — Multi(θ)
(b) Draw wb,1, wb,2 — Multi(φzb)
</listItem>
<bodyText confidence="0.965099333333333">
In the above process, zb is the topic assign-
ment latent variable of biterm b. To infer the
parameters φ and θ, collapsed Gibbs sampling
</bodyText>
<figureCaption confidence="0.576579">
Figure 1: Graphical representation of (a) BTM, (b)
Twitter-BTM
</figureCaption>
<bodyText confidence="0.994714230769231">
algorithm (Griffiths and Steyvers, 2004) is used
for approximate inference.
Compared with the strong assumption that a
short document only covers a single topic (Diao et
al., 2012; Ding et al., 2013), BTM makes a looser
assumption that two words will be assigned the
same topic label if they have co-occurred. Thus a
short document could cover more than one topic,
which is more close to the reality. But this assump-
tion causes another issue, those commonly used
words and those topical words are treated equally.
Obviously it is inappropriate to assign same topic
label to those words.
</bodyText>
<sectionHeader confidence="0.985985" genericHeader="method">
3 Twitter-BTM
</sectionHeader>
<bodyText confidence="0.99965725">
In this Section, we introduce our Twitter-BTM
model. Figure 1(b) shows the graphical represen-
tation of Twitter-BTM. The generative process of
Twitter-BTM is as follows:
</bodyText>
<listItem confidence="0.993830916666667">
1 Draw φB — Dir(Q)
2 For each topic t=1,...,T
(a) Draw φt — Dir(Q)
3 For each user u=1,...,U
(a) Draw θu — Dir(α), 7ru — Beta(γ)
(b) For each biterm b = 1,...,Nu
(i) Draw zu,b — Multi(θu)
(ii) For each word n = 1,2
(A) Draw yu,b,n — Bern(7ru)
(B) if yu,b,n = 0 Draw wu,b,n —
Multi(φB)
if yu,b,n =
</listItem>
<figure confidence="0.99171288">
Multi(φzu,b)
(a) BTM (b) Twitter-BTM
y w
π
γ
0k
e
z
α
K
2
Nu
U
0B
P
0k
w
z
α
e
K
2
NB
P
1 Draw wu,b,n —
</figure>
<page confidence="0.99265">
490
</page>
<bodyText confidence="0.999532259259259">
In the above process, user u’s topic interest Ou
is a multinomial distribution over K topics drawn
from a Dirichlet prior Dir(α). The background
topic B is associated with a multinomial distribu-
tion φB drawn from a Dirichlet prior Dir(Q). The
assumption that each user has a different prefer-
ence between topical words and background word-
s is shown to be effective in (Sasaki et al., 2014).
We adopt this assumption in Twitter-BTM. User
u’s preference is represented as a Bernoulli distri-
bution with parameter Tru drawn from a beta prior
Beta(-y). Nu is the number of biterms of user u,
zu,b is the topic assignment latent variable of user
u’s biterm b. For user u and his/her biterm b, n=1
or 2, we use a latent variable yu,b,n to indicate the
word type of the word wb,n. When yu,b,n = 1, wb,n
is generated from topic zu,b. When yu,b,n = 0,
wb,n is generated from the background topic B.
We adopt collapsed Gibbs Sampling to estimate
the parameters. Because of the limitations of s-
pace, we leave out the details about the sampling
algorithm. Since we can’t get a document’s distri-
bution over topics from the parameters estimated
by Twitter-BTM directly, we utilize the following
formula (Yan et al., 2013) to infer the topic distri-
bution of document d. Given a document d whose
author is user u:
</bodyText>
<equation confidence="0.934564333333333">
Nb
P(z = tId) = P(z = t|bi)P(bi|d) (1)
i
Now the problem is converted to how to estimate
P(bi|d) and P(z = t|bi). P(bi|d) is estimated by
empirical distribution in d:
Nbi
P(bi|d) = (2)
Nb
</equation>
<bodyText confidence="0.9996955">
where Nbi is the number of biterm bi occurred in
d, Nb is the total number of biterms in d. We
can apply Bayes’ rule to compute P(z = t|bi) via
following expression:
</bodyText>
<equation confidence="0.9931372">
θt [πuφBwi,1 + (1 − πu)φwi,1] 1πuφwBi,2 + (1 − πu)φti,2
� � � �
Pk θu πuφB wi,1 + (1 − πu)φk πuφB wi,2 + (1 − πu)φk
k wi,1 wi,2
(3)
</equation>
<sectionHeader confidence="0.999457" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999710307692308">
In this Section, we describe our experiments car-
ried on a Twitter dataset collected form 10th Jun,
2009 to 31st Dec, 2009. Stop words and words
occur less than 5 times are removed. We also filter
tweets which only have one or two words. All
letters are converted into lower case. The dataset is
divided into two parts. The first part whose statis-
tics is shown in Table 1 is used for training. The
second part which consists of 22,496,107 tweets
is used as the external dataset in topic coherence
evaluation task in Section 4.1.
We compare the performance of Twitter-BTM
with five baselines:
</bodyText>
<listItem confidence="0.998387307692308">
• LDA-U, user based aggregation is applied
before training LDA.
• Twitter-LDA (Zhao et al., 2011), which
makes a strong assumption that a tweet only
covers one topic.
• TwitterUB-LDA (Sasaki et al., 2014), an im-
proved version of Twitter-LDA, which mod-
els the user level preference between topical
words and background words.
• BTM (Yan et al., 2013), the Biterm Topic
Model.
• BTM-U, a simplified version of Twitter-BTM
without background topic.
</listItem>
<bodyText confidence="0.998540666666667">
For all the above models, we use symmetric
Dirichlet priors. The hyperparameters are set as
follows: for all the models, we set α = 50/K,
Q = 0.01; for Twitter-LDA, TwitterUB-LDA and
Twitter-BTM, we set -y = 0.5. We run Gibbs
sampling for 400 iterations.
</bodyText>
<table confidence="0.9985782">
DataSet Twitter
#tweets 1,201,193
#users 12,006
#vocabulary 71,038
#avgTweetLen 7.04
</table>
<tableCaption confidence="0.999695">
Table 1: Summary of dataset
</tableCaption>
<bodyText confidence="0.984064142857143">
Perplexity metric is not used in our experiments
since it is not a suitable evaluation metric for BTM
(Cheng et al., 2014). The first reason is that
BTM and LDA optimize different likelihood. The
second reason is that topic models which have bet-
ter perplexity may infer less semantically topics
(Chang et al., 2009).
</bodyText>
<subsectionHeader confidence="0.960093">
4.1 Topic Coherence
</subsectionHeader>
<bodyText confidence="0.999732">
We use PMI-Score (Newman et al., 2010) to quan-
titatively evaluate the quality of topic component.
</bodyText>
<page confidence="0.989122">
491
</page>
<table confidence="0.99974225">
K 50 100
method Top5 Top10 Top20 Top5 Top10 Top20
LDA-U 2.83±0.07 1.93±0.06 1.40±0.04 3.11±0.09 1.89±0.09 1.15±0.04
Twitter-LDA 2.58±0.04 1.90±0.03 1.39±0.03 2.97±0.20 1.98±0.09 1.44±0.06
TwitterUB-LDA 2.57±0.05 1.87±0.07 1.45±0.04 3.07±0.11 2.05±0.05 1.45±0.05
BTM 2.88±0.14 2.01±0.09 1.44±0.08 3.25±0.14 2.13±0.06 1.49±0.06
BTM-U 2.92±0.10 1.89±0.05 1.33±0.04 3.03±0.07 1.95±0.05 1.34±0.07
Twitter-BTM 3.04±0.10 2.05±0.08 1.47±0.05 3.27±0.12 2.15±0.08 1.48±0.05
</table>
<tableCaption confidence="0.999258">
Table 2: PMI-Score of different topic models
</tableCaption>
<bodyText confidence="0.857297">
Equation (4) defines PMI (Pointwise Mutual In-
formation) for two words wz and wj:
</bodyText>
<equation confidence="0.999798">
P(wi, wj) + �
PMI(wi, wj) = log (4)
P(wi)P(wj)
</equation>
<bodyText confidence="0.999014666666667">
c is an extremely small constant (Stevens et al.,
2012), which is equal to 10−12 in this paper. The
word probabilities and the co-occurrence proba-
bilities are computed on the large-scale external
dataset empirically. Here we use the second part
Twitter dataset as the external dataset. Then for a
topic t and its top T words ranked by topic-word
probability Otw, the PMI-Score of topic t is defined
as follow:
</bodyText>
<equation confidence="0.984325666666667">
PMI − Score(t) = T(T 1− 1) PMI (wi, wj)
1≤i&lt;j≤T
(5)
</equation>
<bodyText confidence="0.999937647058823">
The model’s PMI-Score is defined as the mean
of all the topics’ PMI-Score. Table 2 shows the
average results over 10 runs of different models.
When K = 50, Twitter-BTM outperforms all
other models significantly. When K = 100, The
PMI-Score of BTM and Twitter-BTM are very
close. BTM-U is worse than BTM, the reason may
be that each user’s biterm sets provide extremely
limited words co-occurring information.
Table 3 shows top 10 words of topic “food”
learned by BTM, BTM-U and Twitter-BTM when
K = 50. We use italic fonts to indicate back-
ground words labeled by human judgement. Com-
pared with BTM and BTM-U, Twitter-BTM can
rank those background words at lower level. It
demonstrates that representative words learned by
Twitter-BTM are more coherent and meaningful.
</bodyText>
<subsectionHeader confidence="0.990847">
4.2 Document Representation
</subsectionHeader>
<bodyText confidence="0.915389333333333">
Topic models are powerful dimension reduction
methods for texts. Given a tweet d, we can in-
fer its probability distribution over K topics with
</bodyText>
<table confidence="0.822155727272727">
BTM BTM-U Twitter-BTM
food food vegan
eat vegan food
chicken eat eat
good good chicken
vegan chicken chocolate
lol #vegan cheese
cheese cream cream
chocolate cheese #vegan
love chocolate ice
dinner ice dinner
</table>
<tableCaption confidence="0.997383">
Table 3: Top 10 words of topic food
</tableCaption>
<bodyText confidence="0.316676">
equation (1). Thus d can be represented as a topic
probability vector:
</bodyText>
<equation confidence="0.917299">
d = [P(z = 1|d), ..., P(z = K|d)] (6)
</equation>
<bodyText confidence="0.998224045454546">
We use document classification task (Cheng et
al., 2014) and document clustering task (Duan
et al., 2012) to measure the quality of the docu-
ments’ topic proportions. Tweets in Twitter have
no explicit label information. But some tweets
are labeled by one or more hashtags (a type of
label whose form is “#keyword”) manually by its
author to indicate the topic the tweets involve. We
follow previous works (Cheng et al., 2014; Wang
et al., 2014) and use hashtags as the tweets’ labels.
Table 4 lists 38 frequent (at least appears in 100
tweets ) hashtags relating to certain topic or event
manually selected in our dataset.
We choose those tweets which contain only one
of these hashtags appear in Table 4 from our o-
riginal data in the following experiments. When
we infer a tweet’s topic distribution, the hashtag is
ignored. Because it doesn’t make sense to use the
label information to construct the feature vector
directly.
We classify these selected tweets by Random
Forest classifier (Breiman, 2001) implemented in
</bodyText>
<page confidence="0.996868">
492
</page>
<bodyText confidence="0.999617285714286">
aaliyah afghanistan beatcancer birding
blogtalkradio digguser dmv dontyouhate fact
giladshalit gno gov green haiku healthcare
honduras india iranelection jazz jesus krp lgbt
mindsetshift nfl nn oink rhoa slaughterhouse
socialmedia tech travel trueblood vegan vegas
voss weeklyfitnesschallenge wordpress yyj
</bodyText>
<tableCaption confidence="0.991713">
Table 4: Hashtags selected for evaluation
</tableCaption>
<figureCaption confidence="0.99527">
Figure 2: Performance of classification
</figureCaption>
<bodyText confidence="0.999710708333334">
sklearn 2 python module with 10-fold cross valida-
tion. Using accuracy as the evaluation metric, we
report the classification performance of different
topic models in Figure 2. With the increase of
the topic number K, all the models’ accuracies
are tending to increase. BTM is worse than all
other models, which confirms the effectiveness of
user based aggregation. Twitter-BTM and BTM-
U always outperform LDA-U, Twitter-LDA and
TwitterUB-LDA. Twitter-BTM’s accuracy is a lit-
tle higher than BTM-U, which demonstrates that
the background topic is helpful to capture more
accurate topic representation of documents.
We adopt k-means algorithm implemented in
sklearn python module as our clustering method.
The number of cluster is set to 38. Consider-
ing we have the knowledge of ground truth class
assignments of each tweet, and Adjusted Rand
Index (ARI) and Normalized Mutual Information
are used as cluster validation indices in our exper-
iments. As shown in Figure 3 and Figure 4, The
higher ARI and NMI value indicate that Twitter-
BTM outperform other models. And BTM per-
forms worse than all other models.
</bodyText>
<sectionHeader confidence="0.994126" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.815032">
In this paper, we investigate the problem of topic
modeling over short texts with user factor. Us-
</bodyText>
<footnote confidence="0.997961">
2See http://scikit-learn.org/stable/
</footnote>
<figureCaption confidence="0.999995">
Figure 3: Performance of clustering (ARI)
Figure 4: Performance of clustering (NMI)
</figureCaption>
<bodyText confidence="0.999861071428572">
er individualities are sacrificed to obtain the cor-
pus level words co-occurrence patterns in BTM.
However, unlike LDA, simple user based aggre-
gation will reduce the topic coherence for BTM.
To address this problem, we propose Twitter-BTM
which loosens the inappropriate assumption that
two co-occurring words must have same topic la-
bel made in BTM by leveraging user based ag-
gregation and incorporating a background topic in
BTM. The experimental results show that Twitter-
BTM substantially outperforms BTM.
In the future, we plan to study the influence of
other factors such as temporal information to BTM
and its variants.
</bodyText>
<sectionHeader confidence="0.989773" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998504166666667">
This work is supported by 973 Program with
Grant No.2014CB340405, NSFC with Grant
No.61272340. Yan Zhang is supported by NSFC
with Grant No.61370054. We thank the three
anonymous reviewers for their comments and
constructive criticism.
</bodyText>
<figure confidence="0.999825849056604">
0.30
0.25
0.70
0.65
0.60
0.55
Accuracy
0.50
0.45
0.40
0.35
LOA-U
Twitter-LOA
TwitterUB-LOA
10 20 30 40 50 60 70 80 90 100
Number of Topics
BTM
Twitter-BTM
BTM-U
0.50
LOA-U
Twitter-LOA
TwitterUB-LOA
10 20 30 40 50 60 70 80 90 100
Number of Topics
ARI 0.45
0.40
0.35
0.30
0.25
0.20
0.15
0.10
BTM
Twitter-BTM
BTM-U
0.70
LDA-U
Twitter-LDA
TwitterUB-LDA
10 20 30 40 50 60 70 80 90 100
Number of Topics
NMI 0.65
0.60
0.55
0.50
0.45
0.40
0.35
0.30
BTM
Twitter-BTM
BTM-U
</figure>
<page confidence="0.99861">
493
</page>
<sectionHeader confidence="0.990132" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999018504672897">
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of
machine Learning research, 3:993–1022.
Leo Breiman. 2001. Random forests. Machine
Learning, 45(1):5–32.
Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L
Boyd-Graber, and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Advances in neural information processing systems,
pages 288–296.
Xueqi Cheng, Xiaohui Yan, Yanyan Lan, and Jiafeng
Guo. 2014. Btm: Topic modeling over short
texts. IEEE TRANSACTIONS ON KNOWLEDGE
AND DATA ENGINEERING.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng
Lim. 2012. Finding bursty topics from microblogs.
In ACL (1), pages 536–544. The Association for
Computer Linguistics.
Zhuoye Ding, Xipeng Qiu, Qi Zhang, and Xuanjing
Huang. 2013. Learning topical translation
model for microblog hashtag suggestion. In
IJCAI 2013, Proceedings of the 23rd International
Joint Conference on Artificial Intelligence, Beijing,
China, August 3-9, 2013.
Dongsheng Duan, Yuhua Li, Ruixuan Li, Rui Zhang,
and Aiming Wen. 2012. Ranktopic: Ranking based
topic modeling. In ICDM, pages 211–220.
Fr´ederic Godin, Viktor Slavkovikj, Wesley De Neve,
Benjamin Schrauwen, and Rik Van de Walle.
2013. Using topic models for twitter hashtag
recommendation. In Proceedings of the 22nd
international conference on World Wide Web com-
panion, pages 593–596. International World Wide
Web Conferences Steering Committee.
T. L. Griffiths and M. Steyvers. 2004. Finding
scientific topics. Proceedings of the National
Academy of Sciences, 101:5228–5235.
Liangjie Hong and Brian D. Davison. 2010. Empirical
study of topic modeling in twitter. In Proceedings
of the First Workshop on Social Media Analytics,
SOMA ’10, pages 80–88, New York, NY, USA.
ACM.
Zhuoye Ding Qi Zhang XuanJing Huang. 2012.
Automatic hashtag recommendation for microblogs
using topic-specific translation model. In 24th Inter-
national Conference on Computational Linguistics,
page 265. Citeseer.
Zongyang Ma, Aixin Sun, Quan Yuan, and Gao
Cong. 2014. Tagging your tweets: A probabilistic
modeling of hashtag annotation in twitter. In
Proceedings of the 23rd ACM International Confer-
ence on Conference on Information and Knowledge
Management, pages 999–1008. ACM.
David Newman, Jey Han Lau, Karl Grieser, and
Timothy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 100–108. Association for Com-
putational Linguistics.
Kentaro Sasaki, Tomohiro Yoshikawa, and Takeshi
Furuhashi. 2014. Online topic model for twitter
considering dynamics of user interests and topic
trends. In Proceedings of the 2014 Conference on
Empircal Methods in Natural Language Processing,
pages 1977–1985.
Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,
Huayi Li, and Xiaotie Deng. 2013. Exploiting topic
based twitter sentiment for stock prediction. In ACL
(2), pages 24–29.
Keith Stevens, Philip Kegelmeyer, David Andrzejew-
ski, and David Buttler. 2012. Exploring topic
coherence over many models and many topics.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning,
pages 952–961. Association for Computational
Linguistics.
Yuan Wang, Jie Liu, Jishi Qu, Yalou Huang, Jimeng
Chen, and Xia Feng. 2014. Hashtag graph based
topic model for tweet mining. In 2014 IEEE
International Conference on Data Mining, ICDM
2014, Shenzhen, China, December 14-17, 2014,
pages 1025–1030.
Jianshu Weng, Ee-Peng Lim, Jing Jiang, and Qi He.
2010. Twitterrank: finding topic-sensitive influen-
tial twitterers. In WSDM, pages 261–270. ACM.
Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi
Cheng. 2013. A biterm topic model for short texts.
In Proceedings of the 22nd international conference
on World Wide Web, pages 1445–1456. International
World Wide Web Conferences Steering Committee.
Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Jun Xu, and
Xueqi Cheng. 2015. A probabilistic model for
bursty topic discovery in microblogs.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,
Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic
models. In Advances in Information Retrieval,
pages 338–349. Springer.
Xin Wayne Zhao, Yanwei Guo, Yulan He, Han
Jiang, Yuexin Wu, and Xiaoming Li. 2014. We
know what you want to buy: a demographic-
based system for product recommendation on
microblogs. In Proceedings of the 20th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 1935–1944. ACM.
</reference>
<page confidence="0.998974">
494
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.909311">
<title confidence="0.995862">User Based Aggregation for Biterm Topic Model</title>
<author confidence="0.993785">Weizheng Chen</author>
<author confidence="0.993785">Jinpeng Wang</author>
<author confidence="0.993785">Yan Zhang</author>
<affiliation confidence="0.999997">School of Electronic Engineering and Computer Science, Peking University,</affiliation>
<email confidence="0.957277">zhy@cis.pku.edu.cn,lxm@pku.edu.cn</email>
<abstract confidence="0.99834936">Biterm Topic Model (BTM) is designed to model the generative process of the word co-occurrence patterns in short texts such as tweets. However, two aspects of BTM may restrict its performance: 1) user individualities are ignored to obtain the corpus level words co-occurrence patterns; and 2) the strong assumptions that two co-occurring words will be assigned the same topic label could not distinguish background words from topical words. In this paper, we propose Twitter-BTM model to address those issues by considering user level personalization in BTM. Firstly, we use user based biterms aggregation to learn user specific topic distribution. Secondly, each user’s preference between background words and topical words is estimated by incorporating a background topic. Experiments on a real-world Twitter dataset show that Twitter-BTM outperforms several stateof-the-art baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1477" citStr="Blei et al., 2003" startWordPosition="209" endWordPosition="212">ed biterms aggregation to learn user specific topic distribution. Secondly, each user’s preference between background words and topical words is estimated by incorporating a background topic. Experiments on a large-scale real-world Twitter dataset show that Twitter-BTM outperforms several stateof-the-art baselines. 1 Introduction In recent years, short texts are increasingly prevalent due to the explosive growth of online social media. For example, about 500 million tweets are published per day on Twitter1, one of the most popular online social networking services. Probabilistic topic models (Blei et al., 2003) are broadly used to uncover the hidden topics of tweets, since the low-dimensional semantic representation is crucial for many applications, such as product recommendation (Zhao et al., 2014), hashtag recommendation (Ma et al., 2014), user interest tracking (Sasaki et al., 2014), sentiment analysis 1See https://about.Twitter.com/company (Si et al., 2013). However, the scarcity of context and the noisy words restrict LDA and its variations in topic modeling over short texts. Previous works model topic distribution at three different levels for tweets: 1) document, the standard LDA assumes each</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="14436" citStr="Breiman, 2001" startWordPosition="2405" endWordPosition="2406">previous works (Cheng et al., 2014; Wang et al., 2014) and use hashtags as the tweets’ labels. Table 4 lists 38 frequent (at least appears in 100 tweets ) hashtags relating to certain topic or event manually selected in our dataset. We choose those tweets which contain only one of these hashtags appear in Table 4 from our original data in the following experiments. When we infer a tweet’s topic distribution, the hashtag is ignored. Because it doesn’t make sense to use the label information to construct the feature vector directly. We classify these selected tweets by Random Forest classifier (Breiman, 2001) implemented in 492 aaliyah afghanistan beatcancer birding blogtalkradio digguser dmv dontyouhate fact giladshalit gno gov green haiku healthcare honduras india iranelection jazz jesus krp lgbt mindsetshift nfl nn oink rhoa slaughterhouse socialmedia tech travel trueblood vegan vegas voss weeklyfitnesschallenge wordpress yyj Table 4: Hashtags selected for evaluation Figure 2: Performance of classification sklearn 2 python module with 10-fold cross validation. Using accuracy as the evaluation metric, we report the classification performance of different topic models in Figure 2. With the increa</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>Jordan L Boyd-Graber</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models. In Advances in neural information processing systems,</title>
<date>2009</date>
<pages>288--296</pages>
<contexts>
<context position="10916" citStr="Chang et al., 2009" startWordPosition="1831" endWordPosition="1834"> hyperparameters are set as follows: for all the models, we set α = 50/K, Q = 0.01; for Twitter-LDA, TwitterUB-LDA and Twitter-BTM, we set -y = 0.5. We run Gibbs sampling for 400 iterations. DataSet Twitter #tweets 1,201,193 #users 12,006 #vocabulary 71,038 #avgTweetLen 7.04 Table 1: Summary of dataset Perplexity metric is not used in our experiments since it is not a suitable evaluation metric for BTM (Cheng et al., 2014). The first reason is that BTM and LDA optimize different likelihood. The second reason is that topic models which have better perplexity may infer less semantically topics (Chang et al., 2009). 4.1 Topic Coherence We use PMI-Score (Newman et al., 2010) to quantitatively evaluate the quality of topic component. 491 K 50 100 method Top5 Top10 Top20 Top5 Top10 Top20 LDA-U 2.83±0.07 1.93±0.06 1.40±0.04 3.11±0.09 1.89±0.09 1.15±0.04 Twitter-LDA 2.58±0.04 1.90±0.03 1.39±0.03 2.97±0.20 1.98±0.09 1.44±0.06 TwitterUB-LDA 2.57±0.05 1.87±0.07 1.45±0.04 3.07±0.11 2.05±0.05 1.45±0.05 BTM 2.88±0.14 2.01±0.09 1.44±0.08 3.25±0.14 2.13±0.06 1.49±0.06 BTM-U 2.92±0.10 1.89±0.05 1.33±0.04 3.03±0.07 1.95±0.05 1.34±0.07 Twitter-BTM 3.04±0.10 2.05±0.08 1.47±0.05 3.27±0.12 2.15±0.08 1.48±0.05 Table 2: PMI</context>
</contexts>
<marker>Chang, Gerrish, Wang, Boyd-Graber, Blei, 2009</marker>
<rawString>Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L Boyd-Graber, and David M Blei. 2009. Reading tea leaves: How humans interpret topic models. In Advances in neural information processing systems, pages 288–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xueqi Cheng</author>
<author>Xiaohui Yan</author>
<author>Yanyan Lan</author>
<author>Jiafeng Guo</author>
</authors>
<title>Btm: Topic modeling over short texts.</title>
<date>2014</date>
<journal>IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING.</journal>
<contexts>
<context position="10723" citStr="Cheng et al., 2014" startWordPosition="1799" endWordPosition="1802"> words. • BTM (Yan et al., 2013), the Biterm Topic Model. • BTM-U, a simplified version of Twitter-BTM without background topic. For all the above models, we use symmetric Dirichlet priors. The hyperparameters are set as follows: for all the models, we set α = 50/K, Q = 0.01; for Twitter-LDA, TwitterUB-LDA and Twitter-BTM, we set -y = 0.5. We run Gibbs sampling for 400 iterations. DataSet Twitter #tweets 1,201,193 #users 12,006 #vocabulary 71,038 #avgTweetLen 7.04 Table 1: Summary of dataset Perplexity metric is not used in our experiments since it is not a suitable evaluation metric for BTM (Cheng et al., 2014). The first reason is that BTM and LDA optimize different likelihood. The second reason is that topic models which have better perplexity may infer less semantically topics (Chang et al., 2009). 4.1 Topic Coherence We use PMI-Score (Newman et al., 2010) to quantitatively evaluate the quality of topic component. 491 K 50 100 method Top5 Top10 Top20 Top5 Top10 Top20 LDA-U 2.83±0.07 1.93±0.06 1.40±0.04 3.11±0.09 1.89±0.09 1.15±0.04 Twitter-LDA 2.58±0.04 1.90±0.03 1.39±0.03 2.97±0.20 1.98±0.09 1.44±0.06 TwitterUB-LDA 2.57±0.05 1.87±0.07 1.45±0.04 3.07±0.11 2.05±0.05 1.45±0.05 BTM 2.88±0.14 2.01±0.</context>
<context position="13488" citStr="Cheng et al., 2014" startWordPosition="2243" endWordPosition="2246">BTM are more coherent and meaningful. 4.2 Document Representation Topic models are powerful dimension reduction methods for texts. Given a tweet d, we can infer its probability distribution over K topics with BTM BTM-U Twitter-BTM food food vegan eat vegan food chicken eat eat good good chicken vegan chicken chocolate lol #vegan cheese cheese cream cream chocolate cheese #vegan love chocolate ice dinner ice dinner Table 3: Top 10 words of topic food equation (1). Thus d can be represented as a topic probability vector: d = [P(z = 1|d), ..., P(z = K|d)] (6) We use document classification task (Cheng et al., 2014) and document clustering task (Duan et al., 2012) to measure the quality of the documents’ topic proportions. Tweets in Twitter have no explicit label information. But some tweets are labeled by one or more hashtags (a type of label whose form is “#keyword”) manually by its author to indicate the topic the tweets involve. We follow previous works (Cheng et al., 2014; Wang et al., 2014) and use hashtags as the tweets’ labels. Table 4 lists 38 frequent (at least appears in 100 tweets ) hashtags relating to certain topic or event manually selected in our dataset. We choose those tweets which cont</context>
</contexts>
<marker>Cheng, Yan, Lan, Guo, 2014</marker>
<rawString>Xueqi Cheng, Xiaohui Yan, Yanyan Lan, and Jiafeng Guo. 2014. Btm: Topic modeling over short texts. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiming Diao</author>
<author>Jing Jiang</author>
<author>Feida Zhu</author>
<author>Ee-Peng Lim</author>
</authors>
<title>Finding bursty topics from microblogs.</title>
<date>2012</date>
<journal>In ACL</journal>
<volume>1</volume>
<pages>536--544</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="6404" citStr="Diao et al., 2012" startWordPosition="1012" endWordPosition="1015"> ..., bNB}, where bi = (wi1, wi2), is as follows: 1 For each topic t=1,...,T (a) Draw φt — Dir(Q) 2 For the whole tweets collection (a) Draw θ — Dir(α) 3 For each biterm b = 1,...,NB (a) Draw zb — Multi(θ) (b) Draw wb,1, wb,2 — Multi(φzb) In the above process, zb is the topic assignment latent variable of biterm b. To infer the parameters φ and θ, collapsed Gibbs sampling Figure 1: Graphical representation of (a) BTM, (b) Twitter-BTM algorithm (Griffiths and Steyvers, 2004) is used for approximate inference. Compared with the strong assumption that a short document only covers a single topic (Diao et al., 2012; Ding et al., 2013), BTM makes a looser assumption that two words will be assigned the same topic label if they have co-occurred. Thus a short document could cover more than one topic, which is more close to the reality. But this assumption causes another issue, those commonly used words and those topical words are treated equally. Obviously it is inappropriate to assign same topic label to those words. 3 Twitter-BTM In this Section, we introduce our Twitter-BTM model. Figure 1(b) shows the graphical representation of Twitter-BTM. The generative process of Twitter-BTM is as follows: 1 Draw φB</context>
</contexts>
<marker>Diao, Jiang, Zhu, Lim, 2012</marker>
<rawString>Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim. 2012. Finding bursty topics from microblogs. In ACL (1), pages 536–544. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoye Ding</author>
<author>Xipeng Qiu</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
</authors>
<title>Learning topical translation model for microblog hashtag suggestion.</title>
<date>2013</date>
<booktitle>In IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence,</booktitle>
<location>Beijing, China,</location>
<contexts>
<context position="6424" citStr="Ding et al., 2013" startWordPosition="1016" endWordPosition="1019">i = (wi1, wi2), is as follows: 1 For each topic t=1,...,T (a) Draw φt — Dir(Q) 2 For the whole tweets collection (a) Draw θ — Dir(α) 3 For each biterm b = 1,...,NB (a) Draw zb — Multi(θ) (b) Draw wb,1, wb,2 — Multi(φzb) In the above process, zb is the topic assignment latent variable of biterm b. To infer the parameters φ and θ, collapsed Gibbs sampling Figure 1: Graphical representation of (a) BTM, (b) Twitter-BTM algorithm (Griffiths and Steyvers, 2004) is used for approximate inference. Compared with the strong assumption that a short document only covers a single topic (Diao et al., 2012; Ding et al., 2013), BTM makes a looser assumption that two words will be assigned the same topic label if they have co-occurred. Thus a short document could cover more than one topic, which is more close to the reality. But this assumption causes another issue, those commonly used words and those topical words are treated equally. Obviously it is inappropriate to assign same topic label to those words. 3 Twitter-BTM In this Section, we introduce our Twitter-BTM model. Figure 1(b) shows the graphical representation of Twitter-BTM. The generative process of Twitter-BTM is as follows: 1 Draw φB — Dir(Q) 2 For each</context>
</contexts>
<marker>Ding, Qiu, Zhang, Huang, 2013</marker>
<rawString>Zhuoye Ding, Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013. Learning topical translation model for microblog hashtag suggestion. In IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence, Beijing, China, August 3-9, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongsheng Duan</author>
<author>Yuhua Li</author>
<author>Ruixuan Li</author>
<author>Rui Zhang</author>
<author>Aiming Wen</author>
</authors>
<title>Ranktopic: Ranking based topic modeling.</title>
<date>2012</date>
<booktitle>In ICDM,</booktitle>
<pages>211--220</pages>
<contexts>
<context position="13537" citStr="Duan et al., 2012" startWordPosition="2251" endWordPosition="2254"> Representation Topic models are powerful dimension reduction methods for texts. Given a tweet d, we can infer its probability distribution over K topics with BTM BTM-U Twitter-BTM food food vegan eat vegan food chicken eat eat good good chicken vegan chicken chocolate lol #vegan cheese cheese cream cream chocolate cheese #vegan love chocolate ice dinner ice dinner Table 3: Top 10 words of topic food equation (1). Thus d can be represented as a topic probability vector: d = [P(z = 1|d), ..., P(z = K|d)] (6) We use document classification task (Cheng et al., 2014) and document clustering task (Duan et al., 2012) to measure the quality of the documents’ topic proportions. Tweets in Twitter have no explicit label information. But some tweets are labeled by one or more hashtags (a type of label whose form is “#keyword”) manually by its author to indicate the topic the tweets involve. We follow previous works (Cheng et al., 2014; Wang et al., 2014) and use hashtags as the tweets’ labels. Table 4 lists 38 frequent (at least appears in 100 tweets ) hashtags relating to certain topic or event manually selected in our dataset. We choose those tweets which contain only one of these hashtags appear in Table 4 </context>
</contexts>
<marker>Duan, Li, Li, Zhang, Wen, 2012</marker>
<rawString>Dongsheng Duan, Yuhua Li, Ruixuan Li, Rui Zhang, and Aiming Wen. 2012. Ranktopic: Ranking based topic modeling. In ICDM, pages 211–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fr´ederic Godin</author>
<author>Viktor Slavkovikj</author>
<author>Wesley De Neve</author>
<author>Benjamin Schrauwen</author>
<author>Rik Van de Walle</author>
</authors>
<title>Using topic models for twitter hashtag recommendation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd international conference on World Wide Web companion,</booktitle>
<pages>593--596</pages>
<institution>International World Wide Web Conferences Steering Committee.</institution>
<marker>Godin, Slavkovikj, De Neve, Schrauwen, Van de Walle, 2013</marker>
<rawString>Fr´ederic Godin, Viktor Slavkovikj, Wesley De Neve, Benjamin Schrauwen, and Rik Van de Walle. 2013. Using topic models for twitter hashtag recommendation. In Proceedings of the 22nd international conference on World Wide Web companion, pages 593–596. International World Wide Web Conferences Steering Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>101--5228</pages>
<contexts>
<context position="6265" citStr="Griffiths and Steyvers, 2004" startWordPosition="989" endWordPosition="992">n φt over a vocabulary of V unique words drawn from a Dirichlet prior Dir(Q). The generative process for a corpus which consists of NB biterms B = {b1, ..., bNB}, where bi = (wi1, wi2), is as follows: 1 For each topic t=1,...,T (a) Draw φt — Dir(Q) 2 For the whole tweets collection (a) Draw θ — Dir(α) 3 For each biterm b = 1,...,NB (a) Draw zb — Multi(θ) (b) Draw wb,1, wb,2 — Multi(φzb) In the above process, zb is the topic assignment latent variable of biterm b. To infer the parameters φ and θ, collapsed Gibbs sampling Figure 1: Graphical representation of (a) BTM, (b) Twitter-BTM algorithm (Griffiths and Steyvers, 2004) is used for approximate inference. Compared with the strong assumption that a short document only covers a single topic (Diao et al., 2012; Ding et al., 2013), BTM makes a looser assumption that two words will be assigned the same topic label if they have co-occurred. Thus a short document could cover more than one topic, which is more close to the reality. But this assumption causes another issue, those commonly used words and those topical words are treated equally. Obviously it is inappropriate to assign same topic label to those words. 3 Twitter-BTM In this Section, we introduce our Twitt</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101:5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangjie Hong</author>
<author>Brian D Davison</author>
</authors>
<title>Empirical study of topic modeling in twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the First Workshop on Social Media Analytics, SOMA ’10,</booktitle>
<pages>80--88</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2365" citStr="Hong and Davison, 2010" startWordPosition="345" endWordPosition="348">i et al., 2014), sentiment analysis 1See https://about.Twitter.com/company (Si et al., 2013). However, the scarcity of context and the noisy words restrict LDA and its variations in topic modeling over short texts. Previous works model topic distribution at three different levels for tweets: 1) document, the standard LDA assumes each document is associated with a topic distribution (Godin et al., 2013; Huang, 2012). LDA and its variations suffer from context sparsity in each tweet. 2) user, user based aggregation is utilized to alleviate the sparsity problem in short texts (Weng et al., 2010; Hong and Davison, 2010). In these models, all the tweets of the same user are aggregated together as a pseudo document based on the observation that the tweets written by the same user are more similar. 3) corpus, BTM (Yan et al., 2013) assumes that all the biterms (co-occurring word pairs) are generated by a corpus level topic distribution to benefit from the global rich word co-occurrence patterns. As far as we know, how to incorporate user factor into BTM has not been studied yet. User based aggregation has proven effective for LDA. But unfortunately, our preliminary experiments indicate that simple user-based ag</context>
</contexts>
<marker>Hong, Davison, 2010</marker>
<rawString>Liangjie Hong and Brian D. Davison. 2010. Empirical study of topic modeling in twitter. In Proceedings of the First Workshop on Social Media Analytics, SOMA ’10, pages 80–88, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<title>Zhuoye Ding Qi Zhang XuanJing Huang.</title>
<date>2012</date>
<booktitle>In 24th International Conference on Computational Linguistics,</booktitle>
<pages>265</pages>
<publisher>Citeseer.</publisher>
<marker>2012</marker>
<rawString>Zhuoye Ding Qi Zhang XuanJing Huang. 2012. Automatic hashtag recommendation for microblogs using topic-specific translation model. In 24th International Conference on Computational Linguistics, page 265. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zongyang Ma</author>
<author>Aixin Sun</author>
<author>Quan Yuan</author>
<author>Gao Cong</author>
</authors>
<title>Tagging your tweets: A probabilistic modeling of hashtag annotation in twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,</booktitle>
<pages>999--1008</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1711" citStr="Ma et al., 2014" startWordPosition="245" endWordPosition="248">ter dataset show that Twitter-BTM outperforms several stateof-the-art baselines. 1 Introduction In recent years, short texts are increasingly prevalent due to the explosive growth of online social media. For example, about 500 million tweets are published per day on Twitter1, one of the most popular online social networking services. Probabilistic topic models (Blei et al., 2003) are broadly used to uncover the hidden topics of tweets, since the low-dimensional semantic representation is crucial for many applications, such as product recommendation (Zhao et al., 2014), hashtag recommendation (Ma et al., 2014), user interest tracking (Sasaki et al., 2014), sentiment analysis 1See https://about.Twitter.com/company (Si et al., 2013). However, the scarcity of context and the noisy words restrict LDA and its variations in topic modeling over short texts. Previous works model topic distribution at three different levels for tweets: 1) document, the standard LDA assumes each document is associated with a topic distribution (Godin et al., 2013; Huang, 2012). LDA and its variations suffer from context sparsity in each tweet. 2) user, user based aggregation is utilized to alleviate the sparsity problem in s</context>
</contexts>
<marker>Ma, Sun, Yuan, Cong, 2014</marker>
<rawString>Zongyang Ma, Aixin Sun, Quan Yuan, and Gao Cong. 2014. Tagging your tweets: A probabilistic modeling of hashtag annotation in twitter. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 999–1008. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>100--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10976" citStr="Newman et al., 2010" startWordPosition="1841" endWordPosition="1844"> set α = 50/K, Q = 0.01; for Twitter-LDA, TwitterUB-LDA and Twitter-BTM, we set -y = 0.5. We run Gibbs sampling for 400 iterations. DataSet Twitter #tweets 1,201,193 #users 12,006 #vocabulary 71,038 #avgTweetLen 7.04 Table 1: Summary of dataset Perplexity metric is not used in our experiments since it is not a suitable evaluation metric for BTM (Cheng et al., 2014). The first reason is that BTM and LDA optimize different likelihood. The second reason is that topic models which have better perplexity may infer less semantically topics (Chang et al., 2009). 4.1 Topic Coherence We use PMI-Score (Newman et al., 2010) to quantitatively evaluate the quality of topic component. 491 K 50 100 method Top5 Top10 Top20 Top5 Top10 Top20 LDA-U 2.83±0.07 1.93±0.06 1.40±0.04 3.11±0.09 1.89±0.09 1.15±0.04 Twitter-LDA 2.58±0.04 1.90±0.03 1.39±0.03 2.97±0.20 1.98±0.09 1.44±0.06 TwitterUB-LDA 2.57±0.05 1.87±0.07 1.45±0.04 3.07±0.11 2.05±0.05 1.45±0.05 BTM 2.88±0.14 2.01±0.09 1.44±0.08 3.25±0.14 2.13±0.06 1.49±0.06 BTM-U 2.92±0.10 1.89±0.05 1.33±0.04 3.03±0.07 1.95±0.05 1.34±0.07 Twitter-BTM 3.04±0.10 2.05±0.08 1.47±0.05 3.27±0.12 2.15±0.08 1.48±0.05 Table 2: PMI-Score of different topic models Equation (4) defines PMI (P</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 100–108. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Sasaki</author>
<author>Tomohiro Yoshikawa</author>
<author>Takeshi Furuhashi</author>
</authors>
<title>Online topic model for twitter considering dynamics of user interests and topic trends.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empircal Methods in Natural Language Processing,</booktitle>
<pages>1977--1985</pages>
<contexts>
<context position="1757" citStr="Sasaki et al., 2014" startWordPosition="252" endWordPosition="255">rms several stateof-the-art baselines. 1 Introduction In recent years, short texts are increasingly prevalent due to the explosive growth of online social media. For example, about 500 million tweets are published per day on Twitter1, one of the most popular online social networking services. Probabilistic topic models (Blei et al., 2003) are broadly used to uncover the hidden topics of tweets, since the low-dimensional semantic representation is crucial for many applications, such as product recommendation (Zhao et al., 2014), hashtag recommendation (Ma et al., 2014), user interest tracking (Sasaki et al., 2014), sentiment analysis 1See https://about.Twitter.com/company (Si et al., 2013). However, the scarcity of context and the noisy words restrict LDA and its variations in topic modeling over short texts. Previous works model topic distribution at three different levels for tweets: 1) document, the standard LDA assumes each document is associated with a topic distribution (Godin et al., 2013; Huang, 2012). LDA and its variations suffer from context sparsity in each tweet. 2) user, user based aggregation is utilized to alleviate the sparsity problem in short texts (Weng et al., 2010; Hong and Daviso</context>
<context position="3314" citStr="Sasaki et al. (2014)" startWordPosition="501" endWordPosition="504">t from the global rich word co-occurrence patterns. As far as we know, how to incorporate user factor into BTM has not been studied yet. User based aggregation has proven effective for LDA. But unfortunately, our preliminary experiments indicate that simple user-based aggregation for BTM will generate incoherent topics. To distinguish between commonly used words (e.g., good, people, etc) and topical words (e.g., food, travel, etc), a background topic is often incorporated into the topic models. Zhao et al. (2011) use a background topic in Twitter-LDA to distill discriminative words in tweets. Sasaki et al. (2014) reduce the perplexity of Twitter-LDA by estimating the ratio between choosing background words and topical words for each user. They both make a very strong assumption that one tweet only covers one topic. Yan et al. (2015) use a background topic to distinguish between common biterms and bursty biterms, which need external data to evaluate the burstiness of each biterm as prior knowledge. Unlike those above, we incorporate a background 489 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Proc</context>
<context position="7788" citStr="Sasaki et al., 2014" startWordPosition="1273" endWordPosition="1276">,b — Multi(θu) (ii) For each word n = 1,2 (A) Draw yu,b,n — Bern(7ru) (B) if yu,b,n = 0 Draw wu,b,n — Multi(φB) if yu,b,n = Multi(φzu,b) (a) BTM (b) Twitter-BTM y w π γ 0k e z α K 2 Nu U 0B P 0k w z α e K 2 NB P 1 Draw wu,b,n — 490 In the above process, user u’s topic interest Ou is a multinomial distribution over K topics drawn from a Dirichlet prior Dir(α). The background topic B is associated with a multinomial distribution φB drawn from a Dirichlet prior Dir(Q). The assumption that each user has a different preference between topical words and background words is shown to be effective in (Sasaki et al., 2014). We adopt this assumption in Twitter-BTM. User u’s preference is represented as a Bernoulli distribution with parameter Tru drawn from a beta prior Beta(-y). Nu is the number of biterms of user u, zu,b is the topic assignment latent variable of user u’s biterm b. For user u and his/her biterm b, n=1 or 2, we use a latent variable yu,b,n to indicate the word type of the word wb,n. When yu,b,n = 1, wb,n is generated from topic zu,b. When yu,b,n = 0, wb,n is generated from the background topic B. We adopt collapsed Gibbs Sampling to estimate the parameters. Because of the limitations of space, w</context>
<context position="9991" citStr="Sasaki et al., 2014" startWordPosition="1678" endWordPosition="1681">oved. We also filter tweets which only have one or two words. All letters are converted into lower case. The dataset is divided into two parts. The first part whose statistics is shown in Table 1 is used for training. The second part which consists of 22,496,107 tweets is used as the external dataset in topic coherence evaluation task in Section 4.1. We compare the performance of Twitter-BTM with five baselines: • LDA-U, user based aggregation is applied before training LDA. • Twitter-LDA (Zhao et al., 2011), which makes a strong assumption that a tweet only covers one topic. • TwitterUB-LDA (Sasaki et al., 2014), an improved version of Twitter-LDA, which models the user level preference between topical words and background words. • BTM (Yan et al., 2013), the Biterm Topic Model. • BTM-U, a simplified version of Twitter-BTM without background topic. For all the above models, we use symmetric Dirichlet priors. The hyperparameters are set as follows: for all the models, we set α = 50/K, Q = 0.01; for Twitter-LDA, TwitterUB-LDA and Twitter-BTM, we set -y = 0.5. We run Gibbs sampling for 400 iterations. DataSet Twitter #tweets 1,201,193 #users 12,006 #vocabulary 71,038 #avgTweetLen 7.04 Table 1: Summary o</context>
</contexts>
<marker>Sasaki, Yoshikawa, Furuhashi, 2014</marker>
<rawString>Kentaro Sasaki, Tomohiro Yoshikawa, and Takeshi Furuhashi. 2014. Online topic model for twitter considering dynamics of user interests and topic trends. In Proceedings of the 2014 Conference on Empircal Methods in Natural Language Processing, pages 1977–1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Si</author>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
<author>Qing Li</author>
<author>Huayi Li</author>
<author>Xiaotie Deng</author>
</authors>
<title>Exploiting topic based twitter sentiment for stock prediction.</title>
<date>2013</date>
<booktitle>In ACL (2),</booktitle>
<pages>24--29</pages>
<contexts>
<context position="1834" citStr="Si et al., 2013" startWordPosition="260" endWordPosition="263">s are increasingly prevalent due to the explosive growth of online social media. For example, about 500 million tweets are published per day on Twitter1, one of the most popular online social networking services. Probabilistic topic models (Blei et al., 2003) are broadly used to uncover the hidden topics of tweets, since the low-dimensional semantic representation is crucial for many applications, such as product recommendation (Zhao et al., 2014), hashtag recommendation (Ma et al., 2014), user interest tracking (Sasaki et al., 2014), sentiment analysis 1See https://about.Twitter.com/company (Si et al., 2013). However, the scarcity of context and the noisy words restrict LDA and its variations in topic modeling over short texts. Previous works model topic distribution at three different levels for tweets: 1) document, the standard LDA assumes each document is associated with a topic distribution (Godin et al., 2013; Huang, 2012). LDA and its variations suffer from context sparsity in each tweet. 2) user, user based aggregation is utilized to alleviate the sparsity problem in short texts (Weng et al., 2010; Hong and Davison, 2010). In these models, all the tweets of the same user are aggregated tog</context>
</contexts>
<marker>Si, Mukherjee, Liu, Li, Li, Deng, 2013</marker>
<rawString>Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, Huayi Li, and Xiaotie Deng. 2013. Exploiting topic based twitter sentiment for stock prediction. In ACL (2), pages 24–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Stevens</author>
<author>Philip Kegelmeyer</author>
<author>David Andrzejewski</author>
<author>David Buttler</author>
</authors>
<title>Exploring topic coherence over many models and many topics.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>952--961</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11732" citStr="Stevens et al., 2012" startWordPosition="1944" endWordPosition="1947">0.06 1.40±0.04 3.11±0.09 1.89±0.09 1.15±0.04 Twitter-LDA 2.58±0.04 1.90±0.03 1.39±0.03 2.97±0.20 1.98±0.09 1.44±0.06 TwitterUB-LDA 2.57±0.05 1.87±0.07 1.45±0.04 3.07±0.11 2.05±0.05 1.45±0.05 BTM 2.88±0.14 2.01±0.09 1.44±0.08 3.25±0.14 2.13±0.06 1.49±0.06 BTM-U 2.92±0.10 1.89±0.05 1.33±0.04 3.03±0.07 1.95±0.05 1.34±0.07 Twitter-BTM 3.04±0.10 2.05±0.08 1.47±0.05 3.27±0.12 2.15±0.08 1.48±0.05 Table 2: PMI-Score of different topic models Equation (4) defines PMI (Pointwise Mutual Information) for two words wz and wj: P(wi, wj) + � PMI(wi, wj) = log (4) P(wi)P(wj) c is an extremely small constant (Stevens et al., 2012), which is equal to 10−12 in this paper. The word probabilities and the co-occurrence probabilities are computed on the large-scale external dataset empirically. Here we use the second part Twitter dataset as the external dataset. Then for a topic t and its top T words ranked by topic-word probability Otw, the PMI-Score of topic t is defined as follow: PMI − Score(t) = T(T 1− 1) PMI (wi, wj) 1≤i&lt;j≤T (5) The model’s PMI-Score is defined as the mean of all the topics’ PMI-Score. Table 2 shows the average results over 10 runs of different models. When K = 50, Twitter-BTM outperforms all other mod</context>
</contexts>
<marker>Stevens, Kegelmeyer, Andrzejewski, Buttler, 2012</marker>
<rawString>Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler. 2012. Exploring topic coherence over many models and many topics. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 952–961. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Wang</author>
<author>Jie Liu</author>
<author>Jishi Qu</author>
<author>Yalou Huang</author>
<author>Jimeng Chen</author>
<author>Xia Feng</author>
</authors>
<title>Hashtag graph based topic model for tweet mining.</title>
<date>2014</date>
<booktitle>In 2014 IEEE International Conference on Data Mining, ICDM 2014,</booktitle>
<pages>1025--1030</pages>
<location>Shenzhen, China,</location>
<contexts>
<context position="13876" citStr="Wang et al., 2014" startWordPosition="2310" endWordPosition="2313">colate ice dinner ice dinner Table 3: Top 10 words of topic food equation (1). Thus d can be represented as a topic probability vector: d = [P(z = 1|d), ..., P(z = K|d)] (6) We use document classification task (Cheng et al., 2014) and document clustering task (Duan et al., 2012) to measure the quality of the documents’ topic proportions. Tweets in Twitter have no explicit label information. But some tweets are labeled by one or more hashtags (a type of label whose form is “#keyword”) manually by its author to indicate the topic the tweets involve. We follow previous works (Cheng et al., 2014; Wang et al., 2014) and use hashtags as the tweets’ labels. Table 4 lists 38 frequent (at least appears in 100 tweets ) hashtags relating to certain topic or event manually selected in our dataset. We choose those tweets which contain only one of these hashtags appear in Table 4 from our original data in the following experiments. When we infer a tweet’s topic distribution, the hashtag is ignored. Because it doesn’t make sense to use the label information to construct the feature vector directly. We classify these selected tweets by Random Forest classifier (Breiman, 2001) implemented in 492 aaliyah afghanistan </context>
</contexts>
<marker>Wang, Liu, Qu, Huang, Chen, Feng, 2014</marker>
<rawString>Yuan Wang, Jie Liu, Jishi Qu, Yalou Huang, Jimeng Chen, and Xia Feng. 2014. Hashtag graph based topic model for tweet mining. In 2014 IEEE International Conference on Data Mining, ICDM 2014, Shenzhen, China, December 14-17, 2014, pages 1025–1030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianshu Weng</author>
<author>Ee-Peng Lim</author>
<author>Jing Jiang</author>
<author>Qi He</author>
</authors>
<title>Twitterrank: finding topic-sensitive influential twitterers.</title>
<date>2010</date>
<booktitle>In WSDM,</booktitle>
<pages>261--270</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2340" citStr="Weng et al., 2010" startWordPosition="341" endWordPosition="344">est tracking (Sasaki et al., 2014), sentiment analysis 1See https://about.Twitter.com/company (Si et al., 2013). However, the scarcity of context and the noisy words restrict LDA and its variations in topic modeling over short texts. Previous works model topic distribution at three different levels for tweets: 1) document, the standard LDA assumes each document is associated with a topic distribution (Godin et al., 2013; Huang, 2012). LDA and its variations suffer from context sparsity in each tweet. 2) user, user based aggregation is utilized to alleviate the sparsity problem in short texts (Weng et al., 2010; Hong and Davison, 2010). In these models, all the tweets of the same user are aggregated together as a pseudo document based on the observation that the tweets written by the same user are more similar. 3) corpus, BTM (Yan et al., 2013) assumes that all the biterms (co-occurring word pairs) are generated by a corpus level topic distribution to benefit from the global rich word co-occurrence patterns. As far as we know, how to incorporate user factor into BTM has not been studied yet. User based aggregation has proven effective for LDA. But unfortunately, our preliminary experiments indicate </context>
</contexts>
<marker>Weng, Lim, Jiang, He, 2010</marker>
<rawString>Jianshu Weng, Ee-Peng Lim, Jing Jiang, and Qi He. 2010. Twitterrank: finding topic-sensitive influential twitterers. In WSDM, pages 261–270. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohui Yan</author>
<author>Jiafeng Guo</author>
<author>Yanyan Lan</author>
<author>Xueqi Cheng</author>
</authors>
<title>A biterm topic model for short texts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd international conference on World Wide Web,</booktitle>
<pages>1445--1456</pages>
<institution>International World Wide Web Conferences Steering Committee.</institution>
<contexts>
<context position="2578" citStr="Yan et al., 2013" startWordPosition="384" endWordPosition="387">ious works model topic distribution at three different levels for tweets: 1) document, the standard LDA assumes each document is associated with a topic distribution (Godin et al., 2013; Huang, 2012). LDA and its variations suffer from context sparsity in each tweet. 2) user, user based aggregation is utilized to alleviate the sparsity problem in short texts (Weng et al., 2010; Hong and Davison, 2010). In these models, all the tweets of the same user are aggregated together as a pseudo document based on the observation that the tweets written by the same user are more similar. 3) corpus, BTM (Yan et al., 2013) assumes that all the biterms (co-occurring word pairs) are generated by a corpus level topic distribution to benefit from the global rich word co-occurrence patterns. As far as we know, how to incorporate user factor into BTM has not been studied yet. User based aggregation has proven effective for LDA. But unfortunately, our preliminary experiments indicate that simple user-based aggregation for BTM will generate incoherent topics. To distinguish between commonly used words (e.g., good, people, etc) and topical words (e.g., food, travel, etc), a background topic is often incorporated into th</context>
<context position="4874" citStr="Yan et al., 2013" startWordPosition="742" endWordPosition="745">n and the background topic in BTM. Finally, experiments on a Twitter dataset show that Twitter-BTM not only can discover more coherent topics but also can give more accurate topic representation of tweets compared with several state-of-the-art baselines. We organize the rest of the paper as follows. Section 2 gives a brief review for BTM. Section 3 introduces our Twitter-BTM model and its implementation. Section 4 describes experimental results on a large-scale Twitter dataset. Finally, Section 5 contains a conclusion and future work. 2 BTM There are two major differences between BTM and LDA (Yan et al., 2013). For one thing, considering a topic is a mixture of highly correlated words, which implies that they often occur together in the same document, BTM models the generative process of the word co-occurrence patterns directly. Thus a document made up of n words will be converted to C2n biterms. For another, LDA and its variants suffer from the severe data sparsity in short documents. BTM uses global co-occurrence patterns to model the topic distribution over corpus level instead of document level. The graphical representation of BTM (Yan et al., 2013) is shown in Figure 1(a). It assumes that the </context>
<context position="8605" citStr="Yan et al., 2013" startWordPosition="1417" endWordPosition="1420">,b is the topic assignment latent variable of user u’s biterm b. For user u and his/her biterm b, n=1 or 2, we use a latent variable yu,b,n to indicate the word type of the word wb,n. When yu,b,n = 1, wb,n is generated from topic zu,b. When yu,b,n = 0, wb,n is generated from the background topic B. We adopt collapsed Gibbs Sampling to estimate the parameters. Because of the limitations of space, we leave out the details about the sampling algorithm. Since we can’t get a document’s distribution over topics from the parameters estimated by Twitter-BTM directly, we utilize the following formula (Yan et al., 2013) to infer the topic distribution of document d. Given a document d whose author is user u: Nb P(z = tId) = P(z = t|bi)P(bi|d) (1) i Now the problem is converted to how to estimate P(bi|d) and P(z = t|bi). P(bi|d) is estimated by empirical distribution in d: Nbi P(bi|d) = (2) Nb where Nbi is the number of biterm bi occurred in d, Nb is the total number of biterms in d. We can apply Bayes’ rule to compute P(z = t|bi) via following expression: θt [πuφBwi,1 + (1 − πu)φwi,1] 1πuφwBi,2 + (1 − πu)φti,2 � � � � Pk θu πuφB wi,1 + (1 − πu)φk πuφB wi,2 + (1 − πu)φk k wi,1 wi,2 (3) 4 Experiments In this S</context>
<context position="10136" citStr="Yan et al., 2013" startWordPosition="1703" endWordPosition="1706">first part whose statistics is shown in Table 1 is used for training. The second part which consists of 22,496,107 tweets is used as the external dataset in topic coherence evaluation task in Section 4.1. We compare the performance of Twitter-BTM with five baselines: • LDA-U, user based aggregation is applied before training LDA. • Twitter-LDA (Zhao et al., 2011), which makes a strong assumption that a tweet only covers one topic. • TwitterUB-LDA (Sasaki et al., 2014), an improved version of Twitter-LDA, which models the user level preference between topical words and background words. • BTM (Yan et al., 2013), the Biterm Topic Model. • BTM-U, a simplified version of Twitter-BTM without background topic. For all the above models, we use symmetric Dirichlet priors. The hyperparameters are set as follows: for all the models, we set α = 50/K, Q = 0.01; for Twitter-LDA, TwitterUB-LDA and Twitter-BTM, we set -y = 0.5. We run Gibbs sampling for 400 iterations. DataSet Twitter #tweets 1,201,193 #users 12,006 #vocabulary 71,038 #avgTweetLen 7.04 Table 1: Summary of dataset Perplexity metric is not used in our experiments since it is not a suitable evaluation metric for BTM (Cheng et al., 2014). The first r</context>
</contexts>
<marker>Yan, Guo, Lan, Cheng, 2013</marker>
<rawString>Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2013. A biterm topic model for short texts. In Proceedings of the 22nd international conference on World Wide Web, pages 1445–1456. International World Wide Web Conferences Steering Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohui Yan</author>
<author>Jiafeng Guo</author>
<author>Yanyan Lan</author>
<author>Jun Xu</author>
<author>Xueqi Cheng</author>
</authors>
<title>A probabilistic model for bursty topic discovery in microblogs.</title>
<date>2015</date>
<contexts>
<context position="3538" citStr="Yan et al. (2015)" startWordPosition="539" endWordPosition="542">periments indicate that simple user-based aggregation for BTM will generate incoherent topics. To distinguish between commonly used words (e.g., good, people, etc) and topical words (e.g., food, travel, etc), a background topic is often incorporated into the topic models. Zhao et al. (2011) use a background topic in Twitter-LDA to distill discriminative words in tweets. Sasaki et al. (2014) reduce the perplexity of Twitter-LDA by estimating the ratio between choosing background words and topical words for each user. They both make a very strong assumption that one tweet only covers one topic. Yan et al. (2015) use a background topic to distinguish between common biterms and bursty biterms, which need external data to evaluate the burstiness of each biterm as prior knowledge. Unlike those above, we incorporate a background 489 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 489–494, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics topic to absorb non-discriminative common words in each biterm. And we also estimate the user’s prefere</context>
</contexts>
<marker>Yan, Guo, Lan, Xu, Cheng, 2015</marker>
<rawString>Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Jun Xu, and Xueqi Cheng. 2015. A probabilistic model for bursty topic discovery in microblogs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
<author>Jianshu Weng</author>
<author>Jing He</author>
<author>Ee-Peng Lim</author>
<author>Hongfei Yan</author>
<author>Xiaoming Li</author>
</authors>
<title>Comparing twitter and traditional media using topic models.</title>
<date>2011</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>338--349</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3212" citStr="Zhao et al. (2011)" startWordPosition="483" endWordPosition="486">l the biterms (co-occurring word pairs) are generated by a corpus level topic distribution to benefit from the global rich word co-occurrence patterns. As far as we know, how to incorporate user factor into BTM has not been studied yet. User based aggregation has proven effective for LDA. But unfortunately, our preliminary experiments indicate that simple user-based aggregation for BTM will generate incoherent topics. To distinguish between commonly used words (e.g., good, people, etc) and topical words (e.g., food, travel, etc), a background topic is often incorporated into the topic models. Zhao et al. (2011) use a background topic in Twitter-LDA to distill discriminative words in tweets. Sasaki et al. (2014) reduce the perplexity of Twitter-LDA by estimating the ratio between choosing background words and topical words for each user. They both make a very strong assumption that one tweet only covers one topic. Yan et al. (2015) use a background topic to distinguish between common biterms and bursty biterms, which need external data to evaluate the burstiness of each biterm as prior knowledge. Unlike those above, we incorporate a background 489 Proceedings of the 53rd Annual Meeting of the Associa</context>
<context position="9884" citStr="Zhao et al., 2011" startWordPosition="1660" endWordPosition="1663">set collected form 10th Jun, 2009 to 31st Dec, 2009. Stop words and words occur less than 5 times are removed. We also filter tweets which only have one or two words. All letters are converted into lower case. The dataset is divided into two parts. The first part whose statistics is shown in Table 1 is used for training. The second part which consists of 22,496,107 tweets is used as the external dataset in topic coherence evaluation task in Section 4.1. We compare the performance of Twitter-BTM with five baselines: • LDA-U, user based aggregation is applied before training LDA. • Twitter-LDA (Zhao et al., 2011), which makes a strong assumption that a tweet only covers one topic. • TwitterUB-LDA (Sasaki et al., 2014), an improved version of Twitter-LDA, which models the user level preference between topical words and background words. • BTM (Yan et al., 2013), the Biterm Topic Model. • BTM-U, a simplified version of Twitter-BTM without background topic. For all the above models, we use symmetric Dirichlet priors. The hyperparameters are set as follows: for all the models, we set α = 50/K, Q = 0.01; for Twitter-LDA, TwitterUB-LDA and Twitter-BTM, we set -y = 0.5. We run Gibbs sampling for 400 iteratio</context>
</contexts>
<marker>Zhao, Jiang, Weng, He, Lim, Yan, Li, 2011</marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011. Comparing twitter and traditional media using topic models. In Advances in Information Retrieval, pages 338–349. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Wayne Zhao</author>
<author>Yanwei Guo</author>
<author>Yulan He</author>
<author>Han Jiang</author>
<author>Yuexin Wu</author>
<author>Xiaoming Li</author>
</authors>
<title>We know what you want to buy: a demographicbased system for product recommendation on microblogs.</title>
<date>2014</date>
<booktitle>In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>1935--1944</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1669" citStr="Zhao et al., 2014" startWordPosition="239" endWordPosition="242">Experiments on a large-scale real-world Twitter dataset show that Twitter-BTM outperforms several stateof-the-art baselines. 1 Introduction In recent years, short texts are increasingly prevalent due to the explosive growth of online social media. For example, about 500 million tweets are published per day on Twitter1, one of the most popular online social networking services. Probabilistic topic models (Blei et al., 2003) are broadly used to uncover the hidden topics of tweets, since the low-dimensional semantic representation is crucial for many applications, such as product recommendation (Zhao et al., 2014), hashtag recommendation (Ma et al., 2014), user interest tracking (Sasaki et al., 2014), sentiment analysis 1See https://about.Twitter.com/company (Si et al., 2013). However, the scarcity of context and the noisy words restrict LDA and its variations in topic modeling over short texts. Previous works model topic distribution at three different levels for tweets: 1) document, the standard LDA assumes each document is associated with a topic distribution (Godin et al., 2013; Huang, 2012). LDA and its variations suffer from context sparsity in each tweet. 2) user, user based aggregation is utili</context>
</contexts>
<marker>Zhao, Guo, He, Jiang, Wu, Li, 2014</marker>
<rawString>Xin Wayne Zhao, Yanwei Guo, Yulan He, Han Jiang, Yuexin Wu, and Xiaoming Li. 2014. We know what you want to buy: a demographicbased system for product recommendation on microblogs. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1935–1944. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>