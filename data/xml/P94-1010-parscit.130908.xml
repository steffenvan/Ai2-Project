<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000631">
<title confidence="0.9917365">
A STOCHASTIC FINITE-STATE WORD-SEGMENTATION ALGORITHM
FOR CHINESE
</title>
<author confidence="0.978416">
Richard Sproat Nancy Chang
</author>
<affiliation confidence="0.642325666666667">
Chain Shih Harvard University
William Gale Division of Applied Sciences
AT&amp;T Bell Laboratories Harvard University
</affiliation>
<address confidence="0.858163">
600 Mountain Avenue, Cambridge, MA 02138
Room {2d-451,2d-453,2c-278} nchang@das.harvard.edu
Murray Hill, NJ, USA, 07974-0636
</address>
<email confidence="0.991852">
frws,c1s,galel@researchsatt.com
</email>
<sectionHeader confidence="0.995514" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99951375">
We present a stochastic finite-state model for segment-
ing Chinese text into dictionary entries and produc-
tively derived words, and providing pronunciations for
these words; the method incorporates a class-based
model in its treatment of personal names. We also
evaluate the system&apos;s performance, taking into account
the fact that people often do not agree on a single seg-
mentation.
</bodyText>
<sectionHeader confidence="0.775449" genericHeader="categories and subject descriptors">
THE PROBLEM
</sectionHeader>
<bodyText confidence="0.999415947368421">
The initial step of any text analysis task is the tok-
enization of the input into words. For many writing
systems, using whitespace as a delimiter for words
yields reasonable results. However, for Chinese and
other systems where whitespace is not used to delimit
words, such trivial schemes will not work. Chinese
writing is moiphosyllabic (DeFrancis, 1984), meaning
that each hanzi– &apos;Chinese character&apos; – (nearly always)
represents a single syllable that is (usually) also a sin-
gle morpheme. Since in Chinese, as in English, words
may be polysyllabic, and since hanzi are written with
no intervening spaces, it is not trivial to reconstruct
which hanzi to group into words.
While for some applications it may be possible
to bypass the word-segmentation problem and work
straight from hanzi, there are several reasons why this
approach will not work in a text-to-speech (TI&apos;S) sys-
tem for Mandarin Chinese — the primary intended
application of our segmenter. These reasons include:
</bodyText>
<listItem confidence="0.784186">
1. Many hanzi are homographs whose pronunciation
depends upon word affiliation. So, (1/9 is pronounced
de01 when it is a prenominal modification marker,
but di4 in the word El Efl mu4di4 &apos;goal&apos;: tt is nor-
mally ganl &apos;dry&apos;, but qian2 in a person&apos;s given name.
2. Some phonological rules depend upon correct word-
segmentation, including Third Tone Sandhi (Shih,
1986), which changes a 3 tone into a 2 tone be-
</listItem>
<bodyText confidence="0.981025090909091">
fore another 3 tone: /.1N xiao3 [lao3 shu3] &apos;lit-
&apos;We use pinyin transliteration with numbers representing
tones.
tle rat&apos;, becomes xiao3 I lao2-shu3 1, rather than
xiao2 I lao2-shu3 J, because the rule first applies
within the word lao3-shu3, blocking its phrasal ap-
plication.
While a minimal requirement for building a Chi-
nese word-segmenter is a dictionary, a dictionary is in-
sufficient since there are several classes of words that
are not generally found in dictionaries. Among these:
</bodyText>
<listItem confidence="0.9979875">
1. Morphologically Derived Words: 44311 xiao3-
jiang4-men0 (little general-plural) &apos;little generals&apos;.
2. Personal Names: yam zhoul enl-lai2 &apos;Zhou
Enlai&apos;.
3. Transliterated Foreign Names: /00)J -±a bu4-
lang3-shi4-wei2-ke4 &apos;Brunswick&apos;.
</listItem>
<bodyText confidence="0.999911764705882">
We present a stochastic finite-state model for seg-
menting Chinese text into dictionary entries and words
derived via the above-mentioned productive processes;
as part of the treatment of personal names, we dis-
cuss a class-based model which uses the Good-Turing
method to estimate costs of previously unseen personal
names. The segmenter handles the grouping of hanzi
into words and outputs word pronunciations, with de-
fault pronunciations for hanzi it cannot group; we focus
here primarily on the system&apos;s ability to segment text
appropriately (rather than on its pronunciation abili-
ties). We evaluate various specific aspects of the seg-
mentation, and provide an evaluation of the overall
segmentation performance: this latter evaluation com-
pares the performance of the system with that of several
human judges, since even people do not agree on a sin-
gle correct way to segment a text.
</bodyText>
<sectionHeader confidence="0.970093" genericHeader="related work">
PREVIOUS WORK
</sectionHeader>
<bodyText confidence="0.72806825">
There is a sizable literature on Chinese word segmenta-
tion: recent reviews include (Wang et al., 1990; Wu and
Tseng, 1993). Roughly, previous work can be classi-
fied into purely statistical approaches (Sproat and Shih,
1990), statistical approaches which incorporate lexical
knowledge (Fan and Tsai, 1988; Lin et al., 1993), and
approaches that include lexical knowledge combined
with heuristics (Chen and Liu, 1992).
</bodyText>
<page confidence="0.985782">
66
</page>
<bodyText confidence="0.99995044">
Chen and Liu&apos;s (1992) algorithm matches words of
an input sentence against a dictionary; in cases where
various parses are possible, a set of heuristics is applied
to disambiguate the analyses. Various morphological
rules are then applied to allow for morphologically
complex words that are not in the dictionary. Preci-
sion and recall rates of over 99% are reported, but note
that this covers only words that are in the dictionary:
&amp;quot;the . . . statistics do not count the mistakes [that occur]
due to the existence of derived words or proper names&amp;quot;
(Chen and Liu, 1992, page 105). Lin et al. (1993) de-
scribe a sophisticated model that includes a dictionary
and a morphological analyzer. They also present a gen-
eral statistical model for detecting &apos;unknown words&apos;
based on hanzi and part-of-speech sequences. How-
ever, their unknown word model has the disadvantage
that it does not identify a sequence of hanzi as an un-
known word of a particular category, but merely as an
unknown word (of indeterminate category). For an ap-
plication like TTS, however, it is necessary to know that
a particular sequence of hanzi is of a particular category
because, for example, that knowledge could affect the
pronunciation. We therefore prefer to build particular
models for different classes of unknown words, rather
than building a single general model.
</bodyText>
<sectionHeader confidence="0.99164" genericHeader="method">
DICTIONARY REPRESENTATION
</sectionHeader>
<bodyText confidence="0.99963980952381">
The lexicon of basic words and stems is represented as a
weighted finite-state tranducer (WFST) (Pereira et al.,
1994). Most transitions represent mappings between
hanzi and pronunciations, and are costless. Transitions
between orthographic words and their parts-of-speech
are represented by f -to-category transductions and a
unigram cost (negative log probability) of that word
estimated from a 20M hanzi training corpus; a portion
of the WFST is given in Figure 1.2 Besides dictionary
words, the lexicon contains all hanzi in the Big 5 Chi-
nese code, with their pronunciation(s), plus entries for
other characters (e.g., roman letters, numerals, special
symbols).
Given this dictionary representation, recognizing a
single Chinese word involves representing the input as
a finite-state acceptor (FSA) where each arc is labeled
with a single hanzi of the input. The left-restriction
of the dictionary WFST with the input FSA contains
all and only the (single) lexical entries correspond-
ing to the input. This WFST includes the word costs
on arcs transducing € to category labels. Now, input
</bodyText>
<footnote confidence="0.8454051">
2The costs are actually for strings rather than words: we
currently lack estimates for the words themselves. We assign
the string cost to lexical entries with the likeliest pronuncia-
tion, and a large cost to all other entries. Thus 115/adv, with
the commonest pronunciation jiang./ has cost 5.98, whereas
wInc, with the rarer pronunciationjiang4, is assigned a high
cost. Note also that the current model is zeroeth order in that
it uses only unigram costs. Higher order models, e.g. bigram
word models, could easily be incorporated into the present
architecture if desired.
</footnote>
<bodyText confidence="0.9991974">
sentences consist of one or more entries from the dic-
tionary, and we can generalize the word recognition
problem to the word segmentation problem, by left-
restricting the transitive closure of the dictionary with
the input. The result of this left-restriction is an WFST
that gives all and only the possible analyses of the in-
put FSA into dictionary entries. In general we do not
want all possible analyses but rather the best analysis.
This is obtained by computing the least-cost path in the
output WFST. The final stage of segmentation involves
traversing the best path, collecting into words all se-
quences of hanzi delimited by part-of-speech-labeled
arcs. Figure 2 shows an example of segmentation: the
sentence 19 &amp;quot;How do you say octopus
in Japanese?&amp;quot;, consists of four words, namely El Sc
</bodyText>
<subsubsectionHeader confidence="0.851052">
ri4-wen2 &apos;Japanese&apos;, zhang 1 -yu2 &apos;octopus&apos;, X
</subsubsectionHeader>
<bodyText confidence="0.9774798">
ff zen3-mo &apos;how&apos;, and shuo I &apos;say&apos;. In this case,
El ri4 is also a word (e.g. a common abbreviation for
Japan) as are SC* wen2-zhangl &apos;essay&apos;, and yu2
&apos;fish&apos;, so there is (at least) one alternate analysis to be
considered.
</bodyText>
<sectionHeader confidence="0.954243" genericHeader="method">
MORPHOLOGICAL ANALYSIS
</sectionHeader>
<bodyText confidence="0.9915554">
The method just described segments dictionary words,
but as noted there are several classes of words that
should be handled that are not in the dictionary. One
class comprises words derived by productive morpho-
logical processes, such as plural noun formation us-
ing the suffix in men0. The morphological anal-
ysis itself can be handled using well-known tech-
niques from finite-state morphology (Koskenniemi,
1983; Antworth, 1990; Tzoukermann and Liberman,
1990; Karttunen et al., 1992; Sproat, 1992); so, we
represent the fact that f attaches to nouns by allowing
E.-transitions from the final states of all noun entries,
to the initial state of the sub-WFST representing I.
However, for our purposes it is not sufficient to rep-
resent the morphological decomposition of, say, plu-
ral nouns: we also need an estimate of the cost of
the resulting word. For derived words that occur in
our corpus we can estimate these costs as we would
the costs for an underived dictionary entry. So, nim
jiang4-men0 `(military) generals&apos; occurs and we esti-
mate its cost at 15.02. But we also need an estimate
of the probability for a non-occurring though possi-
ble plural form like 1IflUfl nan2-gual -men° &apos;pump-
kins&apos;. Here we use the Good-Turing estimate (Baayen,
1989; Church and Gale, 1991), whereby the aggre-
gate probability of previously unseen members of a
construction is estimated as NI/N, where N is the
total number of observed tokens and N1 is the num-
ber of types observed only once. For II this gives
prob(unseen(111) I fl), and to get the aggregate prob-
ability of novel fi-constructions in a corpus we multi-
ply this by probtezt(in) to get probrext(uuseen(11)).
Finally, to estimate the probability of particular unseen
word MAIM, we use the simple bigram backoff model
prob(*))413) prob(M)11)probtert(unseen(f1));
</bodyText>
<page confidence="0.941558">
67
</page>
<figure confidence="0.838608">
E : _NP : al°
</figure>
<figureCaption confidence="0.990043">
Figure 1: Partial chinese Lexicon (NC = noun; NP = proper noun)
Figure 2: Input lattice (top) and two segmentations (bottom) of the sentence &apos;How do you say octopus in Japanese&apos;. A
non-optimal analysis is shown with dotted lines in the bottom frame.
</figureCaption>
<figure confidence="0.9948994">
5.55
7.96
10.63
13.18
Et 3C #1, ot
0.1
ESSAY FISH
E :wen2 *:zhang1 E :_nc :yu2
JAPAN Ei :ri4
.. ...... ...... ....... ........ ....... ,„Ist)
6.51 9.51
...• ...
_nc E : _
I JAPANESE OCTOPUS 10. HOW SAY
El :ri4 3C :wen2 E :_nc *:zhang1 At :yu2 E:_nci.T.■ :zen3 e :MOO E:_adv gt :shuo1 E:_vb
</figure>
<page confidence="0.852881">
68
</page>
<figureCaption confidence="0.999302">
Figure 3: An example of affixation: the plural affix
</figureCaption>
<bodyText confidence="0.996595869565218">
cost ()A 111 ) is computed in the obvious way. Fig-
ure 3 shows how this model is implemented as part of
the dictionary WFST. There is a (costless) transition
between the NC node and 411. The transition from
fl to a final state transduces c to the grammatical tag
\ PL with cost costt„t(unseen(n)): cost()1449
) = cost(C)Z) costtext(unseen(f1)), as desired.
For the seen word 4411 &apos;generals&apos;, there is an cnc
transduction from I to the node preceding IN; this
arc has cost cost() — costtert(unseen({11)), so
that the cost of the whole path is the desired cost oil
). This representation gives Wiluj an appropriate mor-
phological decomposition, preserving information that
would be lost by simply listing nfil as an unanalyzed
form. Note that the backoff model assumes that there is
a positive correlation between the frequency of a singu-
lar noun and its plural. An analysis of nouns that occur
both in the singular and the plural in our database re-
veals that there is indeed a slight but significant positive
correlation — R2 = 0.20, p &lt; 0.005. This suggests
that the backoff model is as reasonable a model as we
can use in the absence of further information about the
expected cost of a plural form.
</bodyText>
<sectionHeader confidence="0.978664" genericHeader="method">
CHINESE PERSONAL NAMES
</sectionHeader>
<bodyText confidence="0.976624583333334">
Full Chinese personal names are in one respect sim-
ple: they are always of the form FAMILY+GIVEN.
The FAMILY name set is restricted: there are a few
hundred single-hanzi FAMILY names, and about ten
double-hanzi ones. Given names are most commonly
two hanzi long, occasionally one-hanzi long: there
are thus four possible name types. The difficulty is that
GIVEN names can consist, in principle, of any hanzi or
pair of hanzi, so the possible GIVEN names are limited
only by the total number of hanzi, though some hanzi
are certainly far more likely than others. For a sequence
of hanzi that is a possible name, we wish to assign a
probability to that sequence qua name. We use an esti-
mate derived from (Chang et al., 1992). For example,
given a potential name of the form Fl G1 G2, where Fl
is a legal FAMILY name and G1 and G2 are each hanzi,
we estimate the probability of that name as the prod-
uct of the probability of finding any name in text; the
probability of Fl as a FAMILY name; the probability
of the first hanzi of a double GIVEN name being Gl;
the probability of the second hanzi of a double GIVEN
name being G2; and the probability of a name of the
form SINGLE-FAMILY+DOUBLE-GIVEN. The first
probability is estimated from a name count in a text
database, whereas the last four probabilities are esti-
mated from a large list of personal names.3 This model
is easily incorporated into the segmenter by building an
WFST restricting the names to the four licit types, with
costs on the arcs for any particular name summing to
an estimate of the cost of that name. This WFST is then
summed with the WFST implementing the dictionary
and morphological rules, and the transitive closure of
the resulting transducer is computed.
3We have two such lists, one containing about 17,000 full
names, and another containing frequencies of hanzi in the
various name positions, derived from a million names.
</bodyText>
<page confidence="0.997343">
69
</page>
<bodyText confidence="0.999874818181818">
There are two weaknesses in Chang et al.&apos;s (1992)
model, which we improve upon. First, the model as-
sumes independence between the first and second hanzi
of a double GIVEN name. Yet, some hanzi are far more
probable in women&apos;s names than they are in men&apos;s
names, and there is a similar list of male-oriented hanzi:
mixing hanzi from these two lists is generally less likely
than would be predicted by the independence model.
As a partial solution, for pairs of hanzi that cooccur suf-
ficiently often in our namelists, we use the estimated
bigram cost, rather than the independence-based cost.
The second weakness is that Chang et al. (1992) as-
sign a uniform small cost to unseen hanzi in GIVEN
names; but we know that some unseen hanzi are merely
accidentally missing, whereas others are missing for a
reason — e.g., because they have a bad connotation.
We can address this problem by first observing that
for many hanzi, the general &apos;meaning&apos; is indicated by
its so-called &apos;semantic radical&apos;. Hanzi that share the
same &apos;radical&apos;, share an easily identifiable structural
component: the plant names NI, M and M share the
GRASS radical; malady names 4, , and M share
the SICKNESS radical; and ratlike animal names 21,
M, and a share the RAT radical. Some classes are bet-
ter for names than others: in our corpora, many names
are picked from the GRASS class, very few from the
SICKNESS class, and none from the RAT class. We
can thus better predict the probability of an unseen
hanzi occurring in a name by computing a within-class
Good-Turing estimate for each radical class. Assum-
ing unseen objects within each class are equiprobable,
their probabilities are given by the Good-Turing theo-
rem as:
</bodyText>
<equation confidence="0.975855333333333">
Po cis E(Nis)
oc
N * E(Nes)
</equation>
<bodyText confidence="0.99999475">
where p8is is the probability of one unseen hanzi in
class cls, E(N) is the expected number of hanzi
in cls seen once, N is the total number of hanzi, and
E(N) is the expected number of unseen hanzi in
class cls. The use of the Good-Turing equation pre-
sumes suitable estimates of the unknown expectations
it requires. In the denominator, the Nis are well mea-
sured by counting, and we replace the expectation by
the observation. In the numerator, however, the counts
of NI are quite irregular, including several zeros (e.g.
RAT, none of whose members were seen). However,
there is a strong relationship between Nis and the
number of hanzi in the class. For E(.113), then, we
substitute a smooth against the number of class ele-
ments. This smooth guarantees that there are no zeroes
estimated. The final estimating equation is then:
</bodyText>
<equation confidence="0.948717">
cli S(A715 )
Po
</equation>
<bodyText confidence="0.999897727272727">
The total of all these class estimates was about 10% off
from the Turing estimate NUN for the probability of
all unseen hanzi, and we renormalized the estimates so
that they would sum to MIN.
This class-based model gives reasonable results:
for six radical classes, Table 1 gives the estimated cost
for an unseen hanzi in the class occurring as the second
hanzi in a double GIVEN name. Note that the good
classes JADE, GOLD and GRASS have lower costs
than the bad classes SICKNESS, DEATH and RAT, as
desired.
</bodyText>
<sectionHeader confidence="0.999591" genericHeader="method">
TRANSLITERATIONS OF FOREIGN
WORDS
</sectionHeader>
<bodyText confidence="0.999986222222222">
Foreign names are usually transliterated using hanzi
whose sequential pronunciation mimics the source lan-
guage pronunciation of the name. Since foreign names
can be of any length, and since their original pronunci-
ation is effectively unlimited, the identification of such
names is tricky. Fortunately, there are only a few hun-
dred hanzi that are particularly common in translitera-
tions; indeed, the commonest ones, such as 6 bal,fg
er3, andliT1 1 al are often clear indicators that a sequence
of hanzi containing them is foreign: even a name like
xia4-mi3-er3 &apos;Shamir&apos;, which is a legal Chi-
nese personal name, retains a foreign flavor because
of . As a first step towards modeling transliterated
names, we have collected all hanzi occurring more than
once in the roughly 750 foreign names in our dictionary,
and we estimate the probability of occurrence of each
hanzi in a transliteration (pr N(hanzii))using the max-
imum likelihood estimate. As with personal names,
we also derive an estimate from text of the probabil-
ity of finding a transliterated name of any kind (pTN).
Finally, we model the probability of a new transliter-
ated name as the product of pTN and pTN(hanzii)
for each hanzi i in the putative name.4 The foreign
name model is implemented as an WFST, which is then
summed with the WFST implementing the dictionary,
morphological rules, and personal names; the transitive
closure of the resulting machine is then computed.
</bodyText>
<sectionHeader confidence="0.984814" genericHeader="evaluation">
EVALUATION
</sectionHeader>
<bodyText confidence="0.91590025">
In this section we present a partial evaluation of the
current system in three parts. The first is an evaluation
of the system&apos;s ability to mimic humans at the task of
segmenting text into word-sized units; the second eval-
uates the proper name identification; the third measures
the performance on morphological analysis. To date
we have not done a separate evaluation of foreign name
recognition.
Evaluation of the Segmentation as a Whole: Pre-
vious reports on Chinese segmentation have invariably
4The current model is too simplistic in several respects.
For instance, the common &apos;suffixes&apos;, -nia (e.g., Virginia) and
-sia are normally transliterated as ft&apos;, ni2-ya3 and N.g
xil -ya3, respectively. The interdependence between TO, or
N, and g2 is not captured by our model, but this could easily
be remedied.
</bodyText>
<page confidence="0.999154">
70
</page>
<tableCaption confidence="0.999768">
Table 1: The cost as a novel GIVEN name (second position) for hanzi from various radical classes.
</tableCaption>
<table confidence="0.331678">
JADE GOLD GRASS SICKNESS DEATH RAT
14.98 15.52 15.76 16.25 16.30 16.42
</table>
<bodyText confidence="0.999603666666667">
cited performance either in terms of a single percent-
correct score, or else a single precision-recall pair. The
problem with these styles of evaluation is that, as we
shall demonstrate, even human judges do not agree
perfectly on how to segment a given text. Thus, rather
than give a single evaluative score, we prefer to com-
pare the performance of our method with the judgments
of several human subjects. To this end, we picked 100
sentences at random containing 4372 total hanzi from
a test corpus. We asked six native speakers — three
from Taiwan (T1–T3), and three from the Mainland
(M 1 –M3) — to segment the corpus. Since we could
not bias the subjects towards a particular segmentation
and did not presume linguistic sophistication on their
part, the instructions were simple: subjects were to
mark all places they might plausibly pause if they were
reading the text aloud. An examination of the subjects&apos;
bracketings confirmed that these instructions were sat-
isfactory in yielding plausible word-sized units.
Various segmentation approaches were then com-
pared with human performance:
</bodyText>
<listItem confidence="0.922722">
1. A greedy algorithm, GR: proceed through the sen-
tence, taking the longest match with a dictionary
entry at each point.
2. An &apos;anti-greedy&apos; algorithm, AG: instead of the
longest match, take the shortest match at each point.
3. The method being described — henceforth ST.
Two measures that can be used to compare judgments
are:
1. Precision. For each pair of judges consider one
judge as the standard, computing the precision of
the other&apos;s judgments relative to this standard.
2. Recall. For each pair of judges, consider one judge
as the standard, computing the recall of the other&apos;s
judgments relative to this standard.
</listItem>
<bodyText confidence="0.999451969230769">
Obviously, for judges J1 and J2, taking J1 as stan-
dard and computing the precision and recall for &lt;12
yields the same results as taking J2 as the standard,
and computing for J1, respectively, the recall and pre-
cision. We therefore used the arithmetic mean of each
interjudge precision-recall pair as a single measure of
interjudge similarity. Table 2 shows these similarity
measures. The average agreement among the human
judges is .76, and the average agreement between ST
and the humans is .75, or about 99% of the inter-human
agreement. (GR is .73 or 96%.) One can better visu-
alize the precision-recall similarity matrix by produc-
ing from that matrix a distance matrix, computing a
multidimensional scaling on that distance matrix, and
plotting the first two most significant dimensions. The
result of this is shown in Figure 4. In addition to the
automatic methods, AG, GR and ST, just discussed,
we also added to the plot the values for the current
algorithm using only dictionary entries (i.e., no pro-
ductively derived words, or names). This is to allow
for fair comparison between the statistical method, and
GR, which is also purely dictionary-based. As can
be seen, GR and this &apos;pared-down&apos; statistical method
perform quite similarly, though the statistical method is
still slightly better. AG clearly performs much less like
humans than these methods, whereas the full statisti-
cal algorithm, including morphological derivatives and
names, performs most closely to humans among the
automatic methods. It can be also seen clearly in this
plot, two of the Taiwan speakers cluster very closely
together, and the third Taiwan speaker is also close in
the most significant dimension (the x axis). Two of the
Mainlanders also cluster close together but, interest-
ingly, not particularly close to the Taiwan speakers; the
third Mainlander is much more similar to the Taiwan
speakers.
Personal Name Identification: To evaluate personal
name identification, we randomly selected 186 sen-
tences containing 12,000 hanzi from our test corpus,
and segmented the text automatically, tagging personal
names; note that for names there is always a single un-
ambiguous answer, unlike the more general question
of which segmentation is correct. The performance
was 80.99% recall and 61.83% precision. Interest-
ingly, Chang etal. reported 80.67% recall and 91.87%
precision on an 11,000 word corpus: seemingly, our
system finds as many names as their system, but with
four times as many false hits. However, we have reason
to doubt Chang et al.&apos;s performance claims. Without
using the same test corpus, direct comparison is ob-
viously difficult; fortunately Chang et al. included a
list of about 60 example sentence fragments that ex-
emplified various categories of performance for their
system. The performance of our system on those sen-
tences appeared rather better than theirs. Now, on a set
of 11 sentence fragments where they reported 100% re-
call and precision for name identification, we had 80%
precision and 73% recall. However, they listed two
sets, one consisting of 28 fragments and the other of 22
fragments in which they had 0% precision and recall.
On the first of these our system had 86% precision and
64% recall; on the second it had 19% precision and
33% recall. Note that it is in precision that our over-
all performance would appear to be poorer than that of
Chang et al., yet based on their published examples, our
</bodyText>
<page confidence="0.99913">
71
</page>
<tableCaption confidence="0.998215">
Table 2: Similarity matrix for segmentation judgments
</tableCaption>
<table confidence="0.997482333333333">
Judges AG GR ST M1 M2 M3 Ti T2 T3
AG 0.70 0.70 0.43 0.42 0.60 0.60 0.62 0.59
GR 0.99 0.62 0.64 0.79 0.82 0.81 0.72
ST 0.64 0.67 0.80 0.84 0.82 0.74
M1 0.77 0.69 0.71 0.69 0.70
M2 0.72 0.73 0.71 0.70
M3 0.89 0.87 0.80
Ti 0.88 0.82
T2 0.78
</table>
<bodyText confidence="0.999160647058824">
system appears to be doing better precisionwise. Thus
we have some confidence that our own performance is
at least as good that of (Chang et al., 1992).5
Evaluation of Morphological Analysis: In Table 3
we present results from small test corpora for some
productive affixes; as with names, the segmentation of
morphologically derived words is generally either right
or wrong. The first four affixes are so-called resultative
affixes: they denote some property of the resultant state
of an verb, as in Es wang4-bo-lia03 (forget-not-
attain) &apos;cannot forget&apos;. The last affix is the nominal
plural. Note that in 4.,T 7 is normally pronounced
as le0, but when part of a resultative it is liao3. In the
table are the (typical) classes of words to which the affix
attaches, the number found in the test corpus by the
method, the number correct (with a precision measure),
and the number missed (with a recall measure).
</bodyText>
<sectionHeader confidence="0.996671" genericHeader="conclusions">
CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999596785714286">
In this paper we have shown that good performance
can be achieved on Chinese word segmentation by us-
ing probabilistic methods incorporated into a uniform
stochastic finite-state model. We believe that the ap-
proach reported here compares favorably with other
reported approaches, though obviously it is impossible
to make meaningful comparisons in the absence of uni-
form test databases for Chinese segmentation. Perhaps
the single most important difference between our work
and previous work is the form of the evaluation. As
we have observed there is often no single right answer
to word segmentation in Chinese. Therefore, claims to
the effect that a particular algorithm gets 99% accuracy
are meaningless without a clear definition of accuracy.
</bodyText>
<sectionHeader confidence="0.998752" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.9948967">
We thank United Informatics for providing us with
our corpus of Chinese text, and BDC for the `Behav-
5We were recently pointed to (Wang et al., 1992), which
we had unfortunately missed in our previous literature search.
We hope to compare our method with that of Wang et al. in
a future version of this paper.
ior Chinese-English Electronic Dictionary&apos;. We fur-
ther thank Dr. J.-S. Chang of Tsinghua University, for
kindly providing us with the name corpora. Finally, we
thank two anonymous ACL reviewers for comments.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.9994832">
Evan Antworth. 1990. PC-KIMMO: A Two-Level Pro-
cessor for Morphological Analysis. Occasional
Publications in Academic Computing, 16. Sum-
mer Institute of Linguistics, Dallas, TX.
Harald Baayen. 1989. A Corpus-Based Approach to
Morphological Productivity: Statistical Analysis
and Psycholinguistic Interpretation. Ph.D. thesis,
Free University, Amsterdam.
Jyun-Shen Chang, Shun-De Chen, Ying Zheng, Xian-
Thong Liu, and Shu-Jin Ke. 1992. Large-corpus-
based methods for Chinese personal name recogni-
tion. Journal of Chinese Information Processing,
6(3):7-15.
Keh-Jiann Chen and Shing-Huan Liu. 1992. Word
identification for Mandarin Chinese sentences.
In Proceedings of COLING-92, pages 101-107.
COLING.
Kenneth Ward Church and William Gale. 1991. A
comparison of the enhanced Good-Turing and
deleted estimation methods for estimating prob-
abilities of English bigrams. Computer Speech
and Language, 5(1):19-54.
John DeFrancis. 1984. The Chinese Language. Uni-
versity of Hawaii Press, Honolulu.
C.-K. Fan and W.-H. Tsai. 1988. Automatic word
identification in Chinese sentences by the relax-
ation technique. Computer Processing of Chinese
and Oriental Languages, 4:33-56.
Lauri Karttunen, Ronald Kaplan, and Annie Zaenen.
1992. Two-level morphology with composition.
In COLING-92, pages 141-148. COLING.
Kimmo Koskenniemi. 1983. Two-Level Morphology:
a General Computational Model for Word-Form
Recognition and Production. Ph.D. thesis, Uni-
versity of Helsinki, Helsinki.
</reference>
<page confidence="0.990022">
72
</page>
<figure confidence="0.997378444444445">
•
Dimension 2 (14%) 0 + antlgreedy •
0 X greedy •
o current method
o dict. only
• Taiwan
• Mainland
-0.3 -0.2 -0.1 0.0 0.1 0.2
Dimension 1 (62%)
</figure>
<figureCaption confidence="0.969513">
Figure 4: Classical metric multidimensional scaling of distance matrix, showing the two most significant dimensions.
The percentage scores on the axis labels represent the amount of data explained by the dimension in question.
</figureCaption>
<tableCaption confidence="0.998662">
Table 3: Performance on morphological analysis.
</tableCaption>
<table confidence="0.990059333333333">
Affix Pron Base category N found N correct (prec.) N missed (rec.)
TT-A bu2-xia4 verb 20 20 (100%) 12 (63%)
bu2-xia4-qu4 verb 30 29 (97%) 1 (97%)
bu4-liao3 verb 72 72 (100%) 15 (83%)
de2-liao3 verb 36 36 (100%) 11(77%)
men0 noun 141 139 (99%) 6(96%)
</table>
<reference confidence="0.998415352941177">
Ming-Yu Lin, Tung-Hui Chiang, and Keh-Yi Su. 1993.
A preliminary study on unknown word problem
in Chinese word segmentation. In ROCLING 6,
pages 119-141. ROCLING.
Fernando Pereira, Michael Riley, and Richard Sproat.
1994. Weighted rational transductions and their
application to human language processing. In
ARPA Workshop on Human Language Technol-
ogy, pages 249-254. Advanced Research Projects
Agency, March 8-11.
Chilin Shih. 1986. The Prosodic Domain of Tone
Sandhi in Chinese. Ph.D. thesis, UCSD, La Jolla,
CA.
Richard Sproat and Chilin Shih. 1990. A statistical
method for finding word boundaries in Chinese
text. Computer Processing of Chinese and Orien-
tal Languages, 4:336-351.
Richard Sproat. 1992. Morphology and Computation.
MIT Press, Cambridge, MA.
Evelyne Tzoukermann and Mark Liberman. 1990. A
finite-state morphological processor for Spanish.
In COLING-90, Volume 3, pages 3: 277-286.
COLING.
Yongheng Wang, Haiju Su, and Yan Mo. 1990. Au-
tomatic processing of chinese words. Journal of
Chinese Information Processing, 4(4): 1-11.
Liang-Jyh Wang, Wei-Chuan Li, and Chao-Huang
Chang. 1992. Recognizing unregistered names
for mandarin word identification. In Proceedings
of COLING-92, pages 1239-1243. COLING.
Zimin Wu and Gwyneth Tseng. 1993. Chinese text
segmentation for text retrieval: Achievements and
problems. Journal of the American Society for
Information Science, 44(9):532-542.
</reference>
<page confidence="0.999298">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.9981215">A STOCHASTIC FINITE-STATE WORD-SEGMENTATION ALGORITHM FOR CHINESE</title>
<author confidence="0.999687">Richard Sproat Nancy Chang</author>
<affiliation confidence="0.986238">Chain Shih Harvard University</affiliation>
<author confidence="0.826415">William Gale Division of Applied Sciences</author>
<affiliation confidence="0.999911">AT&amp;T Bell Laboratories Harvard University</affiliation>
<address confidence="0.999884">600 Mountain Avenue, Cambridge, MA 02138</address>
<email confidence="0.846268">Room{2d-451,2d-453,2c-278}nchang@das.harvard.edu</email>
<address confidence="0.965263">Murray Hill, NJ, USA, 07974-0636</address>
<email confidence="0.999566">frws,c1s,galel@researchsatt.com</email>
<abstract confidence="0.99318154">We present a stochastic finite-state model for segmenting Chinese text into dictionary entries and productively derived words, and providing pronunciations for these words; the method incorporates a class-based model in its treatment of personal names. We also evaluate the system&apos;s performance, taking into account the fact that people often do not agree on a single segmentation. THE PROBLEM The initial step of any text analysis task is the tokenization of the input into words. For many writing systems, using whitespace as a delimiter for words yields reasonable results. However, for Chinese and other systems where whitespace is not used to delimit words, such trivial schemes will not work. Chinese is 1984), meaning each character&apos; – (nearly always) represents a single syllable that is (usually) also a sinmorpheme. Since in Chinese, as in English, be polysyllabic, and since written with no intervening spaces, it is not trivial to reconstruct group into words. for some applications it possible to bypass the word-segmentation problem and work from are several reasons why this approach will not work in a text-to-speech (TI&apos;S) system for Mandarin Chinese — the primary intended application of our segmenter. These reasons include: Many homographs whose pronunciation upon word affiliation. So, is pronounced when it is a prenominal modification marker, the word Efl norbut a person&apos;s given name. 2. Some phonological rules depend upon correct wordsegmentation, including Third Tone Sandhi (Shih, which changes a 3 tone into a 2 tone beanother 3 tone: [lao3 shu3] &apos;lituse with numbers representing tones. rat&apos;, becomes Ilao2-shu3 1, than I lao2-shu3 J, the rule first applies the word its phrasal application. While a minimal requirement for building a Chinese word-segmenter is a dictionary, a dictionary is insufficient since there are several classes of words that are not generally found in dictionaries. Among these: Morphologically Derived Words: &apos;little generals&apos;.</abstract>
<note confidence="0.586632">Personal Names: enl-lai2 Enlai&apos;. Transliterated Foreign Names: /00)J</note>
<abstract confidence="0.986060420731709">We present a stochastic finite-state model for segmenting Chinese text into dictionary entries and words derived via the above-mentioned productive processes; as part of the treatment of personal names, we discuss a class-based model which uses the Good-Turing method to estimate costs of previously unseen personal The segmenter handles the grouping of into words and outputs word pronunciations, with depronunciations for cannot group; we focus here primarily on the system&apos;s ability to segment text appropriately (rather than on its pronunciation abili- We evaluate various of the segand provide an evaluation of the segmentation performance: this latter evaluation compares the performance of the system with that of several human judges, since even people do not agree on a single correct way to segment a text. PREVIOUS WORK There is a sizable literature on Chinese word segmentation: recent reviews include (Wang et al., 1990; Wu and Tseng, 1993). Roughly, previous work can be classified into purely statistical approaches (Sproat and Shih, statistical approaches which incorporate lexical knowledge (Fan and Tsai, 1988; Lin et al., 1993), and approaches that include lexical knowledge combined with heuristics (Chen and Liu, 1992). 66 Chen and Liu&apos;s (1992) algorithm matches words of an input sentence against a dictionary; in cases where various parses are possible, a set of heuristics is applied to disambiguate the analyses. Various morphological rules are then applied to allow for morphologically complex words that are not in the dictionary. Precision and recall rates of over 99% are reported, but note this covers that are in the dictionary: &amp;quot;the . . . statistics do not count the mistakes [that occur] due to the existence of derived words or proper names&amp;quot; (Chen and Liu, 1992, page 105). Lin et al. (1993) describe a sophisticated model that includes a dictionary and a morphological analyzer. They also present a general statistical model for detecting &apos;unknown words&apos; on part-of-speech sequences. However, their unknown word model has the disadvantage it does not identify a sequence of an unword a particular category, merely as an unknown word (of indeterminate category). For an application like TTS, however, it is necessary to know that particular sequence of is a particular category because, for example, that knowledge could affect the pronunciation. We therefore prefer to build particular models for different classes of unknown words, rather than building a single general model. DICTIONARY REPRESENTATION The lexicon of basic words and stems is represented as a tranducer (Pereira et al., 1994). Most transitions represent mappings between pronunciations, and are costless. Transitions between orthographic words and their parts-of-speech are represented by f -to-category transductions and a (negative log probability) of that word from a 20M corpus; a portion the WFST is given in Figure Besides dictionary the lexicon contains all the Big 5 Chinese code, with their pronunciation(s), plus entries for other characters (e.g., roman letters, numerals, special symbols). Given this dictionary representation, recognizing a word involves representing the input as a finite-state acceptor (FSA) where each arc is labeled a single the input. The of the dictionary WFST with the input FSA contains all and only the (single) lexical entries corresponding to the input. This WFST includes the word costs on arcs transducing € to category labels. Now, input costs are actually for than currently lack estimates for the words themselves. We assign the string cost to lexical entries with the likeliest pronunciation, and a large cost to all other entries. Thus 115/adv, with commonest pronunciation cost 5.98, whereas the rarer pronunciationjiang4, is assigned a high cost. Note also that the current model is zeroeth order in that it uses only unigram costs. Higher order models, e.g. bigram word models, could easily be incorporated into the present architecture if desired. sentences consist of one or more entries from the dictionary, and we can generalize the word recognition problem to the word segmentation problem, by leftrestricting the transitive closure of the dictionary with the input. The result of this left-restriction is an WFST that gives all and only the possible analyses of the input FSA into dictionary entries. In general we do not all possible analyses but rather the This is obtained by computing the least-cost path in the output WFST. The final stage of segmentation involves traversing the best path, collecting into words all seof by part-of-speech-labeled arcs. Figure 2 shows an example of segmentation: the do you say octopus Japanese?&amp;quot;, consists of four words, namely Sc 1 X and I In this case, also a word (e.g. a common abbreviation for as are and yu2 &apos;fish&apos;, so there is (at least) one alternate analysis to be considered. MORPHOLOGICAL ANALYSIS The method just described segments dictionary words, but as noted there are several classes of words that should be handled that are not in the dictionary. One class comprises words derived by productive morphological processes, such as plural noun formation usthe suffix in morphological analysis itself can be handled using well-known techniques from finite-state morphology (Koskenniemi, 1983; Antworth, 1990; Tzoukermann and Liberman, 1990; Karttunen et al., 1992; Sproat, 1992); so, we represent the fact that f attaches to nouns by allowing from the final states of all noun entries, to the initial state of the sub-WFST representing I. However, for our purposes it is not sufficient to represent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word. For derived words that occur in our corpus we can estimate these costs as we would costs for an underived dictionary entry. So, generals&apos; occurs and we estimate its cost at 15.02. But we also need an estimate of the probability for a non-occurring though possiplural form like &apos;pumpkins&apos;. Here we use the Good-Turing estimate (Baayen, 1989; Church and Gale, 1991), whereby the aggregate probability of previously unseen members of a is estimated as the number of observed tokens and the numof types observed only once. fl), to get the aggregate probof novel fi-constructions a corpus we multiby get Finally, to estimate the probability of particular unseen word MAIM, we use the simple bigram backoff model 67 : _NP : Figure 1: Partial chinese Lexicon (NC = noun; NP = proper noun) Figure 2: Input lattice (top) and two segmentations (bottom) of the sentence &apos;How do you say octopus in Japanese&apos;. A non-optimal analysis is shown with dotted lines in the bottom frame. 5.55 7.96 10.63 13.18 3C #1, 0.1 ESSAY FISH :wen2 *:zhang1 :yu2 JAPAN Ei :ri4 ...... ...... ....... ........ 6.51 9.51 ...• ... _ I JAPANESE OCTOPUS 10. HOW SAY 3C :wen2 *:zhang1 At :yu2 E:_nci.T.■ :zen3 e :shuo1 E:_vb 68 Figure 3: An example of affixation: the plural affix 111 ) is computed in the obvious way. Figure 3 shows how this model is implemented as part of the dictionary WFST. There is a (costless) transition the NC node and transition from a final state transduces the grammatical tag PL with cost cost()1449 = cost(C)Z) desired. For the seen word 4411 &apos;generals&apos;, there is an cnc from Ito the node preceding has cost cost() — the cost of the whole path is the desired This representation gives appropriate morphological decomposition, preserving information that be lost by simply listing an unanalyzed form. Note that the backoff model assumes that there is a positive correlation between the frequency of a singular noun and its plural. An analysis of nouns that occur both in the singular and the plural in our database reveals that there is indeed a slight but significant positive — = 0.20, &lt; This suggests that the backoff model is as reasonable a model as we can use in the absence of further information about the expected cost of a plural form. CHINESE PERSONAL NAMES Full Chinese personal names are in one respect simple: they are always of the form FAMILY+GIVEN. The FAMILY name set is restricted: there are a few names, and about ten double-hanzi ones. Given names are most commonly occasionally there are thus four possible name types. The difficulty is that names can consist, in principle, of any of the possible GIVEN names are limited by the total number of some are certainly far more likely than others. For a sequence is a we wish to assign a to that sequence We use an estimate derived from (Chang et al., 1992). For example, given a potential name of the form Fl G1 G2, where Fl a legal FAMILY name and G1 and G2 are each we estimate the probability of that name as the prodof the probability of finding in text; the probability of Fl as a FAMILY name; the probability the first a double GIVEN name being Gl; probability of the second a double GIVEN name being G2; and the probability of a name of the form SINGLE-FAMILY+DOUBLE-GIVEN. The first probability is estimated from a name count in a text database, whereas the last four probabilities are estifrom a large list of personal This model is easily incorporated into the segmenter by building an WFST restricting the names to the four licit types, with costs on the arcs for any particular name summing to an estimate of the cost of that name. This WFST is then the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed. have two such lists, one containing about 17,000 full another containing frequencies of the various name positions, derived from a million names. 69 There are two weaknesses in Chang et al.&apos;s (1992) model, which we improve upon. First, the model asindependence between the first and second a double GIVEN name. Yet, some far more probable in women&apos;s names than they are in men&apos;s and there is a similar list of male-oriented these two lists is generally less likely than would be predicted by the independence model. a partial solution, for pairs of cooccur sufficiently often in our namelists, we use the estimated bigram cost, rather than the independence-based cost. The second weakness is that Chang et al. (1992) asa uniform small cost to unseen GIVEN but we some unseen merely accidentally missing, whereas others are missing for a reason — e.g., because they have a bad connotation. We can address this problem by first observing that many general &apos;meaning&apos; is indicated by so-called &apos;semantic radical&apos;. share the same &apos;radical&apos;, share an easily identifiable structural the and the radical; 4, M share SICKNESS radical; and animal the RAT radical. Some classes are better for names than others: in our corpora, many names are picked from the GRASS class, very few from the class, and none from the We can thus better predict the probability of an unseen in a name by computing a Good-Turing estimate for each radical class. Assumunseen objects each class equiprobable, their probabilities are given by the Good-Turing theorem as: Po cis E(Nis) oc N * E(Nes) is the probability of one unseen E(N) the expected number of once, the total number of is the expected number of unseen use of the Good-Turing equation presumes suitable estimates of the unknown expectations requires. In the denominator, the are well measured by counting, and we replace the expectation by the observation. In the numerator, however, the counts are quite irregular, including several zeros (e.g. RAT, none of whose members were seen). However, is a strong relationship between the of the class. For we substitute a smooth against the number of class elements. This smooth guarantees that there are no zeroes estimated. The final estimating equation is then: ) Po The total of all these class estimates was about 10% off the Turing estimate the probability of we renormalized the estimates so they would sum to gives reasonable results: for six radical classes, Table 1 gives the estimated cost an unseen the class occurring as the second a double GIVEN name. Note that the good classes JADE, GOLD and GRASS have lower costs than the bad classes SICKNESS, DEATH and RAT, as desired. TRANSLITERATIONS OF FOREIGN WORDS names are usually transliterated using whose sequential pronunciation mimics the source language pronunciation of the name. Since foreign names can be of any length, and since their original pronunciation is effectively unlimited, the identification of such names is tricky. Fortunately, there are only a few hunare particularly common in transliteraindeed, the commonest ones, such as 1 often clear indicators that a sequence them is foreign: even a name like which is a legal Chinese personal name, retains a foreign flavor because of . As a first step towards modeling transliterated we have collected all more than once in the roughly 750 foreign names in our dictionary, and we estimate the probability of occurrence of each a transliteration N(hanzii))using maximum likelihood estimate. As with personal names, we also derive an estimate from text of the probabilof finding a transliterated name of any kind Finally, we model the probability of a new translitername as the product of each iin the putative The foreign name model is implemented as an WFST, which is then summed with the WFST implementing the dictionary, morphological rules, and personal names; the transitive closure of the resulting machine is then computed. EVALUATION In this section we present a partial evaluation of the current system in three parts. The first is an evaluation of the system&apos;s ability to mimic humans at the task of segmenting text into word-sized units; the second evaluates the proper name identification; the third measures the performance on morphological analysis. To date we have not done a separate evaluation of foreign name recognition. of the Segmentation as a Whole: Previous reports on Chinese segmentation have invariably current model is too simplistic in several respects. instance, the common &apos;suffixes&apos;, normally transliterated as N.g The interdependence between TO, or g2 is not captured by our model, but this could easily be remedied. 70 1: The cost as a novel GIVEN name (second position) for various radical classes. JADE GOLD GRASS SICKNESS DEATH RAT 14.98 15.52 15.76 16.25 16.30 16.42 cited performance either in terms of a single percentcorrect score, or else a single precision-recall pair. The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text. Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments subjects. To this end, we picked 100 at random containing 4372 total a test corpus. We asked six native speakers — three from Taiwan (T1–T3), and three from the Mainland (M 1 –M3) — to segment the corpus. Since we could not bias the subjects towards a particular segmentation and did not presume linguistic sophistication on their part, the instructions were simple: subjects were to mark all places they might plausibly pause if they were reading the text aloud. An examination of the subjects&apos; bracketings confirmed that these instructions were satisfactory in yielding plausible word-sized units. Various segmentation approaches were then compared with human performance: A greedy through the sentence, taking the longest match with a dictionary entry at each point. An of the longest match, take the shortest match at each point. The method being described — henceforth Two measures that can be used to compare judgments are: Precision. each pair of judges consider one judge as the standard, computing the precision of the other&apos;s judgments relative to this standard. Recall. each pair of judges, consider one judge as the standard, computing the recall of the other&apos;s judgments relative to this standard. for judges J1 and J2,taking J1 as stanand computing the precision and recall for the same results as taking the standard, and computing for J1, respectively, the recall and precision. We therefore used the arithmetic mean of each interjudge precision-recall pair as a single measure of interjudge similarity. Table 2 shows these similarity measures. The average agreement among the human is .76, and the average agreement between and the humans is .75, or about 99% of the inter-human .73 or 96%.) One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a multidimensional scaling on that distance matrix, and plotting the first two most significant dimensions. The result of this is shown in Figure 4. In addition to the methods, GR discussed, we also added to the plot the values for the current only dictionary entries no productively derived words, or names). This is to allow for fair comparison between the statistical method, and is also purely dictionary-based. As can seen, this &apos;pared-down&apos; statistical method perform quite similarly, though the statistical method is slightly better. performs much less like humans than these methods, whereas the full statistical algorithm, including morphological derivatives and names, performs most closely to humans among the automatic methods. It can be also seen clearly in this plot, two of the Taiwan speakers cluster very closely together, and the third Taiwan speaker is also close in most significant dimension (the Two of the Mainlanders also cluster close together but, interestingly, not particularly close to the Taiwan speakers; the third Mainlander is much more similar to the Taiwan speakers. Name Identification: evaluate personal name identification, we randomly selected 186 sencontaining 12,000 our test corpus, and segmented the text automatically, tagging personal names; note that for names there is always a single unambiguous answer, unlike the more general question of which segmentation is correct. The performance was 80.99% recall and 61.83% precision. Interestingly, Chang etal. reported 80.67% recall and 91.87% precision on an 11,000 word corpus: seemingly, our system finds as many names as their system, but with four times as many false hits. However, we have reason to doubt Chang et al.&apos;s performance claims. Without using the same test corpus, direct comparison is obviously difficult; fortunately Chang et al. included a list of about 60 example sentence fragments that exemplified various categories of performance for their system. The performance of our system on those sentences appeared rather better than theirs. Now, on a set of 11 sentence fragments where they reported 100% recall and precision for name identification, we had 80% precision and 73% recall. However, they listed two sets, one consisting of 28 fragments and the other of 22 fragments in which they had 0% precision and recall. On the first of these our system had 86% precision and 64% recall; on the second it had 19% precision and 33% recall. Note that it is in precision that our overall performance would appear to be poorer than that of Chang et al., yet based on their published examples, our 71 Table 2: Similarity matrix for segmentation judgments Judges AG GR ST M1 M2 M3 Ti T2 T3 AG 0.70 0.70 0.43 0.42 0.60 0.60 0.62 0.59 GR 0.99 0.62 0.64 0.79 0.82 0.81 0.72 ST 0.64 0.67 0.80 0.84 0.82 0.74 M1 0.77 0.69 0.71 0.69 0.70 M2 0.72 0.73 0.71 0.70 M3 0.89 0.87 0.80 Ti 0.88 0.82 T2 0.78 system appears to be doing better precisionwise. Thus we have some confidence that our own performance is least as good that of (Chang et al., of Morphological Analysis: Table 3 we present results from small test corpora for some productive affixes; as with names, the segmentation of morphologically derived words is generally either right or wrong. The first four affixes are so-called resultative affixes: they denote some property of the resultant state an verb, as in attain) &apos;cannot forget&apos;. The last affix is the nominal Note that in 7 normally pronounced when part of a resultative it is the table are the (typical) classes of words to which the affix attaches, the number found in the test corpus by the method, the number correct (with a precision measure), and the number missed (with a recall measure). CONCLUSIONS In this paper we have shown that good performance can be achieved on Chinese word segmentation by using probabilistic methods incorporated into a uniform stochastic finite-state model. We believe that the approach reported here compares favorably with other reported approaches, though obviously it is impossible to make meaningful comparisons in the absence of uniform test databases for Chinese segmentation. Perhaps the single most important difference between our work and previous work is the form of the evaluation. As we have observed there is often no single right answer to word segmentation in Chinese. Therefore, claims to the effect that a particular algorithm gets 99% accuracy are meaningless without a clear definition of accuracy. ACKNOWLEDGEMENTS We thank United Informatics for providing us with corpus of Chinese text, and BDC for the `Behavwere recently pointed to (Wang et al., 1992), which we had unfortunately missed in our previous literature search. We hope to compare our method with that of Wang et al. in a future version of this paper. ior Chinese-English Electronic Dictionary&apos;. We further thank Dr. J.-S. Chang of Tsinghua University, for kindly providing us with the name corpora. Finally, we thank two anonymous ACL reviewers for comments.</abstract>
<note confidence="0.663874611764706">REFERENCES Antworth. 1990. A Two-Level Profor Morphological Analysis. Publications in Academic Computing, 16. Summer Institute of Linguistics, Dallas, TX. Baayen. 1989. Corpus-Based Approach to Morphological Productivity: Statistical Analysis Psycholinguistic Interpretation. thesis, Free University, Amsterdam. Jyun-Shen Chang, Shun-De Chen, Ying Zheng, Xian- Thong Liu, and Shu-Jin Ke. 1992. Large-corpusbased methods for Chinese personal name recogniof Chinese Information Processing, 6(3):7-15. Keh-Jiann Chen and Shing-Huan Liu. 1992. Word identification for Mandarin Chinese sentences. of COLING-92, 101-107. COLING. Kenneth Ward Church and William Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probof English bigrams. Speech and Language, 5(1):19-54. DeFrancis. 1984. Chinese Language. University of Hawaii Press, Honolulu. Fan and 1988. Automatic word identification in Chinese sentences by the relaxtechnique. Processing of Chinese Oriental Languages, Lauri Karttunen, Ronald Kaplan, and Annie Zaenen. 1992. Two-level morphology with composition. 141-148. COLING. Koskenniemi. 1983. Morphology: a General Computational Model for Word-Form and Production. thesis, University of Helsinki, Helsinki. 72 • Dimension 2 (14%) 0 + antlgreedy • 0 o current method • o dict. only • Taiwan • Mainland -0.3 -0.2 -0.1 0.0 0.1 0.2 Dimension 1 (62%) Figure 4: Classical metric multidimensional scaling of distance matrix, showing the two most significant dimensions. The percentage scores on the axis labels represent the amount of data explained by the dimension in question. Table 3: Performance on morphological analysis. Affix Pron verb verb verb verb noun 20 30 72 36 141 (prec.) 20 (100%) 29 (97%) 72 (100%) 36 (100%) 139 (99%) (rec.) 12 (63%) 1 (97%) 15 (83%) 11(77%) 6(96%) bu2-xia4 bu2-xia4-qu4 bu4-liao3 de2-liao3 men0 Ming-Yu Lin, Tung-Hui Chiang, and Keh-Yi Su. 1993. A preliminary study on unknown word problem Chinese word segmentation. In 6, pages 119-141. ROCLING. Fernando Pereira, Michael Riley, and Richard Sproat. 1994. Weighted rational transductions and their application to human language processing. In ARPA Workshop on Human Language Technol- 249-254. Advanced Research Projects Agency, March 8-11. Shih. 1986. Prosodic Domain of Tone in Chinese. thesis, UCSD, La Jolla, CA. Richard Sproat and Chilin Shih. 1990. A statistical method for finding word boundaries in Chinese Processing of Chinese and Orien- Languages, Sproat. 1992. and Computation. MIT Press, Cambridge, MA. Evelyne Tzoukermann and Mark Liberman. 1990. A finite-state morphological processor for Spanish. Volume 3, 3: 277-286. COLING. Yongheng Wang, Haiju Su, and Yan Mo. 1990. Auprocessing of chinese words. of Information Processing, 1-11. Liang-Jyh Wang, Wei-Chuan Li, and Chao-Huang Chang. 1992. Recognizing unregistered names mandarin word identification. In COLING-92, 1239-1243. COLING. Zimin Wu and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: Achievements and of the American Society for Science, 73</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Evan Antworth</author>
</authors>
<title>PC-KIMMO: A Two-Level Processor for Morphological Analysis.</title>
<date>1990</date>
<booktitle>Occasional Publications in Academic Computing, 16. Summer Institute of Linguistics,</booktitle>
<location>Dallas, TX.</location>
<contexts>
<context position="8739" citStr="Antworth, 1990" startWordPosition="1371" endWordPosition="1372">is also a word (e.g. a common abbreviation for Japan) as are SC* wen2-zhangl &apos;essay&apos;, and yu2 &apos;fish&apos;, so there is (at least) one alternate analysis to be considered. MORPHOLOGICAL ANALYSIS The method just described segments dictionary words, but as noted there are several classes of words that should be handled that are not in the dictionary. One class comprises words derived by productive morphological processes, such as plural noun formation using the suffix in men0. The morphological analysis itself can be handled using well-known techniques from finite-state morphology (Koskenniemi, 1983; Antworth, 1990; Tzoukermann and Liberman, 1990; Karttunen et al., 1992; Sproat, 1992); so, we represent the fact that f attaches to nouns by allowing E.-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing I. However, for our purposes it is not sufficient to represent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word. For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry. So, nim jiang4-men0 `(military) generals&apos; occurs a</context>
</contexts>
<marker>Antworth, 1990</marker>
<rawString>Evan Antworth. 1990. PC-KIMMO: A Two-Level Processor for Morphological Analysis. Occasional Publications in Academic Computing, 16. Summer Institute of Linguistics, Dallas, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
</authors>
<title>A Corpus-Based Approach to Morphological Productivity: Statistical Analysis and Psycholinguistic Interpretation.</title>
<date>1989</date>
<tech>Ph.D. thesis,</tech>
<institution>Free University,</institution>
<location>Amsterdam.</location>
<contexts>
<context position="9560" citStr="Baayen, 1989" startWordPosition="1510" endWordPosition="1511">state of the sub-WFST representing I. However, for our purposes it is not sufficient to represent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word. For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry. So, nim jiang4-men0 `(military) generals&apos; occurs and we estimate its cost at 15.02. But we also need an estimate of the probability for a non-occurring though possible plural form like 1IflUfl nan2-gual -men° &apos;pumpkins&apos;. Here we use the Good-Turing estimate (Baayen, 1989; Church and Gale, 1991), whereby the aggregate probability of previously unseen members of a construction is estimated as NI/N, where N is the total number of observed tokens and N1 is the number of types observed only once. For II this gives prob(unseen(111) I fl), and to get the aggregate probability of novel fi-constructions in a corpus we multiply this by probtezt(in) to get probrext(uuseen(11)). Finally, to estimate the probability of particular unseen word MAIM, we use the simple bigram backoff model prob(*))413) prob(M)11)probtert(unseen(f1)); 67 E : _NP : al° Figure 1: Partial chinese</context>
</contexts>
<marker>Baayen, 1989</marker>
<rawString>Harald Baayen. 1989. A Corpus-Based Approach to Morphological Productivity: Statistical Analysis and Psycholinguistic Interpretation. Ph.D. thesis, Free University, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jyun-Shen Chang</author>
<author>Ying Zheng Shun-De Chen</author>
<author>XianThong Liu</author>
<author>Shu-Jin Ke</author>
</authors>
<title>Large-corpusbased methods for Chinese personal name recognition.</title>
<date>1992</date>
<journal>Journal of Chinese Information Processing,</journal>
<pages>6--3</pages>
<marker>Chang, Shun-De Chen, Liu, Ke, 1992</marker>
<rawString>Jyun-Shen Chang, Shun-De Chen, Ying Zheng, XianThong Liu, and Shu-Jin Ke. 1992. Large-corpusbased methods for Chinese personal name recognition. Journal of Chinese Information Processing, 6(3):7-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Shing-Huan Liu</author>
</authors>
<title>Word identification for Mandarin Chinese sentences.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>101--107</pages>
<publisher>COLING.</publisher>
<contexts>
<context position="4189" citStr="Chen and Liu, 1992" startWordPosition="637" endWordPosition="640">ation performance: this latter evaluation compares the performance of the system with that of several human judges, since even people do not agree on a single correct way to segment a text. PREVIOUS WORK There is a sizable literature on Chinese word segmentation: recent reviews include (Wang et al., 1990; Wu and Tseng, 1993). Roughly, previous work can be classified into purely statistical approaches (Sproat and Shih, 1990), statistical approaches which incorporate lexical knowledge (Fan and Tsai, 1988; Lin et al., 1993), and approaches that include lexical knowledge combined with heuristics (Chen and Liu, 1992). 66 Chen and Liu&apos;s (1992) algorithm matches words of an input sentence against a dictionary; in cases where various parses are possible, a set of heuristics is applied to disambiguate the analyses. Various morphological rules are then applied to allow for morphologically complex words that are not in the dictionary. Precision and recall rates of over 99% are reported, but note that this covers only words that are in the dictionary: &amp;quot;the . . . statistics do not count the mistakes [that occur] due to the existence of derived words or proper names&amp;quot; (Chen and Liu, 1992, page 105). Lin et al. (199</context>
</contexts>
<marker>Chen, Liu, 1992</marker>
<rawString>Keh-Jiann Chen and Shing-Huan Liu. 1992. Word identification for Mandarin Chinese sentences. In Proceedings of COLING-92, pages 101-107. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>William Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--1</pages>
<contexts>
<context position="9584" citStr="Church and Gale, 1991" startWordPosition="1512" endWordPosition="1515">ub-WFST representing I. However, for our purposes it is not sufficient to represent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word. For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry. So, nim jiang4-men0 `(military) generals&apos; occurs and we estimate its cost at 15.02. But we also need an estimate of the probability for a non-occurring though possible plural form like 1IflUfl nan2-gual -men° &apos;pumpkins&apos;. Here we use the Good-Turing estimate (Baayen, 1989; Church and Gale, 1991), whereby the aggregate probability of previously unseen members of a construction is estimated as NI/N, where N is the total number of observed tokens and N1 is the number of types observed only once. For II this gives prob(unseen(111) I fl), and to get the aggregate probability of novel fi-constructions in a corpus we multiply this by probtezt(in) to get probrext(uuseen(11)). Finally, to estimate the probability of particular unseen word MAIM, we use the simple bigram backoff model prob(*))413) prob(M)11)probtert(unseen(f1)); 67 E : _NP : al° Figure 1: Partial chinese Lexicon (NC = noun; NP </context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Kenneth Ward Church and William Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5(1):19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeFrancis</author>
</authors>
<title>The Chinese Language.</title>
<date>1984</date>
<publisher>University of Hawaii Press,</publisher>
<location>Honolulu.</location>
<contexts>
<context position="1123" citStr="DeFrancis, 1984" startWordPosition="158" endWordPosition="159">viding pronunciations for these words; the method incorporates a class-based model in its treatment of personal names. We also evaluate the system&apos;s performance, taking into account the fact that people often do not agree on a single segmentation. THE PROBLEM The initial step of any text analysis task is the tokenization of the input into words. For many writing systems, using whitespace as a delimiter for words yields reasonable results. However, for Chinese and other systems where whitespace is not used to delimit words, such trivial schemes will not work. Chinese writing is moiphosyllabic (DeFrancis, 1984), meaning that each hanzi– &apos;Chinese character&apos; – (nearly always) represents a single syllable that is (usually) also a single morpheme. Since in Chinese, as in English, words may be polysyllabic, and since hanzi are written with no intervening spaces, it is not trivial to reconstruct which hanzi to group into words. While for some applications it may be possible to bypass the word-segmentation problem and work straight from hanzi, there are several reasons why this approach will not work in a text-to-speech (TI&apos;S) system for Mandarin Chinese — the primary intended application of our segmenter.</context>
</contexts>
<marker>DeFrancis, 1984</marker>
<rawString>John DeFrancis. 1984. The Chinese Language. University of Hawaii Press, Honolulu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-K Fan</author>
<author>W-H Tsai</author>
</authors>
<title>Automatic word identification in Chinese sentences by the relaxation technique.</title>
<date>1988</date>
<booktitle>Computer Processing of Chinese and Oriental Languages,</booktitle>
<pages>4--33</pages>
<contexts>
<context position="4077" citStr="Fan and Tsai, 1988" startWordPosition="620" endWordPosition="623">es). We evaluate various specific aspects of the segmentation, and provide an evaluation of the overall segmentation performance: this latter evaluation compares the performance of the system with that of several human judges, since even people do not agree on a single correct way to segment a text. PREVIOUS WORK There is a sizable literature on Chinese word segmentation: recent reviews include (Wang et al., 1990; Wu and Tseng, 1993). Roughly, previous work can be classified into purely statistical approaches (Sproat and Shih, 1990), statistical approaches which incorporate lexical knowledge (Fan and Tsai, 1988; Lin et al., 1993), and approaches that include lexical knowledge combined with heuristics (Chen and Liu, 1992). 66 Chen and Liu&apos;s (1992) algorithm matches words of an input sentence against a dictionary; in cases where various parses are possible, a set of heuristics is applied to disambiguate the analyses. Various morphological rules are then applied to allow for morphologically complex words that are not in the dictionary. Precision and recall rates of over 99% are reported, but note that this covers only words that are in the dictionary: &amp;quot;the . . . statistics do not count the mistakes [th</context>
</contexts>
<marker>Fan, Tsai, 1988</marker>
<rawString>C.-K. Fan and W.-H. Tsai. 1988. Automatic word identification in Chinese sentences by the relaxation technique. Computer Processing of Chinese and Oriental Languages, 4:33-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Ronald Kaplan</author>
<author>Annie Zaenen</author>
</authors>
<title>Two-level morphology with composition.</title>
<date>1992</date>
<booktitle>In COLING-92,</booktitle>
<pages>141--148</pages>
<publisher>COLING.</publisher>
<contexts>
<context position="8795" citStr="Karttunen et al., 1992" startWordPosition="1377" endWordPosition="1380">apan) as are SC* wen2-zhangl &apos;essay&apos;, and yu2 &apos;fish&apos;, so there is (at least) one alternate analysis to be considered. MORPHOLOGICAL ANALYSIS The method just described segments dictionary words, but as noted there are several classes of words that should be handled that are not in the dictionary. One class comprises words derived by productive morphological processes, such as plural noun formation using the suffix in men0. The morphological analysis itself can be handled using well-known techniques from finite-state morphology (Koskenniemi, 1983; Antworth, 1990; Tzoukermann and Liberman, 1990; Karttunen et al., 1992; Sproat, 1992); so, we represent the fact that f attaches to nouns by allowing E.-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing I. However, for our purposes it is not sufficient to represent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word. For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry. So, nim jiang4-men0 `(military) generals&apos; occurs and we estimate its cost at 15.02. But we also need an es</context>
</contexts>
<marker>Karttunen, Kaplan, Zaenen, 1992</marker>
<rawString>Lauri Karttunen, Ronald Kaplan, and Annie Zaenen. 1992. Two-level morphology with composition. In COLING-92, pages 141-148. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-Level Morphology: a General Computational Model for Word-Form Recognition and Production.</title>
<date>1983</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Helsinki,</institution>
<location>Helsinki.</location>
<contexts>
<context position="8723" citStr="Koskenniemi, 1983" startWordPosition="1369" endWordPosition="1370"> this case, El ri4 is also a word (e.g. a common abbreviation for Japan) as are SC* wen2-zhangl &apos;essay&apos;, and yu2 &apos;fish&apos;, so there is (at least) one alternate analysis to be considered. MORPHOLOGICAL ANALYSIS The method just described segments dictionary words, but as noted there are several classes of words that should be handled that are not in the dictionary. One class comprises words derived by productive morphological processes, such as plural noun formation using the suffix in men0. The morphological analysis itself can be handled using well-known techniques from finite-state morphology (Koskenniemi, 1983; Antworth, 1990; Tzoukermann and Liberman, 1990; Karttunen et al., 1992; Sproat, 1992); so, we represent the fact that f attaches to nouns by allowing E.-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing I. However, for our purposes it is not sufficient to represent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word. For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry. So, nim jiang4-men0 `(military) ge</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi. 1983. Two-Level Morphology: a General Computational Model for Word-Form Recognition and Production. Ph.D. thesis, University of Helsinki, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Yu Lin</author>
<author>Tung-Hui Chiang</author>
<author>Keh-Yi Su</author>
</authors>
<title>A preliminary study on unknown word problem in Chinese word segmentation.</title>
<date>1993</date>
<booktitle>In ROCLING 6,</booktitle>
<pages>119--141</pages>
<publisher>ROCLING.</publisher>
<contexts>
<context position="4096" citStr="Lin et al., 1993" startWordPosition="624" endWordPosition="627">ious specific aspects of the segmentation, and provide an evaluation of the overall segmentation performance: this latter evaluation compares the performance of the system with that of several human judges, since even people do not agree on a single correct way to segment a text. PREVIOUS WORK There is a sizable literature on Chinese word segmentation: recent reviews include (Wang et al., 1990; Wu and Tseng, 1993). Roughly, previous work can be classified into purely statistical approaches (Sproat and Shih, 1990), statistical approaches which incorporate lexical knowledge (Fan and Tsai, 1988; Lin et al., 1993), and approaches that include lexical knowledge combined with heuristics (Chen and Liu, 1992). 66 Chen and Liu&apos;s (1992) algorithm matches words of an input sentence against a dictionary; in cases where various parses are possible, a set of heuristics is applied to disambiguate the analyses. Various morphological rules are then applied to allow for morphologically complex words that are not in the dictionary. Precision and recall rates of over 99% are reported, but note that this covers only words that are in the dictionary: &amp;quot;the . . . statistics do not count the mistakes [that occur] due to th</context>
</contexts>
<marker>Lin, Chiang, Su, 1993</marker>
<rawString>Ming-Yu Lin, Tung-Hui Chiang, and Keh-Yi Su. 1993. A preliminary study on unknown word problem in Chinese word segmentation. In ROCLING 6, pages 119-141. ROCLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
<author>Richard Sproat</author>
</authors>
<title>Weighted rational transductions and their application to human language processing.</title>
<date>1994</date>
<booktitle>In ARPA Workshop on Human Language Technology,</booktitle>
<pages>249--254</pages>
<contexts>
<context position="5677" citStr="Pereira et al., 1994" startWordPosition="881" endWordPosition="884">that it does not identify a sequence of hanzi as an unknown word of a particular category, but merely as an unknown word (of indeterminate category). For an application like TTS, however, it is necessary to know that a particular sequence of hanzi is of a particular category because, for example, that knowledge could affect the pronunciation. We therefore prefer to build particular models for different classes of unknown words, rather than building a single general model. DICTIONARY REPRESENTATION The lexicon of basic words and stems is represented as a weighted finite-state tranducer (WFST) (Pereira et al., 1994). Most transitions represent mappings between hanzi and pronunciations, and are costless. Transitions between orthographic words and their parts-of-speech are represented by f -to-category transductions and a unigram cost (negative log probability) of that word estimated from a 20M hanzi training corpus; a portion of the WFST is given in Figure 1.2 Besides dictionary words, the lexicon contains all hanzi in the Big 5 Chinese code, with their pronunciation(s), plus entries for other characters (e.g., roman letters, numerals, special symbols). Given this dictionary representation, recognizing a </context>
</contexts>
<marker>Pereira, Riley, Sproat, 1994</marker>
<rawString>Fernando Pereira, Michael Riley, and Richard Sproat. 1994. Weighted rational transductions and their application to human language processing. In ARPA Workshop on Human Language Technology, pages 249-254. Advanced Research Projects Agency, March 8-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chilin Shih</author>
</authors>
<title>The Prosodic Domain of Tone Sandhi</title>
<date>1986</date>
<booktitle>in Chinese. Ph.D. thesis, UCSD,</booktitle>
<location>La Jolla, CA.</location>
<contexts>
<context position="2110" citStr="Shih, 1986" startWordPosition="318" endWordPosition="319">he word-segmentation problem and work straight from hanzi, there are several reasons why this approach will not work in a text-to-speech (TI&apos;S) system for Mandarin Chinese — the primary intended application of our segmenter. These reasons include: 1. Many hanzi are homographs whose pronunciation depends upon word affiliation. So, (1/9 is pronounced de01 when it is a prenominal modification marker, but di4 in the word El Efl mu4di4 &apos;goal&apos;: tt is normally ganl &apos;dry&apos;, but qian2 in a person&apos;s given name. 2. Some phonological rules depend upon correct wordsegmentation, including Third Tone Sandhi (Shih, 1986), which changes a 3 tone into a 2 tone before another 3 tone: /.1N xiao3 [lao3 shu3] &apos;lit&apos;We use pinyin transliteration with numbers representing tones. tle rat&apos;, becomes xiao3 I lao2-shu3 1, rather than xiao2 I lao2-shu3 J, because the rule first applies within the word lao3-shu3, blocking its phrasal application. While a minimal requirement for building a Chinese word-segmenter is a dictionary, a dictionary is insufficient since there are several classes of words that are not generally found in dictionaries. Among these: 1. Morphologically Derived Words: 44311 xiao3- jiang4-men0 (little gene</context>
</contexts>
<marker>Shih, 1986</marker>
<rawString>Chilin Shih. 1986. The Prosodic Domain of Tone Sandhi in Chinese. Ph.D. thesis, UCSD, La Jolla, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
</authors>
<title>A statistical method for finding word boundaries</title>
<date>1990</date>
<booktitle>in Chinese text. Computer Processing of Chinese and Oriental Languages,</booktitle>
<pages>4--336</pages>
<contexts>
<context position="3997" citStr="Sproat and Shih, 1990" startWordPosition="610" endWordPosition="613">em&apos;s ability to segment text appropriately (rather than on its pronunciation abilities). We evaluate various specific aspects of the segmentation, and provide an evaluation of the overall segmentation performance: this latter evaluation compares the performance of the system with that of several human judges, since even people do not agree on a single correct way to segment a text. PREVIOUS WORK There is a sizable literature on Chinese word segmentation: recent reviews include (Wang et al., 1990; Wu and Tseng, 1993). Roughly, previous work can be classified into purely statistical approaches (Sproat and Shih, 1990), statistical approaches which incorporate lexical knowledge (Fan and Tsai, 1988; Lin et al., 1993), and approaches that include lexical knowledge combined with heuristics (Chen and Liu, 1992). 66 Chen and Liu&apos;s (1992) algorithm matches words of an input sentence against a dictionary; in cases where various parses are possible, a set of heuristics is applied to disambiguate the analyses. Various morphological rules are then applied to allow for morphologically complex words that are not in the dictionary. Precision and recall rates of over 99% are reported, but note that this covers only words</context>
</contexts>
<marker>Sproat, Shih, 1990</marker>
<rawString>Richard Sproat and Chilin Shih. 1990. A statistical method for finding word boundaries in Chinese text. Computer Processing of Chinese and Oriental Languages, 4:336-351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
</authors>
<title>Morphology and Computation.</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8810" citStr="Sproat, 1992" startWordPosition="1381" endWordPosition="1382">angl &apos;essay&apos;, and yu2 &apos;fish&apos;, so there is (at least) one alternate analysis to be considered. MORPHOLOGICAL ANALYSIS The method just described segments dictionary words, but as noted there are several classes of words that should be handled that are not in the dictionary. One class comprises words derived by productive morphological processes, such as plural noun formation using the suffix in men0. The morphological analysis itself can be handled using well-known techniques from finite-state morphology (Koskenniemi, 1983; Antworth, 1990; Tzoukermann and Liberman, 1990; Karttunen et al., 1992; Sproat, 1992); so, we represent the fact that f attaches to nouns by allowing E.-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing I. However, for our purposes it is not sufficient to represent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word. For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry. So, nim jiang4-men0 `(military) generals&apos; occurs and we estimate its cost at 15.02. But we also need an estimate of the p</context>
</contexts>
<marker>Sproat, 1992</marker>
<rawString>Richard Sproat. 1992. Morphology and Computation. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evelyne Tzoukermann</author>
<author>Mark Liberman</author>
</authors>
<title>A finite-state morphological processor for Spanish.</title>
<date>1990</date>
<booktitle>In COLING-90,</booktitle>
<volume>3</volume>
<pages>3--277</pages>
<publisher>COLING.</publisher>
<contexts>
<context position="8771" citStr="Tzoukermann and Liberman, 1990" startWordPosition="1373" endWordPosition="1376">e.g. a common abbreviation for Japan) as are SC* wen2-zhangl &apos;essay&apos;, and yu2 &apos;fish&apos;, so there is (at least) one alternate analysis to be considered. MORPHOLOGICAL ANALYSIS The method just described segments dictionary words, but as noted there are several classes of words that should be handled that are not in the dictionary. One class comprises words derived by productive morphological processes, such as plural noun formation using the suffix in men0. The morphological analysis itself can be handled using well-known techniques from finite-state morphology (Koskenniemi, 1983; Antworth, 1990; Tzoukermann and Liberman, 1990; Karttunen et al., 1992; Sproat, 1992); so, we represent the fact that f attaches to nouns by allowing E.-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing I. However, for our purposes it is not sufficient to represent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word. For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry. So, nim jiang4-men0 `(military) generals&apos; occurs and we estimate its cost at 15.02</context>
</contexts>
<marker>Tzoukermann, Liberman, 1990</marker>
<rawString>Evelyne Tzoukermann and Mark Liberman. 1990. A finite-state morphological processor for Spanish. In COLING-90, Volume 3, pages 3: 277-286. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongheng Wang</author>
<author>Haiju Su</author>
<author>Yan Mo</author>
</authors>
<title>Automatic processing of chinese words.</title>
<date>1990</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>4</volume>
<issue>4</issue>
<pages>1--11</pages>
<contexts>
<context position="3875" citStr="Wang et al., 1990" startWordPosition="591" endWordPosition="594">tputs word pronunciations, with default pronunciations for hanzi it cannot group; we focus here primarily on the system&apos;s ability to segment text appropriately (rather than on its pronunciation abilities). We evaluate various specific aspects of the segmentation, and provide an evaluation of the overall segmentation performance: this latter evaluation compares the performance of the system with that of several human judges, since even people do not agree on a single correct way to segment a text. PREVIOUS WORK There is a sizable literature on Chinese word segmentation: recent reviews include (Wang et al., 1990; Wu and Tseng, 1993). Roughly, previous work can be classified into purely statistical approaches (Sproat and Shih, 1990), statistical approaches which incorporate lexical knowledge (Fan and Tsai, 1988; Lin et al., 1993), and approaches that include lexical knowledge combined with heuristics (Chen and Liu, 1992). 66 Chen and Liu&apos;s (1992) algorithm matches words of an input sentence against a dictionary; in cases where various parses are possible, a set of heuristics is applied to disambiguate the analyses. Various morphological rules are then applied to allow for morphologically complex words</context>
</contexts>
<marker>Wang, Su, Mo, 1990</marker>
<rawString>Yongheng Wang, Haiju Su, and Yan Mo. 1990. Automatic processing of chinese words. Journal of Chinese Information Processing, 4(4): 1-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang-Jyh Wang</author>
<author>Wei-Chuan Li</author>
<author>Chao-Huang Chang</author>
</authors>
<title>Recognizing unregistered names for mandarin word identification.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>1239--1243</pages>
<publisher>COLING.</publisher>
<marker>Wang, Li, Chang, 1992</marker>
<rawString>Liang-Jyh Wang, Wei-Chuan Li, and Chao-Huang Chang. 1992. Recognizing unregistered names for mandarin word identification. In Proceedings of COLING-92, pages 1239-1243. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zimin Wu</author>
<author>Gwyneth Tseng</author>
</authors>
<title>Chinese text segmentation for text retrieval: Achievements and problems.</title>
<date>1993</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>44--9</pages>
<contexts>
<context position="3896" citStr="Wu and Tseng, 1993" startWordPosition="595" endWordPosition="598">ations, with default pronunciations for hanzi it cannot group; we focus here primarily on the system&apos;s ability to segment text appropriately (rather than on its pronunciation abilities). We evaluate various specific aspects of the segmentation, and provide an evaluation of the overall segmentation performance: this latter evaluation compares the performance of the system with that of several human judges, since even people do not agree on a single correct way to segment a text. PREVIOUS WORK There is a sizable literature on Chinese word segmentation: recent reviews include (Wang et al., 1990; Wu and Tseng, 1993). Roughly, previous work can be classified into purely statistical approaches (Sproat and Shih, 1990), statistical approaches which incorporate lexical knowledge (Fan and Tsai, 1988; Lin et al., 1993), and approaches that include lexical knowledge combined with heuristics (Chen and Liu, 1992). 66 Chen and Liu&apos;s (1992) algorithm matches words of an input sentence against a dictionary; in cases where various parses are possible, a set of heuristics is applied to disambiguate the analyses. Various morphological rules are then applied to allow for morphologically complex words that are not in the </context>
</contexts>
<marker>Wu, Tseng, 1993</marker>
<rawString>Zimin Wu and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: Achievements and problems. Journal of the American Society for Information Science, 44(9):532-542.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>