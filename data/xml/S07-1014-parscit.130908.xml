<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001719">
<title confidence="0.873571">
SemEval-2007 Task 15: TempEval Temporal Relation Identification
</title>
<author confidence="0.9150265">
Marc Verhagen†, Robert Gaizauskas$, Frank Schilder*, Mark Hepple$,
Graham Katz* and James Pustejovsky†
</author>
<affiliation confidence="0.63254875">
† Brandeis University, {marc,jamesp}@cs.Brandeis.edu
$ University of Sheffield, {r.gaizauskas,m.hepple}@dcs.shef.ac.uk
* Thomson Legal &amp; Regulatory, frank.schilder@thomson.com,
* Stanford University, egkatz@stanford.edu
</affiliation>
<sectionHeader confidence="0.989492" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99974">
The TempEval task proposes a simple way
to evaluate automatic extraction of temporal
relations. It avoids the pitfalls of evaluat-
ing a graph of inter-related labels by defin-
ing three sub tasks that allow pairwise eval-
uation of temporal relations. The task not
only allows straightforward evaluation, it
also avoids the complexities of full tempo-
ral parsing.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962837209302">
Newspaper texts, narratives and other texts describe
events that occur in time and specify the temporal
location and order of these events. Text comprehen-
sion, amongst other capabilities, clearly requires the
capability to identify the events described in a text
and locate these in time. This capability is crucial to
a wide range of NLP applications, from document
summarization and question answering to machine
translation.
Recent work on the annotation of events and tem-
poral relations has resulted in both a de-facto stan-
dard for expressing these relations and a hand-built
gold standard of annotated texts. TimeML (Puste-
jovsky et al., 2003a) is an emerging ISO standard
for annotation of events, temporal expressions and
the anchoring and ordering relations between them.
TimeBank (Pustejovsky et al., 2003b; Boguraev et
al., forthcoming) was originally conceived of as a
proof of concept that illustrates the TimeML lan-
guage, but has since gone through several rounds of
revisions and can now be considered a gold standard
for temporal information. TimeML and TimeBank
have already been used as the basis for automatic
time, event and temporal relation annotation tasks in
a number of research projects in recent years (Mani
et al., 2006; Boguraev et al., forthcoming).
An open evaluation challenge in the area of tem-
poral annotation should serve to drive research for-
ward, as it has in other areas of NLP. The auto-
matic identification of all temporal referring expres-
sions, events and temporal relations within a text is
the ultimate aim of research in this area. However,
addressing this aim in a first evaluation challenge
was judged to be too difficult, both for organizers
and participants, and a staged approach was deemed
more effective. Thus we here present an initial eval-
uation exercise based on three limited tasks that we
believe are realistic both from the perspective of as-
sembling resources for development and testing and
from the perspective of developing systems capable
of addressing the tasks. They are also tasks, which
should they be performable automatically, have ap-
plication potential.
</bodyText>
<sectionHeader confidence="0.988873" genericHeader="introduction">
2 Task Description
</sectionHeader>
<bodyText confidence="0.9993243">
The tasks as originally proposed were modified
slightly during the course of resource development
for the evaluation exercise due to constraints on data
and annotator availability. In the following we de-
scribe the tasks as they were ultimately realized in
the evaluation.
There were three tasks – A, B and C. For all
three tasks the data provided for testing and train-
ing includes annotations identifying: (1) sentence
boundaries; (2) all temporal referring expression as
</bodyText>
<page confidence="0.981996">
75
</page>
<bodyText confidence="0.991212657142857">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 75–80,
Prague, June 2007. c�2007 Association for Computational Linguistics
specified by TIMEX3; (3) all events as specified
in TimeML; (4) selected instances of temporal re-
lations, as relevant to the given task. For tasks A and
B a restricted set of event terms were identified –
those whose stems occurred twenty times or more in
TimeBank. This set is referred to as the Event Target
List or ETL.
TASK A This task addresses only the temporal re-
lations holding between time and event expressions
that occur within the same sentence. Furthermore
only event expressions that occur within the ETL are
considered. In the training and test data, TLINK an-
notations for these temporal relations are provided,
the difference being that in the test data the relation
type is withheld. The task is to supply this label.
TASK B This task addresses only the temporal
relations holding between the Document Creation
Time (DCT) and event expressions. Again only
event expressions that occur within the ETL are con-
sidered. As in Task A, TLINK annotations for these
temporal relations are provided in both training and
test data, and again the relation type is withheld in
the test data and the task is to supply this label.
TASK C Task C relies upon the idea of their being
a main event within a sentence, typically the syn-
tactically dominant verb. The aim is to assign the
temporal relation between the main events of adja-
cent sentences. In both training and test data the
main events are identified (via an attribute in the
event annotation) and TLINKs between these main
events are supplied. As for Tasks A and B, the task
here is to supply the correct relation label for these
TLINKs.
</bodyText>
<sectionHeader confidence="0.81144" genericHeader="method">
3 Data Description and Data Preparation
</sectionHeader>
<bodyText confidence="0.99964425">
The TempEval annotation language is a simplified
version of TimeML 1. For TempEval, we use the fol-
lowing five tags: TempEval, s, TIMEX3, EVENT,
and TLINK. TempEval is the document root and s
marks sentence boundaries. All sentence tags in the
TempEval data are automatically created using the
Alembic Natural Language processing tools. The
other three tags are discussed here in more detail:
</bodyText>
<footnote confidence="0.954531">
1See http://www.timeml.org for language specifica-
tions and annotation guidelines
</footnote>
<listItem confidence="0.998002307692307">
• TIMEX3. Tags the time expressions in the text.
It is identical to the TIMEX3 tag in TimeML.
See the TimeML specifications and guidelines
for further details on this tag and its attributes.
Each document has one special TIMEX3 tag,
the Document Creation Time, which is inter-
preted as an interval that spans a whole day.
• EVENT. Tags the event expressions in the text.
The interpretation of what an event is is taken
from TimeML where an event is a cover term
for predicates describing situations that happen
or occur as well as some, but not all, stative
predicates. Events can be denoted by verbs,
nouns or adjectives. The TempEval event an-
notation scheme is somewhat simpler than that
used in TimeML, whose complexity was de-
signed to handle event expressions that intro-
duced multiple event instances (consider, e.g.
He taught on Wednesday and Friday). This
complication was not necessary for the Tem-
pEval data. The most salient attributes encode
tense, aspect, modality and polarity informa-
tion. For TempEval task C, one extra attribute
is added: mainevent, with possible values
YES and NO.
• TLINK. This is a simplified version of the
</listItem>
<bodyText confidence="0.9112065">
TimeML TLINK tag. The relation types for the
TimeML version form a fine-grained set based
on James Allen’s interval logic (Allen, 1983).
For TempEval, we use only six relation types
including the three core relations BEFORE, AF-
TER, and OVERLAP, the two less specific re-
lations BEFORE-OR-OVERLAP and OVERLAP-
OR-AFTER for ambiguous cases, and finally the
relation VAGUE for those cases where no partic-
ular relation can be established.
As stated above the TLINKs of concern for each
task are explicitly included in the training and in the
test data. However, in the latter the relType at-
tribute of each TLINK is set to UNKNOWN. For each
task the system must replace the UNKNOWN values
with one of the six allowed values listed above.
The EVENT and TIMEX3 annotations were taken
verbatim from TimeBank version 1.2.2 The annota-
</bodyText>
<footnote confidence="0.9979885">
2TimeBank 1.2 is available for free through the Linguistic
Data Consortium, see http://www.timeml.org for more
</footnote>
<page confidence="0.986949">
76
</page>
<bodyText confidence="0.999953774193548">
tion procedure for TLINK tags involved dual anno-
tation by seven annotators using a web-based anno-
tation interface. After this phase, three experienced
annotators looked at all occurrences where two an-
notators differed as to what relation type to select
and decided on the best option. For task C, there
was an extra annotation phase where the main events
were marked up. Main events are those events that
are syntactically dominant in the sentences.
It should be noted that annotation of temporal re-
lations is not an easy task for humans due to ram-
pant temporal vagueness in natural language. As a
result, inter-annotator agreement scores are well be-
low the often kicked-around threshold of 90%, both
for the TimeML relation set as well as the TempEval
relation set. For TimeML temporal links, an inter-
annotator agreement of 0.77 was reported, where
agreement was measured by the average of preci-
sion and recall. The numbers for TempEval are even
lower, with an agreement of 0.72 for anchorings of
events to times (tasks A and B) and an agreement of
0.65 for event orderings (task C). Obviously, num-
bers like this temper the expectations for automatic
temporal linking.
The lower number for TempEval came a bit as
a surprise because, after all, there were fewer rela-
tions to choose form. However, the TempEval an-
notation task is different in the sense that it did not
give the annotator the option to ignore certain pairs
of events and made it therefore impossible to skip
hard-to-classify temporal relations.
</bodyText>
<sectionHeader confidence="0.988415" genericHeader="method">
4 Evaluating Temporal Relations
</sectionHeader>
<bodyText confidence="0.999806666666667">
In full temporal annotation, evaluation of temporal
annotation runs into the same issues as evaluation of
anaphora chains: simple pairwise comparisons may
not be the best way to evaluate. In temporal annota-
tion, for example, one may wonder how the response
in (1) should be evaluated given the key in (2).
</bodyText>
<listItem confidence="0.9975885">
(1) {A before B, A before C, B equals C}
(2) {A after B, A after C, B equals C}
</listItem>
<bodyText confidence="0.968093954545454">
Scoring (1) at 0.33 precision misses the interde-
pendence between the temporal relations. What we
need to compare is not individual judgements but
two partial orders.
details.
For TempEval however, the tasks are defined in
a such a way that a simple pairwise comparison is
possible since we do not aim to create a full temporal
graph and judgements are made in isolation.
Recall that there are three basic temporal relations
(BEFORE, OVERLAP, and AFTER) as well as three
disjunctions over this set (BEFORE-OR-OVERLAP,
OVERLAP-OR-AFTER and VAGUE). The addition
of these disjunctions raises the question of how to
score a response of, for example, BEFORE given a
key of BEFORE-OR-OVERLAP. We use two scor-
ing schemes: strict and relaxed. The strict scoring
scheme only counts exact matches as success. For
example, if the key is OVERLAP and the response
BEFORE-OR-OVERLAP than this is counted as fail-
ure. We can use standard definitions of precision
and recall
</bodyText>
<equation confidence="0.950535">
Precision = Rc/R
Recall = Rc/K
</equation>
<bodyText confidence="0.9990954">
where Rc is number of correct answers in the re-
sponse, R the total number of answers in the re-
sponse, and K the total number of answers in the
key. For the relaxed scoring scheme, precision and
recall are defined as
</bodyText>
<equation confidence="0.8627345">
Precision = Rcw/R
Recall = Rcw/K
</equation>
<bodyText confidence="0.9985025">
where Rcw reflects the weighted number of correct
answers. A response is not simply counted as 1 (cor-
rect) or 0 (incorrect), but is assigned one of the val-
ues in table 1.
</bodyText>
<table confidence="0.998424571428571">
B O A B-O O-A V
B 1 0 0 0.5 0 0.33
O 0 1 0 0.5 0.5 0.33
A 0 0 1 0 0.5 0.33
B-O 0.5 0.5 0 1 0.5 0.67
O-A 0 0.5 0.5 0.5 1 0.67
V 0.33 0.33 0.33 0.67 0.67 1
</table>
<tableCaption confidence="0.999929">
Table 1: Evaluation weights
</tableCaption>
<bodyText confidence="0.9998578">
This scheme gives partial credit for disjunctions,
but not so much that non-commitment edges out pre-
cise assignments. For example, assigning VAGUE as
the relation type for every temporal relation results
in a precision of 0.33.
</bodyText>
<page confidence="0.998855">
77
</page>
<sectionHeader confidence="0.996592" genericHeader="method">
5 Participants
</sectionHeader>
<bodyText confidence="0.999988291139241">
Six teams participated in the TempEval tasks. Three
of the teams used statistics exclusively, one used a
rule-based system and the other two employed a hy-
brid approach. This section gives a short description
of the participating systems.
CU-TMP trained three support vector machine
(SVM) models, one for each task. All models used
the gold-standard TimeBank features for events and
times as well as syntactic features derived from the
text. Additionally, the relation types obtained by
running the task B system on the training data for
Task A and Task C, were added as a feature to the
two latter systems. A subset of features was selected
using cross-validations on the training data, dis-
carding features whose removal improved the cross-
validation F-score. When applied to the test data,
the Task B system was run first in order to supply
the necessary features to the Task A and Task C sys-
tems.
LCC-TE automatically identifies temporal refer-
ring expressions, events and temporal relations in
text using a hybrid approach, leveraging various
NLP tools and linguistic resources at LCC. For tem-
poral expression labeling and normalization, they
used a syntactic pattern matching tool that deploys a
large set of hand-crafted finite state rules. For event
detection, they used a small set of heuristics as well
as a lexicon to determine whether or not a token is
an event, based on the lemma, part of speech and
WordNet senses. For temporal relation discovery,
LCC-TE used a large set of syntactic and semantic
features as input to a machine learning components.
NAIST-japan defined the temporal relation iden-
tification task as a sequence labeling model, in
which the target pairs – a TIMEX3 and an EVENT
– are linearly ordered in the document. For analyz-
ing the relative positions, they used features from
dependency trees which are obtained from a depen-
dency parser. The relative position between the tar-
get EVENT and a word in the target TIMEX3 is used
as a feature for a machine learning based relation
identifier. The relative positions between a word in
the target entities and another word are also intro-
duced.
The USFD system uses an off-the-shelf Machine
Learning suite(WEKA), treating the assignment of
temporal relations as a simple classification task.
The features used were the ones provided in the
TempEval data annotation together with a few fea-
tures straightforwardly computed from the docu-
ment without any deeper NLP analysis.
WVALI’s approach for discovering intra-
sentence temporal relations relies on sentence-level
syntactic tree generation, bottom-up propaga-
tion of the temporal relations between syntactic
constituents, a temporal reasoning mechanism
that relates the two targeted temporal entities to
their closest ancestor and then to each other, and
on conflict resolution heuristics. In establishing
the temporal relation between an event and the
Document Creation Time (DCT), the temporal ex-
pressions directly or indirectly linked to that event
are first analyzed and, if no relation is detected,
the temporal relation with the DCT is propagated
top-down in the syntactic tree. Inter-sentence tem-
poral relations are discovered by applying several
heuristics and by using statistical data extracted
from the training corpus.
XRCE-T used a rule-based system that relies on
a deep syntactic analyzer that was extended to treat
temporal expressions. Temporal processing is inte-
grated into a more generic tool, a general purpose
linguistic analyzer, and is thus a complement for a
better general purpose text understanding system.
Temporal analysis is intertwined with syntactico-
semantic text processing like deep syntactic analy-
sis and determination of thematic roles. TempEval-
specific treatment is performed in a post-processing
stage.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.99629">
The results for the six teams are presented in tables
2, 3, and 4.
</bodyText>
<table confidence="0.9989884">
team strict relaxed
P R F P R F
CU-TMP 0.61 0.61 0.61 0.63 0.63 0.63
LCC-TE 0.59 0.57 0.58 0.61 0.60 0.60
NAIST 0.61 0.61 0.61 0.63 0.63 0.63
USFD* 0.59 0.59 0.59 0.60 0.60 0.60
WVALI 0.62 0.62 0.62 0.64 0.64 0.64
XRCE-T 0.53 0.25 0.34 0.63 0.30 0.41
average 0.59 0.54 0.56 0.62 0.57 0.59
stddev 0.03 0.13 0.10 0.01 0.12 0.08
</table>
<tableCaption confidence="0.985092">
Table 2: Results for Task A
</tableCaption>
<page confidence="0.934709">
78
</page>
<table confidence="0.9998372">
team strict relaxed
P R F P R F
CU-TMP 0.75 0.75 0.75 0.76 0.76 0.76
LCC-TE 0.75 0.71 0.73 0.76 0.72 0.74
NAIST 0.75 0.75 0.75 0.76 0.76 0.76
USFD* 0.73 0.73 0.73 0.74 0.74 0.74
WVALI 0.80 0.80 0.80 0.81 0.81 0.81
XRCE-T 0.78 0.57 0.66 0.84 0.62 0.71
average 0.76 0.72 0.74 0.78 0.74 0.75
stddev 0.03 0.08 0.05 0.03 0.06 0.03
</table>
<tableCaption confidence="0.941574">
Table 3: Results for Task B
</tableCaption>
<table confidence="0.9997639">
team strict relaxed
P R F P R F
CU-TMP 0.54 0.54 0.54 0.58 0.58 0.58
LCC-TE 0.55 0.55 0.55 0.58 0.58 0.58
NAIST 0.49 0.49 0.49 0.53 0.53 0.53
USFD* 0.54 0.54 0.54 0.57 0.57 0.57
WVALI 0.54 0.54 0.54 0.64 0.64 0.64
XRCE-T 0.42 0.42 0.42 0.58 0.58 0.58
average 0.51 0.51 0.51 0.58 0.58 0.58
stddev 0.05 0.05 0.05 0.04 0.04 0.04
</table>
<tableCaption confidence="0.998861">
Table 4: Results for Task C
</tableCaption>
<bodyText confidence="0.99993724">
All tables give precision, recall and f-measure for
both the strict and the relaxed scoring scheme, as
well as averages and standard deviation on the pre-
cision, recall and f-measure numbers. The entry for
USFD is starred because the system developers are
co-organizers of the TempEval task.3
For task A, the f-measure scores range from 0.34
to 0.62 for the strict scheme and from 0.41 to 0.63
for the relaxed scheme. For task B, the scores range
from 0.66 to 0.80 (strict) and 0.71 to 0.81 (relaxed).
Finally, task C scores range from 0.42 to 0.55 (strict)
and from 0.56 to 0.66 (relaxed).
The differences between the systems is not spec-
tacular. WVALI’s hybrid approach outperforms the
other systems in task B and, using relaxed scoring,
in task C as well. But for task A, the winners barely
edge out the rest of the field. Similarly, for task C
using strict scoring, there is no system that clearly
separates itself from the field.
It should be noted that for task A, and in lesser ex-
tent for task B, the XRCE-T system has recall scores
that are far below all other systems. This seems
mostly due to a choice by the developers to not as-
sign a temporal relation if the syntactic analyzer did
not find a clear syntactic relation between the two
</bodyText>
<footnote confidence="0.627943333333333">
3There was a strict separation between people assisting in
the annotation of the evaluation corpus and people involved in
system development.
</footnote>
<bodyText confidence="0.9981045">
elements that needed to be linked for the TempEval
task.
</bodyText>
<sectionHeader confidence="0.9764725" genericHeader="method">
7 Conclusion: the Future of Temporal
Evaluation
</sectionHeader>
<bodyText confidence="0.999982809523809">
The evaluation approach of TempEval avoids the in-
terdependencies that are inherent to a network of
temporal relations, where relations in one part of the
network may constrain relations in any other part of
the network. To accomplish that, TempEval delib-
erately focused on subtasks of the larger problem of
automatic temporal annotation.
One thing we may want to change to the present
TempEval is the definition of task A. Currently, it
instructs to temporally link all events in a sentence
to all time expressions in the same sentence. In the
future we may consider splitting this into two tasks,
where one subtask focuses on those anchorings that
are very local, like ”...White House spokesman Mar-
lin Fitzwater [said] [late yesterday] that...”. We ex-
pect both inter-annotator agreement and system per-
formance to be higher on this subtask.
There are two research avenues that loom beyond
the current TempEval: (1) definition of other sub-
tasks with the ultimate goal of establishing a hierar-
chy of subtasks ranked on performance of automatic
taggers, and (2) an approach to evaluate entire time-
lines.
Some other temporal linking tasks that can be
considered are ordering of consecutive events in a
sentence, ordering of events that occur in syntactic
subordination relations, ordering events in coordi-
nations, and temporal linking of reporting events to
the document creation time. Once enough temporal
links from all these subtasks are added to the en-
tire temporal graph, it becomes possible to let confi-
dence scores from the separate subtasks drive a con-
straint propagation algorithm as proposed in (Allen,
1983), in effect using high-precision relations to
constrain lower-precision relations elsewhere in the
graph.
With this more complete temporal annotation it
is no longer possible to simply evaluate the entire
graph by scoring pairwise comparisons. Instead
the entire timeline must be evaluated. Initial ideas
regarding this focus on transforming the temporal
graph of a document into a set of partial orders built
</bodyText>
<page confidence="0.993954">
79
</page>
<bodyText confidence="0.999664">
around precedence and inclusion relations and then
evaluating each of these partial orders using some
kind of edit distance measure.4
We hope to have taken the first baby steps with
the three TempEval tasks.
</bodyText>
<sectionHeader confidence="0.997039" genericHeader="conclusions">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.949283">
We would like to thank all the people who helped
prepare the data for TempEval, listed here in no
</bodyText>
<reference confidence="0.844180083333333">
particular order: Amber Stubbs, Jessica Littman,
Hongyuan Qiu, Emin Mimaroglu, Emma Barker,
Catherine Havasi, Yonit Boussany, Roser Sauri, and
Anna Rumshisky.
Thanks also to all participants to this new task:
Steven Bethard and James Martin (University of
Colorado at Boulder), Congmin Min, Munirath-
nam Srikanth and Abraham Fowler (Language Com-
puter Corporation), Yuchang Cheng, Masayuki Asa-
hara and Yuji Matsumoto (Nara Institute of Science
and Technology), Mark Hepple, Andrea Setzer and
Rob Gaizauskas (University of Sheffield), Caroline
</reference>
<bodyText confidence="0.889670857142857">
Hage`ge and Xavier Tannier (XEROX Research Cen-
tre Europe), and Georgiana Pus¸cas¸u (University of
Wolverhampton and University of Alicante).
Part of the work in this paper was funded by
the DTO/AQUAINT program under grant num-
ber N61339-06-C-0140 and part funded by the EU
VIKEF project (IST- 2002-507173).
</bodyText>
<sectionHeader confidence="0.998286" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997968296296296">
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832–843.
Bran Boguraev, James Pustejovsky, Rie Ando, and Marc
Verhagen. forthcoming. Timebank evolution as a
community resource for timeml parsing. Language
Resources and Evaluation.
Inderjeet Mani, Ben Wellner, Marc Verhagen, Chong Min
Lee, and James Pustejovsky. 2006. Machine learn-
ing of temporal relations. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics, Sydney, Australia. ACL.
James Pustejovsky, Jos´e Casta˜no, Robert Ingria, Roser
Sauri, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003a. TimeML: Robust specification of
4Edit distance was proposed by Ben Wellner as a way to
evaluate partial orders of precedence relations (personal com-
munication).
event and temporal expressions in text. In Proceedings
of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, January.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003b. The TIMEBANK corpus. In
Proceedings of Corpus Linguistics 2003, pages 647–
656, Lancaster, March.
</reference>
<page confidence="0.998247">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.430700">
<title confidence="0.997631">SemEval-2007 Task 15: TempEval Temporal Relation Identification</title>
<author confidence="0.819662">Robert Frank Mark James</author>
<affiliation confidence="0.9314735">University, of Sheffield, Legal &amp; Regulatory, University,</affiliation>
<abstract confidence="0.9925367">The TempEval task proposes a simple way to evaluate automatic extraction of temporal relations. It avoids the pitfalls of evaluating a graph of inter-related labels by defining three sub tasks that allow pairwise evaluation of temporal relations. The task not only allows straightforward evaluation, it also avoids the complexities of full temporal parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>particular order Amber Stubbs</author>
<author>Jessica Littman</author>
</authors>
<institution>Hongyuan Qiu, Emin Mimaroglu, Emma Barker, Catherine Havasi, Yonit Boussany, Roser Sauri, and Anna Rumshisky.</institution>
<marker>Stubbs, Littman, </marker>
<rawString>particular order: Amber Stubbs, Jessica Littman, Hongyuan Qiu, Emin Mimaroglu, Emma Barker, Catherine Havasi, Yonit Boussany, Roser Sauri, and Anna Rumshisky.</rawString>
</citation>
<citation valid="false">
<title>Thanks also to all participants to this new task: Steven Bethard and James</title>
<institution>Martin (University of Colorado at Boulder), Congmin Min, Munirathnam Srikanth and Abraham Fowler (Language Computer Corporation), Yuchang Cheng, Masayuki Asahara and Yuji Matsumoto (Nara Institute of Science and Technology), Mark Hepple, Andrea Setzer and Rob Gaizauskas (University of Sheffield), Caroline</institution>
<marker></marker>
<rawString>Thanks also to all participants to this new task: Steven Bethard and James Martin (University of Colorado at Boulder), Congmin Min, Munirathnam Srikanth and Abraham Fowler (Language Computer Corporation), Yuchang Cheng, Masayuki Asahara and Yuji Matsumoto (Nara Institute of Science and Technology), Mark Hepple, Andrea Setzer and Rob Gaizauskas (University of Sheffield), Caroline</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Maintaining knowledge about temporal intervals.</title>
<date>1983</date>
<journal>Communications of the ACM,</journal>
<volume>26</volume>
<issue>11</issue>
<contexts>
<context position="6936" citStr="Allen, 1983" startWordPosition="1106" endWordPosition="1107">omewhat simpler than that used in TimeML, whose complexity was designed to handle event expressions that introduced multiple event instances (consider, e.g. He taught on Wednesday and Friday). This complication was not necessary for the TempEval data. The most salient attributes encode tense, aspect, modality and polarity information. For TempEval task C, one extra attribute is added: mainevent, with possible values YES and NO. • TLINK. This is a simplified version of the TimeML TLINK tag. The relation types for the TimeML version form a fine-grained set based on James Allen’s interval logic (Allen, 1983). For TempEval, we use only six relation types including the three core relations BEFORE, AFTER, and OVERLAP, the two less specific relations BEFORE-OR-OVERLAP and OVERLAPOR-AFTER for ambiguous cases, and finally the relation VAGUE for those cases where no particular relation can be established. As stated above the TLINKs of concern for each task are explicitly included in the training and in the test data. However, in the latter the relType attribute of each TLINK is set to UNKNOWN. For each task the system must replace the UNKNOWN values with one of the six allowed values listed above. The E</context>
<context position="19506" citStr="Allen, 1983" startWordPosition="3248" endWordPosition="3249">asks ranked on performance of automatic taggers, and (2) an approach to evaluate entire timelines. Some other temporal linking tasks that can be considered are ordering of consecutive events in a sentence, ordering of events that occur in syntactic subordination relations, ordering events in coordinations, and temporal linking of reporting events to the document creation time. Once enough temporal links from all these subtasks are added to the entire temporal graph, it becomes possible to let confidence scores from the separate subtasks drive a constraint propagation algorithm as proposed in (Allen, 1983), in effect using high-precision relations to constrain lower-precision relations elsewhere in the graph. With this more complete temporal annotation it is no longer possible to simply evaluate the entire graph by scoring pairwise comparisons. Instead the entire timeline must be evaluated. Initial ideas regarding this focus on transforming the temporal graph of a document into a set of partial orders built 79 around precedence and inclusion relations and then evaluating each of these partial orders using some kind of edit distance measure.4 We hope to have taken the first baby steps with the t</context>
</contexts>
<marker>Allen, 1983</marker>
<rawString>James Allen. 1983. Maintaining knowledge about temporal intervals. Communications of the ACM, 26(11):832–843.</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcoming</author>
</authors>
<title>Timebank evolution as a community resource for timeml parsing. Language Resources and Evaluation.</title>
<marker>forthcoming, </marker>
<rawString>Bran Boguraev, James Pustejovsky, Rie Ando, and Marc Verhagen. forthcoming. Timebank evolution as a community resource for timeml parsing. Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Ben Wellner</author>
<author>Marc Verhagen</author>
<author>Chong Min Lee</author>
<author>James Pustejovsky</author>
</authors>
<title>Machine learning of temporal relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>ACL.</publisher>
<location>Sydney, Australia.</location>
<contexts>
<context position="2014" citStr="Mani et al., 2006" startWordPosition="292" endWordPosition="295">ovsky et al., 2003a) is an emerging ISO standard for annotation of events, temporal expressions and the anchoring and ordering relations between them. TimeBank (Pustejovsky et al., 2003b; Boguraev et al., forthcoming) was originally conceived of as a proof of concept that illustrates the TimeML language, but has since gone through several rounds of revisions and can now be considered a gold standard for temporal information. TimeML and TimeBank have already been used as the basis for automatic time, event and temporal relation annotation tasks in a number of research projects in recent years (Mani et al., 2006; Boguraev et al., forthcoming). An open evaluation challenge in the area of temporal annotation should serve to drive research forward, as it has in other areas of NLP. The automatic identification of all temporal referring expressions, events and temporal relations within a text is the ultimate aim of research in this area. However, addressing this aim in a first evaluation challenge was judged to be too difficult, both for organizers and participants, and a staged approach was deemed more effective. Thus we here present an initial evaluation exercise based on three limited tasks that we bel</context>
</contexts>
<marker>Mani, Wellner, Verhagen, Lee, Pustejovsky, 2006</marker>
<rawString>Inderjeet Mani, Ben Wellner, Marc Verhagen, Chong Min Lee, and James Pustejovsky. 2006. Machine learning of temporal relations. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics, Sydney, Australia. ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Pustejovsky</author>
<author>Jos´e Casta˜no</author>
<author>Robert Ingria</author>
<author>Roser Sauri</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Graham Katz</author>
</authors>
<title>TimeML: Robust specification of 4Edit distance was proposed by Ben Wellner as a way to evaluate partial orders of precedence relations (personal communication). event and temporal expressions in text.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fifth International Workshop on Computational Semantics (IWCS-5),</booktitle>
<location>Tilburg,</location>
<marker>Pustejovsky, Casta˜no, Ingria, Sauri, Gaizauskas, Setzer, Katz, 2003</marker>
<rawString>James Pustejovsky, Jos´e Casta˜no, Robert Ingria, Roser Sauri, Robert Gaizauskas, Andrea Setzer, and Graham Katz. 2003a. TimeML: Robust specification of 4Edit distance was proposed by Ben Wellner as a way to evaluate partial orders of precedence relations (personal communication). event and temporal expressions in text. In Proceedings of the Fifth International Workshop on Computational Semantics (IWCS-5), Tilburg, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Sauri</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Dragomir Radev</author>
<author>Beth Sundheim</author>
<author>David Day</author>
<author>Lisa Ferro</author>
<author>Marcia Lazo</author>
</authors>
<title>The TIMEBANK corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of Corpus Linguistics</booktitle>
<pages>647--656</pages>
<location>Lancaster,</location>
<contexts>
<context position="1415" citStr="Pustejovsky et al., 2003" startWordPosition="196" endWordPosition="200">tives and other texts describe events that occur in time and specify the temporal location and order of these events. Text comprehension, amongst other capabilities, clearly requires the capability to identify the events described in a text and locate these in time. This capability is crucial to a wide range of NLP applications, from document summarization and question answering to machine translation. Recent work on the annotation of events and temporal relations has resulted in both a de-facto standard for expressing these relations and a hand-built gold standard of annotated texts. TimeML (Pustejovsky et al., 2003a) is an emerging ISO standard for annotation of events, temporal expressions and the anchoring and ordering relations between them. TimeBank (Pustejovsky et al., 2003b; Boguraev et al., forthcoming) was originally conceived of as a proof of concept that illustrates the TimeML language, but has since gone through several rounds of revisions and can now be considered a gold standard for temporal information. TimeML and TimeBank have already been used as the basis for automatic time, event and temporal relation annotation tasks in a number of research projects in recent years (Mani et al., 2006;</context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, Lazo, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, and Marcia Lazo. 2003b. The TIMEBANK corpus. In Proceedings of Corpus Linguistics 2003, pages 647– 656, Lancaster, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>