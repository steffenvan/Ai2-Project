<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<note confidence="0.390916666666667">
A LANGUAGE-INDEPENDENT ANAPHORA RESOLUTION
SYSTEM FOR UNDERSTANDING MULTILINGUAL TEXTS
Chinatsu Aone and Douglas McKee
</note>
<address confidence="0.840134">
Systems Research and Applications (SRA)
2000 15th Street North
Arlington, VA 22201
</address>
<email confidence="0.795321">
aonec©sra.com, mckeedCO.sra.corn
</email>
<sectionHeader confidence="0.978676" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974222222222">
This paper describes a new discourse module
within our multilingual NLP system. Because of
its unique data-driven architecture, the discourse
module is language-independent. Moreover, the
use of hierarchically organized multiple knowledge
sources makes the module robust and trainable using
discourse-tagged corpora. Separating discourse phe-
nomena from knowledge sources makes the discourse
module easily extensible to additional phenomena.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999388">
This paper describes a new discourse module within
our multilingual natural language processing system
which has been used for understanding texts in En-
glish, Spanish and Japanese (cf. [1, 2]).1 The follow-
ing design principles underlie the discourse module:
</bodyText>
<listItem confidence="0.99689825">
• Language-independence: No processing code de-
pends on language-dependent facts.
• Extensibility: It is easy to handle additional phe-
nomena.
• Robustness: The discourse module does its best
even when its input is incomplete or wrong.
• Trainability: The performance can be tuned for
particular domains and applications.
</listItem>
<bodyText confidence="0.9974588">
In the following, we first describe the architecture
of the discourse module. Then, we discuss how its
performance is evaluated and trained using discourse-
tagged corpora. Finally, we compare our approach to
other research.
</bodyText>
<figure confidence="0.97748725">
Semsnocs
M&apos;d
perform-semantic m,,,kuk
make-ck
-&amp;quot;411•4.10
Test Processing
Controller
Discourse Module
</figure>
<figureCaption confidence="0.99985">
Figure 1: Discourse Architecture
</figureCaption>
<sectionHeader confidence="0.97129" genericHeader="method">
2 Discourse Architecture
</sectionHeader>
<bodyText confidence="0.999945388888889">
Our discourse module consists of two discourse pro-
cessing submodules (the Discourse Administrator and
the Resolution Engine), and three discourse knowl-
edge bases (the Discourse Knowledge Source KB,
the Discourse Phenomenon KB, and the Discourse
Domain KB). The Discourse Administrator is a
development-time tool for defining the three dis-
course KB&apos;s. The Resolution Engine, on the other
hand, is the run-time processing module which ac-
tually performs anaphora resolution using these dis-
course KB&apos;s.
The Resolution Engine also has access to an ex-
ternal discourse data structure called the global dis-
course world, which is created by the top-level text
processing controller. The global discourse world
holds syntactic, semantic, rhetorical, and other infor-
mation about the input text derived by other parts
of the system. The architecture is shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.361121">
2.1 Discourse Data Structures
</subsectionHeader>
<bodyText confidence="0.3247695">
Resolunon
Engme
■41,13,
perform-dircourse updare-du.
I Our system has been used in several data extraction tasks There are four major discourse data types within the
and a prototype machine translation system, global discourse world: Discourse World (DW), Dis-
</bodyText>
<page confidence="0.997601">
156
</page>
<bodyText confidence="0.999055259259259">
course Clause (DC), Discourse Marker (DM), and
File Card (FC), as shown in Figure 2.
The global discourse world corresponds to an entire
text, and its sub-discourse worlds correspond to sub-
components of the text such as paragraphs. Discourse
worlds form a tree representing a text&apos;s structure.
A discourse clause is created for each syntactic
structure of category S by the semantics module. It
can correspond to either a full sentence or a part of a
full sentence. Each discourse clause is typed accord-
ing to its syntactic properties.
A discourse marker (cf. Kamp [14], or &amp;quot;discourse
entity&amp;quot; in Ayuso [3]) is created for each noun or verb
in the input sentence during semantic interpretation.
A discourse marker is static in that once it is intro-
duced to the discourse world, the information within
it is never changed.
Unlike a discourse marker, a file card (cf. Heim [11],
&amp;quot;discourse referent&amp;quot; in Karttunen [15], or &amp;quot;discourse
entity&amp;quot; in Webber [19]) is dynamic in a sense that
it is continually updated as the discourse process-
ing proceeds. While an indefinite discourse marker
starts a file card, a definite discourse marker updates
an already existing file card corresponding to its an-
tecedent. In this way, a file card keeps track of all
its co-referring discourse markers, and accumulates
semantic information within them.
</bodyText>
<subsectionHeader confidence="0.9984">
2.2 Discourse Administrator
</subsectionHeader>
<bodyText confidence="0.999946785714286">
Our discourse module is customized at development
time by creating and modifying the three discourse
KB&apos;s using the Discourse Administrator. First, a dis-
course domain is established for a particular NLP ap-
plication. Next, a set of discourse phenomena which
should be handled within that domain by the dis-
course module is chosen (e.g. definite NP, 3rd per-
son pronoun, etc.) because some phenomena may
not be necessary to handle for a particular applica-
tion domain. Then, for each selected discourse phe-
nomenon, a set of discourse knowledge sources are
chosen which are applied during anaphora resolution,
since different discourse phenomena require different
sets of knowledge sources.
</bodyText>
<subsectionHeader confidence="0.521536">
2.2.1 Discourse Knowledge Source KB
</subsectionHeader>
<bodyText confidence="0.999863857142857">
The discourse knowledge source KB houses small
well-defined anaphora resolution strategies. Each
knowledge source (KS) is an object in the hierarchi-
cally organized KB, and information in a specific KS
can be inherited from a more general KS.
There are three kinds of KS&apos;s: a generator, a filter
and an orderer. A generator is used to generate pos-
</bodyText>
<figure confidence="0.71599">
1.coisse
</figure>
<figureCaption confidence="0.999385">
Figure 3: Discourse Knowledge Source KB
</figureCaption>
<bodyText confidence="0.9999295625">
sible antecedent hypotheses from the global discourse
world. Unlike other discourse systems, we have multi-
ple generators because different discourse phenomena
exhibit different antecedent distribution patterns (cf.
Guindon et al. [10]). A filter is used to eliminate im-
possible hypotheses, while an orderer is used to rank
possible hypotheses in a preference order. The KS
tree is shown in Figure 3.
Each KS contains three slots: ks-function, ks-data,
and ks-language. The ks-function slot contains a
functional definition of the KS. For example, the func-
tional definition of the Syntactic-Gender filter defines
when the syntactic gender of an anaphor is compati-
ble with that of an antecedent hypothesis. A ks-data
slot contains data used by ks-function. The sepa-
ration of data from function is desirable because a
parent KS can specify ks-function while its sub-KS&apos;s
inherit the same ks-function but specify their own
data. For example, in languages like English and
Japanese, the syntactic gender of a pronoun imposes
a semantic gender restriction on its antecedent. An
English pronoun &amp;quot;he&amp;quot;, for instance, can never refer
to an NP whose semantic gender is female like &amp;quot;Ms.
Smith&amp;quot;. The top-level Semantic-Gender KS, then,
defines only ks-function, while its sub-KS&apos;s for En-
glish and Japanese specify their own ks-data and in-
herit the same ks-function. A k.s-language slot speci-
fies languages if a particular KS is applicable for spe-
cific languages.
Most of the KS&apos;s are language-independent (e.g.
all the generators and the semantic type filters), and
even when they are language-specific, the function
</bodyText>
<figure confidence="0.939731428571428">
.1. t .1.
Is....,-.......o..1
fs..........-...1
s....*I
1...........-I \11::.....-.....„.., .1,...ail
s.......-,...11,......*u ,1.1.-.11
s,u. Is......,..c*I 1
as....,.......lk.......-..........I
................1
3,.........-...1
Is,....a.4.4..1
s.....1*-1
157
(defframe discourse-world (discourse-data-structure) ; DW
</figure>
<figureCaption confidence="0.998013653846154">
date ; date of the text
location ; location where the text is originated
topics ; semantic concepts which correspond to global topics of the text
position ; the corresponding character position in the text
discourse-clauses ; a list of discourse clauses in the current DW
sub-discourse-worlds) ; a list of DWs subordinate to the current one
(defframe discourse-clause (discourse-data-structure) ; DC
discourse-markers ; a list of discourse markers in the current DC
syntax ; an f-structure for the current DC
parse-tree ; a parse tree of this S
semantics ; a semantic (KB) object representing the current DC
position ; the corresponding character position in the text
date ; date of the current DC
location ; location of the current DC
subordinate-discourse-clause ; a DC subordinate to the current DC
coordinate-discourse-clauses) ; coordinate DC&apos;s which a conjoined sentence consists of
(defframe discourse-marker (discourse-data-structure) ; DM
position ; the corresponding character position in the text
discourse-clause ; a pointer back to DC:
syntax ; an f-structure for the current DM
semantics ; a semantic (KB) object
file card) ; a pointer to the file card
(defframe file-card (discourse-data-structure) ; FC
co-referring-discourse-markers ; a list of co-referring DM&apos;s
updated-semantic-info) ; a semantic (KB) object which contains cumulative 55 mantiCs
Figure 2: Discourse World, Discourse Clause, Discourse Marker, and File Card
</figureCaption>
<bodyText confidence="0.998653">
definitions are shared. In this way, much of the dis-
course knowledge source KB is sharable across differ-
ent languages.
</bodyText>
<subsubsectionHeader confidence="0.90166">
2.2.2 Discourse Phenomenon KB
</subsubsectionHeader>
<bodyText confidence="0.991361514285714">
The discourse phenomenon KB contains hierarchi-
cally organized discourse phenomenon objects as
shown in Figure 4. Each discourse phenomenon ob-
ject has four slots (dp-definition, dp-main-strategy,
dp-backup-strategy, and dp-language) whose values
can be inherited. The dp-definition of a discourse
phenomenon object specifies a definition of the dis-
course phenomenon so that an anaphoric discourse
marker can be classified as one of the discourse phe-
nomena. The dp-main-strategy slot specifies, for each
phenomenon, a set of KS&apos;s to apply to resolve this
particular discourse phenomenon. The dp-backup-
strategy slot, on the other hand, provides a set of
backup strategies to use in case the main strategy
fails to propose any antecedent hypothesis. The dp-
language slot specifies languages when the discourse
phenomenon is only applicable to certain languages
(e.g. Japanese &amp;quot;dou&amp;quot; ellipsis).
When different languages use different sets of KS&apos;s
for main strategies or backup strategies for the same
discourse phenomenon, language specific (11)-main-
strategy or dp-backup-strategy values are specified.
For example, when an anaphor is a 3rd person pro-
noun in a partitive construction (i.e. :3PRO-Partitive-
Parent)2, Japanese uses a different generator for the
main strategy (Current-and-Previous-DC) than En-
glish and Spanish (Current-and-Previous-Sentence).
2e.g. &amp;quot;three of them&amp;quot; in English, &amp;quot;tres de riles&amp;quot; in Spanish,
&amp;quot;itchi san-nin&amp;quot; in Japanese
Because the discourse KS&apos;s are independent of dis-
course phenomena, the same discourse KS can be
shared by different discourse phenomena. For exam-
ple, the Semantic-Superclass filter is used by both
Definite-NP and Pronoun, and the Recency orderer
is used by most discourse phenomena.
</bodyText>
<subsubsectionHeader confidence="0.748999">
2.2.3 Discourse Domain KB
</subsubsectionHeader>
<bodyText confidence="0.999988444444444">
The discourse domain KB contains discourse domain
objects each of which defines a set of discourse phe-
nomena to handle in a particular domain. Since
texts in different domains exhibit different sets of dis-
course phenomena, and since different applications
even within the same domain may not have to handle
the same set of discourse phenomena, the discourse
domain KB is a way to customize and constrain the
workload of the discourse module.
</bodyText>
<subsectionHeader confidence="0.994518">
2.3 Resolution Engine
</subsectionHeader>
<bodyText confidence="0.9999218">
The Resolution Engine is the run-time processing
module which finds the best antecedent hypothesis
for a given anaphor by using data in both the global
discourse world and the discourse KB&apos;s. The Resolu-
tion Engine&apos;s basic operations are shown in Figure 5.
</bodyText>
<subsubsectionHeader confidence="0.511097">
2.3.1 Finding Antecedents
</subsubsectionHeader>
<bodyText confidence="0.999741571428571">
The Resolution Engine uses the discourse phe-
nomenon KB to classify an anaphor as one of the
discourse phenomena (using dp-definition values) and
to determine a set of KS&apos;s to apply to the anaphor
(using dp-main-strategy values). The Engine then
applies the generator KS to get an initial set of hy-
potheses and removes those that do not pass the filter
</bodyText>
<page confidence="0.962152">
158
</page>
<figure confidence="0.998019125">
ill• CAL(
tlitserw-R■namal
F11.1.[1.111.1.paratrilea-F11.1.112,01
M-(111p.sIVP11,1
2....-.N....a..11-119.0-hrtill.•••-•.MPRO1,13M-PirattIve-P....-1,.......9,..,
Cum-t-Preqn1
Annt-In-MMItheIPPARMI
VIIMILEC
</figure>
<figureCaption confidence="0.994139">
Figure 4: Discourse Phenomenon KB
</figureCaption>
<figure confidence="0.975402923076923">
For each anaphoric discourse marker in the current sentence:
Find-Antecedent
Input: anaphor to resolve, global discourse world
Get-KSs-for-Discourse-Phenomenon
Input: anaphor to resolve, discourse phenomenon KB
Output: a set of discourse KS&apos;s
Apply-KSs
Input: anaphor to resolve, global discourse world, discourse KS&apos;s
Output: the best hypothesis
Output: the best hypothesis
Update-Discourse-World
Input: anaphor, best hypothesis, global discourse world
Output: updated global discourse world
</figure>
<figureCaption confidence="0.999959">
Figure 5: Resolution Engine Operations
</figureCaption>
<bodyText confidence="0.999868163265306">
KS&apos;s. If only one hypothesis remains, it is returned as
the anaphor&apos;s referent, but there may be more than
one hypothesis or none at all.
When there is more than one hypothesis, orderer
KS&apos;s are invoked. However, when more than one or-
derer KS could apply to the anaphor, we face the
problem of how to combine the preference values re-
turned by these multiple orderers. Some anaphora
resolution systems (cf. Carbonell and Brown [6], Rich
and LuperFoy [16], Rimon et al. [17]) assign scores
to antecedent hypotheses, and the hypotheses are
ranked according to their scores. Deciding the scores
output by the orderers as well as the way the scores
are combined requires more research with larger data.
In our current system, therefore, when there are mul-
tiple hypotheses left, the most &amp;quot;promising&amp;quot; orderer
is chosen for each discourse phenomenon. In Section
3, we discuss how we choose such an orderer for each
discourse phenomenon by using statistical preference.
In the future, we will experiment with ways for each
orderer to assign &amp;quot;meaningful&amp;quot; scores to hypotheses.
When there is no hypothesis left after the main
strategy for a discourse phenomenon is performed, a
series of backup strategies specified in the discourse
phenomenon KB are invoked. Like the main strat-
egy, a backup strategy specifies which generators, fil-
ters, and orderers to use. For example, a backup
strategy may choose a new generator which gener-
ates more hypotheses, or it may turn off some of the
filters used by the main strategy to accept previously
rejected hypotheses. How to choose a new generator
or how to use only a subset of filters can be deter-
mined by training the discourse module on a corpus
tagged with discourse relations, which is discussed in
Section 3.
Thus, for example, in order to resolve a 3rd per-
son pronoun in a partitive in an appositive (e.g.
anaphor ID=1023 in Figure 7), the phenomenon KB
specifies the following main strategy for Japanese:
generator = Head-NP, filters = {Semantic-Amount,
Semantic-Class, Semantic-Supercla.ssl, orderer = Re-
cency. This particular generator is chosen because in
almost every example in 50 Japanese texts, this type
of anaphora has its antecedent in its head NP. No
syntactic filters are used because the anaphor has no
useful syntactic information. As a backup strategy,
a new generator, Adjacent-NP, is chosen in case the
parse fails to create an appositive relation between
the antecedent NP ID=1022 and the anaphor.
</bodyText>
<page confidence="0.992505">
159
</page>
<figure confidence="0.8744142">
The AIDS Surveillance Committee Three of them were
confirmed 7 AIDS patients yesterday. hemophiliac.
FC-5
coreferring-DM&apos;s: 1DM-1 DM-2)
semantics: Patient 101 ^ Person.102
</figure>
<figureCaption confidence="0.99962">
Figure 6: Updating Discourse World
</figureCaption>
<subsectionHeader confidence="0.978784">
2.3.2 Updating the Global Discourse World
</subsectionHeader>
<bodyText confidence="0.999989238095239">
After each anaphor resolution, the global discourse
world is updated as it would be in File Change Se-
mantics (cf. Heim [11]), and as shown in Figure 6.
First, the discourse marker for the anaphor is in-
corporated into the file card to which its antecedent
discourse marker points so that the co-referring dis-
course markers point to the same file card. Then, the
semantics information of the file card is updated so
that it reflects the union of the information from all
the co-referring discourse markers. In this way, a file
card accumulates more information as the discourse
processing proceeds.
The motivation for having both discourse markers
and file cards is to make the discourse processing a
monotonic operation. Thus, the discourse process-
ing does not replace an anaphoric discourse marker
with its antecedent discourse marker, but only creates
or updates file cards. This is both theoretically and
computationally advantageous because the discourse
processing can be redone by just retracting the file
cards and reusing the same discourse markers.
</bodyText>
<subsectionHeader confidence="0.998175">
2.4 Advantages of Our Approach
</subsectionHeader>
<bodyText confidence="0.999991575757576">
Now that we have described the discourse module in
detail, we summarize its unique advantages. First,
it is the only working language-independent discourse
system we are aware of. By &amp;quot;language-independent,&amp;quot;
we mean that the discourse module can be used for
different languages if discourse knowledge is added
for a new language.
Second, since the anaphora resolution algorithm is
not hard-coded in the Resolution Engine, but is kept
in the discourse KB&apos;s, the discourse module is ex-
tensible to a new discourse phenomenon by choosing
existing discourse KS&apos;s or adding new discourse KS&apos;s
which the new phenomenon requires.
Making the discourse module robust is another im-
portant goal especially when dealing with real-world
input, since by the time the input is processed and
passed to the discourse module, the syntactic or se-
mantic information of the input is often not as accu-
rate as one would hope. The discourse module must
be able to deal with partial information to make a
decision. By dividing such decision-making into mul-
tiple discourse KS&apos;s and by letting just the applicable
KS&apos;s fire, our discourse module handles partial infor-
mation robustly.
Robustness of the discourse module is also mani-
fested when the imperfect discourse KB&apos;s or an inac-
curate input cause initial anaphor resolution to fail.
When the main strategy fails, a set of backup strate-
gies specified in the discourse phenomenon KB pro-
vides alternative ways to get the best antecedent hy-
pothesis. Thus, the system tolerates its own insuffi-
ciency in the discourse KB&apos;s as well as degraded input
in a robust fashion.
</bodyText>
<sectionHeader confidence="0.9801675" genericHeader="method">
3 Evaluating and Training the
Discourse Module
</sectionHeader>
<bodyText confidence="0.999997">
In order to choose the most effective KS&apos;s for a par-
ticular phenomenon, as well as to debug and track
progress of the discourse module, we must be able to
evaluate the performance of discourse processing. To
perform objective evaluation, we compare the results
of running our discourse module over a corpus with
a set of manually created discourse tags. Examples
of discourse-tagged text are shown in Figure 7. The
metrics we use for evaluation are detailed in Figure 8.
</bodyText>
<subsectionHeader confidence="0.999206">
3.1 Evaluating the Discourse Module
</subsectionHeader>
<bodyText confidence="0.9999834">
We evaluate overall performance by calculating re-
call and precision of anaphora resolution results. The
higher these measures are, the better the discourse
module is working. In addition, we evaluate the dis-
course performance over new texts, using blackbox
evaluation (e.g. scoring the results of a data extrac-
tion task.)
To calculate a generator&apos;s failure rate, a filter&apos;s false
positive rate, and an orderer&apos;s effectiveness, the algo-
rithms in Figure 9 are used.&apos;
</bodyText>
<subsectionHeader confidence="0.999769">
3.2 Choosing Main Strategies
</subsectionHeader>
<bodyText confidence="0.9995815">
The uniqueness of our approach to discourse analysis
is also shown by the fact that our discourse mod-
ule can be trained for a particular domain, similar
to the ways grammars have been trained (cf. Black
</bodyText>
<footnote confidence="0.597365">
3 &amp;quot;The remaining antecedent hypotheses&amp;quot; are the hypothe-
ses left after all the filters are applied for an anaphor.
</footnote>
<table confidence="0.956946576923077">
DM-1
semantics: Patient.101
DM-2
semantics: Person.102
160
Overall Performance: Recall = Ne/I, Precision=---. Ne./Nh
I Number of anaphors in input
N, Number of correct resolutions
Nh Number of resolutions attempted
Filter: Recall = OP/IP, Precision = 0 P,I 0 P
IP, Number of correct pairs in input
IP Number of pairs in input
OP Number of pairs output and passed by filter
OP, Number of correct pairs output by filter
1 — OP/IF&apos; Fraction of input pairs filtered out
1 — OP/IP. Fraction of correct answers filtered out (false positive rate)
Generator: Recall = Ne/I, Precision = NelNh
I Number of anaphors in input
Nh Number of hypotheses in input
N, Number of times correct answer in output
Nh// Average number of hypotheses
1 — Ncl I Fraction of correct answers not returned (failure rate)
Orderer:
I Number of anaphors in input
N, Number of correct answers output first
Ncl I Success rate (effectiveness)
</table>
<figureCaption confidence="0.995651">
Figure 8: Metrics used for Evaluating and Training Discourse
</figureCaption>
<bodyText confidence="0.972329818181818">
For each discourse phenomenon,
given anaphor and antecedent pairs in the corpus,
calculate how often the generator fails to generate the antecedents.
For each discourse phenomenon,
given anaphor and antecedent pairs in the corpus,
for each filter,
calculate how often the filter incorrectly eliminates the antecedents.
For each anaphor exhibiting a given discourse phenomenon in the corpus,
given the remaining antecedent hypotheses for the anaphor,
for each applicable orderer,
test if the orderer chooses the correct antecedent as the best hypothesis.
</bodyText>
<figureCaption confidence="0.966919">
Figure 9: Algorithms for Evaluating Discourse Knowledge Sources
</figureCaption>
<page confidence="0.96191">
161
</page>
<table confidence="0.878764148148148">
(1A5F115tATAII41) 4)---,cf zrfit-,
MfiR • Wilft•--niftc41)i. AlEI,
&lt;Dfo m=1 cothm 4 X.IIM-LA,./Dm&gt; (4)m 11)=1001 Type=3PARTA
Ref=l000&gt; 5 t&gt;&lt;a)m›ttEA) LALt.t Iffit
[The AIDS Surveillance Committee of the Health and Welfare Ministry
(Chairman, Professor Emeritus Junichi Shiokawa), on the 6th, newly
confirmed 7 AIDS patients (of them 3 are dead) and 17 infected people.]
&lt;DM ID=I 020 Type.DNP Ref=-100o&gt;41,11M-LA&lt;mm&gt;
9)3 -6&lt;DM ID=I 02 1&gt;E1A&lt;/DM&gt;bl&lt;DM ID=1022 Type=HE Refr-1021&gt;
TIIFint/DM&gt; (&lt;DM ID=I023 Typc=3PAETA Ref=l 021&gt; &apos;5
&lt;/DM&gt;3tt------A) &lt;DM 1D=1024 Type=ZPARTF Ref=1020&gt;&lt;MM&gt;—A
lfatitta)Ait, &lt;DM ID=I 025 Typec-.ZPARTF Ref=1020&gt;eJDM&gt;
&lt;HM 11)=1026&gt;_7../YIDM&gt; (&lt;DM 1D=1027 Type=1DEL Ref=1026&gt;F4
&lt;fE)M&gt;—A)b,&amp;quot;tMgaMEMPAIR.:1 a).
[4 of the 7 newly discovered patients were male bomosexuals&lt;I022&gt;
(of them&lt;1023&gt; 2 are dead). I is tietero=xual woman. and 2 (diuo 1)
are by contaminated blood product.]
La Comisio&apos;n de Te&apos;cnicos del SIDA informo&apos; ayer
de que existen &lt;DM ID=2000&gt;196 enfermos de
&lt;DM ID=2001&gt;SIDA&lt;/DM&gt;&lt;/DM&gt; en la Comunidad
Valenciana. De &lt;DM ID=2002 Type=PRO Ref=2000&gt;ellos
&lt;/DM&gt;, 147 corresponden a Valencia; 34, a Alicante;
y 15, a Castello&apos;n. Mayoritariamente &lt;DM ID=2003
Type=DNP Ref=2001&gt;la enfermedad&lt;/DM&gt; afecta a &lt;DM
I13=2004 Type=GEN&gt;los hombres&lt;/DM&gt;, con 158 casos.
Entre &lt;DM ID=2005 Type=DNP Ref=2000&gt;los afectados
&lt;/DM&gt; se encuentran nueve nin&apos;os menores de 13 an&apos;os.
</table>
<figureCaption confidence="0.996976">
Figure 7: Discourse Tagged Corpora
</figureCaption>
<bodyText confidence="0.999821206896552">
[4]). As Walker [18] reports, different discourse algo-
rithms (i.e. Brennan, Friedman and Pollard&apos;s center-
ing approach [5] vs. Hobbs&apos; algorithm [12]) perform
differently on different types of data. This suggests
that different sets of KS&apos;s are suitable for different
domains.
In order to determine, for each discourse phe-
nomenon, the most effective combination of gener-
ators, filters, and orderers, we evaluate overall per-
formance of the discourse module (cf. Section 3.1) at
different rate settings. We. measure particular gen-
erators, filters, and orders for different phenomena
to identify promising strategies. We try to mini-
mize the failure rate and the false positive rate while
minimizing the average number of hypotheses that
the generator suggests and maximizing the number
of hypotheses that the filter eliminates. As for or-
derers, those with highest effectiveness measures are
chosen for each phenomenon. The discourse module
is &amp;quot;trained&amp;quot; until a set of rate settings at which the
overall performance of the discourse module becomes
highest is obtained.
Our approach is more general than Dagan and Rai
[7], which reports on training their anaphora reso-
lution component so that &amp;quot;it&amp;quot; can be resolved to its
correct antecedent using statistical data on lexical re-
lations derived from large corpora. We will certainly
incorporate such statistical data into our discourse
KS&apos;s.
</bodyText>
<subsectionHeader confidence="0.998559">
3.3 Determining Backup Strategies
</subsectionHeader>
<bodyText confidence="0.999983571428572">
If the main strategy for resolving a particular anaphor
fails, a backup strategy that includes either a new
set of filters or a new generator is attempted. Since
backup strategies are employed only when the main
strategy does not return a hypothesis, a backup strat-
egy will either contain fewer filters than the main
strategy or it will employ a generator that returns
more hypotheses.
If the generator has a non-zero failure rate 4, a new
generator with more generating capability is chosen
from the generator tree in the knowledge source KB
as a backup strategy. Filters that occur in the main
strategy but have false positive rates above a certain
threshold are not included in the backup strategy.
</bodyText>
<sectionHeader confidence="0.999869" genericHeader="conclusions">
4 Related Work
</sectionHeader>
<bodyText confidence="0.9999888">
Our discourse module is similar to Carbonell and
Brown [6] and Rich and LuperFoy&apos;s [16] work in us-
ing multiple KS&apos;s rather than a monolithic approach
(c.f. Grosz, Joshi and Weinstein [9], Grosz and Sidner
[8], Hobbs [12], Ingria and Stallard [13]) for anaphora
resolution. However, the main difference is that our
system can deal with multiple languages as well as
multiple discourse phenomena&apos; because of our more
fine-grained and hierarchically organized KS&apos;s. Also,
our system can be evaluated and tuned at a low level
because each KS is independent of discourse phenom-
ena and can be turned off and on for automatic eval-
uation. This feature is very important because we
use our system to process real-world data in different
domains for tasks involving text understanding.
</bodyText>
<sectionHeader confidence="0.99848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999531727272727">
[1] Chinatsu Aone, Hatte Blejer, Sharon Flank,
Douglas McKee, and Sandy Shinn. The
Murasaki Project: Multilingual Natural Lan-
guage Understanding. In Proceedings of the
ARPA Human Language Technology Workshop,
1993.
[2] Chinatsu Aone, Doug McKee, Sandy Shinn,
and Hatte Blejer. SRA: Description of the
SOLOMON System as Used for MUC-4. In Pro-
ceedings of Fourth Message Understanding Con-
ference (MUG-4), 1992.
</reference>
<footnote confidence="0.6046152">
4 Zero failure rate means that the hypotheses generated by
a generator always contained the correct antecedent.
5 Carbonell and Brown&apos;s system handles only intersentential
3rd person pronouns and some definite NPs, and Rich and
LuperFoy&apos;s system handles only 3rd person pronouns.
</footnote>
<page confidence="0.986144">
162
</page>
<reference confidence="0.999957015625">
[3] Damaris Ayuso. Discourse Entities in JANUS.
In Proceedings of 27th Annual Meeting of the
ACL, 1989.
[4] Ezra Black, John Lafferty, and Salim Roukos.
Development and Evaluation of a Broad-
Coverage Probablistic Grammar of English-
Language Computer Manuals. In Proceedings of
30th Annual Meeting of the ACL, 1992.
[5] Susan Brennan, Marilyn Friedman, and Carl
Pollard. A Centering Approach to Pronouns. In
Proceedings of 25th Annual Meeting of the ACL,
1987.
[6] Jaime G. Carbonell and Ralf D. Brown.
Anaphora Resolution: A Multi-Strategy Ap-
proach. In Proceedings of the 12th International
Conference on Computational Linguistics, 1988.
[7] Ido Dagan and Alon Itai. Automatic Acquisition
of Constraints for the Resolution of Anaphora
References and Syntactic Ambiguities. In Pro-
ceedings of the 13th International Conference on
Computational Linguistics, 1990.
[8] Barbara Grosz and Candace L. Sidner. Atten-
tions, Intentions and the Structure of Discourse.
Computational Linguistics, 12, 1986.
[9] Barbara J. Grosz, Aravind K. Joshi, and Scott
Weinstein. Providing a Unified Account of Def-
inite Noun Phrases in Discourse. In Proceedings
of 21st Annual Meeting of the ACL, 1983.
[10] Raymonde Guindon, Paul Stadky, Hans Brun-
ner, and Joyce Conner. The Structure of User-
Adviser Dialogues: Is there Method in their
Madness? In Proceedings of 24th Annual Meet-
ing of the ACL, 1986.
[11] Irene Heim. The Semantics of Definite and In-
definite Noun Phrases. PhD thesis, University of
Massachusetts, 1982.
[12] Jerry R. Hobbs. Pronoun Resolution. Technical
Report 76-1, Department of Computer Science,
City College, City University of New York, 1976.
[13] Robert Ingria and David Stallard. A Computa-
tional Mechanism for Pronominal Reference. In
Proceedings of 27th Annual Meeting of the ACL,
1989.
[14] Hans Kamp. A Theory of Truth and Semantic
Representation. In J. Groenendijk et al., edi-
tors, Formal Methods in the Study of Language.
Mathematical Centre, Amsterdam, 1981.
[15] Lauri Karttunen. Discourse Referents. In J. Mc-
Cawley, editor, Syntax and Semantics 7. Aca-
demic Press, New York, 1976.
[16] Elaine Rich and Susan LuperFoy. An Architec-
ture for Anaphora Resolution. In Proceedings of
the Second Conference on Applied Natural Lan-
guage Processing, 1988.
[17] Mori Rirnon, Michael, C. McCord, Ulrike
Schwa, and Pilar Martinez. Advances in Ma-
chine Translation Research in IBM. In Proceed-
ings of Machine Translation Summit III, 1991.
[18] Marilyn A. Walker. Evaluating Discourse Pro-
cessing Algorithms. In Proceedings of 27th An-
nual Meeting of the ACL, 1989.
[19] Bonnie Webber. A Formal Approach to Dis-
course Anaphora. Technical report, Bolt, Be-
ranek, and Newman, 1978.
</reference>
<page confidence="0.999123">
163
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.944357">
<title confidence="0.9979395">A LANGUAGE-INDEPENDENT ANAPHORA RESOLUTION SYSTEM FOR UNDERSTANDING MULTILINGUAL TEXTS</title>
<author confidence="0.997372">Chinatsu Aone</author>
<author confidence="0.997372">Douglas McKee</author>
<affiliation confidence="0.997965">Systems Research and Applications (SRA)</affiliation>
<address confidence="0.9989585">2000 15th Street North Arlington, VA 22201</address>
<email confidence="0.981444">aonec©sra.com,mckeedCO.sra.corn</email>
<abstract confidence="0.9971183">This paper describes a new discourse module our multilingual Because of its unique data-driven architecture, the discourse module is language-independent. Moreover, the use of hierarchically organized multiple knowledge sources makes the module robust and trainable using discourse-tagged corpora. Separating discourse phenomena from knowledge sources makes the discourse module easily extensible to additional phenomena.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chinatsu Aone</author>
<author>Hatte Blejer</author>
<author>Sharon Flank</author>
<author>Douglas McKee</author>
<author>Sandy Shinn</author>
</authors>
<title>The Murasaki Project: Multilingual Natural Language Understanding.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop,</booktitle>
<contexts>
<context position="891" citStr="[1, 2]" startWordPosition="116" endWordPosition="117">le within our multilingual NLP system. Because of its unique data-driven architecture, the discourse module is language-independent. Moreover, the use of hierarchically organized multiple knowledge sources makes the module robust and trainable using discourse-tagged corpora. Separating discourse phenomena from knowledge sources makes the discourse module easily extensible to additional phenomena. 1 Introduction This paper describes a new discourse module within our multilingual natural language processing system which has been used for understanding texts in English, Spanish and Japanese (cf. [1, 2]).1 The following design principles underlie the discourse module: • Language-independence: No processing code depends on language-dependent facts. • Extensibility: It is easy to handle additional phenomena. • Robustness: The discourse module does its best even when its input is incomplete or wrong. • Trainability: The performance can be tuned for particular domains and applications. In the following, we first describe the architecture of the discourse module. Then, we discuss how its performance is evaluated and trained using discoursetagged corpora. Finally, we compare our approach to other </context>
</contexts>
<marker>[1]</marker>
<rawString>Chinatsu Aone, Hatte Blejer, Sharon Flank, Douglas McKee, and Sandy Shinn. The Murasaki Project: Multilingual Natural Language Understanding. In Proceedings of the ARPA Human Language Technology Workshop, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chinatsu Aone</author>
<author>Doug McKee</author>
<author>Sandy Shinn</author>
<author>Hatte Blejer</author>
</authors>
<title>SRA: Description of the SOLOMON System as Used for MUC-4.</title>
<date>1992</date>
<booktitle>In Proceedings of Fourth Message Understanding Conference (MUG-4),</booktitle>
<contexts>
<context position="891" citStr="[1, 2]" startWordPosition="116" endWordPosition="117">le within our multilingual NLP system. Because of its unique data-driven architecture, the discourse module is language-independent. Moreover, the use of hierarchically organized multiple knowledge sources makes the module robust and trainable using discourse-tagged corpora. Separating discourse phenomena from knowledge sources makes the discourse module easily extensible to additional phenomena. 1 Introduction This paper describes a new discourse module within our multilingual natural language processing system which has been used for understanding texts in English, Spanish and Japanese (cf. [1, 2]).1 The following design principles underlie the discourse module: • Language-independence: No processing code depends on language-dependent facts. • Extensibility: It is easy to handle additional phenomena. • Robustness: The discourse module does its best even when its input is incomplete or wrong. • Trainability: The performance can be tuned for particular domains and applications. In the following, we first describe the architecture of the discourse module. Then, we discuss how its performance is evaluated and trained using discoursetagged corpora. Finally, we compare our approach to other </context>
</contexts>
<marker>[2]</marker>
<rawString>Chinatsu Aone, Doug McKee, Sandy Shinn, and Hatte Blejer. SRA: Description of the SOLOMON System as Used for MUC-4. In Proceedings of Fourth Message Understanding Conference (MUG-4), 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damaris Ayuso</author>
</authors>
<title>Discourse Entities in JANUS.</title>
<date>1989</date>
<booktitle>In Proceedings of 27th Annual Meeting of the ACL,</booktitle>
<contexts>
<context position="3418" citStr="[3]" startWordPosition="502" endWordPosition="502">urse Clause (DC), Discourse Marker (DM), and File Card (FC), as shown in Figure 2. The global discourse world corresponds to an entire text, and its sub-discourse worlds correspond to subcomponents of the text such as paragraphs. Discourse worlds form a tree representing a text&apos;s structure. A discourse clause is created for each syntactic structure of category S by the semantics module. It can correspond to either a full sentence or a part of a full sentence. Each discourse clause is typed according to its syntactic properties. A discourse marker (cf. Kamp [14], or &amp;quot;discourse entity&amp;quot; in Ayuso [3]) is created for each noun or verb in the input sentence during semantic interpretation. A discourse marker is static in that once it is introduced to the discourse world, the information within it is never changed. Unlike a discourse marker, a file card (cf. Heim [11], &amp;quot;discourse referent&amp;quot; in Karttunen [15], or &amp;quot;discourse entity&amp;quot; in Webber [19]) is dynamic in a sense that it is continually updated as the discourse processing proceeds. While an indefinite discourse marker starts a file card, a definite discourse marker updates an already existing file card corresponding to its antecedent. In t</context>
</contexts>
<marker>[3]</marker>
<rawString>Damaris Ayuso. Discourse Entities in JANUS. In Proceedings of 27th Annual Meeting of the ACL, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>John Lafferty</author>
<author>Salim Roukos</author>
</authors>
<title>Development and Evaluation of a BroadCoverage Probablistic Grammar of EnglishLanguage Computer Manuals.</title>
<date>1992</date>
<booktitle>In Proceedings of 30th Annual Meeting of the ACL,</booktitle>
<contexts>
<context position="22194" citStr="[4]" startWordPosition="3384" endWordPosition="3384">s tietero=xual woman. and 2 (diuo 1) are by contaminated blood product.] La Comisio&apos;n de Te&apos;cnicos del SIDA informo&apos; ayer de que existen &lt;DM ID=2000&gt;196 enfermos de &lt;DM ID=2001&gt;SIDA&lt;/DM&gt;&lt;/DM&gt; en la Comunidad Valenciana. De &lt;DM ID=2002 Type=PRO Ref=2000&gt;ellos &lt;/DM&gt;, 147 corresponden a Valencia; 34, a Alicante; y 15, a Castello&apos;n. Mayoritariamente &lt;DM ID=2003 Type=DNP Ref=2001&gt;la enfermedad&lt;/DM&gt; afecta a &lt;DM I13=2004 Type=GEN&gt;los hombres&lt;/DM&gt;, con 158 casos. Entre &lt;DM ID=2005 Type=DNP Ref=2000&gt;los afectados &lt;/DM&gt; se encuentran nueve nin&apos;os menores de 13 an&apos;os. Figure 7: Discourse Tagged Corpora [4]). As Walker [18] reports, different discourse algorithms (i.e. Brennan, Friedman and Pollard&apos;s centering approach [5] vs. Hobbs&apos; algorithm [12]) perform differently on different types of data. This suggests that different sets of KS&apos;s are suitable for different domains. In order to determine, for each discourse phenomenon, the most effective combination of generators, filters, and orderers, we evaluate overall performance of the discourse module (cf. Section 3.1) at different rate settings. We. measure particular generators, filters, and orders for different phenomena to identify promising st</context>
</contexts>
<marker>[4]</marker>
<rawString>Ezra Black, John Lafferty, and Salim Roukos. Development and Evaluation of a BroadCoverage Probablistic Grammar of EnglishLanguage Computer Manuals. In Proceedings of 30th Annual Meeting of the ACL, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Brennan</author>
<author>Marilyn Friedman</author>
<author>Carl Pollard</author>
</authors>
<title>A Centering Approach to Pronouns.</title>
<date>1987</date>
<booktitle>In Proceedings of 25th Annual Meeting of the ACL,</booktitle>
<contexts>
<context position="22312" citStr="[5]" startWordPosition="3401" endWordPosition="3401">yer de que existen &lt;DM ID=2000&gt;196 enfermos de &lt;DM ID=2001&gt;SIDA&lt;/DM&gt;&lt;/DM&gt; en la Comunidad Valenciana. De &lt;DM ID=2002 Type=PRO Ref=2000&gt;ellos &lt;/DM&gt;, 147 corresponden a Valencia; 34, a Alicante; y 15, a Castello&apos;n. Mayoritariamente &lt;DM ID=2003 Type=DNP Ref=2001&gt;la enfermedad&lt;/DM&gt; afecta a &lt;DM I13=2004 Type=GEN&gt;los hombres&lt;/DM&gt;, con 158 casos. Entre &lt;DM ID=2005 Type=DNP Ref=2000&gt;los afectados &lt;/DM&gt; se encuentran nueve nin&apos;os menores de 13 an&apos;os. Figure 7: Discourse Tagged Corpora [4]). As Walker [18] reports, different discourse algorithms (i.e. Brennan, Friedman and Pollard&apos;s centering approach [5] vs. Hobbs&apos; algorithm [12]) perform differently on different types of data. This suggests that different sets of KS&apos;s are suitable for different domains. In order to determine, for each discourse phenomenon, the most effective combination of generators, filters, and orderers, we evaluate overall performance of the discourse module (cf. Section 3.1) at different rate settings. We. measure particular generators, filters, and orders for different phenomena to identify promising strategies. We try to minimize the failure rate and the false positive rate while minimizing the average number of hypot</context>
</contexts>
<marker>[5]</marker>
<rawString>Susan Brennan, Marilyn Friedman, and Carl Pollard. A Centering Approach to Pronouns. In Proceedings of 25th Annual Meeting of the ACL, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime G Carbonell</author>
<author>Ralf D Brown</author>
</authors>
<title>Anaphora Resolution: A Multi-Strategy Approach.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics,</booktitle>
<contexts>
<context position="12853" citStr="[6]" startWordPosition="1913" endWordPosition="1913"> best hypothesis Update-Discourse-World Input: anaphor, best hypothesis, global discourse world Output: updated global discourse world Figure 5: Resolution Engine Operations KS&apos;s. If only one hypothesis remains, it is returned as the anaphor&apos;s referent, but there may be more than one hypothesis or none at all. When there is more than one hypothesis, orderer KS&apos;s are invoked. However, when more than one orderer KS could apply to the anaphor, we face the problem of how to combine the preference values returned by these multiple orderers. Some anaphora resolution systems (cf. Carbonell and Brown [6], Rich and LuperFoy [16], Rimon et al. [17]) assign scores to antecedent hypotheses, and the hypotheses are ranked according to their scores. Deciding the scores output by the orderers as well as the way the scores are combined requires more research with larger data. In our current system, therefore, when there are multiple hypotheses left, the most &amp;quot;promising&amp;quot; orderer is chosen for each discourse phenomenon. In Section 3, we discuss how we choose such an orderer for each discourse phenomenon by using statistical preference. In the future, we will experiment with ways for each orderer to assi</context>
<context position="24382" citStr="[6]" startWordPosition="3733" endWordPosition="3733">ategies are employed only when the main strategy does not return a hypothesis, a backup strategy will either contain fewer filters than the main strategy or it will employ a generator that returns more hypotheses. If the generator has a non-zero failure rate 4, a new generator with more generating capability is chosen from the generator tree in the knowledge source KB as a backup strategy. Filters that occur in the main strategy but have false positive rates above a certain threshold are not included in the backup strategy. 4 Related Work Our discourse module is similar to Carbonell and Brown [6] and Rich and LuperFoy&apos;s [16] work in using multiple KS&apos;s rather than a monolithic approach (c.f. Grosz, Joshi and Weinstein [9], Grosz and Sidner [8], Hobbs [12], Ingria and Stallard [13]) for anaphora resolution. However, the main difference is that our system can deal with multiple languages as well as multiple discourse phenomena&apos; because of our more fine-grained and hierarchically organized KS&apos;s. Also, our system can be evaluated and tuned at a low level because each KS is independent of discourse phenomena and can be turned off and on for automatic evaluation. This feature is very import</context>
</contexts>
<marker>[6]</marker>
<rawString>Jaime G. Carbonell and Ralf D. Brown. Anaphora Resolution: A Multi-Strategy Approach. In Proceedings of the 12th International Conference on Computational Linguistics, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
</authors>
<title>Automatic Acquisition of Constraints for the Resolution of Anaphora References and Syntactic Ambiguities.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics,</booktitle>
<contexts>
<context position="23305" citStr="[7]" startWordPosition="3556" endWordPosition="3556"> particular generators, filters, and orders for different phenomena to identify promising strategies. We try to minimize the failure rate and the false positive rate while minimizing the average number of hypotheses that the generator suggests and maximizing the number of hypotheses that the filter eliminates. As for orderers, those with highest effectiveness measures are chosen for each phenomenon. The discourse module is &amp;quot;trained&amp;quot; until a set of rate settings at which the overall performance of the discourse module becomes highest is obtained. Our approach is more general than Dagan and Rai [7], which reports on training their anaphora resolution component so that &amp;quot;it&amp;quot; can be resolved to its correct antecedent using statistical data on lexical relations derived from large corpora. We will certainly incorporate such statistical data into our discourse KS&apos;s. 3.3 Determining Backup Strategies If the main strategy for resolving a particular anaphor fails, a backup strategy that includes either a new set of filters or a new generator is attempted. Since backup strategies are employed only when the main strategy does not return a hypothesis, a backup strategy will either contain fewer fil</context>
</contexts>
<marker>[7]</marker>
<rawString>Ido Dagan and Alon Itai. Automatic Acquisition of Constraints for the Resolution of Anaphora References and Syntactic Ambiguities. In Proceedings of the 13th International Conference on Computational Linguistics, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attentions, Intentions and the Structure of Discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<marker>[8]</marker>
<rawString>Barbara Grosz and Candace L. Sidner. Attentions, Intentions and the Structure of Discourse. Computational Linguistics, 12, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Providing a Unified Account of Definite Noun Phrases in Discourse.</title>
<date>1983</date>
<booktitle>In Proceedings of 21st Annual Meeting of the ACL,</booktitle>
<contexts>
<context position="24510" citStr="[9]" startWordPosition="3755" endWordPosition="3755">rs than the main strategy or it will employ a generator that returns more hypotheses. If the generator has a non-zero failure rate 4, a new generator with more generating capability is chosen from the generator tree in the knowledge source KB as a backup strategy. Filters that occur in the main strategy but have false positive rates above a certain threshold are not included in the backup strategy. 4 Related Work Our discourse module is similar to Carbonell and Brown [6] and Rich and LuperFoy&apos;s [16] work in using multiple KS&apos;s rather than a monolithic approach (c.f. Grosz, Joshi and Weinstein [9], Grosz and Sidner [8], Hobbs [12], Ingria and Stallard [13]) for anaphora resolution. However, the main difference is that our system can deal with multiple languages as well as multiple discourse phenomena&apos; because of our more fine-grained and hierarchically organized KS&apos;s. Also, our system can be evaluated and tuned at a low level because each KS is independent of discourse phenomena and can be turned off and on for automatic evaluation. This feature is very important because we use our system to process real-world data in different domains for tasks involving text understanding. References</context>
</contexts>
<marker>[9]</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Providing a Unified Account of Definite Noun Phrases in Discourse. In Proceedings of 21st Annual Meeting of the ACL, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymonde Guindon</author>
<author>Paul Stadky</author>
<author>Hans Brunner</author>
<author>Joyce Conner</author>
</authors>
<title>The Structure of UserAdviser Dialogues: Is there Method in their Madness?</title>
<date>1986</date>
<booktitle>In Proceedings of 24th Annual Meeting of the ACL,</booktitle>
<contexts>
<context position="5522" citStr="[10]" startWordPosition="831" endWordPosition="831">ource KB houses small well-defined anaphora resolution strategies. Each knowledge source (KS) is an object in the hierarchically organized KB, and information in a specific KS can be inherited from a more general KS. There are three kinds of KS&apos;s: a generator, a filter and an orderer. A generator is used to generate pos1.coisse Figure 3: Discourse Knowledge Source KB sible antecedent hypotheses from the global discourse world. Unlike other discourse systems, we have multiple generators because different discourse phenomena exhibit different antecedent distribution patterns (cf. Guindon et al. [10]). A filter is used to eliminate impossible hypotheses, while an orderer is used to rank possible hypotheses in a preference order. The KS tree is shown in Figure 3. Each KS contains three slots: ks-function, ks-data, and ks-language. The ks-function slot contains a functional definition of the KS. For example, the functional definition of the Syntactic-Gender filter defines when the syntactic gender of an anaphor is compatible with that of an antecedent hypothesis. A ks-data slot contains data used by ks-function. The separation of data from function is desirable because a parent KS can speci</context>
</contexts>
<marker>[10]</marker>
<rawString>Raymonde Guindon, Paul Stadky, Hans Brunner, and Joyce Conner. The Structure of UserAdviser Dialogues: Is there Method in their Madness? In Proceedings of 24th Annual Meeting of the ACL, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Heim</author>
</authors>
<title>The Semantics of Definite and Indefinite Noun Phrases.</title>
<date>1982</date>
<tech>PhD thesis,</tech>
<institution>University of Massachusetts,</institution>
<contexts>
<context position="3687" citStr="[11]" startWordPosition="549" endWordPosition="549">ing a text&apos;s structure. A discourse clause is created for each syntactic structure of category S by the semantics module. It can correspond to either a full sentence or a part of a full sentence. Each discourse clause is typed according to its syntactic properties. A discourse marker (cf. Kamp [14], or &amp;quot;discourse entity&amp;quot; in Ayuso [3]) is created for each noun or verb in the input sentence during semantic interpretation. A discourse marker is static in that once it is introduced to the discourse world, the information within it is never changed. Unlike a discourse marker, a file card (cf. Heim [11], &amp;quot;discourse referent&amp;quot; in Karttunen [15], or &amp;quot;discourse entity&amp;quot; in Webber [19]) is dynamic in a sense that it is continually updated as the discourse processing proceeds. While an indefinite discourse marker starts a file card, a definite discourse marker updates an already existing file card corresponding to its antecedent. In this way, a file card keeps track of all its co-referring discourse markers, and accumulates semantic information within them. 2.2 Discourse Administrator Our discourse module is customized at development time by creating and modifying the three discourse KB&apos;s using the</context>
<context position="15254" citStr="[11]" startWordPosition="2301" endWordPosition="2301"> because the anaphor has no useful syntactic information. As a backup strategy, a new generator, Adjacent-NP, is chosen in case the parse fails to create an appositive relation between the antecedent NP ID=1022 and the anaphor. 159 The AIDS Surveillance Committee Three of them were confirmed 7 AIDS patients yesterday. hemophiliac. FC-5 coreferring-DM&apos;s: 1DM-1 DM-2) semantics: Patient 101 ^ Person.102 Figure 6: Updating Discourse World 2.3.2 Updating the Global Discourse World After each anaphor resolution, the global discourse world is updated as it would be in File Change Semantics (cf. Heim [11]), and as shown in Figure 6. First, the discourse marker for the anaphor is incorporated into the file card to which its antecedent discourse marker points so that the co-referring discourse markers point to the same file card. Then, the semantics information of the file card is updated so that it reflects the union of the information from all the co-referring discourse markers. In this way, a file card accumulates more information as the discourse processing proceeds. The motivation for having both discourse markers and file cards is to make the discourse processing a monotonic operation. Thu</context>
</contexts>
<marker>[11]</marker>
<rawString>Irene Heim. The Semantics of Definite and Indefinite Noun Phrases. PhD thesis, University of Massachusetts, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Pronoun Resolution.</title>
<date>1976</date>
<tech>Technical Report 76-1,</tech>
<institution>Department of Computer Science, City College, City University of</institution>
<location>New York,</location>
<contexts>
<context position="22338" citStr="[12]" startWordPosition="3405" endWordPosition="3405">=2000&gt;196 enfermos de &lt;DM ID=2001&gt;SIDA&lt;/DM&gt;&lt;/DM&gt; en la Comunidad Valenciana. De &lt;DM ID=2002 Type=PRO Ref=2000&gt;ellos &lt;/DM&gt;, 147 corresponden a Valencia; 34, a Alicante; y 15, a Castello&apos;n. Mayoritariamente &lt;DM ID=2003 Type=DNP Ref=2001&gt;la enfermedad&lt;/DM&gt; afecta a &lt;DM I13=2004 Type=GEN&gt;los hombres&lt;/DM&gt;, con 158 casos. Entre &lt;DM ID=2005 Type=DNP Ref=2000&gt;los afectados &lt;/DM&gt; se encuentran nueve nin&apos;os menores de 13 an&apos;os. Figure 7: Discourse Tagged Corpora [4]). As Walker [18] reports, different discourse algorithms (i.e. Brennan, Friedman and Pollard&apos;s centering approach [5] vs. Hobbs&apos; algorithm [12]) perform differently on different types of data. This suggests that different sets of KS&apos;s are suitable for different domains. In order to determine, for each discourse phenomenon, the most effective combination of generators, filters, and orderers, we evaluate overall performance of the discourse module (cf. Section 3.1) at different rate settings. We. measure particular generators, filters, and orders for different phenomena to identify promising strategies. We try to minimize the failure rate and the false positive rate while minimizing the average number of hypotheses that the generator s</context>
</contexts>
<marker>[12]</marker>
<rawString>Jerry R. Hobbs. Pronoun Resolution. Technical Report 76-1, Department of Computer Science, City College, City University of New York, 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Ingria</author>
<author>David Stallard</author>
</authors>
<title>A Computational Mechanism for Pronominal Reference.</title>
<date>1989</date>
<booktitle>In Proceedings of 27th Annual Meeting of the ACL,</booktitle>
<marker>[13]</marker>
<rawString>Robert Ingria and David Stallard. A Computational Mechanism for Pronominal Reference. In Proceedings of 27th Annual Meeting of the ACL, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
</authors>
<title>A Theory of Truth and Semantic Representation.</title>
<date>1981</date>
<booktitle>Formal Methods in the Study of Language. Mathematical Centre,</booktitle>
<editor>In J. Groenendijk et al., editors,</editor>
<location>Amsterdam,</location>
<contexts>
<context position="3382" citStr="[14]" startWordPosition="496" endWordPosition="496">orld: Discourse World (DW), Dis156 course Clause (DC), Discourse Marker (DM), and File Card (FC), as shown in Figure 2. The global discourse world corresponds to an entire text, and its sub-discourse worlds correspond to subcomponents of the text such as paragraphs. Discourse worlds form a tree representing a text&apos;s structure. A discourse clause is created for each syntactic structure of category S by the semantics module. It can correspond to either a full sentence or a part of a full sentence. Each discourse clause is typed according to its syntactic properties. A discourse marker (cf. Kamp [14], or &amp;quot;discourse entity&amp;quot; in Ayuso [3]) is created for each noun or verb in the input sentence during semantic interpretation. A discourse marker is static in that once it is introduced to the discourse world, the information within it is never changed. Unlike a discourse marker, a file card (cf. Heim [11], &amp;quot;discourse referent&amp;quot; in Karttunen [15], or &amp;quot;discourse entity&amp;quot; in Webber [19]) is dynamic in a sense that it is continually updated as the discourse processing proceeds. While an indefinite discourse marker starts a file card, a definite discourse marker updates an already existing file card c</context>
</contexts>
<marker>[14]</marker>
<rawString>Hans Kamp. A Theory of Truth and Semantic Representation. In J. Groenendijk et al., editors, Formal Methods in the Study of Language. Mathematical Centre, Amsterdam, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Discourse Referents. In</title>
<date>1976</date>
<booktitle>Syntax and Semantics 7.</booktitle>
<editor>J. McCawley, editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="3727" citStr="[15]" startWordPosition="554" endWordPosition="554">se is created for each syntactic structure of category S by the semantics module. It can correspond to either a full sentence or a part of a full sentence. Each discourse clause is typed according to its syntactic properties. A discourse marker (cf. Kamp [14], or &amp;quot;discourse entity&amp;quot; in Ayuso [3]) is created for each noun or verb in the input sentence during semantic interpretation. A discourse marker is static in that once it is introduced to the discourse world, the information within it is never changed. Unlike a discourse marker, a file card (cf. Heim [11], &amp;quot;discourse referent&amp;quot; in Karttunen [15], or &amp;quot;discourse entity&amp;quot; in Webber [19]) is dynamic in a sense that it is continually updated as the discourse processing proceeds. While an indefinite discourse marker starts a file card, a definite discourse marker updates an already existing file card corresponding to its antecedent. In this way, a file card keeps track of all its co-referring discourse markers, and accumulates semantic information within them. 2.2 Discourse Administrator Our discourse module is customized at development time by creating and modifying the three discourse KB&apos;s using the Discourse Administrator. First, a disco</context>
</contexts>
<marker>[15]</marker>
<rawString>Lauri Karttunen. Discourse Referents. In J. McCawley, editor, Syntax and Semantics 7. Academic Press, New York, 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elaine Rich</author>
<author>Susan LuperFoy</author>
</authors>
<title>An Architecture for Anaphora Resolution.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<contexts>
<context position="12877" citStr="[16]" startWordPosition="1917" endWordPosition="1917">-Discourse-World Input: anaphor, best hypothesis, global discourse world Output: updated global discourse world Figure 5: Resolution Engine Operations KS&apos;s. If only one hypothesis remains, it is returned as the anaphor&apos;s referent, but there may be more than one hypothesis or none at all. When there is more than one hypothesis, orderer KS&apos;s are invoked. However, when more than one orderer KS could apply to the anaphor, we face the problem of how to combine the preference values returned by these multiple orderers. Some anaphora resolution systems (cf. Carbonell and Brown [6], Rich and LuperFoy [16], Rimon et al. [17]) assign scores to antecedent hypotheses, and the hypotheses are ranked according to their scores. Deciding the scores output by the orderers as well as the way the scores are combined requires more research with larger data. In our current system, therefore, when there are multiple hypotheses left, the most &amp;quot;promising&amp;quot; orderer is chosen for each discourse phenomenon. In Section 3, we discuss how we choose such an orderer for each discourse phenomenon by using statistical preference. In the future, we will experiment with ways for each orderer to assign &amp;quot;meaningful&amp;quot; scores t</context>
<context position="24411" citStr="[16]" startWordPosition="3738" endWordPosition="3738">en the main strategy does not return a hypothesis, a backup strategy will either contain fewer filters than the main strategy or it will employ a generator that returns more hypotheses. If the generator has a non-zero failure rate 4, a new generator with more generating capability is chosen from the generator tree in the knowledge source KB as a backup strategy. Filters that occur in the main strategy but have false positive rates above a certain threshold are not included in the backup strategy. 4 Related Work Our discourse module is similar to Carbonell and Brown [6] and Rich and LuperFoy&apos;s [16] work in using multiple KS&apos;s rather than a monolithic approach (c.f. Grosz, Joshi and Weinstein [9], Grosz and Sidner [8], Hobbs [12], Ingria and Stallard [13]) for anaphora resolution. However, the main difference is that our system can deal with multiple languages as well as multiple discourse phenomena&apos; because of our more fine-grained and hierarchically organized KS&apos;s. Also, our system can be evaluated and tuned at a low level because each KS is independent of discourse phenomena and can be turned off and on for automatic evaluation. This feature is very important because we use our system</context>
</contexts>
<marker>[16]</marker>
<rawString>Elaine Rich and Susan LuperFoy. An Architecture for Anaphora Resolution. In Proceedings of the Second Conference on Applied Natural Language Processing, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mori Rirnon</author>
<author>C McCord Michael</author>
<author>Ulrike Schwa</author>
<author>Pilar Martinez</author>
</authors>
<date>1991</date>
<booktitle>Advances in Machine Translation Research in IBM. In Proceedings of Machine Translation Summit III,</booktitle>
<contexts>
<context position="12896" citStr="[17]" startWordPosition="1921" endWordPosition="1921">put: anaphor, best hypothesis, global discourse world Output: updated global discourse world Figure 5: Resolution Engine Operations KS&apos;s. If only one hypothesis remains, it is returned as the anaphor&apos;s referent, but there may be more than one hypothesis or none at all. When there is more than one hypothesis, orderer KS&apos;s are invoked. However, when more than one orderer KS could apply to the anaphor, we face the problem of how to combine the preference values returned by these multiple orderers. Some anaphora resolution systems (cf. Carbonell and Brown [6], Rich and LuperFoy [16], Rimon et al. [17]) assign scores to antecedent hypotheses, and the hypotheses are ranked according to their scores. Deciding the scores output by the orderers as well as the way the scores are combined requires more research with larger data. In our current system, therefore, when there are multiple hypotheses left, the most &amp;quot;promising&amp;quot; orderer is chosen for each discourse phenomenon. In Section 3, we discuss how we choose such an orderer for each discourse phenomenon by using statistical preference. In the future, we will experiment with ways for each orderer to assign &amp;quot;meaningful&amp;quot; scores to hypotheses. When </context>
</contexts>
<marker>[17]</marker>
<rawString>Mori Rirnon, Michael, C. McCord, Ulrike Schwa, and Pilar Martinez. Advances in Machine Translation Research in IBM. In Proceedings of Machine Translation Summit III, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
</authors>
<title>Evaluating Discourse Processing Algorithms.</title>
<date>1989</date>
<booktitle>In Proceedings of 27th Annual Meeting of the ACL,</booktitle>
<contexts>
<context position="22211" citStr="[18]" startWordPosition="3387" endWordPosition="3387">oman. and 2 (diuo 1) are by contaminated blood product.] La Comisio&apos;n de Te&apos;cnicos del SIDA informo&apos; ayer de que existen &lt;DM ID=2000&gt;196 enfermos de &lt;DM ID=2001&gt;SIDA&lt;/DM&gt;&lt;/DM&gt; en la Comunidad Valenciana. De &lt;DM ID=2002 Type=PRO Ref=2000&gt;ellos &lt;/DM&gt;, 147 corresponden a Valencia; 34, a Alicante; y 15, a Castello&apos;n. Mayoritariamente &lt;DM ID=2003 Type=DNP Ref=2001&gt;la enfermedad&lt;/DM&gt; afecta a &lt;DM I13=2004 Type=GEN&gt;los hombres&lt;/DM&gt;, con 158 casos. Entre &lt;DM ID=2005 Type=DNP Ref=2000&gt;los afectados &lt;/DM&gt; se encuentran nueve nin&apos;os menores de 13 an&apos;os. Figure 7: Discourse Tagged Corpora [4]). As Walker [18] reports, different discourse algorithms (i.e. Brennan, Friedman and Pollard&apos;s centering approach [5] vs. Hobbs&apos; algorithm [12]) perform differently on different types of data. This suggests that different sets of KS&apos;s are suitable for different domains. In order to determine, for each discourse phenomenon, the most effective combination of generators, filters, and orderers, we evaluate overall performance of the discourse module (cf. Section 3.1) at different rate settings. We. measure particular generators, filters, and orders for different phenomena to identify promising strategies. We try </context>
</contexts>
<marker>[18]</marker>
<rawString>Marilyn A. Walker. Evaluating Discourse Processing Algorithms. In Proceedings of 27th Annual Meeting of the ACL, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
</authors>
<title>A Formal Approach to Discourse Anaphora.</title>
<date>1978</date>
<tech>Technical report,</tech>
<location>Bolt, Beranek, and Newman,</location>
<contexts>
<context position="3765" citStr="[19]" startWordPosition="560" endWordPosition="560">ture of category S by the semantics module. It can correspond to either a full sentence or a part of a full sentence. Each discourse clause is typed according to its syntactic properties. A discourse marker (cf. Kamp [14], or &amp;quot;discourse entity&amp;quot; in Ayuso [3]) is created for each noun or verb in the input sentence during semantic interpretation. A discourse marker is static in that once it is introduced to the discourse world, the information within it is never changed. Unlike a discourse marker, a file card (cf. Heim [11], &amp;quot;discourse referent&amp;quot; in Karttunen [15], or &amp;quot;discourse entity&amp;quot; in Webber [19]) is dynamic in a sense that it is continually updated as the discourse processing proceeds. While an indefinite discourse marker starts a file card, a definite discourse marker updates an already existing file card corresponding to its antecedent. In this way, a file card keeps track of all its co-referring discourse markers, and accumulates semantic information within them. 2.2 Discourse Administrator Our discourse module is customized at development time by creating and modifying the three discourse KB&apos;s using the Discourse Administrator. First, a discourse domain is established for a parti</context>
</contexts>
<marker>[19]</marker>
<rawString>Bonnie Webber. A Formal Approach to Discourse Anaphora. Technical report, Bolt, Beranek, and Newman, 1978.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>