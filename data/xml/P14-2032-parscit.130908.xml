<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000994">
<title confidence="0.9984735">
Two Knives Cut Better Than One:
Chinese Word Segmentation with Dual Decomposition
</title>
<author confidence="0.998624">
Mengqiu Wang Rob Voigt Christopher D. Manning
</author>
<affiliation confidence="0.999337">
Computer Science Department Linguistics Department Computer Science Department
Stanford University Stanford University Stanford University
</affiliation>
<address confidence="0.878655">
Stanford, CA 94305 Stanford, CA 94305 Stanford, CA 94305
</address>
<email confidence="0.999704">
{mengqiu,manning}@cs.stanford.edu robvoigt@stanford.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984888888889">
There are two dominant approaches to
Chinese word segmentation: word-based
and character-based models, each with re-
spective strengths. Prior work has shown
that gains in segmentation performance
can be achieved from combining these
two types of models; however, past efforts
have not provided a practical technique
to allow mainstream adoption. We pro-
pose a method that effectively combines
the strength of both segmentation schemes
using an efficient dual-decomposition al-
gorithm for joint inference. Our method
is simple and easy to implement. Ex-
periments on SIGHAN 2003 and 2005
evaluation datasets show that our method
achieves the best reported results to date
on 6 out of 7 datasets.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956290909091">
Chinese text is written without delimiters between
words; as a result, Chinese word segmentation
(CWS) is an essential foundational step for many
tasks in Chinese natural language processing. As
demonstrated by (Shi and Wang, 2007; Bai et
al., 2008; Chang et al., 2008; Kummerfeld et al.,
2013), the quality and consistency of segmentation
has important downstream impacts on system per-
formance in machine translation, POS tagging and
parsing.
State-of-the-art performance in CWS is high,
with F-scores in the upper 90s. Still, challenges
remain. Unknown words, also known as out-of-
vocabulary (OOV) words, lead to difficulties for
word- or dictionary-based approaches. Ambiguity
can cause errors when the appropriate segmenta-
tion is determined contextually, such as 才能 (“tal-
ent”) and 才 / 能 (“just able”) (Gao et al., 2003).
There are two primary classes of models:
character-based, where the foundational units for
processing are individual Chinese characters (Xue,
2003; Tseng et al., 2005; Zhang et al., 2006;
Wang et al., 2010), and word-based, where the
units are full words based on some dictionary or
training lexicon (Andrew, 2006; Zhang and Clark,
2007). Sun (2010) details their respective theo-
retical strengths: character-based approaches bet-
ter model the internal compositional structure of
words and are therefore more effective at inducing
new OOV words; word-based approaches are bet-
ter at reproducing the words of the training lexi-
con and can capture information from significantly
larger contextual spans. Prior work has shown per-
formance gains from combining these two types
of models to exploit their respective strengths, but
such approaches are often complex to implement
and computationally expensive.
In this work, we propose a simple and prin-
cipled joint decoding method for combining
character-based and word-based segmenters based
on dual decomposition. This method has strong
optimality guarantees and works very well empir-
ically. It is easy to implement and does not re-
quire retraining of existing character- and word-
based segmenters. Perhaps most importantly, this
work presents a much more practical and usable
form of classifier combination in the CWS context
than existing methods offer.
Experimental results on standard SIGHAN
2003 and 2005 bake-off evaluations show that our
model outperforms the character and word base-
lines by a significant margin. In particular, out
approach improves OOV recall rates and segmen-
tation consistency, and gives the best reported re-
sults to date on 6 out of 7 datasets.
</bodyText>
<sectionHeader confidence="0.99654" genericHeader="method">
2 Models for CWS
</sectionHeader>
<bodyText confidence="0.99895325">
Here we describe the character-based and word-
based models we use as baselines, review existing
approaches to combination, and describe our algo-
rithm for joint decoding with dual decomposition.
</bodyText>
<page confidence="0.988415">
193
</page>
<bodyText confidence="0.7015285">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 193–198,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.857131">
2.1 Character-based Models
</subsectionHeader>
<bodyText confidence="0.998224333333333">
In the most commonly used contemporary ap-
proach to character-based segmentation, first pro-
posed by (Xue, 2003), CWS is seen as a charac-
ter sequence tagging task, where each character
is tagged on whether it is at the beginning, mid-
dle, or end of a word. Conditional random fields
(CRF) (Lafferty et al., 2001) have been widely
adopted for this task, and give state-of-the-art re-
sults (Tseng et al., 2005). In a first-order linear-
chain CRF model, the conditional probability of a
label sequence y given a word sequence x is de-
fined as:
Algorithm 1 Dual decomposition inference algo-
rithm, and modified Viterbi and beam-search algo-
rithms.
</bodyText>
<equation confidence="0.942740045454546">
∀i ∈ {1 to |x|}: ∀k ∈ {0, 1}: ui(k) = 0
fort ← 1toTdo
yc* = argmax P(yc|x) + E
y iE|x|
yw* = argmax F(yw|x) − E uj(ywj )
yEGEN(x) jE|x|
if yc* = yw* then
return (yc*, yw*)
end if
for all i ∈ {1 to |x|} do
∀k ∈ {0, 1} : ui(k) = ui(k) + αt(2k − 1)(yw*
i −
yi* )
end for
end for
return (yc*, yw*)
ui(yci)
1 |y |exp (θ · f(x, yt, yt+1)) Viterbi:
P(y|x) = ZL V1(1) = 1, V1(0) = 0
t=1 for i = 2 to |x |do
∀k ∈ {0, 1} : Vi(k) = argmaxPi(k|k&apos;)Vi_1k&apos; +
k&apos;
</equation>
<bodyText confidence="0.962921">
f(x, yt, yt−1) are feature functions that typically
include surrounding character n-gram and mor-
phological suffix/prefix features. These types of
features capture the compositional properties of
characters and are likely to generalize well to un-
known words. However, the Markov assumption
in CRF limits the context of such features; it is
difficult to capture long-range word features in this
model.
</bodyText>
<subsectionHeader confidence="0.976318">
2.2 Word-based Models
</subsectionHeader>
<bodyText confidence="0.999979">
Word-based models search through lists of word
candidates using scoring functions that directly
assign scores to each. Early word-based seg-
mentation work employed simple heuristics like
dictionary-lookup maximum matching (Chen and
Liu, 1992). More recently, Zhang and Clark
(2007) reported success using a linear model
trained with the average perceptron algorithm
(Collins, 2002). Formally, given input x, their
model seeks a segmentation y such that:
</bodyText>
<equation confidence="0.996824">
F(y|x) = max (α · φ(y))
y∈GEN(x)
</equation>
<bodyText confidence="0.999889111111111">
F(y|x) is the score of segmentation result y.
Searching through the entire GEN(x) space is
intractable even with a local model, so a beam-
search algorithm is used. The search algorithm
consumes one character input token at a time, and
iterates through the existing beams to score two
new alternative hypotheses by either appending
the new character to the last word in the beam, or
starting a new word at the current position.
</bodyText>
<equation confidence="0.986748285714286">
ui(k)
end for
Beam-Search:
for i = 1 to |x |do
for item v = {w0, · · · ,wj} in beam(i) do
append xi to wj, score(v) += ui(0)
v = {w0, · · · ,wj, xi}, score(v) += ui(1)
</equation>
<subsectionHeader confidence="0.80928425">
end for
end for
2.3 Combining Models with Dual
Decomposition
</subsectionHeader>
<bodyText confidence="0.999959272727273">
Various mixing approaches have been proposed to
combine the above two approaches (Wang et al.,
2006; Lin, 2009; Sun et al., 2009; Sun, 2010;
Wang et al., 2010). These mixing models perform
well on standard datasets, but are not in wide use
because of their high computational costs and dif-
ficulty of implementation.
Dual decomposition (DD) (Rush et al., 2010)
offers an attractive framework for combining these
two types of models without incurring high costs
in model complexity (in contrast to (Sun et al.,
2009)) or decoding efficiency (in contrast to bag-
ging in (Wang et al., 2006; Sun, 2010)). DD has
been successfully applied to similar situations for
combining local with global models; for example,
in dependency parsing (Koo et al., 2010), bilingual
sequence tagging (Wang et al., 2013) and word
alignment (DeNero and Macherey, 2011).
The idea is that jointly modelling both
character-sequence and word information can be
computationally challenging, so instead we can try
to find outputs that the two models are most likely
</bodyText>
<page confidence="0.999297">
194
</page>
<tableCaption confidence="0.9925665">
Table 1: Results on SIGHAN 2005 datasets. Roov denotes OOV recall, and C denotes segmentation
consistency. Best number in each column is highlighted in bold.
</tableCaption>
<figure confidence="0.994581655737705">
R
Char-based CRF 95.2
Word-based Perceptron 95.8
Dual-decomp 95.9
R
Char-based CRF 94.7
Word-based Perceptron 94.3
Dual-decomp 95.0
Academia Sinica
P F1 Rooms C
0.064
0.060
0.055
City Univ. of Hong Kong
P F1 Rooms C
0.065
0.073
0.062
Peking Univ.
R P F1 Rooms C
93.6 94.4 58.9
95.0 95.4 69.5
94.9 95.4 67.7
94.0 94.3 76.1
94.0 94.2 71.7
94.4 94.7 75.3
94.6
94.1
94.8
95.3
95.5
95.7
94.9
94.8
95.3
77.8
76.7
78.7
0.089
0.099
0.086
Microsoft Research
R P F1 Rooms C
96.4
97.0
97.3
96.6
97.2
97.4
96.5
97.1
97.4
71.3
74.6
76.0
0.074
0.063
0.055
to agree on. Formally, the objective of DD is:
max P(yc|x) + F(yw|x) s.t. yc = yw (1)
yc,yw
</figure>
<bodyText confidence="0.999258142857143">
where yc is the output of character-based CRF, yw
is the output of word-based perceptron, and the
agreements are expressed as constraints. s.t. is
a shorthand for “such that”.
Solving this constrained optimization problem
directly is difficult. Instead, we take the La-
grangian relaxation of this term as:
</bodyText>
<equation confidence="0.985713333333333">
L (yc, yw, U) = (2)
P(yc|x) + F(yw|x) + X ui(yci − ywi )
iE|x|
</equation>
<bodyText confidence="0.99939425">
where U is the set of Lagrangian multipliers that
consists of a multiplier ui at each word position i.
We can rewrite the original objective with the
Lagrangian relaxation as:
</bodyText>
<equation confidence="0.96279">
L (yc, yw, U) (3)
</equation>
<bodyText confidence="0.999469428571429">
We can then form the dual of this problem by
taking the min outside of the max, which is an up-
per bound on the original problem. The dual form
can then be decomposed into two sub-components
(the two max problems in Eq. 4), each of which is
local with respect to the set of Lagrangian multi-
pliers:
</bodyText>
<equation confidence="0.99015">
⎡ ⎤
X
⎣P(yc|x) + ui(yci) ⎦(4)
iE|x|
⎡ F(yw|x) −Xuj(ywj )
jE|x |I !
</equation>
<bodyText confidence="0.9985513">
This method is called dual decomposition (DD)
(Rush et al., 2010). Similar to previous work
(Rush and Collins, 2012), we solve this DD prob-
lem by iteratively updating the sub-gradient as de-
picted in Algorithm 1.1 In each iteration, if the
best segmentations provided by the two models do
not agree, then the two models will receive penal-
ties for the decisions they made that differ from the
other. This penalty exchange is similar to message
passing, and as the penalty accumulates over itera-
tions, the two models are pushed towards agreeing
with each other. We also give an updated Viterbi
decoding algorithm for CRF and a modified beam-
search algorithm for perceptron in Algorithm 1. T
is the maximum number of iterations before early
stopping, and αt is the learning rate at time t. We
adopt a learning rate update rule from Koo et al.
(2010) where αt is defined as 1N, where N is the
number of times we observed a consecutive dual
value increase from iteration 1 to t.
</bodyText>
<sectionHeader confidence="0.999579" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9999920625">
We conduct experiments on the SIGHAN 2003
(Sproat and Emerson, 2003) and 2005 (Emer-
son, 2005) bake-off datasets to evaluate the ef-
fectiveness of the proposed dual decomposition
algorithm. We use the publicly available Stan-
ford CRF segmenter (Tseng et al., 2005)2 as our
character-based baseline model, and reproduce
the perceptron-based segmenter from Zhang and
Clark (2007) as our word-based baseline model.
We adopted the development setting from
(Zhang and Clark, 2007), and used CTB sections
1-270 for training and sections 400-931 for devel-
opment in hyper-parameter setting; for all results
given in tables, the models are trained and eval-
uated on the standard train/test split for the given
dataset. The optimized hyper-parameters used are:
</bodyText>
<footnote confidence="0.974607333333333">
1See Rush and Collins (2012) for a full introduction to
DD.
2http://nlp.stanford.edu/software/segmenter.shtml
</footnote>
<figure confidence="0.9938106">
min
U
max
yc,yw
min
U
max
yc
+ max
yw
</figure>
<page confidence="0.996656">
195
</page>
<bodyText confidence="0.99989825">
E2 regularization parameter A in CRF is set to
3; the perceptron is trained for 10 iterations with
beam size 200; dual decomposition is run to max
iteration of 100 (T in Algo. 1) with step size 0.1
(αt in Algo. 1).
Beyond standard precision (P), recall (R) and
F1 scores, we also evaluate segmentation consis-
tency as proposed by (Chang et al., 2008), who
have shown that increased segmentation consis-
tency is correlated with better machine transla-
tion performance. The consistency measure cal-
culates the entropy of segmentation variations —
the lower the score the better. We also report
out-of-vocabulary recall (Rook) as an estimation of
the model’s generalizability to previously unseen
words.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999873666666667">
Table 1 shows our empirical results on SIGHAN
2005 dataset. Our dual decomposition method
outperforms both the word-based and character-
based baselines consistently across all four sub-
sets in both F1 and OOV recall (Rook). Our
method demonstrates a robustness across domains
and segmentation standards regardless of which
baseline model was stronger. Of particular note
is DD’s is much more robust in Rook, where the
two baselines swing a lot. This is an important
property for downstream applications such as en-
tity recognition. The DD algorithm is also more
consistent, which would likely lead to improve-
ments in applications such as machine translation
(Chang et al., 2008).
The improvement over our word- and character-
based baselines is also seen in our results on the
earlier SIGHAN 2003 dataset. Table 2 puts our
method in the context of earlier systems for CWS.
Our method achieves the best reported score on 6
out of 7 datasets.
</bodyText>
<sectionHeader confidence="0.99186" genericHeader="evaluation">
5 Discussion and Error Analysis
</sectionHeader>
<bodyText confidence="0.9994787">
On the whole, dual decomposition produces state-
of-the-art segmentations that are more accurate,
more consistent, and more successful at induc-
ing OOV words than the baseline systems that it
combines. On the SIGHAN 2005 test set, in
over 99.1% of cases the DD algorithm converged
within 100 iterations, which gives an optimality
guarantee. In 77.4% of the cases, DD converged
in the first iteration. The number of iterations to
convergence histogram is plotted in Figure 1.
</bodyText>
<note confidence="0.747613">
SIGHAN 2005
</note>
<table confidence="0.99951125">
AS PU CU MSR
Best 05 95.2 95.0 94.3 96.4
Zhang et al. 06 94.7 94.5 94.6 96.4
Z&amp;C 07 94.6 94.5 95.1 97.2
Sun et al. 09 - 95.2 94.6 97.3
Sun 10 95.2 95.2 95.6 96.9
Dual-decomp 95.4 95.3 94.7 97.4
SIGHAN 2003
Best 03 96.1 95.1 94.0
Peng et al. 04 95.6 94.1 92.8
Z&amp;C 07 96.5 94.0 94.6
Dual-decomp 97.1 95.4 94.9
</table>
<tableCaption confidence="0.997187">
Table 2: Performance of dual decomposition in
</tableCaption>
<bodyText confidence="0.956391941176471">
comparison to past published results on SIGHAN
2003 and 2005 datasets. Best reported F1 score
for each dataset is highlighted in bold. Z&amp;C 07
refers to Zhang and Clark (2007). Best 03, 05 are
results of the winning systems for each dataset in
the respective shared tasks.
Error analysis In many cases the relative con-
fidence of each model means that dual decom-
position is capable of using information from
both sources to generate a series of correct
segmentations better than either baseline model
alone. The example below shows a difficult-to-
segment proper name comprised of common char-
acters, which results in undersegmentation by the
character-based CRF and oversegmentation by the
word-based perceptron, but our method achieves
the correct middle ground.
</bodyText>
<equation confidence="0.7876798">
Gloss Tian Yage / ’s / creations
Gold 田雅各 / (7 / IM作
CRF 田雅各(7 / IM作
PCPT 田雅 / 各 / (7 / IM作
DD 田雅各 / (7 / IM作
</equation>
<bodyText confidence="0.961940846153846">
A powerful feature of the dual decomposition
approach is that it can generate correct segmenta-
tion decisions in cases where a voting or product-
of-experts model could not, since joint decod-
ing allows the sharing of information at decod-
ing time. In the following example, both baseline
models miss the contextually clear use of the word
点ib (“sweets / snack food”) and instead attach 点
to the prior word to produce the otherwise com-
mon compound 一点点 (“a little bit”); dual de-
composition allows the model to generate the cor-
rect segmentation.
Gloss Enjoy / a bit of / snack food / , ...
</bodyText>
<equation confidence="0.473681">
Gold 享受 / 一点 / 点ib / ,
CRF 享受 / 一点点 / ib / ,
PCPT享受 / 一点点 / ib / ,
DD 享受 / 一点 / 点ib / ,
</equation>
<page confidence="0.997835">
196
</page>
<figureCaption confidence="0.999806">
Figure 1: No. of iterations till DD convergence.
</figureCaption>
<bodyText confidence="0.9999318">
We found more than 400 such surprisingly ac-
curate instances in our dual decomposition output.
Finally, since dual decomposition is a method of
joint decoding, it is still liable to reproduce errors
made by the constituent systems.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999466333333334">
In this paper we presented an approach to Chinese
word segmentation using dual decomposition for
system combination. We demonstrated that this
method allows for joint decoding of existing CWS
systems that is more accurate and consistent than
either system alone, and further achieves the best
performance reported to date on standard datasets
for the task. Perhaps most importantly, our ap-
proach is straightforward to implement and does
not require retraining of the underlying segmenta-
tion models used. This suggests its potential for
broader applicability in real-world settings than
existing approaches to combining character-based
and word-based models for Chinese word segmen-
tation.
</bodyText>
<sectionHeader confidence="0.996488" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999965875">
We gratefully acknowledge the support of the U.S.
Defense Advanced Research Projects Agency
(DARPA) Broad Operational Language Transla-
tion (BOLT) program through IBM. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of DARPA, or
the US government.
</bodyText>
<sectionHeader confidence="0.996105" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999511423076923">
Galen Andrew. 2006. A hybrid Markov/semi-Markov
conditional random field for sequence segmentation.
In Proceedings of EMNLP.
Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.
2008. Improving word alignment by adjusting chi-
nese word segmentation. In Proceedings of the third
International Joint Conference on Natural Lan-
guage Processing (IJCNLP).
Pichuan Chang, Michel Galley, and Chris Manning.
2008. Optimizing chinese word segmentation for
machine translation performance. In Proceedings of
the ACL Workshop on Statistical Machine Transla-
tion.
Keh-Jiann Chen and Shing-Huan Liu. 1992. Word
identification for mandarin chinese sentences. In
Proceedings of COLING.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of ACL.
Thomas Emerson. 2005. The second international
Chinese word segmentation bakeoff. In Proceed-
ings of the fourth SIGHAN workshop on Chinese
language Processing.
Jianfeng Gao, Mu Li, and Chang-Ning Huang. 2003.
Improved source-channel models for Chinese word
segmentation. In Proceedings ofACL.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of EMNLP.
Jonathan K. Kummerfeld, Daniel Tse, James R. Cur-
ran, and Dan Klein. 2013. An empirical examina-
tion of challenges in chinese parsing. In Proceed-
ings of ACL-Short.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of 18th International
Conference on Machine Learning (ICML).
Dekang Lin. 2009. Combining language modeling and
discriminative classification for word segmentation.
In Proceedings of the 10th International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing).
Alexander M. Rush and Michael Collins. 2012. A tu-
torial on dual decomposition and Lagrangian relax-
ation for inference in natural language processing.
JAIR, 45:305–362.
</reference>
<page confidence="0.979871">
197
</page>
<reference confidence="0.999884916666667">
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proceedings of EMNLP.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of
Joint Conferences on Artificial Intelligence (IJCAI).
Richard Sproat and Thomas Emerson. 2003. The
first international Chinese word segmentation bake-
off. In Proceedings of the second SIGHAN work-
shop on Chinese language Processing.
Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun’ichi Tsujii. 2009. A dis-
criminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of HLT-NAACL.
Weiwei Sun. 2010. Word-based and character-
basedword segmentation models: Comparison and
combination. In Proceedings of COLING.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurasfky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the fourth SIGHAN
workshop on Chinese language Processing.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of the fifth SIGHAN workshop on Chi-
nese language Processing.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A character-based joint model for chinese word seg-
mentation. In Proceedings of COLING.
Mengqiu Wang, Wanxiang Che, and Christopher D.
Manning. 2013. Joint word alignment and bilingual
named entity recognition using dual decomposition.
In Proceedings of ACL.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Compu-
tational Linguistics and Chinese Language Process-
ing, pages 29–48.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings of ACL.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro
Sumita. 2006. Subword-based tagging by condi-
tional random fields for Chinese word segmentation.
In Proceedings of HLT-NAACL.
</reference>
<page confidence="0.997718">
198
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.969465">
<title confidence="0.9973625">Two Knives Cut Better Than One: Chinese Word Segmentation with Dual Decomposition</title>
<author confidence="0.999802">Mengqiu Wang Rob Voigt Christopher D Manning</author>
<affiliation confidence="0.9998555">Computer Science Department Linguistics Department Computer Science Department Stanford University Stanford University Stanford University</affiliation>
<address confidence="0.99973">Stanford, CA 94305 Stanford, CA 94305 Stanford, CA 94305</address>
<email confidence="0.99979">robvoigt@stanford.edu</email>
<abstract confidence="0.998694578947368">There are two dominant approaches to Chinese word segmentation: word-based and character-based models, each with respective strengths. Prior work has shown that gains in segmentation performance can be achieved from combining these two types of models; however, past efforts have not provided a practical technique to allow mainstream adoption. We propose a method that effectively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference. Our method is simple and easy to implement. Experiments on SIGHAN 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
</authors>
<title>A hybrid Markov/semi-Markov conditional random field for sequence segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2231" citStr="Andrew, 2006" startWordPosition="328" endWordPosition="329">l, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary classes of models: character-based, where the foundational units for processing are individual Chinese characters (Xue, 2003; Tseng et al., 2005; Zhang et al., 2006; Wang et al., 2010), and word-based, where the units are full words based on some dictionary or training lexicon (Andrew, 2006; Zhang and Clark, 2007). Sun (2010) details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new OOV words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans. Prior work has shown performance gains from combining these two types of models to exploit their respective strengths, but such approaches are often complex to implement and computationally expensive. In this work, </context>
</contexts>
<marker>Andrew, 2006</marker>
<rawString>Galen Andrew. 2006. A hybrid Markov/semi-Markov conditional random field for sequence segmentation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Hong Bai</author>
<author>Keh-Jiann Chen</author>
<author>Jason S Chang</author>
</authors>
<title>Improving word alignment by adjusting chinese word segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the third International Joint Conference on Natural Language Processing (IJCNLP).</booktitle>
<contexts>
<context position="1342" citStr="Bai et al., 2008" startWordPosition="189" endWordPosition="192">option. We propose a method that effectively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference. Our method is simple and easy to implement. Experiments on SIGHAN 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets. 1 Introduction Chinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing. As demonstrated by (Shi and Wang, 2007; Bai et al., 2008; Chang et al., 2008; Kummerfeld et al., 2013), the quality and consistency of segmentation has important downstream impacts on system performance in machine translation, POS tagging and parsing. State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary cl</context>
</contexts>
<marker>Bai, Chen, Chang, 2008</marker>
<rawString>Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang. 2008. Improving word alignment by adjusting chinese word segmentation. In Proceedings of the third International Joint Conference on Natural Language Processing (IJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pichuan Chang</author>
<author>Michel Galley</author>
<author>Chris Manning</author>
</authors>
<title>Optimizing chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1362" citStr="Chang et al., 2008" startWordPosition="193" endWordPosition="196"> a method that effectively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference. Our method is simple and easy to implement. Experiments on SIGHAN 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets. 1 Introduction Chinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing. As demonstrated by (Shi and Wang, 2007; Bai et al., 2008; Chang et al., 2008; Kummerfeld et al., 2013), the quality and consistency of segmentation has important downstream impacts on system performance in machine translation, POS tagging and parsing. State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary classes of models: cha</context>
<context position="11718" citStr="Chang et al., 2008" startWordPosition="1945" endWordPosition="1948">rained and evaluated on the standard train/test split for the given dataset. The optimized hyper-parameters used are: 1See Rush and Collins (2012) for a full introduction to DD. 2http://nlp.stanford.edu/software/segmenter.shtml min U max yc,yw min U max yc + max yw 195 E2 regularization parameter A in CRF is set to 3; the perceptron is trained for 10 iterations with beam size 200; dual decomposition is run to max iteration of 100 (T in Algo. 1) with step size 0.1 (αt in Algo. 1). Beyond standard precision (P), recall (R) and F1 scores, we also evaluate segmentation consistency as proposed by (Chang et al., 2008), who have shown that increased segmentation consistency is correlated with better machine translation performance. The consistency measure calculates the entropy of segmentation variations — the lower the score the better. We also report out-of-vocabulary recall (Rook) as an estimation of the model’s generalizability to previously unseen words. 4 Results Table 1 shows our empirical results on SIGHAN 2005 dataset. Our dual decomposition method outperforms both the word-based and characterbased baselines consistently across all four subsets in both F1 and OOV recall (Rook). Our method demonstra</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pichuan Chang, Michel Galley, and Chris Manning. 2008. Optimizing chinese word segmentation for machine translation performance. In Proceedings of the ACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Shing-Huan Liu</author>
</authors>
<title>Word identification for mandarin chinese sentences.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="5828" citStr="Chen and Liu, 1992" startWordPosition="920" endWordPosition="923">pically include surrounding character n-gram and morphological suffix/prefix features. These types of features capture the compositional properties of characters and are likely to generalize well to unknown words. However, the Markov assumption in CRF limits the context of such features; it is difficult to capture long-range word features in this model. 2.2 Word-based Models Word-based models search through lists of word candidates using scoring functions that directly assign scores to each. Early word-based segmentation work employed simple heuristics like dictionary-lookup maximum matching (Chen and Liu, 1992). More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). Formally, given input x, their model seeks a segmentation y such that: F(y|x) = max (α · φ(y)) y∈GEN(x) F(y|x) is the score of segmentation result y. Searching through the entire GEN(x) space is intractable even with a local model, so a beamsearch algorithm is used. The search algorithm consumes one character input token at a time, and iterates through the existing beams to score two new alternative hypotheses by either appending the new character to the </context>
</contexts>
<marker>Chen, Liu, 1992</marker>
<rawString>Keh-Jiann Chen and Shing-Huan Liu. 1992. Word identification for mandarin chinese sentences. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5967" citStr="Collins, 2002" startWordPosition="942" endWordPosition="943">ies of characters and are likely to generalize well to unknown words. However, the Markov assumption in CRF limits the context of such features; it is difficult to capture long-range word features in this model. 2.2 Word-based Models Word-based models search through lists of word candidates using scoring functions that directly assign scores to each. Early word-based segmentation work employed simple heuristics like dictionary-lookup maximum matching (Chen and Liu, 1992). More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). Formally, given input x, their model seeks a segmentation y such that: F(y|x) = max (α · φ(y)) y∈GEN(x) F(y|x) is the score of segmentation result y. Searching through the entire GEN(x) space is intractable even with a local model, so a beamsearch algorithm is used. The search algorithm consumes one character input token at a time, and iterates through the existing beams to score two new alternative hypotheses by either appending the new character to the last word in the beam, or starting a new word at the current position. ui(k) end for Beam-Search: for i = 1 to |x |do for item v = {w0, · ·</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Klaus Macherey</author>
</authors>
<title>Modelbased aligner combination using dual decomposition.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7570" citStr="DeNero and Macherey, 2011" startWordPosition="1220" endWordPosition="1223">but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al., 2010), bilingual sequence tagging (Wang et al., 2013) and word alignment (DeNero and Macherey, 2011). The idea is that jointly modelling both character-sequence and word information can be computationally challenging, so instead we can try to find outputs that the two models are most likely 194 Table 1: Results on SIGHAN 2005 datasets. Roov denotes OOV recall, and C denotes segmentation consistency. Best number in each column is highlighted in bold. R Char-based CRF 95.2 Word-based Perceptron 95.8 Dual-decomp 95.9 R Char-based CRF 94.7 Word-based Perceptron 94.3 Dual-decomp 95.0 Academia Sinica P F1 Rooms C 0.064 0.060 0.055 City Univ. of Hong Kong P F1 Rooms C 0.065 0.073 0.062 Peking Univ.</context>
</contexts>
<marker>DeNero, Macherey, 2011</marker>
<rawString>John DeNero and Klaus Macherey. 2011. Modelbased aligner combination using dual decomposition. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international Chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the fourth SIGHAN workshop on Chinese language Processing.</booktitle>
<contexts>
<context position="10563" citStr="Emerson, 2005" startWordPosition="1759" endWordPosition="1761">alty accumulates over iterations, the two models are pushed towards agreeing with each other. We also give an updated Viterbi decoding algorithm for CRF and a modified beamsearch algorithm for perceptron in Algorithm 1. T is the maximum number of iterations before early stopping, and αt is the learning rate at time t. We adopt a learning rate update rule from Koo et al. (2010) where αt is defined as 1N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. 3 Experiments We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. We use the publicly available Stanford CRF segmenter (Tseng et al., 2005)2 as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the gi</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international Chinese word segmentation bakeoff. In Proceedings of the fourth SIGHAN workshop on Chinese language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Improved source-channel models for Chinese word segmentation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1916" citStr="Gao et al., 2003" startWordPosition="277" endWordPosition="280">ed by (Shi and Wang, 2007; Bai et al., 2008; Chang et al., 2008; Kummerfeld et al., 2013), the quality and consistency of segmentation has important downstream impacts on system performance in machine translation, POS tagging and parsing. State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary classes of models: character-based, where the foundational units for processing are individual Chinese characters (Xue, 2003; Tseng et al., 2005; Zhang et al., 2006; Wang et al., 2010), and word-based, where the units are full words based on some dictionary or training lexicon (Andrew, 2006; Zhang and Clark, 2007). Sun (2010) details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new OOV words; word-based approaches are better at reproducing the wo</context>
</contexts>
<marker>Gao, Li, Huang, 2003</marker>
<rawString>Jianfeng Gao, Mu Li, and Chang-Ning Huang. 2003. Improved source-channel models for Chinese word segmentation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7475" citStr="Koo et al., 2010" startWordPosition="1206" endWordPosition="1209">Sun, 2010; Wang et al., 2010). These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al., 2010), bilingual sequence tagging (Wang et al., 2013) and word alignment (DeNero and Macherey, 2011). The idea is that jointly modelling both character-sequence and word information can be computationally challenging, so instead we can try to find outputs that the two models are most likely 194 Table 1: Results on SIGHAN 2005 datasets. Roov denotes OOV recall, and C denotes segmentation consistency. Best number in each column is highlighted in bold. R Char-based CRF 95.2 Word-based Perceptron 95.8 Dual-decomp 95.9 R Char-based CRF 94.7 Word-based Perceptron 94.3 Dual-decomp 95.0 Academia Sinica P F</context>
<context position="10328" citStr="Koo et al. (2010)" startWordPosition="1715" endWordPosition="1718">ion, if the best segmentations provided by the two models do not agree, then the two models will receive penalties for the decisions they made that differ from the other. This penalty exchange is similar to message passing, and as the penalty accumulates over iterations, the two models are pushed towards agreeing with each other. We also give an updated Viterbi decoding algorithm for CRF and a modified beamsearch algorithm for perceptron in Algorithm 1. T is the maximum number of iterations before early stopping, and αt is the learning rate at time t. We adopt a learning rate update rule from Koo et al. (2010) where αt is defined as 1N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. 3 Experiments We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. We use the publicly available Stanford CRF segmenter (Tseng et al., 2005)2 as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. We adopted the development setting from (Zhang an</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>Daniel Tse</author>
<author>James R Curran</author>
<author>Dan Klein</author>
</authors>
<title>An empirical examination of challenges in chinese parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL-Short.</booktitle>
<contexts>
<context position="1388" citStr="Kummerfeld et al., 2013" startWordPosition="197" endWordPosition="200">tively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference. Our method is simple and easy to implement. Experiments on SIGHAN 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets. 1 Introduction Chinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing. As demonstrated by (Shi and Wang, 2007; Bai et al., 2008; Chang et al., 2008; Kummerfeld et al., 2013), the quality and consistency of segmentation has important downstream impacts on system performance in machine translation, POS tagging and parsing. State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary classes of models: character-based, where the fo</context>
</contexts>
<marker>Kummerfeld, Tse, Curran, Klein, 2013</marker>
<rawString>Jonathan K. Kummerfeld, Daniel Tse, James R. Curran, and Dan Klein. 2013. An empirical examination of challenges in chinese parsing. In Proceedings of ACL-Short.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of 18th International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="4391" citStr="Lafferty et al., 2001" startWordPosition="659" endWordPosition="662">and describe our algorithm for joint decoding with dual decomposition. 193 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 193–198, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2.1 Character-based Models In the most commonly used contemporary approach to character-based segmentation, first proposed by (Xue, 2003), CWS is seen as a character sequence tagging task, where each character is tagged on whether it is at the beginning, middle, or end of a word. Conditional random fields (CRF) (Lafferty et al., 2001) have been widely adopted for this task, and give state-of-the-art results (Tseng et al., 2005). In a first-order linearchain CRF model, the conditional probability of a label sequence y given a word sequence x is defined as: Algorithm 1 Dual decomposition inference algorithm, and modified Viterbi and beam-search algorithms. ∀i ∈ {1 to |x|}: ∀k ∈ {0, 1}: ui(k) = 0 fort ← 1toTdo yc* = argmax P(yc|x) + E y iE|x| yw* = argmax F(yw|x) − E uj(ywj ) yEGEN(x) jE|x| if yc* = yw* then return (yc*, yw*) end if for all i ∈ {1 to |x|} do ∀k ∈ {0, 1} : ui(k) = ui(k) + αt(2k − 1)(yw* i − yi* ) end for end f</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of 18th International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Combining language modeling and discriminative classification for word segmentation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing).</booktitle>
<contexts>
<context position="6838" citStr="Lin, 2009" startWordPosition="1103" endWordPosition="1104"> used. The search algorithm consumes one character input token at a time, and iterates through the existing beams to score two new alternative hypotheses by either appending the new character to the last word in the beam, or starting a new word at the current position. ui(k) end for Beam-Search: for i = 1 to |x |do for item v = {w0, · · · ,wj} in beam(i) do append xi to wj, score(v) += ui(0) v = {w0, · · · ,wj, xi}, score(v) += ui(1) end for end for 2.3 Combining Models with Dual Decomposition Various mixing approaches have been proposed to combine the above two approaches (Wang et al., 2006; Lin, 2009; Sun et al., 2009; Sun, 2010; Wang et al., 2010). These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for example, in </context>
</contexts>
<marker>Lin, 2009</marker>
<rawString>Dekang Lin. 2009. Combining language modeling and discriminative classification for word segmentation. In Proceedings of the 10th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing.</title>
<date>2012</date>
<journal>JAIR,</journal>
<pages>45--305</pages>
<contexts>
<context position="9600" citStr="Rush and Collins, 2012" startWordPosition="1587" endWordPosition="1590">onsists of a multiplier ui at each word position i. We can rewrite the original objective with the Lagrangian relaxation as: L (yc, yw, U) (3) We can then form the dual of this problem by taking the min outside of the max, which is an upper bound on the original problem. The dual form can then be decomposed into two sub-components (the two max problems in Eq. 4), each of which is local with respect to the set of Lagrangian multipliers: ⎡ ⎤ X ⎣P(yc|x) + ui(yci) ⎦(4) iE|x| ⎡ F(yw|x) −Xuj(ywj ) jE|x |I ! This method is called dual decomposition (DD) (Rush et al., 2010). Similar to previous work (Rush and Collins, 2012), we solve this DD problem by iteratively updating the sub-gradient as depicted in Algorithm 1.1 In each iteration, if the best segmentations provided by the two models do not agree, then the two models will receive penalties for the decisions they made that differ from the other. This penalty exchange is similar to message passing, and as the penalty accumulates over iterations, the two models are pushed towards agreeing with each other. We also give an updated Viterbi decoding algorithm for CRF and a modified beamsearch algorithm for perceptron in Algorithm 1. T is the maximum number of iter</context>
<context position="11245" citStr="Rush and Collins (2012)" startWordPosition="1862" endWordPosition="1865">posed dual decomposition algorithm. We use the publicly available Stanford CRF segmenter (Tseng et al., 2005)2 as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. The optimized hyper-parameters used are: 1See Rush and Collins (2012) for a full introduction to DD. 2http://nlp.stanford.edu/software/segmenter.shtml min U max yc,yw min U max yc + max yw 195 E2 regularization parameter A in CRF is set to 3; the perceptron is trained for 10 iterations with beam size 200; dual decomposition is run to max iteration of 100 (T in Algo. 1) with step size 0.1 (αt in Algo. 1). Beyond standard precision (P), recall (R) and F1 scores, we also evaluate segmentation consistency as proposed by (Chang et al., 2008), who have shown that increased segmentation consistency is correlated with better machine translation performance. The consist</context>
</contexts>
<marker>Rush, Collins, 2012</marker>
<rawString>Alexander M. Rush and Michael Collins. 2012. A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing. JAIR, 45:305–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7087" citStr="Rush et al., 2010" startWordPosition="1143" endWordPosition="1146">word at the current position. ui(k) end for Beam-Search: for i = 1 to |x |do for item v = {w0, · · · ,wj} in beam(i) do append xi to wj, score(v) += ui(0) v = {w0, · · · ,wj, xi}, score(v) += ui(1) end for end for 2.3 Combining Models with Dual Decomposition Various mixing approaches have been proposed to combine the above two approaches (Wang et al., 2006; Lin, 2009; Sun et al., 2009; Sun, 2010; Wang et al., 2010). These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al., 2010), bilingual sequence tagging (Wang et al., 2013) and word alignment (DeNero and Macherey, 2011). The idea is that jointly modelling both character-sequence and word information can be computationally challenging,</context>
<context position="9549" citStr="Rush et al., 2010" startWordPosition="1579" endWordPosition="1582"> U is the set of Lagrangian multipliers that consists of a multiplier ui at each word position i. We can rewrite the original objective with the Lagrangian relaxation as: L (yc, yw, U) (3) We can then form the dual of this problem by taking the min outside of the max, which is an upper bound on the original problem. The dual form can then be decomposed into two sub-components (the two max problems in Eq. 4), each of which is local with respect to the set of Lagrangian multipliers: ⎡ ⎤ X ⎣P(yc|x) + ui(yci) ⎦(4) iE|x| ⎡ F(yw|x) −Xuj(ywj ) jE|x |I ! This method is called dual decomposition (DD) (Rush et al., 2010). Similar to previous work (Rush and Collins, 2012), we solve this DD problem by iteratively updating the sub-gradient as depicted in Algorithm 1.1 In each iteration, if the best segmentations provided by the two models do not agree, then the two models will receive penalties for the decisions they made that differ from the other. This penalty exchange is similar to message passing, and as the penalty accumulates over iterations, the two models are pushed towards agreeing with each other. We also give an updated Viterbi decoding algorithm for CRF and a modified beamsearch algorithm for percept</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanxin Shi</author>
<author>Mengqiu Wang</author>
</authors>
<title>A dual-layer crfs based joint decoding method for cascaded segmentation and labeling tasks.</title>
<date>2007</date>
<booktitle>In Proceedings of Joint Conferences on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="1324" citStr="Shi and Wang, 2007" startWordPosition="185" endWordPosition="188"> allow mainstream adoption. We propose a method that effectively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference. Our method is simple and easy to implement. Experiments on SIGHAN 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets. 1 Introduction Chinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing. As demonstrated by (Shi and Wang, 2007; Bai et al., 2008; Chang et al., 2008; Kummerfeld et al., 2013), the quality and consistency of segmentation has important downstream impacts on system performance in machine translation, POS tagging and parsing. State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There </context>
</contexts>
<marker>Shi, Wang, 2007</marker>
<rawString>Yanxin Shi and Mengqiu Wang. 2007. A dual-layer crfs based joint decoding method for cascaded segmentation and labeling tasks. In Proceedings of Joint Conferences on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<title>The first international Chinese word segmentation bakeoff.</title>
<date>2003</date>
<booktitle>In Proceedings of the second SIGHAN workshop on Chinese language Processing.</booktitle>
<contexts>
<context position="10538" citStr="Sproat and Emerson, 2003" startWordPosition="1753" endWordPosition="1756">r to message passing, and as the penalty accumulates over iterations, the two models are pushed towards agreeing with each other. We also give an updated Viterbi decoding algorithm for CRF and a modified beamsearch algorithm for perceptron in Algorithm 1. T is the maximum number of iterations before early stopping, and αt is the learning rate at time t. We adopt a learning rate update rule from Koo et al. (2010) where αt is defined as 1N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. 3 Experiments We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. We use the publicly available Stanford CRF segmenter (Tseng et al., 2005)2 as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard tr</context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson. 2003. The first international Chinese word segmentation bakeoff. In Proceedings of the second SIGHAN workshop on Chinese language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Yaozhong Zhang</author>
<author>Takuya Matsuzaki</author>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A discriminative latent variable chinese segmenter with hybrid word/character information.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="6856" citStr="Sun et al., 2009" startWordPosition="1105" endWordPosition="1108">search algorithm consumes one character input token at a time, and iterates through the existing beams to score two new alternative hypotheses by either appending the new character to the last word in the beam, or starting a new word at the current position. ui(k) end for Beam-Search: for i = 1 to |x |do for item v = {w0, · · · ,wj} in beam(i) do append xi to wj, score(v) += ui(0) v = {w0, · · · ,wj, xi}, score(v) += ui(1) end for end for 2.3 Combining Models with Dual Decomposition Various mixing approaches have been proposed to combine the above two approaches (Wang et al., 2006; Lin, 2009; Sun et al., 2009; Sun, 2010; Wang et al., 2010). These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing</context>
</contexts>
<marker>Sun, Zhang, Matsuzaki, Tsuruoka, Tsujii, 2009</marker>
<rawString>Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshimasa Tsuruoka, and Jun’ichi Tsujii. 2009. A discriminative latent variable chinese segmenter with hybrid word/character information. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>Word-based and characterbasedword segmentation models: Comparison and combination.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="2267" citStr="Sun (2010)" startWordPosition="334" endWordPosition="335">lso known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary classes of models: character-based, where the foundational units for processing are individual Chinese characters (Xue, 2003; Tseng et al., 2005; Zhang et al., 2006; Wang et al., 2010), and word-based, where the units are full words based on some dictionary or training lexicon (Andrew, 2006; Zhang and Clark, 2007). Sun (2010) details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new OOV words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans. Prior work has shown performance gains from combining these two types of models to exploit their respective strengths, but such approaches are often complex to implement and computationally expensive. In this work, we propose a simple and principled j</context>
<context position="6867" citStr="Sun, 2010" startWordPosition="1109" endWordPosition="1110">onsumes one character input token at a time, and iterates through the existing beams to score two new alternative hypotheses by either appending the new character to the last word in the beam, or starting a new word at the current position. ui(k) end for Beam-Search: for i = 1 to |x |do for item v = {w0, · · · ,wj} in beam(i) do append xi to wj, score(v) += ui(0) v = {w0, · · · ,wj, xi}, score(v) += ui(1) end for end for 2.3 Combining Models with Dual Decomposition Various mixing approaches have been proposed to combine the above two approaches (Wang et al., 2006; Lin, 2009; Sun et al., 2009; Sun, 2010; Wang et al., 2010). These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al</context>
</contexts>
<marker>Sun, 2010</marker>
<rawString>Weiwei Sun. 2010. Word-based and characterbasedword segmentation models: Comparison and combination. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurasfky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for sighan bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of the fourth SIGHAN workshop on Chinese language Processing.</booktitle>
<contexts>
<context position="2084" citStr="Tseng et al., 2005" startWordPosition="301" endWordPosition="304"> on system performance in machine translation, POS tagging and parsing. State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary classes of models: character-based, where the foundational units for processing are individual Chinese characters (Xue, 2003; Tseng et al., 2005; Zhang et al., 2006; Wang et al., 2010), and word-based, where the units are full words based on some dictionary or training lexicon (Andrew, 2006; Zhang and Clark, 2007). Sun (2010) details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new OOV words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans. Prior work has shown performance gains from combining these two type</context>
<context position="4486" citStr="Tseng et al., 2005" startWordPosition="675" endWordPosition="678"> Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 193–198, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2.1 Character-based Models In the most commonly used contemporary approach to character-based segmentation, first proposed by (Xue, 2003), CWS is seen as a character sequence tagging task, where each character is tagged on whether it is at the beginning, middle, or end of a word. Conditional random fields (CRF) (Lafferty et al., 2001) have been widely adopted for this task, and give state-of-the-art results (Tseng et al., 2005). In a first-order linearchain CRF model, the conditional probability of a label sequence y given a word sequence x is defined as: Algorithm 1 Dual decomposition inference algorithm, and modified Viterbi and beam-search algorithms. ∀i ∈ {1 to |x|}: ∀k ∈ {0, 1}: ui(k) = 0 fort ← 1toTdo yc* = argmax P(yc|x) + E y iE|x| yw* = argmax F(yw|x) − E uj(ywj ) yEGEN(x) jE|x| if yc* = yw* then return (yc*, yw*) end if for all i ∈ {1 to |x|} do ∀k ∈ {0, 1} : ui(k) = ui(k) + αt(2k − 1)(yw* i − yi* ) end for end for return (yc*, yw*) ui(yci) 1 |y |exp (θ · f(x, yt, yt+1)) Viterbi: P(y|x) = ZL V1(1) = 1, V1(</context>
<context position="10731" citStr="Tseng et al., 2005" startWordPosition="1784" endWordPosition="1787">fied beamsearch algorithm for perceptron in Algorithm 1. T is the maximum number of iterations before early stopping, and αt is the learning rate at time t. We adopt a learning rate update rule from Koo et al. (2010) where αt is defined as 1N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. 3 Experiments We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. We use the publicly available Stanford CRF segmenter (Tseng et al., 2005)2 as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. The optimized hyper-parameters used are: 1See Rush and Collins (2012) for a full introduction to DD. 2http://nlp.stanford.edu/software/segmenter.shtml min </context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurasfky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurasfky, and Christopher Manning. 2005. A conditional random field word segmenter for sighan bakeoff 2005. In Proceedings of the fourth SIGHAN workshop on Chinese language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinhao Wang</author>
<author>Xiaojun Lin</author>
<author>Dianhai Yu</author>
<author>Hao Tian</author>
<author>Xihong Wu</author>
</authors>
<title>Chinese word segmentation with maximum entropy and n-gram language model.</title>
<date>2006</date>
<booktitle>In Proceedings of the fifth SIGHAN workshop on Chinese language Processing.</booktitle>
<contexts>
<context position="6827" citStr="Wang et al., 2006" startWordPosition="1099" endWordPosition="1102">search algorithm is used. The search algorithm consumes one character input token at a time, and iterates through the existing beams to score two new alternative hypotheses by either appending the new character to the last word in the beam, or starting a new word at the current position. ui(k) end for Beam-Search: for i = 1 to |x |do for item v = {w0, · · · ,wj} in beam(i) do append xi to wj, score(v) += ui(0) v = {w0, · · · ,wj, xi}, score(v) += ui(1) end for end for 2.3 Combining Models with Dual Decomposition Various mixing approaches have been proposed to combine the above two approaches (Wang et al., 2006; Lin, 2009; Sun et al., 2009; Sun, 2010; Wang et al., 2010). These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for e</context>
</contexts>
<marker>Wang, Lin, Yu, Tian, Wu, 2006</marker>
<rawString>Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and Xihong Wu. 2006. Chinese word segmentation with maximum entropy and n-gram language model. In Proceedings of the fifth SIGHAN workshop on Chinese language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Wang</author>
<author>Chengqing Zong</author>
<author>Keh-Yih Su</author>
</authors>
<title>A character-based joint model for chinese word segmentation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="2124" citStr="Wang et al., 2010" startWordPosition="309" endWordPosition="312">ation, POS tagging and parsing. State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary classes of models: character-based, where the foundational units for processing are individual Chinese characters (Xue, 2003; Tseng et al., 2005; Zhang et al., 2006; Wang et al., 2010), and word-based, where the units are full words based on some dictionary or training lexicon (Andrew, 2006; Zhang and Clark, 2007). Sun (2010) details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new OOV words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans. Prior work has shown performance gains from combining these two types of models to exploit their respective </context>
<context position="6887" citStr="Wang et al., 2010" startWordPosition="1111" endWordPosition="1114"> character input token at a time, and iterates through the existing beams to score two new alternative hypotheses by either appending the new character to the last word in the beam, or starting a new word at the current position. ui(k) end for Beam-Search: for i = 1 to |x |do for item v = {w0, · · · ,wj} in beam(i) do append xi to wj, score(v) += ui(0) v = {w0, · · · ,wj, xi}, score(v) += ui(1) end for end for 2.3 Combining Models with Dual Decomposition Various mixing approaches have been proposed to combine the above two approaches (Wang et al., 2006; Lin, 2009; Sun et al., 2009; Sun, 2010; Wang et al., 2010). These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al., 2010), bilingual </context>
</contexts>
<marker>Wang, Zong, Su, 2010</marker>
<rawString>Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010. A character-based joint model for chinese word segmentation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Wanxiang Che</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint word alignment and bilingual named entity recognition using dual decomposition.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7523" citStr="Wang et al., 2013" startWordPosition="1213" endWordPosition="1216">els perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al., 2010), bilingual sequence tagging (Wang et al., 2013) and word alignment (DeNero and Macherey, 2011). The idea is that jointly modelling both character-sequence and word information can be computationally challenging, so instead we can try to find outputs that the two models are most likely 194 Table 1: Results on SIGHAN 2005 datasets. Roov denotes OOV recall, and C denotes segmentation consistency. Best number in each column is highlighted in bold. R Char-based CRF 95.2 Word-based Perceptron 95.8 Dual-decomp 95.9 R Char-based CRF 94.7 Word-based Perceptron 94.3 Dual-decomp 95.0 Academia Sinica P F1 Rooms C 0.064 0.060 0.055 City Univ. of Hong K</context>
</contexts>
<marker>Wang, Che, Manning, 2013</marker>
<rawString>Mengqiu Wang, Wanxiang Che, and Christopher D. Manning. 2013. Joint word alignment and bilingual named entity recognition using dual decomposition. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>International Journal of Computational Linguistics and Chinese Language Processing,</booktitle>
<pages>29--48</pages>
<contexts>
<context position="2064" citStr="Xue, 2003" startWordPosition="299" endWordPosition="300">eam impacts on system performance in machine translation, POS tagging and parsing. State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary classes of models: character-based, where the foundational units for processing are individual Chinese characters (Xue, 2003; Tseng et al., 2005; Zhang et al., 2006; Wang et al., 2010), and word-based, where the units are full words based on some dictionary or training lexicon (Andrew, 2006; Zhang and Clark, 2007). Sun (2010) details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new OOV words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans. Prior work has shown performance gains from comb</context>
<context position="4192" citStr="Xue, 2003" startWordPosition="624" endWordPosition="625">eported results to date on 6 out of 7 datasets. 2 Models for CWS Here we describe the character-based and wordbased models we use as baselines, review existing approaches to combination, and describe our algorithm for joint decoding with dual decomposition. 193 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 193–198, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2.1 Character-based Models In the most commonly used contemporary approach to character-based segmentation, first proposed by (Xue, 2003), CWS is seen as a character sequence tagging task, where each character is tagged on whether it is at the beginning, middle, or end of a word. Conditional random fields (CRF) (Lafferty et al., 2001) have been widely adopted for this task, and give state-of-the-art results (Tseng et al., 2005). In a first-order linearchain CRF model, the conditional probability of a label sequence y given a word sequence x is defined as: Algorithm 1 Dual decomposition inference algorithm, and modified Viterbi and beam-search algorithms. ∀i ∈ {1 to |x|}: ∀k ∈ {0, 1}: ui(k) = 0 fort ← 1toTdo yc* = argmax P(yc|x)</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. International Journal of Computational Linguistics and Chinese Language Processing, pages 29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2255" citStr="Zhang and Clark, 2007" startWordPosition="330" endWordPosition="333">remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary classes of models: character-based, where the foundational units for processing are individual Chinese characters (Xue, 2003; Tseng et al., 2005; Zhang et al., 2006; Wang et al., 2010), and word-based, where the units are full words based on some dictionary or training lexicon (Andrew, 2006; Zhang and Clark, 2007). Sun (2010) details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new OOV words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans. Prior work has shown performance gains from combining these two types of models to exploit their respective strengths, but such approaches are often complex to implement and computationally expensive. In this work, we propose a simple and </context>
<context position="5867" citStr="Zhang and Clark (2007)" startWordPosition="926" endWordPosition="929">r n-gram and morphological suffix/prefix features. These types of features capture the compositional properties of characters and are likely to generalize well to unknown words. However, the Markov assumption in CRF limits the context of such features; it is difficult to capture long-range word features in this model. 2.2 Word-based Models Word-based models search through lists of word candidates using scoring functions that directly assign scores to each. Early word-based segmentation work employed simple heuristics like dictionary-lookup maximum matching (Chen and Liu, 1992). More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). Formally, given input x, their model seeks a segmentation y such that: F(y|x) = max (α · φ(y)) y∈GEN(x) F(y|x) is the score of segmentation result y. Searching through the entire GEN(x) space is intractable even with a local model, so a beamsearch algorithm is used. The search algorithm consumes one character input token at a time, and iterates through the existing beams to score two new alternative hypotheses by either appending the new character to the last word in the beam, or starting a ne</context>
<context position="10844" citStr="Zhang and Clark (2007)" startWordPosition="1799" endWordPosition="1802">opping, and αt is the learning rate at time t. We adopt a learning rate update rule from Koo et al. (2010) where αt is defined as 1N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. 3 Experiments We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. We use the publicly available Stanford CRF segmenter (Tseng et al., 2005)2 as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. The optimized hyper-parameters used are: 1See Rush and Collins (2012) for a full introduction to DD. 2http://nlp.stanford.edu/software/segmenter.shtml min U max yc,yw min U max yc + max yw 195 E2 regularization parameter A in CRF is set to 3; the perceptron is trained</context>
<context position="14056" citStr="Zhang and Clark (2007)" startWordPosition="2337" endWordPosition="2340">ration. The number of iterations to convergence histogram is plotted in Figure 1. SIGHAN 2005 AS PU CU MSR Best 05 95.2 95.0 94.3 96.4 Zhang et al. 06 94.7 94.5 94.6 96.4 Z&amp;C 07 94.6 94.5 95.1 97.2 Sun et al. 09 - 95.2 94.6 97.3 Sun 10 95.2 95.2 95.6 96.9 Dual-decomp 95.4 95.3 94.7 97.4 SIGHAN 2003 Best 03 96.1 95.1 94.0 Peng et al. 04 95.6 94.1 92.8 Z&amp;C 07 96.5 94.0 94.6 Dual-decomp 97.1 95.4 94.9 Table 2: Performance of dual decomposition in comparison to past published results on SIGHAN 2003 and 2005 datasets. Best reported F1 score for each dataset is highlighted in bold. Z&amp;C 07 refers to Zhang and Clark (2007). Best 03, 05 are results of the winning systems for each dataset in the respective shared tasks. Error analysis In many cases the relative confidence of each model means that dual decomposition is capable of using information from both sources to generate a series of correct segmentations better than either baseline model alone. The example below shows a difficult-tosegment proper name comprised of common characters, which results in undersegmentation by the character-based CRF and oversegmentation by the word-based perceptron, but our method achieves the correct middle ground. Gloss Tian Yag</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Genichiro Kikui</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Subword-based tagging by conditional random fields for Chinese word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="2104" citStr="Zhang et al., 2006" startWordPosition="305" endWordPosition="308">ce in machine translation, POS tagging and parsing. State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary classes of models: character-based, where the foundational units for processing are individual Chinese characters (Xue, 2003; Tseng et al., 2005; Zhang et al., 2006; Wang et al., 2010), and word-based, where the units are full words based on some dictionary or training lexicon (Andrew, 2006; Zhang and Clark, 2007). Sun (2010) details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new OOV words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans. Prior work has shown performance gains from combining these two types of models to explo</context>
</contexts>
<marker>Zhang, Kikui, Sumita, 2006</marker>
<rawString>Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. 2006. Subword-based tagging by conditional random fields for Chinese word segmentation. In Proceedings of HLT-NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>