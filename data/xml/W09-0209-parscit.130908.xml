<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.988525">
SVD Feature Selection for Probabilistic Taxonomy Learning
</title>
<author confidence="0.990219">
Fallucchi Francesca
</author>
<affiliation confidence="0.7596575">
Disp, University “Tor Vergata”
Rome, Italy
</affiliation>
<email confidence="0.96354">
fallucchi@info.uniroma2.it
</email>
<author confidence="0.984463">
Fabio Massimo Zanzotto
</author>
<affiliation confidence="0.7539225">
Disp, University “Tor Vergata”
Rome, Italy
</affiliation>
<email confidence="0.980612">
zanzotto@info.uniroma2.it
</email>
<sectionHeader confidence="0.9934" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999897555555556">
In this paper, we propose a novel way
to include unsupervised feature selection
methods in probabilistic taxonomy learn-
ing models. We leverage on the computa-
tion of logistic regression to exploit unsu-
pervised feature selection of singular value
decomposition (SVD). Experiments show
that this way of using SVD for feature se-
lection positively affects performances.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999699142857143">
Taxonomies are extremely important knowledge
repositories in a variety of applications for nat-
ural language processing and knowledge repre-
sentation. Yet, manually built taxonomies such
as WordNet (Miller, 1995) often lack in cover-
age when used in specific knowledge domains.
Automatically creating or extending taxonomies
for specific domains is then a very interesting
area of research (O’Sullivan et al., 1995; Magnini
and Speranza, 2001; Snow et al., 2006). Auto-
matic methods for learning taxonomies from cor-
pora often use distributional hypothesis (Harris,
1964) and exploit some induced lexical-syntactic
patterns (Hearst, 1992; Pantel and Pennacchiotti,
2006). In these models, within a very large set,
candidate word pairs are selected as new word
pairs in hyperonymy and added to an existing tax-
onomy. Candidate pairs are represented in some
feature space. Often, these feature spaces are
huge and, then, models may take into considera-
tion noisy features.
In machine learning, feature selection has been
often used to reduce the dimensions in huge fea-
ture spaces. This has many advantages, e.g., re-
ducing the computational cost and improving per-
formances by removing noisy features (Guyon and
Elisseeff, 2003).
In this paper, we propose a novel way to in-
</bodyText>
<subsubsectionHeader confidence="0.891519">
Proceedins the EAC 2009 Worksho on EMS: GEo
</subsubsectionHeader>
<bodyText confidence="0.753858">
clud ` depe
</bodyText>
<sectionHeader confidence="0.985753" genericHeader="introduction">
9 I
</sectionHeader>
<bodyText confidence="0.999956833333334">
probabilistic taxonomy learning models. Given
the probabilistic taxonomy learning model intro-
duced by (Snow et al., 2006), we leverage on the
computation of logistic regression to exploit sin-
gular value decomposition (SVD) as unsupervised
feature selection. SVD is used to compute the
pseudo-inverse matrix needed in logistic regres-
sion.
To describe our idea, we firstly review how
SVD can be used as unsupervised feature selec-
tion (Sec. 2). In Section 3 we then describe the
probabilistic taxonomy learning model introduced
by (Snow et al., 2006). We will then shortly re-
view the logistic regression used to compute the
taxonomy learning model to describe where SVD
can be naturally used. We will describe our ex-
periments in Sec. 4. Finally, we will draw some
conclusions and describe our future work (Sec. 5).
</bodyText>
<sectionHeader confidence="0.819386" genericHeader="method">
2 Unsupervised feature selection with
Singular Value Decomposition
</sectionHeader>
<bodyText confidence="0.999679125">
Singular value decomposition (SVD) is one of the
possible factorization of a rectangular matrix that
has been largely used in information retrieval for
reducing the dimension of the document vector
space (Deerwester et al., 1990).
The decomposition can be defined as follows.
Given a generic rectangular n x m matrix A, its
singular value decomposition is:
</bodyText>
<equation confidence="0.636371">
A = UEV T
</equation>
<bodyText confidence="0.871535727272727">
where U is a matrix n x r, VT is a r x m and E
is a diagonal matrix r x r. The two matrices U
and V are unitary, i.e., UT U = I and V T V = I.
The diagonal elements of the E are the singular
values such as 61 &gt; 62 &gt; ... &gt; 6r &gt; 0 where r is
the rank of the matrix A. For the decomposition,
SVD exploits the linear combination of rows and
columns of A.
A first trivial way of using SVD as unsupervised
g •
c l Modelo atural n e Semantidv a 7
</bodyText>
<page confidence="0.886335">
3
</page>
<footnote confidence="0.750775">
41E����11�0a4��i s
</footnote>
<page confidence="0.963703">
66
</page>
<bodyText confidence="0.999935">
of training examples represented in a feature space
of n features, we can observe it as a matrix, i.e.
a sequence of examples E = (i...e). With
SVD, the n x m matrix E can be factorized as
E = UEVT . This factorization implies we can
focus the learning problem on a new space using
the transformation provided by the matrix U. This
new space is represented by the matrix:
</bodyText>
<equation confidence="0.901579">
E&apos;= UTE = EVT (1)
</equation>
<bodyText confidence="0.999605333333333">
where each example is represented with r new fea-
tures. Each new feature is obtained as a linear
combination of the original features, i.e. each fea-
ture vector ei can be seen as a new feature vector
ei 0 = UT ei . When the target feature space is big
whereas the cardinality of the training set is small,
i.e., n &gt;&gt; m, the application of SVD results in a
reduction of the original feature space as the rank
r of the matrix E is r &lt; min(n, m).
A more interesting way of using SVD as unsu-
pervised feature selection model is to exploit its
approximated computations, i.e. :
</bodyText>
<equation confidence="0.569643">
A P Ak = UmxkEkxkVkTxn
</equation>
<bodyText confidence="0.999002714285714">
where k is smaller than the rank r. The compu-
tation algorithm (Golub and Kahan, 1965) is al-
lowed to stop at a given k different from the real
rank r. The property of the singular values, i.e.,
61 &gt; 62 &gt; ... &gt; 6r &gt; 0, guarantees that the
first k are bigger than the discarded ones. There
is a direct relation between the informativeness of
the dimension and the value of the singular value.
High singular values correspond to dimensions of
the new space where examples have more vari-
ability whereas low singular values determine di-
mensions where examples have a smaller variabil-
ity (see (Liu, 2007)). These dimensions can not
be used as discriminative features in learning al-
gorithms. The possibility of computing the ap-
proximated version of the matrix gives a power-
ful method for feature selection and filtering as
we can decide in advance how many features or,
better, linear combination of original features we
want to use.
As feature selection model, SVD is unsuper-
vised in the sense that the feature selection is done
without taking into account the final classes of the
training examples. This is not always the case,
feature selection models such as those based on
Information Gain largely use the final classes of
training examples. SVD as feature selection is in-
dependent from the classification problem.
</bodyText>
<sectionHeader confidence="0.944086" genericHeader="method">
3 Probabilistic Taxonomy Learning and
SVD feature selection
</sectionHeader>
<bodyText confidence="0.999209947368421">
Recently, Snow et al. (2006) introduced a prob-
abilistic model for learning taxonomies form cor-
pora. This probabilistic formulation exploits the
two well known hypotheses: the distributional hy-
pothesis (Harris, 1964) and the exploitation of
the lexico-syntactic patterns as in (Robison, 1970;
Hearst, 1992). Yet, in this formulation, we can
positively and naturally introduce our use of SVD
as feature selection model.
In the rest of this section we will firstly intro-
duce the probabilistic model (Sec. 3.1) and, then,
we will describe how SVD is used as feature se-
lector in the logistic regression that estimates the
probabilities of the model. To describe this part we
need to go in depth into the definition of the logis-
tic regression (Sec. 3.2) and the way of estimating
the regression coefficients (Sec. 3.3). This will
open the possibility of describing how we exploit
SVD (Sec. 3.4)
</bodyText>
<subsectionHeader confidence="0.997651">
3.1 Probabilistic model
</subsectionHeader>
<bodyText confidence="0.99919425">
In the probabilistic formulation (Snow et al.,
2006), the task of learning taxonomies from a cor-
pus is seen as a probability maximization prob-
lem. The taxonomy is seen as a set T of asser-
tions R over pairs Ri,j. If Ri,j is in T, i is a con-
cept and j is one of its generalization (i.e., the di-
rect or the indirect generalization). For example,
Rdog,animal E T describes that dog is an animal.
The main innovation of this probabilistic method
is the ability of taking into account in a single
probability the information coming from the cor-
pus and an existing taxonomy T.
The main probabilities are then: (1) the prior
probability P(Ri,j E T) of an assertion Ri,j to
belong to the taxonomy T and (2) the posterior
probability P(Ri,j E T|e i,j) of an assertion Ri,j
to belong to the taxonomy T given a set of evi-
dences a i,j derived from the corpus. Evidences
is a feature vector associated with a pair (i, j). For
examples, a feature may describe how many times
i and j are seen in patterns like ”i as j” or ”i is
a j”. These among many other features are in-
dicators of an is-a relation between i and j (see
(Hearst, 1992)).
Given a set of evidences E over all the relevant
word pairs, in (Snow et al., 2006), the probabilis-
tic taxonomy learning task is defined as the prob-
lem of finding the taxonomy T that maximizes the
</bodyText>
<page confidence="0.966432">
67
</page>
<bodyText confidence="0.523539">
probability of having the evidences E, i.e.:
</bodyText>
<equation confidence="0.987777">
T = arg max
T
</equation>
<bodyText confidence="0.999718375">
In (Snow et al., 2006), this maximization prob-
lem is solved with a local search. What is max-
imized at each step is the increase of the probabil-
ity P (E|T) of the taxonomy when the taxonomy
changes from T to T&apos; = T U N where N are the
relations added at each step. This increase of prob-
abilities is defined as multiplicative change 0(N)
as follows:
</bodyText>
<equation confidence="0.999376">
0(N) = P(E|T&apos;)/P(E|T) (2)
</equation>
<bodyText confidence="0.997639285714286">
The main innovation of the model in (Snow et al.,
2006) is the possibility of adding at each step the
best relation N = {Ri,j} as well as N = I(Ri,j)
that is Ri,j with all the relations by the existing
taxonomy. We will then experiment with our fea-
ture selection methodology in the two different
models:
flat: at each iteration step, a single relation is
added, i.e. �Ri,j = arg maxRi,j A(Ri,j)
inductive: at each iteration step, a set of re-
lations is added, i.e. I(�Ri,j) where �Ri,j =
argmaxRi,j A(I(Ri,j)).
The last important fact is that it is possible to
demonstrate that
</bodyText>
<equation confidence="0.983292">
�(Ei,j) = k · 1 P
— (Ri,j E T|�� e i,j) =
P (Ri,j E T |�� e i,j)
= k · odds(Ri,j)
</equation>
<bodyText confidence="0.999979666666667">
where k is a constant (see (Snow et al., 2006))
that will be neglected in the maximization process.
This last equation gives the possibility of using the
logistic regression as it is. In the next sections we
will see how SVD and the related feature selection
can be used to compute the odds.
</bodyText>
<subsectionHeader confidence="0.999057">
3.2 Logistic Regression
</subsectionHeader>
<bodyText confidence="0.992395391304348">
Logistic Regression (Cox, 1958) is a particular
type of statistical model for relating responses Y
to linear combinations of predictor variables X. It
is a specific kind of Generalized Linear Model (see
(Nelder and Wedderburn, 1972)) where its func-
tion is the logit function and the independent vari-
able Y is a binary or dicothomic variable which
has a Bernoulli distribution. The dependent vari-
able Y takes value 0 or 1. The probability that
Y has value 1 is function of the regressors x =
(1, x1, ..., xk).
The probabilistic taxonomy learner model in-
troduced in the previous section falls in the cat-
egory of probabilistic models where the logistic
regression can be applied as Ri,j E T is the bi-
nary dependent variable and e i,j is the vector of
its regressors. In the rest of the section we will see
how the odds, i.e., the multiplicative change, can
be computed.
We start from formally describing the Logistic
Regression Model. Given the two stochastic vari-
ables Y and X, we can define as p the probability
of Y to be 1 given that X=x, i.e.:
</bodyText>
<equation confidence="0.990141">
p = P(Y = 1|X = x)
</equation>
<bodyText confidence="0.531937">
The distribution of the variable Y is a Bernulli dis-
tribution, i.e.:
</bodyText>
<equation confidence="0.904575">
Y — Bernoulli(p)
Given the definition of the logit(p) as:
logit(p) = ln I p p// I (3)
1 —
</equation>
<bodyText confidence="0.9993815">
and given the fact that Y is a Bernoulli distribution,
the logistic regression foresees that the logit is a
linear combination of the values of the regressors,
i.e.,
</bodyText>
<equation confidence="0.999477">
logit(p) = Q0 + Q1x1 + ... + Qkxk (4)
</equation>
<bodyText confidence="0.999855857142857">
where Q0, Q1, ..., Qk are called regression coeffi-
cients of the variables x1, ..., xk respectively.
Given the regression coefficients, it is possible
to compute the probability of a given event where
we observe the regressors x to be Y = 1 or in our
case to belong to the taxonomy. This probability
can be computed as follows:
</bodyText>
<equation confidence="0.999926">
p(x) = 1 + exp(Q0 + Q1x1 + ... + Qkxk)
exp(Q0 + Q1x1 + ... + Qkxk)
</equation>
<bodyText confidence="0.99764">
It is obviously trivial to determine the
odds(Ri,j) related to the multiplicative change
of the probabilistic taxonomy model. The odds
is the ratio between the positive and the negative
event. It is defined as follows:
</bodyText>
<equation confidence="0.99982425">
odds(Ri,j) = P(Ri,jET|�� e i,j) (5)
1−P(Ri,jET |e i,j)
Then, it is strictly related with the logit, i.e.:
odds(Ri,j) = exp(Q0 + e T i,jQ) (6)
</equation>
<bodyText confidence="0.887164">
The relationship between the possible values of
the probability, odds and logit is show in the Table
1.
</bodyText>
<equation confidence="0.480667">
P(E|T)
</equation>
<page confidence="0.63414">
68
</page>
<figure confidence="0.9514998">
Probability Odds Logit
0 ≤ p &lt; 0.5 [0, 1) (−∞, 0]
0.5 &lt; p ≤ 1 [1, ∞) [0, ∞)
1988):
y=XQ+E
</figure>
<tableCaption confidence="0.976058">
Table 1: Relationship between probability, odds
and logit
</tableCaption>
<subsectionHeader confidence="0.998096">
3.3 Estimating Regression Coefficients
</subsectionHeader>
<bodyText confidence="0.99881128">
The remaining problem is how to estimate the re-
gression coefficients. This estimation is done us-
ing the maximal likelihood estimation to prepare a
set of linear equations using the above logit defini-
tion and, then, solving a linear problem. This will
give us the possibility of introducing the necessity
of determining a pseudo-inverse matrix where we
will use the singular value decomposition and its
natural possibility of performing feature selection.
Once we have the regression coefficients, we have
the possibility of assigning estimating a probabil-
ity P(Ri,j ∈ T|−→e i,j) given any configuration of
the values of the regressors →−e i,j, i.e., the observed
values of the features. For sake of simplicity we
will hereafter refer to →−e i,j as →−e l.
Let assume we have a multiset O of observa-
tions extracted from Y × E where Y ∈ {0, 1} and
we know that some of them are positive observa-
tions (i.e., Y = 1) and some of them are negative
observations (i.e., Y = 0).
For each pairs the relative configuration →−e l ∈
E that appeared at least once in O, we can de-
termine using the maximal likelihood estimation
P (Y = 1|−→e l). Then, from the equation of the
logit (Eq. 4), we have a linear equation system,
</bodyText>
<equation confidence="0.917419666666667">
i.e.:
−−−−−→
logit(p) = QQ (7)
</equation>
<bodyText confidence="0.994597">
where Q is a matrix that includes a constant col-
umn of 1, necessary for the Q0 of the linear combi-
nation of the values of the regression. Moreover it
includes the transpose of the evidence matrix, i.e.
E = (−→e 1...−→e m). Therefore the matrix will be:
</bodyText>
<equation confidence="0.662618">
� 1 e11 e12 ··· e1n
Q = 1 e21 e22 ··· �
� � � � � ... ... ... .. e2n
1 em1 em2 .. .. � � � � �
··· emn
</equation>
<bodyText confidence="0.9570352">
The set of equations in Eq. 7 can be solved us-
ing multiple linear regression.
In their general form, the equations of multiple
linear regression may be written as (Caron et al.,
where:
</bodyText>
<listItem confidence="0.9943393">
• y is a column vector n × 1 that includes the
observed values of the dependent variables
Y1, ..., Yk;
• X is a matrix n × m of the values of the re-
gressors that we have observed;
• Q is a column vector m × 1 of the regression
coefficients;
• E is a column vector including the stochastic
components that have not been observed and
that will not be considered later.
</listItem>
<bodyText confidence="0.9993628">
In the case X is a rectangular and singular matrix,
the system y = XQ has not a solution. Yet, it is
possible to use the principle of the Least Square
Estimation. This principle determines the solution
Q that minimize the residual norm, i.e.:
</bodyText>
<equation confidence="0.991206">
Q = arg min kXQ − yk2 (8)
</equation>
<bodyText confidence="0.941521">
This problem can be solved by the Moore-
Penrose pseudoinverse X+ (Penrose, 1955).
Then, the final equation to determine the Q is
Q=X+y
It is important to remark that if the inverse matrix
exist X+ = X−1 and that X+X and XX+ are
symmetric.
For our case, the following equation is valid:
</bodyText>
<equation confidence="0.9906595">
Q = Q+−−−−−→
logit(p)
</equation>
<subsectionHeader confidence="0.990806">
3.4 Computing Pseudoinverse Matrix with
SVD Analysis
</subsectionHeader>
<bodyText confidence="0.999987916666667">
We finally reached the point where it is possible
to explain our idea that is naturally using singular
value decomposition (SVD) as feature selection in
a probabilistic taxonomy learner. In the previous
sections we described how the probabilities of the
taxonomy learner can be estimated using logistic
regressions and we concluded that a way to de-
termine the regression coefficients Q is computing
the Moore-Penrose pseudoinverse Q+. It is pos-
sible to compute the Moore-Penrose pseudoin-
verse using the SVD in the following way (Pen-
rose, 1955). Given an SVD decomposition of the
</bodyText>
<page confidence="0.992681">
69
</page>
<bodyText confidence="0.8881495">
matrix Q = UΣV T the pseudo-inverse matrix that
minimizes the Eq. 9 is:
</bodyText>
<equation confidence="0.997799">
Q+ = V Σ+UT (9)
</equation>
<bodyText confidence="0.999924846153846">
The diagonal matrix Σ+ is a matrix r x r obtained
first transposing Σ and then calculating the recip-
rocals of the singular value of Σ. So the diagonal
elements of the Σ+ are δ1 1 , δ2 1 , ...,, δr 1 .
We have now our opportunity of using SVD as
natural feature selector as we can compute differ-
ent approximations of the pseudo-inverse matrix.
As we saw in Sec. 2, the algorithm for computing
the singular value decomposition can be stopped a
different dimensions. We called k the number of
dimensions. As we can obtain different SVD as
approximations of the original matrix (Eq. 2), we
can define different approximations of :
</bodyText>
<equation confidence="0.959569">
+ Pz +=U E+ UT
Q Qk nxk k×k k×m
</equation>
<bodyText confidence="0.999539333333333">
In our experiments we will use different values
of k to explore the benefits of SVD as feature se-
lector.
</bodyText>
<sectionHeader confidence="0.998313" genericHeader="evaluation">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999993583333333">
In this section, we want to empirically explore
whether our use of SVD feature selection pos-
itively affects performances of the probabilistic
taxonomy learner. The best way of determining
how a taxonomy learner is performing is to see if it
can replicate an existing ”taxonomy”. We will ex-
periment with the attempt of replicating a portion
of WordNet (Miller, 1995). In the experiments, we
will address two issues: 1) determining to what
extent SVD feature selection affect performances
of the taxonomy learner; 2) determining if SVD
as unsupervised feature selection is better for the
task than some simpler model for taxonomy learn-
ing. We will explore the effects on both the flat
and the inductive probabilistic taxonomy learner.
The rest of the section is organized as follows.
In Sec. 4.1 we will describe the experimental set-
up in terms of: how we selected the portion of
WordNet, the description of the corpus used to ex-
tract evidences, a description of the feature space
we used, and, finally, the description of a baseline
models for taxonomy learning we have used. In
Sec. 4.2 we will present the results of the experi-
ments in term of performance.
</bodyText>
<subsectionHeader confidence="0.939634">
4.1 Experimental Set-up
</subsectionHeader>
<bodyText confidence="0.999986306122449">
To completely define the experiments we need to
describe some issues: how we defined the taxon-
omy to replicate, which corpus we have used to
extract evidences for pairs of words, which feature
space we used, and, finally, the baseline model we
compared our feature selection model against.
As target taxonomy we selected a portion of
WordNet1 (Miller, 1995). Namely, we started
from the 44 concrete nouns listed in (McRae et
al., 2005) and divided in 3 classes: animal, arti-
fact, and vegetable. For sake of comprehension,
this set is described in Tab. 2. For each word w,
we selected the synset sw that is compliant with
the class it belongs to. We then obtained a set S of
synsets (see Tab. 2). We then expanded the set to
S0 adding the siblings (i.e., the coordinate terms)
for each synset in S. The set S0 contains 265 co-
ordinate terms plus the 44 original concrete nouns.
For each element in S we collected its hyperonym,
obtaining the set H. We then removed from the set
H the 4 topmosts: entity, unit, object, and whole.
The set H contains 77 hyperonyms. For the pur-
pose of the experiments we both derived from the
previous sets a taxonomy T and produced a set of
negative examples T. The two sets have been ob-
tained as follows. The taxonomy T is the portion
of WordNet implied by O = H U S0, i.e., T con-
tains all the (s, h) E O x O that are in WordNet.
On the contrary, T contains all the (s, h) E O x O
that are not in WordNet. We then have 5108 posi-
tive pairs in T and 52892 negative pairs in T.
We then split the set T UT in two parts, training
and testing. As we want to see if it is possible to
attach the set S0 to the right hyperonym, the split
has been done as follows. We randomly divided
the set S0 in two parts Str and Sts, respectively,
of 70% and 30% of the original S0. We then se-
lected as training Ttr all the pairs in T containing
a synset in Str and as testing set Tts those pairs of
T containing a synset of Sts. For the probabilistic
model, Ttr is the initial taxonomy whereas Tts UT
is the unknown set.
As corpus we used the English Web as Corpus
(ukWaC) (Ferraresi et al., 2008). This is a web
extracted corpus of about 2700000 web pages con-
taining more than 2 billion words. The corpus con-
tains documents of different topics such as web,
computers, education, public sphere, etc.. It has
been largely demonstrated that the web documents
</bodyText>
<footnote confidence="0.997731">
1We used the version 3.0
</footnote>
<page confidence="0.983755">
70
</page>
<table confidence="0.999733913043478">
Concrete nouns Clas Sense Concrete nouns Clas Sense
1 banana Vegetable 1 23 boat Artifact 0
2 bottle Artifact 0 24 bowl Artifact 0
3 car Artifact 0 25 cat Animal 0
4 cherry Vegetable 2 26 chicken Animal 1
5 chisel Artifact 0 27 corn Vegetable 2
6 cow Animal 0 28 cup Artifact 0
7 dog Animal 0 29 duck Animal 0
8 eagle Animal 0 30 elephant Animal 0
9 hammer Artifact 1 31 helicopter Artifact 0
10 kettle Artifact 0 32 knife Artifact 0
11 lettuce Vegetable 2 33 lion Animal 0
12 motorcycle Artifact 0 34 mushroom Vegetable 4
13 onion Vegetable 2 35 owl Animal 0
14 peacock Animal 1 36 pear Vegetable 0
15 pen Artifact 0 37 pencil Artifact 0
16 penguin Animal 0 38 pig Animal 0
17 pineapple Vegetable 1 39 potato Vegetable 2
18 rocket Artifact 0 40 scissors Artifact 0
19 screwdriver Artifact 0 41 ship Artifact 0
20 snail Animal 0 42 spoon Artifact 0
21 swan Animal 0 43 telephone Artifact 1
22 truck Artifact 0 44 turtle Animal 1
</table>
<tableCaption confidence="0.999715">
Table 2: Concrete nouns, Classes and senses selected in WordNet
</tableCaption>
<bodyText confidence="0.999493481481482">
are good models for natural language (Lapata and
Keller, 2004).
As the focus of the paper is the analysis of the
effect of the SVD feature selection, we used as fea-
ture spaces both n-grams and bag-of-words. Out
of the T U T, we selected only those pairs that
appeared at a distance of at most 3 tokens. Us-
ing these 3 tokens, we generated three spaces:
(1) 1-gram that contains monograms, (2) 2-gram
that contains monograms and bigrams, and (3) the
3-gram space that contains monograms, bigrams,
and trigrams. For the purpose of this experiment,
we used a reduced stop list as classical stop words
as punctuation, parenthesis, the verb to be are very
relevant in the context of features for learning a
taxonomy.
Finally, we want to describe our baseline model
for taxonomy learning. This model only contains
Heart’s patterns (Hearst, 1992) as features. The
feature value is the point-wise mutual information.
These features are in some sense the best features
for the task as these have been manually selected
after a process of corpus analysis. These baseline
features are included in our 3-gram model. We can
then compare our best models with this baseline
features in order to see if our SVD feature selec-
tion model outperforms manual feature selection.
</bodyText>
<subsectionHeader confidence="0.681414">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999981157894737">
In the first set of experiments we want to focus on
the issue whether or not performances of the prob-
abilistic taxonomy learner is positively affected
by the proposed feature selection model based on
the singular value decomposition. We then deter-
mined the performance with respect to different
values of k. This latter represents the number of
surviving dimensions where the pseudo-inverse is
computed. Then, it represents the number of fea-
tures the model adopts. We performed this first set
of experiments in the 1-gram feature space. Punc-
tuation has been considered. Figure 1 plots the ac-
curacy of the probabilistic learner with respect to
the size of the feature set, i.e. the number k of sin-
gle values considered for computing the pseudo-
inverse matrix. To determine if the effect of the
feature selection is preserved during the iteration
of the local search algorithm, we report curves at
different sizes of the set of added pairs. Curves are
</bodyText>
<page confidence="0.989798">
71
</page>
<figure confidence="0.99740172">
flat
inductive
accuracy
0.6
0.5
0.4
0.3
0.2
0.1
20 added pairs
40 added pairs
60 added pairs
80 added pairs
100 added pairs
accuracy
0.6
0.5
0.4
0.3
0.2
0.1
40 added pairs
80 added pairs
130 added pairs
0 200 400 600 800 1000 0 200 400 600 800 1000
</figure>
<figureCaption confidence="0.9765525">
k: dimension of the reduced space k: dimension of the reduced space
Figure 1: Accuracy over different cuts of the feature space
</figureCaption>
<figure confidence="0.958834">
0 100 200 300 400 500 600
added pairs
</figure>
<figureCaption confidence="0.9198925">
Figure 2: Comparison of different feature spaces
with k=400
</figureCaption>
<bodyText confidence="0.999988735294118">
reported for both the flat model and the inductive
model. The flat algorithm adds one pair at each
iteration. Then, we reported curves for each 20
added pairs. Each curve shows that accuracy does
not increase after a dimension of k=700. This size
of the space is necessary only for the first 20 added
pairs. Accuracy keeps increasing to k=700 and
then decreases. When we add more pairs, the opti-
mal size of the space is around k=200. For the in-
ductive model we report the accuracies for around
40, 80, 130 added pairs. Here, at each iteration,
more than one pair is added. The optimal dimen-
sion of the feature space seems to be around 500
as after that value performances decrease or stay
stable. SVD feature selection has then a positive
effect for both the flat and the inductive probabilis-
tic taxonomy learners. This has beneficial effects
both on the performances and on the computation
time.
In the second set of experiments we want to de-
termine whether or not SVD feature selection for
the probabilistic taxonomy learner behaves better
than a reduced set of known features. We then
fixed the dimension k to 400 and we compared the
baseline model with different probabilistic models
with different feature sets: 1-gram, 2-gram, and
3-gram. We can consider that the trigram model
before the cut on its dimensions contains feature
subsuming the baseline model. Figure 2 shows re-
sults. Curves report accuracy after n added pairs.
All the probabilistic models outperform the base-
line model. As what happened for the first series of
experiments (see Fig. 1) more informative spaces
such as 3-gram behaves better when the number of
</bodyText>
<figure confidence="0.989347454545455">
accuracy
0.6
0.5
0.4
0.3
0.2
0.1
baseline
1-gram
2-gram
3-gram
</figure>
<page confidence="0.988977">
72
</page>
<bodyText confidence="0.999806166666667">
added pairs is small. Performances of the three re-
duced pairs become similar after 100 added pairs.
These experiments show that SVD feature selec-
tion has a positive effect on performances as re-
sulting models are always better with respect to
the baseline.
</bodyText>
<sectionHeader confidence="0.998103" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999967">
We presented a model to naturally introduce
SVD feature selection in a probabilistic taxonomy
learner. The method is effective as allows the de-
signing of better probabilistic taxonomy learners.
We still need to explore at least two issues. First,
we need to determine whether or not the posi-
tive effect of SVD feature selection is preserved
in more complex feature spaces such as syntactic
feature spaces as those used in (Snow et al., 2006).
Second, we need to compare the SVD feature se-
lection with other unsupervised feature selection
models to determine whether or not this is the best
method to use in the case of probabilistic taxon-
omy learning.
</bodyText>
<sectionHeader confidence="0.99911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999478">
D. Caron, W. Hospital, and P. N. Corey. 1988.
Variance estimation of linear regression coefficients
in complex sampling situation. Sampling Error:
Methodology, Software and Application, pages 688–
694.
D. R. Cox. 1958. The regression analysis of binary
sequences. Journal of the Royal Statistical Society.
Series B (Methodological), 20(2):215–242.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. L, and Richard Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391–
407.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernar-
dini. 2008. Introducing and evaluating ukwac, a
very large web-derived corpus of english. In In
Proceed-ings of the WAC4 Workshop at LREC 2008,
Marrakesh, Morocco.
G. Golub and W. Kahan. 1965. Calculating the singu-
lar values and pseudo-inverse of a matrix. Journal of
the Society for Industrial and Applied Mathematics,
Series B: Numerical Analysis, 2(2):205–224.
Isabelle Guyon and Andr´e Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3:1157–1182, March.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics, New York. Oxford University
Press.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 15th International Conference on Computational
Linguistics (CoLing-92), Nantes, France.
Mirella Lapata and Frank Keller. 2004. The web as
a baseline: Evaluating the performance of unsuper-
vised web-based models for a range of nlp tasks.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, Boston,
MA.
Bing Liu. 2007. Web Data Mining: Exploring Hy-
perlinks, Contents, and Usage Data. Data-Centric
Systems and Applications. Springer.
Bernardo Magnini and Manuela Speranza. 2001. In-
tegrating generic and specialized wordnets. In In
Proceedings of the Euroconference RANLP 2001,
Tzigov Chark, Bulgaria.
K. McRae, G.S. Cree, M.S. Seidenberg, and C. McNor-
gan. 2005. Semantic feature production norms for a
large set of living and nonliving things. pages 547–
559, Behavioral Research Methods, Instruments,
and Computers.
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39–41, November.
J. A. Nelder and R. W. M. Wedderburn. 1972. Gener-
alized linear models. Journal of the Royal Statistical
Society. SeriesA (General), 135(3):370–384.
Donie O’Sullivan, A. McElligott, and Richard F. E.
Sutcliffe. 1995. Augmenting the princeton wordnet
with a domain specific ontology. In Proceedings of
the Workshop on Basic Issues in Knowledge Sharing
at the 14th International Joint Conference on Artifi-
cial Intelligence. Montreal, Canada.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
113–120, Sydney, Australia, July. Association for
Computational Linguistics.
R. Penrose. 1955. A generalized inverse for matrices.
In Proc. Cambridge Philosophical Society.
Harold R. Robison. 1970. Computer-detectable se-
mantic structures. Information Storage and Re-
trieval, 6(3):273–288.
Rion Snow, Daniel Jurafsky, and A. Y. Ng. 2006. Se-
mantic taxonomy induction from heterogenous evi-
dence. In In ACL, pages 801–808.
</reference>
<page confidence="0.999299">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.489303">
<title confidence="0.998321">SVD Feature Selection for Probabilistic Taxonomy Learning</title>
<author confidence="0.790368">Fallucchi</author>
<affiliation confidence="0.99242">Disp, University “Tor</affiliation>
<address confidence="0.70318">Rome,</address>
<email confidence="0.998152">fallucchi@info.uniroma2.it</email>
<author confidence="0.999468">Fabio Massimo</author>
<affiliation confidence="0.999496">Disp, University “Tor</affiliation>
<address confidence="0.901526">Rome,</address>
<email confidence="0.994807">zanzotto@info.uniroma2.it</email>
<abstract confidence="0.9983106">In this paper, we propose a novel way to include unsupervised feature selection methods in probabilistic taxonomy learning models. We leverage on the computation of logistic regression to exploit unsupervised feature selection of singular value decomposition (SVD). Experiments show that this way of using SVD for feature selection positively affects performances.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Caron</author>
<author>W Hospital</author>
<author>P N Corey</author>
</authors>
<title>Variance estimation of linear regression coefficients in complex sampling situation. Sampling Error: Methodology, Software and Application,</title>
<date>1988</date>
<pages>688--694</pages>
<marker>Caron, Hospital, Corey, 1988</marker>
<rawString>D. Caron, W. Hospital, and P. N. Corey. 1988. Variance estimation of linear regression coefficients in complex sampling situation. Sampling Error: Methodology, Software and Application, pages 688– 694.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Cox</author>
</authors>
<title>The regression analysis of binary sequences.</title>
<date>1958</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="9692" citStr="Cox, 1958" startWordPosition="1696" endWordPosition="1697">nductive: at each iteration step, a set of relations is added, i.e. I(�Ri,j) where �Ri,j = argmaxRi,j A(I(Ri,j)). The last important fact is that it is possible to demonstrate that �(Ei,j) = k · 1 P — (Ri,j E T|�� e i,j) = P (Ri,j E T |�� e i,j) = k · odds(Ri,j) where k is a constant (see (Snow et al., 2006)) that will be neglected in the maximization process. This last equation gives the possibility of using the logistic regression as it is. In the next sections we will see how SVD and the related feature selection can be used to compute the odds. 3.2 Logistic Regression Logistic Regression (Cox, 1958) is a particular type of statistical model for relating responses Y to linear combinations of predictor variables X. It is a specific kind of Generalized Linear Model (see (Nelder and Wedderburn, 1972)) where its function is the logit function and the independent variable Y is a binary or dicothomic variable which has a Bernoulli distribution. The dependent variable Y takes value 0 or 1. The probability that Y has value 1 is function of the regressors x = (1, x1, ..., xk). The probabilistic taxonomy learner model introduced in the previous section falls in the category of probabilistic models </context>
</contexts>
<marker>Cox, 1958</marker>
<rawString>D. R. Cox. 1958. The regression analysis of binary sequences. Journal of the Royal Statistical Society. Series B (Methodological), 20(2):215–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>K L Thomas</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<pages>407</pages>
<contexts>
<context position="3059" citStr="Deerwester et al., 1990" startWordPosition="462" endWordPosition="465">istic taxonomy learning model introduced by (Snow et al., 2006). We will then shortly review the logistic regression used to compute the taxonomy learning model to describe where SVD can be naturally used. We will describe our experiments in Sec. 4. Finally, we will draw some conclusions and describe our future work (Sec. 5). 2 Unsupervised feature selection with Singular Value Decomposition Singular value decomposition (SVD) is one of the possible factorization of a rectangular matrix that has been largely used in information retrieval for reducing the dimension of the document vector space (Deerwester et al., 1990). The decomposition can be defined as follows. Given a generic rectangular n x m matrix A, its singular value decomposition is: A = UEV T where U is a matrix n x r, VT is a r x m and E is a diagonal matrix r x r. The two matrices U and V are unitary, i.e., UT U = I and V T V = I. The diagonal elements of the E are the singular values such as 61 &gt; 62 &gt; ... &gt; 6r &gt; 0 where r is the rank of the matrix A. For the decomposition, SVD exploits the linear combination of rows and columns of A. A first trivial way of using SVD as unsupervised g • c l Modelo atural n e Semantidv a 7 3 41E����11�0a4��i s 6</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Thomas, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. L, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41:391– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ferraresi</author>
<author>E Zanchetta</author>
<author>M Baroni</author>
<author>S Bernardini</author>
</authors>
<title>Introducing and evaluating ukwac, a very large web-derived corpus of english. In</title>
<date>2008</date>
<booktitle>In Proceed-ings of the WAC4 Workshop at LREC</booktitle>
<location>Marrakesh, Morocco.</location>
<contexts>
<context position="19670" citStr="Ferraresi et al., 2008" startWordPosition="3541" endWordPosition="3544"> negative pairs in T. We then split the set T UT in two parts, training and testing. As we want to see if it is possible to attach the set S0 to the right hyperonym, the split has been done as follows. We randomly divided the set S0 in two parts Str and Sts, respectively, of 70% and 30% of the original S0. We then selected as training Ttr all the pairs in T containing a synset in Str and as testing set Tts those pairs of T containing a synset of Sts. For the probabilistic model, Ttr is the initial taxonomy whereas Tts UT is the unknown set. As corpus we used the English Web as Corpus (ukWaC) (Ferraresi et al., 2008). This is a web extracted corpus of about 2700000 web pages containing more than 2 billion words. The corpus contains documents of different topics such as web, computers, education, public sphere, etc.. It has been largely demonstrated that the web documents 1We used the version 3.0 70 Concrete nouns Clas Sense Concrete nouns Clas Sense 1 banana Vegetable 1 23 boat Artifact 0 2 bottle Artifact 0 24 bowl Artifact 0 3 car Artifact 0 25 cat Animal 0 4 cherry Vegetable 2 26 chicken Animal 1 5 chisel Artifact 0 27 corn Vegetable 2 6 cow Animal 0 28 cup Artifact 0 7 dog Animal 0 29 duck Animal 0 8 </context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini. 2008. Introducing and evaluating ukwac, a very large web-derived corpus of english. In In Proceed-ings of the WAC4 Workshop at LREC 2008, Marrakesh, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Golub</author>
<author>W Kahan</author>
</authors>
<title>Calculating the singular values and pseudo-inverse of a matrix.</title>
<date>1965</date>
<journal>Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="4729" citStr="Golub and Kahan, 1965" startWordPosition="804" endWordPosition="807">ach new feature is obtained as a linear combination of the original features, i.e. each feature vector ei can be seen as a new feature vector ei 0 = UT ei . When the target feature space is big whereas the cardinality of the training set is small, i.e., n &gt;&gt; m, the application of SVD results in a reduction of the original feature space as the rank r of the matrix E is r &lt; min(n, m). A more interesting way of using SVD as unsupervised feature selection model is to exploit its approximated computations, i.e. : A P Ak = UmxkEkxkVkTxn where k is smaller than the rank r. The computation algorithm (Golub and Kahan, 1965) is allowed to stop at a given k different from the real rank r. The property of the singular values, i.e., 61 &gt; 62 &gt; ... &gt; 6r &gt; 0, guarantees that the first k are bigger than the discarded ones. There is a direct relation between the informativeness of the dimension and the value of the singular value. High singular values correspond to dimensions of the new space where examples have more variability whereas low singular values determine dimensions where examples have a smaller variability (see (Liu, 2007)). These dimensions can not be used as discriminative features in learning algorithms. T</context>
</contexts>
<marker>Golub, Kahan, 1965</marker>
<rawString>G. Golub and W. Kahan. 1965. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, 2(2):205–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Andr´e Elisseeff</author>
</authors>
<title>An introduction to variable and feature selection.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1157</pages>
<contexts>
<context position="1846" citStr="Guyon and Elisseeff, 2003" startWordPosition="265" endWordPosition="268"> lexical-syntactic patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006). In these models, within a very large set, candidate word pairs are selected as new word pairs in hyperonymy and added to an existing taxonomy. Candidate pairs are represented in some feature space. Often, these feature spaces are huge and, then, models may take into consideration noisy features. In machine learning, feature selection has been often used to reduce the dimensions in huge feature spaces. This has many advantages, e.g., reducing the computational cost and improving performances by removing noisy features (Guyon and Elisseeff, 2003). In this paper, we propose a novel way to inProceedins the EAC 2009 Worksho on EMS: GEo clud ` depe 9 I probabilistic taxonomy learning models. Given the probabilistic taxonomy learning model introduced by (Snow et al., 2006), we leverage on the computation of logistic regression to exploit singular value decomposition (SVD) as unsupervised feature selection. SVD is used to compute the pseudo-inverse matrix needed in logistic regression. To describe our idea, we firstly review how SVD can be used as unsupervised feature selection (Sec. 2). In Section 3 we then describe the probabilistic taxon</context>
</contexts>
<marker>Guyon, Elisseeff, 2003</marker>
<rawString>Isabelle Guyon and Andr´e Elisseeff. 2003. An introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157–1182, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1964</date>
<booktitle>The Philosophy of Linguistics,</booktitle>
<editor>In Jerrold J. Katz and Jerry A. Fodor, editors,</editor>
<publisher>University Press.</publisher>
<location>New York. Oxford</location>
<contexts>
<context position="1195" citStr="Harris, 1964" startWordPosition="165" endWordPosition="166">ositively affects performances. 1 Introduction Taxonomies are extremely important knowledge repositories in a variety of applications for natural language processing and knowledge representation. Yet, manually built taxonomies such as WordNet (Miller, 1995) often lack in coverage when used in specific knowledge domains. Automatically creating or extending taxonomies for specific domains is then a very interesting area of research (O’Sullivan et al., 1995; Magnini and Speranza, 2001; Snow et al., 2006). Automatic methods for learning taxonomies from corpora often use distributional hypothesis (Harris, 1964) and exploit some induced lexical-syntactic patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006). In these models, within a very large set, candidate word pairs are selected as new word pairs in hyperonymy and added to an existing taxonomy. Candidate pairs are represented in some feature space. Often, these feature spaces are huge and, then, models may take into consideration noisy features. In machine learning, feature selection has been often used to reduce the dimensions in huge feature spaces. This has many advantages, e.g., reducing the computational cost and improving performances by </context>
<context position="6232" citStr="Harris, 1964" startWordPosition="1055" endWordPosition="1056">n the sense that the feature selection is done without taking into account the final classes of the training examples. This is not always the case, feature selection models such as those based on Information Gain largely use the final classes of training examples. SVD as feature selection is independent from the classification problem. 3 Probabilistic Taxonomy Learning and SVD feature selection Recently, Snow et al. (2006) introduced a probabilistic model for learning taxonomies form corpora. This probabilistic formulation exploits the two well known hypotheses: the distributional hypothesis (Harris, 1964) and the exploitation of the lexico-syntactic patterns as in (Robison, 1970; Hearst, 1992). Yet, in this formulation, we can positively and naturally introduce our use of SVD as feature selection model. In the rest of this section we will firstly introduce the probabilistic model (Sec. 3.1) and, then, we will describe how SVD is used as feature selector in the logistic regression that estimates the probabilities of the model. To describe this part we need to go in depth into the definition of the logistic regression (Sec. 3.2) and the way of estimating the regression coefficients (Sec. 3.3). T</context>
</contexts>
<marker>Harris, 1964</marker>
<rawString>Zellig Harris. 1964. Distributional structure. In Jerrold J. Katz and Jerry A. Fodor, editors, The Philosophy of Linguistics, New York. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (CoLing-92),</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="1261" citStr="Hearst, 1992" startWordPosition="173" endWordPosition="174">mely important knowledge repositories in a variety of applications for natural language processing and knowledge representation. Yet, manually built taxonomies such as WordNet (Miller, 1995) often lack in coverage when used in specific knowledge domains. Automatically creating or extending taxonomies for specific domains is then a very interesting area of research (O’Sullivan et al., 1995; Magnini and Speranza, 2001; Snow et al., 2006). Automatic methods for learning taxonomies from corpora often use distributional hypothesis (Harris, 1964) and exploit some induced lexical-syntactic patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006). In these models, within a very large set, candidate word pairs are selected as new word pairs in hyperonymy and added to an existing taxonomy. Candidate pairs are represented in some feature space. Often, these feature spaces are huge and, then, models may take into consideration noisy features. In machine learning, feature selection has been often used to reduce the dimensions in huge feature spaces. This has many advantages, e.g., reducing the computational cost and improving performances by removing noisy features (Guyon and Elisseeff, 2003). In this paper</context>
<context position="6322" citStr="Hearst, 1992" startWordPosition="1068" endWordPosition="1069">es of the training examples. This is not always the case, feature selection models such as those based on Information Gain largely use the final classes of training examples. SVD as feature selection is independent from the classification problem. 3 Probabilistic Taxonomy Learning and SVD feature selection Recently, Snow et al. (2006) introduced a probabilistic model for learning taxonomies form corpora. This probabilistic formulation exploits the two well known hypotheses: the distributional hypothesis (Harris, 1964) and the exploitation of the lexico-syntactic patterns as in (Robison, 1970; Hearst, 1992). Yet, in this formulation, we can positively and naturally introduce our use of SVD as feature selection model. In the rest of this section we will firstly introduce the probabilistic model (Sec. 3.1) and, then, we will describe how SVD is used as feature selector in the logistic regression that estimates the probabilities of the model. To describe this part we need to go in depth into the definition of the logistic regression (Sec. 3.2) and the way of estimating the regression coefficients (Sec. 3.3). This will open the possibility of describing how we exploit SVD (Sec. 3.4) 3.1 Probabilisti</context>
<context position="8050" citStr="Hearst, 1992" startWordPosition="1386" endWordPosition="1387">information coming from the corpus and an existing taxonomy T. The main probabilities are then: (1) the prior probability P(Ri,j E T) of an assertion Ri,j to belong to the taxonomy T and (2) the posterior probability P(Ri,j E T|e i,j) of an assertion Ri,j to belong to the taxonomy T given a set of evidences a i,j derived from the corpus. Evidences is a feature vector associated with a pair (i, j). For examples, a feature may describe how many times i and j are seen in patterns like ”i as j” or ”i is a j”. These among many other features are indicators of an is-a relation between i and j (see (Hearst, 1992)). Given a set of evidences E over all the relevant word pairs, in (Snow et al., 2006), the probabilistic taxonomy learning task is defined as the problem of finding the taxonomy T that maximizes the 67 probability of having the evidences E, i.e.: T = arg max T In (Snow et al., 2006), this maximization problem is solved with a local search. What is maximized at each step is the increase of the probability P (E|T) of the taxonomy when the taxonomy changes from T to T&apos; = T U N where N are the relations added at each step. This increase of probabilities is defined as multiplicative change 0(N) as</context>
<context position="21789" citStr="Hearst, 1992" startWordPosition="3930" endWordPosition="3931">we selected only those pairs that appeared at a distance of at most 3 tokens. Using these 3 tokens, we generated three spaces: (1) 1-gram that contains monograms, (2) 2-gram that contains monograms and bigrams, and (3) the 3-gram space that contains monograms, bigrams, and trigrams. For the purpose of this experiment, we used a reduced stop list as classical stop words as punctuation, parenthesis, the verb to be are very relevant in the context of features for learning a taxonomy. Finally, we want to describe our baseline model for taxonomy learning. This model only contains Heart’s patterns (Hearst, 1992) as features. The feature value is the point-wise mutual information. These features are in some sense the best features for the task as these have been manually selected after a process of corpus analysis. These baseline features are included in our 3-gram model. We can then compare our best models with this baseline features in order to see if our SVD feature selection model outperforms manual feature selection. 4.2 Results In the first set of experiments we want to focus on the issue whether or not performances of the probabilistic taxonomy learner is positively affected by the proposed fea</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 15th International Conference on Computational Linguistics (CoLing-92), Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>The web as a baseline: Evaluating the performance of unsupervised web-based models for a range of nlp tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="21013" citStr="Lapata and Keller, 2004" startWordPosition="3794" endWordPosition="3797"> 0 11 lettuce Vegetable 2 33 lion Animal 0 12 motorcycle Artifact 0 34 mushroom Vegetable 4 13 onion Vegetable 2 35 owl Animal 0 14 peacock Animal 1 36 pear Vegetable 0 15 pen Artifact 0 37 pencil Artifact 0 16 penguin Animal 0 38 pig Animal 0 17 pineapple Vegetable 1 39 potato Vegetable 2 18 rocket Artifact 0 40 scissors Artifact 0 19 screwdriver Artifact 0 41 ship Artifact 0 20 snail Animal 0 42 spoon Artifact 0 21 swan Animal 0 43 telephone Artifact 1 22 truck Artifact 0 44 turtle Animal 1 Table 2: Concrete nouns, Classes and senses selected in WordNet are good models for natural language (Lapata and Keller, 2004). As the focus of the paper is the analysis of the effect of the SVD feature selection, we used as feature spaces both n-grams and bag-of-words. Out of the T U T, we selected only those pairs that appeared at a distance of at most 3 tokens. Using these 3 tokens, we generated three spaces: (1) 1-gram that contains monograms, (2) 2-gram that contains monograms and bigrams, and (3) the 3-gram space that contains monograms, bigrams, and trigrams. For the purpose of this experiment, we used a reduced stop list as classical stop words as punctuation, parenthesis, the verb to be are very relevant in </context>
</contexts>
<marker>Lapata, Keller, 2004</marker>
<rawString>Mirella Lapata and Frank Keller. 2004. The web as a baseline: Evaluating the performance of unsupervised web-based models for a range of nlp tasks. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data. Data-Centric Systems and Applications.</title>
<date>2007</date>
<publisher>Springer.</publisher>
<contexts>
<context position="5241" citStr="Liu, 2007" startWordPosition="899" endWordPosition="900">mxkEkxkVkTxn where k is smaller than the rank r. The computation algorithm (Golub and Kahan, 1965) is allowed to stop at a given k different from the real rank r. The property of the singular values, i.e., 61 &gt; 62 &gt; ... &gt; 6r &gt; 0, guarantees that the first k are bigger than the discarded ones. There is a direct relation between the informativeness of the dimension and the value of the singular value. High singular values correspond to dimensions of the new space where examples have more variability whereas low singular values determine dimensions where examples have a smaller variability (see (Liu, 2007)). These dimensions can not be used as discriminative features in learning algorithms. The possibility of computing the approximated version of the matrix gives a powerful method for feature selection and filtering as we can decide in advance how many features or, better, linear combination of original features we want to use. As feature selection model, SVD is unsupervised in the sense that the feature selection is done without taking into account the final classes of the training examples. This is not always the case, feature selection models such as those based on Information Gain largely u</context>
</contexts>
<marker>Liu, 2007</marker>
<rawString>Bing Liu. 2007. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data. Data-Centric Systems and Applications. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Manuela Speranza</author>
</authors>
<title>Integrating generic and specialized wordnets. In</title>
<date>2001</date>
<booktitle>In Proceedings of the Euroconference RANLP 2001, Tzigov Chark,</booktitle>
<contexts>
<context position="1068" citStr="Magnini and Speranza, 2001" startWordPosition="144" endWordPosition="147">it unsupervised feature selection of singular value decomposition (SVD). Experiments show that this way of using SVD for feature selection positively affects performances. 1 Introduction Taxonomies are extremely important knowledge repositories in a variety of applications for natural language processing and knowledge representation. Yet, manually built taxonomies such as WordNet (Miller, 1995) often lack in coverage when used in specific knowledge domains. Automatically creating or extending taxonomies for specific domains is then a very interesting area of research (O’Sullivan et al., 1995; Magnini and Speranza, 2001; Snow et al., 2006). Automatic methods for learning taxonomies from corpora often use distributional hypothesis (Harris, 1964) and exploit some induced lexical-syntactic patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006). In these models, within a very large set, candidate word pairs are selected as new word pairs in hyperonymy and added to an existing taxonomy. Candidate pairs are represented in some feature space. Often, these feature spaces are huge and, then, models may take into consideration noisy features. In machine learning, feature selection has been often used to reduce the di</context>
</contexts>
<marker>Magnini, Speranza, 2001</marker>
<rawString>Bernardo Magnini and Manuela Speranza. 2001. Integrating generic and specialized wordnets. In In Proceedings of the Euroconference RANLP 2001, Tzigov Chark, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McRae</author>
<author>G S Cree</author>
<author>M S Seidenberg</author>
<author>C McNorgan</author>
</authors>
<title>Semantic feature production norms for a large set of living and nonliving things. pages 547– 559, Behavioral Research Methods, Instruments, and Computers.</title>
<date>2005</date>
<contexts>
<context position="17999" citStr="McRae et al., 2005" startWordPosition="3206" endWordPosition="3209">ly, the description of a baseline models for taxonomy learning we have used. In Sec. 4.2 we will present the results of the experiments in term of performance. 4.1 Experimental Set-up To completely define the experiments we need to describe some issues: how we defined the taxonomy to replicate, which corpus we have used to extract evidences for pairs of words, which feature space we used, and, finally, the baseline model we compared our feature selection model against. As target taxonomy we selected a portion of WordNet1 (Miller, 1995). Namely, we started from the 44 concrete nouns listed in (McRae et al., 2005) and divided in 3 classes: animal, artifact, and vegetable. For sake of comprehension, this set is described in Tab. 2. For each word w, we selected the synset sw that is compliant with the class it belongs to. We then obtained a set S of synsets (see Tab. 2). We then expanded the set to S0 adding the siblings (i.e., the coordinate terms) for each synset in S. The set S0 contains 265 coordinate terms plus the 44 original concrete nouns. For each element in S we collected its hyperonym, obtaining the set H. We then removed from the set H the 4 topmosts: entity, unit, object, and whole. The set </context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>K. McRae, G.S. Cree, M.S. Seidenberg, and C. McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. pages 547– 559, Behavioral Research Methods, Instruments, and Computers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="839" citStr="Miller, 1995" startWordPosition="111" endWordPosition="112">o.uniroma2.it Abstract In this paper, we propose a novel way to include unsupervised feature selection methods in probabilistic taxonomy learning models. We leverage on the computation of logistic regression to exploit unsupervised feature selection of singular value decomposition (SVD). Experiments show that this way of using SVD for feature selection positively affects performances. 1 Introduction Taxonomies are extremely important knowledge repositories in a variety of applications for natural language processing and knowledge representation. Yet, manually built taxonomies such as WordNet (Miller, 1995) often lack in coverage when used in specific knowledge domains. Automatically creating or extending taxonomies for specific domains is then a very interesting area of research (O’Sullivan et al., 1995; Magnini and Speranza, 2001; Snow et al., 2006). Automatic methods for learning taxonomies from corpora often use distributional hypothesis (Harris, 1964) and exploit some induced lexical-syntactic patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006). In these models, within a very large set, candidate word pairs are selected as new word pairs in hyperonymy and added to an existing taxonomy. </context>
<context position="16747" citStr="Miller, 1995" startWordPosition="2997" endWordPosition="2998">imations of the original matrix (Eq. 2), we can define different approximations of : + Pz +=U E+ UT Q Qk nxk k×k k×m In our experiments we will use different values of k to explore the benefits of SVD as feature selector. 4 Experimental Evaluation In this section, we want to empirically explore whether our use of SVD feature selection positively affects performances of the probabilistic taxonomy learner. The best way of determining how a taxonomy learner is performing is to see if it can replicate an existing ”taxonomy”. We will experiment with the attempt of replicating a portion of WordNet (Miller, 1995). In the experiments, we will address two issues: 1) determining to what extent SVD feature selection affect performances of the taxonomy learner; 2) determining if SVD as unsupervised feature selection is better for the task than some simpler model for taxonomy learning. We will explore the effects on both the flat and the inductive probabilistic taxonomy learner. The rest of the section is organized as follows. In Sec. 4.1 we will describe the experimental setup in terms of: how we selected the portion of WordNet, the description of the corpus used to extract evidences, a description of the </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for English. Communications of the ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Nelder</author>
<author>R W M Wedderburn</author>
</authors>
<title>Generalized linear models.</title>
<date>1972</date>
<journal>Journal of the Royal Statistical Society. SeriesA (General),</journal>
<volume>135</volume>
<issue>3</issue>
<contexts>
<context position="9893" citStr="Nelder and Wedderburn, 1972" startWordPosition="1726" endWordPosition="1729">Ei,j) = k · 1 P — (Ri,j E T|�� e i,j) = P (Ri,j E T |�� e i,j) = k · odds(Ri,j) where k is a constant (see (Snow et al., 2006)) that will be neglected in the maximization process. This last equation gives the possibility of using the logistic regression as it is. In the next sections we will see how SVD and the related feature selection can be used to compute the odds. 3.2 Logistic Regression Logistic Regression (Cox, 1958) is a particular type of statistical model for relating responses Y to linear combinations of predictor variables X. It is a specific kind of Generalized Linear Model (see (Nelder and Wedderburn, 1972)) where its function is the logit function and the independent variable Y is a binary or dicothomic variable which has a Bernoulli distribution. The dependent variable Y takes value 0 or 1. The probability that Y has value 1 is function of the regressors x = (1, x1, ..., xk). The probabilistic taxonomy learner model introduced in the previous section falls in the category of probabilistic models where the logistic regression can be applied as Ri,j E T is the binary dependent variable and e i,j is the vector of its regressors. In the rest of the section we will see how the odds, i.e., the multi</context>
</contexts>
<marker>Nelder, Wedderburn, 1972</marker>
<rawString>J. A. Nelder and R. W. M. Wedderburn. 1972. Generalized linear models. Journal of the Royal Statistical Society. SeriesA (General), 135(3):370–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donie O’Sullivan</author>
<author>A McElligott</author>
<author>Richard F E Sutcliffe</author>
</authors>
<title>Augmenting the princeton wordnet with a domain specific ontology.</title>
<date>1995</date>
<booktitle>In Proceedings of the Workshop on Basic Issues in Knowledge Sharing at the 14th International Joint Conference on Artificial Intelligence.</booktitle>
<location>Montreal, Canada.</location>
<marker>O’Sullivan, McElligott, Sutcliffe, 1995</marker>
<rawString>Donie O’Sullivan, A. McElligott, and Richard F. E. Sutcliffe. 1995. Augmenting the princeton wordnet with a domain specific ontology. In Proceedings of the Workshop on Basic Issues in Knowledge Sharing at the 14th International Joint Conference on Artificial Intelligence. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>113--120</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1294" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="175" endWordPosition="178"> knowledge repositories in a variety of applications for natural language processing and knowledge representation. Yet, manually built taxonomies such as WordNet (Miller, 1995) often lack in coverage when used in specific knowledge domains. Automatically creating or extending taxonomies for specific domains is then a very interesting area of research (O’Sullivan et al., 1995; Magnini and Speranza, 2001; Snow et al., 2006). Automatic methods for learning taxonomies from corpora often use distributional hypothesis (Harris, 1964) and exploit some induced lexical-syntactic patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006). In these models, within a very large set, candidate word pairs are selected as new word pairs in hyperonymy and added to an existing taxonomy. Candidate pairs are represented in some feature space. Often, these feature spaces are huge and, then, models may take into consideration noisy features. In machine learning, feature selection has been often used to reduce the dimensions in huge feature spaces. This has many advantages, e.g., reducing the computational cost and improving performances by removing noisy features (Guyon and Elisseeff, 2003). In this paper, we propose a novel way to inPro</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 113–120, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Penrose</author>
</authors>
<title>A generalized inverse for matrices.</title>
<date>1955</date>
<booktitle>In Proc. Cambridge Philosophical Society.</booktitle>
<contexts>
<context position="14641" citStr="Penrose, 1955" startWordPosition="2626" endWordPosition="2627">; • X is a matrix n × m of the values of the regressors that we have observed; • Q is a column vector m × 1 of the regression coefficients; • E is a column vector including the stochastic components that have not been observed and that will not be considered later. In the case X is a rectangular and singular matrix, the system y = XQ has not a solution. Yet, it is possible to use the principle of the Least Square Estimation. This principle determines the solution Q that minimize the residual norm, i.e.: Q = arg min kXQ − yk2 (8) This problem can be solved by the MoorePenrose pseudoinverse X+ (Penrose, 1955). Then, the final equation to determine the Q is Q=X+y It is important to remark that if the inverse matrix exist X+ = X−1 and that X+X and XX+ are symmetric. For our case, the following equation is valid: Q = Q+−−−−−→ logit(p) 3.4 Computing Pseudoinverse Matrix with SVD Analysis We finally reached the point where it is possible to explain our idea that is naturally using singular value decomposition (SVD) as feature selection in a probabilistic taxonomy learner. In the previous sections we described how the probabilities of the taxonomy learner can be estimated using logistic regressions and </context>
</contexts>
<marker>Penrose, 1955</marker>
<rawString>R. Penrose. 1955. A generalized inverse for matrices. In Proc. Cambridge Philosophical Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold R Robison</author>
</authors>
<title>Computer-detectable semantic structures.</title>
<date>1970</date>
<journal>Information Storage and Retrieval,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="6307" citStr="Robison, 1970" startWordPosition="1066" endWordPosition="1067">the final classes of the training examples. This is not always the case, feature selection models such as those based on Information Gain largely use the final classes of training examples. SVD as feature selection is independent from the classification problem. 3 Probabilistic Taxonomy Learning and SVD feature selection Recently, Snow et al. (2006) introduced a probabilistic model for learning taxonomies form corpora. This probabilistic formulation exploits the two well known hypotheses: the distributional hypothesis (Harris, 1964) and the exploitation of the lexico-syntactic patterns as in (Robison, 1970; Hearst, 1992). Yet, in this formulation, we can positively and naturally introduce our use of SVD as feature selection model. In the rest of this section we will firstly introduce the probabilistic model (Sec. 3.1) and, then, we will describe how SVD is used as feature selector in the logistic regression that estimates the probabilities of the model. To describe this part we need to go in depth into the definition of the logistic regression (Sec. 3.2) and the way of estimating the regression coefficients (Sec. 3.3). This will open the possibility of describing how we exploit SVD (Sec. 3.4) 3</context>
</contexts>
<marker>Robison, 1970</marker>
<rawString>Harold R. Robison. 1970. Computer-detectable semantic structures. Information Storage and Retrieval, 6(3):273–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence. In</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>801--808</pages>
<contexts>
<context position="1088" citStr="Snow et al., 2006" startWordPosition="148" endWordPosition="151">ction of singular value decomposition (SVD). Experiments show that this way of using SVD for feature selection positively affects performances. 1 Introduction Taxonomies are extremely important knowledge repositories in a variety of applications for natural language processing and knowledge representation. Yet, manually built taxonomies such as WordNet (Miller, 1995) often lack in coverage when used in specific knowledge domains. Automatically creating or extending taxonomies for specific domains is then a very interesting area of research (O’Sullivan et al., 1995; Magnini and Speranza, 2001; Snow et al., 2006). Automatic methods for learning taxonomies from corpora often use distributional hypothesis (Harris, 1964) and exploit some induced lexical-syntactic patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006). In these models, within a very large set, candidate word pairs are selected as new word pairs in hyperonymy and added to an existing taxonomy. Candidate pairs are represented in some feature space. Often, these feature spaces are huge and, then, models may take into consideration noisy features. In machine learning, feature selection has been often used to reduce the dimensions in huge fea</context>
<context position="2498" citStr="Snow et al., 2006" startWordPosition="373" endWordPosition="376">l way to inProceedins the EAC 2009 Worksho on EMS: GEo clud ` depe 9 I probabilistic taxonomy learning models. Given the probabilistic taxonomy learning model introduced by (Snow et al., 2006), we leverage on the computation of logistic regression to exploit singular value decomposition (SVD) as unsupervised feature selection. SVD is used to compute the pseudo-inverse matrix needed in logistic regression. To describe our idea, we firstly review how SVD can be used as unsupervised feature selection (Sec. 2). In Section 3 we then describe the probabilistic taxonomy learning model introduced by (Snow et al., 2006). We will then shortly review the logistic regression used to compute the taxonomy learning model to describe where SVD can be naturally used. We will describe our experiments in Sec. 4. Finally, we will draw some conclusions and describe our future work (Sec. 5). 2 Unsupervised feature selection with Singular Value Decomposition Singular value decomposition (SVD) is one of the possible factorization of a rectangular matrix that has been largely used in information retrieval for reducing the dimension of the document vector space (Deerwester et al., 1990). The decomposition can be defined as f</context>
<context position="6045" citStr="Snow et al. (2006)" startWordPosition="1027" endWordPosition="1030">ture selection and filtering as we can decide in advance how many features or, better, linear combination of original features we want to use. As feature selection model, SVD is unsupervised in the sense that the feature selection is done without taking into account the final classes of the training examples. This is not always the case, feature selection models such as those based on Information Gain largely use the final classes of training examples. SVD as feature selection is independent from the classification problem. 3 Probabilistic Taxonomy Learning and SVD feature selection Recently, Snow et al. (2006) introduced a probabilistic model for learning taxonomies form corpora. This probabilistic formulation exploits the two well known hypotheses: the distributional hypothesis (Harris, 1964) and the exploitation of the lexico-syntactic patterns as in (Robison, 1970; Hearst, 1992). Yet, in this formulation, we can positively and naturally introduce our use of SVD as feature selection model. In the rest of this section we will firstly introduce the probabilistic model (Sec. 3.1) and, then, we will describe how SVD is used as feature selector in the logistic regression that estimates the probabiliti</context>
<context position="8136" citStr="Snow et al., 2006" startWordPosition="1401" endWordPosition="1404">ties are then: (1) the prior probability P(Ri,j E T) of an assertion Ri,j to belong to the taxonomy T and (2) the posterior probability P(Ri,j E T|e i,j) of an assertion Ri,j to belong to the taxonomy T given a set of evidences a i,j derived from the corpus. Evidences is a feature vector associated with a pair (i, j). For examples, a feature may describe how many times i and j are seen in patterns like ”i as j” or ”i is a j”. These among many other features are indicators of an is-a relation between i and j (see (Hearst, 1992)). Given a set of evidences E over all the relevant word pairs, in (Snow et al., 2006), the probabilistic taxonomy learning task is defined as the problem of finding the taxonomy T that maximizes the 67 probability of having the evidences E, i.e.: T = arg max T In (Snow et al., 2006), this maximization problem is solved with a local search. What is maximized at each step is the increase of the probability P (E|T) of the taxonomy when the taxonomy changes from T to T&apos; = T U N where N are the relations added at each step. This increase of probabilities is defined as multiplicative change 0(N) as follows: 0(N) = P(E|T&apos;)/P(E|T) (2) The main innovation of the model in (Snow et al., </context>
<context position="9391" citStr="Snow et al., 2006" startWordPosition="1644" endWordPosition="1647"> each step the best relation N = {Ri,j} as well as N = I(Ri,j) that is Ri,j with all the relations by the existing taxonomy. We will then experiment with our feature selection methodology in the two different models: flat: at each iteration step, a single relation is added, i.e. �Ri,j = arg maxRi,j A(Ri,j) inductive: at each iteration step, a set of relations is added, i.e. I(�Ri,j) where �Ri,j = argmaxRi,j A(I(Ri,j)). The last important fact is that it is possible to demonstrate that �(Ei,j) = k · 1 P — (Ri,j E T|�� e i,j) = P (Ri,j E T |�� e i,j) = k · odds(Ri,j) where k is a constant (see (Snow et al., 2006)) that will be neglected in the maximization process. This last equation gives the possibility of using the logistic regression as it is. In the next sections we will see how SVD and the related feature selection can be used to compute the odds. 3.2 Logistic Regression Logistic Regression (Cox, 1958) is a particular type of statistical model for relating responses Y to linear combinations of predictor variables X. It is a specific kind of Generalized Linear Model (see (Nelder and Wedderburn, 1972)) where its function is the logit function and the independent variable Y is a binary or dicothomi</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and A. Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In In ACL, pages 801–808.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>