<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.7094325">
Conceptual Language Models for Dialog systems
Renato De Mori
LIA CNRS BP 1228
84911 Avignon Cedex 9 - France
</note>
<email confidence="0.629802">
renato.demori, @lia.univ-avignon.fr
</email>
<note confidence="0.491961333333333">
Frederic Béchet
LIA CNRS BP 1228
84911 Avignon Cedex 9 - France
</note>
<email confidence="0.914969">
frederic.bechet, @lia.univ-avignon.fr
</email>
<sectionHeader confidence="0.975682" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991230862068966">
The purpose of computer speech understanding is to
find conceptual representations from signs coded into
the speech signal.
Contrary to speech interpretation by humans in which
the same discourse may be interpreted differently by
different subjects, for practical applications of computer
understanding the result of interpretation should be
unique for a given signal. Usually it is represented by an
object which is an instance of class corresponding to a
semantic structure which can be fairly complex even if
it is built with instances of conceptual constituents be-
longing to a small set of major ontological categories.
The mapping process that leads to a semantic interpreta-
tion can be derived manually because human interpreta-
tion of sentences can be completely explained with a
logical formalism or it can be inferred by machine
learning algorithms in order to ensure a large coverage
of possible sentence patterns. Theories and practical
implementations of these approaches are proposed in
[1],[2].
Limitations of coverage in the manual approach and in
precision of machine learning can be reduced by making
manually a detailed analysis of a limited number of ex-
amples and generalizing each analysis with automatic
methods. In particular, a well structured lexicon can be
very useful, in which the meaning of words is repre-
sented together with suggestions of possible syntactic
and conceptual structures.
Word associations found with networks of word rela-
tions [3] can also be useful for suggesting compositions
of semantic constituents into conceptual structures.
Thus, given an observed example, other examples can
be manually derived and generalized automatically.
(LMs) could be adapted based on expectations of con-
cepts predicted by a system belief. With this perspec-
tive, it is important to notice that, while the observation
of only certain words may be sufficient for hypothesiz-
ing a conceptual structure, complete details of word
phrases expressing a conceptual structure have to be
known in order to adapt a generic LM to the expectation
of such a structure.
This paper introduces a search method and a learning
paradigm based on the just introduced considerations.
The search engine built with this method finds the best
common path between the system knowledge repre-
sented by the composition of Stochastic Finite State
Transducers (SFST) and a Stochastic Finite State
Automaton (SFSA) representing the lattice of word hy-
potheses generated by an Automatic Speech Recogni-
tion System (ASR).
2 Hypothesis evaluation and search
Let a dialogue system have a belief which generates
expectations B about conceptual structures.
Expectation uncertainty is represented by a probability
distribution P(B) which is non-zero for a set of concep-
tual structures expected at a given time. Thus for a gen-
eral concept structure Γ and a description Y of the
speech signal, one gets:
</bodyText>
<equation confidence="0.99866305">
Γ =
* arg max P(  |Y ) arg
Γ =
Γ Γ
Y )
P( , Y , B )
= ∑ Γ
= ∑ Γ
P (W , , Y , B)
W
max P( Y  |W) P (W , , B)
Γ
W
arg
max{ P(Y  |W ) P (W , , B) }
Γ
max P ( , Y )
Γ
P ( , Y , B)
Γ
</equation>
<bodyText confidence="0.9998688">
Computer understanding of a spoken sentences is prob-
lem solving activity whose central engine is a search
process involving various types of models.
Searching for concepts can be combined with searching
for words. This suggests that statistical language models
</bodyText>
<equation confidence="0.833933461538461">
Γ,
P(
Γ
* ≈
≈ max P (
B
≈
(1)
P(W  |B) P (B )
Γ,
W , B
P(Γ, W , B ) = P( Γ
B
</equation>
<bodyText confidence="0.862952">
A general concept structure Γ can be represented as a
string of parenthesized terminals and non-terminals.
</bodyText>
<equation confidence="0.5521596">
Γ,
Y , B )
|
BW
)
</equation>
<bodyText confidence="0.999363866666667">
These expressions can be decomposed into chunks. A
sentence may contain only one or more chunks of an
incomplete structure, Thus, a system should be able to
generate interpretation hypotheses about parts of a con-
ceptual structure. In this case, symbol Γ makes refer-
ence only to a set of components.
Probability P(Γ|BW) can be simply set equal to 0 for a
conceptual structure which cannot be inferred from W.
If the conceptual structure is part of the expectations of
system beliefs and can be inferred unambiguously from
W, then P(Γ|BW) as in many practical applications in-
cluding the one considered in this paper, then P(Γ|BW).
Otherwise, let [ c 1... c γ.... c Γ ] be the sequence of con-
cept symbols corresponding to the preterminal symbols
in Γ. Probability P(Γ|BW) can be expressed as follows:
</bodyText>
<equation confidence="0.988839">
P(  |BW)
Γ
P(c  |BW )
1
At least, for some values of
the probability
γ
P{cγ|[c1 ...cγ−1]BW} is one for a class of applica-
</equation>
<bodyText confidence="0.935357909090909">
tions.
Let
be the set of conceptual components, chunks of
them or conceptual structures known to the system. Ex-
pectations derived from the system belief can be
grouped into a set
Let B2 the complement of
w.r.t.
and F be a filler structure representing all the
conceptual structures not in the application or just ig-
nored by ignorance of the system knowledge.
</bodyText>
<equation confidence="0.737561142857143">
B2
and F are
Φ
B1.
B1
Φ
B1,
</equation>
<bodyText confidence="0.928348">
the possible values for B in the (1) and their
probabilities P(B) can be established subjectively or by
evaluating counts for user responses consistent with the
belief, consistent with the application but not with the
belief and inconsistent with the application knowledge.
Probability
is that of an LM which is adapted to
the system belief. It can be obtained with an
</bodyText>
<equation confidence="0.5842735">
P(W|B)
LM built in
</equation>
<bodyText confidence="0.5648036">
the following way.
Each conceptual structure or part of it
is represented
Γ
N(Γ).
</bodyText>
<footnote confidence="0.60222975">
P(B1). A similar structure is built for
the automata corresponding to structures in B2. A filler
F is also considered containing a network derived by a
trigram LM. A network
</footnote>
<bodyText confidence="0.9520445">
is obtained by the con-
catenation of finite-state automata
inferre
a probability
</bodyText>
<equation confidence="0.795099">
N(Γ)
C(Γ)
</equation>
<bodyText confidence="0.910096222222222">
d with
the procedure described in the next section representing
chunks of knowledge with fillers F. These automata
output components of conceptual structures.
all the
in parallel an
N(Γ)
d dynamically assigning each
network of B1 a probability :
</bodyText>
<equation confidence="0.9986505">
P ( B 1 ) (3)
P [ N ( Γ )] =
</equation>
<bodyText confidence="0.997044111111111">
where
1 indicates the number of elements in
Probabilities of networks in B2 are assigned in a similar
way.
A word sequence W always corresponds to a path in F
and may correspond to one or more conceptual struc-
tures represented by paths in networks in B1 and B2. In
the second case, the likelihood of W in F will be much
lower than
</bodyText>
<equation confidence="0.7668625">
B
B1.
</equation>
<bodyText confidence="0.998356">
the likelihood in B1 or B2 because phrases
recognized by the chunk automata of the network are
boosted as it will be shown later. Thus the best path for
W, in this case, will go through a network whose auto-
mata produce as output the components of a conceptual
structure.
</bodyText>
<sectionHeader confidence="0.996454" genericHeader="categories and subject descriptors">
3 Knowledge inference
</sectionHeader>
<bodyText confidence="0.963360176470588">
Usually, when an application is developed, an even
small training corpus is available.
for an application. They can
be modified when the ap-
plication is deployed in order to correct errors or add
missing constituents.
words and semantic features is part
of the semantic
knowledge of the system.
(2)
by a finite-state network
All the networks corresponding to structures in B1 are
connected in parallel in a single structure with associ-
ated
A search is performed by finding the most likely com-
mon path in the network and in the automaton derived
from a lattice of word hypotheses generated by the
speech recognizer with the generic trigram LM. System
belief make vary the topology of the network by dy-
namically changing the composition of sets B1 and B2.
Network recompilation can be avoided by just putting
Semantic categories and functions are manually derived
A number of words in the lexicon have lexical entries
containing their syntactic category, syntactic constructs
which can appear in the same sentence, semantic fea-
tures and constructs they can be part of. When one of
these words is encountered in the training corpus, it is
considered as a trigger for the semantic categories con-
tained in its lexical entry. The association between
The presence of a category in the sentence under analy-
sis can be verified manually or by deriving it from the
parse tree of the sentence. As lexical entries, grammars
and rules for deriving semantic structures fr
om parse
</bodyText>
<figure confidence="0.76383315">
. . .
γ
c
=
=
] |
BW)
c 1
. . . .
c Γ
P([
P c  |[c . . .c ] BW
{ }
γ 1 γ − 1
∏
γ
Γ
2
=
B1
</figure>
<bodyText confidence="0.992833222222222">
2 Set the lexical entries for the words that are semanti-
cally relevant for the application.
trees may be imprecise or incomplete, a single example
can be carefully examined and validated manually.
Once a single example is available with a detailed syn-
tactic and semantic analysis, it can be generalized. A
sentence may contain a complete or partial semantic
structure or just one component concept. Let F represent
such a semantic interpretation. Furthermore, each struc-
ture may correspond to a pattern made of phrases and
fillers of the sentence represented by a sequence of
words W. Semantic Classification Trees (SCT) pro-
posed in [1] can be used for automatically deriving sen-
tence patterns corresponding to conceptual structures.
The purpose of learning is to build or modify a SFST
that accepts a sequence of words and output a semantic
interpretation F.
The initial analysis of an example starts by using a tag-
ger for replacing words with their preterminal syntactic
categories.
Then, semantic tags are automatically associated with
sequences of syntactic tags manually or using the se-
mantic knowledge. A tag expression made of syntactic
and semantic tags is obtained in this way as a represen-
tation for of F. As a by-product, expressions for the
constituents of and components of F are built and
added to the semantic knowledge.
Generalization of the example uses a phrase generator to
produce sequences of words from the tag expression.
These sequences of words enrich the finite state transla-
tor which has to map word sequences into the concep-
tual structure F.
Further generalization can be obtained by inferring
synonyms with a WordNet. If generalization has pro-
vided erroneous sequences of words, these sequences
can be removed by manual inspection or when it is ob-
served that the system has made an interpretation error
because of them. With a similar procedure, new se-
quences of words can be added to the automaton for F.
Once it has been found that a word (noun or verb) con-
tributed to hypothesize a concept in the semantic struc-
ture, the concept is added as semantic feature in the
lexical entry of the word.
In summary learning of semantic knowledge follows the
following steps:
</bodyText>
<listItem confidence="0.847627714285714">
1 Set the semantic categories for the application.
3 For every analyzed sentence
• if semantic interpretation is correct then do
nothing,
• if a phrase is misplaced in the representation of
a semantic structure then remove it,
• if a phrase is missed in the representation of a
</listItem>
<bodyText confidence="0.98164175">
semantic structure, but the corresponding tag
expressions is present in the semantic knowl-
edge, then the phrase is added to the corre-
sponding SFST,
• if the tag expression does not exist in the se-
mantic knowledge, then it is built and se-
quences of words are generated from it with
the above outlined generalization procedure.
A set of SFST is built in this way. They are added to the
LM to provide concept specific components and to pro-
duce semantic interpretations at the same time with a
translation process.
</bodyText>
<sectionHeader confidence="0.999275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.949146428571429">
[1] Kuhn R. and De Mori R. (1995). The Application of
Semantic Classification Trees to Natural Language Un-
derstanding. IEEE Trans. on Pattern Analysis and Ma-
chine Intelligence, 17 : 449-460.
[2] Pieraccini R., Levin E., and Lee C.-H. (1991). Sto-
chastic Representation of Conceptual Structure in the
ATIS Task. Proceedings of the, 1991 Speech and Natu-
ral Language Workshop, 121-124, Morgan Kaufmann
publ, Los Altos, CA.
[3] Vossen P. Diez-Orzas P. and Peters W., (1997)
The multilingual design of EuroWordnet.
Proc ACL/EACL workshop on automatic information
extraction and building of lexical semantic resources for
NLP applications, Madrid, 1997.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931050">
<title confidence="0.999818">Conceptual Language Models for Dialog systems</title>
<author confidence="0.99786">Renato De</author>
<affiliation confidence="0.993572">LIA CNRS BP</affiliation>
<address confidence="0.999192">84911 Avignon Cedex 9 - France</address>
<email confidence="0.997503">renato.demori,@lia.univ-avignon.fr</email>
<author confidence="0.959941">Frederic</author>
<affiliation confidence="0.997105">LIA CNRS BP</affiliation>
<address confidence="0.998935">84911 Avignon Cedex 9 - France</address>
<email confidence="0.983951">frederic.bechet,@lia.univ-avignon.fr</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R De Mori</author>
</authors>
<title>The Application of Semantic Classification Trees to Natural Language Understanding.</title>
<date>1995</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence,</journal>
<volume>17</volume>
<pages>449--460</pages>
<contexts>
<context position="1263" citStr="[1]" startWordPosition="191" endWordPosition="191">an object which is an instance of class corresponding to a semantic structure which can be fairly complex even if it is built with instances of conceptual constituents belonging to a small set of major ontological categories. The mapping process that leads to a semantic interpretation can be derived manually because human interpretation of sentences can be completely explained with a logical formalism or it can be inferred by machine learning algorithms in order to ensure a large coverage of possible sentence patterns. Theories and practical implementations of these approaches are proposed in [1],[2]. Limitations of coverage in the manual approach and in precision of machine learning can be reduced by making manually a detailed analysis of a limited number of examples and generalizing each analysis with automatic methods. In particular, a well structured lexicon can be very useful, in which the meaning of words is represented together with suggestions of possible syntactic and conceptual structures. Word associations found with networks of word relations [3] can also be useful for suggesting compositions of semantic constituents into conceptual structures. Thus, given an observed exam</context>
<context position="8915" citStr="[1]" startWordPosition="1568" endWordPosition="1568"> words that are semantically relevant for the application. trees may be imprecise or incomplete, a single example can be carefully examined and validated manually. Once a single example is available with a detailed syntactic and semantic analysis, it can be generalized. A sentence may contain a complete or partial semantic structure or just one component concept. Let F represent such a semantic interpretation. Furthermore, each structure may correspond to a pattern made of phrases and fillers of the sentence represented by a sequence of words W. Semantic Classification Trees (SCT) proposed in [1] can be used for automatically deriving sentence patterns corresponding to conceptual structures. The purpose of learning is to build or modify a SFST that accepts a sequence of words and output a semantic interpretation F. The initial analysis of an example starts by using a tagger for replacing words with their preterminal syntactic categories. Then, semantic tags are automatically associated with sequences of syntactic tags manually or using the semantic knowledge. A tag expression made of syntactic and semantic tags is obtained in this way as a representation for of F. As a by-product, exp</context>
</contexts>
<marker>[1]</marker>
<rawString>Kuhn R. and De Mori R. (1995). The Application of Semantic Classification Trees to Natural Language Understanding. IEEE Trans. on Pattern Analysis and Machine Intelligence, 17 : 449-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pieraccini</author>
<author>E Levin</author>
<author>C-H Lee</author>
</authors>
<date>1991</date>
<booktitle>Stochastic Representation of Conceptual Structure in the ATIS Task. Proceedings of the, 1991 Speech and Natural Language Workshop,</booktitle>
<pages>121--124</pages>
<publisher>Morgan Kaufmann</publisher>
<location>publ, Los Altos, CA.</location>
<contexts>
<context position="1267" citStr="[2]" startWordPosition="191" endWordPosition="191">bject which is an instance of class corresponding to a semantic structure which can be fairly complex even if it is built with instances of conceptual constituents belonging to a small set of major ontological categories. The mapping process that leads to a semantic interpretation can be derived manually because human interpretation of sentences can be completely explained with a logical formalism or it can be inferred by machine learning algorithms in order to ensure a large coverage of possible sentence patterns. Theories and practical implementations of these approaches are proposed in [1],[2]. Limitations of coverage in the manual approach and in precision of machine learning can be reduced by making manually a detailed analysis of a limited number of examples and generalizing each analysis with automatic methods. In particular, a well structured lexicon can be very useful, in which the meaning of words is represented together with suggestions of possible syntactic and conceptual structures. Word associations found with networks of word relations [3] can also be useful for suggesting compositions of semantic constituents into conceptual structures. Thus, given an observed example,</context>
</contexts>
<marker>[2]</marker>
<rawString>Pieraccini R., Levin E., and Lee C.-H. (1991). Stochastic Representation of Conceptual Structure in the ATIS Task. Proceedings of the, 1991 Speech and Natural Language Workshop, 121-124, Morgan Kaufmann publ, Los Altos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vossen P Diez-Orzas P</author>
<author>W Peters</author>
</authors>
<title>The multilingual design of EuroWordnet. Proc ACL/EACL workshop on automatic information extraction and building of lexical semantic resources for NLP applications,</title>
<date>1997</date>
<location>Madrid,</location>
<contexts>
<context position="1734" citStr="[3]" startWordPosition="265" endWordPosition="265">nsure a large coverage of possible sentence patterns. Theories and practical implementations of these approaches are proposed in [1],[2]. Limitations of coverage in the manual approach and in precision of machine learning can be reduced by making manually a detailed analysis of a limited number of examples and generalizing each analysis with automatic methods. In particular, a well structured lexicon can be very useful, in which the meaning of words is represented together with suggestions of possible syntactic and conceptual structures. Word associations found with networks of word relations [3] can also be useful for suggesting compositions of semantic constituents into conceptual structures. Thus, given an observed example, other examples can be manually derived and generalized automatically. (LMs) could be adapted based on expectations of concepts predicted by a system belief. With this perspective, it is important to notice that, while the observation of only certain words may be sufficient for hypothesizing a conceptual structure, complete details of word phrases expressing a conceptual structure have to be known in order to adapt a generic LM to the expectation of such a struct</context>
</contexts>
<marker>[3]</marker>
<rawString>Vossen P. Diez-Orzas P. and Peters W., (1997) The multilingual design of EuroWordnet. Proc ACL/EACL workshop on automatic information extraction and building of lexical semantic resources for NLP applications, Madrid, 1997.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>