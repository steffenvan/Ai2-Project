<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.992586">
Constructing parse forests that include exactly the n-best PCFG trees
</title>
<author confidence="0.980188">
Pierre Boullier1, Alexis Nasr2 and Benoit Sagot1
</author>
<affiliation confidence="0.5638705">
1. Alpage, INRIA Paris-Rocquencourt &amp; Universit´e Paris 7
Domaine de Voluceau Rocquencourt, BP 105 78153 Le Chesnay Cedex, France
</affiliation>
<email confidence="0.923289">
lPierre.Boullier,Benoit.Sagotl@inria.fr
</email>
<note confidence="0.764283">
2. LIF, Univ. de la M´editerrann´ee
163, avenue de Luminy - Case 901 13288 Marseille Cedex 9, France
</note>
<email confidence="0.985546">
Alexis.Nasr@lif.univ-mrs.fr
</email>
<sectionHeader confidence="0.99348" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965466666667">
This paper describes and compares two al-
gorithms that take as input a shared PCFG
parse forest and produce shared forests
that contain exactly the n most likely trees
of the initial forest. Such forests are
suitable for subsequent processing, such
as (some types of) reranking or LFG f-
structure computation, that can be per-
formed ontop of a shared forest, but that
may have a high (e.g., exponential) com-
plexity w.r.t. the number of trees contained
in the forest. We evaluate the perfor-
mances of both algorithms on real-scale
NLP forests generated with a PCFG ex-
tracted from the Penn Treebank.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965492063492">
The output of a CFG parser based on dynamic
programming, such as an Earley parser (Earley,
1970), is a compact representation of all syntac-
tic parses of the parsed sentence, called a shared
parse forest (Lang, 1974; Lang, 1994). It can rep-
resent an exponential number of parses (with re-
spect to the length of the sentence) in a cubic size
structure. This forest can be used for further pro-
cessing, as reranking (Huang, 2008) or machine
translation (Mi et al., 2008).
When a CFG is associated with probabilistic in-
formation, as in a Probabilistic CFG (PCFG), it
can be interesting to process only the n most likely
trees of the forest. Standard state-of-the-art algo-
rithms that extract the n best parses (Huang and
Chiang, 2005) produce a collection of trees, los-
ing the factorization that has been achieved by the
parser, and reproduce some identical sub-trees in
several parses.
This situation is not satisfactory since post-
parsing processes, such as reranking algorithms
or attribute computation, cannot take advantage
of this lost factorization and may reproduce some
identical work on common sub-trees, with a com-
putational cost that can be exponentally high.
One way to solve the problem is to prune the
forest by eliminating sub-forests that do not con-
tribute to any of the n most likely trees. But this
over-generates: the pruned forest contains more
than the n most likely trees. This is particularly
costly for post-parsing processes that may require
in the worst cases an exponential execution time
w.r.t. the number of trees in the forest, such as
LFG f-structures construction or some advanced
reranking techniques. The experiments detailed
in the last part of this paper show that the over-
generation factor of pruned sub-forest is more or
less constant (see 6): after pruning the forest so as
to keep the n best trees, the resulting forest con-
tains approximately 103n trees. At least for some
post-parsing processes, this overhead is highly
problematic. For example, although LFG parsing
can be achieved by computing LFG f-structures
on top of a c-structure parse forest with a reason-
able efficiency (Boullier and Sagot, 2005), it is
clear that a 103 factor drastically affects the overall
speed of the LFG parser.
Therefore, simply pruning the forest is not an
adequate solution. However, it will prove useful
for comparison purposes.
The new direction that we explore in this pa-
per is the production of shared forests that con-
tain exactly the n most likely trees, avoiding both
the explicit construction of n different trees and
the over-generation of pruning techniques. This
can be seen as a transduction which is applied on
a forest and produces another forest. The trans-
duction applies some local transformations on the
structure of the forest, developing some parts of
the forest when necessary.
The structure of this paper is the following. Sec-
tion 2 defines the basic objects we will be dealing
with. Section 3 describes how to prune a shared
</bodyText>
<page confidence="0.968854">
117
</page>
<note confidence="0.877855">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 117–128,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999916">
forest, and introduces two approaches for build-
ing shared forests that contain exactly the n most
likely parses. Section 4 describes experiments that
were carried out on the Penn Treebank and sec-
tion 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.990326" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.955487">
2.1 Instantiated grammars
</subsectionHeader>
<bodyText confidence="0.9978055">
Let G = hN, T , P, Si be a context-free grammar
(CFG), defined in the usual way (Aho and Ullman,
1972). Throughout this paper, we suppose that we
manipulate only non-cyclic CFGs,1 but they may
(and usually do) include E-productions. Given a
production p ∈ P, we note lhs(p) its left-hand
side, rhs(p) its right-hand side and |p |the length
of rhs(p). Moreover, we note rhsk(p), with 1 ≤
k ≤ |p|, the kth symbol of rhs(p). We call A-
production any production p ∈ P of G such that
</bodyText>
<equation confidence="0.635761">
lhs(p) = A.
A complete derivation of a sentence w =
t1 ... t|w |(∀i ≤ |w|, ti ∈ T) w.r.t. G is of the form
αX1X2 ... XrQ ⇒ ∗
G,w
</equation>
<bodyText confidence="0.987039026315789">
inition, A → X1X2 ... Xr is a production of G.
Each of A, X1, X2, ... , Xr spans a unique oc-
currence of a substring ti+1 ... tj of w, that can
be identified by the corresponding range, noted
i..j. A complete derivation represents a parse tree
whose yield is w, in which each symbol X of
range i..j roots a subtree whose yield is ti+1 ... tj
(i.e., a derivation of the form X ⇒∗
G,w
Let us define the w-instantiation operation (or
instantiation). It can be applied to symbols and
productions of G, and to G itself, w.r.t. a string
w. It corresponds to the well-known intersection
of G with the linear automaton that corresponds
to the string w. We shall go into further detail for
terminology, notation and illustration purposes.
An instantiated non terminal symbol is a triple
noted Ai..j where A ∈ N and 0 ≤ i ≤ j ≤ |w|.
Similarly, an instantiated terminal symbol is a
triple noted Ti..j where T ∈ T and 0 ≤ i ≤ j =
i + 1 ≤ |w|. An instantiated symbol, terminal or
non terminal, is noted Xi..j. For any instantiated
symbol Xi..j, i (resp. j) is called its lower bound
1Actually, cyclic CFG can be treated as well, but not
cyclic parse forests. Therefore, if using a cyclic CFG which,
on a particular sentence, builds a cyclic parse forest, cycles
have to be removed before the algorithms descibed in the next
sections are applied. This is the case in the SYNTAX system
(see below).
(resp. upper bound), and can be extracted by the
operator lb() (resp. ub()).
An instantiated production (or instantiated
rule) is a context-free production Ai..j →
X1i1..j1X2i2..j2 . . . Xrir..jr whose left-hand side is an
instantiated non terminal symbol and whose right-
hand side is a (possibly empty) sequence of in-
stantiated (terminal or non terminal) symbols, pro-
vided the followings conditions hold:
</bodyText>
<listItem confidence="0.9926582">
1. the indexes involved are such that i = i1, j =
jr, and ∀l such that 1 ≤ l &lt; r, jl = il+1;
2. the corresponding non-instantiated produc-
tion A → X1X2 ... Xr is a production of
G.
</listItem>
<bodyText confidence="0.966620136363636">
If lhs(p) = Ai..j, we set lb(p) = i and ub(p) = j.
In a complete derivation S ⇒∗
G,w
w, any symbol X that spans
the range i..j can be replaced by the instantiated
symbols Xi..j. For example, the axiom S can be
replaced by the instantiated axiom S0..|w |in the
head of the derivation. If applied to the whole
derivation, this operation creates an instantiated
derivation, whose rewriting operations define a
particular set of instantiated productions. Given
G and w, the set of all instantiated productions in-
volved in at least one complete derivation of w is
unique, and noted Pw. An instantiated derivation
represents an instantiated parse tree, i.e., a parse
tree whose node labels are instantiated symbols.
In an instantiated parse tree, each node label is
unique, and therefore we shall not distinguish be-
tween a node in an instantiated parse tree and its
label (i.e., an instantiated symbol).
Then, the w-instantiated grammar Gw for G
and w is a CFG hNw, Tw, Pw, S0..|w|i such that:
</bodyText>
<listItem confidence="0.945649285714286">
1. Pw is defined as explained above;
2. Nw is a set of instantiated non terminal sym-
bols;
3. Tw is a set of instantiated terminal symbols.
It follows from the definition of Pw that (instan-
tiated) symbols of Gw have the following prop-
erties: Ai..j ∈ Nw ⇔ A ⇒ ti+1 ... tj, and
</listItem>
<equation confidence="0.684768">
G,w
Ti..j ∈ Tw ⇔ T = tj.
</equation>
<bodyText confidence="0.998066">
The w-instantiated CFG Gw represents all parse
trees for w in a shared (factorized) way. It is the
grammar representation of the parse forest of w
</bodyText>
<equation confidence="0.9368445">
S ⇒ ∗ αAQ ⇒
G,w G,w
w. By def-
ti+1 ... tj).
αAQ ⇒
G,w
αX1X2 ... XrQ ⇒ ∗
G,w
</equation>
<page confidence="0.982379">
118
</page>
<bodyText confidence="0.999287777777778">
w.r.t. G.2 In fact, L(Gw) = {w} and the set
of parses of w with respect to Gw is isomorphic
to the set of parses of w with respect to G, the
isomorphism being the w-instantiation operation.
The size of a forest is defined as the size of the
grammar that represents it, i.e., as the number of
symbol occurrences in this grammar, which is de-
fined as the number of productions plus the sum of
the lengths of all right-hand sides.
</bodyText>
<subsectionHeader confidence="0.570214">
Example 1: First running example.
</subsectionHeader>
<bodyText confidence="0.999797">
Let us illustrate these definitions by an example.
Given the sentence w = the boy saw a man with a
telescope and the grammar G (that the reader has
in mind), the instantiated productions of Gw are:
</bodyText>
<equation confidence="0.999967444444444">
Det0..1 — the0..1 N1..2 — boy1..2
NP0..2 — Det0..1 N1..2 V2..3 — saw2..3
Det3..4 — a3..4 N4..5 — man4..5
NP3..5 — Det3..4 N4..5 Prep5..6 — with5..6
Det6..7 — a6..7 N7..8 — telescope7..8
NP6..8 — Det6..7 N7..8 PP5..8 — Prep5..6 NP6..8
NP3..8 — NP3..5 PP5..8 VP2..8 — V2..3 NP3..8
VP2..5 — V2..3 NP3..5 VP2..8 — VP2..5 PP5..8
S0..8 — NP0..2 VP2..8
</equation>
<bodyText confidence="0.965923285714286">
They represent the parse forest of w according to
G. This parse forest contains two trees, since there
is one ambiguity: VP2..8 can be rewritten in two
different ways.
The instantiated grammar Gw can be repre-
sented as an hypergraph (as in (Klein and Man-
ning, 2001) or (Huang and Chiang, 2005)) where
the instantiated symbols of Gw correspond to the
vertices of the hypergraph and the instantiated pro-
ductions to the hyperarcs.
We define the extension of an instantiated sym-
bol Xi..j, noted £(Xi..j), as the set of instantiated
parse trees that have Xi..j as a root. The set of all
parse trees of w w.r.t. G is therefore £(S0..|w|). In
the same way, we define the extension of an in-
stantiated production Xi..j — α to be the subset
of £(Xi..j) that corresponds to derivations of the
form Xi..j ==&gt;. α � ti+1 ... tj (i.e., trees rooted
G,w G,w
in Xi..j and where the daughters of the node Xi..j
are the symbols of α).
</bodyText>
<subsectionHeader confidence="0.999656">
2.2 Forest traversals
</subsectionHeader>
<bodyText confidence="0.949691692307692">
Let us suppose that we deal with non-cyclic
forests, i.e., we only consider forests that are rep-
2In particular, if G is a binary grammar, its w-instantation
(i.e., the parse forest of w) has a size O(|w|3), whereas it rep-
resents a potentially exponential number of parse trees w.r.t
|w |since we manipulate only non-cyclic grammars.
resented by a non-recursive instantiated CFG. In
this case, we can define two different kinds of for-
est traversals.
A bottom-up traversal of a forest is a traversal
with the following constraint: an Ai..j-production
is visited if and only if all its instantiated right-
hand side symbols have already been visited; the
instantiated symbol Ai..j is visited once all Ai..j-
productions have been visited. The bottom-up
visit starts by visiting all instantiated productions
with right-hand sides that are empty or contain
only (instantiated) terminal symbols.
A top-down traversal of a forest is a traversal
with the following constraint: a node Ai..j is vis-
ited if and only if all the instantiated productions
in which it occurs in right-hand side have already
been visited; once an instantiated production Ai..j
has been visited, all its Ai..j-productions are vis-
ited as well. Of course the top-down visit starts by
the visit of the axiom S0..|w|.
</bodyText>
<subsectionHeader confidence="0.994174">
2.3 Ranked instantiated grammar
</subsectionHeader>
<bodyText confidence="0.999318814814815">
When an instantiated grammar Gw =
(Nw,Tw,Pw,S0..|w|) is built on a PCFG, ev-
ery parse tree in £(S0..|w|) has a probability that
is computed in the usual way (Booth, 1969). We
might be interested in extracting the kth most
likely tree of the forest represented by Gw,3 with-
out unfolding the forest, i.e., without enumerating
trees. In order to do so, we need to add some
extra structure to the instantiated grammar. The
augmented instantiated grammar will be called a
ranked instantiated grammar.
This extra structure takes the form of n-best ta-
bles that are associated with each instantiated non
terminal symbol (Huang and Chiang, 2005), thus
leading to ranked instantiated non terminal sym-
bols, or simply instantiated symbols when the con-
text is non ambiguous. A ranked instantiated non
terminal symbol is written (Ai..j, T (Ai..j)), where
T (Ai..j) is the n-best table associated with the in-
stantiated symbol Ai..j.
T (Ai..j) is a table of at most n entries. The
k-th entry of the table, noted e, describes how to
build the k-th most likely tree of £(Ai..j). This
tree will be called the k-th extention of Ai..j, noted
£k(Ai..j). More precisely, e indicates the instanti-
ated Ai..j-production p such that £k(Ai..j) E £(p).
It indicates furthermore which trees of the exten-
</bodyText>
<footnote confidence="0.9550045">
3In this paper, we shall use the kth most likely tree and the
tree of rank k as synonyms.
</footnote>
<page confidence="0.998312">
119
</page>
<bodyText confidence="0.9407725">
sions of p’s right-hand side symbols must be com-
bined together in order to build Ek(Ai..j).
We also define the m, n-extension of Ai..j as
follows: Em,n(Ai..j) = ∪m≤k≤nEk(Ai..j).
Example 2: n-best tables for the first running
example.
Let us illustrate this idea on ourfirst running ex-
ample. Recall that in Example 1, the symbol VP2..8
can be rewritten using the two following produc-
tions :
</bodyText>
<equation confidence="0.9995776">
VP2..8 → V2..3 NP3..8
VP2..8 → VP2..5 PP5..8
T (VP2..8) has the following form:
1 P1 VP2..8 → V2..3 NP3..8 h1, 1i 1
2 P2 VP2..8 → VP2..5 PP5..8 h1, 1i 1
</equation>
<bodyText confidence="0.893208925925926">
This table indicates that the most likely tree
associated with VP2..8 (line one) has probability
P1 and is built using the production VP2..8 →
V2..3 NP3..8 by combining the most likely tree of
E(V2..3) (indicated by thefirst 1 in h1, 1i) with the
most likely tree of E(NP3..8) (indicated by the sec-
ond 1 in h1,1i). It also indicates that the most
likely tree of E(VP2..8) is the most likely tree of
E(VP2..8 → V2..3 NP3..8) (indicated by the pres-
ence of 1 in the last column of entry 1) and the
second most likely tree of E(VP2..8) is the most
likely tree of E(VP2..8 → VP2..5 PP5..8). This last
integer is called the local rank of the entry.
More formally, the entry T (Ai..j)[k] is defined
as a 4-tuple hPk, pk, ~vk, lki where k is the rank
of the entry, Pk is the probability of the tree
Ek(Ai..j), pk is the instantiated production such
that Ek(Ai..j) ∈ E(pk), ~vk is a tuple of |rhs(pk)|
integers and lk is the local rank.
The tree Ek(Ai..j) is rooted by Ai..j, and its
daughters root N = |rhs(pk) |subtrees that are
E~vk[1](rhs1(pk)), . . . , E ~vk[N](rhsN(pk)).
Given an instantiated symbol Ai..j and an in-
stantitated production p ∈ P(Ai..j), we define
the n-best table of p to be the table composed
of the entries hPk, pk, ~vk, lki of T (Ai..j) such that
pk = p.
</bodyText>
<subsectionHeader confidence="0.601599">
Example 3: Second running example.
</subsectionHeader>
<bodyText confidence="0.849645">
The following is a standard PCFG (probabili-
ties are shown next to the corresponding clauses).
</bodyText>
<figure confidence="0.719668769230769">
S → A B 1
A → A1 0.7 A1 → a 1
A → A2 0.3 A2 → a 1
B → B1 0.6 B1 → b 1
B → B2 0.4 B2 → b 1
The instantiation of the underlying (non-
probabilistic) CFG grammar by the input text
w = a b is the following.
S1..3 → A1..2 B2..3
A1..2 → A11..2 A11..2 → a1..2
A1..2 → A21..2 A21..2 → a1..2
B2..3 → B12..3 B12..3 → b2..3
B2..3 → B22..3 B22..3 → b2..3
</figure>
<bodyText confidence="0.873460466666667">
This grammar represents a parse forest that con-
tains four different trees, since on the one hand one
can reach (parse) the instantiated terminal symbol
a1..2 through A1 or A2, and on the other hand one
can reach (parse) the instantiated terminal sym-
bol b1..2 through B1 or B2. Therefore, when dis-
cussing this example in the remainder of the paper,
each of thesefour trees will be named accordingly:
the tree obtained by reaching a through Ai and b
through Bj (i and j are 1 or 2) shall be called
Ti,j.
The corresponding n-best tables are trivial
(only one line) for all instantiated symbols but
A1..2, B2..3 and S1..3. That of A1..2 is the follow-
ing 2-line table.
</bodyText>
<listItem confidence="0.4938445">
1 0.7 A → A1 h1i 1
2 0.3 A → A2 h1i 1
</listItem>
<bodyText confidence="0.761702">
The n-best table for B2..3 is similar. The n-best
table for S1..3 is:
</bodyText>
<table confidence="0.75738">
1 0.42 S1..3 → A1..2 B2..3 h1,1i 1
2 0.28 S1..3 → A1..2 B2..3 h1,2i 2
3 0.18 S1..3 → A1..2 B2..3 h2,1i 3
4 0.12 S1..3 → A1..2 B2..3 h2,2i 4
</table>
<bodyText confidence="0.986037642857143">
Thanks to the algorithm sketched in section 2.4,
these tables allow to compute the following obvi-
ous result: the best tree is T1,1, the second-best
tree is T1,2, the third-best tree is T2,1 and the worst
tree is T2,2.
If n = 3, the pruned forest over-generates: all
instantiated productions take part in at least one
of the three best trees, and therefore the pruned
forest is the full forest itself, which contains four
trees.
We shall use this example later on so as to il-
lustrate both methods we introduce for building
forests that contain exactly the n best trees, with-
out overgenerating.
</bodyText>
<subsectionHeader confidence="0.998197">
2.4 Extracting the kth-best tree
</subsectionHeader>
<bodyText confidence="0.999954666666667">
An efficient algorithm for the extraction of the n-
best trees is introduced in (Huang and Chiang,
2005), namely the authors’ algorithm 3, which
</bodyText>
<page confidence="0.970545">
120
</page>
<bodyText confidence="0.9848474">
is a re-formulation of a procedure originally pro-
posed by (Jim´enez and Marzal, 2000). Contrar-
ily to (Huang and Chiang, 2005), we shall sketch
this algorithm with the terminology introduced
above (whereas the authors use the notion of hy-
pergraph). The algorithm relies on the n-best ta-
bles described above: extracting the kth-best tree
consists in extending the n-best tables as much as
necessary by computing all lines in each n-best ta-
ble up to those that concern the kth-best tree.4
The algorithm can be divided in two sub-
algorithms: (1) a bottom-up traversal of the for-
est for extracting the best tree; (2) a top-down
traversal for extracting the kth-best tree provided
the (k − 1)th-best has been already extracted.
The extraction of the best tree can be seen as a
bottom-up traversal that initializes the n-best ta-
bles: when visiting a node Ai..j, the best probabil-
ity of each Ai..j-production is computed by using
the tables associated with each of their right-hand
side symbols. The best of these probabilities gives
the first line of the n-best table for Ai..j (the result
for other productions are stored for possible later
use). Once the traversal is completed (the instanti-
ated axiom has been reached), the best tree can be
easily output by following recursively where the
first line of the axiom’s n-best table leads to.
Let us now assume we have extracted all k′-best
trees, 1 &lt; k′ &lt; k, for a given k &lt; n. We want
to extract the kth-best tree. We achieve this recur-
sively by a top-down traversal of the forest. In or-
der to start the construction of the kth-best tree, we
need to know the following:
• which instantiated production p must be used
for rewriting the instantiated axiom,
</bodyText>
<listItem confidence="0.8003356">
• for each of p’s right-hand side symbols Ai..j,
which subtree rooted in Ai..j must be used;
this subtree is identified by its local rank
kAi..j, i.e., the rank of its probability among
all subtrees rooted in Ai..j.
</listItem>
<bodyText confidence="0.99906725">
This information is given by the kth line of the n-
best table associated with the instantiated axiom.
If this kth line has not been filled yet, it is com-
puted recursively.5 Once the kth line of the n-best
</bodyText>
<footnote confidence="0.9941195">
4In the remainder of this paper, we shall use “extracting
the kth-best tree” as a shortcut for “extending the n-best ta-
bles up to what is necessary to extract the kth-best tree” (i.e.,
we do not necessarily really build or print the kth-best tree).
5Because the k − 1th-best tree has been computed, this n-
best table is filled exactly up to line k −1. The kth line is then
</footnote>
<bodyText confidence="0.999407">
table is known, i.e., p and all kAi..j’s are known,
the rank k is added to p’s so-called rankset, noted
ρ(p). Then, the top-down traversal extracts recur-
sively for each Ai..j the appropriate subtree as de-
fined by kAi..j. After having extracted the n-th
best tree, we know that a given production p is in-
cluded in the kth-best tree, 1 &lt; k &lt; n, if and only
if k E ρ(p).
</bodyText>
<sectionHeader confidence="0.728985" genericHeader="method">
3 Computing sub-forests that only
contain the n best trees
</sectionHeader>
<bodyText confidence="0.9999879">
Given a ranked instantiated grammar Gw, we are
interested in building a new instantiated grammar
which contains exactly the n most likely trees of
£(Gw). In this section, we introduce two algo-
rithms that compute such a grammar (or forest).
Both methods rely on the construction of new
symbols, obtained by decorating instantiated sym-
bols of Gw.
An empirical comparison of the two methods is
described in section 4. In order to evaluate the
size of the new constructed grammars (forests),
we consider as a lower bound the so-called pruned
forest, which is the smallest sub-grammar of the
initial instantiated grammar that includes the n
best trees. It is built simply by pruning produc-
tions with an empty rankset: no new symbols
are created, original instantiated symbols are kept.
Therefore, it is a lower bound in terms of size.
However, the pruned forest usually overgenerates,
as illustrated by Example 3.
</bodyText>
<subsectionHeader confidence="0.990089">
3.1 The ranksets method
</subsectionHeader>
<bodyText confidence="0.998881190476191">
The algorithm described in this section builds an
instantiated grammar Gnw by decorating the sym-
bols of Gw. The new (decorated) symbols have
the form Aρi..j where ρ is a set of integers called
a rankset. An integer r is a rank iff we have
1 &lt; r &lt; n.
The starting point of this algorithm is set of n-
best tables, built as explained in section 2.4, with-
out explicitely unfolding the forest.
computed as follows: while constructing the k′th-best trees
for each k′ between 1 and k−1, we have identified many pos-
sible rewritings of the instantiated axiom, i.e., many (produc-
tion, right-hand side local ranks) pairs; we know the proba-
bility of all these rewritings, although only some of them con-
situte a line of the instantiated axiom’s n-best table; we now
identify new rewritings, starting from known rewritings and
incrementing only one of their local ranks; we compute (re-
cursively) the probability of these newly identified rewritings;
the rewriting that has the best probability among all those that
are not yet a line of the n-best table is then added: it is its kth
line.
</bodyText>
<page confidence="0.991919">
121
</page>
<bodyText confidence="0.972352057692308">
A preliminary top-down step uses these n-best
tables for building a parse forest whose non-
terminal symbols (apart from the axiom) have the
form Aρi..j where p is a singleton {r}: the sub-
forest rooted in A{r}
i..j contains only one tree, that
of local rank r. Only the axiom is not decorated,
and remains unique. Terminal symbols are not af-
fected either.
At this point, the purpose of the algorithm is to
merge productions with identical right-hand sides,
whenever possible. This is achieved in a bottom-
up fashion as follows. Consider two symbols Aρ1
i..j
and Aρ2
i..j, which differ only by their underlying
ranksets. These symbols correspond to two dif-
ferent production sets, namely the set of all Aρ1
i..j-
productions (resp. Aρ2
i..j-productions). Each of
these production sets define a set of right-hand
sides. If these two right-hand side sets are iden-
tical we say that Aρ1
i..j and Aρ2
i..j are equivalent. In
that case introduce the rankset p = p1 U p2 and
create a new non-terminal symbol Aρi..j. We now
simply replace all occurrences of Aρ1
i..j and Aρ2
i..j
in left- and right-hand sides by Aρi..j. Of course
(newly) identical productions are erased. After
such a transformation, the newly created symbol
may appear in the right-hand side of productions
that now only differ by their left-hand sides; the
factorization spreads to this symbol in a bottom-
up way. Therefore, we perform this transforma-
tion until no new pair of equivalent symbols is
found, starting from terminal leaves and percolat-
ing bottom-up as far as possible.
Example 4: Applying the ranksets method to
the second running example.
Let us come back to the grammar ofExample 3,
and the same input text w = a b as before. As
in Example 3, we consider the case when we are
interested in the n = 3 best trees.
Starting from the instantiated grammar and the
n-best tables given in Example 3, the preliminary
top-down step builds the following forest (for clar-
ity, ranksets have not been shown on symbols that
root sub-forests containing only one tree):
</bodyText>
<equation confidence="0.999488583333333">
S1..3 — A{1}
1..2 B{1}
2..3
S1..3 — A{1}
1..2 B{2}
2..3
S1..3 — A{2} 1..2B{1}
2..3
1..2 — A11..2 A11..2 — a1..2
1..2 — A21..2 A21..2 — a1..2
2..3 — B12..3 B12..3 — b2..3
B{2} 2..3 — B22..3 B22..3 — b2..3
</equation>
<bodyText confidence="0.999535857142857">
In this example, the bottom-up step doesn’t fac-
torize out any other symbols, and this is therefore
the final output of the ranksets method. It con-
tains 2 more productions and 3 more symbols than
the pruned forest (which is the same as the origi-
nal forest), but it contains exactly the 3 best trees,
contrarily to the pruned forest.
</bodyText>
<subsectionHeader confidence="0.995313">
3.2 The rectangles method
</subsectionHeader>
<bodyText confidence="0.970643542857143">
In this section only, we assume that the grammar
G is binary (and therefore the forest, i.e., the gram-
mar Gw, is binary). Standard binarization algo-
rithms can be found in the litterature (Aho and Ull-
man, 1972).
The algorithm described in this section per-
forms, as the preceding one, a decoration of the
symbols of Gw. The new (decorated) symbols
have the form Ax,y
i..j, where x and y denote ranks
such that 1 &lt; x &lt; y &lt; n. The semantics of the
decoration is closely related to the x, y extention
of Ai..j, introduced in 2.3:
£(Ax,y
i..j) = £x,y(Ai..j)
It corresponds to ranksets (in the sense of the
previous section) that are intervals: Ax,y
i..j is equiv-
alent to the previous section’s A{x,x+1,...,y−1,y} . In
i..j
other words, the sub-forest rooted with Ax,y i..jcon-
tains exactly the trees of the initial forest, rooted
with Ai..j, which rank range from x to y.
The algorithm performs a top-down traversal of
the initial instantiated grammar Gw. This traver-
sal also takes as input two parameters x and y. It
starts with the symbol S0..|w |and parameters 1 and
n. At the end of the traversal, a new decorated for-
est is built which contains exactly n most likely
the parses. During the traversal, every instantiated
symbol Ai..j will give birth to decorated instanti-
ated symbols of the form Ax,y
i..j where x and y are
determined during the traversal. Two different ac-
tions are performed depending on whether we are
</bodyText>
<page confidence="0.995474">
122
</page>
<bodyText confidence="0.993841">
visiting an instantiated symbol or an instantiated
production.
</bodyText>
<subsectionHeader confidence="0.940542">
3.2.1 Visiting an instantiated symbol
</subsectionHeader>
<bodyText confidence="0.9994513">
When visiting an instantiated symbol Ai..j with
parameters x and y, a new decorated instan-
tiated symbol Ax,y
i,j is created and the traver-
sal continues on the instantiated productions of
P(Ai..j) with parameters that have to be com-
puted. These parameters depend on how the el-
ements of £x,y(Ai..j) are “distributed” among the
sets £(p) with p E P(Ai..j). In other words, we
need to determine xk’s and yk’s such that:
</bodyText>
<equation confidence="0.9969825">
U£x,y(Ai..j) = £xk,yk(pk)
pk∈P(Ai..j)
</equation>
<bodyText confidence="0.999790166666667">
The idea can be easily illustrated on an exam-
ple. Suppose we are visiting the instantiated sym-
bol Ai..j with parameters 5 and 10. Suppose also
that Ai..j can be rewritten using the two instanti-
ated productions p1 and p2. Suppose finally that
the 5 to 10 entries of 7 (Ai..j) are as follows6:
</bodyText>
<figure confidence="0.999248">
5 p1 4
6 p2 2
7 p2 3
8 p1 5
9 p2 4
10 p1 6
</figure>
<bodyText confidence="0.7961345">
This table says that £5(Ai..j) = £4(p1) i.e. the
5th most likely analysis of £(Ai..j) is the 4th most
likely analysis of £(p1) and £6(Ai..j) = £2(p2)
and so on. From this table we can deduce that:
</bodyText>
<equation confidence="0.99223">
£5,10(Ai..j) = £4,6(p1) U £2,4(p2)
</equation>
<bodyText confidence="0.9982485">
The traversal therefore continues on p1 and p2
with parameters 4, 6 and 2, 4.
</bodyText>
<subsectionHeader confidence="0.960201">
3.2.2 Visiting an instantiated production
</subsectionHeader>
<bodyText confidence="0.928377">
When visiting an instantiated production p of the
form Ai..j ­+ Bi..l Cl..j with parameters x and y,
a collection of q instantiated productions pr of the
,2rCform AZ,s ­+ BZlxl, with 1 &lt; r &lt; q,
</bodyText>
<equation confidence="0.731658">
r,y2 r
</equation>
<bodyText confidence="0.8297215">
are built, where the parameters x1r, x2r, y1r, y2r and
q have to be computed.
Once the parameters q and x1r, x2r, y1r, y2r with
1 &lt; r &lt; q, have been computed, the traversal
continues independently on Bi..l with parameters
x1r and x2r and on Cl..j with parameters y1r and y2 r.
</bodyText>
<footnote confidence="0.798692">
6Only the relevant part of the table have been kept in the
figure.
</footnote>
<bodyText confidence="0.9976675">
The computation of the parameters x1r, x2r, y1r
and y2r for 1 &lt; r &lt; q, is the most complex part of
the algorithm, it relies on the three notions of rect-
angles, q-partitions and n-best matrices, which are
defined below.
Given a 4-tuple of parameters x1r, x2r, y1r, y2r,
a rectangle is simply a pairing of the form
((x1r, x2r), (y1r, y2r)). A rectangle can be interpreted
as a couple of rank ranges : (x1r, y1r) and (x2r, y2r).
It denotes the cartesian product [x1r, x2 ]X[y1 r, y2 ].
</bodyText>
<equation confidence="0.809311">
r r
</equation>
<bodyText confidence="0.9940195">
Let ((x1 1,x2 1), (y1 1,y2 1)),...,((x1 q,x2 q), (y1 q, y2 q))
be a collection of q rectangles. It will be called a
q-partition of the instantiated production p iff the
following is true:
</bodyText>
<equation confidence="0.962823125">
£x,y(p) = U £ ( B
A 1 2 1 2
i..j i..l
�,y �r,xr Cyr,yr
1≤r≤q
To put it differently, this definition means that
((x11,x21), (y11,y21)),..., ((x1q,x2q), (y1q, y2q)) is a q
partition of p if any tree of £ (BZ l&apos;xr) combined
</equation>
<bodyText confidence="0.997621375">
with any tree of £(Cl �&apos;y2r) is a tree of £x,y(p) and,
conversely, any tree of £x,y(p) is the combination
of a tree of £ (BZ i&apos;xr) and a tree of £ (Cl,yr )
The n-best matrix associated with an instanti-
ated production p, introduced in (Huang and Chi-
ang, 2005), is merely a two dimensional represen-
tation of the n-best table of p. Such a matrix, rep-
resents how the n most likely trees of £(p) are
built. An example of an n-best matrix is repre-
sented in figure 1. This matrix says that the first
most likely tree of p is built by combining the
tree £1(Bi..l) with the tree £1(Cl..j) (there is a 1
in the cell of coordinate (1,1)). The second most
likely tree is built by combining the tree £1(Bi..l)
and £2(Cl..j) (there is a 2 in the cell of coordinate
(1, 2)) and so on.
</bodyText>
<figure confidence="0.938388875">
Cl..j
1 2 3 4 5 6
1 2 6 8 14 15
3 5 11 13 18 29
4 9 12 17 24 30
7 10 20 21 26 33
16 19 22 25 27 35
23 28 31 32 34 36
</figure>
<figureCaption confidence="0.999747">
Figure 1: n-best matrix
</figureCaption>
<bodyText confidence="0.978715">
An n-best matrix M has, by construction, the
remarkable following properties:
</bodyText>
<figure confidence="0.941365">
1
2
Bi..l 3
4
5
6
</figure>
<page confidence="0.910335">
123
</page>
<equation confidence="0.5829825">
M(i, y) &lt; M(x, y) Vi 1 &lt; i &lt; x
M(x,j) &lt; M(x, y) Vj 1 &lt; j &lt; y
</equation>
<bodyText confidence="0.89009">
Given an n-best matrix M of dimensions d =
X · Y and two integers x and y such that 1 &lt; x &lt;
y &lt; d, M can be decomposed into three regions:
</bodyText>
<listItem confidence="0.961023666666667">
• the lower region, composed of the cells
which contain ranks i with 1 &lt; i &lt; x
• the intermediate region, composed of the
cells which contain ranks i with x &lt; i &lt; y
• the upper region, composed of the cells
which contain ranks i such that y &lt; i &lt; d.
</listItem>
<bodyText confidence="0.974520666666667">
The three regions of the matrix of figure 1, for
x = 4 and y = 27 have been delimited with bold
lines in figure 2.
</bodyText>
<figure confidence="0.976249">
Cl..j
1 2 3 4 5 6
1 2 6 8 14 15
3 5 11 13 18 29
4 9 12 17 24 30
7 10 20 21 26 33
16 19 22 25 27 35
23 28 31 32 34 36
</figure>
<figureCaption confidence="0.986189666666667">
Figure 2: Decomposition of an n-best matrix into
a lower, an intermediate and an upper region with
parameters 4 and 27.
</figureCaption>
<bodyText confidence="0.998983631578947">
It can be seen that a rectangle, as introduced
earlier, defines a sub-matrix of the n-best matrix.
For example the rectangle ((2, 5), (2, 5)) defines
the sub-matrix which north west corner is M(2, 2)
and south east corner is M(5, 5), as represented in
figure 3.
When visiting an instantiated production p, hav-
ing M as an n-best matrix, with the two parame-
ters x and y, the intermediate region of M, with
respect to x and y, contains, by definition, all the
ranks that we are interested in (the ranks rang-
ing from x to y). This region can be partitioned
into a collection of disjoint rectangular regions.
Each such partition therefore defines a collection
of rectangles or a q-partition.
The computation of the parameters x1r, y1r, x2r
and y2 r for an instantiated production p therefore
boils down to the computation of a partition of the
intermediate region of the n-best matrix of p.
</bodyText>
<figure confidence="0.8223765">
Cl..j
2 5
5 11 13 18
9 12 17 24
10 20 21 26
19 22 25 27
</figure>
<figureCaption confidence="0.9937675">
Figure 3: The sub-matrix corresponding to the
rectangle ((2, 5), (2, 5))
</figureCaption>
<bodyText confidence="0.999213">
We have represented schematically, in figure 4,
two 4-partitions and a 3-partition of the interme-
diate region of the matrix of figure 2. The left-
most (resp. rightmost) partition will be called the
vertical (resp. horizontal) partition. The middle
partition will be called an optimal partition, it de-
composes the intermediate region into a minimal
number of sub-matrices.
</bodyText>
<figureCaption confidence="0.980995">
Figure 4: Three partitions of an n-best matrix
</figureCaption>
<bodyText confidence="0.860566">
The three partitions of figure 4 will give birth to
the following instantiated productions:
</bodyText>
<listItem confidence="0.801316">
• Vertical partition
</listItem>
<figure confidence="0.694584846153846">
A4,27 B3,6 C1,1 A4,27 B2,5 C2,2
i..j i..l l..j i..j i..l l..j
4,27 1,5 3,5 4,27 1,1 6,6
Ai.. j Bi..l Cl..j Ai.. j Bi..l Cl.. j
• Optimal partition
A4,27 B1,1 C3,6 A4,27 B2,5 C2,5
i..j i..l l..j i..j i..l l..j
A4,27 B3,6 C1,1
i.. j i..l l..j
• Horizontal partition
A4,27 B1,1 C3,6 A4,27 B2,2 C2,5
i..j i..l l..j i..j i..l l..j
4,27 3,5 1,5 4,27 6,6 1,1
</figure>
<figureCaption confidence="0.447065">
Ai.. j Bi..l Cl.. j Ai.. j Bi..l Cl.. j
</figureCaption>
<bodyText confidence="0.9974586">
Vertical and horizontal partition of the interme-
diate region of a n-best matrix can easily be com-
puted. We are not aware of an efficient method that
computes an optimal partition. In the implemen-
tation used for experiments described in section 4,
</bodyText>
<figure confidence="0.998927078947368">
IV
III
II
I
���
���
���
���
��
��
III
II
I
���
���
���
���
��
��
IV
III
���
���
���
���
��
��
II
I
1
2
3
Bi..l 4
5
6
2
Bi..l
5
</figure>
<page confidence="0.992852">
124
</page>
<bodyText confidence="0.993912">
a simple heuristic has been used which computes
horizontal and vertical partitions and keeps the
partition with the lower number of parts.
The size of the new forest is clearly linked to
the partitions that are computed: a partition with
a lower number of parts will give birth to a lower
number of decorated instantiated productions and
therefore a smaller forest. But this optimization
is local, it does not take into account the fact that
an instantiated symbol may be shared in the initial
forest. During the computation of the new forest,
an instantiated production p can therefore be vis-
ited several times, with different parameters. Sev-
eral partitions of p will therefore be computed. If
a rectangle is shared by several partitions, this will
tend to decrease the size of the new forest. The
global optimal must therefore take into account all
the partitions of an instantiated production that are
computed during the construction of the new for-
est.
Example 5: Applying the rectangles method to
the second running example.
We now illustrate more concretely the rectan-
gles method on our second running example intro-
duced in Example 3. Let us recall that we are in-
terested in the n = 3 best trees, the original forest
containing 4 trees.
As said above, this method starts on the instan-
tiated axiom S1..3. Since it is the left-hand side
of only one production, this production is visited
with parameters 1, 3. Moreover, its n-best table is
the same as that of S1..3, given in Example 3. We
show here the corresponding n-best matrix, with
the empty lower region, the intermediate region
(cells corresponding to ranks 1 to 3) and the upper
region:
</bodyText>
<table confidence="0.964299">
B2..3
1 2
1 1 2
A1..2
3 4
</table>
<page confidence="0.403013">
2
</page>
<bodyText confidence="0.6565052">
As can be seen on that matrix, there are two op-
timal 2-partitions, namely the horizontal and the
vertical partitions, illustrated as follows:
Let us arbitrarily chose the vertical partition. It
gives birth to two S1..3-productions, namely:
</bodyText>
<footnote confidence="0.887970857142857">
S1,3
1..3 →A1,2
1..2 B1,1
2..3
S1,3
1..3 → A1,1
1..2 B2,2
</footnote>
<page confidence="0.869146">
2..3
</page>
<bodyText confidence="0.9877635">
Since this is the only non-trivial step while apply-
ing the rectangles algorithm to this example, we
can now give its final result, in which the axiom’s
(unnecessary) decorations have been removed:
</bodyText>
<table confidence="0.998685222222222">
A1,2 S1..3 → A1,2 }
1..2 1..2 B{1,1} → a1..2
A1,2 2..3 → a1..2
1..2 2,2 → b2..3
B1,2 S1..3 → A1, 2 B2..3 → b2..3
2..3 → A11..2 A11..2
B2,2 → A21..2 A21..2
2..3 →B12..3 B12..3
→B22..3 B22..3
</table>
<bodyText confidence="0.996534">
Compared to the forest built by the ranksets algo-
rithm, this forest has one less production and one
less non-terminal symbol. It has only one more
production than the over-generating pruned for-
est.
</bodyText>
<sectionHeader confidence="0.97665" genericHeader="evaluation">
4 Experiments on the Penn Treebank
</sectionHeader>
<bodyText confidence="0.99481775">
The methods described in section 3 have been
tested on a PCFG G extracted from the Penn Tree-
bank (Marcus et al., 1993). G has been extracted
naively: the trees have been decomposed into bi-
nary context free rules, and the probability of ev-
ery rule has been estimated by its relative fre-
quency (number of occurrences of the rule divided
by the number of occurrences of its left hand side).
Rules occurring less than 3 times and rules with
probabilities lower than 3 × 10−4 have been elim-
inated. The grammar produced contains 932 non
terminals and 3,439 rules.7
The parsing has been realized using the SYN-
TAX system which implements, and optimizes, the
Earley algorithm (Boullier, 2003).
The evaluation has been conducted on the 1, 845
sentences of section 1, which constitute our test
set. For every sentence and for increasing values
of n, an n-best sub-forest has been built using the
rankset and the rectangles method.
The performances of the algorithms have been
measured by the average compression rate they
7We used this test set only to generate practical NLP
forests, with a real NLP grammar, and evaluate the perfor-
mances of our algorithms for constucting sub-forests that
contain only the n-best trees, both in terms of compression
rate and execution time. Therefore, the evaluation carried out
here has nothing to do with the usual evaluation of the pre-
cision and recall of parsers based on the Penn Treebank. In
particular, we are not interested here in the accuracy of such
a grammar, its only purpose is to generate parse forests from
which n-best sub-forests will be built.
</bodyText>
<figure confidence="0.986349631578948">
I
II
II
I
125
avg. nb of trees in the pruned forest
2e+05
8e+05
7e+05
5e+05
4e+05
3e+05
1e+05
0e+00
9e+05
6e+05
0 100 200 300 400 500 600 700 800 900 1000
1 10 100 1000
n
</figure>
<figureCaption confidence="0.849582">
Figure 6: Average compression rates
n
Figure 5: Overgeneration of the pruned n-best forest
</figureCaption>
<figure confidence="0.957291625">
compression rate
1000
100
10
1
pruned forest
rectangles
ranksets
</figure>
<bodyText confidence="0.99994959375">
achieve for different values of n. The compres-
sion rate is obtained by dividing the size of the
n-best sub-forest of a sentence, as defined in sec-
tion 2, by the size of the (unfolded) n-best forest.
The latter is the sum of the sizes of all trees in the
forest, where every tree is seen as an instantiated
grammar, its size is therefore the size of the corre-
sponding instantiated grammar.
The size of the n-best forest constitutes a natu-
ral upper bound for the representation of the n-best
trees. Unfortunately, we have no natural lower
bound for the size of such an object. Neverthe-
less, we have computed the compression rates of
the pruned n-best forest and used it as an imperfect
lower bound. As already mentioned, its imper-
fection comes from the fact that a pruned n-best
forest contains more trees than the n best ones.
This overgeneration appears clearly in Figure 5
which shows, for increasing values of n, the av-
erage number of trees in the n-best pruned forest
for all sentences in our test set.
Figure 6 shows the average compression rates
achieved by the three methods (forest pruning,
rectangles and ranksets) on the test set for increas-
ing values of n. As predicted, the performances lie
between 1 (no compression) and the compression
of the n-best pruned forest. The rectangle method
outperforms the ranksets algorithm for every value
of n.
The time needed to build an 100-best forest with
the rectangle and the ranksets algorithms is shown
in Figure 7. This figure shows the average parsing
</bodyText>
<page confidence="0.98465">
126
</page>
<figure confidence="0.999161230769231">
time in milliseconds
1200
1000
200
800
400
600
0
parsing
ranksets
rectangles
5 10 15 20 25 30 35 40 45
sentence length
</figure>
<figureCaption confidence="0.999986">
Figure 7: Processing time
</figureCaption>
<bodyText confidence="0.99995825">
time for sentences of a given length, as well as the
average time necessary for building the 100-best
forest using the two aforementioned algorithms.
This time includes the parsing time i.e. it is the
time necessary for parsing a sentence and build-
ing the 100-best forest. As shown by the figure,
the time complexities of the two methods are very
close.
</bodyText>
<sectionHeader confidence="0.967353" genericHeader="conclusions">
5 Conclusion and perspectives
</sectionHeader>
<bodyText confidence="0.999965636363637">
This work presented two methods to build n-
best sub-forests. The so called rectangle meth-
ods showed to be the most promising, for it al-
lows to build efficient sub-forests with little time
overhead. Future work will focus on computing
optimized partitions of the n-best matrices, a cru-
cial part of the rectangle method, and adapting the
method to arbitrary (non binary) CFG. Another
line of research will concentrate on performing
re-ranking of the n-best trees directly on the sub-
forest.
</bodyText>
<sectionHeader confidence="0.998375" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.982551">
This research is supported by the French National
Research Agency (ANR) in the context of the
SEQUOIA project (ANR-08-EMER-013).
</bodyText>
<sectionHeader confidence="0.999027" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999529">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory ofParsing, Translation, and Compiling, vol-
ume 1. Prentice-Hall, Englewood Cliffs, NJ.
Taylor L. Booth. 1969. Probabilistic representation of
formal languages. In Tenth Annual Symposium on
Switching and Automata Theory, pages 74–81.
Pierre Boullier and Philippe Deschamp. 1988.
Le syst`eme SYNTAXTM - manuel d’utilisation.
http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.
Pierre Boullier and Benot Sagot. 2005. Efficient and
robust LFG parsing: SXLFG. In Proceedings of
IWPT’05, Vancouver, Canada.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings ofIWPT’03, pages 43–54.
Jay Earley. 1970. An efficient context-free parsing
algorithm. Communication of the ACM, 13(2):94–
102.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings ofIWPT’05, pages 53–64.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL’08, pages 586–594.
Victor M. Jim´enez and Andr´es Marzal. 2000. Com-
putation of the n best parse trees for weighted and
stochastic context-free grammars. In Proceedings
of the Joint IAPR International Workshops on Ad-
vances in Pattern Recognition, pages 183–192, Lon-
don, United Kingdom. Springer-Verlag.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings ofIWPT’01.
Bernard Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In J. Loeckx, ed-
itor, Proceedings of the Second Colloquium on Au-
tomata, Languages and Programming, volume 14 of
Lecture Notes in Computer Science, pages 255–269.
Springer-Verlag.
</reference>
<page confidence="0.971123">
127
</page>
<reference confidence="0.999624444444444">
Bernard Lang. 1994. Recognition can be harder then
parsing. Computational Intelligence, 10:486–494.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313–330, June.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings ofACL-08: HLT,
pages 192–199.
</reference>
<page confidence="0.996739">
128
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.237863">
<title confidence="0.996443">parse forests that include exactly the PCFG trees</title>
<author confidence="0.996284">Alexis Benoit</author>
<note confidence="0.7382054">1. Alpage, INRIA Paris-Rocquencourt &amp; Universit´e Paris Domaine de Voluceau Rocquencourt, BP 105 78153 Le Chesnay Cedex, 2. LIF, Univ. de la M´editerrann´ee 163, avenue de Luminy - Case 901 13288 Marseille Cedex 9, Alexis.Nasr@lif.univ-mrs.fr</note>
<abstract confidence="0.997175466666667">This paper describes and compares two algorithms that take as input a shared PCFG parse forest and produce shared forests contain exactly the likely trees of the initial forest. Such forests are suitable for subsequent processing, such as (some types of) reranking or LFG fstructure computation, that can be performed ontop of a shared forest, but that may have a high (e.g., exponential) complexity w.r.t. the number of trees contained in the forest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG ex-</abstract>
<note confidence="0.49776">tracted from the Penn Treebank.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory ofParsing, Translation, and Compiling,</booktitle>
<volume>1</volume>
<publisher>Prentice-Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="4538" citStr="Aho and Ullman, 1972" startWordPosition="736" endWordPosition="739">he basic objects we will be dealing with. Section 3 describes how to prune a shared 117 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 117–128, Paris, October 2009. c�2009 Association for Computational Linguistics forest, and introduces two approaches for building shared forests that contain exactly the n most likely parses. Section 4 describes experiments that were carried out on the Penn Treebank and section 5 concludes the paper. 2 Preliminaries 2.1 Instantiated grammars Let G = hN, T , P, Si be a context-free grammar (CFG), defined in the usual way (Aho and Ullman, 1972). Throughout this paper, we suppose that we manipulate only non-cyclic CFGs,1 but they may (and usually do) include E-productions. Given a production p ∈ P, we note lhs(p) its left-hand side, rhs(p) its right-hand side and |p |the length of rhs(p). Moreover, we note rhsk(p), with 1 ≤ k ≤ |p|, the kth symbol of rhs(p). We call Aproduction any production p ∈ P of G such that lhs(p) = A. A complete derivation of a sentence w = t1 ... t|w |(∀i ≤ |w|, ti ∈ T) w.r.t. G is of the form αX1X2 ... XrQ ⇒ ∗ G,w inition, A → X1X2 ... Xr is a production of G. Each of A, X1, X2, ... , Xr spans a unique occur</context>
<context position="24832" citStr="Aho and Ullman, 1972" startWordPosition="4397" endWordPosition="4401">2..3 B12..3 — b2..3 B{2} 2..3 — B22..3 B22..3 — b2..3 In this example, the bottom-up step doesn’t factorize out any other symbols, and this is therefore the final output of the ranksets method. It contains 2 more productions and 3 more symbols than the pruned forest (which is the same as the original forest), but it contains exactly the 3 best trees, contrarily to the pruned forest. 3.2 The rectangles method In this section only, we assume that the grammar G is binary (and therefore the forest, i.e., the grammar Gw, is binary). Standard binarization algorithms can be found in the litterature (Aho and Ullman, 1972). The algorithm described in this section performs, as the preceding one, a decoration of the symbols of Gw. The new (decorated) symbols have the form Ax,y i..j, where x and y denote ranks such that 1 &lt; x &lt; y &lt; n. The semantics of the decoration is closely related to the x, y extention of Ai..j, introduced in 2.3: £(Ax,y i..j) = £x,y(Ai..j) It corresponds to ranksets (in the sense of the previous section) that are intervals: Ax,y i..j is equivalent to the previous section’s A{x,x+1,...,y−1,y} . In i..j other words, the sub-forest rooted with Ax,y i..jcontains exactly the trees of the initial f</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory ofParsing, Translation, and Compiling, volume 1. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor L Booth</author>
</authors>
<title>Probabilistic representation of formal languages.</title>
<date>1969</date>
<booktitle>In Tenth Annual Symposium on Switching and Automata Theory,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="11944" citStr="Booth, 1969" startWordPosition="2082" endWordPosition="2083">d) terminal symbols. A top-down traversal of a forest is a traversal with the following constraint: a node Ai..j is visited if and only if all the instantiated productions in which it occurs in right-hand side have already been visited; once an instantiated production Ai..j has been visited, all its Ai..j-productions are visited as well. Of course the top-down visit starts by the visit of the axiom S0..|w|. 2.3 Ranked instantiated grammar When an instantiated grammar Gw = (Nw,Tw,Pw,S0..|w|) is built on a PCFG, every parse tree in £(S0..|w|) has a probability that is computed in the usual way (Booth, 1969). We might be interested in extracting the kth most likely tree of the forest represented by Gw,3 without unfolding the forest, i.e., without enumerating trees. In order to do so, we need to add some extra structure to the instantiated grammar. The augmented instantiated grammar will be called a ranked instantiated grammar. This extra structure takes the form of n-best tables that are associated with each instantiated non terminal symbol (Huang and Chiang, 2005), thus leading to ranked instantiated non terminal symbols, or simply instantiated symbols when the context is non ambiguous. A ranked</context>
</contexts>
<marker>Booth, 1969</marker>
<rawString>Taylor L. Booth. 1969. Probabilistic representation of formal languages. In Tenth Annual Symposium on Switching and Automata Theory, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Boullier</author>
<author>Philippe Deschamp</author>
</authors>
<date>1988</date>
<note>Le syst`eme SYNTAXTM - manuel d’utilisation. http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.</note>
<marker>Boullier, Deschamp, 1988</marker>
<rawString>Pierre Boullier and Philippe Deschamp. 1988. Le syst`eme SYNTAXTM - manuel d’utilisation. http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Boullier</author>
<author>Benot Sagot</author>
</authors>
<title>Efficient and robust LFG parsing: SXLFG.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT’05,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="3175" citStr="Boullier and Sagot, 2005" startWordPosition="511" endWordPosition="514">. the number of trees in the forest, such as LFG f-structures construction or some advanced reranking techniques. The experiments detailed in the last part of this paper show that the overgeneration factor of pruned sub-forest is more or less constant (see 6): after pruning the forest so as to keep the n best trees, the resulting forest contains approximately 103n trees. At least for some post-parsing processes, this overhead is highly problematic. For example, although LFG parsing can be achieved by computing LFG f-structures on top of a c-structure parse forest with a reasonable efficiency (Boullier and Sagot, 2005), it is clear that a 103 factor drastically affects the overall speed of the LFG parser. Therefore, simply pruning the forest is not an adequate solution. However, it will prove useful for comparison purposes. The new direction that we explore in this paper is the production of shared forests that contain exactly the n most likely trees, avoiding both the explicit construction of n different trees and the over-generation of pruning techniques. This can be seen as a transduction which is applied on a forest and produces another forest. The transduction applies some local transformations on the </context>
</contexts>
<marker>Boullier, Sagot, 2005</marker>
<rawString>Pierre Boullier and Benot Sagot. 2005. Efficient and robust LFG parsing: SXLFG. In Proceedings of IWPT’05, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Boullier</author>
</authors>
<title>Guided Earley parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ofIWPT’03,</booktitle>
<pages>43--54</pages>
<contexts>
<context position="36140" citStr="Boullier, 2003" startWordPosition="6536" endWordPosition="6537">acted from the Penn Treebank (Marcus et al., 1993). G has been extracted naively: the trees have been decomposed into binary context free rules, and the probability of every rule has been estimated by its relative frequency (number of occurrences of the rule divided by the number of occurrences of its left hand side). Rules occurring less than 3 times and rules with probabilities lower than 3 × 10−4 have been eliminated. The grammar produced contains 932 non terminals and 3,439 rules.7 The parsing has been realized using the SYNTAX system which implements, and optimizes, the Earley algorithm (Boullier, 2003). The evaluation has been conducted on the 1, 845 sentences of section 1, which constitute our test set. For every sentence and for increasing values of n, an n-best sub-forest has been built using the rankset and the rectangles method. The performances of the algorithms have been measured by the average compression rate they 7We used this test set only to generate practical NLP forests, with a real NLP grammar, and evaluate the performances of our algorithms for constucting sub-forests that contain only the n-best trees, both in terms of compression rate and execution time. Therefore, the eva</context>
</contexts>
<marker>Boullier, 2003</marker>
<rawString>Pierre Boullier. 2003. Guided Earley parsing. In Proceedings ofIWPT’03, pages 43–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communication of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>102</pages>
<contexts>
<context position="1132" citStr="Earley, 1970" startWordPosition="176" endWordPosition="177">st and produce shared forests that contain exactly the n most likely trees of the initial forest. Such forests are suitable for subsequent processing, such as (some types of) reranking or LFG fstructure computation, that can be performed ontop of a shared forest, but that may have a high (e.g., exponential) complexity w.r.t. the number of trees contained in the forest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank. 1 Introduction The output of a CFG parser based on dynamic programming, such as an Earley parser (Earley, 1970), is a compact representation of all syntactic parses of the parsed sentence, called a shared parse forest (Lang, 1974; Lang, 1994). It can represent an exponential number of parses (with respect to the length of the sentence) in a cubic size structure. This forest can be used for further processing, as reranking (Huang, 2008) or machine translation (Mi et al., 2008). When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only the n most likely trees of the forest. Standard state-of-the-art algorithms that extract the n best </context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communication of the ACM, 13(2):94– 102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings ofIWPT’05,</booktitle>
<pages>53--64</pages>
<contexts>
<context position="1763" citStr="Huang and Chiang, 2005" startWordPosition="283" endWordPosition="286">ompact representation of all syntactic parses of the parsed sentence, called a shared parse forest (Lang, 1974; Lang, 1994). It can represent an exponential number of parses (with respect to the length of the sentence) in a cubic size structure. This forest can be used for further processing, as reranking (Huang, 2008) or machine translation (Mi et al., 2008). When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only the n most likely trees of the forest. Standard state-of-the-art algorithms that extract the n best parses (Huang and Chiang, 2005) produce a collection of trees, losing the factorization that has been achieved by the parser, and reproduce some identical sub-trees in several parses. This situation is not satisfactory since postparsing processes, such as reranking algorithms or attribute computation, cannot take advantage of this lost factorization and may reproduce some identical work on common sub-trees, with a computational cost that can be exponentally high. One way to solve the problem is to prune the forest by eliminating sub-forests that do not contribute to any of the n most likely trees. But this over-generates: t</context>
<context position="9822" citStr="Huang and Chiang, 2005" startWordPosition="1720" endWordPosition="1723">.1 N1..2 — boy1..2 NP0..2 — Det0..1 N1..2 V2..3 — saw2..3 Det3..4 — a3..4 N4..5 — man4..5 NP3..5 — Det3..4 N4..5 Prep5..6 — with5..6 Det6..7 — a6..7 N7..8 — telescope7..8 NP6..8 — Det6..7 N7..8 PP5..8 — Prep5..6 NP6..8 NP3..8 — NP3..5 PP5..8 VP2..8 — V2..3 NP3..8 VP2..5 — V2..3 NP3..5 VP2..8 — VP2..5 PP5..8 S0..8 — NP0..2 VP2..8 They represent the parse forest of w according to G. This parse forest contains two trees, since there is one ambiguity: VP2..8 can be rewritten in two different ways. The instantiated grammar Gw can be represented as an hypergraph (as in (Klein and Manning, 2001) or (Huang and Chiang, 2005)) where the instantiated symbols of Gw correspond to the vertices of the hypergraph and the instantiated productions to the hyperarcs. We define the extension of an instantiated symbol Xi..j, noted £(Xi..j), as the set of instantiated parse trees that have Xi..j as a root. The set of all parse trees of w w.r.t. G is therefore £(S0..|w|). In the same way, we define the extension of an instantiated production Xi..j — α to be the subset of £(Xi..j) that corresponds to derivations of the form Xi..j ==&gt;. α � ti+1 ... tj (i.e., trees rooted G,w G,w in Xi..j and where the daughters of the node Xi..j </context>
<context position="12410" citStr="Huang and Chiang, 2005" startWordPosition="2156" endWordPosition="2159">antiated grammar Gw = (Nw,Tw,Pw,S0..|w|) is built on a PCFG, every parse tree in £(S0..|w|) has a probability that is computed in the usual way (Booth, 1969). We might be interested in extracting the kth most likely tree of the forest represented by Gw,3 without unfolding the forest, i.e., without enumerating trees. In order to do so, we need to add some extra structure to the instantiated grammar. The augmented instantiated grammar will be called a ranked instantiated grammar. This extra structure takes the form of n-best tables that are associated with each instantiated non terminal symbol (Huang and Chiang, 2005), thus leading to ranked instantiated non terminal symbols, or simply instantiated symbols when the context is non ambiguous. A ranked instantiated non terminal symbol is written (Ai..j, T (Ai..j)), where T (Ai..j) is the n-best table associated with the instantiated symbol Ai..j. T (Ai..j) is a table of at most n entries. The k-th entry of the table, noted e, describes how to build the k-th most likely tree of £(Ai..j). This tree will be called the k-th extention of Ai..j, noted £k(Ai..j). More precisely, e indicates the instantiated Ai..j-production p such that £k(Ai..j) E £(p). It indicates</context>
<context position="17065" citStr="Huang and Chiang, 2005" startWordPosition="3029" endWordPosition="3032">lt: the best tree is T1,1, the second-best tree is T1,2, the third-best tree is T2,1 and the worst tree is T2,2. If n = 3, the pruned forest over-generates: all instantiated productions take part in at least one of the three best trees, and therefore the pruned forest is the full forest itself, which contains four trees. We shall use this example later on so as to illustrate both methods we introduce for building forests that contain exactly the n best trees, without overgenerating. 2.4 Extracting the kth-best tree An efficient algorithm for the extraction of the nbest trees is introduced in (Huang and Chiang, 2005), namely the authors’ algorithm 3, which 120 is a re-formulation of a procedure originally proposed by (Jim´enez and Marzal, 2000). Contrarily to (Huang and Chiang, 2005), we shall sketch this algorithm with the terminology introduced above (whereas the authors use the notion of hypergraph). The algorithm relies on the n-best tables described above: extracting the kth-best tree consists in extending the n-best tables as much as necessary by computing all lines in each n-best table up to those that concern the kth-best tree.4 The algorithm can be divided in two subalgorithms: (1) a bottom-up tr</context>
<context position="28985" citStr="Huang and Chiang, 2005" startWordPosition="5163" endWordPosition="5167"> q), (y1 q, y2 q)) be a collection of q rectangles. It will be called a q-partition of the instantiated production p iff the following is true: £x,y(p) = U £ ( B A 1 2 1 2 i..j i..l �,y �r,xr Cyr,yr 1≤r≤q To put it differently, this definition means that ((x11,x21), (y11,y21)),..., ((x1q,x2q), (y1q, y2q)) is a q partition of p if any tree of £ (BZ l&apos;xr) combined with any tree of £(Cl �&apos;y2r) is a tree of £x,y(p) and, conversely, any tree of £x,y(p) is the combination of a tree of £ (BZ i&apos;xr) and a tree of £ (Cl,yr ) The n-best matrix associated with an instantiated production p, introduced in (Huang and Chiang, 2005), is merely a two dimensional representation of the n-best table of p. Such a matrix, represents how the n most likely trees of £(p) are built. An example of an n-best matrix is represented in figure 1. This matrix says that the first most likely tree of p is built by combining the tree £1(Bi..l) with the tree £1(Cl..j) (there is a 1 in the cell of coordinate (1,1)). The second most likely tree is built by combining the tree £1(Bi..l) and £2(Cl..j) (there is a 2 in the cell of coordinate (1, 2)) and so on. Cl..j 1 2 3 4 5 6 1 2 6 8 14 15 3 5 11 13 18 29 4 9 12 17 24 30 7 10 20 21 26 33 16 19 2</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings ofIWPT’05, pages 53–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL’08,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="1460" citStr="Huang, 2008" startWordPosition="234" endWordPosition="235">e number of trees contained in the forest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank. 1 Introduction The output of a CFG parser based on dynamic programming, such as an Earley parser (Earley, 1970), is a compact representation of all syntactic parses of the parsed sentence, called a shared parse forest (Lang, 1974; Lang, 1994). It can represent an exponential number of parses (with respect to the length of the sentence) in a cubic size structure. This forest can be used for further processing, as reranking (Huang, 2008) or machine translation (Mi et al., 2008). When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only the n most likely trees of the forest. Standard state-of-the-art algorithms that extract the n best parses (Huang and Chiang, 2005) produce a collection of trees, losing the factorization that has been achieved by the parser, and reproduce some identical sub-trees in several parses. This situation is not satisfactory since postparsing processes, such as reranking algorithms or attribute computation, cannot take advantage of </context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL’08, pages 586–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor M Jim´enez</author>
<author>Andr´es Marzal</author>
</authors>
<title>Computation of the n best parse trees for weighted and stochastic context-free grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint IAPR International Workshops on Advances in Pattern Recognition,</booktitle>
<pages>183--192</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, United Kingdom.</location>
<marker>Jim´enez, Marzal, 2000</marker>
<rawString>Victor M. Jim´enez and Andr´es Marzal. 2000. Computation of the n best parse trees for weighted and stochastic context-free grammars. In Proceedings of the Joint IAPR International Workshops on Advances in Pattern Recognition, pages 183–192, London, United Kingdom. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings ofIWPT’01.</booktitle>
<contexts>
<context position="9794" citStr="Klein and Manning, 2001" startWordPosition="1714" endWordPosition="1718">ns of Gw are: Det0..1 — the0..1 N1..2 — boy1..2 NP0..2 — Det0..1 N1..2 V2..3 — saw2..3 Det3..4 — a3..4 N4..5 — man4..5 NP3..5 — Det3..4 N4..5 Prep5..6 — with5..6 Det6..7 — a6..7 N7..8 — telescope7..8 NP6..8 — Det6..7 N7..8 PP5..8 — Prep5..6 NP6..8 NP3..8 — NP3..5 PP5..8 VP2..8 — V2..3 NP3..8 VP2..5 — V2..3 NP3..5 VP2..8 — VP2..5 PP5..8 S0..8 — NP0..2 VP2..8 They represent the parse forest of w according to G. This parse forest contains two trees, since there is one ambiguity: VP2..8 can be rewritten in two different ways. The instantiated grammar Gw can be represented as an hypergraph (as in (Klein and Manning, 2001) or (Huang and Chiang, 2005)) where the instantiated symbols of Gw correspond to the vertices of the hypergraph and the instantiated productions to the hyperarcs. We define the extension of an instantiated symbol Xi..j, noted £(Xi..j), as the set of instantiated parse trees that have Xi..j as a root. The set of all parse trees of w w.r.t. G is therefore £(S0..|w|). In the same way, we define the extension of an instantiated production Xi..j — α to be the subset of £(Xi..j) that corresponds to derivations of the form Xi..j ==&gt;. α � ti+1 ... tj (i.e., trees rooted G,w G,w in Xi..j and where the </context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and hypergraphs. In Proceedings ofIWPT’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Deterministic techniques for efficient non-deterministic parsers.</title>
<date>1974</date>
<booktitle>Proceedings of the Second Colloquium on Automata, Languages and Programming,</booktitle>
<volume>14</volume>
<pages>255--269</pages>
<editor>In J. Loeckx, editor,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1250" citStr="Lang, 1974" startWordPosition="196" endWordPosition="197">le for subsequent processing, such as (some types of) reranking or LFG fstructure computation, that can be performed ontop of a shared forest, but that may have a high (e.g., exponential) complexity w.r.t. the number of trees contained in the forest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank. 1 Introduction The output of a CFG parser based on dynamic programming, such as an Earley parser (Earley, 1970), is a compact representation of all syntactic parses of the parsed sentence, called a shared parse forest (Lang, 1974; Lang, 1994). It can represent an exponential number of parses (with respect to the length of the sentence) in a cubic size structure. This forest can be used for further processing, as reranking (Huang, 2008) or machine translation (Mi et al., 2008). When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only the n most likely trees of the forest. Standard state-of-the-art algorithms that extract the n best parses (Huang and Chiang, 2005) produce a collection of trees, losing the factorization that has been achieved by the </context>
</contexts>
<marker>Lang, 1974</marker>
<rawString>Bernard Lang. 1974. Deterministic techniques for efficient non-deterministic parsers. In J. Loeckx, editor, Proceedings of the Second Colloquium on Automata, Languages and Programming, volume 14 of Lecture Notes in Computer Science, pages 255–269. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Recognition can be harder then parsing.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<pages>10--486</pages>
<contexts>
<context position="1263" citStr="Lang, 1994" startWordPosition="198" endWordPosition="199">quent processing, such as (some types of) reranking or LFG fstructure computation, that can be performed ontop of a shared forest, but that may have a high (e.g., exponential) complexity w.r.t. the number of trees contained in the forest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank. 1 Introduction The output of a CFG parser based on dynamic programming, such as an Earley parser (Earley, 1970), is a compact representation of all syntactic parses of the parsed sentence, called a shared parse forest (Lang, 1974; Lang, 1994). It can represent an exponential number of parses (with respect to the length of the sentence) in a cubic size structure. This forest can be used for further processing, as reranking (Huang, 2008) or machine translation (Mi et al., 2008). When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only the n most likely trees of the forest. Standard state-of-the-art algorithms that extract the n best parses (Huang and Chiang, 2005) produce a collection of trees, losing the factorization that has been achieved by the parser, and r</context>
</contexts>
<marker>Lang, 1994</marker>
<rawString>Bernard Lang. 1994. Recognition can be harder then parsing. Computational Intelligence, 10:486–494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="35575" citStr="Marcus et al., 1993" startWordPosition="6438" endWordPosition="6441">inal result, in which the axiom’s (unnecessary) decorations have been removed: A1,2 S1..3 → A1,2 } 1..2 1..2 B{1,1} → a1..2 A1,2 2..3 → a1..2 1..2 2,2 → b2..3 B1,2 S1..3 → A1, 2 B2..3 → b2..3 2..3 → A11..2 A11..2 B2,2 → A21..2 A21..2 2..3 →B12..3 B12..3 →B22..3 B22..3 Compared to the forest built by the ranksets algorithm, this forest has one less production and one less non-terminal symbol. It has only one more production than the over-generating pruned forest. 4 Experiments on the Penn Treebank The methods described in section 3 have been tested on a PCFG G extracted from the Penn Treebank (Marcus et al., 1993). G has been extracted naively: the trees have been decomposed into binary context free rules, and the probability of every rule has been estimated by its relative frequency (number of occurrences of the rule divided by the number of occurrences of its left hand side). Rules occurring less than 3 times and rules with probabilities lower than 3 × 10−4 have been eliminated. The grammar produced contains 932 non terminals and 3,439 rules.7 The parsing has been realized using the SYNTAX system which implements, and optimizes, the Earley algorithm (Boullier, 2003). The evaluation has been conducted</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="1501" citStr="Mi et al., 2008" startWordPosition="239" endWordPosition="242">rest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank. 1 Introduction The output of a CFG parser based on dynamic programming, such as an Earley parser (Earley, 1970), is a compact representation of all syntactic parses of the parsed sentence, called a shared parse forest (Lang, 1974; Lang, 1994). It can represent an exponential number of parses (with respect to the length of the sentence) in a cubic size structure. This forest can be used for further processing, as reranking (Huang, 2008) or machine translation (Mi et al., 2008). When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only the n most likely trees of the forest. Standard state-of-the-art algorithms that extract the n best parses (Huang and Chiang, 2005) produce a collection of trees, losing the factorization that has been achieved by the parser, and reproduce some identical sub-trees in several parses. This situation is not satisfactory since postparsing processes, such as reranking algorithms or attribute computation, cannot take advantage of this lost factorization and may reproduce</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings ofACL-08: HLT, pages 192–199.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>