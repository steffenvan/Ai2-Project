<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011088">
<title confidence="0.9905195">
UCM-I: A Rule-based Syntactic Approach for Resolving the Scope of
Negation
</title>
<author confidence="0.911587">
Jorge Carrillo de Albornoz, Laura Plaza, Alberto Diaz and Miguel Ballesteros
</author>
<affiliation confidence="0.804848">
Universidad Complutense de Madrid
</affiliation>
<address confidence="0.9000365">
C/ Prof. Jos´e Garc´ıa Santesmases, s/n
28040 Madrid (Spain)
</address>
<email confidence="0.999415">
{jcalbornoz,lplazam,albertodiaz,miballes}@fdi.ucm.es
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987275">
This paper presents one of the two contribu-
tions from the Universidad Complutense de
Madrid to the *SEM Shared Task 2012 on Re-
solving the Scope and Focus of Negation. We
describe a rule-based system for detecting the
presence of negations and delimitating their
scope. It was initially intended for process-
ing negation in opinionated texts, and has been
adapted to fit the task requirements. It first
detects negation cues using a list of explicit
negation markers (such as not or nothing), and
infers other implicit negations (such as affixal
negations, e.g, undeniable or improper) by us-
ing semantic information from WordNet con-
cepts and relations. It next uses the informa-
tion from the syntax tree of the sentence in
which the negation arises to get a first approxi-
mation to the negation scope, which is later re-
fined using a set of post-processing rules that
bound or expand such scope.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999924348837209">
Detecting negation is important for many NLP tasks,
as it may reverse the meaning of the text affected
by it. In information extraction, for instance, it is
obviously important to distinguish negated informa-
tion from affirmative one (Kim and Park, 2006). It
may also improve automatic indexing (Mutalik et
al., 2001). In sentiment analysis, detecting and deal-
ing with negation is critical, as it may change the
polarity of a text (Wiegand et al., 2010). How-
ever, research on negation has mainly focused on the
biomedical domain, and addressed the problem of
detecting if a medical term is negated or not (Chap-
man et al., 2001), or the scope of different negation
signals (Morante et al., 2008).
During the last years, the importance of process-
ing negation is gaining recognition by the NLP re-
search community, as evidenced by the success of
several initiatives such as the Negation and Spec-
ulation in Natural Language Processing workshop
(NeSp-NLP 2010)1 or the CoNLL-2010 Shared
Task2, which aimed at identifying hedges and their
scope in natural language texts. In spite of this, most
of the approaches proposed so far deal with negation
in a superficial manner.
This paper describes our contribution to the
*SEM Shared Task 2012 on Resolving the Scope
and Focus of Negation. As its name suggests, the
task aims at detecting the scope and focus of nega-
tion, as a means of encouraging research in negation
processing. In particular, we participate in Task 1:
scope detection. For each negation in the text, the
negation cue must be detected, and its scope marked.
Moreover, the event or property that is negated must
be recognized. A comprehensive description of the
task may be found in (Morante and Blanco, 2012).
For the sake of clarity, it is important to define
what the organization of the task understands by
negation cue, scope of negation and negated event.
The words that express negation are called negation
cues. Not and no are common examples of such
cues. Scope is defined as the part of the mean-
ing that is negated, and encloses all negated con-
cepts. The negated event is the property that is
</bodyText>
<footnote confidence="0.9999795">
1http://www.clips.ua.ac.be/NeSpNLP2010/
2www.inf.u-szeged.hu/rgai/conll2010st/
</footnote>
<page confidence="0.918966">
282
</page>
<note confidence="0.539537">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 282–287,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9873938">
negated by the cue. For instance, in the sentence:
[Holmes] did not [say anything], the scope is en-
closed in square brackets, the negation cue is under-
lined and the negated event is shown in bold. More
details about the annotation of negation cues, scopes
and negated events may be found in (Morante and
Daelemans, 2012).
The system presented to the shared task is an
adaptation of the one published in (Carrillo de Al-
bornoz et al., 2010), whose aim was to detect and
process negation in opinionated text in order to im-
prove polarity and intensity classification. When
classifying sentiments and opinions it is important
to deal with the presence of negations and their ef-
fect on the emotional meaning of the text affected by
</bodyText>
<listItem confidence="0.784678">
them. Consider the sentence (1) and (2). Sentence
(1) expresses a positive opinion, whereas that in sen-
tence (2) the negation word not reverses the polarity
of such opinion.
(1) I liked this hotel.
(2) I didn’t like this hotel.
</listItem>
<bodyText confidence="0.999355">
Our system has the main advantage of being sim-
ple and highly generic. Even though it was origi-
nally conceived for treating negations in opinionated
texts, a few simple modifications have been suffi-
cient to successfully address negation in a very dif-
ferent type of texts, such as Conan Doyle stories. It
is rule-based and does not need to be trained. It also
uses semantic information in order to automatically
detect the negation cues.
</bodyText>
<sectionHeader confidence="0.996478" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.99993175">
As already told, the UCM-I system is a modified ver-
sion of the one presented in (Carrillo de Albornoz
et al., 2010). Next sections detail the modifications
performed to undertake the present task.
</bodyText>
<subsectionHeader confidence="0.999624">
2.1 Detecting negation cues
</subsectionHeader>
<bodyText confidence="0.999878222222222">
Our previous work was focused on explicit nega-
tions (i.e., those introduced by negation tokens such
as not, never). In contrast, in the present work
we also consider what we call implicit negations,
which includes affixal negation (i.,e., words with
prefixes such as dis-, un- or suffixes such as -less;
e.g., impatient or careless), inffixal negation (i.e.,
pointlessness, where the negation cue less is in the
middle of the noun phrase). Note that we did not
</bodyText>
<tableCaption confidence="0.998429">
Table 1: Examples of negation cues.
</tableCaption>
<table confidence="0.523549833333333">
Explicit negation cues
no not non nor
nobody never nowhere ...
Words with implicit negation cues
unpleasant unnatural dislike impatient
fearless hopeless illegal ...
</table>
<bodyText confidence="0.999963076923077">
have into account these negation cues when ana-
lyzing opinionated texts because these words them-
selves usually appear in affective lexicons with their
corresponding polarity values (i.e., impatient, for in-
stance, appears in SentiWordNet with a negative po-
larity value).
In order to detect negation cues, we use a list of
predefined negation signals, along with an automatic
method for detecting new ones. The list has been
extracted from different previous works (Councill et
al., 2010; Morante, 2010). This list also includes the
most frequent contracted forms (e.g., don’t, didn’t,
etc.). The automated method, in turn, is intended
for discovering in text new affixal negation cues. To
this end, we first find in the text all words with pre-
fixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suf-
fix -less that present the appropriate part of speech.
Since not all words with such affixes are negation
cues, we use semantic information from WordNet
concepts and relations to decide. In this way, we re-
trieve from WordNet the synset that correspond to
each word, using WordNet::SenseRelate (Patward-
han et al., 2005) to correctly disambiguate the mean-
ing of the word according to its context, along with
all its antonym synsets. We next check if, after re-
moving the affix, the word exists in WordNet and
belongs to any of the antonym synsets. If so, we
consider the original word to be a negation cue (i.e.,
the word without the affix has the opposite meaning
than the lexical item with the affix).
Table 1 presents some examples of explicit nega-
tion cues and words with implicit negation cues. For
space reasons, not all cues are shown. We also con-
sider common spelling errors such as the omission
of apostrophes (e.g., isnt or nt). They are not likely
to be found in literary texts, but are quite frequent in
user-generated content.
This general processing is, however, improved
with two rules:
</bodyText>
<page confidence="0.99921">
283
</page>
<tableCaption confidence="0.99848">
Table 2: Examples of false negation cues.
</tableCaption>
<bodyText confidence="0.945147888888889">
no doubt without a doubt not merely not just
not even not only no wonder ...
1. False negation cues: Some negation words
may be also used in other expressions with-
out constituting a negation, as in sentence (3).
Therefore, when the negation token belongs
to such expressions, this is not processed as a
negation. Examples of false negation cues are
shown in Table 2.
</bodyText>
<listItem confidence="0.985316909090909">
(3) ... the evidence may implicate not only your
friend Mr. Stapleton but his wife as well.
2. Tag questions: Some sentences in the cor-
pora present negative tag questions in old En-
glish grammatical form, as it may shown in
sentences (4) and (5). We have implemented a
specific rule to deal with this type of construc-
tions, so that they are not treated as negations.
(4) You could easily recognize it , could you not?.
(5) But your family have been with us for several
generations, have they not?
</listItem>
<subsectionHeader confidence="0.999089">
2.2 Delimiting the scope of negation
</subsectionHeader>
<bodyText confidence="0.999791263157895">
The scope of a negation is determined by using the
syntax tree of the sentence in which the negation
arises, as generated by the Stanford Parser.3 To this
end, we find in the syntax tree the first common an-
cestor that encloses the negation token and the word
immediately after it, and assume all descendant leaf
nodes to the right of the negation token to be af-
fected by it. This process may be seen in Figure
1, where the syntax tree for the sentence: [Watson
did] not [solve the case] is shown. In this sentence,
the method identifies the negation token not and as-
sumes its scope to be all descendant leaf nodes of the
common ancestor of the words not and solve (i.e.,
solve the case).
This modeling has the main advantage of being
highly generic, as it serves to delimit the scope of
negation regardless of what the negated event is (i.e.,
the verb, the subject, the object of the verb, an ad-
jective or an adverb). As shown in (Carrillo de Al-
</bodyText>
<footnote confidence="0.912695">
3http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<figureCaption confidence="0.972973">
Figure 1: Syntax tree of the sentence: Watson did not
solve the case.
</figureCaption>
<bodyText confidence="0.997879">
bornoz et al., 2010), it behaves well when determin-
ing the scope of negation for the purpose of classi-
fying product reviews in polarity classes. However,
we have found that this scope is not enough for the
present task, and thus we have implemented a set of
post-processing rules to expand and limit the scope
according to the task guidelines:
</bodyText>
<listItem confidence="0.921948833333333">
1. Expansion to subject. This rule expands the
negation scope in order to include the subject of
the sentence within it. In this way, in sentence
(6) the appropriate rule is fired to include “This
theory” within the negation scope.
(6) [This theory would] not [work].
</listItem>
<bodyText confidence="0.9959685625">
It must be noted that, for polarity classifica-
tion purposes, we do not consider the subject
of the sentence to be part of this scope. Con-
sider, for instance, the sentence: The beauti-
ful views of the Eiffel Tower are not guaranteed
in all rooms. According to traditional polarity
classification approaches, if the subject is con-
sidered as part of the negation scope, the polar-
ity of the positive polar expression “beautiful”
should be changed, and considered as negative.
2. Subordinate boundaries. Our original nega-
tion scope detection method works well with
coordinate sentences, in which negation cues
scope only over their clause, as if a “boundary”
exists between the different clauses. This oc-
curs, for instance, in the sentence:
</bodyText>
<page confidence="0.998185">
284
</page>
<tableCaption confidence="0.999161">
Table 3: List of negation scope delimiters.
</tableCaption>
<table confidence="0.9970325">
Tokens POS
so, because, if, while
until, since, unless IN
before, than, despite IN
what, whose WP
why, where WRB
however RB
“,”, - , :, ;, (, ), !, ?, . -
</table>
<listItem confidence="0.9254955">
(7) [It may be that you are] not [yourself lumi-
nous], but you are a conductor of light.
</listItem>
<bodyText confidence="0.9995604">
It also works properly in subordinate sentences,
when the negation occurs in the subordinate
clause, as in: You can imagine my surprise
when I found that [there was] no [one there].
However, it may fail in some types of subor-
dinate sentences, where the scope should be
limited to the main clause, but our model pre-
dict both clauses to be affected by the negation.
This is the case for the sentences where the de-
pendent clause is introduced by the subordinate
conjunctions in Table 3. An example of such
type of sentence is (8), where the conjunction
token because introduces a subordinate clause
which is out of the negation scope. To solve this
problem, the negation scope detection method
includes a set of rules to delimit the scope in
those cases, using as delimiters the conjunc-
tions in Table 3. Note that, since some of these
delimiters are ambiguous, their part of speech
tags are used to disambiguate them.
</bodyText>
<listItem confidence="0.8935764">
(8) [Her father] refused [to have anything to do
with her] because she had married without his
consent.
3. Prepositional phrases: Our original method
also fails to correctly determine the negation
</listItem>
<bodyText confidence="0.984776888888889">
scope when the negated event is followed by
a prepositional phrase, as it may be seen in
Figure 2, where the syntax tree for the sen-
tence: [There was] no [attempt at robbery] is
shown. Note that, according to our original
model, the phrase “at robbery” does not belong
to the negation scope. This is an error that was
not detected before, but has been fixed for the
presenttask.
</bodyText>
<figureCaption confidence="0.9880495">
Figure 2: Syntax tree for the sentence: There was no at-
tempt at robbery.
</figureCaption>
<subsectionHeader confidence="0.997742">
2.3 Finding negated events
</subsectionHeader>
<bodyText confidence="0.999871">
We only consider a single type of negated events,
so that, when a cue word contains a negative affix,
the word after removing the affix is annotated as the
negated event. In this way, “doubtedly” is correctly
annotated as the negated event in sentence (9). How-
ever, the remaining types of negated events are rele-
gated to future work.
</bodyText>
<listItem confidence="0.9437975">
(9) [The oval seal is] undoubtedly [a plain
sleeve-link].
</listItem>
<sectionHeader confidence="0.996382" genericHeader="method">
3 Evaluation Setup
</sectionHeader>
<bodyText confidence="0.999912166666667">
The data collection consists of a development set, a
training set, and two test sets of 787, 3644, 496 and
593 sentences, respectively from different stories by
Conan Doyle (see (Morante and Blanco, 2012) for
details). Performance is measured in terms of recall,
precision and F-measure for the following subtasks:
</bodyText>
<listItem confidence="0.999528">
• Predicting negation cues.
• Predicting both the scope and cue.
• Predicting the scope, the cue does not need to
be correct.
• Predicting the scope tokens, where not a full
scope match is required.
• Predicting negated events.
• Full evaluation, which requires all elements to
be correct.
</listItem>
<page confidence="0.999017">
285
</page>
<tableCaption confidence="0.995994">
Table 4: Results for the development set.
</tableCaption>
<table confidence="0.999612428571429">
Metric Pr. Re. F-1
Cues 92.55 86.13 89.22
Scope (cue match) 86.05 44.05 58.27
Scope (no cue match) 86.05 44.05 58.27
Scope tokens (no cue match) 88.05 59.05 70.69
Negated (no cue match) 65.00 10.74 18.43
Full negation 74.47 20.23 31.82
</table>
<sectionHeader confidence="0.981537" genericHeader="method">
4 Evaluation Results
</sectionHeader>
<bodyText confidence="0.999970192307692">
The results of our system when evaluated on the de-
velopment set and the two test sets (both jointly and
separately), are shown in Tables 4, 5, and 6.
It may be seen from these tables that our sys-
tem behaves quite well in the prediction of negation
cues subtask, achieving around 90% F-measure in
all data sets, and the second position in the com-
petition. Performance in the scope prediction task,
however, is around 60% F-1, and the same results
are obtained if the correct prediction of cues is re-
quired (Scope (cue match)). This seems to indicate
that, for all correct scope predictions, our system
have also predicted the negation cues correctly. Ob-
viously these results improve for the Scope tokens
measure, achieving more than 77% F-1 for the Card-
board data set. We also got the second position in
the competition for these three subtasks. Concerning
detection of negated events, our system gets poor re-
sults, 22.85% and 19.81% F-1, respectively, in each
test data set. These results affect the performance
of the full negation prediction task, where we get
32.18% and 32.96% F-1, respectively. Surprisingly,
the result in the test sets are slightly better than those
in the development set, and this is due to a better be-
havior of the WordNet-based cue detection method
in the formers than in the later.
</bodyText>
<sectionHeader confidence="0.998563" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999640065217391">
We next discuss and analyze the results above.
Firstly, and regarding detection of negation cues, our
initial list covers all explicit negations in the devel-
opment set, while the detection of affixal negation
cues using our WordNet-based method presents a
precision of 100% but a recall of 53%. In particu-
lar, our method fails when discovering negation cues
such as unburned, uncommonly or irreproachable,
where the word after removing the affix is a derived
form of a verb or adjective.
Secondly, and concerning delimitation of the
scope, our method behaves considerably well. We
have found that it correctly annotates the negation
scope when the negation affects the predicate that
expresses the event, but sometimes fails to include
the subject of the sentence in such scope, as in:
[I know absolutely] nothing [about the fate of this
man], where our method only recognizes as the
negation scope the terms about the fate of this man.
The results have also shown that the method fre-
quently fails when the subject of the sentence or the
object of an event are negated. This occurs, for
instance, in sentences: I think, Watson, [a brandy
and soda would do him] no [harm] and No [woman
would ever send a reply-paid telegram], where we
only point to “harm” and “woman” as the scopes.
We have found a further category of errors in the
scope detection tasks, which concern some types
of complex sentences with subordinate conjunctions
where our method limits the negation scope to the
main clause, as in sentence: [Where they came from,
or who they are,] nobody [has an idea] , where our
method limits the scope to “has an idea”. However,
if the negation cue occurs in the subordinate clause,
the method behaves correctly.
Thirdly, with respect to negated event detection,
as already told our method gets quite poor results.
This was expected, since our system was not orig-
inally designed to face this task and thus it only
covers one type of negated events. Specifically,
it correctly identifies the negated events for sen-
tences with affixal negation cues, as in: It is most
improper, most outrageous, where the negated event
is “proper”. However, it usually fails to identify
these events when the negation affects the subject
of the sentence or the object of an event.
</bodyText>
<sectionHeader confidence="0.999212" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999965875">
This paper presents one of the two contributions
from the Universidad Complutense de Madrid to the
*SEM Shared Task 2012. The results have shown
that our method successes in identifying negation
cues and performs reasonably well when determin-
ing the negation scope, which seems to indicate that
a simple unsupervised method based on syntactic in-
formation and a reduced set of post-processing rules
</bodyText>
<page confidence="0.998359">
286
</page>
<tableCaption confidence="0.993296">
Table 5: Results for the test sets (jointly).
</tableCaption>
<table confidence="0.999714714285714">
Metric Gold System Tp Fp Fn Precision Recall F-1
Cues 264 278 241 29 23 89.26 91.29 90.26
Scopes (cue match) 249 254 116 24 133 82.86 46.59 59.64
Scopes (no cue match) 249 254 116 24 133 82.86 46.59 59.64
Scope tokens (no cue match) 1805 1449 1237 212 568 85.37 68.53 76.03
Negated (no cue match) 173 33 22 11 151 66.67 12.72 21.36
Full negation 264 278 57 29 207 66.28 21.59 32.57
</table>
<tableCaption confidence="0.774173">
Table 6: Results for the Cardboard and Circle test sets.
</tableCaption>
<table confidence="0.997766">
Metric Cardboard set Pr. Circle set F-1
Pr. Re. F-1 Re.
Cues 90.23 90.23 90.23 88.32 92.37 90.30
Scope (cue match) 83.33 46.88 60.00 82.35 46.28 59.26
Scope (no cue match) 83.33 46.88 60.00 82.35 46.28 59.26
Scope tokens (no cue match) 84.91 72.08 77.97 85.96 64.50 73.70
Negated (no cue match) 66.67 13.79 22.85 66.67 11.63 19.81
Full negation 68.29 21.05 32.18 64.44 22.14 32.96
</table>
<bodyText confidence="0.99933375">
is a viable approach for dealing with negation. How-
ever, detection of negated events is the main weak-
ness of our approach, and this should be tackled in
future work. We also plan to improve our method
for detecting affixal negations to increment its recall,
by using further WordNet relations such as “derived
from adjective”, and “pertains to noun”, as well as
to extend this method to detect infixal negations.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999798666666667">
This research is funded by the Spanish Ministry of
Science and Innovation (TIN2009-14659-C03-01)
and the Ministry of Education (FPU program).
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99953612">
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gerv´as. 2010. A hybrid approach to emotional sen-
tence polarity and intensity classification. In Proceed-
ings of the 14th Conference on Computational Natural
Language Learning (CoNLL 2010), pages 153–161.
W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper,
and B.G. Buchanan. 2001. A simple algorithm for
identifying negated findings and diseases in discharge
summaries. J Biomed Inform, 34:301–310.
Isaac Councill, Ryan McDonald, and Leonid Velikovich.
2010. What’s great and what’s not: learning to classify
the scope of negation for improved sentiment analysis.
In Proceedings of the Workshop on Negation and Spec-
ulation in Natural Language Processing, pages 51–59.
Jung-Jae Kim and Jong C. Park. 2006. Extracting con-
trastive information from negation patterns in biomed-
ical literature. ACM Trans. on Asian Language Infor-
mation Processing, 5(1):44–60.
Roser Morante and Eduardo Blanco. 2012. Sem 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the 1st Joint Conference on
Lexical and Computational Semantics (*SEM 2012).
Roser Morante and Walter Daelemans. 2012.
Conandoyle-neg: Annotation of negation in conan
doyle stories. In Proceedings of the 8th International
Conference on Language Resources and Evaluation.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the scope of negation in
biomedical texts. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 715–724.
Roser Morante. 2010. Descriptive Analysis of Negation
Cues in Biomedical Texts. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation.
A.G. Mutalik, A. Deshpande, and P.M. Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents. A quantita-
tive study using the UMLS. JAm Med Inform Assoc,
8(6):598–609.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2005. SenseRelate::TargetWord: a generalized
framework for word sense disambiguation. In Pro-
ceedings of the ACL 2005 on Interactive poster and
demonstration sessions, pages 73–76.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, pages 60–68.
</reference>
<page confidence="0.997377">
287
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.764573">
<title confidence="0.9989465">UCM-I: A Rule-based Syntactic Approach for Resolving the Scope of Negation</title>
<author confidence="0.969757">Jorge Carrillo de_Albornoz</author>
<author confidence="0.969757">Laura Plaza</author>
<author confidence="0.969757">Alberto Diaz</author>
<author confidence="0.969757">Miguel</author>
<affiliation confidence="0.883215">Universidad Complutense de C/ Prof. Jos´e Garc´ıa Santesmases,</affiliation>
<address confidence="0.998262">28040 Madrid</address>
<abstract confidence="0.99958819047619">This paper presents one of the two contributions from the Universidad Complutense de Madrid to the *SEM Shared Task 2012 on Resolving the Scope and Focus of Negation. We describe a rule-based system for detecting the presence of negations and delimitating their scope. It was initially intended for processing negation in opinionated texts, and has been adapted to fit the task requirements. It first detects negation cues using a list of explicit markers (such as and infers other implicit negations (such as affixal e.g, by using semantic information from WordNet concepts and relations. It next uses the information from the syntax tree of the sentence in which the negation arises to get a first approximation to the negation scope, which is later refined using a set of post-processing rules that bound or expand such scope.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jorge Carrillo de Albornoz</author>
<author>Laura Plaza</author>
<author>Pablo Gerv´as</author>
</authors>
<title>A hybrid approach to emotional sentence polarity and intensity classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th Conference on Computational Natural Language Learning (CoNLL</booktitle>
<pages>153--161</pages>
<marker>de Albornoz, Plaza, Gerv´as, 2010</marker>
<rawString>Jorge Carrillo de Albornoz, Laura Plaza, and Pablo Gerv´as. 2010. A hybrid approach to emotional sentence polarity and intensity classification. In Proceedings of the 14th Conference on Computational Natural Language Learning (CoNLL 2010), pages 153–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Chapman</author>
<author>W Bridewell</author>
<author>P Hanbury</author>
<author>G F Cooper</author>
<author>B G Buchanan</author>
</authors>
<title>A simple algorithm for identifying negated findings and diseases in discharge summaries.</title>
<date>2001</date>
<journal>J Biomed Inform,</journal>
<pages>34--301</pages>
<contexts>
<context position="1841" citStr="Chapman et al., 2001" startWordPosition="291" endWordPosition="295">ting negation is important for many NLP tasks, as it may reverse the meaning of the text affected by it. In information extraction, for instance, it is obviously important to distinguish negated information from affirmative one (Kim and Park, 2006). It may also improve automatic indexing (Mutalik et al., 2001). In sentiment analysis, detecting and dealing with negation is critical, as it may change the polarity of a text (Wiegand et al., 2010). However, research on negation has mainly focused on the biomedical domain, and addressed the problem of detecting if a medical term is negated or not (Chapman et al., 2001), or the scope of different negation signals (Morante et al., 2008). During the last years, the importance of processing negation is gaining recognition by the NLP research community, as evidenced by the success of several initiatives such as the Negation and Speculation in Natural Language Processing workshop (NeSp-NLP 2010)1 or the CoNLL-2010 Shared Task2, which aimed at identifying hedges and their scope in natural language texts. In spite of this, most of the approaches proposed so far deal with negation in a superficial manner. This paper describes our contribution to the *SEM Shared Task</context>
</contexts>
<marker>Chapman, Bridewell, Hanbury, Cooper, Buchanan, 2001</marker>
<rawString>W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper, and B.G. Buchanan. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries. J Biomed Inform, 34:301–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac Councill</author>
<author>Ryan McDonald</author>
<author>Leonid Velikovich</author>
</authors>
<title>What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,</booktitle>
<pages>51--59</pages>
<contexts>
<context position="6352" citStr="Councill et al., 2010" startWordPosition="1029" endWordPosition="1032"> non nor nobody never nowhere ... Words with implicit negation cues unpleasant unnatural dislike impatient fearless hopeless illegal ... have into account these negation cues when analyzing opinionated texts because these words themselves usually appear in affective lexicons with their corresponding polarity values (i.e., impatient, for instance, appears in SentiWordNet with a negative polarity value). In order to detect negation cues, we use a list of predefined negation signals, along with an automatic method for detecting new ones. The list has been extracted from different previous works (Councill et al., 2010; Morante, 2010). This list also includes the most frequent contracted forms (e.g., don’t, didn’t, etc.). The automated method, in turn, is intended for discovering in text new affixal negation cues. To this end, we first find in the text all words with prefixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suffix -less that present the appropriate part of speech. Since not all words with such affixes are negation cues, we use semantic information from WordNet concepts and relations to decide. In this way, we retrieve from WordNet the synset that correspond to each word, using WordNet::Sense</context>
</contexts>
<marker>Councill, McDonald, Velikovich, 2010</marker>
<rawString>Isaac Councill, Ryan McDonald, and Leonid Velikovich. 2010. What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 51–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jung-Jae Kim</author>
<author>Jong C Park</author>
</authors>
<title>Extracting contrastive information from negation patterns in biomedical literature.</title>
<date>2006</date>
<journal>ACM Trans. on Asian Language Information Processing,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1468" citStr="Kim and Park, 2006" startWordPosition="227" endWordPosition="230">xal negations, e.g, undeniable or improper) by using semantic information from WordNet concepts and relations. It next uses the information from the syntax tree of the sentence in which the negation arises to get a first approximation to the negation scope, which is later refined using a set of post-processing rules that bound or expand such scope. 1 Introduction Detecting negation is important for many NLP tasks, as it may reverse the meaning of the text affected by it. In information extraction, for instance, it is obviously important to distinguish negated information from affirmative one (Kim and Park, 2006). It may also improve automatic indexing (Mutalik et al., 2001). In sentiment analysis, detecting and dealing with negation is critical, as it may change the polarity of a text (Wiegand et al., 2010). However, research on negation has mainly focused on the biomedical domain, and addressed the problem of detecting if a medical term is negated or not (Chapman et al., 2001), or the scope of different negation signals (Morante et al., 2008). During the last years, the importance of processing negation is gaining recognition by the NLP research community, as evidenced by the success of several init</context>
</contexts>
<marker>Kim, Park, 2006</marker>
<rawString>Jung-Jae Kim and Jong C. Park. 2006. Extracting contrastive information from negation patterns in biomedical literature. ACM Trans. on Asian Language Information Processing, 5(1):44–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Eduardo Blanco</author>
</authors>
<title>Sem</title>
<date>2012</date>
<booktitle>In Proceedings of the 1st Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="2930" citStr="Morante and Blanco, 2012" startWordPosition="473" endWordPosition="476">the approaches proposed so far deal with negation in a superficial manner. This paper describes our contribution to the *SEM Shared Task 2012 on Resolving the Scope and Focus of Negation. As its name suggests, the task aims at detecting the scope and focus of negation, as a means of encouraging research in negation processing. In particular, we participate in Task 1: scope detection. For each negation in the text, the negation cue must be detected, and its scope marked. Moreover, the event or property that is negated must be recognized. A comprehensive description of the task may be found in (Morante and Blanco, 2012). For the sake of clarity, it is important to define what the organization of the task understands by negation cue, scope of negation and negated event. The words that express negation are called negation cues. Not and no are common examples of such cues. Scope is defined as the part of the meaning that is negated, and encloses all negated concepts. The negated event is the property that is 1http://www.clips.ua.ac.be/NeSpNLP2010/ 2www.inf.u-szeged.hu/rgai/conll2010st/ 282 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 282–287, Montr´eal, Canada, June 7-8, 2012. c�2</context>
<context position="13621" citStr="Morante and Blanco, 2012" startWordPosition="2301" endWordPosition="2304">ed events We only consider a single type of negated events, so that, when a cue word contains a negative affix, the word after removing the affix is annotated as the negated event. In this way, “doubtedly” is correctly annotated as the negated event in sentence (9). However, the remaining types of negated events are relegated to future work. (9) [The oval seal is] undoubtedly [a plain sleeve-link]. 3 Evaluation Setup The data collection consists of a development set, a training set, and two test sets of 787, 3644, 496 and 593 sentences, respectively from different stories by Conan Doyle (see (Morante and Blanco, 2012) for details). Performance is measured in terms of recall, precision and F-measure for the following subtasks: • Predicting negation cues. • Predicting both the scope and cue. • Predicting the scope, the cue does not need to be correct. • Predicting the scope tokens, where not a full scope match is required. • Predicting negated events. • Full evaluation, which requires all elements to be correct. 285 Table 4: Results for the development set. Metric Pr. Re. F-1 Cues 92.55 86.13 89.22 Scope (cue match) 86.05 44.05 58.27 Scope (no cue match) 86.05 44.05 58.27 Scope tokens (no cue match) 88.05 59</context>
</contexts>
<marker>Morante, Blanco, 2012</marker>
<rawString>Roser Morante and Eduardo Blanco. 2012. Sem 2012 shared task: Resolving the scope and focus of negation. In Proceedings of the 1st Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>Conandoyle-neg: Annotation of negation in conan doyle stories.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="3896" citStr="Morante and Daelemans, 2012" startWordPosition="624" endWordPosition="627">ted concepts. The negated event is the property that is 1http://www.clips.ua.ac.be/NeSpNLP2010/ 2www.inf.u-szeged.hu/rgai/conll2010st/ 282 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 282–287, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics negated by the cue. For instance, in the sentence: [Holmes] did not [say anything], the scope is enclosed in square brackets, the negation cue is underlined and the negated event is shown in bold. More details about the annotation of negation cues, scopes and negated events may be found in (Morante and Daelemans, 2012). The system presented to the shared task is an adaptation of the one published in (Carrillo de Albornoz et al., 2010), whose aim was to detect and process negation in opinionated text in order to improve polarity and intensity classification. When classifying sentiments and opinions it is important to deal with the presence of negations and their effect on the emotional meaning of the text affected by them. Consider the sentence (1) and (2). Sentence (1) expresses a positive opinion, whereas that in sentence (2) the negation word not reverses the polarity of such opinion. (1) I liked this hot</context>
</contexts>
<marker>Morante, Daelemans, 2012</marker>
<rawString>Roser Morante and Walter Daelemans. 2012. Conandoyle-neg: Annotation of negation in conan doyle stories. In Proceedings of the 8th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Anthony Liekens</author>
<author>Walter Daelemans</author>
</authors>
<title>Learning the scope of negation in biomedical texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>715--724</pages>
<contexts>
<context position="1908" citStr="Morante et al., 2008" startWordPosition="303" endWordPosition="306">e meaning of the text affected by it. In information extraction, for instance, it is obviously important to distinguish negated information from affirmative one (Kim and Park, 2006). It may also improve automatic indexing (Mutalik et al., 2001). In sentiment analysis, detecting and dealing with negation is critical, as it may change the polarity of a text (Wiegand et al., 2010). However, research on negation has mainly focused on the biomedical domain, and addressed the problem of detecting if a medical term is negated or not (Chapman et al., 2001), or the scope of different negation signals (Morante et al., 2008). During the last years, the importance of processing negation is gaining recognition by the NLP research community, as evidenced by the success of several initiatives such as the Negation and Speculation in Natural Language Processing workshop (NeSp-NLP 2010)1 or the CoNLL-2010 Shared Task2, which aimed at identifying hedges and their scope in natural language texts. In spite of this, most of the approaches proposed so far deal with negation in a superficial manner. This paper describes our contribution to the *SEM Shared Task 2012 on Resolving the Scope and Focus of Negation. As its name sug</context>
</contexts>
<marker>Morante, Liekens, Daelemans, 2008</marker>
<rawString>Roser Morante, Anthony Liekens, and Walter Daelemans. 2008. Learning the scope of negation in biomedical texts. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 715–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
</authors>
<title>Descriptive Analysis of Negation Cues in Biomedical Texts.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="6368" citStr="Morante, 2010" startWordPosition="1033" endWordPosition="1034">owhere ... Words with implicit negation cues unpleasant unnatural dislike impatient fearless hopeless illegal ... have into account these negation cues when analyzing opinionated texts because these words themselves usually appear in affective lexicons with their corresponding polarity values (i.e., impatient, for instance, appears in SentiWordNet with a negative polarity value). In order to detect negation cues, we use a list of predefined negation signals, along with an automatic method for detecting new ones. The list has been extracted from different previous works (Councill et al., 2010; Morante, 2010). This list also includes the most frequent contracted forms (e.g., don’t, didn’t, etc.). The automated method, in turn, is intended for discovering in text new affixal negation cues. To this end, we first find in the text all words with prefixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suffix -less that present the appropriate part of speech. Since not all words with such affixes are negation cues, we use semantic information from WordNet concepts and relations to decide. In this way, we retrieve from WordNet the synset that correspond to each word, using WordNet::SenseRelate (Patwardh</context>
</contexts>
<marker>Morante, 2010</marker>
<rawString>Roser Morante. 2010. Descriptive Analysis of Negation Cues in Biomedical Texts. In Proceedings of the 7th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A G Mutalik</author>
<author>A Deshpande</author>
<author>P M Nadkarni</author>
</authors>
<title>Use of general-purpose negation detection to augment concept indexing of medical documents. A quantitative study using the UMLS. JAm Med Inform Assoc,</title>
<date>2001</date>
<contexts>
<context position="1531" citStr="Mutalik et al., 2001" startWordPosition="237" endWordPosition="240"> information from WordNet concepts and relations. It next uses the information from the syntax tree of the sentence in which the negation arises to get a first approximation to the negation scope, which is later refined using a set of post-processing rules that bound or expand such scope. 1 Introduction Detecting negation is important for many NLP tasks, as it may reverse the meaning of the text affected by it. In information extraction, for instance, it is obviously important to distinguish negated information from affirmative one (Kim and Park, 2006). It may also improve automatic indexing (Mutalik et al., 2001). In sentiment analysis, detecting and dealing with negation is critical, as it may change the polarity of a text (Wiegand et al., 2010). However, research on negation has mainly focused on the biomedical domain, and addressed the problem of detecting if a medical term is negated or not (Chapman et al., 2001), or the scope of different negation signals (Morante et al., 2008). During the last years, the importance of processing negation is gaining recognition by the NLP research community, as evidenced by the success of several initiatives such as the Negation and Speculation in Natural Languag</context>
</contexts>
<marker>Mutalik, Deshpande, Nadkarni, 2001</marker>
<rawString>A.G. Mutalik, A. Deshpande, and P.M. Nadkarni. 2001. Use of general-purpose negation detection to augment concept indexing of medical documents. A quantitative study using the UMLS. JAm Med Inform Assoc, 8(6):598–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>SenseRelate::TargetWord: a generalized framework for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>73--76</pages>
<contexts>
<context position="6984" citStr="Patwardhan et al., 2005" startWordPosition="1135" endWordPosition="1139">e, 2010). This list also includes the most frequent contracted forms (e.g., don’t, didn’t, etc.). The automated method, in turn, is intended for discovering in text new affixal negation cues. To this end, we first find in the text all words with prefixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suffix -less that present the appropriate part of speech. Since not all words with such affixes are negation cues, we use semantic information from WordNet concepts and relations to decide. In this way, we retrieve from WordNet the synset that correspond to each word, using WordNet::SenseRelate (Patwardhan et al., 2005) to correctly disambiguate the meaning of the word according to its context, along with all its antonym synsets. We next check if, after removing the affix, the word exists in WordNet and belongs to any of the antonym synsets. If so, we consider the original word to be a negation cue (i.e., the word without the affix has the opposite meaning than the lexical item with the affix). Table 1 presents some examples of explicit negation cues and words with implicit negation cues. For space reasons, not all cues are shown. We also consider common spelling errors such as the omission of apostrophes (e</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2005</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2005. SenseRelate::TargetWord: a generalized framework for word sense disambiguation. In Proceedings of the ACL 2005 on Interactive poster and demonstration sessions, pages 73–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wiegand</author>
<author>Alexandra Balahur</author>
<author>Benjamin Roth</author>
<author>Dietrich Klakow</author>
<author>Andr´es Montoyo</author>
</authors>
<title>A survey on the role of negation in sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,</booktitle>
<pages>60--68</pages>
<contexts>
<context position="1667" citStr="Wiegand et al., 2010" startWordPosition="261" endWordPosition="264">n arises to get a first approximation to the negation scope, which is later refined using a set of post-processing rules that bound or expand such scope. 1 Introduction Detecting negation is important for many NLP tasks, as it may reverse the meaning of the text affected by it. In information extraction, for instance, it is obviously important to distinguish negated information from affirmative one (Kim and Park, 2006). It may also improve automatic indexing (Mutalik et al., 2001). In sentiment analysis, detecting and dealing with negation is critical, as it may change the polarity of a text (Wiegand et al., 2010). However, research on negation has mainly focused on the biomedical domain, and addressed the problem of detecting if a medical term is negated or not (Chapman et al., 2001), or the scope of different negation signals (Morante et al., 2008). During the last years, the importance of processing negation is gaining recognition by the NLP research community, as evidenced by the success of several initiatives such as the Negation and Speculation in Natural Language Processing workshop (NeSp-NLP 2010)1 or the CoNLL-2010 Shared Task2, which aimed at identifying hedges and their scope in natural lang</context>
</contexts>
<marker>Wiegand, Balahur, Roth, Klakow, Montoyo, 2010</marker>
<rawString>Michael Wiegand, Alexandra Balahur, Benjamin Roth, Dietrich Klakow, and Andr´es Montoyo. 2010. A survey on the role of negation in sentiment analysis. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 60–68.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>