<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000032">
<title confidence="0.9993345">
An Improved MDL-Based Compression Algorithm for
Unsupervised Word Segmentation
</title>
<author confidence="0.995495">
Ruey-Cheng Chen
</author>
<affiliation confidence="0.996744">
National Taiwan University
</affiliation>
<address confidence="0.760655">
1 Roosevelt Rd. Sec. 4
Taipei 106, Taiwan
</address>
<email confidence="0.998081">
rueycheng@turing.csie.ntu.edu.tw
</email>
<sectionHeader confidence="0.993872" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999791">
We study the mathematical properties of
a recently proposed MDL-based unsuper-
vised word segmentation algorithm, called
regularized compression. Our analysis
shows that its objective function can be
efficiently approximated using the nega-
tive empirical pointwise mutual informa-
tion. The proposed extension improves the
baseline performance in both efficiency
and accuracy on a standard benchmark.
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999653470588235">
Hierarchical Bayes methods have been main-
stream in unsupervised word segmentation since
the dawn of hierarchical Dirichlet process (Gold-
water et al., 2009) and adaptors grammar (Johnson
and Goldwater, 2009). Despite this wide recog-
nition, they are also notoriously computational
prohibitive and have limited adoption on larger
corpora. While much effort has been directed
to mitigating this issue within the Bayes frame-
work (Borschinger and Johnson, 2011), many
have found minimum description length (MDL)
based methods more promising in addressing the
scalability problem.
MDL-based methods (Rissanen, 1978) rely on
underlying search algorithms to segment the text
in as many possible ways and use description
length to decide which to output. As differ-
ent algorithms explore different trajectories in
the search space, segmentation accuracy depends
largely on the search coverage. Early work in this
line focused more on existing segmentation algo-
rithm, such as branching entropy (Tanaka-Ishii,
2005; Zhikov et al., 2010) and bootstrap voting ex-
perts (Hewlett and Cohen, 2009; Hewlett and Co-
hen, 2011). A recent study (Chen et al., 2012) on
a compression-based algorithm, regularized com-
pression, has achieved comparable performance
result to hierarchical Bayes methods.
Along this line, in this paper we present a novel
extension to the regularized compressor algorithm.
We propose a lower-bound approximate to the
original objective and show that, through analy-
sis and experimentation, this amendment improves
segmentation performance and runtime efficiency.
</bodyText>
<sectionHeader confidence="0.996403" genericHeader="method">
2 Regularized Compression
</sectionHeader>
<bodyText confidence="0.999983142857143">
The dynamics behind regularized compression is
similar to digram coding (Witten et al., 1999). One
first breaks the text down to a sequence of char-
acters (W0) and then works from that represen-
tation up in an agglomerative fashion, iteratively
removing word boundaries between the two se-
lected word types. Hence, a new sequence Wi
is created in the i-th iteration by merging all the
occurrences of some selected bigram (x, y) in the
original sequence Wi−1. Unlike in digram cod-
ing, where the most frequent pair of word types is
always selected, in regularized compression a spe-
cialized decision criterion is used to balance com-
pression rate and vocabulary complexity:
</bodyText>
<equation confidence="0.997685">
min. −αf(x, y) + |Wi−1|A ˜H(Wi−1, Wi)
s.t. either x or y is a character
f(x, y) &gt; nms.
</equation>
<bodyText confidence="0.993894571428571">
Here, the criterion is written slightly differ-
ently. Note that f(x, y) is the bigram fre-
quency, |Wi−1 |the sequence length of Wi−1, and
A ˜H(Wi−1, Wi) = ˜H(Wi) − ˜H(Wi−1) is the dif-
ference between the empirical Shannon entropy
measured on Wi and Wi−1, using maximum like-
lihood estimates. Specifically, this empirical esti-
</bodyText>
<listItem confidence="0.6353655">
˜H(W) for a sequence W corresponds to:
1 log |W |− |W |1: f (x) log f (x)
</listItem>
<subsectionHeader confidence="0.441558">
x:types
</subsectionHeader>
<bodyText confidence="0.9355635">
For this equation to work, one needs to estimate
other model parameters. See Chen et al. (2012)
for a comprehensive treatment.
mate
</bodyText>
<page confidence="0.970455">
166
</page>
<note confidence="0.347585">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 166–170,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<equation confidence="0.990718">
f(x) f(y) f(z) |W|
Wi−1 k l 0 N
Wi k − m l − m m N − m
</equation>
<bodyText confidence="0.874209166666667">
Table 1: The change between iterations in word
frequency and sequence length in regularized
compression. In the new sequence Wi, each oc-
currence of the x-y bigram is replaced with a new
(conceptually unseen) word z. This has an effect
of reducing the number of words in the sequence.
</bodyText>
<sectionHeader confidence="0.992148" genericHeader="method">
3 Change in Description Length
</sectionHeader>
<bodyText confidence="0.999743285714286">
The second term of the aforementioned objective
is in fact an approximate to the change in descrip-
tion length. This is made obvious by coding up a
sequence W using the Shannon code, with which
the description length of W is equal to |W  |˜H(W).
Here, the change in description length between se-
quences Wi−1 and Wi is written as:
</bodyText>
<equation confidence="0.995895">
ΔL = |Wi |˜H(W) − |Wi−1|˜H(Wi−1). (1)
</equation>
<bodyText confidence="0.999964571428572">
Let us focus on this equation. Suppose that the
original sequence Wi−1 is N-word long, the se-
lected word type pair x and y each occurs k and l
times, respectively, and altogether x-y bigram oc-
curs m times in Wi−1. In the new sequence Wi,
each of the m bigrams is replaced with an un-
seen word z = xy. These altogether have reduced
the sequence length by m. The end result is that
compression moves probability masses from one
place to the other, causing a change in descrip-
tion length. See Table 1 for a summary to this
exchange.
Now, as we expand Equation (1) and reorganize
the remaining, we find that:
</bodyText>
<equation confidence="0.680614">
ΔL = (N − m) log(N − m) − N log N
+ k log k − (k − m) log(k − m)
+ l log l − (l − m) log(l − m) (2)
</equation>
<bodyText confidence="0.974406">
+ 0 log 0 − m log m
Note that each line in Equation (2) is of the form
x1 log x1 − x2 log x2 for some x1, x2 &gt; 0. We
exploit this pattern and derive a bound for ΔL
through analysis. Consider g(x) = x log x. Since
g&apos;&apos;(x) &gt; 0 for x &gt; 0, by the Taylor series we have
the following relations for any x1, x2 &gt; 0:
</bodyText>
<equation confidence="0.975312333333333">
g(x1) − g(x2) &lt; (x1 − x2)g&apos;(x1),
g(x1) − g(x2) &gt; (x1 − x2)g&apos;(x2).
Plugging these into Equation (2), we have:
(k − m)(l − m)
m log &lt; ΔL &lt; oc. (3)
Nm
</equation>
<bodyText confidence="0.999948">
The lower bound1 at the left-hand side is a best-
case estimate. As our aim is to minimize ΔL, we
use this quantity to serve as an approximate.
</bodyText>
<sectionHeader confidence="0.987668" genericHeader="method">
4 Proposed Method
</sectionHeader>
<bodyText confidence="0.971840666666667">
Based on this finding, we propose the following
two variations (see Figure 1) for the regularized
compression framework:
</bodyText>
<listItem confidence="0.973700909090909">
• G1: Replacing the second term in the origi-
nal objective with the lower bound in Equa-
tion (3). The new objective function is writ-
ten out as Equation (4).
• G2: Same as G1 except that the lower bound
is divided by f(x, y) beforehand. The nor-
malized lower bound approximates the per-
word change in description length, as shown
in Equation (5). With this variation, the func-
tion remains in a scalarized form as the orig-
inal does.
</listItem>
<bodyText confidence="0.9999922">
We use the following procedure to compute de-
scription length. Given a word sequence W, we
write out all the induced word types (say, M types
in total) entry by entry as a character sequence, de-
noted as C. Then the overall description length is:
</bodyText>
<equation confidence="0.9966285">
|W |˜H(W) + |C |˜H(C) + M − 1
2 log |W|. (6)
</equation>
<bodyText confidence="0.909286388888889">
Three free parameters, α, ρ, and nms remain to
be estimated. A detailed treatment on parameter
estimation is given in the following paragraphs.
Trade-off α This parameter controls the bal-
ance between compression rate and vocabulary
complexity. Throughout this experiment, we es-
timated this parameter using MDL-based grid
search. Multiple search runs at different granular-
ity levels were employed as necessary.
Compression rate ρ This is the minimum
threshold value for compression rate. The com-
pressor algorithm would go on as many iteration
as possible until the overall compression rate (i.e.,
1Sharp-eyed readers may have noticed the similarity be-
tween the lower bound and the negative (empirical) point-
wise mutual information. In fact, when f(z) &gt; 0 in Wi−1, it
can be shown that limm→0 OL/m converges to the empirical
pointwise mutual information (proof omitted here).
</bodyText>
<page confidence="0.898736">
167
</page>
<equation confidence="0.99479075">
G1 ≡ f (x, y) I log (f (x) − f (x, y)) (f (y) − f (x, y)) − αl (4)
|Wi−1 |f (x, y) J
G2 ≡ −αf(x, y) + log (f(x) − f(x, y))(f(y) − f(x, y)) (5)
|Wi−1|f(x, y)
</equation>
<figureCaption confidence="0.998977">
Figure 1: The two newly-proposed objective functions.
</figureCaption>
<bodyText confidence="0.996975714285714">
word/character ratio) is lower than p. Setting this
value to 0 forces the compressor to go on until
no more can be done. In this paper, we exper-
imented with predetermined p values as well as
those learned from MDL-based grid search.
Minimum support nms We simply followed the
suggested setting nms = 3 (Chen et al., 2012).
</bodyText>
<equation confidence="0.975859875">
Run
Baseline
G1 (a) α :
G1 (b) p :
G1 (c) α :
G2 (a) α :
G2 (b) p :
G2 (c) α :
</equation>
<table confidence="0.994729625">
P R F
76.9 81.6 79.2
0.030 76.4 79.9 78.1
0.38 73.4 80.2 76.8
0.010 75.7 80.4 78.0
0.002 82.1 80.0 81.0
0.36 79.1 81.7 80.4
0.004 79.3 84.2 81.7
</table>
<sectionHeader confidence="0.99574" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.868526">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999867909090909">
In the experiment, we tested our methods on
Brent’s derivation of the Bernstein-Ratner cor-
pus (Brent and Cartwright, 1996; Bernstein-
Ratner, 1987). This dataset is distributed via the
CHILDES project (MacWhinney and Snow, 1990)
and has been commonly used as a standard bench-
mark for phonetic segmentation. Our baseline
method is the original regularized compressor al-
gorithm (Chen et al., 2012). In our experiment, we
considered the following three search settings for
finding the model parameters:
</bodyText>
<listItem confidence="0.994727833333333">
(a) Fix p to 0 and vary α to find the best value (in
the sense of description length);
(b) Fix α to the best value found in setting (a)
and vary p;
(c) Set p to a heuristic value 0.37 (Chen et al.,
2012) and vary α.
</listItem>
<bodyText confidence="0.63107875">
Settings (a) and (b) can be seen as running a
stochastic grid searcher one round for each param-
eter2. Note that we tested (c) here only to compare
with the best baseline setting.
</bodyText>
<subsectionHeader confidence="0.964443">
5.2 Result
</subsectionHeader>
<bodyText confidence="0.622029666666667">
Table 2 summarizes the result for each objective
and each search setting. The best (α, p) pair for
2A more formal way to estimate both α and ρ is to run
a stochastic searcher that varies between settings (a) and (b),
fixing the best value found in the previous run. Here, for
simplicity, we leave this to future work.
</bodyText>
<tableCaption confidence="0.7240495">
Table 2: The performance result on the Bernstein-
Ratner corpus. Segmentation performance is mea-
</tableCaption>
<bodyText confidence="0.987396096774194">
sured using word-level precision (P), recall (R),
and F-measure (F).
G1 is (0.03, 0.38) and the best for G2 is (0.002,
0.36). On one hand, the performance of G1 is con-
sistently inferior to the baseline across all settings.
Although approximation error was one possible
cause, we noticed that the compression process
was no longer properly regularized, since f(x, y)
and the AL estimate in the objective are intermin-
gled. In this case, adjusting α has little effect in
balancing compression rate and complexity.
The second objective G2, on the other hand,
did not suffer as much from the aforementioned
lack of regularization. We found that, in all three
settings, G2 outperforms the baseline by 1 to 2
percentage points in F-measure. The best perfor-
mance result achieved by G2 in our experiment is
81.7 in word-level F-measure, although this was
obtained from search setting (c), using a heuristic
p value 0.37. It is interesting to note that G1 (b)
and G2 (b) also gave very close estimates to this
heuristic value. Nevertheless, it remains an open
issue whether there is a connection between the
optimal p value and the true word/token ratio (≈
0.35 for Bernstein-Ratner corpus).
The result has led us to conclude that MDL-
based grid search is efficient in optimizing seg-
mentation accuracy. Minimization of descrip-
tion length is in general aligned with perfor-
mance improvement, although under finer gran-
ularity MDL-based search may not be as effec-
</bodyText>
<page confidence="0.996703">
168
</page>
<table confidence="0.999835833333333">
Method P R F
Adaptors grammar, colloc3-syllable Johnson and Goldwater (2009) 86.1 88.4 87.2
Regularized compression + MDL, G2 (b) — 79.1 81.7 80.4
Regularized compression + MDL Chen et al. (2012) 76.9 81.6 79.2
Adaptors grammar, colloc Johnson and Goldwater (2009) 78.4 75.7 77.1
Particle filter, unigram B¨orschinger and Johnson (2012) – – 77.1
Regularized compression + MDL, G1 (b) — 73.4 80.2 76.8
Bootstrap voting experts + MDL Hewlett and Cohen (2011) 79.3 73.4 76.2
Nested Pitman-Yor process, bigram Mochihashi et al. (2009) 74.8 76.7 75.7
Branching entropy + MDL Zhikov et al. (2010) 76.3 74.5 75.4
Particle filter, bigram B¨orschinger and Johnson (2012) – – 74.5
Hierarchical Dirichlet process Goldwater et al. (2009) 75.2 69.6 72.3
</table>
<tableCaption confidence="0.749394">
Table 3: The performance chart on the Bernstein-Ratner corpus, in descending order of word-level F-
measure. We deliberately reproduced the results for adaptors grammar and regularized compression. The
other measurements came directly from the literature.
</tableCaption>
<bodyText confidence="0.999685307692308">
tive. In our experiment, search setting (b) won
out on description length for both objectives, while
the best performance was in fact achieved by the
others. It would be interesting to confirm this
by studying the correlation between description
length and word-level F-measure.
In Table 3, we summarize many published re-
sults for segmentation methods ever tested on the
Bernstein-Ratner corpus. Of the proposed meth-
ods, we include only setting (b) since it is more
general than the others. From Table 3, we find that
the performance of G2 (b) is competitive to other
state-of-the-art hierarchical Bayesian models and
MDL methods, though it still lags 7 percentage
points behind the best result achieved by adap-
tors grammar with colloc3-syllable. We also com-
pare adaptors grammar to regularized compressor
on average running time, which is shown in Ta-
ble 4. On our test machine, it took roughly 15
hours for one instance of adaptors grammar with
colloc3-syllable to run to the finish. Yet an im-
proved regularized compressor could deliver the
result in merely 1.25 second. In other words, even
in an 100 × 100 grid search, the regularized com-
pressor algorithm can still finish 4 to 5 times ear-
lier than one single adaptors grammar instance.
</bodyText>
<sectionHeader confidence="0.93823" genericHeader="conclusions">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.9995415">
In this paper, we derive a new lower-bound ap-
proximate to the objective function used in the
regularized compression algorithm. As computing
the approximate no longer relies on the change in
lexicon entropy, the new compressor algorithm is
made more efficient than the original. Besides run-
</bodyText>
<table confidence="0.998711666666667">
Method Time (s)
Adaptors grammar, colloc3-syllable 53826
Adaptors grammar, colloc 10498
Regularized compressor 1.51
Regularized compressor, G1 (b) 0.60
Regularized compressor, G2 (b) 1.25
</table>
<tableCaption confidence="0.872588">
Table 4: The average running time in seconds on
</tableCaption>
<bodyText confidence="0.99901619047619">
the Bernstein-Ratner corpus for adaptors grammar
(per fold, based on trace output) and regularized
compressors, tested on an Intel Xeon 2.5GHz 8-
core machine with 8GB RAM.
time efficiency, our experiment result also shows
improved performance. Using MDL alone, one
proposed method outperforms the original regu-
larized compressor (Chen et al., 2012) in preci-
sion by 2 percentage points and in F-measure by 1.
Its performance is only second to the state of the
art, achieved by adaptors grammar with colloc3-
syllable (Johnson and Goldwater, 2009).
A natural extension of this work is to repro-
duce this result on some other word segmenta-
tion benchmarks, specifically those in other Asian
languages (Emerson, 2005; Zhikov et al., 2010).
Furthermore, it would be interesting to investigate
stochastic optimization techniques for regularized
compression that simultaneously fit both α and p.
We believe this would be the key to adapt the al-
gorithm to larger datasets.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998578">
We thank the anonymous reviewers for their valu-
able feedback.
</bodyText>
<page confidence="0.996236">
169
</page>
<bodyText confidence="0.979255">
Brian MacWhinney and Catherine Snow. 1990. The
child language data exchange system: an update.
Journal of child language, 17(2):457–472, June.
</bodyText>
<sectionHeader confidence="0.465309" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.752051">
Nan Bernstein-Ratner. 1987. The phonology of parent
child speech. Children’s language, 6:159–174.
</bodyText>
<reference confidence="0.996553722891566">
Benjamin Borschinger and Mark Johnson. 2011. A
particle filter algorithm for bayesian word segmen-
tation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 10–
18, Canberra, Australia, December.
Benjamin B¨orschinger and Mark Johnson. 2012. Us-
ing rejuvenation to improve particle filtering for
bayesian word segmentation. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 85–89, Jeju Island, Korea, July. Association
for Computational Linguistics.
Michael R. Brent and Timothy A. Cartwright. 1996.
Distributional regularity and phonotactic constraints
are useful for segmentation. In Cognition, pages 93–
125.
Ruey-Cheng Chen, Chiung-Min Tsai, and Jieh Hsiang.
2012. A regularized compression method to unsu-
pervised word segmentation. In Proceedings of the
Twelfth Meeting of the Special Interest Group on
Computational Morphology and Phonology, SIG-
MORPHON ’12, pages 26–34, Montreal, Canada.
Association for Computational Linguistics.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, volume 133. Jeju Island, Korea.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21–54, July.
Daniel Hewlett and Paul Cohen. 2009. Bootstrap vot-
ing experts. In Proceedings of the 21st international
jont conference on Artifical intelligence, IJCAI’09,
pages 1071–1076, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Daniel Hewlett and Paul Cohen. 2011. Fully unsuper-
vised word segmentation with BVE and MDL. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
’11, pages 540–545, Portland, Oregon. Association
for Computational Linguistics.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ’09, pages
317–325, Boulder, Colorado. Association for Com-
putational Linguistics.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of theAFNLP: Volume 1 - Volume 1, ACL ’09, pages
100–108, Suntec, Singapore. Association for Com-
putational Linguistics.
Jorma Rissanen. 1978. Modeling by shortest data de-
scription. Automatica, 14(5):465–471, September.
Kumiko Tanaka-Ishii. 2005. Entropy as an indica-
tor of context boundaries: An experiment using a
web search engine. In Robert Dale, Kam-Fai Wong,
Jian Su, and Oi Kwong, editors, Natural Language
Processing IJCNLP 2005, volume 3651 of Lecture
Notes in Computer Science, chapter 9, pages 93–
105. Springer Berlin / Heidelberg, Berlin, Heidel-
berg.
Ian H. Witten, Alistair Moffat, and Timothy C. Bell.
1999. Managing gigabytes (2nd ed.): compressing
and indexing documents and images. Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA.
Valentin Zhikov, Hiroya Takamura, and Manabu Oku-
mura. 2010. An efficient algorithm for unsuper-
vised word segmentation with branching entropy
and MDL. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’10, pages 832–842, Cambridge,
Massachusetts. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.997407">
170
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.296923">
<title confidence="0.995553">An Improved MDL-Based Compression Algorithm Unsupervised Word Segmentation</title>
<author confidence="0.423463">Ruey-Cheng</author>
<affiliation confidence="0.655736">National Taiwan</affiliation>
<address confidence="0.629047">1 Roosevelt Rd. Sec. Taipei 106,</address>
<email confidence="0.959042">rueycheng@turing.csie.ntu.edu.tw</email>
<abstract confidence="0.997537818181818">We study the mathematical properties of a recently proposed MDL-based unsupervised word segmentation algorithm, called regularized compression. Our analysis shows that its objective function can be efficiently approximated using the negative empirical pointwise mutual information. The proposed extension improves the baseline performance in both efficiency and accuracy on a standard benchmark.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Benjamin Borschinger</author>
<author>Mark Johnson</author>
</authors>
<title>A particle filter algorithm for bayesian word segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>10--18</pages>
<location>Canberra, Australia,</location>
<contexts>
<context position="1072" citStr="Borschinger and Johnson, 2011" startWordPosition="143" endWordPosition="146">ative empirical pointwise mutual information. The proposed extension improves the baseline performance in both efficiency and accuracy on a standard benchmark. 1 Introduction Hierarchical Bayes methods have been mainstream in unsupervised word segmentation since the dawn of hierarchical Dirichlet process (Goldwater et al., 2009) and adaptors grammar (Johnson and Goldwater, 2009). Despite this wide recognition, they are also notoriously computational prohibitive and have limited adoption on larger corpora. While much effort has been directed to mitigating this issue within the Bayes framework (Borschinger and Johnson, 2011), many have found minimum description length (MDL) based methods more promising in addressing the scalability problem. MDL-based methods (Rissanen, 1978) rely on underlying search algorithms to segment the text in as many possible ways and use description length to decide which to output. As different algorithms explore different trajectories in the search space, segmentation accuracy depends largely on the search coverage. Early work in this line focused more on existing segmentation algorithm, such as branching entropy (Tanaka-Ishii, 2005; Zhikov et al., 2010) and bootstrap voting experts (H</context>
</contexts>
<marker>Borschinger, Johnson, 2011</marker>
<rawString>Benjamin Borschinger and Mark Johnson. 2011. A particle filter algorithm for bayesian word segmentation. In Proceedings of the Australasian Language Technology Association Workshop 2011, pages 10– 18, Canberra, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin B¨orschinger</author>
<author>Mark Johnson</author>
</authors>
<title>Using rejuvenation to improve particle filtering for bayesian word segmentation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>85--89</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<marker>B¨orschinger, Johnson, 2012</marker>
<rawString>Benjamin B¨orschinger and Mark Johnson. 2012. Using rejuvenation to improve particle filtering for bayesian word segmentation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 85–89, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
<author>Timothy A Cartwright</author>
</authors>
<title>Distributional regularity and phonotactic constraints are useful for segmentation.</title>
<date>1996</date>
<booktitle>In Cognition,</booktitle>
<pages>93--125</pages>
<contexts>
<context position="8360" citStr="Brent and Cartwright, 1996" startWordPosition="1466" endWordPosition="1469">rces the compressor to go on until no more can be done. In this paper, we experimented with predetermined p values as well as those learned from MDL-based grid search. Minimum support nms We simply followed the suggested setting nms = 3 (Chen et al., 2012). Run Baseline G1 (a) α : G1 (b) p : G1 (c) α : G2 (a) α : G2 (b) p : G2 (c) α : P R F 76.9 81.6 79.2 0.030 76.4 79.9 78.1 0.38 73.4 80.2 76.8 0.010 75.7 80.4 78.0 0.002 82.1 80.0 81.0 0.36 79.1 81.7 80.4 0.004 79.3 84.2 81.7 5 Evaluation 5.1 Setup In the experiment, we tested our methods on Brent’s derivation of the Bernstein-Ratner corpus (Brent and Cartwright, 1996; BernsteinRatner, 1987). This dataset is distributed via the CHILDES project (MacWhinney and Snow, 1990) and has been commonly used as a standard benchmark for phonetic segmentation. Our baseline method is the original regularized compressor algorithm (Chen et al., 2012). In our experiment, we considered the following three search settings for finding the model parameters: (a) Fix p to 0 and vary α to find the best value (in the sense of description length); (b) Fix α to the best value found in setting (a) and vary p; (c) Set p to a heuristic value 0.37 (Chen et al., 2012) and vary α. Setting</context>
</contexts>
<marker>Brent, Cartwright, 1996</marker>
<rawString>Michael R. Brent and Timothy A. Cartwright. 1996. Distributional regularity and phonotactic constraints are useful for segmentation. In Cognition, pages 93– 125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruey-Cheng Chen</author>
<author>Chiung-Min Tsai</author>
<author>Jieh Hsiang</author>
</authors>
<title>A regularized compression method to unsupervised word segmentation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology, SIGMORPHON ’12,</booktitle>
<pages>26--34</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montreal, Canada.</location>
<contexts>
<context position="1756" citStr="Chen et al., 2012" startWordPosition="248" endWordPosition="251">ore promising in addressing the scalability problem. MDL-based methods (Rissanen, 1978) rely on underlying search algorithms to segment the text in as many possible ways and use description length to decide which to output. As different algorithms explore different trajectories in the search space, segmentation accuracy depends largely on the search coverage. Early work in this line focused more on existing segmentation algorithm, such as branching entropy (Tanaka-Ishii, 2005; Zhikov et al., 2010) and bootstrap voting experts (Hewlett and Cohen, 2009; Hewlett and Cohen, 2011). A recent study (Chen et al., 2012) on a compression-based algorithm, regularized compression, has achieved comparable performance result to hierarchical Bayes methods. Along this line, in this paper we present a novel extension to the regularized compressor algorithm. We propose a lower-bound approximate to the original objective and show that, through analysis and experimentation, this amendment improves segmentation performance and runtime efficiency. 2 Regularized Compression The dynamics behind regularized compression is similar to digram coding (Witten et al., 1999). One first breaks the text down to a sequence of charact</context>
<context position="3458" citStr="Chen et al. (2012)" startWordPosition="528" endWordPosition="531"> rate and vocabulary complexity: min. −αf(x, y) + |Wi−1|A ˜H(Wi−1, Wi) s.t. either x or y is a character f(x, y) &gt; nms. Here, the criterion is written slightly differently. Note that f(x, y) is the bigram frequency, |Wi−1 |the sequence length of Wi−1, and A ˜H(Wi−1, Wi) = ˜H(Wi) − ˜H(Wi−1) is the difference between the empirical Shannon entropy measured on Wi and Wi−1, using maximum likelihood estimates. Specifically, this empirical esti˜H(W) for a sequence W corresponds to: 1 log |W |− |W |1: f (x) log f (x) x:types For this equation to work, one needs to estimate other model parameters. See Chen et al. (2012) for a comprehensive treatment. mate 166 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 166–170, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics f(x) f(y) f(z) |W| Wi−1 k l 0 N Wi k − m l − m m N − m Table 1: The change between iterations in word frequency and sequence length in regularized compression. In the new sequence Wi, each occurrence of the x-y bigram is replaced with a new (conceptually unseen) word z. This has an effect of reducing the number of words in the sequence. 3 Change in Description Length Th</context>
<context position="7990" citStr="Chen et al., 2012" startWordPosition="1387" endWordPosition="1390">L/m converges to the empirical pointwise mutual information (proof omitted here). 167 G1 ≡ f (x, y) I log (f (x) − f (x, y)) (f (y) − f (x, y)) − αl (4) |Wi−1 |f (x, y) J G2 ≡ −αf(x, y) + log (f(x) − f(x, y))(f(y) − f(x, y)) (5) |Wi−1|f(x, y) Figure 1: The two newly-proposed objective functions. word/character ratio) is lower than p. Setting this value to 0 forces the compressor to go on until no more can be done. In this paper, we experimented with predetermined p values as well as those learned from MDL-based grid search. Minimum support nms We simply followed the suggested setting nms = 3 (Chen et al., 2012). Run Baseline G1 (a) α : G1 (b) p : G1 (c) α : G2 (a) α : G2 (b) p : G2 (c) α : P R F 76.9 81.6 79.2 0.030 76.4 79.9 78.1 0.38 73.4 80.2 76.8 0.010 75.7 80.4 78.0 0.002 82.1 80.0 81.0 0.36 79.1 81.7 80.4 0.004 79.3 84.2 81.7 5 Evaluation 5.1 Setup In the experiment, we tested our methods on Brent’s derivation of the Bernstein-Ratner corpus (Brent and Cartwright, 1996; BernsteinRatner, 1987). This dataset is distributed via the CHILDES project (MacWhinney and Snow, 1990) and has been commonly used as a standard benchmark for phonetic segmentation. Our baseline method is the original regularize</context>
<context position="11202" citStr="Chen et al. (2012)" startWordPosition="1954" endWordPosition="1957">, it remains an open issue whether there is a connection between the optimal p value and the true word/token ratio (≈ 0.35 for Bernstein-Ratner corpus). The result has led us to conclude that MDLbased grid search is efficient in optimizing segmentation accuracy. Minimization of description length is in general aligned with performance improvement, although under finer granularity MDL-based search may not be as effec168 Method P R F Adaptors grammar, colloc3-syllable Johnson and Goldwater (2009) 86.1 88.4 87.2 Regularized compression + MDL, G2 (b) — 79.1 81.7 80.4 Regularized compression + MDL Chen et al. (2012) 76.9 81.6 79.2 Adaptors grammar, colloc Johnson and Goldwater (2009) 78.4 75.7 77.1 Particle filter, unigram B¨orschinger and Johnson (2012) – – 77.1 Regularized compression + MDL, G1 (b) — 73.4 80.2 76.8 Bootstrap voting experts + MDL Hewlett and Cohen (2011) 79.3 73.4 76.2 Nested Pitman-Yor process, bigram Mochihashi et al. (2009) 74.8 76.7 75.7 Branching entropy + MDL Zhikov et al. (2010) 76.3 74.5 75.4 Particle filter, bigram B¨orschinger and Johnson (2012) – – 74.5 Hierarchical Dirichlet process Goldwater et al. (2009) 75.2 69.6 72.3 Table 3: The performance chart on the Bernstein-Ratner</context>
<context position="14138" citStr="Chen et al., 2012" startWordPosition="2416" endWordPosition="2419">nt than the original. Besides runMethod Time (s) Adaptors grammar, colloc3-syllable 53826 Adaptors grammar, colloc 10498 Regularized compressor 1.51 Regularized compressor, G1 (b) 0.60 Regularized compressor, G2 (b) 1.25 Table 4: The average running time in seconds on the Bernstein-Ratner corpus for adaptors grammar (per fold, based on trace output) and regularized compressors, tested on an Intel Xeon 2.5GHz 8- core machine with 8GB RAM. time efficiency, our experiment result also shows improved performance. Using MDL alone, one proposed method outperforms the original regularized compressor (Chen et al., 2012) in precision by 2 percentage points and in F-measure by 1. Its performance is only second to the state of the art, achieved by adaptors grammar with colloc3- syllable (Johnson and Goldwater, 2009). A natural extension of this work is to reproduce this result on some other word segmentation benchmarks, specifically those in other Asian languages (Emerson, 2005; Zhikov et al., 2010). Furthermore, it would be interesting to investigate stochastic optimization techniques for regularized compression that simultaneously fit both α and p. We believe this would be the key to adapt the algorithm to la</context>
</contexts>
<marker>Chen, Tsai, Hsiang, 2012</marker>
<rawString>Ruey-Cheng Chen, Chiung-Min Tsai, and Jieh Hsiang. 2012. A regularized compression method to unsupervised word segmentation. In Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology, SIGMORPHON ’12, pages 26–34, Montreal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 133. Jeju Island,</booktitle>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 133. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="772" citStr="Goldwater et al., 2009" startWordPosition="98" endWordPosition="102">ei 106, Taiwan rueycheng@turing.csie.ntu.edu.tw Abstract We study the mathematical properties of a recently proposed MDL-based unsupervised word segmentation algorithm, called regularized compression. Our analysis shows that its objective function can be efficiently approximated using the negative empirical pointwise mutual information. The proposed extension improves the baseline performance in both efficiency and accuracy on a standard benchmark. 1 Introduction Hierarchical Bayes methods have been mainstream in unsupervised word segmentation since the dawn of hierarchical Dirichlet process (Goldwater et al., 2009) and adaptors grammar (Johnson and Goldwater, 2009). Despite this wide recognition, they are also notoriously computational prohibitive and have limited adoption on larger corpora. While much effort has been directed to mitigating this issue within the Bayes framework (Borschinger and Johnson, 2011), many have found minimum description length (MDL) based methods more promising in addressing the scalability problem. MDL-based methods (Rissanen, 1978) rely on underlying search algorithms to segment the text in as many possible ways and use description length to decide which to output. As differe</context>
<context position="11732" citStr="Goldwater et al. (2009)" startWordPosition="2038" endWordPosition="2041">d compression + MDL, G2 (b) — 79.1 81.7 80.4 Regularized compression + MDL Chen et al. (2012) 76.9 81.6 79.2 Adaptors grammar, colloc Johnson and Goldwater (2009) 78.4 75.7 77.1 Particle filter, unigram B¨orschinger and Johnson (2012) – – 77.1 Regularized compression + MDL, G1 (b) — 73.4 80.2 76.8 Bootstrap voting experts + MDL Hewlett and Cohen (2011) 79.3 73.4 76.2 Nested Pitman-Yor process, bigram Mochihashi et al. (2009) 74.8 76.7 75.7 Branching entropy + MDL Zhikov et al. (2010) 76.3 74.5 75.4 Particle filter, bigram B¨orschinger and Johnson (2012) – – 74.5 Hierarchical Dirichlet process Goldwater et al. (2009) 75.2 69.6 72.3 Table 3: The performance chart on the Bernstein-Ratner corpus, in descending order of word-level Fmeasure. We deliberately reproduced the results for adaptors grammar and regularized compression. The other measurements came directly from the literature. tive. In our experiment, search setting (b) won out on description length for both objectives, while the best performance was in fact achieved by the others. It would be interesting to confirm this by studying the correlation between description length and word-level F-measure. In Table 3, we summarize many published results for</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hewlett</author>
<author>Paul Cohen</author>
</authors>
<title>Bootstrap voting experts.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI’09,</booktitle>
<pages>1071--1076</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="1694" citStr="Hewlett and Cohen, 2009" startWordPosition="236" endWordPosition="239">), many have found minimum description length (MDL) based methods more promising in addressing the scalability problem. MDL-based methods (Rissanen, 1978) rely on underlying search algorithms to segment the text in as many possible ways and use description length to decide which to output. As different algorithms explore different trajectories in the search space, segmentation accuracy depends largely on the search coverage. Early work in this line focused more on existing segmentation algorithm, such as branching entropy (Tanaka-Ishii, 2005; Zhikov et al., 2010) and bootstrap voting experts (Hewlett and Cohen, 2009; Hewlett and Cohen, 2011). A recent study (Chen et al., 2012) on a compression-based algorithm, regularized compression, has achieved comparable performance result to hierarchical Bayes methods. Along this line, in this paper we present a novel extension to the regularized compressor algorithm. We propose a lower-bound approximate to the original objective and show that, through analysis and experimentation, this amendment improves segmentation performance and runtime efficiency. 2 Regularized Compression The dynamics behind regularized compression is similar to digram coding (Witten et al., </context>
</contexts>
<marker>Hewlett, Cohen, 2009</marker>
<rawString>Daniel Hewlett and Paul Cohen. 2009. Bootstrap voting experts. In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI’09, pages 1071–1076, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hewlett</author>
<author>Paul Cohen</author>
</authors>
<title>Fully unsupervised word segmentation with BVE and MDL.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>540--545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon.</location>
<contexts>
<context position="1720" citStr="Hewlett and Cohen, 2011" startWordPosition="240" endWordPosition="244">m description length (MDL) based methods more promising in addressing the scalability problem. MDL-based methods (Rissanen, 1978) rely on underlying search algorithms to segment the text in as many possible ways and use description length to decide which to output. As different algorithms explore different trajectories in the search space, segmentation accuracy depends largely on the search coverage. Early work in this line focused more on existing segmentation algorithm, such as branching entropy (Tanaka-Ishii, 2005; Zhikov et al., 2010) and bootstrap voting experts (Hewlett and Cohen, 2009; Hewlett and Cohen, 2011). A recent study (Chen et al., 2012) on a compression-based algorithm, regularized compression, has achieved comparable performance result to hierarchical Bayes methods. Along this line, in this paper we present a novel extension to the regularized compressor algorithm. We propose a lower-bound approximate to the original objective and show that, through analysis and experimentation, this amendment improves segmentation performance and runtime efficiency. 2 Regularized Compression The dynamics behind regularized compression is similar to digram coding (Witten et al., 1999). One first breaks th</context>
<context position="11463" citStr="Hewlett and Cohen (2011)" startWordPosition="1996" endWordPosition="1999">curacy. Minimization of description length is in general aligned with performance improvement, although under finer granularity MDL-based search may not be as effec168 Method P R F Adaptors grammar, colloc3-syllable Johnson and Goldwater (2009) 86.1 88.4 87.2 Regularized compression + MDL, G2 (b) — 79.1 81.7 80.4 Regularized compression + MDL Chen et al. (2012) 76.9 81.6 79.2 Adaptors grammar, colloc Johnson and Goldwater (2009) 78.4 75.7 77.1 Particle filter, unigram B¨orschinger and Johnson (2012) – – 77.1 Regularized compression + MDL, G1 (b) — 73.4 80.2 76.8 Bootstrap voting experts + MDL Hewlett and Cohen (2011) 79.3 73.4 76.2 Nested Pitman-Yor process, bigram Mochihashi et al. (2009) 74.8 76.7 75.7 Branching entropy + MDL Zhikov et al. (2010) 76.3 74.5 75.4 Particle filter, bigram B¨orschinger and Johnson (2012) – – 74.5 Hierarchical Dirichlet process Goldwater et al. (2009) 75.2 69.6 72.3 Table 3: The performance chart on the Bernstein-Ratner corpus, in descending order of word-level Fmeasure. We deliberately reproduced the results for adaptors grammar and regularized compression. The other measurements came directly from the literature. tive. In our experiment, search setting (b) won out on descri</context>
</contexts>
<marker>Hewlett, Cohen, 2011</marker>
<rawString>Daniel Hewlett and Paul Cohen. 2011. Fully unsupervised word segmentation with BVE and MDL. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 540–545, Portland, Oregon. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>317--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado.</location>
<contexts>
<context position="823" citStr="Johnson and Goldwater, 2009" startWordPosition="106" endWordPosition="109">w Abstract We study the mathematical properties of a recently proposed MDL-based unsupervised word segmentation algorithm, called regularized compression. Our analysis shows that its objective function can be efficiently approximated using the negative empirical pointwise mutual information. The proposed extension improves the baseline performance in both efficiency and accuracy on a standard benchmark. 1 Introduction Hierarchical Bayes methods have been mainstream in unsupervised word segmentation since the dawn of hierarchical Dirichlet process (Goldwater et al., 2009) and adaptors grammar (Johnson and Goldwater, 2009). Despite this wide recognition, they are also notoriously computational prohibitive and have limited adoption on larger corpora. While much effort has been directed to mitigating this issue within the Bayes framework (Borschinger and Johnson, 2011), many have found minimum description length (MDL) based methods more promising in addressing the scalability problem. MDL-based methods (Rissanen, 1978) rely on underlying search algorithms to segment the text in as many possible ways and use description length to decide which to output. As different algorithms explore different trajectories in the</context>
<context position="11083" citStr="Johnson and Goldwater (2009)" startWordPosition="1933" endWordPosition="1936">value 0.37. It is interesting to note that G1 (b) and G2 (b) also gave very close estimates to this heuristic value. Nevertheless, it remains an open issue whether there is a connection between the optimal p value and the true word/token ratio (≈ 0.35 for Bernstein-Ratner corpus). The result has led us to conclude that MDLbased grid search is efficient in optimizing segmentation accuracy. Minimization of description length is in general aligned with performance improvement, although under finer granularity MDL-based search may not be as effec168 Method P R F Adaptors grammar, colloc3-syllable Johnson and Goldwater (2009) 86.1 88.4 87.2 Regularized compression + MDL, G2 (b) — 79.1 81.7 80.4 Regularized compression + MDL Chen et al. (2012) 76.9 81.6 79.2 Adaptors grammar, colloc Johnson and Goldwater (2009) 78.4 75.7 77.1 Particle filter, unigram B¨orschinger and Johnson (2012) – – 77.1 Regularized compression + MDL, G1 (b) — 73.4 80.2 76.8 Bootstrap voting experts + MDL Hewlett and Cohen (2011) 79.3 73.4 76.2 Nested Pitman-Yor process, bigram Mochihashi et al. (2009) 74.8 76.7 75.7 Branching entropy + MDL Zhikov et al. (2010) 76.3 74.5 75.4 Particle filter, bigram B¨orschinger and Johnson (2012) – – 74.5 Hiera</context>
<context position="14335" citStr="Johnson and Goldwater, 2009" startWordPosition="2450" endWordPosition="2453">gularized compressor, G2 (b) 1.25 Table 4: The average running time in seconds on the Bernstein-Ratner corpus for adaptors grammar (per fold, based on trace output) and regularized compressors, tested on an Intel Xeon 2.5GHz 8- core machine with 8GB RAM. time efficiency, our experiment result also shows improved performance. Using MDL alone, one proposed method outperforms the original regularized compressor (Chen et al., 2012) in precision by 2 percentage points and in F-measure by 1. Its performance is only second to the state of the art, achieved by adaptors grammar with colloc3- syllable (Johnson and Goldwater, 2009). A natural extension of this work is to reproduce this result on some other word segmentation benchmarks, specifically those in other Asian languages (Emerson, 2005; Zhikov et al., 2010). Furthermore, it would be interesting to investigate stochastic optimization techniques for regularized compression that simultaneously fit both α and p. We believe this would be the key to adapt the algorithm to larger datasets. Acknowledgments We thank the anonymous reviewers for their valuable feedback. 169 Brian MacWhinney and Catherine Snow. 1990. The child language data exchange system: an update. Journ</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 317–325, Boulder, Colorado. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of theAFNLP: Volume 1 - Volume 1, ACL ’09,</booktitle>
<pages>100--108</pages>
<institution>Suntec, Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="11537" citStr="Mochihashi et al. (2009)" startWordPosition="2007" endWordPosition="2010">ormance improvement, although under finer granularity MDL-based search may not be as effec168 Method P R F Adaptors grammar, colloc3-syllable Johnson and Goldwater (2009) 86.1 88.4 87.2 Regularized compression + MDL, G2 (b) — 79.1 81.7 80.4 Regularized compression + MDL Chen et al. (2012) 76.9 81.6 79.2 Adaptors grammar, colloc Johnson and Goldwater (2009) 78.4 75.7 77.1 Particle filter, unigram B¨orschinger and Johnson (2012) – – 77.1 Regularized compression + MDL, G1 (b) — 73.4 80.2 76.8 Bootstrap voting experts + MDL Hewlett and Cohen (2011) 79.3 73.4 76.2 Nested Pitman-Yor process, bigram Mochihashi et al. (2009) 74.8 76.7 75.7 Branching entropy + MDL Zhikov et al. (2010) 76.3 74.5 75.4 Particle filter, bigram B¨orschinger and Johnson (2012) – – 74.5 Hierarchical Dirichlet process Goldwater et al. (2009) 75.2 69.6 72.3 Table 3: The performance chart on the Bernstein-Ratner corpus, in descending order of word-level Fmeasure. We deliberately reproduced the results for adaptors grammar and regularized compression. The other measurements came directly from the literature. tive. In our experiment, search setting (b) won out on description length for both objectives, while the best performance was in fact a</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of theAFNLP: Volume 1 - Volume 1, ACL ’09, pages 100–108, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Modeling by shortest data description.</title>
<date>1978</date>
<journal>Automatica,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="1225" citStr="Rissanen, 1978" startWordPosition="165" endWordPosition="166">uction Hierarchical Bayes methods have been mainstream in unsupervised word segmentation since the dawn of hierarchical Dirichlet process (Goldwater et al., 2009) and adaptors grammar (Johnson and Goldwater, 2009). Despite this wide recognition, they are also notoriously computational prohibitive and have limited adoption on larger corpora. While much effort has been directed to mitigating this issue within the Bayes framework (Borschinger and Johnson, 2011), many have found minimum description length (MDL) based methods more promising in addressing the scalability problem. MDL-based methods (Rissanen, 1978) rely on underlying search algorithms to segment the text in as many possible ways and use description length to decide which to output. As different algorithms explore different trajectories in the search space, segmentation accuracy depends largely on the search coverage. Early work in this line focused more on existing segmentation algorithm, such as branching entropy (Tanaka-Ishii, 2005; Zhikov et al., 2010) and bootstrap voting experts (Hewlett and Cohen, 2009; Hewlett and Cohen, 2011). A recent study (Chen et al., 2012) on a compression-based algorithm, regularized compression, has achie</context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>Jorma Rissanen. 1978. Modeling by shortest data description. Automatica, 14(5):465–471, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kumiko Tanaka-Ishii</author>
</authors>
<title>Entropy as an indicator of context boundaries: An experiment using a web search engine.</title>
<date>2005</date>
<booktitle>Natural Language Processing IJCNLP 2005,</booktitle>
<volume>3651</volume>
<pages>93--105</pages>
<editor>In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Kwong, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg, Berlin, Heidelberg.</location>
<contexts>
<context position="1618" citStr="Tanaka-Ishii, 2005" startWordPosition="225" endWordPosition="226">ng this issue within the Bayes framework (Borschinger and Johnson, 2011), many have found minimum description length (MDL) based methods more promising in addressing the scalability problem. MDL-based methods (Rissanen, 1978) rely on underlying search algorithms to segment the text in as many possible ways and use description length to decide which to output. As different algorithms explore different trajectories in the search space, segmentation accuracy depends largely on the search coverage. Early work in this line focused more on existing segmentation algorithm, such as branching entropy (Tanaka-Ishii, 2005; Zhikov et al., 2010) and bootstrap voting experts (Hewlett and Cohen, 2009; Hewlett and Cohen, 2011). A recent study (Chen et al., 2012) on a compression-based algorithm, regularized compression, has achieved comparable performance result to hierarchical Bayes methods. Along this line, in this paper we present a novel extension to the regularized compressor algorithm. We propose a lower-bound approximate to the original objective and show that, through analysis and experimentation, this amendment improves segmentation performance and runtime efficiency. 2 Regularized Compression The dynamics</context>
</contexts>
<marker>Tanaka-Ishii, 2005</marker>
<rawString>Kumiko Tanaka-Ishii. 2005. Entropy as an indicator of context boundaries: An experiment using a web search engine. In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Kwong, editors, Natural Language Processing IJCNLP 2005, volume 3651 of Lecture Notes in Computer Science, chapter 9, pages 93– 105. Springer Berlin / Heidelberg, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Alistair Moffat</author>
<author>Timothy C Bell</author>
</authors>
<title>Managing gigabytes (2nd ed.): compressing and indexing documents and images.</title>
<date>1999</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="2299" citStr="Witten et al., 1999" startWordPosition="322" endWordPosition="325">and Cohen, 2009; Hewlett and Cohen, 2011). A recent study (Chen et al., 2012) on a compression-based algorithm, regularized compression, has achieved comparable performance result to hierarchical Bayes methods. Along this line, in this paper we present a novel extension to the regularized compressor algorithm. We propose a lower-bound approximate to the original objective and show that, through analysis and experimentation, this amendment improves segmentation performance and runtime efficiency. 2 Regularized Compression The dynamics behind regularized compression is similar to digram coding (Witten et al., 1999). One first breaks the text down to a sequence of characters (W0) and then works from that representation up in an agglomerative fashion, iteratively removing word boundaries between the two selected word types. Hence, a new sequence Wi is created in the i-th iteration by merging all the occurrences of some selected bigram (x, y) in the original sequence Wi−1. Unlike in digram coding, where the most frequent pair of word types is always selected, in regularized compression a specialized decision criterion is used to balance compression rate and vocabulary complexity: min. −αf(x, y) + |Wi−1|A ˜</context>
</contexts>
<marker>Witten, Moffat, Bell, 1999</marker>
<rawString>Ian H. Witten, Alistair Moffat, and Timothy C. Bell. 1999. Managing gigabytes (2nd ed.): compressing and indexing documents and images. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Zhikov</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>An efficient algorithm for unsupervised word segmentation with branching entropy and MDL.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>832--842</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1640" citStr="Zhikov et al., 2010" startWordPosition="227" endWordPosition="230"> the Bayes framework (Borschinger and Johnson, 2011), many have found minimum description length (MDL) based methods more promising in addressing the scalability problem. MDL-based methods (Rissanen, 1978) rely on underlying search algorithms to segment the text in as many possible ways and use description length to decide which to output. As different algorithms explore different trajectories in the search space, segmentation accuracy depends largely on the search coverage. Early work in this line focused more on existing segmentation algorithm, such as branching entropy (Tanaka-Ishii, 2005; Zhikov et al., 2010) and bootstrap voting experts (Hewlett and Cohen, 2009; Hewlett and Cohen, 2011). A recent study (Chen et al., 2012) on a compression-based algorithm, regularized compression, has achieved comparable performance result to hierarchical Bayes methods. Along this line, in this paper we present a novel extension to the regularized compressor algorithm. We propose a lower-bound approximate to the original objective and show that, through analysis and experimentation, this amendment improves segmentation performance and runtime efficiency. 2 Regularized Compression The dynamics behind regularized co</context>
<context position="11597" citStr="Zhikov et al. (2010)" startWordPosition="2018" endWordPosition="2021">search may not be as effec168 Method P R F Adaptors grammar, colloc3-syllable Johnson and Goldwater (2009) 86.1 88.4 87.2 Regularized compression + MDL, G2 (b) — 79.1 81.7 80.4 Regularized compression + MDL Chen et al. (2012) 76.9 81.6 79.2 Adaptors grammar, colloc Johnson and Goldwater (2009) 78.4 75.7 77.1 Particle filter, unigram B¨orschinger and Johnson (2012) – – 77.1 Regularized compression + MDL, G1 (b) — 73.4 80.2 76.8 Bootstrap voting experts + MDL Hewlett and Cohen (2011) 79.3 73.4 76.2 Nested Pitman-Yor process, bigram Mochihashi et al. (2009) 74.8 76.7 75.7 Branching entropy + MDL Zhikov et al. (2010) 76.3 74.5 75.4 Particle filter, bigram B¨orschinger and Johnson (2012) – – 74.5 Hierarchical Dirichlet process Goldwater et al. (2009) 75.2 69.6 72.3 Table 3: The performance chart on the Bernstein-Ratner corpus, in descending order of word-level Fmeasure. We deliberately reproduced the results for adaptors grammar and regularized compression. The other measurements came directly from the literature. tive. In our experiment, search setting (b) won out on description length for both objectives, while the best performance was in fact achieved by the others. It would be interesting to confirm th</context>
</contexts>
<marker>Zhikov, Takamura, Okumura, 2010</marker>
<rawString>Valentin Zhikov, Hiroya Takamura, and Manabu Okumura. 2010. An efficient algorithm for unsupervised word segmentation with branching entropy and MDL. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 832–842, Cambridge, Massachusetts. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>