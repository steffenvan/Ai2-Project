<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000755">
<title confidence="0.965061">
Illinois-LH: A Denotational and Distributional Approach to Semantics
</title>
<author confidence="0.995915">
Alice Lai and Julia Hockenmaier
</author>
<affiliation confidence="0.9993535">
Department of Computer Science
University of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.993658">
{aylai2, juliahmr}@illinois.edu
</email>
<sectionHeader confidence="0.997325" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999702333333333">
This paper describes and analyzes our Se-
mEval 2014 Task 1 system. Its features
are based on distributional and denota-
tional similarities; word alignment; nega-
tion; and hypernym/hyponym, synonym,
and antonym relations.
</bodyText>
<sectionHeader confidence="0.991446" genericHeader="method">
1 Task Description
</sectionHeader>
<bodyText confidence="0.999882173913043">
SemEval 2014 Task 1 (Marelli et al., 2014a) eval-
uates system predictions of semantic relatedness
(SR) and textual entailment (TE) relations on sen-
tence pairs from the SICK dataset (Marelli et al.,
2014b). The dataset is intended to test compo-
sitional knowledge without requiring the world
knowledge that is often required for paraphrase
classification or Recognizing Textual Entailment
tasks. SR scores range from 1 to 5. TE relations
are ‘entailment,’ ‘contradiction,’ and ‘neutral.’
Our system uses features that depend on the
amount of word overlap and alignment between
the two sentences, the presence of negation, and
the semantic similarities of the words and sub-
strings that are not shared across the two sen-
tences. We use simple distributional similarities
as well as the recently proposed denotational sim-
ilarities of Young et al. (2014), which are intended
as more precise metrics for tasks that require en-
tailment. Both similarity types are estimated on
Young et al.’s corpus, which contains 31,783 im-
ages of everyday scenes, each paired with five de-
scriptive captions.
</bodyText>
<sectionHeader confidence="0.989857" genericHeader="method">
2 Our System
</sectionHeader>
<bodyText confidence="0.9937215">
Our system combines different sources of seman-
tic similarity to predict semantic relatedness and
</bodyText>
<footnote confidence="0.8510316">
This work is licensed under a Creative Commons At-
tribution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License de-
tails: http://creativecommons.org/licenses/
by/4.0/
</footnote>
<bodyText confidence="0.997134">
textual entailment. We use distributional sim-
ilarity features, denotational similarity features,
and alignment features based on shallow syntac-
tic structure.
</bodyText>
<subsectionHeader confidence="0.961558">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999989416666667">
We lemmatize all sentences with the Stanford
CoreNLP system1 and extract syntactic chunks
with the Illinois Chunker (Punyakanok and Roth,
2001). Like Young et al. (2014), we use the Malt
parser (Nivre et al., 2006) to identify 5 sets of con-
stituents for each sentence: subject NPs, verbs,
VPs, direct object NPs, and other NPs.
For stopwords, we use the NLTK English stop-
word list of 127 high-frequency words. We re-
move negation words (no, not, and nor) from the
stopword list since their presence is informative
for this dataset and task.
</bodyText>
<subsectionHeader confidence="0.995226">
2.2 Distributional Similarities
</subsectionHeader>
<bodyText confidence="0.9983264">
After stopword removal and lemmatization, we
compute vectors for tokens that appear at least 10
times in Young et al. (2014)’s image description
corpus. In the vector space, each dimension corre-
sponds to one of the 1000 most frequent lemmas
(contexts). The jth entry of the vector of wi is the
positive normalized pointwise mutual information
(pnPMI) between target wi and context wj:
We define P(wi) as the fraction of images with
at least one caption containing wi, and P(wi, wj)
as the fraction of images whose captions contain
both wi and wj. Following recent work that ex-
tends distributional similarities to phrases and sen-
tences (Mitchell and Lapata, 2010; Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
</bodyText>
<footnote confidence="0.864483">
1http://nlp.stanford.edu/software/
corenlp.shtml
</footnote>
<equation confidence="0.999573666666667">
pnPMI(wi, wj) = max ⎝0,
log1 p( )ip( wj) /
− log \ (P (wi, wj))
</equation>
<page confidence="0.982133">
329
</page>
<note confidence="0.758009">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 329–334,
Dublin, Ireland, August 23-24, 2014.
</note>
<table confidence="0.9219465">
Features Description # of features
Negation True if either sentence contains explicit negation; False otherwise 1
Word overlap Ratio of overlapping word types to total word types in s1 and s2 1
Denotational constituent similarity Positive normalized PMI of constituent nodes in the denotation 30
graph
Distributional constituent similarity Cosine similarity of vector representations of constituent phrases 30
</table>
<tableCaption confidence="0.977828166666666">
Alignment Ratio of number of aligned words to length of s1 and s2; max, min, 23
average unaligned chunk length; number of unaligned chunks
Unaligned matching Ratio of number of matched chunks to unaligned chunks; max, min, 31
average matched chunk similarity; number of crossings in matching
Chunk alignment Number of chunks; number of unaligned chunk labels; ratio of un- 17
aligned chunk labels to number of chunks; number of matched la-
bels; ratio of matched to unmatched chunk labels
Synonym Number of matched synonym pairs (w1, w2) 1
Hypernym Number of matched hypernym pairs (w1, w2), number of matched 2
hypernym pairs (w2, w1)
Antonym Number of matched antonym pairs (w1, w2) 1
Table 1: Summary of features.
</tableCaption>
<bodyText confidence="0.957325333333333">
2011; Socher et al., 2012), we define a phrase vec-
tor p to be the pointwise multiplication product of
the vectors of the words in the phrase:
</bodyText>
<equation confidence="0.912256">
p = w1 O ... Own
</equation>
<bodyText confidence="0.9995825">
where O is the multiplication of corresponding
vector components, i.e. pi = ui · vi.
</bodyText>
<subsectionHeader confidence="0.998365">
2.3 Denotational Similarities
</subsectionHeader>
<bodyText confidence="0.999983315789474">
In Young et al. (2014), we introduce denotational
similarities, which we argue provide a more pre-
cise metric for semantic inferences. We use an
image-caption corpus to define the (visual) de-
notation of a phrase as the set of images it de-
scribes, and construct a denotation graph, i.e. a
subsumption hierarchy (lattice) of phrases paired
with their denotations. For example, the denota-
tion of the node man is the set of images in the
corpus that contain a man, and the denotation of
the node person is rock climbing is the set of im-
ages that depict a person rock climbing. We de-
fine the (symmetric) denotational similarity of two
phrases as the pnPMI between their correspond-
ing sets of images. We associate each constituent
in the SICK dataset with a node in the denotation
graph, but new nodes that are unique to the SICK
data have no quantifiable similarity to other nodes
in the graph.
</bodyText>
<subsectionHeader confidence="0.887955">
2.4 Features
</subsectionHeader>
<bodyText confidence="0.9852934">
Table 1 summarizes our features. Since TE is a
directional task and SR is symmetric, we express
features that depend on sentence order twice: 1)
f1 are the features of s1 and f2 are the features of
s2, 2) f1 are the features of the longer sentence
and f2 are the features of the shorter sentence.
These directional features are specified in the
following feature descriptions.
Negation In this dataset, contradictory sentence
pairs are often marked by explicit negation, e.g. s1
= “The man is stirring the sauce for the chicken”
and s2 = “The man is not stirring the sauce for
the chicken.” A binary feature is set to 1 if either
sentence contains not, no, or nobody, and set to 0
otherwise.
</bodyText>
<subsectionHeader confidence="0.240076">
Word Overlap We compute �W1�W2�
</subsectionHeader>
<bodyText confidence="0.930308625">
�W1�W2� on lemma-
tized sentences without stopwords where Wi is
the set of word types that appear in si. Training
a MaxEnt or log-linear model using this feature
achieves better performance than the word overlap
baseline provided by the task organizers.
Denotational Constituent Similarity Denota-
tional similarity captures entailment-like relations
between events. For example, sit and eat lunch
have a high pnPMI, which follows our intuition
that a person who is eating lunch is likely to be
sitting. We use the same denotational constituent
features that Young et al. (2014) use for a textual
similarity task. C are original nodes, Canc are par-
ent and grandparent nodes, and sim(Ca, Cb) is the
maximum pnPMI of any pair of nodes a E Ca,
</bodyText>
<subsectionHeader confidence="0.346785">
b E Cb.
</subsectionHeader>
<bodyText confidence="0.9995618">
C-C features compare constituents of the same
type. These features express how often we expect
corresponding constituents to describe the same
situation. For example, s1 = “Girls are doing
backbends and playing outdoors” and s2 = “Chil-
</bodyText>
<page confidence="0.985375">
330
</page>
<bodyText confidence="0.9481535">
dren are doing backbends” have subject nodes
{girl} and {child}. Girls are sometimes de-
scribed as children, so sim(girl, child) = 0.498.
In addition, child is a parent node of girl, so
max(sim(anc(girl), child)) = 1. There are 15
C-C features: sim(C1, C2), max(sim(C1, Canc
</bodyText>
<equation confidence="0.91199775">
2 ),
sim(Canc
1 , C2)), sim(Canc
1 , Canc
</equation>
<bodyText confidence="0.998265946236559">
2 ) for each con-
stituent type.
C-all features compare different constituent
types. These features express how often we
expect any pair of constituents to describe the
same scene. For example, s1 = “Two teams are
competing in a football match” and s2 = “A
player is throwing a football” are topically related
sentences. Comparing constituents of different
types like player and compete or player and
football match gives us more information about
the similarity of the sentences. There are 15 C-all
features: the maximum, minimum, and sum of
sim(Ct1, C2) and sim(C1, Ct2) for each constituent
type.
Distributional Constituent Similarity Distribu-
tional vector-based similarity may alleviate the
sparsity of the denotation graph. For example,
for subject NP C-C features, we have non-zero
distributional similarity for 87% of instances in
the trial data, but non-zero denotational simi-
larity for only 56% of the same instances. The
football and team nodes may have no common
images in the denotation graph, but we still
have distributional vectors for football and for
team. The 30 distributional similarity features are
the same as the denotational similarity features
except sim(a, b) is the cosine similarity between
constituent phrase vectors.
Alignment Since contradictory and entailing sen-
tences have limited syntactic variation in this
dataset, aligning sentences can help to predict se-
mantic relatedness and textual entailment. We use
the Needleman-Wunsch algorithm (1970) to com-
pute an alignment based on exact word matches
between two lemmatized sentences. The similar-
ity between two lemmas is 1.0 if the words are
identical and 0.0 otherwise, and we do not penal-
ize gaps. This gives us the longest subsequence of
matching lemmas.
The alignment algorithm results in a sentence
pair alignment and 2 unaligned chunk sets defined
by syntactic chunks. For example, s1 = “A brown
and white dog is running through the tall grass”
and s2 = “A brown and white dog is moving
through the wild grass” are mostly aligned, with
the remaining chunks u1 = {[VP run], [NP tall]}
and u2 = {[VP move], [NP wild]}.
There are 23 alignment features. Directional
features per sentence are the number of words
(2 features), the number of aligned words (2
features), and the ratio between those counts (2
features). These features are expressed twice,
once according to the sentence order in the dataset
and once ordered by longer sentence before
shorter sentence, for a total of 12 directional fea-
tures. Non-directional features are the maximum,
minimum, and average unaligned chunk length for
each sentence and for both sentences combined (9
features), and the number of unaligned chunks in
each sentence (2 features).
Unaligned Chunk Matching We want to know
the similarity of the remaining unaligned chunks
because when two sentences have a high overlap,
their differences are very informative. For exam-
ple, in the case that two sentences are identical
except for a single word in each sentence, if we
know that the two words are synonymous, then we
should predict that the two sentences are highly
similar. However, if the two words are antonyms,
the sentences are likely to be contradictory.
We use phrase vector similarity to compute the
most likely matches between unaligned chunks.
We repeat the matching process twice: for sim-
ple matching, any 2 chunks with non-zero phrase
similarity can be matched across sentences, while
for strict matching, chunks can match only if they
have the same type, e.g. NP or VP. This gives us
two sets of features.
For s1 = “A brown and white dog is running
through the tall grass” and s2 = “A brown and
white dog is moving through the wild grass,” the
unaligned chunks are u1 = {[VP run], [NP tall]}
and u2 = {[VP move], [NP wild]}. For strict
matching, the only valid matches are [VP run]–
[VP move] and [NP tall]–[NP wild]. For simple
matching, [NP tall] could also match [VP move]
instead and [VP run] could match [NP wild].
There are a total of 31 unaligned chunk match-
ing features. Directional features per sentence
include the number of unaligned chunks (2
features) and the ratio of the number of matched
chunks to the total number of chunks (2 fea-
</bodyText>
<page confidence="0.997426">
331
</page>
<bodyText confidence="0.995075684210526">
tures). These features are expressed twice, once
according to the sentence order in the dataset and
once ordered by longer sentence before shorter
sentence, for a total of 8 directional features.
Non-directional features per sentence pair include
the maximum, minimum, and average similarity
of the matched chunks (3 features); the maximum,
minimum, and average length of the matched
chunks (3 features); and the number of matched
chunks (1 feature). We extract these 15 features
for both simple matching and strict matching. In
addition, we also count the number of crossings
that result from matching the unaligned chunks in
place (1 feature). This penalizes matched sets that
contain many crossings or long-distance matches.
Chunk Label Alignment and Matching Since
similar sentences in this dataset often have
similar syntax, we compare their chunk label
sequences, e.g. [NP A brown and white dog]
[VP is running] [PP through] [NP the tall grass]
becomes NP VP PP NP. We compute 17 features
based on aligning and matching these chunk
label sequences. Directional features are the total
number of labels in the sequence (2 features),
the number of unaligned labels (2 features), the
ratio of the number of unaligned labels to the
total number of labels (2 features), and the ratio
of the number of matched labels to the number of
unaligned labels (2 features). These features are
expressed twice, once according to the sentence
order in the dataset and once ordered by longer
sentence before shorter sentence, for a total of 16
directional features. We also count the number of
matched labels for the sentence pair (1 feature).
Synonyms and Hypernyms We count the num-
ber of synonyms and hypernyms in the matched
chunks for each sentence pair. Synonyms are
words that share a WordNet synset, and hyper-
nyms are words that have a hypernym relation
in WordNet. There are two hypernym features
because hypernymy is directional: num hyp1 is
the number of words in s1 that have a hypernym
in s2, while num hyp2 is the number of words
in s2 that have a hypernym in s1. For example,
s1 = “A woman is cutting a lemon” and s2 = “A
woman is cutting a fruit” have num hyp1 = 1.
For synonyms, num syn is the number of word
pairs in s1 and s2 that are synonyms. For example,
s1 = “A brown and white dog is running through
the tall grass” and s2 = “A brown and white
dog is moving through the wild grass” have
num syn = 1.
Antonyms When we match unaligned chunks, the
highest similarity pair are sometimes antonyms,
e.g. s1 = “Some people are on a crowded street”
and s2 = “Some people are on an empty street.”
In other cases, they are terms that we think of as
mutually exclusive, e.g. man and woman. In both
cases, the sentences are unlikely to be in an en-
tailing relationship. Since resources like WordNet
will fail to identify the mutually exclusive pairs
that are common in this dataset, e.g. bike and car
or piano and guitar, we use the training data to
build a list of these pairs. We identify the matched
chunks that occur in contradictory or neutral sen-
tences but not entailed sentences. We exclude syn-
onyms and hypernyms and apply a frequency filter
of n = 2. Commonly matched chunks in neutral
or contradictory sentences include sit–stand, boy–
girl, and cat–dog. These are terms with differ-
ent and often mutually exclusive meanings. Com-
monly matched chunks in entailed sentences in-
clude man–person, and lady–woman. These are
terms that could easily be used to describe the
same situation. However, cut–slice is a common
pair in both neutral and entailed sentences and we
do not want to count it as an antonym pair. There-
fore, we consider frequent pairs that occur in con-
tradictory or neutral but not entailed sentences to
be antonyms.
The feature num ant is the number of matched
antonyms in a sentence pair. We identify an
antonym if ca and cb are on the antonym list or
occur in one of these patterns: X–not X, X–no X,
X–no head-noun(X) (e.g. blue hat–no hat), X–
no hypernym(X) (e.g. poodle–no dog), X–no syn-
onym(X) (e.g. kid–no child). For each antonym
pair, we set the similarity score of that match to
0.0.
For example, num ant = 1 for s1 = “A small
white dog is running across a lawn” and s2 = “A
big white dog is running across a lawn.” In addi-
tion, num ant = 1 for s1 = “A woman is leaning
on the ledge of a balcony” and s2 = “A man is
leaning on the ledge of a balcony.”
</bodyText>
<subsectionHeader confidence="0.99128">
2.5 Models
</subsectionHeader>
<bodyText confidence="0.9976405">
For the SR task, we implement a log-linear regres-
sion model using Weka (Hall et al., 2009). Specif-
</bodyText>
<page confidence="0.993361">
332
</page>
<table confidence="0.999580833333333">
Accuracy Pearson ρ
Chance baseline 33.3 –
Majority baseline 56.7 –
Probability baseline 41.8 –
Overlap baseline 56.2 0.627
Submitted system 84.5 0.799
</table>
<tableCaption confidence="0.929072">
Table 2: TE and SR results on test data.
</tableCaption>
<table confidence="0.999830333333333">
Model Accuracy Pearson ρ
Overlap baseline 56.8 0.646
Negation 61.0 0.093
Word overlap 65.0 0.694
(+Vector composition) 66.4 0.697
+Denotational similarity 74.4 0.751
+Distributional similarity 71.8 0.756
+Den +Dist 77.0 0.782
+Alignment 70.4 0.697
+Unaligned chunk matching 75.8 0.719
+Align +Match 75.2 0.728
+Synonyms 65.2 0.696
+Hypernyms 66.8 0.716
+Antonyms 71.0 0.704
All features 84.2 0.802
</table>
<tableCaption confidence="0.999792">
Table 3: TE and SR results on trial data.
</tableCaption>
<bodyText confidence="0.999864714285714">
ically, under Weka’s default settings, we train a
ridge regression model with regularization param-
eter α = 1×10−8. For the TE task, we use a Max-
Ent model implemented with MALLET (McCal-
lum, 2002). The MaxEnt model is optimized with
L-BFGS, using the default settings. Both models
use the same set of features.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999545">
Our submitted system was trained on the full train-
ing and trial data (5000 sentences). Table 2 shows
our results on the test data. We substantially out-
perform all baselines.
</bodyText>
<subsectionHeader confidence="0.996823">
3.1 Feature Ablation
</subsectionHeader>
<bodyText confidence="0.999663428571429">
We train models on the training data and test on
the trial data. Models marked with + include our
word overlap feature. We also examine a single
compositional feature (vector composition): the
cosine similarity of two sentence vectors. A sen-
tence vector is the pointwise multiplication prod-
uct of component word vectors.
Table 3 compares performance on both tasks.
For TE, unaligned chunk matching outperforms
other features. Denotational constituent similarity
also does well. For SR, distributional and deno-
tational features have the highest correlation with
gold scores. Combining them further improves
performance.
</bodyText>
<table confidence="0.999065125">
% Accuracy
Model N E C
Overlap baseline 77.3 44.8 0.0
Negation 85.4 0.0 86.4
Word overlap 82.9 63.8 0.0
(+Vector composition) 84.7 64.5 0.0
+Denotational similarity 83.6 67.3 52.7
+Distributional similarity 86.5 60.4 37.8
+Den +Dist 85.4 68.7 60.8
+Alignment 87.9 50.6 41.8
+Unaligned chunk matching 90.4 66.6 37.8
+Align +Match 88.6 61.8 50.0
+Synonyms 82.2 65.2 0.0
+Hypernyms 84.0 68.0 0.0
+Antonyms 83.6 82.6 0.0
All features 86.5 83.3 77.0
</table>
<tableCaption confidence="0.873624666666667">
Table 4: TE accuracy on trial data by entailment
type (Neutral, Entailment, Contradiction).
Table 4 shows TE accuracy of each model by
</tableCaption>
<bodyText confidence="0.994143235294118">
entailment label. On contradictions, the negation
model has 86.0% accuracy while our final system
has only 77.0% accuracy. However, the negation
model cannot identify entailment. Its performance
is due to the high proportion of contradictions that
can be identified by explicit negation.
We expected antonyms to improve classifica-
tion of contradictions, but the antonym feature
actually has the highest accuracy of any feature
on entailed sentences. The dataset contains few
contradictions, and most involve explicit negation,
not antonyms. The antonym feature indicates that
when two sentences have high word overlap and
no antonyms, one is likely to entail the other. Neu-
tral sentences often contain word pairs that are
mutually exclusive, so the antonym feature distin-
guishes between neutral and entailed sentences.
</bodyText>
<sectionHeader confidence="0.999492" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999958625">
Our system combines multiple similarity metrics
to predict semantic relatedness and textual entail-
ment. A binary negation feature and similarity
comparisons based on chunking do very well, as
do denotational constituent similarity features. In
the future, we would like to focus on multiword
paraphrases and prepositional phrases, which our
current system has trouble analyzing.
</bodyText>
<sectionHeader confidence="0.998724" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9790015">
We gratefully acknowledge support of the Na-
tional Science Foundation under grants 1053856
and 1205627, as well as an NSF Graduate Re-
search Fellowship to Alice Lai.
</bodyText>
<page confidence="0.999018">
333
</page>
<sectionHeader confidence="0.998347" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999674611111111">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183–1193, Cambridge, MA, October.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1394–1404,
Edinburgh, Scotland, UK., July.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1):10–18.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faela Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. SemEval-2014 task 1: Evaluation
of compositional distributional semantic models on
full sentences through semantic relatedness and tex-
tual entailment. In Proceedings of SemEval 2014:
International Workshop on Semantic Evaluation.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaela Bernardi, and Roberto Zampar-
elli. 2014b. A SICK cure for the evaluation of com-
positional distributional semantic models. In Pro-
ceedings of LREC.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, vol-
ume 6, pages 2216–2219.
Vasin Punyakanok and Dan Roth. 2001. The use of
classifiers in sequential inference. In NIPS, pages
995–1001. MIT Press.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211, Jeju Island, Korea, July.
Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From image descriptions to
visual denotations: New similarity metrics for se-
mantic inference over event descriptions. Transac-
tions of the Association for Computational Linguis-
tics, 2:67–78.
</reference>
<page confidence="0.999049">
334
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.985425">Illinois-LH: A Denotational and Distributional Approach to Semantics</title>
<author confidence="0.99905">Alice Lai</author>
<author confidence="0.99905">Julia</author>
<affiliation confidence="0.986061">Department of Computer University of Illinois at</affiliation>
<abstract confidence="0.99768753802817">This paper describes and analyzes our SemEval 2014 Task 1 system. Its features are based on distributional and denotational similarities; word alignment; negation; and hypernym/hyponym, synonym, and antonym relations. 1 Task Description SemEval 2014 Task 1 (Marelli et al., 2014a) evaluates system predictions of semantic relatedness (SR) and textual entailment (TE) relations on sentence pairs from the SICK dataset (Marelli et al., 2014b). The dataset is intended to test compositional knowledge without requiring the world knowledge that is often required for paraphrase classification or Recognizing Textual Entailment tasks. SR scores range from 1 to 5. TE relations are ‘entailment,’ ‘contradiction,’ and ‘neutral.’ Our system uses features that depend on the amount of word overlap and alignment between the two sentences, the presence of negation, and the semantic similarities of the words and substrings that are not shared across the two sentences. We use simple distributional similarities well as the recently proposed similarities of Young et al. (2014), which are intended as more precise metrics for tasks that require entailment. Both similarity types are estimated on Young et al.’s corpus, which contains 31,783 images of everyday scenes, each paired with five descriptive captions. 2 Our System Our system combines different sources of semantic similarity to predict semantic relatedness and This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License deby/4.0/ textual entailment. We use distributional similarity features, denotational similarity features, and alignment features based on shallow syntactic structure. 2.1 Preprocessing We lemmatize all sentences with the Stanford and extract syntactic chunks with the Illinois Chunker (Punyakanok and Roth, 2001). Like Young et al. (2014), we use the Malt parser (Nivre et al., 2006) to identify 5 sets of constituents for each sentence: subject NPs, verbs, VPs, direct object NPs, and other NPs. For stopwords, we use the NLTK English stopword list of 127 high-frequency words. We renegation words and from the stopword list since their presence is informative for this dataset and task. 2.2 Distributional Similarities After stopword removal and lemmatization, we compute vectors for tokens that appear at least 10 times in Young et al. (2014)’s image description corpus. In the vector space, each dimension corresponds to one of the 1000 most frequent lemmas The entry of the vector of the positive normalized pointwise mutual information between target context define the fraction of images with least one caption containing and as the fraction of images whose captions contain Following recent work that extends distributional similarities to phrases and sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, corenlp.shtml = max 329 of the 8th International Workshop on Semantic Evaluation (SemEval pages Dublin, Ireland, August 23-24, 2014. Features Description # of features Negation True if either sentence contains explicit negation; False otherwise 1 overlap Ratio of overlapping word types to total word types in Denotational constituent similarity Positive normalized PMI of constituent nodes in the denotation 30 graph Distributional constituent similarity Cosine similarity of vector representations of constituent phrases 30 Ratio of number of aligned words to length of max, min, average unaligned chunk length; number of unaligned chunks Unaligned matching Ratio of number of matched chunks to unaligned chunks; max, min, average matched chunk similarity; number of crossings in matching Chunk alignment Number of chunks; number of unaligned chunk labels; ratio of unaligned chunk labels to number of chunks; number of matched labels; ratio of matched to unmatched chunk labels Number of matched synonym pairs Number of matched hypernym pairs number of matched 2 pairs Number of matched antonym pairs Table 1: Summary of features. 2011; Socher et al., 2012), we define a phrase vecbe the pointwise multiplication product of the vectors of the words in the phrase: the multiplication of corresponding components, i.e. 2.3 Denotational Similarities Young et al. (2014), we introduce similarities, which we argue provide a more precise metric for semantic inferences. We use an corpus to define the dea phrase as the set of images it deand construct a i.e. a subsumption hierarchy (lattice) of phrases paired with their denotations. For example, the denotaof the node the set of images in the corpus that contain a man, and the denotation of node is rock climbing the set of images that depict a person rock climbing. We define the (symmetric) denotational similarity of two phrases as the pnPMI between their corresponding sets of images. We associate each constituent in the SICK dataset with a node in the denotation graph, but new nodes that are unique to the SICK data have no quantifiable similarity to other nodes in the graph. 2.4 Features Table 1 summarizes our features. Since TE is a directional task and SR is symmetric, we express features that depend on sentence order twice: 1) the features of the features of 2) the features of the longer sentence the features of the shorter sentence. These directional features are specified in the following feature descriptions. this dataset, contradictory sentence are often marked by explicit negation, e.g. man is stirring the sauce for the chicken” man is not stirring the sauce for chicken.” binary feature is set to 1 if either contains or and set to 0 otherwise. Overlap compute lemmasentences without stopwords where set of word types that appear in Training a MaxEnt or log-linear model using this feature achieves better performance than the word overlap baseline provided by the task organizers. Constituent Similarity Denotational similarity captures entailment-like relations events. For example, lunch have a high pnPMI, which follows our intuition a person who is lunch likely to be We use the same denotational constituent features that Young et al. (2014) use for a textual task. original nodes, are parand grandparent nodes, and the pnPMI of any pair of nodes C-C features compare constituents of the same type. These features express how often we expect corresponding constituents to describe the same For example, are doing and playing outdoors” “Chil- 330 are doing backbends” subject nodes sometimes deas so 0.498. addition, a parent node of so 1. are 15 features: each constituent type. C-all features compare different constituent types. These features express how often we expect any pair of constituents to describe the scene. For example, teams are in a football match” is throwing a football” topically related sentences. Comparing constituents of different like match us more information about the similarity of the sentences. There are 15 C-all features: the maximum, minimum, and sum of each constituent type. Constituent Similarity Distributional vector-based similarity may alleviate the sparsity of the denotation graph. For example, for subject NP C-C features, we have non-zero distributional similarity for 87% of instances in the trial data, but non-zero denotational similarity for only 56% of the same instances. The may have no common images in the denotation graph, but we still distributional vectors for for The 30 distributional similarity features are the same as the denotational similarity features the cosine similarity between constituent phrase vectors. contradictory and entailing sentences have limited syntactic variation in this dataset, aligning sentences can help to predict semantic relatedness and textual entailment. We use the Needleman-Wunsch algorithm (1970) to compute an alignment based on exact word matches between two lemmatized sentences. The similarity between two lemmas is 1.0 if the words are identical and 0.0 otherwise, and we do not penalize gaps. This gives us the longest subsequence of matching lemmas. The alignment algorithm results in a sentence pair alignment and 2 unaligned chunk sets defined syntactic chunks. For example, brown and white dog is running through the tall grass” brown and white dog is moving the wild grass” mostly aligned, with remaining chunks There are 23 alignment features. Directional features per sentence are the number of words (2 features), the number of aligned words (2 features), and the ratio between those counts (2 features). These features are expressed twice, once according to the sentence order in the dataset and once ordered by longer sentence before shorter sentence, for a total of 12 directional features. Non-directional features are the maximum, minimum, and average unaligned chunk length for each sentence and for both sentences combined (9 features), and the number of unaligned chunks in each sentence (2 features). Chunk Matching want to know the similarity of the remaining unaligned chunks because when two sentences have a high overlap, their differences are very informative. For example, in the case that two sentences are identical except for a single word in each sentence, if we know that the two words are synonymous, then we should predict that the two sentences are highly similar. However, if the two words are antonyms, the sentences are likely to be contradictory. We use phrase vector similarity to compute the most likely matches between unaligned chunks. We repeat the matching process twice: for simple matching, any 2 chunks with non-zero phrase similarity can be matched across sentences, while for strict matching, chunks can match only if they the same type, e.g. This gives us two sets of features. brown and white dog is running the tall grass” brown and dog is moving through the wild grass,” chunks are For strict the only valid matches are and For simple could also match and could match There are a total of 31 unaligned chunk matching features. Directional features per sentence include the number of unaligned chunks (2 features) and the ratio of the number of matched to the total number of chunks (2 fea- 331 tures). These features are expressed twice, once according to the sentence order in the dataset and once ordered by longer sentence before shorter sentence, for a total of 8 directional features. Non-directional features per sentence pair include the maximum, minimum, and average similarity of the matched chunks (3 features); the maximum, minimum, and average length of the matched chunks (3 features); and the number of matched chunks (1 feature). We extract these 15 features for both simple matching and strict matching. In addition, we also count the number of crossings that result from matching the unaligned chunks in place (1 feature). This penalizes matched sets that contain many crossings or long-distance matches. Label Alignment and Matching similar sentences in this dataset often have similar syntax, we compare their chunk label e.g. [NP brown and white [PP [NP tall VP PP We compute 17 features based on aligning and matching these chunk label sequences. Directional features are the total number of labels in the sequence (2 features), the number of unaligned labels (2 features), the ratio of the number of unaligned labels to the total number of labels (2 features), and the ratio of the number of matched labels to the number of unaligned labels (2 features). These features are expressed twice, once according to the sentence order in the dataset and once ordered by longer sentence before shorter sentence, for a total of 16 directional features. We also count the number of matched labels for the sentence pair (1 feature). and Hypernyms count the number of synonyms and hypernyms in the matched chunks for each sentence pair. Synonyms are words that share a WordNet synset, and hypernyms are words that have a hypernym relation in WordNet. There are two hypernym features hypernymy is directional: number of words in have a hypernym while the number of words have a hypernym in For example, woman is cutting a is cutting a 1. synonyms, syn the number of word in are synonyms. For example, brown and white dog is tall grass” brown and white is the wild grass” syn 1. we match unaligned chunks, the highest similarity pair are sometimes antonyms, people are on a people are on an In other cases, they are terms that we think of as exclusive, e.g. In both cases, the sentences are unlikely to be in an entailing relationship. Since resources like WordNet will fail to identify the mutually exclusive pairs are common in this dataset, e.g. we use the training data to build a list of these pairs. We identify the matched chunks that occur in contradictory or neutral sentences but not entailed sentences. We exclude synonyms and hypernyms and apply a frequency filter 2. matched chunks in neutral contradictory sentences include and These are terms with different and often mutually exclusive meanings. Commonly matched chunks in entailed sentences inand These are terms that could easily be used to describe the situation. However, a common pair in both neutral and entailed sentences and we do not want to count it as an antonym pair. Therefore, we consider frequent pairs that occur in contradictory or neutral but not entailed sentences to be antonyms. feature ant the number of matched antonyms in a sentence pair. We identify an if and on the antonym list or in one of these patterns: hat–no syn- For each antonym pair, we set the similarity score of that match to 0.0. example, ant 1 dog is running across a lawn” dog is running across a lawn.” addiant 1 leaning the ledge of a balcony” leaning on the ledge of a balcony.” 2.5 Models For the SR task, we implement a log-linear regresmodel using Weka (Hall et al., 2009). Specif-</abstract>
<note confidence="0.861119739130435">332 Accuracy Chance baseline 33.3 – Majority baseline 56.7 – Probability baseline 41.8 – Overlap baseline 56.2 0.627 Submitted system 84.5 0.799 Table 2: TE and SR results on test data. Model Accuracy Overlap baseline 56.8 0.646 Negation 61.0 0.093 Word overlap 65.0 0.694 (+Vector composition) 66.4 0.697 +Denotational similarity 74.4 0.751 +Distributional similarity 71.8 0.756 +Den +Dist 77.0 0.782 +Alignment 70.4 0.697 +Unaligned chunk matching 75.8 0.719 +Align +Match 75.2 0.728 +Synonyms 65.2 0.696 +Hypernyms 66.8 0.716 +Antonyms 71.0 0.704 All features 84.2 0.802</note>
<abstract confidence="0.9605405">Table 3: TE and SR results on trial data. ically, under Weka’s default settings, we train a ridge regression model with regularization param- For the TE task, we use a Max- Ent model implemented with MALLET (McCallum, 2002). The MaxEnt model is optimized with L-BFGS, using the default settings. Both models use the same set of features. 3 Results Our submitted system was trained on the full training and trial data (5000 sentences). Table 2 shows our results on the test data. We substantially outperform all baselines. 3.1 Feature Ablation We train models on the training data and test on the trial data. Models marked with + include our word overlap feature. We also examine a single compositional feature (vector composition): the cosine similarity of two sentence vectors. A sentence vector is the pointwise multiplication product of component word vectors. Table 3 compares performance on both tasks. For TE, unaligned chunk matching outperforms other features. Denotational constituent similarity also does well. For SR, distributional and denotational features have the highest correlation with gold scores. Combining them further improves performance. % Accuracy Model N E C</abstract>
<note confidence="0.949853117647059">Overlap baseline 77.3 44.8 0.0 Negation 85.4 0.0 86.4 Word overlap 82.9 63.8 0.0 (+Vector composition) 84.7 64.5 0.0 +Denotational similarity 83.6 67.3 52.7 +Distributional similarity 86.5 60.4 37.8 +Den +Dist 85.4 68.7 60.8 +Alignment 87.9 50.6 41.8 +Unaligned chunk matching 90.4 66.6 37.8 +Align +Match 88.6 61.8 50.0 +Synonyms 82.2 65.2 0.0 +Hypernyms 84.0 68.0 0.0 +Antonyms 83.6 82.6 0.0 All features 86.5 83.3 77.0 Table 4: TE accuracy on trial data by entailment type (Neutral, Entailment, Contradiction). Table 4 shows TE accuracy of each model by</note>
<abstract confidence="0.976627096774194">entailment label. On contradictions, the negation model has 86.0% accuracy while our final system has only 77.0% accuracy. However, the negation model cannot identify entailment. Its performance is due to the high proportion of contradictions that can be identified by explicit negation. We expected antonyms to improve classification of contradictions, but the antonym feature actually has the highest accuracy of any feature on entailed sentences. The dataset contains few contradictions, and most involve explicit negation, not antonyms. The antonym feature indicates that when two sentences have high word overlap and no antonyms, one is likely to entail the other. Neutral sentences often contain word pairs that are mutually exclusive, so the antonym feature distinguishes between neutral and entailed sentences. 4 Conclusion Our system combines multiple similarity metrics to predict semantic relatedness and textual entailment. A binary negation feature and similarity comparisons based on chunking do very well, as do denotational constituent similarity features. In the future, we would like to focus on multiword paraphrases and prepositional phrases, which our current system has trouble analyzing. Acknowledgements We gratefully acknowledge support of the National Science Foundation under grants 1053856 and 1205627, as well as an NSF Graduate Research Fellowship to Alice Lai.</abstract>
<pubnum confidence="0.373984">333</pubnum>
<title confidence="0.900396">References</title>
<author confidence="0.946107">Nouns</author>
<abstract confidence="0.7147195">are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical in Natural Language pages</abstract>
<address confidence="0.73561">1183–1193, Cambridge, MA, October.</address>
<note confidence="0.9786796">Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical composidistributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods Natural Language pages 1394–1404, Edinburgh, Scotland, UK., July. Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An up- 11(1):10–18.</note>
<author confidence="0.9106765">Marco Marelli</author>
<author confidence="0.9106765">Luisa Bentivogli</author>
<author confidence="0.9106765">Marco Baroni</author>
<author confidence="0.9106765">Raffaela Bernardi</author>
<author confidence="0.9106765">Stefano Menini</author>
<author confidence="0.9106765">Roberto Zam-</author>
<abstract confidence="0.5491425">parelli. 2014a. SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and texentailment. In of SemEval 2014:</abstract>
<title confidence="0.900086">Workshop on Semantic</title>
<author confidence="0.937899">Marco Marelli</author>
<author confidence="0.937899">Stefano Menini</author>
<author confidence="0.937899">Marco Baroni</author>
<author confidence="0.937899">Luisa Bentivogli</author>
<author confidence="0.937899">Raffaela Bernardi</author>
<author confidence="0.937899">Roberto Zampar-</author>
<abstract confidence="0.893761">elli. 2014b. A SICK cure for the evaluation of comdistributional semantic models. In Proof Kachites McCallum. 2002. Mallet: A machine learning for language toolkit.</abstract>
<web confidence="0.911074">http://mallet.cs.umass.edu.</web>
<note confidence="0.96845025">Jeff Mitchell and Mirella Lapata. 2010. Composition distributional models of semantics. Sci- 34(8):1388–1429. Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.</note>
<title confidence="0.759273">Maltparser: A data-driven parser-generator for de-</title>
<abstract confidence="0.650177666666667">parsing. In of volume 6, pages 2216–2219. Vasin Punyakanok and Dan Roth. 2001. The use of in sequential inference. In pages 995–1001. MIT Press. Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Natural Language pages 1201–1211, Jeju Island, Korea, July. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for seinference over event descriptions. Transactions of the Association for Computational Linguis- 2:67–78.</abstract>
<intro confidence="0.636135">334</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="3295" citStr="Baroni and Zamparelli, 2010" startWordPosition="504" endWordPosition="507">r tokens that appear at least 10 times in Young et al. (2014)’s image description corpus. In the vector space, each dimension corresponds to one of the 1000 most frequent lemmas (contexts). The jth entry of the vector of wi is the positive normalized pointwise mutual information (pnPMI) between target wi and context wj: We define P(wi) as the fraction of images with at least one caption containing wi, and P(wi, wj) as the fraction of images whose captions contain both wi and wj. Following recent work that extends distributional similarities to phrases and sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 1http://nlp.stanford.edu/software/ corenlp.shtml pnPMI(wi, wj) = max ⎝0, log1 p( )ip( wj) / − log \ (P (wi, wj)) 329 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 329–334, Dublin, Ireland, August 23-24, 2014. Features Description # of features Negation True if either sentence contains explicit negation; False otherwise 1 Word overlap Ratio of overlapping word types to total word types in s1 and s2 1 Denotational constituent similarity Positive normalized PMI of constituent nodes in the denotation 30 graph Distributional</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<location>Edinburgh, Scotland, UK.,</location>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404, Edinburgh, Scotland, UK., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="16569" citStr="Hall et al., 2009" startWordPosition="2771" endWordPosition="2774">list or occur in one of these patterns: X–not X, X–no X, X–no head-noun(X) (e.g. blue hat–no hat), X– no hypernym(X) (e.g. poodle–no dog), X–no synonym(X) (e.g. kid–no child). For each antonym pair, we set the similarity score of that match to 0.0. For example, num ant = 1 for s1 = “A small white dog is running across a lawn” and s2 = “A big white dog is running across a lawn.” In addition, num ant = 1 for s1 = “A woman is leaning on the ledge of a balcony” and s2 = “A man is leaning on the ledge of a balcony.” 2.5 Models For the SR task, we implement a log-linear regression model using Weka (Hall et al., 2009). Specif332 Accuracy Pearson ρ Chance baseline 33.3 – Majority baseline 56.7 – Probability baseline 41.8 – Overlap baseline 56.2 0.627 Submitted system 84.5 0.799 Table 2: TE and SR results on test data. Model Accuracy Pearson ρ Overlap baseline 56.8 0.646 Negation 61.0 0.093 Word overlap 65.0 0.694 (+Vector composition) 66.4 0.697 +Denotational similarity 74.4 0.751 +Distributional similarity 71.8 0.756 +Den +Dist 77.0 0.782 +Alignment 70.4 0.697 +Unaligned chunk matching 75.8 0.719 +Align +Match 75.2 0.728 +Synonyms 65.2 0.696 +Hypernyms 66.8 0.716 +Antonyms 71.0 0.704 All features 84.2 0.80</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaela Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="654" citStr="Marelli et al., 2014" startWordPosition="89" endWordPosition="92">stributional Approach to Semantics Alice Lai and Julia Hockenmaier Department of Computer Science University of Illinois at Urbana-Champaign {aylai2, juliahmr}@illinois.edu Abstract This paper describes and analyzes our SemEval 2014 Task 1 system. Its features are based on distributional and denotational similarities; word alignment; negation; and hypernym/hyponym, synonym, and antonym relations. 1 Task Description SemEval 2014 Task 1 (Marelli et al., 2014a) evaluates system predictions of semantic relatedness (SR) and textual entailment (TE) relations on sentence pairs from the SICK dataset (Marelli et al., 2014b). The dataset is intended to test compositional knowledge without requiring the world knowledge that is often required for paraphrase classification or Recognizing Textual Entailment tasks. SR scores range from 1 to 5. TE relations are ‘entailment,’ ‘contradiction,’ and ‘neutral.’ Our system uses features that depend on the amount of word overlap and alignment between the two sentences, the presence of negation, and the semantic similarities of the words and substrings that are not shared across the two sentences. We use simple distributional similarities as well as the recently proposed den</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaela Bernardi, Stefano Menini, and Roberto Zamparelli. 2014a. SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Luisa Bentivogli</author>
<author>Raffaela Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>A SICK cure for the evaluation of compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="654" citStr="Marelli et al., 2014" startWordPosition="89" endWordPosition="92">stributional Approach to Semantics Alice Lai and Julia Hockenmaier Department of Computer Science University of Illinois at Urbana-Champaign {aylai2, juliahmr}@illinois.edu Abstract This paper describes and analyzes our SemEval 2014 Task 1 system. Its features are based on distributional and denotational similarities; word alignment; negation; and hypernym/hyponym, synonym, and antonym relations. 1 Task Description SemEval 2014 Task 1 (Marelli et al., 2014a) evaluates system predictions of semantic relatedness (SR) and textual entailment (TE) relations on sentence pairs from the SICK dataset (Marelli et al., 2014b). The dataset is intended to test compositional knowledge without requiring the world knowledge that is often required for paraphrase classification or Recognizing Textual Entailment tasks. SR scores range from 1 to 5. TE relations are ‘entailment,’ ‘contradiction,’ and ‘neutral.’ Our system uses features that depend on the amount of word overlap and alignment between the two sentences, the presence of negation, and the semantic similarities of the words and substrings that are not shared across the two sentences. We use simple distributional similarities as well as the recently proposed den</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaela Bernardi, and Roberto Zamparelli. 2014b. A SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="17407" citStr="McCallum, 2002" startWordPosition="2905" endWordPosition="2907">son ρ Overlap baseline 56.8 0.646 Negation 61.0 0.093 Word overlap 65.0 0.694 (+Vector composition) 66.4 0.697 +Denotational similarity 74.4 0.751 +Distributional similarity 71.8 0.756 +Den +Dist 77.0 0.782 +Alignment 70.4 0.697 +Unaligned chunk matching 75.8 0.719 +Align +Match 75.2 0.728 +Synonyms 65.2 0.696 +Hypernyms 66.8 0.716 +Antonyms 71.0 0.704 All features 84.2 0.802 Table 3: TE and SR results on trial data. ically, under Weka’s default settings, we train a ridge regression model with regularization parameter α = 1×10−8. For the TE task, we use a MaxEnt model implemented with MALLET (McCallum, 2002). The MaxEnt model is optimized with L-BFGS, using the default settings. Both models use the same set of features. 3 Results Our submitted system was trained on the full training and trial data (5000 sentences). Table 2 shows our results on the test data. We substantially outperform all baselines. 3.1 Feature Ablation We train models on the training data and test on the trial data. Models marked with + include our word overlap feature. We also examine a single compositional feature (vector composition): the cosine similarity of two sentence vectors. A sentence vector is the pointwise multiplic</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="3266" citStr="Mitchell and Lapata, 2010" startWordPosition="500" endWordPosition="503">tion, we compute vectors for tokens that appear at least 10 times in Young et al. (2014)’s image description corpus. In the vector space, each dimension corresponds to one of the 1000 most frequent lemmas (contexts). The jth entry of the vector of wi is the positive normalized pointwise mutual information (pnPMI) between target wi and context wj: We define P(wi) as the fraction of images with at least one caption containing wi, and P(wi, wj) as the fraction of images whose captions contain both wi and wj. Following recent work that extends distributional similarities to phrases and sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 1http://nlp.stanford.edu/software/ corenlp.shtml pnPMI(wi, wj) = max ⎝0, log1 p( )ip( wj) / − log \ (P (wi, wj)) 329 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 329–334, Dublin, Ireland, August 23-24, 2014. Features Description # of features Negation True if either sentence contains explicit negation; False otherwise 1 Word overlap Ratio of overlapping word types to total word types in s1 and s2 1 Denotational constituent similarity Positive normalized PMI of constituent nodes in the denot</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>2216--2219</pages>
<contexts>
<context position="2247" citStr="Nivre et al., 2006" startWordPosition="330" endWordPosition="333">relatedness and This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/ by/4.0/ textual entailment. We use distributional similarity features, denotational similarity features, and alignment features based on shallow syntactic structure. 2.1 Preprocessing We lemmatize all sentences with the Stanford CoreNLP system1 and extract syntactic chunks with the Illinois Chunker (Punyakanok and Roth, 2001). Like Young et al. (2014), we use the Malt parser (Nivre et al., 2006) to identify 5 sets of constituents for each sentence: subject NPs, verbs, VPs, direct object NPs, and other NPs. For stopwords, we use the NLTK English stopword list of 127 high-frequency words. We remove negation words (no, not, and nor) from the stopword list since their presence is informative for this dataset and task. 2.2 Distributional Similarities After stopword removal and lemmatization, we compute vectors for tokens that appear at least 10 times in Young et al. (2014)’s image description corpus. In the vector space, each dimension corresponds to one of the 1000 most frequent lemmas (</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC, volume 6, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In NIPS,</booktitle>
<pages>995--1001</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2176" citStr="Punyakanok and Roth, 2001" startWordPosition="316" endWordPosition="319"> system combines different sources of semantic similarity to predict semantic relatedness and This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/ by/4.0/ textual entailment. We use distributional similarity features, denotational similarity features, and alignment features based on shallow syntactic structure. 2.1 Preprocessing We lemmatize all sentences with the Stanford CoreNLP system1 and extract syntactic chunks with the Illinois Chunker (Punyakanok and Roth, 2001). Like Young et al. (2014), we use the Malt parser (Nivre et al., 2006) to identify 5 sets of constituents for each sentence: subject NPs, verbs, VPs, direct object NPs, and other NPs. For stopwords, we use the NLTK English stopword list of 127 high-frequency words. We remove negation words (no, not, and nor) from the stopword list since their presence is informative for this dataset and task. 2.2 Distributional Similarities After stopword removal and lemmatization, we compute vectors for tokens that appear at least 10 times in Young et al. (2014)’s image description corpus. In the vector spac</context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>Vasin Punyakanok and Dan Roth. 2001. The use of classifiers in sequential inference. In NIPS, pages 995–1001. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="4730" citStr="Socher et al., 2012" startWordPosition="726" endWordPosition="729"> unaligned chunks Unaligned matching Ratio of number of matched chunks to unaligned chunks; max, min, 31 average matched chunk similarity; number of crossings in matching Chunk alignment Number of chunks; number of unaligned chunk labels; ratio of un- 17 aligned chunk labels to number of chunks; number of matched labels; ratio of matched to unmatched chunk labels Synonym Number of matched synonym pairs (w1, w2) 1 Hypernym Number of matched hypernym pairs (w1, w2), number of matched 2 hypernym pairs (w2, w1) Antonym Number of matched antonym pairs (w1, w2) 1 Table 1: Summary of features. 2011; Socher et al., 2012), we define a phrase vector p to be the pointwise multiplication product of the vectors of the words in the phrase: p = w1 O ... Own where O is the multiplication of corresponding vector components, i.e. pi = ui · vi. 2.3 Denotational Similarities In Young et al. (2014), we introduce denotational similarities, which we argue provide a more precise metric for semantic inferences. We use an image-caption corpus to define the (visual) denotation of a phrase as the set of images it describes, and construct a denotation graph, i.e. a subsumption hierarchy (lattice) of phrases paired with their deno</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Young</author>
<author>Alice Lai</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics,</title>
<date>2014</date>
<pages>2--67</pages>
<contexts>
<context position="1299" citStr="Young et al. (2014)" startWordPosition="189" endWordPosition="192">d to test compositional knowledge without requiring the world knowledge that is often required for paraphrase classification or Recognizing Textual Entailment tasks. SR scores range from 1 to 5. TE relations are ‘entailment,’ ‘contradiction,’ and ‘neutral.’ Our system uses features that depend on the amount of word overlap and alignment between the two sentences, the presence of negation, and the semantic similarities of the words and substrings that are not shared across the two sentences. We use simple distributional similarities as well as the recently proposed denotational similarities of Young et al. (2014), which are intended as more precise metrics for tasks that require entailment. Both similarity types are estimated on Young et al.’s corpus, which contains 31,783 images of everyday scenes, each paired with five descriptive captions. 2 Our System Our system combines different sources of semantic similarity to predict semantic relatedness and This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/ by/4.0/ textual entailment. We use distributional </context>
<context position="2729" citStr="Young et al. (2014)" startWordPosition="411" endWordPosition="414">actic chunks with the Illinois Chunker (Punyakanok and Roth, 2001). Like Young et al. (2014), we use the Malt parser (Nivre et al., 2006) to identify 5 sets of constituents for each sentence: subject NPs, verbs, VPs, direct object NPs, and other NPs. For stopwords, we use the NLTK English stopword list of 127 high-frequency words. We remove negation words (no, not, and nor) from the stopword list since their presence is informative for this dataset and task. 2.2 Distributional Similarities After stopword removal and lemmatization, we compute vectors for tokens that appear at least 10 times in Young et al. (2014)’s image description corpus. In the vector space, each dimension corresponds to one of the 1000 most frequent lemmas (contexts). The jth entry of the vector of wi is the positive normalized pointwise mutual information (pnPMI) between target wi and context wj: We define P(wi) as the fraction of images with at least one caption containing wi, and P(wi, wj) as the fraction of images whose captions contain both wi and wj. Following recent work that extends distributional similarities to phrases and sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 1htt</context>
<context position="5000" citStr="Young et al. (2014)" startWordPosition="777" endWordPosition="780">bels to number of chunks; number of matched labels; ratio of matched to unmatched chunk labels Synonym Number of matched synonym pairs (w1, w2) 1 Hypernym Number of matched hypernym pairs (w1, w2), number of matched 2 hypernym pairs (w2, w1) Antonym Number of matched antonym pairs (w1, w2) 1 Table 1: Summary of features. 2011; Socher et al., 2012), we define a phrase vector p to be the pointwise multiplication product of the vectors of the words in the phrase: p = w1 O ... Own where O is the multiplication of corresponding vector components, i.e. pi = ui · vi. 2.3 Denotational Similarities In Young et al. (2014), we introduce denotational similarities, which we argue provide a more precise metric for semantic inferences. We use an image-caption corpus to define the (visual) denotation of a phrase as the set of images it describes, and construct a denotation graph, i.e. a subsumption hierarchy (lattice) of phrases paired with their denotations. For example, the denotation of the node man is the set of images in the corpus that contain a man, and the denotation of the node person is rock climbing is the set of images that depict a person rock climbing. We define the (symmetric) denotational similarity </context>
<context position="7178" citStr="Young et al. (2014)" startWordPosition="1150" endWordPosition="1153">0 otherwise. Word Overlap We compute �W1�W2� �W1�W2� on lemmatized sentences without stopwords where Wi is the set of word types that appear in si. Training a MaxEnt or log-linear model using this feature achieves better performance than the word overlap baseline provided by the task organizers. Denotational Constituent Similarity Denotational similarity captures entailment-like relations between events. For example, sit and eat lunch have a high pnPMI, which follows our intuition that a person who is eating lunch is likely to be sitting. We use the same denotational constituent features that Young et al. (2014) use for a textual similarity task. C are original nodes, Canc are parent and grandparent nodes, and sim(Ca, Cb) is the maximum pnPMI of any pair of nodes a E Ca, b E Cb. C-C features compare constituents of the same type. These features express how often we expect corresponding constituents to describe the same situation. For example, s1 = “Girls are doing backbends and playing outdoors” and s2 = “Chil330 dren are doing backbends” have subject nodes {girl} and {child}. Girls are sometimes described as children, so sim(girl, child) = 0.498. In addition, child is a parent node of girl, so max(s</context>
</contexts>
<marker>Young, Lai, Hodosh, Hockenmaier, 2014</marker>
<rawString>Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67–78.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>