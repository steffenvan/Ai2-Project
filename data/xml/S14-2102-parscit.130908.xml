<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006829">
<title confidence="0.9951985">
SSMT: A Machine Translation Evaluation View to Paragraph-to-Sentence
Semantic Similarity
</title>
<author confidence="0.998256">
Pingping Huang
</author>
<affiliation confidence="0.988722666666667">
Department of Linguistic Engineering
School of Software and Microelectronics
Peking University, China
</affiliation>
<email confidence="0.99464">
girlhpp@163.com
</email>
<sectionHeader confidence="0.993759" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999891875">
This paper presents the system SSMT
measuring the semantic similarity between
a paragraph and a sentence submitted to
the SemEval 2014 task3: Cross-level Se-
mantic Similarity. The special difficulty
of this task is the length disparity between
the two semantic comparison texts. We
adapt several machine translation evalua-
tion metrics for features to cope with this
difficulty, then train a regression model for
the semantic similarity prediction. This
system is straightforward in intuition and
easy in implementation. Our best run gets
0.808 in Pearson correlation. METEOR-
derived features are the most effective
ones in our experiment.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999792722222222">
Cross level semantic similarity measures the simi-
larity between different levels of text unit, for ex-
ample, between a document and a paragraph, or
between a phrase and a word.
Paragraph and sentence are the natural language
units to convey opinions or state events in daily
life. We can see posts on forums, questions and
answers in Q&amp;A communities and customer re-
views on E-commerce websites, are mainly organ-
ised in these two units. Better similarity measure-
ment across them will be helpful in clustering sim-
ilar answers or reviews.
The paragraph-to-sentence semantic similarity
subtask in SemEval2014 task3 (Jurgens et al.,
2014) is the first semantic similarity competition
across these two language levels. The special
difficulty of this task is the length disparity be-
tween the compared pair: a paragraph contains
</bodyText>
<footnote confidence="0.9572215">
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<author confidence="0.406861">
Baobao Chang
</author>
<affiliation confidence="0.5762365">
Key Laboratory of Computational
Linguistics, Ministry of Education
Institute of Computational Linguistics
Peking University, China
</affiliation>
<email confidence="0.971133">
chbb@pku.edu.cn
</email>
<bodyText confidence="0.998669333333333">
3.67 times the words of a sentence on average in
the training set.
Semantic similarity on different levels, for ex-
ample, on word level (Mikolov et al., 2013), sen-
tences level (B¨ar et al., 2012), document level
(Turney and Pantel, 2010), have been well studied,
yet methods on one level can hardly be applied to
a different level, let alone be applied for the cross-
level tasks. The work of Pilehvar et al.(2013) was
an exception. They proposed a unified method for
semantic comparison at multi-levels all the way
from comparing word senses to comparing text
documents
Our work is inspired by automatic machine
translation(MT) evaluation, in which different
metrics are designed to compare the adequacy and
fluency of a MT system’s output, called hypothe-
sis, against a gold standard translation, called ref-
erence. As MT evaluation metrics measure sen-
tence pair similarity, it is a natural idea to general-
ize them for paragraph-sentence pair.
In this paper, we follow the motivations of sev-
eral MT evaluation metrics yet made adaption to
cope with the length disparity difficulty of this
task, and combine these features in a regression
model. Our system SSMT (Semantic Similarity in
view of Machine Translation evaluation) involves
no extensive resource or strenuous computation,
yet gives promising result with just a few simple
features.
</bodyText>
<sectionHeader confidence="0.995892" genericHeader="method">
2 Regression Framework
</sectionHeader>
<bodyText confidence="0.999491833333333">
In our experiment, we use features adapted from
some MT evaluation metrics and combine them
in a regression model for the semantic similarity
measurement. We exploit the following two sim-
ple models:
A linear regression model is presented as:
</bodyText>
<equation confidence="0.949395">
y = w1xz + w2xz.. + wnxn + e
</equation>
<page confidence="0.95356">
585
</page>
<note confidence="0.704687">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 585–589,
Dublin, Ireland, August 23-24, 2014.
</note>
<equation confidence="0.92617025">
A log-linear model is presented as:
y = x11· x22 .. ·xwn
n · eε
Where y is the similarity score, {x1, x2.., xn} are
</equation>
<bodyText confidence="0.988450166666667">
the feature values.
We can see that in a log-linear model, if any
feature xi get a value of 0, the output y will suck in
0 forever no matter what the values other features
get. In our experiment we resort to smoothing to
avoid this “0-trap” for some features (Section 4.3).
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999819666666667">
MT evaluation metrics vary from lexical level to
syntactic level to semantic level. We consider only
lexical ones to avoid complicated steps like pars-
ing or semantic role labelling, which are computa-
tional expensive and may bring extra noise.
But instead of directly using the MT evaluation
metrics, we use the factors in them as features, the
idea is that the overall score of the original metric
is highly related to the length of both of the com-
pared pair, but its factors are often related to the
length of just one side yet still carry useful simi-
larity information.
</bodyText>
<subsectionHeader confidence="0.997559">
3.1 BLEU-Derived Features
</subsectionHeader>
<bodyText confidence="0.999987923076923">
As the most wildly used MT evaluation metric,
BLEU (Papineni et al., 2002) uses the geomet-
ric mean of n-gram precisions to measure the hy-
potheses against references. It is a corpus-based
and precision-based metric, and uses “brevity
penalty” as a replacement for recall. Yet this
penalty is meaningless on sentence level. There-
fore we considers only the precision factors in
BLEU:
We use the modified n-gram precision here and
regard “paragraph” as “reference”, and “sentence”
as the “hypothesis”. N= 1,2,3,4. We call these
four features BLEU-derived features.
</bodyText>
<subsectionHeader confidence="0.996473">
3.2 ROUGE-L-Derived Features
</subsectionHeader>
<bodyText confidence="0.999929">
ROUGE-L (Lin and Och, 2004) measures the
largest common subsequence(LCS) between a
compared pair. BLEU implies the n-gram to be
consecutive, yet ROUGE-L allows for gaps be-
tween them. By considering only in-sequence
words, ROUGE-L captures sentence level struc-
ture in a natural way, then:
</bodyText>
<equation confidence="0.981913666666667">
LC5(re f, hyo)
Rlcs = length(hyo)
LC5(ref, hyo)
</equation>
<bodyText confidence="0.9997168">
Where LC5(ref, hoy) is the length of LCS of the
compared pair. We set 0 = 1, which means we
don’t want to make much distinction between the
“reference” and “hypothesis” here. We call these
three features ROUGE-L-derived features.
</bodyText>
<subsectionHeader confidence="0.989933">
3.3 ROUGE-S-Derived Features
</subsectionHeader>
<bodyText confidence="0.991382833333333">
ROUGE-S (Lin and Och, 2004) uses skip-bigram
co-occurrence statistics for similarity measure-
ment. One advantage of skip-bigram over BLEU
is that it does not require consecutive matches but
is still sensitive to word order. Given the reference
of length n, and hypothesis of length m, then:
</bodyText>
<equation confidence="0.916826333333333">
skip2(ref, hyo)
C(m, 2)
skip2(ref, hyo)
C(n, 2)
(1 + 02)Pskip2Rskip2
Rskip2 + 02Pskip2
</equation>
<bodyText confidence="0.99836825">
Where C is combination, and skip2(ref, hyo) is
the number of common skip-bigrams. We also
set 0 = 1 here, and call these three indicators
ROUGE-S-derived features.
</bodyText>
<subsectionHeader confidence="0.920614">
3.4 METEOR-Derived Features
</subsectionHeader>
<bodyText confidence="0.9979305">
METEOR (Banerjee and Lavie, 2005) evaluates
a hypothesis by aligning it to a reference trans-
lation and gives sentence-level similarity scores.
It uses a generalized concept of unigram mapping
that matches words in the following types: ex-
act match on words surface forms , stem match
on words stems, synonym match according to the
synonym sets in WordNet, and paraphrase match
(Denkowski and Lavie, 2010).
METEOR also makes distinction between con-
tent words and function words. Each type of
match mi is weighted by wi, let (mi(hc), mi(hf))
be the number of content and function words
covered by this type in the hypothesis, and
</bodyText>
<equation confidence="0.973803375">
PnBLEU = Ngramref
Ngramref n Ngramhyo
Plcs =length(ref)
Flcs = (1 + 02)RlcsPlcs)
Rlcs + 02Plcs
Pskip2 =
Rskip2 =
Fskip2 =
586
(mi(rc), mi(rf)) be the counts in the reference,
then:
P = �i=1 wi · (δ · mi(hi) + (1 − δ) · mi(hf))
δ·  |hc  |+(1 − δ)· |hf |
�i=1 wi · (δ · mi(ri) + (1 − δ) · mi(rf))
R =
δ·  |rc  |+(1 − δ)·  |rf |
</equation>
<bodyText confidence="0.999579">
To account for word order difference, the frag-
mentation penalty is calculated using the total
number of matched words(m) and the number of
chunks1(ch) in the hypothesis:
</bodyText>
<equation confidence="0.9175585">
(ch )β
m
</equation>
<bodyText confidence="0.67832">
And the final METEOR score is:
</bodyText>
<equation confidence="0.771136">
Score = (1 − Pen) · Fmean
Parameters α, β, -y, δand wi...wn are tuned to
</equation>
<bodyText confidence="0.999731875">
maximize correlation with human judgements
(Denkowski and Lavie, 2014). We use Meteor1.5
system2 for scoring. Parameters are tuned on
WMT12, and the paraphrase table is extracted on
the WMT data.
We use the p, r, frag(frag = ch/m) and
score as features and call them METEOR-derived
features.
</bodyText>
<sectionHeader confidence="0.995564" genericHeader="evaluation">
4 Experiment and Discussion
</sectionHeader>
<subsectionHeader confidence="0.990358">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999971">
The SemEval2014 task3 subtask gives a train-
ing set of 500 paragraph-sentence pairs, with hu-
man annotated continuous score of 0 − 4. These
pairs are labelled with genres of “Newswire/ cqa3/
metaphoric/ scientific/ travel/ review”. Systems
are asked to predict the similarity scores for 500
pairs in the test set. Performance is evaluated in
Pearson correlation and Spearman correlation.
</bodyText>
<subsectionHeader confidence="0.985553">
4.2 Data Processing
</subsectionHeader>
<bodyText confidence="0.9999158">
To avoid meaningless n-gram match “the a”, or
words surface form difference, we employ very
simple data processings here: for features derived
from BLEU, ROUGE-L and ROUGE-S, we re-
move stop words and stem the sentences with
</bodyText>
<footnote confidence="0.98362275">
1Chunk is defined as a series of matched unigrams that is
contiguous and identically ordered in both sentences
2https://www.cs.cmu.edu/ alavie/METEOR/
3cqa:Community Question Answering site text
</footnote>
<bodyText confidence="0.997978666666667">
coreNLP4. For METEOR-derived features, we use
the tool’s option for text normalization before
matching.
</bodyText>
<subsectionHeader confidence="0.988646">
4.3 Result
</subsectionHeader>
<bodyText confidence="0.998672166666667">
Though texts with different genres may have dif-
ferent regression parameters, we just train one
model for all for simplicity. Table 1 compares
the result. Run1 is submitted as SSMT in the
official evaluation. It’s a log-linear model. We
choose more dense features for log-linear model
and use smoothing to avoid the “0-trap” men-
tioned in (Section 2). The features include
P1,2BLEU,PROUGE−L,PROUGE−S 4 features, and
4 METEOR-derived features, altogether 8 fea-
tures. When calculation the first 4 features, we
plus 1 to both numerator and denominator as
smoothing. Run2 is a linear-regression model with
the same features as Run1. Run3 is a simple linear
regression model, which is free from the “0-trap”,
thus we use all the 14 features without smoothing.
We use Matlab for regression. The baseline is of-
ficially given using LCS.
</bodyText>
<table confidence="0.9971602">
Run Regression Pearson Spearman
Baseline LCS 0.527 0.613
run1 log-linear 0.789 0.777
run2 linear 0.794 0.777
run3 linear 0.808 0.792
</table>
<tableCaption confidence="0.999481">
Table 1: System Performance.
</tableCaption>
<subsectionHeader confidence="0.998388">
4.4 System Analysis
</subsectionHeader>
<bodyText confidence="0.999745">
We compares the effectiveness of different fea-
tures in a linear regression model. Table 2
shows the result. “All” refers to all the fea-
tures, “-METEOR” means the feature set ex-
cludes METEOR-derived features. We can see the
METEOR-derived features are the most effective
ones here.
Figure 1 shows the performance of our sys-
tem submitted as SSMT in the SemEval2014 task3
competition. It shows quite good correlation with
the gold standard.
A well predicted example is the #trial-p2s-5 pair
in the trial set:
</bodyText>
<footnote confidence="0.5459656">
Paragraph: Olympic champion Usain Bolt re-
gained his 100m world title and won a fourth in-
dividual World Championships gold with a sea-
son’s best of 9.77 seconds in Moscow. In heavy
4http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<equation confidence="0.998935">
Fmean = P · R
αP + (1 − α)R
Pen = -y ·
</equation>
<page confidence="0.992705">
587
</page>
<table confidence="0.998905666666667">
Feature Pearson Spearman
All 0.808 0.792
-METEOR 0.772 0.756
- ROUGE-L 0.802 0.789
- ROUGE-S 0.807 0.793
- BLEU 0.807 0.790
</table>
<tableCaption confidence="0.990213666666667">
Table 2: Effectiveness of Different Features.
“-METEOR” means the feature set excluding
METEOR-derived features.
</tableCaption>
<figureCaption confidence="0.999022">
Figure 1: Result Scatter of SSMT.
</figureCaption>
<bodyText confidence="0.991875413043478">
rain, the 26-year-old Jamaican made amends for
his false start in Daegu two years ago and fur-
ther cemented his status as the greatest sprinter
in history. The six-time Olympic champion over-
took Justin Gatlin in the final stages, forcing the
American to settle for silver in 9.85. Bolt’s com-
patriot Nesta Carter (9.95) claimed bronze, while
Britain’s James Dasaolu was eighth (10.21).
Sentence: Germany’s Robert Harting beats
Iran’s Ehsan Hadadi and adds the Olympic discus
title to his world crown.
The system gives a prediction of 1.253 against
the gold standard 1.25. We can see that topic
words like “Olympic” , “world crown”, “beats” in
the short text correspond to expressions of “world
title ” , “champion” across several sentences in the
long text, but this pair of texts are not talking about
the same event. The model captures and models
this commonness and difference very well .
But Figure 1 also reveals an interesting phe-
nomenon: the system seldom gives the boundary
scores of 0 or 4. In other words, it tends to over-
score or underscore the boundary conditions. An
example in point is the #trial-p2s-17 pair in the
trial data, it is actually the worst predicted pair by
our system in the trail set:
Paragraph: A married couple who met at work
is not a particularly rare thing. Three in ten work-
ers who have dated a colleague said in a recent
survey by CareerBuilder.com that their office ro-
mance eventually led to marriage.
Sentence: Marrying a coworker isn’t uncom-
mon given that 30% of workers who dated a
coworker ended up marrying them.
The system gives a 1.773 score against the gold
standard of 4. It should fail to detect the equality
of expressions between “three in ten” and “30%”.
Thus better detection of phrase similarity is de-
sired. We think this is the main reason to under-
score the similarity. For test pairs with the genre of
“Metaphoric”, the system almost underscores all
of them. This failure has been expected, though.
Because “Metaphoric” pairs demand full under-
standing of the semantic meaning and paragraph
structure, which is far beyond the reach of lexical
match metrics.
</bodyText>
<sectionHeader confidence="0.998832" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999870421052632">
MT evaluation metrics have been directly used
as features in paraphrase (Finch et al., 2005) de-
tection and sentence pair semantic comparison
(Souza et al., 2012). But paragraph-to-sentence
pair faces significant length disparity, we try a way
out to alleviate this impact yet still follow the mo-
tivations underlying these metrics. By factorizing
down the original metrics, the linear model can
flexibly pick out factors that are not sensitive to
the length disparity problem.
We derive features from BLEU, ROUGE-
L, ROUGE-S and METEOR, and show that
METEOR-derived features make the most signifi-
cant contributions here. Being easy and light, our
submitted SSMT achieves 0.789 in Pearson and
0.777 in Spearman correlation, and ranks 11 out
of the 34 systems in this subtask. Our best try
achieves 0.808 in Pearson and 0.786 in Spearman
correlation.
</bodyText>
<sectionHeader confidence="0.996762" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.87529525">
This work is supported by National Natu-
ral Science Foundation of China under Grant
No.61273318 and National Key Basic Research
Program of China 2014CB340504.
</bodyText>
<page confidence="0.997379">
588
</page>
<sectionHeader confidence="0.990101" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995382">
Andrew Finch, Yong S. Hwang, Eiichiro Sumita. Us-
ing machine translation evaluation techniques to de-
termine sentence-level semantic equivalence. Pro-
ceedings of the Third International Workshop on
Paraphrasing(IWP2005), 2005: 17-24.
Chin Y. Lin,Franz J. Och. Automatic evaluation of ma-
chine translation quality using longest common sub-
sequence and skip-bigram statistics. Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics. ACL, 2004: 605.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, et al.
Ukp: Computing semantic textual similarity by
combining multiple content similarity measures.
Proceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation. ACL, 2012: 435-
440.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. SemEval-2014 Task 3: Cross-
Level Semantic Similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014)., August 23-24, 2014, Dublin, Ire-
land.
George Miller,Christiane Fellbaum. WordNet.
http://wordnet.princton.edu/, 2007.
Jos´e G C de Souza, Matteo Negri, Yashar Mehdad.
FBK: machine translation evaluation and word sim-
ilarity metrics for semantic textual similarity. Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation. ACL, 2012: 624-
630.
Kishore Papineni, Salim Roukos, Todd Ward, et al.
BLEU: a method for automatic evaluation of ma-
chine translation. Proceedings of the 40th annual
meeting on association for computational linguis-
tics. ACL, 2002: 311-318.
Michael Denkowski, Alon Lavie. Extending the ME-
TEOR machine translation evaluation metric to the
phrase level. Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics. ACL, 2010: 250-253.
Michael Denkowski, Alon Lavie. Meteor Univer-
sal: Language Specific Translation /Evaluation for
Any Target Language translation. Proceedings of the
EACL 2014 Workshop on Statistical Machine Trans-
lation, 2014.
Mohammad T Pilehvar, David Jurgens, Roberto Nav-
igli. Align, Disambiguate and Walk: A Unified
Approach for Measuring Semantic Similarity Pro-
ceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics,ACL, 2013:
1341-1351.
Peter D. Turney and Patrick Pantel. From frequency to
meaning: Vector space models of semantics Artifi-
cial Intelligence Research, 2010. 37(1): 141-188
Satanjeev Banerjee, Alon Lavie. METEOR: an auto-
matic metric for MT Evaluation with improved cor-
relation with human judgements. Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summa-
rization., 2005: 65-72.
Tomas Mikolov, Kai Chen, Greg Corrado, et al. Ef-
ficient estimation of word representations in vector
space. 2013. arXiv preprint arXiv:1301.3781,
</reference>
<page confidence="0.998839">
589
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.616522">
<title confidence="0.986122">SSMT: A Machine Translation Evaluation View to Paragraph-to-Sentence Semantic Similarity</title>
<author confidence="0.643898">Pingping</author>
<affiliation confidence="0.999013666666667">Department of Linguistic School of Software and Peking University,</affiliation>
<email confidence="0.998641">girlhpp@163.com</email>
<abstract confidence="0.998805176470588">This paper presents the system SSMT measuring the semantic similarity between a paragraph and a sentence submitted to the SemEval 2014 task3: Cross-level Semantic Similarity. The special difficulty of this task is the length disparity between the two semantic comparison texts. We adapt several machine translation evaluation metrics for features to cope with this difficulty, then train a regression model for the semantic similarity prediction. This system is straightforward in intuition and easy in implementation. Our best run gets 0.808 in Pearson correlation. METEORderived features are the most effective ones in our experiment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Yong S Hwang</author>
</authors>
<title>Eiichiro Sumita. Using machine translation evaluation techniques to determine sentence-level semantic equivalence.</title>
<date>2005</date>
<booktitle>Proceedings of the Third International Workshop on Paraphrasing(IWP2005),</booktitle>
<pages>17--24</pages>
<marker>Finch, Hwang, 2005</marker>
<rawString>Andrew Finch, Yong S. Hwang, Eiichiro Sumita. Using machine translation evaluation techniques to determine sentence-level semantic equivalence. Proceedings of the Third International Workshop on Paraphrasing(IWP2005), 2005: 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin Y Lin</author>
<author>Franz J Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.</title>
<date>2004</date>
<booktitle>Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. ACL,</booktitle>
<pages>605</pages>
<contexts>
<context position="5466" citStr="Lin and Och, 2004" startWordPosition="860" endWordPosition="863"> As the most wildly used MT evaluation metric, BLEU (Papineni et al., 2002) uses the geometric mean of n-gram precisions to measure the hypotheses against references. It is a corpus-based and precision-based metric, and uses “brevity penalty” as a replacement for recall. Yet this penalty is meaningless on sentence level. Therefore we considers only the precision factors in BLEU: We use the modified n-gram precision here and regard “paragraph” as “reference”, and “sentence” as the “hypothesis”. N= 1,2,3,4. We call these four features BLEU-derived features. 3.2 ROUGE-L-Derived Features ROUGE-L (Lin and Och, 2004) measures the largest common subsequence(LCS) between a compared pair. BLEU implies the n-gram to be consecutive, yet ROUGE-L allows for gaps between them. By considering only in-sequence words, ROUGE-L captures sentence level structure in a natural way, then: LC5(re f, hyo) Rlcs = length(hyo) LC5(ref, hyo) Where LC5(ref, hoy) is the length of LCS of the compared pair. We set 0 = 1, which means we don’t want to make much distinction between the “reference” and “hypothesis” here. We call these three features ROUGE-L-derived features. 3.3 ROUGE-S-Derived Features ROUGE-S (Lin and Och, 2004) uses</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin Y. Lin,Franz J. Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. ACL, 2004: 605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation. ACL,</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, et al. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation. ACL, 2012: 435-440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval-2014 Task 3: CrossLevel Semantic Similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1515" citStr="Jurgens et al., 2014" startWordPosition="222" endWordPosition="225"> semantic similarity measures the similarity between different levels of text unit, for example, between a document and a paragraph, or between a phrase and a word. Paragraph and sentence are the natural language units to convey opinions or state events in daily life. We can see posts on forums, questions and answers in Q&amp;A communities and customer reviews on E-commerce websites, are mainly organised in these two units. Better similarity measurement across them will be helpful in clustering similar answers or reviews. The paragraph-to-sentence semantic similarity subtask in SemEval2014 task3 (Jurgens et al., 2014) is the first semantic similarity competition across these two language levels. The special difficulty of this task is the length disparity between the compared pair: a paragraph contains This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ Baobao Chang Key Laboratory of Computational Linguistics, Ministry of Education Institute of Computational Linguistics Peking University, China chbb@pku.edu.cn 3.67 times the words of a sentence on a</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. SemEval-2014 Task 3: CrossLevel Semantic Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014)., August 23-24, 2014, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>WordNet http wordnet princton edu</author>
</authors>
<date>2007</date>
<marker>edu, 2007</marker>
<rawString>George Miller,Christiane Fellbaum. WordNet. http://wordnet.princton.edu/, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e G C de Souza</author>
<author>Matteo Negri</author>
</authors>
<title>Yashar Mehdad. FBK: machine translation evaluation and word similarity metrics for semantic textual similarity.</title>
<date>2012</date>
<booktitle>Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation. ACL,</booktitle>
<pages>624--630</pages>
<marker>de Souza, Negri, 2012</marker>
<rawString>Jos´e G C de Souza, Matteo Negri, Yashar Mehdad. FBK: machine translation evaluation and word similarity metrics for semantic textual similarity. Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation. ACL, 2012: 624-630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th annual meeting on association for computational linguistics. ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="4923" citStr="Papineni et al., 2002" startWordPosition="777" endWordPosition="780">yntactic level to semantic level. We consider only lexical ones to avoid complicated steps like parsing or semantic role labelling, which are computational expensive and may bring extra noise. But instead of directly using the MT evaluation metrics, we use the factors in them as features, the idea is that the overall score of the original metric is highly related to the length of both of the compared pair, but its factors are often related to the length of just one side yet still carry useful similarity information. 3.1 BLEU-Derived Features As the most wildly used MT evaluation metric, BLEU (Papineni et al., 2002) uses the geometric mean of n-gram precisions to measure the hypotheses against references. It is a corpus-based and precision-based metric, and uses “brevity penalty” as a replacement for recall. Yet this penalty is meaningless on sentence level. Therefore we considers only the precision factors in BLEU: We use the modified n-gram precision here and regard “paragraph” as “reference”, and “sentence” as the “hypothesis”. N= 1,2,3,4. We call these four features BLEU-derived features. 3.2 ROUGE-L-Derived Features ROUGE-L (Lin and Och, 2004) measures the largest common subsequence(LCS) between a c</context>
</contexts>
<marker>Papineni, Roukos, Ward, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, et al. BLEU: a method for automatic evaluation of machine translation. Proceedings of the 40th annual meeting on association for computational linguistics. ACL, 2002: 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Extending the METEOR machine translation evaluation metric to the phrase level. Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics. ACL,</booktitle>
<pages>250--253</pages>
<contexts>
<context position="7006" citStr="Denkowski and Lavie, 2010" startWordPosition="1105" endWordPosition="1108">Pskip2Rskip2 Rskip2 + 02Pskip2 Where C is combination, and skip2(ref, hyo) is the number of common skip-bigrams. We also set 0 = 1 here, and call these three indicators ROUGE-S-derived features. 3.4 METEOR-Derived Features METEOR (Banerjee and Lavie, 2005) evaluates a hypothesis by aligning it to a reference translation and gives sentence-level similarity scores. It uses a generalized concept of unigram mapping that matches words in the following types: exact match on words surface forms , stem match on words stems, synonym match according to the synonym sets in WordNet, and paraphrase match (Denkowski and Lavie, 2010). METEOR also makes distinction between content words and function words. Each type of match mi is weighted by wi, let (mi(hc), mi(hf)) be the number of content and function words covered by this type in the hypothesis, and PnBLEU = Ngramref Ngramref n Ngramhyo Plcs =length(ref) Flcs = (1 + 02)RlcsPlcs) Rlcs + 02Plcs Pskip2 = Rskip2 = Fskip2 = 586 (mi(rc), mi(rf)) be the counts in the reference, then: P = �i=1 wi · (δ · mi(hi) + (1 − δ) · mi(hf)) δ· |hc |+(1 − δ)· |hf | �i=1 wi · (δ · mi(ri) + (1 − δ) · mi(rf)) R = δ· |rc |+(1 − δ)· |rf | To account for word order difference, the fragmentation</context>
</contexts>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>Michael Denkowski, Alon Lavie. Extending the METEOR machine translation evaluation metric to the phrase level. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. ACL, 2010: 250-253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
</authors>
<title>Alon Lavie. Meteor Universal: Language Specific Translation /Evaluation for Any Target Language translation.</title>
<date>2014</date>
<booktitle>Proceedings of the EACL 2014 Workshop on Statistical Machine Translation,</booktitle>
<marker>Denkowski, 2014</marker>
<rawString>Michael Denkowski, Alon Lavie. Meteor Universal: Language Specific Translation /Evaluation for Any Target Language translation. Proceedings of the EACL 2014 Workshop on Statistical Machine Translation, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad T Pilehvar</author>
<author>David Jurgens</author>
<author>Roberto Navigli</author>
</authors>
<title>Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</title>
<date>2013</date>
<booktitle>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,ACL,</booktitle>
<pages>1341--1351</pages>
<marker>Pilehvar, Jurgens, Navigli, 2013</marker>
<rawString>Mohammad T Pilehvar, David Jurgens, Roberto Navigli. Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,ACL, 2013: 1341-1351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics</title>
<date>2010</date>
<journal>Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>141--188</pages>
<contexts>
<context position="2312" citStr="Turney and Pantel, 2010" startWordPosition="336" endWordPosition="339">graph contains This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ Baobao Chang Key Laboratory of Computational Linguistics, Ministry of Education Institute of Computational Linguistics Peking University, China chbb@pku.edu.cn 3.67 times the words of a sentence on average in the training set. Semantic similarity on different levels, for example, on word level (Mikolov et al., 2013), sentences level (B¨ar et al., 2012), document level (Turney and Pantel, 2010), have been well studied, yet methods on one level can hardly be applied to a different level, let alone be applied for the crosslevel tasks. The work of Pilehvar et al.(2013) was an exception. They proposed a unified method for semantic comparison at multi-levels all the way from comparing word senses to comparing text documents Our work is inspired by automatic machine translation(MT) evaluation, in which different metrics are designed to compare the adequacy and fluency of a MT system’s output, called hypothesis, against a gold standard translation, called reference. As MT evaluation metric</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics Artificial Intelligence Research, 2010. 37(1): 141-188</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
</authors>
<title>Alon Lavie. METEOR: an automatic metric for MT Evaluation with improved correlation with human judgements.</title>
<date>2005</date>
<booktitle>Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.,</booktitle>
<pages>65--72</pages>
<marker>Banerjee, 2005</marker>
<rawString>Satanjeev Banerjee, Alon Lavie. METEOR: an automatic metric for MT Evaluation with improved correlation with human judgements. Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization., 2005: 65-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<note>arXiv preprint arXiv:1301.3781,</note>
<contexts>
<context position="2233" citStr="Mikolov et al., 2013" startWordPosition="323" endWordPosition="326">culty of this task is the length disparity between the compared pair: a paragraph contains This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ Baobao Chang Key Laboratory of Computational Linguistics, Ministry of Education Institute of Computational Linguistics Peking University, China chbb@pku.edu.cn 3.67 times the words of a sentence on average in the training set. Semantic similarity on different levels, for example, on word level (Mikolov et al., 2013), sentences level (B¨ar et al., 2012), document level (Turney and Pantel, 2010), have been well studied, yet methods on one level can hardly be applied to a different level, let alone be applied for the crosslevel tasks. The work of Pilehvar et al.(2013) was an exception. They proposed a unified method for semantic comparison at multi-levels all the way from comparing word senses to comparing text documents Our work is inspired by automatic machine translation(MT) evaluation, in which different metrics are designed to compare the adequacy and fluency of a MT system’s output, called hypothesis,</context>
</contexts>
<marker>Mikolov, Chen, Corrado, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, et al. Efficient estimation of word representations in vector space. 2013. arXiv preprint arXiv:1301.3781,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>