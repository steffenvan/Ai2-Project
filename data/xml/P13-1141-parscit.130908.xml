<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000496">
<title confidence="0.58443">
SenseSpotting: Never let your parallel data tie you to an old domain
</title>
<author confidence="0.9155475">
Marine Carpuat&apos;, Hal Daum´e III2, Katharine Henry3,
Ann Irvine4, Jagadeesh Jagarlamudi5, Rachel Rudinger6
</author>
<affiliation confidence="0.924290333333333">
&apos; National Research Council Canada, marine.carpuat@nrc.gc.ca
2 CLIP, University of Maryland, me@hal3.name
3 CS, University of Chicago, kehenry@uchicago.edu
</affiliation>
<sectionHeader confidence="0.95302075" genericHeader="abstract">
4 CLSP, Johns Hopkins University, anni@jhu.edu
5 IBM T.J. Watson Research Center, jags@us.ibm.com
6 CLSP, Johns Hopkins University, rachel.rudinger@aya.yale.edu
Abstract
</sectionHeader>
<bodyText confidence="0.999975571428571">
Words often gain new senses in new do-
mains. Being able to automatically iden-
tify, from a corpus of monolingual text,
which word tokens are being used in a pre-
viously unseen sense has applications to
machine translation and other tasks sensi-
tive to lexical semantics. We define a task,
SENSESPOTTING, in which we build sys-
tems to spot tokens that have new senses
in new domain text. Instead of difficult
and expensive annotation, we build a gold-
standard by leveraging cheaply available
parallel corpora, targeting our approach to
the problem of domain adaptation for ma-
chine translation. Our system is able to
achieve F-measures of as much as 80%,
when applied to word types it has never
seen before. Our approach is based on
a large set of novel features that capture
varied aspects of how words change when
used in new domains.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999634666666667">
As Magnini et al. (2002) observed, the domain of
the text that a word occurs in is a useful signal for
performing word sense disambiguation (e.g. in a
text about finance, bank is likely to refer to a finan-
cial institution while in a text about geography, it
is likely to refer to a river bank). However, in the
classic WSD task, ambiguous word types and a set
of possible senses are known in advance. In this
work, we focus on the setting where we observe
texts in two different domains and want to iden-
tify words in the second text that have a sense that
did not appear in the first text, without any lexical
knowledge in the new domain.
To illustrate the task, consider the French noun
rapport. In the parliament domain, this means
</bodyText>
<table confidence="0.975182875">
´etat rapport r´egime
Govt. geo. state report (political) regime
Medical state (mind) report diet
geo. state ratio (political) regime
Science geo. state ratio (political) regime
report diet
Movies geo. state report (political) regime
diet
</table>
<tableCaption confidence="0.9903315">
Table 1: Examples of French words and their most
frequent senses (translations) in four domains.
</tableCaption>
<bodyText confidence="0.99977975862069">
(and is translated as) “report.” However, in mov-
ing to a medical or scientific domain, the word
gains a new sense: “ratio”, which simply does not
exist in the parliament domain. In a science do-
main, the “report” sense exists, but it is dominated
about 12:1 by “ratio.” In a medical domain, the
“report” sense remains dominant (about 2:1), but
the new “ratio” sense appears frequently.
In this paper we define a new task that we call
SENSESPOTTING. The goal of this task is to iden-
tify words in a new domain monolingual text that
appeared in old domain text but which have a
new, previously unseen sense1. We operate un-
der the framework of phrase sense disambiguation
(Carpuat and Wu, 2007), in which we take au-
tomatically align parallel data in an old domain
to generate an initial old-domain sense inventory.
This sense inventory provides the set of “known”
word senses in the form of phrasal translations.
Concrete examples are shown in Table 1. One of
our key contributions is the development of a rich
set of features based on monolingual text that are
indicative of new word senses.
This work is driven by an application need.
When machine translation (MT) systems are ap-
plied in a new domain, many errors are a result
of: (1) previously unseen (OOV) source language
words, or (2) source language words that appear
with a new sense and which require new transla-
</bodyText>
<footnote confidence="0.975427">
1All features, code, data and raw results are at: github.
com/hal3/IntrinsicPSDEvaluation
</footnote>
<page confidence="0.848792">
1435
</page>
<note confidence="0.924536">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.993164952380952">
tions2 (Carpuat et al., 2012). Given monolingual
text in a new domain, OOVs are easy to identify,
and their translations can be acquired using dictio-
nary extraction techniques (Rapp, 1995; Fung and
Yee, 1998; Schafer and Yarowsky, 2002; Schafer,
2006; Haghighi et al., 2008; Mausam et al., 2010;
Daum´e III and Jagarlamudi, 2011), or active learn-
ing (Bloodgood and Callison-Burch, 2010). How-
ever, previously seen (even frequent) words which
require new translations are harder to spot.
Because our motivation is translation, one sig-
nificant point of departure between our work and
prior related work (§3) is that we focus on word
tokens. That is, we are not interested only in the
question of “has this known word (type) gained
a new sense?”, but the much more specific ques-
tion of “is this particular (token) occurrence of this
known word being used in a new sense?” Note
that for both the dictionary mining setting and the
active learning setting, it is important to consider
words in context when acquiring their translations.
</bodyText>
<sectionHeader confidence="0.995285" genericHeader="method">
2 Task Definition
</sectionHeader>
<bodyText confidence="0.980952542857143">
Our task is defined by two data components. De-
tails about their creation are in §5. First, we need
an old-domain sense dictionary, extracted from
French-English parallel text (in our case, parlia-
mentary proceedings). Next, we need new-domain
monolingual French text (we use medical text, sci-
entific text and movie subtitle text). Given these
two inputs, our challenge is to find tokens in the
new-domain text that are being used in a new sense
(w.r.t. the old-domain dictionary).
We assume that we have access to a small
amount of new domain parallel “tuning data.”
From this data, we can extract a small new do-
main dictionary (§5). By comparing this new do-
main dictionary to the old domain dictionary, we
can identify which words have gained new senses.
In this way, we turn the SENSESPOTTING problem
into a supervised binary classification problem: an
example is a French word in context (in the new
domain monolingual text) and its label is positive
when it is being used in a sense that did not ex-
ist in the old domain dictionary. In this task, the
classifier is always making predictions on words
2Sense shifts do not always demand new translations;
some ambiguities are preserved across languages. E.g.,
fenˆetre can refer to a window of a building or on a moni-
tor, but translates as “window” either way. Our experiments
use bilingual data with an eye towards improving MT perfor-
mance: we focus on words that demand new translations.
outside this tuning data on word types it has never
seen before! From an applied perspective, the as-
sumption of a small amount of parallel data in the
new domain is reasonable: if we want an MT sys-
tem for a new domain, we will likely have some
data for system tuning and evaluation.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999961976744186">
While word senses have been studied extensively
in lexical semantics, research has focused on word
sense disambiguation, the task of disambiguating
words in context given a predefined sense inven-
tory (e.g., Agirre and Edmonds (2006)), and word
sense induction, the task of learning sense inven-
tories from text (e.g., Agirre and Soroa (2007)). In
contrast, detecting novel senses has not received as
much attention, and is typically addressed within
word sense induction, rather than as a distinct
SENSESPOTTING task. Novel sense detection
has been mostly motivated by the study of lan-
guage change over time. Most approaches model
changes in co-occurrence patterns for word types
when moving between corpora of old and modern
language (Sagi et al., 2009; Cook and Stevenson,
2010; Gulordava and Baroni, 2011).
Since these type-based models do not capture
polysemy in the new language, there have been a
few attempts at detecting new senses at the token-
level as in SENSESPOTTING. Lau et al. (2012)
leverage a common framework to address sense
induction and disambiguation based on topic mod-
els (Blei et al., 2003). Sense induction is framed
as learning topic distributions for a word type,
while disambiguation consists of assigning topics
to word tokens. This model can interestingly be
used to detect newly coined senses, which might
co-exist with old senses in recent language. Bam-
man and Crane (2011) use parallel Latin-English
data to learn to disambiguate Latin words into En-
glish senses. New English translations are used as
evidence that Latin words have shifted sense. In
contrast, the SENSESPOTTING task consists of de-
tecting when senses are unknown in parallel data.
Such novel sense induction methods require
manually annotated datasets for the purpose of
evaluation. This is an expensive process and there-
fore evaluation is typically conducted on a very
small scale. In contrast, our SENSESPOTTING task
leverages automatically word-aligned parallel cor-
pora as a source of annotation for supervision dur-
ing training and evaluation.
</bodyText>
<page confidence="0.986838">
1436
</page>
<bodyText confidence="0.99993384">
The impact of domain on novel senses has also
received some attention. Most approaches oper-
ate at the type-level, thus capturing changes in the
most frequent sense of a word when shifting do-
mains (McCarthy et al., 2004; McCarthy et al.,
2007; Erk, 2006; Chan and Ng, 2007). Chan and
Ng (2007) notably show that detecting changes in
predominant sense as modeled by domain sense
priors can improve sense disambiguation, even af-
ter performing adaptation using active learning.
Finally, SENSESPOTTING has not been ad-
dressed directly in MT. There has been much inter-
est in translation mining from parallel or compara-
ble corpora for unknown words, where it is easy to
identify which words need translations. In con-
trast, SENSESPOTTING detects when words have
new senses and, thus, frequently a new translation.
Work on active learning for machine translation
has focused on collecting translations for longer
unknown segments (e.g., Bloodgood and Callison-
Burch (2010)). There has been some interest in
detecting which phrases that are hard to translate
for a given system (Mohit and Hwa, 2007), but dif-
ficulties can arise for many reasons: SENSESPOT-
TING focuses on a single problem.
</bodyText>
<sectionHeader confidence="0.985202" genericHeader="method">
4 New Sense Indicators
</sectionHeader>
<bodyText confidence="0.999995933333333">
We define features over both word types and word
tokens. In our classification setting, each instance
consists of a French word token in context. Our
word type features ignore this context and rely on
statistics computed over our entire new domain
corpus. In contrast, our word token features con-
sider the context of the particular instance of the
word. If it were the case that only one sense ex-
isted for all word tokens of a particular type within
a single domain, we would expect our word type
features to be able to spot new senses without the
help of the word token features. However, in fact,
even within a single domain, we find that often a
word type is used with several senses, suggesting
that word token features may also be useful.
</bodyText>
<subsectionHeader confidence="0.978333">
4.1 Type-level Features
</subsectionHeader>
<bodyText confidence="0.969989105263158">
Lexical Item Frequency Features A very ba-
sic property of the new domain that we hope to
capture is that word frequencies change, and such
changes might be indicative of a domain shift. As
such, we compute unigram log probabilities (via
smoothed relative frequencies) of each word un-
der consideration in the old domain and the new
domain. We then add as features these two log
probabilities as well as their difference. These are
our Type:RelFreq features.
N-gram Probability Features The goal of the
Type:NgramProb feature is to capture the fact
that “unusual contexts” might imply new senses.
To capture this, we can look at the log probability
of the word under consideration given its N-gram
context, both according to an old-domain language
model (call this `old
ng ) and a new-domain language
model (call this `new
ng ). However, we do not sim-
ply want to capture unusual words, but words that
are unlikely in context, so we also need to look at
the respective unigram log probabilities: `old
ug and
`new
ug . From these four values, we compute corpus-
level (and therefore type-based) statistics of the
new domain n-gram log probability (�new
ng , the dif-
ference between the n-gram probabilities in each
domain (�new
ng − `old
ng ), the difference between the
n-gram and unigram probabilities in the new do-
main (�new
ng − `new
ug ), and finally the combined differ-
ence: `new
</bodyText>
<equation confidence="0.598698333333333">
ng − `new
ug + `old
ug − `old
</equation>
<bodyText confidence="0.99708215625">
ng ). For each of these
four values, we compute the following type-based
statistics over the monolingual text: mean, stan-
dard deviation, minimum value, maximum value
and sum. We use trigram models.
Topic Model Feature The intuition behind the
topic model feature is that if a word’s distribu-
tion over topics changes when moving into a new
domain, it is likely to also gain a new sense.
For example, suppose that in our old domain, the
French word enceinte is only used with the sense
“wall,” but in our new domain, enceinte may have
senses corresponding to either “wall” or to “preg-
nant.” We would expect to see this reflected in
enceinte’s distribution over topics: the topic that
places relatively high probabilities on words such
as “b´eb´e” (English “baby”) and enfant (English
“child”) will also place a high probability on en-
ceinte when trained on new domain data. In the
old domain, however, we would not expect a sim-
ilar topic (if it exists) to give a high probabil-
ity to enceinte. Based on this intuition, for all
words w, where To and Tn are the set of old
and new topics and Po and Pn are the old and
new distributions defined over them, respectively,
and cos is the cosine similarity between a pair
of topics, we define the feature Type:TopicSim:
E
tcTn,t�cTo Pn(t|w)Po(t&apos;|w) cos(t, t&apos;). For a
word w, the feature value will be high if, for
each new domain topic t that places high proba-
bility on w, there is an old domain topic t&apos; that
</bodyText>
<page confidence="0.966822">
1437
</page>
<bodyText confidence="0.98342164">
is similar to t and also places a high probabil-
ity on w. Conversely, if no such topic exists, the
score will be low, indicating the word has gained
a new sense. We use the online LDA (Blei et
al., 2003; Hoffman et al., 2010), implemented
in http://hunch.net/˜vw/ to compute topics on
the two domains separately. We use 100 topics.
Context Feature It is expected that words acquir-
ing new senses will tend to neighbor different sets
of words (e.g. different arguments, prepositions,
parts of speech, etc.). Thus, we define an addi-
tional type level feature to be the ratio of the num-
ber of new domain n-grams (up to length three)
that contain word w and which do not appear in
the old domain to the total number of new domain
n-grams containing w. With Nw indicating the set
of n-grams in the new domain which contain w,
Ow indicating the set of n-grams in the old domain
which contain w, and |Nw − Ow |indicating the
n-grams which contain w and appear in the new
but not the old domain, we define Type:Contextas
|Nw−Ow|. We do not count n-grams containing
|Nw |
OOVs, as they may simply be instances of apply-
ing the same sense of a word to a new argument
</bodyText>
<subsectionHeader confidence="0.972885">
4.2 Token-level Features
</subsectionHeader>
<bodyText confidence="0.996793128571429">
N-gram Probability Features Akin to the N-
gram probability features at the type level (namely,
Token:NgramProb), we compute the same val-
ues at the token level (new/old domain and un-
igram/trigram). Instead of computing statistics
over the entire monolingual corpus, we use the in-
stantaneous values of these features for the token
under consideration. The six features we construct
are: unigram (and trigram) log probabilities in the
old domain, the new domain, and their difference.
Context Features Following the type-level n-
gram feature, we define features for a particular
word token based on its n-gram context. For token
wi, in position i in a given sentence, we consider
its context words in a five word window: wi−2,
wi−1, wi+1, and wi+2. For each of the four con-
textual words in positions p = {−2, −1, 1, 2},
relative to i, we define the following feature, To-
ken:CtxCnt: log(cw,) where cw, is the number
of times word wp appeared in position p relative
to wi in the OLD-domain data. We also define a
single feature which is the percent of the four con-
textual words which had been seen in the OLD-
domain data, Token:Ctx%.
Token-Level PSD Features These features aim
to capture generalized characteristics of a context.
Towards this end, first, we pose the problem as a
phrase sense disambiguation (PSD) problem over
the known sense inventory. Given a source word in
a context, we train a classifier to predict the most
likely target translation. The ground truth labels
(target translation for a given source word) for this
classifier are generated from the phrase table of
the old domain data. We use the same set of fea-
tures as in Carpuat and Wu (2007). Second, given
a source word s, we use this classifier to com-
pute the probability distribution of target transla-
tions (p(t|s)). Subsequently, we use this prob-
ability distribution to define new features for the
SENSESPOTTING task. The idea is that, if a word
is used in one of the known senses then its con-
text must have been seen previously and hence we
hope that the PSD classifier outputs a spiky dis-
tribution. On the other hand, if the word takes a
new sense then hopefully it is used in an unseen
context resulting in the PSD classifier outputting
an uniform distribution. Based on this intuition,
we add the following features: MaxProb is the
maximum probability of any target translation:
maxt p(t|s). Entropy is the entropy of the proba-
bility distribution: − Et p(t|s) log p(t|s). Spread
is the difference between maximum and mini-
mum probabilities of the probability distribution:
( maxt p(t|s) − mint p(t|s)). Confusion is the
uncertainty in the most likely prediction given the
source token: mediantp(t|s) The use of median in
maxt p(t  |s )
the numerator rather than the second best is mo-
tivated by the observation that, in most cases, top
ranked translations are of the same sense but differ
in morphology.
We train the PSD classifier in two modes:
1) a single global classifier that predicts the
target translation given any source word; 2) a
local classifier for each source word. When
training the global PSD classifier, we include
some lexical features that depend on the source
word. For both modes, we use real valued
and binned features giving rise to four families
of features Token:G-PSD, Token:G-PSDBin,
</bodyText>
<sectionHeader confidence="0.559343" genericHeader="evaluation">
Token:L-PSD and Token:L-PSDBin.
</sectionHeader>
<bodyText confidence="0.996275714285714">
Prior vs. Posterior PSD Features When the
PSD classifier is trained in the second mode, i.e.
one classifier per word type, we can define ad-
ditional features based on the prior (with out the
word context) and posterior (given the word’s
context) probability distributions output by the
classifier, i.e. pprior(t|s) and ppost.(t|s) respec-
</bodyText>
<page confidence="0.914298">
1438
</page>
<table confidence="0.999904666666667">
Domain Sentences Lang Tokens Types
Hansard 8,107,356 fr 161,695,309 191,501
en 144,490,268 186,827
EMEA 472,231 fr 6,544,093 34,624
en 5,904,296 29,663
Science 139,215 fr 4,292,620 117,669
en 3,602,799 114,217
Subs 19,239,980 fr 154,952,432 361,584
en 174,430,406 293,249
</table>
<tableCaption confidence="0.998761">
Table 2: Basic characteristics of the parallel data.
</tableCaption>
<bodyText confidence="0.999948052631579">
tively. We compute the following set of fea-
tures referred to as Token:PSDRatio: SameMax
checks if both the prior and posterior distri-
butions have the same translation as the most
likely translation. SameMin is same as the
above feature but check if the least likely trans-
lation is same. X-OR MinMax is the exclusive-
OR of SameMax and SameMin features. KL
is the KL-divergence between the two distri-
butions. Since KL-divergence is asymmetric,
we use KL(ppriorllppost.) and KL(ppost.||pprior).
MaxNorm is the ratio of maximum probabilities
in prior and posterior distributions. SpreadNorm
is the ratio of spread of the prior and posterior dis-
tributions, where spared is the difference between
maximum and minimum probabilities of the dis-
tribution as defined earlier. ConfusionNorm is the
ratio of confusion of the prior and posterior distri-
butions, where confusion is defined as earlier.
</bodyText>
<sectionHeader confidence="0.991723" genericHeader="conclusions">
5 Data and Gold Standard
</sectionHeader>
<bodyText confidence="0.999948380952381">
The first component of our task is a parallel cor-
pus of old domain data, for which we use the
French-English Hansard parliamentary proceed-
ings (http://www.parl.gc.ca). From this, we
extract an old domain sense dictionary, using the
Moses MT framework (Koehn et al., 2007). This
defines our old domain sense dictionary. For new
domains, we use three sources: (1) the EMEA
medical corpus (Tiedemann, 2009), (2) a corpus of
scientific abstracts, and (3) a corpus of translated
movie subtitles (Tiedemann, 2009). Basic statis-
tics are shown in Table 2. In all parallel corpora,
we normalize the English for American spelling.
To create the gold standard truth, we followed
a lexical sample apparoach and collected a set
of 300 “representative types” that are interest-
ing to evaluate on, because they have multiple
senses within a single domain or whose senses
are likely to change in a new domain. We used
a semi-automatic approach to identify represen-
tative types. We first used the phrase table from
</bodyText>
<table confidence="0.999609">
Sents Parallel Repr. Repr. % New
fr-tok Types Tokens Sense
EMEA 24k 270k 399 35,266 52.0%
Science 22k 681k 425 8,355 24.3%
Subs 36k 247k 388 22,598 43.4%
</table>
<tableCaption confidence="0.99835">
Table 3: Statistics about representative words and
</tableCaption>
<bodyText confidence="0.998860777777778">
the size of the development sets. The columns
show: the total amount of parallel development
data (# of sentences and tokens in French), # of
representative types that appear in this corpus, the
corresponding # of tokens, and the percentage of
these tokens that correspond to “new senses.”
the Moses output to rank phrases in each domain
using TF-IDF scores with Okapi BM25 weight-
ing. For each of the three new domains (EMEA,
Science, and Subs), we found the intersection of
phrases between the old and the new domain. We
then looked at the different translations that each
had in the phrase table and a French speaker se-
lected a subset that have multiple senses.3
In practice, we limited our set almost entirely
to source words, and included only a single multi-
word phrase, vue des enfants, which usually trans-
lates as “for children” in the old domain but al-
most always translates as “sight of children” in
the EMEA domain (as in “... should be kept out
of the sight of children”). Nothing in the way we
have defined, approached, or evaluated the SENS-
ESPOTTING task is dependent on the use of rep-
resentative words instead of longer representative
phrases. We chose to consider mostly source lan-
guage words for simplicity and because it was eas-
ier to identify good candidate words.
In addition to the manually chosen words, we
also identified words where the translation with
the highest lexical weight varied in different do-
mains, with the intuition being that are the words
that are likely to have acquired a new sense. The
top 200 words from this were added to the man-
ually selected representative words to form a list
of 450. Table 3 shows some statistics about these
words across our three test domains.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="acknowledgments">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997189">
6.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999984333333333">
Our goal in evaluation is to be able to under-
stand what our approach is realistically capa-
ble of. One challenge is that the distribution
</bodyText>
<footnote confidence="0.826633333333333">
3In order to create the evaluation data, we used both sides
of the full parallel text; we do not use the English side of the
parallel data for actually building systems.
</footnote>
<page confidence="0.991825">
1439
</page>
<bodyText confidence="0.999990659574468">
of representative words is highly skewed.4 We
present results in terms of area under the ROC
curve (AUC),5 micro-averaged precision/recall/f-
measure and macro-averaged precision/recall/f-
measure. For macro-averaging, we compute a sin-
gle confusion matrix over all the test data and
determining P/R/F from that matrix. For micro-
averaging, we compute a separate confusion ma-
trix for each word type on the French side, com-
pute P/R/F for each of these separately, and then
average the results. (Thus, micro-F is not a
function of micro-P and micro-R.) The AUC and
macro-averaged scores give a sense of how well
the system is doing on a type-level basis (es-
sentially weighted by type frequency), while the
micro-averaged scores give a sense as to how well
the system is doing on individual types, not taking
into account their frequencies.
For most of our results, we present standard
deviations to help assess significance (±2u is
roughly a 90% confidence interval). For our re-
sults, in which we use new-domain training data,
we compute these results via 16-fold cross valida-
tion. The folds are split across types so the sys-
tem is never being tested on a word type that it has
seen before. We do this because it more closely re-
sembles our application goals. We do 16-fold for
convenience, because we divide the data into bi-
nary folds recursively (thus having a power-of-two
is easier), with an attempt to roughly balance the
size of the training sets in each fold (this is tricky
because of the skewed nature of the data). This en-
tire 16-fold cross-validation procedure is repeated
10 times and averages and standard deviations are
over the 160 replicates.
We evaluate performance using our type-level
features only, TYPEONLY, our token-level fea-
tures only, TOKENONLY, and using both our type
and our token level features, ALLFEATURES.
We compare our results with two baselines:
RANDOM and CONSTANT. RANDOM predicts
new-sense or not-new-sense randomly and with
equal probability. CONSTANT always predicts
new-sense, achieving 100% recall and a macro-
level precision that is equal to the percent of repre-
sentative words which do have a new sense, mod-
ulo cross-validation splits (see Table 3). Addi-
</bodyText>
<footnote confidence="0.9121548">
4The most frequent (voie) appears 3881 times; there are
60 singleton words on average across the three new domains.
5AUC is the probability that the classifier will assign a
higher score to a randomly chosen positive example than to a
randomly chosen negative example (Wikipedia, 2013).
</footnote>
<bodyText confidence="0.999439833333333">
tionally, we compare our results with a type-level
oracle, TYPEORACLE. For all tokens of a given
word type, the oracle predicts the majority label
(new-sense or not-new-sense) for that word type.
These results correspond to an upper bound for the
TYPEONLY experiments.
</bodyText>
<subsectionHeader confidence="0.998943">
6.2 Classification Setup
</subsectionHeader>
<bodyText confidence="0.999969833333333">
For all experiments, we use a linear classifier
trained by stochastic gradient descent to optimize
logistic loss. We also did some initial experi-
ments on development data using boosted deci-
sion trees instead and other loss functions (hinge
loss, squared loss), but they never performed as
well. In all cases, we perform 20 passes over
the training data, using development data to per-
form early stopping (considered at the end of each
pass). We also use development data to tune a
regularizer (either Ei or E2) and its regularization
weight.6 Finally, all real valued features are au-
tomatically bucketed into 10 consecutive buckets,
each with (approximately) the same number of
elements. Each learner uses a small amount of
development data to tune a threshold on scores
for predicting new-sense or not-a-new-sense, us-
ing macro F-measure as an objective.
</bodyText>
<subsectionHeader confidence="0.995238">
6.3 Result Summary
</subsectionHeader>
<bodyText confidence="0.999775523809524">
Table 4 shows our results on the SENSESPOT-
TING task. Classifiers based on the features
that we defined outperform both baselines in all
macro-level evaluations for the SENSESPOTTING
task. Using AUC as an evaluation metric, the
TOKENONLY, TYPEONLY, and ALLFEATURES
models performed best on EMEA, Science, and
Subtitles data, respectively. Our token-level fea-
tures perform particularly poorly on the Science
and Subtitles data. Although the model trained on
only those features achieves reasonable precision
(72.59 and 70.00 on Science and Subs, respec-
tively), its recall is very low (20.41 and 35.15), in-
dicating that the model classifies many new-sense
words as not-new-sense. Most of our token-level
features capture the intuition that when a word to-
ken appears in new or infrequent contexts, it is
likely to have gained a new sense. Our results indi-
cate that this intuition was more fruitful for EMEA
than for Science or Subs.
In contrast, the type-only features (TYPEONLY)
</bodyText>
<footnote confidence="0.987895666666667">
6We use http://hunch.net/˜vw/ version 7.1.2,
and run it with the following arguments that affect learning
behavior: --exact adaptive norm --power t 0.5
</footnote>
<page confidence="0.939112">
1440
</page>
<table confidence="0.999964608695652">
AUC Macro F P Micro F
P R R
EMEA
RANDOM 50.34 f 0.60 51.24 f 0.59 50.09 f 1.18 50.19 f 0.75 47.04 f 0.60 56.07 f 1.99 37.27 f 0.91
CONSTANT 50.00 f 0.00 50.99 f 0.00 100.0 f 0.00 67.09 f 0.00 45.80 f 0.00 100.0 f 0.00 52.30 f 0.00
TYPEONLY 55.91 f 1.13 69.76 f 3.45 43.13 f 1.42 41.61 f 1.07 77.92 f 2.04 50.12 f 2.35 31.26 f 0.63
TYPEORACLE 88.73 f 0.00 87.32 f 0.00 86.76 f 0.00 87.04 f 0.00 90.01 f 0.00 67.46 f 0.00 59.39 f 0.00
TOKENONLY 78.80 f 0.52 69.83 f 1.59 75.58 f 2.61 69.40 f 1.92 59.03 f 1.70 62.53 f 1.66 43.39 f 0.94
ALLFEATURES 79.60 f 1.20 68.11 f 1.19 79.84 f 2.27 71.64 f 1.83 55.28 f 1.11 71.50 f 1.62 46.83 f 0.62
Science
RANDOM 50.18 f 0.78 24.48 f 0.57 50.32 f 1.33 32.92 f 0.79 46.99 f 0.51 60.32 f 1.06 34.72 f 1.03
CONSTANT 50.00 f 0.00 24.34 f 0.00 100.0 f 0.00 39.15 f 0.00 44.39 f 0.00 100.0 f 0.00 50.44 f 0.00
TYPEONLY 77.06 f 1.23 66.07 f 2.80 36.28 f 4.10 34.50 f 4.06 84.97 f 0.82 36.81 f 2.33 24.22 f 1.70
TYPEORACLE 88.76 f 0.00 78.43 f 0.00 69.29 f 0.00 73.54 f 0.00 84.19 f 0.00 67.41 f 0.00 52.67 f 0.00
TOKENONLY 66.62 f 0.47 60.50 f 3.11 28.05 f 2.06 30.81 f 2.75 76.21 f 1.78 36.57 f 2.23 24.68 f 1.36
ALLFEATURES 73.91 f 0.66 50.59 f 2.08 60.60 f 2.04 47.54 f 1.52 66.72 f 1.19 62.30 f 1.36 40.22 f 1.03
Subs
RANDOM 50.26 f 0.69 42.47 f 0.60 50.17 f 0.84 45.68 f 0.68 52.18 f 1.32 54.63 f 2.01 39.87 f 2.10
CONSTANT 50.00 f 0.00 42.51 f 0.00 100.0 f 0.00 59.37 f 0.00 50.63 f 0.00 100.0 f 0.00 58.67 f 0.00
TYPEONLY 67.16 f 0.73 76.41 f 1.51 31.91 f 3.15 36.37 f 2.58 90.03 f 0.61 34.78 f 1.12 26.20 f 0.61
TYPEORACLE 81.35 f 0.00 83.12 f 0.00 70.23 f 0.00 76.12 f 0.00 90.62 f 0.00 52.37 f 0.00 44.43 f 0.00
TOKENONLY 63.30 f 0.99 63.17 f 2.31 45.38 f 2.07 43.30 f 1.29 76.38 f 1.68 49.70 f 1.76 37.92 f 1.20
ALLFEATURES 69.26 f 0.60 63.48 f 1.77 56.22 f 2.66 52.78 f 1.96 67.55 f 0.83 62.18 f 1.45 43.85 f 0.90
</table>
<tableCaption confidence="0.742166">
Table 4: Complete SENSESPOTTING results for all domains. The scores are from cross-validation on
a single domain; in all cases, higher is better. Two standard deviations of performance over the cross-
validation are shown in small type. For all domains and metrics, the highest (not necessarily statistically
significant) non-oracle results are bolded.
</tableCaption>
<bodyText confidence="0.999721611111111">
are relatively weak for predicting new senses on
EMEA data but stronger on Subs (TYPEONLY
AUC performance is higher than both baselines)
and even stronger on Science data (TYPEONLY
AUC and f-measure performance is higher
than both baselines as well as the ALLFEA-
TURESmodel). In our experience with the three
datasets, we know that the Science data, which
contains abstracts from a wide variety of scientific
disciplines, is the most diverse, followed by the
Subs data, and then EMEA, which mostly consists
of text from drug labels and tends to be quite repet-
itive. Thus, it makes sense that type-level features
would be the most informative for the least homo-
geneous dataset. Representative words in scien-
tific text are likely to appear in variety of contexts,
while in the EMEA data they may only appear in
a few, making it easier to contrast them with the
distributions observed in the old domain data.
For all domains, in micro-level evaluation, our
models fail to outperform the CONSTANT base-
line. Recall that the micro-level evaluation com-
putes precision, recall, and f-measure for all word
tokens of a given word type and then averages
across word types. We observe that words that are
less frequent in both the old and the new domains
are more likely to have a new sense than more fre-
quent words, which causes the CONSTANT base-
line to perform reasonably well. In contrast, it is
more difficult for our models to make good pre-
dictions for less frequent words. A low frequency
in the new domain makes type level features (esti-
mated over only a few instances) noisy and unreli-
able. Similarly, a low frequency in the old domain
makes the our token level features, which all con-
trast with old domain instances of the word type.
</bodyText>
<subsectionHeader confidence="0.997266">
6.4 Feature Ablation
</subsectionHeader>
<bodyText confidence="0.999990947368421">
In the previous section, we observed that (with one
exception) both Type-level and Token-level fea-
tures are useful in our task (in some cases, essen-
tial). In this section, we look at finer-grained fea-
ture distinctions through a process of feature ab-
lation. In this setting, we begin with all features
in a model and remove one feature at a time, al-
ways removing the feature that hurts performance
least. For these experiments, we determine which
feature to remove using AUC. Note that we’re ac-
tually able to beat (by 2-4 points AUC) the scores
from Table 4 by removing features!
The results here are somewhat mixed. In EMEA
and Science, one can actually get by (accord-
ing to AUC) with very few features: just two
(Type:NgramProband Type:Context) are suffi-
cient to achieve optimal AUC scores. To get
higher Macro-F scores requires nearly all the fea-
tures, though this is partially due to the choice of
</bodyText>
<page confidence="0.964033">
1441
</page>
<table confidence="0.999858952380953">
EMEA AUC MacF
ALLFEATURES 79.60 71.64
–Token:L-PSDBin 77.09 70.50
–Type:RelFreq 78.43 72.19
–Token:G-PSD 79.66 72.11
–Type:Context 79.66 72.45
–Token:Ctx% 78.91 73.37
–Type:TopicSim 78.05 71.33
–Token:CtxCnt 76.90 71.72
–Token:L-PSD 76.03 73.35
–Type:NgramProb 73.32 69.54
–Token:G-PSDBin 74.41 69.76
–Token:NgramProb 69.78 68.89
–Token:PSDRatio 48.38 3.45
Science AUC MacF
ALLFEATURES 73.91 47.54
–Token:L-PSDBin 76.26 53.69
–Token:G-PSD 77.04 53.56
–Token:G-PSDBin 77.44 54.54
–Token:L-PSD 77.85 56.05
–Token:PSDRatio 77.92 57.34
–Token:CtxCnt 77.85 54.42
–Type:Context 78.17 55.45
–Token:Ctx% 78.06 55.04
–Type:TopicSim 77.83 54.57
–Token:NgramProb 76.98 51.02
–Type:RelFreq 74.25 49.57
–Type:NgramProb 50.00 0.00
Subs AUC MacF
ALLFEATURES 69.26 52.78
–Type:NgramProb 69.13 53.33
–Token:G-PSDBin 70.23 54.72
–Token:CtxCnt 71.23 58.35
–Token:L-PSDBin 72.07 57.85
–Token:G-PSD 72.17 57.33
–Type:TopicSim 72.31 58.41
–Token:Ctx% 72.17 56.17
–Token:NgramProb 71.35 59.26
–Token:PSDRatio 70.33 46.88
–Token:L-PSD 69.05 53.31
–Type:RelFreq 65.25 48.22
–Type:Context 50.00 0.00
</table>
<tableCaption confidence="0.869908">
Table 5: Feature ablation results for all three corpora. Selection criteria is AUC, but Macro-F is presented
for completeness. Feature selection is run independently on each of the three datasets. The features
toward the bottom were the first selected.
</tableCaption>
<table confidence="0.99988925">
AUC Macro-F Micro-F
EMEA
TYPEONLY 71.43 ± 0.94 52.62 ± 3.41 38.67 ± 1.35
TOKENONLY 73.75 ± 1.11 67.77 ± 4.18 45.49 ± 3.96
ALLFEATURES 72.19 ± 4.07 67.26 ± 7.88 49.29 ± 3.55
XV-ALLFEATURES 79.60 ± 1.20 71.64 ± 1.83 46.83 ± 0.62
Science
TYPEONLY 75.19 ± 0.89 51.53 ± 2.55 37.14 ± 4.41
TOKENONLY 71.24 ± 1.45 47.27 ± 1.11 40.48 ± 1.84
ALLFEATURES 74.14 ± 0.93 48.86 ± 3.94 43.20 ± 3.16
XV-ALLFEATURES 73.91 ± 0.66 47.54 ± 1.52 40.22 ± 1.03
Subs
TYPEONLY 60.90 ± 1.47 39.21 ± 14.78 24.77 ± 2.78
TOKENONLY 62.00 ± 1.16 49.74 ± 6.30 42.95 ± 3.92
ALLFEATURES 60.12 ± 2.11 50.16 ± 8.63 38.56 ± 5.20
XV-ALLFEATURES 69.26 ± 0.60 52.78 ± 1.96 43.85 ± 0.90
</table>
<tableCaption confidence="0.997381">
Table 6: Cross-domain test results on the SENS-
</tableCaption>
<bodyText confidence="0.982978714285714">
ESPOTTING task. Two standard deviations are
shown in small type. Only AUC, Macro-F and
Micro-F are shown for brevity.
AUC as the measure on which to ablate. It’s quite
clear that for Science, all the useful information
is in the type-level features, a result that echoes
what we saw in the previous section. While for
EMEA and Subs, both type- and token-level fea-
tures play a significant role. Considering the six
most useful features in each domain, the ones that
pop out as frequently most useful are the global
PSD features, the ngram probability features (ei-
ther type- or token-based), the relative frequency
features and the context features.
</bodyText>
<subsectionHeader confidence="0.992597">
6.5 Cross-Domain Training
</subsectionHeader>
<bodyText confidence="0.999967533333333">
One disadvantage to the previous method for eval-
uating the SENSESPOTTING task is that it requires
parallel data in a new domain. Suppose we have no
parallel data in the new domain at all, yet still want
to attack the SENSESPOTTING task. One option is
to train a system on domains for which we do have
parallel data, and then apply it in a new domain.
This is precisely the setting we explore in this sec-
tion. Now, instead of performing cross-validation
in a single domain (for instance, Science), we take
the union of all of the training data in the other
domains (e.g., EMEA and Subs), train a classifier,
and then apply it to Science. This classifier will al-
most certainly be worse than one trained on NEW
(Science) but does not require any parallel data in
that domain. (Hyperparameters are chosen by de-
velopment data from the OLD union.)
The results of this experiment are shown in
Table 6. We include results for TOKENONLY,
TYPEONLY and ALLFEATURES; all of these are
trained in the cross-domain setting. To ease com-
parison to the results that do not suffer from do-
main shift, we also present “XV-ALLFEATURES”,
which are results copied from Table 4 in which
parallel data from NEW is used. Overall, there is a
drop of about 7.3% absolute in AUC, moving from
XV-ALLFEATURES to ALLFEATURES, including
a small improvement in Science (likely because
Science is markedly smaller than Subs, and “more
difficult” than EMEA with many word types).
</bodyText>
<subsectionHeader confidence="0.827095">
6.6 Detecting Most Frequent Sense Changes
</subsectionHeader>
<bodyText confidence="0.999926444444444">
We define a second, related task: MOSTFRE-
QSENSECHANGE. In this task, instead of predict-
ing if a given word token has a sense which is
brand new with respect to the old domain, we pre-
dict whether it is being used with a a sense which
is not the one that was observed most frequently
in the old domain. In our EMEA, Science, and
Subtitles data, 68.2%, 48.3%, and 69.6% of word
tokens’ predominant sense changes.
</bodyText>
<page confidence="0.983061">
1442
</page>
<figure confidence="0.999613814814815">
EMEA
Subs
6 12
25 50 100
6 12
25 50 100
% of data
% of data
Macro−F
.63
.50
.40
.32
6 12
Science
% of data
25 50 100
.79
.63
.50
.40
.63
.50
.40
TypeOracle
Random
AllFeatures
</figure>
<figureCaption confidence="0.941270666666667">
Figure 1: Learning curves for the three domains. X-axis is percent of data used, Y-axis is Macro-F score.
Both axes are in log scale to show the fast rate of growth. A horizontal bar corresponding to random
predictions, and the TYPEORACLE results are shown for comparison.
</figureCaption>
<table confidence="0.997348">
AUC Macro-F Micro-F
EMEA
RANDOM 50.54 f 0.41 58.23 f 0.34 49.69 f 0.85
CONSTANT 50.00 f 0.00 82.15 f 0.00 74.43 f 0.00
TYPEONLY 55.05 f 1.00 67.45 f 1.35 65.72 f 0.59
TYPEORACLE 88.36 f 0.00 90.64 f 0.00 77.46 f 0.00
TOKENONLY 66.42 f 1.07 80.27 f 0.50 68.96 f 0.58
ALLFEATURES 58.64 f 3.45 80.57 f 0.45 69.40 f 0.51
Science
RANDOM 50.13 f 0.78 49.05 f 0.82 48.19 f 1.47
CONSTANT 50.00 f 0.00 65.21 f 0.00 73.22 f 0.00
TYPEONLY 68.32 f 1.05 54.70 f 2.35 57.04 f 1.52
TYPEORACLE 91.41 f 0.00 86.71 f 0.00 74.26 f 0.00
TOKENONLY 68.49 f 0.59 62.76 f 0.89 64.40 f 1.08
ALLFEATURES 68.31 f 0.93 64.73 f 1.93 67.20 f 1.65
Subs
RANDOM 50.27 f 0.27 56.93 f 0.29 50.93 f 1.11
CONSTANT 50.00 f 0.00 79.96 f 0.00 76.26 f 0.00
TYPEONLY 60.36 f 0.90 67.78 f 1.98 61.58 f 1.78
TYPEORACLE 82.16 f 0.00 87.96 f 0.00 73.87 f 0.00
TOKENONLY 59.49 f 1.04 77.79 f 0.82 73.51 f 0.68
ALLFEATURES 54.97 f 0.89 77.30 f 1.58 72.29 f 1.68
</table>
<tableCaption confidence="0.839332333333333">
Table 7: Cross-validation results on the MOST-
FREQSENSECHANGE task. Two standard devia-
tions are shown in small type.
</tableCaption>
<bodyText confidence="0.99986575862069">
We use the same set of features and learn-
ing framework to generate and evaluate models
for this task. While the SENSESPOTTING task
has MT utility in suggesting which new domain
words demand a new translation, the MOSTFRE-
QSENSECHANGE task has utility in suggesting
which words demand a new translation proba-
bility distribution when shifting to a new do-
main. Table 7 shows the results of our MOSTFRE-
QSENSECHANGE task experiments.
Results on the MOSTFREQSENSECHANGE
task are somewhat similar to those for the SENS-
ESPOTTING task. Again, our models perform bet-
ter under a macro-level evaluation than under a
micro-level evaluation. However, in contrast to
the SENSESPOTTING results, token-level features
perform quite well on their own for all domains.
It makes sense that our token level features have a
better chance of success on this task. The impor-
tant comparison now is between a new domain to-
ken in context and the majority of the old domain
tokens of the same word type. This comparison
is likely to be more informative than when we are
equally interested in identifying overlap between
the current token and any old domain senses. Like
the SENSESPOTTING results, when doing a micro-
level evaluation, our models do not perform as
well as the CONSTANT baseline, and, as before,
we attribute this to data sparsity.
</bodyText>
<subsectionHeader confidence="0.993324">
6.7 Learning Curves
</subsectionHeader>
<bodyText confidence="0.992756681818182">
All of the results presented so far use classi-
fiers trained on instances of representative types
(i.e. “representative tokens”) extracted from fairly
large new domain parallel corpora (see Table 3),
consisting of between 22 and 36 thousand parallel
sentences, which yield between 8 and 35 thousand
representative tokens. Although we expect some
new domain parallel tuning data to be available
in most MT settings, we would like to know how
many representative types are required to achieve
good performance on the SENSESPOTTING task.
Figure 6.5 shows learning curves over the num-
ber of representative tokens that are used to train
SENSESPOTTING classifiers. In fact, only about
25-50% of the data we used is really necessary to
achieve the performance observed before.
Acknowledgments We gratefully acknowledge the support
of the JHU summer workshop program (and its funders), the
entire DAMT team (http://hal3.name/DAMT/), San-
jeev Khudanpur, support from the NRC for Marine Carpuat,
as well as DARPA CSSG Grant D11AP00279 for Hal Daum´e
III and Jagadeesh Jagarlamudi.
</bodyText>
<page confidence="0.983993">
1443
</page>
<sectionHeader confidence="0.996307" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999926309090909">
E. Agirre and P.G. Edmonds. 2006. Word Sense Dis-
ambiguation: Algorithms and Applications. Text,
Speech, and Language Technology Series. Springer
Science+Business Media B.V.
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7–12.
David Bamman and Gregory Crane. 2011. Measuring
historical word sense variation. In Proceedings of
the 2011 Joint International Conference on Digital
Libraries (JCDL 2011), pages 1–10.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
allocation. Journal of Machine Learning Research
(JMLR), 3.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 854–864,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving
Statistical Machine Translation using Word Sense
Disambiguation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2007), pages 61–
72, Prague, June.
Marine Carpuat, Hal Daum´e III, Alexander Fraser,
Chris Quirk, Fabienne Braune, Ann Clifton, Ann
Irvine, Jagadeesh Jagarlamudi, John Morgan, Ma-
jid Razmara, Aleˇs Tamchyna, Katharine Henry, and
Rachel Rudinger. 2012. Domain adaptation in ma-
chine translation: Final report. In 2012 Johns Hop-
kins Summer Workshop Final Report.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In Proceedings of the Association for
Computational Linguistics.
Paul Cook and Suzanne Stevenson. 2010. Automati-
cally identifying changes in the semantic orientation
of words. In Proceedings of the 7th International
Conference on Language Resources and Evaluation,
pages 28–34, Valletta, Malta.
Hal Daum´e III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by min-
ing unseen words. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Katrin Erk. 2006. Unknown word sense detection as
outlier detection. In Proceedings of the main confer-
ence on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, pages 128–135.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Kristina Gulordava and Marco Baroni. 2011. A distri-
butional similarity approach to the detection of se-
mantic change in the google books ngram corpus. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 67–71, Edinburgh, UK, July. Association for
Computational Linguistics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Matthew Hoffman, David Blei, and Francis Bach.
2010. Online learning for latent dirichlet allocation.
In Advances in Neural Information Processing Sys-
tems (NIPS).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, Timothy Baldwin, and Lexical Computing.
2012. Word sense induction for novel sense de-
tection. In Proceedings of the 13th Conference of
the European Chapter of the Association for compu-
tational Linguistics (EACL 2012), pages 591–601.
Citeseer.
Bernardo Magnini, Carlo Strapparava, Giovanni Pez-
zulo, and Alfio Gliozzo. 2002. The role of domain
information in word sense disambiguation. Natural
Language Engineering, 8(04):359–373.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sam-
mer, and Jeff Bilmes. 2010. Panlingual lexical
translation via probabilistic inference. Artificial In-
telligence, 174:619–637, June.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, page 279. Association for Computational Lin-
guistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553–590.
</reference>
<page confidence="0.87851">
1444
</page>
<reference confidence="0.9960742">
Behrang Mohit and Rebecca Hwa. 2007. Localiza-
tion of difficult-to-translate phrases. In proceedings
of the 2nd ACL Workshop on Statistical Machine
Translations.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and phonetic space. In Proceedings
of the EACL 2009 Workshop on GEMS: GEometical
Models of Natural Language Semantics, pages 104–
111, Athens, Greece, March.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Charles Schafer. 2006. Translation Discovery Using
Diverse Similarity Measures. Ph.D. thesis, Johns
Hopkins University.
J¨org Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In N. Nicolov, K. Bontcheva, G. An-
gelova, and R. Mitkov, editors, Recent Advances in
Natural Language Processing (RANLP).
Wikipedia. 2013. Receiver operating characteristic.
http://en.wikipedia.org/wiki/Receiver_
operating_characteristic#Area_Under_
the_Curve, February.
</reference>
<page confidence="0.992754">
1445
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.406370">
<title confidence="0.998365">SenseSpotting: Never let your parallel data tie you to an old domain</title>
<author confidence="0.8912095">Hal Daum´e Katharine Jagadeesh Rachel</author>
<affiliation confidence="0.915691">Research Council Canada, 2CLIP, University of Maryland, 3CS, University of Chicago, 4CLSP, Johns Hopkins University, 5IBM T.J. Watson Research Center, 6CLSP, Johns Hopkins University,</affiliation>
<abstract confidence="0.999154909090909">Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, word being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. We define a task, in which we build systems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to F-measures of as much as applied to word types it has Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>P G Edmonds</author>
</authors>
<title>Word Sense Disambiguation: Algorithms and Applications. Text, Speech, and Language Technology Series.</title>
<date>2006</date>
<publisher>Springer Science+Business Media B.V.</publisher>
<contexts>
<context position="7083" citStr="Agirre and Edmonds (2006)" startWordPosition="1176" endWordPosition="1179"> with an eye towards improving MT performance: we focus on words that demand new translations. outside this tuning data on word types it has never seen before! From an applied perspective, the assumption of a small amount of parallel data in the new domain is reasonable: if we want an MT system for a new domain, we will likely have some data for system tuning and evaluation. 3 Related Work While word senses have been studied extensively in lexical semantics, research has focused on word sense disambiguation, the task of disambiguating words in context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). In contrast, detecting novel senses has not received as much attention, and is typically addressed within word sense induction, rather than as a distinct SENSESPOTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based </context>
</contexts>
<marker>Agirre, Edmonds, 2006</marker>
<rawString>E. Agirre and P.G. Edmonds. 2006. Word Sense Disambiguation: Algorithms and Applications. Text, Speech, and Language Technology Series. Springer Science+Business Media B.V.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>7--12</pages>
<contexts>
<context position="7191" citStr="Agirre and Soroa (2007)" startWordPosition="1194" endWordPosition="1197">ning data on word types it has never seen before! From an applied perspective, the assumption of a small amount of parallel data in the new domain is reasonable: if we want an MT system for a new domain, we will likely have some data for system tuning and evaluation. 3 Related Work While word senses have been studied extensively in lexical semantics, research has focused on word sense disambiguation, the task of disambiguating words in context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). In contrast, detecting novel senses has not received as much attention, and is typically addressed within word sense induction, rather than as a distinct SENSESPOTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses a</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Gregory Crane</author>
</authors>
<title>Measuring historical word sense variation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Joint International Conference on Digital Libraries (JCDL</booktitle>
<pages>1--10</pages>
<contexts>
<context position="8253" citStr="Bamman and Crane (2011)" startWordPosition="1364" endWordPosition="1368"> Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in SENSESPOTTING. Lau et al. (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al., 2003). Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. This model can interestingly be used to detect newly coined senses, which might co-exist with old senses in recent language. Bamman and Crane (2011) use parallel Latin-English data to learn to disambiguate Latin words into English senses. New English translations are used as evidence that Latin words have shifted sense. In contrast, the SENSESPOTTING task consists of detecting when senses are unknown in parallel data. Such novel sense induction methods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our SENSESPOTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supe</context>
</contexts>
<marker>Bamman, Crane, 2011</marker>
<rawString>David Bamman and Gregory Crane. 2011. Measuring historical word sense variation. In Proceedings of the 2011 Joint International Conference on Digital Libraries (JCDL 2011), pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<volume>3</volume>
<contexts>
<context position="7962" citStr="Blei et al., 2003" startWordPosition="1319" endWordPosition="1322">nct SENSESPOTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in SENSESPOTTING. Lau et al. (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al., 2003). Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. This model can interestingly be used to detect newly coined senses, which might co-exist with old senses in recent language. Bamman and Crane (2011) use parallel Latin-English data to learn to disambiguate Latin words into English senses. New English translations are used as evidence that Latin words have shifted sense. In contrast, the SENSESPOTTING task consists of detecting when senses are unknown in parallel data. Such novel sense induction methods </context>
<context position="13929" citStr="Blei et al., 2003" startWordPosition="2343" endWordPosition="2346">To and Tn are the set of old and new topics and Po and Pn are the old and new distributions defined over them, respectively, and cos is the cosine similarity between a pair of topics, we define the feature Type:TopicSim: E tcTn,t�cTo Pn(t|w)Po(t&apos;|w) cos(t, t&apos;). For a word w, the feature value will be high if, for each new domain topic t that places high probability on w, there is an old domain topic t&apos; that 1437 is similar to t and also places a high probability on w. Conversely, if no such topic exists, the score will be low, indicating the word has gained a new sense. We use the online LDA (Blei et al., 2003; Hoffman et al., 2010), implemented in http://hunch.net/˜vw/ to compute topics on the two domains separately. We use 100 topics. Context Feature It is expected that words acquiring new senses will tend to neighbor different sets of words (e.g. different arguments, prepositions, parts of speech, etc.). Thus, we define an additional type level feature to be the ratio of the number of new domain n-grams (up to length three) that contain word w and which do not appear in the old domain to the total number of new domain n-grams containing w. With Nw indicating the set of n-grams in the new domain </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research (JMLR), 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bloodgood</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Bucking the trend: Large-scale cost-focused active learning for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>854--864</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="4451" citStr="Bloodgood and Callison-Burch, 2010" startWordPosition="726" endWordPosition="729">esults are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (token) occurrence of this known word being used in a new sense?” Note that for both the dictionary mining setting and the active learning setting, it is important to consider words in</context>
</contexts>
<marker>Bloodgood, Callison-Burch, 2010</marker>
<rawString>Michael Bloodgood and Chris Callison-Burch. 2010. Bucking the trend: Large-scale cost-focused active learning for statistical machine translation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854–864, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving Statistical Machine Translation using Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>61--72</pages>
<location>Prague,</location>
<contexts>
<context position="3104" citStr="Carpuat and Wu, 2007" startWordPosition="512" endWordPosition="515">scientific domain, the word gains a new sense: “ratio”, which simply does not exist in the parliament domain. In a science domain, the “report” sense exists, but it is dominated about 12:1 by “ratio.” In a medical domain, the “report” sense remains dominant (about 2:1), but the new “ratio” sense appears frequently. In this paper we define a new task that we call SENSESPOTTING. The goal of this task is to identify words in a new domain monolingual text that appeared in old domain text but which have a new, previously unseen sense1. We operate under the framework of phrase sense disambiguation (Carpuat and Wu, 2007), in which we take automatically align parallel data in an old domain to generate an initial old-domain sense inventory. This sense inventory provides the set of “known” word senses in the form of phrasal translations. Concrete examples are shown in Table 1. One of our key contributions is the development of a rich set of features based on monolingual text that are indicative of new word senses. This work is driven by an application need. When machine translation (MT) systems are applied in a new domain, many errors are a result of: (1) previously unseen (OOV) source language words, or (2) sou</context>
<context position="16564" citStr="Carpuat and Wu (2007)" startWordPosition="2803" endWordPosition="2806">cent of the four contextual words which had been seen in the OLDdomain data, Token:Ctx%. Token-Level PSD Features These features aim to capture generalized characteristics of a context. Towards this end, first, we pose the problem as a phrase sense disambiguation (PSD) problem over the known sense inventory. Given a source word in a context, we train a classifier to predict the most likely target translation. The ground truth labels (target translation for a given source word) for this classifier are generated from the phrase table of the old domain data. We use the same set of features as in Carpuat and Wu (2007). Second, given a source word s, we use this classifier to compute the probability distribution of target translations (p(t|s)). Subsequently, we use this probability distribution to define new features for the SENSESPOTTING task. The idea is that, if a word is used in one of the known senses then its context must have been seen previously and hence we hope that the PSD classifier outputs a spiky distribution. On the other hand, if the word takes a new sense then hopefully it is used in an unseen context resulting in the PSD classifier outputting an uniform distribution. Based on this intuitio</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving Statistical Machine Translation using Word Sense Disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007), pages 61– 72, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Hal Daum´e Alexander Fraser</author>
<author>Chris Quirk</author>
<author>Fabienne Braune</author>
</authors>
<title>Domain adaptation in machine translation: Final report.</title>
<date>2012</date>
<booktitle>In 2012 Johns Hopkins Summer Workshop Final Report.</booktitle>
<location>Ann Clifton, Ann Irvine, Jagadeesh Jagarlamudi, John Morgan, Majid Razmara, Aleˇs Tamchyna, Katharine</location>
<contexts>
<context position="4094" citStr="Carpuat et al., 2012" startWordPosition="670" endWordPosition="673">ndicative of new word senses. This work is driven by an application need. When machine translation (MT) systems are applied in a new domain, many errors are a result of: (1) previously unseen (OOV) source language words, or (2) source language words that appear with a new sense and which require new transla1All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word</context>
</contexts>
<marker>Carpuat, Fraser, Quirk, Braune, 2012</marker>
<rawString>Marine Carpuat, Hal Daum´e III, Alexander Fraser, Chris Quirk, Fabienne Braune, Ann Clifton, Ann Irvine, Jagadeesh Jagarlamudi, John Morgan, Majid Razmara, Aleˇs Tamchyna, Katharine Henry, and Rachel Rudinger. 2012. Domain adaptation in machine translation: Final report. In 2012 Johns Hopkins Summer Workshop Final Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Domain adaptation with active learning for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9170" citStr="Chan and Ng, 2007" startWordPosition="1511" endWordPosition="1514">n methods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our SENSESPOTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supervision during training and evaluation. 1436 The impact of domain on novel senses has also received some attention. Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al., 2004; McCarthy et al., 2007; Erk, 2006; Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. Finally, SENSESPOTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, SENSESPOTTING detects when words have new senses and, thus, frequently a new translation. Work on active learning for machine translation has focused on coll</context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation with active learning for word sense disambiguation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Automatically identifying changes in the semantic orientation of words.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation,</booktitle>
<pages>28--34</pages>
<location>Valletta,</location>
<contexts>
<context position="7629" citStr="Cook and Stevenson, 2010" startWordPosition="1263" endWordPosition="1266">in context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). In contrast, detecting novel senses has not received as much attention, and is typically addressed within word sense induction, rather than as a distinct SENSESPOTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in SENSESPOTTING. Lau et al. (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al., 2003). Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. This model can interestingly be used to detect newly coined senses, which might co-exist with old senses in recent language.</context>
</contexts>
<marker>Cook, Stevenson, 2010</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2010. Automatically identifying changes in the semantic orientation of words. In Proceedings of the 7th International Conference on Language Resources and Evaluation, pages 28–34, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Daum´e, Jagarlamudi, 2011</marker>
<rawString>Hal Daum´e III and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Unknown word sense detection as outlier detection.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="9150" citStr="Erk, 2006" startWordPosition="1509" endWordPosition="1510">se induction methods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our SENSESPOTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supervision during training and evaluation. 1436 The impact of domain on novel senses has also received some attention. Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al., 2004; McCarthy et al., 2007; Erk, 2006; Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. Finally, SENSESPOTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, SENSESPOTTING detects when words have new senses and, thus, frequently a new translation. Work on active learning for machine translation</context>
</contexts>
<marker>Erk, 2006</marker>
<rawString>Katrin Erk. 2006. Unknown word sense detection as outlier detection. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4272" citStr="Fung and Yee, 1998" startWordPosition="699" endWordPosition="702">sly unseen (OOV) source language words, or (2) source language words that appear with a new sense and which require new transla1All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (toke</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An IR approach for translating new words from nonparallel, comparable texts. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Gulordava</author>
<author>Marco Baroni</author>
</authors>
<title>A distributional similarity approach to the detection of semantic change in the google books ngram corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>67--71</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, UK,</location>
<contexts>
<context position="7658" citStr="Gulordava and Baroni, 2011" startWordPosition="1267" endWordPosition="1270">ned sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). In contrast, detecting novel senses has not received as much attention, and is typically addressed within word sense induction, rather than as a distinct SENSESPOTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in SENSESPOTTING. Lau et al. (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al., 2003). Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. This model can interestingly be used to detect newly coined senses, which might co-exist with old senses in recent language. Bamman and Crane (2011) use </context>
</contexts>
<marker>Gulordava, Baroni, 2011</marker>
<rawString>Kristina Gulordava and Marco Baroni. 2011. A distributional similarity approach to the detection of semantic change in the google books ngram corpus. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 67–71, Edinburgh, UK, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4338" citStr="Haghighi et al., 2008" startWordPosition="709" endWordPosition="712">words that appear with a new sense and which require new transla1All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (token) occurrence of this known word being used in a new sense?” Note </context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Hoffman</author>
<author>David Blei</author>
<author>Francis Bach</author>
</authors>
<title>Online learning for latent dirichlet allocation.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="13952" citStr="Hoffman et al., 2010" startWordPosition="2347" endWordPosition="2350">et of old and new topics and Po and Pn are the old and new distributions defined over them, respectively, and cos is the cosine similarity between a pair of topics, we define the feature Type:TopicSim: E tcTn,t�cTo Pn(t|w)Po(t&apos;|w) cos(t, t&apos;). For a word w, the feature value will be high if, for each new domain topic t that places high probability on w, there is an old domain topic t&apos; that 1437 is similar to t and also places a high probability on w. Conversely, if no such topic exists, the score will be low, indicating the word has gained a new sense. We use the online LDA (Blei et al., 2003; Hoffman et al., 2010), implemented in http://hunch.net/˜vw/ to compute topics on the two domains separately. We use 100 topics. Context Feature It is expected that words acquiring new senses will tend to neighbor different sets of words (e.g. different arguments, prepositions, parts of speech, etc.). Thus, we define an additional type level feature to be the ratio of the number of new domain n-grams (up to length three) that contain word w and which do not appear in the old domain to the total number of new domain n-grams containing w. With Nw indicating the set of n-grams in the new domain which contain w, Ow ind</context>
</contexts>
<marker>Hoffman, Blei, Bach, 2010</marker>
<rawString>Matthew Hoffman, David Blei, and Francis Bach. 2010. Online learning for latent dirichlet allocation. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="20068" citStr="Koehn et al., 2007" startWordPosition="3367" endWordPosition="3370">stributions. SpreadNorm is the ratio of spread of the prior and posterior distributions, where spared is the difference between maximum and minimum probabilities of the distribution as defined earlier. ConfusionNorm is the ratio of confusion of the prior and posterior distributions, where confusion is defined as earlier. 5 Data and Gold Standard The first component of our task is a parallel corpus of old domain data, for which we use the French-English Hansard parliamentary proceedings (http://www.parl.gc.ca). From this, we extract an old domain sense dictionary, using the Moses MT framework (Koehn et al., 2007). This defines our old domain sense dictionary. For new domains, we use three sources: (1) the EMEA medical corpus (Tiedemann, 2009), (2) a corpus of scientific abstracts, and (3) a corpus of translated movie subtitles (Tiedemann, 2009). Basic statistics are shown in Table 2. In all parallel corpora, we normalize the English for American spelling. To create the gold standard truth, we followed a lexical sample apparoach and collected a set of 300 “representative types” that are interesting to evaluate on, because they have multiple senses within a single domain or whose senses are likely to ch</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Paul Cook</author>
<author>Diana McCarthy</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
<author>Lexical Computing</author>
</authors>
<title>Word sense induction for novel sense detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for computational Linguistics (EACL 2012),</booktitle>
<pages>591--601</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="7846" citStr="Lau et al. (2012)" startWordPosition="1300" endWordPosition="1303"> has not received as much attention, and is typically addressed within word sense induction, rather than as a distinct SENSESPOTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in SENSESPOTTING. Lau et al. (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al., 2003). Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. This model can interestingly be used to detect newly coined senses, which might co-exist with old senses in recent language. Bamman and Crane (2011) use parallel Latin-English data to learn to disambiguate Latin words into English senses. New English translations are used as evidence that Latin words have shifted sense. In contrast, the SE</context>
</contexts>
<marker>Lau, Cook, McCarthy, Newman, Baldwin, Computing, 2012</marker>
<rawString>Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, Timothy Baldwin, and Lexical Computing. 2012. Word sense induction for novel sense detection. In Proceedings of the 13th Conference of the European Chapter of the Association for computational Linguistics (EACL 2012), pages 591–601. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Carlo Strapparava</author>
<author>Giovanni Pezzulo</author>
<author>Alfio Gliozzo</author>
</authors>
<title>The role of domain information in word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>04</issue>
<contexts>
<context position="1369" citStr="Magnini et al. (2002)" startWordPosition="210" endWordPosition="213"> lexical semantics. We define a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to achieve F-measures of as much as 80%, when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains. 1 Introduction As Magnini et al. (2002) observed, the domain of the text that a word occurs in is a useful signal for performing word sense disambiguation (e.g. in a text about finance, bank is likely to refer to a financial institution while in a text about geography, it is likely to refer to a river bank). However, in the classic WSD task, ambiguous word types and a set of possible senses are known in advance. In this work, we focus on the setting where we observe texts in two different domains and want to identify words in the second text that have a sense that did not appear in the first text, without any lexical knowledge in t</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2002</marker>
<rawString>Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo, and Alfio Gliozzo. 2002. The role of domain information in word sense disambiguation. Natural Language Engineering, 8(04):359–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
<author>Kobi Reiter</author>
<author>Michael Skinner</author>
<author>Marcus Sammer</author>
<author>Jeff Bilmes</author>
</authors>
<title>Panlingual lexical translation via probabilistic inference.</title>
<date>2010</date>
<journal>Artificial Intelligence,</journal>
<volume>174</volume>
<contexts>
<context position="4359" citStr="Mausam et al., 2010" startWordPosition="713" endWordPosition="716">a new sense and which require new transla1All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (token) occurrence of this known word being used in a new sense?” Note that for both the dic</context>
</contexts>
<marker>Mausam, Etzioni, Weld, Reiter, Skinner, Sammer, Bilmes, 2010</marker>
<rawString>Mausam, Stephen Soderland, Oren Etzioni, Daniel S. Weld, Kobi Reiter, Michael Skinner, Marcus Sammer, and Jeff Bilmes. 2010. Panlingual lexical translation via probabilistic inference. Artificial Intelligence, 174:619–637, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>279</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9116" citStr="McCarthy et al., 2004" startWordPosition="1501" endWordPosition="1504">s are unknown in parallel data. Such novel sense induction methods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our SENSESPOTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supervision during training and evaluation. 1436 The impact of domain on novel senses has also received some attention. Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al., 2004; McCarthy et al., 2007; Erk, 2006; Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. Finally, SENSESPOTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, SENSESPOTTING detects when words have new senses and, thus, frequently a new translation. Work on activ</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 279. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Unsupervised acquisition of predominant word senses.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="9139" citStr="McCarthy et al., 2007" startWordPosition="1505" endWordPosition="1508">el data. Such novel sense induction methods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our SENSESPOTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supervision during training and evaluation. 1436 The impact of domain on novel senses has also received some attention. Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al., 2004; McCarthy et al., 2007; Erk, 2006; Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. Finally, SENSESPOTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, SENSESPOTTING detects when words have new senses and, thus, frequently a new translation. Work on active learning for machine </context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2007</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2007. Unsupervised acquisition of predominant word senses. Computational Linguistics, 33(4):553–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behrang Mohit</author>
<author>Rebecca Hwa</author>
</authors>
<title>Localization of difficult-to-translate phrases.</title>
<date>2007</date>
<booktitle>In proceedings of the 2nd ACL Workshop on Statistical Machine Translations.</booktitle>
<contexts>
<context position="9985" citStr="Mohit and Hwa, 2007" startWordPosition="1639" endWordPosition="1642">rning. Finally, SENSESPOTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, SENSESPOTTING detects when words have new senses and, thus, frequently a new translation. Work on active learning for machine translation has focused on collecting translations for longer unknown segments (e.g., Bloodgood and CallisonBurch (2010)). There has been some interest in detecting which phrases that are hard to translate for a given system (Mohit and Hwa, 2007), but difficulties can arise for many reasons: SENSESPOTTING focuses on a single problem. 4 New Sense Indicators We define features over both word types and word tokens. In our classification setting, each instance consists of a French word token in context. Our word type features ignore this context and rely on statistics computed over our entire new domain corpus. In contrast, our word token features consider the context of the particular instance of the word. If it were the case that only one sense existed for all word tokens of a particular type within a single domain, we would expect our </context>
</contexts>
<marker>Mohit, Hwa, 2007</marker>
<rawString>Behrang Mohit and Rebecca Hwa. 2007. Localization of difficult-to-translate phrases. In proceedings of the 2nd ACL Workshop on Statistical Machine Translations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4252" citStr="Rapp, 1995" startWordPosition="697" endWordPosition="698"> (1) previously unseen (OOV) source language words, or (2) source language words that appear with a new sense and which require new transla1All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is t</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Sagi</author>
<author>Stefan Kaufmann</author>
<author>Brady Clark</author>
</authors>
<title>Semantic density analysis: Comparing word meaning across time and phonetic space.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics,</booktitle>
<pages>104--111</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="7603" citStr="Sagi et al., 2009" startWordPosition="1259" endWordPosition="1262">sambiguating words in context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). In contrast, detecting novel senses has not received as much attention, and is typically addressed within word sense induction, rather than as a distinct SENSESPOTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in SENSESPOTTING. Lau et al. (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al., 2003). Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. This model can interestingly be used to detect newly coined senses, which might co-exist with old </context>
</contexts>
<marker>Sagi, Kaufmann, Clark, 2009</marker>
<rawString>Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009. Semantic density analysis: Comparing word meaning across time and phonetic space. In Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 104– 111, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing translation lexicons via diverse similarity measures and bridge languages.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="4300" citStr="Schafer and Yarowsky, 2002" startWordPosition="703" endWordPosition="706">rce language words, or (2) source language words that appear with a new sense and which require new transla1All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (token) occurrence of this known </context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Charles Schafer and David Yarowsky. 2002. Inducing translation lexicons via diverse similarity measures and bridge languages. In Proceedings of the Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
</authors>
<title>Translation Discovery Using Diverse Similarity Measures.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="4315" citStr="Schafer, 2006" startWordPosition="707" endWordPosition="708">ource language words that appear with a new sense and which require new transla1All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (token) occurrence of this known word being used</context>
</contexts>
<marker>Schafer, 2006</marker>
<rawString>Charles Schafer. 2006. Translation Discovery Using Diverse Similarity Measures. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>News from OPUS - A collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing (RANLP).</booktitle>
<editor>In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors,</editor>
<contexts>
<context position="20200" citStr="Tiedemann, 2009" startWordPosition="3390" endWordPosition="3391"> and minimum probabilities of the distribution as defined earlier. ConfusionNorm is the ratio of confusion of the prior and posterior distributions, where confusion is defined as earlier. 5 Data and Gold Standard The first component of our task is a parallel corpus of old domain data, for which we use the French-English Hansard parliamentary proceedings (http://www.parl.gc.ca). From this, we extract an old domain sense dictionary, using the Moses MT framework (Koehn et al., 2007). This defines our old domain sense dictionary. For new domains, we use three sources: (1) the EMEA medical corpus (Tiedemann, 2009), (2) a corpus of scientific abstracts, and (3) a corpus of translated movie subtitles (Tiedemann, 2009). Basic statistics are shown in Table 2. In all parallel corpora, we normalize the English for American spelling. To create the gold standard truth, we followed a lexical sample apparoach and collected a set of 300 “representative types” that are interesting to evaluate on, because they have multiple senses within a single domain or whose senses are likely to change in a new domain. We used a semi-automatic approach to identify representative types. We first used the phrase table from Sents </context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J¨org Tiedemann. 2009. News from OPUS - A collection of multilingual parallel corpora with tools and interfaces. In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing (RANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia</author>
</authors>
<title>Receiver operating characteristic. http://en.wikipedia.org/wiki/Receiver_ operating_characteristic#Area_Under_ the_Curve,</title>
<date>2013</date>
<contexts>
<context position="25530" citStr="Wikipedia, 2013" startWordPosition="4289" endWordPosition="4290">two baselines: RANDOM and CONSTANT. RANDOM predicts new-sense or not-new-sense randomly and with equal probability. CONSTANT always predicts new-sense, achieving 100% recall and a macrolevel precision that is equal to the percent of representative words which do have a new sense, modulo cross-validation splits (see Table 3). Addi4The most frequent (voie) appears 3881 times; there are 60 singleton words on average across the three new domains. 5AUC is the probability that the classifier will assign a higher score to a randomly chosen positive example than to a randomly chosen negative example (Wikipedia, 2013). tionally, we compare our results with a type-level oracle, TYPEORACLE. For all tokens of a given word type, the oracle predicts the majority label (new-sense or not-new-sense) for that word type. These results correspond to an upper bound for the TYPEONLY experiments. 6.2 Classification Setup For all experiments, we use a linear classifier trained by stochastic gradient descent to optimize logistic loss. We also did some initial experiments on development data using boosted decision trees instead and other loss functions (hinge loss, squared loss), but they never performed as well. In all ca</context>
</contexts>
<marker>Wikipedia, 2013</marker>
<rawString>Wikipedia. 2013. Receiver operating characteristic. http://en.wikipedia.org/wiki/Receiver_ operating_characteristic#Area_Under_ the_Curve, February.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>