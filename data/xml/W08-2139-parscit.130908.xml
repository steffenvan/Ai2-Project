<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000611">
<title confidence="0.900565">
Probabilistic Model for Syntactic and Semantic Dependency Parsing
</title>
<author confidence="0.994272">
Enhong Chen
</author>
<affiliation confidence="0.8736965">
Department of Computer
Science, University of Sci-
ence and Technology of
China, Hefei, China
</affiliation>
<email confidence="0.996046">
cheneh@ustc.edu.cn
</email>
<author confidence="0.991708">
Liu Shi
</author>
<affiliation confidence="0.87344775">
Department of Computer
Science, University of Sci-
ence and Technology of
China, Hefei, China
</affiliation>
<email confidence="0.998625">
shiliu@ustc.edu
</email>
<author confidence="0.996624">
Dawei Hu
</author>
<affiliation confidence="0.873621">
Department of Computer
Science, University of Sci-
ence and Technology of
China, Hefei, China
</affiliation>
<email confidence="0.997227">
dwhu@mail.ustc.edu.cn
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987325">
This paper proposes a novel method to
analyze syntactic dependencies and label
semantic dependencies around both the
verbal predicates and the nouns. In this
method, a probabilistic model is designed
to obtain a global optimal result. More-
over, a predicate identification model and
a disambiguation model are proposed to
label predicates and their senses. The ex-
perimental results obtained on the wsj
and brown test sets show that our system
obtains 77% of labeled macro F1 score
for the whole task, 84.47% of labeled at-
tachment score for syntactic dependency
task, and 69.45% of labeled F1 score for
semantic dependency task.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999988647058824">
There are two difficulties in the CoNLL 2008
shared task. One is how to label semantic role on
a dependency-based representation and how to
label verbal predicates and nouns. The other one
is how to combine the syntactic task with the
semantic task together.
On the basis of statistical analysis of labeling
results, we optimize the traditional approaches of
syntactic dependency parsing and semantic role
labeling. Moreover, we design a predicate
identification model and a disambiguation model,
which will be described in section 2.3, for
labeling predicates and their senses. In the
disambiguation model, an exhaustion method is
used to find the best sense which is
corresponding to a frame of predicate. In order to
obtain a global optimization result for every
</bodyText>
<footnote confidence="0.9517565">
© 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
</footnote>
<bodyText confidence="0.989791833333333">
sentence, a probabilistic model is designed to
combine all subtasks.
The rest of this paper is organized as follows:
our system is described in section 2; and section
3 reports our results on development and test sets;
at last we conclude the paper in section 4.
</bodyText>
<sectionHeader confidence="0.876463" genericHeader="method">
2 A Probabilistic Model for Syntactic
and Semantic Dependency Labeling
</sectionHeader>
<bodyText confidence="0.999964269230769">
Compared with previous tasks, this shared task is
more complex. It aims to merge both syntactic
and semantic dependencies under a unified
representation. Obviously, it can be divided into
two subtasks: syntactic dependency parsing and
semantic dependency labeling. For the second
subtask, predicates and their senses should be
labeled before semantic arguments for predicates
are labeled. Since many predicates have only one
sense, it is inefficient to build a multi-label
classifier to classify each predicate. When a
classification approach is used, it is mandatory to
consider multiple senses for those predicates
with only one or two senses. To prevent
assigning irrelevant senses to predicates, we do
not adopt classification approach. Instead, two
more subtasks, i.e., predicate identification and
predicate sense labeling, are introduced in this
paper. The predicate sense labeling and semantic
dependency labeling are performed together with
a disambiguation model.
To ensure that we can get an optimal overall
syntactic and semantic dependency results
through integrating the above steps, a probability
model is proposed. The probabilistic model is
described in Equation (1), where the score P
</bodyText>
<subsubsectionHeader confidence="0.482913">
sent
</subsubsectionHeader>
<bodyText confidence="0.9906395">
of a sentence labeling is the combined
conditional probability of its all subtasks, is
</bodyText>
<subsubsectionHeader confidence="0.494692">
Psyn
</subsubsectionHeader>
<bodyText confidence="0.817504333333333">
the probability of syntactic dependency parsing,
Ppred is the probability of predicate
identification, Psem (i) is the probability of
</bodyText>
<page confidence="0.976158">
263
</page>
<reference confidence="0.2408065">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 263–267
Manchester, August 2008
</reference>
<bodyText confidence="0.9971825">
semantic dependency labeling for the ith-
predicate, and n is the number of predicates.
</bodyText>
<equation confidence="0.9938795">
n
P sent P syn * P pred * P sem ( i)
= ∏ (1)
i=1
</equation>
<bodyText confidence="0.9956764">
For each sentence, its top-N candidates using
syntactic dependency parsing are obtained. Then
for each candidate, predicates and semantic ar-
guments are labeled. At last, the best one with
the highest Psent is chosen as final labeling result.
</bodyText>
<subsectionHeader confidence="0.9969">
2.1 Syntactic Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.9999104">
There are several approaches for syntactic de-
pendency parsing, as demonstrated in the CoNLL
2007 shared task. A commonly used LR algo-
rithm is applied to this task. Unlike the best-first
probabilistic shift-reduce LR algorithm used by
(Kenji and Jun, 2007), here a combined probabil-
ity of all parsing steps is used to evaluate parsing
results, and the best one is obtained as the final
result. The probability of syntactic dependency
parsing is defined in Equation (2).
</bodyText>
<equation confidence="0.993313666666667">
j
P syn =∏Pact(i) (2)
i=1
</equation>
<bodyText confidence="0.998787">
where is the probability of every LR ac-
</bodyText>
<equation confidence="0.987951">
P (i)
act
</equation>
<bodyText confidence="0.948463428571429">
tion act at step i, and j is the number of all steps.
As the search space of LR parser is exponen-
tial growth with the word number, the maximum
size of candidate states is limited to 50.
The features that we use are similar to (Kenji
and Jun, 2007). Hence we do not describe them
in this paper.
</bodyText>
<subsectionHeader confidence="0.996775">
2.2 Predicate Identification
</subsectionHeader>
<bodyText confidence="0.999328">
In this subtask, a MaxEnt model is adopted for
classification. The features we used are as follow:
</bodyText>
<listItem confidence="0.997418891891892">
• Base info: FORM, LEMMA, POS (GPOS
if available, or is PPOS), SPLIT_FORM,
SPLIT_LEMMA, PPOSS.
• Base syntactic dependency info:
o Number of modifiers;
o Number of modifiers of the previous word;
o Number of modifiers of the next word;
o PPOSS of left-most modifier;
o Deprel of left-most modifier;
o PPOSS of right-most modifier;
o Deprel of right-most modifier.
• Modifiers info
o POS list of all modifiers: if GPOS is avail-
able, POS is GPOS. Otherwise it is PPOS.
o DEPREL list of all modifiers;
o SPLIT_LEMMA list of all modifiers;
o PPOSS list of all modifiers.
• Head’s base info
• Head’s base syntactic dependency info
• Head’s modifiers info
• Deprel: the syntactic dependency relation
to head.
• Word stem
• Stem of right-most modifier
• PPOSS of right-most modifier
• Suffix: The suffix of the word. We use the
last 3 characters as this feature.
• Voice: Check if the word is a verb and is
passive voice.
• Previous word info: Check if the previous
word is a predicate.
• Pos path to ROOT: PPOSS list from
word to ROOT through the syntactic de-
pendency path.
• Deprel path to ROOT: DEPREL list from
word to ROOT through the syntactic de-
pendency path.
</listItem>
<bodyText confidence="0.99357">
Through statistical analysis, we find that
PPOSS of nearly all predicates are in a particular
category which contains NN, NNP, NNS, VB,
VBD, VBG, VBN, VBP, VBZ, and JJ. Hence we
ignore the words without these PPOSS to reduce
the number of samples and speed up the process
of training and recognition. Meanwhile, we also
ignore the words having no relational frame in
PropBank or NomBank.
</bodyText>
<subsectionHeader confidence="0.998798">
2.3 Predicate Sense Labeling
</subsectionHeader>
<bodyText confidence="0.999990130434783">
In this subtask, we label the sense of each predi-
cate. Different predicates are usually unrelated
even if they have the same sense number, which
makes us hardly use a classifier to label them.
Hence, we design a disambiguation model to
solve this problem.
Firstly, for each word which has been identi-
fied to be a predicate, we find out all of its prob-
able sense forms (corresponding to the field of
“PRED”). According to statistical analysis, only
about 0.05% PREDs are not described in
PropBank frames or NomBank frames. So it is
reasonable to assume that all PREDs could be
found in PropBank or NomBank. Moreover, we
find that about 96% PREDs are formed as
“SPLIT LEMMA + .sense” or “LEMMA
+ .sense”. As a result, when a word is identified
to be a predicate, we use its LEMMA and
SPLIT_LEMMA to find all possible PREDs
from PropBank and NomBank. Furthermore, if
some special words are unsuitable for these two
forms, we should convert them into their original
forms first and then find their possible PREDs.
</bodyText>
<page confidence="0.988908">
264
</page>
<bodyText confidence="0.999961214285714">
For the rest anomalistic words, we build a map-
ping dictionary from training data.
Secondly, for each possible sense form, we la-
bel semantic argument for all words. If a word is
not a semantic argument, it would be labeled as
“ ”. The score of the current possible sense form
is calculated as the combination of all probability
of each labeling. More details about semantic
dependency labeling will be described in section
2.4.
Thirdly, we choose the sense form and its se-
mantic arguments with the highest score. The
above steps will be repeated until all predicates
have definite senses.
</bodyText>
<subsectionHeader confidence="0.991806">
2.4 Semantic Dependency Labeling
</subsectionHeader>
<bodyText confidence="0.999538692307693">
Unlike CoNLL-2005 shared task, this shared
task performing Semantic Role Labeling on a
dependency-based representation (DSRL). It is a
novel way for SRL and the traditional SRL
methods can not directly be used here.
Constituent-based SRL model needs to find out
all probable constituents, while DSRL only
considers the semantic dependency between
word and predicate. Moreover, DSRL uses
syntactic dependency parsing tree instead of
traditional full syntactic parsing tree. As a result,
the traditional features need to be amended
accordingly. The features we used are as follows:
</bodyText>
<listItem confidence="0.999522789473684">
• Deprel
• Word stem
• POS: if GPOS is available, POS is GPOS.
Otherwise it is PPOS.
• Stem of right-most modifier
• PPOSS of right-most modifier
• Predicate: the FORM of predicate.
• PPOSS of predicate
• Suffix of predicate
• Voice: voice of predicate
• Position: The position of the word with re-
spect to its predicate. It has three values,
“before”, “is” and “after”, for the predicate.
• Deprel path to predicate: DEPREL list
from word to its predicate through the syn-
tactic dependency path.
• Length of syntactic dependency path to
predicate
• Sense: the sense of predicate
</listItem>
<bodyText confidence="0.9993211">
Moreover, we try to find more features with
frames. Since the PropBank and NomBank are
available and all predicates with senses are avail-
able for this subtask. Statistical analysis shows
that nearly all core semantic arguments (AA, A0,
A1, A2 ...) of a predicate are described in the
frame of predicate. But it is incorrect contrarily.
Based on these observations, we design features
the following features for five frequently used
core arguments:
</bodyText>
<listItem confidence="0.9995675">
• A0 is in predicate’s frame: Have two
values: “YES” and “NO”.
• A1 is in predicate’s frame
• A2 is in predicate’s frame
• A3 is in predicate’s frame
• A4 is in predicate’s frame
</listItem>
<bodyText confidence="0.999836625">
Because the other core semantic arguments are
rare, we do not need to design features for them.
With this method, the labeling efficiency is im-
proved while the precision almost keeps un-
changed.
As the frame information has been used in fea-
tures, we do not add any valency check on the
labeling result.
</bodyText>
<sectionHeader confidence="0.998202" genericHeader="evaluation">
3 Experiments and Analysis
</sectionHeader>
<subsectionHeader confidence="0.974504">
3.1 Data and Environment
</subsectionHeader>
<bodyText confidence="0.999784619047619">
The data provided for this Closed Challenge of
shared task is part of TreeBank and Brown cor-
pus. Training set covers sections 02-21 of Tree-
Bank. Development set covers section 24 of
TreeBank. Wsj test set covers section 23 of
TreeBank. Brown test set covers sections ck01,
ck02, and ck03 of the Brown corpus.
The maximum entropy classier (Berger et al,
1996) used is Le Zhang&apos;s Maximum Entropy
Modeling Toolkit and the L-BFGS parameter
estimation algorithm with gaussian prior smooth-
ing (Chen and Rosenfeld, 1999). The gaussian
prior is set to 2 and the iteration count is set to
500. All results we list here are post-evaluated
because there are some small modifications.
The experiments are performed on a PC with
AMD AthlonTM 64 x2 4400+ CPU and 2GB
main memory running Microsoft Windows XP
with sp2. Our system is developed using C++.
In our experimental analysis, the abbreviations
used are listed as follows:
</bodyText>
<listItem confidence="0.999988555555556">
• LAS1: Labeled attachment score
• UAS: Unlabeled attachment score
• LAS2: Label accuracy score
• LP: Labeled precision
• LR: Labeled recall
• LF1: Labeled F1
• UP: Unlabeled precision
• UR: Unlabeled recall
• UF1: Unlabeled F1
</listItem>
<page confidence="0.996402">
265
</page>
<subsectionHeader confidence="0.997902">
3.2 Syntactic Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999943944444444">
We trained two LR models for syntactic depend-
ency parsing. The first LR model uses MaxEnt
classification to determine possible parser actions
and their probabilities. The second LR model
also uses MaxEnt classification, but parsing is
performed backwards simply by reversing the
sentence before parsing starts.
For a sentence, each model can label top-N
candidates and calculate the probability for every
result. We join these two models by finding the
candidate with the highest probability from all
candidates as the final result for the sentence.
Table 1 shows the results of each model and joint
model. We can see that the two LR models ob-
tain similar results. The joint model can obtain
better result and increase almost one percentage.
The processing time of joint model is twice more
than that of the two other models.
</bodyText>
<table confidence="0.99977075">
LR LR-back Joint
Model Model Model
dev LAS1 83.05 83.38 84.43
UAS 86.36 86.74 87.74
LAS2 89.15 89.63 90.08
wsj LAS1 84.84 84.06 85.48
UAS 87.60 86.74 88.13
LAS2 90.70 90.47 91.21
brown LAS1 77.29 76.95 78.91
UAS 82.75 82.61 84.38
LAS2 85.00 84.82 85.76
wsj + LAS1 84.00 83.27 84.75
brown
UAS 87.06 86.28 87.71
LAS2 90.07 89.84 90.6
Speed (sec/sent) 0.49 0.42 0.92
</table>
<tableCaption confidence="0.999936">
Table 1: Syntactic dependency parsing results
</tableCaption>
<bodyText confidence="0.998860166666667">
The gold HEAD and DEPREL and PRED fields
is used to test the model.
Statistical analysis shows that, for about 99%
semantic argument labels, the length of syntactic
dependency path from word to predicate is less
than 7. So we ignore the words with the length of
7 or more.
The final results of semantic dependency la-
beling are shown in Table 3. The labeling for
each sentence spends about 10ms.
Brown set is an out-of-domain set and wsj set
is an in-domain set. Usually, the results on wsj
are much better than those on brown. But here
we found that the unlabeled scores are nearly the
same between wsj and brown. It shows that our
model performs well at unlabeled labeling on
out-of-domain set, and should be improved at
labeled labeling.
</bodyText>
<table confidence="0.998098714285714">
dev wsj brown
LP 80.50 82.47 77.29
LR 70.73 73.58 67.16
LF1 75.30 77.77 71.87
UP 92.10 92.65 92.87
UR 80.92 82.65 80.69
UF1 86.15 87.36 86.35
</table>
<tableCaption confidence="0.999658">
Table 3: Semantic dependency labeling results
</tableCaption>
<subsectionHeader confidence="0.979173">
3.5 Overall Result
</subsectionHeader>
<bodyText confidence="0.998593375">
As described in section 2, we use a probabilistic
model to integrate all subtasks. In the probabilis-
tic model, syntactic dependency parsing should
parse top-N candidate results. We do the rest
parsing for each candidate result and get N inte-
grated results. Then, for each integrated result, its
Psent is calculated and the best one is chose as
the final result.
</bodyText>
<subsectionHeader confidence="0.986225">
3.3 Predicate Identification
</subsectionHeader>
<bodyText confidence="0.9922556">
Our predicate identification approach is de-
scribed in section 2.2. We use the gold HEAD
and DEPREL fields to test our approach. The
results are shown in Table 2. The labeling for
each sentence spends about 14ms.
</bodyText>
<table confidence="0.99725425">
dev wsj brown
Precision 93.56 93.61 87.51
Recall 93.24 93.39 89.04
F1 93.40 93.50 88.27
</table>
<tableCaption confidence="0.996693">
Table 2: Predicate identification results
</tableCaption>
<subsectionHeader confidence="0.969513">
3.4 Semantic Dependency Labeling
</subsectionHeader>
<bodyText confidence="0.999011875">
Semantic dependency labeling is the last sub-
task. Our DSRL model uses MaxEnt classifica-
tion to determine the semantic dependency be-
tween each word and its corresponding predicate.
The DSRL results around verbal predicates
and nouns on wsj set are shown in Table 4. It
shows that verbal predicates are labeled much
better than nouns.
</bodyText>
<table confidence="0.9995145">
Unlabeled Labeled Labeled Semantic
Predicate Predicate Arguments
NN* 87.79 79.52 58.09
VB* 96.85 80.25 73.77
</table>
<tableCaption confidence="0.9875875">
Table 4: The F1 values of DSRL around verbal
predicates and nouns on wsj
</tableCaption>
<bodyText confidence="0.998404666666667">
Table 5 shows the overall results with differ-
ent N. The results are improved when N changes
from 1 to 2. However, there is nearly no im-
provement by increasing N from 2 to 3. So N is
set to be 2 in our system. Meanwhile, the effect
of this approach is not obvious. We find that
</bodyText>
<page confidence="0.992586">
266
</page>
<bodyText confidence="0.979147888888889">
there are nearly only one or two different points
between the top-2 candidate dependency parsing
results. This leads to that the DSRL results with
these top-2 candidate results are almost the same.
This is the probable reason that the approach is
not much improved with the increase of N. In the
future it would be necessary for us to consider
the number of different points when finding the
top-N dependency results.
</bodyText>
<table confidence="0.999693666666667">
N=1 N=2 N=3
dev LP 78.58 78.93 79.01
LR 75.58 75.52 75.33
LF1 77.05 77.19 77.13
UP 86.56 86.95 87.07
UR 83.04 82.94 82.75
UF1 84.76 84.90 84.85
wsj LP 79.41 79.76 79.96
LR 76.67 76.59 76.49
LF1 78.02 78.15 78.19
UP 86.59 86.92 87.11
UR 83.40 83.25 83.10
UF1 84.97 85.04 85.06
brown LP 70.52 70.95 70.79
LR 68 67.88 67.54
LF1 69.24 69.38 69.13
UP 81.87 82.39 82.28
UR 78.65 78.47 78.14
UF1 80.23 80.39 80.16
wsj + LP 78.45 78.8 78.96
brown
LR 75.72 75.64 75.5
LF1 77.06 77.18 77.19
UP 86.08 86.43 86.59
UR 82.89 82.73 82.56
UF1 84.45 84.54 84.53
Speed (sec/sent) 0.93 0.94 0.95
</table>
<tableCaption confidence="0.998865">
Table 5: Overall macro scores (Wsem = 0.50)
</tableCaption>
<sectionHeader confidence="0.996562" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999977944444444">
We divide this shared task into four subtasks:
syntactic dependency parsing, predicate identifi-
cation, predicate sense labeling and semantic
dependency labeling. Then, we design a prob-
abilistic model to combine them. The purpose of
our system is to find a global optimal result for
every sentence. If a syntactic dependency parsing
result has the highest probability but it is unrea-
sonable, it would be difficult to get a semantic
parsing result with high probability again. Hence,
a more reasonable result may be found with
lower syntactic dependency parsing probability.
In our system, we have not distinguished be-
tween nouns and verbal predicates. The experi-
mental results show that the results of verbal
predicates are much better than those of nouns.
In the future, it is necessary for us to deal with
them separately.
</bodyText>
<sectionHeader confidence="0.999362" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.613375">
This work was supported by National Natural
Science Foundation of China (No.60573077,
No.60775037), Specialized Research Fund for
the Doctoral Program of Higher Education
(No.2007105), and Program for New Century
Excellent Talents in University (No.NCET-05-
0549).
</reference>
<sectionHeader confidence="0.976629" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999745904761904">
Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
A maximum entropy approach to naturallanguage
processing. Computational Linguistics, 22(1):39–
71.
Che Wanxiang, Ting Liu, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings
of Computational Natural Language Learning
(CoNLL-2005).
Gildea Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Duan Xiangyu, Zhao Jun and Xu Bo. 2007. Probabil-
istic Parsing Action Models for Multi-Lingual De-
pendency Parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007.
Hacioglu K. 2004. Semantic Role Labeling Using
Dependency Trees. In Proceedings of COLING-
2004.
Johansson R. and Nugues P. 2007. Extended Con-
stituent-to-dependency Conversion for English. In
Proceedings of NODALIDA 2007.
Sagae, Kenji and Tsujii, Jun&apos;ichi. 2007. Dependency
Parsing and Domain Adaptation with LR Models
and Parser Ensembles. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL
2007.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Lluís Màrquez, and Joakim Nivre. 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings
of the 12th Conference on Computational Natural
Language Learning (CoNLL-2008).
Tsai Tzong-Han, Chia-Wei Wu, Yu-Chun Lin, and
Wen-Lian Hsu. 2005. Exploiting full parsing in-
formation to label semantic roles using an ensem-
ble of me and svm via integer linear programming.
In Proceedings of Computational Natural Lan-
guage Learning (CoNLL-2005).
</reference>
<page confidence="0.997297">
267
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.216508">
<title confidence="0.99931">Probabilistic Model for Syntactic and Semantic Dependency Parsing</title>
<author confidence="0.573459">Enhong</author>
<affiliation confidence="0.988183">Department of University of ence and Technology</affiliation>
<address confidence="0.999689">China, Hefei, China</address>
<email confidence="0.957067">cheneh@ustc.edu.cn</email>
<author confidence="0.710634">Liu</author>
<affiliation confidence="0.989294333333333">Department of University of ence and Technology</affiliation>
<address confidence="0.997777">China, Hefei, China</address>
<email confidence="0.999883">shiliu@ustc.edu</email>
<author confidence="0.619652">Dawei</author>
<affiliation confidence="0.989392333333333">Department of University of ence and Technology</affiliation>
<address confidence="0.999869">China, Hefei, China</address>
<email confidence="0.991514">dwhu@mail.ustc.edu.cn</email>
<abstract confidence="0.999363117647059">This paper proposes a novel method to analyze syntactic dependencies and label semantic dependencies around both the verbal predicates and the nouns. In this method, a probabilistic model is designed to obtain a global optimal result. Moreover, a predicate identification model and a disambiguation model are proposed to label predicates and their senses. The experimental results obtained on the wsj and brown test sets show that our system obtains 77% of labeled macro F1 score for the whole task, 84.47% of labeled attachment score for syntactic dependency task, and 69.45% of labeled F1 score for semantic dependency task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>CoNLL</author>
</authors>
<date>2008</date>
<booktitle>Proceedings of the 12th Conference on Computational Natural Language Learning,</booktitle>
<pages>263--267</pages>
<location>Manchester,</location>
<contexts>
<context position="1123" citStr="CoNLL 2008" startWordPosition="169" endWordPosition="170">el semantic dependencies around both the verbal predicates and the nouns. In this method, a probabilistic model is designed to obtain a global optimal result. Moreover, a predicate identification model and a disambiguation model are proposed to label predicates and their senses. The experimental results obtained on the wsj and brown test sets show that our system obtains 77% of labeled macro F1 score for the whole task, 84.47% of labeled attachment score for syntactic dependency task, and 69.45% of labeled F1 score for semantic dependency task. 1 Introduction There are two difficulties in the CoNLL 2008 shared task. One is how to label semantic role on a dependency-based representation and how to label verbal predicates and nouns. The other one is how to combine the syntactic task with the semantic task together. On the basis of statistical analysis of labeling results, we optimize the traditional approaches of syntactic dependency parsing and semantic role labeling. Moreover, we design a predicate identification model and a disambiguation model, which will be described in section 2.3, for labeling predicates and their senses. In the disambiguation model, an exhaustion method is used to find</context>
</contexts>
<marker>CoNLL, 2008</marker>
<rawString>CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 263–267 Manchester, August 2008</rawString>
</citation>
<citation valid="false">
<title>This work was supported by National Natural Science Foundation of China</title>
<booktitle>(No.60573077, No.60775037), Specialized Research Fund for the Doctoral Program of Higher Education (No.2007105), and Program</booktitle>
<institution>for New Century Excellent Talents in University (No.NCET-05-0549).</institution>
<marker></marker>
<rawString>This work was supported by National Natural Science Foundation of China (No.60573077, No.60775037), Specialized Research Fund for the Doctoral Program of Higher Education (No.2007105), and Program for New Century Excellent Talents in University (No.NCET-05-0549).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Della Pietra Berger</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to naturallanguage processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>71</pages>
<marker>Berger, Pietra, 1996</marker>
<rawString>Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to naturallanguage processing. Computational Linguistics, 22(1):39– 71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Che Wanxiang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
<author>Yuxuan Hu</author>
<author>Huaijun Liu</author>
</authors>
<title>Semantic role labeling system using maximum entropy classifier.</title>
<date>2005</date>
<booktitle>In Proceedings of Computational Natural Language Learning (CoNLL-2005).</booktitle>
<marker>Wanxiang, Liu, Li, Hu, Liu, 2005</marker>
<rawString>Che Wanxiang, Ting Liu, Sheng Li, Yuxuan Hu, and Huaijun Liu. 2005. Semantic role labeling system using maximum entropy classifier. In Proceedings of Computational Natural Language Learning (CoNLL-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gildea Daniel</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<marker>Daniel, Jurafsky, 2002</marker>
<rawString>Gildea Daniel and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duan Xiangyu</author>
<author>Zhao Jun</author>
<author>Xu Bo</author>
</authors>
<title>Probabilistic Parsing Action Models for Multi-Lingual Dependency Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</booktitle>
<marker>Xiangyu, Jun, Bo, 2007</marker>
<rawString>Duan Xiangyu, Zhao Jun and Xu Bo. 2007. Probabilistic Parsing Action Models for Multi-Lingual Dependency Parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hacioglu</author>
</authors>
<title>Semantic Role Labeling Using Dependency Trees.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING2004.</booktitle>
<marker>Hacioglu, 2004</marker>
<rawString>Hacioglu K. 2004. Semantic Role Labeling Using Dependency Trees. In Proceedings of COLING2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Extended Constituent-to-dependency Conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA</booktitle>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Johansson R. and Nugues P. 2007. Extended Constituent-to-dependency Conversion for English. In Proceedings of NODALIDA 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</booktitle>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Sagae, Kenji and Tsujii, Jun&apos;ichi. 2007. Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMU-CS-99-108.</tech>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical Report CMU-CS-99-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluís Màrquez</author>
<author>Joakim Nivre</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</booktitle>
<marker>Surdeanu, Johansson, Meyers, Màrquez, Nivre, 2008</marker>
<rawString>Surdeanu, Mihai, Richard Johansson, Adam Meyers, Lluís Màrquez, and Joakim Nivre. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsai Tzong-Han</author>
<author>Chia-Wei Wu</author>
<author>Yu-Chun Lin</author>
<author>Wen-Lian Hsu</author>
</authors>
<title>Exploiting full parsing information to label semantic roles using an ensemble of me and svm via integer linear programming.</title>
<date>2005</date>
<booktitle>In Proceedings of Computational Natural Language Learning (CoNLL-2005).</booktitle>
<marker>Tzong-Han, Wu, Lin, Hsu, 2005</marker>
<rawString>Tsai Tzong-Han, Chia-Wei Wu, Yu-Chun Lin, and Wen-Lian Hsu. 2005. Exploiting full parsing information to label semantic roles using an ensemble of me and svm via integer linear programming. In Proceedings of Computational Natural Language Learning (CoNLL-2005).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>