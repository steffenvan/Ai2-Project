<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.041622">
<title confidence="0.9926155">
The University of Maryland Statistical Machine Translation System for
the Fifth Workshop on Machine Translation
</title>
<author confidence="0.979841">
Vladimir Eidelman†, Chris Dyer†‡, and Philip Resnik†‡†UMIACS Laboratory for Computational Linguistics and Information Processing
</author>
<affiliation confidence="0.997565">
‡Department of Linguistics
University of Maryland, College Park
</affiliation>
<email confidence="0.999386">
{vlad,redpony,resnik}@umiacs.umd.edu
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999755625">
This paper describes the system we devel-
oped to improve German-English transla-
tion of News text for the shared task of
the Fifth Workshop on Statistical Machine
Translation. Working within cdec, an
open source modular framework for ma-
chine translation, we explore the benefits
of several modifications to our hierarchical
phrase-based model, including segmenta-
tion lattices, minimum Bayes Risk de-
coding, grammar extraction methods, and
varying language models. Furthermore,
we analyze decoder speed and memory
performance across our set of models and
show there is an important trade-off that
needs to be made.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999890166666667">
For the shared translation task of the Fifth Work-
shop on Machine Translation (WMT10), we par-
ticipated in German to English translation under
the constraint setting. We were especially inter-
ested in translating from German due to set of
challenges it poses for translation. Namely, Ger-
man possesses a rich inflectional morphology, pro-
ductive compounding, and significant word re-
ordering with respect to English. Therefore, we
directed our system design and experimentation
toward addressing these complications and mini-
mizing their negative impact on translation qual-
ity.
The rest of this paper is structured as follows.
After a brief description of the baseline system
in Section 2, we detail the steps taken to improve
upon it in Section 3, followed by experimental re-
sults and analysis of decoder performance metrics.
</bodyText>
<sectionHeader confidence="0.982412" genericHeader="method">
2 Baseline system
</sectionHeader>
<bodyText confidence="0.987240717948718">
As our baseline system, we employ a hierarchical
phrase-based translation model, which is formally
based on the notion of a synchronous context-free
grammar (SCFG) (Chiang, 2007). These gram-
mars contain pairs of CFG rules with aligned non-
terminals, and by introducing these nonterminals
into the grammar, such a system is able to uti-
lize both word and phrase level reordering to cap-
ture the hierarchical structure of language. SCFG
translation models have been shown to be well
suited for German-English translation, as they are
able to both exploit lexical information for and ef-
ficiently compute all possible reorderings using a
CKY-based decoder (Dyer et al., 2009).
Our system is implemented within cdec, an ef-
ficient and modular open source framework for
aligning, training, and decoding with a num-
ber of different translation models, including
SCFGs (Dyer et al., 2010).1 cdec’s modular
framework facilitates seamless integration of a
translation model with different language models,
pruning strategies and inference algorithms. As
input, cdec expects a string, lattice, or context-free
forest, and uses it to generate a hypergraph repre-
sentation, which represents the full translation for-
est without any pruning. The forest can now be
rescored, by intersecting it with a language model
for instance, to obtain output translations. The
above capabilities of cdec allow us to perform the
experiments described below, which would other-
wise be quite cumbersome to carry out in another
system.
The set of features used in our model were the
rule translation relative frequency P(e|f), a target
n-gram language model P(e), a ‘pass-through’
penalty when passing a source language word
to the target side without translating it, lexical
translation probabilities Plex(e|f) and
Plex(f|e),
</bodyText>
<footnote confidence="0.995787">
1http://cdec-decoder.org
</footnote>
<page confidence="0.980459">
72
</page>
<note confidence="0.4572415">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 72–76,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999791727272727">
a count of the number of times that arity-0,1, or 2
SCFG rules were used, a count of the total num-
ber of rules used, a source word penalty, a target
word penalty, the segmentation model cost, and a
count of the number of times the glue rule is used.
The number of non-terminals allowed in a syn-
chronous grammar rule was restricted to two, and
the non-terminal span limit was 12 for non-glue
grammars. The hierarchical phrase-base transla-
tion grammar was extracted using a suffix array
rule extractor (Lopez, 2007).
</bodyText>
<subsectionHeader confidence="0.996542">
2.1 Data preparation
</subsectionHeader>
<bodyText confidence="0.999997961538461">
In order to extract the translation grammar nec-
essary for our model, we used the provided Eu-
roparl and News Commentary parallel training
data. The lowercased and tokenized training data
was then filtered for length and aligned using the
GIZA++ implementation of IBM Model 4 (Och
and Ney, 2003) to obtain one-to-many alignments
in both directions and symmetrized by combining
both into a single alignment using the grow-diag-
final-and method (Koehn et al., 2003). We con-
structed a 5-gram language model using the SRI
language modeling toolkit (Stolcke, 2002) from
the provided English monolingual training data
and the non-Europarl portions of the parallel data
with modified Kneser-Ney smoothing (Chen and
Goodman, 1996). Since the beginnings and ends
of sentences often display unique characteristics
that are not easily captured within the context of
the model, and have previously been demonstrated
to significantly improve performance (Dyer et al.,
2009), we explicitly annotate beginning and end
of sentence markers as part of our translation
process. We used the 2525 sentences in news-
test2009 as our dev set on which we tuned the fea-
ture weights, and report results on the 2489 sen-
tences of the news-test2010 test set.
</bodyText>
<subsectionHeader confidence="0.999217">
2.2 Viterbi envelope semiring training
</subsectionHeader>
<bodyText confidence="0.99996704">
To optimize the feature weights for our model,
we use Viterbi envelope semiring training (VEST),
which is an implementation of the minimum er-
ror rate training (MERT) algorithm (Dyer et al.,
2010; Och, 2003) for training with an arbitrary
loss function. VEST reinterprets MERT within
a semiring framework, which is a useful mathe-
matical abstraction for defining two general oper-
ations, addition (⊕) and multiplication (⊗) over
a set of values. Formally, a semiring is a 5-tuple
(K, ⊕, ⊗, 0, 1), where addition must be commu-
nicative and associative, multiplication must be as-
sociative and must distribute over addition, and an
identity element exists for both. For VEST, hav-
ing K be the set of line segments, ⊕ be the union
of them, and ⊗ be Minkowski addition of the lines
represented as points in the dual plane, allows us
to compute the necessary MERT line search with
the INSIDE algorithm.2 The error function we use
is BLEU (Papineni et al., 2002), and the decoder is
configured to use cube pruning (Huang and Chi-
ang, 2007) with a limit of 100 candidates at each
node. During decoding of the test set, we raise
the cube pruning limit to 1000 candidates at each
node.
</bodyText>
<subsectionHeader confidence="0.997777">
2.3 Compound segmentation lattices
</subsectionHeader>
<bodyText confidence="0.9999091875">
To deal with the aforementioned problem in Ger-
man of productive compounding, where words
are formed by the concatenation of several mor-
phemes and the orthography does not delineate the
morpheme boundaries, we utilize word segmen-
tation lattices. These lattices serve to encode al-
ternative ways of segmenting compound words,
and as such, when presented as the input to the
system allow the decoder to automatically choose
which segmentation is best for translation, leading
to markedly improved results (Dyer, 2009).
In order to construct diverse and accurate seg-
mentation lattices, we built a maximum entropy
model of compound word splitting which makes
use of a small number of dense features, such
as frequency of hypothesized morphemes as sep-
arate units in a monolingual corpus, number of
predicted morphemes, and number of letters in
a predicted morpheme. The feature weights are
tuned to maximize conditional log-likelihood us-
ing a small amount of manually created reference
lattices which encode linguistically plausible seg-
mentations for a selected set of compound words.3
To create lattices for the dev and test sets, a lat-
tice consisting of all possible segmentations for
every word consisting of more than 6 letters was
created, and the paths were weighted by the pos-
terior probability assigned by the segmentation
model. Then, max-marginals were computed us-
ing the forward-backward algorithm and used to
prune out paths that were greater than a factor of
2.3 from the best path, as recommended by Dyer
</bodyText>
<footnote confidence="0.99910775">
2This algorithm is equivalent to the hypergraph MERT al-
gorithm described by Kumar et al. (2009).
3The reference segmentation lattices used for training are
available in the cdec distribution.
</footnote>
<page confidence="0.998478">
73
</page>
<bodyText confidence="0.999891571428572">
(2009).4 To create the translation model for lattice
input, we segmented the training data using the
1-best segmentation predicted by the segmenta-
tion model, and word aligned this with the English
side. This version of the parallel corpus was con-
catenated with the original training parallel cor-
pus.
</bodyText>
<sectionHeader confidence="0.998951" genericHeader="method">
3 Experimental variation
</sectionHeader>
<bodyText confidence="0.99986675">
This section describes the experiments we per-
formed in attempting to assess the challenges
posed by current methods and our exploration of
new ones.
</bodyText>
<subsectionHeader confidence="0.998912">
3.1 Bloom filter language model
</subsectionHeader>
<bodyText confidence="0.999916060606061">
Language models play a crucial role in transla-
tion performance, both in terms of quality, and in
terms of practical aspects such as decoder memory
usage and speed. Unfortunately, these two con-
cerns tend to trade-off one another, as increasing
to a higher-order more complex language model
improves performance, but comes at the cost of
increased size and difficulty in deployment. Ide-
ally, the language model will be loaded into mem-
ory locally by the decoder, but given memory con-
straints, it is entirely possible that the only option
is to resort to a remote language model server that
needs to be queried, thus introducing significant
decoding speed delays.
One possible alternative is a randomized lan-
guage model (RandLM) (Talbot and Osborne,
2007). Using Bloom filters, which are a ran-
domized data structure for set representation, we
can construct language models which signifi-
cantly decrease space requirements, thus becom-
ing amenable to being stored locally in memory,
while only introducing a quantifiable number of
false positives. In order to assess what the im-
pact on translation quality would be, we trained
a system identical to the one described above, ex-
cept using a RandLM. Conveniently, it is possi-
ble to construct a RandLM directly from an exist-
ing SRILM, which is the route we followed in us-
ing the SRILM described in Section 2.1 to create
our RandLM.5 Table 1 shows the comparison of
SRILM and RandLM with respect to performance
on BLEU and TER (Snover et al., 2006) on the test
set.
</bodyText>
<footnote confidence="0.9990965">
4While normally the forward-backward algorithm com-
putes sum-marginals, by changing the addition operator to
max, we can obtain max-marginals.
5Default settings were used for constructing the RandLM.
</footnote>
<table confidence="0.992777">
Language Model BLEU TER
RandLM 22.4 69.1
SRILM 23.1 68.0
</table>
<tableCaption confidence="0.999736">
Table 1: Impact of language model on translation
</tableCaption>
<subsectionHeader confidence="0.998247">
3.2 Minimum Bayes risk decoding
</subsectionHeader>
<bodyText confidence="0.992913125">
During minimum error rate training, the decoder
employs a maximum derivation decision rule.
However, upon exploration of alternative strate-
gies, we have found benefits to using a mini-
mum risk decision rule (Kumar and Byrne, 2004),
wherein we want the translation E of the input F
that has the least expected loss, again as measured
by some loss function L:
</bodyText>
<table confidence="0.490533857142857">
FIP(E|F)[L(E, El)]
E = arg min
E&apos;
� P(E|F)L(E,E&apos;)
= arg min
E&apos;
E
</table>
<bodyText confidence="0.985831357142857">
Using our system, we generate a unique 500-
best list of translations to approximate the poste-
rior distribution P(E|F) and the set of possible
translations. Assuming H(E, F) is the weight of
the decoder’s current path, this can be written as:
P(E|F) a exp αH(E, F)
where α is a free parameter which depends on
the models feature functions and weights as well
as pruning method employed, and thus needs to
be separately empirically optimized on a held out
development set. For this submission, we used
α = 0.5 and BLEU as the loss function. Table 2
shows the results on the test set for MBR decod-
ing.
</bodyText>
<table confidence="0.9989192">
Language Model Decoder BLEU TER
RandLM Max-D 22.4 69.1
MBR 22.7 68.8
SRILM Max-D 23.1 68.0
MBR 23.4 67.7
</table>
<tableCaption confidence="0.9366765">
Table 2: Comparison of maximum derivation ver-
sus MBR decoding
</tableCaption>
<subsectionHeader confidence="0.996663">
3.3 Grammar extraction
</subsectionHeader>
<bodyText confidence="0.999912666666667">
Although the grammars employed in a SCFG
model allow increased expressivity and translation
quality, they do so at the cost of having a large
</bodyText>
<page confidence="0.998199">
74
</page>
<table confidence="0.999751857142857">
Language Model Grammar Decoder Memory (GB) Decoder time (Sec/Sentence)
Local SRILM corpus 14.293 ± 1.228 5.254 ± 3.768
Local SRILM sentence 10.964 ± .964 5.517 ± 3.884
Remote SRILM corpus 3.771 ± .235 15.252 ± 10.878
Remote SRILM sentence .443 ± .235 14.751 ± 10.370
RandLM corpus 7.901 ± .721 9.398 ± 6.965
RandLM sentence 4.612 ± .699 9.561 ± 7.149
</table>
<tableCaption confidence="0.9487965">
Table 3: Decoding memory and speed requirements for language model and grammar extraction varia-
tions
</tableCaption>
<bodyText confidence="0.999936575757576">
number of rules, thus efficiently storing and ac-
cessing grammar rules can become a major prob-
lem. Since a grammar consists of the set of rules
extracted from a parallel corpus containing tens of
millions of words, the resulting number of rules
can be in the millions. Besides storing the whole
grammar locally in memory, other approaches
have been developed, such as suffix arrays, which
lookup and extract rules on the fly from the phrase
table (Lopez, 2007). Thus, the memory require-
ments for decoding have either been for the gram-
mar, when extracted beforehand, or the corpus, for
suffix arrays. In cdec, however, loading grammars
for single sentences from a disk is very fast relative
to decoding time, thus we explore the additional
possibility of having sentence-specific grammars
extracted and loaded on an as-needed basis by the
decoder. This strategy is shown to massively re-
duce the memory footprint of the decoder, while
having no observable impact on decoding speed,
introducing the possibility of more computational
resources for translation. Thus, in addition to the
large corpus grammar extracted in Section 2.1,
we extract sentence-specific grammars for each of
the test sentences. We measure the performance
across using both grammar extraction mechanisms
and the three different language model configu-
rations: local SRILM, remote SRILM, and Ran-
dLM.
As Table 3 shows, there is a marked trade-
off between memory usage and decoding speed.
Using a local SRILM regardless of grammar in-
creases decoding speed by a factor of 3 compared
to the remote SRILM, and approximately a fac-
tor of 2 against the RandLM. However, this speed
comes at the cost of its memory footprint. With a
corpus grammar, the memory footprint of the lo-
cal SRILM is twice as large as the RandLM, and
almost 4 times as large as the remote SRILM. Us-
ing sentence-specific grammars, the difference be-
comes increasingly glaring, as the remote SRILM
memory footprint drops to ≈450MB, a factor of
nearly 24 compared to the local SRILM and a fac-
tor of 10 compared to the process size with the
RandLM. Thus, using the remote SRILM reduces
the memory footprint substantially but at the cost
of significantly slower decoding speed, and con-
versely, using the local SRILM produces increased
decoder speed but introduces a substantial mem-
ory overhead. The RandLM provides a median
between the two extremes: reduced memory and
(relatively) fast decoding at the price of somewhat
decreased translation quality. Since we are using
a relatively large beam of 1000 candidates for de-
coding, the time presented in Table 3 does not rep-
resent an accurate basis for comparison of cdec to
other decoders, which should be done using the
results presented in Dyer et al. (2010).
We also tried one other grammar extraction
configuration, which was with so-called ‘loose’
phrase extraction heuristics, which permit un-
aligned words at the edges of phrases (Ayan and
Dorr, 2006). When decoded using the SRILM and
MBR, this achieved the best performance for our
system, with a BLEU score of 23.6 and TER of
67.7.
</bodyText>
<sectionHeader confidence="0.999492" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999983583333333">
We presented the University of Maryland hier-
archical phrase-based system for the WMT2010
shared translation task. Using cdec, we experi-
mented with a number of methods that are shown
above to lead to improved German-to-English
translation quality over our baseline according to
BLEU and TER evaluation. These include methods
to directly address German morphological com-
plexity, such as appropriate feature functions, seg-
mentation lattices, and a model for automatically
constructing the lattices, as well as alternative de-
coding strategies, such as MBR. We also presented
</bodyText>
<page confidence="0.995957">
75
</page>
<bodyText confidence="0.999935">
several language model configuration alternatives,
as well as grammar extraction methods, and em-
phasized the trade-off that must be made between
decoding time, memory overhead, and translation
quality in current statistical machine translation
systems.
</bodyText>
<sectionHeader confidence="0.999122" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999555">
The authors gratefully acknowledge partial sup-
port from the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-2-001 and NSF award IIS0838801.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the view of the
sponsors.
</bodyText>
<sectionHeader confidence="0.999207" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998028175">
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In Proceedings
of the Joint Conference of the International Com-
mittee on Computational Linguistics and the As-
sociation for Computational Linguistics (COLING-
ACL’2006), pages 9–16, Sydney.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310–318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In Computational Linguistics, volume 33(2),
pages 201–228.
Chris Dyer, Hendra Setiawan, Yuval Marton, and
P. Resnik. 2009. The University of Maryland sta-
tistical machine translation system for the Fourth
Workshop on Machine Translation. In Proceedings
of the EACL-2009 Workshop on Statistical Machine
Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL-HLT.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144–151.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48–54.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL 2004: Main Proceedings.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 163–171.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP,
pages 976–985.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29(21), pages
19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223–231.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Intl. Conf. on Spoken
Language Processing.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, June.
</reference>
<page confidence="0.991819">
76
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.207371">
<title confidence="0.987605">The University of Maryland Statistical Machine Translation System the Fifth Workshop on Machine Translation</title>
<degree confidence="0.3032925">Chris and Philip Laboratory for Computational Linguistics and Information of</degree>
<affiliation confidence="0.864847">University of Maryland, College</affiliation>
<abstract confidence="0.997293352941177">This paper describes the system we developed to improve German-English translation of News text for the shared task of the Fifth Workshop on Statistical Machine Translation. Working within cdec, an open source modular framework for machine translation, we explore the benefits of several modifications to our hierarchical phrase-based model, including segmentation lattices, minimum Bayes Risk decoding, grammar extraction methods, and varying language models. Furthermore, we analyze decoder speed and memory performance across our set of models and show there is an important trade-off that needs to be made.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Going beyond AER: An extensive analysis of word alignments and their impact on MT.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLINGACL’2006),</booktitle>
<pages>9--16</pages>
<location>Sydney.</location>
<contexts>
<context position="15614" citStr="Ayan and Dorr, 2006" startWordPosition="2519" endWordPosition="2522"> overhead. The RandLM provides a median between the two extremes: reduced memory and (relatively) fast decoding at the price of somewhat decreased translation quality. Since we are using a relatively large beam of 1000 candidates for decoding, the time presented in Table 3 does not represent an accurate basis for comparison of cdec to other decoders, which should be done using the results presented in Dyer et al. (2010). We also tried one other grammar extraction configuration, which was with so-called ‘loose’ phrase extraction heuristics, which permit unaligned words at the edges of phrases (Ayan and Dorr, 2006). When decoded using the SRILM and MBR, this achieved the best performance for our system, with a BLEU score of 23.6 and TER of 67.7. 4 Conclusion We presented the University of Maryland hierarchical phrase-based system for the WMT2010 shared translation task. Using cdec, we experimented with a number of methods that are shown above to lead to improved German-to-English translation quality over our baseline according to BLEU and TER evaluation. These include methods to directly address German morphological complexity, such as appropriate feature functions, segmentation lattices, and a model fo</context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going beyond AER: An extensive analysis of word alignments and their impact on MT. In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLINGACL’2006), pages 9–16, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="5072" citStr="Chen and Goodman, 1996" startWordPosition="769" endWordPosition="772">rl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method (Koehn et al., 2003). We constructed a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the provided English monolingual training data and the non-Europarl portions of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, and have previously been demonstrated to significantly improve performance (Dyer et al., 2009), we explicitly annotate beginning and end of sentence markers as part of our translation process. We used the 2525 sentences in newstest2009 as our dev set on which we tuned the feature weights, and report results on the 2489 sentences of the news-test2010 test set. 2.2 Viterbi envelope semiring training To optimize the feature weights for our model, we use V</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>33</volume>
<issue>2</issue>
<pages>201--228</pages>
<contexts>
<context position="1993" citStr="Chiang, 2007" startWordPosition="290" endWordPosition="291">nglish. Therefore, we directed our system design and experimentation toward addressing these complications and minimizing their negative impact on translation quality. The rest of this paper is structured as follows. After a brief description of the baseline system in Section 2, we detail the steps taken to improve upon it in Section 3, followed by experimental results and analysis of decoder performance metrics. 2 Baseline system As our baseline system, we employ a hierarchical phrase-based translation model, which is formally based on the notion of a synchronous context-free grammar (SCFG) (Chiang, 2007). These grammars contain pairs of CFG rules with aligned nonterminals, and by introducing these nonterminals into the grammar, such a system is able to utilize both word and phrase level reordering to capture the hierarchical structure of language. SCFG translation models have been shown to be well suited for German-English translation, as they are able to both exploit lexical information for and efficiently compute all possible reorderings using a CKY-based decoder (Dyer et al., 2009). Our system is implemented within cdec, an efficient and modular open source framework for aligning, training</context>
<context position="6643" citStr="Chiang, 2007" startWordPosition="1035" endWordPosition="1037">ion (⊗) over a set of values. Formally, a semiring is a 5-tuple (K, ⊕, ⊗, 0, 1), where addition must be communicative and associative, multiplication must be associative and must distribute over addition, and an identity element exists for both. For VEST, having K be the set of line segments, ⊕ be the union of them, and ⊗ be Minkowski addition of the lines represented as points in the dual plane, allows us to compute the necessary MERT line search with the INSIDE algorithm.2 The error function we use is BLEU (Papineni et al., 2002), and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. 2.3 Compound segmentation lattices To deal with the aforementioned problem in German of productive compounding, where words are formed by the concatenation of several morphemes and the orthography does not delineate the morpheme boundaries, we utilize word segmentation lattices. These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose w</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. In Computational Linguistics, volume 33(2), pages 201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Hendra Setiawan</author>
<author>Yuval Marton</author>
<author>P Resnik</author>
</authors>
<title>The University of Maryland statistical machine translation system for the Fourth Workshop on Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL-2009 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="2483" citStr="Dyer et al., 2009" startWordPosition="368" endWordPosition="371">hrase-based translation model, which is formally based on the notion of a synchronous context-free grammar (SCFG) (Chiang, 2007). These grammars contain pairs of CFG rules with aligned nonterminals, and by introducing these nonterminals into the grammar, such a system is able to utilize both word and phrase level reordering to capture the hierarchical structure of language. SCFG translation models have been shown to be well suited for German-English translation, as they are able to both exploit lexical information for and efficiently compute all possible reorderings using a CKY-based decoder (Dyer et al., 2009). Our system is implemented within cdec, an efficient and modular open source framework for aligning, training, and decoding with a number of different translation models, including SCFGs (Dyer et al., 2010).1 cdec’s modular framework facilitates seamless integration of a translation model with different language models, pruning strategies and inference algorithms. As input, cdec expects a string, lattice, or context-free forest, and uses it to generate a hypergraph representation, which represents the full translation forest without any pruning. The forest can now be rescored, by intersecting</context>
<context position="5310" citStr="Dyer et al., 2009" startWordPosition="804" endWordPosition="807">ctions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method (Koehn et al., 2003). We constructed a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the provided English monolingual training data and the non-Europarl portions of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, and have previously been demonstrated to significantly improve performance (Dyer et al., 2009), we explicitly annotate beginning and end of sentence markers as part of our translation process. We used the 2525 sentences in newstest2009 as our dev set on which we tuned the feature weights, and report results on the 2489 sentences of the news-test2010 test set. 2.2 Viterbi envelope semiring training To optimize the feature weights for our model, we use Viterbi envelope semiring training (VEST), which is an implementation of the minimum error rate training (MERT) algorithm (Dyer et al., 2010; Och, 2003) for training with an arbitrary loss function. VEST reinterprets MERT within a semiring</context>
</contexts>
<marker>Dyer, Setiawan, Marton, Resnik, 2009</marker>
<rawString>Chris Dyer, Hendra Setiawan, Yuval Marton, and P. Resnik. 2009. The University of Maryland statistical machine translation system for the Fourth Workshop on Machine Translation. In Proceedings of the EACL-2009 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL System Demonstrations.</booktitle>
<contexts>
<context position="2690" citStr="Dyer et al., 2010" startWordPosition="401" endWordPosition="404">ntroducing these nonterminals into the grammar, such a system is able to utilize both word and phrase level reordering to capture the hierarchical structure of language. SCFG translation models have been shown to be well suited for German-English translation, as they are able to both exploit lexical information for and efficiently compute all possible reorderings using a CKY-based decoder (Dyer et al., 2009). Our system is implemented within cdec, an efficient and modular open source framework for aligning, training, and decoding with a number of different translation models, including SCFGs (Dyer et al., 2010).1 cdec’s modular framework facilitates seamless integration of a translation model with different language models, pruning strategies and inference algorithms. As input, cdec expects a string, lattice, or context-free forest, and uses it to generate a hypergraph representation, which represents the full translation forest without any pruning. The forest can now be rescored, by intersecting it with a language model for instance, to obtain output translations. The above capabilities of cdec allow us to perform the experiments described below, which would otherwise be quite cumbersome to carry o</context>
<context position="5811" citStr="Dyer et al., 2010" startWordPosition="889" endWordPosition="892">context of the model, and have previously been demonstrated to significantly improve performance (Dyer et al., 2009), we explicitly annotate beginning and end of sentence markers as part of our translation process. We used the 2525 sentences in newstest2009 as our dev set on which we tuned the feature weights, and report results on the 2489 sentences of the news-test2010 test set. 2.2 Viterbi envelope semiring training To optimize the feature weights for our model, we use Viterbi envelope semiring training (VEST), which is an implementation of the minimum error rate training (MERT) algorithm (Dyer et al., 2010; Och, 2003) for training with an arbitrary loss function. VEST reinterprets MERT within a semiring framework, which is a useful mathematical abstraction for defining two general operations, addition (⊕) and multiplication (⊗) over a set of values. Formally, a semiring is a 5-tuple (K, ⊕, ⊗, 0, 1), where addition must be communicative and associative, multiplication must be associative and must distribute over addition, and an identity element exists for both. For VEST, having K be the set of line segments, ⊕ be the union of them, and ⊗ be Minkowski addition of the lines represented as points </context>
<context position="15417" citStr="Dyer et al. (2010)" startWordPosition="2489" endWordPosition="2492">e memory footprint substantially but at the cost of significantly slower decoding speed, and conversely, using the local SRILM produces increased decoder speed but introduces a substantial memory overhead. The RandLM provides a median between the two extremes: reduced memory and (relatively) fast decoding at the price of somewhat decreased translation quality. Since we are using a relatively large beam of 1000 candidates for decoding, the time presented in Table 3 does not represent an accurate basis for comparison of cdec to other decoders, which should be done using the results presented in Dyer et al. (2010). We also tried one other grammar extraction configuration, which was with so-called ‘loose’ phrase extraction heuristics, which permit unaligned words at the edges of phrases (Ayan and Dorr, 2006). When decoded using the SRILM and MBR, this achieved the best performance for our system, with a BLEU score of 23.6 and TER of 67.7. 4 Conclusion We presented the University of Maryland hierarchical phrase-based system for the WMT2010 shared translation task. Using cdec, we experimented with a number of methods that are shown above to lead to improved German-to-English translation quality over our b</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of ACL System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>Using a maximum entropy model to build segmentation lattices for mt.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="7335" citStr="Dyer, 2009" startWordPosition="1147" endWordPosition="1148"> raise the cube pruning limit to 1000 candidates at each node. 2.3 Compound segmentation lattices To deal with the aforementioned problem in German of productive compounding, where words are formed by the concatenation of several morphemes and the orthography does not delineate the morpheme boundaries, we utilize word segmentation lattices. These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009). In order to construct diverse and accurate segmentation lattices, we built a maximum entropy model of compound word splitting which makes use of a small number of dense features, such as frequency of hypothesized morphemes as separate units in a monolingual corpus, number of predicted morphemes, and number of letters in a predicted morpheme. The feature weights are tuned to maximize conditional log-likelihood using a small amount of manually created reference lattices which encode linguistically plausible segmentations for a selected set of compound words.3 To create lattices for the dev and</context>
</contexts>
<marker>Dyer, 2009</marker>
<rawString>Chris Dyer. 2009. Using a maximum entropy model to build segmentation lattices for mt. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="6643" citStr="Huang and Chiang, 2007" startWordPosition="1033" endWordPosition="1037">ultiplication (⊗) over a set of values. Formally, a semiring is a 5-tuple (K, ⊕, ⊗, 0, 1), where addition must be communicative and associative, multiplication must be associative and must distribute over addition, and an identity element exists for both. For VEST, having K be the set of line segments, ⊕ be the union of them, and ⊗ be Minkowski addition of the lines represented as points in the dual plane, allows us to compute the necessary MERT line search with the INSIDE algorithm.2 The error function we use is BLEU (Papineni et al., 2002), and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. 2.3 Compound segmentation lattices To deal with the aforementioned problem in German of productive compounding, where words are formed by the concatenation of several morphemes and the orthography does not delineate the morpheme boundaries, we utilize word segmentation lattices. These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose w</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="4813" citStr="Koehn et al., 2003" startWordPosition="731" endWordPosition="734"> for non-glue grammars. The hierarchical phrase-base translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). 2.1 Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method (Koehn et al., 2003). We constructed a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the provided English monolingual training data and the non-Europarl portions of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, and have previously been demonstrated to significantly improve performance (Dyer et al., 2009), we explicitly annotate beginning and end of sentence markers as part of our translation process. We u</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Main Proceedings.</booktitle>
<contexts>
<context position="11105" citStr="Kumar and Byrne, 2004" startWordPosition="1756" endWordPosition="1759"> on BLEU and TER (Snover et al., 2006) on the test set. 4While normally the forward-backward algorithm computes sum-marginals, by changing the addition operator to max, we can obtain max-marginals. 5Default settings were used for constructing the RandLM. Language Model BLEU TER RandLM 22.4 69.1 SRILM 23.1 68.0 Table 1: Impact of language model on translation 3.2 Minimum Bayes risk decoding During minimum error rate training, the decoder employs a maximum derivation decision rule. However, upon exploration of alternative strategies, we have found benefits to using a minimum risk decision rule (Kumar and Byrne, 2004), wherein we want the translation E of the input F that has the least expected loss, again as measured by some loss function L: FIP(E|F)[L(E, El)] E = arg min E&apos; � P(E|F)L(E,E&apos;) = arg min E&apos; E Using our system, we generate a unique 500- best list of translations to approximate the posterior distribution P(E|F) and the set of possible translations. Assuming H(E, F) is the weight of the decoder’s current path, this can be written as: P(E|F) a exp αH(E, F) where α is a free parameter which depends on the models feature functions and weights as well as pruning method employed, and thus needs to be</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In HLT-NAACL 2004: Main Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>163--171</pages>
<contexts>
<context position="8429" citStr="Kumar et al. (2009)" startWordPosition="1323" endWordPosition="1326">which encode linguistically plausible segmentations for a selected set of compound words.3 To create lattices for the dev and test sets, a lattice consisting of all possible segmentations for every word consisting of more than 6 letters was created, and the paths were weighted by the posterior probability assigned by the segmentation model. Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer 2This algorithm is equivalent to the hypergraph MERT algorithm described by Kumar et al. (2009). 3The reference segmentation lattices used for training are available in the cdec distribution. 73 (2009).4 To create the translation model for lattice input, we segmented the training data using the 1-best segmentation predicted by the segmentation model, and word aligned this with the English side. This version of the parallel corpus was concatenated with the original training parallel corpus. 3 Experimental variation This section describes the experiments we performed in attempting to assess the challenges posed by current methods and our exploration of new ones. 3.1 Bloom filter language </context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 163–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>976--985</pages>
<contexts>
<context position="4330" citStr="Lopez, 2007" startWordPosition="656" endWordPosition="657">R, pages 72–76, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics a count of the number of times that arity-0,1, or 2 SCFG rules were used, a count of the total number of rules used, a source word penalty, a target word penalty, the segmentation model cost, and a count of the number of times the glue rule is used. The number of non-terminals allowed in a synchronous grammar rule was restricted to two, and the non-terminal span limit was 12 for non-glue grammars. The hierarchical phrase-base translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). 2.1 Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method (Koehn et al., 2003). We constructed a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the provided En</context>
<context position="13144" citStr="Lopez, 2007" startWordPosition="2115" endWordPosition="2116"> ± 6.965 RandLM sentence 4.612 ± .699 9.561 ± 7.149 Table 3: Decoding memory and speed requirements for language model and grammar extraction variations number of rules, thus efficiently storing and accessing grammar rules can become a major problem. Since a grammar consists of the set of rules extracted from a parallel corpus containing tens of millions of words, the resulting number of rules can be in the millions. Besides storing the whole grammar locally in memory, other approaches have been developed, such as suffix arrays, which lookup and extract rules on the fly from the phrase table (Lopez, 2007). Thus, the memory requirements for decoding have either been for the grammar, when extracted beforehand, or the corpus, for suffix arrays. In cdec, however, loading grammars for single sentences from a disk is very fast relative to decoding time, thus we explore the additional possibility of having sentence-specific grammars extracted and loaded on an as-needed basis by the decoder. This strategy is shown to massively reduce the memory footprint of the decoder, while having no observable impact on decoding speed, introducing the possibility of more computational resources for translation. Thu</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In Proceedings of EMNLP, pages 976–985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>29</volume>
<issue>21</issue>
<pages>pages</pages>
<contexts>
<context position="4646" citStr="Och and Ney, 2003" startWordPosition="706" endWordPosition="709">number of times the glue rule is used. The number of non-terminals allowed in a synchronous grammar rule was restricted to two, and the non-terminal span limit was 12 for non-glue grammars. The hierarchical phrase-base translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). 2.1 Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method (Koehn et al., 2003). We constructed a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the provided English monolingual training data and the non-Europarl portions of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, and have previously been demon</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. In Computational Linguistics, volume 29(21), pages 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="5823" citStr="Och, 2003" startWordPosition="893" endWordPosition="894">l, and have previously been demonstrated to significantly improve performance (Dyer et al., 2009), we explicitly annotate beginning and end of sentence markers as part of our translation process. We used the 2525 sentences in newstest2009 as our dev set on which we tuned the feature weights, and report results on the 2489 sentences of the news-test2010 test set. 2.2 Viterbi envelope semiring training To optimize the feature weights for our model, we use Viterbi envelope semiring training (VEST), which is an implementation of the minimum error rate training (MERT) algorithm (Dyer et al., 2010; Och, 2003) for training with an arbitrary loss function. VEST reinterprets MERT within a semiring framework, which is a useful mathematical abstraction for defining two general operations, addition (⊕) and multiplication (⊗) over a set of values. Formally, a semiring is a 5-tuple (K, ⊕, ⊗, 0, 1), where addition must be communicative and associative, multiplication must be associative and must distribute over addition, and an identity element exists for both. For VEST, having K be the set of line segments, ⊕ be the union of them, and ⊗ be Minkowski addition of the lines represented as points in the dual </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="6567" citStr="Papineni et al., 2002" startWordPosition="1020" endWordPosition="1023">matical abstraction for defining two general operations, addition (⊕) and multiplication (⊗) over a set of values. Formally, a semiring is a 5-tuple (K, ⊕, ⊗, 0, 1), where addition must be communicative and associative, multiplication must be associative and must distribute over addition, and an identity element exists for both. For VEST, having K be the set of line segments, ⊕ be the union of them, and ⊗ be Minkowski addition of the lines represented as points in the dual plane, allows us to compute the necessary MERT line search with the INSIDE algorithm.2 The error function we use is BLEU (Papineni et al., 2002), and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. 2.3 Compound segmentation lattices To deal with the aforementioned problem in German of productive compounding, where words are formed by the concatenation of several morphemes and the orthography does not delineate the morpheme boundaries, we utilize word segmentation lattices. These lattices serve to encode alternative ways of segmenting compound words, and as such, when pres</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation. In</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="10521" citStr="Snover et al., 2006" startWordPosition="1666" endWordPosition="1669">ls which significantly decrease space requirements, thus becoming amenable to being stored locally in memory, while only introducing a quantifiable number of false positives. In order to assess what the impact on translation quality would be, we trained a system identical to the one described above, except using a RandLM. Conveniently, it is possible to construct a RandLM directly from an existing SRILM, which is the route we followed in using the SRILM described in Section 2.1 to create our RandLM.5 Table 1 shows the comparison of SRILM and RandLM with respect to performance on BLEU and TER (Snover et al., 2006) on the test set. 4While normally the forward-backward algorithm computes sum-marginals, by changing the addition operator to max, we can obtain max-marginals. 5Default settings were used for constructing the RandLM. Language Model BLEU TER RandLM 22.4 69.1 SRILM 23.1 68.0 Table 1: Impact of language model on translation 3.2 Minimum Bayes risk decoding During minimum error rate training, the decoder employs a maximum derivation decision rule. However, upon exploration of alternative strategies, we have found benefits to using a minimum risk decision rule (Kumar and Byrne, 2004), wherein we wan</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In In Proceedings of Association for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Intl. Conf. on Spoken Language Processing.</booktitle>
<contexts>
<context position="4909" citStr="Stolcke, 2002" startWordPosition="748" endWordPosition="749"> array rule extractor (Lopez, 2007). 2.1 Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method (Koehn et al., 2003). We constructed a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the provided English monolingual training data and the non-Europarl portions of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, and have previously been demonstrated to significantly improve performance (Dyer et al., 2009), we explicitly annotate beginning and end of sentence markers as part of our translation process. We used the 2525 sentences in newstest2009 as our dev set on which we tuned the feature weights, and</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Intl. Conf. on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<contexts>
<context position="9786" citStr="Talbot and Osborne, 2007" startWordPosition="1540" endWordPosition="1543">ch as decoder memory usage and speed. Unfortunately, these two concerns tend to trade-off one another, as increasing to a higher-order more complex language model improves performance, but comes at the cost of increased size and difficulty in deployment. Ideally, the language model will be loaded into memory locally by the decoder, but given memory constraints, it is entirely possible that the only option is to resort to a remote language model server that needs to be queried, thus introducing significant decoding speed delays. One possible alternative is a randomized language model (RandLM) (Talbot and Osborne, 2007). Using Bloom filters, which are a randomized data structure for set representation, we can construct language models which significantly decrease space requirements, thus becoming amenable to being stored locally in memory, while only introducing a quantifiable number of false positives. In order to assess what the impact on translation quality would be, we trained a system identical to the one described above, except using a RandLM. Conveniently, it is possible to construct a RandLM directly from an existing SRILM, which is the route we followed in using the SRILM described in Section 2.1 to</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>