<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.989067">
Scalable Modified Kneser-Ney Language Model Estimation
</title>
<author confidence="0.998342">
Kenneth Heafield*,t Ivan Pouzyrevskyt Jonathan H. Clarkt Philipp Koehn*
</author>
<affiliation confidence="0.998835">
*University of Edinburgh tCarnegie Mellon University tYandex
</affiliation>
<address confidence="0.993747">
10 Crichton Street 5000 Forbes Avenue Zelenograd, bld. 455 fl. 128
Edinburgh EH8 9AB, UK Pittsburgh, PA 15213, USA Moscow 124498, Russia
</address>
<email confidence="0.987">
heafield@cs.cmu.edu ivan.pouzyrevsky@gmail.com jhclark@cs.cmu.edu pkoehn@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.517317" genericHeader="abstract">
Abstract MapReduce Steps Optimized
</sectionHeader>
<bodyText confidence="0.86917848">
Filesystem
Map
Reduce 1
Reduce 2
...
...
We present an efficient algorithm to es-
timate large modified Kneser-Ney mod-
els including interpolation. Streaming
and sorting enables the algorithm to scale
to much larger models by using a fixed
amount of RAM and variable amount of
disk. Using one machine with 140 GB
RAM for 2.8 days, we built an unpruned
model on 126 billion tokens. Machine
translation experiments with this model
show improvement of 0.8 BLEU point
over constrained systems for the 2013
Workshop on Machine Translation task in
three language pairs. Our algorithm is also
faster for small models: we estimated a
model on 302 million tokens using 7.7%
of the RAM and 14.0% of the wall time
taken by SRILM. The code is open source
as part of KenLM.
</bodyText>
<sectionHeader confidence="0.998085" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98860225">
Relatively low perplexity has made modified
Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1998) a popular choice for
language modeling. However, existing estima-
tion methods require either large amounts of RAM
(Stolcke, 2002) or machines (Brants et al., 2007).
As a result, practitioners have chosen to use
less data (Callison-Burch et al., 2012) or simpler
smoothing methods (Brants et al., 2007).
Backoff-smoothed n-gram language models
(Katz, 1987) assign probability to a word wn in
context wn−1
</bodyText>
<equation confidence="0.7420035">
1 according to the recursive equation
i 1) _�Xwn|wi−1), if w n1 was seen
p(wn|w b(wn−1
1 )p(wn|wn 2 ),
</equation>
<bodyText confidence="0.507938">
otherwise
The task is to estimate probability p and backoff
b from text for each seen entry wn1. This paper
</bodyText>
<figure confidence="0.987654857142857">
Filesystem
Map
Reduce 1
Filesystem
Identity Map
Reduce 2
Filesystem
</figure>
<figureCaption confidence="0.998845">
Figure 1: Each MapReduce performs three copies
</figureCaption>
<bodyText confidence="0.973417285714286">
over the network when only one is required. Ar-
rows denote copies over the network (i.e. to and
from a distributed filesystem). Both options use
local disk within each reducer for merge sort.
contributes an efficient multi-pass streaming algo-
rithm using disk and a user-specified amount of
RAM.
</bodyText>
<sectionHeader confidence="0.999801" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999968823529412">
Brants et al. (2007) showed how to estimate
Kneser-Ney models with a series of five MapRe-
duces (Dean and Ghemawat, 2004). On 31 billion
words, estimation took 400 machines for two days.
Recently, Google estimated a pruned Kneser-Ney
model on 230 billion words (Chelba and Schalk-
wyk, 2013), though no cost was provided.
Each MapReduce consists of one layer of map-
pers and an optional layer of reducers. Mappers
read from a network filesystem, perform optional
processing, and route data to reducers. Reducers
process input and write to a network filesystem.
Ideally, reducers would send data directly to an-
other layer of reducers, but this is not supported.
Their workaround, a series of MapReduces, per-
forms unnecessary copies over the network (Fig-
ure 1). In both cases, reducers use local disk.
</bodyText>
<page confidence="0.456765">
690
</page>
<note confidence="0.660034">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690–696,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999047843137255">
Writing and reading from the distributed filesys-
tem improves fault tolerance. However, the same
level of fault tolerance could be achieved by
checkpointing to the network filesystem then only
reading in the case of failures. Doing so would en-
able reducers to start processing without waiting
for the network filesystem to write all the data.
Our code currently runs on a single machine
while MapReduce targets clusters. Appuswamy et
al. (2013) identify several problems with the scale-
out approach of distributed computation and put
forward several scenarios in which a single ma-
chine scale-up approach is more cost effective in
terms of both raw performance and performance
per dollar.
Brants et al. (2007) contributed Stupid Backoff,
a simpler form of smoothing calculated at runtime
from counts. With Stupid Backoff, they scaled to
1.8 trillion tokens. We agree that Stupid Backoff
is cheaper to estimate, but contend that this work
makes Kneser-Ney smoothing cheap enough.
Another advantage of Stupid Backoff has been
that it stores one value, a count, per n-gram in-
stead of probability and backoff. In previous work
(Heafield et al., 2012), we showed how to collapse
probability and backoff into a single value without
changing sentence-level probabilities. However,
local scores do change and, like Stupid Backoff,
are no longer probabilities.
MSRLM (Nguyen et al., 2007) aims to scal-
ably estimate language models on a single ma-
chine. Counting is performed with streaming algo-
rithms similarly to this work. Their parallel merge
sort also has the potential to be faster than ours.
The biggest difference is that their pipeline de-
lays some computation (part of normalization and
all of interpolation) until query time. This means
that it cannot produce a standard ARPA file and
that more time and memory are required at query
time. Moreover, they use memory mapping on en-
tire files and these files may be larger than physi-
cal RAM. We have found that, even with mostly-
sequential access, memory mapping is slower be-
cause the kernel does not explicitly know where
to read ahead or write behind. In contrast, we use
dedicated threads for reading and writing. Perfor-
mance comparisons are omitted because we were
unable to compile and run MSRLM on recent ver-
sions of Linux.
SRILM (Stolcke, 2002) estimates modified
Kneser-Ney models by storing n-grams in RAM.
</bodyText>
<figure confidence="0.843040666666667">
Corpus
Counting
Adjusting Counts
Division
Interpolation
Model
</figure>
<figureCaption confidence="0.845585">
Figure 2: Data flow in the estimation pipeline.
</figureCaption>
<bodyText confidence="0.997796384615385">
Normalization has two threads per order: sum-
ming and division. Thick arrows indicate sorting.
It also offers a disk-based pipeline for initial steps
(i.e. counting). However, the later steps store
all n-grams that survived count pruning in RAM.
Without pruning, both options use the same RAM.
IRSTLM (Federico et al., 2008) does not imple-
ment modified Kneser-Ney but rather an approxi-
mation dubbed “improved Kneser-Ney” (or “mod-
ified shift-beta” depending on the version). Esti-
mation is done in RAM. It can also split the corpus
into pieces and separately build each piece,intro-
ducing further approximation.
</bodyText>
<sectionHeader confidence="0.974946" genericHeader="method">
3 Estimation Pipeline
</sectionHeader>
<bodyText confidence="0.998934">
Estimation has four streaming passes: counting,
adjusting counts, normalization, and interpolation.
Data is sorted between passes, three times in total.
Figure 2 shows the flow of data.
</bodyText>
<subsectionHeader confidence="0.999663">
3.1 Counting
</subsectionHeader>
<bodyText confidence="0.891884235294118">
For a language model of order N, this step counts
all N-grams (with length exactly N) by streaming
through the corpus. Words near the beginning of
sentence also form N-grams padded by the marker
&lt;s&gt; (possibly repeated multiple times). The end
of sentence marker &lt;/s&gt; is appended to each sen-
tence and acts like a normal token.
Unpruned N-gram counts are sufficient, so
lower-order n-grams (n &lt; N) are not counted.
Even pruned models require unpruned N-gram
counts to compute smoothing statistics.
Vocabulary mapping is done with a hash table.1
Token strings are written to disk and a 64-bit Mur-
1This hash table is the only part of the pipeline that can
grow. Users can specify an estimated vocabulary size for
memory budgeting. In future work, we plan to support lo-
cal vocabularies with renumbering.
</bodyText>
<figure confidence="0.999250916666667">
Summing
691
Context
2 1 3
Z A B
B B B
Z B A
Suffix
3 2 1
Z B A
Z A B
B B B
</figure>
<figureCaption confidence="0.986825">
Figure 3: In suffix order, the last word is primary.
</figureCaption>
<bodyText confidence="0.991823">
In context order, the penultimate word is primary.
murHash2 token identifier is retained in RAM.
Counts are combined in a hash table and spilled
to disk when a fixed amount of memory is full.
Merge sort also combines identical N-grams (Bit-
ton and DeWitt, 1983).
</bodyText>
<subsectionHeader confidence="0.999753">
3.2 Adjusting Counts
</subsectionHeader>
<bodyText confidence="0.888381">
The counts c are replaced with adjusted counts a.
(
</bodyText>
<equation confidence="0.914793">
c(wn 1 ), if n = N or w1 = &lt;s&gt;
a(wn 1 ) =
|v : c(vwn1 ) &gt; 0|, otherwise
</equation>
<bodyText confidence="0.9997755">
Adjusted counts are computed by streaming
through N-grams sorted in suffix order (Figure 3).
The algorithm keeps a running total a(wNi ) for
each i and compares consecutive N-grams to de-
cide which adjusted counts to output or increment.
Smoothing statistics are also collected. For each
length n, it collects the number tn,k of n-grams
with adjusted count k E [1, 4].
</bodyText>
<equation confidence="0.767293">
tn,k =|{wn1 : a(wn1) = k}|
</equation>
<bodyText confidence="0.9212725">
These are used to compute closed-form estimates
(Chen and Goodman, 1998) of discounts Dn(k)
</bodyText>
<equation confidence="0.9543522">
Dn(k) = k − (k + 1)tn,1tn,k+1
(tn,1 + 2tn,2)tn,k
for k E [1, 3]. Other cases are Dn(0) = 0 and
Dn(k) = Dn(3) for k ≥ 3. Less formally, counts
0 (unknown) through 2 have special discounts.
</equation>
<subsectionHeader confidence="0.840125">
3.3 Normalization
</subsectionHeader>
<bodyText confidence="0.858708">
Normalization computes pseudo probability u
</bodyText>
<equation confidence="0.903523827586207">
1 ))
u(wn|wn−1
1 ) = a(wn 1 ) − Dn(a(wn
P x a(wn−1
1 x)
and backoff b
P3 i=1 Dn(i)|{x : a(wn−1
1 x) = i}|
b(wn−1
1 ) =
Px a(wn−1
1 x)
2https://code.google.com/p/smhasher/
The difficulty lies in computing denominator
Px a(wn−1
1 x) for all wn−1
1 . For this, we sort in
context order (Figure 3) so that, for every wn−1
1 ,
the entries wn−1
1 x are consecutive. One pass col-
lects both the denominator and backoff3 terms
|{x : a(wn−1
1 x) = i} |for i E [1, 3].
P A problem arises in that denominator
x a(wn−1
1 x) is known only after streaming
through all wn−1
1 x, but is needed immediately
</equation>
<bodyText confidence="0.909315272727273">
to compute each u(wn|wn−1
1 ). One option is to
buffer in memory, taking O(N|vocabulary|) space
since each order is run independently in parallel.
Instead, we use two threads for each order. The
sum thread reads ahead to compute Px a(wn−1
1 x)
and b(wn−1
1 ) then places these in a secondary
stream. The divide thread reads the input and the
secondary stream then writes records of the form
</bodyText>
<equation confidence="0.992140666666667">
(wn 1 ,u(wn|wn−1
1 ),b(wn−1
1 )) (1)
</equation>
<bodyText confidence="0.987249636363636">
The secondary stream is short so that data read by
the sum thread will likely be cached when read by
the divide thread. This sort of optimization is not
possible with most MapReduce implementations.
Because normalization streams through wn−1
1 x
in context order, the backoffs b(wn−1
1 ) are com-
puted in suffix order. This will be useful later
(§3.5), so backoffs are written to secondary files
(one for each order) as bare values without keys.
</bodyText>
<subsectionHeader confidence="0.5421">
3.4 Interpolation
</subsectionHeader>
<bodyText confidence="0.9997232">
Chen and Goodman (1998) found that perplex-
ity improves when the various orders within the
same model are interpolated. The interpolation
step computes final probability p according to the
recursive equation
</bodyText>
<equation confidence="0.9565295">
p(wn|wn−1
1 ) = u(wn|wn−1
1 )+b(wn−1
1 )p(wn|wn−1
2 )
(2)
</equation>
<bodyText confidence="0.871098">
Recursion terminates when unigrams are interpo-
lated with the uniform distribution
</bodyText>
<equation confidence="0.9056715">
p(wn) = u(wn) + b(c) 1
|vocabulary|
</equation>
<bodyText confidence="0.87352875">
where c denotes the empty string. The unknown
word counts as part of the vocabulary and has
count zero,4 so its probability is b(c)/|vocabulary|.
3Sums and counts are done with exact integer arithmetic.
Thus, every floating-point value generated by our toolkit is
the result of O(N) floating-point operations. SRILM has nu-
merical precision issues because it uses O(N|vocabulary|)
floating-point operations to compute backoff.
</bodyText>
<equation confidence="0.7601528">
4SRILM implements “another hack” that computes
pSRILM(wn) = u(wn) and pSRILM(&lt;unk&gt;) = b(e) when-
everp(&lt;unk&gt;) &lt; 3 × 10−6, as it usually is. We implement
both and suspect their motivation was numerical precision.
692
</equation>
<bodyText confidence="0.962617090909091">
Probabilities are computed by streaming in suf-
fix lexicographic order: wn appears before wnn−1,
which in turn appears before wnn−2. In this way,
p(wn) is computed before it is needed to compute
p(wn|wn−1), and so on. This is implemented by
jointly iterating through N streams, one for each
length of n-gram. The relevant pseudo probability
u(wn|wn−1
1 ) and backoff b(wn−1
1 ) appear in the
input records (Equation 1).
</bodyText>
<subsectionHeader confidence="0.969535">
3.5 Joining
</subsectionHeader>
<bodyText confidence="0.993628692307692">
The last task is to unite b(wn1) computed in §3.3
with p(wn|wn−1
1 ) computed in §3.4 for storage in
the model. We note that interpolation (Equation 2)
used the different backoff b(wn−1
1 ) and so b(wn1 )
is not immediately available. However, the back-
off values were saved in suffix order (§3.3) and in-
terpolation produces probabilities in suffix order.
During the same streaming pass as interpolation,
we merge the two streams.5 Suffix order is also
convenient because the popular reverse trie data
structure can be built in the same pass.6
</bodyText>
<sectionHeader confidence="0.993387" genericHeader="method">
4 Sorting
</sectionHeader>
<bodyText confidence="0.999762652173913">
Much work has been done on efficient disk-based
merge sort. Particularly important is arity, the
number of blocks that are merged at once. Low
arity leads to more passes while high arity in-
curs more disk seeks. Abello and Vitter (1999)
modeled these costs and derived an optimal strat-
egy: use fixed-size read buffers (one for each
block being merged) and set arity to the number of
buffers that fit in RAM. The optimal buffer size is
hardware-dependent; we use 64 MB by default. To
overcome the operating system limit on file han-
dles, multiple blocks are stored in the same file.
To further reduce the costs of merge sort, we
implemented pipelining (Dementiev et al., 2008).
If there is enough RAM, input is lazily merged
and streamed to the algorithm. Output is cut into
blocks, sorted in the next step’s desired order, and
then written to disk. These optimizations elim-
inate up to two copies to disk if enough RAM
is available. Input, the algorithm, block sorting,
and output are all threads on a chain of producer-
consumer queues. Therefore, computation and
disk operations happen simultaneously.
</bodyText>
<footnote confidence="0.39301075">
5Backoffs only exist if the n-gram is the context of some
n + 1-gram, so merging skips n-grams that are not contexts.
6With quantization (Whittaker and Raj, 2001), the quan-
tizer is trained in a first pass and applied in a second pass.
</footnote>
<figure confidence="0.989069">
0 200 400 600 800 1000
Tokens (millions)
0 200 400 600 800 1000
Tokens (millions)
</figure>
<figureCaption confidence="0.999691">
Figure 5: CPU usage (system plus user).
</figureCaption>
<bodyText confidence="0.999943285714286">
Each n-gram record is an array of n vocabu-
lary identifiers (4 bytes each) and an 8-byte count
or probability and backoff. At peak, records are
stored twice on disk because lazy merge sort is
not easily amenable to overwriting the input file.
Additional costs are the secondary backoff file (4
bytes per backoff) and the vocabulary in plaintext.
</bodyText>
<sectionHeader confidence="0.999412" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999807">
Experiments use ClueWeb09.7 After spam filter-
ing (Cormack et al., 2011), removing markup, se-
lecting English, splitting sentences (Koehn, 2005),
deduplicating, tokenizing (Koehn et al., 2007),
and truecasing, 126 billion tokens remained.
</bodyText>
<figure confidence="0.979034">
7http://lemurproject.org/clueweb09/
SRI
SRI compact
IRST
This work
</figure>
<figureCaption confidence="0.898099">
Figure 4: Peak virtual memory usage.
</figureCaption>
<figure confidence="0.942286681818182">
CPU time (hours)
14
12
10
4
6
2
0
8
SRI
SRI compact
IRST
This work
RAM (GB) 50
40
30
20
10
0
693
1 2 3 4 5
393 3,775 17,629 39,919 59,794
</figure>
<tableCaption confidence="0.8066445">
Table 1: Counts of unique n-grams (in millions)
for the 5 orders in the large LM.
</tableCaption>
<subsectionHeader confidence="0.992945">
5.1 Estimation Comparison
</subsectionHeader>
<bodyText confidence="0.99998975">
We estimated unpruned language models in bi-
nary format on sentences randomly sampled from
ClueWeb09. SRILM and IRSTLM were run un-
til the test machine ran out of RAM (64 GB).
For our code, the memory limit was set to 3.5
GB because larger limits did not improve perfor-
mance on this small data. Results are in Figures
4 and 5. Our code used an average of 1.34–1.87
CPUs, so wall time is better than suggested in Fig-
ure 5 despite using disk. Other toolkits are single-
threaded. SRILM’s partial disk pipeline is not
shown; it used the same RAM and took more time.
IRSTLM’s splitting approximation took 2.5 times
as much CPU and about one-third the memory (for
a 3-way split) compared with normal IRSTLM.
For 302 million tokens, our toolkit used 25.4%
of SRILM’s CPU time, 14.0% of the wall time,
and 7.7% of the RAM. Compared with IRSTLM,
our toolkit used 16.4% of the CPU time, 9.0% of
the wall time, and 16.6% of the RAM.
</bodyText>
<subsectionHeader confidence="0.999439">
5.2 Scaling
</subsectionHeader>
<bodyText confidence="0.999800238095238">
We built an unpruned model (Table 1) on 126 bil-
lion tokens. Estimation used a machine with 140
GB RAM and six hard drives in a RAID5 configu-
ration (sustained read: 405 MB/s). It took 123 GB
RAM, 2.8 days wall time, and 5.4 CPU days. A
summary of Google’s results from 2007 on differ-
ent data and hardware appears in §2.
We then used this language model as an ad-
ditional feature in unconstrained Czech-English,
French-English, and Spanish-English submissions
to the 2013 Workshop on Machine Translation.8
Our baseline is the University of Edinburgh’s
phrase-based Moses (Koehn et al., 2007) submis-
sion (Durrani et al., 2013), which used all con-
strained data specified by the evaluation (7 billion
tokens of English). It placed first by BLEU (Pap-
ineni et al., 2002) among constrained submissions
in each language pair we consider.
In order to translate, the large model was quan-
tized (Whittaker and Raj, 2001) to 10 bits and
compressed to 643 GB with KenLM (Heafield,
</bodyText>
<footnote confidence="0.742697">
8http://statmt.org/wmt13/
</footnote>
<table confidence="0.84045775">
Source Baseline Large
Czech 27.4 28.2
French 32.6 33.4
Spanish 31.8 32.6
</table>
<tableCaption confidence="0.9745405">
Table 2: Uncased BLEU results from the 2013
Workshop on Machine Translation.
</tableCaption>
<bodyText confidence="0.998985666666667">
2011) then copied to a machine with 1 TB RAM.
Better compression methods (Guthrie and Hepple,
2010; Talbot and Osborne, 2007) and distributed
language models (Brants et al., 2007) could reduce
hardware requirements. Feature weights were re-
tuned with PRO (Hopkins and May, 2011) for
Czech-English and batch MIRA (Cherry and Fos-
ter, 2012) for French-English and Spanish-English
because these worked best for the baseline. Un-
cased BLEU scores on the 2013 test set are shown
in Table 2. The improvement is remarkably con-
sistent at 0.8 BLEU point in each language pair.
</bodyText>
<sectionHeader confidence="0.999141" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999480166666667">
Our open-source (LGPL) estimation code is avail-
able from kheafield.com/code/kenlm/
and should prove useful to the community. Sort-
ing makes it scalable; efficient merge sort makes
it fast. In future work, we plan to extend to the
Common Crawl corpus and improve parallelism.
</bodyText>
<sectionHeader confidence="0.995626" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99765385">
Miles Osborne preprocessed ClueWeb09. Mo-
hammed Mediani contributed to early designs.
Jianfeng Gao clarified how MSRLM operates.
This work used the Extreme Science and Engi-
neering Discovery Environment (XSEDE), which
is supported by National Science Foundation grant
number OCI-1053575. We used Stampede and
Trestles under allocation TG-CCR110017. Sys-
tem administrators from the Texas Advanced
Computing Center (TACC) at The University of
Texas at Austin made configuration changes on
our request. This work made use of the resources
provided by the Edinburgh Compute and Data Fa-
cility (http://www.ecdf.ed.ac.uk/). The
ECDF is partially supported by the eDIKT ini-
tiative (http://www.edikt.org.uk/). The
research leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment 287658 (EU BRIDGE).
</bodyText>
<page confidence="0.886092">
694
</page>
<sectionHeader confidence="0.995037" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99936678504673">
James M. Abello and Jeffrey Scott Vitter, editors.
1999. External memory algorithms. American
Mathematical Society, Boston, MA, USA.
Raja Appuswamy, Christos Gkantsidis, Dushyanth
Narayanan, Orion Hodson, and Antony Rowstron.
2013. Nobody ever got fired for buying a cluster.
Technical Report MSR-TR-2013-2, Microsoft Re-
search.
Dina Bitton and David J DeWitt. 1983. Duplicate
record elimination in large data files. ACM Trans-
actions on database systems (TODS), 8(2):255–265.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Language Learning, pages 858–867, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. Association for
Computational Linguistics.
Ciprian Chelba and Johan Schalkwyk, 2013. Em-
pirical Exploration of Language Modeling for the
google.com Query Stream as Applied to Mobile
Voice Search, pages 197–229. Springer, New York.
Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Harvard
University, August.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427–436. Association for Computational Lin-
guistics.
Gordon V Cormack, Mark D Smucker, and Charles LA
Clarke. 2011. Efficient and effective spam filtering
and re-ranking for large web datasets. Information
retrieval, 14(5):441–465.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapRe-
duce: Simplified data processing on large clusters.
In OSDI’04: Sixth Symposium on Operating Sys-
tem Design and Implementation, San Francisco, CA,
USA, 12.
Roman Dementiev, Lutz Kettner, and Peter Sanders.
2008. STXXL: standard template library for XXL
data sets. Software: Practice and Experience,
38(6):589–637.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh’s machine trans-
lation systems for European language pairs. In Pro-
ceedings of the ACL 2013 Eighth Workshop on Sta-
tistical Machine Translation, Sofia, Bulgaria, Au-
gust.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of Interspeech, Brisbane, Australia.
David Guthrie and Mark Hepple. 2010. Storing the
web in memory: Space efficient language mod-
els with constant time retrieval. In Proceedings of
EMNLP 2010, Los Angeles, CA.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012. Language model rest costs and space-efficient
storage. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, Jeju Island, Korea.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362, Edinburgh, Scotland, July.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, ASSP-35(3):400–
401, March.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, pages
181–184.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), Prague, Czech Repub-
lic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.
Patrick Nguyen, Jianfeng Gao, and Milind Mahajan.
2007. MSRLM: a scalable language modeling
toolkit. Technical Report MSR-TR-2007-144, Mi-
crosoft Research.
</reference>
<page confidence="0.613477">
695
</page>
<reference confidence="0.999833055555556">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
PA, July.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, pages 901–904.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine trans-
lation. In Proceedings of ACL, pages 512–519,
Prague, Czech Republic.
Edward Whittaker and Bhiksha Raj. 2001.
Quantization-based language model compres-
sion. In Proceedings of Eurospeech, pages 33–36,
Aalborg, Denmark, December.
</reference>
<page confidence="0.949099">
696
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.120382">
<title confidence="0.999925">Scalable Modified Kneser-Ney Language Model Estimation</title>
<author confidence="0.842947">H</author>
<affiliation confidence="0.977934">of Edinburgh Mellon University</affiliation>
<address confidence="0.970446">10 Crichton Street 5000 Forbes Avenue Zelenograd, bld. 455 fl. 128 Edinburgh EH8 9AB, UK Pittsburgh, PA 15213, USA Moscow 124498, Russia</address>
<email confidence="0.9768">heafield@cs.cmu.eduivan.pouzyrevsky@gmail.comjhclark@cs.cmu.edupkoehn@inf.ed.ac.uk</email>
<title confidence="0.680449">Steps Optimized Filesystem Map</title>
<abstract confidence="0.955481913043478">Reduce 1 Reduce 2 ... ... We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>M James</author>
</authors>
<title>Abello and Jeffrey Scott Vitter, editors. 1999. External memory algorithms.</title>
<publisher>American Mathematical Society,</publisher>
<location>Boston, MA, USA.</location>
<marker>James, </marker>
<rawString>James M. Abello and Jeffrey Scott Vitter, editors. 1999. External memory algorithms. American Mathematical Society, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raja Appuswamy</author>
<author>Christos Gkantsidis</author>
<author>Dushyanth Narayanan</author>
<author>Orion Hodson</author>
<author>Antony Rowstron</author>
</authors>
<title>Nobody ever got fired for buying a cluster.</title>
<date>2013</date>
<tech>Technical Report MSR-TR-2013-2, Microsoft Research.</tech>
<contexts>
<context position="3799" citStr="Appuswamy et al. (2013)" startWordPosition="595" endWordPosition="598">ings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690–696, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Writing and reading from the distributed filesystem improves fault tolerance. However, the same level of fault tolerance could be achieved by checkpointing to the network filesystem then only reading in the case of failures. Doing so would enable reducers to start processing without waiting for the network filesystem to write all the data. Our code currently runs on a single machine while MapReduce targets clusters. Appuswamy et al. (2013) identify several problems with the scaleout approach of distributed computation and put forward several scenarios in which a single machine scale-up approach is more cost effective in terms of both raw performance and performance per dollar. Brants et al. (2007) contributed Stupid Backoff, a simpler form of smoothing calculated at runtime from counts. With Stupid Backoff, they scaled to 1.8 trillion tokens. We agree that Stupid Backoff is cheaper to estimate, but contend that this work makes Kneser-Ney smoothing cheap enough. Another advantage of Stupid Backoff has been that it stores one val</context>
</contexts>
<marker>Appuswamy, Gkantsidis, Narayanan, Hodson, Rowstron, 2013</marker>
<rawString>Raja Appuswamy, Christos Gkantsidis, Dushyanth Narayanan, Orion Hodson, and Antony Rowstron. 2013. Nobody ever got fired for buying a cluster. Technical Report MSR-TR-2013-2, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dina Bitton</author>
<author>David J DeWitt</author>
</authors>
<title>Duplicate record elimination in large data files.</title>
<date>1983</date>
<journal>ACM Transactions on database systems (TODS),</journal>
<volume>8</volume>
<issue>2</issue>
<contexts>
<context position="7839" citStr="Bitton and DeWitt, 1983" startWordPosition="1267" endWordPosition="1271">itten to disk and a 64-bit Mur1This hash table is the only part of the pipeline that can grow. Users can specify an estimated vocabulary size for memory budgeting. In future work, we plan to support local vocabularies with renumbering. Summing 691 Context 2 1 3 Z A B B B B Z B A Suffix 3 2 1 Z B A Z A B B B B Figure 3: In suffix order, the last word is primary. In context order, the penultimate word is primary. murHash2 token identifier is retained in RAM. Counts are combined in a hash table and spilled to disk when a fixed amount of memory is full. Merge sort also combines identical N-grams (Bitton and DeWitt, 1983). 3.2 Adjusting Counts The counts c are replaced with adjusted counts a. ( c(wn 1 ), if n = N or w1 = &lt;s&gt; a(wn 1 ) = |v : c(vwn1 ) &gt; 0|, otherwise Adjusted counts are computed by streaming through N-grams sorted in suffix order (Figure 3). The algorithm keeps a running total a(wNi ) for each i and compares consecutive N-grams to decide which adjusted counts to output or increment. Smoothing statistics are also collected. For each length n, it collects the number tn,k of n-grams with adjusted count k E [1, 4]. tn,k =|{wn1 : a(wn1) = k}| These are used to compute closed-form estimates (Chen and </context>
</contexts>
<marker>Bitton, DeWitt, 1983</marker>
<rawString>Dina Bitton and David J DeWitt. 1983. Duplicate record elimination in large data files. ACM Transactions on database systems (TODS), 8(2):255–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="1493" citStr="Brants et al., 2007" startWordPosition="223" endWordPosition="226">how improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM. 1 Introduction Relatively low perplexity has made modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) a popular choice for language modeling. However, existing estimation methods require either large amounts of RAM (Stolcke, 2002) or machines (Brants et al., 2007). As a result, practitioners have chosen to use less data (Callison-Burch et al., 2012) or simpler smoothing methods (Brants et al., 2007). Backoff-smoothed n-gram language models (Katz, 1987) assign probability to a word wn in context wn−1 1 according to the recursive equation i 1) _�Xwn|wi−1), if w n1 was seen p(wn|w b(wn−1 1 )p(wn|wn 2 ), otherwise The task is to estimate probability p and backoff b from text for each seen entry wn1. This paper Filesystem Map Reduce 1 Filesystem Identity Map Reduce 2 Filesystem Figure 1: Each MapReduce performs three copies over the network when only one is</context>
<context position="4062" citStr="Brants et al. (2007)" startWordPosition="637" endWordPosition="640"> the same level of fault tolerance could be achieved by checkpointing to the network filesystem then only reading in the case of failures. Doing so would enable reducers to start processing without waiting for the network filesystem to write all the data. Our code currently runs on a single machine while MapReduce targets clusters. Appuswamy et al. (2013) identify several problems with the scaleout approach of distributed computation and put forward several scenarios in which a single machine scale-up approach is more cost effective in terms of both raw performance and performance per dollar. Brants et al. (2007) contributed Stupid Backoff, a simpler form of smoothing calculated at runtime from counts. With Stupid Backoff, they scaled to 1.8 trillion tokens. We agree that Stupid Backoff is cheaper to estimate, but contend that this work makes Kneser-Ney smoothing cheap enough. Another advantage of Stupid Backoff has been that it stores one value, a count, per n-gram instead of probability and backoff. In previous work (Heafield et al., 2012), we showed how to collapse probability and backoff into a single value without changing sentence-level probabilities. However, local scores do change and, like St</context>
<context position="16863" citStr="Brants et al., 2007" startWordPosition="2839" endWordPosition="2842">s of English). It placed first by BLEU (Papineni et al., 2002) among constrained submissions in each language pair we consider. In order to translate, the large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8http://statmt.org/wmt13/ Source Baseline Large Czech 27.4 28.2 French 32.6 33.4 Spanish 31.8 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the community. Sorting makes it scalable; efficient merge sort makes it fast. In future wor</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, pages 858–867, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1580" citStr="Callison-Burch et al., 2012" startWordPosition="237" endWordPosition="240">p on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM. 1 Introduction Relatively low perplexity has made modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) a popular choice for language modeling. However, existing estimation methods require either large amounts of RAM (Stolcke, 2002) or machines (Brants et al., 2007). As a result, practitioners have chosen to use less data (Callison-Burch et al., 2012) or simpler smoothing methods (Brants et al., 2007). Backoff-smoothed n-gram language models (Katz, 1987) assign probability to a word wn in context wn−1 1 according to the recursive equation i 1) _�Xwn|wi−1), if w n1 was seen p(wn|w b(wn−1 1 )p(wn|wn 2 ), otherwise The task is to estimate probability p and backoff b from text for each seen entry wn1. This paper Filesystem Map Reduce 1 Filesystem Identity Map Reduce 2 Filesystem Figure 1: Each MapReduce performs three copies over the network when only one is required. Arrows denote copies over the network (i.e. to and from a distributed filesy</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Johan Schalkwyk</author>
</authors>
<title>Empirical Exploration of Language Modeling for the google.com Query Stream as Applied to Mobile Voice Search,</title>
<date>2013</date>
<pages>197--229</pages>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="2657" citStr="Chelba and Schalkwyk, 2013" startWordPosition="415" endWordPosition="419">pReduce performs three copies over the network when only one is required. Arrows denote copies over the network (i.e. to and from a distributed filesystem). Both options use local disk within each reducer for merge sort. contributes an efficient multi-pass streaming algorithm using disk and a user-specified amount of RAM. 2 Related Work Brants et al. (2007) showed how to estimate Kneser-Ney models with a series of five MapReduces (Dean and Ghemawat, 2004). On 31 billion words, estimation took 400 machines for two days. Recently, Google estimated a pruned Kneser-Ney model on 230 billion words (Chelba and Schalkwyk, 2013), though no cost was provided. Each MapReduce consists of one layer of mappers and an optional layer of reducers. Mappers read from a network filesystem, perform optional processing, and route data to reducers. Reducers process input and write to a network filesystem. Ideally, reducers would send data directly to another layer of reducers, but this is not supported. Their workaround, a series of MapReduces, performs unnecessary copies over the network (Figure 1). In both cases, reducers use local disk. 690 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</context>
</contexts>
<marker>Chelba, Schalkwyk, 2013</marker>
<rawString>Ciprian Chelba and Johan Schalkwyk, 2013. Empirical Exploration of Language Modeling for the google.com Query Stream as Applied to Mobile Voice Search, pages 197–229. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University,</institution>
<contexts>
<context position="1330" citStr="Chen and Goodman, 1998" startWordPosition="198" endWordPosition="201">le amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM. 1 Introduction Relatively low perplexity has made modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) a popular choice for language modeling. However, existing estimation methods require either large amounts of RAM (Stolcke, 2002) or machines (Brants et al., 2007). As a result, practitioners have chosen to use less data (Callison-Burch et al., 2012) or simpler smoothing methods (Brants et al., 2007). Backoff-smoothed n-gram language models (Katz, 1987) assign probability to a word wn in context wn−1 1 according to the recursive equation i 1) _�Xwn|wi−1), if w n1 was seen p(wn|w b(wn−1 1 )p(wn|wn 2 ), otherwise The task is to estimate probability p and backoff b from text for each seen entry w</context>
<context position="8453" citStr="Chen and Goodman, 1998" startWordPosition="1382" endWordPosition="1385">tt, 1983). 3.2 Adjusting Counts The counts c are replaced with adjusted counts a. ( c(wn 1 ), if n = N or w1 = &lt;s&gt; a(wn 1 ) = |v : c(vwn1 ) &gt; 0|, otherwise Adjusted counts are computed by streaming through N-grams sorted in suffix order (Figure 3). The algorithm keeps a running total a(wNi ) for each i and compares consecutive N-grams to decide which adjusted counts to output or increment. Smoothing statistics are also collected. For each length n, it collects the number tn,k of n-grams with adjusted count k E [1, 4]. tn,k =|{wn1 : a(wn1) = k}| These are used to compute closed-form estimates (Chen and Goodman, 1998) of discounts Dn(k) Dn(k) = k − (k + 1)tn,1tn,k+1 (tn,1 + 2tn,2)tn,k for k E [1, 3]. Other cases are Dn(0) = 0 and Dn(k) = Dn(3) for k ≥ 3. Less formally, counts 0 (unknown) through 2 have special discounts. 3.3 Normalization Normalization computes pseudo probability u 1 )) u(wn|wn−1 1 ) = a(wn 1 ) − Dn(a(wn P x a(wn−1 1 x) and backoff b P3 i=1 Dn(i)|{x : a(wn−1 1 x) = i}| b(wn−1 1 ) = Px a(wn−1 1 x) 2https://code.google.com/p/smhasher/ The difficulty lies in computing denominator Px a(wn−1 1 x) for all wn−1 1 . For this, we sort in context order (Figure 3) so that, for every wn−1 1 , the entr</context>
<context position="10226" citStr="Chen and Goodman (1998)" startWordPosition="1714" endWordPosition="1717"> stream. The divide thread reads the input and the secondary stream then writes records of the form (wn 1 ,u(wn|wn−1 1 ),b(wn−1 1 )) (1) The secondary stream is short so that data read by the sum thread will likely be cached when read by the divide thread. This sort of optimization is not possible with most MapReduce implementations. Because normalization streams through wn−1 1 x in context order, the backoffs b(wn−1 1 ) are computed in suffix order. This will be useful later (§3.5), so backoffs are written to secondary files (one for each order) as bare values without keys. 3.4 Interpolation Chen and Goodman (1998) found that perplexity improves when the various orders within the same model are interpolated. The interpolation step computes final probability p according to the recursive equation p(wn|wn−1 1 ) = u(wn|wn−1 1 )+b(wn−1 1 )p(wn|wn−1 2 ) (2) Recursion terminates when unigrams are interpolated with the uniform distribution p(wn) = u(wn) + b(c) 1 |vocabulary| where c denotes the empty string. The unknown word counts as part of the vocabulary and has count zero,4 so its probability is b(c)/|vocabulary|. 3Sums and counts are done with exact integer arithmetic. Thus, every floating-point value gene</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>427--436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17020" citStr="Cherry and Foster, 2012" startWordPosition="2863" endWordPosition="2867">e large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8http://statmt.org/wmt13/ Source Baseline Large Czech 27.4 28.2 French 32.6 33.4 Spanish 31.8 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the community. Sorting makes it scalable; efficient merge sort makes it fast. In future work, we plan to extend to the Common Crawl corpus and improve parallelism. Acknowledgements Miles Osborne preprocessed ClueWeb09. Mohammed Mediani contributed </context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gordon V Cormack</author>
<author>Mark D Smucker</author>
<author>Charles LA Clarke</author>
</authors>
<title>Efficient and effective spam filtering and re-ranking for large web datasets. Information retrieval,</title>
<date>2011</date>
<pages>14--5</pages>
<contexts>
<context position="14096" citStr="Cormack et al., 2011" startWordPosition="2360" endWordPosition="2363">e quantizer is trained in a first pass and applied in a second pass. 0 200 400 600 800 1000 Tokens (millions) 0 200 400 600 800 1000 Tokens (millions) Figure 5: CPU usage (system plus user). Each n-gram record is an array of n vocabulary identifiers (4 bytes each) and an 8-byte count or probability and backoff. At peak, records are stored twice on disk because lazy merge sort is not easily amenable to overwriting the input file. Additional costs are the secondary backoff file (4 bytes per backoff) and the vocabulary in plaintext. 5 Experiments Experiments use ClueWeb09.7 After spam filtering (Cormack et al., 2011), removing markup, selecting English, splitting sentences (Koehn, 2005), deduplicating, tokenizing (Koehn et al., 2007), and truecasing, 126 billion tokens remained. 7http://lemurproject.org/clueweb09/ SRI SRI compact IRST This work Figure 4: Peak virtual memory usage. CPU time (hours) 14 12 10 4 6 2 0 8 SRI SRI compact IRST This work RAM (GB) 50 40 30 20 10 0 693 1 2 3 4 5 393 3,775 17,629 39,919 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison We estimated unpruned language models in binary format on sentences randomly sampled</context>
</contexts>
<marker>Cormack, Smucker, Clarke, 2011</marker>
<rawString>Gordon V Cormack, Mark D Smucker, and Charles LA Clarke. 2011. Efficient and effective spam filtering and re-ranking for large web datasets. Information retrieval, 14(5):441–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>MapReduce: Simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>In OSDI’04: Sixth Symposium on Operating System Design and Implementation,</booktitle>
<pages>12</pages>
<location>San Francisco, CA, USA,</location>
<contexts>
<context position="2489" citStr="Dean and Ghemawat, 2004" startWordPosition="389" endWordPosition="392">imate probability p and backoff b from text for each seen entry wn1. This paper Filesystem Map Reduce 1 Filesystem Identity Map Reduce 2 Filesystem Figure 1: Each MapReduce performs three copies over the network when only one is required. Arrows denote copies over the network (i.e. to and from a distributed filesystem). Both options use local disk within each reducer for merge sort. contributes an efficient multi-pass streaming algorithm using disk and a user-specified amount of RAM. 2 Related Work Brants et al. (2007) showed how to estimate Kneser-Ney models with a series of five MapReduces (Dean and Ghemawat, 2004). On 31 billion words, estimation took 400 machines for two days. Recently, Google estimated a pruned Kneser-Ney model on 230 billion words (Chelba and Schalkwyk, 2013), though no cost was provided. Each MapReduce consists of one layer of mappers and an optional layer of reducers. Mappers read from a network filesystem, perform optional processing, and route data to reducers. Reducers process input and write to a network filesystem. Ideally, reducers would send data directly to another layer of reducers, but this is not supported. Their workaround, a series of MapReduces, performs unnecessary </context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: Simplified data processing on large clusters. In OSDI’04: Sixth Symposium on Operating System Design and Implementation, San Francisco, CA, USA, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Dementiev</author>
<author>Lutz Kettner</author>
<author>Peter Sanders</author>
</authors>
<title>STXXL: standard template library for XXL data sets.</title>
<date>2008</date>
<journal>Software: Practice and Experience,</journal>
<volume>38</volume>
<issue>6</issue>
<contexts>
<context position="12883" citStr="Dementiev et al., 2008" startWordPosition="2152" endWordPosition="2155">rge sort. Particularly important is arity, the number of blocks that are merged at once. Low arity leads to more passes while high arity incurs more disk seeks. Abello and Vitter (1999) modeled these costs and derived an optimal strategy: use fixed-size read buffers (one for each block being merged) and set arity to the number of buffers that fit in RAM. The optimal buffer size is hardware-dependent; we use 64 MB by default. To overcome the operating system limit on file handles, multiple blocks are stored in the same file. To further reduce the costs of merge sort, we implemented pipelining (Dementiev et al., 2008). If there is enough RAM, input is lazily merged and streamed to the algorithm. Output is cut into blocks, sorted in the next step’s desired order, and then written to disk. These optimizations eliminate up to two copies to disk if enough RAM is available. Input, the algorithm, block sorting, and output are all threads on a chain of producerconsumer queues. Therefore, computation and disk operations happen simultaneously. 5Backoffs only exist if the n-gram is the context of some n + 1-gram, so merging skips n-grams that are not contexts. 6With quantization (Whittaker and Raj, 2001), the quanti</context>
</contexts>
<marker>Dementiev, Kettner, Sanders, 2008</marker>
<rawString>Roman Dementiev, Lutz Kettner, and Peter Sanders. 2008. STXXL: standard template library for XXL data sets. Software: Practice and Experience, 38(6):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Barry Haddow</author>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s machine translation systems for European language pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL 2013 Eighth Workshop on Statistical Machine Translation,</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="16165" citStr="Durrani et al., 2013" startWordPosition="2726" endWordPosition="2729">e built an unpruned model (Table 1) on 126 billion tokens. Estimation used a machine with 140 GB RAM and six hard drives in a RAID5 configuration (sustained read: 405 MB/s). It took 123 GB RAM, 2.8 days wall time, and 5.4 CPU days. A summary of Google’s results from 2007 on different data and hardware appears in §2. We then used this language model as an additional feature in unconstrained Czech-English, French-English, and Spanish-English submissions to the 2013 Workshop on Machine Translation.8 Our baseline is the University of Edinburgh’s phrase-based Moses (Koehn et al., 2007) submission (Durrani et al., 2013), which used all constrained data specified by the evaluation (7 billion tokens of English). It placed first by BLEU (Papineni et al., 2002) among constrained submissions in each language pair we consider. In order to translate, the large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8http://statmt.org/wmt13/ Source Baseline Large Czech 27.4 28.2 French 32.6 33.4 Spanish 31.8 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie</context>
</contexts>
<marker>Durrani, Haddow, Heafield, Koehn, 2013</marker>
<rawString>Nadir Durrani, Barry Haddow, Kenneth Heafield, and Philipp Koehn. 2013. Edinburgh’s machine translation systems for European language pairs. In Proceedings of the ACL 2013 Eighth Workshop on Statistical Machine Translation, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>IRSTLM: an open source toolkit for handling large scale language models.</title>
<date>2008</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<location>Brisbane, Australia.</location>
<contexts>
<context position="6146" citStr="Federico et al., 2008" startWordPosition="974" endWordPosition="977">e comparisons are omitted because we were unable to compile and run MSRLM on recent versions of Linux. SRILM (Stolcke, 2002) estimates modified Kneser-Ney models by storing n-grams in RAM. Corpus Counting Adjusting Counts Division Interpolation Model Figure 2: Data flow in the estimation pipeline. Normalization has two threads per order: summing and division. Thick arrows indicate sorting. It also offers a disk-based pipeline for initial steps (i.e. counting). However, the later steps store all n-grams that survived count pruning in RAM. Without pruning, both options use the same RAM. IRSTLM (Federico et al., 2008) does not implement modified Kneser-Ney but rather an approximation dubbed “improved Kneser-Ney” (or “modified shift-beta” depending on the version). Estimation is done in RAM. It can also split the corpus into pieces and separately build each piece,introducing further approximation. 3 Estimation Pipeline Estimation has four streaming passes: counting, adjusting counts, normalization, and interpolation. Data is sorted between passes, three times in total. Figure 2 shows the flow of data. 3.1 Counting For a language model of order N, this step counts all N-grams (with length exactly N) by strea</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an open source toolkit for handling large scale language models. In Proceedings of Interspeech, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Guthrie</author>
<author>Mark Hepple</author>
</authors>
<title>Storing the web in memory: Space efficient language models with constant time retrieval.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP 2010,</booktitle>
<location>Los Angeles, CA.</location>
<contexts>
<context position="16782" citStr="Guthrie and Hepple, 2010" startWordPosition="2827" endWordPosition="2830">, 2013), which used all constrained data specified by the evaluation (7 billion tokens of English). It placed first by BLEU (Papineni et al., 2002) among constrained submissions in each language pair we consider. In order to translate, the large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8http://statmt.org/wmt13/ Source Baseline Large Czech 27.4 28.2 French 32.6 33.4 Spanish 31.8 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the commun</context>
</contexts>
<marker>Guthrie, Hepple, 2010</marker>
<rawString>David Guthrie and Mark Hepple. 2010. Storing the web in memory: Space efficient language models with constant time retrieval. In Proceedings of EMNLP 2010, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
<author>Alon Lavie</author>
</authors>
<title>Language model rest costs and space-efficient storage.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="4499" citStr="Heafield et al., 2012" startWordPosition="707" endWordPosition="710">on and put forward several scenarios in which a single machine scale-up approach is more cost effective in terms of both raw performance and performance per dollar. Brants et al. (2007) contributed Stupid Backoff, a simpler form of smoothing calculated at runtime from counts. With Stupid Backoff, they scaled to 1.8 trillion tokens. We agree that Stupid Backoff is cheaper to estimate, but contend that this work makes Kneser-Ney smoothing cheap enough. Another advantage of Stupid Backoff has been that it stores one value, a count, per n-gram instead of probability and backoff. In previous work (Heafield et al., 2012), we showed how to collapse probability and backoff into a single value without changing sentence-level probabilities. However, local scores do change and, like Stupid Backoff, are no longer probabilities. MSRLM (Nguyen et al., 2007) aims to scalably estimate language models on a single machine. Counting is performed with streaming algorithms similarly to this work. Their parallel merge sort also has the potential to be faster than ours. The biggest difference is that their pipeline delays some computation (part of normalization and all of interpolation) until query time. This means that it ca</context>
</contexts>
<marker>Heafield, Koehn, Lavie, 2012</marker>
<rawString>Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012. Language model rest costs and space-efficient storage. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, UK,</location>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, UK, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="16961" citStr="Hopkins and May, 2011" startWordPosition="2854" endWordPosition="2857">each language pair we consider. In order to translate, the large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8http://statmt.org/wmt13/ Source Baseline Large Czech 27.4 28.2 French 32.6 33.4 Spanish 31.8 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the community. Sorting makes it scalable; efficient merge sort makes it fast. In future work, we plan to extend to the Common Crawl corpus and improve parallelism. Acknowledgements Miles Os</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<pages>401</pages>
<contexts>
<context position="1685" citStr="Katz, 1987" startWordPosition="253" endWordPosition="254"> on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM. 1 Introduction Relatively low perplexity has made modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) a popular choice for language modeling. However, existing estimation methods require either large amounts of RAM (Stolcke, 2002) or machines (Brants et al., 2007). As a result, practitioners have chosen to use less data (Callison-Burch et al., 2012) or simpler smoothing methods (Brants et al., 2007). Backoff-smoothed n-gram language models (Katz, 1987) assign probability to a word wn in context wn−1 1 according to the recursive equation i 1) _�Xwn|wi−1), if w n1 was seen p(wn|w b(wn−1 1 )p(wn|wn 2 ), otherwise The task is to estimate probability p and backoff b from text for each seen entry wn1. This paper Filesystem Map Reduce 1 Filesystem Identity Map Reduce 2 Filesystem Figure 1: Each MapReduce performs three copies over the network when only one is required. Arrows denote copies over the network (i.e. to and from a distributed filesystem). Both options use local disk within each reducer for merge sort. contributes an efficient multi-pas</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, ASSP-35(3):400– 401, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="1305" citStr="Kneser and Ney, 1995" startWordPosition="194" endWordPosition="197">ount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM. 1 Introduction Relatively low perplexity has made modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) a popular choice for language modeling. However, existing estimation methods require either large amounts of RAM (Stolcke, 2002) or machines (Brants et al., 2007). As a result, practitioners have chosen to use less data (Callison-Burch et al., 2012) or simpler smoothing methods (Brants et al., 2007). Backoff-smoothed n-gram language models (Katz, 1987) assign probability to a word wn in context wn−1 1 according to the recursive equation i 1) _�Xwn|wi−1), if w n1 was seen p(wn|w b(wn−1 1 )p(wn|wn 2 ), otherwise The task is to estimate probability p and backoff b from t</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra</location>
<contexts>
<context position="14215" citStr="Koehn et al., 2007" startWordPosition="2375" endWordPosition="2378">00 800 1000 Tokens (millions) Figure 5: CPU usage (system plus user). Each n-gram record is an array of n vocabulary identifiers (4 bytes each) and an 8-byte count or probability and backoff. At peak, records are stored twice on disk because lazy merge sort is not easily amenable to overwriting the input file. Additional costs are the secondary backoff file (4 bytes per backoff) and the vocabulary in plaintext. 5 Experiments Experiments use ClueWeb09.7 After spam filtering (Cormack et al., 2011), removing markup, selecting English, splitting sentences (Koehn, 2005), deduplicating, tokenizing (Koehn et al., 2007), and truecasing, 126 billion tokens remained. 7http://lemurproject.org/clueweb09/ SRI SRI compact IRST This work Figure 4: Peak virtual memory usage. CPU time (hours) 14 12 10 4 6 2 0 8 SRI SRI compact IRST This work RAM (GB) 50 40 30 20 10 0 693 1 2 3 4 5 393 3,775 17,629 39,919 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison We estimated unpruned language models in binary format on sentences randomly sampled from ClueWeb09. SRILM and IRSTLM were run until the test machine ran out of RAM (64 GB). For our code, the memory limi</context>
<context position="16131" citStr="Koehn et al., 2007" startWordPosition="2720" endWordPosition="2723"> 16.6% of the RAM. 5.2 Scaling We built an unpruned model (Table 1) on 126 billion tokens. Estimation used a machine with 140 GB RAM and six hard drives in a RAID5 configuration (sustained read: 405 MB/s). It took 123 GB RAM, 2.8 days wall time, and 5.4 CPU days. A summary of Google’s results from 2007 on different data and hardware appears in §2. We then used this language model as an additional feature in unconstrained Czech-English, French-English, and Spanish-English submissions to the 2013 Workshop on Machine Translation.8 Our baseline is the University of Edinburgh’s phrase-based Moses (Koehn et al., 2007) submission (Durrani et al., 2013), which used all constrained data specified by the evaluation (7 billion tokens of English). It placed first by BLEU (Papineni et al., 2002) among constrained submissions in each language pair we consider. In order to translate, the large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8http://statmt.org/wmt13/ Source Baseline Large Czech 27.4 28.2 French 32.6 33.4 Spanish 31.8 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. 2011) then copied to a machine with 1 TB RAM. B</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<contexts>
<context position="14167" citStr="Koehn, 2005" startWordPosition="2371" endWordPosition="2372">00 800 1000 Tokens (millions) 0 200 400 600 800 1000 Tokens (millions) Figure 5: CPU usage (system plus user). Each n-gram record is an array of n vocabulary identifiers (4 bytes each) and an 8-byte count or probability and backoff. At peak, records are stored twice on disk because lazy merge sort is not easily amenable to overwriting the input file. Additional costs are the secondary backoff file (4 bytes per backoff) and the vocabulary in plaintext. 5 Experiments Experiments use ClueWeb09.7 After spam filtering (Cormack et al., 2011), removing markup, selecting English, splitting sentences (Koehn, 2005), deduplicating, tokenizing (Koehn et al., 2007), and truecasing, 126 billion tokens remained. 7http://lemurproject.org/clueweb09/ SRI SRI compact IRST This work Figure 4: Peak virtual memory usage. CPU time (hours) 14 12 10 4 6 2 0 8 SRI SRI compact IRST This work RAM (GB) 50 40 30 20 10 0 693 1 2 3 4 5 393 3,775 17,629 39,919 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison We estimated unpruned language models in binary format on sentences randomly sampled from ClueWeb09. SRILM and IRSTLM were run until the test machine ran o</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Nguyen</author>
<author>Jianfeng Gao</author>
<author>Milind Mahajan</author>
</authors>
<title>MSRLM: a scalable language modeling toolkit.</title>
<date>2007</date>
<tech>Technical Report MSR-TR-2007-144, Microsoft Research.</tech>
<contexts>
<context position="4732" citStr="Nguyen et al., 2007" startWordPosition="741" endWordPosition="744">othing calculated at runtime from counts. With Stupid Backoff, they scaled to 1.8 trillion tokens. We agree that Stupid Backoff is cheaper to estimate, but contend that this work makes Kneser-Ney smoothing cheap enough. Another advantage of Stupid Backoff has been that it stores one value, a count, per n-gram instead of probability and backoff. In previous work (Heafield et al., 2012), we showed how to collapse probability and backoff into a single value without changing sentence-level probabilities. However, local scores do change and, like Stupid Backoff, are no longer probabilities. MSRLM (Nguyen et al., 2007) aims to scalably estimate language models on a single machine. Counting is performed with streaming algorithms similarly to this work. Their parallel merge sort also has the potential to be faster than ours. The biggest difference is that their pipeline delays some computation (part of normalization and all of interpolation) until query time. This means that it cannot produce a standard ARPA file and that more time and memory are required at query time. Moreover, they use memory mapping on entire files and these files may be larger than physical RAM. We have found that, even with mostlysequen</context>
</contexts>
<marker>Nguyen, Gao, Mahajan, 2007</marker>
<rawString>Patrick Nguyen, Jianfeng Gao, and Milind Mahajan. 2007. MSRLM: a scalable language modeling toolkit. Technical Report MSR-TR-2007-144, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evalution of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="16305" citStr="Papineni et al., 2002" startWordPosition="2750" endWordPosition="2754">ration (sustained read: 405 MB/s). It took 123 GB RAM, 2.8 days wall time, and 5.4 CPU days. A summary of Google’s results from 2007 on different data and hardware appears in §2. We then used this language model as an additional feature in unconstrained Czech-English, French-English, and Spanish-English submissions to the 2013 Workshop on Machine Translation.8 Our baseline is the University of Edinburgh’s phrase-based Moses (Koehn et al., 2007) submission (Durrani et al., 2013), which used all constrained data specified by the evaluation (7 billion tokens of English). It placed first by BLEU (Papineni et al., 2002) among constrained submissions in each language pair we consider. In order to translate, the large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8http://statmt.org/wmt13/ Source Baseline Large Czech 27.4 28.2 French 32.6 33.4 Spanish 31.8 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Featu</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evalution of machine translation. In Proceedings 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the Seventh International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="1459" citStr="Stolcke, 2002" startWordPosition="219" endWordPosition="220">xperiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM. 1 Introduction Relatively low perplexity has made modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) a popular choice for language modeling. However, existing estimation methods require either large amounts of RAM (Stolcke, 2002) or machines (Brants et al., 2007). As a result, practitioners have chosen to use less data (Callison-Burch et al., 2012) or simpler smoothing methods (Brants et al., 2007). Backoff-smoothed n-gram language models (Katz, 1987) assign probability to a word wn in context wn−1 1 according to the recursive equation i 1) _�Xwn|wi−1), if w n1 was seen p(wn|w b(wn−1 1 )p(wn|wn 2 ), otherwise The task is to estimate probability p and backoff b from text for each seen entry wn1. This paper Filesystem Map Reduce 1 Filesystem Identity Map Reduce 2 Filesystem Figure 1: Each MapReduce performs three copies</context>
<context position="5648" citStr="Stolcke, 2002" startWordPosition="901" endWordPosition="902">and all of interpolation) until query time. This means that it cannot produce a standard ARPA file and that more time and memory are required at query time. Moreover, they use memory mapping on entire files and these files may be larger than physical RAM. We have found that, even with mostlysequential access, memory mapping is slower because the kernel does not explicitly know where to read ahead or write behind. In contrast, we use dedicated threads for reading and writing. Performance comparisons are omitted because we were unable to compile and run MSRLM on recent versions of Linux. SRILM (Stolcke, 2002) estimates modified Kneser-Ney models by storing n-grams in RAM. Corpus Counting Adjusting Counts Division Interpolation Model Figure 2: Data flow in the estimation pipeline. Normalization has two threads per order: summing and division. Thick arrows indicate sorting. It also offers a disk-based pipeline for initial steps (i.e. counting). However, the later steps store all n-grams that survived count pruning in RAM. Without pruning, both options use the same RAM. IRSTLM (Federico et al., 2008) does not implement modified Kneser-Ney but rather an approximation dubbed “improved Kneser-Ney” (or “</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the Seventh International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>512--519</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="16809" citStr="Talbot and Osborne, 2007" startWordPosition="2831" endWordPosition="2834">nstrained data specified by the evaluation (7 billion tokens of English). It placed first by BLEU (Papineni et al., 2002) among constrained submissions in each language pair we consider. In order to translate, the large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8http://statmt.org/wmt13/ Source Baseline Large Czech 27.4 28.2 French 32.6 33.4 Spanish 31.8 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the community. Sorting makes it scala</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In Proceedings of ACL, pages 512–519, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Whittaker</author>
<author>Bhiksha Raj</author>
</authors>
<title>Quantization-based language model compression.</title>
<date>2001</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<pages>33--36</pages>
<location>Aalborg, Denmark,</location>
<contexts>
<context position="13471" citStr="Whittaker and Raj, 2001" startWordPosition="2250" endWordPosition="2253"> pipelining (Dementiev et al., 2008). If there is enough RAM, input is lazily merged and streamed to the algorithm. Output is cut into blocks, sorted in the next step’s desired order, and then written to disk. These optimizations eliminate up to two copies to disk if enough RAM is available. Input, the algorithm, block sorting, and output are all threads on a chain of producerconsumer queues. Therefore, computation and disk operations happen simultaneously. 5Backoffs only exist if the n-gram is the context of some n + 1-gram, so merging skips n-grams that are not contexts. 6With quantization (Whittaker and Raj, 2001), the quantizer is trained in a first pass and applied in a second pass. 0 200 400 600 800 1000 Tokens (millions) 0 200 400 600 800 1000 Tokens (millions) Figure 5: CPU usage (system plus user). Each n-gram record is an array of n vocabulary identifiers (4 bytes each) and an 8-byte count or probability and backoff. At peak, records are stored twice on disk because lazy merge sort is not easily amenable to overwriting the input file. Additional costs are the secondary backoff file (4 bytes per backoff) and the vocabulary in plaintext. 5 Experiments Experiments use ClueWeb09.7 After spam filteri</context>
<context position="16449" citStr="Whittaker and Raj, 2001" startWordPosition="2774" endWordPosition="2777">ent data and hardware appears in §2. We then used this language model as an additional feature in unconstrained Czech-English, French-English, and Spanish-English submissions to the 2013 Workshop on Machine Translation.8 Our baseline is the University of Edinburgh’s phrase-based Moses (Koehn et al., 2007) submission (Durrani et al., 2013), which used all constrained data specified by the evaluation (7 billion tokens of English). It placed first by BLEU (Papineni et al., 2002) among constrained submissions in each language pair we consider. In order to translate, the large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8http://statmt.org/wmt13/ Source Baseline Large Czech 27.4 28.2 French 32.6 33.4 Spanish 31.8 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spani</context>
</contexts>
<marker>Whittaker, Raj, 2001</marker>
<rawString>Edward Whittaker and Bhiksha Raj. 2001. Quantization-based language model compression. In Proceedings of Eurospeech, pages 33–36, Aalborg, Denmark, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>