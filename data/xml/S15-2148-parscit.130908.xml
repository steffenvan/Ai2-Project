<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.152624">
<title confidence="0.9698865">
UCD : Diachronic Text Classification
with Character, Word, and Syntactic N-grams
</title>
<author confidence="0.998634">
Terrence Szymanski
</author>
<affiliation confidence="0.997799333333333">
Insight Centre for Data Analytics
School of Computer Science and Informatics
University College Dublin, Ireland
</affiliation>
<email confidence="0.95722">
terrence.szymanski@ucd.ie
</email>
<author confidence="0.996805">
Gerard Lynch
</author>
<affiliation confidence="0.9967715">
Centre for Applied Data Analytics Research
University College Dublin, Ireland
</affiliation>
<email confidence="0.981153">
gerard.lynch.@ucd.ie
</email>
<sectionHeader confidence="0.995478" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999805384615385">
We present our submission to SemEval-2015
Task 7: Diachronic Text Evaluation, in which
we approach the task of assigning a date to
a text as a multi-class classification problem.
We extract n-gram features from the text at
the letter, word, and syntactic level, and use
these to train a classifier on date-labeled train-
ing data. We also incorporate date probabili-
ties of syntactic features as estimated from a
very large external corpus of books. Our sys-
tem achieved the highest performance of all
systems on subtask 2: identifying texts by spe-
cific time language use.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999753125">
This paper describes our submission to the
SemEval-2015 Task 7, “Diachronic Text Evalua-
tion” (Popescu and Strapparava, 2015). The aim
of this shared task is to evaluate approaches to-
ward diachronic text analysis of a corpus of English-
language news articles from The Spectator1 archive,
originally published between 1700 and 2014.
We solely address subtask 2: “texts with specific
time language usage.” The goal of this subtask is
to infer the composition date of a text based on im-
plicit clues in language of the text, as opposed to
overt mentions of datable named entities or events.
This task has inherent utility, for example, for his-
torians dating texts in an archive with no external
datable properties. However, it is equally interest-
ing as an investigation into methods for quantifying
</bodyText>
<footnote confidence="0.890687">
1http://www.spectator.co.uk/.
</footnote>
<bodyText confidence="0.99523252631579">
changes in language and writing style over a period
of centuries.
We approach this task in a similar manner as pre-
vious work on stylistic text classification (Argamon-
Engelson et al., 1998) in that we aim to model stylis-
tic, rather than topical, features of the text. From
each text we extract a variety of character, lexical,
and syntactic features, as described in section 3. We
also use a set of syntactic features whose frequencies
over time have been estimated from a very large cor-
pus of books (Goldberg and Orwant, 2013). While
many of these features have previously been used for
stylistic analysis, our approach is not to model style
per se. Many types of variation may be captured
indirectly by our features: the spelling, typogra-
phy, lexicon, and grammar of English have changed
markedly over the past centuries, as has the genre of
news writing. We consider any time-correlated vari-
ation to be useful for dating.
</bodyText>
<sectionHeader confidence="0.988847" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999661923076923">
We used the two training sets of texts provided by
the challenge organizers for subtask 2. After remov-
ing errors (repeated items, items containing no text,
items with invalid dates), our training set consisted
of 4130 items. Each item contains the text of a snip-
pet of news, typically consisting of a few sentences
(the average length of a text is 70 words), and three
year-range labels: one for each of the Fine (6-year),
Medium (12-year) and Coarse (20-year) granulari-
ties specified in the task.
The given labels are not well-suited for classifica-
tion, since the set of labels used for one text is not
necessarily the same as the set of labels used for an-
</bodyText>
<page confidence="0.982123">
879
</page>
<bodyText confidence="0.70597575">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 879–883,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
other text. For example, here are the labels provided
for two texts in the training set:
</bodyText>
<equation confidence="0.9968245">
&lt;text id=&amp;quot;378rn324911597&amp;quot;&gt;
&lt;textF yes=&amp;quot;1698-1704&amp;quot; no=&amp;quot;1705- 1711&amp;quot; ...
&lt;textM yes=&amp;quot;1695-1707&amp;quot; no=&amp;quot;1708- 1720&amp;quot; ...
&lt;textC yes=&amp;quot;1691-1711&amp;quot; no=&amp;quot;1712- 1732&amp;quot; ...
&lt;text id=&amp;quot;74gi329732114&amp;quot;&gt;
&lt;textF yes=&amp;quot;1699-1705&amp;quot; no=&amp;quot;1706- 1712&amp;quot; ...
&lt;textM yes=&amp;quot;1696-1708&amp;quot; no=&amp;quot;1709- 1721&amp;quot; ...
&lt;textC yes=&amp;quot;1692-1712&amp;quot; no=&amp;quot;1713- 1733&amp;quot; ...
</equation>
<bodyText confidence="0.999800541666667">
These two texts are very close in date, yet
have completely different (and incomparable) year
ranges. Therefore, we create our own non-
overlapping year-range classes at 6-, 12-, 20-, and
50-year levels. We assume that the true date of a
text is the midpoint of the “yes” year ranges and
assign a non-overlapping class appropriately. All
of our training, cross-evaluation, and prediction is
done using these non-overlapping classes. To make
predictions for our official submission, we predict
whichever given year range has the greatest overlap
with our predicted class.
The training data is unevenly distributed over the
possible range of years from 1700 to 2014. Just three
years (1717, 1817, and 1897) account for 11% (444
of 4130) of the training instances, while 48% (150
of 314) of the years in the possible range are unat-
tested in the training data. Overall, there is a general
bias towards earlier years in the time range. We do
not attempt to control for this bias in the data, since
we assume that the test data will be drawn from a
similar distribution. While the uneven distribution
may artificially boost the accuracy of our classifiers,
the baseline classifier captures this effect.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="method">
3 Features for Classification
</sectionHeader>
<bodyText confidence="0.999960979166667">
We extract four types of features from each text:
character n-grams (Char), part-of-speech tag n-
grams (POS), word n-grams (Word) and syntactic
phrase-structure rule occurrences (Syn). We refer to
the combined feature set as CPWS. (Stamou, 2008)
surveys diachronic classification of literary text and
finds that parts of speech, character frequencies, and
function word frequencies are all used in chronolog-
ically dating text composition. Part-of-speech and
word n-grams have been used for stylistic text clas-
sification (Argamon-Engelson et al., 1998), and syn-
tactic phrase-structure rules have successfully been
used as stylometric features for detecting deceptive
writing in online reviews (Feng et al., 2012). We
have not included document-level stylistic features
(e.g. average sentence length, average word length,
lexical richness, lexical density, and readability mea-
sures) although they have been used successfully for
diachronic stylistic analysis (ˇStajner and Zampien,
2013), and could be incorporated in our classifica-
tion approach. However, our n-gram features may
capture features such as sentence length by proxy
(e.g. in the frequency of periods).
Character n-grams are an expressive feature set
which can capture variation on the morphological
level (word stems), syntactic level (gaps between
words and punctuation) and also word-level fre-
quency fluctuations (prepositions and conjunctions).
Character bigrams were used previously on Latin
text by (Frontini et al., 2008) to date the Donation of
Constantine, a study which did not verify the work
as a forgery but did place it in the correct stylistically
implied period.2 Additionally, character n-grams are
used in stylometric tasks such as authorship attribu-
tion (Keselj et al., 2003) and detection of transla-
tionese (Popescu, 2011).
All n-gram features were extracted for n ∈
{1, 2, 3} using an in-house Java concordancer.
Punctuation and spacing was not modified during
this process, although case information was dis-
carded. No stop words were removed. Raw fre-
quency counts of the features were used in the pro-
cess, and those features with less than 20 occur-
rences in the entire corpus were discarded. Texts
were parsed with the Stanford parser,3 and the 250
most-frequent syntactic rules in the training set were
used as features. The dependency parse was also
produced and used as described below.
</bodyText>
<subsectionHeader confidence="0.999609">
3.1 Google Syntactic N-grams
</subsectionHeader>
<bodyText confidence="0.999867428571429">
As an external source of data, we used the Google
Books Syntactic N-Grams (GSN) database (Gold-
berg and Orwant, 2013). Due to the size of the
datasets and time limitations, we focused solely on
the nodes collection of the Eng-1M corpus, a sam-
ple of 1 million English-language books dating from
1520 to 2008. Each data point in the nodes collec-
</bodyText>
<footnote confidence="0.998245333333333">
2Verification of forgery was based on false information con-
tained in the text, rather than stylistic idiosyncracy.
3http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<page confidence="0.994181">
880
</page>
<bodyText confidence="0.999925909090909">
tion is a POS-tagged word and the label of the syn-
tactic dependency between that word and its head,
which gives a sense of the word’s syntactic function
in a given sentence. For each node, the total number
of occurrences in each year is provided.
Because the GSN database is particularly sparse
for years prior to 1800, we smoothed all node
counts by averaging over the five nearest years with
nonzero counts. Then the smoothed counts are nor-
malized within each year to estimate the probability
of a node in a given year.
We use a Naive Bayes classifier (Googlenb) to
predict the most likely year for a given text, repre-
sented as a set of nodes extracted from the depen-
dency parse. We also produce a GSN feature set
consisting of 308 features (one for each year in the
range 1700-2008), whose values are based on the to-
tal log probability of the text in that year, normalized
to the interval [0,1] for each text. The normaliza-
tion controls for text length and allows comparison
between texts. These features are then used in the
combined CPWS+G classifier.
</bodyText>
<subsectionHeader confidence="0.994655">
3.2 Feature Informativeness
</subsectionHeader>
<bodyText confidence="0.998789222222222">
When all features are combined, the GSN features
are the most predictive. In order to assess the ef-
fectiveness of the other features and also to reduce
the feature set for classification, we performed at-
tribute selection using the Weka data mining soft-
ware (Hall et al., 2009). Table 1 shows the top-
ranked CPWS features using the the 50-year class
labels, using Weka’s information gain attribute eval-
uation with 10-fold cross-validation.
</bodyText>
<table confidence="0.999543727272727">
Rank Attribute Type Rank Attribute Type
1 NN P-1 10.9 t C-1
2 i C-1 11.7 l C-1
3.5 C-1 13.3 o C-1
3.9 . → . S 13.8 . W-1
5 ROOT → S S 15.7 [ ’ d ] C-2
5.8 a C-1 15.9 [ . ] C-2
7.3 e C-1 16.3 r C-1
8.1 . C-1 18.6 [ JJ NN ] P-2
8.5 n C-1 19.3 JJ P-1
10.7 s C-1 19.8 c C-1
</table>
<tableCaption confidence="0.999974">
Table 1: Top 20 CPWS features using Information Gain
</tableCaption>
<bodyText confidence="0.999600714285714">
The rankings show that the character n-gram fea-
tures were particularly expressive in capturing tem-
poral variation, yet it can be difficult to assign a
linguistic motivation to them. Because our feature
counts are not normalized by text length, many of
these features may simply be redundantly capturing
an overall length effect.
</bodyText>
<figureCaption confidence="0.972922">
Figure 1: Changing frequencies of verb endings in the
Google Books English corpus, 1700-2000.
</figureCaption>
<bodyText confidence="0.999887363636363">
However, some meaningful features can clearly
be recognized, such as [ ’ d ], refering to the 18th
century abbreviation of -ed as a past participle verb
ending in English. The frequency of verb-final ’d in
the POS-tagged Google N-grams dataset (Lin et al.,
2012), shown in Figure 1, illustrates how use of this
linguistic feature has declined over time.
Another highly informative feature is the charac-
ter bigram [ . ]. In some texts, punctuation has been
separated from the neighboring words with a space,
possibly due to OCR errors on older texts.
</bodyText>
<sectionHeader confidence="0.989191" genericHeader="method">
4 Classification and Evaluation
</sectionHeader>
<bodyText confidence="0.999692142857143">
We employ attribute selection as above in all of our
cross-validation experiments and our official sub-
mission. Table 2 illustrates how SVM classifica-
tion accuracy varies with feature set size. The value
of 4000 features was chosen to maximize accuracy
while minimizing running time, and was used to pro-
duce all of the results described in this paper.
</bodyText>
<table confidence="0.99950475">
JFJ 6-Year 12-Year 20-Year 50-Year
4000 37.61 39.30 52.07 67.74
2000 35.75 37.61 50.77 67.59
1000 32.55 37.26 51.24 67.53
500 33.09 38.62 52.22 65.94
200 33.77 35.28 50.41 64.07
100 31.87 33.62 47.10 60.32
50 29.57 31.82 44.91 57.07
</table>
<tableCaption confidence="0.991408">
Table 2: Effect of feature set size (IFI) on classification
accuracy. (Char+POS+Google features)
</tableCaption>
<bodyText confidence="0.759952">
Assigning a date to a text is not a typical classifi-
</bodyText>
<page confidence="0.995813">
881
</page>
<bodyText confidence="0.999595583333333">
cation problem, because the classes are not indepen-
dent of one another. We experimented with SVM re-
gression, but this produced lower accuracy than the
SVM classifier. Ordinal classification is a method
that may be used when classes exhibit a natural or-
der, as in this task. We performed some experiments
with the Weka implementation of ordinal regression
(Frank and Hall, 2001) using a SVM base classi-
fier, but these produced lower accuracy than the stan-
dard SVM classifier. Therefore, we used a standard
multi-class SVM classifier for all of our evaluations
and predictions.
</bodyText>
<table confidence="0.999816888888889">
System 6-Year 12-Year 20-Year 50-Year
Baseline 10.4 12.6 20.5 36.6
Googlenb 10.9 18.7 31.7 52.4
Char 36.1 38.4 47.9 64.5
POS 24.6 26.8 36.3 53.6
Word 26.1 29.6 37.2 54.6
Syn 23.4 26.3 38.5 54.6
CPWS 36.9 40.1 50.7 67.8
CPWS+G 41.5 45.9 55.3 73.3
</table>
<tableCaption confidence="0.9982655">
Table 3: Classification accuracy of various feature sets,
using 10-fold cross-validiation on the training data set.
</tableCaption>
<bodyText confidence="0.997994909090909">
Table 3 lists the cross-validation classification ac-
curacy for our various models. The baseline clas-
sifier looks only at the class labels and chooses the
most frequent class. The Googlenb classifier is a
Naive Bayes classifier using only the GSN probabil-
ities and assuming a uniform prior over years. This
represents a classifier with no domain knowledge of
the text genre or date range distribution.
The remaining rows show the results for SVM
classifiers trained independently on each of the four
stylistic feature sets. While each feature type outper-
forms the baseline, the character n-gram features are
clearly the single most effective feature type. The
combination of all four features together (CPWS)
outperforms any single feature set individually, and
this represents the maximal performance we achieve
using solely the training data provided by the task
organizers.
The final row shows the performance of a SVM
classifier using all of our stylistic features plus fea-
tures derived from the GSN probabilities. This
achieves the highest accuracy and this is the system
we submitted to the task.
Table 4 shows the official results of the CPWS+G
classifier, trained on the full training set and evalu-
ated on a test set of 1041 texts whose true dates were
unknown to us. The accuracy values are in line with
our cross-validation scores. The score is a weighted
classification metric that rewards predictions that are
not fully correct but are near the correct date. The
third row lists the mean deviation of our predictions
from the true date. By all three measures, our system
was the top performing submission to this subtask.
</bodyText>
<table confidence="0.9990676">
Fine Medium Coarse
(6-year) (12-year) (20-year)
Accuracy 46.3 47.3 54.3
Score 0.7592 0.8466 0.9104
Avg. Years Off 14 19 19
</table>
<tableCaption confidence="0.999458">
Table 4: Official results on the SemEval test data.
</tableCaption>
<bodyText confidence="0.999796">
Our 73.3% accuracy on the 50-year class may
be loosely compared to (Mihalcea and Nastase,
2012), who achieve 62% classification accuracy dat-
ing words in context to 50-year epochs. Their task,
word epoch disambiguation, is comparable but dif-
ferent: they classify words, not texts, using local
context features and a targeted set of 165 words.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985">
We have shown that a stylistic classification ap-
proach is capable of accurately predicting the date
when a text from the sample category was written.
Additionally, our approach is straightforward to im-
plement and can function well using only a moderate
sized sample of training data, although its accuracy
can be improved by incorporating features trained
from a large external corpus.
We cast a wide net in order to produce a large fea-
ture set and allow the classifier to select whichever
features most improved the classification accuracy.
While this produces good classification results, it
remains difficult to interpret the linguistic or stylis-
tic significance of the most-predictive features. It
is also unknown how the results would differ on
other data sets in different languages, genres, or
time periods. In addition to the features we have
explored, there are a number of others, such as
sentence length, capitalization, and lexical richness
measures which might be considered in future work.
</bodyText>
<page confidence="0.995794">
882
</page>
<sectionHeader confidence="0.998323" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999556">
This work is supported by Science Foundation Ire-
land through the Insight Centre for Data Analytics
under grant number SFI/12/RC/2289 and Enterprise
Ireland through the Centre for Applied Data Analyt-
ics Research under grant number TC 2013 0013.
Thanks to Mark Keane for his feedback and sug-
gestions, particularly on the use of syntactic features
for dating. Thanks also to the Insight Centre Future
of News discussion group for their feedback on a
presentation of an early version of this work.
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999896666666667">
Shlomo Argamon-Engelson, Moshe Koppel, and Galit
Avneri. 1998. Style-based text categorization: What
newspaper am I reading? Technical report, AAAI.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012. Syn-
tactic stylometry for deception detection. In Proceed-
ings of ACL 2012: Short Papers, pages 171–175.
Eibe Frank and Mark Hall. 2001. A simple approach to
ordinal classification. Technical report, University of
Waikato.
Francesca Frontini, Gerard Lynch, and Carl Vogel. 2008.
Revisiting the ‘Donation of Constantine’. In Proceed-
ings of AISB 2008, pages 1–9.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large corpus of
English books. In Proceedings of *SEM 2013, pages
241–247.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Vlado Keselj, Fuchun Peng, Nick Cercone, and Calvin
Thomas. 2003. N-gram-based author profiles for
authorship attribution. In Proceedings of PACLING
2003, pages 255–264.
Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the Google Books ngram
corpus. In Proceedings of the ACL 2012 System
Demonstrations, pages 169–174.
Rada Mihalcea and Vivi Nastase. 2012. Word epoch
disambiguation: Finding how words change over time.
In Proceedings of ACL 2012.
Octavian Popescu and Carlo Strapparava. 2015.
Semeval-2015 task 7: Diachronic text evaluation. In
Proceedings of SemEval 2015.
Marius Popescu. 2011. Studying translationese at the
character level. In Proceedings of RANLP 2011, pages
634–639.
Sanja ˇStajner and Marcos Zampien. 2013. Stylistic
changes for temporal text classification. In Proceed-
ings of TSD 2013, pages 519–526.
Constantina Stamou. 2008. Stylochronometry: Stylis-
tic development, sequence of composition, and rel-
ative dating. Literary and Linguistic Computing,
23(2):181–199.
</reference>
<page confidence="0.99922">
883
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.916080">
<title confidence="0.998254">UCD : Diachronic Text with Character, Word, and Syntactic N-grams</title>
<author confidence="0.99993">Terrence Szymanski</author>
<affiliation confidence="0.996364">Insight Centre for Data School of Computer Science and University College Dublin,</affiliation>
<email confidence="0.966591">terrence.szymanski@ucd.ie</email>
<author confidence="0.997695">Gerard Lynch</author>
<affiliation confidence="0.9943595">Centre for Applied Data Analytics University College Dublin,</affiliation>
<email confidence="0.992463">gerard.lynch.@ucd.ie</email>
<abstract confidence="0.998707214285714">We present our submission to SemEval-2015 Task 7: Diachronic Text Evaluation, in which we approach the task of assigning a date to a text as a multi-class classification problem. We extract n-gram features from the text at the letter, word, and syntactic level, and use these to train a classifier on date-labeled training data. We also incorporate date probabilities of syntactic features as estimated from a very large external corpus of books. Our system achieved the highest performance of all systems on subtask 2: identifying texts by specific time language use.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shlomo Argamon-Engelson</author>
<author>Moshe Koppel</author>
<author>Galit Avneri</author>
</authors>
<title>Style-based text categorization: What newspaper am I reading?</title>
<date>1998</date>
<tech>Technical report, AAAI.</tech>
<contexts>
<context position="5716" citStr="Argamon-Engelson et al., 1998" startWordPosition="899" endWordPosition="902">rs, the baseline classifier captures this effect. 3 Features for Classification We extract four types of features from each text: character n-grams (Char), part-of-speech tag ngrams (POS), word n-grams (Word) and syntactic phrase-structure rule occurrences (Syn). We refer to the combined feature set as CPWS. (Stamou, 2008) surveys diachronic classification of literary text and finds that parts of speech, character frequencies, and function word frequencies are all used in chronologically dating text composition. Part-of-speech and word n-grams have been used for stylistic text classification (Argamon-Engelson et al., 1998), and syntactic phrase-structure rules have successfully been used as stylometric features for detecting deceptive writing in online reviews (Feng et al., 2012). We have not included document-level stylistic features (e.g. average sentence length, average word length, lexical richness, lexical density, and readability measures) although they have been used successfully for diachronic stylistic analysis (ˇStajner and Zampien, 2013), and could be incorporated in our classification approach. However, our n-gram features may capture features such as sentence length by proxy (e.g. in the frequency </context>
</contexts>
<marker>Argamon-Engelson, Koppel, Avneri, 1998</marker>
<rawString>Shlomo Argamon-Engelson, Moshe Koppel, and Galit Avneri. 1998. Style-based text categorization: What newspaper am I reading? Technical report, AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Ritwik Banerjee</author>
<author>Yejin Choi</author>
</authors>
<title>Syntactic stylometry for deception detection.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL 2012: Short Papers,</booktitle>
<pages>171--175</pages>
<contexts>
<context position="5876" citStr="Feng et al., 2012" startWordPosition="922" endWordPosition="925">ag ngrams (POS), word n-grams (Word) and syntactic phrase-structure rule occurrences (Syn). We refer to the combined feature set as CPWS. (Stamou, 2008) surveys diachronic classification of literary text and finds that parts of speech, character frequencies, and function word frequencies are all used in chronologically dating text composition. Part-of-speech and word n-grams have been used for stylistic text classification (Argamon-Engelson et al., 1998), and syntactic phrase-structure rules have successfully been used as stylometric features for detecting deceptive writing in online reviews (Feng et al., 2012). We have not included document-level stylistic features (e.g. average sentence length, average word length, lexical richness, lexical density, and readability measures) although they have been used successfully for diachronic stylistic analysis (ˇStajner and Zampien, 2013), and could be incorporated in our classification approach. However, our n-gram features may capture features such as sentence length by proxy (e.g. in the frequency of periods). Character n-grams are an expressive feature set which can capture variation on the morphological level (word stems), syntactic level (gaps between </context>
</contexts>
<marker>Feng, Banerjee, Choi, 2012</marker>
<rawString>Song Feng, Ritwik Banerjee, and Yejin Choi. 2012. Syntactic stylometry for deception detection. In Proceedings of ACL 2012: Short Papers, pages 171–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Mark Hall</author>
</authors>
<title>A simple approach to ordinal classification.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>University of Waikato.</institution>
<contexts>
<context position="12044" citStr="Frank and Hall, 2001" startWordPosition="1961" endWordPosition="1964">2 65.94 200 33.77 35.28 50.41 64.07 100 31.87 33.62 47.10 60.32 50 29.57 31.82 44.91 57.07 Table 2: Effect of feature set size (IFI) on classification accuracy. (Char+POS+Google features) Assigning a date to a text is not a typical classifi881 cation problem, because the classes are not independent of one another. We experimented with SVM regression, but this produced lower accuracy than the SVM classifier. Ordinal classification is a method that may be used when classes exhibit a natural order, as in this task. We performed some experiments with the Weka implementation of ordinal regression (Frank and Hall, 2001) using a SVM base classifier, but these produced lower accuracy than the standard SVM classifier. Therefore, we used a standard multi-class SVM classifier for all of our evaluations and predictions. System 6-Year 12-Year 20-Year 50-Year Baseline 10.4 12.6 20.5 36.6 Googlenb 10.9 18.7 31.7 52.4 Char 36.1 38.4 47.9 64.5 POS 24.6 26.8 36.3 53.6 Word 26.1 29.6 37.2 54.6 Syn 23.4 26.3 38.5 54.6 CPWS 36.9 40.1 50.7 67.8 CPWS+G 41.5 45.9 55.3 73.3 Table 3: Classification accuracy of various feature sets, using 10-fold cross-validiation on the training data set. Table 3 lists the cross-validation clas</context>
</contexts>
<marker>Frank, Hall, 2001</marker>
<rawString>Eibe Frank and Mark Hall. 2001. A simple approach to ordinal classification. Technical report, University of Waikato.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesca Frontini</author>
<author>Gerard Lynch</author>
<author>Carl Vogel</author>
</authors>
<title>Revisiting the ‘Donation of Constantine’.</title>
<date>2008</date>
<booktitle>In Proceedings of AISB</booktitle>
<pages>1--9</pages>
<contexts>
<context position="6654" citStr="Frontini et al., 2008" startWordPosition="1032" endWordPosition="1035">ity measures) although they have been used successfully for diachronic stylistic analysis (ˇStajner and Zampien, 2013), and could be incorporated in our classification approach. However, our n-gram features may capture features such as sentence length by proxy (e.g. in the frequency of periods). Character n-grams are an expressive feature set which can capture variation on the morphological level (word stems), syntactic level (gaps between words and punctuation) and also word-level frequency fluctuations (prepositions and conjunctions). Character bigrams were used previously on Latin text by (Frontini et al., 2008) to date the Donation of Constantine, a study which did not verify the work as a forgery but did place it in the correct stylistically implied period.2 Additionally, character n-grams are used in stylometric tasks such as authorship attribution (Keselj et al., 2003) and detection of translationese (Popescu, 2011). All n-gram features were extracted for n ∈ {1, 2, 3} using an in-house Java concordancer. Punctuation and spacing was not modified during this process, although case information was discarded. No stop words were removed. Raw frequency counts of the features were used in the process, </context>
</contexts>
<marker>Frontini, Lynch, Vogel, 2008</marker>
<rawString>Francesca Frontini, Gerard Lynch, and Carl Vogel. 2008. Revisiting the ‘Donation of Constantine’. In Proceedings of AISB 2008, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Jon Orwant</author>
</authors>
<title>A dataset of syntactic-ngrams over time from a very large corpus of English books.</title>
<date>2013</date>
<booktitle>In Proceedings of *SEM 2013,</booktitle>
<pages>241--247</pages>
<contexts>
<context position="2292" citStr="Goldberg and Orwant, 2013" startWordPosition="359" endWordPosition="362"> equally interesting as an investigation into methods for quantifying 1http://www.spectator.co.uk/. changes in language and writing style over a period of centuries. We approach this task in a similar manner as previous work on stylistic text classification (ArgamonEngelson et al., 1998) in that we aim to model stylistic, rather than topical, features of the text. From each text we extract a variety of character, lexical, and syntactic features, as described in section 3. We also use a set of syntactic features whose frequencies over time have been estimated from a very large corpus of books (Goldberg and Orwant, 2013). While many of these features have previously been used for stylistic analysis, our approach is not to model style per se. Many types of variation may be captured indirectly by our features: the spelling, typography, lexicon, and grammar of English have changed markedly over the past centuries, as has the genre of news writing. We consider any time-correlated variation to be useful for dating. 2 Data We used the two training sets of texts provided by the challenge organizers for subtask 2. After removing errors (repeated items, items containing no text, items with invalid dates), our training</context>
<context position="7683" citStr="Goldberg and Orwant, 2013" startWordPosition="1202" endWordPosition="1206">r. Punctuation and spacing was not modified during this process, although case information was discarded. No stop words were removed. Raw frequency counts of the features were used in the process, and those features with less than 20 occurrences in the entire corpus were discarded. Texts were parsed with the Stanford parser,3 and the 250 most-frequent syntactic rules in the training set were used as features. The dependency parse was also produced and used as described below. 3.1 Google Syntactic N-grams As an external source of data, we used the Google Books Syntactic N-Grams (GSN) database (Goldberg and Orwant, 2013). Due to the size of the datasets and time limitations, we focused solely on the nodes collection of the Eng-1M corpus, a sample of 1 million English-language books dating from 1520 to 2008. Each data point in the nodes collec2Verification of forgery was based on false information contained in the text, rather than stylistic idiosyncracy. 3http://nlp.stanford.edu/software/lex-parser.shtml 880 tion is a POS-tagged word and the label of the syntactic dependency between that word and its head, which gives a sense of the word’s syntactic function in a given sentence. For each node, the total numbe</context>
</contexts>
<marker>Goldberg, Orwant, 2013</marker>
<rawString>Yoav Goldberg and Jon Orwant. 2013. A dataset of syntactic-ngrams over time from a very large corpus of English books. In Proceedings of *SEM 2013, pages 241–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="9440" citStr="Hall et al., 2009" startWordPosition="1502" endWordPosition="1505">(one for each year in the range 1700-2008), whose values are based on the total log probability of the text in that year, normalized to the interval [0,1] for each text. The normalization controls for text length and allows comparison between texts. These features are then used in the combined CPWS+G classifier. 3.2 Feature Informativeness When all features are combined, the GSN features are the most predictive. In order to assess the effectiveness of the other features and also to reduce the feature set for classification, we performed attribute selection using the Weka data mining software (Hall et al., 2009). Table 1 shows the topranked CPWS features using the the 50-year class labels, using Weka’s information gain attribute evaluation with 10-fold cross-validation. Rank Attribute Type Rank Attribute Type 1 NN P-1 10.9 t C-1 2 i C-1 11.7 l C-1 3.5 C-1 13.3 o C-1 3.9 . → . S 13.8 . W-1 5 ROOT → S S 15.7 [ ’ d ] C-2 5.8 a C-1 15.9 [ . ] C-2 7.3 e C-1 16.3 r C-1 8.1 . C-1 18.6 [ JJ NN ] P-2 8.5 n C-1 19.3 JJ P-1 10.7 s C-1 19.8 c C-1 Table 1: Top 20 CPWS features using Information Gain The rankings show that the character n-gram features were particularly expressive in capturing temporal variation, </context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vlado Keselj</author>
<author>Fuchun Peng</author>
<author>Nick Cercone</author>
<author>Calvin Thomas</author>
</authors>
<title>N-gram-based author profiles for authorship attribution.</title>
<date>2003</date>
<booktitle>In Proceedings of PACLING</booktitle>
<pages>255--264</pages>
<contexts>
<context position="6920" citStr="Keselj et al., 2003" startWordPosition="1076" endWordPosition="1079">n the frequency of periods). Character n-grams are an expressive feature set which can capture variation on the morphological level (word stems), syntactic level (gaps between words and punctuation) and also word-level frequency fluctuations (prepositions and conjunctions). Character bigrams were used previously on Latin text by (Frontini et al., 2008) to date the Donation of Constantine, a study which did not verify the work as a forgery but did place it in the correct stylistically implied period.2 Additionally, character n-grams are used in stylometric tasks such as authorship attribution (Keselj et al., 2003) and detection of translationese (Popescu, 2011). All n-gram features were extracted for n ∈ {1, 2, 3} using an in-house Java concordancer. Punctuation and spacing was not modified during this process, although case information was discarded. No stop words were removed. Raw frequency counts of the features were used in the process, and those features with less than 20 occurrences in the entire corpus were discarded. Texts were parsed with the Stanford parser,3 and the 250 most-frequent syntactic rules in the training set were used as features. The dependency parse was also produced and used as</context>
</contexts>
<marker>Keselj, Peng, Cercone, Thomas, 2003</marker>
<rawString>Vlado Keselj, Fuchun Peng, Nick Cercone, and Calvin Thomas. 2003. N-gram-based author profiles for authorship attribution. In Proceedings of PACLING 2003, pages 255–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri Lin</author>
<author>Jean-Baptiste Michel</author>
<author>Erez Lieberman Aiden</author>
<author>Jon Orwant</author>
<author>Will Brockman</author>
<author>Slav Petrov</author>
</authors>
<title>Syntactic annotations for the Google Books ngram corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>169--174</pages>
<contexts>
<context position="10610" citStr="Lin et al., 2012" startWordPosition="1723" endWordPosition="1726">ly expressive in capturing temporal variation, yet it can be difficult to assign a linguistic motivation to them. Because our feature counts are not normalized by text length, many of these features may simply be redundantly capturing an overall length effect. Figure 1: Changing frequencies of verb endings in the Google Books English corpus, 1700-2000. However, some meaningful features can clearly be recognized, such as [ ’ d ], refering to the 18th century abbreviation of -ed as a past participle verb ending in English. The frequency of verb-final ’d in the POS-tagged Google N-grams dataset (Lin et al., 2012), shown in Figure 1, illustrates how use of this linguistic feature has declined over time. Another highly informative feature is the character bigram [ . ]. In some texts, punctuation has been separated from the neighboring words with a space, possibly due to OCR errors on older texts. 4 Classification and Evaluation We employ attribute selection as above in all of our cross-validation experiments and our official submission. Table 2 illustrates how SVM classification accuracy varies with feature set size. The value of 4000 features was chosen to maximize accuracy while minimizing running tim</context>
</contexts>
<marker>Lin, Michel, Aiden, Orwant, Brockman, Petrov, 2012</marker>
<rawString>Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden, Jon Orwant, Will Brockman, and Slav Petrov. 2012. Syntactic annotations for the Google Books ngram corpus. In Proceedings of the ACL 2012 System Demonstrations, pages 169–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Vivi Nastase</author>
</authors>
<title>Word epoch disambiguation: Finding how words change over time.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="14508" citStr="Mihalcea and Nastase, 2012" startWordPosition="2366" endWordPosition="2369">he accuracy values are in line with our cross-validation scores. The score is a weighted classification metric that rewards predictions that are not fully correct but are near the correct date. The third row lists the mean deviation of our predictions from the true date. By all three measures, our system was the top performing submission to this subtask. Fine Medium Coarse (6-year) (12-year) (20-year) Accuracy 46.3 47.3 54.3 Score 0.7592 0.8466 0.9104 Avg. Years Off 14 19 19 Table 4: Official results on the SemEval test data. Our 73.3% accuracy on the 50-year class may be loosely compared to (Mihalcea and Nastase, 2012), who achieve 62% classification accuracy dating words in context to 50-year epochs. Their task, word epoch disambiguation, is comparable but different: they classify words, not texts, using local context features and a targeted set of 165 words. 5 Conclusion We have shown that a stylistic classification approach is capable of accurately predicting the date when a text from the sample category was written. Additionally, our approach is straightforward to implement and can function well using only a moderate sized sample of training data, although its accuracy can be improved by incorporating f</context>
</contexts>
<marker>Mihalcea, Nastase, 2012</marker>
<rawString>Rada Mihalcea and Vivi Nastase. 2012. Word epoch disambiguation: Finding how words change over time. In Proceedings of ACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Octavian Popescu</author>
<author>Carlo Strapparava</author>
</authors>
<title>Semeval-2015 task 7: Diachronic text evaluation.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval</booktitle>
<contexts>
<context position="1067" citStr="Popescu and Strapparava, 2015" startWordPosition="154" endWordPosition="157">in which we approach the task of assigning a date to a text as a multi-class classification problem. We extract n-gram features from the text at the letter, word, and syntactic level, and use these to train a classifier on date-labeled training data. We also incorporate date probabilities of syntactic features as estimated from a very large external corpus of books. Our system achieved the highest performance of all systems on subtask 2: identifying texts by specific time language use. 1 Introduction This paper describes our submission to the SemEval-2015 Task 7, “Diachronic Text Evaluation” (Popescu and Strapparava, 2015). The aim of this shared task is to evaluate approaches toward diachronic text analysis of a corpus of Englishlanguage news articles from The Spectator1 archive, originally published between 1700 and 2014. We solely address subtask 2: “texts with specific time language usage.” The goal of this subtask is to infer the composition date of a text based on implicit clues in language of the text, as opposed to overt mentions of datable named entities or events. This task has inherent utility, for example, for historians dating texts in an archive with no external datable properties. However, it is </context>
</contexts>
<marker>Popescu, Strapparava, 2015</marker>
<rawString>Octavian Popescu and Carlo Strapparava. 2015. Semeval-2015 task 7: Diachronic text evaluation. In Proceedings of SemEval 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Popescu</author>
</authors>
<title>Studying translationese at the character level.</title>
<date>2011</date>
<booktitle>In Proceedings of RANLP 2011,</booktitle>
<pages>634--639</pages>
<contexts>
<context position="6968" citStr="Popescu, 2011" startWordPosition="1085" endWordPosition="1086">expressive feature set which can capture variation on the morphological level (word stems), syntactic level (gaps between words and punctuation) and also word-level frequency fluctuations (prepositions and conjunctions). Character bigrams were used previously on Latin text by (Frontini et al., 2008) to date the Donation of Constantine, a study which did not verify the work as a forgery but did place it in the correct stylistically implied period.2 Additionally, character n-grams are used in stylometric tasks such as authorship attribution (Keselj et al., 2003) and detection of translationese (Popescu, 2011). All n-gram features were extracted for n ∈ {1, 2, 3} using an in-house Java concordancer. Punctuation and spacing was not modified during this process, although case information was discarded. No stop words were removed. Raw frequency counts of the features were used in the process, and those features with less than 20 occurrences in the entire corpus were discarded. Texts were parsed with the Stanford parser,3 and the 250 most-frequent syntactic rules in the training set were used as features. The dependency parse was also produced and used as described below. 3.1 Google Syntactic N-grams A</context>
</contexts>
<marker>Popescu, 2011</marker>
<rawString>Marius Popescu. 2011. Studying translationese at the character level. In Proceedings of RANLP 2011, pages 634–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanja ˇStajner</author>
<author>Marcos Zampien</author>
</authors>
<title>Stylistic changes for temporal text classification.</title>
<date>2013</date>
<booktitle>In Proceedings of TSD 2013,</booktitle>
<pages>519--526</pages>
<marker>ˇStajner, Zampien, 2013</marker>
<rawString>Sanja ˇStajner and Marcos Zampien. 2013. Stylistic changes for temporal text classification. In Proceedings of TSD 2013, pages 519–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantina Stamou</author>
</authors>
<title>Stylochronometry: Stylistic development, sequence of composition, and relative dating.</title>
<date>2008</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>23--2</pages>
<contexts>
<context position="5410" citStr="Stamou, 2008" startWordPosition="858" endWordPosition="859">verall, there is a general bias towards earlier years in the time range. We do not attempt to control for this bias in the data, since we assume that the test data will be drawn from a similar distribution. While the uneven distribution may artificially boost the accuracy of our classifiers, the baseline classifier captures this effect. 3 Features for Classification We extract four types of features from each text: character n-grams (Char), part-of-speech tag ngrams (POS), word n-grams (Word) and syntactic phrase-structure rule occurrences (Syn). We refer to the combined feature set as CPWS. (Stamou, 2008) surveys diachronic classification of literary text and finds that parts of speech, character frequencies, and function word frequencies are all used in chronologically dating text composition. Part-of-speech and word n-grams have been used for stylistic text classification (Argamon-Engelson et al., 1998), and syntactic phrase-structure rules have successfully been used as stylometric features for detecting deceptive writing in online reviews (Feng et al., 2012). We have not included document-level stylistic features (e.g. average sentence length, average word length, lexical richness, lexical</context>
</contexts>
<marker>Stamou, 2008</marker>
<rawString>Constantina Stamou. 2008. Stylochronometry: Stylistic development, sequence of composition, and relative dating. Literary and Linguistic Computing, 23(2):181–199.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>