<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.236234">
<title confidence="0.985914">
Instance-driven Discovery of Ontological Relation Labels
</title>
<author confidence="0.926771">
Marieke van Erp, Antal van den Bosch, Sander Wubben, Steve Hunt
</author>
<affiliation confidence="0.88482125">
ILK Research Group
Tilburg centre for Creative Computing
Tilburg University
The Netherlands
</affiliation>
<email confidence="0.998253">
{M.G.J.vanErp,Antal.vdnBosch,S.Wubben,S.J.Hunt}@uvt.nl
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972111111111">
An approach is presented to the auto-
matic discovery of labels of relations be-
tween pairs of ontological classes. Using
a hyperlinked encyclopaedic resource, we
gather evidence for likely predicative la-
bels by searching for sentences that de-
scribe relations between terms. The terms
are instances of the pair of ontological
classes under consideration, drawn from
a populated knowledge base. Verbs or
verb phrases are automatically extracted,
yielding a ranked list of candidate rela-
tions. Human judges rate the extracted
relations. The extracted relations provide
a basis for automatic ontology discovery
from a non-relational database. The ap-
proach is demonstrated on a database from
the natural history domain.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999903203703704">
The rapid growth in the digitisation of data has
caused many curators, researchers, and data man-
agers of cultural heritage institutions (libraries,
archives, museums) to turn to knowledge man-
agement systems. Using these systems typically
causes them to think about the ontological struc-
ture of their domain, involving the identification
of key classes in object data and metadata fea-
tures, and importantly, their relations. The start-
ing point of this process is often a more classi-
cal “flat” database matrix model of size n x m,
where n is the number of collection items, and m
is a fixed number of database columns, typically
denoting object metadata features, as cultural her-
itage institutions are generally well accustomed to
using databases of that type. An ontology can be
bootstrapped from such a database by first assum-
ing that the database columns can be mapped onto
the domain’s ontological classes. The next step
is then to determine which classes are related to
each other, and by which relation. In this paper
we present a method that partially automates this
process.
To gather evidence for a relation to exist be-
tween two ontological classes, it is not possible to
simply look up the classes in text. Rather, classes
are realised typically as a multitude of terms or
phrases. For example, the natural history class
“species” is realised as many different instances of
species names in text. The automatic discovery of
relations between ontological classes thus requires
at least a two-step approach: first, the identifica-
tion of instances of ontological classes in text and
their particular relations, and second, the aggrega-
tion of these analyses in order to find evidence for
a most likely relation.
It is common in ontology construction to use
predicative labels for relations. Although no regu-
lations for label names exist, often a verb or verb
phrase head is taken, optionally combined with a
prepositional head of the subsequent verb-attached
phrase (e. g., “occurs in”, or “donated by”). In
this study, we make the assumption that good can-
didate labels are frequent verbs or verb phrases
found between instances from a particular pair of
classes, and that this may sometimes involve a
verb-attached prepositional phrase containing one
of the two terms. In this paper we explore this
route, and present a case study on the discovery
of predicative labels on relations in an ontology
for animal specimen collections. The first step,
identifying instances of ontological classes, is per-
formed by selecting pairs of instances from a flat
n x m specimen database, in which the instances
</bodyText>
<note confidence="0.952118666666667">
Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education –LaTeCH – SHELT&amp;R 2009, pages 60–68,
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998102">
60
</page>
<bodyText confidence="0.999984105263158">
are organised by the database columns, and there
is a one-to-one relationship between the database
columns and the classes in our ontology.
Any approach that bases itself on text to dis-
cover relations, is dependent on the quality of
that text. In this study we opt for Wikipedia
as a resource from which to extract relations be-
tween terms. Although the status of Wikipedia
as a dependable resource is debated, in part be-
cause of its dynamic nature, there is some evi-
dence that Wikipedia can be as reliable a source
as one that is maintained solely by experts (Giles,
2005). Wikipedia is also an attractive resource due
to its size (currently nearly 12 million articles in
over 250 languages). Additionally, Wikipedia’s
strongly hyperlinked structure closely resembles a
semantic net, with its untyped (but directed) re-
lations between the concepts represented by the
article topics. Since the hyperlinks in Wikipedia
indicate a relations between two encyclopaedia ar-
ticles, we aim at discovering the type of relation
such a link denotes through the use of syntactic
parsing of the text in which the link occurs.
The idea of using Wikipedia for relation ex-
traction is not new (Auer and Lehmann, 2007;
Nakayama et al., 2008; Nguyen et al., 2007;
Suchanek et al., 2006; Syed et al., 2008). How-
ever, most studies so far focus on the structured
information already explicit in Wikipedia, such as
its infoboxes and categories. The main contribu-
tions of our work are that we focus on the in-
formation need emerging from a specific domain,
and that we test a method of pre-selection of sen-
tences to extract relations from. The selection is
based on the assumption that the strongest and
most reliable lexical relations are those expressed
by hyperlinks in Wikipedia pages that relate an ar-
ticle topic to another page (Kamps and Koolen,
2008). The selection procedure retains only sen-
tences in which the topic of the article, identified
by matching words in the article title, links to an-
other Wikipedia article. The benefit of the pre-
selection of sentences is that it reduces the work-
load for the syntactic parser.
Since the system is intentionally kept
lightweight, the extraction of relations from
Wikipedia is sufficiently fast, and we observe that
the results are sufficient to build a basic ontology
from the data. This paper is organised as follows.
In Section 2 we review related work. In Section 3
the data used in this work is described, followed
by the system in Section 4 and an explanation
of how we evaluated the possible relations our
system discovered is presented in Section 5. We
report on the results of our study in Section 6, and
formulate our conclusions and points for further
research in Section 7.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999967441860465">
A key property of Wikipedia is that it is for the
greater part unstructured. On the one hand, ed-
itors are encouraged to supply their articles with
categories. These categories can be subsumed by
broader categories, thus creating a taxonomy-like
structure. On the other hand, editors can link to
any other page in Wikipedia, no matter if it is part
of the same category, or any category at all. An
article can be assigned multiple categories, but the
number of hyperlinks provided in an average arti-
cle typically exceeds the number of categories as-
signed to it.
The free associative hyperlink structure of
Wikipedia is intrinsically different from the hier-
archical top down architecture as seen in Word-
Net, as a hyperlink has a direction, but not a
type. A Wikipedia article can contain any num-
ber of links, pointing to any other Wikipedia arti-
cle. Wikipedia guidelines state however that wik-
ilinks (hyperlinks referring to another Wikipedia
page) should only be added when relevant to the
topic of the article. Due to the fact that most users
tend to adhere to guidelines for editing Wikipedia
pages and the fact that articles are under constant
scrutiny of their viewers, most links in Wikipedia
are indeed relevant (Blohm and Cimiano, 2007;
Kamps and Koolen, 2008).
The structure and breadth of Wikipedia is a po-
tentially powerful resource for information extrac-
tion which has not gone unnoticed in the natu-
ral language processing (NLP) community. Pre-
processing of Wikipedia content in order to ex-
tract non-trivial relations has been addressed in a
number of studies. (Syed et al., 2008) for instance
utilise the category structure in Wikipedia as an
upper ontology to predict concepts common to a
set of documents. In (Suchanek et al., 2006) an
ontology is constructed by combining entities and
relations between these extracted from Wikipedia
through Wikipedia’s category structure and Word-
Net. This results in a large “is-a” hierarchy, draw-
ing on the basis of WordNet, while further rela-
tion enrichments come from Wikipedia’s category
</bodyText>
<page confidence="0.998642">
61
</page>
<bodyText confidence="0.999978229166667">
structure. (Chernov et al., 2006) also exploit the
Wikipedia category structure to which concepts in
the articles are linked to extract relations.
(Auer and Lehmann, 2007) take a different ap-
proach in that they focus on utilising the structure
present in infoboxes. Infoboxes are consistently
formatted tables in articles that provide summary
information, such as information about area, pop-
ulation and language for countries, and birth dates
and places for persons. Although infoboxes pro-
vide rich structured information, their templates
are not yet standardised, and their use has not per-
meated throughout the whole of Wikipedia.
Although the category and infobox structures in
Wikipedia already provide a larger coverage at the
concept or term level than for instance WordNet,
they do not express all possibly relevant seman-
tic relations. Especially in specific domains, re-
lations occur that would make the Wikipedia data
structure unnecessarily dense if added, thus an ap-
proach that exploits more of the linguistic content
of Wikipedia is desirable.
Such approaches can be found in (Nakayama et
al., 2008) and (Nguyen et al., 2007). In both works
full sections of Wikipedia articles are parsed, en-
tities are identified, and the verb between the enti-
ties is taken as the relation. They also extract re-
lations that are not backed by a link in Wikipedia,
resulting in common-sense factoids such as ‘Bres-
cia is a city’. For a domain specific application
this approach lacks precision. In our approach, we
care more for high precision in finding relations
than for recall; hence, we carefully pre-select on-
tological classes among which relations need to be
found, and use these as filters on our search.
The usefulness of the link structure in
Wikipedia has been remarked upon by (V¨olkel et
al., 2006). They acknowledge that the link struc-
ture in Wikipedia denotes a potentially meaning-
ful relation between two articles, though the re-
lation type is unknown. They propose an exten-
sion to the editing software of Wikipedia to enable
users to define the type of relation when they add
a link in Wikipedia. Potentially this can enrich
Wikipedia tremendously, but the work involved
would be tremendous as well. We believe some of
the type information is already available through
the linguistic content of Wikipedia.
</bodyText>
<sectionHeader confidence="0.982277" genericHeader="method">
3 Data Preparation
</sectionHeader>
<subsectionHeader confidence="0.99688">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999994836734694">
The data used in this work comes from a manu-
ally created, non-relational research database of a
collection of reptiles and amphibians at a natural
history museum. The information contained in the
cells describes when a specimen entered the col-
lection, under what circumstances it was collected,
its current location, registration number, etc. We
argue that the act of retrieving information from
this flat database could be enhanced by providing
a meta-structure that describes relations between
the different database columns. If for instance a
relation of the type ”is part of” can be defined be-
tween the database columns province and country,
then queries for specimens found at a particular
location can be expanded accordingly.
Even though the main language of the database
is Dutch, we still chose to use the English
Wikipedia as the resource for retrieval of rela-
tion label candidates. Explicitly choosing the En-
glish Wikipedia has as a consequence that the
relation labels we are bound to discover will
be English phrases. Furthermore, articles in
the English Wikipedia on animal taxonomy have
a broader coverage and are far more elaborate
than those contained in the Dutch Wikipedia.
Since these database values use a Latin-based
nomenclature, using the wider-coverage English
Wikipedia yields a much higher recall than the
Dutch Wikipedia. The values of the other columns
mainly contain proper names, such as person
names and geographic locations and dates, which
are often the same; moreover, English and Dutch
are closely related languages. Different names ex-
ist for different countries in each language, but
here the inconsistency of the database aids us, as
it in fact contains many database entries partially
or fully in English, as well as some in German and
Portuguese.
The database contains 16,870 records in 39
columns. In this work we focus on 20 columns;
the rest are discarded as they are either extrinsic
features not directly pertaining to the object they
describe, e.g., a unique database key, or elaborate
textual information that would require a separate
processing approach. The columns we focus on
describe the position of the specimen in the zo-
ological taxonomy (6 columns), the geographical
location in which it was found (4 columns), some
of its physical properties (3 columns), its collector
</bodyText>
<page confidence="0.998035">
62
</page>
<table confidence="0.999498611111111">
Column Name Value
Taxonomic Class Reptilia
Taxonomic Order Crocodylia
Amphisbaenia
Taxonomic Genus Acanthophis
Xenobatrachus
Country Indonesia
Suriname
Location city walls
near Lake Mahalona
Collection Date 01.02.1888
02.01.1995
Type holotype
paralectotype
Determinator A. Dubois
M. S. Hoogmoed
Species defined by (Linnaeus, 1758)
(LeSueur, 1827)
</table>
<tableCaption confidence="0.999895">
Table 1: Example classes from test data
</tableCaption>
<bodyText confidence="0.9999794">
and/or determiner, donator and associated date (4
columns), and other information (3 columns). The
values in most columns are short, often consist-
ing of a single word. Table 1 lists some example
database values.
</bodyText>
<subsectionHeader confidence="0.998267">
3.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.994029422222222">
As the database was created manually, it was nec-
essary to normalise spelling errors, as well as
variations on diacritics, names and date formats.
The database values were also stripped of all non-
alphanumeric characters.
In order to find meaningful relations between
two database columns, query pairs are generated
by combining two values occurring together in a
record. This approach already limits the number
of queries applied to Wikipedia, as no relations are
attempted to be found between values that would
not normally occur together. This approach yields
a query pair such as Reptilia Crocodylia from the
taxonomic class and order columns, but not Am-
phibia Crocodylia. Because not every database
field is filled, and some combinations occur more
often, this procedure results in 186,141 query
pairs.
For this study we use a database snapshot of the
English Wikipedia of July 27, 2008. This dump
contains about 2.5 million articles, including a vast
amount of domain-specific articles that one would
typically not find in general encyclopaedias. An
index was built of a subset of the link structure
present in Wikipedia. The subset of links included
in the index is constrained to those links occur-
ring in sentences from each article in which the
main topic of the Wikipedia article (as taken from
the title name) occurs. For example, from the
Wikipedia article onAnura the following sentence
would be included in the experiments1:
The frog is an [[amphibian]] in the order Anura
(meaning “tail-less”, from Greek an-, without +
oura, tail), formerly referred to as Salientia (Latin
saltare, to jump)
whereas we would exclude the sentence:
An exception is the [[fire-bellied toad]] (Bombina
bombina): while its skin is slightly warty, it
prefers a watery habitat.
This approach limits the link paths to only
those between pages that are probably semanti-
cally strongly connected to each other. In the
following section the computation of the link
paths indicating semantic relatedness between two
Wikipedia pages is explained.
</bodyText>
<subsectionHeader confidence="0.999586">
3.3 Computing Semantic Relatedness
</subsectionHeader>
<bodyText confidence="0.999981318181818">
Relation discovery between terms (instantiations
of different ontological classes) that have a page
in Wikipedia is best performed after establishing
if a sufficiently strong relation between the two
terms under consideration actually exists. To do
this, the semantic relatedness of those two terms
or concepts needs to be computed first. Seman-
tic relatedness can denote every possible relation
between two concepts, unlike semantic similarity,
which typically denotes only certain hierarchical
relations (like hypernymy and synonymy) and is
often computed using hierarchical networks like
WordNet (Budanitsky and Hirst, 2006).
A simple and effective way of computing se-
mantic relatedness between two concepts c1 and c2
is measuring their distance in a semantic network.
This results in a semantic distance metric, which
can be inversed to yield a semantic relatedness
metric. Computing the path-length between terms
c1 and c2 can be done using Formula 1 where P is
the set of paths connecting c1 to c2 and Np is the
number of nodes in path p.
</bodyText>
<footnote confidence="0.977129">
1The double brackets indicate Wikilinks
</footnote>
<page confidence="0.982921">
63
</page>
<equation confidence="0.982132333333333">
1
relpath(c1, c2) = argmaxp�P (1)
�p
</equation>
<bodyText confidence="0.9999878">
We search for shortest paths in a semantic net-
work that is constructed by mapping the concepts
in Wikipedia to nodes, and the links between the
concepts to edges. This generates a very large
network (millions of nodes and tens of millions
of edges), but due to the fact that Wikipedia is
scale-free (Barabasi and Albert, 1999) (its con-
nectedness degree distribution follows a power-
law), paths stay relatively short. By indexing
both incoming and outgoing links, a bidirectional
breadth-first search can be used to find shortest
paths between concepts. This means that the
search is divided in two chains: a forward chain
from c1 and a backward chain to c2. As soon as the
two chains are connected, a shortest path is found.
</bodyText>
<sectionHeader confidence="0.988778" genericHeader="method">
4 Extracting Relations from Wikipedia
</sectionHeader>
<bodyText confidence="0.996542">
Each query pair containing two values from two
database columns are sent to the system. The sys-
tem processes each term pair in four steps. A
schematic overview of the system is given in Fig-
ure 1.
</bodyText>
<figureCaption confidence="0.999406">
Figure 1: Schematic overview of the system
</figureCaption>
<bodyText confidence="0.999933534482759">
Step 1 We look for the most relevant Wikipedia
page for each term, by looking up the term in ti-
tles of Wikipedia articles. As Wikipedia format-
ting requires the article title to be an informative
and concise description of the article’s main topic,
we assume that querying only for article titles will
yield reliable results.
Step 2 The system finds the shortest link path be-
tween the two selected Wikipedia articles. If the
path distance is 1, this means that the two con-
cepts are linked directly to each other via their
Wikipedia articles. This is for instance the case
for Megophrys from the genus column, and Anura
from the order column. In the Wikipedia article on
Megophrys, a link is found to the Wikipedia arti-
cle on Anura. There is no reverse link from Anura
to Megophrys; hierarchical relationships in the zo-
ological taxonomy such as this one are often uni-
directional in Wikipedia as to not overcrowd the
parent article with links to its children.
Step 3 The sentence containing both target con-
cepts as links is selected from the articles.
From the Megophrys article this is for instance
“Megophrys is a genus offrogs, order [[Anura]],
in the [[Megophryidae]] family.”
Step 4 If the shortest path length between two
Wikipedia articles is 2, the two concepts are linked
via one intermediate article. In that case the sys-
tem checks whether the title of the intermediate ar-
ticle occurs as a value in a database column other
than the two database columns in focus for the
query. If this is indeed the case, the two addi-
tional relations between the first term and the in-
termediate article are also investigated, as well as
the second term and that of the intermediate ar-
ticle. Such a bridging relation pair is found for
instance for the query pair Hylidae from the tax-
onomic order column, and Brazil from the coun-
try column. Here, the initial path we find is Hyl-
idae H Sphaenorhynchys --+ Brazil. We find that
the article-in-the-middle value (Sphaenorhynchys)
indeed occurs in our database, in the taxonomic
genus column. We assume this link is evi-
dence for co-occurrence. Thus, the relevant sen-
tences from the Wikipedia articles on Hylidae
and Sphaenorhynchys, and between articles on
Sphaenorhynchys and Brazil are added to the pos-
sible relations between “order” – “genus” and
“genus” – “country”.
Subsequently, the selected sentences are POS-
tagged and parsed using the Memory Based Shal-
low Parser (Daelemans et al., 1999). This parser
provides tokenisation, POS-tagging, chunking,
and grammatical relations such as subject and di-
rect object between verbs and phrases, and is
based on memory-based classification as imple-
mented in TiMBL (Daelemans et al., 2004). The
five most frequently recurring phrases that occur
</bodyText>
<figure confidence="0.987458433333333">
Step 4
Step 1
Step 2
Step 3
Art.
1
...&lt;term 1&gt;...&lt;term 2&gt;...
Term 1
if path length == 1
find path length
Indexed
Wikipedia
corpus
if path
length == 2
Term 2
Art.
2
Term 1
Interm.
Term
find intermediate
value in database
if
found
Interm.
Term
Term 2
extract
&lt;term 1&gt;&lt;relation&gt;&lt;term 2&gt;
</figure>
<page confidence="0.996448">
64
</page>
<bodyText confidence="0.999983727272727">
between the column pairs, where the subject of the
sentence is a value from one of the two columns,
are presented to the human annotators. The cut-off
of five was chosen to prevent the annotators from
having to evaluate too many relations and to only
present those that occur more often, and are hence
less likely to be misses. Misses can for instance
be induced by ambiguous person names that also
accidentally match location names (e.g., Dakota).
In Section 7 we discuss methods to remedy this in
future work.
</bodyText>
<sectionHeader confidence="0.954557" genericHeader="method">
5 Evaluating Relations from Wikipedia
</sectionHeader>
<bodyText confidence="0.999967555555555">
Four human judges evaluated the relations be-
tween the ontological class pairs that were ex-
tracted from Wikipedia. Evaluating semantic rela-
tions automatically is hard, if not impossible, since
the same relation can be expressed in many ways,
and would require a gold standard of some sort,
which for this domain (as well as for many cul-
tural heritage domains) is not available.
The judges were presented with the five highest-
ranked candidate labels per column pair, as well a
longer snippet of text containing the candidate la-
bel, to resolve possible ambiguity. The items in
each list were scored according to the total recip-
rocal rank (TRR) (Radev et al., 2002). For every
correct answer 1/n points are given, where n de-
notes the position of the answer in the ranked list.
If there is more than 1 correct answer the points
will be added up. For example, if in a list of five,
two correct answers occur on positions 2 and 4, the
TRR would be calculated as (1/2 + 1/4) _ .75.
The TRR scores were normalised for the number
of relation candidates that were retrieved, as for
some column pairs less than five relation candi-
dates were retrieved.
As an example, for the column pair “Province”
and “Genus”, the judges were presented with the
relations shown in Table 2. The direction arrow
in the first column denotes that the “Genus” value
occurred before the “Province” value.
The human judges were sufficiently familiar
with the domain to evaluate the relations, and had
the possibility to gain extra knowledge about the
class pairs through access to the full Wikipedia
articles from which the relations were extracted.
Inter-annotator agreement was measured using
Fleiss’s Kappa coefficient (Fleiss, 1971).
</bodyText>
<sectionHeader confidence="0.997111" genericHeader="evaluation">
6 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.99996474">
As expected, between certain columns there are
more relations than between others. In total 140
relation candidates were retrieved directly, and
303 relation label candidates were retrieved via an
intermediate Wikipedia article. We work with the
assumption that these columns have a stronger on-
tological relation than others. For some database
columns we could not retrieve any relations, such
as the “collection date” field. This is not sur-
prising, as even though Wikipedia contains pages
about dates (‘what happened on this day’), it is
unlikely that it would link to such a domain spe-
cific event such as an animal specimen collec-
tion. Relations between instances denoting per-
sons and other concepts in our domain are also not
discovered through this approach. This is due to
the fact that many of the biologists named in the
database do not have a Wikipedia page dedicated
to them, indicating the boundaries of Wikipedia’s
domain specific content. Although not ideal, a
named-entity recognition filter could be applied to
the database after which person names can be re-
trieved from other resources.
Occasionally we retrieve a Wikipedia article for
a value from a person name column, but in most
cases this mistakenly matches with a Wikipedia
article on a location, as last names in Dutch are
often derived from place names. Another problem
induced by incorrect data is the incorrect match
of Wikipedia pages on certain values from the
“Town” and “Province” columns. Incorrect rela-
tion candidates are retrieved because for instance
the value ‘China’ occurs in both the “Town” and
the “Province” columns. A data cleaning step
would solve these two problems.
From each column pair the highest rated rela-
tion was selected with which we constructed the
ontology displayed in Figure 2. As the figure
shows, the relations that are discovered are not
only ‘is a’-relations one would find in strictly hier-
archical resources such as a zoological taxonomy
or geographical resource.
The numbers in the relation labels in Figure 2
denote the average TRR scores given by the four
judges on all relation label candidates that the
judges were presented with for that column pair.
The scores for the relations between the taxo-
nomic classes in our domain were particularly
high, meaning that in many cases all relation can-
didates presented to the judges were assessed as
</bodyText>
<page confidence="0.999114">
65
</page>
<table confidence="0.9953796">
Direction Label Snippet
� is found in is a genus of venomous pitvipers found in Asia from Pakistan, through India,
� is endemic to Cross Frogs) is a genus of microhylid frogs endemic to Southern Philippine,
� are native to are native to only two countries: the United States and
� is known as is a genus of pond turtles also known as Cooter Turtles, especially in the state of
</table>
<tableCaption confidence="0.998956">
Table 2: Relation candidates for Province and Genus column pair
</tableCaption>
<bodyText confidence="0.999810466666667">
correct. The inter-annotator agreement was n _
0.63, which is not perfect, but reasonable. Most
disagreement is due to vague relation labels such
as ‘may refer to’ as found between “Province” and
“Country”. If a relation that occurred fewer than 5
times was judged incorrect by the majority of the
judges the relation was not included in Figure 2.
Manual fine-tuning and post-processing of the
results could filter out synonyms such as those
found for relations between “Town” and other
classes in the domain. This would for instance de-
fine one particular relation label for the relations
‘is a town in’ and ‘is a municipality in’ that the sys-
tem discovered between “Town” and “Province”
and “Town” and “Country”, respectively.
</bodyText>
<sectionHeader confidence="0.99579" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999990907692308">
In this work we have shown that it is possible
to extract ontological relation labels for domain-
specific data from Wikipedia. The main contri-
bution that makes our work different from other
work on relation extraction from Wikipedia is that
the link structure is used as a strong indication of
the presence of a meaningful relation. The pres-
ence of a link is incorporated in our system by only
using sentences from Wikipedia articles that con-
tain links to other Wikipedia articles. Only those
sentences are parsed that contain the two terms we
aim to find a relation between, after which the verb
phrase and possibly the article or preposition fol-
lowing it are selected for evaluation by four human
judges.
The advantage of the pre-selection of content
that may contain a meaningful relation makes our
approach fast, as it is not necessary to parse the
whole corpus. By adding the constraint that at
least one of the query terms should be the sub-
ject of a sentence, and by ranking results by fre-
quency, our system succeeds in extracting correct
and informative relations labels. However, there is
clearly some room for improvement, for instance
in the coverage of more general types of infor-
mation such as dates and person names. For this
we intend to incorporate more domain specific re-
sources, such as research papers from the domain
that may mention persons from our database. We
are also looking into sending queries to the web,
whilst keeping the constraint of hyperlink pres-
ence.
Another factor that may help back up the rela-
tions already discovered is more evidence for ev-
ery relation. Currently we only include sentences
in our Wikipedia corpus that contain the literal
words from the title of the article, to ensure we
have content that is actually about the article and
not a related topic. This causes many sentences
in which the topic is referred to via anaphoric ex-
pressions to be missed. (Nguyen et al., 2007) take
the most frequently used pronoun in the article as
referring to the topic. This still leaves the prob-
lem of cases in which a person is first mentioned
by his/her full name and subsequently only by last
name. Coreference resolution may help to solve
this, although accuracies of current systems for en-
cyclopaedic text are often not much higher than
baselines such as those adopted by (Nguyen et al.,
2007).
Errors in the database lead to some noise in
the selection of the correct Wikipedia article. The
queries we used are mostly single-word and two-
word terms, which makes disambiguation hard.
Fortunately, we have access to the class label (i.e.,
the database column name) which may be added
to the query to prevent retrieval of an article about
a country when a value from a person name col-
umn is queried. We would also like to inves-
tigate whether querying terms from a particular
database column to Wikipedia can identify incon-
sistencies in the database and hence perform a
database cleanup. Potentially, extraction of re-
lation labels from Wikipedia articles can also be
used to assign types to links in Wikipedia.
</bodyText>
<sectionHeader confidence="0.998817" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994773">
The authors would like to thank the anonymous
reviewers for their comments. This research
</bodyText>
<page confidence="0.931206">
66
</page>
<figureCaption confidence="0.996651">
Figure 2: Graph of relations between columns, with TRR scores in parentheses
</figureCaption>
<figure confidence="0.999411808510638">
is a
is a
(1.000)
(0.833)
Location
is found in
(0.566)
Order
Species
Town
Type
is a
is a town in
(0.794)
on the island of
(0.500)
(1.000)
Family
is a municipality in
(0.891)
is a
(1.000)
Type Name
is found in
(0.573)
is in
(0.500)
Country
Class
(0.750)
is a
Genus
occur in
(0.750)
is a town in
(0.759)
is found in
(0.635)
may refer to
(0.560)
occur in
(0.333)
may refer to
(0.482)
Province
is a
(0.854)
</figure>
<bodyText confidence="0.99954575">
was funded as part of the Continuous Access to
Cultural Heritage (CATCH) programme of the
Netherlands Organisation for Scientific Research
(NWO).
</bodyText>
<sectionHeader confidence="0.998642" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999102637931034">
S¨oren Auer and Jens Lehmann. 2007. What have inns-
bruck and leipzig in common? extracting seman-
tics from wiki content. In Franconi et al., editor,
Proceedings of European Semantic Web Conference
(ESWC’07), volume 4519 of Lecture Notes in Com-
puter Science, pages 503–517, Innsbruck, Austria,
June 3 - 7. Springer.
A. L. Barabasi and R. Albert. 1999. Emer-
gence of scaling in random networks. Science,
286(5439):509–512, October.
Sebastian Blohm and Philipp Cimiano. 2007. Using
the web to reduce data sparseness in pattern-based
information extraction. In Proceedings of the 11th
European Conference on Principles and Practice of
Knowledge Discovery in Databases (PKDD), War-
saw, Poland, September. Springer.
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic relat-
edness. Computational Linguistics, 32(1):13–47.
Sergey Chernov, Tereza Iofciu, Wolfgang Nejdl, and
Xuan Zhou. 2006. Extracting semantic relation-
ships between wikipedia categories. In Proceedings
of the First Workshop on Semantic Wikis - From Wiki
to Semantics [SemWiki2006] - at ESWC 2006, pages
153 – 163, Karlsruhe, Germany, May 15.
Walter Daelemans, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing. In Pro-
ceedings of CoNLL’99, pages 53–60, Bergen, Nor-
way, June 12.
Walter Daelemans, Jakub Zavrel, Ko Van der Sloot,
and Antal Van den Bosch. 2004. Timbl: Tilburg
memory based learner, version 5.1, reference guide.
Technical Report 04-02, ILK/Tilburg University.
J. L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378–382.
Jim Giles. 2005. Internet encyclopaedias go head to
head. Nature, 438:900–901.
Jaap Kamps and Marijn Koolen. 2008. The impor-
tance of link evidence in wikipedia. In Craig Mac-
donald, Iadh Ounis, Vassilis Plachouras, Ian Rutven,
and Ryen W. White, editors, Advances in Infor-
mation Retrieval: 30th European Conference on
IR Research (ECIR 2008), volume 4956 of Lecture
Notes in Computer Science, pages 270–282, Glas-
gow, Scotland, March 30 - April 3. Springer Verlag.
Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio.
2008. Wikipedia link structure and text mining for
semantic relation extraction towards a huge scale
global web ontology. In Proceedings of Sem-
Search 2008 CEUR Workshop, pages 59–73, Tener-
ife, Spain, June 2.
Dat P. T. Nguyen, Yutaka Matsuo, and Mitsuru
Ishizuka. 2007. Exploiting syntactic and semantic
information for relation extraction from wikipedia.
In Proceedings of Workshop on Text-Mining &amp; Link-
Analysis (TextLink 2007) at IJCAI 2007, pages
1414–1420, Hyderabad, India, January 7.
</reference>
<page confidence="0.988621">
67
</page>
<reference confidence="0.999752294117647">
Dragomir R. Radev, Hong Q, Harris Wu, and Weiguo
Fan. 2002. Evaluating web-based question answer-
ing systems. In Demo section, LREC 2002, Las Pal-
mas, Spain, June.
F. M. Suchanek, G. Ifrim, and G. Wiekum. 2006.
Leila: Learning to extract information by linguistic
analysis. In Proceedings of the ACL-06 Workshop
on Ontology Learning and Population, pages 18–25,
Sydney, Australia, July.
Zareen Saba Syed, Tim Finin, and Anupam Joshi.
2008. Wikitology: Using wikipedia as an ontology.
Technical report, University of Maryland, Baltimore
County.
Max V¨olkel, Markus Kr¨otzsch, Denny Vrandecic,
Heiko Haller, and Rudi Studer. 2006. Semantic
wikipedia. In WWW 2006, pages 585–594, Edin-
burgh, Scotland.
</reference>
<page confidence="0.999447">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.171310">
<title confidence="0.998685">Instance-driven Discovery of Ontological Relation Labels</title>
<author confidence="0.998737">Marieke van_Erp</author>
<author confidence="0.998737">Antal van_den_Bosch</author>
<author confidence="0.998737">Sander Wubben</author>
<author confidence="0.998737">Steve</author>
<affiliation confidence="0.76352">ILK Research Tilburg centre for Creative Tilburg</affiliation>
<address confidence="0.252044">The</address>
<abstract confidence="0.99827747368421">An approach is presented to the automatic discovery of labels of relations between pairs of ontological classes. Using a hyperlinked encyclopaedic resource, we gather evidence for likely predicative labels by searching for sentences that describe relations between terms. The terms are instances of the pair of ontological classes under consideration, drawn from a populated knowledge base. Verbs or verb phrases are automatically extracted, yielding a ranked list of candidate relations. Human judges rate the extracted relations. The extracted relations provide a basis for automatic ontology discovery from a non-relational database. The approach is demonstrated on a database from the natural history domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Jens Lehmann</author>
</authors>
<title>What have innsbruck and leipzig in common? extracting semantics from wiki content.</title>
<date>2007</date>
<booktitle>Proceedings of European Semantic Web Conference (ESWC’07),</booktitle>
<volume>4519</volume>
<pages>503--517</pages>
<editor>In Franconi et al., editor,</editor>
<publisher>Springer.</publisher>
<location>Innsbruck, Austria,</location>
<contexts>
<context position="5066" citStr="Auer and Lehmann, 2007" startWordPosition="799" endWordPosition="802">ikipedia is also an attractive resource due to its size (currently nearly 12 million articles in over 250 languages). Additionally, Wikipedia’s strongly hyperlinked structure closely resembles a semantic net, with its untyped (but directed) relations between the concepts represented by the article topics. Since the hyperlinks in Wikipedia indicate a relations between two encyclopaedia articles, we aim at discovering the type of relation such a link denotes through the use of syntactic parsing of the text in which the link occurs. The idea of using Wikipedia for relation extraction is not new (Auer and Lehmann, 2007; Nakayama et al., 2008; Nguyen et al., 2007; Suchanek et al., 2006; Syed et al., 2008). However, most studies so far focus on the structured information already explicit in Wikipedia, such as its infoboxes and categories. The main contributions of our work are that we focus on the information need emerging from a specific domain, and that we test a method of pre-selection of sentences to extract relations from. The selection is based on the assumption that the strongest and most reliable lexical relations are those expressed by hyperlinks in Wikipedia pages that relate an article topic to ano</context>
<context position="8810" citStr="Auer and Lehmann, 2007" startWordPosition="1426" endWordPosition="1429">or instance utilise the category structure in Wikipedia as an upper ontology to predict concepts common to a set of documents. In (Suchanek et al., 2006) an ontology is constructed by combining entities and relations between these extracted from Wikipedia through Wikipedia’s category structure and WordNet. This results in a large “is-a” hierarchy, drawing on the basis of WordNet, while further relation enrichments come from Wikipedia’s category 61 structure. (Chernov et al., 2006) also exploit the Wikipedia category structure to which concepts in the articles are linked to extract relations. (Auer and Lehmann, 2007) take a different approach in that they focus on utilising the structure present in infoboxes. Infoboxes are consistently formatted tables in articles that provide summary information, such as information about area, population and language for countries, and birth dates and places for persons. Although infoboxes provide rich structured information, their templates are not yet standardised, and their use has not permeated throughout the whole of Wikipedia. Although the category and infobox structures in Wikipedia already provide a larger coverage at the concept or term level than for instance </context>
</contexts>
<marker>Auer, Lehmann, 2007</marker>
<rawString>S¨oren Auer and Jens Lehmann. 2007. What have innsbruck and leipzig in common? extracting semantics from wiki content. In Franconi et al., editor, Proceedings of European Semantic Web Conference (ESWC’07), volume 4519 of Lecture Notes in Computer Science, pages 503–517, Innsbruck, Austria, June 3 - 7. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Barabasi</author>
<author>R Albert</author>
</authors>
<title>Emergence of scaling in random networks.</title>
<date>1999</date>
<journal>Science,</journal>
<volume>286</volume>
<issue>5439</issue>
<contexts>
<context position="17439" citStr="Barabasi and Albert, 1999" startWordPosition="2800" endWordPosition="2803"> be inversed to yield a semantic relatedness metric. Computing the path-length between terms c1 and c2 can be done using Formula 1 where P is the set of paths connecting c1 to c2 and Np is the number of nodes in path p. 1The double brackets indicate Wikilinks 63 1 relpath(c1, c2) = argmaxp�P (1) �p We search for shortest paths in a semantic network that is constructed by mapping the concepts in Wikipedia to nodes, and the links between the concepts to edges. This generates a very large network (millions of nodes and tens of millions of edges), but due to the fact that Wikipedia is scale-free (Barabasi and Albert, 1999) (its connectedness degree distribution follows a powerlaw), paths stay relatively short. By indexing both incoming and outgoing links, a bidirectional breadth-first search can be used to find shortest paths between concepts. This means that the search is divided in two chains: a forward chain from c1 and a backward chain to c2. As soon as the two chains are connected, a shortest path is found. 4 Extracting Relations from Wikipedia Each query pair containing two values from two database columns are sent to the system. The system processes each term pair in four steps. A schematic overview of t</context>
</contexts>
<marker>Barabasi, Albert, 1999</marker>
<rawString>A. L. Barabasi and R. Albert. 1999. Emergence of scaling in random networks. Science, 286(5439):509–512, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Blohm</author>
<author>Philipp Cimiano</author>
</authors>
<title>Using the web to reduce data sparseness in pattern-based information extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD),</booktitle>
<publisher>Springer.</publisher>
<location>Warsaw, Poland,</location>
<contexts>
<context position="7837" citStr="Blohm and Cimiano, 2007" startWordPosition="1272" endWordPosition="1275">intrinsically different from the hierarchical top down architecture as seen in WordNet, as a hyperlink has a direction, but not a type. A Wikipedia article can contain any number of links, pointing to any other Wikipedia article. Wikipedia guidelines state however that wikilinks (hyperlinks referring to another Wikipedia page) should only be added when relevant to the topic of the article. Due to the fact that most users tend to adhere to guidelines for editing Wikipedia pages and the fact that articles are under constant scrutiny of their viewers, most links in Wikipedia are indeed relevant (Blohm and Cimiano, 2007; Kamps and Koolen, 2008). The structure and breadth of Wikipedia is a potentially powerful resource for information extraction which has not gone unnoticed in the natural language processing (NLP) community. Preprocessing of Wikipedia content in order to extract non-trivial relations has been addressed in a number of studies. (Syed et al., 2008) for instance utilise the category structure in Wikipedia as an upper ontology to predict concepts common to a set of documents. In (Suchanek et al., 2006) an ontology is constructed by combining entities and relations between these extracted from Wiki</context>
</contexts>
<marker>Blohm, Cimiano, 2007</marker>
<rawString>Sebastian Blohm and Philipp Cimiano. 2007. Using the web to reduce data sparseness in pattern-based information extraction. In Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), Warsaw, Poland, September. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="16615" citStr="Budanitsky and Hirst, 2006" startWordPosition="2654" endWordPosition="2657"> Relation discovery between terms (instantiations of different ontological classes) that have a page in Wikipedia is best performed after establishing if a sufficiently strong relation between the two terms under consideration actually exists. To do this, the semantic relatedness of those two terms or concepts needs to be computed first. Semantic relatedness can denote every possible relation between two concepts, unlike semantic similarity, which typically denotes only certain hierarchical relations (like hypernymy and synonymy) and is often computed using hierarchical networks like WordNet (Budanitsky and Hirst, 2006). A simple and effective way of computing semantic relatedness between two concepts c1 and c2 is measuring their distance in a semantic network. This results in a semantic distance metric, which can be inversed to yield a semantic relatedness metric. Computing the path-length between terms c1 and c2 can be done using Formula 1 where P is the set of paths connecting c1 to c2 and Np is the number of nodes in path p. 1The double brackets indicate Wikilinks 63 1 relpath(c1, c2) = argmaxp�P (1) �p We search for shortest paths in a semantic network that is constructed by mapping the concepts in Wiki</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Chernov</author>
<author>Tereza Iofciu</author>
<author>Wolfgang Nejdl</author>
<author>Xuan Zhou</author>
</authors>
<title>Extracting semantic relationships between wikipedia categories.</title>
<date>2006</date>
<booktitle>In Proceedings of the First Workshop on Semantic Wikis - From Wiki to Semantics [SemWiki2006] - at ESWC</booktitle>
<pages>153--163</pages>
<location>Karlsruhe, Germany,</location>
<contexts>
<context position="8672" citStr="Chernov et al., 2006" startWordPosition="1405" endWordPosition="1408">ocessing of Wikipedia content in order to extract non-trivial relations has been addressed in a number of studies. (Syed et al., 2008) for instance utilise the category structure in Wikipedia as an upper ontology to predict concepts common to a set of documents. In (Suchanek et al., 2006) an ontology is constructed by combining entities and relations between these extracted from Wikipedia through Wikipedia’s category structure and WordNet. This results in a large “is-a” hierarchy, drawing on the basis of WordNet, while further relation enrichments come from Wikipedia’s category 61 structure. (Chernov et al., 2006) also exploit the Wikipedia category structure to which concepts in the articles are linked to extract relations. (Auer and Lehmann, 2007) take a different approach in that they focus on utilising the structure present in infoboxes. Infoboxes are consistently formatted tables in articles that provide summary information, such as information about area, population and language for countries, and birth dates and places for persons. Although infoboxes provide rich structured information, their templates are not yet standardised, and their use has not permeated throughout the whole of Wikipedia. A</context>
</contexts>
<marker>Chernov, Iofciu, Nejdl, Zhou, 2006</marker>
<rawString>Sergey Chernov, Tereza Iofciu, Wolfgang Nejdl, and Xuan Zhou. 2006. Extracting semantic relationships between wikipedia categories. In Proceedings of the First Workshop on Semantic Wikis - From Wiki to Semantics [SemWiki2006] - at ESWC 2006, pages 153 – 163, Karlsruhe, Germany, May 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Sabine Buchholz</author>
<author>Jorn Veenstra</author>
</authors>
<title>Memory-based shallow parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of CoNLL’99,</booktitle>
<pages>53--60</pages>
<location>Bergen, Norway,</location>
<contexts>
<context position="20542" citStr="Daelemans et al., 1999" startWordPosition="3332" endWordPosition="3335">rom the country column. Here, the initial path we find is Hylidae H Sphaenorhynchys --+ Brazil. We find that the article-in-the-middle value (Sphaenorhynchys) indeed occurs in our database, in the taxonomic genus column. We assume this link is evidence for co-occurrence. Thus, the relevant sentences from the Wikipedia articles on Hylidae and Sphaenorhynchys, and between articles on Sphaenorhynchys and Brazil are added to the possible relations between “order” – “genus” and “genus” – “country”. Subsequently, the selected sentences are POStagged and parsed using the Memory Based Shallow Parser (Daelemans et al., 1999). This parser provides tokenisation, POS-tagging, chunking, and grammatical relations such as subject and direct object between verbs and phrases, and is based on memory-based classification as implemented in TiMBL (Daelemans et al., 2004). The five most frequently recurring phrases that occur Step 4 Step 1 Step 2 Step 3 Art. 1 ...&lt;term 1&gt;...&lt;term 2&gt;... Term 1 if path length == 1 find path length Indexed Wikipedia corpus if path length == 2 Term 2 Art. 2 Term 1 Interm. Term find intermediate value in database if found Interm. Term Term 2 extract &lt;term 1&gt;&lt;relation&gt;&lt;term 2&gt; 64 between the column</context>
</contexts>
<marker>Daelemans, Buchholz, Veenstra, 1999</marker>
<rawString>Walter Daelemans, Sabine Buchholz, and Jorn Veenstra. 1999. Memory-based shallow parsing. In Proceedings of CoNLL’99, pages 53–60, Bergen, Norway, June 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko Van der Sloot</author>
<author>Antal Van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory based learner, version 5.1, reference guide.</title>
<date>2004</date>
<tech>Technical Report 04-02,</tech>
<institution>ILK/Tilburg University.</institution>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2004</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, and Antal Van den Bosch. 2004. Timbl: Tilburg memory based learner, version 5.1, reference guide. Technical Report 04-02, ILK/Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="23375" citStr="Fleiss, 1971" startWordPosition="3814" endWordPosition="3815"> less than five relation candidates were retrieved. As an example, for the column pair “Province” and “Genus”, the judges were presented with the relations shown in Table 2. The direction arrow in the first column denotes that the “Genus” value occurred before the “Province” value. The human judges were sufficiently familiar with the domain to evaluate the relations, and had the possibility to gain extra knowledge about the class pairs through access to the full Wikipedia articles from which the relations were extracted. Inter-annotator agreement was measured using Fleiss’s Kappa coefficient (Fleiss, 1971). 6 Results and Evaluation As expected, between certain columns there are more relations than between others. In total 140 relation candidates were retrieved directly, and 303 relation label candidates were retrieved via an intermediate Wikipedia article. We work with the assumption that these columns have a stronger ontological relation than others. For some database columns we could not retrieve any relations, such as the “collection date” field. This is not surprising, as even though Wikipedia contains pages about dates (‘what happened on this day’), it is unlikely that it would link to suc</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>J. L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Giles</author>
</authors>
<title>Internet encyclopaedias go head to head.</title>
<date>2005</date>
<journal>Nature,</journal>
<pages>438--900</pages>
<contexts>
<context position="4441" citStr="Giles, 2005" startWordPosition="702" endWordPosition="703">iation for Computational Linguistics 60 are organised by the database columns, and there is a one-to-one relationship between the database columns and the classes in our ontology. Any approach that bases itself on text to discover relations, is dependent on the quality of that text. In this study we opt for Wikipedia as a resource from which to extract relations between terms. Although the status of Wikipedia as a dependable resource is debated, in part because of its dynamic nature, there is some evidence that Wikipedia can be as reliable a source as one that is maintained solely by experts (Giles, 2005). Wikipedia is also an attractive resource due to its size (currently nearly 12 million articles in over 250 languages). Additionally, Wikipedia’s strongly hyperlinked structure closely resembles a semantic net, with its untyped (but directed) relations between the concepts represented by the article topics. Since the hyperlinks in Wikipedia indicate a relations between two encyclopaedia articles, we aim at discovering the type of relation such a link denotes through the use of syntactic parsing of the text in which the link occurs. The idea of using Wikipedia for relation extraction is not ne</context>
</contexts>
<marker>Giles, 2005</marker>
<rawString>Jim Giles. 2005. Internet encyclopaedias go head to head. Nature, 438:900–901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Marijn Koolen</author>
</authors>
<title>The importance of link evidence in wikipedia. In</title>
<date>2008</date>
<booktitle>Advances in Information Retrieval: 30th European Conference on IR Research (ECIR 2008),</booktitle>
<volume>4956</volume>
<pages>270--282</pages>
<editor>Craig Macdonald, Iadh Ounis, Vassilis Plachouras, Ian Rutven, and Ryen W. White, editors,</editor>
<publisher>Springer Verlag.</publisher>
<location>Glasgow, Scotland,</location>
<contexts>
<context position="5700" citStr="Kamps and Koolen, 2008" startWordPosition="908" endWordPosition="911">et al., 2008; Nguyen et al., 2007; Suchanek et al., 2006; Syed et al., 2008). However, most studies so far focus on the structured information already explicit in Wikipedia, such as its infoboxes and categories. The main contributions of our work are that we focus on the information need emerging from a specific domain, and that we test a method of pre-selection of sentences to extract relations from. The selection is based on the assumption that the strongest and most reliable lexical relations are those expressed by hyperlinks in Wikipedia pages that relate an article topic to another page (Kamps and Koolen, 2008). The selection procedure retains only sentences in which the topic of the article, identified by matching words in the article title, links to another Wikipedia article. The benefit of the preselection of sentences is that it reduces the workload for the syntactic parser. Since the system is intentionally kept lightweight, the extraction of relations from Wikipedia is sufficiently fast, and we observe that the results are sufficient to build a basic ontology from the data. This paper is organised as follows. In Section 2 we review related work. In Section 3 the data used in this work is descr</context>
<context position="7862" citStr="Kamps and Koolen, 2008" startWordPosition="1276" endWordPosition="1279">rom the hierarchical top down architecture as seen in WordNet, as a hyperlink has a direction, but not a type. A Wikipedia article can contain any number of links, pointing to any other Wikipedia article. Wikipedia guidelines state however that wikilinks (hyperlinks referring to another Wikipedia page) should only be added when relevant to the topic of the article. Due to the fact that most users tend to adhere to guidelines for editing Wikipedia pages and the fact that articles are under constant scrutiny of their viewers, most links in Wikipedia are indeed relevant (Blohm and Cimiano, 2007; Kamps and Koolen, 2008). The structure and breadth of Wikipedia is a potentially powerful resource for information extraction which has not gone unnoticed in the natural language processing (NLP) community. Preprocessing of Wikipedia content in order to extract non-trivial relations has been addressed in a number of studies. (Syed et al., 2008) for instance utilise the category structure in Wikipedia as an upper ontology to predict concepts common to a set of documents. In (Suchanek et al., 2006) an ontology is constructed by combining entities and relations between these extracted from Wikipedia through Wikipedia’s</context>
</contexts>
<marker>Kamps, Koolen, 2008</marker>
<rawString>Jaap Kamps and Marijn Koolen. 2008. The importance of link evidence in wikipedia. In Craig Macdonald, Iadh Ounis, Vassilis Plachouras, Ian Rutven, and Ryen W. White, editors, Advances in Information Retrieval: 30th European Conference on IR Research (ECIR 2008), volume 4956 of Lecture Notes in Computer Science, pages 270–282, Glasgow, Scotland, March 30 - April 3. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kotaro Nakayama</author>
<author>Takahiro Hara</author>
<author>Shojiro Nishio</author>
</authors>
<title>Wikipedia link structure and text mining for semantic relation extraction towards a huge scale global web ontology.</title>
<date>2008</date>
<booktitle>In Proceedings of SemSearch 2008 CEUR Workshop,</booktitle>
<pages>59--73</pages>
<location>Tenerife, Spain,</location>
<contexts>
<context position="5089" citStr="Nakayama et al., 2008" startWordPosition="803" endWordPosition="806">active resource due to its size (currently nearly 12 million articles in over 250 languages). Additionally, Wikipedia’s strongly hyperlinked structure closely resembles a semantic net, with its untyped (but directed) relations between the concepts represented by the article topics. Since the hyperlinks in Wikipedia indicate a relations between two encyclopaedia articles, we aim at discovering the type of relation such a link denotes through the use of syntactic parsing of the text in which the link occurs. The idea of using Wikipedia for relation extraction is not new (Auer and Lehmann, 2007; Nakayama et al., 2008; Nguyen et al., 2007; Suchanek et al., 2006; Syed et al., 2008). However, most studies so far focus on the structured information already explicit in Wikipedia, such as its infoboxes and categories. The main contributions of our work are that we focus on the information need emerging from a specific domain, and that we test a method of pre-selection of sentences to extract relations from. The selection is based on the assumption that the strongest and most reliable lexical relations are those expressed by hyperlinks in Wikipedia pages that relate an article topic to another page (Kamps and Ko</context>
<context position="9748" citStr="Nakayama et al., 2008" startWordPosition="1572" endWordPosition="1575">rovide rich structured information, their templates are not yet standardised, and their use has not permeated throughout the whole of Wikipedia. Although the category and infobox structures in Wikipedia already provide a larger coverage at the concept or term level than for instance WordNet, they do not express all possibly relevant semantic relations. Especially in specific domains, relations occur that would make the Wikipedia data structure unnecessarily dense if added, thus an approach that exploits more of the linguistic content of Wikipedia is desirable. Such approaches can be found in (Nakayama et al., 2008) and (Nguyen et al., 2007). In both works full sections of Wikipedia articles are parsed, entities are identified, and the verb between the entities is taken as the relation. They also extract relations that are not backed by a link in Wikipedia, resulting in common-sense factoids such as ‘Brescia is a city’. For a domain specific application this approach lacks precision. In our approach, we care more for high precision in finding relations than for recall; hence, we carefully pre-select ontological classes among which relations need to be found, and use these as filters on our search. The us</context>
</contexts>
<marker>Nakayama, Hara, Nishio, 2008</marker>
<rawString>Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio. 2008. Wikipedia link structure and text mining for semantic relation extraction towards a huge scale global web ontology. In Proceedings of SemSearch 2008 CEUR Workshop, pages 59–73, Tenerife, Spain, June 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dat P T Nguyen</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Exploiting syntactic and semantic information for relation extraction from wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of Workshop on Text-Mining &amp; LinkAnalysis (TextLink</booktitle>
<pages>1414--1420</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="5110" citStr="Nguyen et al., 2007" startWordPosition="807" endWordPosition="810">its size (currently nearly 12 million articles in over 250 languages). Additionally, Wikipedia’s strongly hyperlinked structure closely resembles a semantic net, with its untyped (but directed) relations between the concepts represented by the article topics. Since the hyperlinks in Wikipedia indicate a relations between two encyclopaedia articles, we aim at discovering the type of relation such a link denotes through the use of syntactic parsing of the text in which the link occurs. The idea of using Wikipedia for relation extraction is not new (Auer and Lehmann, 2007; Nakayama et al., 2008; Nguyen et al., 2007; Suchanek et al., 2006; Syed et al., 2008). However, most studies so far focus on the structured information already explicit in Wikipedia, such as its infoboxes and categories. The main contributions of our work are that we focus on the information need emerging from a specific domain, and that we test a method of pre-selection of sentences to extract relations from. The selection is based on the assumption that the strongest and most reliable lexical relations are those expressed by hyperlinks in Wikipedia pages that relate an article topic to another page (Kamps and Koolen, 2008). The sele</context>
<context position="9774" citStr="Nguyen et al., 2007" startWordPosition="1577" endWordPosition="1580">mation, their templates are not yet standardised, and their use has not permeated throughout the whole of Wikipedia. Although the category and infobox structures in Wikipedia already provide a larger coverage at the concept or term level than for instance WordNet, they do not express all possibly relevant semantic relations. Especially in specific domains, relations occur that would make the Wikipedia data structure unnecessarily dense if added, thus an approach that exploits more of the linguistic content of Wikipedia is desirable. Such approaches can be found in (Nakayama et al., 2008) and (Nguyen et al., 2007). In both works full sections of Wikipedia articles are parsed, entities are identified, and the verb between the entities is taken as the relation. They also extract relations that are not backed by a link in Wikipedia, resulting in common-sense factoids such as ‘Brescia is a city’. For a domain specific application this approach lacks precision. In our approach, we care more for high precision in finding relations than for recall; hence, we carefully pre-select ontological classes among which relations need to be found, and use these as filters on our search. The usefulness of the link struc</context>
<context position="28881" citStr="Nguyen et al., 2007" startWordPosition="4739" endWordPosition="4742"> research papers from the domain that may mention persons from our database. We are also looking into sending queries to the web, whilst keeping the constraint of hyperlink presence. Another factor that may help back up the relations already discovered is more evidence for every relation. Currently we only include sentences in our Wikipedia corpus that contain the literal words from the title of the article, to ensure we have content that is actually about the article and not a related topic. This causes many sentences in which the topic is referred to via anaphoric expressions to be missed. (Nguyen et al., 2007) take the most frequently used pronoun in the article as referring to the topic. This still leaves the problem of cases in which a person is first mentioned by his/her full name and subsequently only by last name. Coreference resolution may help to solve this, although accuracies of current systems for encyclopaedic text are often not much higher than baselines such as those adopted by (Nguyen et al., 2007). Errors in the database lead to some noise in the selection of the correct Wikipedia article. The queries we used are mostly single-word and twoword terms, which makes disambiguation hard. </context>
</contexts>
<marker>Nguyen, Matsuo, Ishizuka, 2007</marker>
<rawString>Dat P. T. Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka. 2007. Exploiting syntactic and semantic information for relation extraction from wikipedia. In Proceedings of Workshop on Text-Mining &amp; LinkAnalysis (TextLink 2007) at IJCAI 2007, pages 1414–1420, Hyderabad, India, January 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Q Hong</author>
<author>Harris Wu</author>
<author>Weiguo Fan</author>
</authors>
<title>Evaluating web-based question answering systems.</title>
<date>2002</date>
<booktitle>In Demo section, LREC</booktitle>
<location>Las Palmas, Spain,</location>
<contexts>
<context position="22335" citStr="Radev et al., 2002" startWordPosition="3636" endWordPosition="3639">between the ontological class pairs that were extracted from Wikipedia. Evaluating semantic relations automatically is hard, if not impossible, since the same relation can be expressed in many ways, and would require a gold standard of some sort, which for this domain (as well as for many cultural heritage domains) is not available. The judges were presented with the five highestranked candidate labels per column pair, as well a longer snippet of text containing the candidate label, to resolve possible ambiguity. The items in each list were scored according to the total reciprocal rank (TRR) (Radev et al., 2002). For every correct answer 1/n points are given, where n denotes the position of the answer in the ranked list. If there is more than 1 correct answer the points will be added up. For example, if in a list of five, two correct answers occur on positions 2 and 4, the TRR would be calculated as (1/2 + 1/4) _ .75. The TRR scores were normalised for the number of relation candidates that were retrieved, as for some column pairs less than five relation candidates were retrieved. As an example, for the column pair “Province” and “Genus”, the judges were presented with the relations shown in Table 2.</context>
</contexts>
<marker>Radev, Hong, Wu, Fan, 2002</marker>
<rawString>Dragomir R. Radev, Hong Q, Harris Wu, and Weiguo Fan. 2002. Evaluating web-based question answering systems. In Demo section, LREC 2002, Las Palmas, Spain, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Suchanek</author>
<author>G Ifrim</author>
<author>G Wiekum</author>
</authors>
<title>Leila: Learning to extract information by linguistic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL-06 Workshop on Ontology Learning and Population,</booktitle>
<pages>18--25</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="5133" citStr="Suchanek et al., 2006" startWordPosition="811" endWordPosition="814">early 12 million articles in over 250 languages). Additionally, Wikipedia’s strongly hyperlinked structure closely resembles a semantic net, with its untyped (but directed) relations between the concepts represented by the article topics. Since the hyperlinks in Wikipedia indicate a relations between two encyclopaedia articles, we aim at discovering the type of relation such a link denotes through the use of syntactic parsing of the text in which the link occurs. The idea of using Wikipedia for relation extraction is not new (Auer and Lehmann, 2007; Nakayama et al., 2008; Nguyen et al., 2007; Suchanek et al., 2006; Syed et al., 2008). However, most studies so far focus on the structured information already explicit in Wikipedia, such as its infoboxes and categories. The main contributions of our work are that we focus on the information need emerging from a specific domain, and that we test a method of pre-selection of sentences to extract relations from. The selection is based on the assumption that the strongest and most reliable lexical relations are those expressed by hyperlinks in Wikipedia pages that relate an article topic to another page (Kamps and Koolen, 2008). The selection procedure retains</context>
<context position="8340" citStr="Suchanek et al., 2006" startWordPosition="1355" endWordPosition="1358">cles are under constant scrutiny of their viewers, most links in Wikipedia are indeed relevant (Blohm and Cimiano, 2007; Kamps and Koolen, 2008). The structure and breadth of Wikipedia is a potentially powerful resource for information extraction which has not gone unnoticed in the natural language processing (NLP) community. Preprocessing of Wikipedia content in order to extract non-trivial relations has been addressed in a number of studies. (Syed et al., 2008) for instance utilise the category structure in Wikipedia as an upper ontology to predict concepts common to a set of documents. In (Suchanek et al., 2006) an ontology is constructed by combining entities and relations between these extracted from Wikipedia through Wikipedia’s category structure and WordNet. This results in a large “is-a” hierarchy, drawing on the basis of WordNet, while further relation enrichments come from Wikipedia’s category 61 structure. (Chernov et al., 2006) also exploit the Wikipedia category structure to which concepts in the articles are linked to extract relations. (Auer and Lehmann, 2007) take a different approach in that they focus on utilising the structure present in infoboxes. Infoboxes are consistently formatte</context>
</contexts>
<marker>Suchanek, Ifrim, Wiekum, 2006</marker>
<rawString>F. M. Suchanek, G. Ifrim, and G. Wiekum. 2006. Leila: Learning to extract information by linguistic analysis. In Proceedings of the ACL-06 Workshop on Ontology Learning and Population, pages 18–25, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zareen Saba Syed</author>
<author>Tim Finin</author>
<author>Anupam Joshi</author>
</authors>
<title>Wikitology: Using wikipedia as an ontology.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>University of Maryland, Baltimore County.</institution>
<contexts>
<context position="5153" citStr="Syed et al., 2008" startWordPosition="815" endWordPosition="818">es in over 250 languages). Additionally, Wikipedia’s strongly hyperlinked structure closely resembles a semantic net, with its untyped (but directed) relations between the concepts represented by the article topics. Since the hyperlinks in Wikipedia indicate a relations between two encyclopaedia articles, we aim at discovering the type of relation such a link denotes through the use of syntactic parsing of the text in which the link occurs. The idea of using Wikipedia for relation extraction is not new (Auer and Lehmann, 2007; Nakayama et al., 2008; Nguyen et al., 2007; Suchanek et al., 2006; Syed et al., 2008). However, most studies so far focus on the structured information already explicit in Wikipedia, such as its infoboxes and categories. The main contributions of our work are that we focus on the information need emerging from a specific domain, and that we test a method of pre-selection of sentences to extract relations from. The selection is based on the assumption that the strongest and most reliable lexical relations are those expressed by hyperlinks in Wikipedia pages that relate an article topic to another page (Kamps and Koolen, 2008). The selection procedure retains only sentences in w</context>
<context position="8185" citStr="Syed et al., 2008" startWordPosition="1329" endWordPosition="1332">n relevant to the topic of the article. Due to the fact that most users tend to adhere to guidelines for editing Wikipedia pages and the fact that articles are under constant scrutiny of their viewers, most links in Wikipedia are indeed relevant (Blohm and Cimiano, 2007; Kamps and Koolen, 2008). The structure and breadth of Wikipedia is a potentially powerful resource for information extraction which has not gone unnoticed in the natural language processing (NLP) community. Preprocessing of Wikipedia content in order to extract non-trivial relations has been addressed in a number of studies. (Syed et al., 2008) for instance utilise the category structure in Wikipedia as an upper ontology to predict concepts common to a set of documents. In (Suchanek et al., 2006) an ontology is constructed by combining entities and relations between these extracted from Wikipedia through Wikipedia’s category structure and WordNet. This results in a large “is-a” hierarchy, drawing on the basis of WordNet, while further relation enrichments come from Wikipedia’s category 61 structure. (Chernov et al., 2006) also exploit the Wikipedia category structure to which concepts in the articles are linked to extract relations.</context>
</contexts>
<marker>Syed, Finin, Joshi, 2008</marker>
<rawString>Zareen Saba Syed, Tim Finin, and Anupam Joshi. 2008. Wikitology: Using wikipedia as an ontology. Technical report, University of Maryland, Baltimore County.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max V¨olkel</author>
<author>Markus Kr¨otzsch</author>
<author>Denny Vrandecic</author>
<author>Heiko Haller</author>
<author>Rudi Studer</author>
</authors>
<title>Semantic wikipedia.</title>
<date>2006</date>
<booktitle>In WWW 2006,</booktitle>
<pages>585--594</pages>
<location>Edinburgh, Scotland.</location>
<marker>V¨olkel, Kr¨otzsch, Vrandecic, Haller, Studer, 2006</marker>
<rawString>Max V¨olkel, Markus Kr¨otzsch, Denny Vrandecic, Heiko Haller, and Rudi Studer. 2006. Semantic wikipedia. In WWW 2006, pages 585–594, Edinburgh, Scotland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>