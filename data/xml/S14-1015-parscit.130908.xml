<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.962307">
See No Evil, Say No Evil:
Description Generation from Densely Labeled Images
</title>
<author confidence="0.920861">
Mark Yatskar 1∗ Michel Galley2 Lucy Vanderwende2 Luke Zettlemoyer1
</author>
<email confidence="0.906946">
my89@cs.washington.edu mgalley@microsoft.com lucyv@microsoft.com lsz@cs.washington.edu
</email>
<affiliation confidence="0.795009">
1Computer Science &amp; Engineering 2Microsoft Research
University of Washington One Microsoft Way
Seattle, WA, 98195, USA Redmond, WA, 98052, USA
</affiliation>
<sectionHeader confidence="0.956835" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983388888889">
This paper studies generation of descrip-
tive sentences from densely annotated im-
ages. Previous work studied generation
from automatically detected visual infor-
mation but produced a limited class of sen-
tences, hindered by currently unreliable
recognition of activities and attributes. In-
stead, we collect human annotations of ob-
jects, parts, attributes and activities in im-
ages. These annotations allow us to build
a significantly more comprehensive model
of language generation and allow us to
study what visual information is required
to generate human-like descriptions. Ex-
periments demonstrate high quality output
and that activity annotations and relative
spatial location of objects contribute most
to producing high quality sentences.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.963927384615384">
Image descriptions compactly summarize com-
plex visual scenes. For example, consider the de-
scriptions of the image in Figure 1, which vary in
content but focus on the women and what they are
doing. Automatically generating such descriptions
is challenging: a full system must understand the
image, select the relevant visual content to present,
and construct complete sentences. Existing sys-
tems aim to address all of these challenges but
use visual detectors for only a small vocabulary
of words, typically nouns, associated with objects
that can be reliably found.1 Such systems are blind
∗This work was conducted at Microsoft Research.
</bodyText>
<footnote confidence="0.96110625">
1While object recognition is improving (ImageNet accu-
racy is over 90% for 1000 classes) progress in activity recog-
nition has been slower; the state of the art is below 50% mean
average precision for 40 activity classes (Yao et al., 2011).
</footnote>
<reference confidence="0.78013128">
children (Count:2)
Isa: kids, children ...
Doing: biking, riding ...
Has: pants, bike ...
Attrib: young, small ...
cars (Count:3)
Isa: ride, vehicle,...
Doing: parking,...
Has: steering wheel,...
Attrib: black, shiny,...
bike (Count:1)
Isa: bike, bicycle,...
Doing: playing,...
Has: chain, pedal,...
Attrib: silver, white,...
woman(Count:1)
Isa: person, female,...
Doing: pointing,...
Has: nose, legs,...
Attrib: tall, skinny,...
sidewalk(Count:1) women(Count:3) purses(Count:3) kids(Count:5)
Isa: sidewalk, street,... Isa: girls, models,... Isa: accessory,... Isa: group, teens,...
Doing: laying,... Doing: smiling,... Doing: containing,... Doing: walking,...
Has: stone, cracks,... Has: shorts, bags,... Has: body, straps,... Has: shoes, bags,...
Attrib: flat, wide,... Attrib: young, tan,... Attrib: black, soft,... Attrib: young,...
</reference>
<construct confidence="0.8330284">
Five young people on the street, two sharing a bicycle.
Several young people are walking near parked vehicles.
Three girls with large handbags walking down the sidewalk.
Three women walk down a city street, as seen from above.
Three young woman walking down a sidewalk looking up.
</construct>
<figureCaption confidence="0.9939156">
Figure 1: An annotated image with human generated sen-
tence descriptions. Each bounding polygon encompasses one
or more objects and is associated with a count and text la-
bels.This image has 9 high level objects annotated with over
250 textual labels.
</figureCaption>
<bodyText confidence="0.999539166666667">
to much of the visual content needed to generate
complete, human-like sentences.
In this paper, we instead study generation with
more complete visual support, as provided by hu-
man annotations, allowing us to develop more
comprehensive models than previously consid-
ered. Such models have the dual benefit of (1)
providing new insights into how to construct more
human-like sentences and (2) allowing us to per-
form experiments that systematically study the
contribution of different visual cues in generation,
suggesting which automatic detectors would be
most beneficial for generation.
In an effort to approximate relatively complete
visual recognition, we collected manually labeled
representations of objects, parts, attributes and ac-
tivities for a benchmark caption generation dataset
that includes images paired with human authored
</bodyText>
<note confidence="0.730993">
tree(Count:1)
Isa: plant,...
Doing: growing,...
Has: branches,...
Attrib: tall, green,...
</note>
<page confidence="0.9678">
110
</page>
<note confidence="0.981056">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 110–120,
Dublin, Ireland, August 23-24 2014.
</note>
<bodyText confidence="0.999725222222222">
descriptions (Rashtchian et al., 2010).2 As seen
in Figure 1, the labels include object boundaries
and descriptive text, here including the facts that
the children are “riding” and “walking” and that
they are “young.” Our goal is to be as exhaustive
as possible, giving equal treatment to all objects.
For example, the annotations in Figure 1 contain
enough information to generate the first three sen-
tences and most of the content in the remaining
two. Labels gathered in this way are a type of fea-
ture norms (McRae et al., 2005), which have been
used in the cognitive science literature to approxi-
mate human perception and were recently used as
a visual proxy in distributional semantics (Silberer
and Lapata, 2012). We present the first effort, that
we are aware of, for using feature norms to study
image description generation.
Such rich data allows us to develop significantly
more comprehensive generation models. We di-
vide generation into choices about which visual
content to select and how to realize a sentence that
describes that content. Our approach is grammar-
based, feature-rich, and jointly models both deci-
sions. The content selection model includes la-
tent variables that align phrases to visual objects
and features that, for example, measure how vi-
sual salience and spatial relationships influence
which objects are mentioned. The realization ap-
proach considers a number of cues, including lan-
guage model scores, word specificity, and relative
spatial information (e.g. to produce the best spa-
tial prepositions), when producing the final sen-
tence. When used with a reranking model, includ-
ing global cues such as sentence length, this ap-
proach provides a full generation system.
Our experiments demonstrate high quality vi-
sual content selection, within 90% of human per-
formance on unigram BLEU, and improved com-
plete sentence generation, nearly halving the dif-
ference from human performance to two base-
lines on 4-gram BLEU. In ablations, we measure
the importance of different annotations and visual
cues, showing that annotation of activities and rel-
ative bounding box information between objects
are crucial to generating human-like description.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999736333333333">
A number of approaches have been proposed
for constructing sentences from images, includ-
ing copying captions from other images (Farhadi
</bodyText>
<footnote confidence="0.959116">
2Available at : http://homes.cs.washington.edu/˜my89/
</footnote>
<bodyText confidence="0.999953960784314">
et al., 2010; Ordonez et al., 2011), using text
surrounding an image in a news article (Feng
and Lapata, 2010), filling visual sentence tem-
plates (Kulkarni et al., 2011; Yang et al., 2011;
Elliott and Keller, 2013), and stitching together ex-
isting sentence descriptions (Gupta and Mannem,
2012; Kuznetsova et al., 2012). However, due to
the lack of reliable detectors, especially for activi-
ties, many previous systems have a small vocab-
ulary and must generate many words, including
verbs, with no direct visual support. These prob-
lems also extend to video caption systems (Yu and
Siskind, 2013; Krishnamoorthy et al., 2013).
The Midge algorithm (Mitchell et al., 2012)
is most closely related to our approach, and will
provide a baseline in our experiments. Midge is
syntax-driven but again uses a small vocabulary
without direct visual support for every word. It
outputs a large set of sentences to describe all
triplets of recognized objects in the scene, but does
not include a content selection model to select the
best sentence. We extend Midge with content and
sentence selection rules to use it as a baseline.
The visual facts we annotate are motivated by
research in machine vision. Attributes are a
good intermediate representation for categoriza-
tion (Farhadi et al., 2009). Activity recognition
is an emerging area in images (Li and Fei-Fei,
2007; Yao et al., 2011; Sharma et al., 2013) and
video (Weinland et al., 2011), although less stud-
ied than object recognition. Also, parts have been
widely used in object recognition (Felzenszwalb
et al., 2010). Yet, no work tests the contribution of
these labels for sentence generation.
There is also a significant amount of work
on other grounded language problems, where re-
lated models have been developed. Visual re-
ferring expression generation systems (Krahmer
and Van Deemter, 2012; Mitchell et al., 2013;
FitzGerald et al., 2013) aim to identify specific
objects, a sub-problem we deal with when de-
scribing images more generally. Other research
generates descriptions in simulated worlds and,
like this work, uses feature rich models (Angeli
et al., 2010), or syntactic structures like PCFGs
(Chen et al., 2010; Konstas and Lapata, 2012) but
does not combine the two. Finally, Zitnick and
Parikh (2013) study sentences describing clipart
scenes. They present a number of factors influenc-
ing overall descriptive quality, several of which we
use in sentence generation for the first time.
</bodyText>
<page confidence="0.999249">
111
</page>
<sectionHeader confidence="0.998739" genericHeader="method">
3 Dataset
</sectionHeader>
<bodyText confidence="0.999392027027027">
We collected a dataset of richly annotated images
to approximate gold standard visual recognition.
In collecting the data, we sought a visual annota-
tion with sufficient coverage to support the gen-
eration of as many of the words in the original
image descriptions as possible. We also aimed to
make it as visually exhaustive as possible—giving
equal treatment to all visible objects. This ensures
less bias from annotators’ perception about which
objects are important, since one of the problems
we would like to solve is content selection. This
dataset will be available for future experiments.
We built on the dataset from (Rashtchian et
al., 2010) which contained 8,000 Flickr images
and associated descriptions gathered using Ama-
zon Mechanical Turk (MTurk). Restricting our-
selves to Creative Commons images, we sampled
500 images for annotation.
We collected annotations of images in three
stages using MTurk, and assigned each annotation
task to 3-5 workers to improve quality through re-
dundancy (Callison-Burch, 2009). Below we de-
scribe the process for annotating a single image.
Stage 1: We prompted five turkers to list all ob-
jects in an image, ignoring objects that are parts of
larger objects (e.g., the arms of a person), which
we collected later in Stage 3. This list also in-
cluded groups, such as crowds of people.
Stage 2: For each unique object label from
Stage 1, we asked two turkers to draw a polygon
around the object identified.3 In cases where the
object is a group, we also asked for the number of
objects present (1-6 or many). Finally, we created
a list of all references to the object from the first
stage, which we call the Object facet.
Stage 3: For each object or group, we prompted
three turkers to provide descriptive phrases of:
</bodyText>
<listItem confidence="0.998008222222222">
• Doing – actions the object participates in, e.g.
“jumping.”
• Parts – physical parts e.g. “legs”, or other
items in the possession of the object e.g.
“shirt.”
• Attributes – adjectives describing the object,
e.g. “red.”
• Isa – alternative names for a object e.g.
“boy”, “rider.”
</listItem>
<figureCaption confidence="0.552902">
Figure 1 shows more examples for objects
</figureCaption>
<footnote confidence="0.455133">
3We modified LabelMe (Torralba et al., 2010).
</footnote>
<bodyText confidence="0.9972673">
in a labeled image.4 We refer to all of these
annotations, including the merged Object la-
bels, as facets. These labels provide feature
norms (McRae et al., 2005), which have recently
used as a visual proxy in distributional seman-
tics (Silberer and Lapata, 2012; Silberer et al.,
2013) but have not been previous studied for gen-
eration. This annotation of 500 images (2500
sentences) yielded over 4000 object instances and
100,000 textual labels.
</bodyText>
<sectionHeader confidence="0.994739" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.9997759375">
Given such rich annotations, we can now de-
velop significantly more comprehensive genera-
tion models. In this section, we present an ap-
proach that first uses a generative model and then
a reranker. The generative model defines a dis-
tribution over content selection and content real-
ization choices, using diverse cues from the image
annotations. The reranker trades off our generative
model score, language model score (to encourage
fluency), and length to produce the final sentence.
Generative Model We want to generate a sen-
tence w� = (wi ... wn) where each word wi E V
comes from a fixed vocabulary V . The vocabu-
lary V includes all 2700 words used in descriptive
sentences in the training set.5
The model conditions on an annotated image I
that contains a set of objects O, where each ob-
ject o E O has a bounding polygon and a number
of facets containing string labels. To model the
naming of specific objects, words wi can be asso-
ciated with alignment variables ai that range over
O. One such variable is introduced for each head
noun in the sentence. Figure 2 shows alignment
variable settings with colors that match objects in
the image. Finally, as a byproduct of the hierarchi-
cal generative process, we construct an undirected
dependency tree d� over the words in w.
The complete generative model defines the
probability p(w,d, d�  |I) of a sentence w, word
alignments d, and undirected dependency tree d,
given the annotated input image I. The overall
process unfolds recursively, as seen in Figure 3.
</bodyText>
<footnote confidence="0.744827">
4In the experiments, Parts and Isa facets do not improve
performance, so we do not use them in the final model. Isa
is redundant with the Object facet, as seen in Figure 1. Also
parts like clothing, were often annotated as separate objects.
5We do not generate from image facets directly, because
only 20% of the sentences in our data can be produced like
this. Instead, we develop features which consider the similar-
ity between labels in the image and words in the vocabulary.
</footnote>
<page confidence="0.987119">
112
</page>
<figure confidence="0.974982375">
from
wv
pv(w |as,
~dc)
from
ao
pa(a&apos; |as, wv, ~dc)
wo from pn(w  |ao, ~dc)
</figure>
<figureCaption confidence="0.8112655">
Figure 2: One path through the generative model and the
Bayesian network it induces. The first row of colored circles
are alignment variables to objects in the image. The second
row is words, generated conditioned on alignments.
</figureCaption>
<bodyText confidence="0.999853333333333">
The main clause is produced by first selecting the
subject alignment as followed by the subject word
ws. It then chooses the verb and optionally the ob-
ject alignment ao and word wo. The process then
continues recursively, modifying the subject, verb,
and object of the sentence with noun and prepo-
sitional modifiers. The recursion begins at Step
2 in Figure 3. Given a parent word w and that
word’s relevant alignment variable a, the model
creates attachments where w is the grammatical
head of subsequently produced words. Choices
about whether to create noun modifiers or preposi-
tional modifiers are made in steps (a) and (b). The
process chooses values for the alignment variables
and then chooses content words, adding connec-
tive prepositions in the case of prepositional mod-
ifiers. It then chooses to end or submits new word-
alignment pairs to be recursively modified.
Each line defines a decision that must be made
according to a local probability distribution. For
example, Step 1.a defines the probability of align-
ing a subject word to various objects in the im-
age. The distributions are maximum entropy mod-
els, similar to previous work (Angeli et al., 2010),
using features described in the next section. The
induced undirected dependency tree d~ has an edge
between each word and the previously generated
word (or the input word w in Steps 2.a.i and 2.a.ii,
when no previous word is available). Figure 2
shows a possible output from the process, along
with the Bayesian network that encodes what each
decision was conditioned on during generation.
Learning We learn the model from data
{(~wi, ~di, Ii)  |i = 1... m} containing sentences
~wi, dependency trees ~di, computed with the Stan-
ford parser (de Marneffe et al., 2006), and images
</bodyText>
<figure confidence="0.905504545454545">
(f) end with
or go to (2) with
pstop
(ws, as)
(g) end with pstop or go to (2) with (wv, as)
(h) end with pstop or go to (2) with (wo, ao)
2. fora (word, alignment) (w&apos;, a) (a,b are optional):
(a) if w&apos; not verb: modify w&apos; with noun, select:
from
wn
pn(w  |a,
or go to (2) with
(b) modify w&apos; with preposition, select:
i. preposition word
if w&apos; not a verb: from
a,
else: from
a,
ii. object alignment
from
pstop
(am,wn)
wp
pp(w|
~dc)
pp(w|
wv,~dc)
ap
pa(a&apos; |a, wp, ~dc)
from
wn
pn(w  |ap,
1. for a main clause (d,e are
</figure>
<figureCaption confidence="0.706755666666667">
iv. end with
or go to (2) with
Figure 3: Generative process for producing words
</figureCaption>
<equation confidence="0.475299">
align-
ments
dependencies
Each distribution is conditioned
</equation>
<bodyText confidence="0.895790076923077">
on the partially complete path through generative process
to establish sentence context. The notation
is short hand
for
the stopping distribution.
Ii. The dependency trees define the path that was
taken through the generative process in Figure 3
and are used to create a Bayesian network for ev-
ery sentence, like in Figure 2. However, object
alignments
are latent during learning and we
must marginalize over them.
The model is trained to maximize the condi-
</bodyText>
<equation confidence="0.833352785714286">
tional marginal log-li
pstop
(ap,wn)
~w,
a~and
~d.
~dc
pstop
pstop(STOP|~w,~dc)
~ai
kelihood of the data with reg-
ularization:
L(θ) = �
i a~
</equation>
<bodyText confidence="0.992097875">
where
is the set of parameters and r is the regu-
larization coefficient. In essence, we maximize the
likelihood of every
observed Bayesian
network, while marginalizing over content selec-
tion variables we did not observe.
Because the model only includes pairwise de-
pendencies between the hidden alignment vari-
ables
the inference problem is quadratic in the
number of objects and non-convex because
is
unobserved. We optimize this objective directly
using the junction-tree algorithm to
compute the sum an
</bodyText>
<figure confidence="0.9688153">
θ
sentence’s
~a,
a~
L-BFGS,
d the gradient.6
optional), select:
(a) subject as alignment from pa(a).
(b) subject word ws from pn(w  |as,
(c) verb word
~dc)
(d) object alignment
(e) object word
i. modifier word
~dc).
ii. end with
iii. object word
~dc).
p(~a, ~wi,
with
</figure>
<footnote confidence="0.92575525">
compute the gradient, we differentiate the recurrence
in the junction-tree algori
6To
thm by applying the product rule.
</footnote>
<figure confidence="0.960313428571429">
Three girls with large handbags walking down the sidewalk
a=
𝑤=
𝑑 =
� Iii
log ~di|
θ)−r|θ|2
</figure>
<page confidence="0.995437">
113
</page>
<bodyText confidence="0.999133333333333">
Inference To describe an image, we need to
maximize over word, alignment, and the depen-
dency parse variables:
</bodyText>
<equation confidence="0.6477235">
arg majg(w, d, d  |I)
w,d,d
</equation>
<bodyText confidence="0.999840307692308">
This computation is intractable because we
need to consider all possible sentences, so we use
beam search for strings up to a fixed length.
Reranking Generating directly from the process
in Figure 3 results in sentences that may be short
and repetitive because the model score is a product
of locally normalized distributions. The reranker
takes as input a candidate list c, for an image I, as
decoded from the generative model. The candidate
list includes the top-k scoring hypotheses for each
sentence length up to a fixed maximum. A linear
scoring function is used for reranking optimized
with MERT (Och, 2003) to maximize BLEU-2.
</bodyText>
<sectionHeader confidence="0.999668" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.999880074074074">
We construct indicator features to capture vari-
ation in usage in different parts of the sen-
tence, types of objects that are mentioned, visual
salience, and semantic and visual coordination be-
tween objects. The features are included in the
maximum entropy models used to parameterize
the distributions described in Figure 3. Whenever
possible, we use WordNet Synsets (Miller, 1995)
instead of lexical features to limit over-fitting.
Features in the generative model use tests for
local properties, such as the identity of a synset
of a word in WordNet, conjoined with an iden-
tifier that indicates context in the generative pro-
cess.7 Generative model features indicate (1) vi-
sual and semantic information about objects in dis-
tributions over alignments (content selection) and
(2) preferences for referring to objects in distribu-
tions over words (content realization). Features in
the reranking model indicate global properties of
candidate sentences. Exact formulas for comput-
ing the features are in the appendix.
Visual features, such as an object’s position in
the image, are used for content selection. Pairwise
visual information between two objects, for exam-
ple the bounding box overlap between objects or
the relative position of the two objects, is included
in distributions where selection of an alignment
</bodyText>
<footnote confidence="0.97624">
7For example, in Figure 2 the context for the word “side-
walk” would be “word,syntactic-object,verb,preposition” in-
dicating it is a word, in the syntactic object of a preposition,
which was attached to a verb modifying prepositional phrase.
</footnote>
<bodyText confidence="0.999768517241379">
variable conditions on previously generated align-
ments. For verbs (Step 1.d in Figure 3) and prepo-
sitions (Step 2.b.ii), these features are conjoined
with the stem of the connective.
Semantic types of objects are also used in con-
tent selection. We define semantic types by finding
synsets of labels in objects that correspond to high
level types, a list motivated by the animacy hierar-
chy (Zaenen et al., 2004).8 Type features indicate
the type of the object referred to by an alignment
variable as well as the cross product of types when
an alignment variable is on conditioning side of
a distribution (e.g. Step 1.d). Like above, in the
presence of a connective word, these features are
conjoined with the stem of the connective.
Content realization features help select words
when conditioning on chosen alignments (e.g.
Step 1.b). These features include the identity of
the WordNet synset corresponding to a word, the
word’s depth in the synset hierarchy, the language
model score for adding that word9 and whether the
word matches labels in facets corresponding to the
object referenced by an alignment variable.
Reranking features are primarily used to over-
come issues of repetition and length in the genera-
tive distributions, more commonly used for align-
ment, than to create sentences. We use only four
features: length, the number of repetitions, gener-
ative model score, and language model score.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999822133333333">
Data We used 70% of the data for training (1750
sentences, 350 images), 15% for development, and
15% for testing (375 sentences, 75 images).
Parameters The regularization parameter was
set on the held out data to r = 8. The reranker
candidate list included the top 500 sentences for
each sentence length up to 15 and weights were
optimized with Z-MERT (Zaidan, 2009).
Metrics Our evaluation is based on BLEU-n
(Papineni et al., 2001), which considers all n-
grams up to length n. To assess human perfor-
mance using BLEU, we score each of the five ref-
erences against the four other ones and finally av-
erage the five BLEU scores. In order to make these
results comparable to BLEU scores for our model
</bodyText>
<footnote confidence="0.9813414">
8For example, human, animal, artifact (a human created
object), natural body (trees, water, ect.), or natural artifact
(stick, leaf, rock).
9We use tri-grams with Kneser-Ney smoothing over the 1
million caption data set (Ordonez et al., 2011).
</footnote>
<page confidence="0.998368">
114
</page>
<bodyText confidence="0.999698673469388">
and baselines, we perform the same five-fold aver-
aging when computing BLEU for each system.
We also compute accuracy for different syn-
tactic positions in the sentence. We look at a
number of categories: the main clause’s compo-
nents (S,V,O), prepositional phrase components,
the preposition (Pp) and their objects (Po) and
noun modifying words (N), including determiners.
Phrases match if they have an exact string match
and share context identifiers as defined in the fea-
tures sections.
Human Evaluation Annotators rated sentences
output by our full model against either human or a
baseline system generated descriptions. Three cri-
teria were evaluated: grammaticality, which sen-
tence is more complete and well formed; truthful-
ness, which sentence is more accurately capturing
something true in the image; and salience, which
sentence is capturing important things in the image
while still being concise. Two annotators anno-
tated all test pairs for all criteria for a given pair of
systems. Six annotators were used (none authors)
and agreement was high (Cohen’s kappa = 0.963,
0.823 and 0.703 for grammar, truth and salience).
Machine Translation Baseline The first base-
line is designed to see if it is possible to generate
good sentences from the facet string labels alone,
with no visual information. We use an extension of
phrase-based machine translation techniques (Och
et al., 1999). We created a virtual bitext by pair-
ing each image description (the target sentence)
with a sequence10 of visual identifiers (the source
“sentence”) listing strings from the facet labels.
Since phrases produced by turkers lack many of
the functions words needed to create fluent sen-
tences, we added one of 47 function words either
at the start or the end of each output phrase.
The translation model included standard fea-
tures such as language model score (using our cap-
tion language model described previously), word
count, phrase count, linear distortion, and the
count of deleted source words. We also define
three features that count the number of Object, Isa,
and Doing phrases, to learn a preference for types
of phrases. The feature weights are tuned with
MERT (Och, 2003) to maximize BLEU-4.
Midge Baseline As described in related work,
the Midge system creates a set of sentences to de-
scribe everything in an input image. These sen-
</bodyText>
<footnote confidence="0.9293675">
10We defined a consistent ordering of visual identifiers and
set the distortion limit of the phrase-based decoder to infinity.
</footnote>
<table confidence="0.9996998">
BL-1 BL-2 BL-3 BL-4
Human 61.0 42.0 27.8 18.3
Full Model 57.1 35.7 18.3 9.5
MT Baseline 39.8 23.6 13.2 6.1
Midge Baseline 43.5 20.2 9.4 0.0
</table>
<tableCaption confidence="0.961694">
Table 1: Results for the test set for the BLEU1-4 metrics.
</tableCaption>
<table confidence="0.99995425">
Grammar Full Other Equal
Full vs Human 7.65 19.4 72.94
Full vs MT 6.47 5.29 88.23
Full vs Midge 40.59 15.88 43.53
Truth Full Other Equal
Full vs Human 0.59 67.65 31.76
Full vs MT 30.0 10.59 59.41
Full vs Midge 51.76 27.71 23.53
Salience Full Other Equal
Full vs Human 8.82 88.24 2.94
Full vs MT 51.76 16.47 31.77
Full vs Midge 71.18 14.71 14.12
</table>
<tableCaption confidence="0.8270386">
Table 2: Human evaluation of our Full-Model in heads
up tests against Human authored sentences and baseline sys-
tems, the machine translation baseline (MT) and the Midge
inspired baseline. Bold indicates the better system. Other is
not the Full system. Equal indicates neither sentence is better.
</tableCaption>
<bodyText confidence="0.9997707">
tences must all be true, but do not have to select
the same content that a person would. It can be
adapted to our task by adding object selection and
sentence ranking rules. For object selection, we
choose the three most frequently named objects
in the scene according to a background corpus of
image descriptions. For sentence selection, we
take all sentences within one word of the average
length of a sentence in our corpus, 11, and select
the one with best Midge generation score.
</bodyText>
<sectionHeader confidence="0.9999" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999847133333333">
We report experiments for our generation pipeline
and ablations that remove data and features.
Overall Performance Table 1 shows the re-
sults on the test set. The full model consis-
tently achieves the highest BLEU scores. Overall,
these numbers suggest strong content selection by
getting high recall for individual words (BLEU-
1), but fall further behind human performance as
the length of the n-gram grows (BLEU-2 through
BLEU-4). These number match our perception
that the model is learning to produce high quality
sentences, but does not always describe all of the
important aspects of the scene or use exactly the
expected wording. Table 4 presents example out-
put, which we will discuss in more detail shortly.
</bodyText>
<page confidence="0.997378">
115
</page>
<table confidence="0.999870125">
Model BL-1 BL-2 BL-3 BL-4 S V O Pp Po N
Human 64.7 46.0 31.5 20.1 - - - - - -
Full-Model 59.0 36.9 19.3 10.5 64.9 40.4 36.8 50.0 20.7 69.1
– doing 51.1 32.6 16.9 9.2 63.2 15.8 10.5 45.5 21.6 69.7
– count 55.4 33.5 16.0 8.5 59.6 35.1 15.4 53.7 19.5 66.7
– properties 57.8 37.2 18.8 10.0 61.4 36.8 36.8 47.1 20.7 73.5
– visual 56.7 35.1 18.9 9.4 64.9 36.8 50.0 41.8 15.3 71.6
– pairwise 56.9 35.5 16.5 8.2 64.9 40.4 45.5 42.4 21.2 70.9
</table>
<tableCaption confidence="0.7789166">
Table 3: Ablation results on development data using BLEU1-4 and reporting match accuracy for sentence structures.
S: A girl playing a
guitar in the grass
R: A woman with a nylon stringed
guitar is playing in a field
</tableCaption>
<figure confidence="0.737889333333333">
S: A man playing with two
dogs in the water
R: A man is throwing a log into
a waterway while two dogs watch
S: Two men playing with
a bench in the grass
</figure>
<table confidence="0.58632">
R: Nine men are playing a game
in the park, shirts versus skins
S: Three kids sitting on a road
R: A boy runs in a race
while onlookers watch
</table>
<tableCaption confidence="0.985883666666667">
Table 4: Two good examples of output (top), and two ex-
amples of poor performance (bottom). Each image has two
captions, the system output S and a human reference R.
</tableCaption>
<bodyText confidence="0.999964222222222">
Human Evaluation Table 2 presents the results
of a human evaluation. The full model outper-
forms all baselines on every measure, but is not
always competitive with human descriptions. It
performs the best on grammaticality, where it is
judged to be as grammatical as humans. How-
ever, surprisingly, in many cases it is also often
judged equal to the other baselines. Examination
of baseline output reveals that the MT baseline of-
ten generates short sentences, having little chance
of being judged ungrammatical. Furthermore, the
Midge baseline, like our system, is a syntax-based
system and therefore often produces grammatical
sentences. Although our system performs well
with respect to the baselines on truthfulness, of-
ten the system constructs sentences with incorrect
prepositions, an issue that could be improved with
better estimates of 3-d position in the image. On
truthfulness, the MT baseline is comparable to our
system, often being judged equal, because its out-
put is short. Our system’s strength is salience, a
factor the baselines do not model.
Data Ablation Table 3 shows annotation abla-
tion experiments on the development set, where
we remove different classes of data labels to mea-
sure the performance that can be achieved with
less visual information. In all cases, the overall
behavior of the system varies, as it tries to learn to
compensate for the missing information.
Ablating actions is by far the most detrimental.
Overall BLEU score suffers and prediction accu-
racy of the verb (V) degrades significantly causing
cascading errors that affect the object of the verb
(O). Removing count information affects noun at-
tachment (N) performance. Images where deter-
miner use is important or where groups of objects
are best identified by the number (for example,
three dogs) are difficult to describe naturally. Fi-
nally, we see a tradeoff when removing properties.
There is an increase in noun modifier accuracy (N)
but a decrease in content selection quality (BL-1),
showing recall has gone down. In essence, the ap-
proach learns to stop trying to generate adjectives
and other modifiers that would rely on the missing
properties. The difference in BLEU score with the
Full-Model is small, even without these modifiers,
because there often still exists a a short output with
high accuracy.
Feature Ablation The bottom two rows in Ta-
ble 3 show ablations of the visual and pairwise
features, measuring the contribution of the visual
information provided by the bounding box anno-
tations. The ablated visual information includes
bounding-box positions and relative pairwise vi-
sual information. The pairwise ablation removes
the ability to model any interactions between ob-
jects, for example, relative bounding box or pair-
wise object type information.
Overall, prepositional phrase accuracy is most
affected. Ablating visual features significantly im-
pacts accuracy of prepositional phrases (Pp and
Po), affecting the use of preposition words the
most, and lowering fluency (BL-4). Precision in
</bodyText>
<page confidence="0.997724">
116
</page>
<bodyText confidence="0.999964789473684">
the object of the verb (O) rises; the model makes
∼ 50% fewer predictions in that position than the
Full-Model because it lacks features to coordinate
subject and object of the verb. Ablating pairwise
features has similar results. While the model cor-
rects errors in the object of the preposition (Po)
with the addition of visual features, fluency is still
worse than Full-Model, as reflected by BL-4.
Qualitative Results Table 4 has examples of
good and bad system output. The first two im-
ages are good examples, including both system
output (S) and a human reference (R). The sec-
ond two contain lower quality outputs. Overall,
the model captures common ways to refer to peo-
ple and scenes. However, it does better for images
with fewer sentient objects because content selec-
tion is less ambiguous.
Our system does well at finding important ob-
jects. For example, in the first good image, we
mention the guitar instead of the house, both of
which are prominent and have high overlap with
the woman. In the second case, we identify that
both dogs and humans tend to be important actors
in scenes but poorly identify their relationship.
The bad examples show difficult scenes. In the
first description the broad context is not identi-
fied, instead focusing on the bench (highlighted in
red). The second example identifies a weakness
in our annotation: it encodes contradictory group-
ings of the people. The groupings covers all of
the children, including the boy running, and many
subsets of the people near the grass. This causes
ambiguity and our methods cannot differentiate
them, incorrectly mentioning just the children and
picking an inappropriate verb (one participant in
the group is not sitting). Improved annotation of
groups would enable the study of generation for
more complex scenes, such as these.
</bodyText>
<sectionHeader confidence="0.987099" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999925230769231">
In this work we used dense annotations of images
to study description generation. The annotations
allowed us to not only develop new models, better
capable of generating human-like sentences, but
also to explore what visual information is crucial
for description generation. Experiments showed
that activity and bounding-box information is im-
portant and demonstrated areas of future work. In
images that are more complex, for example multi-
ple sentient objects, object grouping and reference
will be important to generating good descriptions.
Issues of this type can be explored with annota-
tions of increasing complexity.
</bodyText>
<sectionHeader confidence="0.711435" genericHeader="acknowledgments">
Appendix A
</sectionHeader>
<bodyText confidence="0.9999896">
This appendix describes the feature templates for
the generative model in greater detail.
Features in the generative model conjoin indica-
tors for local tests, such as STEM(w) which in-
dicates the stem of a word w, with a global contex-
tual identifier CONTEXT(v, d) that indicates
properties of the generation history, as described
in detail below. Table 5 provides a reference for
which feature templates are used in the generative
model distributions, as defined in Figure 3.
</bodyText>
<subsectionHeader confidence="0.994734">
8.1 Feature Templates
</subsectionHeader>
<bodyText confidence="0.999993571428571">
CONTEXT(n, d) is an indicator for a contex-
tual identifier for a variable n in the model de-
pending on the dependency structure d. There is
an indicator for all combinations of the type of n
(alignment or word), the position of n (subject,
syntactic object, verb, noun-modifier, or preposi-
tion), the position of the earliest variable along
the path to generate n, and the type of attach-
ment to that variable (noun or prepositional mod-
ifier). For example, in Figure 2 the context for
the word “sidewalk” would be “word,syntactic-
object,verb,preposition” indicating it is a word, the
object of a preposition, whose path was along a
verb modifying prepositional phrase.11
TYPE(a) indicates the high level type of an
object referred to by alignment variable a. We
use synsets to define high level types including
human, animal, artifact, natural artifact and var-
ious synsets that capture scene information,12 a
list motivated by the animacy hierarchy (Zaenen
et al., 2004). Each object is assigned a type by
finding the synset for its name (object facet), and
tracing the hypernym structure in Wordnet to find
the appropriate class, if one exists. Additionally,
the type indicates whether the object is a group or
not. For example, in Figure 2, the blue polygon
has type “person,group”, or the red bike polygon
has type “artifact,single.”
</bodyText>
<footnote confidence="0.992645111111111">
11Similarly “large” is “word,noun,subject,preposition”
while “girls” is special cased to “word,subject,root” be-
cause it has no initial attachment. The alignment vari-
able above the word handbags is “alignment,syntactic-
object,subject,preposition” because it an alignment variable,
is in the syntactic object position of a preposition and can be
located by following a subject attached pp.
12WordNet divides these into synsets expressing water,
weather, nature and a few more.
</footnote>
<page confidence="0.983449">
117
</page>
<table confidence="0.999203846153846">
Feature Family Included In Steps
CONTEXT(a&apos;, 9dc)® pa(a&apos;|9dc) 1.a, 1.d, 2.b.ii
{TYPE(a&apos;), MENTION(a&apos;, do), MENTION(a&apos;, obi), VISUAL(a&apos;)} pa(a&apos;  |a, w, 9dc)
CONTEXT(a&apos;, 9dc)® {TYPE(a) ® TYPE(a&apos;), VISUAL2(a, a&apos;)} pa(a&apos;  |a, w, 9dc) 1.d, 2.b.i
CONTEXT(a&apos;, 9dc)® pa(a&apos;  |a, w, 9dc) 1.d, 2.b.i
{TYPE(a) ® TYPE(a&apos;) ® STEM(w), VISUAL2(a, a&apos;) ® STEM(w)}
CONTEXT(a, 9dc)® pn(w  |a, 9dc) 1.b, 1.e, 2.a.i
{WORDNET(w), MATCH(w, a), SPECIFICITY(w, a), 2.b.ii
ADJECTIVE(w, a), DETERMINER(w, a)}
CONTEXT(a, 9dc) ® {MATCH(w, a), TYPE(a) ® STEM(w)} pv(w  |a, 9dc) 1.c
CONTEXT(a&apos;, 9dc) ® TYPE(a) ® STEM(wp) pp(w  |a, 9dc) 2.b.i
pp(w  |a, wv, 9dc)
CONTEXT(a&apos;, 9dc) ® STEM(wv) ® STEM(w) pp(w  |a, wv, 9dc) 2.b.i
</table>
<tableCaption confidence="0.9668365">
Table 5: Feature families and distributions that include them. ⊗ indicates the cross-product of the indi-
cator features. Distributions are listed more than once to indicate they use multiple feature families.
</tableCaption>
<bodyText confidence="0.999500453125">
VISUAL(a) returns indicators for visual facts
about the object that a aligns to. There is an in-
dicator for two quantities: (1) overlap of object’s
polygon with every horizontal third of the image,
as a fraction of the object’s area, and (2) the ob-
ject’s distance to the center of the image as frac-
tion of the diagonal of the image. Each quantity,
v, is put into three overlapping buckets: if v &gt; .1,
if v &gt; .5, and if v &gt; .9.
VISUAL2(a, a0) indicates pairwise visual
facts about two objects. There is an indicator for
the following quantities bucketed: the amount of
overlap between the polygons for a and a0 as a
fraction of the size of a’s polygon, the distance
between the center of the polygon for a and a0 as
a fraction of image’s diagonal, and the slope be-
tween the center of a and a0. Each quantity, v, is
put into three overlapping buckets: if v &gt; .1, if
v &gt; .5, and if v &gt; .9. There is an indicator for the
relative position of extremities a and a0: whether
the rightmost point of a is further right than a0’s
rightmost or leftmost point, and the same for top,
left, and bottom.
WORDNET(w) returns indicators for all hy-
pernyms of a word w. The two most specific
synsets are not used when there at least 8 options.
MENTION(a, facet) returns the union of the
WORDNET(w) features for all words w in the
facet facet for the object referred to alignment a.
ADJECTIVE(w, a) indicates four types
of features specific to adjective usage. If
MENTION(w, Attributes) is not empty, indi-
cate : (1) the satellite adjective synset of w in
Wordnet, (2) the head adjective synset of w in
Wordnet, (3) the head adjective synset conjoined
with TYPE(a), and (4) the number of times there
exists a label in the Attributes facet of a that has
the same head adjective synset as w.
DETERMINER(w, a) indicates four deter-
miner specific features. If w is a determiner, then
indicate : (1) the identity of w conjoined with the
count (the label for numerosity) of a, (2) the iden-
tity of w conjoined with an indicator for if the
count of a is greater than one, (3) the identity of w
conjoined with TYPE(a) and (4) the frequency
with which w appears before its head word in the
Flikr corpus (Ordonez et al., 2011).
MATCH(w, a), indicates all facets of object
a that contain words with the same stem as w.
SPECIFICITY(w, a) is an indicator of the
specificity of the word w when referring to the ob-
ject aligned to a. Indicates the relative depth of
w in Wordnet, as compared to all words w0 where
MATCH(w0, a) is not empty. The depth is buck-
eted into quintiles.
STEM(w) returns the Porter2 stem of w.13
The distribution for stopping, pstop(STOP |
dc, w), contains two types of features. (1) Struc-
tural features indicating for the number of times
a contextual identifier has appeared so far in the
derivation and (2) mention features indicating the
types of objects mentioned.14 To compute men-
tion features, we consider all possible types of ob-
jects, t, then there is an indicator for: (1) if 3o, 3w E
</bodyText>
<reference confidence="0.502834125">
w9: MATCH(w, o) =� 0 n TYPE(o) = t, (2) whether
3o, Aw E w9: MATCH(w, o) =� 0 n TYPE(o) = t and
(3) if (1) does not hold but (2) does.
Acknowledgments This work is partially funded by DARPA
CSSG (D11AP00277) and ARO (W911NF-12-1-0197). We
thank L. Zitnick, B. Dolan, M. Mitchell, C. Quirk, A. Farhadi,
B. Russell for helpful conversations. Also, L. Zilles, Y. Atrzi,
N. FitzGerald, T. Kwiatkowski and reviewers for comments.
</reference>
<footnote confidence="0.982268">
13http://snowball.tartarus.org/algorithms/english/stemmer.html
14Object mention features cannot contain a9 because that
creates large dependencies in inference for learning.
</footnote>
<page confidence="0.993221">
118
</page>
<sectionHeader confidence="0.98924" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999806219047619">
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon’s Me-
chanical Turk. In EMNLP, pages 286–295, August.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. JAIR,
37:397–435.
Marie-Catherine de Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generating
typed dependency parses from phrase structure
parses. In LREC, volume 6, pages 449–454.
Desmond Elliott and Frank Keller. 2013. Image De-
scription using Visual Dependency Representations.
In EMNLP.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Computer Vision and Pattern Recogni-
tion, 2009. CVPR 2009. IEEE Conference on, pages
1778–1785. IEEE.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: Generating sentences from images.
In Proceedings of the 11th European conference on
Computer Vision, ECCV’10, pages 15–29.
Pedro F Felzenszwalb, Ross B Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part-based
models. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 32(9):1627–1645.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? Automatic caption gener-
ation for news images. In ACL, pages 1239–1249.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logi-
cal forms for referring expression generation. In
EMNLP.
Ankush Gupta and Prashanth Mannem. 2012. From
image annotation to image description. In NIPS,
volume 7667, pages 196–204.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
ACL, pages 369–378.
Emiel Krahmer and Kees Van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173–218.
Niveda Krishnamoorthy, Girish Malkarnenkar, Ray-
mond Mooney, Kate Saenko, and Sergio Guadar-
rama. 2013. Generating natural-language video de-
scriptions using text-mined knowledge. Procedings
ofAAAI, 2013(2):3.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C.
Berg, and T.L. Berg. 2011. Baby talk: Understand-
ing and generating simple image descriptions. In
Computer Vision and Pattern Recognition (CVPR),
pages 1601–1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C
Berg, Tamara L Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL, pages 359–368.
Li-Jia Li and Li Fei-Fei. 2007. What, where and who?
Classifying events by scene and object recognition.
In ICCV, pages 1–8. IEEE.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris Mcnorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior Research Methods, 37(4):547–
559.
George A Miller. 1995. WordNet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, and Hal Daum´e, III.
2012. Midge: Generating image descriptions from
computer vision detections. In EACL, pages 747–
756.
Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2013. Generating expressions that refer to vis-
ible objects. In Proceedings of NAACL-HLT, pages
1174–1184.
F. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint Conf. of Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 20–28.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160–
167.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing images using 1 million
captioned photographs. In NIPS, pages 1143–1151.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL.
C. Rashtchian, P. Young, M. Hodosh, and J. Hock-
enmaier. 2010. Collecting image annotations us-
ing Amazon’s Mechanical Turk. In Proceedings
of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon’s Mechan-
ical Turk, pages 139–147.
</reference>
<page confidence="0.990288">
119
</page>
<reference confidence="0.99980111627907">
Gaurav Sharma, Fr´ed´eric Jurie, Cordelia Schmid, et al.
2013. Expanded parts model for human attribute
and action recognition in still images. In CVPR.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In EMNLP, July.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In ACL, pages 572–582.
Antonio Torralba, Bryan C Russell, and Jenny Yuen.
2010. LabelMe: Online image annotation and appli-
cations. Proceedings of the IEEE, 98(8):1467–1484.
Daniel Weinland, Remi Ronfard, and Edmond Boyer.
2011. A survey of vision-based methods for action
representation, segmentation and recognition. Com-
puter Vision and Image Understanding, 115(2):224–
241.
Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing.
Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai
Lin, Leonidas J. Guibas, and Li Fei-Fei. 2011.
Action recognition by learning bases of action at-
tributes and parts. In ICCV, Barcelona, Spain,
November.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, volume 1, pages 53–63.
Annie Zaenen, Jean Carletta, Gregory Garretson,
Joan Bresnan, Andrew Koontz-Garboden, Tatiana
Nikitina, M Catherine O’Connor, and Tom Wasow.
2004. Animacy encoding in English: why and how.
In ACL Workshop on Discourse Annotation, pages
118–125.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
C. Lawrence Zitnick and Devi Parikh. 2013. Bring-
ing semantics into focus using visual abstraction. In
CVPR.
</reference>
<page confidence="0.996253">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.917715">
<title confidence="0.9974195">See No Evil, Say No Evil: Description Generation from Densely Labeled Images</title>
<author confidence="0.968747">Yatskar</author>
<email confidence="0.969651">my89@cs.washington.edumgalley@microsoft.comlucyv@microsoft.comlsz@cs.washington.edu</email>
<affiliation confidence="0.9921855">Science &amp; Engineering Research University of Washington One Microsoft Way</affiliation>
<address confidence="0.999951">Seattle, WA, 98195, USA Redmond, WA, 98052, USA</address>
<abstract confidence="0.999681736842105">This paper studies generation of descriptive sentences from densely annotated images. Previous work studied generation from automatically detected visual information but produced a limited class of sentences, hindered by currently unreliable recognition of activities and attributes. Instead, we collect human annotations of objects, parts, attributes and activities in images. These annotations allow us to build a significantly more comprehensive model of language generation and allow us to study what visual information is required to generate human-like descriptions. Experiments demonstrate high quality output and that activity annotations and relative spatial location of objects contribute most to producing high quality sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>children</author>
</authors>
<title>Isa: kids, children ... Doing: biking, riding ... Has: pants, bike ... Attrib: young, small ...</title>
<marker>children, </marker>
<rawString>children (Count:2) Isa: kids, children ... Doing: biking, riding ... Has: pants, bike ... Attrib: young, small ...</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>