<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.850832">
Two Is Bigger (and Better) Than One: the Wikipedia Bitaxonomy Project
</title>
<author confidence="0.594457">
Tiziano Flati, Daniele Vannella, Tommaso Pasini and Roberto Navigli
</author>
<affiliation confidence="0.330357">
Dipartimento di Informatica
</affiliation>
<address confidence="0.242627">
Sapienza Universit`a di Roma
</address>
<email confidence="0.843376">
{flati,vannella,navigli}@di.uniroma1.it
p.tommaso@gmail.com
</email>
<sectionHeader confidence="0.99379" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999878416666667">
We present WiBi, an approach to the
automatic creation of a bitaxonomy for
Wikipedia, that is, an integrated taxon-
omy of Wikipage pages and categories.
We leverage the information available in
either one of the taxonomies to reinforce
the creation of the other taxonomy. Our
experiments show higher quality and cov-
erage than state-of-the-art resources like
DBpedia, YAGO, MENTA, WikiNet and
WikiTaxonomy. WiBi is available at
http://wibitaxonomy.org.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985426470589">
Knowledge has unquestionably become a key
component of current intelligent systems in many
fields of Artificial Intelligence. The creation and
use of machine-readable knowledge has not only
entailed researchers (Mitchell, 2005; Mirkin et al.,
2009; Poon et al., 2010) developing huge, broad-
coverage knowledge bases (Hovy et al., 2013;
Suchanek and Weikum, 2013), but it has also
hit big industry players such as Google (Singhal,
2012) and IBM (Ferrucci, 2012), which are mov-
ing fast towards large-scale knowledge-oriented
systems.
The creation of very large knowledge bases
has been made possible by the availability of
collaboratively-curated online resources such as
Wikipedia and Wiktionary. These resources are
increasingly becoming enriched with new con-
tent in many languages and, although they are
only partially structured, they provide a great deal
of valuable knowledge which can be harvested
and transformed into structured form (Medelyan
et al., 2009; Hovy et al., 2013). Prominent
examples include DBpedia (Bizer et al., 2009),
BabelNet (Navigli and Ponzetto, 2012), YAGO
(Hoffart et al., 2013) and WikiNet (Nastase and
Strube, 2013). The types of semantic relation
in these resources range from domain-specific, as
in Freebase (Bollacker et al., 2008), to unspec-
ified relations, as in BabelNet. However, un-
like the case with smaller manually-curated re-
sources such as WordNet (Fellbaum, 1998), in
many large automatically-created resources the
taxonomical information is either missing, mixed
across resources, e.g., linking Wikipedia cate-
gories to WordNet synsets as in YAGO, or coarse-
grained, as in DBpedia whose hypernyms link to a
small upper taxonomy.
Current approaches in the literature have mostly
focused on the extraction of taxonomies from the
network of Wikipedia categories. WikiTaxonomy
(Ponzetto and Strube, 2007), the first approach
of this kind, is based on the use of heuristics
to determine whether is-a relations hold between
a category and its subcategories. Subsequent ap-
proaches have also exploited heuristics, but have
extended them to any kind of semantic relation
expressed in the category names (Nastase and
Strube, 2013). But while the aforementioned at-
tempts provide structure for categories that sup-
ply meta-information for Wikipedia pages, sur-
prisingly little attention has been paid to the ac-
quisition of a full-fledged taxonomy for Wikipedia
pages themselves. For instance, Ruiz-Casado et
al. (2005) provide a general vector-based method
which, however, is incapable of linking pages
which do not have a WordNet counterpart. Higher
coverage is provided by de Melo and Weikum
(2010) thanks to the use of a set of effective heuris-
tics, however, the approach also draws on Word-
Net and sense frequency information.
In this paper we address the task of taxono-
mizing Wikipedia in a way that is fully indepen-
dent of other existing resources such as WordNet.
We present WiBi, a novel approach to the cre-
ation of a Wikipedia bitaxonomy, that is, a tax-
onomy of Wikipedia pages aligned to a taxonomy
of categories. At the core of our approach lies the
idea that the information at the page and category
</bodyText>
<page confidence="0.979319">
945
</page>
<note confidence="0.8311965">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 945–955,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9977945">
level are mutually beneficial for inducing a wide-
coverage and fine-grained integrated taxonomy.
</bodyText>
<sectionHeader confidence="0.814043" genericHeader="introduction">
2 WiBi: A Wikipedia Bitaxonomy
</sectionHeader>
<bodyText confidence="0.9768065">
We induce a Wikipedia bitaxonomy, i.e., a taxon-
omy of pages and categories, in 3 phases:
</bodyText>
<listItem confidence="0.954287647058824">
1. Creation of the initial page taxonomy: we
first create a taxonomy for the Wikipedia
pages by parsing textual definitions, ex-
tracting the hypernym(s) and disambiguating
them according to the page inventory.
2. Creation of the bitaxonomy: we leverage
the hypernyms in the page taxonomy, to-
gether with their links to the corresponding
categories, so as to induce a taxonomy over
Wikipedia categories in an iterative way. At
each iteration, the links in the page taxonomy
are used to identify category hypernyms and,
conversely, the new category hypernyms are
used to identify more page hypernyms.
3. Refinement of the category taxonomy: fi-
nally we employ structural heuristics to over-
come inherent problems affecting categories.
</listItem>
<bodyText confidence="0.996498333333333">
The output of our three-phase approach is a bitax-
onomy of millions of pages and hundreds of thou-
sands of categories for the English Wikipedia.
</bodyText>
<sectionHeader confidence="0.965488" genericHeader="method">
3 Phase 1: Inducing the Page Taxonomy
</sectionHeader>
<bodyText confidence="0.999980470588235">
The goal of the first phase is to induce a taxonomy
of Wikipedia pages. Let P be the set of all the
pages and let TP = (P, E) be the page taxonomy
directed graph whose nodes are pages and whose
edge set E is initially empty (E := ∅). For each
p E P our aim is to identify the most suitable gen-
eralization ph E P so that we can create the edge
(p, ph) and add it to E. For instance, given the
page APPLE, which represents the fruit meaning
of apple, we want to determine that its hypernym
is FRUIT and add the hypernym edge connecting
the two pages (i.e., E := EU{(APPLE, FRUIT)}).
To do this, we perform a syntactic step, in which
the hypernyms are extracted from the page’s tex-
tual definition, and a semantic step, in which the
extracted hypernyms are disambiguated according
to the Wikipedia inventory.
</bodyText>
<subsectionHeader confidence="0.99401">
3.1 Syntactic step: hypernym extraction
</subsectionHeader>
<bodyText confidence="0.99982725">
In the syntactic step, for each page p E P, we
extract zero, one or more hypernym lemmas, that
is, we output potentially ambiguous hypernyms
for the page. The first assumption, which follows
</bodyText>
<figure confidence="0.7535335">
Julia Fiona Roberts is an American actress
NNP NNP NNP VBZ DT JJ NN
</figure>
<figureCaption confidence="0.999755">
Figure 1: A dependency tree example with copula.
</figureCaption>
<bodyText confidence="0.988361782608696">
the Wikipedia guidelines and is validated in the
literature (Navigli and Velardi, 2010; Navigli and
Ponzetto, 2012), is that the first sentence of each
Wikipedia page p provides a textual definition for
the concept represented by p. The second assump-
tion we build upon is the idea that a lexical tax-
onomy can be obtained by extracting hypernyms
from textual definitions. This idea dates back to
the early 1970s (Calzolari et al., 1973), with later
developments in the 1980s (Amsler, 1981; Calzo-
lari, 1982) and the 1990s (Ide and V´eronis, 1993).
To extract hypernym lemmas, we draw on the
notion of copula, that is, the relation between the
complement of a copular verb and the copular verb
itself. Therefore, we apply the Stanford parser
(Klein and Manning, 2003) to the definition of a
page in order to extract all the dependency rela-
tions of the sentence. For example, given the def-
inition of the page JULIA ROBERTS, i.e., “Julia
Fiona Roberts is an American actress.”, the Stan-
ford parser outputs the set of dependencies shown
in Figure 1. The noun involved in the copula re-
lation is actress and thus it is taken as the page’s
hypernym lemma. However, the extracted hyper-
nym is sometimes overgeneral (one, kind, type,
etc.). For instance, given the definition of the
page APOLLO, “Apollo is one of the most impor-
tant and complex of the Olympian deities in an-
cient Greek and Roman religion [...].”, the only
copula relation extracted is between is and one.
To cope with this problem we use a list of stop-
words.1When such a term is extracted as hyper-
nym, we replace it with the rightmost noun of the
first following noun sequence (e.g., deity in the
above example). If the resulting lemma is again a
stopword we repeat the procedure, until a valid hy-
pernym or no appropriate hypernym can be found.
Finally, to capture multiple hypernyms, we iter-
atively follow the conj and and conj or relations
starting from the initially extracted hypernym. For
example, consider the definition of ARISTOTLE:
“Aristotle was a Greek philosopher and polymath,
a student of Plato and teacher of Alexander the
Great.” Initially, the philosopher hypernym is
selected thanks to the copula relation, then, fol-
1E.g., species, genus, one, etc. Full list available online.
</bodyText>
<figure confidence="0.566793333333333">
nn
amod
nsubj
cop
det
nn
</figure>
<page confidence="0.99302">
946
</page>
<bodyText confidence="0.999978428571428">
lowing the conjunction relations, also polymath,
student and teacher are extracted as hypernyms.
While more sophisticated approaches like Word-
Class Lattices could be applied (Navigli and Ve-
lardi, 2010), we found that, in practice, our hy-
pernym extraction approach provides higher cov-
erage, which is critical in our case.
</bodyText>
<subsectionHeader confidence="0.999693">
3.2 Semantic step: hypernym disambiguation
</subsectionHeader>
<bodyText confidence="0.999977666666667">
Since our aim is to connect pairs of pages via
hypernym relations, our second step consists of
disambiguating the obtained hypernym lemmas of
page p by associating the most suitable page with
each hypernym. Following previous work (Ruiz-
Casado et al., 2005; Navigli and Ponzetto, 2012),
as the inventory for a given lemma we consider the
set of pages whose main title is the lemma itself,
except for the sense specification in parenthesis.
For instance, given fruit as the hypernym for AP-
PLE we would like to link APPLE to FRUIT as op-
posed to, e.g., FRUIT (BAND) or FRUIT (ALBUM).
</bodyText>
<subsectionHeader confidence="0.951843">
3.2.1 Hypernym linkers
</subsectionHeader>
<bodyText confidence="0.9999777">
To disambiguate hypernym lemmas, we exploit
the structural features of Wikipedia through a
pipeline of hypernym linkers L = {Li}, applied
in cascade order (cf. Section 3.3.1). We start with
the set of page-hypernym pairs H = {(p, h)} as
obtained from the syntactic step. The successful
application of a linker to a pair (p, h) E H yields
a page ph as the most suitable sense of h, result-
ing in setting isa(p, h) = ph. At step i, the i-
th linker Li E L is applied to H and all the hy-
pernyms which the linker could disambiguate are
removed from H. This prevents lower-precision
linkers from overriding decisions taken by more
accurate ones.
We now describe the hypernym linkers. In what
follows we denote with p h*h ph the fact that the
definition of a Wikipedia page p contains an oc-
currence of h linked to page ph. Note that ph is
not necessarily a sense of h.
Crowdsourced linker If p h* ph, i.e., the hyper-
nym h is found to have been manually linked to ph
in p by Wikipedians, we assign isa(p, h) = ph.
For example, because capital was linked in the
BRUSSELS page definition to CAPITAL CITY, we
set isa(BRUSSELS, capital) = CAPITAL CITY.
Category linker Given the set W C P of
Wikipedia pages which have at least one category
in common with p, we select the majority sense
of h, if there is one, as hyperlinked across all the
definitions of pages in W:
</bodyText>
<equation confidence="0.857498">
isa(p, h) = arg max � 1(p0 h* h ph)
ph p,∈W
</equation>
<bodyText confidence="0.976255956521739">
where 1(p0 h*h ph) is the characteristic function
which equals 1 if h is linked to ph in page
p0, 0 otherwise. For example, the linker sets
isa(EGGPLANT, plant) = PLANT because most of
the pages associated with TROPICAL FRUIT, a cat-
egory of EGGPLANT, contain in their definitions
the term plant linked to the PLANT page.
Multiword linker If p mh* ph and m is a
multiword expression containing the lemma h
as one of its words, set isa(p, h) = ph. For
example, we set isa(PROTEIN, compound) =
CHEMICAL COMPOUND, as chemical compound
is linked to CHEMICAL COMPOUND in the defini-
tion of PROTEIN.
Monosemous linker If h is monosemous in
Wikipedia (i.e., there is only a single page ph for
that lemma), link it to its only sense by setting
isa(p, h) = ph. For example, we extract the
hypernym businessperson from the definition of
MERCHANT and, as it is unambiguous, we link
it to BUSINESSPERSON.
Distributional linker Finally, we provide a dis-
tributional approach to hypernym disambiguation.
We represent the textual definition of page p as a
distributional vector vp whose components are all
the English lemmas in Wikipedia. The value of
each component is the occurrence count of the cor-
responding content word in the definition of p.
The goal of this approach is to find the best
link for hypernym h of p among the pages h is
linked to, across the whole set of definitions in
Wikipedia. Formally, for each ph such that h
is linked to ph in some definition, we define the
set of pages P(ph) whose definitions contain a
link to ph, i.e., P(ph) = {p0 E P|p0 h*h ph}.
We then build a distributional vector vp, for each
p0 E P(ph) as explained above and create an ag-
gregate vector vph = Ep, Vp,. Finally, we de-
termine the similarity of p to each ph by calcu-
lating the dot product between the two vectors
sim(p, ph) = Vp · vph. If sim(p, ph) &gt; 0 for any
ph we perform the following association:
isa(p, h) = arg max sim(p, ph)
ph
For example, thanks to this linker we set
isa(VACUUM CLEANER, device) = MACHINE.
</bodyText>
<page confidence="0.992911">
947
</page>
<table confidence="0.940460333333333">
Prec. Rec. Cov.
Lemma 94.83 90.20 98.50
Sense 82.77 75.10 89.20
</table>
<tableCaption confidence="0.973502">
Table 1: Page taxonomy performance.
</tableCaption>
<figureCaption confidence="0.997653">
Figure 2: Distribution of linked hypernyms.
</figureCaption>
<subsectionHeader confidence="0.998466">
3.3 Page Taxonomy Evaluation
</subsectionHeader>
<bodyText confidence="0.999965861111111">
Statistics We applied the above linkers to the
October 2012 English Wikipedia dump. Out of
the 3,829,058 total pages, 4,270,232 hypernym
lemmas were extracted in the syntactic step for
3,697,113 pages (covering more than 96% of the
total). Due to illformed definitions, though, it
was not always possible to extract the hypernym
lemma: for example, 6 APRIL 2010 BAGHDAD
BOMBINGS is defined as “A series of bomb ex-
plosions destroyed several buildings in Baghdad”,
which only implicitly provides the hypernym.
The semantic step disambiguated 3,718,612 hy-
pernyms for 3,294,562 Wikipedia pages, i.e., cov-
ering more than 86% of the English pages with at
least one disambiguated hypernym. Figure 2 plots
the number and distribution of hypernyms disam-
biguated by our hypernym linkers.
Taxonomy quality To evaluate the quality of
our page taxonomy we randomly sampled 1,000
Wikipedia pages. For each page we provided: i)
a list of suitable hypernym lemmas for the page,
mainly selected from its definition; ii) for each
lemma the correct hypernym page(s). We calcu-
lated precision as the average ratio of correct hy-
pernym lemmas (senses) to the total number of
lemmas (senses) returned for all the pages in the
dataset, recall as the number of correct lemmas
(senses) over the total number of lemmas (senses)
in the dataset, and coverage as the fraction of
pages for which at least one lemma (sense) was
returned, independently of its correctness. Results,
both at lemma- and sense-level, are reported in Ta-
ble 1. Not only does our taxonomy show high pre-
cision and recall in extracting ambiguous hyper-
nyms, it also disambiguates more than 3/4 of the
hypernyms with high precision.
</bodyText>
<subsectionHeader confidence="0.962712">
3.3.1 Hypernym linker order
</subsectionHeader>
<bodyText confidence="0.999970666666667">
The optimal order of application of the above
linkers is the same as that presented in Section
3.2.1. It was established by selecting the combina-
tion, among all possible permutations, which max-
imized precision on a tuning set of 100 randomly
sampled pages, disjoint from our page dataset.
</bodyText>
<sectionHeader confidence="0.925657" genericHeader="method">
4 Phase 2: Inducing the Bitaxonomy
</sectionHeader>
<bodyText confidence="0.999892">
The page taxonomy built in Section 3 will serve
as a stable, pivotal input to the second phase, the
aim of which is to build our bitaxonomy, that is, a
taxonomy of pages and categories. Our key idea
is that the generalization-specialization informa-
tion available in each of the two taxonomies is
mutually beneficial. We implement this idea by
exploiting one taxonomy to update the other, and
vice versa, in an iterative way, until a fixed point
is reached. The final output of this phase is, on the
one hand, a page taxonomy augmented with addi-
tional hypernymy relations and, on the other hand,
a category taxonomy which is built from scratch.
</bodyText>
<subsectionHeader confidence="0.954824">
4.1 Initialization
</subsectionHeader>
<bodyText confidence="0.999978444444444">
Our bitaxonomy B = {TP, TC} is a pair consist-
ing of the page taxonomy TP = (P, E), as ob-
tained in Section 3, and the category taxonomy
TC = (C, ∅), which initially contains all the cate-
gories as nodes but does not include any hypernym
edge between category nodes. In the following
we describe the core algorithm of our approach,
which iteratively and mutually populates and re-
fines the edge sets E(TP) and E(TC).
</bodyText>
<subsectionHeader confidence="0.998174">
4.2 The Bitaxonomy Algorithm
</subsectionHeader>
<bodyText confidence="0.99944675">
Preliminaries Before proceeding, we define
some basic concepts that will turn out to be use-
ful for presenting our algorithm. We denote by
superT (t) the set of all ancestors of a node t in the
taxonomy T (be it TP or TC). We further define a
verification function t --*T t&apos; which, in the case of
TC, returns true if there is a path in the Wikipedia
category network between t and t&apos;, false other-
wise, and, in the case of TP, returns true if t&apos; is
a sense, i.e., a page, of a hypernym h of t (that
is, (t, h) E H, cf. Section 3.2.1). For instance,
SPORTSMEN --*TC MEN BY OCCUPATION holds
for categories because the former is a sub-category
of the latter in Wikipedia, and RADIOHEAD --*TP
BAND (MUSIC) for pages, because band is a hy-
pernym extracted from the textual definition of
RADIOHEAD and BAND (MUSIC) is a sense of
band in Wikipedia. Note that, while the super
function returns information that we have already
learned, i.e., it is in TP and TC, the --* operator
</bodyText>
<page confidence="0.991146">
948
</page>
<bodyText confidence="0.999937653846154">
holds just for candidate is-a relations, as it uses
knowledge from Wikipedia itself which is poten-
tially incorrect. For instance, SPORTSMEN --*TC
MEN’S SPORTS in the Wikipedia category net-
work, and RADIOHEAD --*TP BAND (RADIO) be-
tween the two Wikipedia pages, both hold accord-
ing to our definition of --*, while connecting the
wrong hypernym candidates. At the core of our
algorithm, explained below, is the mutual lever-
aging of the super function from one of the two
taxonomies (pages or categories) to decide about
which candidates (for which a --* relation holds)
in the other taxonomy are real hypernyms.
Finally, we define the projection operator 7r,
such that 7r(c), c E C, is the set of pages
categorized with c, and 7r(p), p E P, is the
set of categories associated with page p in
Wikipedia. For instance, the pages which belong
to the category OLYMPIC SPORTS are given by
7r(OLYMPIC SPORTS) = {BASEBALL, BOXING,
... , TRIATHLON}. Vice versa, 7r(TRIATHLON) =
{MULTISPORTS, OLYMPIC SPORTS, ... , OPEN
WATER SWIMMING}. The projection operator 7r
enables us to jump from one taxonomy to the other
and expresses the mutual membership relation be-
tween pages and categories.
Algorithm We now describe in detail the bitax-
onomy algorithm, whose pseudocode is given in
Algorithm 1. The algorithm takes as input the two
taxonomies, initialized as described in Section 4.1.
Starting from the category taxonomy (line 1), the
algorithm updates the two taxonomies in turn, un-
til convergence is reached, i.e., no more edges can
be added to any side of the bitaxonomy. Let T be
the current taxonomy considered at a given mo-
ment and T0 its dual taxonomy. The algorithm
proceeds by selecting a node t E V (T) for which
no hypernym edge (t, th) could be found up until
that moment (line 3), and then tries to infer such
a relation by drawing on the dual taxonomy T0
(lines 5-12). This is the core of the bitaxonomy al-
gorithm, in which hypernymy knowledge is trans-
ferred from one taxonomy to the other. By apply-
ing the projection operator 7r to t, the algorithm
considers those nodes t0 aligned to t in the dual
taxonomy (line 5) and obtains their hypernyms t0h
using the superT0 function (line 6). The nodes
reached in T0 act as a clue for discovering the suit-
able hypernyms for the starting node t E V (T).
To perform the discovery, the algorithm projects
each such hypernym node t0h E S and increments
the count of each projection th E 7r(t0h) (line
</bodyText>
<figure confidence="0.503877">
Algorithm 1 The Bitaxonomy Algorithm
Input: TP, TC
1: T := TC, T0 := TP
2: repeat
3: for all t E V (T) s.t. �(t, th) E E(T) do
4: reset count
5: for all t0 E ir(t) do
6: S := superT (t0)
7: for all t0h E S do
8: for all th E ir(t0h) do count(th)++ end for
9: end for
10: end for
11: ˆth := argmaxth, t❀T th count(th)
12: if count(ˆth) &gt; 0 then E(T) := E(T)U{(t, ˆth)}
13: end for
14: swap T and T0
15: until convergence
16: return {T, T0}
</figure>
<bodyText confidence="0.996939461538462">
8). Finally, the node ˆth E V (T) with maximum
count, and such that t --*T ˆth holds, if one exists,
is promoted as hypernym of t and a new hypernym
edge (t, ˆth) is added to E(T) (line 12). Finally, the
role of the two taxonomies is swapped and the pro-
cess is repeated until no more change is possible.
Example Let us illustrate the algorithm by way
of an example. Assume we are in the first iteration
(T = TC) and consider the Wikipedia category
t = OLYMPICS (line 3) and its super-categories
{MULTI-SPORT EVENTS, SPORT AND POLITICS,
INTERNATIONAL SPORTS COMPETITIONS}. This
category has 27 pages associated with it (line
5), 23 of which provide a hypernym page in TP
(line 6): e.g., PARALYMPIC GAMES, associated
with the category OLYMPICS, is a MULTI-SPORT
EVENT and is therefore contained in S. By con-
sidering and counting the categories of each page
in S (lines 7-8), we end up counting the category
MULTI-SPORT EVENTS four times and other
categories, such as AWARDS and SWIMSUITS,
once. As MULTI-SPORT EVENTS has the highest
count and is connected to OLYMPICS by a path
in the Wikipedia category network (line 11),
the hypernym edge (OLYMPICS, MULTI-SPORT
EVENTS) is added to TC (line 12).
</bodyText>
<sectionHeader confidence="0.960261" genericHeader="method">
5 Phase 3: Category taxonomy
refinement
</sectionHeader>
<bodyText confidence="0.999880142857143">
As the final phase, we refine and enrich the cate-
gory taxonomy. The goal of this phase is to pro-
vide broader coverage to the category taxonomy
TC created as explained in Section 4. We apply
three enrichment heuristics which add hypernyms
to those categories c for which no hypernym could
be found in phase 2, i.e., �c0 s.t. (c, c0) E E(TC).
</bodyText>
<page confidence="0.995427">
949
</page>
<bodyText confidence="0.999417243902439">
Single super-category As a first structural re-
finement, we automatically link an uncovered cat-
egory c to c&apos; if c&apos; is the only direct super-category
of c in Wikipedia.
Sub-categories We increase the hypernym cov-
erage by exploiting the sub-categories of each un-
covered category c (see Figure 3a). In detail,
for each uncovered category c we consider the
set sub(c) of all the Wikipedia subcategories of
c (nodes c1, c2, ... , cn in Figure 3a) and then let
each category vote, according to its direct hyper-
nym categories in TC (the vote is as in Algo-
rithm 1). Then we proceed in decreasing order
of vote and select the highest-ranking category c&apos;
which is connected to c via a path in TC, i.e.,
c ❀TC c&apos;. We then pick up the direct ancestor
c&apos;&apos; of c which lies in the path from c to c&apos; and
add the hypernym edge (c, c&apos;&apos;) to E(TC). For ex-
ample, consider the category FRENCH TELEVI-
SION PEOPLE; since this category has no asso-
ciated pages, in phase 2 no hypernym could be
found. However, by applying the sub-categories
heuristic, we discover that TELEVISION PEOPLE
BY COUNTRY is the hypernym most voted by our
target category’s descendants, such as FRENCH
TELEVISION ACTORS and FRENCH TELEVISION
DIRECTORS. Since TELEVISION PEOPLE BY
COUNTRY is at distance 1 in the Wikipedia
category network from FRENCH TELEVISION
PEOPLE, we add (FRENCH TELEVISION PEOPLE,
TELEVISION PEOPLE BY COUNTRY) to E(TC).
Super-categories We then apply a similar
heuristic involving super-categories (see Figure
3b). Given an uncovered category c, we consider
its direct Wikipedia super-categories and let them
vote, according to their hypernym categories in
TC. Then we proceed in decreasing order of vote
and select the highest-ranking category c&apos; which is
connected to c in TC, i.e., c ❀TC c&apos;. We then pick
up the direct ancestor c&apos;&apos; of c which lies in the path
from c to c&apos; and add the edge (c, c&apos;&apos;) to E(TC).
</bodyText>
<subsectionHeader confidence="0.989149">
5.1 Bitaxonomy Evaluation
</subsectionHeader>
<bodyText confidence="0.987884333333333">
Category taxonomy statistics We applied
phases 2 and 3 to the output of phase 1, which
was evaluated in Section 3.3. In Figure 4a we
show the increase in category coverage at each
iteration throughout the execution of the two
phases (1SUP, SUB and SUPER correspond to
the three above heuristics of phase 3). The final
outcome is a category taxonomy which includes
594,917 hypernymy links between categories,
Figure 3: Heuristic patterns for the coverage re-
finement of the category taxonomy.
covering more than 96% of the 618,641 categories
in the October 2012 English Wikipedia dump.
The graph shows the steepest slope in the first
iterations of phase 2, which converges around
400k categories at iteration 30, and a significant
boost due to phase 3 producing another 175k
hypernymy edges, with the super-category heuris-
tic contributing most. 78.90% of the nodes in
TC belong to the same connected component.
The average height of the biggest component of
TC is 23.26 edges and the maximum height is
49. We note that the average height of TC is
much greater than that of TP, which reflects the
category taxonomy distinguishing between very
subtle classes, such as ALBUMS BY ARTISTS,
ALBUMS BY RECORDING LOCATION, etc.
Category taxonomy quality To estimate the
quality of the category taxonomy, we ran-
domly sampled 1,000 categories and, for each of
them, we manually associated the super-categories
which were deemed to be appropriate hypernyms.
Figure 4b shows the performance trend as the al-
gorithm iteratively covers more and more cate-
gories. Phase 2 is particularly robust across it-
erations, as it leads to increased recall while re-
taining very high precision. As regards phase 3,
the super-categories heuristic leads to only a slight
precision decrease, while improving recall consid-
erably. Overall, the final taxonomy TC achieves
85.80% precision, 83.40% recall and 97.20% cov-
erage on our dataset.
Page taxonomy improvement As a result of
phase 2, 141,105 additional hypernymy links were
also added to the page taxonomy, resulting in
an overall 82.99% precision, 77.90% recall and
92.10% coverage, with a non-negligible 3% boost
from phase 1 to phase 2 in terms of recall and cov-
erage on our Wikipedia page dataset.
We also calculated some statistics for the result-
ing taxonomy obtained by aggregating the 3.8M
</bodyText>
<figure confidence="0.998060833333333">
(a) Sub categ. heuristic.
(b) Super categ. heuristic.
hypernym in TC
Wikipedia super-category
c0
c1 cL ... cn
d e
c
c00
c0 c000
c1 c00 ... cm
c
</figure>
<page confidence="0.709548">
950
</page>
<figureCaption confidence="0.999872">
Figure 4: Category taxonomy evaluation.
</figureCaption>
<bodyText confidence="0.9869555">
hypernym links in a single directed graph. Over-
all, 99% of nodes belong to the same connected
component, with a maximum height of 29 and an
average height on the biggest component of 6.98.
</bodyText>
<sectionHeader confidence="0.999955" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.99995986746988">
Although the extraction of taxonomies from
machine-readable dictionaries was already being
studied in the early 1970s (Calzolari et al., 1973),
pioneering work on large amounts of data only
appeared in the 1990s (Hearst, 1992; Ide and
V´eronis, 1993). Approaches based on hand-
crafted patterns and pattern matching techniques
have been developed to provide a supertype for
the extracted terms (Etzioni et al., 2004; Blohm,
2007; Kozareva and Hovy, 2010; Navigli and Ve-
lardi, 2010; Velardi et al., 2013, inter alia). How-
ever, these methods do not link terms to existing
knowledge resources such as WordNet, whereas
those that explicitly link do so by adding new
leaves to the existing taxonomy instead of acquir-
ing wide-coverage taxonomies from scratch (Pan-
tel and Ravichandran, 2004; Snow et al., 2006).
The recent upsurge of interest in collabo-
rative knowledge curation has enabled several
approaches to large-scale taxonomy acquisition
(Hovy et al., 2013). Most approaches initially
focused on the Wikipedia category network, an
entangled set of generalization-containment rela-
tions between Wikipedia categories, to extract the
hypernymy taxonomy as a subset of the network.
The first approach of this kind was WikiTaxonomy
(Ponzetto and Strube, 2007; Ponzetto and Strube,
2011), based on simple, yet effective lightweight
heuristics, totaling more than 100k is-a relations.
Other approaches, such as YAGO (Suchanek et
al., 2008; Hoffart et al., 2013), yield a taxonom-
ical backbone by linking Wikipedia categories to
WordNet. However, the categories are linked to
the first, i.e., most frequent, sense of the category
head in WordNet, involving only leaf categories in
the linking.
Interest in taxonomizing Wikipedia pages, in-
stead, developed with DBpedia (Auer et al., 2007),
which pioneered the current stream of work aimed
at extracting semi-structured information from
Wikipedia templates and infoboxes. In DBpedia,
entities are mapped to a coarse-grained ontology
which is collaboratively maintained and contains
only about 270 classes corresponding to popular
named entity types, in contrast to our goal of struc-
turing the full set of Wikipedia articles in a larger
and finer-grained taxonomy.
A few notable efforts to reconcile the two sides
of Wikipedia, i.e., pages and categories, have
been put forward very recently: WikiNet (Nas-
tase et al., 2010; Nastase and Strube, 2013) is a
project which heuristically exploits different as-
pects of Wikipedia to obtain a multilingual con-
cept network by deriving not only is-a relations,
but also other types of relations. A second project,
MENTA (de Melo and Weikum, 2010), creates
one of the largest multilingual lexical knowledge
bases by interconnecting more than 13M articles
in 271 languages. In contrast to our work, hy-
pernym extraction is supervised in that decisions
are made on the basis of labelled training exam-
ples and requires a reconciliation step owing to
the heterogeneous nature of the hypernyms, some-
thing that we only do for categories, due to their
noisy network. While WikiNet and MENTA bring
together the knowledge available both at the page
and category level, like we do, they either achieve
low precision and coverage of the taxonomical
structure or exhibit overly general hypernyms, as
we show in our experiments in the next section.
Our work differs from the others in at least three
respects: first, in marked contrast to most other re-
sources, but similarly to WikiNet and WikiTaxon-
omy, our resource is self-contained and does not
depend on other resources such as WordNet; sec-
ond, we address the taxonomization task on both
sides, i.e., pages and categories, by providing an
algorithm which mutually and iteratively transfers
knowledge from one side of the bitaxonomy to the
other; third, we provide a wide coverage bitaxon-
omy closer in structure and granularity to a manual
WordNet-like taxonomy, in contrast, for example,
to DBpedia’s flat entity-focused hierarchy.2
</bodyText>
<footnote confidence="0.51898">
2Note that all the competitors on categories have average
height between 1 and 3.69 on their biggest component, while
we have 23.26, while on pages their height is between 1.9 and
4.22, while ours is 6.98. Since WordNet’s average height is
8.07 we deem WiBi to be the resource structurally closest to
WordNet.
</footnote>
<page confidence="0.989947">
951
</page>
<table confidence="0.9998708">
Dataset System Prec. Rec. Cov.
WiBi 84.11 79.40 92.57
Pages WikiNet 57.29†† 71.45†† 82.01
DBpedia 87.06 51.50†† 55.93
MENTA 81.52 72.49† 88.92
WiBi 85.18 82.88 97.31
WikiTax 88.50 54.83†† 59.43
Categories YAGO 94.13 53.41†† 56.74
MENTA 87.11 84.63 97.15
MENTA−ENT 85.18 71.95†† 84.47
</table>
<tableCaption confidence="0.634681">
Table 2: Page and category taxonomy evaluation.
† (††) denotes statistically significant difference,
using x2 test, p &lt; 0.02 (p &lt; 0.01) between WiBi
and the daggered resource.
</tableCaption>
<sectionHeader confidence="0.944117" genericHeader="method">
7 Comparative Evaluation
</sectionHeader>
<subsectionHeader confidence="0.968526">
7.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999987708333333">
We compared our resource (WiBi) against the
Wikipedia taxonomies of the major knowledge re-
sources in the literature providing hypernym links,
namely DBpedia, WikiNet, MENTA, WikiTax-
onomy and YAGO (see Section 6). As datasets,
we used our gold standards of 1,000 randomly-
sampled pages (see Section 3.3) and categories
(see Section 5.1). In order to ensure a level playing
field, we detected those pages (categories) which
do not exist in any of the above resources and re-
moved them to ensure full coverage of the dataset
across all resources. For each resource we cal-
culated precision, by manually marking each hy-
pernym returned for each page (category) as cor-
rect or not. As regards recall, we note that in
two cases (i.e., DBpedia returning page super-
types from its upper taxonomy, YAGO linking cat-
egories to WordNet synsets) the generalizations
are neither pages nor categories and that MENTA
returns heterogeneous hypernyms as mixed sets of
WordNet synsets, Wikipedia pages and categories.
Given this heterogeneity, standard recall across re-
sources could not be calculated. For this reason we
calculated recall as described in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.846279">
7.2 Results
</subsectionHeader>
<bodyText confidence="0.999993072727273">
Wikipedia pages We first report the results of
the knowledge resources which provide page hy-
pernyms, i.e., we compare against WikiNet, DB-
pedia and MENTA. We use the original outputs
from the three resources: the first two are based
on dumps which are from the same year as the one
used in WiBi (cf. Section 3.3), while MENTA is
based on a dump dating back to 2010 (consisting
of 3.25M pages and 565k categories). We decided
to include the latter for comparison purposes, as it
uses knowledge from 271 Wikipedias to build the
final taxonomy. However, we recognize its perfor-
mance might be relatively higher on a 2012 dump.
We show the results on our page hypernym
dataset in Table 2 (top). As can be seen, WikiNet
obtains the lowest precision, due to the high num-
ber of hypernyms provided, many of which are
incorrect, with a recall between that of DBpe-
dia and MENTA. WiBi outperforms all other re-
sources with 84.11% precision, 79.40% recall and
92.57% coverage. MENTA seems to be the clos-
est resource to ours, however, we remark that the
hypernyms output by MENTA are very heteroge-
neous: 48% of answers are represented by a Word-
Net synset, 37% by Wikipedia categories and 15%
are Wikipedia pages. In contrast to all other re-
sources, WiBi outputs page hypernyms only.
Wikipedia categories We then compared all the
knowledge resources which deal with categories,
i.e., WikiTaxonomy, YAGO and MENTA. For the
latter two, the above considerations about the 2012
dump hold, whereas we reimplemented WikiTax-
onomy, which was based on a 2009 dump, to run it
on the same dump as WiBi. We excluded WikiNet
from our comparison because it turned out to have
low coverage of categories (i.e., less than 1%).
We show the results on our category dataset
in Table 2 (bottom). Despite other systems ex-
hibiting higher precision, WiBi generally achieves
higher recall, thanks also to its higher category
coverage. YAGO obtains the lowest recall and
coverage, because only leaf categories are consid-
ered. MENTA is the closest system to ours, ob-
taining slightly higher precision and recall. No-
tably, however, MENTA outputs the first WordNet
sense of entity for 13.17% of all the given answers,
which, despite being correct and accounted in pre-
cision and recall, is uninformative. Since a system
which always outputs entity would maximise all
the three measures, we also calculated the perfor-
mance for MENTA when discarding entity as an
answer; as Table 2 shows (bottom, MENTA−ENT),
recall drops to 71.95%. Further analysis, pre-
sented below, shows that the specificity of its hy-
pernyms is considerably lower than that of WiBi.
</bodyText>
<subsectionHeader confidence="0.999378">
7.3 Analysis of the results
</subsectionHeader>
<bodyText confidence="0.9999922">
To get further insight into our results we per-
formed two additional analyses of the data. First,
we estimated the level of specialization of the
hypernyms in the different resources on our two
datasets. The idea is that a hypernym should be
</bodyText>
<page confidence="0.991672">
952
</page>
<table confidence="0.999884571428571">
Dataset System (X) WiBi=X WiBi&gt;X WiBi&lt;X
WikiNet 33.38 34.94 31.68
Pages DBpedia 31.68 56.71 11.60
MENTA 19.04 50.85 30.12
WikiTax 43.11 38.51 18.38
Categories YAGO 12.36 81.14 6.50
MENTA 12.36 73.69 13.95
</table>
<tableCaption confidence="0.999828">
Table 3: Specificity comparison.
</tableCaption>
<bodyText confidence="0.999961133333334">
valid while at the same time being as specific as
possible (e.g., SINGER should be preferred over
PERSON). We therefore calculated a measure,
which we called specificity, that computes the per-
centage of times a system outputs a more specific
answer than another system. To do this, we anno-
tated each hypernym returned by a system as fol-
lows: −1 if the answer was wrong, 0 if missing, &gt;
0 if correct; more specific answers were assigned
higher scores. When comparing two systems, we
select the respective most specific answers a1, a2
and say the first system is more specific than the
latter whenever score(a1) &gt; score(a2). Table 3
shows the results for all the resources and for both
the page and category taxonomies: WiBi consis-
tently provides considerably more specific hyper-
nyms than any other resource (middle column).
A second important aspect that we analyzed was
the granularity of each taxonomy, determined by
drawing each resource on a bidimensional plane
with the number of distinct hypernyms on the
x axis and the total number of hypernyms (i.e.,
edges) in the taxonomy on the y axis. Figures 5a
and 5b show the position of each resource for the
page and the category taxonomies, respectively.
As can be seen, WiBi, as well as the page tax-
onomy of MENTA, is the resource with the best
granularity, as not only does it attain high cover-
age, but it also provides a larger variety of classes
as generalizations of pages and categories. Specif-
ically, WiBi provides over 3M hypernym pages
chosen from a range of 94k distinct hypernyms,
while others exhibit a considerably smaller range
of distinct hypernyms (e.g., DBpedia by design,
but also WikiNet, with around 11k distinct page
hypernyms). The large variety of classes provided
by MENTA, however, is due to including more
than 100k Wikipedia categories (among which,
categories about deaths and births alone repre-
sent about 2% of the distinct hypernyms). As re-
gards categories, while the number of distinct hy-
pernyms of WiBi and WikiTaxonomy is approxi-
mately the same (around 130k), the total number
of hypernyms (around 580k for both taxonomies)
is distributed over half of the categories in Wiki-
</bodyText>
<figure confidence="0.996467">
(a) Page taxonomies (b) Category taxonomies
</figure>
<figureCaption confidence="0.999935">
Figure 5: Hypernym granularity for the resources.
</figureCaption>
<bodyText confidence="0.997315">
Taxonomy compared to WiBi, resulting in a dou-
ble number of hypernyms per category, but lower
coverage (cf. Table 2).
</bodyText>
<sectionHeader confidence="0.998806" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999641833333333">
In this paper we have presented WiBi, an auto-
matic 3-phase approach to the construction of a
bitaxonomy for the English Wikipedia, i.e., a full-
fledged, integrated page and category taxonomy:
first, using a set of high-precision linkers, the page
taxonomy is populated; next, a fixed point algo-
rithm populates the category taxonomy while en-
riching the page taxonomy iteratively; finally, the
category taxonomy undergoes structural refine-
ments. Coverage, quality and granularity of the
bitaxonomy are considerably higher than the tax-
onomy of state-of-the-art resources like DBpedia,
YAGO, MENTA, WikiNet and WikiTaxonomy.
Our contributions are three-fold: i) we propose
a unified, effective approach to the construction of
a Wikipedia bitaxonomy, a richer structure than
those produced in the literature; ii) our method for
building the bitaxonomy is self-contained, thanks
to its independence from external resources (like
WordNet) and the virtual absence of supervision,
making WiBi replicable on any new version of
Wikipedia; iii) the taxonomy provides nearly full
coverage of pages and categories, encompassing
the entire encyclopedic knowledge in Wikipedia.
We will apply our video games with a purpose
(Vannella et al., 2014) to validate WiBi. We also
plan to integrate WiBi into BabelNet (Navigli and
Ponzetto, 2012), so as to fully taxonomize it, and
exploit its high quality for improving semantic
predicates (Flati and Navigli, 2013).
</bodyText>
<sectionHeader confidence="0.998299" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999981166666667">
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We thank Luca Telesca for his implementation of
WikiTaxonomy and Jim McManus for his com-
ments on the manuscript.
</bodyText>
<page confidence="0.998298">
953
</page>
<sectionHeader confidence="0.99035" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996544320754717">
Robert A. Amsler. 1981. A Taxonomy for English
Nouns and Verbs. In Proceedings ofAssociation for
Computational Linguistics (ACL ’81), pages 133–
138, Stanford, California, USA.
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ive.
2007. DBpedia: A nucleus for a web of open data.
In Proceedings of 6th International Semantic Web
Conference joint with 2nd Asian Semantic Web Con-
ference (ISWC+ASWC 2007), pages 722–735, Bu-
san, Korea.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S¨oren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia - a crystal-
lization point for the Web of Data. Web Semantics,
7(3):154–165.
Sebastian Blohm. 2007. Using the web to reduce data
sparseness in pattern-based information extraction.
In Proceedings of the 11th European Conference on
Principles and Practice of Knowledge Discovery in
Databases (PKDD), pages 18–29, Warsaw, Poland.
Springer.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the International
Conference on Management of Data (SIGMOD ’08),
SIGMOD ’08, pages 1247–1250, New York, NY,
USA.
Nicoletta Calzolari, Laura Pecchia, and Antonio Zam-
polli. 1973. Working on the Italian Machine Dictio-
nary: a Semantic Approach. In Proceedings of the
5th Conference on Computational Linguistics (COL-
ING ’73), pages 49–70, Pisa, Italy.
Nicoletta Calzolari. 1982. Towards the organization of
lexical definitions on a database structure. In Proc.
of the 9th Conference on Computational Linguistics
(COLING ’82), pages 61–64, Prague, Czechoslo-
vakia.
Gerard de Melo and Gerhard Weikum. 2010. MENTA:
Inducing Multilingual Taxonomies from Wikipedia.
In Proceedings of Conference on Information and
Knowledge Management (CIKM ’10), pages 1099–
1108, New York, NY, USA.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in know-
ItAll: (preliminary results). In Proceedings of the
13th International Conference on World Wide Web
(WWW ’04), pages 100–110, New York, NY, USA.
ACM.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
David A. Ferrucci. 2012. Introduction to ”This is Wat-
son”. IBM Journal of Research and Development,
56(3):1.
Tiziano Flati and Roberto Navigli. 2013. SPred:
Large-scale Harvesting of Semantic Predicates. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1222–1232, Sofia, Bulgaria.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the International Conference on Computational
Linguistics (COLING ’92), pages 539–545, Nantes,
France.
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. YAGO2: A
spatially and temporally enhanced knowledge base
from Wikipedia. Artificial Intelligence, 194:28–61.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2–27.
Nancy Ide and Jean V´eronis. 1993. Extracting
knowledge bases from machine-readable dictionar-
ies: Have we wasted our time? In Proceedings of
the Workshop on Knowledge Bases and Knowledge
Structures, pages 257–266, Tokyo, Japan.
Dan Klein and Christopher D. Manning. 2003. Fast
Exact Inference with a Factored Model for Natural
Language Parsing. In Advances in Neural Infor-
mation Processing Systems 15 (NIPS), pages 3–10,
Vancouver, British Columbia, Canada.
Zornitsa Kozareva and Eduard H. Hovy. 2010. A
Semi-Supervised Method to Learn and Construct
Taxonomies Using the Web. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ’10), pages 1110–1118,
Seattle, WA, USA.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. International Journal of Human-
Computer Studies, 67(9):716–754.
Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009.
Evaluating the inferential utility of lexical-semantic
resources. In Proceedings of the 12th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL), pages 558–566,
Athens, Greece.
Tom Mitchell. 2005. Reading the Web: A Break-
through Goal for AI. AI Magazine.
Vivi Nastase and Michael Strube. 2013. Transform-
ing Wikipedia into a large scale multilingual concept
network. Artificial Intelligence, 194:62–85.
</reference>
<page confidence="0.988673">
954
</page>
<reference confidence="0.999808075">
Vivi Nastase, Michael Strube, Benjamin Boerschinger,
Caecilia Zirn, and Anas Elghafari. 2010. WikiNet:
A Very Large Scale Multi-Lingual Concept Net-
work. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10), Valletta, Malta.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for Definition and Hypernym
Extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2010), pages 1318–1327, Uppsala, Sweden,
July. Association for Computational Linguistics.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL HLT
2013), Boston, Massachusetts, 2–7 May 2004, pages
321–328.
Simone Paolo Ponzetto and Michael Strube. 2007.
Deriving a large scale taxonomy from Wikipedia.
In Proceedings of the 22nd Conference on the Ad-
vancement ofArtificial Intelligence (AAAI ’07), Van-
couver, B.C., Canada, 22–26 July 2007, pages
1440–1445.
Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively built
knowledge repository. Artificial Intelligence, 175(9-
10):1737–1756.
Hoifung Poon, Janara Christensen, Pedro Domingos,
Oren Etzioni, Raphael Hoffmann, Chloe Kiddon,
Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Ste-
fan Schoenmackers, Stephen Soderland, Dan Weld,
Fei Wu, and Congle Zhang. 2010. Machine Read-
ing at the University of Washington. In Proceedings
of the 1st International Workshop on Formalisms
and Methodology for Learning by Reading in con-
junction with NAACL-HLT 2010, pages 87–95, Los
Angeles, California, USA.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Ad-
vances in Web Intelligence, volume 3528 of Lec-
ture Notes in Computer Science, pages 380–386.
Springer Verlag.
Amit Singhal. 2012. Introducing the Knowledge
Graph: Things, Not Strings. Technical report, Of-
ficial Blog (of Google). Retrieved May 18, 2012.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics (COLING-ACL 2006), pages 801–
808.
Fabian Suchanek and Gerhard Weikum. 2013. Knowl-
edge harvesting from text and Web sources. In IEEE
29th International Conference on Data Engineer-
ing (ICDE 2013), pages 1250–1253, Brisbane, Aus-
tralia. IEEE Computer Society.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203–217.
Daniele Vannella, David Jurgens, Daniele Scarfini,
Domenico Toscani, and Roberto Navigli. 2014.
Validating and Extending Semantic Knowledge
Bases using Video Games with a Purpose. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2014),
Baltimore, USA.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algo-
rithm for taxonomy induction. Computational Lin-
guistics, 39(3):665–707.
</reference>
<page confidence="0.998685">
955
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.307637">
<title confidence="0.916668">Two Is Bigger (and Better) Than One: the Wikipedia Bitaxonomy Project Flati, Daniele Vannella, Tommaso Pasini</title>
<author confidence="0.7782325">Dipartimento di_Sapienza Universit`a di</author>
<email confidence="0.998341">p.tommaso@gmail.com</email>
<abstract confidence="0.994281">We present WiBi, an approach to the automatic creation of a bitaxonomy for Wikipedia, that is, an integrated taxonomy of Wikipage pages and categories. We leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and</abstract>
<intro confidence="0.556013">WikiTaxonomy. WiBi is available at</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robert A Amsler</author>
</authors>
<title>A Taxonomy for English Nouns and Verbs.</title>
<date>1981</date>
<booktitle>In Proceedings ofAssociation for Computational Linguistics (ACL ’81),</booktitle>
<pages>133--138</pages>
<location>Stanford, California, USA.</location>
<contexts>
<context position="6834" citStr="Amsler, 1981" startWordPosition="1088" endWordPosition="1089">follows Julia Fiona Roberts is an American actress NNP NNP NNP VBZ DT JJ NN Figure 1: A dependency tree example with copula. the Wikipedia guidelines and is validated in the literature (Navigli and Velardi, 2010; Navigli and Ponzetto, 2012), is that the first sentence of each Wikipedia page p provides a textual definition for the concept represented by p. The second assumption we build upon is the idea that a lexical taxonomy can be obtained by extracting hypernyms from textual definitions. This idea dates back to the early 1970s (Calzolari et al., 1973), with later developments in the 1980s (Amsler, 1981; Calzolari, 1982) and the 1990s (Ide and V´eronis, 1993). To extract hypernym lemmas, we draw on the notion of copula, that is, the relation between the complement of a copular verb and the copular verb itself. Therefore, we apply the Stanford parser (Klein and Manning, 2003) to the definition of a page in order to extract all the dependency relations of the sentence. For example, given the definition of the page JULIA ROBERTS, i.e., “Julia Fiona Roberts is an American actress.”, the Stanford parser outputs the set of dependencies shown in Figure 1. The noun involved in the copula relation is</context>
</contexts>
<marker>Amsler, 1981</marker>
<rawString>Robert A. Amsler. 1981. A Taxonomy for English Nouns and Verbs. In Proceedings ofAssociation for Computational Linguistics (ACL ’81), pages 133– 138, Stanford, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Richard Cyganiak</author>
<author>Zachary Ive</author>
</authors>
<title>DBpedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>In Proceedings of 6th International Semantic Web Conference joint with 2nd Asian Semantic Web Conference (ISWC+ASWC</booktitle>
<pages>722--735</pages>
<location>Busan,</location>
<contexts>
<context position="28125" citStr="Auer et al., 2007" startWordPosition="4779" endWordPosition="4782">t of the network. The first approach of this kind was WikiTaxonomy (Ponzetto and Strube, 2007; Ponzetto and Strube, 2011), based on simple, yet effective lightweight heuristics, totaling more than 100k is-a relations. Other approaches, such as YAGO (Suchanek et al., 2008; Hoffart et al., 2013), yield a taxonomical backbone by linking Wikipedia categories to WordNet. However, the categories are linked to the first, i.e., most frequent, sense of the category head in WordNet, involving only leaf categories in the linking. Interest in taxonomizing Wikipedia pages, instead, developed with DBpedia (Auer et al., 2007), which pioneered the current stream of work aimed at extracting semi-structured information from Wikipedia templates and infoboxes. In DBpedia, entities are mapped to a coarse-grained ontology which is collaboratively maintained and contains only about 270 classes corresponding to popular named entity types, in contrast to our goal of structuring the full set of Wikipedia articles in a larger and finer-grained taxonomy. A few notable efforts to reconcile the two sides of Wikipedia, i.e., pages and categories, have been put forward very recently: WikiNet (Nastase et al., 2010; Nastase and Stru</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, Ive, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ive. 2007. DBpedia: A nucleus for a web of open data. In Proceedings of 6th International Semantic Web Conference joint with 2nd Asian Semantic Web Conference (ISWC+ASWC 2007), pages 722–735, Busan, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Bizer</author>
<author>Jens Lehmann</author>
<author>Georgi Kobilarov</author>
<author>S¨oren Auer</author>
<author>Christian Becker</author>
<author>Richard Cyganiak</author>
<author>Sebastian Hellmann</author>
</authors>
<title>DBpedia - a crystallization point for the Web of Data.</title>
<date>2009</date>
<journal>Web Semantics,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="1768" citStr="Bizer et al., 2009" startWordPosition="250" endWordPosition="253">(Singhal, 2012) and IBM (Ferrucci, 2012), which are moving fast towards large-scale knowledge-oriented systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a great deal of valuable knowledge which can be harvested and transformed into structured form (Medelyan et al., 2009; Hovy et al., 2013). Prominent examples include DBpedia (Bizer et al., 2009), BabelNet (Navigli and Ponzetto, 2012), YAGO (Hoffart et al., 2013) and WikiNet (Nastase and Strube, 2013). The types of semantic relation in these resources range from domain-specific, as in Freebase (Bollacker et al., 2008), to unspecified relations, as in BabelNet. However, unlike the case with smaller manually-curated resources such as WordNet (Fellbaum, 1998), in many large automatically-created resources the taxonomical information is either missing, mixed across resources, e.g., linking Wikipedia categories to WordNet synsets as in YAGO, or coarsegrained, as in DBpedia whose hypernyms </context>
</contexts>
<marker>Bizer, Lehmann, Kobilarov, Auer, Becker, Cyganiak, Hellmann, 2009</marker>
<rawString>Christian Bizer, Jens Lehmann, Georgi Kobilarov, S¨oren Auer, Christian Becker, Richard Cyganiak, and Sebastian Hellmann. 2009. DBpedia - a crystallization point for the Web of Data. Web Semantics, 7(3):154–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Blohm</author>
</authors>
<title>Using the web to reduce data sparseness in pattern-based information extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD),</booktitle>
<pages>18--29</pages>
<publisher>Springer.</publisher>
<location>Warsaw, Poland.</location>
<contexts>
<context position="26775" citStr="Blohm, 2007" startWordPosition="4575" endWordPosition="4576"> single directed graph. Overall, 99% of nodes belong to the same connected component, with a maximum height of 29 and an average height on the biggest component of 6.98. 6 Related Work Although the extraction of taxonomies from machine-readable dictionaries was already being studied in the early 1970s (Calzolari et al., 1973), pioneering work on large amounts of data only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network,</context>
</contexts>
<marker>Blohm, 2007</marker>
<rawString>Sebastian Blohm. 2007. Using the web to reduce data sparseness in pattern-based information extraction. In Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), pages 18–29, Warsaw, Poland. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: A collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Management of Data (SIGMOD ’08), SIGMOD ’08,</booktitle>
<pages>1247--1250</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="1994" citStr="Bollacker et al., 2008" startWordPosition="284" endWordPosition="287">ed online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a great deal of valuable knowledge which can be harvested and transformed into structured form (Medelyan et al., 2009; Hovy et al., 2013). Prominent examples include DBpedia (Bizer et al., 2009), BabelNet (Navigli and Ponzetto, 2012), YAGO (Hoffart et al., 2013) and WikiNet (Nastase and Strube, 2013). The types of semantic relation in these resources range from domain-specific, as in Freebase (Bollacker et al., 2008), to unspecified relations, as in BabelNet. However, unlike the case with smaller manually-curated resources such as WordNet (Fellbaum, 1998), in many large automatically-created resources the taxonomical information is either missing, mixed across resources, e.g., linking Wikipedia categories to WordNet synsets as in YAGO, or coarsegrained, as in DBpedia whose hypernyms link to a small upper taxonomy. Current approaches in the literature have mostly focused on the extraction of taxonomies from the network of Wikipedia categories. WikiTaxonomy (Ponzetto and Strube, 2007), the first approach of</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the International Conference on Management of Data (SIGMOD ’08), SIGMOD ’08, pages 1247–1250, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicoletta Calzolari</author>
<author>Laura Pecchia</author>
<author>Antonio Zampolli</author>
</authors>
<title>Working on the Italian Machine Dictionary: a Semantic Approach.</title>
<date>1973</date>
<booktitle>In Proceedings of the 5th Conference on Computational Linguistics (COLING ’73),</booktitle>
<pages>49--70</pages>
<location>Pisa, Italy.</location>
<contexts>
<context position="6782" citStr="Calzolari et al., 1973" startWordPosition="1078" endWordPosition="1081"> ambiguous hypernyms for the page. The first assumption, which follows Julia Fiona Roberts is an American actress NNP NNP NNP VBZ DT JJ NN Figure 1: A dependency tree example with copula. the Wikipedia guidelines and is validated in the literature (Navigli and Velardi, 2010; Navigli and Ponzetto, 2012), is that the first sentence of each Wikipedia page p provides a textual definition for the concept represented by p. The second assumption we build upon is the idea that a lexical taxonomy can be obtained by extracting hypernyms from textual definitions. This idea dates back to the early 1970s (Calzolari et al., 1973), with later developments in the 1980s (Amsler, 1981; Calzolari, 1982) and the 1990s (Ide and V´eronis, 1993). To extract hypernym lemmas, we draw on the notion of copula, that is, the relation between the complement of a copular verb and the copular verb itself. Therefore, we apply the Stanford parser (Klein and Manning, 2003) to the definition of a page in order to extract all the dependency relations of the sentence. For example, given the definition of the page JULIA ROBERTS, i.e., “Julia Fiona Roberts is an American actress.”, the Stanford parser outputs the set of dependencies shown in F</context>
<context position="26491" citStr="Calzolari et al., 1973" startWordPosition="4528" endWordPosition="4531">o calculated some statistics for the resulting taxonomy obtained by aggregating the 3.8M (a) Sub categ. heuristic. (b) Super categ. heuristic. hypernym in TC Wikipedia super-category c0 c1 cL ... cn d e c c00 c0 c000 c1 c00 ... cm c 950 Figure 4: Category taxonomy evaluation. hypernym links in a single directed graph. Overall, 99% of nodes belong to the same connected component, with a maximum height of 29 and an average height on the biggest component of 6.98. 6 Related Work Although the extraction of taxonomies from machine-readable dictionaries was already being studied in the early 1970s (Calzolari et al., 1973), pioneering work on large amounts of data only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from</context>
</contexts>
<marker>Calzolari, Pecchia, Zampolli, 1973</marker>
<rawString>Nicoletta Calzolari, Laura Pecchia, and Antonio Zampolli. 1973. Working on the Italian Machine Dictionary: a Semantic Approach. In Proceedings of the 5th Conference on Computational Linguistics (COLING ’73), pages 49–70, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicoletta Calzolari</author>
</authors>
<title>Towards the organization of lexical definitions on a database structure.</title>
<date>1982</date>
<booktitle>In Proc. of the 9th Conference on Computational Linguistics (COLING ’82),</booktitle>
<pages>61--64</pages>
<location>Prague, Czechoslovakia.</location>
<contexts>
<context position="6852" citStr="Calzolari, 1982" startWordPosition="1090" endWordPosition="1092">Fiona Roberts is an American actress NNP NNP NNP VBZ DT JJ NN Figure 1: A dependency tree example with copula. the Wikipedia guidelines and is validated in the literature (Navigli and Velardi, 2010; Navigli and Ponzetto, 2012), is that the first sentence of each Wikipedia page p provides a textual definition for the concept represented by p. The second assumption we build upon is the idea that a lexical taxonomy can be obtained by extracting hypernyms from textual definitions. This idea dates back to the early 1970s (Calzolari et al., 1973), with later developments in the 1980s (Amsler, 1981; Calzolari, 1982) and the 1990s (Ide and V´eronis, 1993). To extract hypernym lemmas, we draw on the notion of copula, that is, the relation between the complement of a copular verb and the copular verb itself. Therefore, we apply the Stanford parser (Klein and Manning, 2003) to the definition of a page in order to extract all the dependency relations of the sentence. For example, given the definition of the page JULIA ROBERTS, i.e., “Julia Fiona Roberts is an American actress.”, the Stanford parser outputs the set of dependencies shown in Figure 1. The noun involved in the copula relation is actress and thus </context>
</contexts>
<marker>Calzolari, 1982</marker>
<rawString>Nicoletta Calzolari. 1982. Towards the organization of lexical definitions on a database structure. In Proc. of the 9th Conference on Computational Linguistics (COLING ’82), pages 61–64, Prague, Czechoslovakia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard de Melo</author>
<author>Gerhard Weikum</author>
</authors>
<title>MENTA: Inducing Multilingual Taxonomies from Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of Conference on Information and Knowledge Management (CIKM ’10),</booktitle>
<pages>1099--1108</pages>
<location>New York, NY, USA.</location>
<marker>de Melo, Weikum, 2010</marker>
<rawString>Gerard de Melo and Gerhard Weikum. 2010. MENTA: Inducing Multilingual Taxonomies from Wikipedia. In Proceedings of Conference on Information and Knowledge Management (CIKM ’10), pages 1099– 1108, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>Stanley Kok</author>
<author>Ana-Maria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Web-scale information extraction in knowItAll: (preliminary results).</title>
<date>2004</date>
<booktitle>In Proceedings of the 13th International Conference on World Wide Web (WWW ’04),</booktitle>
<pages>100--110</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="26762" citStr="Etzioni et al., 2004" startWordPosition="4571" endWordPosition="4574">n. hypernym links in a single directed graph. Overall, 99% of nodes belong to the same connected component, with a maximum height of 29 and an average height on the biggest component of 6.98. 6 Related Work Although the extraction of taxonomies from machine-readable dictionaries was already being studied in the early 1970s (Calzolari et al., 1973), pioneering work on large amounts of data only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia cate</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, Yates, 2004</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, Stanley Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2004. Web-scale information extraction in knowItAll: (preliminary results). In Proceedings of the 13th International Conference on World Wide Web (WWW ’04), pages 100–110, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Ferrucci</author>
</authors>
<title>Introduction to ”This is Watson”.</title>
<date>2012</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>56</volume>
<issue>3</issue>
<contexts>
<context position="1189" citStr="Ferrucci, 2012" startWordPosition="167" endWordPosition="168"> coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and WikiTaxonomy. WiBi is available at http://wibitaxonomy.org. 1 Introduction Knowledge has unquestionably become a key component of current intelligent systems in many fields of Artificial Intelligence. The creation and use of machine-readable knowledge has not only entailed researchers (Mitchell, 2005; Mirkin et al., 2009; Poon et al., 2010) developing huge, broadcoverage knowledge bases (Hovy et al., 2013; Suchanek and Weikum, 2013), but it has also hit big industry players such as Google (Singhal, 2012) and IBM (Ferrucci, 2012), which are moving fast towards large-scale knowledge-oriented systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a great deal of valuable knowledge which can be harvested and transformed into structured form (Medelyan et al., 2009; Hovy et al., 2013). Prominent examples include DBpedia (Bizer et al., 2009), BabelNet (Navigli a</context>
</contexts>
<marker>Ferrucci, 2012</marker>
<rawString>David A. Ferrucci. 2012. Introduction to ”This is Watson”. IBM Journal of Research and Development, 56(3):1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiziano Flati</author>
<author>Roberto Navigli</author>
</authors>
<title>SPred: Large-scale Harvesting of Semantic Predicates.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1222--1232</pages>
<location>Sofia, Bulgaria.</location>
<marker>Flati, Navigli, 2013</marker>
<rawString>Tiziano Flati and Roberto Navigli. 2013. SPred: Large-scale Harvesting of Semantic Predicates. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 1222–1232, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING ’92),</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="26574" citStr="Hearst, 1992" startWordPosition="4544" endWordPosition="4545">ub categ. heuristic. (b) Super categ. heuristic. hypernym in TC Wikipedia super-category c0 c1 cL ... cn d e c c00 c0 c000 c1 c00 ... cm c 950 Figure 4: Category taxonomy evaluation. hypernym links in a single directed graph. Overall, 99% of nodes belong to the same connected component, with a maximum height of 29 and an average height on the biggest component of 6.98. 6 Related Work Although the extraction of taxonomies from machine-readable dictionaries was already being studied in the early 1970s (Calzolari et al., 1973), pioneering work on large amounts of data only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the International Conference on Computational Linguistics (COLING ’92), pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Fabian M Suchanek</author>
<author>Klaus Berberich</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--28</pages>
<contexts>
<context position="1836" citStr="Hoffart et al., 2013" startWordPosition="260" endWordPosition="263">wards large-scale knowledge-oriented systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a great deal of valuable knowledge which can be harvested and transformed into structured form (Medelyan et al., 2009; Hovy et al., 2013). Prominent examples include DBpedia (Bizer et al., 2009), BabelNet (Navigli and Ponzetto, 2012), YAGO (Hoffart et al., 2013) and WikiNet (Nastase and Strube, 2013). The types of semantic relation in these resources range from domain-specific, as in Freebase (Bollacker et al., 2008), to unspecified relations, as in BabelNet. However, unlike the case with smaller manually-curated resources such as WordNet (Fellbaum, 1998), in many large automatically-created resources the taxonomical information is either missing, mixed across resources, e.g., linking Wikipedia categories to WordNet synsets as in YAGO, or coarsegrained, as in DBpedia whose hypernyms link to a small upper taxonomy. Current approaches in the literature</context>
<context position="27801" citStr="Hoffart et al., 2013" startWordPosition="4729" endWordPosition="4732">st in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of generalization-containment relations between Wikipedia categories, to extract the hypernymy taxonomy as a subset of the network. The first approach of this kind was WikiTaxonomy (Ponzetto and Strube, 2007; Ponzetto and Strube, 2011), based on simple, yet effective lightweight heuristics, totaling more than 100k is-a relations. Other approaches, such as YAGO (Suchanek et al., 2008; Hoffart et al., 2013), yield a taxonomical backbone by linking Wikipedia categories to WordNet. However, the categories are linked to the first, i.e., most frequent, sense of the category head in WordNet, involving only leaf categories in the linking. Interest in taxonomizing Wikipedia pages, instead, developed with DBpedia (Auer et al., 2007), which pioneered the current stream of work aimed at extracting semi-structured information from Wikipedia templates and infoboxes. In DBpedia, entities are mapped to a coarse-grained ontology which is collaboratively maintained and contains only about 270 classes correspond</context>
</contexts>
<marker>Hoffart, Suchanek, Berberich, Weikum, 2013</marker>
<rawString>Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia. Artificial Intelligence, 194:28–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Collaboratively built semistructured content and Artificial Intelligence: The story so far.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--2</pages>
<contexts>
<context position="1063" citStr="Hovy et al., 2013" startWordPosition="144" endWordPosition="147">ailable in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and WikiTaxonomy. WiBi is available at http://wibitaxonomy.org. 1 Introduction Knowledge has unquestionably become a key component of current intelligent systems in many fields of Artificial Intelligence. The creation and use of machine-readable knowledge has not only entailed researchers (Mitchell, 2005; Mirkin et al., 2009; Poon et al., 2010) developing huge, broadcoverage knowledge bases (Hovy et al., 2013; Suchanek and Weikum, 2013), but it has also hit big industry players such as Google (Singhal, 2012) and IBM (Ferrucci, 2012), which are moving fast towards large-scale knowledge-oriented systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a great deal of valuable knowledge which can be harvested and transformed into structured</context>
<context position="27305" citStr="Hovy et al., 2013" startWordPosition="4657" endWordPosition="4660">eloped to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of generalization-containment relations between Wikipedia categories, to extract the hypernymy taxonomy as a subset of the network. The first approach of this kind was WikiTaxonomy (Ponzetto and Strube, 2007; Ponzetto and Strube, 2011), based on simple, yet effective lightweight heuristics, totaling more than 100k is-a relations. Other approaches, such as YAGO (Suchanek et al., 2008; Hoffart et al., 2013), yield a taxonomical backbone by linking Wikipedia categories to WordNet. However, the categories are l</context>
</contexts>
<marker>Hovy, Navigli, Ponzetto, 2013</marker>
<rawString>Eduard H. Hovy, Roberto Navigli, and Simone Paolo Ponzetto. 2013. Collaboratively built semistructured content and Artificial Intelligence: The story so far. Artificial Intelligence, 194:2–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Jean V´eronis</author>
</authors>
<title>Extracting knowledge bases from machine-readable dictionaries: Have we wasted our time?</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Knowledge Bases and Knowledge Structures,</booktitle>
<pages>257--266</pages>
<location>Tokyo, Japan.</location>
<marker>Ide, V´eronis, 1993</marker>
<rawString>Nancy Ide and Jean V´eronis. 1993. Extracting knowledge bases from machine-readable dictionaries: Have we wasted our time? In Proceedings of the Workshop on Knowledge Bases and Knowledge Structures, pages 257–266, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15 (NIPS),</booktitle>
<pages>3--10</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="7111" citStr="Klein and Manning, 2003" startWordPosition="1133" endWordPosition="1136">nce of each Wikipedia page p provides a textual definition for the concept represented by p. The second assumption we build upon is the idea that a lexical taxonomy can be obtained by extracting hypernyms from textual definitions. This idea dates back to the early 1970s (Calzolari et al., 1973), with later developments in the 1980s (Amsler, 1981; Calzolari, 1982) and the 1990s (Ide and V´eronis, 1993). To extract hypernym lemmas, we draw on the notion of copula, that is, the relation between the complement of a copular verb and the copular verb itself. Therefore, we apply the Stanford parser (Klein and Manning, 2003) to the definition of a page in order to extract all the dependency relations of the sentence. For example, given the definition of the page JULIA ROBERTS, i.e., “Julia Fiona Roberts is an American actress.”, the Stanford parser outputs the set of dependencies shown in Figure 1. The noun involved in the copula relation is actress and thus it is taken as the page’s hypernym lemma. However, the extracted hypernym is sometimes overgeneral (one, kind, type, etc.). For instance, given the definition of the page APOLLO, “Apollo is one of the most important and complex of the Olympian deities in anci</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast Exact Inference with a Factored Model for Natural Language Parsing. In Advances in Neural Information Processing Systems 15 (NIPS), pages 3–10, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard H Hovy</author>
</authors>
<title>A Semi-Supervised Method to Learn and Construct Taxonomies Using the Web.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’10),</booktitle>
<pages>1110--1118</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="26800" citStr="Kozareva and Hovy, 2010" startWordPosition="4577" endWordPosition="4580">ted graph. Overall, 99% of nodes belong to the same connected component, with a maximum height of 29 and an average height on the biggest component of 6.98. 6 Related Work Although the extraction of taxonomies from machine-readable dictionaries was already being studied in the early 1970s (Calzolari et al., 1973), pioneering work on large amounts of data only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of gene</context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard H. Hovy. 2010. A Semi-Supervised Method to Learn and Construct Taxonomies Using the Web. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’10), pages 1110–1118, Seattle, WA, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Olena Medelyan</author>
</authors>
<location>David Milne, Catherine Legg, and</location>
<marker>Medelyan, </marker>
<rawString>Olena Medelyan, David Milne, Catherine Legg, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
</authors>
<title>Mining meaning from Wikipedia.</title>
<date>2009</date>
<journal>International Journal of HumanComputer Studies,</journal>
<volume>67</volume>
<issue>9</issue>
<marker>Witten, 2009</marker>
<rawString>Ian H. Witten. 2009. Mining meaning from Wikipedia. International Journal of HumanComputer Studies, 67(9):716–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Ido Dagan</author>
<author>Eyal Shnarch</author>
</authors>
<title>Evaluating the inferential utility of lexical-semantic resources.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>558--566</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="977" citStr="Mirkin et al., 2009" startWordPosition="130" endWordPosition="133"> an integrated taxonomy of Wikipage pages and categories. We leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and WikiTaxonomy. WiBi is available at http://wibitaxonomy.org. 1 Introduction Knowledge has unquestionably become a key component of current intelligent systems in many fields of Artificial Intelligence. The creation and use of machine-readable knowledge has not only entailed researchers (Mitchell, 2005; Mirkin et al., 2009; Poon et al., 2010) developing huge, broadcoverage knowledge bases (Hovy et al., 2013; Suchanek and Weikum, 2013), but it has also hit big industry players such as Google (Singhal, 2012) and IBM (Ferrucci, 2012), which are moving fast towards large-scale knowledge-oriented systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a g</context>
</contexts>
<marker>Mirkin, Dagan, Shnarch, 2009</marker>
<rawString>Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009. Evaluating the inferential utility of lexical-semantic resources. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 558–566, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mitchell</author>
</authors>
<title>Reading the Web: A Breakthrough Goal for AI.</title>
<date>2005</date>
<publisher>AI Magazine.</publisher>
<contexts>
<context position="956" citStr="Mitchell, 2005" startWordPosition="128" endWordPosition="129">ipedia, that is, an integrated taxonomy of Wikipage pages and categories. We leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and WikiTaxonomy. WiBi is available at http://wibitaxonomy.org. 1 Introduction Knowledge has unquestionably become a key component of current intelligent systems in many fields of Artificial Intelligence. The creation and use of machine-readable knowledge has not only entailed researchers (Mitchell, 2005; Mirkin et al., 2009; Poon et al., 2010) developing huge, broadcoverage knowledge bases (Hovy et al., 2013; Suchanek and Weikum, 2013), but it has also hit big industry players such as Google (Singhal, 2012) and IBM (Ferrucci, 2012), which are moving fast towards large-scale knowledge-oriented systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structu</context>
</contexts>
<marker>Mitchell, 2005</marker>
<rawString>Tom Mitchell. 2005. Reading the Web: A Breakthrough Goal for AI. AI Magazine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Michael Strube</author>
</authors>
<title>Transforming Wikipedia into a large scale multilingual concept network.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--62</pages>
<contexts>
<context position="1875" citStr="Nastase and Strube, 2013" startWordPosition="266" endWordPosition="269">d systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a great deal of valuable knowledge which can be harvested and transformed into structured form (Medelyan et al., 2009; Hovy et al., 2013). Prominent examples include DBpedia (Bizer et al., 2009), BabelNet (Navigli and Ponzetto, 2012), YAGO (Hoffart et al., 2013) and WikiNet (Nastase and Strube, 2013). The types of semantic relation in these resources range from domain-specific, as in Freebase (Bollacker et al., 2008), to unspecified relations, as in BabelNet. However, unlike the case with smaller manually-curated resources such as WordNet (Fellbaum, 1998), in many large automatically-created resources the taxonomical information is either missing, mixed across resources, e.g., linking Wikipedia categories to WordNet synsets as in YAGO, or coarsegrained, as in DBpedia whose hypernyms link to a small upper taxonomy. Current approaches in the literature have mostly focused on the extraction </context>
<context position="28734" citStr="Nastase and Strube, 2013" startWordPosition="4872" endWordPosition="4875">er et al., 2007), which pioneered the current stream of work aimed at extracting semi-structured information from Wikipedia templates and infoboxes. In DBpedia, entities are mapped to a coarse-grained ontology which is collaboratively maintained and contains only about 270 classes corresponding to popular named entity types, in contrast to our goal of structuring the full set of Wikipedia articles in a larger and finer-grained taxonomy. A few notable efforts to reconcile the two sides of Wikipedia, i.e., pages and categories, have been put forward very recently: WikiNet (Nastase et al., 2010; Nastase and Strube, 2013) is a project which heuristically exploits different aspects of Wikipedia to obtain a multilingual concept network by deriving not only is-a relations, but also other types of relations. A second project, MENTA (de Melo and Weikum, 2010), creates one of the largest multilingual lexical knowledge bases by interconnecting more than 13M articles in 271 languages. In contrast to our work, hypernym extraction is supervised in that decisions are made on the basis of labelled training examples and requires a reconciliation step owing to the heterogeneous nature of the hypernyms, something that we onl</context>
</contexts>
<marker>Nastase, Strube, 2013</marker>
<rawString>Vivi Nastase and Michael Strube. 2013. Transforming Wikipedia into a large scale multilingual concept network. Artificial Intelligence, 194:62–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Michael Strube</author>
<author>Benjamin Boerschinger</author>
<author>Caecilia Zirn</author>
<author>Anas Elghafari</author>
</authors>
<title>WikiNet: A Very Large Scale Multi-Lingual Concept Network.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="28707" citStr="Nastase et al., 2010" startWordPosition="4867" endWordPosition="4871">loped with DBpedia (Auer et al., 2007), which pioneered the current stream of work aimed at extracting semi-structured information from Wikipedia templates and infoboxes. In DBpedia, entities are mapped to a coarse-grained ontology which is collaboratively maintained and contains only about 270 classes corresponding to popular named entity types, in contrast to our goal of structuring the full set of Wikipedia articles in a larger and finer-grained taxonomy. A few notable efforts to reconcile the two sides of Wikipedia, i.e., pages and categories, have been put forward very recently: WikiNet (Nastase et al., 2010; Nastase and Strube, 2013) is a project which heuristically exploits different aspects of Wikipedia to obtain a multilingual concept network by deriving not only is-a relations, but also other types of relations. A second project, MENTA (de Melo and Weikum, 2010), creates one of the largest multilingual lexical knowledge bases by interconnecting more than 13M articles in 271 languages. In contrast to our work, hypernym extraction is supervised in that decisions are made on the basis of labelled training examples and requires a reconciliation step owing to the heterogeneous nature of the hyper</context>
</contexts>
<marker>Nastase, Strube, Boerschinger, Zirn, Elghafari, 2010</marker>
<rawString>Vivi Nastase, Michael Strube, Benjamin Boerschinger, Caecilia Zirn, and Anas Elghafari. 2010. WikiNet: A Very Large Scale Multi-Lingual Concept Network. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<pages>250</pages>
<contexts>
<context position="1807" citStr="Navigli and Ponzetto, 2012" startWordPosition="255" endWordPosition="258">ci, 2012), which are moving fast towards large-scale knowledge-oriented systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a great deal of valuable knowledge which can be harvested and transformed into structured form (Medelyan et al., 2009; Hovy et al., 2013). Prominent examples include DBpedia (Bizer et al., 2009), BabelNet (Navigli and Ponzetto, 2012), YAGO (Hoffart et al., 2013) and WikiNet (Nastase and Strube, 2013). The types of semantic relation in these resources range from domain-specific, as in Freebase (Bollacker et al., 2008), to unspecified relations, as in BabelNet. However, unlike the case with smaller manually-curated resources such as WordNet (Fellbaum, 1998), in many large automatically-created resources the taxonomical information is either missing, mixed across resources, e.g., linking Wikipedia categories to WordNet synsets as in YAGO, or coarsegrained, as in DBpedia whose hypernyms link to a small upper taxonomy. Current</context>
<context position="6462" citStr="Navigli and Ponzetto, 2012" startWordPosition="1022" endWordPosition="1025">yms are extracted from the page’s textual definition, and a semantic step, in which the extracted hypernyms are disambiguated according to the Wikipedia inventory. 3.1 Syntactic step: hypernym extraction In the syntactic step, for each page p E P, we extract zero, one or more hypernym lemmas, that is, we output potentially ambiguous hypernyms for the page. The first assumption, which follows Julia Fiona Roberts is an American actress NNP NNP NNP VBZ DT JJ NN Figure 1: A dependency tree example with copula. the Wikipedia guidelines and is validated in the literature (Navigli and Velardi, 2010; Navigli and Ponzetto, 2012), is that the first sentence of each Wikipedia page p provides a textual definition for the concept represented by p. The second assumption we build upon is the idea that a lexical taxonomy can be obtained by extracting hypernyms from textual definitions. This idea dates back to the early 1970s (Calzolari et al., 1973), with later developments in the 1980s (Amsler, 1981; Calzolari, 1982) and the 1990s (Ide and V´eronis, 1993). To extract hypernym lemmas, we draw on the notion of copula, that is, the relation between the complement of a copular verb and the copular verb itself. Therefore, we ap</context>
<context position="9274" citStr="Navigli and Ponzetto, 2012" startWordPosition="1494" endWordPosition="1497">n relations, also polymath, student and teacher are extracted as hypernyms. While more sophisticated approaches like WordClass Lattices could be applied (Navigli and Velardi, 2010), we found that, in practice, our hypernym extraction approach provides higher coverage, which is critical in our case. 3.2 Semantic step: hypernym disambiguation Since our aim is to connect pairs of pages via hypernym relations, our second step consists of disambiguating the obtained hypernym lemmas of page p by associating the most suitable page with each hypernym. Following previous work (RuizCasado et al., 2005; Navigli and Ponzetto, 2012), as the inventory for a given lemma we consider the set of pages whose main title is the lemma itself, except for the sense specification in parenthesis. For instance, given fruit as the hypernym for APPLE we would like to link APPLE to FRUIT as opposed to, e.g., FRUIT (BAND) or FRUIT (ALBUM). 3.2.1 Hypernym linkers To disambiguate hypernym lemmas, we exploit the structural features of Wikipedia through a pipeline of hypernym linkers L = {Li}, applied in cascade order (cf. Section 3.3.1). We start with the set of page-hypernym pairs H = {(p, h)} as obtained from the syntactic step. The succes</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Learning Word-Class Lattices for Definition and Hypernym Extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010),</booktitle>
<pages>1318--1327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6433" citStr="Navigli and Velardi, 2010" startWordPosition="1018" endWordPosition="1021">c step, in which the hypernyms are extracted from the page’s textual definition, and a semantic step, in which the extracted hypernyms are disambiguated according to the Wikipedia inventory. 3.1 Syntactic step: hypernym extraction In the syntactic step, for each page p E P, we extract zero, one or more hypernym lemmas, that is, we output potentially ambiguous hypernyms for the page. The first assumption, which follows Julia Fiona Roberts is an American actress NNP NNP NNP VBZ DT JJ NN Figure 1: A dependency tree example with copula. the Wikipedia guidelines and is validated in the literature (Navigli and Velardi, 2010; Navigli and Ponzetto, 2012), is that the first sentence of each Wikipedia page p provides a textual definition for the concept represented by p. The second assumption we build upon is the idea that a lexical taxonomy can be obtained by extracting hypernyms from textual definitions. This idea dates back to the early 1970s (Calzolari et al., 1973), with later developments in the 1980s (Amsler, 1981; Calzolari, 1982) and the 1990s (Ide and V´eronis, 1993). To extract hypernym lemmas, we draw on the notion of copula, that is, the relation between the complement of a copular verb and the copular </context>
<context position="8827" citStr="Navigli and Velardi, 2010" startWordPosition="1422" endWordPosition="1426"> follow the conj and and conj or relations starting from the initially extracted hypernym. For example, consider the definition of ARISTOTLE: “Aristotle was a Greek philosopher and polymath, a student of Plato and teacher of Alexander the Great.” Initially, the philosopher hypernym is selected thanks to the copula relation, then, fol1E.g., species, genus, one, etc. Full list available online. nn amod nsubj cop det nn 946 lowing the conjunction relations, also polymath, student and teacher are extracted as hypernyms. While more sophisticated approaches like WordClass Lattices could be applied (Navigli and Velardi, 2010), we found that, in practice, our hypernym extraction approach provides higher coverage, which is critical in our case. 3.2 Semantic step: hypernym disambiguation Since our aim is to connect pairs of pages via hypernym relations, our second step consists of disambiguating the obtained hypernym lemmas of page p by associating the most suitable page with each hypernym. Following previous work (RuizCasado et al., 2005; Navigli and Ponzetto, 2012), as the inventory for a given lemma we consider the set of pages whose main title is the lemma itself, except for the sense specification in parenthesis</context>
<context position="26827" citStr="Navigli and Velardi, 2010" startWordPosition="4581" endWordPosition="4585">f nodes belong to the same connected component, with a maximum height of 29 and an average height on the biggest component of 6.98. 6 Related Work Although the extraction of taxonomies from machine-readable dictionaries was already being studied in the early 1970s (Calzolari et al., 1973), pioneering work on large amounts of data only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of generalization-containment rela</context>
</contexts>
<marker>Navigli, Velardi, 2010</marker>
<rawString>Roberto Navigli and Paola Velardi. 2010. Learning Word-Class Lattices for Definition and Hypernym Extraction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages 1318–1327, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2013),</booktitle>
<pages>321--328</pages>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="27130" citStr="Pantel and Ravichandran, 2004" startWordPosition="4630" endWordPosition="4634">ng work on large amounts of data only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of generalization-containment relations between Wikipedia categories, to extract the hypernymy taxonomy as a subset of the network. The first approach of this kind was WikiTaxonomy (Ponzetto and Strube, 2007; Ponzetto and Strube, 2011), based on simple, yet effective lightweight heuristics, totaling more than 100k is-a relations. Other</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically labeling semantic classes. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2013), Boston, Massachusetts, 2–7 May 2004, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Deriving a large scale taxonomy from Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd Conference on the Advancement ofArtificial Intelligence (AAAI ’07),</booktitle>
<pages>1440--1445</pages>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="2571" citStr="Ponzetto and Strube, 2007" startWordPosition="369" endWordPosition="372">specific, as in Freebase (Bollacker et al., 2008), to unspecified relations, as in BabelNet. However, unlike the case with smaller manually-curated resources such as WordNet (Fellbaum, 1998), in many large automatically-created resources the taxonomical information is either missing, mixed across resources, e.g., linking Wikipedia categories to WordNet synsets as in YAGO, or coarsegrained, as in DBpedia whose hypernyms link to a small upper taxonomy. Current approaches in the literature have mostly focused on the extraction of taxonomies from the network of Wikipedia categories. WikiTaxonomy (Ponzetto and Strube, 2007), the first approach of this kind, is based on the use of heuristics to determine whether is-a relations hold between a category and its subcategories. Subsequent approaches have also exploited heuristics, but have extended them to any kind of semantic relation expressed in the category names (Nastase and Strube, 2013). But while the aforementioned attempts provide structure for categories that supply meta-information for Wikipedia pages, surprisingly little attention has been paid to the acquisition of a full-fledged taxonomy for Wikipedia pages themselves. For instance, Ruiz-Casado et al. (2</context>
<context position="27600" citStr="Ponzetto and Strube, 2007" startWordPosition="4699" endWordPosition="4702">plicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of generalization-containment relations between Wikipedia categories, to extract the hypernymy taxonomy as a subset of the network. The first approach of this kind was WikiTaxonomy (Ponzetto and Strube, 2007; Ponzetto and Strube, 2011), based on simple, yet effective lightweight heuristics, totaling more than 100k is-a relations. Other approaches, such as YAGO (Suchanek et al., 2008; Hoffart et al., 2013), yield a taxonomical backbone by linking Wikipedia categories to WordNet. However, the categories are linked to the first, i.e., most frequent, sense of the category head in WordNet, involving only leaf categories in the linking. Interest in taxonomizing Wikipedia pages, instead, developed with DBpedia (Auer et al., 2007), which pioneered the current stream of work aimed at extracting semi-struc</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2007. Deriving a large scale taxonomy from Wikipedia. In Proceedings of the 22nd Conference on the Advancement ofArtificial Intelligence (AAAI ’07), Vancouver, B.C., Canada, 22–26 July 2007, pages 1440–1445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Taxonomy induction based on a collaboratively built knowledge repository.</title>
<date>2011</date>
<journal>Artificial Intelligence,</journal>
<pages>175--9</pages>
<contexts>
<context position="27628" citStr="Ponzetto and Strube, 2011" startWordPosition="4703" endWordPosition="4706">ng new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of generalization-containment relations between Wikipedia categories, to extract the hypernymy taxonomy as a subset of the network. The first approach of this kind was WikiTaxonomy (Ponzetto and Strube, 2007; Ponzetto and Strube, 2011), based on simple, yet effective lightweight heuristics, totaling more than 100k is-a relations. Other approaches, such as YAGO (Suchanek et al., 2008; Hoffart et al., 2013), yield a taxonomical backbone by linking Wikipedia categories to WordNet. However, the categories are linked to the first, i.e., most frequent, sense of the category head in WordNet, involving only leaf categories in the linking. Interest in taxonomizing Wikipedia pages, instead, developed with DBpedia (Auer et al., 2007), which pioneered the current stream of work aimed at extracting semi-structured information from Wikip</context>
</contexts>
<marker>Ponzetto, Strube, 2011</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2011. Taxonomy induction based on a collaboratively built knowledge repository. Artificial Intelligence, 175(9-10):1737–1756.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hoifung Poon</author>
<author>Janara Christensen</author>
<author>Pedro Domingos</author>
<author>Oren Etzioni</author>
<author>Raphael Hoffmann</author>
<author>Chloe Kiddon</author>
<author>Thomas Lin</author>
<author>Xiao Ling</author>
<author>Alan Ritter Mausam</author>
<author>Stefan Schoenmackers</author>
<author>Stephen Soderland</author>
<author>Dan Weld</author>
<author>Fei Wu</author>
<author>Congle Zhang</author>
</authors>
<title>Machine Reading at the University of Washington.</title>
<date>2010</date>
<booktitle>In Proceedings of the 1st International Workshop on Formalisms and Methodology for Learning by Reading in conjunction with NAACL-HLT 2010,</booktitle>
<pages>87--95</pages>
<location>Los Angeles, California, USA.</location>
<contexts>
<context position="997" citStr="Poon et al., 2010" startWordPosition="134" endWordPosition="137">my of Wikipage pages and categories. We leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and WikiTaxonomy. WiBi is available at http://wibitaxonomy.org. 1 Introduction Knowledge has unquestionably become a key component of current intelligent systems in many fields of Artificial Intelligence. The creation and use of machine-readable knowledge has not only entailed researchers (Mitchell, 2005; Mirkin et al., 2009; Poon et al., 2010) developing huge, broadcoverage knowledge bases (Hovy et al., 2013; Suchanek and Weikum, 2013), but it has also hit big industry players such as Google (Singhal, 2012) and IBM (Ferrucci, 2012), which are moving fast towards large-scale knowledge-oriented systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a great deal of valuabl</context>
</contexts>
<marker>Poon, Christensen, Domingos, Etzioni, Hoffmann, Kiddon, Lin, Ling, Mausam, Schoenmackers, Soderland, Weld, Wu, Zhang, 2010</marker>
<rawString>Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann, Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers, Stephen Soderland, Dan Weld, Fei Wu, and Congle Zhang. 2010. Machine Reading at the University of Washington. In Proceedings of the 1st International Workshop on Formalisms and Methodology for Learning by Reading in conjunction with NAACL-HLT 2010, pages 87–95, Los Angeles, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Ruiz-Casado</author>
<author>Enrique Alfonseca</author>
<author>Pablo Castells</author>
</authors>
<title>Automatic assignment of Wikipedia encyclopedic entries to WordNet synsets.</title>
<date>2005</date>
<booktitle>In Advances in Web Intelligence,</booktitle>
<volume>3528</volume>
<pages>380--386</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="3175" citStr="Ruiz-Casado et al. (2005)" startWordPosition="462" endWordPosition="465">tto and Strube, 2007), the first approach of this kind, is based on the use of heuristics to determine whether is-a relations hold between a category and its subcategories. Subsequent approaches have also exploited heuristics, but have extended them to any kind of semantic relation expressed in the category names (Nastase and Strube, 2013). But while the aforementioned attempts provide structure for categories that supply meta-information for Wikipedia pages, surprisingly little attention has been paid to the acquisition of a full-fledged taxonomy for Wikipedia pages themselves. For instance, Ruiz-Casado et al. (2005) provide a general vector-based method which, however, is incapable of linking pages which do not have a WordNet counterpart. Higher coverage is provided by de Melo and Weikum (2010) thanks to the use of a set of effective heuristics, however, the approach also draws on WordNet and sense frequency information. In this paper we address the task of taxonomizing Wikipedia in a way that is fully independent of other existing resources such as WordNet. We present WiBi, a novel approach to the creation of a Wikipedia bitaxonomy, that is, a taxonomy of Wikipedia pages aligned to a taxonomy of categor</context>
</contexts>
<marker>Ruiz-Casado, Alfonseca, Castells, 2005</marker>
<rawString>Maria Ruiz-Casado, Enrique Alfonseca, and Pablo Castells. 2005. Automatic assignment of Wikipedia encyclopedic entries to WordNet synsets. In Advances in Web Intelligence, volume 3528 of Lecture Notes in Computer Science, pages 380–386. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Singhal</author>
</authors>
<title>Introducing the Knowledge Graph: Things, Not Strings.</title>
<date>2012</date>
<tech>Technical report,</tech>
<institution>Official Blog (of Google). Retrieved</institution>
<contexts>
<context position="1164" citStr="Singhal, 2012" startWordPosition="163" endWordPosition="164"> show higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and WikiTaxonomy. WiBi is available at http://wibitaxonomy.org. 1 Introduction Knowledge has unquestionably become a key component of current intelligent systems in many fields of Artificial Intelligence. The creation and use of machine-readable knowledge has not only entailed researchers (Mitchell, 2005; Mirkin et al., 2009; Poon et al., 2010) developing huge, broadcoverage knowledge bases (Hovy et al., 2013; Suchanek and Weikum, 2013), but it has also hit big industry players such as Google (Singhal, 2012) and IBM (Ferrucci, 2012), which are moving fast towards large-scale knowledge-oriented systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a great deal of valuable knowledge which can be harvested and transformed into structured form (Medelyan et al., 2009; Hovy et al., 2013). Prominent examples include DBpedia (Bizer et al., 2</context>
</contexts>
<marker>Singhal, 2012</marker>
<rawString>Amit Singhal. 2012. Introducing the Knowledge Graph: Things, Not Strings. Technical report, Official Blog (of Google). Retrieved May 18, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogeneous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL</booktitle>
<pages>801--808</pages>
<contexts>
<context position="27150" citStr="Snow et al., 2006" startWordPosition="4635" endWordPosition="4638">a only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of generalization-containment relations between Wikipedia categories, to extract the hypernymy taxonomy as a subset of the network. The first approach of this kind was WikiTaxonomy (Ponzetto and Strube, 2007; Ponzetto and Strube, 2011), based on simple, yet effective lightweight heuristics, totaling more than 100k is-a relations. Other approaches, such as</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Semantic taxonomy induction from heterogeneous evidence. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), pages 801– 808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Suchanek</author>
<author>Gerhard Weikum</author>
</authors>
<title>Knowledge harvesting from text and Web sources.</title>
<date>2013</date>
<booktitle>In IEEE 29th International Conference on Data Engineering (ICDE 2013),</booktitle>
<pages>1250--1253</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Brisbane, Australia.</location>
<contexts>
<context position="1091" citStr="Suchanek and Weikum, 2013" startWordPosition="148" endWordPosition="151">ne of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and WikiTaxonomy. WiBi is available at http://wibitaxonomy.org. 1 Introduction Knowledge has unquestionably become a key component of current intelligent systems in many fields of Artificial Intelligence. The creation and use of machine-readable knowledge has not only entailed researchers (Mitchell, 2005; Mirkin et al., 2009; Poon et al., 2010) developing huge, broadcoverage knowledge bases (Hovy et al., 2013; Suchanek and Weikum, 2013), but it has also hit big industry players such as Google (Singhal, 2012) and IBM (Ferrucci, 2012), which are moving fast towards large-scale knowledge-oriented systems. The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary. These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a great deal of valuable knowledge which can be harvested and transformed into structured form (Medelyan et al., 2009</context>
</contexts>
<marker>Suchanek, Weikum, 2013</marker>
<rawString>Fabian Suchanek and Gerhard Weikum. 2013. Knowledge harvesting from text and Web sources. In IEEE 29th International Conference on Data Engineering (ICDE 2013), pages 1250–1253, Brisbane, Australia. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO: A large ontology from Wikipedia and WordNet.</title>
<date>2008</date>
<journal>Journal of Web Semantics,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="27778" citStr="Suchanek et al., 2008" startWordPosition="4725" endWordPosition="4728">ecent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of generalization-containment relations between Wikipedia categories, to extract the hypernymy taxonomy as a subset of the network. The first approach of this kind was WikiTaxonomy (Ponzetto and Strube, 2007; Ponzetto and Strube, 2011), based on simple, yet effective lightweight heuristics, totaling more than 100k is-a relations. Other approaches, such as YAGO (Suchanek et al., 2008; Hoffart et al., 2013), yield a taxonomical backbone by linking Wikipedia categories to WordNet. However, the categories are linked to the first, i.e., most frequent, sense of the category head in WordNet, involving only leaf categories in the linking. Interest in taxonomizing Wikipedia pages, instead, developed with DBpedia (Auer et al., 2007), which pioneered the current stream of work aimed at extracting semi-structured information from Wikipedia templates and infoboxes. In DBpedia, entities are mapped to a coarse-grained ontology which is collaboratively maintained and contains only about</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. YAGO: A large ontology from Wikipedia and WordNet. Journal of Web Semantics, 6(3):203–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Vannella</author>
<author>David Jurgens</author>
<author>Daniele Scarfini</author>
<author>Domenico Toscani</author>
<author>Roberto Navigli</author>
</authors>
<title>Validating and Extending Semantic Knowledge Bases using Video Games with a Purpose.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014),</booktitle>
<location>Baltimore, USA.</location>
<marker>Vannella, Jurgens, Scarfini, Toscani, Navigli, 2014</marker>
<rawString>Daniele Vannella, David Jurgens, Daniele Scarfini, Domenico Toscani, and Roberto Navigli. 2014. Validating and Extending Semantic Knowledge Bases using Video Games with a Purpose. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Velardi</author>
<author>Stefano Faralli</author>
<author>Roberto Navigli</author>
</authors>
<title>OntoLearn Reloaded: A graph-based algorithm for taxonomy induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="26849" citStr="Velardi et al., 2013" startWordPosition="4586" endWordPosition="4589">connected component, with a maximum height of 29 and an average height on the biggest component of 6.98. 6 Related Work Although the extraction of taxonomies from machine-readable dictionaries was already being studied in the early 1970s (Calzolari et al., 1973), pioneering work on large amounts of data only appeared in the 1990s (Hearst, 1992; Ide and V´eronis, 1993). Approaches based on handcrafted patterns and pattern matching techniques have been developed to provide a supertype for the extracted terms (Etzioni et al., 2004; Blohm, 2007; Kozareva and Hovy, 2010; Navigli and Velardi, 2010; Velardi et al., 2013, inter alia). However, these methods do not link terms to existing knowledge resources such as WordNet, whereas those that explicitly link do so by adding new leaves to the existing taxonomy instead of acquiring wide-coverage taxonomies from scratch (Pantel and Ravichandran, 2004; Snow et al., 2006). The recent upsurge of interest in collaborative knowledge curation has enabled several approaches to large-scale taxonomy acquisition (Hovy et al., 2013). Most approaches initially focused on the Wikipedia category network, an entangled set of generalization-containment relations between Wikipedi</context>
</contexts>
<marker>Velardi, Faralli, Navigli, 2013</marker>
<rawString>Paola Velardi, Stefano Faralli, and Roberto Navigli. 2013. OntoLearn Reloaded: A graph-based algorithm for taxonomy induction. Computational Linguistics, 39(3):665–707.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>