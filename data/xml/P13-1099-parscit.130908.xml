<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000238">
<title confidence="0.808735">
Using Supervised Bigram-based ILP for Extractive Summarization
</title>
<author confidence="0.994041">
Chen Li, Xian Qian, and Yang Liu
</author>
<affiliation confidence="0.9915115">
The University of Texas at Dallas
Computer Science Department
</affiliation>
<email confidence="0.99002">
chenli,qx,yangl@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.994626" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999447272727273">
In this paper, we propose a bigram based
supervised method for extractive docu-
ment summarization in the integer linear
programming (ILP) framework. For each
bigram, a regression model is used to es-
timate its frequency in the reference sum-
mary. The regression model uses a vari-
ety of indicative features and is trained dis-
criminatively to minimize the distance be-
tween the estimated and the ground truth
bigram frequency in the reference sum-
mary. During testing, the sentence selec-
tion problem is formulated as an ILP prob-
lem to maximize the bigram gains. We
demonstrate that our system consistently
outperforms the previous ILP method on
different TAC data sets, and performs
competitively compared to the best results
in the TAC evaluations. We also con-
ducted various analysis to show the im-
pact of bigram selection, weight estima-
tion, and ILP setup.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918307692308">
Extractive summarization is a sentence selection
problem: identifying important summary sen-
tences from one or multiple documents. Many
methods have been developed for this problem, in-
cluding supervised approaches that use classifiers
to predict summary sentences, graph based ap-
proaches to rank the sentences, and recent global
optimization methods such as integer linear pro-
gramming (ILP) and submodular methods. These
global optimization methods have been shown to
be quite powerful for extractive summarization,
because they try to select important sentences and
remove redundancy at the same time under the
length constraint.
Gillick and Favre (Gillick and Favre, 2009) in-
troduced the concept-based ILP for summariza-
tion. Their system achieved the best result in the
TAC 09 summarization task based on the ROUGE
evaluation metric. In this approach the goal is
to maximize the sum of the weights of the lan-
guage concepts that appear in the summary. They
used bigrams as such language concepts. The as-
sociation between the language concepts and sen-
tences serves as the constraints. This ILP method
is formally represented as below (see (Gillick and
Favre, 2009) for more details):
</bodyText>
<equation confidence="0.972480857142857">
Ei wici (1)
s.t. sjOccij &lt; ci (2)
Ej sjOccij &gt; ci (3)
E
j ljsj &lt; L (4)
ci E t0, 11 bi (5)
sj E t0, 11 bj (6)
</equation>
<bodyText confidence="0.985585913043479">
ci and sj are binary variables (shown in (5) and
(6)) that indicate the presence of a concept and
a sentence respectively. wi is a concept’s weight
and Occij means the occurrence of concept i in
sentence j. Inequalities (2)(3) associate the sen-
tences and concepts. They ensure that selecting a
sentence leads to the selection of all the concepts
it contains, and selecting a concept only happens
when it is present in at least one of the selected
sentences.
There are two important components in this
concept-based ILP: one is how to select the con-
cepts (ci); the second is how to set up their weights
(wi). Gillick and Favre (Gillick and Favre, 2009)
used bigrams as concepts, which are selected from
a subset of the sentences, and their document fre-
quency as the weight in the objective function.
In this paper, we propose to find a candidate
summary such that the language concepts (e.g., bi-
grams) in this candidate summary and the refer-
ence summary can have the same frequency. We
expect this restriction is more consistent with the
max
</bodyText>
<page confidence="0.966516">
1004
</page>
<note confidence="0.930345">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1004–1013,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.969427625">
ROUGE evaluation metric used for summarization
(Lin, 2004). In addition, in the previous concept-
based ILP method, the constraints are with respect
to the appearance of language concepts, hence it
cannot distinguish the importance of different lan-
guage concepts in the reference summary. Our
method can decide not only which language con-
cepts to use in ILP, but also the frequency of these
language concepts in the candidate summary. To
estimate the bigram frequency in the summary,
we propose to use a supervised regression model
that is discriminatively trained using a variety of
features. Our experiments on several TAC sum-
marization data sets demonstrate this proposed
method outperforms the previous ILP system and
often the best performing TAC system.
</bodyText>
<sectionHeader confidence="0.996702" genericHeader="introduction">
2 Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.996687">
2.1 Bigram Gain Maximization by ILP
</subsectionHeader>
<bodyText confidence="0.998859769230769">
We choose bigrams as the language concepts in
our proposed method since they have been suc-
cessfully used in previous work. In addition, we
expect that the bigram oriented ILP is consistent
with the ROUGE-2 measure widely used for sum-
marization evaluation.
We start the description of our approach for the
scenario where a human abstractive summary is
provided, and the task is to select sentences to
form an extractive summary. Then Our goal is
to make the bigram frequency in this system sum-
mary as close as possible to that in the reference.
For each bigram b, we define its gain:
</bodyText>
<equation confidence="0.811599">
Gain(b, sum) = min{nb,ref, nb,sum} (7)
</equation>
<bodyText confidence="0.999528888888889">
where nb,ref is the frequency of b in the reference
summary, and nb,sum is the frequency of b in the
automatic summary. The gain of a bigram is no
more than its frequency in the reference summary,
hence adding redundant bigrams will not increase
the gain.
The total gain of an extractive summary is de-
fined as the sum of every bigram gain in the sum-
mary:
</bodyText>
<equation confidence="0.9865335">
XGain(sum) = Gain(b, sum)
b
X= Xmin{nb,ref, z(s) ∗ nb,s} (8)
b s
</equation>
<bodyText confidence="0.999931625">
where s is a sentence in the document, nb,s is
the frequency of b in sentence s, z(s) is a binary
variable, indicating whether s is selected in the
summary. The goal is to find z that maximizes
Gain(sum) (formula (8)) under the length con-
straint L.
This problem can be casted as an ILP problem.
First, using the fact that
</bodyText>
<equation confidence="0.984431296296296">
min{a, x} = 0.5(−|x − a |+ x + a), x, a ≥ 0
we have
X Xmin{nb,ref, z(s) ∗ nb,s} =
b s
X X0.5 ∗ (−|nb,ref − z(s) ∗ nb,s |+
b s
Xnb,ref + z(s) ∗ nb,s)
s
Now the problem is equivalent to:
X X(−|nb,ref − z(s) ∗ nb,s |+
max s
z
b
Xnb,ref + z(s) ∗ nb,s)
s
Xs.t. z(s) ∗ |S |≤ L; z(s) ∈ {0,1}
s
This is equivalent to the ILP:
Xmax X z(s) ∗ nb,s − Cb) (9)
b (
s
Xs.t. z(s) ∗ |S |≤ L (10)
s
z(s) ∈ {0, 1} (11)
X−Cb ≤ nb,ref − z(s) ∗ nb,s ≤ Cb
s
(12)
</equation>
<bodyText confidence="0.9997495">
where Cb is an auxiliary variable we introduce that
is equal to |nb,ref − Ps z(s) ∗ nb,s|, and nb,ref is
a constant that can be dropped from the objective
function.
</bodyText>
<subsectionHeader confidence="0.9091825">
2.2 Regression Model for Bigram Frequency
Estimation
</subsectionHeader>
<bodyText confidence="0.9847909">
In the previous section, we assume that nb,ref is
at hand (reference abstractive summary is given)
and propose a bigram-based optimization frame-
work for extractive summarization. However, for
the summarization task, the bigram frequency is
unknown, and thus our first goal is to estimate such
frequency. We propose to use a regression model
for this.
Since a bigram’s frequency depends on the sum-
mary length (L), we use a normalized frequency
</bodyText>
<page confidence="0.911259">
1005
</page>
<bodyText confidence="0.732489">
in our method. Let nb,ref = Nb,ref ∗ L, where
</bodyText>
<equation confidence="0.9809725">
N n(b,ref) is the normalized frequency
b,ref = Eb n(b,ref)
</equation>
<bodyText confidence="0.992148">
in the summary. Now the problem is to automati-
cally estimate Nb,ref.
Since the normalized frequency Nb,ref is a real
number, we choose to use a logistic regression
model to predict it:
</bodyText>
<equation confidence="0.9958845">
exp{w′f(b)} (13)
Nb,ref = Pj exp {w′ f (bj)}
</equation>
<bodyText confidence="0.983084">
where f(bj) is the feature vector of bigram bj and
w′ is the corresponding feature weight. Since even
for identical bigrams bi = bj, their feature vectors
may be different (f(bi) =� f(bj)) due to their dif-
ferent contexts, we sum up frequencies for identi-
calbigrams {bi|bi = b}:
</bodyText>
<equation confidence="0.99688675">
XNb,ref = Nbi,ref
i,bi=b
= P Pj exp{w′ f (bj)} ( )
i,bi=b exp{w′f(bi)} 14
</equation>
<bodyText confidence="0.9999397">
To train this regression model using the given
reference abstractive summaries, rather than trying
to minimize the squared error as typically done,
we propose a new objective function. Since the
normalized frequency satisfies the probability con-
straint Pb Nb,ref = 1, we propose to use KL di-
vergence to measure the distance between the es-
timated frequencies and the ground truth values.
The objective function for training is thus to mini-
mize the KL distance:
</bodyText>
<equation confidence="0.950217">
eNb,ref (15)
Nb,ref
</equation>
<bodyText confidence="0.998156">
where eNb,ref is the true normalized frequency of
bigram b in reference summaries.
Finally, we replace Nb,ref in Formula (15) with
Eq (14) and get the objective function below:
</bodyText>
<listItem confidence="0.998931151515151">
• Word Level:
– 1. Term frequency1: The frequency of
this bigram in the given topic.
– 2. Term frequency2: The frequency of
this bigram in the selected sentences1.
– 3. Stop word ratio: Ratio of stop words
in this bigram. The value can be {0, 0.5,
1}.
– 4. Similarity with topic title: The
number of common tokens in these two
strings, divided by the length of the
longer string.
– 5. Similarity with description of the
topic: Similarity of the bigram with
topic description (see next data section
about the given topics in the summariza-
tion task).
• Sentence Level: (information of sentence
containing the bigram)
– 6. Sentence ratio: Number of sentences
that include this bigram, divided by the
total number of the selected sentences.
– 7. Sentence similarity: Sentence sim-
ilarity with topic’s query, which is the
concatenation of topic title and descrip-
tion.
– 8. Sentence position: Sentence posi-
tion in the document.
– 9. Sentence length: The number of
words in the sentence.
– 10. Paragraph starter: Binary feature
indicating whether this sentence is the
beginning of a paragraph.
</listItem>
<figure confidence="0.9554762">
Xmin eNb,ref log
b
Xmax P 3 Experiments
b i,bi=bexp{w′f(bi)} 3.1 Data
eNb,ref log Pj exp{w′f(bj)} (16)
</figure>
<bodyText confidence="0.99987975">
This shares the same form as the contrastive es-
timation proposed by (Smith and Eisner, 2005).
We use gradient decent method for parameter esti-
mation, initial w is set with zero.
</bodyText>
<subsectionHeader confidence="0.958964">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.999962125">
Each bigram is represented using a set of features
in the above regression model. We use two types
of features: word level and sentence level features.
Some of these features have been used in previous
work (Aker and Gaizauskas, 2009; Brandow et al.,
1995; Edmundson, 1969; Radev, 2001):
We evaluate our method using several recent TAC
data sets, from 2008 to 2011. The TAC summa-
rization task is to generate at most 100 words sum-
maries from 10 documents for a given topic query
(with a title and more detailed description). For
model training, we also included two years’ DUC
data (2006 and 2007). When evaluating on one
TAC data set, we use the other years of the TAC
data plus the two DUC data sets as the training
data.
</bodyText>
<footnote confidence="0.879226">
1See next section about the sentence selection step
</footnote>
<page confidence="0.978258">
1006
</page>
<subsectionHeader confidence="0.999753">
3.2 Summarization System
</subsectionHeader>
<bodyText confidence="0.998745">
We use the same system pipeline described in
(Gillick et al., 2008; McDonald, 2007). The key
modules in the ICSI ILP system (Gillick et al.,
2008) are briefly described below.
</bodyText>
<listItem confidence="0.986660285714286">
• Step 1: Clean documents, split text into sen-
tences.
• Step 2: Extract bigrams from all the sen-
tences, then select those bigrams with doc-
ument frequency equal to more than 3. We
call this subset as initial bigram set in the fol-
lowing.
• Step 3: Select relevant sentences that contain
at least one bigram from the initial bigram
set.
• Step 4: Feed the ILP with sentences and the
bigram set to get the result.
• Step 5: Order sentences identified by ILP as
the final result of summary.
</listItem>
<bodyText confidence="0.999982333333333">
The difference between the ICSI and our system
is in the 4th step. In our method, we first extract all
the bigrams from the selected sentences and then
estimate each bigram’s Nb,ref using the regression
model. Then we use the top-n bigrams with their
Nb,ref and all the selected sentences in our pro-
posed ILP module for summary sentence selec-
tion. When training our bigram regression model,
we use each of the 4 reference summaries sepa-
rately, i.e., the bigram frequency is obtained from
one reference summary. The same pre-selection of
sentences described above is also applied in train-
ing, that is, the bigram instances used in training
are from these selected sentences and the reference
summary.
</bodyText>
<sectionHeader confidence="0.968884" genericHeader="background">
4 Experiment and Analysis
</sectionHeader>
<subsectionHeader confidence="0.989661">
4.1 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9997822">
Table 1 shows the ROUGE-2 results of our pro-
posed system, the ICSI system, and also the best
performing system in the NIST TAC evaluation.
We can see that our proposed system consistently
outperforms ICSI ILP system (the gain is statis-
tically significant based on ROUGE’s 95% confi-
dence internal results). Compared to the best re-
ported TAC result, our method has better perfor-
mance on three data sets, except 2011 data. Note
that the best performing system for the 2009 data
is the ICSI ILP system, with an additional com-
pression step. Our ILP method is purely extrac-
tive. Even without using compression, our ap-
proach performs better than the full ICSI system.
The best performing system for the 2011 data also
has some compression module. We expect that af-
ter applying sentence compression and merging,
we will have even better performance, however,
our focus in this paper is on the bigram-based ex-
tractive summarization.
</bodyText>
<table confidence="0.998856166666667">
ICSI Proposed TAC Rank1
ILP System System
2008 0.1023 0.1076 0.1038
2009 0.1160 0.1246 0.1216
2010 0.1003 0.1067 0.0957
2011 0.1271 0.1327 0.1344
</table>
<tableCaption confidence="0.999646">
Table 1: ROUGE-2 summarization results.
</tableCaption>
<bodyText confidence="0.9999736875">
There are several differences between the ICSI
system and our proposed method. First is the
bigrams (concepts) used. We use the top 100
bigrams from our bigram estimation module;
whereas the ICSI system just used the initial bi-
gram set described in Section 3.2. Second, the
weights for those bigrams differ. We used the es-
timated value from the regression model; the ICSI
system just uses the bigram’s document frequency
in the original text as weight. Finally, two systems
use different ILP setups. To analyze which fac-
tors (or all of them) explain the performance dif-
ference, we conducted various controlled experi-
ments for these three factors (bigrams, weights,
ILP). All of the following experiments use the
TAC 2009 data as the test set.
</bodyText>
<subsectionHeader confidence="0.999031">
4.2 Effect of Bigram Weights
</subsectionHeader>
<bodyText confidence="0.999144823529412">
In this experiment, we vary the weighting methods
for the two systems: our proposed method and the
ICSI system. We use three weighting setups: the
estimated bigram frequency value in our method,
document frequency, or term frequency from the
original text. Table 2 and 3 show the results using
the top 100 bigrams from our system and the ini-
tial bigram set from the ICSI system respectively.
We also evaluate using the two different ILP con-
figurations in these experiments.
First of all, we can see that for both ILP sys-
tems, our estimated bigram weights outperform
the other frequency-based weights. For the ICSI
ILP system, using bigram document frequency
achieves better performance than term frequency
(which verified why document frequency is used
in their system). In contrast, for our ILP method,
</bodyText>
<page confidence="0.875721">
1007
</page>
<table confidence="0.995692571428571">
# Weight ILP ROUGE-2
1 Estimated value Proposed 0.1246
2 ICSI 0.1178
3 Document freq Proposed 0.1109
4 ICSI 0.1132
5 Term freq Proposed 0.1116
6 ICSI 0.1080
</table>
<tableCaption confidence="0.988173333333333">
Table 2: Results using different weighting meth-
ods on the top 100 bigrams generated from our
proposed system.
</tableCaption>
<table confidence="0.638100142857143">
# Weight ILP ROUGE-2
1 Estimated value Proposed 0.1157
2 ICSI 0.1161
3 Document freq Proposed 0.1101
4 ICSI 0.1160
5 Term freq Proposed 0.1109
6 ICSI 0.1072
</table>
<tableCaption confidence="0.918156">
Table 3: Results using different weighting meth-
</tableCaption>
<bodyText confidence="0.997497722222222">
ods based on the initial bigram sets. The average
number of bigrams is around 80 for each topic.
the bigram’s term frequency is slightly more use-
ful than its document frequency. This indicates
that our estimated value is more related to bi-
gram’s term frequency in the original text. When
the weight is document frequency, the ICSI’s re-
sult is better than our proposed ILP; whereas when
using term frequency as the weights, our ILP has
better results, again suggesting term frequency fits
our ILP system better. When the weight is esti-
mated value, the results depend on the bigram set
used. The ICSI’s ILP performs slightly better than
ours when it is equipped with the initial bigram,
but our proposed ILP has much better results us-
ing our selected top100 bigrams. This shows that
the size and quality of the bigrams has an impact
on the ILP modules.
</bodyText>
<subsectionHeader confidence="0.995131">
4.3 The Effect of Bigram Set’s size
</subsectionHeader>
<bodyText confidence="0.999708">
In our proposed system, we use 100 top bigrams.
There are about 80 bigrams used in the ICSI ILP
system. A natural question to ask is the impact
of the number of bigrams and their quality on the
summarization system. Table 4 shows some statis-
tics of the bigrams. We can see that about one
third of bigrams in the reference summary are in
the original text (127.3 out of 321.93), verifying
that people do use different words/bigram when
writing abstractive summaries. We mentioned that
we only use the top-N (n is 100 in previous ex-
periments) bigrams in our summarization system.
On one hand, this is to save computational cost for
the ILP module. On the other hand, we see from
the table that only 127 of these more than 2K bi-
grams are in the reference summary and are thus
expected to help the summary responsiveness. In-
cluding all the bigrams would lead to huge noise.
</bodyText>
<table confidence="0.800704">
# bigrams in ref summary 321.93
# bigrams in text and ref summary 127.3
# bigrams used in our regression model 2140.7
(i.e., in selected sentences)
</table>
<tableCaption confidence="0.9624345">
Table 4: Bigram statistics. The numbers are the
average ones for each topic.
</tableCaption>
<bodyText confidence="0.967103818181818">
Fig 1 shows the bigram coverage (number of bi-
grams used in the system that are also in reference
summaries) when we vary N selected bigrams. As
expected, we can see that as n increases, there
are more reference summary bigrams included in
the system. There are 25 summary bigrams in the
top-50 bigrams and about 38 in top-100 bigrams.
Compared with the ICSI system that has around 80
bigrams in the initial bigram set and 29 in the ref-
erence summary, our estimation module has better
coverage.
</bodyText>
<figureCaption confidence="0.775336">
Figure 1: Coverage of bigrams (number of bi-
</figureCaption>
<bodyText confidence="0.957141545454545">
grams in reference summary) when varying the
number of bigrams used in the ILP systems.
Increasing the number of bigrams used in the
system will lead to better coverage, however, the
incorrect bigrams also increase and have a nega-
tive impact on the system performance. To exam-
ine the best tradeoff, we conduct the experiments
by choosing the different top-N bigram set for the
two ILP systems, as shown in Fig 2. For both the
ILP systems, we used the estimated weight value
for the bigrams.
</bodyText>
<figure confidence="0.994115888888889">
Number of Bigram both in Selected
and Reference
130
120
110
100
40
90
80
70
60
50
30
20
10
0
50 500 950 1400 1850 2300 2750 3200
Number of Selected Bigram
</figure>
<page confidence="0.979971">
1008
</page>
<bodyText confidence="0.992722411764706">
We can see that the ICSI ILP system performs
better when the input bigrams have less noise
(those bigrams that are not in summary). However,
our proposed method is slightly more robust to this
kind of noise, possibly because of the weights we
use in our system – the noisy bigrams have lower
weights and thus less impact on the final system
performance. Overall the two systems have sim-
ilar trends: performance increases at the begin-
ning when using more bigrams, and after certain
points starts degrading with too many bigrams.
The optimal number of bigrams differs for the two
systems, with a larger number of bigrams in our
method. We also notice that the ICSI ILP system
achieved a ROUGE-2 of 0.1218 when using top
60 bigrams, which is better than using the initial
bigram set in their method (0.1160).
</bodyText>
<figureCaption confidence="0.9899875">
Figure 2: Summarization performance when vary-
ing the number of bigrams for two systems.
</figureCaption>
<subsectionHeader confidence="0.994061">
4.4 Oracle Experiments
</subsectionHeader>
<bodyText confidence="0.999935238095238">
Based on the above analysis, we can see the impact
of the bigram set and their weights. The following
experiments are designed to demonstrate the best
system performance we can achieve if we have ac-
cess to good quality bigrams and weights. Here we
use the information from the reference summary.
The first is an oracle experiment, where we use
all the bigrams from the reference summaries that
are also in the original text. In the ICSI ILP
system, the weights are the document frequency
from the multiple reference summaries. In our ILP
module, we use the term frequency of the bigram.
The oracle results are shown in Table 5. We can
see these are significantly better than the automatic
systems.
From Table 5, we notice that ICSI’s ILP per-
forms marginally better than our proposed ILP. We
hypothesize that one reason may be that many bi-
grams in the summary reference only appear once.
Table 6 shows the frequency of the bigrams in the
summary. Indeed 85% of bigram only appear once
</bodyText>
<table confidence="0.762737666666667">
ILP System ROUGE-2
Our ILP 0.2124
ICSI ILP 0.2128
</table>
<tableCaption confidence="0.844153">
Table 5: Oracle experiment: using bigrams and
</tableCaption>
<bodyText confidence="0.948063625">
their frequencies in the reference summary as
weights.
and no bigrams appear more than 9 times. For the
majority of the bigrams, our method and the ICSI
ILP are the same. For the others, our system has
slight disadvantage when using the reference term
frequency. We expect the high term frequency
may need to be properly smoothed/normalized.
</bodyText>
<table confidence="0.959293">
Freq 1 2 3 4 5 6 7 8 9
Ave# 277 32 7.5 3.2 1.1 0.3 0.1 0.1 0.04
</table>
<tableCaption confidence="0.834576">
Table 6: Average number of bigrams for each term
frequency in one topic’s reference summary.
</tableCaption>
<bodyText confidence="0.999964935483871">
We also treat the oracle results as the gold stan-
dard for extractive summarization and compared
how the two automatic summarization systems
differ at the sentence level. This is different from
the results in Table 1, which are the ROUGE re-
sults comparing to human written abstractive sum-
maries at the n-gram level. We found that among
the 188 sentences in this gold standard, our system
hits 31 and ICSI only has 23. This again shows
that our system has better performance, not just
at the word level based on ROUGE measures, but
also at the sentence level. There are on average
3 different sentences per topic between these two
results.
In the second experiment, after we obtain the
estimated Nb,ref for every bigram in the selected
sentences from our regression model, we only
keep those bigrams that are in the reference sum-
mary, and use the estimated weights for both ILP
modules. Table 7 shows the results. We can
consider these as the upper bound the system
can achieve if we use the automatically estimated
weights for the correct bigrams. In this experi-
ment ICSI ILP’s performance still performs better
than ours. This might be attributed to the fact there
is less noise (all the bigrams are the correct ones)
and thus the ICSI ILP system performs well. We
can see that these results are worse than the pre-
vious oracle experiments, but are better than using
the automatically generated bigrams, again show-
ing the bigram and weight estimation is critical for
</bodyText>
<figure confidence="0.9981945">
Rouge-2
0.125
0.123
0.121
0.119
0.117
0.115
0.113
0.111
0.109
40 50 60 70 80 90 100 110 120 130
Number of selected bigram
Proposed ILP
ICSI
</figure>
<page confidence="0.942966">
1009
</page>
<table confidence="0.88787925">
summarization.
# Weight ILP ROUGE-2
1 Estimated value Proposed 0.1888
2 ICSI 0.1942
</table>
<tableCaption confidence="0.946609666666667">
Table 7: Summarization results when using the es-
timated weights and only keeping the bigrams that
are in the reference summary.
</tableCaption>
<figure confidence="0.997466882352941">
Rouge-2
0.125
0.124
0.123
0.122
0.121
0.119
0.118
0.117
0.116
0.115
0.114
0.113
0.112
0.12
20 40 60 80 100 120 140 160 180 200 220 240
Number of trainning topics
</figure>
<figureCaption confidence="0.999811">
Figure 3: Learning curve
</figureCaption>
<subsectionHeader confidence="0.99874">
4.5 Effect of Training Set
</subsectionHeader>
<bodyText confidence="0.99982664">
Since our method uses supervised learning, we
conduct the experiment to show the impact of
training size. In TAC’s data, each topic has two
sets of documents. For set A, the task is a standard
summarization, and there are 4 reference sum-
maries, each 100 words long; for set B, it is an up-
date summarization task – the summary includes
information not mentioned in the summary from
set A. There are also 4 reference summaries, with
400 words in total. Table 8 shows the results on
2009 data when using the data from different years
and different sets for training. We notice that when
the training data only contains set A, the perfor-
mance is always better than using set B or the com-
bined set A and B. This is not surprising because
of the different task definition. Therefore, for the
rest of the study on data size impact, we only use
data set A from the TAC data and the DUC data as
the training set. In total there are about 233 topics
from the two years’ DUC data (06, 07) and three
years’ TAC data (08, 10, 11). We incrementally
add 20 topics every time (from DUC06 to TAC11)
and plot the learning curve, as shown in Fig 3. As
expected, more training data results in better per-
formance.
</bodyText>
<figure confidence="0.8396438">
Training Set # Topics ROUGE-2
08 Corpus (A) 48 0.1192
08 Corpus( B) 48 0.1178
08 Corpus (A+B) 96 0.1188
10 Corpus (A) 46 0.1174
10 Corpus (B) 46 0.1167
10 Corpus (A+B) 92 0.1170
11 Corpus (A) 44 0.1157
11 Corpus (B) 44 0.1130
11 Corpus (A+B) 88 0.1140
</figure>
<tableCaption confidence="0.9476105">
Table 8: Summarization performance when using
different training corpora.
</tableCaption>
<subsectionHeader confidence="0.998674">
4.6 Summary of Analysis
</subsectionHeader>
<bodyText confidence="0.9999798125">
The previous experiments have shown the impact
of the three factors: the quality of the bigrams
themselves, the weights used for these bigrams,
and the ILP module. We found that the bigrams
and their weights are critical for both the ILP se-
tups. However, there is negligible difference be-
tween the two ILP methods.
An important part of our system is the super-
vised method for bigram and weight estimation.
We have already seen for the previous ILP method,
when using our bigrams together with the weights,
better performance can be achieved. Therefore we
ask the question whether this is simply because
we use supervised learning, or whether our pro-
posed regression model is the key. To answer this,
we trained a simple supervised binary classifier
for bigram prediction (positive means that a bi-
gram appears in the summary) using the same set
of features as used in our bigram weight estima-
tion module, and then used their document fre-
quency in the ICSI ILP system. The result for
this method is 0.1128 on the TAC 2009 data. This
is much lower than our result. We originally ex-
pected that using the supervised method may out-
perform the unsupervised bigram selection which
only uses term frequency information. Further ex-
periments are needed to investigate this. From this
we can see that it is not just the supervised meth-
ods or using annotated data that yields the over-
all improved system performance, but rather our
proposed regression setup for bigrams is the main
reason.
</bodyText>
<sectionHeader confidence="0.999969" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999614">
We briefly describe some prior work on summa-
rization in this section. Unsupervised methods
have been widely used. In particular, recently sev-
eral optimization approaches have demonstrated
</bodyText>
<page confidence="0.97406">
1010
</page>
<bodyText confidence="0.999988750000001">
competitive performance for extractive summa-
rization task. Maximum marginal relevance
(MMR) (Carbonell and Goldstein, 1998) uses a
greedy algorithm to find summary sentences. (Mc-
Donald, 2007) improved the MMR algorithm to
dynamic programming. They used a modified ob-
jective function in order to consider whether the
selected sentence is globally optimal. Sentence-
level ILP was also first introduced in (McDon-
ald, 2007), but (Gillick and Favre, 2009) revised
it to concept-based ILP. (Woodsend and Lapata,
2012) utilized ILP to jointly optimize different as-
pects including content selection, surface realiza-
tion, and rewrite rules in summarization. (Gala-
nis et al., 2012) uses ILP to jointly maximize the
importance of the sentences and their diversity
in the summary. (Berg-Kirkpatrick et al., 2011)
applied a similar idea to conduct the sentence
compression and extraction for multiple document
summarization. (Jin et al., 2010) made a com-
parative study on sentence/concept selection and
pairwise and list ranking algorithms, and con-
cluded ILP performed better than MMR and the
diversity penalty strategy in sentence/concept se-
lection. Other global optimization methods in-
clude submodularity (Lin and Bilmes, 2010) and
graph-based approaches (Erkan and Radev, 2004;
Leskovec et al., 2005; Mihalcea and Tarau, 2004).
Various unsupervised probabilistic topic models
have also been investigated for summarization and
shown promising. For example, (Celikyilmaz and
Hakkani-T¨ur, 2011) used it to model the hidden
abstract concepts across documents as well as the
correlation between these concepts to generate
topically coherent and non-redundant summaries.
(Darling and Song, 2011) applied it to separate
the semantically important words from the low-
content function words.
In contrast to these unsupervised approaches,
there are also various efforts on supervised learn-
ing for summarization where a model is trained to
predict whether a sentence is in the summary or
not. Different features and classifiers have been
explored for this task, such as Bayesian method
(Kupiec et al., 1995), maximum entropy (Osborne,
2002), CRF (Galley, 2006), and recently reinforce-
ment learning (Ryang and Abekawa, 2012). (Aker
et al., 2010) used discriminative reranking on mul-
tiple candidates generated by A* search. Recently,
research has also been performed to address some
issues in the supervised setup, such as the class
data imbalance problem (Xie and Liu, 2010).
In this paper, we propose to incorporate the
supervised method into the concept-based ILP
framework. Unlike previous work using sentence-
based supervised learning, we use a regression
model to estimate the bigrams and their weights,
and use these to guide sentence selection. Com-
pared to the direct sentence-based classification or
regression methods mentioned above, our method
has an advantage. When abstractive summaries
are given, one needs to use that information to au-
tomatically generate reference labels (a sentence
is in the summary or not) for extractive summa-
rization. Most researchers have used the similarity
between a sentence in the document and the ab-
stractive summary for labeling. This is not a per-
fect process. In our method, we do not need to
generate this extra label for model training since
ours is based on bigrams – it is straightforward to
obtain the reference frequency for bigrams by sim-
ply looking at the reference summary. We expect
our approach also paves an easy way for future au-
tomatic abstractive summarization. One previous
study that is most related to ours is (Conroy et al.,
2011), which utilized a Naive Bayes classifier to
predict the probability of a bigram, and applied
ILP for the final sentence selection. They used
more features than ours, whereas we use a discrim-
inatively trained regression model and a modified
ILP framework. Our proposed method performs
better than their reported results in TAC 2011 data.
Another study closely related to ours is (Davis et
al., 2012), which leveraged Latent Semantic Anal-
ysis (LSA) to produce term weights and selected
summary sentences by computing an approximate
solution to the Budgeted Maximal Coverage prob-
lem.
</bodyText>
<sectionHeader confidence="0.996456" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999957916666667">
In this paper, we leverage the ILP method as a core
component in our summarization system. Dif-
ferent from the previous ILP summarization ap-
proach, we propose a supervised learning method
(a discriminatively trained regression model) to
determine the importance of the bigrams fed to
the ILP module. In addition, we revise the ILP to
maximize the bigram gain (which is expected to
be highly correlated with ROUGE-2 scores) rather
than the concept/bigram coverage. Our proposed
method yielded better results than the previous
state-of-the-art ILP system on different TAC data
</bodyText>
<page confidence="0.9806">
1011
</page>
<bodyText confidence="0.999843153846154">
sets. From a series of experiments, we found that
there is little difference between the two ILP mod-
ules, and that the improved system performance is
attributed to the fact that our proposed supervised
bigram estimation module can successfully gather
the important bigram and assign them appropriate
weights. There are several directions that warrant
further research. We plan to consider the context
of bigrams to better predict whether a bigram is in
the reference summary. We will also investigate
the relationship between concepts and sentences,
which may help move towards abstractive summa-
rization.
</bodyText>
<sectionHeader confidence="0.998169" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998789">
This work is partly supported by DARPA under
Contract No. HR0011-12-C-0016 and FA8750-
13-2-0041, and NSF IIS-0845484. Any opinions
expressed in this material are those of the authors
and do not necessarily reflect the views of DARPA
or NSF.
</bodyText>
<sectionHeader confidence="0.99858" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999707367088608">
Ahmet Aker and Robert Gaizauskas. 2009. Summary
generation for toponym-referenced images using ob-
ject type language models. In Proceedings of the
International Conference RANLP.
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas.
2010. Multi-document summarization using a*
search and discriminative training. In Proceedings
of the EMNLP.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the ACL.
Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995.
Automatic condensation of electronic publications
by sentence selection. Inf. Process. Manage.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the SIGIR.
Asli Celikyilmaz and Dilek Hakkani-T¨ur. 2011. Dis-
covery of topically coherent sentences for extractive
summarization. In Proceedings of the ACL.
John M. Conroy, Judith D. Schlesinger, Jeff Kubina,
Peter A. Rankel, and Dianne P. O’Leary. 2011.
Classy 2011 at tac: Guided and multi-lingual sum-
maries and evaluation metrics. In Proceedings ofthe
TAC.
William M. Darling and Fei Song. 2011. Probabilistic
document modeling for syntax removal in text sum-
marization. In Proceedings of the ACL.
Sashka T. Davis, John M. Conroy, and Judith D.
Schlesinger. 2012. Occams - an optimal combinato-
rial covering algorithm for multi-document summa-
rization. In Proceedings of the ICDM.
H. P. Edmundson. 1969. New methods in automatic
extracting. J. ACM.
G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res.
Dimitrios Galanis, Gerasimos Lampouras, and Ion An-
droutsopoulos. 2012. Extractive multi-document
summarization with integer linear programming and
support vector regression. In Proceedings of the
COLING.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the EMNLP.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the
Workshop on Integer Linear Programmingfor Natu-
ral Langauge Processing on NAACL.
Dan Gillick, Benoit Favre, and Dilek Hakkani-T¨ur.
2008. In The ICSI Summarization System at TAC
2008.
Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. A
comparative study on ranking and selection strate-
gies for multi-document summarization. In Pro-
ceedings of the COLING.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the SIGIR.
Jure Leskovec, Natasa Milic-Frayling, and Marko Gro-
belnik. 2005. Impact of linguistic analysis on the
semantic graph coverage and learning of document
extracts. In Proceedings of the AAAI.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In Proceedings of the NAACL.
Chin-Yew Lin. 2004. Rouge: a package for auto-
matic evaluation of summaries. In Proceedings of
the ACL.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Pro-
ceedings of the European conference on IR research.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into text. In Proceedings of the
EMNLP.
Miles Osborne. 2002. Using maximum entropy for
sentence extraction. In Proceedings of the ACL-02
Workshop on Automatic Summarization.
</reference>
<page confidence="0.848745">
1012
</page>
<reference confidence="0.999042933333333">
Dragomir R. Radev. 2001. Experiments in single and
multidocument summarization using mead. In In
First Document Understanding Conference.
Seonggi Ryang and Takeshi Abekawa. 2012. Frame-
work of automatic text summarization using rein-
forcement learning. In Proceedings of the EMNLP.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In Proceedings of the ACL.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of the EMNLP.
Shasha Xie and Yang Liu. 2010. Improving supervised
learning for meeting summarization using sampling
and regression. Comput. Speech Lang.
</reference>
<page confidence="0.979039">
1013
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.696588">
<title confidence="0.99993">Using Supervised Bigram-based ILP for Extractive Summarization</title>
<author confidence="0.99516">Xian Qian Li</author>
<affiliation confidence="0.9985115">The University of Texas at Computer Science</affiliation>
<email confidence="0.998738">chenli,qx,yangl@hlt.utdallas.edu</email>
<abstract confidence="0.98671747826087">In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety of indicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmet Aker</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Summary generation for toponym-referenced images using object type language models.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference RANLP.</booktitle>
<contexts>
<context position="9833" citStr="Aker and Gaizauskas, 2009" startWordPosition="1695" endWordPosition="1698">ragraph starter: Binary feature indicating whether this sentence is the beginning of a paragraph. Xmin eNb,ref log b Xmax P 3 Experiments b i,bi=bexp{w′f(bi)} 3.1 Data eNb,ref log Pj exp{w′f(bj)} (16) This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). We use gradient decent method for parameter estimation, initial w is set with zero. 2.3 Features Each bigram is represented using a set of features in the above regression model. We use two types of features: word level and sentence level features. Some of these features have been used in previous work (Aker and Gaizauskas, 2009; Brandow et al., 1995; Edmundson, 1969; Radev, 2001): We evaluate our method using several recent TAC data sets, from 2008 to 2011. The TAC summarization task is to generate at most 100 words summaries from 10 documents for a given topic query (with a title and more detailed description). For model training, we also included two years’ DUC data (2006 and 2007). When evaluating on one TAC data set, we use the other years of the TAC data plus the two DUC data sets as the training data. 1See next section about the sentence selection step 1006 3.2 Summarization System We use the same system pipel</context>
</contexts>
<marker>Aker, Gaizauskas, 2009</marker>
<rawString>Ahmet Aker and Robert Gaizauskas. 2009. Summary generation for toponym-referenced images using object type language models. In Proceedings of the International Conference RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmet Aker</author>
<author>Trevor Cohn</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Multi-document summarization using a* search and discriminative training.</title>
<date>2010</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="28133" citStr="Aker et al., 2010" startWordPosition="4834" endWordPosition="4837">te topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class data imbalance problem (Xie and Liu, 2010). In this paper, we propose to incorporate the supervised method into the concept-based ILP framework. Unlike previous work using sentencebased supervised learning, we use a regression model to estimate the bigrams and their weights, and use these to guide sentence selection. Compared to the direct sentence-based classification or regression methods mentioned above, ou</context>
</contexts>
<marker>Aker, Cohn, Gaizauskas, 2010</marker>
<rawString>Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010. Multi-document summarization using a* search and discriminative training. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="26710" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="4624" endWordPosition="4627">nces. (McDonald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for su</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Brandow</author>
<author>Karl Mitze</author>
<author>Lisa F Rau</author>
</authors>
<title>Automatic condensation of electronic publications by sentence selection.</title>
<date>1995</date>
<journal>Inf. Process. Manage.</journal>
<contexts>
<context position="9855" citStr="Brandow et al., 1995" startWordPosition="1699" endWordPosition="1702">ture indicating whether this sentence is the beginning of a paragraph. Xmin eNb,ref log b Xmax P 3 Experiments b i,bi=bexp{w′f(bi)} 3.1 Data eNb,ref log Pj exp{w′f(bj)} (16) This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). We use gradient decent method for parameter estimation, initial w is set with zero. 2.3 Features Each bigram is represented using a set of features in the above regression model. We use two types of features: word level and sentence level features. Some of these features have been used in previous work (Aker and Gaizauskas, 2009; Brandow et al., 1995; Edmundson, 1969; Radev, 2001): We evaluate our method using several recent TAC data sets, from 2008 to 2011. The TAC summarization task is to generate at most 100 words summaries from 10 documents for a given topic query (with a title and more detailed description). For model training, we also included two years’ DUC data (2006 and 2007). When evaluating on one TAC data set, we use the other years of the TAC data plus the two DUC data sets as the training data. 1See next section about the sentence selection step 1006 3.2 Summarization System We use the same system pipeline described in (Gill</context>
</contexts>
<marker>Brandow, Mitze, Rau, 1995</marker>
<rawString>Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995. Automatic condensation of electronic publications by sentence selection. Inf. Process. Manage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the SIGIR.</booktitle>
<contexts>
<context position="26034" citStr="Carbonell and Goldstein, 1998" startWordPosition="4519" endWordPosition="4522">uses term frequency information. Further experiments are needed to investigate this. From this we can see that it is not just the supervised methods or using annotated data that yields the overall improved system performance, but rather our proposed regression setup for bigrams is the main reason. 5 Related Work We briefly describe some prior work on summarization in this section. Unsupervised methods have been widely used. In particular, recently several optimization approaches have demonstrated 1010 competitive performance for extractive summarization task. Maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998) uses a greedy algorithm to find summary sentences. (McDonald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the s</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Discovery of topically coherent sentences for extractive summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<marker>Celikyilmaz, Hakkani-T¨ur, 2011</marker>
<rawString>Asli Celikyilmaz and Dilek Hakkani-T¨ur. 2011. Discovery of topically coherent sentences for extractive summarization. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Jeff Kubina</author>
<author>Peter A Rankel</author>
<author>Dianne P O’Leary</author>
</authors>
<title>at tac: Guided and multi-lingual summaries and evaluation metrics.</title>
<date>2011</date>
<booktitle>In Proceedings ofthe TAC.</booktitle>
<marker>Conroy, Schlesinger, Kubina, Rankel, O’Leary, 2011</marker>
<rawString>John M. Conroy, Judith D. Schlesinger, Jeff Kubina, Peter A. Rankel, and Dianne P. O’Leary. 2011. Classy 2011 at tac: Guided and multi-lingual summaries and evaluation metrics. In Proceedings ofthe TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Darling</author>
<author>Fei Song</author>
</authors>
<title>Probabilistic document modeling for syntax removal in text summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="27590" citStr="Darling and Song, 2011" startWordPosition="4749" endWordPosition="4752">etter than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates gen</context>
</contexts>
<marker>Darling, Song, 2011</marker>
<rawString>William M. Darling and Fei Song. 2011. Probabilistic document modeling for syntax removal in text summarization. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sashka T Davis</author>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
</authors>
<title>Occams - an optimal combinatorial covering algorithm for multi-document summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of the ICDM.</booktitle>
<contexts>
<context position="29881" citStr="Davis et al., 2012" startWordPosition="5118" endWordPosition="5121">r bigrams by simply looking at the reference summary. We expect our approach also paves an easy way for future automatic abstractive summarization. One previous study that is most related to ours is (Conroy et al., 2011), which utilized a Naive Bayes classifier to predict the probability of a bigram, and applied ILP for the final sentence selection. They used more features than ours, whereas we use a discriminatively trained regression model and a modified ILP framework. Our proposed method performs better than their reported results in TAC 2011 data. Another study closely related to ours is (Davis et al., 2012), which leveraged Latent Semantic Analysis (LSA) to produce term weights and selected summary sentences by computing an approximate solution to the Budgeted Maximal Coverage problem. 6 Conclusion and Future Work In this paper, we leverage the ILP method as a core component in our summarization system. Different from the previous ILP summarization approach, we propose a supervised learning method (a discriminatively trained regression model) to determine the importance of the bigrams fed to the ILP module. In addition, we revise the ILP to maximize the bigram gain (which is expected to be highl</context>
</contexts>
<marker>Davis, Conroy, Schlesinger, 2012</marker>
<rawString>Sashka T. Davis, John M. Conroy, and Judith D. Schlesinger. 2012. Occams - an optimal combinatorial covering algorithm for multi-document summarization. In Proceedings of the ICDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>J. ACM.</journal>
<contexts>
<context position="9872" citStr="Edmundson, 1969" startWordPosition="1703" endWordPosition="1704">r this sentence is the beginning of a paragraph. Xmin eNb,ref log b Xmax P 3 Experiments b i,bi=bexp{w′f(bi)} 3.1 Data eNb,ref log Pj exp{w′f(bj)} (16) This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). We use gradient decent method for parameter estimation, initial w is set with zero. 2.3 Features Each bigram is represented using a set of features in the above regression model. We use two types of features: word level and sentence level features. Some of these features have been used in previous work (Aker and Gaizauskas, 2009; Brandow et al., 1995; Edmundson, 1969; Radev, 2001): We evaluate our method using several recent TAC data sets, from 2008 to 2011. The TAC summarization task is to generate at most 100 words summaries from 10 documents for a given topic query (with a title and more detailed description). For model training, we also included two years’ DUC data (2006 and 2007). When evaluating on one TAC data set, we use the other years of the TAC data plus the two DUC data sets as the training data. 1See next section about the sentence selection step 1006 3.2 Summarization System We use the same system pipeline described in (Gillick et al., 2008;</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>H. P. Edmundson. 1969. New methods in automatic extracting. J. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Int. Res.</journal>
<contexts>
<context position="27176" citStr="Erkan and Radev, 2004" startWordPosition="4692" endWordPosition="4695">on. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. In contrast to these unsupervised approaches, there are also various efforts on supervised le</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Galanis</author>
<author>Gerasimos Lampouras</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Extractive multi-document summarization with integer linear programming and support vector regression.</title>
<date>2012</date>
<booktitle>In Proceedings of the COLING.</booktitle>
<contexts>
<context position="26581" citStr="Galanis et al., 2012" startWordPosition="4603" endWordPosition="4607">ion task. Maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998) uses a greedy algorithm to find summary sentences. (McDonald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Les</context>
</contexts>
<marker>Galanis, Lampouras, Androutsopoulos, 2012</marker>
<rawString>Dimitrios Galanis, Gerasimos Lampouras, and Ion Androutsopoulos. 2012. Extractive multi-document summarization with integer linear programming and support vector regression. In Proceedings of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
</authors>
<title>A skip-chain conditional random field for ranking meeting utterances by importance.</title>
<date>2006</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="28049" citStr="Galley, 2006" startWordPosition="4823" endWordPosition="4824">ts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class data imbalance problem (Xie and Liu, 2010). In this paper, we propose to incorporate the supervised method into the concept-based ILP framework. Unlike previous work using sentencebased supervised learning, we use a regression model to estimate the bigrams and their weights, and use these to guide sentence selection. Compared t</context>
</contexts>
<marker>Galley, 2006</marker>
<rawString>Michel Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programmingfor Natural Langauge Processing on NAACL.</booktitle>
<contexts>
<context position="1740" citStr="Gillick and Favre, 2009" startWordPosition="262" endWordPosition="265">problem: identifying important summary sentences from one or multiple documents. Many methods have been developed for this problem, including supervised approaches that use classifiers to predict summary sentences, graph based approaches to rank the sentences, and recent global optimization methods such as integer linear programming (ILP) and submodular methods. These global optimization methods have been shown to be quite powerful for extractive summarization, because they try to select important sentences and remove redundancy at the same time under the length constraint. Gillick and Favre (Gillick and Favre, 2009) introduced the concept-based ILP for summarization. Their system achieved the best result in the TAC 09 summarization task based on the ROUGE evaluation metric. In this approach the goal is to maximize the sum of the weights of the language concepts that appear in the summary. They used bigrams as such language concepts. The association between the language concepts and sentences serves as the constraints. This ILP method is formally represented as below (see (Gillick and Favre, 2009) for more details): Ei wici (1) s.t. sjOccij &lt; ci (2) Ej sjOccij &gt; ci (3) E j ljsj &lt; L (4) ci E t0, 11 bi (5) </context>
<context position="3010" citStr="Gillick and Favre, 2009" startWordPosition="494" endWordPosition="497">les (shown in (5) and (6)) that indicate the presence of a concept and a sentence respectively. wi is a concept’s weight and Occij means the occurrence of concept i in sentence j. Inequalities (2)(3) associate the sentences and concepts. They ensure that selecting a sentence leads to the selection of all the concepts it contains, and selecting a concept only happens when it is present in at least one of the selected sentences. There are two important components in this concept-based ILP: one is how to select the concepts (ci); the second is how to set up their weights (wi). Gillick and Favre (Gillick and Favre, 2009) used bigrams as concepts, which are selected from a subset of the sentences, and their document frequency as the weight in the objective function. In this paper, we propose to find a candidate summary such that the language concepts (e.g., bigrams) in this candidate summary and the reference summary can have the same frequency. We expect this restriction is more consistent with the max 1004 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1004–1013, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ROUGE evaluation</context>
<context position="26360" citStr="Gillick and Favre, 2009" startWordPosition="4571" endWordPosition="4574">e some prior work on summarization in this section. Unsupervised methods have been widely used. In particular, recently several optimization approaches have demonstrated 1010 competitive performance for extractive summarization task. Maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998) uses a greedy algorithm to find summary sentences. (McDonald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP perf</context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programmingfor Natural Langauge Processing on NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<date>2008</date>
<booktitle>In The ICSI Summarization System at TAC</booktitle>
<marker>Gillick, Favre, Hakkani-T¨ur, 2008</marker>
<rawString>Dan Gillick, Benoit Favre, and Dilek Hakkani-T¨ur. 2008. In The ICSI Summarization System at TAC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Jin</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>A comparative study on ranking and selection strategies for multi-document summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the COLING.</booktitle>
<contexts>
<context position="26840" citStr="Jin et al., 2010" startWordPosition="4643" endWordPosition="4646">the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts a</context>
</contexts>
<marker>Jin, Huang, Zhu, 2010</marker>
<rawString>Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. A comparative study on ranking and selection strategies for multi-document summarization. In Proceedings of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the SIGIR.</booktitle>
<contexts>
<context position="27996" citStr="Kupiec et al., 1995" startWordPosition="4814" endWordPosition="4817">kani-T¨ur, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class data imbalance problem (Xie and Liu, 2010). In this paper, we propose to incorporate the supervised method into the concept-based ILP framework. Unlike previous work using sentencebased supervised learning, we use a regression model to estimate the bigrams and their weights, </context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jure Leskovec</author>
<author>Natasa Milic-Frayling</author>
<author>Marko Grobelnik</author>
</authors>
<title>Impact of linguistic analysis on the semantic graph coverage and learning of document extracts.</title>
<date>2005</date>
<booktitle>In Proceedings of the AAAI.</booktitle>
<contexts>
<context position="27199" citStr="Leskovec et al., 2005" startWordPosition="4696" endWordPosition="4699">12) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarizatio</context>
</contexts>
<marker>Leskovec, Milic-Frayling, Grobelnik, 2005</marker>
<rawString>Jure Leskovec, Natasa Milic-Frayling, and Marko Grobelnik. 2005. Impact of linguistic analysis on the semantic graph coverage and learning of document extracts. In Proceedings of the AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL.</booktitle>
<contexts>
<context position="27126" citStr="Lin and Bilmes, 2010" startWordPosition="4685" endWordPosition="4688">face realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. In contrast to these unsupervised approache</context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Proceedings of the NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="3652" citStr="Lin, 2004" startWordPosition="596" endWordPosition="597"> are selected from a subset of the sentences, and their document frequency as the weight in the objective function. In this paper, we propose to find a candidate summary such that the language concepts (e.g., bigrams) in this candidate summary and the reference summary can have the same frequency. We expect this restriction is more consistent with the max 1004 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1004–1013, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ROUGE evaluation metric used for summarization (Lin, 2004). In addition, in the previous conceptbased ILP method, the constraints are with respect to the appearance of language concepts, hence it cannot distinguish the importance of different language concepts in the reference summary. Our method can decide not only which language concepts to use in ILP, but also the frequency of these language concepts in the candidate summary. To estimate the bigram frequency in the summary, we propose to use a supervised regression model that is discriminatively trained using a variety of features. Our experiments on several TAC summarization data sets demonstrate</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: a package for automatic evaluation of summaries. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the European conference on IR research.</booktitle>
<contexts>
<context position="10488" citStr="McDonald, 2007" startWordPosition="1813" endWordPosition="1814"> Radev, 2001): We evaluate our method using several recent TAC data sets, from 2008 to 2011. The TAC summarization task is to generate at most 100 words summaries from 10 documents for a given topic query (with a title and more detailed description). For model training, we also included two years’ DUC data (2006 and 2007). When evaluating on one TAC data set, we use the other years of the TAC data plus the two DUC data sets as the training data. 1See next section about the sentence selection step 1006 3.2 Summarization System We use the same system pipeline described in (Gillick et al., 2008; McDonald, 2007). The key modules in the ICSI ILP system (Gillick et al., 2008) are briefly described below. • Step 1: Clean documents, split text into sentences. • Step 2: Extract bigrams from all the sentences, then select those bigrams with document frequency equal to more than 3. We call this subset as initial bigram set in the following. • Step 3: Select relevant sentences that contain at least one bigram from the initial bigram set. • Step 4: Feed the ILP with sentences and the bigram set to get the result. • Step 5: Order sentences identified by ILP as the final result of summary. The difference betwee</context>
<context position="26102" citStr="McDonald, 2007" startWordPosition="4531" endWordPosition="4533">. From this we can see that it is not just the supervised methods or using annotated data that yields the overall improved system performance, but rather our proposed regression setup for bigrams is the main reason. 5 Related Work We briefly describe some prior work on summarization in this section. Unsupervised methods have been widely used. In particular, recently several optimization approaches have demonstrated 1010 competitive performance for extractive summarization task. Maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998) uses a greedy algorithm to find summary sentences. (McDonald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of the European conference on IR research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into text.</title>
<date>2004</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="27226" citStr="Mihalcea and Tarau, 2004" startWordPosition="4700" endWordPosition="4703"> maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained </context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
</authors>
<title>Using maximum entropy for sentence extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Automatic Summarization.</booktitle>
<contexts>
<context position="28029" citStr="Osborne, 2002" startWordPosition="4820" endWordPosition="4821">idden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class data imbalance problem (Xie and Liu, 2010). In this paper, we propose to incorporate the supervised method into the concept-based ILP framework. Unlike previous work using sentencebased supervised learning, we use a regression model to estimate the bigrams and their weights, and use these to guide sentence s</context>
</contexts>
<marker>Osborne, 2002</marker>
<rawString>Miles Osborne. 2002. Using maximum entropy for sentence extraction. In Proceedings of the ACL-02 Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
</authors>
<title>Experiments in single and multidocument summarization using mead. In</title>
<date>2001</date>
<booktitle>In First Document Understanding Conference.</booktitle>
<contexts>
<context position="9886" citStr="Radev, 2001" startWordPosition="1705" endWordPosition="1706">s the beginning of a paragraph. Xmin eNb,ref log b Xmax P 3 Experiments b i,bi=bexp{w′f(bi)} 3.1 Data eNb,ref log Pj exp{w′f(bj)} (16) This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). We use gradient decent method for parameter estimation, initial w is set with zero. 2.3 Features Each bigram is represented using a set of features in the above regression model. We use two types of features: word level and sentence level features. Some of these features have been used in previous work (Aker and Gaizauskas, 2009; Brandow et al., 1995; Edmundson, 1969; Radev, 2001): We evaluate our method using several recent TAC data sets, from 2008 to 2011. The TAC summarization task is to generate at most 100 words summaries from 10 documents for a given topic query (with a title and more detailed description). For model training, we also included two years’ DUC data (2006 and 2007). When evaluating on one TAC data set, we use the other years of the TAC data plus the two DUC data sets as the training data. 1See next section about the sentence selection step 1006 3.2 Summarization System We use the same system pipeline described in (Gillick et al., 2008; McDonald, 200</context>
</contexts>
<marker>Radev, 2001</marker>
<rawString>Dragomir R. Radev. 2001. Experiments in single and multidocument summarization using mead. In In First Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seonggi Ryang</author>
<author>Takeshi Abekawa</author>
</authors>
<title>Framework of automatic text summarization using reinforcement learning.</title>
<date>2012</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="28112" citStr="Ryang and Abekawa, 2012" startWordPosition="4830" endWordPosition="4833">en these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class data imbalance problem (Xie and Liu, 2010). In this paper, we propose to incorporate the supervised method into the concept-based ILP framework. Unlike previous work using sentencebased supervised learning, we use a regression model to estimate the bigrams and their weights, and use these to guide sentence selection. Compared to the direct sentence-based classification or regression method</context>
</contexts>
<marker>Ryang, Abekawa, 2012</marker>
<rawString>Seonggi Ryang and Takeshi Abekawa. 2012. Framework of automatic text summarization using reinforcement learning. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="9501" citStr="Smith and Eisner, 2005" startWordPosition="1638" endWordPosition="1641">at include this bigram, divided by the total number of the selected sentences. – 7. Sentence similarity: Sentence similarity with topic’s query, which is the concatenation of topic title and description. – 8. Sentence position: Sentence position in the document. – 9. Sentence length: The number of words in the sentence. – 10. Paragraph starter: Binary feature indicating whether this sentence is the beginning of a paragraph. Xmin eNb,ref log b Xmax P 3 Experiments b i,bi=bexp{w′f(bi)} 3.1 Data eNb,ref log Pj exp{w′f(bj)} (16) This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). We use gradient decent method for parameter estimation, initial w is set with zero. 2.3 Features Each bigram is represented using a set of features in the above regression model. We use two types of features: word level and sentence level features. Some of these features have been used in previous work (Aker and Gaizauskas, 2009; Brandow et al., 1995; Edmundson, 1969; Radev, 2001): We evaluate our method using several recent TAC data sets, from 2008 to 2011. The TAC summarization task is to generate at most 100 words summaries from 10 documents for a given topic query (with a title and more </context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: training log-linear models on unlabeled data. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Multiple aspect summarization using integer linear programming.</title>
<date>2012</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="26421" citStr="Woodsend and Lapata, 2012" startWordPosition="4580" endWordPosition="4583">vised methods have been widely used. In particular, recently several optimization approaches have demonstrated 1010 competitive performance for extractive summarization task. Maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998) uses a greedy algorithm to find summary sentences. (McDonald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in s</context>
</contexts>
<marker>Woodsend, Lapata, 2012</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Xie</author>
<author>Yang Liu</author>
</authors>
<title>Improving supervised learning for meeting summarization using sampling and regression.</title>
<date>2010</date>
<journal>Comput. Speech Lang.</journal>
<contexts>
<context position="28362" citStr="Xie and Liu, 2010" startWordPosition="4870" endWordPosition="4873">various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class data imbalance problem (Xie and Liu, 2010). In this paper, we propose to incorporate the supervised method into the concept-based ILP framework. Unlike previous work using sentencebased supervised learning, we use a regression model to estimate the bigrams and their weights, and use these to guide sentence selection. Compared to the direct sentence-based classification or regression methods mentioned above, our method has an advantage. When abstractive summaries are given, one needs to use that information to automatically generate reference labels (a sentence is in the summary or not) for extractive summarization. Most researchers ha</context>
</contexts>
<marker>Xie, Liu, 2010</marker>
<rawString>Shasha Xie and Yang Liu. 2010. Improving supervised learning for meeting summarization using sampling and regression. Comput. Speech Lang.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>