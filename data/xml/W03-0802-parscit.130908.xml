<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000084">
<title confidence="0.995669">
WHAT: An XSLT-based Infrastructure for the Integration of Natural
Language Processing Components
</title>
<author confidence="0.976212">
Ulrich Schäfer
</author>
<affiliation confidence="0.958403">
Language Technology Lab, German Research Center for Artificial Intelligence (DFKI)
</affiliation>
<address confidence="0.916355">
Stuhlsatzenhausweg 3, D-66123 Saarbrücken, Germany
</address>
<email confidence="0.989972">
Ulrich.Schaefer@dfki.de
</email>
<sectionHeader confidence="0.993627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999868">
The idea of the Whiteboard project is to integrate
deep and shallow natural language processing
components in order to benefit from their synergy.
The project came up with the first fully integrated
hybrid system consisting of a fast HPSG parser that
utilizes tokenization, PoS, morphology, lexical,
named entity, phrase chunk and (for German)
topological sentence field analyses from shallow
components. This integration increases robustness,
directs the search space and hence reduces
processing time of the deep parser. In this paper, we
focus on one of the central integration facilities, the
XSLT-based Whiteboard Annotation Transformer
(WHAT), report on the benefits of XSLT-based
NLP component integration, and present examples
of XSL transformation of shallow and deep
annotations used in the integrated architecture. The
infrastructure is open, portable and well suited for,
but not restricted to the development of hybrid NLP
architectures as well as NLP applications.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998601242424242">
During the last decade, SGML and XML have become
an important interchange format for linguistic data, be
they created manually by linguists, or automatically by
natural language processing (NLP) components.
LT-XML (Brew et al. 2000), XCES (Ide and Romary
2001) and many other are examples for XML-based or
XML-supporting software architectures for natural
language processing.
The main focus of the Whiteboard project (2000-2002)
was to integrate shallow and deep natural language
processing components. The idea was to combine both
in order to benefit from their advantages. Successful and
beneficial integration included tokenization, PoS,
morphology, lexical, named entity, phrase chunk and
(for German) topological sentence field levels in a fully
automated XML-based system. Crysmann et al. (2002)
and Frank et al. (2003) show that this close deep-
shallow combination significantly increases robustness
and performance compared to the (already fast)
standalone deep HPSG parser by Callmeier (2000). The
only comparable architecture so far was described by
Grover et al. (2002), but their integration was limited to
tokenization and PoS tagging (the shallow chunker did
not guide or contribute to deep analysis).
In this paper, we will focus on one of the central
integration facilities, the XSLT-based Whiteboard
Annotation Transformer (WHAT), report on the
benefits of XSLT-based NLP component integration,
and present examples of XSL transformation of shallow
and deep annotations used in the integrated architecture.
Because the infrastructure is in general independent of
deep or shallow paradigms, it can also be applied to
purely shallow or deep systems.
</bodyText>
<sectionHeader confidence="0.955736" genericHeader="introduction">
2 Whiteboard: Deep-Shallow Integration
</sectionHeader>
<bodyText confidence="0.999775620689655">
Deep processing (DNLP) systems1 try to apply as much
linguistic knowledge as possible during the analysis of
sentences and result in a uniformly represented
collection of the knowledge that contributed to the
analysis. The result often consists of many possible
analyses per sentence reflecting the uncertainty which
of the possible readings was intended – or no answer at
all if the linguistic knowledge was contradictory or
insufficient with respect to the input sentence.
Shallow processing (SNLP) systems do not attempt to
achieve such an exhaustive linguistic analysis. They are
desigend for specific tasks ignoring many details in
input and linguistic framework. Utilizing rule-based
(e.g., finite-state) or statistics-based approaches, they
are in general much faster than DNLP. Due to the lack
of efficiency and robustness of DNLP systems, the trend
in application-oriented language processing system
development in the last years was to improve SNLP
systems. They are now capable of analyzing Megabytes
of texts within seconds, but precision and quality
barriers are so obvious (especially on domains the
systems where not designed for or trained on) that a
need for &apos;deeper&apos; systems re-emerged. Moreover,
1 In this paper, &apos;deep&apos; is nearly synonymous to typed
unification-based grammar formalisms, e.g. HPSG
(Pollard and Sag 1994), although the infrastructure may
also apply to other deep linguistic frameworks.
semantics construction from an input sentence is quite
poor and erroneous in typical shallow systems.
But also development of DNLP made advances during
the last few years, especially in the field of efficiency
(Callmeier 2000).
A promising solution to improve quality of natural
language processing is the combination of deep and
shallow technologies. Deep processing benefits from
specialized and fast shallow analysis results, shallow
processing becomes &apos;deeper&apos; using at least partial results
from DNLP.
Many natural language processing applications could
benefit from the synergy of the combination of deep and
shallow, e.g. advanced information extraction, question
answering, or grammar checking systems.
The Whiteboard architecture aims at integrating
different language technology components. Both online
and offline coupling of existing software modules is
supported, i.e., the architecture provides direct access to
standoff XML annotation as well as programming
interfaces. Applications communicate with the
components through programming interfaces. A multi-
layer chart holds the linguistic processing results in the
online system memory while XML annotations can be
accessed online as well as offline. Figure 1 gives an
overview of the general architecture called WHAM
(WHiteboard Annotation Machine).
There are two main points of the architecture that are
important to stress. First, the different paradigms of
DNLP and SNLP are preserved throughout the
architecture, e.g. there is a shallow and a deep
</bodyText>
<sectionHeader confidence="0.752103" genericHeader="method">
WHAM
</sectionHeader>
<figureCaption confidence="0.997">
Figure 1: Whiteboard Architecture: WHAM
</figureCaption>
<bodyText confidence="0.997827846153846">
programming interface.
The second point is that the WHAM offers
programming interfaces which are not simply DOM
interfaces isomorphic to the XML markup they are
based on, but hierarchically defined classes. E.g., a fast
index-sequential storage and retrieval mechanism based
on XML is encapsulated through the shallow
programming interface. However, while the typed
feature structure-based programming interface to deep
components is stable, it turned out that the XML-based
interface was too inflexible when new, mainly shallow,
components with new DTDs had to be integrated.
Therefore, a more flexible approach had to be devised.
</bodyText>
<sectionHeader confidence="0.943135" genericHeader="method">
3 WHAT: The Whiteboard Annotation
Transformer
</sectionHeader>
<bodyText confidence="0.999914730769231">
The main motivation for developing an XSLT-based
infrastructure for NLP components was to provide
flexible access to standoff XML annotations produced
by the components.
XSLT (Clark 1999) is a W3C standard language for the
transformation of XML documents. Input of an XSL
transformation must be XML, while output can be any
syntax (e.g., XML, text, HTML, RTF, or even
programming language source code, etc.). The power of
XSLT mainly comes from its sublanguage XPath (Clark
and DeRose 1999), which supports access to XML
structure, elements, attributes and text through concise
path expressions. An XSL stylesheet consists of
templates with XPath expressions that must match the
input document in order to be executed. The order in
which templates are called is by default top-down, left
to right, but can be modified, augmented, or suppressed
through loops, conditionals, and recursive call of
(named) templates.
WHAT, the WHiteboard Annotation Transformer, is
built on top of a standard XSL transformation engine. It
provides uniform access to standoff annotation through
queries that can either be used from non-XML aware
components to get access to information stored in the
annotation (V and N queries), or to transform (modify,
enrich, merge) XML annotation documents (D queries).
</bodyText>
<sectionHeader confidence="0.593118" genericHeader="method">
result
</sectionHeader>
<figureCaption confidence="0.990548">
Figure 2: WHAT Architecture
</figureCaption>
<bodyText confidence="0.996983">
While the WHAT is written in a programming language
such as Java or C, the XSL query templates that are
specific for a standoff DTD of a component&apos;s XML
output are independent of that programming language,
i.e., they must only be written once for a new
</bodyText>
<figure confidence="0.941110655172414">
shallow
NLP
components
deep NLP
components
multilayer
chart
XML
standoff
annotation
program-
ming
interface
WHAT
NLP-based
application
WHAT
query
component-
specific XSLT
template library
constructed
XSLT
stylesheet
XML
standoff
markup
XSLT
processor
</figure>
<bodyText confidence="0.7201205">
component and are collected in a so-called template
library.
</bodyText>
<subsectionHeader confidence="0.996822">
3.1 WHAT Queries
</subsectionHeader>
<bodyText confidence="0.996619">
Based on an input XML document (or DOM object), a
WHAT query that consists of
</bodyText>
<listItem confidence="0.98436675">
• component name,
• query name, and
• query-specific parameters such as an index or
identifier
</listItem>
<bodyText confidence="0.994574">
is looked up in the XSLT template library for the
specified component, an XSLT stylesheet is returned
and applied to the XML document by the XSLT
processor. The result of stylesheet application is then
returned as the answer to the WHAT query. There are
basically three kinds of results:
</bodyText>
<listItem confidence="0.9713916">
• strings (including non-XML output, e.g. RTF or
even programming language source code)
• lists of unique identifiers denoting references to
nodes in the XML input document
• XML documents
In other words, if we formulate queries as functions, we
get the following query signatures:
• getValue: C x D x P* → S*
• getNodes: C x D x P* → N*
• getDocument: C x D x P* → D
</listItem>
<bodyText confidence="0.99993875">
where C is the component, D an XML document, P* a
(possibly empty) sequence of parameters, S* a sequence
of strings, and N* a sequence of node identifiers.
We now give examples for each of the query types.
</bodyText>
<subsectionHeader confidence="0.572158">
3.1.1 V-queries (getValue)
</subsectionHeader>
<bodyText confidence="0.9827370625">
V-queries return string values from XML attribute
values or text. The simplest case is a single XPath
lookup. As an example, we determine the type of named
entity 23 in a shallow XML annotation produced by the
SPPC system (Piskorski and Neumann 2000).
The WHAT query
getValue(&amp;quot;NE.type&amp;quot;, &amp;quot;de.dfki.lt.sppc&amp;quot;, 23)
would lead to the lookup of the following query in the
XSLT template library for SPPC
&lt;query name=&amp;quot;getValue.NE.type&amp;quot; component=&amp;quot;de.dfki.lt.sppc&amp;quot;&gt;
&lt;!-- returns the type of named entity as number --&gt;
&lt;xsl:param name=&amp;quot;index&amp;quot;/&gt;
&lt;xsl:template match=&amp;quot;/WHITEBOARD/SPPC_XML//NE[@id=$index]&amp;quot;&gt;
&lt;xsl:value-of select=&amp;quot;@type&amp;quot;/&gt;
&lt;/xsl:template&gt;
&lt;/query&gt;
On appropriate SPPC XML annotation, containing the
named entity tag e.g. &lt;NE id=&amp;quot;23&amp;quot;
type=&amp;quot;location&amp;quot;...&gt; somewhere below the root tag,
this query would return the String &amp;quot;location&amp;quot;.
By adding a subsequent lookup to a translation table
(through XML entity definitions or as part of the input
document or of the component-specific template
library), it would also be possible to translate namings,
e.g. in order to map SPPC-annotation-specific namings
to HPSG type names.
We see from this example how the WHAT helps to
abstract from component-specific DTD structure and
namings. However, queries need not be that simple.
Complex computations can be performed and the return
value can also be numbers, e.g., for queries that count
elements, words, etc.
</bodyText>
<subsectionHeader confidence="0.558763">
3.1.2 N-queries (getNodes)
</subsectionHeader>
<bodyText confidence="0.9646545">
An important feature of WHAT is navigation in the
annotation. N-queries compute and return lists of node
identifiers that can again be used as parameters for
subsequent (e.g. V-)queries.
The sample query returns the node identifiers of all
named entities (NE tags) that are in the given range of
tokens (W tags). The template calls a recursive auxiliary
template that seeks the next named entity until the end
of the range is reached. The WHAT query
getNodes(&amp;quot;W.NEinRange&amp;quot;, &amp;quot;de.dfki.lt.sppc&amp;quot;,3,19)
would lead to the lookup of the following query in the
XSLT template library for SPPC.
</bodyText>
<figure confidence="0.988822875">
&lt;query name=&amp;quot;getNodes.W.NEinRange&amp;quot; compon.=&amp;quot;de.dfki.lt.sppc&amp;quot;&gt;
&lt;!-- returns NE nodes starting exactly at token $index to
(at most) token $index2 --&gt;
&lt;xsl:param name=&amp;quot;index&amp;quot;/&gt; &lt;xsl:param name=&amp;quot;index2&amp;quot;/&gt;
&lt;xsl:template match=&amp;quot;/&amp;quot;&gt;
&lt;xsl:variable name=&amp;quot;startX&amp;quot;
select=&amp;quot;/WHITEBOARD/SPPC_XML//W[@id=$index]/ancestor::NE&amp;quot;/&gt;
&lt;xsl:if test=&amp;quot;$startX//W[1]/@id = $index&amp;quot;&gt;
&lt;xsl:call-template name=&amp;quot;checknextX&amp;quot;&gt;
&lt;xsl:with-param name=&amp;quot;nextX&amp;quot; select=&amp;quot;$startX&amp;quot;/&gt;
&lt;xsl:with-param name=&amp;quot;lastW&amp;quot; select=&amp;quot;$index2&amp;quot;/&gt;
&lt;/xsl:call-template&gt;
&lt;/xsl:if&gt;
&lt;/xsl:template&gt;
&lt;xsl:template name=&amp;quot;checknextX&amp;quot;&gt;
&lt;!-- auxiliary template (recursive) --&gt;
&lt;xsl:param name=&amp;quot;nextX&amp;quot;/&gt;
&lt;xsl:param name=&amp;quot;lastW&amp;quot;/&gt;
&lt;xsl:variablename=&amp;quot;Xtokens&amp;quot; select=&amp;quot;$nextX//W&amp;quot;/&gt;
&lt;xsl:iftest=&amp;quot;number(substring($Xtokens[last()]/@id, 2))
&lt;= number(substring($lastW, 2))&amp;quot;&gt;
&lt;xsl:value-of select=&amp;quot;$nextX/@id&amp;quot;/&gt;
&lt;xsl:text&gt; &lt;/xsl:text&gt;
&lt;xsl:call-template name=&amp;quot;checknextX&amp;quot;&gt;
&lt;xsl:with-param name=&amp;quot;nextX&amp;quot;
select=&amp;quot;/WHITEBOARD/SPPC_XML//NE[@id=concat(&apos;N&apos;, string(1 +
number(substring($nextX/@id,2))))]&amp;quot;/&gt;
&lt;xsl:with-param name=&amp;quot;lastW&amp;quot; select=&amp;quot;$lastW&amp;quot;/&gt;
&lt;/xsl:call-template&gt;
&lt;/xsl:if&gt;
&lt;/xsl:template&gt;
&lt;/query&gt;
</figure>
<bodyText confidence="0.998966166666667">
Again, the query forms an abstraction from DTD
structure. E.g., in SPPC XML output, named entity
elements enclose token elements. This need not be the
case for another shallow component; its template would
be defined differently, but the query call syntax would
be the same.
</bodyText>
<sectionHeader confidence="0.424301" genericHeader="method">
3.1.3 D-queries (getDocument)
</sectionHeader>
<bodyText confidence="0.944588">
D-queries return transformed XML documents - this is
the classical, general use of XSLT. Complex
transformations that modify, enrich or produce
(standoff) annotation can be used for many purposes.
Examples are
</bodyText>
<listItem confidence="0.97485185">
• conversion from a different XML format
• merging of several XML documents into one
• auxiliary document modifications, e.g. to add
unique identifiers to elements, sort elements etc.
• providing interface to NLP applications (up to
code generation for a programming language
compiler...)
• visualization and formatting (Thistle, HTML,
PDF, ...)
• perhaps the most important is to do (linguistic)
computation and transformation in order to turn a
WHAT query into a kind of NLP component
itself. This is e.g. intensively used in the shallow
topological field parser integration we describe
below. Multiple queries are applied in a sequence
to transform a topological field tree into a list of
constraints over syntactic spans that are used for
initialization of the deep parser&apos;s chart. One of
these WHAT queries has more than 900 lines of
XSLT code.
</listItem>
<bodyText confidence="0.993955333333333">
We can show only a short example here, a query that
inserts unique identifier attributes into an arbitrary XML
document without id attributes.
</bodyText>
<figure confidence="0.988077785714286">
&lt;query name=&amp;quot;getDocument.generateIDs&amp;quot;&gt;
&lt;!-- generate unique id for each element --&gt;
&lt;xsl:template match=&amp;quot;*&amp;quot;&gt;
&lt;xsl:copy select=&amp;quot;.&amp;quot;&gt;
&lt;xsl:attribute name=&amp;quot;id&amp;quot;&gt;
&lt;xsl:value-of select=&amp;quot;generate-id()&amp;quot;/&gt;
&lt;/xsl:attribute&gt;
&lt;xsl:for-each select=&amp;quot;@*&amp;quot;&gt;
&lt;xsl:copy-of select=&amp;quot;.&amp;quot;/&gt;
&lt;/xsl:for-each&gt;
&lt;xsl:apply-templates/&gt;
&lt;/xsl:copy&gt;
/xsl:template&gt;
&lt;/query&gt;
</figure>
<bodyText confidence="0.994573909090909">
Note that this is an example for a stylesheet that is
completely independent of a DTD, it just works on any
XML document – and thus shows how generic XSL
transformation rules can be.
Another example is transformation of XML tree
representations into Thistle trees (arbora DTD; see
Calder 2000). While the output DTD is fixed, this is
again not true for the input document which can contain
arbitrary element names and branches. Thistle
visualizations generated through WHAT are shown in
Fig. 4, 5 and 6 below.
</bodyText>
<subsectionHeader confidence="0.99985">
3.2 Components of the Hybrid System
</subsectionHeader>
<bodyText confidence="0.8626791">
The WHAT has been successfully used in the
Whiteboard architecture for online analysis of German
newspaper sentences. For more details on motivation
and evaluation cf. Frank et al. (2003) and Becker and
Frank (2002). The simplified diagram in Figure 3
depicts the components and places where WHAT comes
into play in the hybrid integration of deep and shallow
processing components (V, N, D denote the WHAT
query types). The system takes an input sentence, and
runs three shallow systems on it:
</bodyText>
<listItem confidence="0.9939361">
• the rule-based shallow SPPC (Piskorski and
Neumann 2000) for named entity recognition,
• TnT/Chunkie, a statistics-based shallow PoS
tagger and chunker by (Skut and Brants 1998),
• LoPar, a probabilistic context-free parser (Schmid
2000), which takes PoS-tagged tokens as input,
and produces binary tree representations of
sentence fields, e.g., topo.bin in Fig. 4. For a
justification for binary vs. flat trees cf. Becker and
Frank (2002).
</listItem>
<bodyText confidence="0.997432214285714">
The results of the components are three standoff
annotations of the input sentence. Then, a sequence of
D-queries is applied to flatten the binary topological
field trees (result is topo.flat, Fig. 5), merge with
shallow chunk information from Chunkie (topo.chunks,
Fig. 6), and apply the main D-query computing bracket
information for the deep parser from the merged topo
tree (topo.brackets, Fig. 7).
Finally, the deep parser PET (Callmeier 2000), modified
as described in Frank et al. (2003), is started with a
chart initialized using the shallow bracket information
(topo.brackets) through WHAT V and N queries. PET
also accesses lexical and named entity information from
SPPC through V queries.
</bodyText>
<figure confidence="0.572455">
input sentence
</figure>
<figureCaption confidence="0.999018">
Figure 3: WHAT in the hybrid parser
</figureCaption>
<figure confidence="0.9980998">
SPPC TnT
WHAT-based application
D,V,N
topo.bin
topo.flat
LoPar
D
D
PET
V, N
topo.brackets
topo.chunks
D
D
Chunkie
</figure>
<bodyText confidence="0.998242375">
Again, WHAT abstraction facilitates exchange of the
shallow input components of PET without needing to
rewrite the parser&apos;s code.
The dashed lines in Figure 3 indicate that a WHAT-
based application can have access to the standoff
annotation through D, V or N queries.
The Thistle diagrams below are created via D queries
out of the intermediate topo.* trees.
</bodyText>
<figureCaption confidence="0.969898">
Figure 4. A binary tree as result of the topoparser
(topo.bin).
Figure 5. The same tree after flattening (topo.flat).
Figure 6. The topo tree merged with chunks
(topo.chunks).
</figureCaption>
<equation confidence="0.971972083333334">
&lt;TOPO2HPSG type=&amp;quot;root&amp;quot; id=&amp;quot;5608&amp;quot;&gt;
&lt;MAP_CONSTR id=&amp;quot;T1&amp;quot; constr=&amp;quot;v2_cp&amp;quot; left=&amp;quot;W1&amp;quot; right=&amp;quot;W13&amp;quot;/&gt;
&lt;MAP_CONSTR id=&amp;quot;T2&amp;quot; constr=&amp;quot;v2_vf&amp;quot; left=&amp;quot;W1&amp;quot; right=&amp;quot;W2&amp;quot;/&gt;
&lt;MAP_CONSTR id=&amp;quot;T3&amp;quot; constr=&amp;quot;vfronted_vfin+rk&amp;quot; left=&amp;quot;W3&amp;quot; right=&amp;quot;W3&amp;quot;/&gt;
&lt;MAP_CONSTR id=&amp;quot;T4&amp;quot; constr=&amp;quot;vfronted_vfin+vp+rk&amp;quot; left=&amp;quot;W3&amp;quot; right=&amp;quot;W13&amp;quot;/&gt;
&lt;MAP_CONSTR id=&amp;quot;T5&amp;quot; constr=&amp;quot;vfronted_vp+rk&amp;quot; left=&amp;quot;W4&amp;quot; right=&amp;quot;W13&amp;quot;/&gt;
&lt;MAP_CONSTR id=&amp;quot;T6&amp;quot; constr=&amp;quot;vfronted_rk-complex&amp;quot; left=&amp;quot;W7&amp;quot; right=&amp;quot;W7&amp;quot;/&gt;
&lt;MAP_CONSTR id=&amp;quot;T7&amp;quot; constr=&amp;quot;vl_cpfin_compl&amp;quot; left=&amp;quot;W9&amp;quot; right=&amp;quot;W13&amp;quot;/&gt;
&lt;MAP_CONSTR id=&amp;quot;T8&amp;quot; constr=&amp;quot;vl_compl_vp&amp;quot; left=&amp;quot;W10&amp;quot; right=&amp;quot;W13&amp;quot;/&gt;
&lt;MAP_CONSTR id=&amp;quot;T9&amp;quot; constr=&amp;quot;vl_rk_fin+complex+f&amp;quot; left=&amp;quot;W12&amp;quot; right=&amp;quot;W13&amp;quot;/&gt;
&lt;MAP_CONSTR id=&amp;quot;T10&amp;quot; constr=&amp;quot;extrapos_rk+nf&amp;quot; left=&amp;quot;W7&amp;quot; right=&amp;quot;W13&amp;quot;/&gt;
&lt;/TOPO2HPSG&gt;
</equation>
<figureCaption confidence="0.996852">
Figure 7. The extracted brackets (topo.brackets)
</figureCaption>
<subsectionHeader confidence="0.9874985">
3.3 Accessing and Transforming Deep
Annotation
</subsectionHeader>
<bodyText confidence="0.999969833333333">
In the sections so far, we showed examples for shallow
XML annotation. But annotation access should not stop
before deep analysis results. In this section, we turn to
deep XML annotation. Typed feature structures provide
a powerful, universal representation for deep linguistic
knowledge.
While it is in general inefficient to use XML markup to
represent typed feature structures during processing
(e.g. for unification, subsumption operations), there are
several applications that may benefit from a
standardized system-independent XML markup of typed
feature structures, e.g., as exchange format for
</bodyText>
<listItem confidence="0.999915">
• deep NLP component results (e.g. parser chart)
• grammar definitions
• feature structure visualization or editing tools
• feature structure &apos;tree banks&apos; of analysed texts
</listItem>
<bodyText confidence="0.9990705">
Sailer and Richter (2001) propose an XML markup
where the recursive embedding of attribute-value pairs
is decomposed into a kind of definite equivalences or
non-recursive node lists (triples of node ID, type name
and embedded lists of attribute-node pairs). The only
advantage we see for this kind of representation is its
proximity to a particular kind of feature structure
implementation.
We adopt an SGML markup for typed feature
structures originally developed by the Text Encoding
Initiative (TEI) which is very compact and seems to be
widely accepted, e.g. also in the Tree Adjoining
Grammar community (Issac 1998). Langendoen and
Simons (1995) give an in-depth justification for the
naming and structure of a feature structure DTD. We
will focus here on the feature structure DTD subset that
is able to encode the basic data structures of deep
systems such as LKB (Copestake 1999), PET
(Callmeier 2000), PAGE, or the shallow system
SProUT (Becker et al. 2002) which have a subset of
TDL (Krieger and Schäfer 1994) as their common basic
formalism2:
</bodyText>
<table confidence="0.661667714285714">
&lt;?xml version=&amp;quot;1.0&amp;quot; ?&gt;
&lt;!-- minimal typed feature structure DTD --&gt;
&lt;!ELEMENT FS ( F* ) &gt;
&lt;!ATTLIST FS type NMTOKEN #IMPLIED
coref NMTOKEN #IMPLIED &gt;
&lt;!ELEMENT F ( FS ) &gt;
&lt;!ATTLIST F name NMTOKEN #REQUIRED &gt;
</table>
<bodyText confidence="0.995135833333333">
The FS tag encodes typed Feature Structure nodes, F
encodes Features. Atoms are encoded as typed Feature
structure nodes with empty feature list. An important
2 Encoding of type hierarchies or other possibly system
or formalism-specific definitions are of course not
covered by this minimal DTD.
point is the encoding of coreferences (reentrancies)
between feature structure nodes which denote structure
sharing. For the sake of symmetry in the
representation/DTD, we do not declare the coref
attribute as ID/IDREF, but as NMTOKEN.
An application of WHAT access or transformation of
deep annotation would be to specifiy a feature path
under which a value (type, atom, or complex FS) is to
be returned. The problem here are the coreferences
which must be dereferenced at every feature level of the
path. A general solution is to recursively dereference all
nodes in the path.
We give only a limited example here, a query to access
output of the SProUT system. It returns the value (type)
of a feature somewhere under the specified attribute in a
disjunction of typed feature structures, assuming that we
are not interested here in structure sharing between
complex values.
</bodyText>
<figure confidence="0.990069310344828">
&lt;query name=&amp;quot;getValue.fs.attr&amp;quot; component=&amp;quot;de.dfki.lt.sprout&amp;quot;&gt;
&lt;xsl:param name=&amp;quot;disj&amp;quot;/&gt;
&lt;xsl:param name=&amp;quot;attr&amp;quot;/&gt;
&lt;xsl:template match=&apos;DISJ[$disj]&apos;&gt;
&lt;xsl:variable name=&amp;quot;node&amp;quot; select=&apos;.//F[@name=$attr]/FS&apos;/&gt;
&lt;xsl:choose&gt;
&lt;xsl:when test=&amp;quot;$node/@type&amp;quot;&gt;
&lt;xsl:value-of select=&amp;quot;$node/@type&amp;quot;/&gt;
&lt;/xsl:when&gt;
&lt;xsl:otherwise&gt;
&lt;xsl:if test=&amp;quot;$node/@coref&amp;quot;&gt;
&lt;xsl:call-template name=&amp;quot;deref&amp;quot;&gt;
&lt;xsl:with-param name=&amp;quot;coref&amp;quot;
select=&amp;quot;$node/@coref&amp;quot;/&gt;
&lt;/xsl:call-template&gt;
&lt;/xsl:if&gt;
&lt;/xsl:otherwise&gt;
&lt;/xsl:choose&gt;
&lt;xsl:apply-templates/&gt;
&lt;/xsl:template&gt;
&lt;xsl:template name=&amp;quot;deref&amp;quot;&gt;
&lt;xsl:param name=&amp;quot;coref&amp;quot;/&gt;
&lt;xsl:for-each select=&amp;quot;.//FS[@coref=$coref]&amp;quot;&gt;
&lt;xsl:if test=&apos;@type&apos;&gt;
&lt;xsl:value-of select=&amp;quot;@type&amp;quot;/&gt;
&lt;/xsl:if&gt;
&lt;/xsl:for-each&gt;
&lt;/xsl:template&gt;
&lt;/query&gt;
</figure>
<bodyText confidence="0.999655333333333">
To complete the picture of abstraction through WHAT
queries, we can imagine that the same types of query are
possible to access e.g. the same morphology
information in both shallow and in deep annotation,
although their representation within the annotation
might be totally different.
</bodyText>
<subsectionHeader confidence="0.997277">
3.4 Efficiency of XSLT Processors
</subsectionHeader>
<bodyText confidence="0.99328006060606">
Processing speed of current XSLT engines on large
input documents is a problem. Many XSLT
implementations lack efficiency (for an overview cf.
xmlbench.sourceforge.net). Although optimization is
possible (e.g. through DTD specification, indexing etc.),
this is not done seriously in many implementations.
However, there are several WHAT-specific solutions
that can help making queries faster. A pragmatic one is
pre-editing of large annotation files. An HPSG parser
e.g. focuses on one sentence at a time and does not
exceed the sentence boundaries (which can be
determined reliably by shallow methods) so that it
suffices to split shallow XML input into per-sentence
annotations in order to reduce processing time to a
reasonable amount.
Another solution could be packing several independent
queries into a &apos;prepared statement&apos; in one stylesheet.
However, as processing speed is mainly determined by
the size of the input document, this does not speed up
processing time substantially.
WHAT implementations are free to be based on DOM
trees or plain XML text input (strings or streams). DOM
tree representations are used by XSLT implementations
such als libxml/libslt for C/Perl/Python/TCL or Xalan
for Java. Hence, DOM implementations of WHAT are
preferable in order to avoid unnecessary XML parsing
when processing multiple WHAT transformations on
the same input and thus help to improve processing
speed.
As in all programming language, there a multiple
solutions for a problem. An XSL profiling tool (e.g.
xsltprofiler.org) can help to locate inefficient XSLT
code.
</bodyText>
<sectionHeader confidence="0.83625" genericHeader="related work">
3.5 Related Work
</sectionHeader>
<bodyText confidence="0.9999355">
As argued in Thompson and McKelvie (1997), standoff
annotation is a viable solution in order to cope with the
combination of multiple overlapping hierarchies and the
efficiency problem of XML tree modification for large
annotations.
Ide (2000) gives an overview of NLP-related XML core
technologies that also strives XSLT.
We adopt the pragmatic view of Carletta et al. (2002),
who see that computational linguistics greatly benefits
from general XMLification, namely by getting for free
standards and advanced technologies for storing and
manipulating XML annotation, mainly through W3C
and various open source projects. The trade-off for this
benefit is a representation language somewhat limited
with respect to linguistic expressivity.
NiteQL (Evert and Voormann 2002) can be seen as an
extension to XPath within XSLT, has a more concise
syntax especially for document structure-related
expressions and a focus on timeline support with
specialized queries (for speech annotation). The query
language in general does not add expressive power to
XSLT and the implementation currently only supports
Java XSLT engines.
Because of unstable standardization and implementation
status, we did not yet make use of XQuery (Boag et al.
2002). XQuery is a powerful, SQL-like query language
on XML documents where XPath is a subset rather than
a sublanguage as in XSLT.
</bodyText>
<subsectionHeader confidence="0.996709">
3.6 Advantages of WHAT
</subsectionHeader>
<bodyText confidence="0.941183">
WHAT is
</bodyText>
<listItem confidence="0.952337892857143">
• based on standard W3C technology (XSLT)
• portable. As the programming language-specific
wrapper code is relatively small, WHAT can
easily be ported to any programming language for
which an XSLT engine exists. Currently, WHAT
is implemented in Java (JAXP/Xalan) and C/C++
(Gnome libxml/libxslt). Through libxml/libxslt, it
can also easily be ported to Perl, Python, Tcl and
other languages
• easy to extend to new components/DTDs. This
has to be done only once for a component through
XSLT query library definitions, and access will be
available immediately in all programming
languages for which a WHAT implementation
exists
• powerful (mainly through XPath which is part of
XSLT).
WHAT can be used
• to perform computations and complex
transformations on XML annotation,
• as uniform access to abstract from component-
specific namings and DTD structure, and
• to exchange results between components (e.g., to
give non-XML-aware components access to
information encoded XML annotation),
• to define application-specific architectures for
online and offline processing of NLP XML
annotation.
</listItem>
<sectionHeader confidence="0.989176" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999841290322581">
We have presented an open, flexible and powerful
infrastructure based on standard W3C technology for
the online and offline combination of natural language
processing components, with a focus on, but not limited
to, hybrid deep and shallow architectures.
The infrastructure is part of the Whiteboard architecture
and is employed and will be continued in several
successor projects. The infrastructure is well suited for
rapid prototyping of hybrid NLP architectures as well as
for developing NLP applications, and can be used to
both access NLP XML markup from programming
languages and to compute or transform it.
Because WHAT is an open framework, it is worth
considering XQuery as a future extension to WHAT.
Which engine to ask, an XSLT or an XQuery processor,
could be coded in each &lt;query&gt; element of the WHAT
template library.
WHAT can be used to translate to the ingenious Thistle
tool (Calder 2000) for visualization of linguistic
analyses and back from Thistle in editor mode, e.g. for
manual, graphical correction of automatically annotated
texts for training etc.
A proximate approach is to combine WHAT with SDL
(Krieger 2003) to declaratively specify WHAT-based
NLP architectures (pipelines, loops, parallel
transformation) that can be compiled to Java code.
The proximity to W3C standards suggests using WHAT
directly for transformation of NLP results into
application-oriented (W3C) markup, or to use W3C
markup (e.g. RDF) for semantic web integration in
NLP, VoiceXML, etc.
</bodyText>
<sectionHeader confidence="0.999368" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999941333333333">
I would like to thank my collegues, especially Anette
Frank, Bernd Kiefer, Hans-Ulrich Krieger and Günter
Neumann, for cooperation and many discussions.
This work has been supported by a grant from the
German Federal Ministry of Education and Research
(FKZ 01IW002).
This document was generated partly in the context of
the DeepThought project, funded under the Thematic
Programme User-friendly Information Society of the 5th
Framework Programme of the European Community –
(Contract N° IST-2001-37836). The author is solely
responsible for its content, it does not represent the
opinion of the European Community and the
Community is not responsible for any use that might be
made of data appearing therein.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="references">
6 References
</sectionHeader>
<reference confidence="0.999866450980393">
Markus Becker and Anette Frank. 2002. A Stochastic
Topological Parser of German. Proceedings of
COLING-2002, pp 71-77, Taipei.
Markus Becker, Witold Dro2d2ynski, Hans-Ulrich
Krieger, Jakub Piskorski, Ulrich Schäfer, Feiyu Xu.
2002. SProUT - Shallow Processing with Typed
Feature Structures and Unification. Proceedings of
the International Conference on NLP (ICON 2002).
Mumbai, India.
Scott Boag, Don Chamberlin, Mary F. Fernandez,
Daniela Florescu, Jonathan Robie and Jérôme
Siméon. 2002. XQuery 1.0: An XML Query
Language. http://www.w3c.org/TR/xquery
Chris Brew, David McKelvie, Richard Tobin, Henry
Thompson and Andrei Mikheev. 2000. The XML
Library LT XML. User documentation and reference
guide. LTG. University of Edinburgh.
Joe Calder. 2000. Thistle: Diagram Display Engines
and Editors. Technical report. HCRC. University of
Edinburgh.
Ulrich Callmeier. 2000. PET - A platform for
experimentation with efficient HPSG processing
techniques. Natural Language Engineering, 6 (1)
(Special Issue on Efficient Processing with HPSG:
Methods, systems, evaluation). Editors: D. Flickinger,
S.Oepen, H. Uszkoreit, J. Tsujii, pp. 99 – 108.
Cambridge, UK: Cambridge University Press.
Jean Carletta, David McKelvie, Amy Isard, Andreas
Mengel, Marion Klein, Morten Baun Møller. 2002. A
generic approach to software support for linguistic
annotation using XML. Readings in Corpus
Linguistics, ed. G. Sampson and D. McCarthy,
London and NY: Continuum International.
James Clark (ed.). 1999. XSL Transformations (XSLT)
http://www.w3c.org/TR/xslt
James Clark and Steve DeRose (eds.). 1999. XML Path
Language (XPath) http://www.w3c.org/TR/xpath
Anne Copestake. 1999. The (new) LKB system.
ftp://www-csli.stanford.edu/~aac/newdoc.pdf
Berthold Crysmann, Anette Frank, Bernd Kiefer, Hans-
Ulrich Krieger, Stefan Müller, Günter Neumann,
Jakub Piskorski, Ulrich Schäfer, Melanie Siegel, Hans
Uszkoreit, Feiyu Xu. 2002. An Integrated
Architecture for Shallow and Deep Processing.
Proceedings of ACL-2002, Philadelphia, PA.
Stefan Evert with Holger Voormann. 2002. NITE Query
Language. NITE Project Document. Stuttgart.
Anette Frank, Markus Becker, Berthold Crysmann,
Bernd Kiefer and Ulrich Schäfer. 2003. Integrated
Shallow and Deep Parsing. Submitted manuscript.
Claire Grover, Ewan Klein, Alex Lascarides and Maria
Lapata. 2002. XML-based NLP Tools for Analysing
and Annotating Medical Language. Proceedings of
the Second International Workshop on NLP and XML
(NLPXML-2002). Taipei.
Nancy Ide. 2000. The XML Framework and its
Implications for the Development of Natural
Language Processing Tools. Proceedings of the
COLING Workshop on Using Toolsets and
Architectures to Build NLP Systems, Luxembourg.
Nancy Ide and Laurent Romary. 2001. A Common
Framework for Syntactic Annotation. Proceedings of
ACL-2001. pp. 298-305. Toulouse.
Fabrice Issac. 1998. A Standard Representation
Framework for TAG. In Fourth International
Workshop on Tree Adjoining Grammars and Related
Frameworks (TAG+4), Philadelphia, PA.
Hans-Ulrich Krieger. 2003. SDL – A Description
Language for Specifying NLP Systems. DFKI
Technical Report. Saarbrücken.
Hans-Ulrich Krieger and Ulrich Schäfer. 1994. TDL - A
Type Description Language for Constraint-Based
Grammars. Proceedings of COLING-94. Vol. 2 pp.
893-899, Kyoto.
D. Terence Langendoen and Gary F. Simons. 1995. A
rationale for the TEI recommendations for feature-
structure markup. Computers and the Humanities
29(3). Reprinted in Nancy Ide and Jean Veronis, eds.
The Text Encoding Initiative: Background and
Context, pp. 191-209. Dordrecht: Kluwer Acad. Publ.
Jakub Piskorski and Günter Neumann. 2000. An
Intelligent Text Extraction and Navigation System. In
proceedings of 6th RIAO-2000, Paris.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Chicago: University of
Chicago Press.
Manfred Sailer and Frank Richter. 2001. Eine XML-
Kodierung für AVM-Beschreibungen (in German). In
Lobin H. (ed.) Proceedings of the Annual Meeting of
the Gesellschaft für linguistische Datenverarbeitung,
Giessen. pp. 161 – 168.
Helmut Schmid. 2000. LoPar: Design and
Implementation. Arbeitspapiere des
Sonderforschungsbereiches 340, No. 149. University
of Stuttgart.
Wojciech Skut and Thorsten Brants. 1998. Chunk
tagger – statistical recognition of noun phrases. In
Proceedings of the ESSLLI Workshop on Automated
Acquisition of Syntax and Parsing. Saarbrücken.
Henry S. Thompson and David McKelvie. 1997.
Hyperlink Semantics for standoff markup of read-only
documents. In Proc SGML EU 1997.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.751260">
<title confidence="0.9981935">WHAT: An XSLT-based Infrastructure for the Integration of Language Processing Components</title>
<author confidence="0.917768">Ulrich</author>
<affiliation confidence="0.934753">Language Technology Lab, German Research Center for Artificial Intelligence</affiliation>
<address confidence="0.869703">Stuhlsatzenhausweg 3, D-66123 Saarbrücken,</address>
<email confidence="0.949964">Ulrich.Schaefer@dfki.de</email>
<abstract confidence="0.997340380952381">The idea of the Whiteboard project is to integrate deep and shallow natural language processing components in order to benefit from their synergy. The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field analyses from shallow components. This integration increases robustness, directs the search space and hence reduces processing time of the deep parser. In this paper, we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration, and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture. The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Markus Becker</author>
<author>Anette Frank</author>
</authors>
<title>A Stochastic Topological Parser of German.</title>
<date>2002</date>
<booktitle>Proceedings of COLING-2002,</booktitle>
<pages>71--77</pages>
<location>Taipei.</location>
<contexts>
<context position="15350" citStr="Becker and Frank (2002)" startWordPosition="2218" endWordPosition="2221">how generic XSL transformation rules can be. Another example is transformation of XML tree representations into Thistle trees (arbora DTD; see Calder 2000). While the output DTD is fixed, this is again not true for the input document which can contain arbitrary element names and branches. Thistle visualizations generated through WHAT are shown in Fig. 4, 5 and 6 below. 3.2 Components of the Hybrid System The WHAT has been successfully used in the Whiteboard architecture for online analysis of German newspaper sentences. For more details on motivation and evaluation cf. Frank et al. (2003) and Becker and Frank (2002). The simplified diagram in Figure 3 depicts the components and places where WHAT comes into play in the hybrid integration of deep and shallow processing components (V, N, D denote the WHAT query types). The system takes an input sentence, and runs three shallow systems on it: • the rule-based shallow SPPC (Piskorski and Neumann 2000) for named entity recognition, • TnT/Chunkie, a statistics-based shallow PoS tagger and chunker by (Skut and Brants 1998), • LoPar, a probabilistic context-free parser (Schmid 2000), which takes PoS-tagged tokens as input, and produces binary tree representations</context>
</contexts>
<marker>Becker, Frank, 2002</marker>
<rawString>Markus Becker and Anette Frank. 2002. A Stochastic Topological Parser of German. Proceedings of COLING-2002, pp 71-77, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Becker</author>
</authors>
<title>Witold Dro2d2ynski, Hans-Ulrich Krieger, Jakub Piskorski, Ulrich Schäfer, Feiyu Xu.</title>
<date>2002</date>
<booktitle>Proceedings of the International Conference on NLP (ICON 2002).</booktitle>
<location>Mumbai, India.</location>
<marker>Becker, 2002</marker>
<rawString>Markus Becker, Witold Dro2d2ynski, Hans-Ulrich Krieger, Jakub Piskorski, Ulrich Schäfer, Feiyu Xu. 2002. SProUT - Shallow Processing with Typed Feature Structures and Unification. Proceedings of the International Conference on NLP (ICON 2002). Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Boag</author>
<author>Don Chamberlin</author>
<author>Mary F Fernandez</author>
<author>Daniela Florescu</author>
<author>Jonathan Robie</author>
<author>Jérôme Siméon</author>
</authors>
<title>XQuery 1.0: An XML Query Language.</title>
<date>2002</date>
<note>http://www.w3c.org/TR/xquery</note>
<contexts>
<context position="25373" citStr="Boag et al. 2002" startWordPosition="3649" endWordPosition="3652">cts. The trade-off for this benefit is a representation language somewhat limited with respect to linguistic expressivity. NiteQL (Evert and Voormann 2002) can be seen as an extension to XPath within XSLT, has a more concise syntax especially for document structure-related expressions and a focus on timeline support with specialized queries (for speech annotation). The query language in general does not add expressive power to XSLT and the implementation currently only supports Java XSLT engines. Because of unstable standardization and implementation status, we did not yet make use of XQuery (Boag et al. 2002). XQuery is a powerful, SQL-like query language on XML documents where XPath is a subset rather than a sublanguage as in XSLT. 3.6 Advantages of WHAT WHAT is • based on standard W3C technology (XSLT) • portable. As the programming language-specific wrapper code is relatively small, WHAT can easily be ported to any programming language for which an XSLT engine exists. Currently, WHAT is implemented in Java (JAXP/Xalan) and C/C++ (Gnome libxml/libxslt). Through libxml/libxslt, it can also easily be ported to Perl, Python, Tcl and other languages • easy to extend to new components/DTDs. This has </context>
</contexts>
<marker>Boag, Chamberlin, Fernandez, Florescu, Robie, Siméon, 2002</marker>
<rawString>Scott Boag, Don Chamberlin, Mary F. Fernandez, Daniela Florescu, Jonathan Robie and Jérôme Siméon. 2002. XQuery 1.0: An XML Query Language. http://www.w3c.org/TR/xquery</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brew</author>
<author>David McKelvie</author>
<author>Richard Tobin</author>
<author>Henry Thompson</author>
<author>Andrei Mikheev</author>
</authors>
<title>The XML Library LT XML. User documentation and reference guide.</title>
<date>2000</date>
<institution>LTG. University of Edinburgh.</institution>
<contexts>
<context position="1505" citStr="Brew et al. 2000" startWordPosition="208" endWordPosition="211">hiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration, and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture. The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications. 1 Introduction During the last decade, SGML and XML have become an important interchange format for linguistic data, be they created manually by linguists, or automatically by natural language processing (NLP) components. LT-XML (Brew et al. 2000), XCES (Ide and Romary 2001) and many other are examples for XML-based or XML-supporting software architectures for natural language processing. The main focus of the Whiteboard project (2000-2002) was to integrate shallow and deep natural language processing components. The idea was to combine both in order to benefit from their advantages. Successful and beneficial integration included tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field levels in a fully automated XML-based system. Crysmann et al. (2002) and Frank et al. (2003) show </context>
</contexts>
<marker>Brew, McKelvie, Tobin, Thompson, Mikheev, 2000</marker>
<rawString>Chris Brew, David McKelvie, Richard Tobin, Henry Thompson and Andrei Mikheev. 2000. The XML Library LT XML. User documentation and reference guide. LTG. University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joe Calder</author>
</authors>
<title>Thistle: Diagram Display Engines and Editors.</title>
<date>2000</date>
<tech>Technical report. HCRC.</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="14882" citStr="Calder 2000" startWordPosition="2143" endWordPosition="2144">nt.generateIDs&amp;quot;&gt; &lt;!-- generate unique id for each element --&gt; &lt;xsl:template match=&amp;quot;*&amp;quot;&gt; &lt;xsl:copy select=&amp;quot;.&amp;quot;&gt; &lt;xsl:attribute name=&amp;quot;id&amp;quot;&gt; &lt;xsl:value-of select=&amp;quot;generate-id()&amp;quot;/&gt; &lt;/xsl:attribute&gt; &lt;xsl:for-each select=&amp;quot;@*&amp;quot;&gt; &lt;xsl:copy-of select=&amp;quot;.&amp;quot;/&gt; &lt;/xsl:for-each&gt; &lt;xsl:apply-templates/&gt; &lt;/xsl:copy&gt; /xsl:template&gt; &lt;/query&gt; Note that this is an example for a stylesheet that is completely independent of a DTD, it just works on any XML document – and thus shows how generic XSL transformation rules can be. Another example is transformation of XML tree representations into Thistle trees (arbora DTD; see Calder 2000). While the output DTD is fixed, this is again not true for the input document which can contain arbitrary element names and branches. Thistle visualizations generated through WHAT are shown in Fig. 4, 5 and 6 below. 3.2 Components of the Hybrid System The WHAT has been successfully used in the Whiteboard architecture for online analysis of German newspaper sentences. For more details on motivation and evaluation cf. Frank et al. (2003) and Becker and Frank (2002). The simplified diagram in Figure 3 depicts the components and places where WHAT comes into play in the hybrid integration of deep </context>
<context position="27560" citStr="Calder 2000" startWordPosition="3997" endWordPosition="3998">oard architecture and is employed and will be continued in several successor projects. The infrastructure is well suited for rapid prototyping of hybrid NLP architectures as well as for developing NLP applications, and can be used to both access NLP XML markup from programming languages and to compute or transform it. Because WHAT is an open framework, it is worth considering XQuery as a future extension to WHAT. Which engine to ask, an XSLT or an XQuery processor, could be coded in each &lt;query&gt; element of the WHAT template library. WHAT can be used to translate to the ingenious Thistle tool (Calder 2000) for visualization of linguistic analyses and back from Thistle in editor mode, e.g. for manual, graphical correction of automatically annotated texts for training etc. A proximate approach is to combine WHAT with SDL (Krieger 2003) to declaratively specify WHAT-based NLP architectures (pipelines, loops, parallel transformation) that can be compiled to Java code. The proximity to W3C standards suggests using WHAT directly for transformation of NLP results into application-oriented (W3C) markup, or to use W3C markup (e.g. RDF) for semantic web integration in NLP, VoiceXML, etc. 5 Acknowledgemen</context>
</contexts>
<marker>Calder, 2000</marker>
<rawString>Joe Calder. 2000. Thistle: Diagram Display Engines and Editors. Technical report. HCRC. University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Callmeier</author>
</authors>
<title>PET - A platform for experimentation with efficient HPSG processing techniques.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<booktitle>(Special Issue on Efficient Processing with HPSG: Methods, systems, evaluation). Editors:</booktitle>
<volume>6</volume>
<issue>1</issue>
<pages>99--108</pages>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge, UK:</location>
<contexts>
<context position="2274" citStr="Callmeier (2000)" startWordPosition="320" endWordPosition="321"> focus of the Whiteboard project (2000-2002) was to integrate shallow and deep natural language processing components. The idea was to combine both in order to benefit from their advantages. Successful and beneficial integration included tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field levels in a fully automated XML-based system. Crysmann et al. (2002) and Frank et al. (2003) show that this close deepshallow combination significantly increases robustness and performance compared to the (already fast) standalone deep HPSG parser by Callmeier (2000). The only comparable architecture so far was described by Grover et al. (2002), but their integration was limited to tokenization and PoS tagging (the shallow chunker did not guide or contribute to deep analysis). In this paper, we will focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration, and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture. Because the infrastructure is in general independent of deep or shallow parad</context>
<context position="4624" citStr="Callmeier 2000" startWordPosition="673" endWordPosition="674">nds, but precision and quality barriers are so obvious (especially on domains the systems where not designed for or trained on) that a need for &apos;deeper&apos; systems re-emerged. Moreover, 1 In this paper, &apos;deep&apos; is nearly synonymous to typed unification-based grammar formalisms, e.g. HPSG (Pollard and Sag 1994), although the infrastructure may also apply to other deep linguistic frameworks. semantics construction from an input sentence is quite poor and erroneous in typical shallow systems. But also development of DNLP made advances during the last few years, especially in the field of efficiency (Callmeier 2000). A promising solution to improve quality of natural language processing is the combination of deep and shallow technologies. Deep processing benefits from specialized and fast shallow analysis results, shallow processing becomes &apos;deeper&apos; using at least partial results from DNLP. Many natural language processing applications could benefit from the synergy of the combination of deep and shallow, e.g. advanced information extraction, question answering, or grammar checking systems. The Whiteboard architecture aims at integrating different language technology components. Both online and offline c</context>
<context position="16520" citStr="Callmeier 2000" startWordPosition="2403" endWordPosition="2404">put, and produces binary tree representations of sentence fields, e.g., topo.bin in Fig. 4. For a justification for binary vs. flat trees cf. Becker and Frank (2002). The results of the components are three standoff annotations of the input sentence. Then, a sequence of D-queries is applied to flatten the binary topological field trees (result is topo.flat, Fig. 5), merge with shallow chunk information from Chunkie (topo.chunks, Fig. 6), and apply the main D-query computing bracket information for the deep parser from the merged topo tree (topo.brackets, Fig. 7). Finally, the deep parser PET (Callmeier 2000), modified as described in Frank et al. (2003), is started with a chart initialized using the shallow bracket information (topo.brackets) through WHAT V and N queries. PET also accesses lexical and named entity information from SPPC through V queries. input sentence Figure 3: WHAT in the hybrid parser SPPC TnT WHAT-based application D,V,N topo.bin topo.flat LoPar D D PET V, N topo.brackets topo.chunks D D Chunkie Again, WHAT abstraction facilitates exchange of the shallow input components of PET without needing to rewrite the parser&apos;s code. The dashed lines in Figure 3 indicate that a WHATbase</context>
<context position="19974" citStr="Callmeier 2000" startWordPosition="2880" endWordPosition="2881">ind of representation is its proximity to a particular kind of feature structure implementation. We adopt an SGML markup for typed feature structures originally developed by the Text Encoding Initiative (TEI) which is very compact and seems to be widely accepted, e.g. also in the Tree Adjoining Grammar community (Issac 1998). Langendoen and Simons (1995) give an in-depth justification for the naming and structure of a feature structure DTD. We will focus here on the feature structure DTD subset that is able to encode the basic data structures of deep systems such as LKB (Copestake 1999), PET (Callmeier 2000), PAGE, or the shallow system SProUT (Becker et al. 2002) which have a subset of TDL (Krieger and Schäfer 1994) as their common basic formalism2: &lt;?xml version=&amp;quot;1.0&amp;quot; ?&gt; &lt;!-- minimal typed feature structure DTD --&gt; &lt;!ELEMENT FS ( F* ) &gt; &lt;!ATTLIST FS type NMTOKEN #IMPLIED coref NMTOKEN #IMPLIED &gt; &lt;!ELEMENT F ( FS ) &gt; &lt;!ATTLIST F name NMTOKEN #REQUIRED &gt; The FS tag encodes typed Feature Structure nodes, F encodes Features. Atoms are encoded as typed Feature structure nodes with empty feature list. An important 2 Encoding of type hierarchies or other possibly system or formalism-specific definitio</context>
</contexts>
<marker>Callmeier, 2000</marker>
<rawString>Ulrich Callmeier. 2000. PET - A platform for experimentation with efficient HPSG processing techniques. Natural Language Engineering, 6 (1) (Special Issue on Efficient Processing with HPSG: Methods, systems, evaluation). Editors: D. Flickinger, S.Oepen, H. Uszkoreit, J. Tsujii, pp. 99 – 108. Cambridge, UK: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
<author>David McKelvie</author>
<author>Amy Isard</author>
<author>Andreas Mengel</author>
<author>Marion Klein</author>
<author>Morten Baun Møller</author>
</authors>
<title>A generic approach to software support for linguistic annotation using XML.</title>
<date>2002</date>
<booktitle>Readings in Corpus Linguistics,</booktitle>
<editor>ed. G. Sampson and D. McCarthy, London and NY:</editor>
<publisher>Continuum International.</publisher>
<contexts>
<context position="24515" citStr="Carletta et al. (2002)" startWordPosition="3523" endWordPosition="3526">tions on the same input and thus help to improve processing speed. As in all programming language, there a multiple solutions for a problem. An XSL profiling tool (e.g. xsltprofiler.org) can help to locate inefficient XSLT code. 3.5 Related Work As argued in Thompson and McKelvie (1997), standoff annotation is a viable solution in order to cope with the combination of multiple overlapping hierarchies and the efficiency problem of XML tree modification for large annotations. Ide (2000) gives an overview of NLP-related XML core technologies that also strives XSLT. We adopt the pragmatic view of Carletta et al. (2002), who see that computational linguistics greatly benefits from general XMLification, namely by getting for free standards and advanced technologies for storing and manipulating XML annotation, mainly through W3C and various open source projects. The trade-off for this benefit is a representation language somewhat limited with respect to linguistic expressivity. NiteQL (Evert and Voormann 2002) can be seen as an extension to XPath within XSLT, has a more concise syntax especially for document structure-related expressions and a focus on timeline support with specialized queries (for speech anno</context>
</contexts>
<marker>Carletta, McKelvie, Isard, Mengel, Klein, Møller, 2002</marker>
<rawString>Jean Carletta, David McKelvie, Amy Isard, Andreas Mengel, Marion Klein, Morten Baun Møller. 2002. A generic approach to software support for linguistic annotation using XML. Readings in Corpus Linguistics, ed. G. Sampson and D. McCarthy, London and NY: Continuum International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clark</author>
</authors>
<date>1999</date>
<note>XSL Transformations (XSLT) http://www.w3c.org/TR/xslt</note>
<contexts>
<context position="6824" citStr="Clark 1999" startWordPosition="988" endWordPosition="989">eval mechanism based on XML is encapsulated through the shallow programming interface. However, while the typed feature structure-based programming interface to deep components is stable, it turned out that the XML-based interface was too inflexible when new, mainly shallow, components with new DTDs had to be integrated. Therefore, a more flexible approach had to be devised. 3 WHAT: The Whiteboard Annotation Transformer The main motivation for developing an XSLT-based infrastructure for NLP components was to provide flexible access to standoff XML annotations produced by the components. XSLT (Clark 1999) is a W3C standard language for the transformation of XML documents. Input of an XSL transformation must be XML, while output can be any syntax (e.g., XML, text, HTML, RTF, or even programming language source code, etc.). The power of XSLT mainly comes from its sublanguage XPath (Clark and DeRose 1999), which supports access to XML structure, elements, attributes and text through concise path expressions. An XSL stylesheet consists of templates with XPath expressions that must match the input document in order to be executed. The order in which templates are called is by default top-down, left</context>
</contexts>
<marker>Clark, 1999</marker>
<rawString>James Clark (ed.). 1999. XSL Transformations (XSLT) http://www.w3c.org/TR/xslt</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clark</author>
<author>Steve DeRose</author>
</authors>
<date>1999</date>
<note>XML Path Language (XPath) http://www.w3c.org/TR/xpath</note>
<contexts>
<context position="7127" citStr="Clark and DeRose 1999" startWordPosition="1037" endWordPosition="1040">new DTDs had to be integrated. Therefore, a more flexible approach had to be devised. 3 WHAT: The Whiteboard Annotation Transformer The main motivation for developing an XSLT-based infrastructure for NLP components was to provide flexible access to standoff XML annotations produced by the components. XSLT (Clark 1999) is a W3C standard language for the transformation of XML documents. Input of an XSL transformation must be XML, while output can be any syntax (e.g., XML, text, HTML, RTF, or even programming language source code, etc.). The power of XSLT mainly comes from its sublanguage XPath (Clark and DeRose 1999), which supports access to XML structure, elements, attributes and text through concise path expressions. An XSL stylesheet consists of templates with XPath expressions that must match the input document in order to be executed. The order in which templates are called is by default top-down, left to right, but can be modified, augmented, or suppressed through loops, conditionals, and recursive call of (named) templates. WHAT, the WHiteboard Annotation Transformer, is built on top of a standard XSL transformation engine. It provides uniform access to standoff annotation through queries that can</context>
</contexts>
<marker>Clark, DeRose, 1999</marker>
<rawString>James Clark and Steve DeRose (eds.). 1999. XML Path Language (XPath) http://www.w3c.org/TR/xpath</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Copestake</author>
</authors>
<title>The (new) LKB system. ftp://www-csli.stanford.edu/~aac/newdoc.pdf</title>
<date>1999</date>
<contexts>
<context position="19952" citStr="Copestake 1999" startWordPosition="2877" endWordPosition="2878">tage we see for this kind of representation is its proximity to a particular kind of feature structure implementation. We adopt an SGML markup for typed feature structures originally developed by the Text Encoding Initiative (TEI) which is very compact and seems to be widely accepted, e.g. also in the Tree Adjoining Grammar community (Issac 1998). Langendoen and Simons (1995) give an in-depth justification for the naming and structure of a feature structure DTD. We will focus here on the feature structure DTD subset that is able to encode the basic data structures of deep systems such as LKB (Copestake 1999), PET (Callmeier 2000), PAGE, or the shallow system SProUT (Becker et al. 2002) which have a subset of TDL (Krieger and Schäfer 1994) as their common basic formalism2: &lt;?xml version=&amp;quot;1.0&amp;quot; ?&gt; &lt;!-- minimal typed feature structure DTD --&gt; &lt;!ELEMENT FS ( F* ) &gt; &lt;!ATTLIST FS type NMTOKEN #IMPLIED coref NMTOKEN #IMPLIED &gt; &lt;!ELEMENT F ( FS ) &gt; &lt;!ATTLIST F name NMTOKEN #REQUIRED &gt; The FS tag encodes typed Feature Structure nodes, F encodes Features. Atoms are encoded as typed Feature structure nodes with empty feature list. An important 2 Encoding of type hierarchies or other possibly system or formal</context>
</contexts>
<marker>Copestake, 1999</marker>
<rawString>Anne Copestake. 1999. The (new) LKB system. ftp://www-csli.stanford.edu/~aac/newdoc.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berthold Crysmann</author>
<author>Anette Frank</author>
<author>Bernd Kiefer</author>
<author>HansUlrich Krieger</author>
<author>Stefan Müller</author>
<author>Günter Neumann</author>
<author>Jakub Piskorski</author>
<author>Ulrich Schäfer</author>
<author>Melanie Siegel</author>
<author>Hans Uszkoreit</author>
<author>Feiyu Xu</author>
</authors>
<title>An Integrated Architecture for Shallow and Deep Processing.</title>
<date>2002</date>
<booktitle>Proceedings of ACL-2002,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2075" citStr="Crysmann et al. (2002)" startWordPosition="289" endWordPosition="292">ocessing (NLP) components. LT-XML (Brew et al. 2000), XCES (Ide and Romary 2001) and many other are examples for XML-based or XML-supporting software architectures for natural language processing. The main focus of the Whiteboard project (2000-2002) was to integrate shallow and deep natural language processing components. The idea was to combine both in order to benefit from their advantages. Successful and beneficial integration included tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field levels in a fully automated XML-based system. Crysmann et al. (2002) and Frank et al. (2003) show that this close deepshallow combination significantly increases robustness and performance compared to the (already fast) standalone deep HPSG parser by Callmeier (2000). The only comparable architecture so far was described by Grover et al. (2002), but their integration was limited to tokenization and PoS tagging (the shallow chunker did not guide or contribute to deep analysis). In this paper, we will focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component int</context>
</contexts>
<marker>Crysmann, Frank, Kiefer, Krieger, Müller, Neumann, Piskorski, Schäfer, Siegel, Uszkoreit, Xu, 2002</marker>
<rawString>Berthold Crysmann, Anette Frank, Bernd Kiefer, HansUlrich Krieger, Stefan Müller, Günter Neumann, Jakub Piskorski, Ulrich Schäfer, Melanie Siegel, Hans Uszkoreit, Feiyu Xu. 2002. An Integrated Architecture for Shallow and Deep Processing. Proceedings of ACL-2002, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert with Holger Voormann</author>
</authors>
<date>2002</date>
<journal>NITE Query Language. NITE Project Document. Stuttgart.</journal>
<contexts>
<context position="24911" citStr="Voormann 2002" startWordPosition="3579" endWordPosition="3580">and the efficiency problem of XML tree modification for large annotations. Ide (2000) gives an overview of NLP-related XML core technologies that also strives XSLT. We adopt the pragmatic view of Carletta et al. (2002), who see that computational linguistics greatly benefits from general XMLification, namely by getting for free standards and advanced technologies for storing and manipulating XML annotation, mainly through W3C and various open source projects. The trade-off for this benefit is a representation language somewhat limited with respect to linguistic expressivity. NiteQL (Evert and Voormann 2002) can be seen as an extension to XPath within XSLT, has a more concise syntax especially for document structure-related expressions and a focus on timeline support with specialized queries (for speech annotation). The query language in general does not add expressive power to XSLT and the implementation currently only supports Java XSLT engines. Because of unstable standardization and implementation status, we did not yet make use of XQuery (Boag et al. 2002). XQuery is a powerful, SQL-like query language on XML documents where XPath is a subset rather than a sublanguage as in XSLT. 3.6 Advanta</context>
</contexts>
<marker>Voormann, 2002</marker>
<rawString>Stefan Evert with Holger Voormann. 2002. NITE Query Language. NITE Project Document. Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Frank</author>
<author>Markus Becker</author>
<author>Berthold Crysmann</author>
<author>Bernd Kiefer</author>
<author>Ulrich Schäfer</author>
</authors>
<title>Integrated Shallow and Deep Parsing.</title>
<date>2003</date>
<note>Submitted manuscript.</note>
<contexts>
<context position="2099" citStr="Frank et al. (2003)" startWordPosition="294" endWordPosition="297">LT-XML (Brew et al. 2000), XCES (Ide and Romary 2001) and many other are examples for XML-based or XML-supporting software architectures for natural language processing. The main focus of the Whiteboard project (2000-2002) was to integrate shallow and deep natural language processing components. The idea was to combine both in order to benefit from their advantages. Successful and beneficial integration included tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field levels in a fully automated XML-based system. Crysmann et al. (2002) and Frank et al. (2003) show that this close deepshallow combination significantly increases robustness and performance compared to the (already fast) standalone deep HPSG parser by Callmeier (2000). The only comparable architecture so far was described by Grover et al. (2002), but their integration was limited to tokenization and PoS tagging (the shallow chunker did not guide or contribute to deep analysis). In this paper, we will focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration, and present ex</context>
<context position="15322" citStr="Frank et al. (2003)" startWordPosition="2213" endWordPosition="2216">cument – and thus shows how generic XSL transformation rules can be. Another example is transformation of XML tree representations into Thistle trees (arbora DTD; see Calder 2000). While the output DTD is fixed, this is again not true for the input document which can contain arbitrary element names and branches. Thistle visualizations generated through WHAT are shown in Fig. 4, 5 and 6 below. 3.2 Components of the Hybrid System The WHAT has been successfully used in the Whiteboard architecture for online analysis of German newspaper sentences. For more details on motivation and evaluation cf. Frank et al. (2003) and Becker and Frank (2002). The simplified diagram in Figure 3 depicts the components and places where WHAT comes into play in the hybrid integration of deep and shallow processing components (V, N, D denote the WHAT query types). The system takes an input sentence, and runs three shallow systems on it: • the rule-based shallow SPPC (Piskorski and Neumann 2000) for named entity recognition, • TnT/Chunkie, a statistics-based shallow PoS tagger and chunker by (Skut and Brants 1998), • LoPar, a probabilistic context-free parser (Schmid 2000), which takes PoS-tagged tokens as input, and produces</context>
<context position="16566" citStr="Frank et al. (2003)" startWordPosition="2409" endWordPosition="2412">ons of sentence fields, e.g., topo.bin in Fig. 4. For a justification for binary vs. flat trees cf. Becker and Frank (2002). The results of the components are three standoff annotations of the input sentence. Then, a sequence of D-queries is applied to flatten the binary topological field trees (result is topo.flat, Fig. 5), merge with shallow chunk information from Chunkie (topo.chunks, Fig. 6), and apply the main D-query computing bracket information for the deep parser from the merged topo tree (topo.brackets, Fig. 7). Finally, the deep parser PET (Callmeier 2000), modified as described in Frank et al. (2003), is started with a chart initialized using the shallow bracket information (topo.brackets) through WHAT V and N queries. PET also accesses lexical and named entity information from SPPC through V queries. input sentence Figure 3: WHAT in the hybrid parser SPPC TnT WHAT-based application D,V,N topo.bin topo.flat LoPar D D PET V, N topo.brackets topo.chunks D D Chunkie Again, WHAT abstraction facilitates exchange of the shallow input components of PET without needing to rewrite the parser&apos;s code. The dashed lines in Figure 3 indicate that a WHATbased application can have access to the standoff </context>
</contexts>
<marker>Frank, Becker, Crysmann, Kiefer, Schäfer, 2003</marker>
<rawString>Anette Frank, Markus Becker, Berthold Crysmann, Bernd Kiefer and Ulrich Schäfer. 2003. Integrated Shallow and Deep Parsing. Submitted manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Ewan Klein</author>
<author>Alex Lascarides</author>
<author>Maria Lapata</author>
</authors>
<title>XML-based NLP Tools for Analysing and Annotating Medical Language.</title>
<date>2002</date>
<booktitle>Proceedings of the Second International Workshop on NLP and XML (NLPXML-2002).</booktitle>
<location>Taipei.</location>
<contexts>
<context position="2353" citStr="Grover et al. (2002)" startWordPosition="331" endWordPosition="334">eep natural language processing components. The idea was to combine both in order to benefit from their advantages. Successful and beneficial integration included tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field levels in a fully automated XML-based system. Crysmann et al. (2002) and Frank et al. (2003) show that this close deepshallow combination significantly increases robustness and performance compared to the (already fast) standalone deep HPSG parser by Callmeier (2000). The only comparable architecture so far was described by Grover et al. (2002), but their integration was limited to tokenization and PoS tagging (the shallow chunker did not guide or contribute to deep analysis). In this paper, we will focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration, and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture. Because the infrastructure is in general independent of deep or shallow paradigms, it can also be applied to purely shallow or deep systems. 2 Whiteboard: D</context>
</contexts>
<marker>Grover, Klein, Lascarides, Lapata, 2002</marker>
<rawString>Claire Grover, Ewan Klein, Alex Lascarides and Maria Lapata. 2002. XML-based NLP Tools for Analysing and Annotating Medical Language. Proceedings of the Second International Workshop on NLP and XML (NLPXML-2002). Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
</authors>
<title>The XML Framework and its Implications for the Development of Natural Language Processing Tools.</title>
<date>2000</date>
<booktitle>Proceedings of the COLING Workshop on Using Toolsets and Architectures to Build NLP Systems,</booktitle>
<location>Luxembourg.</location>
<contexts>
<context position="24382" citStr="Ide (2000)" startWordPosition="3503" endWordPosition="3504">implementations of WHAT are preferable in order to avoid unnecessary XML parsing when processing multiple WHAT transformations on the same input and thus help to improve processing speed. As in all programming language, there a multiple solutions for a problem. An XSL profiling tool (e.g. xsltprofiler.org) can help to locate inefficient XSLT code. 3.5 Related Work As argued in Thompson and McKelvie (1997), standoff annotation is a viable solution in order to cope with the combination of multiple overlapping hierarchies and the efficiency problem of XML tree modification for large annotations. Ide (2000) gives an overview of NLP-related XML core technologies that also strives XSLT. We adopt the pragmatic view of Carletta et al. (2002), who see that computational linguistics greatly benefits from general XMLification, namely by getting for free standards and advanced technologies for storing and manipulating XML annotation, mainly through W3C and various open source projects. The trade-off for this benefit is a representation language somewhat limited with respect to linguistic expressivity. NiteQL (Evert and Voormann 2002) can be seen as an extension to XPath within XSLT, has a more concise s</context>
</contexts>
<marker>Ide, 2000</marker>
<rawString>Nancy Ide. 2000. The XML Framework and its Implications for the Development of Natural Language Processing Tools. Proceedings of the COLING Workshop on Using Toolsets and Architectures to Build NLP Systems, Luxembourg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Laurent Romary</author>
</authors>
<title>A Common Framework for Syntactic Annotation.</title>
<date>2001</date>
<booktitle>Proceedings of ACL-2001.</booktitle>
<pages>298--305</pages>
<location>Toulouse.</location>
<contexts>
<context position="1533" citStr="Ide and Romary 2001" startWordPosition="213" endWordPosition="216">sformer (WHAT), report on the benefits of XSLT-based NLP component integration, and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture. The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications. 1 Introduction During the last decade, SGML and XML have become an important interchange format for linguistic data, be they created manually by linguists, or automatically by natural language processing (NLP) components. LT-XML (Brew et al. 2000), XCES (Ide and Romary 2001) and many other are examples for XML-based or XML-supporting software architectures for natural language processing. The main focus of the Whiteboard project (2000-2002) was to integrate shallow and deep natural language processing components. The idea was to combine both in order to benefit from their advantages. Successful and beneficial integration included tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field levels in a fully automated XML-based system. Crysmann et al. (2002) and Frank et al. (2003) show that this close deepshallow </context>
</contexts>
<marker>Ide, Romary, 2001</marker>
<rawString>Nancy Ide and Laurent Romary. 2001. A Common Framework for Syntactic Annotation. Proceedings of ACL-2001. pp. 298-305. Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrice Issac</author>
</authors>
<title>A Standard Representation Framework for TAG.</title>
<date>1998</date>
<booktitle>In Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+4),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="19685" citStr="Issac 1998" startWordPosition="2832" endWordPosition="2833">d Richter (2001) propose an XML markup where the recursive embedding of attribute-value pairs is decomposed into a kind of definite equivalences or non-recursive node lists (triples of node ID, type name and embedded lists of attribute-node pairs). The only advantage we see for this kind of representation is its proximity to a particular kind of feature structure implementation. We adopt an SGML markup for typed feature structures originally developed by the Text Encoding Initiative (TEI) which is very compact and seems to be widely accepted, e.g. also in the Tree Adjoining Grammar community (Issac 1998). Langendoen and Simons (1995) give an in-depth justification for the naming and structure of a feature structure DTD. We will focus here on the feature structure DTD subset that is able to encode the basic data structures of deep systems such as LKB (Copestake 1999), PET (Callmeier 2000), PAGE, or the shallow system SProUT (Becker et al. 2002) which have a subset of TDL (Krieger and Schäfer 1994) as their common basic formalism2: &lt;?xml version=&amp;quot;1.0&amp;quot; ?&gt; &lt;!-- minimal typed feature structure DTD --&gt; &lt;!ELEMENT FS ( F* ) &gt; &lt;!ATTLIST FS type NMTOKEN #IMPLIED coref NMTOKEN #IMPLIED &gt; &lt;!ELEMENT F ( F</context>
</contexts>
<marker>Issac, 1998</marker>
<rawString>Fabrice Issac. 1998. A Standard Representation Framework for TAG. In Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+4), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans-Ulrich Krieger</author>
</authors>
<title>SDL – A Description Language for Specifying NLP Systems.</title>
<date>2003</date>
<tech>DFKI Technical Report. Saarbrücken.</tech>
<contexts>
<context position="27792" citStr="Krieger 2003" startWordPosition="4032" endWordPosition="4033">to both access NLP XML markup from programming languages and to compute or transform it. Because WHAT is an open framework, it is worth considering XQuery as a future extension to WHAT. Which engine to ask, an XSLT or an XQuery processor, could be coded in each &lt;query&gt; element of the WHAT template library. WHAT can be used to translate to the ingenious Thistle tool (Calder 2000) for visualization of linguistic analyses and back from Thistle in editor mode, e.g. for manual, graphical correction of automatically annotated texts for training etc. A proximate approach is to combine WHAT with SDL (Krieger 2003) to declaratively specify WHAT-based NLP architectures (pipelines, loops, parallel transformation) that can be compiled to Java code. The proximity to W3C standards suggests using WHAT directly for transformation of NLP results into application-oriented (W3C) markup, or to use W3C markup (e.g. RDF) for semantic web integration in NLP, VoiceXML, etc. 5 Acknowledgements I would like to thank my collegues, especially Anette Frank, Bernd Kiefer, Hans-Ulrich Krieger and Günter Neumann, for cooperation and many discussions. This work has been supported by a grant from the German Federal Ministry of </context>
</contexts>
<marker>Krieger, 2003</marker>
<rawString>Hans-Ulrich Krieger. 2003. SDL – A Description Language for Specifying NLP Systems. DFKI Technical Report. Saarbrücken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans-Ulrich Krieger</author>
<author>Ulrich Schäfer</author>
</authors>
<title>TDL - A Type Description Language for Constraint-Based Grammars.</title>
<date>1994</date>
<booktitle>Proceedings of COLING-94.</booktitle>
<volume>2</volume>
<pages>893--899</pages>
<location>Kyoto.</location>
<contexts>
<context position="20085" citStr="Krieger and Schäfer 1994" startWordPosition="2898" endWordPosition="2901">dopt an SGML markup for typed feature structures originally developed by the Text Encoding Initiative (TEI) which is very compact and seems to be widely accepted, e.g. also in the Tree Adjoining Grammar community (Issac 1998). Langendoen and Simons (1995) give an in-depth justification for the naming and structure of a feature structure DTD. We will focus here on the feature structure DTD subset that is able to encode the basic data structures of deep systems such as LKB (Copestake 1999), PET (Callmeier 2000), PAGE, or the shallow system SProUT (Becker et al. 2002) which have a subset of TDL (Krieger and Schäfer 1994) as their common basic formalism2: &lt;?xml version=&amp;quot;1.0&amp;quot; ?&gt; &lt;!-- minimal typed feature structure DTD --&gt; &lt;!ELEMENT FS ( F* ) &gt; &lt;!ATTLIST FS type NMTOKEN #IMPLIED coref NMTOKEN #IMPLIED &gt; &lt;!ELEMENT F ( FS ) &gt; &lt;!ATTLIST F name NMTOKEN #REQUIRED &gt; The FS tag encodes typed Feature Structure nodes, F encodes Features. Atoms are encoded as typed Feature structure nodes with empty feature list. An important 2 Encoding of type hierarchies or other possibly system or formalism-specific definitions are of course not covered by this minimal DTD. point is the encoding of coreferences (reentrancies) between </context>
</contexts>
<marker>Krieger, Schäfer, 1994</marker>
<rawString>Hans-Ulrich Krieger and Ulrich Schäfer. 1994. TDL - A Type Description Language for Constraint-Based Grammars. Proceedings of COLING-94. Vol. 2 pp. 893-899, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Terence Langendoen</author>
<author>Gary F Simons</author>
</authors>
<title>A rationale for the TEI recommendations for featurestructure markup.</title>
<date>1995</date>
<booktitle>Computers and the Humanities 29(3). Reprinted in Nancy Ide and Jean Veronis, eds. The Text Encoding Initiative: Background and Context,</booktitle>
<pages>191--209</pages>
<publisher>Kluwer Acad. Publ.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="19715" citStr="Langendoen and Simons (1995)" startWordPosition="2834" endWordPosition="2837">01) propose an XML markup where the recursive embedding of attribute-value pairs is decomposed into a kind of definite equivalences or non-recursive node lists (triples of node ID, type name and embedded lists of attribute-node pairs). The only advantage we see for this kind of representation is its proximity to a particular kind of feature structure implementation. We adopt an SGML markup for typed feature structures originally developed by the Text Encoding Initiative (TEI) which is very compact and seems to be widely accepted, e.g. also in the Tree Adjoining Grammar community (Issac 1998). Langendoen and Simons (1995) give an in-depth justification for the naming and structure of a feature structure DTD. We will focus here on the feature structure DTD subset that is able to encode the basic data structures of deep systems such as LKB (Copestake 1999), PET (Callmeier 2000), PAGE, or the shallow system SProUT (Becker et al. 2002) which have a subset of TDL (Krieger and Schäfer 1994) as their common basic formalism2: &lt;?xml version=&amp;quot;1.0&amp;quot; ?&gt; &lt;!-- minimal typed feature structure DTD --&gt; &lt;!ELEMENT FS ( F* ) &gt; &lt;!ATTLIST FS type NMTOKEN #IMPLIED coref NMTOKEN #IMPLIED &gt; &lt;!ELEMENT F ( FS ) &gt; &lt;!ATTLIST F name NMTOKEN</context>
</contexts>
<marker>Langendoen, Simons, 1995</marker>
<rawString>D. Terence Langendoen and Gary F. Simons. 1995. A rationale for the TEI recommendations for featurestructure markup. Computers and the Humanities 29(3). Reprinted in Nancy Ide and Jean Veronis, eds. The Text Encoding Initiative: Background and Context, pp. 191-209. Dordrecht: Kluwer Acad. Publ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Piskorski</author>
<author>Günter Neumann</author>
</authors>
<title>An Intelligent Text Extraction and Navigation System.</title>
<date>2000</date>
<booktitle>In proceedings of 6th RIAO-2000,</booktitle>
<location>Paris.</location>
<contexts>
<context position="9849" citStr="Piskorski and Neumann 2000" startWordPosition="1489" endWordPosition="1492">ries as functions, we get the following query signatures: • getValue: C x D x P* → S* • getNodes: C x D x P* → N* • getDocument: C x D x P* → D where C is the component, D an XML document, P* a (possibly empty) sequence of parameters, S* a sequence of strings, and N* a sequence of node identifiers. We now give examples for each of the query types. 3.1.1 V-queries (getValue) V-queries return string values from XML attribute values or text. The simplest case is a single XPath lookup. As an example, we determine the type of named entity 23 in a shallow XML annotation produced by the SPPC system (Piskorski and Neumann 2000). The WHAT query getValue(&amp;quot;NE.type&amp;quot;, &amp;quot;de.dfki.lt.sppc&amp;quot;, 23) would lead to the lookup of the following query in the XSLT template library for SPPC &lt;query name=&amp;quot;getValue.NE.type&amp;quot; component=&amp;quot;de.dfki.lt.sppc&amp;quot;&gt; &lt;!-- returns the type of named entity as number --&gt; &lt;xsl:param name=&amp;quot;index&amp;quot;/&gt; &lt;xsl:template match=&amp;quot;/WHITEBOARD/SPPC_XML//NE[@id=$index]&amp;quot;&gt; &lt;xsl:value-of select=&amp;quot;@type&amp;quot;/&gt; &lt;/xsl:template&gt; &lt;/query&gt; On appropriate SPPC XML annotation, containing the named entity tag e.g. &lt;NE id=&amp;quot;23&amp;quot; type=&amp;quot;location&amp;quot;...&gt; somewhere below the root tag, this query would return the String &amp;quot;location&amp;quot;. By adding a subseq</context>
<context position="15687" citStr="Piskorski and Neumann 2000" startWordPosition="2274" endWordPosition="2277">AT are shown in Fig. 4, 5 and 6 below. 3.2 Components of the Hybrid System The WHAT has been successfully used in the Whiteboard architecture for online analysis of German newspaper sentences. For more details on motivation and evaluation cf. Frank et al. (2003) and Becker and Frank (2002). The simplified diagram in Figure 3 depicts the components and places where WHAT comes into play in the hybrid integration of deep and shallow processing components (V, N, D denote the WHAT query types). The system takes an input sentence, and runs three shallow systems on it: • the rule-based shallow SPPC (Piskorski and Neumann 2000) for named entity recognition, • TnT/Chunkie, a statistics-based shallow PoS tagger and chunker by (Skut and Brants 1998), • LoPar, a probabilistic context-free parser (Schmid 2000), which takes PoS-tagged tokens as input, and produces binary tree representations of sentence fields, e.g., topo.bin in Fig. 4. For a justification for binary vs. flat trees cf. Becker and Frank (2002). The results of the components are three standoff annotations of the input sentence. Then, a sequence of D-queries is applied to flatten the binary topological field trees (result is topo.flat, Fig. 5), merge with sh</context>
</contexts>
<marker>Piskorski, Neumann, 2000</marker>
<rawString>Jakub Piskorski and Günter Neumann. 2000. An Intelligent Text Extraction and Navigation System. In proceedings of 6th RIAO-2000, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl J Pollard</author>
<author>Ivan A Sag</author>
</authors>
<date>1994</date>
<booktitle>Head-Driven Phrase Structure Grammar.</booktitle>
<publisher>University of Chicago Press.</publisher>
<location>Chicago:</location>
<contexts>
<context position="4316" citStr="Pollard and Sag 1994" startWordPosition="625" endWordPosition="628">istics-based approaches, they are in general much faster than DNLP. Due to the lack of efficiency and robustness of DNLP systems, the trend in application-oriented language processing system development in the last years was to improve SNLP systems. They are now capable of analyzing Megabytes of texts within seconds, but precision and quality barriers are so obvious (especially on domains the systems where not designed for or trained on) that a need for &apos;deeper&apos; systems re-emerged. Moreover, 1 In this paper, &apos;deep&apos; is nearly synonymous to typed unification-based grammar formalisms, e.g. HPSG (Pollard and Sag 1994), although the infrastructure may also apply to other deep linguistic frameworks. semantics construction from an input sentence is quite poor and erroneous in typical shallow systems. But also development of DNLP made advances during the last few years, especially in the field of efficiency (Callmeier 2000). A promising solution to improve quality of natural language processing is the combination of deep and shallow technologies. Deep processing benefits from specialized and fast shallow analysis results, shallow processing becomes &apos;deeper&apos; using at least partial results from DNLP. Many natura</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Sailer</author>
<author>Frank Richter</author>
</authors>
<title>Eine XMLKodierung für AVM-Beschreibungen</title>
<date>2001</date>
<booktitle>Proceedings of the Annual Meeting of the Gesellschaft für linguistische Datenverarbeitung, Giessen.</booktitle>
<pages>161--168</pages>
<editor>(in German). In Lobin H. (ed.)</editor>
<contexts>
<context position="19090" citStr="Sailer and Richter (2001)" startWordPosition="2737" endWordPosition="2740"> annotation. Typed feature structures provide a powerful, universal representation for deep linguistic knowledge. While it is in general inefficient to use XML markup to represent typed feature structures during processing (e.g. for unification, subsumption operations), there are several applications that may benefit from a standardized system-independent XML markup of typed feature structures, e.g., as exchange format for • deep NLP component results (e.g. parser chart) • grammar definitions • feature structure visualization or editing tools • feature structure &apos;tree banks&apos; of analysed texts Sailer and Richter (2001) propose an XML markup where the recursive embedding of attribute-value pairs is decomposed into a kind of definite equivalences or non-recursive node lists (triples of node ID, type name and embedded lists of attribute-node pairs). The only advantage we see for this kind of representation is its proximity to a particular kind of feature structure implementation. We adopt an SGML markup for typed feature structures originally developed by the Text Encoding Initiative (TEI) which is very compact and seems to be widely accepted, e.g. also in the Tree Adjoining Grammar community (Issac 1998). Lan</context>
</contexts>
<marker>Sailer, Richter, 2001</marker>
<rawString>Manfred Sailer and Frank Richter. 2001. Eine XMLKodierung für AVM-Beschreibungen (in German). In Lobin H. (ed.) Proceedings of the Annual Meeting of the Gesellschaft für linguistische Datenverarbeitung, Giessen. pp. 161 – 168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>LoPar: Design and Implementation.</title>
<date>2000</date>
<journal>Arbeitspapiere des Sonderforschungsbereiches</journal>
<volume>340</volume>
<institution>University of Stuttgart.</institution>
<contexts>
<context position="15868" citStr="Schmid 2000" startWordPosition="2302" endWordPosition="2303"> more details on motivation and evaluation cf. Frank et al. (2003) and Becker and Frank (2002). The simplified diagram in Figure 3 depicts the components and places where WHAT comes into play in the hybrid integration of deep and shallow processing components (V, N, D denote the WHAT query types). The system takes an input sentence, and runs three shallow systems on it: • the rule-based shallow SPPC (Piskorski and Neumann 2000) for named entity recognition, • TnT/Chunkie, a statistics-based shallow PoS tagger and chunker by (Skut and Brants 1998), • LoPar, a probabilistic context-free parser (Schmid 2000), which takes PoS-tagged tokens as input, and produces binary tree representations of sentence fields, e.g., topo.bin in Fig. 4. For a justification for binary vs. flat trees cf. Becker and Frank (2002). The results of the components are three standoff annotations of the input sentence. Then, a sequence of D-queries is applied to flatten the binary topological field trees (result is topo.flat, Fig. 5), merge with shallow chunk information from Chunkie (topo.chunks, Fig. 6), and apply the main D-query computing bracket information for the deep parser from the merged topo tree (topo.brackets, Fi</context>
</contexts>
<marker>Schmid, 2000</marker>
<rawString>Helmut Schmid. 2000. LoPar: Design and Implementation. Arbeitspapiere des Sonderforschungsbereiches 340, No. 149. University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
<author>Thorsten Brants</author>
</authors>
<title>Chunk tagger – statistical recognition of noun phrases.</title>
<date>1998</date>
<booktitle>In Proceedings of the ESSLLI Workshop on Automated Acquisition of Syntax and Parsing.</booktitle>
<location>Saarbrücken.</location>
<contexts>
<context position="15808" citStr="Skut and Brants 1998" startWordPosition="2292" endWordPosition="2295">d architecture for online analysis of German newspaper sentences. For more details on motivation and evaluation cf. Frank et al. (2003) and Becker and Frank (2002). The simplified diagram in Figure 3 depicts the components and places where WHAT comes into play in the hybrid integration of deep and shallow processing components (V, N, D denote the WHAT query types). The system takes an input sentence, and runs three shallow systems on it: • the rule-based shallow SPPC (Piskorski and Neumann 2000) for named entity recognition, • TnT/Chunkie, a statistics-based shallow PoS tagger and chunker by (Skut and Brants 1998), • LoPar, a probabilistic context-free parser (Schmid 2000), which takes PoS-tagged tokens as input, and produces binary tree representations of sentence fields, e.g., topo.bin in Fig. 4. For a justification for binary vs. flat trees cf. Becker and Frank (2002). The results of the components are three standoff annotations of the input sentence. Then, a sequence of D-queries is applied to flatten the binary topological field trees (result is topo.flat, Fig. 5), merge with shallow chunk information from Chunkie (topo.chunks, Fig. 6), and apply the main D-query computing bracket information for </context>
</contexts>
<marker>Skut, Brants, 1998</marker>
<rawString>Wojciech Skut and Thorsten Brants. 1998. Chunk tagger – statistical recognition of noun phrases. In Proceedings of the ESSLLI Workshop on Automated Acquisition of Syntax and Parsing. Saarbrücken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry S Thompson</author>
<author>David McKelvie</author>
</authors>
<title>Hyperlink Semantics for standoff markup of read-only documents.</title>
<date>1997</date>
<booktitle>In Proc SGML EU</booktitle>
<contexts>
<context position="24180" citStr="Thompson and McKelvie (1997)" startWordPosition="3471" endWordPosition="3474">ations are free to be based on DOM trees or plain XML text input (strings or streams). DOM tree representations are used by XSLT implementations such als libxml/libslt for C/Perl/Python/TCL or Xalan for Java. Hence, DOM implementations of WHAT are preferable in order to avoid unnecessary XML parsing when processing multiple WHAT transformations on the same input and thus help to improve processing speed. As in all programming language, there a multiple solutions for a problem. An XSL profiling tool (e.g. xsltprofiler.org) can help to locate inefficient XSLT code. 3.5 Related Work As argued in Thompson and McKelvie (1997), standoff annotation is a viable solution in order to cope with the combination of multiple overlapping hierarchies and the efficiency problem of XML tree modification for large annotations. Ide (2000) gives an overview of NLP-related XML core technologies that also strives XSLT. We adopt the pragmatic view of Carletta et al. (2002), who see that computational linguistics greatly benefits from general XMLification, namely by getting for free standards and advanced technologies for storing and manipulating XML annotation, mainly through W3C and various open source projects. The trade-off for t</context>
</contexts>
<marker>Thompson, McKelvie, 1997</marker>
<rawString>Henry S. Thompson and David McKelvie. 1997. Hyperlink Semantics for standoff markup of read-only documents. In Proc SGML EU 1997.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>