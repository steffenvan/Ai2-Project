<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.991219">
The Parameter-optimized ATEC Metric for MT Evaluation
</title>
<author confidence="0.99595">
Billy T-M Wong Chunyu Kit
</author>
<affiliation confidence="0.892380333333333">
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Avenue, Kowloon, Hong Kong
</affiliation>
<email confidence="0.996461">
{ctbwong, ctckit}@cityu.edu.hk
</email>
<sectionHeader confidence="0.995619" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998630428571429">
This paper describes the latest version of the
ATEC metric for automatic MT evaluation,
with parameters optimized for word choice
and word order, the two fundamental features
of language that the metric relies on. The
former is assessed by matching at various
linguistic levels and weighting the informa-
tiveness of both matched and unmatched
words. The latter is quantified in term of
word position and information flow. We also
discuss those aspects of language not yet
covered by other existing evaluation metrics
but carefully considered in the formulation of
our metric.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999990979166667">
It is recognized that the proposal of the BLEU
metric (Papineni et al., 2002) has piloted a para-
digm evolution to MT evaluation. It provides a
computable solution to the task and turns it into
an engineering problem of measuring text simi-
larity and simulating human judgments of trans-
lation quality. Related studies in recent years
have extensively revealed more essential charac-
teristics of BLEU, including its strengths and
weaknesses. This has aroused the proposal of
different new evaluation metrics aimed at ad-
dressing such weaknesses so as to find some oth-
er hopefully better alternatives for the task. Ef-
fort in this direction brings up some advanced
metrics such as METEOR (Banerjee and Lavie,
2005) and TERp (Snover et al., 2009) that seem
to have already achieved considerably strong
correlations with human judgments. Nevertheless,
few metrics have really nurtured our understand-
ing of possible parameters involved in our lan-
guage comprehension and text quality judgment.
This inadequacy limits, inevitably, the applica-
tion of the existing metrics.
The ATEC metric (Wong and Kit, 2008) was
developed as a response to this inadequacy, with
a focus to account for the process of human
comprehension of sentences via two fundamental
features of text, namely word choice and word
order. It integrates various explicit measures for
these two features in order to provide an intuitive
and informative evaluation result. Its previous
version (Wong and Kit, 2009b) has already illu-
strated a highly comparable performance to the
few state-of-the-art evaluation metrics, showing
a great improvement over its initial version for
participation in MetricsMATR081. It is also ap-
plied to evaluate online MT systems for legal
translation, to examine its applicability for lay
users’ use to select appropriate MT systems
(Wong and Kit, 2009a).
In this paper we describe the formulation of
ATEC, including its new features and optimiza-
tion of parameters. In particular we will discuss
how the design of this metric can complement
the inadequacies of other metrics in terms of its
treatment of word choice and word order and its
utilization of multiple references in the evalua-
tion process.
</bodyText>
<sectionHeader confidence="0.970928" genericHeader="method">
2 The ATEC Metric
</sectionHeader>
<subsectionHeader confidence="0.990142">
2.1 Word Choice
</subsectionHeader>
<bodyText confidence="0.999564444444444">
In general, word is the basic meaning bearing
unit of language. In a semantic theory such as
Latent Semantic Analysis (LSA) (Landauer et al.,
1998), lexical selection is even the sole consider-
ation of the meaning of a text. A recent study of
the major errors in MT outputs by Vilar et al.
(2006) also reveals that different kinds of error
related to word choices constitute a majority of
error types. It is therefore of prime importance
</bodyText>
<footnote confidence="0.959196">
1 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/2008/
</footnote>
<page confidence="0.94123">
360
</page>
<note confidence="0.4646265">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 360–364,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.98578">
for MT evaluation metrics to diagnose the ade-
quacy of word selection by an MT system.
It is a general consensus that the performance
of an evaluation metric can be improved by
matching more words between MT outputs and
human references. Linguistic resources like
stemmer and WordNet are widely applied by
many metrics for matching word stems and syn-
onyms. ATEC is equipped with these two mod-
ules as well, and furthermore, with two measures
for word similarity, including a WordNet-based
(Wu and Palmer, 1994) and a corpus-based
measure (Landauer et al., 1998) for matching
word pairs of similar meanings. Our previous
work (Wong, 2010) shows that the inclusion of
semantically similar words results in a positive
correlation gain comparable to the use of Word-
Net for synonym identification.
In addition to increasing the number of legiti-
mate matches, we also consider the importance
of each match. Although most metrics score
every matched word with equal weight, different
words indeed contribute different amount of in-
formation to the meaning of a sentence. In Ex-
ample 1 below, both C1 and C2 contain the same
number of words matched with Ref, but the
matches in C1 are more informative and there-
fore should be assigned higher weights.
Example 1
</bodyText>
<listItem confidence="0.796498">
C1: it was not first time that prime minister con-
fronts northern league ...
C2: this is not the prime the operation with the
north ...
</listItem>
<bodyText confidence="0.998218066666667">
Ref: this is not the first time the prime minister
has faced the northern league ...
The informativeness of a match is weighted by
the tf-idf measure, which has been widely used in
information retrieval to assess the relative impor-
tance of a word as an indexing term for a docu-
ment. A word is more important to a document
when it occurs more frequently in this document
and less in others. In ATEC, we have “document”
to refer to “sentence”, the basic text unit in MT
evaluation. This allows a more sensitive measure
for words in different sentences, and gets around
the problem of an evaluation dataset containing
only one or a few long documents. Accordingly,
the tf-idf measure is formulated as:
</bodyText>
<equation confidence="0.977964">
tfidf i j = tf ⋅
( , ) ,
i j
</equation>
<bodyText confidence="0.999970888888889">
where tfi,j is the occurrences of word wi in sen-
tence sj, sfi the number of sentences containing
word wi, and N the total number of sentences in
the evaluation set. In case of a high-frequency
word whose tf-idf weight is less than 1, it is then
rounded up to 1.
In addition to matched words, unmatched
words are also considered to have a role to play
in determining the quality of word choices of an
MT output. As illustrated in Example 1, the un-
matched words in Ref for C1 and C2 are [this  |is
 |the  |the  |has  |faced  |the] and [first  |time  |mi-
nister  |has  |faced  |northern  |league] respective-
ly. One can see that the words missing in C2 are
more significant. It is therefore necessary to ap-
ply the tf-idf weighting to unmatched reference
words so as to quantify the information missed in
the MT outputs in question.
</bodyText>
<subsectionHeader confidence="0.99952">
2.2 Word Order
</subsectionHeader>
<bodyText confidence="0.963596162162162">
In MT evaluation, word order refers to the extent
to which an MT output is interpretable following
the information flow of its reference translation.
It is not rare that an MT output has many
matched words but does not make sense because
of a problematic word order. Currently it is ob-
served that consecutive matches represent a legi-
timate local ordering, causing some metrics to
extend the unit of matching from word to phrase.
Birch et al. (2010) show, however, that the cur-
rent metrics including BLEU, METEOR and
TER are highly lexical oriented and still cannot
distinguish between sentences of different word
orders. This is a serious problem in MT evalua-
tion, for many MT systems have become capable
of generating more and more suitable words in
translations, resulting in that the quality differ-
ence of their outputs lies more and more crucial-
ly in the variances of word order.
ATEC uses three explicit features for word or-
der, namely position distance, order distance and
phrase size. Position distance refers to the diver-
gence of the locations of matches in an MT out-
put and its reference. Example 2 illustrates two
candidates with the same match, whose position
in C1 is closer to its corresponding position in
Ref than that in C2. We conceive this as a signif-
icant indicator of the accuracy of word order: the
closer the positions of a matched word in the
candidate and reference, the better match it is.
Example 2
C1: non-signatories these acts victims but it
caused to incursion transcendant
C2: non-signatories but it caused to incursion
transcendant these acts victims
Ref: there were no victims in this incident but
they did cause massive damage
</bodyText>
<equation confidence="0.90393525">
N
log(
)
sf i
</equation>
<page confidence="0.976285">
361
</page>
<bodyText confidence="0.963626727272727">
The calculation of position distance is based
on the position indices of words in a sentence. In
particular, we align every word in a candidate to
its closest counterpart in a reference. In Example
3, all the candidate words have a match in the
reference. As illustrated by the two “a” in the
candidate, the shortest alignments (strict lines)
are preferred over any farther alternatives (dash
lines). In a case like this, only two matches, i.e.,
thief and police, vary in position by a distance of
3.
</bodyText>
<table confidence="0.4269705">
Example 3
Candidate: a thief chases a police
Pos distance:
Pos index:
</table>
<bodyText confidence="0.995337">
two matches with the same sum of position dis-
tance. However, the matches are in an identical
sequence in 5a but cause a cross in 5b, resulting
in a larger order distance for the latter.
</bodyText>
<table confidence="0.8885678">
Example 5a
Position index 1 2 3 4
Order index 1 2
Candidate: A B C D
Reference: B E D F
Order index 1 2
Position index 1 2 3 4
Position distance (2-1) + (4-3) = 2
Order distance (1-1) + (2-2) = 0
0 3 0 0 3
1 2 3 4 5
Reference: a police chases a thief
Pos index: 1 2 3 4 5
Example 5b
Position index
Order index
Candidate:
1 2 3 4
1 2
A B C D
</table>
<bodyText confidence="0.9982495">
This position distance is sensitive to sentence
length as it simply makes use of word position
indices without any normalization. Example 4
illustrates two cases of different lengths. The po-
sition distance of the bold matched words is 3 in
C1 but 14 in C2. Indeed, the divergence of word
order in C1 does not hinder our understanding,
but in C2 it poses a serious problem. This exces-
sive length inevitably magnifies the interference
effect of word order divergence.
</bodyText>
<equation confidence="0.550908076923077">
Example 4
C1: Short1 and2 various3 international4 news5
R1: International1 news2 brief3
C2: Is1 on2 a3 popular4 the5 very6 in7 Iraq8 to9
those10 just11 like12 other13 world14 in15
which16 young17 people18 with19 the20 and21
flowers22 while23 awareness24 by25 other26
times27 of28 the29 countries30 of31 the32
R2: Valentine’s1 day2 is3 a4 very5 popular6 day7
in8 Iraq9 as10 it11 is12 in13 the14 other15 coun-
tries16 of17 the18 world19. Young20 men21 ex-
change22 with23 their24 girlfriends25 sweets26,
flowers27, perfumes28 and29 other30 gifts31.
</equation>
<bodyText confidence="0.999491555555556">
Another feature, the order distance, concerns
the information flow of a sentence in the form of
the sequence of matches. Each match in a candi-
date and a reference is first assigned an order
index in a sequential manner. Then, the differ-
ence of two counterpart indices is measured, so
as to see if a variance exists. Examples 5a and 5b
exemplify two kinds of order distance and their
corresponding position distance. Both cases have
</bodyText>
<figure confidence="0.3786648">
Reference: C B E F
Order index 1 2
Position index 1 2 3 4
Position distance (2-2) + (3-1) = 2
Order distance (2-1) + (2-1) = 2
</figure>
<bodyText confidence="0.998202375">
In practice, ATEC operates on phrases like
many other metrics. But unlike these metrics that
count only the number of matched phrases,
ATEC gives extra credit to a longer phrase to
reward its valid word sequence. In Example 6,
C1 and C2 represent two MT outputs of the same
length, with matched words underlined. Both
have 10 matches in 3 phrases, and will receive
the same evaluation score from a metric like
METEOR or TERp, ignoring the subtle differ-
ence in the sizes of the matched phrases, which
are [8,1,1] and [4,3,3] words for C1 and C2 re-
spectively. In contrast, ATEC uses the size of a
phrase as a reduction factor to its position dis-
tance, so as to raise the contribution of a larger
phrase to the metric score.
</bodyText>
<equation confidence="0.48046">
Example 6
C1: w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13
C2: w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13
</equation>
<subsectionHeader confidence="0.996971">
2.3 Multiple References
</subsectionHeader>
<bodyText confidence="0.9998508">
The availability of multiple references allows
more legitimate word choices and word order of
an MT output to be accounted. Some existing
metrics only compute the scores of a candidate
against each reference and select the highest one.
</bodyText>
<page confidence="0.996484">
362
</page>
<bodyText confidence="0.970187636363636">
This deficit can be illustrated by a well-known
example from Papineni et al. (2002), as repli-
cated in Example 7 with slight modification. It
shows that nearly all candidate words can find
their matches in either reference. However, if we
resort to single reference, only around half of
them can have a match, which would seriously
underrate the quality of the candidate.
Example 7
commands of the party.
ATEC exploits multiple references in this fa-
shion to maximize the number of matches in a
candidate. It begins with aligning the longest
matches with either reference. The one with the
shortest position distance is preferred if more
than one alternative available in the same phrase
size. This process repeats until no more candi-
date word can find a match.
an exponential factor e, in proportion to the
length of candidate |c|.
The score of a matched phrase is then
computed by
</bodyText>
<equation confidence="0.447971">
Phrase
otherwise,
</equation>
<bodyText confidence="0.991830333333333">
Limitdis is an upper limit for the distance penalty.
Accordingly, the score C of all phrases in a can-
didate is
</bodyText>
<equation confidence="0.9704615">
C = ∑ Phrase j
j {candidate}
∈
.
</equation>
<bodyText confidence="0.99968">
Then, we move on to calculating the informa-
tion load of unmatched reference words Wunmatch,
approximated as
</bodyText>
<equation confidence="0.5004765">
Info atch
unmWunmatch ∑ (wtypetfidfk
</equation>
<bodyText confidence="0.927310666666667">
The overall score M accounting for both the
matched and unmatched is defined as
if Wunmatch &gt; C·LimitInfo;
otherwise,
LimitInfo is an upper limit for the information
penalty of the unmatched words.
Finally, the ATEC score is computed using the
conventional F-measure in terms of precision P
and recall R as
</bodyText>
<figure confidence="0.986169729166666">
C:
ensures that the
It is a guide to action which
party.
guarantees
R1: It is a guide to action that ensures that the
R2: It is the guiding principle which
military always
military will forever heed Party commands.
obeys the commands of the
the military forces always being under the
⎧
⎨
⎩
=
−
Dis
,
Wmatch
∈
k
{unmatch}
)
.
,
⋅ Limit
C
Info
=
M
−
C
,
⎧
⎨
⎩
Wunmatch
2.4 Formulation of ATEC ATEC = PR
)
R
α + (1 − α
P
M
R=
|
|r
.
,
</figure>
<bodyText confidence="0.999709">
The computation of an ATEC score begins with
alignment of phrases, as described above. For
each matched phase, we first sum up the score of
each word i in the phrase as
</bodyText>
<equation confidence="0.731351">
Infomatch
Wmatch = i∈{phrase} (wtype−tfidfi )
</equation>
<bodyText confidence="0.974599791666667">
where wtype refers to a basic score of a matched
word depending on its match type. It is then
minus its information load, i.e., the tf-idf score of
the matched word with a weight factor, Infomatch.
There is also a distance penalty for a phrase,
,
where dispos and disorder refer to the position
distance and order distance, and wpos and worder
are their corresponding weight factors,
respectively. The position distance is further
weighted according to the size of phrase |p |with
P= M
where  |c |
The parameter α adjusts the weights of P and R,
and |c |and |r |refer to the length of candidate and
reference, respectively. In the case of multiple
references, |r |refers to the average length of ref-
erences.
We have derived the optimized values for the
parameters involved in ATEC calculation using
the development data of NIST MetricsMATR10
with adequacy assessments by a simple hill
climbing approach. The optimal parameter set-
ting is presented in Table 1 below.
</bodyText>
<sectionHeader confidence="0.999529" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<subsectionHeader confidence="0.69525">
Wmatch ⋅ Limitdis, if Dis &gt; Wmatch·Limitdis;
</subsectionHeader>
<bodyText confidence="0.9996884">
In the above sections we have presented the lat-
est version of our ATEC metric with particular
emphasis on word choice and word order as two
fundamental features of language. Each of these
features contains multiple parameters intended to
</bodyText>
<figure confidence="0.9470198">
Dis = wposdispos (1  |p  |) +worderdis
 ||
c
e
order
</figure>
<page confidence="0.591296">
363
</page>
<figure confidence="0.847680769230769">
Parameters Values
wtype 1 (exact match),
0.95 (stem / synonym /
semantically close),
0.15 (unmatch)
Infomatch 0.34
Infounmatch 0.26
wpos 0.02
worder 0.15
e 1.1
Limitdis 0.95
LimitInfo 0.5
α 0.5
</figure>
<tableCaption confidence="0.957392">
Table 1 Optimal parameter values for ATEC
</tableCaption>
<bodyText confidence="0.9984868">
have a comprehensive coverage of different tex-
tual factors involved in our interpretation of a
sentence. The optimal offsetting for the parame-
ters is expected to report an empirical observa-
tion of the relative merits of each factor in ade-
quacy assessment. We are currently exploring
their relation with the errors of MT outputs, to
examine the potential of automatic error analysis.
The ATEC package is obtainable at:
http://mega.ctl.cityu.edu.hk/ctbwong/ATEC/
</bodyText>
<sectionHeader confidence="0.997761" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9973945">
The research work described in this paper was
supported by City University of Hong Kong
through the Strategic Research Grant (SRG)
7002267.
</bodyText>
<sectionHeader confidence="0.999045" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998658120689655">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. Pro-
ceedings of Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summariza-
tion at the 43th Annual Meeting of the Association
of Computational Linguistics (ACL), pages 65-72,
Ann Arbor, Michigan, June 2005.
Alexandra Birch, Miles Osborne and Phil Blunsom.
2010. Metrics for MT Evaluation: Evaluating
Reordering. Machine Translation (forthcoming).
Thomas Landauer, Peter W. Foltz and Darrell Laham.
1998. Introduction to Latent Semantic Analysis.
Discourse Processes 25: 259–284.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. Proceed-
ings of 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311–318,
Philadelphia, PA, July 2002.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments
with a Tunable MT Metric. Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion at the 12th Meeting of the European Chapter
of the Association for Computational Linguistics
(EACL), pages 259-268, Athens, Greece, March,
2009.
David Vilar, Jia Xu, Luis Fernando D&apos;Haro and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. Proceedings of the 5th
International Conference on Language Resources
and Evaluation (LREC), pages 697-702, Genova,
Italy, May 2006.
Billy T-M Wong. 2010. Semantic Evaluation of Ma-
chine Translation. Proceedings of the 7th Interna-
tional Conference on Language Resources and
Evaluation (LREC), Valletta, Malta, May, 2010.
Billy T-M Wong and Chunyu Kit. 2008. Word choice
and Word Position for Automatic MT Evaluation.
AMTA 2008 Workshop: MetricsMATR, 3 pages,
Waikiki, Hawai&apos;i, October, 2008.
Billy T-M Wong and Chunyu Kit. 2009a. Meta-
Evaluation of Machine Translation on Legal Texts.
Proceedings of the 22nd International Conference
on the Computer Processing of Oriental Languag-
es (ICCPOL), pages 343-350, Hong Kong, March,
2009.
Billy Wong and Chunyu Kit. 2009b. ATEC: Automat-
ic Evaluation of Machine Translation via Word
Choice and Word Order. Machine Translation,
23(2):141-155.
Zhibiao Wu and Martha Palmer. 1994. Verb Seman-
tics and Lexical Selection. Proceedings of the 32nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 133-138, Las Cruces,
New Mexico.
</reference>
<page confidence="0.998926">
364
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.638654">
<title confidence="0.999889">The Parameter-optimized ATEC Metric for MT Evaluation</title>
<author confidence="0.999661">Billy T-M Wong Chunyu Kit</author>
<affiliation confidence="0.9076275">Department of Chinese, Translation and City University of Hong Kong</affiliation>
<address confidence="0.731577">Tat Chee Avenue, Kowloon, Hong Kong</address>
<email confidence="0.982818">ctbwong@cityu.edu.hk</email>
<email confidence="0.982818">ctckit@cityu.edu.hk</email>
<abstract confidence="0.9996942">This paper describes the latest version of the ATEC metric for automatic MT evaluation, with parameters optimized for word choice and word order, the two fundamental features of language that the metric relies on. The former is assessed by matching at various linguistic levels and weighting the informativeness of both matched and unmatched words. The latter is quantified in term of word position and information flow. We also discuss those aspects of language not yet covered by other existing evaluation metrics but carefully considered in the formulation of our metric.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at the 43th Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1533" citStr="Banerjee and Lavie, 2005" startWordPosition="237" endWordPosition="240">piloted a paradigm evolution to MT evaluation. It provides a computable solution to the task and turns it into an engineering problem of measuring text similarity and simulating human judgments of translation quality. Related studies in recent years have extensively revealed more essential characteristics of BLEU, including its strengths and weaknesses. This has aroused the proposal of different new evaluation metrics aimed at addressing such weaknesses so as to find some other hopefully better alternatives for the task. Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al., 2009) that seem to have already achieved considerably strong correlations with human judgments. Nevertheless, few metrics have really nurtured our understanding of possible parameters involved in our language comprehension and text quality judgment. This inadequacy limits, inevitably, the application of the existing metrics. The ATEC metric (Wong and Kit, 2008) was developed as a response to this inadequacy, with a focus to account for the process of human comprehension of sentences via two fundamental features of text, namely word choice and word order. It integrates</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at the 43th Annual Meeting of the Association of Computational Linguistics (ACL), pages 65-72, Ann Arbor, Michigan, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Phil Blunsom</author>
</authors>
<title>Metrics for MT Evaluation: Evaluating Reordering. Machine Translation (forthcoming).</title>
<date>2010</date>
<contexts>
<context position="7119" citStr="Birch et al. (2010)" startWordPosition="1180" endWordPosition="1183">t is therefore necessary to apply the tf-idf weighting to unmatched reference words so as to quantify the information missed in the MT outputs in question. 2.2 Word Order In MT evaluation, word order refers to the extent to which an MT output is interpretable following the information flow of its reference translation. It is not rare that an MT output has many matched words but does not make sense because of a problematic word order. Currently it is observed that consecutive matches represent a legitimate local ordering, causing some metrics to extend the unit of matching from word to phrase. Birch et al. (2010) show, however, that the current metrics including BLEU, METEOR and TER are highly lexical oriented and still cannot distinguish between sentences of different word orders. This is a serious problem in MT evaluation, for many MT systems have become capable of generating more and more suitable words in translations, resulting in that the quality difference of their outputs lies more and more crucially in the variances of word order. ATEC uses three explicit features for word order, namely position distance, order distance and phrase size. Position distance refers to the divergence of the locati</context>
</contexts>
<marker>Birch, Osborne, Blunsom, 2010</marker>
<rawString>Alexandra Birch, Miles Osborne and Phil Blunsom. 2010. Metrics for MT Evaluation: Evaluating Reordering. Machine Translation (forthcoming).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes</booktitle>
<volume>25</volume>
<pages>259--284</pages>
<contexts>
<context position="3190" citStr="Landauer et al., 1998" startWordPosition="500" endWordPosition="503">ne its applicability for lay users’ use to select appropriate MT systems (Wong and Kit, 2009a). In this paper we describe the formulation of ATEC, including its new features and optimization of parameters. In particular we will discuss how the design of this metric can complement the inadequacies of other metrics in terms of its treatment of word choice and word order and its utilization of multiple references in the evaluation process. 2 The ATEC Metric 2.1 Word Choice In general, word is the basic meaning bearing unit of language. In a semantic theory such as Latent Semantic Analysis (LSA) (Landauer et al., 1998), lexical selection is even the sole consideration of the meaning of a text. A recent study of the major errors in MT outputs by Vilar et al. (2006) also reveals that different kinds of error related to word choices constitute a majority of error types. It is therefore of prime importance 1 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/2008/ 360 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 360–364, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics for MT evaluation metrics to diagnose the adequacy of word s</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas Landauer, Peter W. Foltz and Darrell Laham. 1998. Introduction to Latent Semantic Analysis. Discourse Processes 25: 259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="903" citStr="Papineni et al., 2002" startWordPosition="135" endWordPosition="138">e ATEC metric for automatic MT evaluation, with parameters optimized for word choice and word order, the two fundamental features of language that the metric relies on. The former is assessed by matching at various linguistic levels and weighting the informativeness of both matched and unmatched words. The latter is quantified in term of word position and information flow. We also discuss those aspects of language not yet covered by other existing evaluation metrics but carefully considered in the formulation of our metric. 1 Introduction It is recognized that the proposal of the BLEU metric (Papineni et al., 2002) has piloted a paradigm evolution to MT evaluation. It provides a computable solution to the task and turns it into an engineering problem of measuring text similarity and simulating human judgments of translation quality. Related studies in recent years have extensively revealed more essential characteristics of BLEU, including its strengths and weaknesses. This has aroused the proposal of different new evaluation metrics aimed at addressing such weaknesses so as to find some other hopefully better alternatives for the task. Effort in this direction brings up some advanced metrics such as MET</context>
<context position="12189" citStr="Papineni et al. (2002)" startWordPosition="2090" endWordPosition="2093">ively. In contrast, ATEC uses the size of a phrase as a reduction factor to its position distance, so as to raise the contribution of a larger phrase to the metric score. Example 6 C1: w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13 C2: w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13 2.3 Multiple References The availability of multiple references allows more legitimate word choices and word order of an MT output to be accounted. Some existing metrics only compute the scores of a candidate against each reference and select the highest one. 362 This deficit can be illustrated by a well-known example from Papineni et al. (2002), as replicated in Example 7 with slight modification. It shows that nearly all candidate words can find their matches in either reference. However, if we resort to single reference, only around half of them can have a match, which would seriously underrate the quality of the candidate. Example 7 commands of the party. ATEC exploits multiple references in this fashion to maximize the number of matches in a candidate. It begins with aligning the longest matches with either reference. The one with the shortest position distance is preferred if more than one alternative available in the same phra</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric.</title>
<date>2009</date>
<booktitle>Proceedings of the Fourth Workshop on Statistical Machine Translation at the 12th Meeting of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>259--268</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="1564" citStr="Snover et al., 2009" startWordPosition="243" endWordPosition="246">valuation. It provides a computable solution to the task and turns it into an engineering problem of measuring text similarity and simulating human judgments of translation quality. Related studies in recent years have extensively revealed more essential characteristics of BLEU, including its strengths and weaknesses. This has aroused the proposal of different new evaluation metrics aimed at addressing such weaknesses so as to find some other hopefully better alternatives for the task. Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al., 2009) that seem to have already achieved considerably strong correlations with human judgments. Nevertheless, few metrics have really nurtured our understanding of possible parameters involved in our language comprehension and text quality judgment. This inadequacy limits, inevitably, the application of the existing metrics. The ATEC metric (Wong and Kit, 2008) was developed as a response to this inadequacy, with a focus to account for the process of human comprehension of sentences via two fundamental features of text, namely word choice and word order. It integrates various explicit measures for </context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric. Proceedings of the Fourth Workshop on Statistical Machine Translation at the 12th Meeting of the European Chapter of the Association for Computational Linguistics (EACL), pages 259-268, Athens, Greece, March, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jia Xu</author>
<author>Luis Fernando D&apos;Haro</author>
<author>Hermann Ney</author>
</authors>
<title>Error Analysis of Statistical Machine Translation Output.</title>
<date>2006</date>
<booktitle>Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>697--702</pages>
<location>Genova, Italy,</location>
<contexts>
<context position="3338" citStr="Vilar et al. (2006)" startWordPosition="529" endWordPosition="532">uding its new features and optimization of parameters. In particular we will discuss how the design of this metric can complement the inadequacies of other metrics in terms of its treatment of word choice and word order and its utilization of multiple references in the evaluation process. 2 The ATEC Metric 2.1 Word Choice In general, word is the basic meaning bearing unit of language. In a semantic theory such as Latent Semantic Analysis (LSA) (Landauer et al., 1998), lexical selection is even the sole consideration of the meaning of a text. A recent study of the major errors in MT outputs by Vilar et al. (2006) also reveals that different kinds of error related to word choices constitute a majority of error types. It is therefore of prime importance 1 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/2008/ 360 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 360–364, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics for MT evaluation metrics to diagnose the adequacy of word selection by an MT system. It is a general consensus that the performance of an evaluation metric can be improved by matching more words between MT o</context>
</contexts>
<marker>Vilar, Xu, D&apos;Haro, Ney, 2006</marker>
<rawString>David Vilar, Jia Xu, Luis Fernando D&apos;Haro and Hermann Ney. 2006. Error Analysis of Statistical Machine Translation Output. Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), pages 697-702, Genova, Italy, May 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy T-M Wong</author>
</authors>
<title>Semantic Evaluation of Machine Translation.</title>
<date>2010</date>
<booktitle>Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="4365" citStr="Wong, 2010" startWordPosition="686" endWordPosition="687">s to diagnose the adequacy of word selection by an MT system. It is a general consensus that the performance of an evaluation metric can be improved by matching more words between MT outputs and human references. Linguistic resources like stemmer and WordNet are widely applied by many metrics for matching word stems and synonyms. ATEC is equipped with these two modules as well, and furthermore, with two measures for word similarity, including a WordNet-based (Wu and Palmer, 1994) and a corpus-based measure (Landauer et al., 1998) for matching word pairs of similar meanings. Our previous work (Wong, 2010) shows that the inclusion of semantically similar words results in a positive correlation gain comparable to the use of WordNet for synonym identification. In addition to increasing the number of legitimate matches, we also consider the importance of each match. Although most metrics score every matched word with equal weight, different words indeed contribute different amount of information to the meaning of a sentence. In Example 1 below, both C1 and C2 contain the same number of words matched with Ref, but the matches in C1 are more informative and therefore should be assigned higher weight</context>
</contexts>
<marker>Wong, 2010</marker>
<rawString>Billy T-M Wong. 2010. Semantic Evaluation of Machine Translation. Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC), Valletta, Malta, May, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy T-M Wong</author>
<author>Chunyu Kit</author>
</authors>
<title>Word choice and Word Position for Automatic MT Evaluation. AMTA</title>
<date>2008</date>
<journal>Workshop: MetricsMATR,</journal>
<volume>3</volume>
<pages>pages,</pages>
<location>Waikiki, Hawai&apos;i,</location>
<contexts>
<context position="1922" citStr="Wong and Kit, 2008" startWordPosition="295" endWordPosition="298">ent new evaluation metrics aimed at addressing such weaknesses so as to find some other hopefully better alternatives for the task. Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al., 2009) that seem to have already achieved considerably strong correlations with human judgments. Nevertheless, few metrics have really nurtured our understanding of possible parameters involved in our language comprehension and text quality judgment. This inadequacy limits, inevitably, the application of the existing metrics. The ATEC metric (Wong and Kit, 2008) was developed as a response to this inadequacy, with a focus to account for the process of human comprehension of sentences via two fundamental features of text, namely word choice and word order. It integrates various explicit measures for these two features in order to provide an intuitive and informative evaluation result. Its previous version (Wong and Kit, 2009b) has already illustrated a highly comparable performance to the few state-of-the-art evaluation metrics, showing a great improvement over its initial version for participation in MetricsMATR081. It is also applied to evaluate onl</context>
</contexts>
<marker>Wong, Kit, 2008</marker>
<rawString>Billy T-M Wong and Chunyu Kit. 2008. Word choice and Word Position for Automatic MT Evaluation. AMTA 2008 Workshop: MetricsMATR, 3 pages, Waikiki, Hawai&apos;i, October, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy T-M Wong</author>
<author>Chunyu Kit</author>
</authors>
<title>MetaEvaluation of Machine Translation on Legal Texts.</title>
<date>2009</date>
<booktitle>Proceedings of the 22nd International Conference on the Computer Processing of Oriental Languages (ICCPOL),</booktitle>
<pages>343--350</pages>
<location>Hong Kong,</location>
<contexts>
<context position="2291" citStr="Wong and Kit, 2009" startWordPosition="354" endWordPosition="357">metrics have really nurtured our understanding of possible parameters involved in our language comprehension and text quality judgment. This inadequacy limits, inevitably, the application of the existing metrics. The ATEC metric (Wong and Kit, 2008) was developed as a response to this inadequacy, with a focus to account for the process of human comprehension of sentences via two fundamental features of text, namely word choice and word order. It integrates various explicit measures for these two features in order to provide an intuitive and informative evaluation result. Its previous version (Wong and Kit, 2009b) has already illustrated a highly comparable performance to the few state-of-the-art evaluation metrics, showing a great improvement over its initial version for participation in MetricsMATR081. It is also applied to evaluate online MT systems for legal translation, to examine its applicability for lay users’ use to select appropriate MT systems (Wong and Kit, 2009a). In this paper we describe the formulation of ATEC, including its new features and optimization of parameters. In particular we will discuss how the design of this metric can complement the inadequacies of other metrics in terms</context>
</contexts>
<marker>Wong, Kit, 2009</marker>
<rawString>Billy T-M Wong and Chunyu Kit. 2009a. MetaEvaluation of Machine Translation on Legal Texts. Proceedings of the 22nd International Conference on the Computer Processing of Oriental Languages (ICCPOL), pages 343-350, Hong Kong, March, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy Wong</author>
<author>Chunyu Kit</author>
</authors>
<title>ATEC: Automatic Evaluation of Machine Translation via Word Choice and Word Order.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="2291" citStr="Wong and Kit, 2009" startWordPosition="354" endWordPosition="357">metrics have really nurtured our understanding of possible parameters involved in our language comprehension and text quality judgment. This inadequacy limits, inevitably, the application of the existing metrics. The ATEC metric (Wong and Kit, 2008) was developed as a response to this inadequacy, with a focus to account for the process of human comprehension of sentences via two fundamental features of text, namely word choice and word order. It integrates various explicit measures for these two features in order to provide an intuitive and informative evaluation result. Its previous version (Wong and Kit, 2009b) has already illustrated a highly comparable performance to the few state-of-the-art evaluation metrics, showing a great improvement over its initial version for participation in MetricsMATR081. It is also applied to evaluate online MT systems for legal translation, to examine its applicability for lay users’ use to select appropriate MT systems (Wong and Kit, 2009a). In this paper we describe the formulation of ATEC, including its new features and optimization of parameters. In particular we will discuss how the design of this metric can complement the inadequacies of other metrics in terms</context>
</contexts>
<marker>Wong, Kit, 2009</marker>
<rawString>Billy Wong and Chunyu Kit. 2009b. ATEC: Automatic Evaluation of Machine Translation via Word Choice and Word Order. Machine Translation, 23(2):141-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb Semantics and Lexical Selection.</title>
<date>1994</date>
<booktitle>Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="4238" citStr="Wu and Palmer, 1994" startWordPosition="664" endWordPosition="667"> MetricsMATR, pages 360–364, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics for MT evaluation metrics to diagnose the adequacy of word selection by an MT system. It is a general consensus that the performance of an evaluation metric can be improved by matching more words between MT outputs and human references. Linguistic resources like stemmer and WordNet are widely applied by many metrics for matching word stems and synonyms. ATEC is equipped with these two modules as well, and furthermore, with two measures for word similarity, including a WordNet-based (Wu and Palmer, 1994) and a corpus-based measure (Landauer et al., 1998) for matching word pairs of similar meanings. Our previous work (Wong, 2010) shows that the inclusion of semantically similar words results in a positive correlation gain comparable to the use of WordNet for synonym identification. In addition to increasing the number of legitimate matches, we also consider the importance of each match. Although most metrics score every matched word with equal weight, different words indeed contribute different amount of information to the meaning of a sentence. In Example 1 below, both C1 and C2 contain the s</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verb Semantics and Lexical Selection. Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 133-138, Las Cruces, New Mexico.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>