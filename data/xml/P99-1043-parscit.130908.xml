<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000339">
<title confidence="0.983624">
Mixed Language Query Disambiguation
</title>
<author confidence="0.879827">
Pascale FUNG, LIU Xiaohu and CHEUNG Chi Shun
</author>
<affiliation confidence="0.8818524">
HKUST
Human Language Technology Center
Department of Electrical and Electronic Engineering
University of Science and Technology, HKUST
Clear Water Bay, Hong Kong
</affiliation>
<email confidence="0.636075">
fpascale,lxiaohu,eepercylfte.ust.hk
</email>
<sectionHeader confidence="0.993796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999718714285714">
We propose a mixed language query disam-
biguation approach by using co-occurrence in-
formation from monolingual data only. A
mixed language query consists of words in a
primary language and a secondary language.
Our method translates the query into mono-
lingual queries in either language. Two novel
features for disambiguation, namely contextual
word voting and 1-best contextual word, are in-
troduced and compared to a baseline feature,
the nearest neighbor. Average query transla-
tion accuracy for the two features are 81.37%
and 83.72%, compared to the baseline accuracy
of 75.50%.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969525423729">
Online information retrieval is now prevalent
because of the ubiquitous World Wide Web.
The Web is also a powerful platform for another
application—interactive spoken language query
systems. Traditionally, such systems were im-
plemented on stand-alone kiosks. Now we can
easily use the Web as a platform. Information
such as airline schedules, movie reservation, car
trading, etc., can all be included in HTML files,
to be accessed by a generic spoken interface to
the Web browser (Zue, 1995; DiDio, 1997; Ray-
mond, 1997; Fang et al., 1998a). Our team
has built a multilingual spoken language inter-
face to the Web, named SALSA (Fung et al.,
1998b; Fung et al., 1998a; Ma and Fung, 1998).
Users can use speech to surf the net via vari-
ous links as well as issue search commands such
as &amp;quot;Show me the latest movie of Jacky Chan&amp;quot;.
The system recognizes commands and queries
in English, Mandarin and Cantonese, as well as
mixed language sentences.
Until recently, most of the search engines han-
dle keyword based queries where the user types
in a series of strings without syntactic structure.
The choice of key words in this case determines
the success rate of the search. In many situa-
tions, the key words are ambiguous.
To resolve ambiguity, query expansion is usu-
ally employed to look for additional keywords.
We believe that a more useful search engine
should allow the user to input natural lan-
guage sentences. Sentence-based queries are
useful because (1) they are more natural to the
user and (2) more importantly, they provide
more contextual information which are impor-
tant for query understanding. To date, the few
sentence-based search engines do not seem to
take advantage of context information in the
query, but merely extracting key words from the
query sentence (AskJeeves, 1998; ElectricMonk,
1998).
In addition to the need for better query un-
derstanding methods for a large variety of do-
mains, it has also become important to han-
dle queries in different languages. Cross-
language information retrieval has emerged
as an important area as the amount of non-
English material is ever increasing (Oard, 1997;
Grefenstette, 1998; Ballesteros and Croft, 1998;
Picchi and Peters, 1998; Davis, 1998; Hull and
Grefenstette, 1996). One of the important tasks
of cross-language IR is to translate queries from
one language to another. The original query
and the translated query are then used to match
documents in both the source and target lan-
guages. Target language documents are either
glossed or translated by other systems. Accord-
ing to (Grefenstette, 1998), three main prob-
lems of query translations are:
</bodyText>
<listItem confidence="0.9760115">
1. generating translation candidates,
2. weighting translation candidates, and
</listItem>
<page confidence="0.98432">
333
</page>
<listItem confidence="0.5739295">
3. pruning translation alternatives for docu-
ment matching.
</listItem>
<bodyText confidence="0.999926490909091">
In cross-language IR, key word disambigua-
tion is even more critical than in monolin-
gual IR (Ballesteros and Croft, 1998) since the
wrong translation can lead to a large amount
of garbage documents in the target language, in
addition to the garbage documents in the source
language. Once again, we believe that sentence-
based queries provide more information than
mere key words in cross-language IR.
In both monolingual IR and cross-language
IR, the query sentence or key words are as-
sumed to be consistently in one language only.
This makes sense in cases where the user is more
likely to be a monolingual person who is looking
for information in any language. It is also eas-
ier to implement a monolingual search engine.
However, we suggest that the typical user of a
cross-language IR system is likely to be bilin-
gual to some extent. Most Web users in the
world know some English. In fact, since En-
glish still constitutes 88% of the current web
pages, speakers of another language would like
to find English contents as well as contents in
their own language. Likewise, English speakers
might want to find information in another lan-
guage. A typical example is a Chinese user look-
ing for the information of an American movie,
s/he might not know the Chinese name of that
movie. His/her query for this movie is likely to
be in mixed language.
Mixed language query is also prevalent in
spoken language. We have observed this to
be a common phenomenon among users of our
SALSA system. The colloquial Hong Kong lan-
guage is Cantonese with mixed English words.
In general, a mixed language consists of a sen-
tence mostly in the primary language with some
words in a secondary language. We are inter-
ested in translating such mixed language queries
into monolingual queries unambiguously.
In this paper, we propose a mixed language
query disambiguation approach which makes
use of the co-occurrence information of words
between those in the primary language and
those in the secondary language. We describe
the overall methodology in Section 2. In Sec-
tions 2.1-3, we present the solutions to the three
disambiguation problems. In Section 2.3 we
present three different discriminative features
for disambiguation, ranging from the baseline
model (Section 2.3.1), to the voting scheme
(Section 2.3.2), and finally the 1-best model
(Section 2.3.3). We describe our evaluation ex-
periments in Section 3, and present the results
in Section 4. We then conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.995051" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.998685421052632">
Mixed language query translation is halfway be-
tween query translation and query disambigua-
tion in that not all words in the query need to
be translated.
There are two ways to use the disambiguated
mixed language queries. In one scenario, all
secondary language words are translated unam-
biguously into the primary language, and the
resulting monolingual query is processed by a
general IR system. In another scenario, the
primary language words are converted into sec-
ondary language and the query is passed to
another IR system in the secondary language.
Our methods allows for both general and cross-
language IR from a mixed language query.
To draw a parallel to the three problems of
query translation, we suggest that the three
main problems of mixed language disambigua-
tion are:
</bodyText>
<listItem confidence="0.997824">
1. generating translation candidates in the
primary language,
2. weighting translation candidates, and
3. pruning translation alternatives for query
translation.
</listItem>
<bodyText confidence="0.9992975625">
Co-occurrence information between neighbor-
ing words and words in the same sentence
has been used in phrase extraction (Smadja,
1993; Fung and Wu, 1994), phrasal translation
(Smadja et al., 1996; Kupiec, 1993; Wu, 1995;
Dagan and Church, 1994), target word selection
(Liu and Li, 1997; Tanaka and Iwasaki, 1996),
domain word translation (Fung and Lo, 1998;
Fung, 1998), sense disambiguation (Brown et
al., 1991; Dagan et al., 1991; Dagan and Itai,
1994; Gale et al., 1992a; Gale et al., 1992b; Gale
et al., 1992c; Shiltze, 1992; Gale et al., 1993;
Yarowsky, 1995), and even recently for query
translation in cross-language IR as well (Banes-
teros and Croft, 1998). Co-occurrence statistics
is collected from either bilingual parallel and
</bodyText>
<page confidence="0.998291">
334
</page>
<bodyText confidence="0.9998811">
non-parallel corpora (Smadja et al., 1996; Ku-
piec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996;
Fung and Lo, 1998), or monolingual corpora
(Smadja, 1993; Fung and Wu, 1994; Liu and
Li, 1997; Shiitze, 1992; Yarowsky, 1995). As
we noted in (Fung and Lo, 1998; Fung, 1998),
parallel corpora are rare in most domains. We
want to devise a method that uses only mono-
lingual data in the primary language to train
co-occurrence information.
</bodyText>
<subsectionHeader confidence="0.996555">
2.1 Translation candidate generation
</subsectionHeader>
<bodyText confidence="0.989984428571429">
Without loss of generality, we suppose the
mixed language sentence consists of the words
S E2, , C, . ,E}, where C is the
only secondary language word&apos;. Since in our
method we want to find the co-occurrence in-
formation between all Ei and C from a mono-
lingual corpus, we need to translate the lat-
ter into the primary language word E. This
corresponds to the first problem in query
translation—translation candidate generation.
We generate translation candidates of C via an
online bilingual dictionary. All translations of
secondary language word C , comprising of mul-
tiple senses, are taken together as a set { Ec2}.
</bodyText>
<subsectionHeader confidence="0.995616">
2.2 Translation candidate weighting
</subsectionHeader>
<bodyText confidence="0.99132475">
Problem two in query translation is to weight
all translation candidates for C. In our method,
the weights are based on co-occurrence informa-
tion. The hypothesis is that the correct transla-
tions of C should co-occur frequently with the
contextual words Ei and incorrect translation
of C should co-occur rarely with the contex-
tual words. Obviously, other information such
as syntactical relationship between words or the
part-of-speech tags could be used as weights too.
However, it is difficult to parse and tag a mixed
language sentence. The only information we
can use to disambiguate C is the co-occurrence
information between its translation candidates
{E,2} and El, E2, , En.
Mutual information is a good measure of the
co-occurrence relationship between two words
(Gale and Church, 1993). We first compute the
mutual information between any word pair from
a monolingual corpus in the primary language 2
</bodyText>
<footnote confidence="0.77687275">
11n actual experiments, each sentence can contain
multiple secondary language words
2This corpus does not need to be in the same domain
as the testing data
</footnote>
<bodyText confidence="0.998413">
using the following formula, where E is a word
and f (E) is the frequency of word E.
</bodyText>
<equation confidence="0.988285">
,
M I (E,, Ej) = f (E, Ej) log f (E,) * f(E3)
(1)
</equation>
<bodyText confidence="0.9965675">
E, and Ej can be either neighboring words or
any two words in the sentence.
</bodyText>
<subsectionHeader confidence="0.994389">
2.3 Translation candidate pruning
</subsectionHeader>
<bodyText confidence="0.985425058823529">
The last problem in query translation is select-
ing the target translation. In our approach, we
need to choose a particular Ec from E. We
call this pruning process translation disam-
biguation.
We present and compare three unsupervised
statistical methods in this paper. The first base-
line method is similar to (Dagan et al., 1991;
Dagan and Itai, 1994; Ballesteros and Croft,
1998; Smadja et al., 1996), where we use the
nearest neighboring word of the secondary lan-
guage word C as feature for disambiguation.
In the second method, we choose all contex-
tual words as disambiguating feature. In the
third method, the most discriminative contex-
tual word is selected as feature.
2.3.1 Baseline: single neighboring word
as disambiguating feature
The first disambiguating feature we present here
is similar to the statistical feature in (Dagan et
al., 1991; Smadja et al., 1996; Dagan and Itai,
1994; Ballesteros and Croft, 1998), namely the
co-occurrence with neighboring words. We do
not use any syntactic relationship as in (Dagan
and Itai, 1994) because such relationship is not
available for mixed-language sentences. The as-
sumption here is that the most powerful word
for disambiguating a word is the one next to it.
Based on mutual information, the primary lan-
guage target word for C is chosen from the set
{Ecj. Suppose the nearest neighboring word
for C in S is Ey , we select the target word
such that the mutual information between Ecf
and Ey is maximum.
</bodyText>
<equation confidence="0.99805">
r = argmax,MI(Eci, Ey) (2)
</equation>
<bodyText confidence="0.994223">
Ey is taken to be either the left or the right
neighbor of our target word.
This idea is illustrated in Figure 1. Mu, rep-
resented by the solid line, is greater than MI2,
</bodyText>
<page confidence="0.99227">
335
</page>
<figure confidence="0.980620111111111">
Word in the secondary language
Word in the primary language
o
El E2
0 0 • •
0
•
Selected translation word
MI I &gt; MI2
</figure>
<bodyText confidence="0.868948166666667">
between all Ec, and all Ei where Ec, is one
of the translation candidates for C and Ei is
one of all n words in S. A mutual information
score matrix is shown in Table 1. whereM/je,
is the mutual information score between contex-
tual word E3 and translation candidate E1.
</bodyText>
<figureCaption confidence="0.9830295">
Figure 1: The neighboring word as disambiguat-
ing feature
</figureCaption>
<bodyText confidence="0.976658375">
represented by the dotted line. Ey is the neigh-
boring word for C. Since M/i is greater than
M/2, Eci is selected as the translation of C.
2.3.2 Voting: multiple contextual
words as disambiguating feature
The baseline method uses only the neighboring
word to disambiguate C. Is one or two neigh-
boring word really sufficient for disambigua-
tion?
The intuition for choosing the nearest neigh-
boring word Ey as the disambiguating feature
for C is based on the assumption that they are
part of a phrase or collocation term, and that
there is only one sense per collocation (Dagan
and Itai, 1994; Yarowsky, 1993). However, in
most cases where C is a single word, there might
be some other words which are more useful for
disambiguating C. In fact, such long-distance
dependency occurs frequently in natural lan-
guage (Rosenfeld, 1995; Huang et al., 1993).
Another reason against using single neighbor-
ing word comes from (Gale and Church, 1994)
where it is argued that as many as 100,000 con-
text words might be needed to have high disam-
biguation accuracy. (Shiitze, 1992; Yarowsky,
1995) all use multiple context words as discrim-
inating features. We have also demonstrated in
our domain translation task that multiple con-
text words are useful (Fung and Lo, 1998; Fung
and McKeown, 1997).
Based on the above arguments, we enlarge
the disambiguation window to be the entire sen-
tence instead of only one word to the left or
right. We use all the contextual words in the
query sentence. Each contextual word &amp;quot;votes&amp;quot;
by its mutual information with all translation
candidates.
Suppose there are n primary language words
in S = El, E2, . . , C, . . . , En, as shown in Fig-
ure 2, we compute mutual information scores
</bodyText>
<table confidence="0.997764285714286">
Eel
E1 M ilCi M I1C2 . .. M IlCm
E2 M/2c1 M/2c2 ... MI2cm
... Mijci MIjc2 . • • MI.icrn
Ei
• • •
En Al I nci M Inc2 . . . 11/1 &apos;nom
</table>
<tableCaption confidence="0.9840905">
Table 1: Mutual information between all trans-
lation candidates and words in the sentence
</tableCaption>
<bodyText confidence="0.9755505">
For each row j in Table 1, the largest scoring
M/jci receives a vote. The rest of the row get
zero&apos;s. At the end, we sum up all the one&apos;s
in each column. The column i receiving the
highest vote is chosen as the one representing
the real translation.
</bodyText>
<figureCaption confidence="0.990988">
Figure 2: Voting for the best translation
</figureCaption>
<bodyText confidence="0.99972375">
To illustrate this idea, Table 2 shows that
candidate 2 is the correct translation for C.
There are four candidates of C and four con-
textual words to disambiguate C.
</bodyText>
<table confidence="0.9696994">
E„ Ee2 E3 E„
E1 0 1 0 0
E2 1 0 0 0
E3 0 0 0 1
El 0 1 0 0
</table>
<tableCaption confidence="0.600448">
Table 2: Candidate 2 is the correct translation
2.3.3 1-best contextual word as
disambiguating feature
</tableCaption>
<bodyText confidence="0.947532">
In the above voting scheme, a candidate receives
either a one vote or a zero vote from all contex-
</bodyText>
<listItem confidence="0.496852">
0 Word in primary language
• Word in soeondary language
0 Selected translation
</listItem>
<page confidence="0.994188">
336
</page>
<bodyText confidence="0.999811961538461">
tual words equally no matter how these words
are related to C. As an example, in the query
&amp;quot;Please show me the latest dianying/movie of
Jacky Chan&amp;quot;, the and Jacky are considered to
be equally important. We believe however, that
if the most powerful word is chosen for disam-
biguation, we can expect better performance.
This is related to the concept of &amp;quot;trigger pairs&amp;quot;
in (Rosenfeld, 1995) and Singular Value Decom-
position in (Shiitze, 1992).
In (Dagan and Itai, 1994), syntactic relation-
ship is used to find the most powerful &amp;quot;trigger
word&amp;quot;. Since syntactic relationship is unavail-
able in a mixed language sentence, we have to
use other type of information. In this method,
we want to choose the best trigger word among
all contextual words. Referring again to Table
1, M/jci is the mutual information score be-
tween contextual word Ej and translation can-
didate Ect.
We compute the disambiguation contribution
ratio for each context word E. For each row
j in Table 1, the largest MI score M/jej and
the second largest MI score M/i„ are chosen to
yield the contribution for word Ej ,which is the
ratio between the two scores
</bodyText>
<equation confidence="0.8225">
Wks (3)
</equation>
<bodyText confidence="0.999968">
If the ratio between M/jef and Milks is close
to one, we reason that Ej is not discriminative
enough as a feature for disambiguating C. On
the other hand, if the ratio between Mhef, and
M/ie8, is noticeably greater than one, we can use
Ej as the feature to disambiguate {Eci } with
high confidence. We choose the word Ey with
maximum contribution as the disambiguating
feature, and select the target word E„, whose
mutual information score with Ey is the highest,
as the translation for C.
</bodyText>
<equation confidence="0.986771">
r = ar g max M I (Ey , E„) (4)
</equation>
<bodyText confidence="0.9995374">
This method is illustrated in Figure 3. Since
E2 is the contextual word with highest contri-
bution score, the candidate Ei is chosen that
the mutual information between E2 and Eci is
the largest.
</bodyText>
<sectionHeader confidence="0.991929" genericHeader="method">
3 Evaluation experiments
</sectionHeader>
<bodyText confidence="0.8515495">
The mutual information between co-occurring
words and its contribution weight is ob-
</bodyText>
<figureCaption confidence="0.922515">
Figure 3: The best contextual word as disam-
biguating feature
</figureCaption>
<bodyText confidence="0.999739769230769">
tamed from a monolingual training corpus—
Wall Street Journal from 1987-1992. The train-
ing corpus size is about 590MB. We evaluate
our methods for mixed language query disam-
biguation on an automatically generated mixed-
language test set. No bilingual corpus, parallel
or comparable, is needed for training.
To evaluate our method, a mixed-language
sentence set is generated from the monolingual
ATIS corpus. The primary language is English
and the secondary language is chosen to be Chi-
nese. Some English words in the original sen-
tences are selected randomly and translated into
Chinese words manually to produce the test-
ing data. These are the mixed language sen-
tences. 500 testing sentences are extracted from
the ARPA ATIS corpus. The ratio of Chinese
words in the sentences varies from 10% to 65%.
We carry out three sets of experiments using
the three different features we have presented in
this paper. In each experiment, the percentage
of primary language words in the sentence is
incrementally increased at 5% steps, from 35%
to 90%. We note the accuracy of unambiguous
translation at each step. Note that at the 35%
stage, the primary language is in fact Chinese.
</bodyText>
<sectionHeader confidence="0.994094" genericHeader="evaluation">
4 Evaluation results
</sectionHeader>
<bodyText confidence="0.999914">
One advantage of using the artificially gener-
ated mixed-language test set is that it becomes
very easy to evaluate the performance of the
disambiguation/translation algorithm. We just
need to compare the translation output with the
original ATIS sentences.
The experimental results are shown in Fig-
ure 4. The horizontal axis represents the per-
centage of English words in the testing data and
the vertical axis represents the translation ac-
curacy. Translation accuracy is the ratio of the
number of secondary language (Chinese) words
disambiguated correctly over the number of all
</bodyText>
<figure confidence="0.9844152">
Mlicf
Contribution (E3, Eci)
CDWord in the primary language
Word in the secondary language
0 Selected translation of C
</figure>
<page confidence="0.991102">
337
</page>
<bodyText confidence="0.998759">
secondary language (Chinese) words present in
the testing sentences. The three different curves
represent the accuracies obtained from the base-
line feature, the voting model, and the 1-best
model.
</bodyText>
<figure confidence="0.996488166666667">
0.95
0.9
0.85
0.8
0.75
0.7
</figure>
<figureCaption confidence="0.9339815">
Figure 4: 1-best is the most discriminating fea-
ture
</figureCaption>
<bodyText confidence="0.9998745">
We can see that both voting contextual words
and the 1-best contextual words are more pow-
erful discriminant than the baseline neighboring
word. The 1-best feature is most effective for
disambiguating secondary language words in a
mixed-language sentence.
</bodyText>
<sectionHeader confidence="0.981069" genericHeader="conclusions">
5 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.999455164383562">
Mixed-language query occurs very often in both
spoken and written form, especially in Asia.
Such queries are usually in complete sentences
instead of concatenated word strings because
they are closer to the spoken language and more
natural for user. A mixed-language sentence
consists of words mostly in a primary language
and some in a secondary language. However,
even though mixed-languages are in sentence
form, they are difficult to parse and tag be-
cause those secondary language words introduce
an ambiguity factor. To understand a query can
mean finding the matched document, in the case
of Web search, or finding the corresponding se-
mantic classes, in the case of an interactive sys-
tem. In order to understand a mixed-language
query, we need to translate the secondary lan-
guage words into primary language &apos;unambigu-
ously.
In this paper, we present an approach of
mixed-language query disambiguation by us-
ing co-occurrence information obtained from a
monolingual corpus. Two new types of dis-
ambiguation features are introduced, namely
voting contextual words and 1-best contextual
word. These two features are compared to the
baseline feature of a single neighboring word.
Assuming the primary language is English and
the secondary language Chinese, our experi-
ments on English-Chinese mixed language show
that the average translation accuracy for the
baseline is 75.50%, for the voting model is
81.37% and for the 1-best model, 83.72%.
The baseline method uses only the neighbor-
ing word to disambiguate C. The assumption is
that the neighboring word is the most semantic
relevant. This method leaves out an important
feature of nature language: long distance de-
pendency. Experimental results show that it is
not sufficient to use only the nearest neighbor-
ing word for disambiguation.
The performance of the voting method is bet-
ter than the baseline because more contextual
words are used. The results are consistent with
the idea in (Gale and Church, 1994; Shiitze,
1992; Yarowsky, 1995).
In our experiments, it is found that 1-best
contextual word is even better than multiple
contextual words. This seemingly counter-
intuitive result leads us to believe that choos-
ing the most discriminative single word is even
more powerful than using multiple contextual
word equally. We believe that this is consistent
with the idea of using &amp;quot;trigger pairs&amp;quot; in (Rosen-
feld, 1995) and Singular Value Decomposition
in (Shiitze, 1992).
We can conclude that sometimes long-
distance contextual words are more discrimi-
nant than immediate neighboring words, and
that multiple contextual words can contribute
to better disambiguation.Our results support
our belief that natural sentence-based queries
are less ambiguous than keyword based queries.
Our method using multiple disambiguating con-
textual words can take advantage of syntactic
information even when parsing or tagging is not
possible, such as in the case of mixed-language
queries.
Other advantages of our approach include:
(1) the training is unsupervised and no domain-
dependent data is necessary, (2) neither bilin-
gual corpora or mixed-language corpora is
needed for training, and (3) it can generate
</bodyText>
<figure confidence="0.3267805">
40 50 eo 70 ao 90
Precentago of Primary Language Wonf a
</figure>
<page confidence="0.993434">
338
</page>
<bodyText confidence="0.999913285714286">
monolingual queries in both primary and sec-
ondary languages, enabling true cross-language
IR.
In our future work, we plan to analyze the
various &amp;quot;discriminating words&amp;quot; contained in a
mixed language or monolingual query to find
out which class of words contribute more to
the final disambiguation. We also want to test
the significance of the co-occurrence informa-
tion of all contextual words between themselves
in the disambiguation task. Finally, we plan
to develop a general mixed-language and cross-
language understanding framework for both
document retrieval and interactive tasks.
</bodyText>
<sectionHeader confidence="0.997742" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992508674418605">
AskJeeves. 1998. http://www.askjeeves.com.
Lisa Ballesteros and W. Bruce Croft. 1998.
Resolving ambiguity for cross-language re-
trieval. In Proceedings of the 21st Annual In-
ternational ACM SIGIR Conference on Re-
search and Development in Information Re-
trieval, pages 64-71, Melbourne, Australia,
August.
P. Brown, J. Lai, and R. Mercer. 1991. Aligning
sentences in parallel corpora. In Proceedings
of the 29th Annual Conference of the Associ-
ation for Computational Linguistics.
Ido Dagan and Kenneth W. Church. 1994. Ter-
might: Identifying and translating technical
terminology. In Proceedings of the 4th Con-
ference on Applied Natural Language Process-
ing, pages 34-40, Stuttgart, Germany, Octo-
ber. r
Ido Dagan and Alon Itai. 1994. Word sense dis-
ambiguation using a second language mono-
lingual corpus. In Computational Linguistics,
pages 564-596.
Ido Dagan, Alon Itai, and Ulrike Schwa11. 1991.
Two languages are more informative than
one. In Proceedings of the 29th Annual Con-
ference of the Association for Computational
Linguistics, pages 130-137, Berkeley, Califor-
nia.
M. Davis. 1998. Free resources and advanced
alignment for cross-language text retrieval.
In Proceedings of the 6th Text Retrieval Con-
ference (TREC-6), NIST, Gaithersburg, MD,
November.
Laura DiDio. 1997. Os/2 let users talk back to
&apos;net. page 12.
ElectricMonk. 1998.
http: / / www. electricmonk . com.
Pascale Fung and Yuen Yee Lo. 1998. An IR
approach for translating new words from non-
parallel, comparable texts. In Proceedings of
the 36th Annual Conference of the Associ-
ation for Computational Linguistics, pages
414-420, Montreal,Canada, August.
Pascale Pang and Kathleen McKeown. 1997.
Finding terminology translations from non-
parallel corpora. In The 5th Annual Work-
shop on Very Large Corpora, pages 192-202,
Hong Kong, Aug.
Pascale Fung and Dekai Wu. 1994. Statistical
augmentation of a Chinese machine-readable
dictionary. In Proceedings of the Second An-
nual Workshop on Very Large Corpora, pages
69-85, Kyoto, Japan, June.
Pascale Fung, CHEUNG Chi Shuen,
LAM Kwok Leung, LIU Wai Kat, and
LO Yuen Yee. 1998a. A speech assisted
online search agent (salsa). In ICSLP.
Pascale Ring, CHEUNG Chi Shuen,
LAM Kwok Leung, LIU Wai Kat, LO Yuen
Yee, and MA Chi Yuen. 1998b. SALSA, a
multilingual speech-based web browser. In
The First AEARU Web Technolgy Workshop,
Nov.
Pascale Pang. 1998. A statistical view of bilin-
gual lexicon extraction: from parallel corpora
to non-parallel corpora. In Proceedings of the
Third Conference of the Association for Ma-
chine Translation in the Americas, Pennsyl-
vania, October.
William A. Gale and Kenneth W. Church.
1993. A program for aligning sentences in
bilingual corpora. Computational Linguis-
tics, 19(1):75-102.
William A. Gale and Kenneth W. Church. 1994.
Discrimination decisions in 100,000 dimen-
sional spaces. Current Issues in Computa-
tional Linguistics: In honour of Don Walker,
pages 429-550.
W. Gale, K. Church, and D. Yarowsky. 1992a.
Estimating upper and lower bounds on the
performance of word-sense disambiguation
programs. In Proceedings of the 30th Con-
ference of the Association for Computational
Linguistics. Association for Computational
Linguistics.
W. Gale, K. Church, and D. Yarowsky. 1992b.
</reference>
<page confidence="0.990143">
339
</page>
<reference confidence="0.997761443181818">
Using bilingual materials to develop word
sense disambiguation methods. In Proceed-
ings of TMI 92.
W. Gale, K. Church, and D. Yarowsky. 1992c.
Work on statistical methods for word sense
disambiguation. In Proceedings of AAAI 92.
W. Gale, K. Church, and D. Yarowsky. 1993. A
method for disambiguating word senses in a
large corpus. In Computers and Humanities,
volume 26, pages 415-439.
Gregory Grefenstette, editor. 1998. Cross-
language Information Retrieval. Kluwer Aca-
demic Publishers.
Xuedong Huang, Fileno Alleva, Hisao-Wuen
Hong, Mei-Yuh Hwang, Kai-Fu Lee, and
Ronald Rosenfeld. 1993. The SPHINX-
II speech recognition system: an overview.
Computer, Speech and Language, pages 137-
148.
David A. Hull and Gregory Grefenstette. 1996.
A dictionary-based approach to multilingual
informaion retrieval. In Proceedings of the
19th International Conference on Research
and Development in Information Retrieval,
pages 49-57.
Julian Kupiec. 1993. An algorithm for finding
noun phrase correspondences in bilingual cor-
pora. In Proceedings of the 31st Annual Con-
ference of the Association for Computational
Linguistics, pages 17-22, Columbus, Ohio,
June.
Xiaohu Liu and Sheng Li. 1997. Statistic-based
target word selection in English-Chinese ma-
chine translation. Journal of Harbin Institute
of Technology, May.
Chi Yuen Ma and Pascale Fung. 1998. Using
English phoneme models for Chinese speech
recognition. In International Symposium on
Chinese Spoken language processing.
D.W. Oard. 1997. Alternative approaches for
cross-language text retrieval. In AAAI Sym-
posium on cross-language text and speech re-
trieval. American Association for Artificial
Intelligence, Mar.
Eugenio Picchi and Carol Peters. 1998. Cross-
language information retrieval: a system
for comparable corpus querying. In Gregory
Grefenstette, editor, Cross-language Infor-
mation Retrieval, pages 81-92. Kluwer Aca-
demic Publishers.
Lau Raymond. 1997. Webgalaxy : Beyond
point and click - a conversational interface to
a browser. In Computer Netowrks &amp; ISDN
Systems, pages 1385-1393.
Rony Rosenfeld. 1995. A Corpus-Based Ap-
proach to Language Learning. Ph.D. thesis,
Carnegie Mellon University.
Hinrich Shfitze. 1992. Dimensions of meaning.
In Proceedings of Supercomputing &apos;92.
Frank Smadja, Kathleen McKeown, and
Vasileios Hatzsivassiloglou. 1996. Translat-
ing collocations for bilingual lexicons: A sta-
tistical approach. Computational Linguistics,
21(4):1-38.
Frank Smadja. 1993. Retrieving collocations
from text: Xtract. Computational Linguis-
tics, 19(1):143-177.
Kumiko Tanaka and Hideya Iwasaki. 1996.
Extraction of lexical translations from non-
aligned corpora. In Proceedings of COLING
96, Copenhagan, Danmark, July.
Delcai Wu. 1995. Grammarless extraction of
phrasal translation examples from parallel
texts. In Proceedings of TMI 95, Leuven, Bel-
gium, July. Submitted.
D. Yarowsky. 1993. One sense per collocation.
In Proceedings of ARPA Human Language
Technology Workshop, Princeton.
D. Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods.
In Proceedings of the 33rd Conference of
the Association for Computational Linguis-
tics, pages 189-196. Association for Compu-
tational Linguistics.
Victor Zue. 1995. Spoken language interfaces to
computers: Achievements and challenges. In
The 33rd Annual Meeting of the Association
of Computational Linguistics, Boston, June.
</reference>
<page confidence="0.998286">
340
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.668086">
<title confidence="0.999926">Mixed Language Query Disambiguation</title>
<author confidence="0.827377">LIU Xiaohu Chi Shun FUNG</author>
<affiliation confidence="0.893384">HKUST Human Language Technology Center Department of Electrical and Electronic Engineering University of Science and Technology, HKUST</affiliation>
<address confidence="0.987234">Clear Water Bay, Hong Kong</address>
<email confidence="0.99781">fpascale,lxiaohu,eepercylfte.ust.hk</email>
<abstract confidence="0.9939964">We propose a mixed language query disambiguation approach by using co-occurrence information from monolingual data only. A mixed language query consists of words in a language a language. Our method translates the query into monolingual queries in either language. Two novel features for disambiguation, namely contextual word voting and 1-best contextual word, are introduced and compared to a baseline feature, the nearest neighbor. Average query translaaccuracy for the two features are 83.72%, compared to the accuracy of 75.50%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AskJeeves</author>
</authors>
<date>1998</date>
<note>http://www.askjeeves.com.</note>
<contexts>
<context position="2670" citStr="AskJeeves, 1998" startWordPosition="423" endWordPosition="424">y situations, the key words are ambiguous. To resolve ambiguity, query expansion is usually employed to look for additional keywords. We believe that a more useful search engine should allow the user to input natural language sentences. Sentence-based queries are useful because (1) they are more natural to the user and (2) more importantly, they provide more contextual information which are important for query understanding. To date, the few sentence-based search engines do not seem to take advantage of context information in the query, but merely extracting key words from the query sentence (AskJeeves, 1998; ElectricMonk, 1998). In addition to the need for better query understanding methods for a large variety of domains, it has also become important to handle queries in different languages. Crosslanguage information retrieval has emerged as an important area as the amount of nonEnglish material is ever increasing (Oard, 1997; Grefenstette, 1998; Ballesteros and Croft, 1998; Picchi and Peters, 1998; Davis, 1998; Hull and Grefenstette, 1996). One of the important tasks of cross-language IR is to translate queries from one language to another. The original query and the translated query are then u</context>
</contexts>
<marker>AskJeeves, 1998</marker>
<rawString>AskJeeves. 1998. http://www.askjeeves.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Ballesteros</author>
<author>W Bruce Croft</author>
</authors>
<title>Resolving ambiguity for cross-language retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>64--71</pages>
<location>Melbourne, Australia,</location>
<contexts>
<context position="3044" citStr="Ballesteros and Croft, 1998" startWordPosition="481" endWordPosition="484">ontextual information which are important for query understanding. To date, the few sentence-based search engines do not seem to take advantage of context information in the query, but merely extracting key words from the query sentence (AskJeeves, 1998; ElectricMonk, 1998). In addition to the need for better query understanding methods for a large variety of domains, it has also become important to handle queries in different languages. Crosslanguage information retrieval has emerged as an important area as the amount of nonEnglish material is ever increasing (Oard, 1997; Grefenstette, 1998; Ballesteros and Croft, 1998; Picchi and Peters, 1998; Davis, 1998; Hull and Grefenstette, 1996). One of the important tasks of cross-language IR is to translate queries from one language to another. The original query and the translated query are then used to match documents in both the source and target languages. Target language documents are either glossed or translated by other systems. According to (Grefenstette, 1998), three main problems of query translations are: 1. generating translation candidates, 2. weighting translation candidates, and 333 3. pruning translation alternatives for document matching. In cross-</context>
<context position="10599" citStr="Ballesteros and Croft, 1998" startWordPosition="1725" endWordPosition="1728">ata using the following formula, where E is a word and f (E) is the frequency of word E. , M I (E,, Ej) = f (E, Ej) log f (E,) * f(E3) (1) E, and Ej can be either neighboring words or any two words in the sentence. 2.3 Translation candidate pruning The last problem in query translation is selecting the target translation. In our approach, we need to choose a particular Ec from E. We call this pruning process translation disambiguation. We present and compare three unsupervised statistical methods in this paper. The first baseline method is similar to (Dagan et al., 1991; Dagan and Itai, 1994; Ballesteros and Croft, 1998; Smadja et al., 1996), where we use the nearest neighboring word of the secondary language word C as feature for disambiguation. In the second method, we choose all contextual words as disambiguating feature. In the third method, the most discriminative contextual word is selected as feature. 2.3.1 Baseline: single neighboring word as disambiguating feature The first disambiguating feature we present here is similar to the statistical feature in (Dagan et al., 1991; Smadja et al., 1996; Dagan and Itai, 1994; Ballesteros and Croft, 1998), namely the co-occurrence with neighboring words. We do </context>
</contexts>
<marker>Ballesteros, Croft, 1998</marker>
<rawString>Lisa Ballesteros and W. Bruce Croft. 1998. Resolving ambiguity for cross-language retrieval. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 64-71, Melbourne, Australia, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Lai</author>
<author>R Mercer</author>
</authors>
<title>Aligning sentences in parallel corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7460" citStr="Brown et al., 1991" startWordPosition="1198" endWordPosition="1201">problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), p</context>
</contexts>
<marker>Brown, Lai, Mercer, 1991</marker>
<rawString>P. Brown, J. Lai, and R. Mercer. 1991. Aligning sentences in parallel corpora. In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Kenneth W Church</author>
</authors>
<title>Termight: Identifying and translating technical terminology.</title>
<date>1994</date>
<booktitle>In Proceedings of the 4th Conference on Applied Natural Language Processing,</booktitle>
<pages>34--40</pages>
<location>Stuttgart, Germany,</location>
<note>r</note>
<contexts>
<context position="7293" citStr="Dagan and Church, 1994" startWordPosition="1172" endWordPosition="1175">ods allows for both general and crosslanguage IR from a mixed language query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and </context>
</contexts>
<marker>Dagan, Church, 1994</marker>
<rawString>Ido Dagan and Kenneth W. Church. 1994. Termight: Identifying and translating technical terminology. In Proceedings of the 4th Conference on Applied Natural Language Processing, pages 34-40, Stuttgart, Germany, October. r</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
</authors>
<title>Word sense disambiguation using a second language monolingual corpus.</title>
<date>1994</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>564--596</pages>
<contexts>
<context position="7502" citStr="Dagan and Itai, 1994" startWordPosition="1206" endWordPosition="1209">n are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), parallel corpora are rare in most domains. </context>
<context position="10570" citStr="Dagan and Itai, 1994" startWordPosition="1721" endWordPosition="1724">omain as the testing data using the following formula, where E is a word and f (E) is the frequency of word E. , M I (E,, Ej) = f (E, Ej) log f (E,) * f(E3) (1) E, and Ej can be either neighboring words or any two words in the sentence. 2.3 Translation candidate pruning The last problem in query translation is selecting the target translation. In our approach, we need to choose a particular Ec from E. We call this pruning process translation disambiguation. We present and compare three unsupervised statistical methods in this paper. The first baseline method is similar to (Dagan et al., 1991; Dagan and Itai, 1994; Ballesteros and Croft, 1998; Smadja et al., 1996), where we use the nearest neighboring word of the secondary language word C as feature for disambiguation. In the second method, we choose all contextual words as disambiguating feature. In the third method, the most discriminative contextual word is selected as feature. 2.3.1 Baseline: single neighboring word as disambiguating feature The first disambiguating feature we present here is similar to the statistical feature in (Dagan et al., 1991; Smadja et al., 1996; Dagan and Itai, 1994; Ballesteros and Croft, 1998), namely the co-occurrence w</context>
<context position="12911" citStr="Dagan and Itai, 1994" startWordPosition="2134" endWordPosition="2137">g word as disambiguating feature represented by the dotted line. Ey is the neighboring word for C. Since M/i is greater than M/2, Eci is selected as the translation of C. 2.3.2 Voting: multiple contextual words as disambiguating feature The baseline method uses only the neighboring word to disambiguate C. Is one or two neighboring word really sufficient for disambiguation? The intuition for choosing the nearest neighboring word Ey as the disambiguating feature for C is based on the assumption that they are part of a phrase or collocation term, and that there is only one sense per collocation (Dagan and Itai, 1994; Yarowsky, 1993). However, in most cases where C is a single word, there might be some other words which are more useful for disambiguating C. In fact, such long-distance dependency occurs frequently in natural language (Rosenfeld, 1995; Huang et al., 1993). Another reason against using single neighboring word comes from (Gale and Church, 1994) where it is argued that as many as 100,000 context words might be needed to have high disambiguation accuracy. (Shiitze, 1992; Yarowsky, 1995) all use multiple context words as discriminating features. We have also demonstrated in our domain translatio</context>
<context position="15509" citStr="Dagan and Itai, 1994" startWordPosition="2617" endWordPosition="2620">, a candidate receives either a one vote or a zero vote from all contex0 Word in primary language • Word in soeondary language 0 Selected translation 336 tual words equally no matter how these words are related to C. As an example, in the query &amp;quot;Please show me the latest dianying/movie of Jacky Chan&amp;quot;, the and Jacky are considered to be equally important. We believe however, that if the most powerful word is chosen for disambiguation, we can expect better performance. This is related to the concept of &amp;quot;trigger pairs&amp;quot; in (Rosenfeld, 1995) and Singular Value Decomposition in (Shiitze, 1992). In (Dagan and Itai, 1994), syntactic relationship is used to find the most powerful &amp;quot;trigger word&amp;quot;. Since syntactic relationship is unavailable in a mixed language sentence, we have to use other type of information. In this method, we want to choose the best trigger word among all contextual words. Referring again to Table 1, M/jci is the mutual information score between contextual word Ej and translation candidate Ect. We compute the disambiguation contribution ratio for each context word E. For each row j in Table 1, the largest MI score M/jej and the second largest MI score M/i„ are chosen to yield the contribution</context>
</contexts>
<marker>Dagan, Itai, 1994</marker>
<rawString>Ido Dagan and Alon Itai. 1994. Word sense disambiguation using a second language monolingual corpus. In Computational Linguistics, pages 564-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
<author>Ulrike Schwa11</author>
</authors>
<title>Two languages are more informative than one.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>130--137</pages>
<location>Berkeley, California.</location>
<marker>Dagan, Itai, Schwa11, 1991</marker>
<rawString>Ido Dagan, Alon Itai, and Ulrike Schwa11. 1991. Two languages are more informative than one. In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics, pages 130-137, Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Davis</author>
</authors>
<title>Free resources and advanced alignment for cross-language text retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the 6th Text Retrieval Conference (TREC-6),</booktitle>
<location>NIST, Gaithersburg, MD,</location>
<contexts>
<context position="3082" citStr="Davis, 1998" startWordPosition="489" endWordPosition="490">derstanding. To date, the few sentence-based search engines do not seem to take advantage of context information in the query, but merely extracting key words from the query sentence (AskJeeves, 1998; ElectricMonk, 1998). In addition to the need for better query understanding methods for a large variety of domains, it has also become important to handle queries in different languages. Crosslanguage information retrieval has emerged as an important area as the amount of nonEnglish material is ever increasing (Oard, 1997; Grefenstette, 1998; Ballesteros and Croft, 1998; Picchi and Peters, 1998; Davis, 1998; Hull and Grefenstette, 1996). One of the important tasks of cross-language IR is to translate queries from one language to another. The original query and the translated query are then used to match documents in both the source and target languages. Target language documents are either glossed or translated by other systems. According to (Grefenstette, 1998), three main problems of query translations are: 1. generating translation candidates, 2. weighting translation candidates, and 333 3. pruning translation alternatives for document matching. In cross-language IR, key word disambiguation i</context>
</contexts>
<marker>Davis, 1998</marker>
<rawString>M. Davis. 1998. Free resources and advanced alignment for cross-language text retrieval. In Proceedings of the 6th Text Retrieval Conference (TREC-6), NIST, Gaithersburg, MD, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura DiDio</author>
</authors>
<title>Os/2 let users talk back to &apos;net.</title>
<date>1997</date>
<pages>12</pages>
<contexts>
<context position="1387" citStr="DiDio, 1997" startWordPosition="205" endWordPosition="206">acy for the two features are 81.37% and 83.72%, compared to the baseline accuracy of 75.50%. 1 Introduction Online information retrieval is now prevalent because of the ubiquitous World Wide Web. The Web is also a powerful platform for another application—interactive spoken language query systems. Traditionally, such systems were implemented on stand-alone kiosks. Now we can easily use the Web as a platform. Information such as airline schedules, movie reservation, car trading, etc., can all be included in HTML files, to be accessed by a generic spoken interface to the Web browser (Zue, 1995; DiDio, 1997; Raymond, 1997; Fang et al., 1998a). Our team has built a multilingual spoken language interface to the Web, named SALSA (Fung et al., 1998b; Fung et al., 1998a; Ma and Fung, 1998). Users can use speech to surf the net via various links as well as issue search commands such as &amp;quot;Show me the latest movie of Jacky Chan&amp;quot;. The system recognizes commands and queries in English, Mandarin and Cantonese, as well as mixed language sentences. Until recently, most of the search engines handle keyword based queries where the user types in a series of strings without syntactic structure. The choice of key </context>
</contexts>
<marker>DiDio, 1997</marker>
<rawString>Laura DiDio. 1997. Os/2 let users talk back to &apos;net. page 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ElectricMonk</author>
</authors>
<date>1998</date>
<note>http: / / www. electricmonk . com.</note>
<contexts>
<context position="2691" citStr="ElectricMonk, 1998" startWordPosition="425" endWordPosition="426"> key words are ambiguous. To resolve ambiguity, query expansion is usually employed to look for additional keywords. We believe that a more useful search engine should allow the user to input natural language sentences. Sentence-based queries are useful because (1) they are more natural to the user and (2) more importantly, they provide more contextual information which are important for query understanding. To date, the few sentence-based search engines do not seem to take advantage of context information in the query, but merely extracting key words from the query sentence (AskJeeves, 1998; ElectricMonk, 1998). In addition to the need for better query understanding methods for a large variety of domains, it has also become important to handle queries in different languages. Crosslanguage information retrieval has emerged as an important area as the amount of nonEnglish material is ever increasing (Oard, 1997; Grefenstette, 1998; Ballesteros and Croft, 1998; Picchi and Peters, 1998; Davis, 1998; Hull and Grefenstette, 1996). One of the important tasks of cross-language IR is to translate queries from one language to another. The original query and the translated query are then used to match document</context>
</contexts>
<marker>ElectricMonk, 1998</marker>
<rawString>ElectricMonk. 1998. http: / / www. electricmonk . com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Yuen Yee Lo</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>414--420</pages>
<location>Montreal,Canada,</location>
<contexts>
<context position="7405" citStr="Fung and Lo, 1998" startWordPosition="1190" endWordPosition="1193"> of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1</context>
<context position="13575" citStr="Fung and Lo, 1998" startWordPosition="2244" endWordPosition="2247"> C is a single word, there might be some other words which are more useful for disambiguating C. In fact, such long-distance dependency occurs frequently in natural language (Rosenfeld, 1995; Huang et al., 1993). Another reason against using single neighboring word comes from (Gale and Church, 1994) where it is argued that as many as 100,000 context words might be needed to have high disambiguation accuracy. (Shiitze, 1992; Yarowsky, 1995) all use multiple context words as discriminating features. We have also demonstrated in our domain translation task that multiple context words are useful (Fung and Lo, 1998; Fung and McKeown, 1997). Based on the above arguments, we enlarge the disambiguation window to be the entire sentence instead of only one word to the left or right. We use all the contextual words in the query sentence. Each contextual word &amp;quot;votes&amp;quot; by its mutual information with all translation candidates. Suppose there are n primary language words in S = El, E2, . . , C, . . . , En, as shown in Figure 2, we compute mutual information scores Eel E1 M ilCi M I1C2 . .. M IlCm E2 M/2c1 M/2c2 ... MI2cm ... Mijci MIjc2 . • • MI.icrn Ei • • • En Al I nci M Inc2 . . . 11/1 &apos;nom Table 1: Mutual info</context>
</contexts>
<marker>Fung, Lo, 1998</marker>
<rawString>Pascale Fung and Yuen Yee Lo. 1998. An IR approach for translating new words from nonparallel, comparable texts. In Proceedings of the 36th Annual Conference of the Association for Computational Linguistics, pages 414-420, Montreal,Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Pang</author>
<author>Kathleen McKeown</author>
</authors>
<title>Finding terminology translations from nonparallel corpora.</title>
<date>1997</date>
<booktitle>In The 5th Annual Workshop on Very Large Corpora,</booktitle>
<pages>192--202</pages>
<location>Hong Kong,</location>
<marker>Pang, McKeown, 1997</marker>
<rawString>Pascale Pang and Kathleen McKeown. 1997. Finding terminology translations from nonparallel corpora. In The 5th Annual Workshop on Very Large Corpora, pages 192-202, Hong Kong, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Dekai Wu</author>
</authors>
<title>Statistical augmentation of a Chinese machine-readable dictionary.</title>
<date>1994</date>
<booktitle>In Proceedings of the Second Annual Workshop on Very Large Corpora,</booktitle>
<pages>69--85</pages>
<location>Kyoto, Japan,</location>
<contexts>
<context position="7202" citStr="Fung and Wu, 1994" startWordPosition="1158" endWordPosition="1161">guage and the query is passed to another IR system in the secondary language. Our methods allows for both general and crosslanguage IR from a mixed language query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-paralle</context>
</contexts>
<marker>Fung, Wu, 1994</marker>
<rawString>Pascale Fung and Dekai Wu. 1994. Statistical augmentation of a Chinese machine-readable dictionary. In Proceedings of the Second Annual Workshop on Very Large Corpora, pages 69-85, Kyoto, Japan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>CHEUNG Chi Shuen</author>
<author>LAM Kwok Leung</author>
<author>LIU Wai Kat</author>
<author>LO Yuen Yee</author>
</authors>
<title>A speech assisted online search agent (salsa).</title>
<date>1998</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="1527" citStr="Fung et al., 1998" startWordPosition="229" endWordPosition="232">al is now prevalent because of the ubiquitous World Wide Web. The Web is also a powerful platform for another application—interactive spoken language query systems. Traditionally, such systems were implemented on stand-alone kiosks. Now we can easily use the Web as a platform. Information such as airline schedules, movie reservation, car trading, etc., can all be included in HTML files, to be accessed by a generic spoken interface to the Web browser (Zue, 1995; DiDio, 1997; Raymond, 1997; Fang et al., 1998a). Our team has built a multilingual spoken language interface to the Web, named SALSA (Fung et al., 1998b; Fung et al., 1998a; Ma and Fung, 1998). Users can use speech to surf the net via various links as well as issue search commands such as &amp;quot;Show me the latest movie of Jacky Chan&amp;quot;. The system recognizes commands and queries in English, Mandarin and Cantonese, as well as mixed language sentences. Until recently, most of the search engines handle keyword based queries where the user types in a series of strings without syntactic structure. The choice of key words in this case determines the success rate of the search. In many situations, the key words are ambiguous. To resolve ambiguity, query e</context>
</contexts>
<marker>Fung, Shuen, Leung, Kat, Yee, 1998</marker>
<rawString>Pascale Fung, CHEUNG Chi Shuen, LAM Kwok Leung, LIU Wai Kat, and LO Yuen Yee. 1998a. A speech assisted online search agent (salsa). In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Ring</author>
<author>CHEUNG Chi Shuen</author>
<author>LAM Kwok Leung</author>
<author>LIU Wai Kat</author>
<author>LO Yuen Yee</author>
<author>MA Chi Yuen</author>
</authors>
<title>SALSA, a multilingual speech-based web browser.</title>
<date>1998</date>
<booktitle>In The First AEARU Web Technolgy Workshop,</booktitle>
<marker>Ring, Shuen, Leung, Kat, Yee, Yuen, 1998</marker>
<rawString>Pascale Ring, CHEUNG Chi Shuen, LAM Kwok Leung, LIU Wai Kat, LO Yuen Yee, and MA Chi Yuen. 1998b. SALSA, a multilingual speech-based web browser. In The First AEARU Web Technolgy Workshop, Nov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Pang</author>
</authors>
<title>A statistical view of bilingual lexicon extraction: from parallel corpora to non-parallel corpora.</title>
<date>1998</date>
<booktitle>In Proceedings of the Third Conference of the Association for Machine Translation in the Americas,</booktitle>
<location>Pennsylvania,</location>
<marker>Pang, 1998</marker>
<rawString>Pascale Pang. 1998. A statistical view of bilingual lexicon extraction: from parallel corpora to non-parallel corpora. In Proceedings of the Third Conference of the Association for Machine Translation in the Americas, Pennsylvania, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="9704" citStr="Gale and Church, 1993" startWordPosition="1565" endWordPosition="1568">he correct translations of C should co-occur frequently with the contextual words Ei and incorrect translation of C should co-occur rarely with the contextual words. Obviously, other information such as syntactical relationship between words or the part-of-speech tags could be used as weights too. However, it is difficult to parse and tag a mixed language sentence. The only information we can use to disambiguate C is the co-occurrence information between its translation candidates {E,2} and El, E2, , En. Mutual information is a good measure of the co-occurrence relationship between two words (Gale and Church, 1993). We first compute the mutual information between any word pair from a monolingual corpus in the primary language 2 11n actual experiments, each sentence can contain multiple secondary language words 2This corpus does not need to be in the same domain as the testing data using the following formula, where E is a word and f (E) is the frequency of word E. , M I (E,, Ej) = f (E, Ej) log f (E,) * f(E3) (1) E, and Ej can be either neighboring words or any two words in the sentence. 2.3 Translation candidate pruning The last problem in query translation is selecting the target translation. In our a</context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>William A. Gale and Kenneth W. Church. 1993. A program for aligning sentences in bilingual corpora. Computational Linguistics, 19(1):75-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>Discrimination decisions in 100,000 dimensional spaces. Current Issues in Computational Linguistics: In honour of Don Walker,</title>
<date>1994</date>
<pages>429--550</pages>
<contexts>
<context position="13258" citStr="Gale and Church, 1994" startWordPosition="2190" endWordPosition="2193">sufficient for disambiguation? The intuition for choosing the nearest neighboring word Ey as the disambiguating feature for C is based on the assumption that they are part of a phrase or collocation term, and that there is only one sense per collocation (Dagan and Itai, 1994; Yarowsky, 1993). However, in most cases where C is a single word, there might be some other words which are more useful for disambiguating C. In fact, such long-distance dependency occurs frequently in natural language (Rosenfeld, 1995; Huang et al., 1993). Another reason against using single neighboring word comes from (Gale and Church, 1994) where it is argued that as many as 100,000 context words might be needed to have high disambiguation accuracy. (Shiitze, 1992; Yarowsky, 1995) all use multiple context words as discriminating features. We have also demonstrated in our domain translation task that multiple context words are useful (Fung and Lo, 1998; Fung and McKeown, 1997). Based on the above arguments, we enlarge the disambiguation window to be the entire sentence instead of only one word to the left or right. We use all the contextual words in the query sentence. Each contextual word &amp;quot;votes&amp;quot; by its mutual information with a</context>
<context position="21476" citStr="Gale and Church, 1994" startWordPosition="3594" endWordPosition="3597">accuracy for the baseline is 75.50%, for the voting model is 81.37% and for the 1-best model, 83.72%. The baseline method uses only the neighboring word to disambiguate C. The assumption is that the neighboring word is the most semantic relevant. This method leaves out an important feature of nature language: long distance dependency. Experimental results show that it is not sufficient to use only the nearest neighboring word for disambiguation. The performance of the voting method is better than the baseline because more contextual words are used. The results are consistent with the idea in (Gale and Church, 1994; Shiitze, 1992; Yarowsky, 1995). In our experiments, it is found that 1-best contextual word is even better than multiple contextual words. This seemingly counterintuitive result leads us to believe that choosing the most discriminative single word is even more powerful than using multiple contextual word equally. We believe that this is consistent with the idea of using &amp;quot;trigger pairs&amp;quot; in (Rosenfeld, 1995) and Singular Value Decomposition in (Shiitze, 1992). We can conclude that sometimes longdistance contextual words are more discriminant than immediate neighboring words, and that multiple </context>
</contexts>
<marker>Gale, Church, 1994</marker>
<rawString>William A. Gale and Kenneth W. Church. 1994. Discrimination decisions in 100,000 dimensional spaces. Current Issues in Computational Linguistics: In honour of Don Walker, pages 429-550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>Estimating upper and lower bounds on the performance of word-sense disambiguation programs.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Conference of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7521" citStr="Gale et al., 1992" startWordPosition="1210" endWordPosition="1213">ranslation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), parallel corpora are rare in most domains. We want to devise a</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992a. Estimating upper and lower bounds on the performance of word-sense disambiguation programs. In Proceedings of the 30th Conference of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>Using bilingual materials to develop word sense disambiguation methods.</title>
<date>1992</date>
<booktitle>In Proceedings of TMI 92.</booktitle>
<contexts>
<context position="7521" citStr="Gale et al., 1992" startWordPosition="1210" endWordPosition="1213">ranslation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), parallel corpora are rare in most domains. We want to devise a</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992b. Using bilingual materials to develop word sense disambiguation methods. In Proceedings of TMI 92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>Work on statistical methods for word sense disambiguation.</title>
<date>1992</date>
<booktitle>In Proceedings of AAAI 92.</booktitle>
<contexts>
<context position="7521" citStr="Gale et al., 1992" startWordPosition="1210" endWordPosition="1213">ranslation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), parallel corpora are rare in most domains. We want to devise a</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992c. Work on statistical methods for word sense disambiguation. In Proceedings of AAAI 92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus.</title>
<date>1993</date>
<booktitle>In Computers and Humanities,</booktitle>
<volume>26</volume>
<pages>415--439</pages>
<contexts>
<context position="7596" citStr="Gale et al., 1993" startWordPosition="1224" endWordPosition="1227">didates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), parallel corpora are rare in most domains. We want to devise a method that uses only monolingual data in the primary language to train co</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1993</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1993. A method for disambiguating word senses in a large corpus. In Computers and Humanities, volume 26, pages 415-439.</rawString>
</citation>
<citation valid="true">
<title>Crosslanguage Information Retrieval.</title>
<date>1998</date>
<editor>Gregory Grefenstette, editor.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>1998</marker>
<rawString>Gregory Grefenstette, editor. 1998. Crosslanguage Information Retrieval. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuedong Huang</author>
<author>Fileno Alleva</author>
<author>Hisao-Wuen Hong</author>
<author>Mei-Yuh Hwang</author>
<author>Kai-Fu Lee</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>The SPHINXII speech recognition system: an overview. Computer, Speech and Language,</title>
<date>1993</date>
<pages>137--148</pages>
<contexts>
<context position="13169" citStr="Huang et al., 1993" startWordPosition="2176" endWordPosition="2179">es only the neighboring word to disambiguate C. Is one or two neighboring word really sufficient for disambiguation? The intuition for choosing the nearest neighboring word Ey as the disambiguating feature for C is based on the assumption that they are part of a phrase or collocation term, and that there is only one sense per collocation (Dagan and Itai, 1994; Yarowsky, 1993). However, in most cases where C is a single word, there might be some other words which are more useful for disambiguating C. In fact, such long-distance dependency occurs frequently in natural language (Rosenfeld, 1995; Huang et al., 1993). Another reason against using single neighboring word comes from (Gale and Church, 1994) where it is argued that as many as 100,000 context words might be needed to have high disambiguation accuracy. (Shiitze, 1992; Yarowsky, 1995) all use multiple context words as discriminating features. We have also demonstrated in our domain translation task that multiple context words are useful (Fung and Lo, 1998; Fung and McKeown, 1997). Based on the above arguments, we enlarge the disambiguation window to be the entire sentence instead of only one word to the left or right. We use all the contextual w</context>
</contexts>
<marker>Huang, Alleva, Hong, Hwang, Lee, Rosenfeld, 1993</marker>
<rawString>Xuedong Huang, Fileno Alleva, Hisao-Wuen Hong, Mei-Yuh Hwang, Kai-Fu Lee, and Ronald Rosenfeld. 1993. The SPHINXII speech recognition system: an overview. Computer, Speech and Language, pages 137-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Hull</author>
<author>Gregory Grefenstette</author>
</authors>
<title>A dictionary-based approach to multilingual informaion retrieval.</title>
<date>1996</date>
<booktitle>In Proceedings of the 19th International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>49--57</pages>
<contexts>
<context position="3112" citStr="Hull and Grefenstette, 1996" startWordPosition="491" endWordPosition="494">To date, the few sentence-based search engines do not seem to take advantage of context information in the query, but merely extracting key words from the query sentence (AskJeeves, 1998; ElectricMonk, 1998). In addition to the need for better query understanding methods for a large variety of domains, it has also become important to handle queries in different languages. Crosslanguage information retrieval has emerged as an important area as the amount of nonEnglish material is ever increasing (Oard, 1997; Grefenstette, 1998; Ballesteros and Croft, 1998; Picchi and Peters, 1998; Davis, 1998; Hull and Grefenstette, 1996). One of the important tasks of cross-language IR is to translate queries from one language to another. The original query and the translated query are then used to match documents in both the source and target languages. Target language documents are either glossed or translated by other systems. According to (Grefenstette, 1998), three main problems of query translations are: 1. generating translation candidates, 2. weighting translation candidates, and 333 3. pruning translation alternatives for document matching. In cross-language IR, key word disambiguation is even more critical than in m</context>
</contexts>
<marker>Hull, Grefenstette, 1996</marker>
<rawString>David A. Hull and Gregory Grefenstette. 1996. A dictionary-based approach to multilingual informaion retrieval. In Proceedings of the 19th International Conference on Research and Development in Information Retrieval, pages 49-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>An algorithm for finding noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>17--22</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7258" citStr="Kupiec, 1993" startWordPosition="1168" endWordPosition="1169">ndary language. Our methods allows for both general and crosslanguage IR from a mixed language query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; </context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Julian Kupiec. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics, pages 17-22, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohu Liu</author>
<author>Sheng Li</author>
</authors>
<title>Statistic-based target word selection in English-Chinese machine translation.</title>
<date>1997</date>
<journal>Journal</journal>
<institution>of Harbin Institute of Technology,</institution>
<contexts>
<context position="7334" citStr="Liu and Li, 1997" startWordPosition="1179" endWordPosition="1182"> from a mixed language query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja</context>
</contexts>
<marker>Liu, Li, 1997</marker>
<rawString>Xiaohu Liu and Sheng Li. 1997. Statistic-based target word selection in English-Chinese machine translation. Journal of Harbin Institute of Technology, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi Yuen Ma</author>
<author>Pascale Fung</author>
</authors>
<title>Using English phoneme models for Chinese speech recognition.</title>
<date>1998</date>
<booktitle>In International Symposium on Chinese Spoken language processing.</booktitle>
<contexts>
<context position="1568" citStr="Ma and Fung, 1998" startWordPosition="237" endWordPosition="240">itous World Wide Web. The Web is also a powerful platform for another application—interactive spoken language query systems. Traditionally, such systems were implemented on stand-alone kiosks. Now we can easily use the Web as a platform. Information such as airline schedules, movie reservation, car trading, etc., can all be included in HTML files, to be accessed by a generic spoken interface to the Web browser (Zue, 1995; DiDio, 1997; Raymond, 1997; Fang et al., 1998a). Our team has built a multilingual spoken language interface to the Web, named SALSA (Fung et al., 1998b; Fung et al., 1998a; Ma and Fung, 1998). Users can use speech to surf the net via various links as well as issue search commands such as &amp;quot;Show me the latest movie of Jacky Chan&amp;quot;. The system recognizes commands and queries in English, Mandarin and Cantonese, as well as mixed language sentences. Until recently, most of the search engines handle keyword based queries where the user types in a series of strings without syntactic structure. The choice of key words in this case determines the success rate of the search. In many situations, the key words are ambiguous. To resolve ambiguity, query expansion is usually employed to look for </context>
</contexts>
<marker>Ma, Fung, 1998</marker>
<rawString>Chi Yuen Ma and Pascale Fung. 1998. Using English phoneme models for Chinese speech recognition. In International Symposium on Chinese Spoken language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W Oard</author>
</authors>
<title>Alternative approaches for cross-language text retrieval.</title>
<date>1997</date>
<booktitle>In AAAI Symposium on cross-language text and speech retrieval. American Association for Artificial Intelligence,</booktitle>
<contexts>
<context position="2995" citStr="Oard, 1997" startWordPosition="477" endWordPosition="478">importantly, they provide more contextual information which are important for query understanding. To date, the few sentence-based search engines do not seem to take advantage of context information in the query, but merely extracting key words from the query sentence (AskJeeves, 1998; ElectricMonk, 1998). In addition to the need for better query understanding methods for a large variety of domains, it has also become important to handle queries in different languages. Crosslanguage information retrieval has emerged as an important area as the amount of nonEnglish material is ever increasing (Oard, 1997; Grefenstette, 1998; Ballesteros and Croft, 1998; Picchi and Peters, 1998; Davis, 1998; Hull and Grefenstette, 1996). One of the important tasks of cross-language IR is to translate queries from one language to another. The original query and the translated query are then used to match documents in both the source and target languages. Target language documents are either glossed or translated by other systems. According to (Grefenstette, 1998), three main problems of query translations are: 1. generating translation candidates, 2. weighting translation candidates, and 333 3. pruning translat</context>
</contexts>
<marker>Oard, 1997</marker>
<rawString>D.W. Oard. 1997. Alternative approaches for cross-language text retrieval. In AAAI Symposium on cross-language text and speech retrieval. American Association for Artificial Intelligence, Mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugenio Picchi</author>
<author>Carol Peters</author>
</authors>
<title>Crosslanguage information retrieval: a system for comparable corpus querying.</title>
<date>1998</date>
<booktitle>Cross-language Information Retrieval,</booktitle>
<pages>81--92</pages>
<editor>In Gregory Grefenstette, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3069" citStr="Picchi and Peters, 1998" startWordPosition="485" endWordPosition="488">re important for query understanding. To date, the few sentence-based search engines do not seem to take advantage of context information in the query, but merely extracting key words from the query sentence (AskJeeves, 1998; ElectricMonk, 1998). In addition to the need for better query understanding methods for a large variety of domains, it has also become important to handle queries in different languages. Crosslanguage information retrieval has emerged as an important area as the amount of nonEnglish material is ever increasing (Oard, 1997; Grefenstette, 1998; Ballesteros and Croft, 1998; Picchi and Peters, 1998; Davis, 1998; Hull and Grefenstette, 1996). One of the important tasks of cross-language IR is to translate queries from one language to another. The original query and the translated query are then used to match documents in both the source and target languages. Target language documents are either glossed or translated by other systems. According to (Grefenstette, 1998), three main problems of query translations are: 1. generating translation candidates, 2. weighting translation candidates, and 333 3. pruning translation alternatives for document matching. In cross-language IR, key word dis</context>
</contexts>
<marker>Picchi, Peters, 1998</marker>
<rawString>Eugenio Picchi and Carol Peters. 1998. Crosslanguage information retrieval: a system for comparable corpus querying. In Gregory Grefenstette, editor, Cross-language Information Retrieval, pages 81-92. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lau Raymond</author>
</authors>
<title>Webgalaxy : Beyond point and click - a conversational interface to a browser.</title>
<date>1997</date>
<journal>In Computer Netowrks &amp; ISDN Systems,</journal>
<pages>1385--1393</pages>
<contexts>
<context position="1402" citStr="Raymond, 1997" startWordPosition="207" endWordPosition="209">wo features are 81.37% and 83.72%, compared to the baseline accuracy of 75.50%. 1 Introduction Online information retrieval is now prevalent because of the ubiquitous World Wide Web. The Web is also a powerful platform for another application—interactive spoken language query systems. Traditionally, such systems were implemented on stand-alone kiosks. Now we can easily use the Web as a platform. Information such as airline schedules, movie reservation, car trading, etc., can all be included in HTML files, to be accessed by a generic spoken interface to the Web browser (Zue, 1995; DiDio, 1997; Raymond, 1997; Fang et al., 1998a). Our team has built a multilingual spoken language interface to the Web, named SALSA (Fung et al., 1998b; Fung et al., 1998a; Ma and Fung, 1998). Users can use speech to surf the net via various links as well as issue search commands such as &amp;quot;Show me the latest movie of Jacky Chan&amp;quot;. The system recognizes commands and queries in English, Mandarin and Cantonese, as well as mixed language sentences. Until recently, most of the search engines handle keyword based queries where the user types in a series of strings without syntactic structure. The choice of key words in this c</context>
</contexts>
<marker>Raymond, 1997</marker>
<rawString>Lau Raymond. 1997. Webgalaxy : Beyond point and click - a conversational interface to a browser. In Computer Netowrks &amp; ISDN Systems, pages 1385-1393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rony Rosenfeld</author>
</authors>
<title>A Corpus-Based Approach to Language Learning.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="13148" citStr="Rosenfeld, 1995" startWordPosition="2174" endWordPosition="2175">aseline method uses only the neighboring word to disambiguate C. Is one or two neighboring word really sufficient for disambiguation? The intuition for choosing the nearest neighboring word Ey as the disambiguating feature for C is based on the assumption that they are part of a phrase or collocation term, and that there is only one sense per collocation (Dagan and Itai, 1994; Yarowsky, 1993). However, in most cases where C is a single word, there might be some other words which are more useful for disambiguating C. In fact, such long-distance dependency occurs frequently in natural language (Rosenfeld, 1995; Huang et al., 1993). Another reason against using single neighboring word comes from (Gale and Church, 1994) where it is argued that as many as 100,000 context words might be needed to have high disambiguation accuracy. (Shiitze, 1992; Yarowsky, 1995) all use multiple context words as discriminating features. We have also demonstrated in our domain translation task that multiple context words are useful (Fung and Lo, 1998; Fung and McKeown, 1997). Based on the above arguments, we enlarge the disambiguation window to be the entire sentence instead of only one word to the left or right. We use</context>
<context position="15430" citStr="Rosenfeld, 1995" startWordPosition="2606" endWordPosition="2607">-best contextual word as disambiguating feature In the above voting scheme, a candidate receives either a one vote or a zero vote from all contex0 Word in primary language • Word in soeondary language 0 Selected translation 336 tual words equally no matter how these words are related to C. As an example, in the query &amp;quot;Please show me the latest dianying/movie of Jacky Chan&amp;quot;, the and Jacky are considered to be equally important. We believe however, that if the most powerful word is chosen for disambiguation, we can expect better performance. This is related to the concept of &amp;quot;trigger pairs&amp;quot; in (Rosenfeld, 1995) and Singular Value Decomposition in (Shiitze, 1992). In (Dagan and Itai, 1994), syntactic relationship is used to find the most powerful &amp;quot;trigger word&amp;quot;. Since syntactic relationship is unavailable in a mixed language sentence, we have to use other type of information. In this method, we want to choose the best trigger word among all contextual words. Referring again to Table 1, M/jci is the mutual information score between contextual word Ej and translation candidate Ect. We compute the disambiguation contribution ratio for each context word E. For each row j in Table 1, the largest MI score </context>
<context position="21887" citStr="Rosenfeld, 1995" startWordPosition="3660" endWordPosition="3662">ghboring word for disambiguation. The performance of the voting method is better than the baseline because more contextual words are used. The results are consistent with the idea in (Gale and Church, 1994; Shiitze, 1992; Yarowsky, 1995). In our experiments, it is found that 1-best contextual word is even better than multiple contextual words. This seemingly counterintuitive result leads us to believe that choosing the most discriminative single word is even more powerful than using multiple contextual word equally. We believe that this is consistent with the idea of using &amp;quot;trigger pairs&amp;quot; in (Rosenfeld, 1995) and Singular Value Decomposition in (Shiitze, 1992). We can conclude that sometimes longdistance contextual words are more discriminant than immediate neighboring words, and that multiple contextual words can contribute to better disambiguation.Our results support our belief that natural sentence-based queries are less ambiguous than keyword based queries. Our method using multiple disambiguating contextual words can take advantage of syntactic information even when parsing or tagging is not possible, such as in the case of mixed-language queries. Other advantages of our approach include: (1)</context>
</contexts>
<marker>Rosenfeld, 1995</marker>
<rawString>Rony Rosenfeld. 1995. A Corpus-Based Approach to Language Learning. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Shfitze</author>
</authors>
<title>Dimensions of meaning.</title>
<date>1992</date>
<booktitle>In Proceedings of Supercomputing &apos;92.</booktitle>
<marker>Shfitze, 1992</marker>
<rawString>Hinrich Shfitze. 1992. Dimensions of meaning. In Proceedings of Supercomputing &apos;92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Kathleen McKeown</author>
<author>Vasileios Hatzsivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="7244" citStr="Smadja et al., 1996" startWordPosition="1164" endWordPosition="1167">IR system in the secondary language. Our methods allows for both general and crosslanguage IR from a mixed language query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 19</context>
<context position="10621" citStr="Smadja et al., 1996" startWordPosition="1729" endWordPosition="1732">la, where E is a word and f (E) is the frequency of word E. , M I (E,, Ej) = f (E, Ej) log f (E,) * f(E3) (1) E, and Ej can be either neighboring words or any two words in the sentence. 2.3 Translation candidate pruning The last problem in query translation is selecting the target translation. In our approach, we need to choose a particular Ec from E. We call this pruning process translation disambiguation. We present and compare three unsupervised statistical methods in this paper. The first baseline method is similar to (Dagan et al., 1991; Dagan and Itai, 1994; Ballesteros and Croft, 1998; Smadja et al., 1996), where we use the nearest neighboring word of the secondary language word C as feature for disambiguation. In the second method, we choose all contextual words as disambiguating feature. In the third method, the most discriminative contextual word is selected as feature. 2.3.1 Baseline: single neighboring word as disambiguating feature The first disambiguating feature we present here is similar to the statistical feature in (Dagan et al., 1991; Smadja et al., 1996; Dagan and Itai, 1994; Ballesteros and Croft, 1998), namely the co-occurrence with neighboring words. We do not use any syntactic </context>
</contexts>
<marker>Smadja, McKeown, Hatzsivassiloglou, 1996</marker>
<rawString>Frank Smadja, Kathleen McKeown, and Vasileios Hatzsivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 21(4):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>Retrieving collocations from text:</title>
<date>1993</date>
<booktitle>Xtract. Computational Linguistics,</booktitle>
<pages>19--1</pages>
<contexts>
<context position="7182" citStr="Smadja, 1993" startWordPosition="1156" endWordPosition="1157"> secondary language and the query is passed to another IR system in the secondary language. Our methods allows for both general and crosslanguage IR from a mixed language query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel</context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>Frank Smadja. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kumiko Tanaka</author>
<author>Hideya Iwasaki</author>
</authors>
<title>Extraction of lexical translations from nonaligned corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING 96,</booktitle>
<location>Copenhagan, Danmark,</location>
<contexts>
<context position="7361" citStr="Tanaka and Iwasaki, 1996" startWordPosition="1183" endWordPosition="1186">uage query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; </context>
</contexts>
<marker>Tanaka, Iwasaki, 1996</marker>
<rawString>Kumiko Tanaka and Hideya Iwasaki. 1996. Extraction of lexical translations from nonaligned corpora. In Proceedings of COLING 96, Copenhagan, Danmark, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delcai Wu</author>
</authors>
<title>Grammarless extraction of phrasal translation examples from parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of TMI 95,</booktitle>
<location>Leuven, Belgium,</location>
<note>Submitted.</note>
<contexts>
<context position="7268" citStr="Wu, 1995" startWordPosition="1170" endWordPosition="1171">. Our methods allows for both general and crosslanguage IR from a mixed language query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Delcai Wu. 1995. Grammarless extraction of phrasal translation examples from parallel texts. In Proceedings of TMI 95, Leuven, Belgium, July. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proceedings of ARPA Human Language Technology Workshop,</booktitle>
<location>Princeton.</location>
<contexts>
<context position="12928" citStr="Yarowsky, 1993" startWordPosition="2138" endWordPosition="2139">ng feature represented by the dotted line. Ey is the neighboring word for C. Since M/i is greater than M/2, Eci is selected as the translation of C. 2.3.2 Voting: multiple contextual words as disambiguating feature The baseline method uses only the neighboring word to disambiguate C. Is one or two neighboring word really sufficient for disambiguation? The intuition for choosing the nearest neighboring word Ey as the disambiguating feature for C is based on the assumption that they are part of a phrase or collocation term, and that there is only one sense per collocation (Dagan and Itai, 1994; Yarowsky, 1993). However, in most cases where C is a single word, there might be some other words which are more useful for disambiguating C. In fact, such long-distance dependency occurs frequently in natural language (Rosenfeld, 1995; Huang et al., 1993). Another reason against using single neighboring word comes from (Gale and Church, 1994) where it is argued that as many as 100,000 context words might be needed to have high disambiguation accuracy. (Shiitze, 1992; Yarowsky, 1995) all use multiple context words as discriminating features. We have also demonstrated in our domain translation task that multi</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>D. Yarowsky. 1993. One sense per collocation. In Proceedings of ARPA Human Language Technology Workshop, Princeton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Conference of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7613" citStr="Yarowsky, 1995" startWordPosition="1228" endWordPosition="1229">ning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiltze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Banesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), parallel corpora are rare in most domains. We want to devise a method that uses only monolingual data in the primary language to train co-occurrence infor</context>
<context position="13401" citStr="Yarowsky, 1995" startWordPosition="2217" endWordPosition="2218">on that they are part of a phrase or collocation term, and that there is only one sense per collocation (Dagan and Itai, 1994; Yarowsky, 1993). However, in most cases where C is a single word, there might be some other words which are more useful for disambiguating C. In fact, such long-distance dependency occurs frequently in natural language (Rosenfeld, 1995; Huang et al., 1993). Another reason against using single neighboring word comes from (Gale and Church, 1994) where it is argued that as many as 100,000 context words might be needed to have high disambiguation accuracy. (Shiitze, 1992; Yarowsky, 1995) all use multiple context words as discriminating features. We have also demonstrated in our domain translation task that multiple context words are useful (Fung and Lo, 1998; Fung and McKeown, 1997). Based on the above arguments, we enlarge the disambiguation window to be the entire sentence instead of only one word to the left or right. We use all the contextual words in the query sentence. Each contextual word &amp;quot;votes&amp;quot; by its mutual information with all translation candidates. Suppose there are n primary language words in S = El, E2, . . , C, . . . , En, as shown in Figure 2, we compute mutu</context>
<context position="21508" citStr="Yarowsky, 1995" startWordPosition="3600" endWordPosition="3601">or the voting model is 81.37% and for the 1-best model, 83.72%. The baseline method uses only the neighboring word to disambiguate C. The assumption is that the neighboring word is the most semantic relevant. This method leaves out an important feature of nature language: long distance dependency. Experimental results show that it is not sufficient to use only the nearest neighboring word for disambiguation. The performance of the voting method is better than the baseline because more contextual words are used. The results are consistent with the idea in (Gale and Church, 1994; Shiitze, 1992; Yarowsky, 1995). In our experiments, it is found that 1-best contextual word is even better than multiple contextual words. This seemingly counterintuitive result leads us to believe that choosing the most discriminative single word is even more powerful than using multiple contextual word equally. We believe that this is consistent with the idea of using &amp;quot;trigger pairs&amp;quot; in (Rosenfeld, 1995) and Singular Value Decomposition in (Shiitze, 1992). We can conclude that sometimes longdistance contextual words are more discriminant than immediate neighboring words, and that multiple contextual words can contribute </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Conference of the Association for Computational Linguistics, pages 189-196. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Zue</author>
</authors>
<title>Spoken language interfaces to computers: Achievements and challenges.</title>
<date>1995</date>
<booktitle>In The 33rd Annual Meeting of the Association of Computational Linguistics,</booktitle>
<location>Boston,</location>
<contexts>
<context position="1374" citStr="Zue, 1995" startWordPosition="203" endWordPosition="204">ation accuracy for the two features are 81.37% and 83.72%, compared to the baseline accuracy of 75.50%. 1 Introduction Online information retrieval is now prevalent because of the ubiquitous World Wide Web. The Web is also a powerful platform for another application—interactive spoken language query systems. Traditionally, such systems were implemented on stand-alone kiosks. Now we can easily use the Web as a platform. Information such as airline schedules, movie reservation, car trading, etc., can all be included in HTML files, to be accessed by a generic spoken interface to the Web browser (Zue, 1995; DiDio, 1997; Raymond, 1997; Fang et al., 1998a). Our team has built a multilingual spoken language interface to the Web, named SALSA (Fung et al., 1998b; Fung et al., 1998a; Ma and Fung, 1998). Users can use speech to surf the net via various links as well as issue search commands such as &amp;quot;Show me the latest movie of Jacky Chan&amp;quot;. The system recognizes commands and queries in English, Mandarin and Cantonese, as well as mixed language sentences. Until recently, most of the search engines handle keyword based queries where the user types in a series of strings without syntactic structure. The c</context>
</contexts>
<marker>Zue, 1995</marker>
<rawString>Victor Zue. 1995. Spoken language interfaces to computers: Achievements and challenges. In The 33rd Annual Meeting of the Association of Computational Linguistics, Boston, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>