<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019729">
<title confidence="0.966958">
Development and Analysis of NLP Pipelines in Argo
</title>
<author confidence="0.99844">
Rafal Rak, Andrew Rowley, Jacob Carter, and Sophia Ananiadou
</author>
<affiliation confidence="0.996447">
National Centre for Text Mining
School of Computer Science, University of Manchester
Manchester Institute of Biotechnology
</affiliation>
<address confidence="0.939526">
131 Princess St, M1 7DN, Manchester, UK
</address>
<email confidence="0.998572">
{rafal.rak,andrew.rowley,jacob.carter,sophia.ananiadou}@manchester.ac.uk
</email>
<sectionHeader confidence="0.993879" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999815941176471">
Developing sophisticated NLP pipelines
composed of multiple processing tools
and components available through differ-
ent providers may pose a challenge in
terms of their interoperability. The Un-
structured Information Management Ar-
chitecture (UIMA) is an industry stan-
dard whose aim is to ensure such in-
teroperability by defining common data
structures and interfaces. The architec-
ture has been gaining attention from in-
dustry and academia alike, resulting in a
large volume of UIMA-compliant process-
ing components. In this paper, we demon-
strate Argo, a Web-based workbench for
the development and processing of NLP
pipelines/workflows. The workbench is
based upon UIMA, and thus has the poten-
tial of using many of the existing UIMA
resources. We present features, and show
examples, of facilitating the distributed de-
velopment of components and the analysis
of processing results. The latter includes
annotation visualisers and editors, as well
as serialisation to RDF format, which en-
ables flexible querying in addition to data
manipulation thanks to the semantic query
language SPARQL. The distributed devel-
opment feature allows users to seamlessly
connect their tools to workflows running
in Argo, and thus take advantage of both
the available library of components (with-
out the need of installing them locally) and
the analytical tools.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99775865">
Building NLP applications usually involves a se-
ries of individual tasks. For instance, the ex-
traction of relationships between named entities
in text is preceded by text segmentation, part-of-
speech recognition, the recognition of named enti-
ties, and dependency parsing. Currently, the avail-
ability of such atomic processing components is
no longer an issue; the problem lies in ensur-
ing their compatibility, as combining components
coming from multiple repositories, written in dif-
ferent programming languages, requiring different
installation procedures, and having incompatible
input/output formats can be a source of frustration
and poses a real challenge for developers.
Unstructured Information Management Archi-
tecture (UIMA) (Ferrucci and Lally, 2004) is a
framework that tackles the problem of interoper-
ability of processing components. Originally de-
veloped by IBM, it is currently an Apache Soft-
ware Foundation open-source project1 that is also
registered at the Organization for the Advance-
ment of Structured Information Standards (OA-
SIS)2. UIMA has been gaining much interest from
industry and academia alike for the past decade.
Notable repositories of UIMA-compliant tools
include U-Compare component library3, DKPro
(Gurevych et al., 2007), cTAKES (Savova et
al., 2010), BioNLP-UIMA Component Reposi-
tory (Baumgartner et al., 2008), and JULIE Lab’s
UIMA Component Repository (JCoRe) (Hahn et
al., 2008).
In this work we demonstrate Argo4, a Web-
based (remotely-accessed) workbench for collabo-
rative development of text-processing workflows.
We focus primarily on the process of development
and analysis of both individual processing com-
ponents and workflows composed of such compo-
nents.
The next section demonstrates general features
of Argo and lays out several technical details about
</bodyText>
<footnote confidence="0.99998525">
1http://uima.apache.org
2http://www.oasis-open.org/committees/uima
3http://nactem.ac.uk/ucompare/
4http://argo.nactem.ac.uk
</footnote>
<page confidence="0.959498">
115
</page>
<bodyText confidence="0.823230125">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 115–120,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
UIMA that will ease the understanding of the re-
maining sections. Sections 3–5 discuss selected
features that are useful in the development and
analysis of components and workflows. Section 6
mentions related efforts, and Section 7 concludes
the paper.
</bodyText>
<sectionHeader confidence="0.716117" genericHeader="method">
2 Overview of Argo
</sectionHeader>
<bodyText confidence="0.999921069767442">
Argo comes equipped with an ever-growing li-
brary of atomic processing components that can be
put together by users to form meaningful pipelines
or workflows. The processing components range
from simple data serialisers to complex text an-
alytics and include text segmentation, part-of-
speech tagging, parsing, named entity recognition,
and discourse analysis.
Users interact with the workbench through a
graphical user interface (GUI) that is accessible
entirely through a Web browser. Figure 1 shows
two views of the interface: the main, resource
management window (Figure 1(a)) and the work-
flow diagramming window (Figure 1(b)). The
main window provides access to emphdocuments,
workflows, and processes separated in easily ac-
cessible panels.
The Documents panel lists primarily user-
owned files that are uploaded (through the GUI)
by users into their respective personal spaces on
the remote host. Documents may also be gener-
ated as a result of executing workflows (e.g., XML
files containing annotations), in which case they
are available for users to download.
The Workflows panel lists users’ workflows,
i.e., the user-defined arrangements of processing
components together with their settings. Users
compose workflows through a flexible, graphi-
cal diagramming editor by connecting the com-
ponents (represented as blocks) with lines signi-
fying the flow of data between components (see
Figure 1(b)). The most common arrangement is to
form a pipeline, i.e., each participating component
has at most one incoming and at most one out-
going connection; however, the system also sup-
ports multiple branching and merging points in the
workflow. An example is shown in Figure 2 dis-
cussed farther in text. For ease of use, components
are categorized into readers, analytics, and con-
sumers, indicating what role they are set to play in
a workflow. Readers are responsible for delivering
data for processing and have only an outgoing port
(represented as a green triangle). The role of an-
</bodyText>
<figure confidence="0.9973625">
(a) Workflow management view
(b) Worflow diagram editor view
</figure>
<figureCaption confidence="0.9803755">
Figure 1: Screenshots of Argo Web browser con-
tent.
</figureCaption>
<bodyText confidence="0.999010333333333">
alytics is to modify incoming data structures and
pass them onto following components in a work-
flow, and thus they have both incoming and outgo-
ing ports. Finally, the consumers are responsible
for serialising or visualising (selected or all) anno-
tations in the data structures without modification,
and so they have only an incoming port.
The Processes panel lists resources that are cre-
ated automatically when workflows are submit-
ted for execution by users. Users may follow the
progress of the executing workflows (processes) as
well as manage the execution from this panel. The
processing of workflows is carried out on remote
servers, and thus frees users from using their own
processing resources.
</bodyText>
<subsectionHeader confidence="0.990444">
2.1 Argo and UIMA
</subsectionHeader>
<bodyText confidence="0.997233">
Argo supports and is based upon UIMA and thus
can run any UIMA-compliant processing compo-
nent. Each such component defines or imports
type systems and modifies common annotation
structures (CAS). A type system is the represen-
</bodyText>
<page confidence="0.994689">
116
</page>
<bodyText confidence="0.999987133333333">
tation of a data model that is shared between com-
ponents, whereas a CAS is the container of data
whose structure complies with the type system. A
CAS stores feature structures, e.g., a token with
its text boundaries and a part-of-speech tag. Fea-
ture structures may, and often do, refer to a sub-
ject of annotation (Sofa), a structure that (in text-
processing applications) stores the text. UIMA
comes with built-in data types including primitive
types (boolean, integer, string, etc.), arrays, lists,
as well as several complex types, e.g., Annotation
that holds a reference to a Sofa the annotation is
asserted about, and two features, begin and end,
for marking boundaries of a span of text. A devel-
oper is free to extend any of the complex types.
</bodyText>
<subsectionHeader confidence="0.998874">
2.2 Architecture
</subsectionHeader>
<bodyText confidence="0.999958666666667">
Although the Apache UIMA project provides an
implementation of the UIMA framework, Argo
incorporates home-grown solutions, especially in
terms of the management of workflow processing.
This includes features such as workflow branching
and merging points, user-interactive components
(see Section 4), as well as distributed processing.
The primary processing is carried out on a
multi-core server. Additionally, in order to in-
crease computing throughput, we have incorpo-
rated cloud computing capabilities into Argo,
which is designed to work with various cloud
computing providers. As a proof of concept,
the current implementation uses HTCondor, an
open-source, high-throughput computing software
framework. Currently, Argo is capable of switch-
ing the processing of workflows to a local cluster
of over 3,000 processor cores. Further extensions
to use the Microsoft Azure5 and Amazon EC26
cloud platforms are also planned.
The Argo platform is available entirely us-
ing RESTful Web services (Fielding and Taylor,
2002), and therefore it is possible to gain access
to all or selected features of Argo by implement-
ing a compliant client. In fact, the “native” Web
interface shown in Figure 1 is an example of such
a client.
</bodyText>
<sectionHeader confidence="0.999299" genericHeader="method">
3 Distributed Development
</sectionHeader>
<bodyText confidence="0.999045666666667">
Argo includes a Generic Listener component that
permits execution of a UIMA component that is
running externally of the Argo system. It is pri-
</bodyText>
<footnote confidence="0.99957">
5http://www.windowsazure.com
6http://aws.amazon.com/ec2
</footnote>
<bodyText confidence="0.9999205">
marily intended to be used during the develop-
ment of processing components, as it allows a de-
veloper to rapidly make any necessary changes,
whilst continuing to make use of the existing com-
ponents available within Argo, which may other-
wise be unavailable if developing on the devel-
oper’s local system. Any component that a user
wishes to deploy on the Argo system has to un-
dergo a verification process, which could lead to
a slower development lifecycle without the avail-
ability of this component.
Generic Listener operates in a reverse manner
to a traditional Web service; rather than Argo con-
necting to the developer’s component, the compo-
nent connects to Argo. This behaviour was de-
liberately chosen to avoid network-related issues,
such as firewall port blocking, which could be-
come a source of frustration to developers.
When a workflow, containing a Generic Lis-
tener, is executed within Argo, it will continue
as normal until the point at which the Generic
Listener receives its first CAS object. Argo will
prompt the user with a unique URL, which must
be supplied to the client component run by the
user, allowing it to connect to the Argo workflow
and continue its execution.
A skeleton Java project has been provided to as-
sist in the production of such components. It con-
tains a Maven structure, Eclipse IDE project files,
and required libraries, in addition to a number of
shell scripts to simplify the running of the compo-
nent. The project provides both a command-line
interface (CLI) and GUI runner applications that
take, as arguments, the name of the class of the lo-
cally developed component and the URL provided
by Argo, upon each run of a workflow containing
the remote component.
An example of a workflow with a Generic Lis-
tener is shown in Figure 2. The workflow is de-
signed for the analysis and evaluation of a solu-
tion (in this case, the automatic extraction of bio-
logical events) that is being developed locally by
the user. The reader (BioNLP ST Data Reader)
provides text documents together with gold (i.e.,
manually created) event annotations prepared for
the BioNLP Shared Task7. The annotations are
selectively removed with the Annotation Remover
and the remaining data is sent onto the Generic
Listener component, and consequently, onto the
developer’s machine. The developer’s task is to
</bodyText>
<footnote confidence="0.969202">
7http://2013.bionlp-st.org/
</footnote>
<page confidence="0.99481">
117
</page>
<figureCaption confidence="0.840920666666667">
Figure 2: Example of a workflow for development,
analysis, and evaluation of a user-developed solu-
tion for the BioNLP Shared Task.
</figureCaption>
<bodyText confidence="0.9995602">
connect to Argo, retrieve CASes from the run-
ning workflow, and for each CAS recreate the re-
moved annotations as faithfully as possible. The
developer can then track the performance of their
solution by observing standard information ex-
traction measures (precision, recall, etc.) com-
puted by the Reference Evaluator component that
compares the original, gold annotations (coming
from the reader) against the developer’s annota-
tions (coming from the Generic Listener), and
saves these measures for each document/CAS into
a tabular-format file. Moreover, the differences
can be tracked visually though the interactive Brat
BioNLP ST Comparator component, discussed in
the next section.
</bodyText>
<sectionHeader confidence="0.720859" genericHeader="method">
4 Annotation Analysis and Manipulation
</sectionHeader>
<bodyText confidence="0.997668428571429">
Traditionally, NLP pipelines (including existing
UIMA-supporting platforms), once set up, are
executed without human involvement. One of
the novelties in Argo is an introduction of user-
interactive components, a special type of analytic
that, if present in a workflow, cause the execu-
tion of the workflow to pause. Argo resumes the
execution only after receiving input from a user.
This feature allows for manual intervention in the
otherwise automatic processing by, e.g., manipu-
lating automatically created annotations. Exam-
ples of user-interactive components include Anno-
tation Editor and Brat BioNLP ST Comparator.
The Brat BioNLP ST Comparator component
</bodyText>
<figureCaption confidence="0.998698714285714">
Figure 3: Example of an annotated fragment of
a document visualised with the Brat BioNLP ST
Comparator component. The component high-
lights (in red and green) differences between two
sources of annotations.
Figure 4: Example of manual annotation with the
user-interactive Annotation Editor component.
</figureCaption>
<bodyText confidence="0.998675115384615">
expects two incoming connections from compo-
nents processing the same subject of annotation.
As a result, using brat visualisation (Stenetorp et
al., 2012), it will show annotation structures by
laying them out above text and mark differences
between the two inputs by colour-coding missing
or additional annotations in each input. A sam-
ple of visualisation coming from the workflow in
Figure 2 is shown in Figure 3. Since in this par-
ticular workflow the Brat BioNLP ST Comparator
receives gold annotations (from the BioNLP ST
Data Reader) as one of its inputs, the highlighted
differences are, in fact, false positives and false
negatives.
Annotation Editor is another example of a user-
interactive component that allows the user to add,
delete or modify annotations. Figure 4 shows the
editor in action. The user has an option to cre-
ate a span-of-text annotation by selecting a text
fragment and assigning an annotation type. More
complex annotation types, such as tokens with
part-of-speech tags or annotations that do not re-
fer to the text (meta-annotations) can be created
or modified using an expandable tree-like struc-
ture (shown on the right-hand side of the figure),
which makes it possible to create any annotation
</bodyText>
<page confidence="0.989843">
118
</page>
<figure confidence="0.880662142857143">
(a) Select query
neAText neACat neBText neBCat count
Ki-67 Protein p53 Protein 85
DC CellType p53 Protein 61
DC CellType KCOT Protein 47
(b) Results (fragment)
(c) Insert query
</figure>
<figureCaption confidence="0.998096">
Figure 5: Example of (a) a SPARQL query that returns biological interactions; (b) a fragment of retrieved
</figureCaption>
<bodyText confidence="0.774599">
results; and (c) a SPARQL query that creates new UIMA feature structures. Namespaces and data types
are omitted for brevity.
structure permissible by a given type system.
</bodyText>
<sectionHeader confidence="0.868145" genericHeader="method">
5 Querying Serialised Data
</sectionHeader>
<bodyText confidence="0.999871285714286">
Argo comes with several (de)serialisation com-
ponents for reading and storing collections of
data, such as a generic reader of text (Document
Reader) or readers and writers of CASes in XMI
format (CAS Reader and CAS Writer). One of
the more useful in terms of annotation analysis
is, however, the RDF Writer component as well
as its counterpart, RDF Reader. RDF Writer se-
rialises data into RDF files and supports several
RDF formats such as RDF/XML, Turtle, and N-
Triple. A resulting RDF graph consists of both the
data model (type system) and the data itself (CAS)
and thus constitutes a self-contained knowledge
base. RDF Writer has an option to create a graph
for each CAS or a single graph for an entire collec-
tion. Such a knowledge base can be queried with
languages such as SPARQL8, an official W3C
Recommendation.
Figure 5 shows an example of a SPARQL query
that is performed on the output of an RDF Writer
in the workflow shown in Figure 1(b). This work-
flow results in several types of annotations in-
cluding the boundaries of sentences, tokens with
part-of-speech tags and lemmas, chunks, as well
as biological entities, such as DNA, RNA, cell
line and cell type. The SPARQL query is meant
to retrieve pairs of seemingly interacting biolog-
ical entities ranked according to their occurrence
in the entire collection. The interaction here is
(naively) defined as co-occurrence of two entities
in the same sentence. The query includes pat-
terns for retrieving the boundaries of sentences
(syn:Sentence) and two biological entities
(sem:NamedEntity) and then filters out the
crossproduct of those by ensuring that the two en-
</bodyText>
<footnote confidence="0.916954">
8http://www.w3.org/TR/2013/REC-sparql11-overview-
20130321/
</footnote>
<page confidence="0.998291">
119
</page>
<bodyText confidence="0.9998715">
tities are enclosed in a sentence. As a result, the
query returns a list of biological entity pairs ac-
companied by their categories and the number of
appearances, as shown in Figure 5(b). Note that
the query itself does not list the four biological cat-
egories; instead, it requests their common seman-
tic ancestor sem:NamedEntity. This is one of
the advantages of using semantically-enabled lan-
guages, such as SPARQL.
SPARQL also supports graph manipulation.
Suppose a user is interested in placing the re-
trieved biological entity interactions from our run-
ning example into the UIMA structure Relation-
ship that simply defines a pair of references to
other structures of any type. This can be accom-
plished, without resorting to programming, by is-
suing a SPARQL insert query shown in Figure
5(c). The query will create triple statements com-
pliant with the definition of Relationship. The re-
sulting modified RDF graph can then be read back
to Argo by the RDF Reader component that will
convert the new RDF graph back into a CAS.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999951285714286">
Other notable examples of NLP platforms that
provide graphical interfaces for managing work-
flows include GATE (Cunningham et al., 2002)
and U-Compare (Kano et al., 2010). GATE is
a standalone suite of text processing and annota-
tion tools and comes with its own programming
interface. In contrast, U-Compare—similarly to
Argo—uses UIMA as its base interoperability
framework. The key features of Argo that distin-
guish it from U-Compare are the Web availabil-
ity of the platform, primarily remote processing
of workflows, a multi-user, collaborative architec-
ture, and the availability of user-interactive com-
ponents.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999887363636364">
Argo emerges as a one-stop solution for develop-
ing and processing NLP tasks. Moreover, the pre-
sented annotation viewer and editor, performance
evaluator, and lastly RDF (de)serialisers are in-
dispensable for the analysis of processing tasks
at hand. Together with the distributed develop-
ment support for developers wishing to create their
own components or run their own tools with the
help of resources available in Argo, the workbench
becomes a powerful development and analytical
NLP tool.
</bodyText>
<sectionHeader confidence="0.996639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.958867">
This work was partially funded by the MRC Text
Mining and Screening grant (MR/J005037/1).
</bodyText>
<sectionHeader confidence="0.995677" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999227021276596">
W A Baumgartner, K B Cohen, and L Hunter. 2008.
An open-source framework for large-scale, flexible
evaluation of biomedical text mining systems. Jour-
nal of biomedical discovery and collaboration, 3:1+.
H Cunningham, D Maynard, K Bontcheva, and
V Tablan. 2002. GATE: A framework and graphical
development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniver-
sary Meeting of the Association for Computational
Linguistics.
D Ferrucci and A Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327–348.
R T Fielding and R N Taylor. 2002. Principled de-
sign of the modern Web architecture. ACM Trans.
Internet Technol., 2(2):115–150, May.
I Gurevych, M M¨uhlh¨auser, C M¨uller, J Steimle,
M Weimer, and T Zesch. 2007. Darmstadt knowl-
edge processing repository based on uima. In Pro-
ceedings of the First Workshop on Unstructured
Information Management Architecture, T¨ubingen,
Germany.
U Hahn, E Buyko, R Landefeld, M M¨uhlhausen,
M Poprat, K Tomanek, and J Wermter. 2008. An
Overview of JCORE, the JULIE Lab UIMA Compo-
nent Repository. In Language Resources and Eval-
uation Workshop, Towards Enhanc. Interoperability
Large HLT Syst.: UIMA NLP, pages 1–8.
Y Kano, R Dorado, L McCrochon, S Ananiadou, and
J Tsujii. 2010. U-Compare: An integrated language
resource evaluation platform including a compre-
hensive UIMA resource library. In Proceedings of
the Seventh International Conference on Language
Resources and Evaluation, pages 428–434.
G K Savova, J J Masanz, P V Ogren, J Zheng, S Sohn,
K C Kipper-Schuler, and C G Chute. 2010. Mayo
clinical Text Analysis and Knowledge Extraction
System (cTAKES): architecture, component evalua-
tion and applications. Journal of the American Med-
ical Informatics Association, 17(5):507–513.
P Stenetorp, S Pyysalo, G Topi´c, T Ohta, S Ananiadou,
and J Tsujii. 2012. brat: a web-based tool for nlp-
assisted text annotation. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 102–107,
Avignon, France.
</reference>
<page confidence="0.996284">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.622864">
<title confidence="0.995561">Development and Analysis of NLP Pipelines in Argo</title>
<author confidence="0.71513">Rafal Rak</author>
<author confidence="0.71513">Andrew Rowley</author>
<author confidence="0.71513">Jacob Carter</author>
<author confidence="0.71513">Sophia</author>
<affiliation confidence="0.907229333333333">National Centre for Text School of Computer Science, University of Manchester Institute of</affiliation>
<address confidence="0.990748">131 Princess St, M1 7DN, Manchester, UK</address>
<abstract confidence="0.998671485714286">Developing sophisticated NLP pipelines composed of multiple processing tools and components available through different providers may pose a challenge in terms of their interoperability. The Unstructured Information Management Architecture (UIMA) is an industry standard whose aim is to ensure such interoperability by defining common data structures and interfaces. The architecture has been gaining attention from industry and academia alike, resulting in a large volume of UIMA-compliant processing components. In this paper, we demonstrate Argo, a Web-based workbench for the development and processing of NLP pipelines/workflows. The workbench is based upon UIMA, and thus has the potential of using many of the existing UIMA resources. We present features, and show examples, of facilitating the distributed development of components and the analysis of processing results. The latter includes annotation visualisers and editors, as well as serialisation to RDF format, which enables flexible querying in addition to data manipulation thanks to the semantic query language SPARQL. The distributed development feature allows users to seamlessly connect their tools to workflows running in Argo, and thus take advantage of both the available library of components (without the need of installing them locally) and the analytical tools.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W A Baumgartner</author>
<author>K B Cohen</author>
<author>L Hunter</author>
</authors>
<title>An open-source framework for large-scale, flexible evaluation of biomedical text mining systems.</title>
<date>2008</date>
<journal>Journal of biomedical discovery and collaboration,</journal>
<volume>3</volume>
<contexts>
<context position="3054" citStr="Baumgartner et al., 2008" startWordPosition="439" endWordPosition="442">cture (UIMA) (Ferrucci and Lally, 2004) is a framework that tackles the problem of interoperability of processing components. Originally developed by IBM, it is currently an Apache Software Foundation open-source project1 that is also registered at the Organization for the Advancement of Structured Information Standards (OASIS)2. UIMA has been gaining much interest from industry and academia alike for the past decade. Notable repositories of UIMA-compliant tools include U-Compare component library3, DKPro (Gurevych et al., 2007), cTAKES (Savova et al., 2010), BioNLP-UIMA Component Repository (Baumgartner et al., 2008), and JULIE Lab’s UIMA Component Repository (JCoRe) (Hahn et al., 2008). In this work we demonstrate Argo4, a Webbased (remotely-accessed) workbench for collaborative development of text-processing workflows. We focus primarily on the process of development and analysis of both individual processing components and workflows composed of such components. The next section demonstrates general features of Argo and lays out several technical details about 1http://uima.apache.org 2http://www.oasis-open.org/committees/uima 3http://nactem.ac.uk/ucompare/ 4http://argo.nactem.ac.uk 115 Proceedings of th</context>
</contexts>
<marker>Baumgartner, Cohen, Hunter, 2008</marker>
<rawString>W A Baumgartner, K B Cohen, and L Hunter. 2008. An open-source framework for large-scale, flexible evaluation of biomedical text mining systems. Journal of biomedical discovery and collaboration, 3:1+.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18073" citStr="Cunningham et al., 2002" startWordPosition="2810" endWordPosition="2813"> running example into the UIMA structure Relationship that simply defines a pair of references to other structures of any type. This can be accomplished, without resorting to programming, by issuing a SPARQL insert query shown in Figure 5(c). The query will create triple statements compliant with the definition of Relationship. The resulting modified RDF graph can then be read back to Argo by the RDF Reader component that will convert the new RDF graph back into a CAS. 6 Related Work Other notable examples of NLP platforms that provide graphical interfaces for managing workflows include GATE (Cunningham et al., 2002) and U-Compare (Kano et al., 2010). GATE is a standalone suite of text processing and annotation tools and comes with its own programming interface. In contrast, U-Compare—similarly to Argo—uses UIMA as its base interoperability framework. The key features of Argo that distinguish it from U-Compare are the Web availability of the platform, primarily remote processing of workflows, a multi-user, collaborative architecture, and the availability of user-interactive components. 7 Conclusions Argo emerges as a one-stop solution for developing and processing NLP tasks. Moreover, the presented annota</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>H Cunningham, D Maynard, K Bontcheva, and V Tablan. 2002. GATE: A framework and graphical development environment for robust NLP tools and applications. In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ferrucci</author>
<author>A Lally</author>
</authors>
<date>2004</date>
<booktitle>Architectural Approach to Unstructured Information Processing in the Corporate Research Environment. Natural Language Engineering,</booktitle>
<pages>10--3</pages>
<publisher>UIMA: An</publisher>
<contexts>
<context position="2468" citStr="Ferrucci and Lally, 2004" startWordPosition="352" endWordPosition="355"> entities in text is preceded by text segmentation, part-ofspeech recognition, the recognition of named entities, and dependency parsing. Currently, the availability of such atomic processing components is no longer an issue; the problem lies in ensuring their compatibility, as combining components coming from multiple repositories, written in different programming languages, requiring different installation procedures, and having incompatible input/output formats can be a source of frustration and poses a real challenge for developers. Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004) is a framework that tackles the problem of interoperability of processing components. Originally developed by IBM, it is currently an Apache Software Foundation open-source project1 that is also registered at the Organization for the Advancement of Structured Information Standards (OASIS)2. UIMA has been gaining much interest from industry and academia alike for the past decade. Notable repositories of UIMA-compliant tools include U-Compare component library3, DKPro (Gurevych et al., 2007), cTAKES (Savova et al., 2010), BioNLP-UIMA Component Repository (Baumgartner et al., 2008), and JULIE La</context>
</contexts>
<marker>Ferrucci, Lally, 2004</marker>
<rawString>D Ferrucci and A Lally. 2004. UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment. Natural Language Engineering, 10(3-4):327–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T Fielding</author>
<author>R N Taylor</author>
</authors>
<title>Principled design of the modern Web architecture.</title>
<date>2002</date>
<journal>ACM Trans. Internet Technol.,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="8916" citStr="Fielding and Taylor, 2002" startWordPosition="1336" endWordPosition="1339">rver. Additionally, in order to increase computing throughput, we have incorporated cloud computing capabilities into Argo, which is designed to work with various cloud computing providers. As a proof of concept, the current implementation uses HTCondor, an open-source, high-throughput computing software framework. Currently, Argo is capable of switching the processing of workflows to a local cluster of over 3,000 processor cores. Further extensions to use the Microsoft Azure5 and Amazon EC26 cloud platforms are also planned. The Argo platform is available entirely using RESTful Web services (Fielding and Taylor, 2002), and therefore it is possible to gain access to all or selected features of Argo by implementing a compliant client. In fact, the “native” Web interface shown in Figure 1 is an example of such a client. 3 Distributed Development Argo includes a Generic Listener component that permits execution of a UIMA component that is running externally of the Argo system. It is pri5http://www.windowsazure.com 6http://aws.amazon.com/ec2 marily intended to be used during the development of processing components, as it allows a developer to rapidly make any necessary changes, whilst continuing to make use of</context>
</contexts>
<marker>Fielding, Taylor, 2002</marker>
<rawString>R T Fielding and R N Taylor. 2002. Principled design of the modern Web architecture. ACM Trans. Internet Technol., 2(2):115–150, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Gurevych</author>
<author>M M¨uhlh¨auser</author>
<author>C M¨uller</author>
<author>J Steimle</author>
<author>M Weimer</author>
<author>T Zesch</author>
</authors>
<title>Darmstadt knowledge processing repository based on uima.</title>
<date>2007</date>
<booktitle>In Proceedings of the First Workshop on Unstructured Information Management Architecture,</booktitle>
<location>T¨ubingen, Germany.</location>
<marker>Gurevych, M¨uhlh¨auser, M¨uller, Steimle, Weimer, Zesch, 2007</marker>
<rawString>I Gurevych, M M¨uhlh¨auser, C M¨uller, J Steimle, M Weimer, and T Zesch. 2007. Darmstadt knowledge processing repository based on uima. In Proceedings of the First Workshop on Unstructured Information Management Architecture, T¨ubingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hahn</author>
<author>E Buyko</author>
<author>R Landefeld</author>
<author>M M¨uhlhausen</author>
<author>M Poprat</author>
<author>K Tomanek</author>
<author>J Wermter</author>
</authors>
<date>2008</date>
<booktitle>An Overview of JCORE, the JULIE Lab UIMA Component Repository. In Language Resources and Evaluation Workshop, Towards Enhanc. Interoperability Large HLT Syst.: UIMA NLP,</booktitle>
<pages>1--8</pages>
<marker>Hahn, Buyko, Landefeld, M¨uhlhausen, Poprat, Tomanek, Wermter, 2008</marker>
<rawString>U Hahn, E Buyko, R Landefeld, M M¨uhlhausen, M Poprat, K Tomanek, and J Wermter. 2008. An Overview of JCORE, the JULIE Lab UIMA Component Repository. In Language Resources and Evaluation Workshop, Towards Enhanc. Interoperability Large HLT Syst.: UIMA NLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kano</author>
<author>R Dorado</author>
<author>L McCrochon</author>
<author>S Ananiadou</author>
<author>J Tsujii</author>
</authors>
<title>U-Compare: An integrated language resource evaluation platform including a comprehensive UIMA resource library.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation,</booktitle>
<pages>428--434</pages>
<contexts>
<context position="18107" citStr="Kano et al., 2010" startWordPosition="2816" endWordPosition="2819"> Relationship that simply defines a pair of references to other structures of any type. This can be accomplished, without resorting to programming, by issuing a SPARQL insert query shown in Figure 5(c). The query will create triple statements compliant with the definition of Relationship. The resulting modified RDF graph can then be read back to Argo by the RDF Reader component that will convert the new RDF graph back into a CAS. 6 Related Work Other notable examples of NLP platforms that provide graphical interfaces for managing workflows include GATE (Cunningham et al., 2002) and U-Compare (Kano et al., 2010). GATE is a standalone suite of text processing and annotation tools and comes with its own programming interface. In contrast, U-Compare—similarly to Argo—uses UIMA as its base interoperability framework. The key features of Argo that distinguish it from U-Compare are the Web availability of the platform, primarily remote processing of workflows, a multi-user, collaborative architecture, and the availability of user-interactive components. 7 Conclusions Argo emerges as a one-stop solution for developing and processing NLP tasks. Moreover, the presented annotation viewer and editor, performanc</context>
</contexts>
<marker>Kano, Dorado, McCrochon, Ananiadou, Tsujii, 2010</marker>
<rawString>Y Kano, R Dorado, L McCrochon, S Ananiadou, and J Tsujii. 2010. U-Compare: An integrated language resource evaluation platform including a comprehensive UIMA resource library. In Proceedings of the Seventh International Conference on Language Resources and Evaluation, pages 428–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Savova</author>
<author>J J Masanz</author>
<author>P V Ogren</author>
<author>J Zheng</author>
<author>S Sohn</author>
<author>K C Kipper-Schuler</author>
<author>C G Chute</author>
</authors>
<title>Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications.</title>
<date>2010</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>17</volume>
<issue>5</issue>
<contexts>
<context position="2993" citStr="Savova et al., 2010" startWordPosition="431" endWordPosition="434"> developers. Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004) is a framework that tackles the problem of interoperability of processing components. Originally developed by IBM, it is currently an Apache Software Foundation open-source project1 that is also registered at the Organization for the Advancement of Structured Information Standards (OASIS)2. UIMA has been gaining much interest from industry and academia alike for the past decade. Notable repositories of UIMA-compliant tools include U-Compare component library3, DKPro (Gurevych et al., 2007), cTAKES (Savova et al., 2010), BioNLP-UIMA Component Repository (Baumgartner et al., 2008), and JULIE Lab’s UIMA Component Repository (JCoRe) (Hahn et al., 2008). In this work we demonstrate Argo4, a Webbased (remotely-accessed) workbench for collaborative development of text-processing workflows. We focus primarily on the process of development and analysis of both individual processing components and workflows composed of such components. The next section demonstrates general features of Argo and lays out several technical details about 1http://uima.apache.org 2http://www.oasis-open.org/committees/uima 3http://nactem.ac</context>
</contexts>
<marker>Savova, Masanz, Ogren, Zheng, Sohn, Kipper-Schuler, Chute, 2010</marker>
<rawString>G K Savova, J J Masanz, P V Ogren, J Zheng, S Sohn, K C Kipper-Schuler, and C G Chute. 2010. Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. Journal of the American Medical Informatics Association, 17(5):507–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Stenetorp</author>
<author>S Pyysalo</author>
<author>G Topi´c</author>
<author>T Ohta</author>
<author>S Ananiadou</author>
<author>J Tsujii</author>
</authors>
<title>brat: a web-based tool for nlpassisted text annotation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>102--107</pages>
<location>Avignon, France.</location>
<marker>Stenetorp, Pyysalo, Topi´c, Ohta, Ananiadou, Tsujii, 2012</marker>
<rawString>P Stenetorp, S Pyysalo, G Topi´c, T Ohta, S Ananiadou, and J Tsujii. 2012. brat: a web-based tool for nlpassisted text annotation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 102–107, Avignon, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>