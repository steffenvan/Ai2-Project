<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000505">
<title confidence="0.996561666666667">
FUH (FernUniversit¨at in Hagen):
Metonymy Recognition Using
Different Kinds of Context for a Memory-Based Learner
</title>
<author confidence="0.976994">
Johannes Leveling
</author>
<affiliation confidence="0.8657585">
Intelligent Information and Communication Systems (IICS)
FernUniversit¨at in Hagen (University of Hagen)
</affiliation>
<email confidence="0.782323">
johannes.leveling@fernuni-hagen.de
</email>
<sectionHeader confidence="0.988666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999808393939394">
For the metonymy resolution task at
SemEval-2007, the use of a memory-based
learner to train classifiers for the identifica-
tion of metonymic location names is inves-
tigated. Metonymy is resolved on different
levels of granularity, differentiating between
literal and non-literal readings on the coarse
level; literal, metonymic, and mixed read-
ings on the medium level; and a number of
classes covering regular cases of metonymy
on a fine level. Different kinds of context
are employed to obtain different features:
1) a sequence of n1 synset IDs represent-
ing subordination information for nouns and
for verbs, 2) n2 prepositions, articles, modal,
and main verbs in the same sentence, and 3)
properties of n3 tokens in a context window
to the left and to the right of the location
name.
Different classifiers were trained on the
Mascara data set to determine which values
for the context sizes n1, n2, and n3 yield
the highest accuracy (n1 = 4, n2 = 3,
and n3 = 7, determined with the leave-one-
out method). Results from these classifiers
served as features for a combined classifier.
In the training phase, the combined classifier
achieved a considerably higher precision for
the Mascara data. In the SemEval submis-
sion, an accuracy of 79.8% on the coarse,
79.5% on the medium, and 78.5% on the
fine level is achieved (the baseline accuracy
is 79.4%).
</bodyText>
<sectionHeader confidence="0.999006" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965153846154">
Metonymy is typically defined as a figure of speech
in which a speaker uses one entity to refer to an-
other that is related to it (Lakoff and Johnson, 1980).
The identification of metonymy becomes important
for NLP tasks such as question answering (Stallard,
1993) or geographic information retrieval (Leveling
and Hartrumpf, 2006).
For regular cases of metonymy for locations and
organizations, Markert and Nissim have proposed
a set of metonymy classes. Annotating a subset of
the BNC (British National Corpus), they extracted a
set of metonymic proper nouns from two categories:
country names (Markert and Nissim, 2002) and or-
ganization names (Nissim and Markert, 2003).
In the metonymy resolution task at SemEval-
2007, the goal was to identify metonymic names in a
subset of the BNC. The task consists of two subtasks
for company and country names, which are further
divided into classification on a coarse level (recog-
nizing literal and non-literal readings), on a medium
level (differentiating non-literal readings into mixed
and metonymic readings), and on a fine level (iden-
tifying classes of regular metonymy, such as a name
referring to the population, place-for-people). The
task is described in more detail by Markert and Nis-
sim (2007).
</bodyText>
<sectionHeader confidence="0.971157" genericHeader="method">
2 System Description
</sectionHeader>
<subsectionHeader confidence="0.987955">
2.1 Tools and Resources
</subsectionHeader>
<bodyText confidence="0.861729">
The following tools and resources are used for the
metonymy classification:
</bodyText>
<listItem confidence="0.9788325">
• TiMBL 5.1 (Daelemans et al., 2004), a
memory-based learner for classification is em-
</listItem>
<page confidence="0.985982">
153
</page>
<bodyText confidence="0.83847775">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 153–156,
Prague, June 2007. c�2007 Association for Computational Linguistics
ployed for training the classifiers (supervised
learning).1
</bodyText>
<listItem confidence="0.910149117647059">
• Mascara 2.0 – Metonymy Annotation Scheme
And Robust Analysis (Markert and Nissim,
2003; Nissim and Markert, 2003; Markert
and Nissim, 2002) contains annotated data for
metonymic names from a subset of the the
BNC.
• WordNet 2.0 (Fellbaum, 1998) serves as a lin-
guistic resource for assigning synset IDs and
for looking up subordination information and
frequency of readings.
• The TreeTagger (Schmid, 1994) is utilized for
sentence boundary detection, lemmatization,
and part-of-speech tagging. The English tag-
ger was trained on the PENN treebank and uses
the English morphological database from the
XTAG project (Karp et al., 1992). The param-
eter files were obtained from the web site.2
</listItem>
<subsectionHeader confidence="0.995666">
2.2 Different Kinds of Context
</subsectionHeader>
<bodyText confidence="0.9999795">
Following the assumption that metonymic location
names can be identified from the context, there are
different kinds of context to consider. At most, the
context comprises a single sentence in this setup.
Three kinds of context were employed to extract fea-
tures for the memory-based learner TiMBL:
</bodyText>
<listItem confidence="0.94582925">
•
C1: Subordination (hyponymy) information for
nouns and verbs from the left and right context
of the possibly metonymic name.
• C2: The sentence context for modal verbs, main
verbs, prepositions, and articles.
• C3: A context window of tokens left and right
of the location name.
</listItem>
<bodyText confidence="0.999674666666667">
The trial data provided (a subset of the Mascara
data) contained 188 non-literal location names (of
925 samples total). For a supervised learning ap-
proach, this is too few data. Therefore, the full
Mascara data was converted to form training data
consisting of feature values for context C1, C2, and
</bodyText>
<footnote confidence="0.8953555">
1Peirsman (2006) also employs TiMBL for metonymy reso-
lution, but trains a single classifier.
2http://www.ims.uni-stuttgart.de/projek-
te/corplex/TreeTagger/
</footnote>
<bodyText confidence="0.999337857142857">
C3. The training data contained 509 metonymic an-
notations (of 2797 samples total). Some cases in
the Mascara corpus are filtered during processing,
including cases annotated as homonyms and cases
whose metonymy class could not be agreed upon.
The test data had a majority baseline of 82.8% accu-
racy for country names.
</bodyText>
<subsectionHeader confidence="0.974181">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.992106">
The Mascara data was processed to extract the fol-
lowing features (no hand-annotated data from Mas-
cara was employed for feature values, i.e. no gram-
matical roles):
</bodyText>
<listItem confidence="0.7017547">
• For C1 (WordNet context): From a context of
n1 verbs and nouns in the same sentence, their
distance to the location name is calculated. A
sequence of eight feature values of WordNet
synset IDs is obtained by iteratively looking up
the most frequent reading for a lemma in Word-
Net and determining its synset ID. Subordina-
tion information between synsets is used to find
a parent synset. This process is repeated until
a top-level parent synset is reached. No actual
word sense disambiguation is employed.
• For C2 (sentence context): Sentence bound-
aries, part-of-speech tags, and lemmatization
are determined from the TreeTagger output.
From a context window of n2 tokens, lemma
and distance are encoded as feature values for
prepositions, articles, modal, and main verbs
• For C3 (word context): From a context of n3
tokens to the left and to the right, the distance
between token and location name, three pre-
</listItem>
<bodyText confidence="0.915039416666667">
fix characters, three suffix characters, part-of-
speech tag, case information (U=upper case,
L=lower case, N=numeric, O=other), and word
length are used as feature values.
Table 1 and Table 2 show results for mem-
ory based learners trained with TiMBL. Perfor-
mance measures were obtained with the leave-one-
out method. The classifiers were trained on fea-
tures for different context sizes (ni ranging from 2
to 7) to determine the setting for which the highest
accuracy is achieved (e.g. 1,, 2,, and 3,). In the
next step, classifiers with a combined context were
</bodyText>
<page confidence="0.999764">
154
</page>
<tableCaption confidence="0.760676">
Table 1: Results for training the classifiers on the
coarse location name classes (2797 instances, 509
non-literal, leave-one-out) for the Mascara data (P =
precision, R = recall, F = F-score).
</tableCaption>
<table confidence="0.993939529411765">
ID n1,n2,n3 coarse class P R F
1c 4,0,0 literal 0.850 0.893 0.871
1c 4,0,0 non-literal 0.377 0.289 0.327
2c 0,3,0 literal 0.848 0.874 0.860
2c 0,3,0 non-literal 0.342 0.295 0.317
3c 0,0,7 literal 0.880 0.889 0.885
3c 0,0,7 non-literal 0.478 0.455 0.467
4c 4,3,0 literal 0.848 0.892 0.896
4c 4,3,0 non-literal 0.368 0.282 0.320
5c 4,0,7 literal 0.860 0.913 0.885
5c 4,0,7 non-literal 0.459 0.332 0.385
6c 0,3,7 literal 0.875 0.905 0.889
6c 0,3,7 non-literal 0.496 0.420 0.455
7c 4,3,7 literal 0.860 0.918 0.888
7c 4,3,7 non-literal 0.473 0.332 0.390
8c res. of 1c–7c literal 0.852 0.968 0.907
8c res. of 1c–7c non-literal 0.639 0.248 0.357
</table>
<bodyText confidence="0.999817666666667">
trained, selecting the setting with the highest accu-
racy for a single context for the combination (e.g.
4,, 5,, 6,, and 7,). As an additional experiment, a
classifier was trained on classification results of the
classifiers described above (combination of 1–7, e.g.
8,). It was expected that the combination of features
from different kinds of context would increase per-
formance, and that the combination of classifier re-
sults would increase performance.
</bodyText>
<sectionHeader confidence="0.996097" genericHeader="method">
3 Evaluation Results
</sectionHeader>
<bodyText confidence="0.959122263157895">
Table 3 shows results for the official submission.
Compared to results from the training phase on
the Mascara data (tested with the leave-one-out
method), performance is considerably lower. For
this data, the combined classifier achieved a consid-
erably higher precision (63.9% for non-literal read-
ings; 57.3% for the fine class place-for-people and
even 83.3% for the rare class place-for-event).
Performance may be affected by several reasons:
A number of problems were encountered while pro-
cessing the data. The TreeTagger automatically to-
kenizes its input and applies sentence boundary de-
tection. In some cases, the sentence boundary detec-
tion did not work well, returning sentences of more
than 170 words. Furthermore, the tagger output had
to be aligned with the test data again, as multi-word
Table 2: Excerpt from results for training the clas-
sifiers on the fine location name classes (2797 in-
stances, leave-one-out) for the Mascara data.
</bodyText>
<table confidence="0.9828122">
fine class P R F
0.851 0.895 0.873
0.366 0.280 0.318
0.370 0.270 0.312
0.848 0.876 0.862
0.332 0.276 0.301
0.222 0.270 0.244
0.878 0.892 0.885
0.463 0.424 0.442
0.279 0.324 0.300
0.851 0.899 0.875
0.358 0.269 0.307
0.435 0.270 0.333
0.861 0.914 0.887
0.452 0.322 0.377
0.550 0.297 0.386
0.871 0.906 0.888
0.468 0.383 0.422
0.400 0.324 0.358
0.861 0.918 0.889
0.459 0.323 0.378
0.500 0.297 0.373
0.854 0.963 0.905
0.573 0.262 0.360
0.833 0.270 0.408
</table>
<bodyText confidence="0.999659">
names (e.g. New York) were split into different to-
kens. In addition, the tag set of the tagger differs
somewhat from the official PENN tag set and in-
cludes additional tags for verbs.
In earlier experiments on metonymy classifica-
tion on a German corpus (Leveling and Hartrumpf,
2006), the data was nearly evenly distributed be-
tween literal and metonymic readings. This seems
to make a classification task easier because there is
no hidden bias in the classifier (i.e. the baseline of
always selecting the literal readings is about 50%).
Features are obtained by shallow NLP methods
only, not making use of a parser or chunker. Thus,
important syntactic or semantic information to de-
cide on metonymy might be missing in the features.
However, semantic features are more difficult to de-
termine, because reliable automatic tools for seman-
tic annotation are still missing. This is also indi-
cated by the fact that the grammatical roles (com-
prising syntactic features) in Mascara data are hand-
annotated.
However, some linguistic phenomena are already
implicitly represented by shallower features from
</bodyText>
<footnote confidence="0.98713324">
ID n1,n2,n3
1f 4,0,0
1f 4,0,0
1f 4,0,0
2f 0,3,0
2f 0,3,0
2f 0,3,0
3f 0,0,7
3f 0,0,7
3f 0,0,7
4f 4,3,0
4f 4,3,0
4f 4,3,0
5f 4,0,7
5f 4,0,7
5f 4,0,7
6f 0,3,7
6f 0,3,7
6f 0,3,7
7f 4,3,7
7f 4,3,7
7f 4,3,7
8f res. of 1f –7f
8f res. of 1f –7f
8f res. of 1f –7f
</footnote>
<figure confidence="0.997021875">
literal
pl.-for-p.
pl.-for-e.
literal
pl.-for-p.
pl.-for-e.
literal
pl.-for-p.
pl.-for-e.
literal
pl.-for-p.
pl.-for-e.
literal
pl.-for-p.
pl.-for-e.
literal
pl.-for-p.
pl.-for-e.
literal
pl.-for-p.
pl.-for-e.
literal
pl.-for-p.
pl.-for-e.
</figure>
<page confidence="0.994281">
155
</page>
<tableCaption confidence="0.688689833333333">
Table 3: Results for the coarse (908 samples: 721
literal, 187 non-literal), medium (721 literal, 167
metonymic, 20 mixed), and fine classification (721
literal, 141 place-for-people, 10 place-for-event, 1
place-for-product, 4 object-for-name, 11 othermet,
20 mixed) of location names.
</tableCaption>
<table confidence="0.729608727272727">
class P R F
FUH.location.coarse (0.798 accuracy)
literal 0.812 0.971 0.884
non-literal 0.543 0.134 0.214
FUH.location.medium (0.795 accuracy)
literal 0.810 0.970 0.883
metonymic 0.500 0.132 0.208
mixed 0.0 0.0 0.0
FUH.location.fine (0.785 accuracy)
literal 0.808 0.965 0.880
place-for-people 0.386 0.120 0.183
</table>
<bodyText confidence="0.998940363636364">
the surface level (given enough training instances).
For instance, active/passive voice may be encoded
by a combination of features for main verb/modal
verbs. If only a small training corpus is available,
overall performance will be higher when utilizing
explicit syntactic or semantic features.
Finally, the data may be too sparse for a super-
vised memory-based learning approach. The iden-
tification of rare classes of metonymy (e.g. place-
for-event) would greatly benefit from a larger corpus
covering these classes.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.998243529411765">
Evaluation results on the training data were very
promising, indicating a boost of precision by com-
bining classification results. In the training phase,
an accuracy of 83.7% was achieved on the coarse
level, compared to the majority baseline accuracy of
81.8%. For the submission for the metonymy res-
olution task at SemEval-2007, accuracy is close to
the majority baseline (79.4%) on the coarse (79.8%),
medium (79.5%), and fine (78.5%) level.
In summary, using different context sizes for dif-
ferent kinds of context and combining results of dif-
ferent classifiers for metonymy resolution increases
performance. The general approach would profit
from combining results of more diverse classifiers,
i.e. classifiers employing features extracted from the
surface, syntactic, and semantic context of a location
name.
</bodyText>
<sectionHeader confidence="0.996522" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9986415">
The research described was in part funded by the
DFG (Deutsche Forschungsgemeinschaft) in the
project IRSAW (Intelligent Information Retrieval on
the Basis of a Semantically Annotated Web).
</bodyText>
<sectionHeader confidence="0.999077" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999775432432432">
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. TiMBL: Tilburg memory
based learner, version 5.1. TR 04-02, ILK.
Christiane Fellbaum, editor. 1998. Wordnet. An Elec-
tronic Lexical Database. MIT Press, Cambridge, Mas-
sachusetts.
Daniel Karp, Yves Schabes, Martin Zaidel, and Dania
Egedi. 1992. A freely available wide coverage mor-
phological analyzer for English. In Proc. of COLING-
92, pages 950–955, Morristown, NJ.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. Chicago University Press.
Johannes Leveling and Sven Hartrumpf. 2006. On
metonymy recognition for GIR. In Proc. of GIR-2006,
the 3rd Workshop on Geographical Information Re-
trieval (held at SIGIR 2006), Seattle, Washington.
Katja Markert and Malvina Nissim. 2002. Towards a
corpus for annotated metonymies: The case of location
names. In Proc. of LREC 2002, Las Palmas, Spain.
Katja Markert and Malvina Nissim. 2003. Corpus-based
metonymy analysis. Metaphor and symbol, 18(3).
Katja Markert and Malvina Nissim. 2007. Task 08:
Metonymy resolution at SemEval-07. In Proc. of Sem-
Eval 2007.
Malvina Nissim and Katja Markert. 2003. Syntactic
features and word similarity for supervised metonymy
resolution. In Proc. of ACL-2003, Sapporo, Japan.
Yves Peirsman. 2006. Example-based metonymy recog-
nition for proper nouns. In Proc. of the Student Re-
search Workshop of EACL-2006, pages 71–78, Trento,
Italy.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing, Manchester,
UK.
David Stallard. 1993. Two kinds of metonymy. In Proc.
of ACL-93, pages 87–94, Columbus, Ohio.
</reference>
<page confidence="0.998789">
156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.415612">
<title confidence="0.977646666666667">FUH (FernUniversit¨at in Hagen): Metonymy Recognition Using Different Kinds of Context for a Memory-Based Learner</title>
<author confidence="0.948009">Johannes Leveling</author>
<affiliation confidence="0.750998">Intelligent Information and Communication Systems (IICS) FernUniversit¨at in Hagen (University of Hagen)</affiliation>
<email confidence="0.996797">johannes.leveling@fernuni-hagen.de</email>
<abstract confidence="0.991765735294117">For the metonymy resolution task at SemEval-2007, the use of a memory-based learner to train classifiers for the identification of metonymic location names is investigated. Metonymy is resolved on different levels of granularity, differentiating between literal and non-literal readings on the coarse level; literal, metonymic, and mixed readings on the medium level; and a number of classes covering regular cases of metonymy on a fine level. Different kinds of context are employed to obtain different features: a sequence of IDs representing subordination information for nouns and verbs, 2) articles, modal, and main verbs in the same sentence, and 3) of in a context window to the left and to the right of the location name. Different classifiers were trained on the Mascara data set to determine which values the context sizes and highest accuracy determined with the leave-oneout method). Results from these classifiers served as features for a combined classifier. In the training phase, the combined classifier achieved a considerably higher precision for the Mascara data. In the SemEval submission, an accuracy of 79.8% on the coarse, 79.5% on the medium, and 78.5% on the fine level is achieved (the baseline accuracy is 79.4%).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TiMBL: Tilburg memory based learner, version 5.1.</title>
<date>2004</date>
<tech>TR 04-02, ILK.</tech>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2004</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2004. TiMBL: Tilburg memory based learner, version 5.1. TR 04-02, ILK.</rawString>
</citation>
<citation valid="true">
<title>Wordnet. An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. Wordnet. An Electronic Lexical Database. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Karp</author>
<author>Yves Schabes</author>
<author>Martin Zaidel</author>
<author>Dania Egedi</author>
</authors>
<title>A freely available wide coverage morphological analyzer for English.</title>
<date>1992</date>
<booktitle>In Proc. of COLING92,</booktitle>
<pages>950--955</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="3965" citStr="Karp et al., 1992" startWordPosition="612" endWordPosition="615"> Metonymy Annotation Scheme And Robust Analysis (Markert and Nissim, 2003; Nissim and Markert, 2003; Markert and Nissim, 2002) contains annotated data for metonymic names from a subset of the the BNC. • WordNet 2.0 (Fellbaum, 1998) serves as a linguistic resource for assigning synset IDs and for looking up subordination information and frequency of readings. • The TreeTagger (Schmid, 1994) is utilized for sentence boundary detection, lemmatization, and part-of-speech tagging. The English tagger was trained on the PENN treebank and uses the English morphological database from the XTAG project (Karp et al., 1992). The parameter files were obtained from the web site.2 2.2 Different Kinds of Context Following the assumption that metonymic location names can be identified from the context, there are different kinds of context to consider. At most, the context comprises a single sentence in this setup. Three kinds of context were employed to extract features for the memory-based learner TiMBL: • C1: Subordination (hyponymy) information for nouns and verbs from the left and right context of the possibly metonymic name. • C2: The sentence context for modal verbs, main verbs, prepositions, and articles. • C3</context>
</contexts>
<marker>Karp, Schabes, Zaidel, Egedi, 1992</marker>
<rawString>Daniel Karp, Yves Schabes, Martin Zaidel, and Dania Egedi. 1992. A freely available wide coverage morphological analyzer for English. In Proc. of COLING92, pages 950–955, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Johnson</author>
</authors>
<title>Metaphors We Live By.</title>
<date>1980</date>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="1801" citStr="Lakoff and Johnson, 1980" startWordPosition="284" endWordPosition="287"> n2, and n3 yield the highest accuracy (n1 = 4, n2 = 3, and n3 = 7, determined with the leave-oneout method). Results from these classifiers served as features for a combined classifier. In the training phase, the combined classifier achieved a considerably higher precision for the Mascara data. In the SemEval submission, an accuracy of 79.8% on the coarse, 79.5% on the medium, and 78.5% on the fine level is achieved (the baseline accuracy is 79.4%). 1 Introduction Metonymy is typically defined as a figure of speech in which a speaker uses one entity to refer to another that is related to it (Lakoff and Johnson, 1980). The identification of metonymy becomes important for NLP tasks such as question answering (Stallard, 1993) or geographic information retrieval (Leveling and Hartrumpf, 2006). For regular cases of metonymy for locations and organizations, Markert and Nissim have proposed a set of metonymy classes. Annotating a subset of the BNC (British National Corpus), they extracted a set of metonymic proper nouns from two categories: country names (Markert and Nissim, 2002) and organization names (Nissim and Markert, 2003). In the metonymy resolution task at SemEval2007, the goal was to identify metonymic</context>
</contexts>
<marker>Lakoff, Johnson, 1980</marker>
<rawString>George Lakoff and Mark Johnson. 1980. Metaphors We Live By. Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Leveling</author>
<author>Sven Hartrumpf</author>
</authors>
<title>On metonymy recognition for GIR.</title>
<date>2006</date>
<booktitle>In Proc. of GIR-2006, the 3rd Workshop on Geographical Information Retrieval (held at SIGIR</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="1976" citStr="Leveling and Hartrumpf, 2006" startWordPosition="307" endWordPosition="310">ned classifier. In the training phase, the combined classifier achieved a considerably higher precision for the Mascara data. In the SemEval submission, an accuracy of 79.8% on the coarse, 79.5% on the medium, and 78.5% on the fine level is achieved (the baseline accuracy is 79.4%). 1 Introduction Metonymy is typically defined as a figure of speech in which a speaker uses one entity to refer to another that is related to it (Lakoff and Johnson, 1980). The identification of metonymy becomes important for NLP tasks such as question answering (Stallard, 1993) or geographic information retrieval (Leveling and Hartrumpf, 2006). For regular cases of metonymy for locations and organizations, Markert and Nissim have proposed a set of metonymy classes. Annotating a subset of the BNC (British National Corpus), they extracted a set of metonymic proper nouns from two categories: country names (Markert and Nissim, 2002) and organization names (Nissim and Markert, 2003). In the metonymy resolution task at SemEval2007, the goal was to identify metonymic names in a subset of the BNC. The task consists of two subtasks for company and country names, which are further divided into classification on a coarse level (recognizing li</context>
<context position="10040" citStr="Leveling and Hartrumpf, 2006" startWordPosition="1599" endWordPosition="1602">.301 0.222 0.270 0.244 0.878 0.892 0.885 0.463 0.424 0.442 0.279 0.324 0.300 0.851 0.899 0.875 0.358 0.269 0.307 0.435 0.270 0.333 0.861 0.914 0.887 0.452 0.322 0.377 0.550 0.297 0.386 0.871 0.906 0.888 0.468 0.383 0.422 0.400 0.324 0.358 0.861 0.918 0.889 0.459 0.323 0.378 0.500 0.297 0.373 0.854 0.963 0.905 0.573 0.262 0.360 0.833 0.270 0.408 names (e.g. New York) were split into different tokens. In addition, the tag set of the tagger differs somewhat from the official PENN tag set and includes additional tags for verbs. In earlier experiments on metonymy classification on a German corpus (Leveling and Hartrumpf, 2006), the data was nearly evenly distributed between literal and metonymic readings. This seems to make a classification task easier because there is no hidden bias in the classifier (i.e. the baseline of always selecting the literal readings is about 50%). Features are obtained by shallow NLP methods only, not making use of a parser or chunker. Thus, important syntactic or semantic information to decide on metonymy might be missing in the features. However, semantic features are more difficult to determine, because reliable automatic tools for semantic annotation are still missing. This is also i</context>
</contexts>
<marker>Leveling, Hartrumpf, 2006</marker>
<rawString>Johannes Leveling and Sven Hartrumpf. 2006. On metonymy recognition for GIR. In Proc. of GIR-2006, the 3rd Workshop on Geographical Information Retrieval (held at SIGIR 2006), Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Towards a corpus for annotated metonymies: The case of location names.</title>
<date>2002</date>
<booktitle>In Proc. of LREC</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="2267" citStr="Markert and Nissim, 2002" startWordPosition="352" endWordPosition="355">ion Metonymy is typically defined as a figure of speech in which a speaker uses one entity to refer to another that is related to it (Lakoff and Johnson, 1980). The identification of metonymy becomes important for NLP tasks such as question answering (Stallard, 1993) or geographic information retrieval (Leveling and Hartrumpf, 2006). For regular cases of metonymy for locations and organizations, Markert and Nissim have proposed a set of metonymy classes. Annotating a subset of the BNC (British National Corpus), they extracted a set of metonymic proper nouns from two categories: country names (Markert and Nissim, 2002) and organization names (Nissim and Markert, 2003). In the metonymy resolution task at SemEval2007, the goal was to identify metonymic names in a subset of the BNC. The task consists of two subtasks for company and country names, which are further divided into classification on a coarse level (recognizing literal and non-literal readings), on a medium level (differentiating non-literal readings into mixed and metonymic readings), and on a fine level (identifying classes of regular metonymy, such as a name referring to the population, place-for-people). The task is described in more detail by M</context>
</contexts>
<marker>Markert, Nissim, 2002</marker>
<rawString>Katja Markert and Malvina Nissim. 2002. Towards a corpus for annotated metonymies: The case of location names. In Proc. of LREC 2002, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Corpus-based metonymy analysis. Metaphor and symbol,</title>
<date>2003</date>
<pages>18--3</pages>
<contexts>
<context position="3420" citStr="Markert and Nissim, 2003" startWordPosition="527" endWordPosition="530">opulation, place-for-people). The task is described in more detail by Markert and Nissim (2007). 2 System Description 2.1 Tools and Resources The following tools and resources are used for the metonymy classification: • TiMBL 5.1 (Daelemans et al., 2004), a memory-based learner for classification is em153 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 153–156, Prague, June 2007. c�2007 Association for Computational Linguistics ployed for training the classifiers (supervised learning).1 • Mascara 2.0 – Metonymy Annotation Scheme And Robust Analysis (Markert and Nissim, 2003; Nissim and Markert, 2003; Markert and Nissim, 2002) contains annotated data for metonymic names from a subset of the the BNC. • WordNet 2.0 (Fellbaum, 1998) serves as a linguistic resource for assigning synset IDs and for looking up subordination information and frequency of readings. • The TreeTagger (Schmid, 1994) is utilized for sentence boundary detection, lemmatization, and part-of-speech tagging. The English tagger was trained on the PENN treebank and uses the English morphological database from the XTAG project (Karp et al., 1992). The parameter files were obtained from the web site.2</context>
</contexts>
<marker>Markert, Nissim, 2003</marker>
<rawString>Katja Markert and Malvina Nissim. 2003. Corpus-based metonymy analysis. Metaphor and symbol, 18(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Task 08: Metonymy resolution at SemEval-07. In</title>
<date>2007</date>
<booktitle>Proc. of SemEval</booktitle>
<contexts>
<context position="2891" citStr="Markert and Nissim (2007)" startWordPosition="452" endWordPosition="456">) and organization names (Nissim and Markert, 2003). In the metonymy resolution task at SemEval2007, the goal was to identify metonymic names in a subset of the BNC. The task consists of two subtasks for company and country names, which are further divided into classification on a coarse level (recognizing literal and non-literal readings), on a medium level (differentiating non-literal readings into mixed and metonymic readings), and on a fine level (identifying classes of regular metonymy, such as a name referring to the population, place-for-people). The task is described in more detail by Markert and Nissim (2007). 2 System Description 2.1 Tools and Resources The following tools and resources are used for the metonymy classification: • TiMBL 5.1 (Daelemans et al., 2004), a memory-based learner for classification is em153 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 153–156, Prague, June 2007. c�2007 Association for Computational Linguistics ployed for training the classifiers (supervised learning).1 • Mascara 2.0 – Metonymy Annotation Scheme And Robust Analysis (Markert and Nissim, 2003; Nissim and Markert, 2003; Markert and Nissim, 2002) contains annotate</context>
</contexts>
<marker>Markert, Nissim, 2007</marker>
<rawString>Katja Markert and Malvina Nissim. 2007. Task 08: Metonymy resolution at SemEval-07. In Proc. of SemEval 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malvina Nissim</author>
<author>Katja Markert</author>
</authors>
<title>Syntactic features and word similarity for supervised metonymy resolution.</title>
<date>2003</date>
<booktitle>In Proc. of ACL-2003,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2317" citStr="Nissim and Markert, 2003" startWordPosition="360" endWordPosition="363">peech in which a speaker uses one entity to refer to another that is related to it (Lakoff and Johnson, 1980). The identification of metonymy becomes important for NLP tasks such as question answering (Stallard, 1993) or geographic information retrieval (Leveling and Hartrumpf, 2006). For regular cases of metonymy for locations and organizations, Markert and Nissim have proposed a set of metonymy classes. Annotating a subset of the BNC (British National Corpus), they extracted a set of metonymic proper nouns from two categories: country names (Markert and Nissim, 2002) and organization names (Nissim and Markert, 2003). In the metonymy resolution task at SemEval2007, the goal was to identify metonymic names in a subset of the BNC. The task consists of two subtasks for company and country names, which are further divided into classification on a coarse level (recognizing literal and non-literal readings), on a medium level (differentiating non-literal readings into mixed and metonymic readings), and on a fine level (identifying classes of regular metonymy, such as a name referring to the population, place-for-people). The task is described in more detail by Markert and Nissim (2007). 2 System Description 2.1</context>
</contexts>
<marker>Nissim, Markert, 2003</marker>
<rawString>Malvina Nissim and Katja Markert. 2003. Syntactic features and word similarity for supervised metonymy resolution. In Proc. of ACL-2003, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
</authors>
<title>Example-based metonymy recognition for proper nouns.</title>
<date>2006</date>
<booktitle>In Proc. of the Student Research Workshop of EACL-2006,</booktitle>
<pages>71--78</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="4947" citStr="Peirsman (2006)" startWordPosition="775" endWordPosition="776">ed learner TiMBL: • C1: Subordination (hyponymy) information for nouns and verbs from the left and right context of the possibly metonymic name. • C2: The sentence context for modal verbs, main verbs, prepositions, and articles. • C3: A context window of tokens left and right of the location name. The trial data provided (a subset of the Mascara data) contained 188 non-literal location names (of 925 samples total). For a supervised learning approach, this is too few data. Therefore, the full Mascara data was converted to form training data consisting of feature values for context C1, C2, and 1Peirsman (2006) also employs TiMBL for metonymy resolution, but trains a single classifier. 2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ C3. The training data contained 509 metonymic annotations (of 2797 samples total). Some cases in the Mascara corpus are filtered during processing, including cases annotated as homonyms and cases whose metonymy class could not be agreed upon. The test data had a majority baseline of 82.8% accuracy for country names. 2.3 Features The Mascara data was processed to extract the following features (no hand-annotated data from Mascara was employed for feature val</context>
</contexts>
<marker>Peirsman, 2006</marker>
<rawString>Yves Peirsman. 2006. Example-based metonymy recognition for proper nouns. In Proc. of the Student Research Workshop of EACL-2006, pages 71–78, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="3739" citStr="Schmid, 1994" startWordPosition="580" endWordPosition="581">International Workshop on Semantic Evaluations (SemEval-2007), pages 153–156, Prague, June 2007. c�2007 Association for Computational Linguistics ployed for training the classifiers (supervised learning).1 • Mascara 2.0 – Metonymy Annotation Scheme And Robust Analysis (Markert and Nissim, 2003; Nissim and Markert, 2003; Markert and Nissim, 2002) contains annotated data for metonymic names from a subset of the the BNC. • WordNet 2.0 (Fellbaum, 1998) serves as a linguistic resource for assigning synset IDs and for looking up subordination information and frequency of readings. • The TreeTagger (Schmid, 1994) is utilized for sentence boundary detection, lemmatization, and part-of-speech tagging. The English tagger was trained on the PENN treebank and uses the English morphological database from the XTAG project (Karp et al., 1992). The parameter files were obtained from the web site.2 2.2 Different Kinds of Context Following the assumption that metonymic location names can be identified from the context, there are different kinds of context to consider. At most, the context comprises a single sentence in this setup. Three kinds of context were employed to extract features for the memory-based lear</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Language Processing, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Stallard</author>
</authors>
<title>Two kinds of metonymy.</title>
<date>1993</date>
<booktitle>In Proc. of ACL-93,</booktitle>
<pages>87--94</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1909" citStr="Stallard, 1993" startWordPosition="301" endWordPosition="302">from these classifiers served as features for a combined classifier. In the training phase, the combined classifier achieved a considerably higher precision for the Mascara data. In the SemEval submission, an accuracy of 79.8% on the coarse, 79.5% on the medium, and 78.5% on the fine level is achieved (the baseline accuracy is 79.4%). 1 Introduction Metonymy is typically defined as a figure of speech in which a speaker uses one entity to refer to another that is related to it (Lakoff and Johnson, 1980). The identification of metonymy becomes important for NLP tasks such as question answering (Stallard, 1993) or geographic information retrieval (Leveling and Hartrumpf, 2006). For regular cases of metonymy for locations and organizations, Markert and Nissim have proposed a set of metonymy classes. Annotating a subset of the BNC (British National Corpus), they extracted a set of metonymic proper nouns from two categories: country names (Markert and Nissim, 2002) and organization names (Nissim and Markert, 2003). In the metonymy resolution task at SemEval2007, the goal was to identify metonymic names in a subset of the BNC. The task consists of two subtasks for company and country names, which are fu</context>
</contexts>
<marker>Stallard, 1993</marker>
<rawString>David Stallard. 1993. Two kinds of metonymy. In Proc. of ACL-93, pages 87–94, Columbus, Ohio.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>