<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.99628">
Data-driven, PCFG-based and Pseudo-PCFG-based Models for Chinese
Dependency Parsing
</title>
<author confidence="0.994122">
Weiwei Sun and Xiaojun Wan
</author>
<affiliation confidence="0.749164">
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
</affiliation>
<email confidence="0.995336">
{ws,wanxiaojun}@pku.edu.cn
</email>
<sectionHeader confidence="0.993782" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982904761905">
We present a comparative study of transition-,
graph- and PCFG-based models aimed at il-
luminating more precisely the likely contri-
bution of CFGs in improving Chinese depen-
dency parsing accuracy, especially by com-
bining heterogeneous models. Inspired by
the impact of a constituency grammar on de-
pendency parsing, we propose several strate-
gies to acquire pseudo CFGs only from de-
pendency annotations. Compared to linguistic
grammars learned from rich phrase-structure
treebanks, well designed pseudo grammars
achieve similar parsing accuracy and have
equivalent contributions to parser ensemble.
Moreover, pseudo grammars increase the di-
versity of base models; therefore, together
with all other models, further improve sys-
tem combination. Based on automatic POS
tagging, our final model achieves a UAS of
87.23%, resulting in a significant improve-
ment of the state of the art.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961489795918">
Popular approaches to dependency parsing can
be divided into two classes: grammar-free and
grammar-based. Data-driven, grammar-free ap-
proaches make essential use of machine learning
from linguistic annotations in order to parse new
sentences. Such approaches, e.g. transition-based
(Nivre, 2008) and graph-based (McDonald, 2006;
Torres Martins et al., 2009) have attracted the most
attention in recent years. In contrast, grammar-
based approaches rely on linguistic grammars (in
either dependency or constituency formalisms) to
shape the search space for possible syntactic anal-
ysis. In particular, CFG-based dependency parsing
exploits a mapping between dependency and con-
stituency representations and reuses parsing algo-
rithms developed for CFG to produce dependency
structures. In previous work, data-driven, discrim-
inative approaches have been widely discussed for
Chinese dependency parsing. On the other hand,
various PCFG-based constituent parsing methods
have been applied to obtain phrase-structures as
well. With rich linguistic rules, phrase-structures of
Chinese sentences can be well transformed to their
corresponding dependency structures (Xue, 2007).
Therefore, PCFG parsers with such conversion rules
can be taken as another type of dependency parser.
We call them PCFG-based parsers, in this paper.
Explicitly defining linguistic rules to express
precisely generic grammatical regularities, a con-
stituency grammar can be applied to arrange sen-
tences into a hierarchy of nested phrases, which de-
termines constructions between larger phrases and
their smaller component phrases. This type of infor-
mation is different from, but highly related to, the
information captured by a dependency representa-
tion. A constituency grammar, thus, has great possi-
ble contributions to dependency parsing. In order
to pave the way for new and better methods, we
study the impact of CFGs on Chinese dependency
parsing. A series of empirical analysis of state-of-
the-art graph-, transition-and PCFG-based parsers is
presented to illuminate more precisely the properties
of heterogeneous models. We show that CFGs have
a great impact on dependency parsing and PCFG-
based models have complementary predictive pow-
ers to data-driven models.
System ensemble is an effective and important
technique to build more accurate parsers based on
multiple, diverse, weaker models. Exploiting differ-
</bodyText>
<page confidence="0.99103">
301
</page>
<bodyText confidence="0.984367148148148">
Transactions of the Association for Computational Linguistics, 1 (2013) 301–314. Action Editor: Jason Eisner.
Submitted 6/2012; Revised 10/2012; Published 7/2013. c�2013 Association for Computational Linguistics.
ent data-driven models, e.g. transition- and graph-
based models, has received the most attention in
dependency parser ensemble (Nivre and McDon-
ald, 2008; Torres Martins et al., 2008; Sagae and
Lavie, 2006). Only a few works investigate inte-
grating data-driven and PCFG-based models (Mc-
Donald, 2006). We argue that grammars can signif-
icantly increase the diversity of base models, which
plays a central role in parser ensemble, and therefore
lead to better and more promising hybrid systems.
We introduce a general classifier enhancing tech-
nique, i.e. bootstrap aggregating (Bagging), to im-
prove dependency parsing accuracy. This technique
can be applied to enhance a single-view parser, or
to combine multiple heterogeneous parsers. Exper-
iments on the CoNLL 09 shared task data demon-
strate its effectiveness: (1) Bagging can improve in-
dividual single-view parsers, especially the PCFG-
based one; (2) Bagging is more effective than pre-
viously introduced ensemble methods to combine
multi-view parsers; (3) Integrating data-driven and
PCFG-based models is more useful than combining
different data-driven models.
Although PCFG-based models have a big con-
tribution to data-driven dependency parsing, they
have a serious limitation: There are no corre-
sponding constituency annotations for some depen-
dency treebanks, e.g. Chinese Dependency Tree-
bank (LDC2012T05). To overcome this limita-
tion, we propose several strategies to acquire pseudo
grammars only from dependency annotations. In
particular, dependency trees are converted to pseudo
constituency trees and PCFGs can be extracted from
such trees. Another motivation of this study is to in-
crease the diversity of candidate models for parser
ensemble. Experiments show that pseudo-PCFG-
based models are very competitive: (1) Pseudo
grammars achieve similar or even better parsing re-
sults than linguistic grammars learned from rich
constituency annotations; (2) Compared to linguistic
grammars, well designed, single-view pseudo gram-
mars have an equivalent contribution to parser en-
semble; (3) Combining different pseudo grammars
even work better for ensemble than linguistic gram-
mars; (4) Pseudo-PCFG-based models increase the
diversity of base models, and therefore lead to fur-
ther improvements for ensemble.
Based on automatic POS tagging, our final model
achieves a UAS of 87.23% on the CoNLL data and
84.65% on CTB5, which yield relative error reduc-
tions of 18-24% over the best published results in
the literature.
</bodyText>
<sectionHeader confidence="0.608584" genericHeader="introduction">
2 Background and related work
</sectionHeader>
<subsectionHeader confidence="0.987994">
2.1 Data-driven dependency parsing
</subsectionHeader>
<bodyText confidence="0.99998103125">
The mainstream work on recent dependency pars-
ing focuses on data-driven approaches that automat-
ically learn to produce dependency graphs for sen-
tences solely from a hand-crafted dependency tree-
bank. The advantage of such models is that they
are easily ported to any language in which labeled
linguistic resources exist. Practically all statisti-
cal models that have been proposed in recent years
can be mainly described as either graph-based or
transition-based (McDonald and Nivre, 2007). Both
models have been adopted to learn Chinese depen-
dency structures (Zhang and Clark, 2011; Zhang
and Nivre, 2011; Huang and Sagae, 2010; Hatori
et al., 2011; Li et al., 2011, 2012). According to
published results, graph-based and transition-based
parsers achieve similar accuracy.
In the graph-based framework, informative evalu-
ation results have been presented in (Li et al., 2011).
First, second and third order projective parsing mod-
els are well evaluated. In the transition-based frame-
work, two advanced techniques have been stud-
ied. First, developing features has been shown
crucial to advancing parsing accuracy and a very
rich feature set is carefully evaluated by Zhang and
Nivre (2011). Second, beyond deterministic greedy
search, principled dynamic programming strategies
can be employed to explore more possible hypothe-
ses (Huang and Sagae, 2010). Both techniques have
been examined and shown helpful for Chinese de-
pendency parsing. Furthermore, Hatori et al. (2011)
combined both and obtained a state-of-the-art super-
vised parsing result.
</bodyText>
<subsectionHeader confidence="0.99969">
2.2 PCFG-based dependency parsing
</subsectionHeader>
<bodyText confidence="0.999802833333333">
PCFG-based dependency parsing approaches are
based on the finding that projective dependency trees
can be transformed from constituency trees by ap-
plying rich linguistic rules. In such approaches, de-
pendency parsing can be resolved by a two-step pro-
cess: constituent parsing and rule-based extraction
</bodyText>
<page confidence="0.998019">
302
</page>
<bodyText confidence="0.999860727272727">
of dependencies from phrase structures. The ad-
vantage of constituency-grammar-based approach is
that all the well-studied parsing methods for such
grammars can be used for dependency parsing as
well. Two language-specific properties essentially
make PCFG-based approaches easy to be applied
to Chinese dependency parsing: (1) Chinese gram-
maticians favor using projective structures;1 (2) Chi-
nese phrase-structure annotations normally contain
richer information and thus are reliable for tree con-
version.
</bodyText>
<subsectionHeader confidence="0.936179">
2.2.1 Constituency parsing
</subsectionHeader>
<bodyText confidence="0.999829233333333">
Compared to many other languages, statistical
constituent parsing for Chinese has reached early
success, due to the fact that the language has rela-
tively fixed word order and extremely poor inflec-
tional morphology. Both facts allow PCFG-based
statistical modeling to perform well. For the con-
stituent parsing, the majority of the state-of-the-
art parsers are based on generative PCFG learn-
ing. For example, the well-known and success-
ful Collins and Charniak&amp;Johnson parsers (Collins,
2003; Charniak, 2000; Charniak and Johnson, 2005)
implement generative lexicalized statistical models.
Apart from lexicalized PCFG parsing, unlex-
icalized parsing with latent variable grammars
(PCFGLA) can also produce comparable accuracy
(Matsuzaki et al., 2005; Petrov et al., 2006). Latent
variable grammars model an observed treebank of
coarse parse trees with a model over more refined,
but unobserved, derivation trees that represent much
more complex syntactic processes. Rather than
attempting to manually specify fine-grained cate-
gories, previous work shows that automatically in-
ducing the sub-categories from data can work quite
well. A PCFGLA parser leverages on an automatic
procedure to learn refined grammars and are there-
fore more robust to parse non-English languages that
are not well studied. For Chinese, such a parser
achieves the state-of-the-art performance and de-
feats many other types of parsers, including Collins
as well as Charniak parser (Che et al., 2012) and
</bodyText>
<footnote confidence="0.9990452">
1For example, as two popular dependency treebanks, the
CoNLL 2009 data and the Chinese Dependency Treebank both
excluede non-projective annotations. It is worth noting that the
former one is converted from a constituency treebank while the
latter one is directly annotated by lingusitics.
</footnote>
<bodyText confidence="0.708723">
discriminative transition-based models (Zhang and
Clark, 2009).
</bodyText>
<subsectionHeader confidence="0.649845">
2.2.2 CS to DS conversion
</subsectionHeader>
<bodyText confidence="0.999980133333333">
In the absence of dependency and constituency
structures for a particular treebank, treebank-guided
parser developers normally apply rich linguistic
rules to convert one representation formalism to an-
other to get necessary data to train parsers. Xue
(2007) examines the linguistic adequacy of depen-
dency structure annotation automatically converted
from phrase structure treebanks with rule-based ap-
proaches. A structural approach is introduced for
the constituency structure (CS) to dependency struc-
ture (DS) conversion for the Chinese Treebank data,
which is the basis of the CoNLL 2009 shared task
data. By applying this conversion procedure on the
outputs of an automatic phrase structure parser, we
can build a PCFG-based dependency parser.
</bodyText>
<subsectionHeader confidence="0.999422">
2.3 Parser ensemble
</subsectionHeader>
<bodyText confidence="0.997040894736842">
NLP systems built on particular single views nor-
mally capture different properties of an original
problem, and therefore differ in predictive powers.
As a result, NLP systems can take advantage of com-
plementary strengths of multiple views. Combining
the outputs of several systems has been shown in the
past to improve parsing performance significantly,
including integrating phrase-structure parsers (Hen-
derson and Brill, 1999), dependency parsers (Nivre
and McDonald, 2008), or both (McDonald, 2006).
Several ensemble models have been proposed for
the parsing of syntactic constituents and dependen-
cies, including learning-based stacking (Nivre and
McDonald, 2008; Torres Martins et al., 2008) and
learning-free post-inference (Henderson and Brill,
1999; Sagae and Lavie, 2006). Surdeanu and Man-
ning (2010) present a systematic analysis of these
ensemble methods and find several non-obvious
facts:
</bodyText>
<listItem confidence="0.994362">
• the diversity of base parsers is more important
than complex models for learning, and
• simplest scoring model for voting and repars-
ing performs essentially as well as other more
complex models.
</listItem>
<page confidence="0.999454">
303
</page>
<sectionHeader confidence="0.952465" genericHeader="method">
3 A comparative analysis of heterogeneous
parsers
</sectionHeader>
<bodyText confidence="0.999977285714286">
The information encoded in a dependency repre-
sentation is different from the information captured
in a constituency representation. While the depen-
dency structure represents head-dependent relations
between words, the constituency structure repre-
sents the grouping of words into phrases, classified
by structural categories. These differences concern
what is explicitly encoded in the respective represen-
tations, and affects data-driven and PCFG-based de-
pendency parsing models substantially. In this sec-
tion, we give a comparative analysis of transition-,
graph- and PCFG-based models aimed at illuminat-
ing more precisely the likely contribution of CFGs
in dependency parsing.
</bodyText>
<subsectionHeader confidence="0.994569">
3.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999924884615385">
Penn Chinese TreeBank (CTB) is a segmented,
POS tagged, and fully bracketed corpus in the con-
stituency formalism, and very popular to evaluate
fundamental NLP tasks, including word segmenta-
tion, POS tagging, constituent parsing as well as de-
pendency parsing. We use CTB 6 as our main corpus
and define the training, development and test sets ac-
cording to the CoNLL 2009 shared task. To evaluate
and analyze dependency parsers, we directly use the
CoNLL data. CTB’s syntactic annotations also in-
cludes functional information and empty categories.
Modern parsers, e.g. Collins and Berkeley parsers,
ignore these types of linguistic knowledge. To train
a constituent parser, we perform a heuristic proce-
dure on the treebank data to delete function tags and
empty categories as well as its associated redundant
ancestors. Many papers reported parsing results of
an older version CTB (namely CTB 5). To compare
with systems introduced in these papers, we evaluate
our final ensemble model on CTB5 in Section 5.4.
For dependency parsing, we choose a second
order graph-based parser2 (Bohnet, 2010) and a
transition-based parser (Hatori et al., 2011), for
experiments. For constituent parsing, we choose
Berkeley parser,3 a well known implementation of
the unlexicalized PCFGLA model and Bikel parser,4
</bodyText>
<footnote confidence="0.976234333333333">
2code.google.com/p/mate-tools/
3code.google.com/p/berkeleyparser/
4cis.upenn.edu/˜dbikel/software.html
</footnote>
<bodyText confidence="0.999893272727273">
a well known implementation of Collins’ lexical-
ized model, for experiments. In data-driven pars-
ing, features consisting of POS tags are very effec-
tive, so typically POS tagging is performed as a pre-
processing. We use the baseline sequential tagger
described in (Sun and Uszkoreit, 2012) to provide
such lexical information to the graph-based parser.
Note that the transition-based parser performs a joint
inference to acquire POS and dependency informa-
tion simultaneously, so there is no need to offer extra
tagging results to it.
</bodyText>
<subsectionHeader confidence="0.999802">
3.2 Overall performance
</subsectionHeader>
<bodyText confidence="0.9998912">
Table 1 (Column 2-6) summarizes the overall accu-
racy of different parsers. Two transition-based pars-
ing results are presented: The first one employ a
simple feature set (Zhang and Clark, 2008) and a
small beam (16); the second one employ rich fea-
tures (Zhang and Nivre, 2011) and a larger beam
(32). Two graph-based parsing results are reported;
the difference between them is whether integrate re-
lation labels into the parsing procedure. Roughly
speaking, currently state-of-the-art data-driven mod-
els achieves slightly better precision than unlexical-
ized PCFG-based models with regard to unlabeled
dependency prediction.
There is a big gap between lexicalized and unlexi-
calized parsing. The same phenomenon has been ob-
served by (Che et al., 2012) and (Zhuang and Zong,
2010). In addition to dependency parsing, Zhuang
and Zong (2010) found that Berkeley parser pro-
duce much more accurate syntactic analyses to assist
a Chinese semantic role labeler than Bikel parser.
Charniak and Stanford parsers are two other well-
known and frequently used tools that can provide
lexicalized parsing results. According to (Che et al.,
2012), they perform even worse than Bikel parser,
at least for Stanford dependencies. Due to the poor
parsing performance, we only concentrate on the un-
lexicalized model in the remainder of this paper.
The performance of labeled dependency predic-
tion of the unlexicalized PCFG-based parser is much
lower. We can learn that the CS to DS conversion is
not robust to assign functional categories to depen-
dencies and simple linguistic rules are not capable
to do fine-grained classification. Previous research
on English indicates that the main difficulty in de-
pendency parsing is the prediction of dependency
</bodyText>
<page confidence="0.99702">
304
</page>
<table confidence="0.978948133333333">
Devel. UAS LAS Compl. Fsib Fgrd
Tran[b=16,Z08] 82.80 N/A 29.00 66.55 79.74
Tran[b=32,Z11] 83.80 N/A 31.61 68.58 80.87
Graph[-lab] 83.66 N/A 29.28 67.96 80.82
Graph[+lab] 84.24 80.55 30.99 69.11 81.38
Unlex 82.86 67.44 27.98 69.07 81.22
Lex 70.38 58.10 - - - - - -
Bagging(15)
Tran[b=16,Z08] 83.25 N/A 28.66 67.17 78.89
Tran[b=32,Z11] 84.25 N/A 31.21 69.14 81.49
Graph[-lab] 83.81 N/A 29.68 68.00 80.62
Graph[+lab] 84.50 N/A 31.44 69.48 81.10
Unlex 84.92 N/A 32.35 71.08 83.66
Bagging(8)
Unlex 84.35 N/A 31.16 70.49 83.57
</table>
<tableCaption confidence="0.885896833333333">
Table 1: Accuracy of different parsers. The first block
presents baseline parsers; the last two blocks present
Bagging-enhanced parsers, where m is respectively set to
15 and 8. Z08 and Z11 distinguish different feature sets;
b=16 and b=32 are beam sizes. +/-lab means whether to
incorporate relation labels to a model.
</tableCaption>
<bodyText confidence="0.999980090909091">
structures, and an extra statistical classifier can be
employed to label automatically recognized depen-
dencies with a high accuracy. Although this issue
is not well studied for Chinese dependency parsing,
previous research on function tag labeling (Sun and
Sui, 2009) and semantic role labeling (Sun, 2010a)
gives us some clues. Their research shows that both
functional and predicate-argument structural infor-
mation is relatively easy to predict if high-quality
syntactic parses are available. We mainly focus on
the UAS metric in the following experiments.
</bodyText>
<subsectionHeader confidence="0.996832">
3.3 Constraints
</subsectionHeader>
<bodyText confidence="0.999915">
A grammar-based model utilizes an explicitly de-
fined formal grammar to shape the search space for
possible syntactic hypotheses. Parameters of a sta-
tistical grammar-based model are related to a gram-
mar rule, and as a result specific language construc-
tions are constrained by each other. For example,
parameters are assigned to rewrite rules for a CFG-
based model. Since the PCFG-based model lever-
ages rewrite rules to locally constrain several possi-
ble dependents for one head word, it does relatively
better for locally connected dependencies. The tra-
ditional evaluation metrics, i.e. UAS and LAS, only
consider bi-lexical (first order) dependencies, which
are smallest pieces of a dependency structure. Be-
sides bi-lexical dependencies, we report the predic-
tion accuracy of grandparent and sibling dependen-
cies, i.e. second order dependencies. The metrics
are defined as follows.
</bodyText>
<listItem confidence="0.950330083333333">
• For every word d whose parent is not the root,
we consider the word triple (d, p, g) among d
and its parent p and grandparent g. A word
triple (d, p, g) from a predicted tree is consid-
ered as correct if it also apprears in the corre-
sponding gold tree. Based on this definition,
precison, recall and f-score of grandparent de-
pendency can be defined in a normal sense. All
punctuations are excluded for calculation.
• For every word h that governs at least two chil-
dren (d1,..., dn), we consider every word triple
(h, di, di+1), among h and its sibling depen-
</listItem>
<bodyText confidence="0.995511833333333">
dents di as well as di+1 (0 &lt; i &lt; n). Similar
to the grandparent dependencies, we can define
evaluation metrics for sibling dependencies.
From Table 1, we can see that the grammar-based
model parses relatively better for slightly larger frag-
ments. For example, the UAS of the graph-based
model is significantly higher than the grammar-
based one, but their sibling and grandparent scores
are similar. In the next section, we will introduce
a general parser enhancement technique and present
more discussions based on enhanced parsing results
(Column 7-14).
</bodyText>
<subsectionHeader confidence="0.820857">
3.4 Endocentric and exocentric constructions
</subsectionHeader>
<figureCaption confidence="0.997614">
Figure 1: Nominal vs. verbal constructions.
</figureCaption>
<bodyText confidence="0.996798333333333">
Arguments in exocentric constructions help com-
plete the meaning of a predicate and are taken to be
obligatory and selected by their heads; adjuncts in
</bodyText>
<figure confidence="0.99009608">
Unlex
Tran[b=32 ,Z0 8] 2 5.2 5
G raph[+lab]
50
40
30
2 0
10
&lt;-NN&lt;- &lt;-NR&lt;-
2 7.61 19.3
2 4.82 17.45
17.82 15.16
&lt;-NT&lt;- &lt;-PN&lt;-
17.2 5 14.0 9
12 .2
13.48 41.32
12 .1 38.12
&lt;-VA &lt;- &lt;-VC&lt;-
39.72 45.51
49.9 51.18
47.7 49.83
&lt;-VE&lt;- &lt;-VV&lt;-
49.83 41.44
42 .14
42 .34
</figure>
<page confidence="0.994905">
305
</page>
<bodyText confidence="0.999314545454546">
endocentric constructions are structurally dispens-
able parts that provide auxiliary information and
taken to be optional and not selected by their heads.
An important annotation policy of the CTB is “one
grammatical relation per bracket”, which means
each constituent falls into one of the three primitive
grammatical relations: (1) head-complementation,
(2) head-adjunction and (3) coordination. Addi-
tionally, the argument is attached at a level that is
“closer” to the head than the adjuncts. Due to the
linguistic properties of different dependents and the
annotation strategies, a grammar-based model can
capture more syntactic preference properties of ar-
guments via hard constraints, i.e. grammar rules,
and are therefore more suitable to analyze exocen-
tric constructions.
Figure 1 is the error rate of unlabeled dependen-
cies considering different construction. A construc-
tion “← X ←” is considered as correctly predicted
if and only if all dependent words and head word of
X are completely correctly found. The error rate
in terms of this metric seems rather high because
the units we consider are normally much larger than
word pairs. From this figure, we can clearly see that
the data-driven parser does better for the prediction
of nominal constructions (NN/NR/NT/PN5), which
relate more on optional adjuncts or modifiers; the
grammar-based parser performs better for the pre-
diction of verbal constructions (VC/VE/VV), which
relate more on obligatory arguments. The evalua-
tion of the nominal and verbal constructions roughly
confirms the strength of grammar-based model to
predict verbal constructions.
</bodyText>
<sectionHeader confidence="0.934587" genericHeader="method">
4 Bagging parsers
</sectionHeader>
<bodyText confidence="0.998509714285714">
The comparative analysis highlights the fundamen-
tal diversity between data-driven and PCFG-based
models. In order to exploit the diversity gain, we ad-
dress the issue of parser combination. We employ
a general ensemble learning technique, i.e. Bag-
ging, to enhance a single-view parser and to com-
bine multi-view parsers.
</bodyText>
<footnote confidence="0.994978">
5For the definition and illustration of these tags, please refer
to the annotation guidelines (http://www.cis.upenn.
edu/˜chinese/posguide.3rd.ch.pdf).
</footnote>
<subsectionHeader confidence="0.991492">
4.1 Applying Bagging to dependency parsing
</subsectionHeader>
<bodyText confidence="0.999984044444445">
Bagging is a machine learning ensemble meta-
algorithm to improve classification and regression
models in terms of stability and classification accu-
racy (Breiman, 1996). It also reduces variance and
helps to avoid overfitting. Given a training set D of
size n, Bagging generates m new training sets Di
of size n′ ≤ n, by sampling examples from D. m
models are separately learned on the m new train-
ing sets and combined by voting (for classification)
or averaging the output (for regression). Hender-
son and Brill (2000) successfully applied Bagging
to enhance a constituent parser. Moreover, Bagging
has been applied to combine multiple solutions for
Chinese lexical processing (Sun, 2010b; Sun and
Uszkoreit, 2012). In this paper, we apply Bagging
to dependency parsing. Since training even one sin-
gle parser takes hours (if not days), experiments on
Bagging is time-consuming. To save time, we con-
duct data-driven parsing experiments based on sim-
ple configuration. More specifically, the beam size
of the transition-based parser is set to 16, and the
simple feature set is utilized; dependency relations
are not incorporated for the graph-based parser.
Bootrapping step. In the training phase, given a
training set D of size n, our model generates m new
training sets Di of size An by sampling uniformly
without replacement. Each Di can be used to train
a single-view parser or multiple parsers according
to different views. Using this strategy, we can get m
weak parsers or km parsers if multiple views are im-
plemented. In the parsing phase, for each sentence,
the (k)m models output (k)m candidate analyses
that are combined in a post-inference procedure.
Aggregating step. Different from classification
problems, simple voting scheme is not suitable for
parsing, which is a typical structured prediction
problem. To aggregate outputs of (k)m sub-models,
a structured inference procedure is needed. Sagae
and Lavie (2006) present a framework for combin-
ing the output of several different parsers to produce
results that are superior to each of the individual
parsers. We implement their method to aggregate
models. Once we have obtained multiple depen-
dency trees respectively from base parsers, we can
build a graph where each word in the sentence is a
</bodyText>
<page confidence="0.99619">
306
</page>
<bodyText confidence="0.999904545454546">
node. We then create weighted directed edges be-
tween the nodes corresponding to words for which
dependencies are obtained from each of the initial
structures. The weights are the word-by-word voting
results of sub-models. Based on this graph, the sen-
tence can be reparsed by a graph-based algorithm.
Taking Chinese as a projective language, we use Eis-
ner’s algorithm (Eisner, 1996) to combine multiple
dependency parses. Surdeanu and Manning (2010)
indicates that reparsing performs essentially as well
as other simpler or more complex models.
</bodyText>
<subsectionHeader confidence="0.990758">
4.2 Parameter tuning
</subsectionHeader>
<bodyText confidence="0.999822375">
We evaluate our combination model on the same
data set used in the last section. The two hyper-
parameters (A and m) of our Bagging model are
tuned on the development (validation) set. On one
hand, with the increase of the size of sub-samples,
i.e. A, the performance of sub-models is improved.
However, since the sub-models overlap more, the di-
versity of base models for ensemble will decrease
and the final prediction accuracy may go down. To
evaluate the effect of A, we separately sample 50%,
60%, 70% and 80% sentences from the original
training data 5 times, train 5 sub-models for each
parser, and combine them together. The beam size
of the transition-based parser is set to 16. Table 2
shows the influence of the choice of A’s. For all fol-
lowing experiments, we set A = 0.7.
</bodyText>
<table confidence="0.81709">
A 50% 60% 70% 80%
Tran+Graph[-lab]+Unlex 83.50 85.96 86.15 85.60
</table>
<tableCaption confidence="0.996764">
Table 2: UAS of Bagging(5) models with different A.
</tableCaption>
<bodyText confidence="0.999903333333333">
The second parameter for Bagging is the number
of sub-models to be used for combination. Figure 2
summarizes the Bagging performance when differ-
ent models are employed and different number (i.e.
m) of subsamples are used. From this figure, we can
learn the influence of the number of sub-models.
</bodyText>
<subsectionHeader confidence="0.9133855">
4.3 Bagging single-view parsers
4.3.1 Results
</subsectionHeader>
<bodyText confidence="0.9940402">
Table 1 indicates that Bagging can improve in-
dividual single-view parsers, especially Berkeley
parser. If we take Bagging as a general parser en-
hancement technique and still consider a Bagging-
enhanced parser as a single view, we conclude
</bodyText>
<figureCaption confidence="0.987999">
Figure 2: Averaged UAS of different Bagging models
with different numbers of sampling data sets.
</figureCaption>
<bodyText confidence="0.999522866666667">
that Bagging-enhanced PCFG-based method works
best among state-of-the-art approaches. For the
transition-based parser, though the score over single
words goes up, the score over sentences goes down.
The main reason is that the reparsing algorithm is a
graph-based one, which performs worse with regard
to the prediction of a whole sentence. The improve-
ment for the graph-based parser is very modest.
We train a Bagging(8)-enhanced Berkeley parser,
which achieves equivalent overall UAS to data-
driven parsers, and compare their parsing abilities
of second order dependencies. Now we can more
clearly see that the Bagging-enhanced PCFG-based
model performs better in the prediction of second
order dependencies.
</bodyText>
<subsubsectionHeader confidence="0.499136">
4.3.2 Related experiments on sequence models
</subsubsectionHeader>
<bodyText confidence="0.9989192">
Bagging has been applied to enhance discrimina-
tive sequence models for Chinese word segmenta-
tion (Sun, 2010b) and POS tagging (Sun and Uszko-
reit, 2012). For word segmentation, experiments
on discriminative Markov and semi-Markov tagging
models are reported. Their experiments showed that
Bagging can consistently enhance a semi-Markov
model but not the Markov one. Experiments on POS
tagging indicated that Bagging Markov models hurts
tagging performance. It seems that the relationships
among basic processing units affect Bagging.
PCFGLA parsers are built upon generative mod-
els with latent annotations. The use of automati-
cally induced latent variables may also affect Bag-
ging. Generative sequence models with latent anno-
</bodyText>
<figure confidence="0.993342153846154">
86.5
85.5
84.5
83.5
82 .5
81.5
3 4 5 6 7 8 9
G raph[-lab]
Tran
U nlex
G raph[-lab]+Unlex
Tran+Unlex
G raph[-lab]+Tran
</figure>
<page confidence="0.99163">
307
</page>
<bodyText confidence="0.999529454545455">
tations can also achieve good performance for Chi-
nese POS tagging. Huang et al. (2009) described
and evaluated a bi-gram HMM tagger that utilizes
latent annotations. Different from negative results of
Bagging discriminative models, our auxiliary exper-
iment shows that Bagging Huang et al.’s tagger can
help Chinese POS tagging. In other words, Bagging
substantially improves both HMMLA and PCFGLA
models, at least for Chinese POS tagging and con-
stituency parsing. It seems that Bagging favors the
use of latent variables.
</bodyText>
<subsectionHeader confidence="0.861484">
4.4 Bagging multi-view parsers
4.4.1 Results
</subsectionHeader>
<bodyText confidence="0.999348">
Figure 2 clearly shows that the Bagging model
taking both data-driven and PCFG-based models as
basic systems outperform the Bagging model taking
either model in isolation as basic systems. The com-
bination of a PCFG-based model and a data-driven
model (either graph-based or transition-based) is
more effective than the combination of two data-
driven models, which has received the most atten-
tion in dependency parser ensemble. Table 3 is
the performance of reparsing on the development
data. From this table, we can see by utilizing more
parsers, Bagging can enhance reparsing. According
to Surdeanu and Manning (2010)’s findings, repars-
ing performs as well as other combination mod-
els. Our auxiliary experiments confirm this finding:
Learning-based stacking cannot achieve better per-
formance. Limited to the document length, we do
not give descriptions of these experiments.
</bodyText>
<table confidence="0.9862298">
Devel. UAS
Reparsing(Tran[b=16,Z08]+Graph[-lab]+Unlex) 85.82
+Bagging(15) 86.37
bagging(reparse(g, t, c)) 86.09
reparse(bagging(g, t, c)) 85.86
</table>
<tableCaption confidence="0.999678">
Table 3: UAS of reparsing and Bagging.
</tableCaption>
<subsectionHeader confidence="0.777809">
4.4.2 Analysis
</subsectionHeader>
<bodyText confidence="0.9999797">
In our proposed model, Bagging has a two-fold
effect: One is as a system combination technique
and the other as a general parser enhancing tech-
nique. Two additional experiments are performed
to evaluate these two effects. To illustrate the differ-
ences between these two experiments, respectively
denote graph-based, transition-based and PCFG-
based parsers as g, t and c; denote the reparsing
procedure as reparse and the Bagging procedure as
bagging. The two experiments are as follows.
</bodyText>
<listItem confidence="0.920367285714286">
• Bagging a hybrid parser. In this experiment,
for each sub-sample Di, we first train three
parsers: gi, ti and ci. Then we combine these
three parsers by reparsing and construct a hy-
brid parser reparse(gi, ti, ci). Finally, all hy-
brid parsers are collected to build the final
parser: bagging(reparse(g, t, c)).
• Combining Bagging-enhanced parsers. In
this experiment, for each model, we first train
three Bagging-enhanced parsers: bagging(g),
bagging(t) and bagging(c). Then these
three Bagging-enhanced parsers are com-
bined by reparsing to build the final parser:
reparse(bagging(g, t, c)).
</listItem>
<bodyText confidence="0.918804">
Evaluation results are presented in Table 3.
</bodyText>
<sectionHeader confidence="0.981663" genericHeader="method">
5 Pseudo-grammar-based models
</sectionHeader>
<bodyText confidence="0.99998">
Although the combination of data-driven and
grammar-based models is very effective, it has a
serious limitation: It is only applicable when con-
stituency annotations are available to learning a
grammar. However, many treebanks, e.g. Chinese
Dependency Treebank (LDC2012T05), do not have
such linguistically rich structures. Our experiments
also suggest that a constituency grammar can sig-
nificantly increase the diversity of base models for
parser ensemble, which plays a major role in boost-
ing prediction accuracy.
In order to reduce the need for phrase-structure
annotations, and to increase the diversity of candi-
date parsers, we study learning pseudo grammars
for dependency parsing. The key idea is very sim-
ple: By converting a dependency structure to a
constituency one, we can reuse the PCFGLA ap-
proach to learn pseudo grammars for dependency
parsing. Figure 3 is an example. The first tree is
an original dependency parse, while the second tree
is the corresponding CTB annotation. The next two
trees are two automatically converted pseudo con-
stituency trees. By applying DS to CS rules, we
can acquire pseudo constituency treebanks and then
learn pseudo grammars from them.
</bodyText>
<page confidence="0.991978">
308
</page>
<figure confidence="0.936836">
(1) Dependency tree (2) Linguistic constituency tree
(3) Flat constituency tree (4) Binarized constituency tree
</figure>
<figureCaption confidence="0.99893">
Figure 3: An example: China encourages private entrepreneurs to invest in national infrastructure.
</figureCaption>
<bodyText confidence="0.999902428571429">
The basic idea of our method is to use parsing
models in one formalism for parsing in another for-
malism. In previous work, PCFGs are used to solve
parsing problems in many other formalisms, includ-
ing dependency (Collins et al., 1999), CCG (Fowler
and Penn, 2010), LFG (Cahill et al., 2004) and HPSG
(Zhang and Krieger, 2011) parsing.
</bodyText>
<subsectionHeader confidence="0.812737">
5.1 Strategies for DS to CS conversion
</subsectionHeader>
<bodyText confidence="0.999962615384615">
The conversion from DS to CS is a non-trivial prob-
lem. One main issue in the conversion is the indeter-
minancy in the choice of a phrasal category given a
dependency relation, the level and position of attach-
ment of a dependent in the constituency structure, as
dependency relations typically do not encode such
information. To convert a DS to a CS, especially
for dependency parsing, we should consider (1) how
to transform between the topological structures, (2)
how to induce a syntactic category, and (3) how to
easily recover dependency trees from pseudo con-
stituency trees. From these three aspects, we present
the following strategies.
</bodyText>
<subsectionHeader confidence="0.814297">
5.1.1 Topological structure
</subsectionHeader>
<bodyText confidence="0.999912555555555">
The topological structures represent the boundary
information of constituents in a given sentence. De-
pendency structures do not directly represent such
boundary information. Nevertheless, a complete
subtree in a projective dependency tree should be
considered as a constituent. We can construct a very
flat constituent tree, of which nodes are associated
with complete subtrees of a dependency parse. The
third tree in Figure 3 is an example of such conver-
sion.
Right-to-left binarization According to the study
in (Sun, 2010a), head words of most phrases in
Chinese are located at the first or the last position.
That means for binarizing most phrases, we only
need sequentially combine the right or left parts to-
gether with their head phrases. Main exceptions are
clauses, of which the head predicate locates inside,
since Chinese is an SVO language. To deal with
these exceptions, we split each phrase whose head
child is inside itself into three parts: left child(ren),
head and right child(ren). We first sequentially com-
bine the head and its right child(ren) that are usu-
ally objects as intermediate phrases, then sequen-
tially combine the left child(ren) until reach the orig-
inal parent node. For example, the first rewrite rule
in follows should be transferred into the second and
third types of rules.
</bodyText>
<footnote confidence="0.466596">
1. XP → X1, ..., X2, ..., XM
</footnote>
<page confidence="0.977828">
309
</page>
<listItem confidence="0.895585">
2. X′p → Xi, Xi+1; Xp′′ → Xp′, Xi+2; ...
3. X∗p → Xi−1, Xp′...′; X∗∗
</listItem>
<equation confidence="0.811306">
p → Xi−2, X∗p; ...
</equation>
<bodyText confidence="0.99939475">
This right-to-left binarization strategy is consistent
with most Chinese treebank annotation schemes.
The fourth tree in Figure 3 is an example of bina-
rized pseudo tree.
</bodyText>
<subsubsectionHeader confidence="0.677448">
5.1.2 Phrasal category
</subsubsectionHeader>
<bodyText confidence="0.999962">
Projection principle is introduced by Chomsky
to link together the levels of syntactic description.
It connects syntactic structures with lexical entries:
Lexical structure must be represented categorically
at every syntactic level, and representations at each
level of syntax are projected from the lexicon in that
they observe the subcategorisation properties of lex-
ical items. According to this principle, it is reason-
able to use the lexical category (POS) of the head
word as the phrasal category of a phrase.
</bodyText>
<subsectionHeader confidence="0.732838">
5.1.3 Auxiliary symbol
</subsectionHeader>
<bodyText confidence="0.912392">
We can use auxiliary symbols to denote the head
phrase position in a CFG rule. In other words, some
categories may be splitted into subcategories accord-
ing to if they are head phrases of their parent nodes
or which children are their head phrases. Auxiliary
symbols could be either assigned to one of the right
hand side or the left hand side. The first choice is to
conveniently use a H symbol to indicate that current
phrase is the head of its parent node. The second
choice is to practically use an L or R symbol to indi-
cate the head of current node is its left or right child,
in a binarized tree. The following table gives an ex-
ample of different rules with auxiliary symbols.
With head symbol With left/right symbol
Xl → Xl#H, Xr Xl#L → Xl, Xr
Xr → Xl, Xr#H Xr#R → Xl, Xr
</bodyText>
<subsectionHeader confidence="0.999347">
5.2 Three conversions
</subsectionHeader>
<bodyText confidence="0.995785269230769">
Taking into account the above strategies, we propose
three concrete DS to CS conversions:
Flat conversion with H auxiliary symbol (Flates).
Just as shown as the third tree in Figure 3, we can
learn a grammar from very flat constituency trees
where the auxiliary symbol H is used for extracting
dependencies.
Right-to-left binarizing with H auxiliary symbol
(Bines). Different from the flat conversion, we bi-
narize a tree according to the right-to-left principle.
Auxiliary symbol H is chosen.
Right-to-left binarizing with LR auxiliary sym-
bol (BinLR). Different from the second type of
conversion, we use auxiliary L/R symbols to denote
head phrases. See the fourth tree in Figure 3 for in-
stance.
Practically, every constituency parse that is pro-
duced by parsers trained with binarized trees exactly
maps to one dependency tree. However, the parser
trained with flat trees may produce very bad con-
stituency results. Sometimes, one parent node may
have zero child that is assigned with H or more than
one children that are are assigned H. In the first case,
we select the right most child as the head of such
parent, while in the second case, we select the right
most one from the children that are assigned H.
</bodyText>
<subsectionHeader confidence="0.900168">
5.3 Evaluation
</subsectionHeader>
<table confidence="0.928236833333333">
5.3.1 Equivalent parsing accuracy
Devel. Base Bagging(15)
CTB 83.49% 84.92%
FlatH 80.15% 83.53%
BinH 81.80% 84.64%
BinLR 82.46% 84.90%
</table>
<tableCaption confidence="0.999613">
Table 4: UAS of pseudo-grammar-based models.
</tableCaption>
<bodyText confidence="0.995743454545455">
Table 4 summarizes the performance of differ-
ent pseudo-grammar-based models. Compared to
the linguistic grammar learned from CTB, we can
see that pseudo grammars are very competitive.
Not that, the FlatH/BinH/BinLR trees are derived
from the CoNLL data, rather than the original CTB.
Among different DS to CS conversion strategies, the
BinLR conversion works best. More interestingly,
when we enhance the PCFGLA method by using
Bagging, the BinLR model performs as well as the
real-grammar-based model.
</bodyText>
<subsectionHeader confidence="0.453217">
5.3.2 Better contribution to ensemble
</subsectionHeader>
<bodyText confidence="0.999904">
The experiments above indicate that we can eas-
ily build good grammar-based dependency parser
without any constituency annotations. The fol-
lowing experiments on parser combination show
that compared to the linguistic grammar, binH and
</bodyText>
<page confidence="0.993684">
310
</page>
<table confidence="0.999278090909091">
Devel. UAS
Tran+Graph+CTB 86.37%
Tran+Graph+FlatH 86.14%
Tran+Graph+BinH 86.29%
Tran+Graph+BinLR 86.28%
Tran+Graph+flat+BinH+BinLR 87.03%
Tran+Graph+CTB+FlatH 86.96%
Tran+Graph+CTB+BinH 87.10%
Tran+Graph+CTB+BinLR 87.15%
Tran+Graph+CTB+BinH+BinLR 87.38%
Tran+Graph+CTB+FlatH+BinH+BinLR 87.35%
</table>
<tableCaption confidence="0.999776">
Table 5: UAS of different Bagging(15) models.
</tableCaption>
<bodyText confidence="0.998629666666667">
binLR grammars have equivalent contributions to
parser ensemble. Table 5 presents the ensem-
ble performance on the development data. By
Bagging, the data-driven models together with ei-
ther real grammar-based or pseudo-grammar-based
model reach a similar UAS.
</bodyText>
<subsectionHeader confidence="0.774317">
5.3.3 Increased parser diversity
</subsectionHeader>
<bodyText confidence="0.999979166666667">
Since pseudo grammars are very different from
real grammars that are induced from large-scale lin-
guistic annotations. Pseudo-grammar-based parsing
models behave very differently with grammar-based
models. In other words, they increase the diver-
sity of model candidates for parser ensemble. As
a result, pseudo-grammar-based models lead to fur-
ther improvements for parser combination. Table 5
shows that the combination of data-driven, PCFG-
based and binarized pseudo-grammar-based models
is significantly better than the combination of data-
driven and PCFG-based models.
</bodyText>
<subsectionHeader confidence="0.998571">
5.4 Comparison to the state-of-the-art
</subsectionHeader>
<bodyText confidence="0.999279428571429">
Table 6 summarizes the parsing performance on the
test data set, as well as the best published result re-
ported in Li et al. (2012). To fairly compare the per-
formance of our parser and other systems which are
built without linguistic constituency trees, we only
use pseudo-PCFGs in this experiment. Based on
automatic POS tagging, our final model achieves a
UAS of 87.23%, which yields a relative error reduc-
tion of 24% over the best published result. Table
6 also presents the results evaluated on the CTB5
data that is more widely used for previous research.
Li et al. (2011) and Hatori et al. (2011) respec-
tively evaluated their graph-based and transition-
based parsers; Zhang and Clark (2011) evaluated
</bodyText>
<table confidence="0.980702875">
CoNLL-test UAS
(Li et al., 2012) 83.23%
Graph+Tran+FlatH+BinH+BinLR 87.23%
CTB5-test UAS
(Li et al., 2011) 80.79%
(Hatori et al., 2011) 81.33%
(Zhang and Clark, 2011) 81.21%
Graph+Tran+FlatH+BinH+BinLR 84.65%
</table>
<tableCaption confidence="0.999406">
Table 6: UAS of different models on the test data.
</tableCaption>
<bodyText confidence="0.997883">
a hybrid data-driven parser. Our model is signifi-
cantly better than these systems: It achieves a UAS
of 84.65%, which obtains an error reduction of 18%
over the best system in the literature.
</bodyText>
<sectionHeader confidence="0.997485" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999978045454545">
There have been several attempts to develop high
accuracy parsers in both constituency and depen-
dency formalisms for Chinese, and many successful
parsing algorithms designed for English have been
applied. However, the state-of-the-art still falls far
short when compared to English. This paper stud-
ies data-driven and PCFG-based models for Chinese
dependency parsing. We present a comparative anal-
ysis of transition-, graph-, and PCFG-based parsers,
which highlights the systematic differences between
data-driven and PCFG-based models. Our analysis
may benefit parser ensemble, parser co-training, ac-
tive learning for treebank construction, and so on.
In order to exploit the diversity gain, we address
the issue of parser combination. To overcome the
limitation of the lack of constituency treebanks, we
study pseudo-grammar-based models. Experimental
results show that combining various data-driven and
PCFG-based models significantly advance the state-
of-the-art, and by converting parse trees, we can still
take advantages of the constituency representation
even without constituency annotations.
</bodyText>
<sectionHeader confidence="0.953024" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.998851625">
We would like to thank thank all anonymous review-
ers whose valuable comments led to signilicant re-
visions. The first author would like to thank Prof.
Hans Uszkoreit for discussion and feedback of an
early version of this work.
The work was supported by NSFC (61170166),
Beijing Nova Program (2008B03) and National
High-Tech R&amp;D Program (2012AA011101).
</bodyText>
<page confidence="0.998279">
311
</page>
<sectionHeader confidence="0.989485" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.930475603773585">
Bernd Bohnet. 2010. Top accuracy and fast de-
pendency parsing is not a contradiction. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages
89–97. Coling 2010 Organizing Committee, Bei-
jing, China. URL http://www.aclweb.
org/anthology/C10-1011.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123–140.
Aoife Cahill, Michael Burke, Ruth O’Donovan,
Josef Van Genabith, and Andy Way. 2004. Long-
distance dependency resolution in automatically
acquired wide-coverage pcfg-based lfg approx-
imations. In Proceedings of the 42nd Meet-
ing of the Association for Computational Lin-
guistics (ACL’04), Main Volume, pages 319–
326. Barcelona, Spain. URL http://www.
aclweb.org/anthology/P04-1041.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the first con-
ference on North American chapter of the Associ-
ation for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL’05), pages 173–180. Associa-
tion for Computational Linguistics, Ann Arbor,
Michigan.
Wanxiang Che, Valentin Spitkovsky, and Ting
Liu. 2012. A comparison of chinese parsers
for stanford dependencies. In Proceedings
of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume
2: Short Papers), pages 11–16. Associa-
tion for Computational Linguistics, Jeju Island,
Korea. URL http://www.aclweb.org/
anthology/P12-2003.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589–637.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
czech. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Lin-
guistics, pages 505–512. Association for Com-
putational Linguistics, College Park, Maryland,
USA. URL http://www.aclweb.org/
anthology/P99-1065.
Jason M. Eisner. 1996. Three new probabilis-
tic models for dependency parsing: an ex-
ploration. In Proceedings of the 16th con-
ference on Computational linguistics - Vol-
</reference>
<bodyText confidence="0.909516486486486">
ume 1, COLING ’96, pages 340–345. Associa-
tion for Computational Linguistics, Stroudsburg,
PA,USA. URLhttp://dx.doi.org/10.
3115/992628.992688.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with combinatory cat-
egorial grammar. In Proceedings of the 48th
Annual Meeting of the Association for Com-
putational Linguistics, pages 335–344. Associ-
ation for Computational Linguistics, Uppsala,
Sweden. URLhttp://www.aclweb.org/
anthology/P10-1035.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2011. Incremental joint pos tag-
ging and dependency parsing in chinese. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 1216–1224.
Asian Federation of Natural Language Process-
ing, Chiang Mai, Thailand. URL http://www.
aclweb.org/anthology/I11-1136.
John Henderson and Eric Brill. 1999. Exploiting di-
versity in natural language processing: Combin-
ing parsers. In In Proceedings of the Fourth Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 187–194.
John C. Henderson and Eric Brill. 2000. Bag-
ging and boosting a treebank parser. In Pro-
ceedings of the 1st North American chapter of
the Association for Computational Linguistics
conference, NAACL 2000, pages 34–41. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA. URL http://dl.acm.org/
citation.cfm?id=974305.974310.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
</bodyText>
<page confidence="0.996539">
312
</page>
<reference confidence="0.999233282608696">
1077–1086. Association for Computational Lin-
guistics, Uppsala, Sweden. URL http://www.
aclweb.org/anthology/P10-1110.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm
part-of-speech tagger by latent annotation and
self-training. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics, Compan-
ion Volume: Short Papers, pages 213–216. As-
sociation for Computational Linguistics, Boulder,
Colorado. URL http://www.aclweb.org/
anthology/N/N09/N09-2054.
Zhenghua Li, Ting Liu, and Wanxiang Che.
2012. Exploiting multiple treebanks for pars-
ing with quasi-synchronous grammars. In Pro-
ceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 675–684. Associa-
tion for Computational Linguistics, Jeju Island,
Korea. URL http://www.aclweb.org/
anthology/P12-1071.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting
Liu, Wenliang Chen, and Haizhou Li. 2011.
Joint models for Chinese pos tagging and depen-
dency parsing. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1180–1191. Association
for Computational Linguistics, Edinburgh, Scot-
land, UK. URL http://www.aclweb.org/
anthology/D11-1109.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi
Tsujii. 2005. Probabilistic cfg with latent an-
notations. In Proceedings of the 43rd An-
nual Meeting on Association for Computational
Linguistics, ACL ’05, pages 75–82. Associa-
tion for Computational Linguistics, Stroudsburg,
PA,USA. URL http://dx.doi.org/10.
3115/1219840.1219850.
Ryan McDonald. 2006. Discriminative learning and
spanning tree algorithms for dependency pars-
ing. Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA, USA. AAI3225503.
Ryan McDonald and Joakim Nivre. 2007. Char-
acterizing the errors of data-driven dependency
parsing models. In Proceedings of the 2007
Joint Conference on Empirical Methods in
Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-
CoNLL), pages 122–131. Association for Com-
putational Linguistics, Prague, Czech Repub-
lic. URL http://www.aclweb.org/
anthology/D/D07/D07-1013.
Joakim Nivre. 2008. Algorithms for de-
terministic incremental dependency pars-
ing. Comput. Linguist., 34:513–553. URL
http://dx.doi.org/10.1162/coli.
07-056-R1-07-027.
Joakim Nivre and Ryan McDonald. 2008. In-
tegrating graph-based and transition-based
dependency parsers. In Proceedings of ACL-08:
HLT, pages 950–958. Association for Compu-
tational Linguistics, Columbus, Ohio. URL
http://www.aclweb.org/anthology/
P/P08/P08-1108.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
433–440. Association for Computational Linguis-
tics, Sydney, Australia.
Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the
Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers,
NAACL-Short ’06, pages 129–132. Association
for Computational Linguistics, Stroudsburg, PA,
USA. URL http://portal.acm.org/
citation.cfm?id=1614049.1614082.
Weiwei Sun. 2010a. Improving Chinese se-
mantic role labeling with rich syntactic fea-
tures. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 168–172. Associ-
ation for Computational Linguistics, Uppsala,
Sweden. URLhttp://www.aclweb.org/
anthology/P10-2031.
Weiwei Sun. 2010b. Word-based and character-
based word segmentation models: Compari-
son and combination. In Proceedings of the
</reference>
<page confidence="0.992582">
313
</page>
<reference confidence="0.977628252747253">
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1211–1219.
Coling 2010 Organizing Committee, Beijing,
China. URL http://www.aclweb.org/
anthology/C10-2139.
Weiwei Sun and Zhifang Sui. 2009. Chinese func-
tion tag labeling. In Proceedings of the 23rd Pa-
cific Asia Conference on Language, Information
and Computation. Hong Kong.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing
paradigmatic and syntagmatic lexical relations:
Towards accurate Chinese part-of-speech tagging.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics. Asso-
ciation for Computational Linguistics.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics, pages 649–652. Association
for Computational Linguistics, Los Angeles, Cal-
ifornia. URL http://www.aclweb.org/
anthology/N10-1091.
Andre Torres Martins, Noah Smith, and Eric Xing.
2009. Concise integer linear programming for-
mulations for dependency parsing. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language Process-
ing of the AFNLP, pages 342–350. Associa-
tion for Computational Linguistics, Suntec, Sin-
gapore. URL http://www.aclweb.org/
anthology/P/P09/P09-1039.
Andr´e Filipe Torres Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking de-
pendency parsers. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 157–166. Associ-
ation for Computational Linguistics, Honolulu,
Hawaii. URL http://www.aclweb.org/
anthology/D08-1017.
Nianwen Xue. 2007. Tapping the implicit infor-
mation for the PS to DS conversion of the Chi-
nese treebank. In Proceedings of the Sixth Inter-
national Workshop on Treebanks and Linguistics
Theories.
Yi Zhang and Hans-Ulrich Krieger. 2011. Large-
scale corpus-driven pcfg approximation of an
hpsg. In Proceedings of the 12th International
Conference on Parsing Technologies, pages 198–
208. Association for Computational Linguistics,
Dublin, Ireland. URL http://www.aclweb.
org/anthology/W11-2923.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
562–571. Association for Computational Linguis-
tics, Honolulu, Hawaii. URL http://www.
aclweb.org/anthology/D08-1059.
Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the Chinese treebank using a
global discriminative model. In Proceedings
of the 11th International Conference on Pars-
ing Technologies (IWPT’09), pages 162–171. As-
sociation for Computational Linguistics, Paris,
France. URL http://www.aclweb.org/
anthology/W09-3825.
Yue Zhang and Stephen Clark. 2011. Syntac-
tic processing using the generalized perceptron
and beam search. Comput. Linguist., 37(1):105–
151. URL http://dx.doi.org/10.1162/
coli_a_00037.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local fea-
tures. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 188–
193. Association for Computational Linguistics,
Portland, Oregon, USA. URL http://www.
aclweb.org/anthology/P11-2033.
Tao Zhuang and Chengqing Zong. 2010. A min-
imum error weighting combination strategy for
Chinese semantic role labeling. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 1362–
1370. Coling 2010 Organizing Committee, Bei-
jing, China. URL http://www.aclweb.
org/anthology/C10-1153.
</reference>
<page confidence="0.999133">
314
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.848335">
<title confidence="0.9956095">Data-driven, PCFG-based and Pseudo-PCFG-based Models for Dependency Parsing</title>
<author confidence="0.923883">Sun</author>
<affiliation confidence="0.992247">Institute of Computer Science and Technology, Peking</affiliation>
<address confidence="0.981564">The MOE Key Laboratory of Computational Linguistics, Peking</address>
<email confidence="0.976993">ws@pku.edu.cn</email>
<email confidence="0.976993">wanxiaojun@pku.edu.cn</email>
<abstract confidence="0.998114363636364">We present a comparative study of transition-, graphand PCFG-based models aimed at illuminating more precisely the likely contribution of CFGs in improving Chinese dependency parsing accuracy, especially by combining heterogeneous models. Inspired by the impact of a constituency grammar on dependency parsing, we propose several strategies to acquire pseudo CFGs only from dependency annotations. Compared to linguistic grammars learned from rich phrase-structure treebanks, well designed pseudo grammars achieve similar parsing accuracy and have equivalent contributions to parser ensemble. Moreover, pseudo grammars increase the diversity of base models; therefore, together with all other models, further improve system combination. Based on automatic POS tagging, our final model achieves a UAS of 87.23%, resulting in a significant improvement of the state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>89--97</pages>
<location>Beijing, China. URL http://www.aclweb.</location>
<contexts>
<context position="14272" citStr="Bohnet, 2010" startWordPosition="2090" endWordPosition="2091">otations also includes functional information and empty categories. Modern parsers, e.g. Collins and Berkeley parsers, ignore these types of linguistic knowledge. To train a constituent parser, we perform a heuristic procedure on the treebank data to delete function tags and empty categories as well as its associated redundant ancestors. Many papers reported parsing results of an older version CTB (namely CTB 5). To compare with systems introduced in these papers, we evaluate our final ensemble model on CTB5 in Section 5.4. For dependency parsing, we choose a second order graph-based parser2 (Bohnet, 2010) and a transition-based parser (Hatori et al., 2011), for experiments. For constituent parsing, we choose Berkeley parser,3 a well known implementation of the unlexicalized PCFGLA model and Bikel parser,4 2code.google.com/p/mate-tools/ 3code.google.com/p/berkeleyparser/ 4cis.upenn.edu/˜dbikel/software.html a well known implementation of Collins’ lexicalized model, for experiments. In data-driven parsing, features consisting of POS tags are very effective, so typically POS tagging is performed as a preprocessing. We use the baseline sequential tagger described in (Sun and Uszkoreit, 2012) to pr</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89–97. Coling 2010 Organizing Committee, Beijing, China. URL http://www.aclweb. org/anthology/C10-1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="23088" citStr="Breiman, 1996" startWordPosition="3464" endWordPosition="3465">een data-driven and PCFG-based models. In order to exploit the diversity gain, we address the issue of parser combination. We employ a general ensemble learning technique, i.e. Bagging, to enhance a single-view parser and to combine multi-view parsers. 5For the definition and illustration of these tags, please refer to the annotation guidelines (http://www.cis.upenn. edu/˜chinese/posguide.3rd.ch.pdf). 4.1 Applying Bagging to dependency parsing Bagging is a machine learning ensemble metaalgorithm to improve classification and regression models in terms of stability and classification accuracy (Breiman, 1996). It also reduces variance and helps to avoid overfitting. Given a training set D of size n, Bagging generates m new training sets Di of size n′ ≤ n, by sampling examples from D. m models are separately learned on the m new training sets and combined by voting (for classification) or averaging the output (for regression). Henderson and Brill (2000) successfully applied Bagging to enhance a constituent parser. Moreover, Bagging has been applied to combine multiple solutions for Chinese lexical processing (Sun, 2010b; Sun and Uszkoreit, 2012). In this paper, we apply Bagging to dependency parsin</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Michael Burke</author>
<author>Ruth O’Donovan</author>
<author>Josef Van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Longdistance dependency resolution in automatically acquired wide-coverage pcfg-based lfg approximations.</title>
<date>2004</date>
<journal>URL</journal>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<volume>http://www.</volume>
<pages>319--326</pages>
<location>Barcelona,</location>
<marker>Cahill, Burke, O’Donovan, Van Genabith, Way, 2004</marker>
<rawString>Aoife Cahill, Michael Burke, Ruth O’Donovan, Josef Van Genabith, and Andy Way. 2004. Longdistance dependency resolution in automatically acquired wide-coverage pcfg-based lfg approximations. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 319– 326. Barcelona, Spain. URL http://www. aclweb.org/anthology/P04-1041.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9215" citStr="Charniak, 2000" startWordPosition="1340" endWordPosition="1341">ions normally contain richer information and thus are reliable for tree conversion. 2.2.1 Constituency parsing Compared to many other languages, statistical constituent parsing for Chinese has reached early success, due to the fact that the language has relatively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&amp;Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from da</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="9244" citStr="Charniak and Johnson, 2005" startWordPosition="1342" endWordPosition="1345">ntain richer information and thus are reliable for tree conversion. 2.2.1 Constituency parsing Compared to many other languages, statistical constituent parsing for Chinese has reached early success, due to the fact that the language has relatively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&amp;Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCF</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 173–180. Association for Computational Linguistics, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Valentin Spitkovsky</author>
<author>Ting Liu</author>
</authors>
<title>A comparison of chinese parsers for stanford dependencies.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>11--16</pages>
<contexts>
<context position="10178" citStr="Che et al., 2012" startWordPosition="1480" endWordPosition="1483">model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCFGLA parser leverages on an automatic procedure to learn refined grammars and are therefore more robust to parse non-English languages that are not well studied. For Chinese, such a parser achieves the state-of-the-art performance and defeats many other types of parsers, including Collins as well as Charniak parser (Che et al., 2012) and 1For example, as two popular dependency treebanks, the CoNLL 2009 data and the Chinese Dependency Treebank both excluede non-projective annotations. It is worth noting that the former one is converted from a constituency treebank while the latter one is directly annotated by lingusitics. discriminative transition-based models (Zhang and Clark, 2009). 2.2.2 CS to DS conversion In the absence of dependency and constituency structures for a particular treebank, treebank-guided parser developers normally apply rich linguistic rules to convert one representation formalism to another to get nec</context>
<context position="15883" citStr="Che et al., 2012" startWordPosition="2324" endWordPosition="2327">first one employ a simple feature set (Zhang and Clark, 2008) and a small beam (16); the second one employ rich features (Zhang and Nivre, 2011) and a larger beam (32). Two graph-based parsing results are reported; the difference between them is whether integrate relation labels into the parsing procedure. Roughly speaking, currently state-of-the-art data-driven models achieves slightly better precision than unlexicalized PCFG-based models with regard to unlabeled dependency prediction. There is a big gap between lexicalized and unlexicalized parsing. The same phenomenon has been observed by (Che et al., 2012) and (Zhuang and Zong, 2010). In addition to dependency parsing, Zhuang and Zong (2010) found that Berkeley parser produce much more accurate syntactic analyses to assist a Chinese semantic role labeler than Bikel parser. Charniak and Stanford parsers are two other wellknown and frequently used tools that can provide lexicalized parsing results. According to (Che et al., 2012), they perform even worse than Bikel parser, at least for Stanford dependencies. Due to the poor parsing performance, we only concentrate on the unlexicalized model in the remainder of this paper. The performance of label</context>
</contexts>
<marker>Che, Spitkovsky, Liu, 2012</marker>
<rawString>Wanxiang Che, Valentin Spitkovsky, and Ting Liu. 2012. A comparison of chinese parsers for stanford dependencies. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 11–16. Association for Computational Linguistics, Jeju Island, Korea. URL http://www.aclweb.org/ anthology/P12-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="9199" citStr="Collins, 2003" startWordPosition="1338" endWordPosition="1339">ructure annotations normally contain richer information and thus are reliable for tree conversion. 2.2.1 Constituency parsing Compared to many other languages, statistical constituent parsing for Chinese has reached early success, due to the fact that the language has relatively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&amp;Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-ca</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Jan Hajic</author>
<author>Lance Ramshaw</author>
<author>Christoph Tillmann</author>
</authors>
<title>A statistical parser for czech.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>505--512</pages>
<location>College Park, Maryland, USA.</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="33347" citStr="Collins et al., 1999" startWordPosition="5072" endWordPosition="5075">re two automatically converted pseudo constituency trees. By applying DS to CS rules, we can acquire pseudo constituency treebanks and then learn pseudo grammars from them. 308 (1) Dependency tree (2) Linguistic constituency tree (3) Flat constituency tree (4) Binarized constituency tree Figure 3: An example: China encourages private entrepreneurs to invest in national infrastructure. The basic idea of our method is to use parsing models in one formalism for parsing in another formalism. In previous work, PCFGs are used to solve parsing problems in many other formalisms, including dependency (Collins et al., 1999), CCG (Fowler and Penn, 2010), LFG (Cahill et al., 2004) and HPSG (Zhang and Krieger, 2011) parsing. 5.1 Strategies for DS to CS conversion The conversion from DS to CS is a non-trivial problem. One main issue in the conversion is the indeterminancy in the choice of a phrasal category given a dependency relation, the level and position of attachment of a dependent in the constituency structure, as dependency relations typically do not encode such information. To convert a DS to a CS, especially for dependency parsing, we should consider (1) how to transform between the topological structures, </context>
</contexts>
<marker>Collins, Hajic, Ramshaw, Tillmann, 1999</marker>
<rawString>Michael Collins, Jan Hajic, Lance Ramshaw, and Christoph Tillmann. 1999. A statistical parser for czech. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 505–512. Association for Computational Linguistics, College Park, Maryland, USA. URL http://www.aclweb.org/ anthology/P99-1065.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: an exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics - Vol1077–1086. Association for Computational Linguistics, Uppsala, Sweden. URL http://www.</booktitle>
<pages>10--1110</pages>
<contexts>
<context position="25561" citStr="Eisner, 1996" startWordPosition="3864" endWordPosition="3865">uperior to each of the individual parsers. We implement their method to aggregate models. Once we have obtained multiple dependency trees respectively from base parsers, we can build a graph where each word in the sentence is a 306 node. We then create weighted directed edges between the nodes corresponding to words for which dependencies are obtained from each of the initial structures. The weights are the word-by-word voting results of sub-models. Based on this graph, the sentence can be reparsed by a graph-based algorithm. Taking Chinese as a projective language, we use Eisner’s algorithm (Eisner, 1996) to combine multiple dependency parses. Surdeanu and Manning (2010) indicates that reparsing performs essentially as well as other simpler or more complex models. 4.2 Parameter tuning We evaluate our combination model on the same data set used in the last section. The two hyperparameters (A and m) of our Bagging model are tuned on the development (validation) set. On one hand, with the increase of the size of sub-samples, i.e. A, the performance of sub-models is improved. However, since the sub-models overlap more, the diversity of base models for ensemble will decrease and the final predictio</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: an exploration. In Proceedings of the 16th conference on Computational linguistics - Vol1077–1086. Association for Computational Linguistics, Uppsala, Sweden. URL http://www. aclweb.org/anthology/P10-1110.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Zhongqiang Huang</author>
<author>Vladimir Eidelman</author>
<author>Mary Harper</author>
</authors>
<title>Improving a simple bigram hmm part-of-speech tagger by latent annotation and self-training.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>213--216</pages>
<location>Boulder, Colorado. URL http://www.aclweb.org/</location>
<contexts>
<context position="29008" citStr="Huang et al. (2009)" startWordPosition="4412" endWordPosition="4415">enhance a semi-Markov model but not the Markov one. Experiments on POS tagging indicated that Bagging Markov models hurts tagging performance. It seems that the relationships among basic processing units affect Bagging. PCFGLA parsers are built upon generative models with latent annotations. The use of automatically induced latent variables may also affect Bagging. Generative sequence models with latent anno86.5 85.5 84.5 83.5 82 .5 81.5 3 4 5 6 7 8 9 G raph[-lab] Tran U nlex G raph[-lab]+Unlex Tran+Unlex G raph[-lab]+Tran 307 tations can also achieve good performance for Chinese POS tagging. Huang et al. (2009) described and evaluated a bi-gram HMM tagger that utilizes latent annotations. Different from negative results of Bagging discriminative models, our auxiliary experiment shows that Bagging Huang et al.’s tagger can help Chinese POS tagging. In other words, Bagging substantially improves both HMMLA and PCFGLA models, at least for Chinese POS tagging and constituency parsing. It seems that Bagging favors the use of latent variables. 4.4 Bagging multi-view parsers 4.4.1 Results Figure 2 clearly shows that the Bagging model taking both data-driven and PCFG-based models as basic systems outperform</context>
</contexts>
<marker>Huang, Eidelman, Harper, 2009</marker>
<rawString>Zhongqiang Huang, Vladimir Eidelman, and Mary Harper. 2009. Improving a simple bigram hmm part-of-speech tagger by latent annotation and self-training. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 213–216. Association for Computational Linguistics, Boulder, Colorado. URL http://www.aclweb.org/ anthology/N/N09/N09-2054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
<author>Wanxiang Che</author>
</authors>
<title>Exploiting multiple treebanks for parsing with quasi-synchronous grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>675--684</pages>
<contexts>
<context position="40670" citStr="Li et al. (2012)" startWordPosition="6234" endWordPosition="6237">do-grammar-based parsing models behave very differently with grammar-based models. In other words, they increase the diversity of model candidates for parser ensemble. As a result, pseudo-grammar-based models lead to further improvements for parser combination. Table 5 shows that the combination of data-driven, PCFGbased and binarized pseudo-grammar-based models is significantly better than the combination of datadriven and PCFG-based models. 5.4 Comparison to the state-of-the-art Table 6 summarizes the parsing performance on the test data set, as well as the best published result reported in Li et al. (2012). To fairly compare the performance of our parser and other systems which are built without linguistic constituency trees, we only use pseudo-PCFGs in this experiment. Based on automatic POS tagging, our final model achieves a UAS of 87.23%, which yields a relative error reduction of 24% over the best published result. Table 6 also presents the results evaluated on the CTB5 data that is more widely used for previous research. Li et al. (2011) and Hatori et al. (2011) respectively evaluated their graph-based and transitionbased parsers; Zhang and Clark (2011) evaluated CoNLL-test UAS (Li et al.</context>
</contexts>
<marker>Li, Liu, Che, 2012</marker>
<rawString>Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Exploiting multiple treebanks for parsing with quasi-synchronous grammars. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675–684. Association for Computational Linguistics, Jeju Island, Korea. URL http://www.aclweb.org/ anthology/P12-1071.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Min Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
<author>Wenliang Chen</author>
<author>Haizhou Li</author>
</authors>
<title>Joint models for Chinese pos tagging and dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1180--1191</pages>
<location>Edinburgh, Scotland, UK.</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="6964" citStr="Li et al., 2011" startWordPosition="1014" endWordPosition="1017">focuses on data-driven approaches that automatically learn to produce dependency graphs for sentences solely from a hand-crafted dependency treebank. The advantage of such models is that they are easily ported to any language in which labeled linguistic resources exist. Practically all statistical models that have been proposed in recent years can be mainly described as either graph-based or transition-based (McDonald and Nivre, 2007). Both models have been adopted to learn Chinese dependency structures (Zhang and Clark, 2011; Zhang and Nivre, 2011; Huang and Sagae, 2010; Hatori et al., 2011; Li et al., 2011, 2012). According to published results, graph-based and transition-based parsers achieve similar accuracy. In the graph-based framework, informative evaluation results have been presented in (Li et al., 2011). First, second and third order projective parsing models are well evaluated. In the transition-based framework, two advanced techniques have been studied. First, developing features has been shown crucial to advancing parsing accuracy and a very rich feature set is carefully evaluated by Zhang and Nivre (2011). Second, beyond deterministic greedy search, principled dynamic programming st</context>
<context position="41116" citStr="Li et al. (2011)" startWordPosition="6310" endWordPosition="6313"> 5.4 Comparison to the state-of-the-art Table 6 summarizes the parsing performance on the test data set, as well as the best published result reported in Li et al. (2012). To fairly compare the performance of our parser and other systems which are built without linguistic constituency trees, we only use pseudo-PCFGs in this experiment. Based on automatic POS tagging, our final model achieves a UAS of 87.23%, which yields a relative error reduction of 24% over the best published result. Table 6 also presents the results evaluated on the CTB5 data that is more widely used for previous research. Li et al. (2011) and Hatori et al. (2011) respectively evaluated their graph-based and transitionbased parsers; Zhang and Clark (2011) evaluated CoNLL-test UAS (Li et al., 2012) 83.23% Graph+Tran+FlatH+BinH+BinLR 87.23% CTB5-test UAS (Li et al., 2011) 80.79% (Hatori et al., 2011) 81.33% (Zhang and Clark, 2011) 81.21% Graph+Tran+FlatH+BinH+BinLR 84.65% Table 6: UAS of different models on the test data. a hybrid data-driven parser. Our model is significantly better than these systems: It achieves a UAS of 84.65%, which obtains an error reduction of 18% over the best system in the literature. 6 Conclusion and Fu</context>
</contexts>
<marker>Li, Zhang, Che, Liu, Chen, Li, 2011</marker>
<rawString>Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Li. 2011. Joint models for Chinese pos tagging and dependency parsing. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1180–1191. Association for Computational Linguistics, Edinburgh, Scotland, UK. URL http://www.aclweb.org/ anthology/D11-1109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic cfg with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>75--82</pages>
<contexts>
<context position="9456" citStr="Matsuzaki et al., 2005" startWordPosition="1369" endWordPosition="1372">hat the language has relatively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&amp;Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCFGLA parser leverages on an automatic procedure to learn refined grammars and are therefore more robust to parse non-English languages that are not well studied. For Chinese, such a parser achieves the state-of-th</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic cfg with latent annotations. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 75–82. Association for Computational Linguistics, Stroudsburg, PA,USA. URL http://dx.doi.org/10. 3115/1219840.1219850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative learning and spanning tree algorithms for dependency parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA, USA. AAI3225503.</location>
<contexts>
<context position="1501" citStr="McDonald, 2006" startWordPosition="208" endWordPosition="209">pseudo grammars increase the diversity of base models; therefore, together with all other models, further improve system combination. Based on automatic POS tagging, our final model achieves a UAS of 87.23%, resulting in a significant improvement of the state of the art. 1 Introduction Popular approaches to dependency parsing can be divided into two classes: grammar-free and grammar-based. Data-driven, grammar-free approaches make essential use of machine learning from linguistic annotations in order to parse new sentences. Such approaches, e.g. transition-based (Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) have attracted the most attention in recent years. In contrast, grammarbased approaches rely on linguistic grammars (in either dependency or constituency formalisms) to shape the search space for possible syntactic analysis. In particular, CFG-based dependency parsing exploits a mapping between dependency and constituency representations and reuses parsing algorithms developed for CFG to produce dependency structures. In previous work, data-driven, discriminative approaches have been widely discussed for Chinese dependency parsing. On the other hand, various PCFG</context>
<context position="4066" citStr="McDonald, 2006" startWordPosition="577" endWordPosition="579">technique to build more accurate parsers based on multiple, diverse, weaker models. Exploiting differ301 Transactions of the Association for Computational Linguistics, 1 (2013) 301–314. Action Editor: Jason Eisner. Submitted 6/2012; Revised 10/2012; Published 7/2013. c�2013 Association for Computational Linguistics. ent data-driven models, e.g. transition- and graphbased models, has received the most attention in dependency parser ensemble (Nivre and McDonald, 2008; Torres Martins et al., 2008; Sagae and Lavie, 2006). Only a few works investigate integrating data-driven and PCFG-based models (McDonald, 2006). We argue that grammars can significantly increase the diversity of base models, which plays a central role in parser ensemble, and therefore lead to better and more promising hybrid systems. We introduce a general classifier enhancing technique, i.e. bootstrap aggregating (Bagging), to improve dependency parsing accuracy. This technique can be applied to enhance a single-view parser, or to combine multiple heterogeneous parsers. Experiments on the CoNLL 09 shared task data demonstrate its effectiveness: (1) Bagging can improve individual single-view parsers, especially the PCFGbased one; (2)</context>
<context position="11829" citStr="McDonald, 2006" startWordPosition="1723" endWordPosition="1724">outputs of an automatic phrase structure parser, we can build a PCFG-based dependency parser. 2.3 Parser ensemble NLP systems built on particular single views normally capture different properties of an original problem, and therefore differ in predictive powers. As a result, NLP systems can take advantage of complementary strengths of multiple views. Combining the outputs of several systems has been shown in the past to improve parsing performance significantly, including integrating phrase-structure parsers (Henderson and Brill, 1999), dependency parsers (Nivre and McDonald, 2008), or both (McDonald, 2006). Several ensemble models have been proposed for the parsing of syntactic constituents and dependencies, including learning-based stacking (Nivre and McDonald, 2008; Torres Martins et al., 2008) and learning-free post-inference (Henderson and Brill, 1999; Sagae and Lavie, 2006). Surdeanu and Manning (2010) present a systematic analysis of these ensemble methods and find several non-obvious facts: • the diversity of base parsers is more important than complex models for learning, and • simplest scoring model for voting and reparsing performs essentially as well as other more complex models. 303</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative learning and spanning tree algorithms for dependency parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA, USA. AAI3225503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>122--131</pages>
<location>Prague, Czech Republic. URL http://www.aclweb.org/</location>
<contexts>
<context position="6787" citStr="McDonald and Nivre, 2007" startWordPosition="983" endWordPosition="986"> reductions of 18-24% over the best published results in the literature. 2 Background and related work 2.1 Data-driven dependency parsing The mainstream work on recent dependency parsing focuses on data-driven approaches that automatically learn to produce dependency graphs for sentences solely from a hand-crafted dependency treebank. The advantage of such models is that they are easily ported to any language in which labeled linguistic resources exist. Practically all statistical models that have been proposed in recent years can be mainly described as either graph-based or transition-based (McDonald and Nivre, 2007). Both models have been adopted to learn Chinese dependency structures (Zhang and Clark, 2011; Zhang and Nivre, 2011; Huang and Sagae, 2010; Hatori et al., 2011; Li et al., 2011, 2012). According to published results, graph-based and transition-based parsers achieve similar accuracy. In the graph-based framework, informative evaluation results have been presented in (Li et al., 2011). First, second and third order projective parsing models are well evaluated. In the transition-based framework, two advanced techniques have been studied. First, developing features has been shown crucial to advan</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 122–131. Association for Computational Linguistics, Prague, Czech Republic. URL http://www.aclweb.org/ anthology/D/D07/D07-1013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<booktitle>Comput. Linguist., 34:513–553. URL http://dx.doi.org/10.1162/coli.</booktitle>
<pages>07--056</pages>
<contexts>
<context position="1469" citStr="Nivre, 2008" startWordPosition="204" endWordPosition="205">to parser ensemble. Moreover, pseudo grammars increase the diversity of base models; therefore, together with all other models, further improve system combination. Based on automatic POS tagging, our final model achieves a UAS of 87.23%, resulting in a significant improvement of the state of the art. 1 Introduction Popular approaches to dependency parsing can be divided into two classes: grammar-free and grammar-based. Data-driven, grammar-free approaches make essential use of machine learning from linguistic annotations in order to parse new sentences. Such approaches, e.g. transition-based (Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) have attracted the most attention in recent years. In contrast, grammarbased approaches rely on linguistic grammars (in either dependency or constituency formalisms) to shape the search space for possible syntactic analysis. In particular, CFG-based dependency parsing exploits a mapping between dependency and constituency representations and reuses parsing algorithms developed for CFG to produce dependency structures. In previous work, data-driven, discriminative approaches have been widely discussed for Chinese dependency parsing.</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Comput. Linguist., 34:513–553. URL http://dx.doi.org/10.1162/coli. 07-056-R1-07-027.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>950--958</pages>
<location>Columbus, Ohio. URL http://www.aclweb.org/anthology/</location>
<contexts>
<context position="3920" citStr="Nivre and McDonald, 2008" startWordPosition="552" endWordPosition="556">mpact on dependency parsing and PCFGbased models have complementary predictive powers to data-driven models. System ensemble is an effective and important technique to build more accurate parsers based on multiple, diverse, weaker models. Exploiting differ301 Transactions of the Association for Computational Linguistics, 1 (2013) 301–314. Action Editor: Jason Eisner. Submitted 6/2012; Revised 10/2012; Published 7/2013. c�2013 Association for Computational Linguistics. ent data-driven models, e.g. transition- and graphbased models, has received the most attention in dependency parser ensemble (Nivre and McDonald, 2008; Torres Martins et al., 2008; Sagae and Lavie, 2006). Only a few works investigate integrating data-driven and PCFG-based models (McDonald, 2006). We argue that grammars can significantly increase the diversity of base models, which plays a central role in parser ensemble, and therefore lead to better and more promising hybrid systems. We introduce a general classifier enhancing technique, i.e. bootstrap aggregating (Bagging), to improve dependency parsing accuracy. This technique can be applied to enhance a single-view parser, or to combine multiple heterogeneous parsers. Experiments on the </context>
<context position="11803" citStr="Nivre and McDonald, 2008" startWordPosition="1717" endWordPosition="1720">ng this conversion procedure on the outputs of an automatic phrase structure parser, we can build a PCFG-based dependency parser. 2.3 Parser ensemble NLP systems built on particular single views normally capture different properties of an original problem, and therefore differ in predictive powers. As a result, NLP systems can take advantage of complementary strengths of multiple views. Combining the outputs of several systems has been shown in the past to improve parsing performance significantly, including integrating phrase-structure parsers (Henderson and Brill, 1999), dependency parsers (Nivre and McDonald, 2008), or both (McDonald, 2006). Several ensemble models have been proposed for the parsing of syntactic constituents and dependencies, including learning-based stacking (Nivre and McDonald, 2008; Torres Martins et al., 2008) and learning-free post-inference (Henderson and Brill, 1999; Sagae and Lavie, 2006). Surdeanu and Manning (2010) present a systematic analysis of these ensemble methods and find several non-obvious facts: • the diversity of base parsers is more important than complex models for learning, and • simplest scoring model for voting and reparsing performs essentially as well as othe</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of ACL-08: HLT, pages 950–958. Association for Computational Linguistics, Columbus, Ohio. URL http://www.aclweb.org/anthology/ P/P08/P08-1108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="9478" citStr="Petrov et al., 2006" startWordPosition="1373" endWordPosition="1376">atively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&amp;Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCFGLA parser leverages on an automatic procedure to learn refined grammars and are therefore more robust to parse non-English languages that are not well studied. For Chinese, such a parser achieves the state-of-the-art performance and </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440. Association for Computational Linguistics, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short ’06,</booktitle>
<pages>129--132</pages>
<location>Stroudsburg, PA, USA.</location>
<note>URL http://portal.acm.org/ citation.cfm?id=1614049.1614082.</note>
<contexts>
<context position="3973" citStr="Sagae and Lavie, 2006" startWordPosition="562" endWordPosition="565">omplementary predictive powers to data-driven models. System ensemble is an effective and important technique to build more accurate parsers based on multiple, diverse, weaker models. Exploiting differ301 Transactions of the Association for Computational Linguistics, 1 (2013) 301–314. Action Editor: Jason Eisner. Submitted 6/2012; Revised 10/2012; Published 7/2013. c�2013 Association for Computational Linguistics. ent data-driven models, e.g. transition- and graphbased models, has received the most attention in dependency parser ensemble (Nivre and McDonald, 2008; Torres Martins et al., 2008; Sagae and Lavie, 2006). Only a few works investigate integrating data-driven and PCFG-based models (McDonald, 2006). We argue that grammars can significantly increase the diversity of base models, which plays a central role in parser ensemble, and therefore lead to better and more promising hybrid systems. We introduce a general classifier enhancing technique, i.e. bootstrap aggregating (Bagging), to improve dependency parsing accuracy. This technique can be applied to enhance a single-view parser, or to combine multiple heterogeneous parsers. Experiments on the CoNLL 09 shared task data demonstrate its effectivene</context>
<context position="12107" citStr="Sagae and Lavie, 2006" startWordPosition="1759" endWordPosition="1762">sult, NLP systems can take advantage of complementary strengths of multiple views. Combining the outputs of several systems has been shown in the past to improve parsing performance significantly, including integrating phrase-structure parsers (Henderson and Brill, 1999), dependency parsers (Nivre and McDonald, 2008), or both (McDonald, 2006). Several ensemble models have been proposed for the parsing of syntactic constituents and dependencies, including learning-based stacking (Nivre and McDonald, 2008; Torres Martins et al., 2008) and learning-free post-inference (Henderson and Brill, 1999; Sagae and Lavie, 2006). Surdeanu and Manning (2010) present a systematic analysis of these ensemble methods and find several non-obvious facts: • the diversity of base parsers is more important than complex models for learning, and • simplest scoring model for voting and reparsing performs essentially as well as other more complex models. 303 3 A comparative analysis of heterogeneous parsers The information encoded in a dependency representation is different from the information captured in a constituency representation. While the dependency structure represents head-dependent relations between words, the constitue</context>
<context position="24844" citStr="Sagae and Lavie (2006)" startWordPosition="3744" endWordPosition="3747">ly without replacement. Each Di can be used to train a single-view parser or multiple parsers according to different views. Using this strategy, we can get m weak parsers or km parsers if multiple views are implemented. In the parsing phase, for each sentence, the (k)m models output (k)m candidate analyses that are combined in a post-inference procedure. Aggregating step. Different from classification problems, simple voting scheme is not suitable for parsing, which is a typical structured prediction problem. To aggregate outputs of (k)m sub-models, a structured inference procedure is needed. Sagae and Lavie (2006) present a framework for combining the output of several different parsers to produce results that are superior to each of the individual parsers. We implement their method to aggregate models. Once we have obtained multiple dependency trees respectively from base parsers, we can build a graph where each word in the sentence is a 306 node. We then create weighted directed edges between the nodes corresponding to words for which dependencies are obtained from each of the initial structures. The weights are the word-by-word voting results of sub-models. Based on this graph, the sentence can be r</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser combination by reparsing. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short ’06, pages 129–132. Association for Computational Linguistics, Stroudsburg, PA, USA. URL http://portal.acm.org/ citation.cfm?id=1614049.1614082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>Improving Chinese semantic role labeling with rich syntactic features.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>168--172</pages>
<location>Uppsala,</location>
<contexts>
<context position="18013" citStr="Sun, 2010" startWordPosition="2662" endWordPosition="2663">y of different parsers. The first block presents baseline parsers; the last two blocks present Bagging-enhanced parsers, where m is respectively set to 15 and 8. Z08 and Z11 distinguish different feature sets; b=16 and b=32 are beam sizes. +/-lab means whether to incorporate relation labels to a model. structures, and an extra statistical classifier can be employed to label automatically recognized dependencies with a high accuracy. Although this issue is not well studied for Chinese dependency parsing, previous research on function tag labeling (Sun and Sui, 2009) and semantic role labeling (Sun, 2010a) gives us some clues. Their research shows that both functional and predicate-argument structural information is relatively easy to predict if high-quality syntactic parses are available. We mainly focus on the UAS metric in the following experiments. 3.3 Constraints A grammar-based model utilizes an explicitly defined formal grammar to shape the search space for possible syntactic hypotheses. Parameters of a statistical grammar-based model are related to a grammar rule, and as a result specific language constructions are constrained by each other. For example, parameters are assigned to rew</context>
<context position="23607" citStr="Sun, 2010" startWordPosition="3550" endWordPosition="3551">on and regression models in terms of stability and classification accuracy (Breiman, 1996). It also reduces variance and helps to avoid overfitting. Given a training set D of size n, Bagging generates m new training sets Di of size n′ ≤ n, by sampling examples from D. m models are separately learned on the m new training sets and combined by voting (for classification) or averaging the output (for regression). Henderson and Brill (2000) successfully applied Bagging to enhance a constituent parser. Moreover, Bagging has been applied to combine multiple solutions for Chinese lexical processing (Sun, 2010b; Sun and Uszkoreit, 2012). In this paper, we apply Bagging to dependency parsing. Since training even one single parser takes hours (if not days), experiments on Bagging is time-consuming. To save time, we conduct data-driven parsing experiments based on simple configuration. More specifically, the beam size of the transition-based parser is set to 16, and the simple feature set is utilized; dependency relations are not incorporated for the graph-based parser. Bootrapping step. In the training phase, given a training set D of size n, our model generates m new training sets Di of size An by s</context>
<context position="28183" citStr="Sun, 2010" startWordPosition="4284" endWordPosition="4285">raph-based one, which performs worse with regard to the prediction of a whole sentence. The improvement for the graph-based parser is very modest. We train a Bagging(8)-enhanced Berkeley parser, which achieves equivalent overall UAS to datadriven parsers, and compare their parsing abilities of second order dependencies. Now we can more clearly see that the Bagging-enhanced PCFG-based model performs better in the prediction of second order dependencies. 4.3.2 Related experiments on sequence models Bagging has been applied to enhance discriminative sequence models for Chinese word segmentation (Sun, 2010b) and POS tagging (Sun and Uszkoreit, 2012). For word segmentation, experiments on discriminative Markov and semi-Markov tagging models are reported. Their experiments showed that Bagging can consistently enhance a semi-Markov model but not the Markov one. Experiments on POS tagging indicated that Bagging Markov models hurts tagging performance. It seems that the relationships among basic processing units affect Bagging. PCFGLA parsers are built upon generative models with latent annotations. The use of automatically induced latent variables may also affect Bagging. Generative sequence models</context>
<context position="34682" citStr="Sun, 2010" startWordPosition="5287" endWordPosition="5288">ese three aspects, we present the following strategies. 5.1.1 Topological structure The topological structures represent the boundary information of constituents in a given sentence. Dependency structures do not directly represent such boundary information. Nevertheless, a complete subtree in a projective dependency tree should be considered as a constituent. We can construct a very flat constituent tree, of which nodes are associated with complete subtrees of a dependency parse. The third tree in Figure 3 is an example of such conversion. Right-to-left binarization According to the study in (Sun, 2010a), head words of most phrases in Chinese are located at the first or the last position. That means for binarizing most phrases, we only need sequentially combine the right or left parts together with their head phrases. Main exceptions are clauses, of which the head predicate locates inside, since Chinese is an SVO language. To deal with these exceptions, we split each phrase whose head child is inside itself into three parts: left child(ren), head and right child(ren). We first sequentially combine the head and its right child(ren) that are usually objects as intermediate phrases, then seque</context>
</contexts>
<marker>Sun, 2010</marker>
<rawString>Weiwei Sun. 2010a. Improving Chinese semantic role labeling with rich syntactic features. In Proceedings of the ACL 2010 Conference Short Papers, pages 168–172. Association for Computational Linguistics, Uppsala, Sweden. URLhttp://www.aclweb.org/ anthology/P10-2031.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>Word-based and characterbased word segmentation models: Comparison and combination.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1211--1219</pages>
<location>Beijing,</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="18013" citStr="Sun, 2010" startWordPosition="2662" endWordPosition="2663">y of different parsers. The first block presents baseline parsers; the last two blocks present Bagging-enhanced parsers, where m is respectively set to 15 and 8. Z08 and Z11 distinguish different feature sets; b=16 and b=32 are beam sizes. +/-lab means whether to incorporate relation labels to a model. structures, and an extra statistical classifier can be employed to label automatically recognized dependencies with a high accuracy. Although this issue is not well studied for Chinese dependency parsing, previous research on function tag labeling (Sun and Sui, 2009) and semantic role labeling (Sun, 2010a) gives us some clues. Their research shows that both functional and predicate-argument structural information is relatively easy to predict if high-quality syntactic parses are available. We mainly focus on the UAS metric in the following experiments. 3.3 Constraints A grammar-based model utilizes an explicitly defined formal grammar to shape the search space for possible syntactic hypotheses. Parameters of a statistical grammar-based model are related to a grammar rule, and as a result specific language constructions are constrained by each other. For example, parameters are assigned to rew</context>
<context position="23607" citStr="Sun, 2010" startWordPosition="3550" endWordPosition="3551">on and regression models in terms of stability and classification accuracy (Breiman, 1996). It also reduces variance and helps to avoid overfitting. Given a training set D of size n, Bagging generates m new training sets Di of size n′ ≤ n, by sampling examples from D. m models are separately learned on the m new training sets and combined by voting (for classification) or averaging the output (for regression). Henderson and Brill (2000) successfully applied Bagging to enhance a constituent parser. Moreover, Bagging has been applied to combine multiple solutions for Chinese lexical processing (Sun, 2010b; Sun and Uszkoreit, 2012). In this paper, we apply Bagging to dependency parsing. Since training even one single parser takes hours (if not days), experiments on Bagging is time-consuming. To save time, we conduct data-driven parsing experiments based on simple configuration. More specifically, the beam size of the transition-based parser is set to 16, and the simple feature set is utilized; dependency relations are not incorporated for the graph-based parser. Bootrapping step. In the training phase, given a training set D of size n, our model generates m new training sets Di of size An by s</context>
<context position="28183" citStr="Sun, 2010" startWordPosition="4284" endWordPosition="4285">raph-based one, which performs worse with regard to the prediction of a whole sentence. The improvement for the graph-based parser is very modest. We train a Bagging(8)-enhanced Berkeley parser, which achieves equivalent overall UAS to datadriven parsers, and compare their parsing abilities of second order dependencies. Now we can more clearly see that the Bagging-enhanced PCFG-based model performs better in the prediction of second order dependencies. 4.3.2 Related experiments on sequence models Bagging has been applied to enhance discriminative sequence models for Chinese word segmentation (Sun, 2010b) and POS tagging (Sun and Uszkoreit, 2012). For word segmentation, experiments on discriminative Markov and semi-Markov tagging models are reported. Their experiments showed that Bagging can consistently enhance a semi-Markov model but not the Markov one. Experiments on POS tagging indicated that Bagging Markov models hurts tagging performance. It seems that the relationships among basic processing units affect Bagging. PCFGLA parsers are built upon generative models with latent annotations. The use of automatically induced latent variables may also affect Bagging. Generative sequence models</context>
<context position="34682" citStr="Sun, 2010" startWordPosition="5287" endWordPosition="5288">ese three aspects, we present the following strategies. 5.1.1 Topological structure The topological structures represent the boundary information of constituents in a given sentence. Dependency structures do not directly represent such boundary information. Nevertheless, a complete subtree in a projective dependency tree should be considered as a constituent. We can construct a very flat constituent tree, of which nodes are associated with complete subtrees of a dependency parse. The third tree in Figure 3 is an example of such conversion. Right-to-left binarization According to the study in (Sun, 2010a), head words of most phrases in Chinese are located at the first or the last position. That means for binarizing most phrases, we only need sequentially combine the right or left parts together with their head phrases. Main exceptions are clauses, of which the head predicate locates inside, since Chinese is an SVO language. To deal with these exceptions, we split each phrase whose head child is inside itself into three parts: left child(ren), head and right child(ren). We first sequentially combine the head and its right child(ren) that are usually objects as intermediate phrases, then seque</context>
</contexts>
<marker>Sun, 2010</marker>
<rawString>Weiwei Sun. 2010b. Word-based and characterbased word segmentation models: Comparison and combination. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1211–1219. Coling 2010 Organizing Committee, Beijing, China. URL http://www.aclweb.org/ anthology/C10-2139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Zhifang Sui</author>
</authors>
<title>Chinese function tag labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation. Hong Kong.</booktitle>
<contexts>
<context position="17975" citStr="Sun and Sui, 2009" startWordPosition="2654" endWordPosition="2657">ex 84.35 N/A 31.16 70.49 83.57 Table 1: Accuracy of different parsers. The first block presents baseline parsers; the last two blocks present Bagging-enhanced parsers, where m is respectively set to 15 and 8. Z08 and Z11 distinguish different feature sets; b=16 and b=32 are beam sizes. +/-lab means whether to incorporate relation labels to a model. structures, and an extra statistical classifier can be employed to label automatically recognized dependencies with a high accuracy. Although this issue is not well studied for Chinese dependency parsing, previous research on function tag labeling (Sun and Sui, 2009) and semantic role labeling (Sun, 2010a) gives us some clues. Their research shows that both functional and predicate-argument structural information is relatively easy to predict if high-quality syntactic parses are available. We mainly focus on the UAS metric in the following experiments. 3.3 Constraints A grammar-based model utilizes an explicitly defined formal grammar to shape the search space for possible syntactic hypotheses. Parameters of a statistical grammar-based model are related to a grammar rule, and as a result specific language constructions are constrained by each other. For e</context>
</contexts>
<marker>Sun, Sui, 2009</marker>
<rawString>Weiwei Sun and Zhifang Sui. 2009. Chinese function tag labeling. In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation. Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Hans Uszkoreit</author>
</authors>
<title>Capturing paradigmatic and syntagmatic lexical relations: Towards accurate Chinese part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14866" citStr="Sun and Uszkoreit, 2012" startWordPosition="2166" endWordPosition="2169">h-based parser2 (Bohnet, 2010) and a transition-based parser (Hatori et al., 2011), for experiments. For constituent parsing, we choose Berkeley parser,3 a well known implementation of the unlexicalized PCFGLA model and Bikel parser,4 2code.google.com/p/mate-tools/ 3code.google.com/p/berkeleyparser/ 4cis.upenn.edu/˜dbikel/software.html a well known implementation of Collins’ lexicalized model, for experiments. In data-driven parsing, features consisting of POS tags are very effective, so typically POS tagging is performed as a preprocessing. We use the baseline sequential tagger described in (Sun and Uszkoreit, 2012) to provide such lexical information to the graph-based parser. Note that the transition-based parser performs a joint inference to acquire POS and dependency information simultaneously, so there is no need to offer extra tagging results to it. 3.2 Overall performance Table 1 (Column 2-6) summarizes the overall accuracy of different parsers. Two transition-based parsing results are presented: The first one employ a simple feature set (Zhang and Clark, 2008) and a small beam (16); the second one employ rich features (Zhang and Nivre, 2011) and a larger beam (32). Two graph-based parsing results</context>
<context position="23634" citStr="Sun and Uszkoreit, 2012" startWordPosition="3552" endWordPosition="3555">ssion models in terms of stability and classification accuracy (Breiman, 1996). It also reduces variance and helps to avoid overfitting. Given a training set D of size n, Bagging generates m new training sets Di of size n′ ≤ n, by sampling examples from D. m models are separately learned on the m new training sets and combined by voting (for classification) or averaging the output (for regression). Henderson and Brill (2000) successfully applied Bagging to enhance a constituent parser. Moreover, Bagging has been applied to combine multiple solutions for Chinese lexical processing (Sun, 2010b; Sun and Uszkoreit, 2012). In this paper, we apply Bagging to dependency parsing. Since training even one single parser takes hours (if not days), experiments on Bagging is time-consuming. To save time, we conduct data-driven parsing experiments based on simple configuration. More specifically, the beam size of the transition-based parser is set to 16, and the simple feature set is utilized; dependency relations are not incorporated for the graph-based parser. Bootrapping step. In the training phase, given a training set D of size n, our model generates m new training sets Di of size An by sampling uniformly without r</context>
<context position="28227" citStr="Sun and Uszkoreit, 2012" startWordPosition="4289" endWordPosition="4293">s worse with regard to the prediction of a whole sentence. The improvement for the graph-based parser is very modest. We train a Bagging(8)-enhanced Berkeley parser, which achieves equivalent overall UAS to datadriven parsers, and compare their parsing abilities of second order dependencies. Now we can more clearly see that the Bagging-enhanced PCFG-based model performs better in the prediction of second order dependencies. 4.3.2 Related experiments on sequence models Bagging has been applied to enhance discriminative sequence models for Chinese word segmentation (Sun, 2010b) and POS tagging (Sun and Uszkoreit, 2012). For word segmentation, experiments on discriminative Markov and semi-Markov tagging models are reported. Their experiments showed that Bagging can consistently enhance a semi-Markov model but not the Markov one. Experiments on POS tagging indicated that Bagging Markov models hurts tagging performance. It seems that the relationships among basic processing units affect Bagging. PCFGLA parsers are built upon generative models with latent annotations. The use of automatically induced latent variables may also affect Bagging. Generative sequence models with latent anno86.5 85.5 84.5 83.5 82 .5 8</context>
</contexts>
<marker>Sun, Uszkoreit, 2012</marker>
<rawString>Weiwei Sun and Hans Uszkoreit. 2012. Capturing paradigmatic and syntagmatic lexical relations: Towards accurate Chinese part-of-speech tagging. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Christopher D Manning</author>
</authors>
<title>Ensemble models for dependency parsing: Cheap and good?</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>649--652</pages>
<location>Los Angeles, California.</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="12136" citStr="Surdeanu and Manning (2010)" startWordPosition="1763" endWordPosition="1767">ke advantage of complementary strengths of multiple views. Combining the outputs of several systems has been shown in the past to improve parsing performance significantly, including integrating phrase-structure parsers (Henderson and Brill, 1999), dependency parsers (Nivre and McDonald, 2008), or both (McDonald, 2006). Several ensemble models have been proposed for the parsing of syntactic constituents and dependencies, including learning-based stacking (Nivre and McDonald, 2008; Torres Martins et al., 2008) and learning-free post-inference (Henderson and Brill, 1999; Sagae and Lavie, 2006). Surdeanu and Manning (2010) present a systematic analysis of these ensemble methods and find several non-obvious facts: • the diversity of base parsers is more important than complex models for learning, and • simplest scoring model for voting and reparsing performs essentially as well as other more complex models. 303 3 A comparative analysis of heterogeneous parsers The information encoded in a dependency representation is different from the information captured in a constituency representation. While the dependency structure represents head-dependent relations between words, the constituency structure represents the </context>
<context position="25628" citStr="Surdeanu and Manning (2010)" startWordPosition="3871" endWordPosition="3874">nt their method to aggregate models. Once we have obtained multiple dependency trees respectively from base parsers, we can build a graph where each word in the sentence is a 306 node. We then create weighted directed edges between the nodes corresponding to words for which dependencies are obtained from each of the initial structures. The weights are the word-by-word voting results of sub-models. Based on this graph, the sentence can be reparsed by a graph-based algorithm. Taking Chinese as a projective language, we use Eisner’s algorithm (Eisner, 1996) to combine multiple dependency parses. Surdeanu and Manning (2010) indicates that reparsing performs essentially as well as other simpler or more complex models. 4.2 Parameter tuning We evaluate our combination model on the same data set used in the last section. The two hyperparameters (A and m) of our Bagging model are tuned on the development (validation) set. On one hand, with the increase of the size of sub-samples, i.e. A, the performance of sub-models is improved. However, since the sub-models overlap more, the diversity of base models for ensemble will decrease and the final prediction accuracy may go down. To evaluate the effect of A, we separately </context>
<context position="30106" citStr="Surdeanu and Manning (2010)" startWordPosition="4581" endWordPosition="4584">1 Results Figure 2 clearly shows that the Bagging model taking both data-driven and PCFG-based models as basic systems outperform the Bagging model taking either model in isolation as basic systems. The combination of a PCFG-based model and a data-driven model (either graph-based or transition-based) is more effective than the combination of two datadriven models, which has received the most attention in dependency parser ensemble. Table 3 is the performance of reparsing on the development data. From this table, we can see by utilizing more parsers, Bagging can enhance reparsing. According to Surdeanu and Manning (2010)’s findings, reparsing performs as well as other combination models. Our auxiliary experiments confirm this finding: Learning-based stacking cannot achieve better performance. Limited to the document length, we do not give descriptions of these experiments. Devel. UAS Reparsing(Tran[b=16,Z08]+Graph[-lab]+Unlex) 85.82 +Bagging(15) 86.37 bagging(reparse(g, t, c)) 86.09 reparse(bagging(g, t, c)) 85.86 Table 3: UAS of reparsing and Bagging. 4.4.2 Analysis In our proposed model, Bagging has a two-fold effect: One is as a system combination technique and the other as a general parser enhancing techn</context>
</contexts>
<marker>Surdeanu, Manning, 2010</marker>
<rawString>Mihai Surdeanu and Christopher D. Manning. 2010. Ensemble models for dependency parsing: Cheap and good? In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 649–652. Association for Computational Linguistics, Los Angeles, California. URL http://www.aclweb.org/ anthology/N10-1091.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Andre Torres Martins</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>342--350</pages>
<contexts>
<context position="1531" citStr="Martins et al., 2009" startWordPosition="211" endWordPosition="214">e the diversity of base models; therefore, together with all other models, further improve system combination. Based on automatic POS tagging, our final model achieves a UAS of 87.23%, resulting in a significant improvement of the state of the art. 1 Introduction Popular approaches to dependency parsing can be divided into two classes: grammar-free and grammar-based. Data-driven, grammar-free approaches make essential use of machine learning from linguistic annotations in order to parse new sentences. Such approaches, e.g. transition-based (Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) have attracted the most attention in recent years. In contrast, grammarbased approaches rely on linguistic grammars (in either dependency or constituency formalisms) to shape the search space for possible syntactic analysis. In particular, CFG-based dependency parsing exploits a mapping between dependency and constituency representations and reuses parsing algorithms developed for CFG to produce dependency structures. In previous work, data-driven, discriminative approaches have been widely discussed for Chinese dependency parsing. On the other hand, various PCFG-based constituent parsing met</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andre Torres Martins, Noah Smith, and Eric Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 342–350. Association for Computational Linguistics, Suntec, Singapore. URL http://www.aclweb.org/ anthology/P/P09/P09-1039.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Filipe Torres Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>157--166</pages>
<location>Honolulu, Hawaii. URL http://www.aclweb.org/</location>
<contexts>
<context position="3949" citStr="Martins et al., 2008" startWordPosition="558" endWordPosition="561">CFGbased models have complementary predictive powers to data-driven models. System ensemble is an effective and important technique to build more accurate parsers based on multiple, diverse, weaker models. Exploiting differ301 Transactions of the Association for Computational Linguistics, 1 (2013) 301–314. Action Editor: Jason Eisner. Submitted 6/2012; Revised 10/2012; Published 7/2013. c�2013 Association for Computational Linguistics. ent data-driven models, e.g. transition- and graphbased models, has received the most attention in dependency parser ensemble (Nivre and McDonald, 2008; Torres Martins et al., 2008; Sagae and Lavie, 2006). Only a few works investigate integrating data-driven and PCFG-based models (McDonald, 2006). We argue that grammars can significantly increase the diversity of base models, which plays a central role in parser ensemble, and therefore lead to better and more promising hybrid systems. We introduce a general classifier enhancing technique, i.e. bootstrap aggregating (Bagging), to improve dependency parsing accuracy. This technique can be applied to enhance a single-view parser, or to combine multiple heterogeneous parsers. Experiments on the CoNLL 09 shared task data dem</context>
<context position="12023" citStr="Martins et al., 2008" startWordPosition="1748" endWordPosition="1751">operties of an original problem, and therefore differ in predictive powers. As a result, NLP systems can take advantage of complementary strengths of multiple views. Combining the outputs of several systems has been shown in the past to improve parsing performance significantly, including integrating phrase-structure parsers (Henderson and Brill, 1999), dependency parsers (Nivre and McDonald, 2008), or both (McDonald, 2006). Several ensemble models have been proposed for the parsing of syntactic constituents and dependencies, including learning-based stacking (Nivre and McDonald, 2008; Torres Martins et al., 2008) and learning-free post-inference (Henderson and Brill, 1999; Sagae and Lavie, 2006). Surdeanu and Manning (2010) present a systematic analysis of these ensemble methods and find several non-obvious facts: • the diversity of base parsers is more important than complex models for learning, and • simplest scoring model for voting and reparsing performs essentially as well as other more complex models. 303 3 A comparative analysis of heterogeneous parsers The information encoded in a dependency representation is different from the information captured in a constituency representation. While the d</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>Andr´e Filipe Torres Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166. Association for Computational Linguistics, Honolulu, Hawaii. URL http://www.aclweb.org/ anthology/D08-1017.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Tapping the implicit information for the PS to DS conversion of the Chinese treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of the Sixth International Workshop on Treebanks and Linguistics Theories.</booktitle>
<contexts>
<context position="2338" citStr="Xue, 2007" startWordPosition="323" endWordPosition="324">possible syntactic analysis. In particular, CFG-based dependency parsing exploits a mapping between dependency and constituency representations and reuses parsing algorithms developed for CFG to produce dependency structures. In previous work, data-driven, discriminative approaches have been widely discussed for Chinese dependency parsing. On the other hand, various PCFG-based constituent parsing methods have been applied to obtain phrase-structures as well. With rich linguistic rules, phrase-structures of Chinese sentences can be well transformed to their corresponding dependency structures (Xue, 2007). Therefore, PCFG parsers with such conversion rules can be taken as another type of dependency parser. We call them PCFG-based parsers, in this paper. Explicitly defining linguistic rules to express precisely generic grammatical regularities, a constituency grammar can be applied to arrange sentences into a hierarchy of nested phrases, which determines constructions between larger phrases and their smaller component phrases. This type of information is different from, but highly related to, the information captured by a dependency representation. A constituency grammar, thus, has great possib</context>
<context position="10818" citStr="Xue (2007)" startWordPosition="1575" endWordPosition="1576">lar dependency treebanks, the CoNLL 2009 data and the Chinese Dependency Treebank both excluede non-projective annotations. It is worth noting that the former one is converted from a constituency treebank while the latter one is directly annotated by lingusitics. discriminative transition-based models (Zhang and Clark, 2009). 2.2.2 CS to DS conversion In the absence of dependency and constituency structures for a particular treebank, treebank-guided parser developers normally apply rich linguistic rules to convert one representation formalism to another to get necessary data to train parsers. Xue (2007) examines the linguistic adequacy of dependency structure annotation automatically converted from phrase structure treebanks with rule-based approaches. A structural approach is introduced for the constituency structure (CS) to dependency structure (DS) conversion for the Chinese Treebank data, which is the basis of the CoNLL 2009 shared task data. By applying this conversion procedure on the outputs of an automatic phrase structure parser, we can build a PCFG-based dependency parser. 2.3 Parser ensemble NLP systems built on particular single views normally capture different properties of an o</context>
</contexts>
<marker>Xue, 2007</marker>
<rawString>Nianwen Xue. 2007. Tapping the implicit information for the PS to DS conversion of the Chinese treebank. In Proceedings of the Sixth International Workshop on Treebanks and Linguistics Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Hans-Ulrich Krieger</author>
</authors>
<title>Largescale corpus-driven pcfg approximation of an hpsg.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies, pages 198– 208. Association for Computational Linguistics,</booktitle>
<pages>11--2923</pages>
<location>Dublin, Ireland. URL http://www.aclweb.</location>
<contexts>
<context position="33438" citStr="Zhang and Krieger, 2011" startWordPosition="5088" endWordPosition="5091"> can acquire pseudo constituency treebanks and then learn pseudo grammars from them. 308 (1) Dependency tree (2) Linguistic constituency tree (3) Flat constituency tree (4) Binarized constituency tree Figure 3: An example: China encourages private entrepreneurs to invest in national infrastructure. The basic idea of our method is to use parsing models in one formalism for parsing in another formalism. In previous work, PCFGs are used to solve parsing problems in many other formalisms, including dependency (Collins et al., 1999), CCG (Fowler and Penn, 2010), LFG (Cahill et al., 2004) and HPSG (Zhang and Krieger, 2011) parsing. 5.1 Strategies for DS to CS conversion The conversion from DS to CS is a non-trivial problem. One main issue in the conversion is the indeterminancy in the choice of a phrasal category given a dependency relation, the level and position of attachment of a dependent in the constituency structure, as dependency relations typically do not encode such information. To convert a DS to a CS, especially for dependency parsing, we should consider (1) how to transform between the topological structures, (2) how to induce a syntactic category, and (3) how to easily recover dependency trees from</context>
</contexts>
<marker>Zhang, Krieger, 2011</marker>
<rawString>Yi Zhang and Hans-Ulrich Krieger. 2011. Largescale corpus-driven pcfg approximation of an hpsg. In Proceedings of the 12th International Conference on Parsing Technologies, pages 198– 208. Association for Computational Linguistics, Dublin, Ireland. URL http://www.aclweb. org/anthology/W11-2923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562--571</pages>
<location>Honolulu, Hawaii. URL http://www.</location>
<contexts>
<context position="15327" citStr="Zhang and Clark, 2008" startWordPosition="2238" endWordPosition="2241">S tags are very effective, so typically POS tagging is performed as a preprocessing. We use the baseline sequential tagger described in (Sun and Uszkoreit, 2012) to provide such lexical information to the graph-based parser. Note that the transition-based parser performs a joint inference to acquire POS and dependency information simultaneously, so there is no need to offer extra tagging results to it. 3.2 Overall performance Table 1 (Column 2-6) summarizes the overall accuracy of different parsers. Two transition-based parsing results are presented: The first one employ a simple feature set (Zhang and Clark, 2008) and a small beam (16); the second one employ rich features (Zhang and Nivre, 2011) and a larger beam (32). Two graph-based parsing results are reported; the difference between them is whether integrate relation labels into the parsing procedure. Roughly speaking, currently state-of-the-art data-driven models achieves slightly better precision than unlexicalized PCFG-based models with regard to unlabeled dependency prediction. There is a big gap between lexicalized and unlexicalized parsing. The same phenomenon has been observed by (Che et al., 2012) and (Zhuang and Zong, 2010). In addition to</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 562–571. Association for Computational Linguistics, Honolulu, Hawaii. URL http://www. aclweb.org/anthology/D08-1059.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Transitionbased parsing of the Chinese treebank using a global discriminative model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>162--171</pages>
<location>Paris,</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="10534" citStr="Zhang and Clark, 2009" startWordPosition="1531" endWordPosition="1534">mars and are therefore more robust to parse non-English languages that are not well studied. For Chinese, such a parser achieves the state-of-the-art performance and defeats many other types of parsers, including Collins as well as Charniak parser (Che et al., 2012) and 1For example, as two popular dependency treebanks, the CoNLL 2009 data and the Chinese Dependency Treebank both excluede non-projective annotations. It is worth noting that the former one is converted from a constituency treebank while the latter one is directly annotated by lingusitics. discriminative transition-based models (Zhang and Clark, 2009). 2.2.2 CS to DS conversion In the absence of dependency and constituency structures for a particular treebank, treebank-guided parser developers normally apply rich linguistic rules to convert one representation formalism to another to get necessary data to train parsers. Xue (2007) examines the linguistic adequacy of dependency structure annotation automatically converted from phrase structure treebanks with rule-based approaches. A structural approach is introduced for the constituency structure (CS) to dependency structure (DS) conversion for the Chinese Treebank data, which is the basis o</context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Yue Zhang and Stephen Clark. 2009. Transitionbased parsing of the Chinese treebank using a global discriminative model. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09), pages 162–171. Association for Computational Linguistics, Paris, France. URL http://www.aclweb.org/ anthology/W09-3825.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Comput. Linguist.,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>151</pages>
<note>URL http://dx.doi.org/10.1162/ coli_a_00037.</note>
<contexts>
<context position="6880" citStr="Zhang and Clark, 2011" startWordPosition="998" endWordPosition="1001"> work 2.1 Data-driven dependency parsing The mainstream work on recent dependency parsing focuses on data-driven approaches that automatically learn to produce dependency graphs for sentences solely from a hand-crafted dependency treebank. The advantage of such models is that they are easily ported to any language in which labeled linguistic resources exist. Practically all statistical models that have been proposed in recent years can be mainly described as either graph-based or transition-based (McDonald and Nivre, 2007). Both models have been adopted to learn Chinese dependency structures (Zhang and Clark, 2011; Zhang and Nivre, 2011; Huang and Sagae, 2010; Hatori et al., 2011; Li et al., 2011, 2012). According to published results, graph-based and transition-based parsers achieve similar accuracy. In the graph-based framework, informative evaluation results have been presented in (Li et al., 2011). First, second and third order projective parsing models are well evaluated. In the transition-based framework, two advanced techniques have been studied. First, developing features has been shown crucial to advancing parsing accuracy and a very rich feature set is carefully evaluated by Zhang and Nivre (</context>
<context position="41234" citStr="Zhang and Clark (2011)" startWordPosition="6328" endWordPosition="6331"> as the best published result reported in Li et al. (2012). To fairly compare the performance of our parser and other systems which are built without linguistic constituency trees, we only use pseudo-PCFGs in this experiment. Based on automatic POS tagging, our final model achieves a UAS of 87.23%, which yields a relative error reduction of 24% over the best published result. Table 6 also presents the results evaluated on the CTB5 data that is more widely used for previous research. Li et al. (2011) and Hatori et al. (2011) respectively evaluated their graph-based and transitionbased parsers; Zhang and Clark (2011) evaluated CoNLL-test UAS (Li et al., 2012) 83.23% Graph+Tran+FlatH+BinH+BinLR 87.23% CTB5-test UAS (Li et al., 2011) 80.79% (Hatori et al., 2011) 81.33% (Zhang and Clark, 2011) 81.21% Graph+Tran+FlatH+BinH+BinLR 84.65% Table 6: UAS of different models on the test data. a hybrid data-driven parser. Our model is significantly better than these systems: It achieves a UAS of 84.65%, which obtains an error reduction of 18% over the best system in the literature. 6 Conclusion and Future Work There have been several attempts to develop high accuracy parsers in both constituency and dependency formal</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Comput. Linguist., 37(1):105– 151. URL http://dx.doi.org/10.1162/ coli_a_00037.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transitionbased dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188– 193. Association for Computational Linguistics,</booktitle>
<pages>11--2033</pages>
<location>Portland, Oregon, USA.</location>
<note>URL http://www.</note>
<contexts>
<context position="6903" citStr="Zhang and Nivre, 2011" startWordPosition="1002" endWordPosition="1005">ependency parsing The mainstream work on recent dependency parsing focuses on data-driven approaches that automatically learn to produce dependency graphs for sentences solely from a hand-crafted dependency treebank. The advantage of such models is that they are easily ported to any language in which labeled linguistic resources exist. Practically all statistical models that have been proposed in recent years can be mainly described as either graph-based or transition-based (McDonald and Nivre, 2007). Both models have been adopted to learn Chinese dependency structures (Zhang and Clark, 2011; Zhang and Nivre, 2011; Huang and Sagae, 2010; Hatori et al., 2011; Li et al., 2011, 2012). According to published results, graph-based and transition-based parsers achieve similar accuracy. In the graph-based framework, informative evaluation results have been presented in (Li et al., 2011). First, second and third order projective parsing models are well evaluated. In the transition-based framework, two advanced techniques have been studied. First, developing features has been shown crucial to advancing parsing accuracy and a very rich feature set is carefully evaluated by Zhang and Nivre (2011). Second, beyond d</context>
<context position="15410" citStr="Zhang and Nivre, 2011" startWordPosition="2254" endWordPosition="2257">. We use the baseline sequential tagger described in (Sun and Uszkoreit, 2012) to provide such lexical information to the graph-based parser. Note that the transition-based parser performs a joint inference to acquire POS and dependency information simultaneously, so there is no need to offer extra tagging results to it. 3.2 Overall performance Table 1 (Column 2-6) summarizes the overall accuracy of different parsers. Two transition-based parsing results are presented: The first one employ a simple feature set (Zhang and Clark, 2008) and a small beam (16); the second one employ rich features (Zhang and Nivre, 2011) and a larger beam (32). Two graph-based parsing results are reported; the difference between them is whether integrate relation labels into the parsing procedure. Roughly speaking, currently state-of-the-art data-driven models achieves slightly better precision than unlexicalized PCFG-based models with regard to unlabeled dependency prediction. There is a big gap between lexicalized and unlexicalized parsing. The same phenomenon has been observed by (Che et al., 2012) and (Zhuang and Zong, 2010). In addition to dependency parsing, Zhuang and Zong (2010) found that Berkeley parser produce much</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transitionbased dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188– 193. Association for Computational Linguistics, Portland, Oregon, USA. URL http://www. aclweb.org/anthology/P11-2033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Zhuang</author>
<author>Chengqing Zong</author>
</authors>
<title>A minimum error weighting combination strategy for Chinese semantic role labeling.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1362--1370</pages>
<location>Beijing, China. URL http://www.aclweb.</location>
<contexts>
<context position="15911" citStr="Zhuang and Zong, 2010" startWordPosition="2329" endWordPosition="2332">le feature set (Zhang and Clark, 2008) and a small beam (16); the second one employ rich features (Zhang and Nivre, 2011) and a larger beam (32). Two graph-based parsing results are reported; the difference between them is whether integrate relation labels into the parsing procedure. Roughly speaking, currently state-of-the-art data-driven models achieves slightly better precision than unlexicalized PCFG-based models with regard to unlabeled dependency prediction. There is a big gap between lexicalized and unlexicalized parsing. The same phenomenon has been observed by (Che et al., 2012) and (Zhuang and Zong, 2010). In addition to dependency parsing, Zhuang and Zong (2010) found that Berkeley parser produce much more accurate syntactic analyses to assist a Chinese semantic role labeler than Bikel parser. Charniak and Stanford parsers are two other wellknown and frequently used tools that can provide lexicalized parsing results. According to (Che et al., 2012), they perform even worse than Bikel parser, at least for Stanford dependencies. Due to the poor parsing performance, we only concentrate on the unlexicalized model in the remainder of this paper. The performance of labeled dependency prediction of </context>
</contexts>
<marker>Zhuang, Zong, 2010</marker>
<rawString>Tao Zhuang and Chengqing Zong. 2010. A minimum error weighting combination strategy for Chinese semantic role labeling. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1362– 1370. Coling 2010 Organizing Committee, Beijing, China. URL http://www.aclweb. org/anthology/C10-1153.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>