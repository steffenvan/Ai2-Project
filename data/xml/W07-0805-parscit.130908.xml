<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000110">
<title confidence="0.995839">
Syllable-Based Speech Recognition for Amharic
</title>
<author confidence="0.960065">
Solomon Teferra Abate Wolfgang Menzel
</author>
<email confidence="0.652827">
solomon_teferra_7@yahoo.com menzel@informatik.uni-hamburg.de
</email>
<affiliation confidence="0.856539">
Uniformity of Hamburg, Department of Informatik Natural Language Systems Groups
</affiliation>
<address confidence="0.286603">
Vogt-Kölln-Strasse. 30, D-22527 Hamburg, Germany
</address>
<sectionHeader confidence="0.956652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999664">
Amharic is the Semitic language that has the
second large number of speakers after Arabic
(Hayward and Richard 1999). Its writing system is
syllabic with Consonant-Vowel (CV) syllable
structure. Amharic orthography has more or less a
one to one correspondence with syllabic sounds.
We have used this feature of Amharic to develop a
CV syllable-based speech recognizer, using Hid-
den Markov Modeling (HMM), and achieved
90.43% word recognition accuracy.
</bodyText>
<sectionHeader confidence="0.998793" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987947411764706">
Most of the Semitic languages are technologically
unfavored. Amharic is one of these languages that
are looking for technological considerations of re-
searchers and developers in the area of natural lan-
guage processing (NLP). Automatic Speech Re-
cognition (ASR) is one of the major areas of NLP
that is understudied in Amharic. Only few attempts
(Solomon, 2001; Kinfe, 2002; Zegaye, 2003;
Martha, 2003; Hussien and Gambäck, 2005;
Solomon et al., 2005; Solomon, 2006) have been
made.
We have developed an ASR for the language
using CV syllables as recognition units. In this
paper we present the development and the
recognition performance of the recognizer
following a brief description of the Amharic
language and speech recognition technology.
</bodyText>
<sectionHeader confidence="0.835828" genericHeader="method">
2 The Amharic Language
</sectionHeader>
<bodyText confidence="0.998863913043478">
Amharic, which belongs to the Semitic language
family, is the official language of Ethiopia. In this
family, Amharic stands second in its number of
speakers after Arabic (Hayward and Richard
1999). Amharic has five dialectical variations (Ad-
dis Ababa, Gojjam, Gonder, Wollo, and Menz)
spoken in different regions of the country (Cowley,
et.al. 1976). The speech of Addis Ababa has
emerged as the standard dialect and has wide cur-
rency across all Amharic-speaking communities
(Hayward and Richard 1999).
As with all of the other languages, Amharic has
its own characterizing phonetic, phonological and
morphological properties. For example, it has a set
of speech sounds that is not found in other lan-
guages. For example the following sounds are not
found in English: [p`], [tS`], [s`], [t`], and [q].
Amharic also has its own inventory of speech
sounds. It has thirty one consonants and seven
vowels. The consonants are generally classified as
stops, fricatives, nasals, liquids, and semi-vowels
(Leslau 2000). Tables 1 and 2 show the classifica-
tion of Amharic consonants and vowels1.
</bodyText>
<table confidence="0.999097736842105">
Man Voic Place of Articulation
of ing
Art
Lab Den Pal Vel Glot
Stops Vs [p] [t] [tS] [k] [?]
Vd [b] [d] [d3] [g]
Glott [p`] [t`] [tS`] [q]
Rd [kw]
[gw]
[qw]
Fric Vs [f] [s] [S] [h]
Vd [z] [3]
Glott [s`]
Rd [hw]
Nas- Vd [m] [n] [n]
als
Liq Vd [l]
[r]
Sv Vd [w] [j]
</table>
<tableCaption confidence="0.998569">
Table 1: Amharic Consonants
</tableCaption>
<keyword confidence="0.2689975">
Key: Lab = Labials; Den = Dentals; Pal = Palat-
als; Vel = Velars; Glot = Glottal; Vs = Voiceless;
</keyword>
<footnote confidence="0.980637">
1International Phonetic Association&apos;s (IPA) standard has
been used for representation.
</footnote>
<page confidence="0.982196">
33
</page>
<note confidence="0.682831">
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 33–40,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
Vd = Voiced; Rd = Rounded; Fric = Fricatives; Liq
= Liquids; Sv = Semi-Vowels.
Positions front center back
high [i] [u]
mid [e] [a] [o]
low [a]
</note>
<tableCaption confidence="0.981538">
Table 2: Amharic Vowels
</tableCaption>
<bodyText confidence="0.999728382352941">
Amharic is one of the languages that have their
own writing system, which is used across all Am-
haric dialects. Getachew (1967) stated that the Am-
haric writing system is phonetic. It allows any one
to write Amharic texts if s/he can speak Amharic
and has knowledge of the Amharic alphabet. Un-
like most known languages, no one needs to learn
how to spell Amharic words. In support of the
above point, Leslaw (1995) noted that no real
problems exist in Amharic orthography, as there is
more or less, a one-to-one correspondence between
the sounds and the graphic symbols, except for the
gemination of consonants and some redundant
symbols.
Many (Bender 1976; Cowley 1976; Baye 1986)
have claimed the Amharic orthography as a syllab-
ary for a relatively long period of time. Recently,
however, Taddesse (1994) and Baye (1997), who
apparently modified his view, have argued it is not.
Both of these arguments are based on the special
feature of the orthography; the possibility of rep-
resenting speech using either isolated phoneme
symbols or concatenated symbols.
In the concatenated feature, commonly known to
most of the population, each orthographic symbol
represents a consonant and a vowel, except for the
sixth order2, which is sometimes realized as a con-
sonant without a vowel and at other times a con-
sonant with a vowel. This representation of concat-
enated speech sounds by a single symbol has been
the basis for the claim made of the writing system,
as syllabary.
Amharic orthography does not indicate
gemination, but since there are relatively few
</bodyText>
<footnote confidence="0.98893575">
2An order in Amharic writing system is a combination of a
consonant with a vowel represented by a symbol. A consonant
has therefore, 7 orders or different symbols that represent its
combination with 7 Amharic vowels.
</footnote>
<bodyText confidence="0.9925454">
minimal pairs of geminations, Amharic readers do
not find this to be a problem. This property of the
writing system is analogous to the vowels of
Arabic and Hebrew, which are not normally
indicated in writing.
The Amharic orthography, as represented in the
Amharic Character set - also called [fidalI] con-
sists of 276 distinct symbols. In addition, there are
twenty numerals and eight punctuation marks. A
sample of the orthographic symbols is given in
</bodyText>
<tableCaption confidence="0.704471">
Table 3.
a u i a e o
h u u• x 9 S u C
l A h A. A i6 A A°
m ou ou. °9 °&apos;L 90 90
r L 4 L L. C C
Table 3: Some Orthographic Symbols of Amharic
</tableCaption>
<bodyText confidence="0.986574818181818">
However, research in speech recognition should
only consider distinct sounds instead of all the or-
thographic symbols, unless there is a need to de-
velop a dictation machine that includes all of the
orthographic symbols. Therefore, redundant ortho-
graphic symbols that represent the same syllabic
sounds can be eliminated. Thus, by eliminating re-
dundant graphemes, we are left with a total of 233
distinct CV syllable characters. In our work, an
HMM model has been developed for each of these
CV syllables.
</bodyText>
<sectionHeader confidence="0.938335" genericHeader="method">
3 HMM-Based Speech Recognition
</sectionHeader>
<bodyText confidence="0.999874733333333">
The most well known and well performing ap-
proach for speech recognition are Hidden Markov
Models (HMM). An HMM can be classified on the
basis of the type of its observation distributions,
the structure in its transition matrix and the number
of states.
The observation distributions of HMMs can be
either discrete, or continuous. In discrete HMMs,
distributions are defined on finite spaces while in
continuous HMMs, distributions are defined as
probability densities on continuous observation
spaces, usually as a mixture of several Gaussian
distributions.
The model topology that is generally adopted for
speech recognition is a left-to-right or Bakis model
</bodyText>
<page confidence="0.998358">
34
</page>
<bodyText confidence="0.996453">
because the speech signal varies in time from left
to right (Deller, Proakis and Hansen 1993).
An HMM is flexible in its size, type, or architec-
ture to model words as well as any sub-word unit.
</bodyText>
<subsectionHeader confidence="0.99939">
3.1 Sub-word Units of Speech Recognition
</subsectionHeader>
<bodyText confidence="0.999966876712329">
Large Vocabulary Automatic Speech Recognition
Systems (LVASRSs) require modeling of speech
in smaller units than words because the acoustic
samples of most words will never be seen during
training, and therefore, can not be trained.
Moreover, in LVASRSs there are thousands of
words and most of them occur very rarely, con-
sequently training of models for whole words is
generally impractical. That is why LVASRSs re-
quire a segmentation of each word in the vocabu-
lary into sub-word units that occur more frequently
and can be trained more robustly than words. Us-
ing sub-word based models enables us to deal with
words which have not been seen during training
since they can just be decomposed into the sub-
word units. As a word can be decomposed in sub-
word units of different granularities, there is a need
to choose the most suitable sub-word unit that fits
the purpose of the system.
Lee et al. (1992) pointed out that there are two
alternatives for choosing the fundamental sub-
word units, namely acoustically-based and linguist-
ically-based units . The acoustic units are the labels
assigned to acoustic segment models, which are
defined on the basis of procuring a set of segment
models that spans the acoustic space determined by
the given, unlabeled training data. The linguistic-
ally-based units include the linguistic units, e.g.
phones, demi-syllables, syllables and morphemes.
It should be clear that there is no ideal (perfect)
set of sub-word units. Although phones are very
small in number and relatively easy to train, they
are much more sensitive to contextual influences
than larger units. The use of triphones, which mod-
el both the right and left context of a phone, has
become the dominant solution to the problem of
the context sensitivity of phones.
Triphones are also relatively inefficient sub-
word units due to their large number. Moreover,
since a triphone unit spans a short time-interval, it
is not suitable for the integration of spectral and
temporal dependencies.
An other alternative is the syllable. Syllables are
longer and less context sensitive than phones and
capable of exploiting both the spectral and tempor-
al characteristics of continuous speech
(Ganapathiraju et al. 1997). Moreover, the syllable
has a close connection to articulation, integrates
some co-articulation phenomena, and has the po-
tential for a relatively compact representation of
conversational speech.
Therefore, different attempts have been made to
use syllables as a unit of recognition for the devel-
opment of ASR. To mention a few: Ganapathiraju
et al. (1997) have explored techniques to accentu-
ate the strengths of syllable-based modeling with a
primary interest of integrating finite-duration mod-
eling and monosyllabic word modeling. Wu et al.
(1998) tried to extract the features of speech over
the syllabic duration (250ms), considering syllable-
length interval to be 100-250ms. Hu et al. (1996)
used a pronunciation dictionary of syllable-like
units that are created from sequences of phones for
which the boundary is difficult to detect. Kanok-
phara (2003) used syllable-structure-based tri-
phones as speech recognition units for Thai.
However, syllables are too many in a number of
languages, such as English, to be trained properly.
Thus ASR researchers in languages like English
are led to choose phones where as for Amharic it
seems promising to consider syllables as an altern-
ative, because Amharic has only 233 distinct CV
syllables.
</bodyText>
<sectionHeader confidence="0.98461" genericHeader="method">
4 Syllable-Based Speech Recognition for
Amharic
</sectionHeader>
<bodyText confidence="0.99998425">
In the development of syllable-based LVASRSs
for Amharic we need to deal with a language mod-
el, pronunciation dictionary, initialization and
training of the HMM models, and identification of
the proper HMM topologies that can be properly
trained with the available data. This section
presents the development and the performance of
syllable based speech recognizers.
</bodyText>
<subsectionHeader confidence="0.994453">
4.1 The Language Model
</subsectionHeader>
<bodyText confidence="0.999987875">
One of the required elements in the development of
LVASRSs is the language model. As there is no
usable language model for Amharic, we have
trained bigram language models using the HTK
statistical language model development modules.
Due to the inflectional and derivativational mor-
phological feature of Amharic our language mod-
els have relatively high perplexities.
</bodyText>
<page confidence="0.995573">
35
</page>
<subsectionHeader confidence="0.910639">
4.2 The Pronunciation Dictionary
</subsectionHeader>
<bodyText confidence="0.999617148148148">
The development of a large vocabulary speaker in-
dependent recognition system requires the availab-
ility of an appropriate pronunciation dictionary. It
specifies the finite set of words that may be output
by the speech recognizer and gives, at least, one
pronunciation for each. A pronunciation dictionary
can be classified as a canonical or alternative on
the basis of the pronunciations it includes.
A canonical pronunciation dictionary includes
only the standard phone (or other sub-word) se-
quence assumed to be pronounced in read speech.
It does not consider pronunciation variations such
as speaker variability, dialect, or co-articulation in
conversational speech. On the other hand, an al-
ternative pronunciation dictionary uses the actual
phone (or other sub-word) sequences pronounced
in speech. In an alternative pronunciation diction-
ary, various pronunciation variations can be in-
cluded (Fukada et al. 1999).
We have used the pronunciation dictionary that
has been developed by Solomon et al. (2005). They
have developed a canonical and an alternative pro-
nunciation dictionaries. Their canonical dictionary
transcribes 50,000 words and the alternative one
transcribes 25,000 words in terms of CV syllables.
Both these pronunciation dictionaries do not
handle the difference between geminated and non-
geminated consonants; the variation of the pronun-
ciation of the sixth order grapheme, with or
without vowel; and the absence or presence of the
glottal stop consonant. Gemination of Amharic
consonants range from a slight lengthening to
much more than doubling. In the dictionary,
however, they are represented with the same tran-
scription symbols.
The sixth order grapheme may be realized with
or without vowel but the pronunciation dictionaries
do not indicate this difference. For example, the
dictionaries used the same symbol for the syllable
[rI] in the word [dЗamarInI] &apos;we started&apos;, whose
vowel part may not be realized, and in the word
[barIzo] &apos;he diluted with water&apos; that is always real-
ized with its vowel sound. That forces a syllable
model to capture two different sounds: a sound of a
consonant followed by a vowel, and a sound of the
consonant only. A similar problem occurs with the
glottal stop consonant [?] which may be uttered or
not.
A sample of pronunciations in the canonical and
alternative pronunciation dictionaries is given in
Table 43. The alternative pronunciation dictionary
contains up to 25 pronunciation variants per word
form. Table 5 illustrates some cases of the vari-
ation.
</bodyText>
<table confidence="0.9993867">
Words Canonical Pro- Alternative Pronun-
nunciation ciation
CAmA CA mA sp CA mA sp
Ca mA sp
Hitey- Hi te yo Pe yA Hi te yo Pe yA sp
oPeyA sp
Hi te yo Pi yA sp
Hi to Pe yA sp
te yo Pe yA sp
to Pe yA sp
</table>
<tableCaption confidence="0.973297">
Table 4: Canonical and Alternative Pronunciation
</tableCaption>
<table confidence="0.989237333333334">
Words Number of pronun-
ciation variants
HiteyoPeyAweyAne 25
HiheHadEge 16
yaHiteyoPeyAne 7
miniseteru 7
yaganezabe 6
HegeziHabehEre 6
yehenene 5
</table>
<tableCaption confidence="0.999114">
Table 5: Number of Pronunciation variants
</tableCaption>
<bodyText confidence="0.998845333333333">
Although it does not handle gemination and pro-
nunciation variabilities, the canonical pronunci-
ation dictionary contains all 233 distinct CV syl-
lables of Amharic, which is 100% syllable cover-
age.
Pronunciation dictionaries of development and
evaluation test sets have been extracted from the
canonical pronunciation dictionary. These test dic-
tionaries have 5,000 and 20,000 words each.
</bodyText>
<subsectionHeader confidence="0.994397">
4.3 The Acoustic Model
</subsectionHeader>
<bodyText confidence="0.999962454545455">
For training and evaluation of our recognizers, we
have used the Amharic read speech corpus that has
been developed by Solomon et al. (2005).
The speech corpus consists of a training set, a
speaker adaptation set, development test sets (for
5,000 and 20,000 vocabularies), and evaluation test
sets (for 5,000 and 20,000 vocabularies). It is a
medium size speech corpus of 20 hours of training
speech that has been read by 100 training speakers
who read a total of 10850 different sentences.
Eighty of the training speakers are from the Addis
</bodyText>
<footnote confidence="0.806078">
3In tables 4 and 5, we used our own transcription
</footnote>
<page confidence="0.996571">
36
</page>
<bodyText confidence="0.999921747368421">
Ababa dialect while the other twenty are from the
other four dialects.
Test and speaker adaptation sets were read by
twenty other speakers of the Addis Ababa dialect
and four speakers of the other four dialects. Each
speaker read 18 different sentences for the 5,000
vocabulary (development and evaluation sets each)
and 20 different sentences for the 20,000 vocabu-
lary (development and evaluation sets each) test
sets. For the adaptation set all of these readers read
53 adaptation sentences that consist of all Amharic
CV syllables.
Initialization: Training HMM models starts
with initialization. Initialization of the model for a
set of sub-word HMMs prior to re-estimation can
be achieved in two different ways: bootstrapping
and flat start. The latter implies that during the first
cycle of embedded re-estimation, each training ut-
terance will be uniformly segmented. The hope of
using such a procedure is that in the second and
subsequent iterations, the models align as intended.
We have initialized HMMs with both methods
and trained them in the same way. The HMMs that
have been initialized with the flat start method per-
formed better (40% word recognition accuracy) on
development test set of 5,000 words.
The problem with the bootstrapping approach is
that any error of the labeler strongly affects the
performance of the resulting model because con-
secutive training steps are influenced by the initial
value of the model. As a result, we did not benefit
from the use of the segmented speech, which has
been transcribed with a speech recognizer that has
low word recognition accuracy, and edited by non-
linguist listeners. We have, therefore, continued
our subsequent experiments with the flat start ini-
tialization method.
Training: We have used the Baum-Welch re-es-
timation procedure for the training. In training sub-
word HMMs that are initialized using the flat-start
procedure, this re-estimation procedure uses the
parameters of continuously spoken utterances as an
input source. A transcription, in terms of sub-word
units, is also needed for each input utterance. Us-
ing the speech parameters and their transcription,
the complete set of sub-word HMMs are re-estim-
ated simultaneously. Then all of the sub-word
HMMs corresponding to the sub-word list are
joined together to make a single composite HMM.
It is important to emphasize that in this process the
transcriptions are only needed to identify the se-
quence of sub-word units in each utterance. No
boundary information is required (Young et al.
2002).
The major problem with HMM training is that it
requires a great amount of speech data. To over-
come the problem of training with insufficient
speech data, a variety of sharing mechanisms can
be implemented. For example, HMM parameters
are tied together so that the training data is pooled
and more robust estimates result. It is also possible
to restrict the model to a variance vector for the de-
scription of output probabilities, instead of a full
covariance matrix. Rabiner and Juang(1993) poin-
ted out that for the continuous HMM models, it is
preferable to use diagonal covariance matrices with
several mixtures, rather than fewer mixtures with
full covariance matrices to perform reliable re-es-
timation of the components of the model from lim-
ited training data. The diagonal covariance
matrices have been used in our work.
IIMM Topologies: To our knowledge, there is
no topology of HMM model that can be taken as a
rule of thumb for modeling syllable HMMs, espe-
cially, for Amharic CV syllables. To have a good
HMM model for Amharic CV syllables, one needs
to conduct experiments to select the optimal model
topology. Designing an HMM topology has to be
done with proper consideration of the size of the
unit of recognition and the amount of the training
speech data. This is because as the size of the re-
cognition unit increases and the size of the model
(in terms of the number of parameters to be re-es-
timated) grows, the model requires more training
data.
We, therefore, carried out a series of experi-
ments using a left-to-right HMM with and without
jumps and skips, with a different number of emit-
ting states (3, 5, 6, 7, 8, 9, 10 and 11) and different
number of Gaussian mixtures (from 2 to 98). By
jump we mean skips from the first non-emitting
state to the middle state and/or from the middle
state to the last non-emitting state. Figure 1 shows
a left-to-right HMM of 5 emitting states with
jumps and skips.
</bodyText>
<figureCaption confidence="0.996261">
Figure 1: An example of HMM topologies
</figureCaption>
<page confidence="0.998237">
37
</page>
<bodyText confidence="0.999984034482759">
We have assumed that the problem of gemina-
tion may be compensated by the looping state
transitions of the HMM. Accordingly, CV syllables
containing geminated consonants should have a
higher loop probability than those with the non-
geminated consonants.
To develop a solution for the problem of the ir-
regularities in the realization of the sixth order
vowel [I] and the glottal stop consonant [?], HMM
topologies with jumps have been used.
We conducted an experiment using HMMs with
a jump from the middle state to the last (non-emit-
ting) state for all of the CV syllables with the sixth
order vowel, and a jump from the first emitting
state to the middle state for all of the CV syllables
with the glottal stop consonant. The CV syllable
with the glottal stop consonant and the 6th order
vowel have both jumps. These topologies have
been chosen so that the models recognize the ab-
sence of the vowel and the glottal stop consonant
of CV syllables. This assumption was confirmed
by the observation that the trained models favor
such a jump. A model, which has 5 emitting states,
of the glottal stop consonant with the sixth order
vowel tends to start emitting with the 3rd emitting
state with a probability of 0.72. The model also has
accumulated a considerable probability (0.38) to
jump from the 3rd emitting state to the last (non-
emitting) state.
A similar model of this consonant with the other
vowels (our example is the 5th order vowel) tend to
start emitting with the 3rd emitting state with a
probability of 0.68. This is two times the probabil-
ity (0.32) of its transition from the starting (non-
emitting state) to the 1st emitting state.
The models of the other consonants with the
sixth order vowel, which are exemplified by the
model of the syllable [jI], tend to jump from the 3rd
emitting state to the last (non-emitting) state with a
probability of 0.39, which is considerably greater
than that of continuing with the next state (0.09).
Since the amount of available training speech is
not enough to train transition probabilities for skip-
ping two or more states, the number of states to be
skipped have been limited to one.
To determine the optimal number of Gaussian
mixtures for the syllable models, we have conduc-
ted a series of experiments by adding two Gaussian
mixtures for all the models until the performance
of the model starts to degrade. Considering the dif-
ference in the frequency of the CV syllables, a hy-
brid number of Gaussian mixtures has been tried.
By hybrid, we mean that Gaussian mixtures are as-
signed to different syllables based on their fre-
quency. For example: the frequent syllables, like
[nI], are assigned up to fifty-eight while rare syl-
lables, like [p`i], are assigned not more than two
Gaussian mixtures.
</bodyText>
<subsectionHeader confidence="0.999423">
4.4 Performance of the Recognizers
</subsectionHeader>
<bodyText confidence="0.8897965">
We present recognition results of only those recog-
nizers which have competitive performance to the
best performing models. For example: the perform-
ance of the model with 11 emitting states with
skips and hybrid Gaussian mixtures is more com-
petitive than those with 7, 8, 9, and 10 emitting
states. We have also systematically left out test res-
ults which are worse than those presented in Table
6. Table 64 shows evaluation results made on the
5k development test set.
</bodyText>
<table confidence="0.999707555555555">
States Transition Mix. Models
Topolo-
gies
AM AM + AM +
LM LM +
SA
3 No skip 18 62.85 88.82
and jump
Hy 60.87 87.63 88.50
skip 12 69.20
jump 12 43.74 79.94
5 No skip 12 69.29 88.99 89.80
and jump
Hy 60.04
skip 12 85.77
jump 12 54.53 84.60
11 skip 12 55.04
Hy 71.83 89.21 89.04
</table>
<tableCaption confidence="0.8512235">
Table 6: Recognition Performance on 5k Develop-
ment test set
</tableCaption>
<bodyText confidence="0.999837">
From Table 6, we can see that the models with
five emitting states, with twelve Gaussian mix-
tures, without skips and jumps has the best
(89.80%) word recognition accuracy. It has
87.69% word recognition accuracy on the 20k de-
velopment test set.
Since the most commonly used number of
HMM states for phone-based speech recognizers is
three emitting states, one may expect a model of
six emitting states to be the best for an HMM of
</bodyText>
<footnote confidence="0.939775">
4In tables 6 and 7, States refers to the number of emitting
states; Mix refers to the number of Gaussian mixtures per
state; Hy refers to hybrid; AM refers to acoustic model; LM
refers to language model; and SA refers to speaker adaptation.
</footnote>
<page confidence="0.998852">
38
</page>
<bodyText confidence="0.999967628571429">
concatenated consonant and vowel. But the result
of our experiment shows that a CV syllable-based
recognizer with only five emitting states performed
better than all the other recognizers.
As we can see from Table 6, models with three
emitting states do have a competitive performance
with 18 and hybrid Gaussian mixtures. They have
the least number of states of all our models. Never-
theless, they require more storage space (33MB
with 18 Gaussian mixtures and 34MB with hybrid
Gaussian mixtures) than the best performing mod-
els (32MB). Models with three emitting states also
have larger number of total Gaussian mixtures5
(30,401 with 18 Gaussian mixtures and 31,384
with hybrid Gaussian mixtures) than the best per-
forming models (13,626 Gaussian mixtures).
The other model topology that is competitive in
word recognition performance is the model with
eleven emitting states, with skip and hybrid Gaus-
sian mixtures, which has a word recognition accur-
acy of 89.21%. It requires the biggest memory
space (40MB) and uses the largest number of total
Gaussian mixtures (36,619) of all the models we
have developed.
We have evaluated the top two models with re-
gard to their word recognition accuracy on the
evaluation test sets. Their performance is presented
in Table 7. As it can be seen from the table, the
models with the better performance on the devel-
opment test sets also showed better results with the
evaluation test sets. We can, therefore, say that the
model with five emitting states without skips and
twelve Gaussian mixtures is preferable not only
with regard to its word recognition accuracy, but
also with regard to its memory requirements.
</bodyText>
<table confidence="0.996536">
Sta Mix. Models
tes
AM + LM AM + LM + SA
5k 20k 5k 20k
5 12 90.43 87.26
11 Hy 89.36 87.13
</table>
<tableCaption confidence="0.853479">
Table 7: Recognition Performance on 5k and 20k
Evaluation test sets
</tableCaption>
<bodyText confidence="0.98962675">
For a comparison purpose, we have developed a
baseline word-internal triphone-based recognizer
using the same corpus. The models of 3 emitting
states, 12 Gaussian mixtures, with skips have the
</bodyText>
<footnote confidence="0.496938">
5We counted the Gaussian mixtures that are physically
saved, instead of what should actually be.
</footnote>
<bodyText confidence="0.999958176470588">
best word recognition accuracy (91.31%) of all the
other triphone-based recognizers that we have de-
veloped. This recognizer also has better word re-
cognition accuracy than that of our syllable-based
recognizer (90.43%). But tying is applied only for
the triphone-based recognizers.
However the triphone-based recognizer requires
much more storage space (38MB) than the syl-
lable-based recognizer that requires only 15MB
space. With regard to their speed of processing, the
syllable-based model was 37% faster than tri-
phone-based one.
These are encouraging results as compared to
the performance reported by Afify et al. (2005) for
Arabic speech recognition (14.2% word error rate).
They have used a trigram language model with a
lexicon of 60k vocabulary.
</bodyText>
<subsectionHeader confidence="0.582782">
4.5 Conclusions and Research Areas in the
Future
</subsectionHeader>
<bodyText confidence="0.999897857142857">
We conclude that the use of CV syllables is a
promising alternative in the development of
ASRSs for Amharic. Although there are still pos-
sibilities of performance improvement, we have
got an encouraging word recognition accuracy
(90.43%). Some of the possibilities of performance
improvement are:
•The pronunciation dictionary that we have used
does not handle the problem of gemination of
consonants and the irregular realization of the
sixth order vowel and the glottal stop consonant,
which has a direct effect on the quality of the
sub-word transcriptions. Proper editing (use of
phonetic transcription) of the pronunciation dic-
tionaries which, however, requires a consider-
able amount of work, certainly will result in a
higher quality of sub-word transcription and
consequently in the improvement of the recog-
nizers&apos; performance. By switching from the
grapheme-based recognizer to phonetic-based
recognizer in Arabic, Afif et al. (2005) gained
relative word error rate reduction of 10% to
14%.
•Since tying is one way of minimizing the prob-
lem of shortage of training speech, tying the syl-
lable-based models would possibly result in a
gain of some degree of performance improve-
ment.
</bodyText>
<page confidence="0.999127">
39
</page>
<sectionHeader confidence="0.998096" genericHeader="references">
5 References
</sectionHeader>
<reference confidence="0.999932875">
Afif, Mohamed, Long Nguyen, Bing Xiang, Sherif Ab-
dou, and John Makhoul. 2005. Recent progress in Ar-
abic broadcast news transcription at BBN. In INTER-
SPEECH-2005, 1637-1640
Baye Yimam and TEAM 503 students. 1997. &amp;quot;ፊ9ል
እን9ገና&amp;quot; Ethiopian Journal of Languages and Literat-
ure 7(1997): 1-32.
Baye Yimam. 1986. &amp;quot;Yአማርኛ ሰዋሰው&amp;quot;. Addis Ababa.
ት.መ.ማ.ማ.ድ.
Bender, L.M. and Ferguson C. 1976. The Ethiopian
Writing System. In Language in Ethiopia. Edited by
M.L. Bender, J.D. Bowen, R.L. Cooper, and C.A.
Ferguson. London: Oxford University press.
Cowley, Roger, Marvin L. Bender and Charles A. Fer-
gusone. 1976. The Amharic Language-Description.
In Language in Ethiopia. Edited by M.L. Bender,
J.D. Bowen, R.L. Cooper, and C.A. Ferguson. Lon-
don: Oxford University press.
Deller, J.R. Jr., Hansen, J.H.L. and Proakis, J.G., Dis-
crete-time Processing of Speech Signals. Macmillan
Publishing Company, New York, 2000.
Fukada, Toshiaki, Takayoshi Yoshimura and Yoshinori
Sagisa. 1999. Automatic generation of multiple pro-
nunciations based on neural networks. Speech Com-
munication 27:63—73
http://citeseer.ist.psu.edu/fukada99automatic.html.
Ganapathiraju, Aravind; Jonathan Hamaker; Mark Or-
dowski; and George R. Doddington. 1997. Joseph Pi-
cone. Syllable-based Large Vocabulary Continuous
Speech Recognition.
Getachew Haile. 1967. The Problems of the Amharic
Writing System. A paper presented in advance for the
interdisciplinary seminar of the Faculty of Arts and
Education. HSIU.
Hayward, Katrina and Richard J. Hayward. 1999. Am-
haric. In Handbook of the International Phonetic As-
sociation: A guide to the use of the International
Phonetic Alphabet. Cambridge: the University Press.
Hu, Zhihong; Johan Schalkwyk; Etienne Barnard; and
Ronald Cole. 1996. Speech recognition using syllable
like units. Proc. Int&apos;l Conf. on Spoken Language Pro-
cessing (ICSLP), 2:426-429.
Kanokphara, Supphanat; Virongrong Tesprasit and
Rachod Thongprasirt. 2003. Pronunciation Variation
Speech Recognition Without Dictionary Modifica-
tion on Sparse Database, IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP 2003, Hong Kong).
Kinfe Tadesse. 2002. Sub-word Based Amharic Word
Recognition: An Experiment Using Hidden Markov
Model (HMM), M.Sc Thesis. Addis Ababa Uni-
versity Faculty of Informatics. Addis Ababa.
Lee, C-H., Gauvain, J-L., Pieraccini, R. and Rabiner, L.
R.. 1992. Large vocabulary speech recognition using
subword units. Proc. ICSST-92, Brisbane, Australia,
pp. 342-353.
Leslau, W. 2000. Introductory Grammar of Amharic,
Wiesbaden: Harrassowitz.
Martha Yifiru. 2003. Application of Amharic speech re-
cognition system to command and control computer:
An experiment with Microsoft Word, M.Sc Thesis.
Addis Ababa University Faculty of Informatics. Ad-
dis Ababa.
Rabiner, L. and Juang, B. 1993. Fundamentals of speech
recognition. Englewood Cliffs, NJ.
Hussien Seid and Björn. Gambäck 2005. A Speaker In-
dependent Continuous Speech Recognizer for Am-
haric. In: INTERSPEECH 2005, 9th European Con-
ference on Speech Communication and Technology.
Lisbon, September 4-9.
Solomon Birihanu. 2001. Isolated Amharic Consonant-
Vowel (CV) Syllable Recognition, M.Sc Thesis. Ad-
dis Ababa University Faculty of Informatics. Addis
Ababa.
Solomon Teferra Abate. 2006. Automatic Speech Re-
cognition for Amharic. Ph.D. Thesis. University of
Hamburg. Hamburg.
Solomon Teferra Abate, Wolfgang Menzel and Bairu
Tafla. 2005. An Amharic Speech Corpus for Large
Vocabulary Continuous Speech Recognition. In: IN-
TERSPEECH 2005, 9th European Conference on
Speech Communication and Technology. Lisbon,
September 4-9.
Tadesse Beyene. 1994. The Ethiopian Writing System.
Paper presented at the 12th International Conference
of Ethiopian Studies, Michigan State University.
Wu, Su-Lin. 1998. Incorporating Information from Syl-
lable-length Time Scales into Automatic Speech Re-
cognition. PhD thesis, University of California,
Berkeley, CA.
Young, Steve; Dan Kershaw; Julian Odell and Dave Ol-
lason. 2002. The HTK Book.
Zegaye Seyifu. 2003. Large vocabulary, speaker inde-
pendent, continuous Amharic speech recognition,
M.Sc Thesis. Addis Ababa University Faculty of In-
formatics. Addis Ababa.
</reference>
<page confidence="0.998639">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.588213">
<title confidence="0.999207">Syllable-Based Speech Recognition for Amharic</title>
<author confidence="0.961295">Solomon Teferra Abate Wolfgang Menzel</author>
<email confidence="0.77486">solomon_teferra_7@yahoo.commenzel@informatik.uni-hamburg.de</email>
<affiliation confidence="0.852461">Uniformity of Hamburg, Department of Informatik Natural Language Systems</affiliation>
<address confidence="0.879405">Vogt-Kölln-Strasse. 30, D-22527 Hamburg, Germany</address>
<abstract confidence="0.990243090909091">Amharic is the Semitic language that has the second large number of speakers after Arabic (Hayward and Richard 1999). Its writing system is syllabic with Consonant-Vowel (CV) syllable structure. Amharic orthography has more or less a one to one correspondence with syllabic sounds. We have used this feature of Amharic to develop a CV syllable-based speech recognizer, using Hidden Markov Modeling (HMM), and achieved 90.43% word recognition accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohamed Afif</author>
<author>Long Nguyen</author>
<author>Bing Xiang</author>
<author>Sherif Abdou</author>
<author>John Makhoul</author>
</authors>
<title>Recent progress in Arabic broadcast news transcription at BBN. In</title>
<date>2005</date>
<booktitle>INTERSPEECH-2005,</booktitle>
<pages>1637--1640</pages>
<marker>Afif, Nguyen, Xiang, Abdou, Makhoul, 2005</marker>
<rawString>Afif, Mohamed, Long Nguyen, Bing Xiang, Sherif Abdou, and John Makhoul. 2005. Recent progress in Arabic broadcast news transcription at BBN. In INTERSPEECH-2005, 1637-1640</rawString>
</citation>
<citation valid="true">
<title>Baye Yimam and TEAM 503 students.</title>
<date>1997</date>
<journal>Ethiopian Journal of Languages and Literature</journal>
<volume>7</volume>
<issue>1997</issue>
<pages>1--32</pages>
<contexts>
<context position="4231" citStr="(1997)" startWordPosition="678" endWordPosition="678">texts if s/he can speak Amharic and has knowledge of the Amharic alphabet. Unlike most known languages, no one needs to learn how to spell Amharic words. In support of the above point, Leslaw (1995) noted that no real problems exist in Amharic orthography, as there is more or less, a one-to-one correspondence between the sounds and the graphic symbols, except for the gemination of consonants and some redundant symbols. Many (Bender 1976; Cowley 1976; Baye 1986) have claimed the Amharic orthography as a syllabary for a relatively long period of time. Recently, however, Taddesse (1994) and Baye (1997), who apparently modified his view, have argued it is not. Both of these arguments are based on the special feature of the orthography; the possibility of representing speech using either isolated phoneme symbols or concatenated symbols. In the concatenated feature, commonly known to most of the population, each orthographic symbol represents a consonant and a vowel, except for the sixth order2, which is sometimes realized as a consonant without a vowel and at other times a consonant with a vowel. This representation of concatenated speech sounds by a single symbol has been the basis for the c</context>
<context position="9769" citStr="(1997)" startWordPosition="1598" endWordPosition="1598">ral and temporal dependencies. An other alternative is the syllable. Syllables are longer and less context sensitive than phones and capable of exploiting both the spectral and temporal characteristics of continuous speech (Ganapathiraju et al. 1997). Moreover, the syllable has a close connection to articulation, integrates some co-articulation phenomena, and has the potential for a relatively compact representation of conversational speech. Therefore, different attempts have been made to use syllables as a unit of recognition for the development of ASR. To mention a few: Ganapathiraju et al. (1997) have explored techniques to accentuate the strengths of syllable-based modeling with a primary interest of integrating finite-duration modeling and monosyllabic word modeling. Wu et al. (1998) tried to extract the features of speech over the syllabic duration (250ms), considering syllablelength interval to be 100-250ms. Hu et al. (1996) used a pronunciation dictionary of syllable-like units that are created from sequences of phones for which the boundary is difficult to detect. Kanokphara (2003) used syllable-structure-based triphones as speech recognition units for Thai. However, syllables a</context>
</contexts>
<marker>1997</marker>
<rawString>Baye Yimam and TEAM 503 students. 1997. &amp;quot;ፊ9ል እን9ገና&amp;quot; Ethiopian Journal of Languages and Literature 7(1997): 1-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baye Yimam</author>
</authors>
<title>Yአማርኛ ሰዋሰው&amp;quot;.</title>
<date>1986</date>
<journal>Addis Ababa. ት.መ.ማ.ማ.ድ.</journal>
<marker>Yimam, 1986</marker>
<rawString>Baye Yimam. 1986. &amp;quot;Yአማርኛ ሰዋሰው&amp;quot;. Addis Ababa. ት.መ.ማ.ማ.ድ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M Bender</author>
<author>C Ferguson</author>
</authors>
<title>The Ethiopian Writing System. In</title>
<date>1976</date>
<booktitle>Language in Ethiopia. Edited</booktitle>
<publisher>University press.</publisher>
<location>London: Oxford</location>
<marker>Bender, Ferguson, 1976</marker>
<rawString>Bender, L.M. and Ferguson C. 1976. The Ethiopian Writing System. In Language in Ethiopia. Edited by M.L. Bender, J.D. Bowen, R.L. Cooper, and C.A. Ferguson. London: Oxford University press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Cowley</author>
<author>Marvin L Bender</author>
<author>Charles A Fergusone</author>
</authors>
<title>The Amharic Language-Description.</title>
<date>1976</date>
<booktitle>In Language in Ethiopia. Edited</booktitle>
<publisher>University press.</publisher>
<location>London: Oxford</location>
<marker>Cowley, Bender, Fergusone, 1976</marker>
<rawString>Cowley, Roger, Marvin L. Bender and Charles A. Fergusone. 1976. The Amharic Language-Description. In Language in Ethiopia. Edited by M.L. Bender, J.D. Bowen, R.L. Cooper, and C.A. Ferguson. London: Oxford University press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Jr Deller</author>
<author>J H L Hansen</author>
<author>J G Proakis</author>
</authors>
<title>Discrete-time Processing of Speech Signals.</title>
<date>2000</date>
<publisher>Macmillan Publishing Company,</publisher>
<location>New York,</location>
<marker>Deller, Hansen, Proakis, 2000</marker>
<rawString>Deller, J.R. Jr., Hansen, J.H.L. and Proakis, J.G., Discrete-time Processing of Speech Signals. Macmillan Publishing Company, New York, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiaki Fukada</author>
</authors>
<title>Takayoshi Yoshimura and Yoshinori Sagisa.</title>
<date>1999</date>
<journal>Speech Communication</journal>
<volume>27</volume>
<note>http://citeseer.ist.psu.edu/fukada99automatic.html.</note>
<marker>Fukada, 1999</marker>
<rawString>Fukada, Toshiaki, Takayoshi Yoshimura and Yoshinori Sagisa. 1999. Automatic generation of multiple pronunciations based on neural networks. Speech Communication 27:63—73 http://citeseer.ist.psu.edu/fukada99automatic.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Ganapathiraju</author>
<author>Jonathan Hamaker</author>
<author>Mark Ordowski</author>
<author>George R Doddington</author>
</authors>
<title>Joseph Picone. Syllable-based Large Vocabulary Continuous Speech Recognition.</title>
<date>1997</date>
<contexts>
<context position="9413" citStr="Ganapathiraju et al. 1997" startWordPosition="1541" endWordPosition="1544">s than larger units. The use of triphones, which model both the right and left context of a phone, has become the dominant solution to the problem of the context sensitivity of phones. Triphones are also relatively inefficient subword units due to their large number. Moreover, since a triphone unit spans a short time-interval, it is not suitable for the integration of spectral and temporal dependencies. An other alternative is the syllable. Syllables are longer and less context sensitive than phones and capable of exploiting both the spectral and temporal characteristics of continuous speech (Ganapathiraju et al. 1997). Moreover, the syllable has a close connection to articulation, integrates some co-articulation phenomena, and has the potential for a relatively compact representation of conversational speech. Therefore, different attempts have been made to use syllables as a unit of recognition for the development of ASR. To mention a few: Ganapathiraju et al. (1997) have explored techniques to accentuate the strengths of syllable-based modeling with a primary interest of integrating finite-duration modeling and monosyllabic word modeling. Wu et al. (1998) tried to extract the features of speech over the s</context>
</contexts>
<marker>Ganapathiraju, Hamaker, Ordowski, Doddington, 1997</marker>
<rawString>Ganapathiraju, Aravind; Jonathan Hamaker; Mark Ordowski; and George R. Doddington. 1997. Joseph Picone. Syllable-based Large Vocabulary Continuous Speech Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Getachew Haile</author>
</authors>
<title>The Problems of the Amharic Writing System. A paper presented in advance for the interdisciplinary seminar of the Faculty of Arts and Education.</title>
<date>1967</date>
<publisher>HSIU.</publisher>
<marker>Haile, 1967</marker>
<rawString>Getachew Haile. 1967. The Problems of the Amharic Writing System. A paper presented in advance for the interdisciplinary seminar of the Faculty of Arts and Education. HSIU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrina Hayward</author>
<author>Richard J Hayward</author>
</authors>
<date>1999</date>
<booktitle>Amharic. In Handbook of the International Phonetic Association: A guide to the use of the International Phonetic Alphabet.</booktitle>
<publisher>the University Press.</publisher>
<location>Cambridge:</location>
<marker>Hayward, Hayward, 1999</marker>
<rawString>Hayward, Katrina and Richard J. Hayward. 1999. Amharic. In Handbook of the International Phonetic Association: A guide to the use of the International Phonetic Alphabet. Cambridge: the University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhihong Hu</author>
<author>Johan Schalkwyk</author>
<author>Etienne Barnard</author>
<author>Ronald Cole</author>
</authors>
<title>Speech recognition using syllable like units.</title>
<date>1996</date>
<booktitle>Proc. Int&apos;l Conf. on Spoken Language Processing (ICSLP),</booktitle>
<pages>2--426</pages>
<contexts>
<context position="10108" citStr="Hu et al. (1996)" startWordPosition="1646" endWordPosition="1649">e co-articulation phenomena, and has the potential for a relatively compact representation of conversational speech. Therefore, different attempts have been made to use syllables as a unit of recognition for the development of ASR. To mention a few: Ganapathiraju et al. (1997) have explored techniques to accentuate the strengths of syllable-based modeling with a primary interest of integrating finite-duration modeling and monosyllabic word modeling. Wu et al. (1998) tried to extract the features of speech over the syllabic duration (250ms), considering syllablelength interval to be 100-250ms. Hu et al. (1996) used a pronunciation dictionary of syllable-like units that are created from sequences of phones for which the boundary is difficult to detect. Kanokphara (2003) used syllable-structure-based triphones as speech recognition units for Thai. However, syllables are too many in a number of languages, such as English, to be trained properly. Thus ASR researchers in languages like English are led to choose phones where as for Amharic it seems promising to consider syllables as an alternative, because Amharic has only 233 distinct CV syllables. 4 Syllable-Based Speech Recognition for Amharic In the </context>
</contexts>
<marker>Hu, Schalkwyk, Barnard, Cole, 1996</marker>
<rawString>Hu, Zhihong; Johan Schalkwyk; Etienne Barnard; and Ronald Cole. 1996. Speech recognition using syllable like units. Proc. Int&apos;l Conf. on Spoken Language Processing (ICSLP), 2:426-429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Supphanat Kanokphara</author>
</authors>
<title>Virongrong Tesprasit and Rachod Thongprasirt.</title>
<date>2003</date>
<booktitle>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP</booktitle>
<location>Hong Kong).</location>
<contexts>
<context position="10270" citStr="Kanokphara (2003)" startWordPosition="1672" endWordPosition="1674"> to use syllables as a unit of recognition for the development of ASR. To mention a few: Ganapathiraju et al. (1997) have explored techniques to accentuate the strengths of syllable-based modeling with a primary interest of integrating finite-duration modeling and monosyllabic word modeling. Wu et al. (1998) tried to extract the features of speech over the syllabic duration (250ms), considering syllablelength interval to be 100-250ms. Hu et al. (1996) used a pronunciation dictionary of syllable-like units that are created from sequences of phones for which the boundary is difficult to detect. Kanokphara (2003) used syllable-structure-based triphones as speech recognition units for Thai. However, syllables are too many in a number of languages, such as English, to be trained properly. Thus ASR researchers in languages like English are led to choose phones where as for Amharic it seems promising to consider syllables as an alternative, because Amharic has only 233 distinct CV syllables. 4 Syllable-Based Speech Recognition for Amharic In the development of syllable-based LVASRSs for Amharic we need to deal with a language model, pronunciation dictionary, initialization and training of the HMM models, </context>
</contexts>
<marker>Kanokphara, 2003</marker>
<rawString>Kanokphara, Supphanat; Virongrong Tesprasit and Rachod Thongprasirt. 2003. Pronunciation Variation Speech Recognition Without Dictionary Modification on Sparse Database, IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2003, Hong Kong).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kinfe Tadesse</author>
</authors>
<title>Sub-word Based Amharic Word Recognition: An Experiment Using Hidden Markov Model (HMM), M.Sc Thesis. Addis Ababa University Faculty of Informatics.</title>
<date>2002</date>
<publisher>Addis Ababa.</publisher>
<marker>Tadesse, 2002</marker>
<rawString>Kinfe Tadesse. 2002. Sub-word Based Amharic Word Recognition: An Experiment Using Hidden Markov Model (HMM), M.Sc Thesis. Addis Ababa University Faculty of Informatics. Addis Ababa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-H Lee</author>
<author>J-L Gauvain</author>
<author>R Pieraccini</author>
<author>L R Rabiner</author>
</authors>
<title>Large vocabulary speech recognition using subword units.</title>
<date>1992</date>
<booktitle>Proc. ICSST-92,</booktitle>
<pages>342--353</pages>
<location>Brisbane, Australia,</location>
<contexts>
<context position="8107" citStr="Lee et al. (1992)" startWordPosition="1336" endWordPosition="1339">nd most of them occur very rarely, consequently training of models for whole words is generally impractical. That is why LVASRSs require a segmentation of each word in the vocabulary into sub-word units that occur more frequently and can be trained more robustly than words. Using sub-word based models enables us to deal with words which have not been seen during training since they can just be decomposed into the subword units. As a word can be decomposed in subword units of different granularities, there is a need to choose the most suitable sub-word unit that fits the purpose of the system. Lee et al. (1992) pointed out that there are two alternatives for choosing the fundamental subword units, namely acoustically-based and linguistically-based units . The acoustic units are the labels assigned to acoustic segment models, which are defined on the basis of procuring a set of segment models that spans the acoustic space determined by the given, unlabeled training data. The linguistically-based units include the linguistic units, e.g. phones, demi-syllables, syllables and morphemes. It should be clear that there is no ideal (perfect) set of sub-word units. Although phones are very small in number an</context>
</contexts>
<marker>Lee, Gauvain, Pieraccini, Rabiner, 1992</marker>
<rawString>Lee, C-H., Gauvain, J-L., Pieraccini, R. and Rabiner, L. R.. 1992. Large vocabulary speech recognition using subword units. Proc. ICSST-92, Brisbane, Australia, pp. 342-353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Leslau</author>
</authors>
<title>Introductory Grammar of Amharic,</title>
<date>2000</date>
<publisher>Harrassowitz.</publisher>
<location>Wiesbaden:</location>
<contexts>
<context position="2526" citStr="Leslau 2000" startWordPosition="376" endWordPosition="377">tandard dialect and has wide currency across all Amharic-speaking communities (Hayward and Richard 1999). As with all of the other languages, Amharic has its own characterizing phonetic, phonological and morphological properties. For example, it has a set of speech sounds that is not found in other languages. For example the following sounds are not found in English: [p`], [tS`], [s`], [t`], and [q]. Amharic also has its own inventory of speech sounds. It has thirty one consonants and seven vowels. The consonants are generally classified as stops, fricatives, nasals, liquids, and semi-vowels (Leslau 2000). Tables 1 and 2 show the classification of Amharic consonants and vowels1. Man Voic Place of Articulation of ing Art Lab Den Pal Vel Glot Stops Vs [p] [t] [tS] [k] [?] Vd [b] [d] [d3] [g] Glott [p`] [t`] [tS`] [q] Rd [kw] [gw] [qw] Fric Vs [f] [s] [S] [h] Vd [z] [3] Glott [s`] Rd [hw] Nas- Vd [m] [n] [n] als Liq Vd [l] [r] Sv Vd [w] [j] Table 1: Amharic Consonants Key: Lab = Labials; Den = Dentals; Pal = Palatals; Vel = Velars; Glot = Glottal; Vs = Voiceless; 1International Phonetic Association&apos;s (IPA) standard has been used for representation. 33 Proceedings of the 5th Workshop on Important </context>
</contexts>
<marker>Leslau, 2000</marker>
<rawString>Leslau, W. 2000. Introductory Grammar of Amharic, Wiesbaden: Harrassowitz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Yifiru</author>
</authors>
<title>Application of Amharic speech recognition system to command and control computer: An experiment with Microsoft Word, M.Sc Thesis. Addis Ababa University Faculty of Informatics.</title>
<date>2003</date>
<publisher>Addis Ababa.</publisher>
<marker>Yifiru, 2003</marker>
<rawString>Martha Yifiru. 2003. Application of Amharic speech recognition system to command and control computer: An experiment with Microsoft Word, M.Sc Thesis. Addis Ababa University Faculty of Informatics. Addis Ababa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
<author>B Juang</author>
</authors>
<title>Fundamentals of speech recognition.</title>
<date>1993</date>
<location>Englewood Cliffs, NJ.</location>
<marker>Rabiner, Juang, 1993</marker>
<rawString>Rabiner, L. and Juang, B. 1993. Fundamentals of speech recognition. Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gambäck</author>
</authors>
<title>A Speaker Independent Continuous Speech Recognizer for Amharic. In:</title>
<date>2005</date>
<booktitle>INTERSPEECH 2005, 9th European Conference on Speech Communication and Technology.</booktitle>
<pages>4--9</pages>
<location>Lisbon,</location>
<contexts>
<context position="1176" citStr="Gambäck, 2005" startWordPosition="164" endWordPosition="165">. We have used this feature of Amharic to develop a CV syllable-based speech recognizer, using Hidden Markov Modeling (HMM), and achieved 90.43% word recognition accuracy. 1 Introduction Most of the Semitic languages are technologically unfavored. Amharic is one of these languages that are looking for technological considerations of researchers and developers in the area of natural language processing (NLP). Automatic Speech Recognition (ASR) is one of the major areas of NLP that is understudied in Amharic. Only few attempts (Solomon, 2001; Kinfe, 2002; Zegaye, 2003; Martha, 2003; Hussien and Gambäck, 2005; Solomon et al., 2005; Solomon, 2006) have been made. We have developed an ASR for the language using CV syllables as recognition units. In this paper we present the development and the recognition performance of the recognizer following a brief description of the Amharic language and speech recognition technology. 2 The Amharic Language Amharic, which belongs to the Semitic language family, is the official language of Ethiopia. In this family, Amharic stands second in its number of speakers after Arabic (Hayward and Richard 1999). Amharic has five dialectical variations (Addis Ababa, Gojjam,</context>
</contexts>
<marker>Gambäck, 2005</marker>
<rawString>Hussien Seid and Björn. Gambäck 2005. A Speaker Independent Continuous Speech Recognizer for Amharic. In: INTERSPEECH 2005, 9th European Conference on Speech Communication and Technology. Lisbon, September 4-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Birihanu</author>
</authors>
<title>Isolated Amharic ConsonantVowel (CV) Syllable Recognition, M.Sc Thesis. Addis Ababa University Faculty of Informatics.</title>
<date>2001</date>
<publisher>Addis Ababa.</publisher>
<marker>Birihanu, 2001</marker>
<rawString>Solomon Birihanu. 2001. Isolated Amharic ConsonantVowel (CV) Syllable Recognition, M.Sc Thesis. Addis Ababa University Faculty of Informatics. Addis Ababa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Teferra Abate</author>
</authors>
<title>Automatic Speech Recognition for Amharic.</title>
<date>2006</date>
<tech>Ph.D. Thesis.</tech>
<institution>University of Hamburg. Hamburg.</institution>
<marker>Abate, 2006</marker>
<rawString>Solomon Teferra Abate. 2006. Automatic Speech Recognition for Amharic. Ph.D. Thesis. University of Hamburg. Hamburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Teferra Abate</author>
<author>Wolfgang Menzel</author>
<author>Bairu Tafla</author>
</authors>
<title>An Amharic Speech Corpus for Large Vocabulary Continuous Speech Recognition. In:</title>
<date>2005</date>
<booktitle>INTERSPEECH 2005, 9th European Conference on Speech Communication and Technology.</booktitle>
<pages>4--9</pages>
<location>Lisbon,</location>
<marker>Abate, Menzel, Tafla, 2005</marker>
<rawString>Solomon Teferra Abate, Wolfgang Menzel and Bairu Tafla. 2005. An Amharic Speech Corpus for Large Vocabulary Continuous Speech Recognition. In: INTERSPEECH 2005, 9th European Conference on Speech Communication and Technology. Lisbon, September 4-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadesse Beyene</author>
</authors>
<title>The Ethiopian Writing System. Paper presented at the</title>
<date>1994</date>
<booktitle>12th International Conference of Ethiopian Studies,</booktitle>
<institution>State University.</institution>
<location>Michigan</location>
<marker>Beyene, 1994</marker>
<rawString>Tadesse Beyene. 1994. The Ethiopian Writing System. Paper presented at the 12th International Conference of Ethiopian Studies, Michigan State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su-Lin Wu</author>
</authors>
<title>Incorporating Information from Syllable-length Time Scales into Automatic Speech Recognition.</title>
<date>1998</date>
<tech>PhD thesis,</tech>
<institution>University of California,</institution>
<location>Berkeley, CA.</location>
<marker>Wu, 1998</marker>
<rawString>Wu, Su-Lin. 1998. Incorporating Information from Syllable-length Time Scales into Automatic Speech Recognition. PhD thesis, University of California, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Dan Kershaw</author>
<author>Julian Odell</author>
<author>Dave Ollason</author>
</authors>
<date>2002</date>
<publisher>The HTK Book.</publisher>
<contexts>
<context position="17964" citStr="Young et al. 2002" startWordPosition="2895" endWordPosition="2898">timation procedure uses the parameters of continuously spoken utterances as an input source. A transcription, in terms of sub-word units, is also needed for each input utterance. Using the speech parameters and their transcription, the complete set of sub-word HMMs are re-estimated simultaneously. Then all of the sub-word HMMs corresponding to the sub-word list are joined together to make a single composite HMM. It is important to emphasize that in this process the transcriptions are only needed to identify the sequence of sub-word units in each utterance. No boundary information is required (Young et al. 2002). The major problem with HMM training is that it requires a great amount of speech data. To overcome the problem of training with insufficient speech data, a variety of sharing mechanisms can be implemented. For example, HMM parameters are tied together so that the training data is pooled and more robust estimates result. It is also possible to restrict the model to a variance vector for the description of output probabilities, instead of a full covariance matrix. Rabiner and Juang(1993) pointed out that for the continuous HMM models, it is preferable to use diagonal covariance matrices with s</context>
</contexts>
<marker>Young, Kershaw, Odell, Ollason, 2002</marker>
<rawString>Young, Steve; Dan Kershaw; Julian Odell and Dave Ollason. 2002. The HTK Book.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zegaye Seyifu</author>
</authors>
<title>Large vocabulary, speaker independent, continuous Amharic speech recognition, M.Sc Thesis. Addis Ababa University Faculty of Informatics.</title>
<date>2003</date>
<publisher>Addis Ababa.</publisher>
<marker>Seyifu, 2003</marker>
<rawString>Zegaye Seyifu. 2003. Large vocabulary, speaker independent, continuous Amharic speech recognition, M.Sc Thesis. Addis Ababa University Faculty of Informatics. Addis Ababa.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>