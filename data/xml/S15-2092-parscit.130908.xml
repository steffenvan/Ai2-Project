<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012634">
<title confidence="0.988457">
RoseMerry: A Baseline Message-level Sentiment Classification System
</title>
<author confidence="0.996831">
Huizhi Liang, Richard Fothergill and Timothy Baldwin
</author>
<affiliation confidence="0.995554">
The University of Melbourne
</affiliation>
<address confidence="0.865837">
VIC 3010, Melbourne
</address>
<email confidence="0.994691">
oklianghuizi@gmail.com, r.fothergill@student.unimelb.edu.au, tb@ldwin.net
</email>
<sectionHeader confidence="0.995653" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999410153846154">
In this paper, we propose a baseline message-
level sentiment classification method, as de-
veloped for SemEval-2015 Task 10, Subtask
B. This system leverages both hand-crafted
features and message-level embedding fea-
tures, and uses an SVM classifier for message-
level sentiment classification. In pre-training
the embedding features, we use one million
randomly-selected tweets. We present re-
sults over SemEval-2015 Task 10, Subtask B,
as well as the Stanford Sentiment Treebank.
Our experiments show the effectiveness of our
method over both datasets.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995325">
The rise of social media such as blogs and micro-
blogs (e.g., Twitter) has fueled interest in sentiment
analysis (Liu, 2012; Pang and Lee, 2008). One
of the most popular settings for carrying out sen-
timent analysis is at the sentence level or over in-
dividual micro-blog posts, using the simple three-
label class set of POSITIVE, NEGATIVE and NEU-
TRAL (Liu, 2012; Pang and Lee, 2008; Rosenthal et
al., 2014). Sentiment classification has been shown
to have utility in various business intelligence ap-
plications, including product marketing, identifying
new business opportunities, and managing a com-
pany’s reputation (Liu, 2012; Pang and Lee, 2008).
Learning effective features plays an impor-
tant role in building sentiment classification sys-
tems (Liu, 2012; Pang and Lee, 2008). For ex-
ample, the winning system in the SemEval-2013
message polarity classification task (Nakov et al.,
2013) was based on a rich set of hand-tuned features
such as word-sentiment association lexicon features,
word n-grams, punctuation, and emoticons, which
were combined using a simple SVM-based classi-
fier (Mohammad et al., 2013). Recently, there has
been a surge of interest in representation learning
— automatically learning word and document rep-
resentations, often in the form of continuous-valued
vectors or “embeddings” — using auto-encoders or
neural network language models (Mikolov et al.,
2013; Le and Mikolov, 2014). Of particular rel-
evance to message-level sentiment analysis, Tang
et al. (2014) proposed a deep learning approach
to learn sentiment-specific word representation fea-
tures, and Le and Mikolov (2014) proposed a neu-
ral network auto-encoder to learn message-level vec-
tors.
In this paper, we detail RoseMerry, a (strong)
baseline sentiment analysis method that combines
hand-crafted features with message-level1 embed-
dings generated by doc2vec (Le and Mikolov,
2014), using a linear-kernel SVM.
</bodyText>
<sectionHeader confidence="0.975182" genericHeader="method">
2 The Proposed Method
</sectionHeader>
<bodyText confidence="0.9990365">
The proposed method combines a set of hand-
crafted features with automatically-generated
message-level representation features. The fea-
tures are concatenated into a combined feature
representation, and fed into a linear-kernel SVM
learner using LibSVM (Chang and Lin, 2011). The
</bodyText>
<footnote confidence="0.99977875">
1Throughout the paper, we will use “message” as a generic
term to refer to both tweets and also sentences in the case of
the Stanford Sentiment Treebank. Note that the method could
potentially be applied to any granularity of document.
</footnote>
<page confidence="0.91696">
551
</page>
<note confidence="0.5515985">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 551–555,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.997572733333333">
Message-level
embedding features
SVM
learner
Unlabelled
pre-training data
doc2vec
auto-encoder
Labelled
training set
Feature
extractor
1 2 ... d d+1 d+2... ... d+N
Hand-crafted
features
</figure>
<figureCaption confidence="0.999992">
Figure 1: System architecture
</figureCaption>
<bodyText confidence="0.999668423076923">
architecture of the method is shown in Figure 1.
Our interest in sentiment analysis stems from a
desire to use it as part of a commercial text analyt-
ics system. As such, there is an overarching con-
straint associated with the system and all third-party
components must be licensed in a manner which is
compatible with commercial use. In our description
below, we point out places where we were unable to
use notable resources because of this constraint.
The message-level embeddings are pre-trained
using doc2vec over the combination of the train-
ing data and a random sample of 1M English tweets,
as detailed in Section 2.1. The hand-crafted features
are based heavily off the work of Mohammad et al.
(2013), and are detailed in Section 2.2. Finally, the
d-dimensional message-level embedding is concate-
nated with the N-dimensional hand-crafted features
to form a d + N-dimensional combined feature vec-
tor. We experiment with each of the two feature sub-
sets, in addition to the combined feature set.
One significant divergence from Mohammad et al.
(2013) is that we do not use many of the sentiment
lexicons, due to non-commercial licensing. Given
that one of the key findings in that work was that lex-
icons are one of the most reliable features, we expect
that this will have a large impact on our results.
</bodyText>
<subsectionHeader confidence="0.965412">
2.1 Message-level embeddings
</subsectionHeader>
<bodyText confidence="0.999992222222222">
The message-level embeddings are generated us-
ing doc2vec (Le and Mikolov, 2014). In this
framework, words and documents are represented in
a common d-dimensional space, using real-valued
vectors. The embeddings are learned by prediction
of each word in a given document based on the doc-
ument embedding and word embeddings of its sur-
rounding context. The document vector acts as an-
other word which captures the larger context of a
word that is missing from its immediate word con-
text.
The word and document vectors are trained using
stochastic gradient descent, based on back propaga-
tion.
After pre-training, the document vector of each
training document is used as its representation, and
test documents are fed through the pre-trained auto-
encoder to generate a message-level embedding.
</bodyText>
<subsectionHeader confidence="0.990021">
2.2 Hand-crafted features
</subsectionHeader>
<bodyText confidence="0.997868">
The hand-crafted features are largely lexical:
</bodyText>
<listItem confidence="0.918417666666667">
• word n-grams: binary features capturing the
presence or absence of word n-grams observed
in the training data, i.e. contiguous sequences
of n words (n E {1, 2,3,4}); we also included
binary features for non-contiguous 3- and 4-
grams included in the training data (n-grams
</listItem>
<page confidence="0.971945">
552
</page>
<bodyText confidence="0.841821">
with one non-final word removed)
</bodyText>
<listItem confidence="0.995012576923077">
• character n-grams: continuous features captur-
ing the proportion of contiguous character n-
grams (n E {3, 4, 5}) of each type observed in
the training data, which make up a given mes-
sage
• proportion of words in all caps: the proportion
of words which are in all caps (e.g. YAY)
• punctuation features: the proportion of tokens
which are made up of multiple exclamation
marks, question marks, or a combination of the
two (e.g. ??!)
• elongated words: the proportion of words
which have “elongated” vowels, i.e. a given
vowel repeated more than twice (e.g. coool)
• proportion of emoticons: the proportion of to-
kens which are (a) positive- and (b) negative-
polarity emoticons, as identified by Chris Potts’
scripts2
• polarity of message-final emoticon: if the last
token is a polarised emoticon, its polarity
(NEGATIVE, POSITIVE or None)
• negated words: the presence or absence of
words in “negated contexts”, where a negated
context is defined as span from a negation
word3 to a punctuation mark (matching the reg-
ular expression [,.:;!?])
</listItem>
<sectionHeader confidence="0.999062" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999811">
In this section, we will detail the experimental setup
and the results of our experiments.
</bodyText>
<subsectionHeader confidence="0.982568">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.99941">
We evaluate our method over two labelled datasets,
and also two unlabelled datasets to pre-train
doc2vec, as detailed below.
</bodyText>
<footnote confidence="0.988617">
2http://sentiment.christopherpotts.net/
tokenizing.html
3Defined based on Chris Potts’ word list: http://
sentiment.christopherpotts.net/lingstruc.
html.
</footnote>
<table confidence="0.9989774">
Training Development Test
Set Set Set
POSITIVE 3043 438 1038
NEGATIVE 1177 212 365
NEUTRAL 4082 542 987
</table>
<tableCaption confidence="0.9326245">
Table 1: The number of POSITIVE, NEGATIVE, NEUTRAL
documents in the SemEval-2015 dataset
</tableCaption>
<table confidence="0.99993275">
Training Set Test Set
POSITIVE 3606 444
NEGATIVE 3304 428
NEUTRAL 1623 226
</table>
<tableCaption confidence="0.872878">
Table 2: The number of POSITIVE, NEGATIVE and
NEUTRAL sentences in the Stanford Sentiment Treebank
dataset
</tableCaption>
<subsectionHeader confidence="0.675345">
3.1.1 Labelled Datasets
</subsectionHeader>
<bodyText confidence="0.984308529411765">
SemEval-2015 Dataset: the official SemEval-
2015 Task 10, subtask B dataset, comprised of
tweets which have been hand-labelled for sentiment
at the message-level (in terms of POSITIVE, NEGA-
TIVE and NEUTRAL sentiment). The dataset is par-
titioned into three components, as detailed in Ta-
ble 1:4 (1) training set, (2) development set, and (3)
test set.
Stanford Sentiment Treebank Dataset: a col-
lection of movie review documents from www.
rottentomatoes.com, which have been sen-
tence tokenised and annotated for sentiment at
the sentence level (Maas et al., 2011) and pre-
partitioned into training and test data, as detailed in
Table 2. Socher et al. (2013) additionally annotated
the data at the phrase and lexical levels, but we use
only the sentence-level annotations in this paper.
</bodyText>
<subsectionHeader confidence="0.893486">
3.1.2 Unlabelled Datasets
</subsectionHeader>
<bodyText confidence="0.98933975">
Twitter Dataset: a random sample of 10M En-
glish tweets from a 5.3TB Twitter dataset crawled
from 18 June to 4 Dec, 2014 using the Twitter Trend-
ing API. This is used as additional data to pre-train
the message-level embeddings for the SemEval-
2015 Dataset.
IMDB Dataset: a 100K sentence movie review
dataset from www.imdb.com, collected by Maas
</bodyText>
<footnote confidence="0.9232535">
4As the labels have not been released for the progress test
set, we omit this from the table.
</footnote>
<page confidence="0.998751">
553
</page>
<bodyText confidence="0.995235333333333">
et al. (2011). This is used as additional data to pre-
train the message-level embeddings for the Stanford
Sentiment Treebank dataset.
</bodyText>
<figure confidence="0.938219666666667">
0.62
0.60
0.56
</figure>
<subsectionHeader confidence="0.995892">
3.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9999725">
To evaluate the effectiveness of the different feature
sets, we report on results as follows:
</bodyText>
<listItem confidence="0.8536825">
• RM-manual: only hand-crafted features
• RM-doc2vec: only message-level embed-
dings
(b) Acc and per-class F1 score
</listItem>
<figureCaption confidence="0.998481">
Figure 2: The learning curve for RM-doc2vec, and the
Acc, F1pos, F1neg, and F1neu results for SemEval-2015
</figureCaption>
<figure confidence="0.988152903846154">
Acc F1-. F1_ F1_
0.8
0.7
0.6
0.5
0.4
0.3
RM-manual
RM-doc2vec
RM-all
0
EL
0.58
0.62
0.60
0.58
0.56
0.54
0.52
0.50
103 104 105 106 107
Number of Unlabelled Documents (Twitter Dataset)
(a) Learning curve
• RM-all: both hand-crafted features and
message-level embeddings
As our primary evaluation metric, we use FlpN,
which is the average F1PN for the POSITIVE (i.e.,
Flpos) and NEGATIVE classes (i.e., F1neg):
0.65�� z
F1pos + F1neg 0.61
= 0.60
0.59
0.58
F1PN
2
0.65
0.64
0.63
0.62
104 105
Number of Unlabelled Documents (IMDB Dataset)
0.8
0.6
0.4
0.2
0.0
1.0
RM-manual
RM- .2—
RM-all
Acc F1-. F1_
F1_
</figure>
<bodyText confidence="0.999408857142857">
We also report the overall classification accuracy
(Acc) across the three classes, and the F1PN score
of each class (i.e., F1pos, F1neg and F1neu).
For the message-level embeddings, we used d =
100 and a context window size of 10. We used
LibSVM with a linear-kernel and default parameter
settings.
</bodyText>
<subsectionHeader confidence="0.999121">
3.3 Experimental results
</subsectionHeader>
<bodyText confidence="0.999608333333333">
In this section, we present the results first over the
SemEval-2015 datasets, and then over the Stanford
Sentiment Treebank.
</bodyText>
<sectionHeader confidence="0.891369" genericHeader="method">
3.3.1 Results for SemEval-2015
</sectionHeader>
<bodyText confidence="0.999990857142857">
The results for the SemEval-2015 test set and
progress test set are shown in Table 3. Figure 2a
is a learning curve of RM-doc2vec, pre-trained
over varying numbers of documents. We can see
that the results plateau at 1M tweets; this is the
document collection size we used for pre-training
RM-doc2vec and RM-all in our official runs. The
overall Acc and F1 of each class for the three fea-
ture sets are shown in Figure 2b. RM-doc2vec is
marginally better than RM-manual overall, and for
the NEGATIVE class in particular. When combined,
RM-all outperforms the two component feature sets
across all classes, pointing to (weak) complementar-
ity between the two feature sets.
</bodyText>
<figure confidence="0.98807">
(a) Learning curve (b) Acc and per-class F1 score
</figure>
<figureCaption confidence="0.958603666666667">
Figure 3: The learning curve for RM-doc2vec, and the
Acc, F1pos, F1neg, and F1neu results for the Stanford Sen-
timent Treebank
</figureCaption>
<sectionHeader confidence="0.9903515" genericHeader="method">
3.3.2 Results for the Stanford Sentiment
Treebank
</sectionHeader>
<bodyText confidence="0.999989538461538">
The learning curve for RM-doc2vec over the
Stanford Sentiment Treebank with varying numbers
of unlabelled (IMDB) documents is given in Fig-
ure 3a. RM-doc2vec performed best when pre-
trained over 50K documents (plus the Stanford Sen-
timent Treebank data), and this is the model we
include in the remainder of our results over this
dataset. Figure 3b shows the Acc, in addition to
the per-class F1 over the Stanford Sentiment Tree-
bank for the three feature sets. The overall trend is
strikingly similar to that for SemEval-2015, with the
combined feature set performing marginally better
than the two component feature sets in all cases.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9998766">
In this paper, we described the method used in
our official submission to the SemEval-2015 mes-
sage polarity classification task, which combines
message-level embeddings with hand-crafted fea-
tures using a simple linear-kernel SVM. We pre-
</bodyText>
<page confidence="0.99625">
554
</page>
<table confidence="0.97824975">
Test Set Progress Test Set
Twitter Twitter LiveJournal SMS Twitter Twitter Twitter
2015 2015 Sarcasm 2014 2013 2013 2014 2014 Sarcasm
0.5118 0.4962 0.6254 0.5300 0.5233 0.6127 0.4925
</table>
<tableCaption confidence="0.999924">
Table 3: The official evaluation results for the SemEval-2015 Test and Progress Test set (F1PN)
</tableCaption>
<bodyText confidence="0.999699">
sented results over the SemEval-2015 dataset and
Stanford Sentiment Treebank, and showed that the
combined feature achieved the best results. The dif-
ference between the combined feature set and the
two component feature sets is not statistically signif-
icant (based on randomised estimation, p &gt; 0.05).
While we were not able to achieve state-of-the-art
results, we commend the proposed approach as a
strong baseline method.
</bodyText>
<sectionHeader confidence="0.996935" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998939">
This research was supported by the Australian Re-
search Council. The authors would like to acknowl-
edge the support of Pitchwise (www.pitchwi.
se), and also the coding assistance of Fei Liu.
</bodyText>
<sectionHeader confidence="0.998066" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999277490196078">
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1–
27:27.
Quoc V. Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. CoRR,
abs/1405.4053.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan &amp; Claypool Publishers.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 142–150, USA.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-
of-the-art in sentiment analysis of tweets. CoRR,
abs/1308.6242.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 312–320,
USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(12):1–135.
Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin
Stoyanov. 2014. Semeval-2014 task 9: Sentiment
analysis in Twitter. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation (SemEval
2014), pages 73–80, Ireland.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
Christopher D. Manning, Andrew Y. Ng, and Christo-
pher Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1631–
1642, USA.
Duyu Tang, Furu Wei, Bing Qin, Ting Liu, and Ming
Zhou. 2014. Coooolll: A deep learning system for
Twitter sentiment classification. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), pages 208–212, Ireland.
</reference>
<page confidence="0.998504">
555
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.663165">
<title confidence="0.999948">RoseMerry: A Baseline Message-level Sentiment Classification System</title>
<author confidence="0.992442">Huizhi Liang</author>
<author confidence="0.992442">Richard Fothergill</author>
<author confidence="0.992442">Timothy</author>
<affiliation confidence="0.951338">The University of</affiliation>
<address confidence="0.687117">VIC 3010, Melbourne</address>
<abstract confidence="0.996565714285714">In this paper, we propose a baseline messagelevel sentiment classification method, as developed for SemEval-2015 Task 10, Subtask B. This system leverages both hand-crafted features and message-level embedding features, and uses an SVM classifier for messagelevel sentiment classification. In pre-training the embedding features, we use one million randomly-selected tweets. We present results over SemEval-2015 Task 10, Subtask B, as well as the Stanford Sentiment Treebank. Our experiments show the effectiveness of our method over both datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<pages>27--27</pages>
<contexts>
<context position="3006" citStr="Chang and Lin, 2011" startWordPosition="435" endWordPosition="438">atures, and Le and Mikolov (2014) proposed a neural network auto-encoder to learn message-level vectors. In this paper, we detail RoseMerry, a (strong) baseline sentiment analysis method that combines hand-crafted features with message-level1 embeddings generated by doc2vec (Le and Mikolov, 2014), using a linear-kernel SVM. 2 The Proposed Method The proposed method combines a set of handcrafted features with automatically-generated message-level representation features. The features are concatenated into a combined feature representation, and fed into a linear-kernel SVM learner using LibSVM (Chang and Lin, 2011). The 1Throughout the paper, we will use “message” as a generic term to refer to both tweets and also sentences in the case of the Stanford Sentiment Treebank. Note that the method could potentially be applied to any granularity of document. 551 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 551–555, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Message-level embedding features SVM learner Unlabelled pre-training data doc2vec auto-encoder Labelled training set Feature extractor 1 2 ... d d+1 d+2... ... d+N Hand-cr</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1– 27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<location>CoRR, abs/1405.4053.</location>
<contexts>
<context position="2220" citStr="Mikolov, 2014" startWordPosition="324" endWordPosition="325">he winning system in the SemEval-2013 message polarity classification task (Nakov et al., 2013) was based on a rich set of hand-tuned features such as word-sentiment association lexicon features, word n-grams, punctuation, and emoticons, which were combined using a simple SVM-based classifier (Mohammad et al., 2013). Recently, there has been a surge of interest in representation learning — automatically learning word and document representations, often in the form of continuous-valued vectors or “embeddings” — using auto-encoders or neural network language models (Mikolov et al., 2013; Le and Mikolov, 2014). Of particular relevance to message-level sentiment analysis, Tang et al. (2014) proposed a deep learning approach to learn sentiment-specific word representation features, and Le and Mikolov (2014) proposed a neural network auto-encoder to learn message-level vectors. In this paper, we detail RoseMerry, a (strong) baseline sentiment analysis method that combines hand-crafted features with message-level1 embeddings generated by doc2vec (Le and Mikolov, 2014), using a linear-kernel SVM. 2 The Proposed Method The proposed method combines a set of handcrafted features with automatically-generate</context>
<context position="5066" citStr="Mikolov, 2014" startWordPosition="768" endWordPosition="769">enated with the N-dimensional hand-crafted features to form a d + N-dimensional combined feature vector. We experiment with each of the two feature subsets, in addition to the combined feature set. One significant divergence from Mohammad et al. (2013) is that we do not use many of the sentiment lexicons, due to non-commercial licensing. Given that one of the key findings in that work was that lexicons are one of the most reliable features, we expect that this will have a large impact on our results. 2.1 Message-level embeddings The message-level embeddings are generated using doc2vec (Le and Mikolov, 2014). In this framework, words and documents are represented in a common d-dimensional space, using real-valued vectors. The embeddings are learned by prediction of each word in a given document based on the document embedding and word embeddings of its surrounding context. The document vector acts as another word which captures the larger context of a word that is missing from its immediate word context. The word and document vectors are trained using stochastic gradient descent, based on back propagation. After pre-training, the document vector of each training document is used as its representa</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. CoRR, abs/1405.4053.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="938" citStr="Liu, 2012" startWordPosition="127" endWordPosition="128">thod, as developed for SemEval-2015 Task 10, Subtask B. This system leverages both hand-crafted features and message-level embedding features, and uses an SVM classifier for messagelevel sentiment classification. In pre-training the embedding features, we use one million randomly-selected tweets. We present results over SemEval-2015 Task 10, Subtask B, as well as the Stanford Sentiment Treebank. Our experiments show the effectiveness of our method over both datasets. 1 Introduction The rise of social media such as blogs and microblogs (e.g., Twitter) has fueled interest in sentiment analysis (Liu, 2012; Pang and Lee, 2008). One of the most popular settings for carrying out sentiment analysis is at the sentence level or over individual micro-blog posts, using the simple threelabel class set of POSITIVE, NEGATIVE and NEUTRAL (Liu, 2012; Pang and Lee, 2008; Rosenthal et al., 2014). Sentiment classification has been shown to have utility in various business intelligence applications, including product marketing, identifying new business opportunities, and managing a company’s reputation (Liu, 2012; Pang and Lee, 2008). Learning effective features plays an important role in building sentiment cl</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--150</pages>
<publisher>USA.</publisher>
<contexts>
<context position="8553" citStr="Maas et al., 2011" startWordPosition="1315" endWordPosition="1318">Stanford Sentiment Treebank dataset 3.1.1 Labelled Datasets SemEval-2015 Dataset: the official SemEval2015 Task 10, subtask B dataset, comprised of tweets which have been hand-labelled for sentiment at the message-level (in terms of POSITIVE, NEGATIVE and NEUTRAL sentiment). The dataset is partitioned into three components, as detailed in Table 1:4 (1) training set, (2) development set, and (3) test set. Stanford Sentiment Treebank Dataset: a collection of movie review documents from www. rottentomatoes.com, which have been sentence tokenised and annotated for sentiment at the sentence level (Maas et al., 2011) and prepartitioned into training and test data, as detailed in Table 2. Socher et al. (2013) additionally annotated the data at the phrase and lexical levels, but we use only the sentence-level annotations in this paper. 3.1.2 Unlabelled Datasets Twitter Dataset: a random sample of 10M English tweets from a 5.3TB Twitter dataset crawled from 18 June to 4 Dec, 2014 using the Twitter Trending API. This is used as additional data to pre-train the message-level embeddings for the SemEval2015 Dataset. IMDB Dataset: a 100K sentence movie review dataset from www.imdb.com, collected by Maas 4As the l</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="2197" citStr="Mikolov et al., 2013" startWordPosition="318" endWordPosition="321">nd Lee, 2008). For example, the winning system in the SemEval-2013 message polarity classification task (Nakov et al., 2013) was based on a rich set of hand-tuned features such as word-sentiment association lexicon features, word n-grams, punctuation, and emoticons, which were combined using a simple SVM-based classifier (Mohammad et al., 2013). Recently, there has been a surge of interest in representation learning — automatically learning word and document representations, often in the form of continuous-valued vectors or “embeddings” — using auto-encoders or neural network language models (Mikolov et al., 2013; Le and Mikolov, 2014). Of particular relevance to message-level sentiment analysis, Tang et al. (2014) proposed a deep learning approach to learn sentiment-specific word representation features, and Le and Mikolov (2014) proposed a neural network auto-encoder to learn message-level vectors. In this paper, we detail RoseMerry, a (strong) baseline sentiment analysis method that combines hand-crafted features with message-level1 embeddings generated by doc2vec (Le and Mikolov, 2014), using a linear-kernel SVM. 2 The Proposed Method The proposed method combines a set of handcrafted features with</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets.</title>
<date>2013</date>
<journal>CoRR,</journal>
<pages>1308--6242</pages>
<contexts>
<context position="1923" citStr="Mohammad et al., 2013" startWordPosition="278" endWordPosition="281">lligence applications, including product marketing, identifying new business opportunities, and managing a company’s reputation (Liu, 2012; Pang and Lee, 2008). Learning effective features plays an important role in building sentiment classification systems (Liu, 2012; Pang and Lee, 2008). For example, the winning system in the SemEval-2013 message polarity classification task (Nakov et al., 2013) was based on a rich set of hand-tuned features such as word-sentiment association lexicon features, word n-grams, punctuation, and emoticons, which were combined using a simple SVM-based classifier (Mohammad et al., 2013). Recently, there has been a surge of interest in representation learning — automatically learning word and document representations, often in the form of continuous-valued vectors or “embeddings” — using auto-encoders or neural network language models (Mikolov et al., 2013; Le and Mikolov, 2014). Of particular relevance to message-level sentiment analysis, Tang et al. (2014) proposed a deep learning approach to learn sentiment-specific word representation features, and Le and Mikolov (2014) proposed a neural network auto-encoder to learn message-level vectors. In this paper, we detail RoseMer</context>
<context position="4357" citStr="Mohammad et al. (2013)" startWordPosition="649" endWordPosition="652">stems from a desire to use it as part of a commercial text analytics system. As such, there is an overarching constraint associated with the system and all third-party components must be licensed in a manner which is compatible with commercial use. In our description below, we point out places where we were unable to use notable resources because of this constraint. The message-level embeddings are pre-trained using doc2vec over the combination of the training data and a random sample of 1M English tweets, as detailed in Section 2.1. The hand-crafted features are based heavily off the work of Mohammad et al. (2013), and are detailed in Section 2.2. Finally, the d-dimensional message-level embedding is concatenated with the N-dimensional hand-crafted features to form a d + N-dimensional combined feature vector. We experiment with each of the two feature subsets, in addition to the combined feature set. One significant divergence from Mohammad et al. (2013) is that we do not use many of the sentiment lexicons, due to non-commercial licensing. Given that one of the key findings in that work was that lexicons are one of the most reliable features, we expect that this will have a large impact on our results.</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets. CoRR, abs/1308.6242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<publisher>USA.</publisher>
<contexts>
<context position="1701" citStr="Nakov et al., 2013" startWordPosition="245" endWordPosition="248">og posts, using the simple threelabel class set of POSITIVE, NEGATIVE and NEUTRAL (Liu, 2012; Pang and Lee, 2008; Rosenthal et al., 2014). Sentiment classification has been shown to have utility in various business intelligence applications, including product marketing, identifying new business opportunities, and managing a company’s reputation (Liu, 2012; Pang and Lee, 2008). Learning effective features plays an important role in building sentiment classification systems (Liu, 2012; Pang and Lee, 2008). For example, the winning system in the SemEval-2013 message polarity classification task (Nakov et al., 2013) was based on a rich set of hand-tuned features such as word-sentiment association lexicon features, word n-grams, punctuation, and emoticons, which were combined using a simple SVM-based classifier (Mohammad et al., 2013). Recently, there has been a surge of interest in representation learning — automatically learning word and document representations, often in the form of continuous-valued vectors or “embeddings” — using auto-encoders or neural network language models (Mikolov et al., 2013; Le and Mikolov, 2014). Of particular relevance to message-level sentiment analysis, Tang et al. (2014)</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in Twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312–320, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<volume>2</volume>
<issue>12</issue>
<contexts>
<context position="959" citStr="Pang and Lee, 2008" startWordPosition="129" endWordPosition="132">veloped for SemEval-2015 Task 10, Subtask B. This system leverages both hand-crafted features and message-level embedding features, and uses an SVM classifier for messagelevel sentiment classification. In pre-training the embedding features, we use one million randomly-selected tweets. We present results over SemEval-2015 Task 10, Subtask B, as well as the Stanford Sentiment Treebank. Our experiments show the effectiveness of our method over both datasets. 1 Introduction The rise of social media such as blogs and microblogs (e.g., Twitter) has fueled interest in sentiment analysis (Liu, 2012; Pang and Lee, 2008). One of the most popular settings for carrying out sentiment analysis is at the sentence level or over individual micro-blog posts, using the simple threelabel class set of POSITIVE, NEGATIVE and NEUTRAL (Liu, 2012; Pang and Lee, 2008; Rosenthal et al., 2014). Sentiment classification has been shown to have utility in various business intelligence applications, including product marketing, identifying new business opportunities, and managing a company’s reputation (Liu, 2012; Pang and Lee, 2008). Learning effective features plays an important role in building sentiment classification systems </context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(12):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Alan Ritter</author>
<author>Preslav Nakov</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2014 task 9: Sentiment analysis in Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>73--80</pages>
<contexts>
<context position="1219" citStr="Rosenthal et al., 2014" startWordPosition="175" endWordPosition="178">lion randomly-selected tweets. We present results over SemEval-2015 Task 10, Subtask B, as well as the Stanford Sentiment Treebank. Our experiments show the effectiveness of our method over both datasets. 1 Introduction The rise of social media such as blogs and microblogs (e.g., Twitter) has fueled interest in sentiment analysis (Liu, 2012; Pang and Lee, 2008). One of the most popular settings for carrying out sentiment analysis is at the sentence level or over individual micro-blog posts, using the simple threelabel class set of POSITIVE, NEGATIVE and NEUTRAL (Liu, 2012; Pang and Lee, 2008; Rosenthal et al., 2014). Sentiment classification has been shown to have utility in various business intelligence applications, including product marketing, identifying new business opportunities, and managing a company’s reputation (Liu, 2012; Pang and Lee, 2008). Learning effective features plays an important role in building sentiment classification systems (Liu, 2012; Pang and Lee, 2008). For example, the winning system in the SemEval-2013 message polarity classification task (Nakov et al., 2013) was based on a rich set of hand-tuned features such as word-sentiment association lexicon features, word n-grams, pun</context>
</contexts>
<marker>Rosenthal, Ritter, Nakov, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin Stoyanov. 2014. Semeval-2014 task 9: Sentiment analysis in Twitter. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 73–80, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<location>USA.</location>
<contexts>
<context position="8646" citStr="Socher et al. (2013)" startWordPosition="1332" endWordPosition="1335">ial SemEval2015 Task 10, subtask B dataset, comprised of tweets which have been hand-labelled for sentiment at the message-level (in terms of POSITIVE, NEGATIVE and NEUTRAL sentiment). The dataset is partitioned into three components, as detailed in Table 1:4 (1) training set, (2) development set, and (3) test set. Stanford Sentiment Treebank Dataset: a collection of movie review documents from www. rottentomatoes.com, which have been sentence tokenised and annotated for sentiment at the sentence level (Maas et al., 2011) and prepartitioned into training and test data, as detailed in Table 2. Socher et al. (2013) additionally annotated the data at the phrase and lexical levels, but we use only the sentence-level annotations in this paper. 3.1.2 Unlabelled Datasets Twitter Dataset: a random sample of 10M English tweets from a 5.3TB Twitter dataset crawled from 18 June to 4 Dec, 2014 using the Twitter Trending API. This is used as additional data to pre-train the message-level embeddings for the SemEval2015 Dataset. IMDB Dataset: a 100K sentence movie review dataset from www.imdb.com, collected by Maas 4As the labels have not been released for the progress test set, we omit this from the table. 553 et a</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631– 1642, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
<author>Ming Zhou</author>
</authors>
<title>Coooolll: A deep learning system for Twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>208--212</pages>
<contexts>
<context position="2301" citStr="Tang et al. (2014)" startWordPosition="334" endWordPosition="337">akov et al., 2013) was based on a rich set of hand-tuned features such as word-sentiment association lexicon features, word n-grams, punctuation, and emoticons, which were combined using a simple SVM-based classifier (Mohammad et al., 2013). Recently, there has been a surge of interest in representation learning — automatically learning word and document representations, often in the form of continuous-valued vectors or “embeddings” — using auto-encoders or neural network language models (Mikolov et al., 2013; Le and Mikolov, 2014). Of particular relevance to message-level sentiment analysis, Tang et al. (2014) proposed a deep learning approach to learn sentiment-specific word representation features, and Le and Mikolov (2014) proposed a neural network auto-encoder to learn message-level vectors. In this paper, we detail RoseMerry, a (strong) baseline sentiment analysis method that combines hand-crafted features with message-level1 embeddings generated by doc2vec (Le and Mikolov, 2014), using a linear-kernel SVM. 2 The Proposed Method The proposed method combines a set of handcrafted features with automatically-generated message-level representation features. The features are concatenated into a com</context>
</contexts>
<marker>Tang, Wei, Qin, Liu, Zhou, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Bing Qin, Ting Liu, and Ming Zhou. 2014. Coooolll: A deep learning system for Twitter sentiment classification. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 208–212, Ireland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>