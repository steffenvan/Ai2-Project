<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.970214">
Company-Oriented Extractive Summarization of Financial News∗
</title>
<author confidence="0.773332">
Katja Filippova†, Mihai Surdeanu‡, Massimiliano Ciaramita‡, Hugo Zaragoza‡
</author>
<affiliation confidence="0.515679">
†EML Research gGmbH ‡Yahoo! Research
</affiliation>
<address confidence="0.472358">
Schloss-Wolfsbrunnenweg 33 Avinguda Diagonal 177
69118 Heidelberg, Germany 08018 Barcelona, Spain
</address>
<email confidence="0.44679">
filippova@eml-research.de,{mihais,massi,hugoz}@yahoo-inc.com
</email>
<sectionHeader confidence="0.956992" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999935">
The paper presents a multi-document sum-
marization system which builds company-
specific summaries from a collection of fi-
nancial news such that the extracted sen-
tences contain novel and relevant infor-
mation about the corresponding organiza-
tion. The user’s familiarity with the com-
pany’s profile is assumed. The goal of
such summaries is to provide information
useful for the short-term trading of the cor-
responding company, i.e., to facilitate the
inference from news to stock price move-
ment in the next day. We introduce a
novel query (i.e., company name) expan-
sion method and a simple unsupervized al-
gorithm for sentence ranking. The sys-
tem shows promising results in compari-
son with a competitive baseline.
</bodyText>
<sectionHeader confidence="0.99517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999859583333333">
Automatic text summarization has been a field of
active research in recent years. While most meth-
ods are extractive, the implementation details dif-
fer considerably depending on the goals of a sum-
marization system. Indeed, the intended use of the
summaries may help significantly to adapt a par-
ticular summarization approach to a specific task
whereas the broadly defined goal of preserving rel-
evant, although generic, information may turn out
to be of little use.
In this paper we present a system whose goal is
to extract sentences from a collection of financial
</bodyText>
<note confidence="0.829112">
∗This work was done during the first author’s internship
at Yahoo! Research. Mihai Surdeanu is currently affiliated
with Stanford University (mihais@stanford.edu).
Massimiliano Ciaramita is currently at Google
(massi@google.com).
</note>
<bodyText confidence="0.992962294117647">
news to inform about important events concern-
ing companies, e.g., to support trading (i.e., buy or
sell) the corresponding symbol on the next day, or
managing a portfolio. For example, a company’s
announcement of surpassing its earnings’ estimate
is likely to have a positive short-term effect on its
stock price, whereas an announcement of job cuts
is likely to have the reverse effect. We demonstrate
how existing methods can be extended to achieve
precisely this goal.
In a way, the described task can be classified
as query-oriented multi-document summarization
because we are mainly interested in information
related to the company and its sector. However,
there are also important differences between the
two tasks.
• The name of the company is not a query,
e.g., as it is specified in the context of the
DUC competitions1, and requires an exten-
sion. Initially, a query consists exclusively
of the “symbol”, i.e., the abbreviation of the
name of a company as it is listed on the stock
market. For example, WPO is the abbrevia-
tion used on the stock market to refer to The
Washington Post–a large media and educa-
tion company. Such symbols are rarely en-
countered in the news and cannot be used to
find all the related information.
• The summary has to provide novel informa-
tion related to the company and should avoid
general facts about it which the user is sup-
posed to know. This point makes the task
related to update summarization where one
has to provide the user with new information
</bodyText>
<note confidence="0.7704505">
1http://duc.nist.gov; since 2008 TAC: http:
//www.nist.gov/tac.
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 246–254,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.83037">
246
</page>
<bodyText confidence="0.994346555555556">
given some background knowledge2. In our
case, general facts about the company are as-
sumed to be known by the user. Given WPO,
we want to distinguish between The Wash-
ington Post is owned by The Washington Post
Company, a diversified education and media
company and The Post recently went through
its third round of job cuts and reported an
11% decline in print advertising revenues for
its first quarter, the former being an example
of background information whereas the lat-
ter is what we would like to appear in the
summary. Thus, the similarity to the query
alone is not the decisive parameter in com-
puting sentence relevance.
• While the summaries must be specific for a
given organization, important but general fi-
nancial events that drive the overall market
must be included in the summary. For exam-
ple, the recent subprime mortgage crisis af-
fected the entire economy regardless of the
sector.
Our system proceeds in the three steps illus-
trated in Figure 1. First, the company symbol is
expanded with terms relevant for the company, ei-
ther directly – e.g., iPod is directly related to Apple
Inc. – or indirectly – i.e., using information about
the industry or sector the company operates in. We
detail our symbol expansion algorithm in Section
3. Second, this information is used to rank sen-
tences based on their relatedness to the expanded
query and their overall importance (Section 4). Fi-
nally, the most relevant sentences are re-ranked
based on the degree of novelty they carry (Section
5).
The paper makes the following contributions.
First, we present a new query expansion tech-
nique which is useful in the context of company-
dependent news summarization as it helps identify
sentences important to the company. Second, we
introduce a simple and efficient method for sen-
tence ranking which foregrounds novel informa-
tion of interest. Our system performs well in terms
of the ROUGE score (Lin &amp; Hovy, 2003) com-
pared with a competitive baseline (Section 6).
</bodyText>
<sectionHeader confidence="0.979554" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.9976945">
The data we work with is a collection of financial
news consolidated and distributed by Yahoo! Fi-
</bodyText>
<footnote confidence="0.64179">
2See the DUC 2007 and 2008 update tracks.
</footnote>
<bodyText confidence="0.999816741935484">
nance3 from various sources4. Each story is la-
beled as being relevant for a company – i.e., it
appears in the company’s RSS feed – if the story
mentions either the company itself or the sector the
company belongs to. Altogether the corpus con-
tains 88,974 news articles from a period of about
5 months (148 days). Some articles are labeled
as being relevant for several companies. The total
number of (company name, news collection) pairs
is 46,444.
The corpus is cleaned of HTML tags, embed-
ded graphics and unrelated information (e.g., ads,
frames) with a set of manually devised rules. The
filtering is not perfect but removes most of the
noise. Each article is passed through a language
processing pipeline (described in (Atserias et al.,
2008)). Sentence boundaries are identified by
means of simple heuristics. The text is tokenized
according to Penn TreeBank style and each to-
ken lemmatized using Wordnet’s morphological
functions. Part of speech tags and named entities
(LOC, PER, ORG, MISC) are identified by means
of a publicly available named-entity tagger5 (Cia-
ramita &amp; Altun, 2006, SuperSense). Apart from
that, all sentences which are shorter than 5 tokens
and contain neither nouns nor verbs are sorted out.
We apply the latter filter as we are interested in
textual information only. Numeric information
contained, e.g., in tables can be easily and more
reliably obtained from the indices tables available
online.
</bodyText>
<sectionHeader confidence="0.980324" genericHeader="method">
3 Query Expansion
</sectionHeader>
<bodyText confidence="0.9998939">
In company-oriented summarization query expan-
sion is crucial because, by default, our query con-
tains only the symbol, that is the abbreviation of
the name of the company. Unfortunately, exist-
ing query expansion techniques which utilize such
knowledge sources as WordNet or Wikipedia are
not useful for symbol expansion. WordNet does
not include organizations in any systematic way.
Wikipedia covers many companies but it is unclear
how it can be used for expansion.
</bodyText>
<footnote confidence="0.996545444444444">
3http://finance.yahoo.com
4http://biz.yahoo.com, http://www.
seekingalpha.com, http://www.marketwatch.
com, http://www.reuters.com, http://www.
fool.com, http://www.thestreet.com, http:
//online.wsj.com, http://www.forbes.com,
http://www.cnbc.com, http://us.ft.com,
http://www.minyanville.com
5http://sourceforge.net/projects/
</footnote>
<figure confidence="0.955032">
supersensetag
247
Symbol
Company
Profile
News
Query
Expansion
Relatedness
to Query
Filtering
Relevant
Sentences
Expanded
Query
Summary
Novelty
Ranking
Yahoo! Finance
</figure>
<figureCaption confidence="0.999987">
Figure 1: System architecture
</figureCaption>
<bodyText confidence="0.999902277777778">
Intuitively, a good expansion method should
provide us with a list of products, or properties,
of the company, the field it operates in, the typi-
cal customers, etc. Such information is normally
found on the profile page of a company at Yahoo!
Finance6. There, so called “business summaries”
provide succinct and financially relevant informa-
tion about the company. Thus, we use business
summaries as follows. For every company sym-
bol in our collection, we download its business
summary, split it into tokens, remove all words
but nouns and verbs which we then lemmatize.
Since words like company are fairly uninforma-
tive in the context of our task, we do not want to
include them in the expanded query. To filter out
such words, we compute the company-dependent
TF*IDF score for every word on the collection of
all business summaries:
</bodyText>
<equation confidence="0.9984065">
score(w) = tfw,c × log „cf / (1)
w/
</equation>
<bodyText confidence="0.99815653125">
where c is the business summary of a company,
tfw,c is the frequency of w in c, N is the total
number of business summaries we have, cfw is
the number of summaries that contain w. This
formula penalizes words occurring in most sum-
maries (e.g., company, produce, offer, operate,
found, headquarter, management). At the mo-
ment of running the experiments, N was about
3,000, slightly less than the total number of sym-
6http://finance.yahoo.com/q/pr?s=AAPL
where the trading symbol of any company can be used
instead of AAPL.
bols because some companies do not have a busi-
ness summary on Yahoo! Finance. It is impor-
tant to point out that companies without a business
summary are usually small and are seldom men-
tioned in news articles: for example, these compa-
nies had relevant news articles in only 5% of the
days monitored in this work.
Table 1 gives the ten high scoring words for
three companies (Apple Inc. – the computer and
software manufacture, Delta Air Lines – the air-
line, and DaVita – dyalisis services). Table 1
shows that this approach succeeds in expanding
the symbol with terms directly related to the com-
pany, e.g., ipod for Apple, but also with more gen-
eral information like the industry or the company
operates in, e.g., software and computer for Apple.
All words whose TF*IDF score is above a certain
threshold 0 are included in the expanded query (0
was tuned to a value of 5.0 on the development
set).
</bodyText>
<sectionHeader confidence="0.970867" genericHeader="method">
4 Relatedness to Query
</sectionHeader>
<bodyText confidence="0.999643454545454">
Once the expanded query is generated, it can be
used for sentence ranking. We chose the system of
Otterbacher et al. (2005) as a a starting point for
our approach and also as a competitive baseline
because it has been successfully tested in a simi-
lar setting–it has been applied to multi-document
query-focused summarization of news documents.
Given a graph G = (5, E), where 5 is the set
of all sentences from all input documents, and E is
the set of edges representing normalized sentence
similarities, Otterbacher et al. (2005) rank all sen-
</bodyText>
<figure confidence="0.988182384615385">
248
AAPL
apple
music
mac
DAL
air
flight
delta
DVA
dialysis
davita
esrd
</figure>
<bodyText confidence="0.9933045">
as in the query are stemmed and stopwords are re-
moved from them). Relevance to the query is de-
fined in Equation (6) which has been previously
used for sentence retrieval (Allan et al., 2003):
</bodyText>
<equation confidence="0.8392792">
kidney
inpatient
X
rel(s|q) _
log(tfw,s + 1) X log(tfw,q + 1) X idfw,S (6)
</equation>
<figure confidence="0.332866777777778">
software
ipod
lines
schedule
computer
peripheral
movie
player
desktop
</figure>
<tableCaption confidence="0.943078">
Table 1: Top 10 scoring words for three companies
</tableCaption>
<bodyText confidence="0.9975815">
tence nodes based on the inter-sentence relations
as well as the relevance to the query q. Sentence
ranks are found iteratively over the set of graph
nodes with the following formula:
</bodyText>
<equation confidence="0.9994265">
re(t�9)+(1—a)t�r_�3�() t)7 9) (2)
r(s, q) _ λP
</equation>
<bodyText confidence="0.999997933333333">
The first term represents the importance of a sen-
tence defined in respect to the query, whereas the
second term infers the importance of the sentence
from its relation to other sentences in the collec-
tion. A ∈ (0, 1) determines the relative importance
of the two terms and is found empirically. Another
parameter whose value is determined experimen-
tally is the sentence similarity threshold T, which
determines the inclusion of a sentence in G. Ot-
terbacher et al. (2005) report 0.2 and 0.95 to be
the optimal values for T and A respectively. These
values turned out to produce the best results also
on our development set and were used in all our
experiments. Similarity between sentences is de-
fined as the cosine of their vector representations:
</bodyText>
<equation confidence="0.99835625">
sim(s, t) _C Pw∈s∩t weight(w)&apos; (3)
qr ∈s weight(w)&apos; X qPw∈t weight(w)&apos;
weight(w) = tfw,sidfw,S (4)
idfw,S|S |+ 1 = log + sfw(5)(0.5
</equation>
<bodyText confidence="0.992203535714286">
where tfw,s is the frequency of w in sentence s,
|S |is the total number of sentences in the docu-
ments from which sentences are to be extracted,
and sfw is the number of sentences which contain
the word w (all words in the documents as well
w∈q
where tfw,x stands for the number of times w ap-
pears in x, be it a sentence (s) or the query (q). If
a sentence shares no words other than stopwords
with the query, the relevance becomes zero. Note
that without the relevance to the query part Equa-
tion 2 takes only inter-sentence similarity into ac-
count and computes the weighted PageRank (Brin
&amp; Page, 1998).
In defining the relevance to the query, in Equa-
tion (6), words which do not appear in too many
sentences in the document collection weigh more.
Indeed, if a word from the query is contained in
many sentences, it should not count much. But it
is also true that not all words from the query are
equally important. As it has been mentioned in
Section 3, words like product or offer appear in
many business summaries and are equally related
to any company. To penalize such words, when
computing the relevance to the query, we multiply
the relevance score of a given word w with the in-
verted document frequency of w on the corpus of
business summaries Q – idfw,Q:
</bodyText>
<equation confidence="0.916031666666667">
( |Q |�
idfw,Q = log (7)
qfw
</equation>
<bodyText confidence="0.99748975">
We also replace tfw,s with the indicator function
s(w) since it has been reported to be more ad-
equate for sentences, in particular for sentence
alignment (Nelken &amp; Shieber, 2006):
</bodyText>
<equation confidence="0.87375775">
�
1 if s contains w
s(w) = (8)
0 otherwise
</equation>
<bodyText confidence="0.728404">
Thus, the modified formula we use to compute
sentence ranks is as follows:
</bodyText>
<equation confidence="0.991526">
rel(s|q) _ X s(w) X log(tfw,q + 1) X idfw,S X idfw,Q (9)
w∈q
</equation>
<bodyText confidence="0.999014166666667">
We call these two ranking algorithms that use
the formula in (2) OTTERBACHER and QUERY
WEIGHTS, the difference being the way the rel-
evance to the query is computed: (6) or (9). We
use the OTTERBACHER algorithm as a baseline in
the experiments reported in Section 6.
</bodyText>
<figure confidence="0.968159272727273">
destination
passenger
cargo
atlanta
fleet
outpatient
patient
hospital
disease
service
249
</figure>
<sectionHeader confidence="0.866489" genericHeader="method">
5 Novelty Bias
</sectionHeader>
<bodyText confidence="0.907874224137932">
Apart from being related to the query, a good sum-
mary should provide the user with novel infor-
mation. According to Equation (2), if there are,
say, two sentences which are highly similar to the
query and which share some words, they are likely
to get a very high score. Experimenting with the
development set, we observed that sentences about
the company, such as e.g., DaVita, Inc. is a lead-
ing provider of kidney care in the United States,
providing dialysis services and education for pa-
tients with chronic kidney failure and end stage re-
nal disease, are ranked high although they do not
contribute new information. However, a non-zero
similarity to the query is indeed a good filter of the
information related to the company and to its sec-
tor and can be used as a prerequisite of a sentence
to be included in the summary. These observations
motivate our proposal for a ranking method which
aims at providing relevant and novel information
at the same time.
Here, we explore two alternative approaches to
add the novelty bias to the system:
• The first approach bypasses the relatedness
to query step introduced in Section 4 com-
pletely. Instead, this method merges the dis-
covery of query relatedness and novelty into
a single algorithm, which uses a sentence
graph that contains edges only between sen-
tences related to the query, (i.e., sentences for
which rel(s|q) &gt; 0). All edges connecting
sentences which are unrelated to the query
are skipped in this graph. In this way we limit
the novelty ranking process to a subset of sen-
tences related to the query.
• The second approach models the problem
in a re-ranking architecture: we take the
top ranked sentences after the relatedness-to-
query filtering component (Section 4) and re-
rank them using the novelty formula intro-
duced below.
The main difference between the two approaches
is that the former uses relatedness-to-query and
novelty information but ignores the overall impor-
tance of a sentence as given by the PageRank al-
gorithm in Section 4, while the latter combines all
these aspects –i.e., importance of sentences, relat-
edness to query, and novelty– using the re-ranking
architecture.
To amend the problem of general information
ranked inappropriately high, we modify the word-
weighting formula (4) so that it implements a nov-
elty bias, thus becoming dependent on the query.
A straightforward way to define the novelty weight
of a word would be to draw a line between the
“known” words, i.e., words appearing in the busi-
ness summary, and the rest. In this approach all
the words from the business summary are equally
related to the company and get the weight of 0:
</bodyText>
<equation confidence="0.973282">
�
0 if Q contains w
weight(w) =
tfw,sidfw,S otherwise
(10)
</equation>
<bodyText confidence="0.9999648">
We call this weighting scheme SIMPLE. As
an alternative, we also introduce a more elab-
orate weighting procedure which incorporates
the relatedness-to-query (or rather distance from
query) in the word weight formula. Intuitively, the
more related to the query a word is (e.g., DaVita,
the name of the company), the more familiar to the
user it is and the smaller its novelty contribution
is. If a word does not appear in the query at all, its
weight becomes equal to the usual tfw,sidfw,S:
</bodyText>
<equation confidence="0.917125666666667">
( × idf 1
weight(w) = 1 − tf w,q w,� /I × tfw,sidfw,S (11)
r-wi∈q tfwi,q × idfwi,Q
</equation>
<bodyText confidence="0.999215857142857">
The overall novelty ranking formula is based
on the query-dependent PageRank introduced in
Equation (2). However, since we already incorpo-
rate the relatedness to the query in these two set-
tings, we focus only on related sentences and thus
may drop the relatedness to the query part from
(2):
</bodyText>
<equation confidence="0.930679">
sim(s, t, q)
�
u∈S sim(t, u, q)
(12)
</equation>
<bodyText confidence="0.999450416666667">
We set A to the same value as in OTTERBACHER.
We deliberately set the sentence similarity thresh-
old T to a very low value (0.05) to prevent the
graph from becoming exceedingly bushy. Note
that this novelty-ranking formula can be equally
applied in both scenarios introduced at the begin-
ning of this section. In the first scenario, S stands
for the set of nodes in the graph that contains only
sentences related to the query. In the second sce-
nario, S contains the highest ranking sentences
detected by the relatedness-to-query component
(Section 4).
</bodyText>
<figure confidence="0.78862425">
�
r’(s,q) = A + (1 − A)
t∈S
250
</figure>
<subsectionHeader confidence="0.982384">
5.1 Redundancy Filter
</subsectionHeader>
<bodyText confidence="0.999880823529412">
Some sentences are repeated several times in the
collection. Such repetitions, which should be
avoided in the summary, can be filtered out ei-
ther before or after the sentence ranking. We ap-
ply a simple repetition check when incrementally
adding ranked sentences to the summary. If a sen-
tence to be added is almost identical to the one
already included in the summary, we skip it. Iden-
tity check is done by counting the percentage of
non-stop word lemmas in common between two
sentences. 95% is taken as the threshold.
We do not filter repetitions before the rank-
ing has taken place because often such repetitions
carry important and relevant information. The re-
dundancy filter is applied to all the systems de-
scribed as they are equally prone to include repe-
titions.
</bodyText>
<table confidence="0.997466666666667">
METHOD ROUGE-2
Otterbacher 0.255 (0.226 - 0.285)
Query Weights 0.289 (0.254 - 0.324)
Novelty Bias (simple) 0.315 (0.287 - 0.342)
Novelty Bias 0.302 (0.277 - 0.329)
Manual 0.472 (0.415 - 0.531)
</table>
<tableCaption confidence="0.995362">
Table 2: Results of the four extraction methods
</tableCaption>
<bodyText confidence="0.9395465">
and human annotators
jackknife for each (query, summary) pair and com-
puted a macro-average to make human and au-
tomatic results comparable (Dang, 2005). The
scores computed on summaries produced by hu-
mans are given in the bottom line (MANUAL) and
serve as upper bound and also as an indicator for
the inter-annotator agreement.
</bodyText>
<sectionHeader confidence="0.997618" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999980258064516">
We randomly selected 23 company stock names,
and constructed a document collection for each
containing all the news provided in the Yahoo! Fi-
nance news feed for that company in a period of
two days (the time period was chosen randomly).
The average length of a news collection is about
600 tokens. When selecting the company names,
we took care of not picking those which have only
a few news articles for that period of time. This
resulted into 9.4 news articles per collection on av-
erage. From each of these, three human annotators
independently selected up to ten sentences. All an-
notators had average to good understanding of the
financial domain. The annotators were asked to
choose the sentences which could best help them
decide whether to buy, sell or retain stock for the
company the following day and present them in
the order of decreasing importance. The anno-
tators compared their summaries of the first four
collections and clarified the procedure before pro-
ceeding with the other ones. These four collec-
tions were then later used as a development set.
All summaries – manually as well as automat-
ically generated – were cut to the first 250 words
which made the summaries 10 words shorter on
average. We evaluated the performance automat-
ically in terms of ROUGE-2 (Lin &amp; Hovy, 2003)
using the parameters and following the methodol-
ogy from the DUC events. The results are pre-
sented in Table 2. We also report the 95% confi-
dence intervals in brackets. As in DUC, we used
</bodyText>
<subsectionHeader confidence="0.987023">
6.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999980586206897">
From Table 2 follows that the modifications we
applied to the baseline are sensible and indeed
bring an improvement. QUERY WEIGHTS per-
forms better than OTTERBACHER and is in turn
outperformed by the algorithms biased to novel in-
formation (the two NOVELTY systems). The over-
lap between the confidence intervals of the base-
line and the simple version of the novelty algo-
rithm is minimal (0.002).
It is remarkable that the achieved improvement
is due to a more balanced relatedness to the query
ranking (9), as well as to the novelty bias re-
ranking. The fact that the simpler novelty weight-
ing formula (10) produced better results than the
more elaborated one (11) requires a deeper anal-
ysis and a larger test set to explain the difference.
Our conjecture so far is that the SIMPLE approach
allows for a better combination of both novelty
and relatedness to query. Since the more complex
novelty ranking formula penalizes terms related
to the query (Equation (11)), it favors a scenario
where novelty is boosted in detriment of related-
ness to query, which is not always realistic.
It is important to note that, compared with the
baseline, we did not do any parameter tuning for
λ and the inter-sentence similarity threshold. The
improvement between the system of Otterbacher
et al. (2005) and our best model is statistically
significant.
</bodyText>
<page confidence="0.666933">
251
</page>
<subsectionHeader confidence="0.997787">
6.2 System Combination
</subsectionHeader>
<bodyText confidence="0.99982588">
Recall from Section 5 that the motivation for pro-
moting novel information came from the fact that
sentences with background information about the
company obtained very high scores: they were re-
lated but not novel. The sentences ranked by OT-
TERBACHER or QUERY WEIGHTS required a re-
ranking to include related and novel sentences in
the summary. We checked whether novelty re-
ranking brings an improvement if added on top
of a system which does not have a novelty bias
(baseline or QUERY WEIGHTS) and compared it
with the setting where we simply limit the novelty
ranking to all the sentences related to the query
(NOVELTY SIMPLE and NOVELTY). In the simi-
larity graph, we left only edges between the first
30 sentences from the ranked list produced by
one of the two algorithms described in Section 4
(OTTERBACHER or QUERY WEIGHTS). Then we
ranked the sentences biased to novel information
the same way as described in Section 5. The re-
sults are presented in Table 3. What we evalu-
ate here is whether a combination of two methods
performs better than the simple heuristics of dis-
carding edges between sentences unrelated to the
query.
</bodyText>
<table confidence="0.9978736">
METHOD ROUGE-2
Otterbacher + Novelty simple 0.280 (0.254 - 0.306)
Otterbacher + Novelty 0.273 (0.245 - 0.301)
Query Weights + Novelty simple 0.275 (0.247 - 0.302)
Query Weights + Novelty 0.265 (0.242 - 0.289)
</table>
<tableCaption confidence="0.9222815">
Table 3: Results of the combinations of the four
methods
</tableCaption>
<bodyText confidence="0.999891357142857">
From the four possible combinations, there is
an improvement over the baseline only (0.255 vs.
0.280 resp. 0.273). None of the combinations per-
forms better than the simple novelty bias algo-
rithm on a subset of edges. This experiment sug-
gests that, at least in the scenario investigated here
(short-term monitoring of publicly-traded compa-
nies), novelty is more important than relatedness
to query. Hence, the simple novelty bias algo-
rithm, which emphasizes novelty and incorporates
relatedness to query only through a loose con-
straint (rel(slq) &gt; 0) performs better than com-
plex models, which are more constrained by the
relatedness to query.
</bodyText>
<sectionHeader confidence="0.999351" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.9999849">
Summarization has been extensively investigated
in recent years and to date there exists a multi-
tude of very different systems. Here, we review
those that come closest to ours in respect to the
task and that concern extractive multi-document
query-oriented summarization. We also mention
some work on using textual news data for stock
indices prediction which we are aware of.
Stock market prediction: W¨uthrich et al.
(1998) were among the first who introduced an au-
tomatic stock indices prediction system which re-
lies on textual information only. The system gen-
erates weighted rules each of which returns the
probability of the stock going up, down or remain-
ing steady. The only information used in the rules
is the presence or absence of certain keyphrases
provided by a human expert who “judged them
to be influential factors potentially moving stock
markets”. In this approach, training data is re-
quired to measure the usefulness of the keyphrases
for each of the three classes. More recently, Ler-
man et al. (2008) introduced a forecasting system
for prediction markets that combines news anal-
ysis with a price trend analysis model. This ap-
proach was shown to be successful for the fore-
casting of public opinion about political candi-
dates in such prediction markets. Our approach
can be seen as a complement to both these ap-
proaches, necessary especially for financial mar-
kets where the news typically cover many events,
only some related to the company of interest.
Unsupervized summarization systems extract
sentences whose relevance can be inferred from
the inter-sentence relations in the document col-
lection. In (Radev et al., 2000), the centroid of
the collection, i.e., the words with the highest
TF*IDF, is considered and the sentences which
contain more words from the centroid are ex-
tracted. Mihalcea &amp; Tarau (2004) explore sev-
eral methods developed for ranking documents
in information retrieval for the single-document
summarization task. Similarly, Erkan &amp; Radev
(2004) apply in-degree and PageRank to build a
summary from a collection of related documents.
They show that their method, called LexRank,
achieves good results. In (Otterbacher et al., 2005;
Erkan, 2006) the ranking function of LexRank is
extended to become applicable to query-focused
summarization. The rank of a sentence is deter-
mined not just by its relation to other sentences in
</bodyText>
<page confidence="0.563475">
252
</page>
<bodyText confidence="0.996630208333333">
the document collection but also by its relevance
to the query. Relevance to the query is defined as
the word-based similarity between query and sen-
tence.
Query expansion has been used for improv-
ing information retrieval (IR) or question answer-
ing (QA) systems with mixed results. One of the
problems is that the queries are expanded word
by word, ignoring the context and as a result the
extensions often become inadequate7. However,
Riezler et al. (2007) take the entire query into ac-
count when adding new words by utilizing tech-
niques used in statistical machine translation.
Query expansion for summarization has not yet
been explored as extensively as in IR or QA.
Nastase (2008) uses Wikipedia and WordNet for
query expansion and proposes that a concept can
be expanded by adding the text of all hyper-
links from the first paragraph of the Wikipedia
article about this concept. The automatic eval-
uation demonstrates that extracting relevant con-
cepts from Wikipedia leads to better performance
compared with WordNet: both expansion systems
outperform the no-expansion version in terms of
the ROUGE score. Although this method proved
helpful on the DUC data, it seems less appropriate
for expanding company names. For small compa-
nies there are short articles with only a few links;
the first paragraphs of the articles about larger
companies often include interesting rather than
relevant information. For example, the text pre-
ceding the contents box in the article about Apple
Inc. (AAPL) states that “Fortune magazine named
Apple the most admired company in the United
States”8. The link to the article about the For-
tune magazine can be hardly considered relevant
for the expansion of AAPL. Wikipedia category
information, which has been successfully used in
some NLP tasks (Ponzetto &amp; Strube, 2006, inter
alia), is too general and does not help discriminate
between two companies from the same sector.
Our work suggests that query expansion is
needed for summarization in the financial domain.
In addition to previous work, we also show that an-
other key factor for success in this task is detecting
and modeling the novelty of the target content.
7E.g., see the proceedings of TREC 9, TREC 10: http:
//trec.nist.gov.
</bodyText>
<note confidence="0.950429">
8Checked on September 17, 2008.
</note>
<sectionHeader confidence="0.996959" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999766692307692">
In this paper we presented a multi-document
company-oriented summarization algorithm
which extracts sentences that are both relevant for
the given organization and novel to the user. The
system is expected to be useful in the context of
stock market monitoring and forecasting, that is,
to help the trader predict the move of the stock
price for the given company. We presented a
novel query expansion method which works par-
ticularly well in the context of company-oriented
summarization. Our sentence ranking method is
unsupervized and requires little parameter tuning.
An automatic evaluation against a competitive
baseline showed supportive results, indicating that
the ranking algorithm is able to select relevant
sentences and promote novel information at the
same time.
In the future, we plan to experiment with po-
sitional features which have proven useful for
generic summarization. We also plan to test the
system extrinsically. For example, it would be of
interest to see if a classifier may predict the move
of stock prices based on a set of features extracted
from company-oriented summaries.
Acknowledgments: We would like to thank the
anonymous reviewers for their helpful feedback.
</bodyText>
<sectionHeader confidence="0.990213" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998415">
Allan, James, Courtney Wade &amp; Alvaro Bolivar
(2003). Retrieval and novelty detection at the
sentence level. In Proceedings of the 26th An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval Toronto, On., Canada, 28 July – 1 Au-
gust 2003, pp. 314–321.
Atserias, Jordi, Hugo Zaragoza, Massimiliano
Ciaramita &amp; Giuseppe Attardi (2008). Se-
mantically annotated snapshot of the English
Wikipedia. In Proceedings of the 6th Interna-
tional Conference on Language Resources and
Evaluation, Marrakech, Morocco, 26 May – 1
June 2008.
Brin, Sergey &amp; Lawrence Page (1998). The
anatomy of a large-scale hypertextual web
search engine. Computer Networks and ISDN
Systems, 30(1–7):107–117.
Ciaramita, Massimiliano &amp; Yasemin Altun
(2006). Broad-coverage sense disambiguation
</reference>
<page confidence="0.637835">
253
</page>
<reference confidence="0.996320629213484">
and information extraction with a supersense
sequence tagger. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, Sydney, Australia,
22–23 July 2006, pp. 594–602.
Dang, Hoa Trang (2005). Overview of DUC
2005. In Proceedings of the 2005 Document
Understanding Conference held at the Human
Language Technology Conference and Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 9–
10 October 2005.
Erkan, G¨unes¸ (2006). Using biased random walks
for focused summarization. In Proceedings
of the 2006 Document Understanding Confer-
ence held at the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,,
New York, N.Y., 8–9 June 2006.
Erkan, G¨unes¸ &amp; Dragomir R. Radev (2004).
LexRank: Graph-based lexical centrality as
salience in text summarization. Journal ofArti-
ficial Intelligence Research, 22:457–479.
Lerman, Kevin, Ari Gilder, Mark Dredze &amp; Fer-
nando Pereira (2008). Reading the markets:
Forecasting public opinion of political candi-
dates by news analysis. In Proceedings of
the 22st International Conference on Computa-
tional Linguistics, Manchester, UK, 18–22 Au-
gust 2008, pp. 473–480.
Lin, Chin-Yew &amp; Eduard H. Hovy (2003). Au-
tomatic evaluation of summaries using N-gram
co-occurrence statistics. In Proceedings of the
Human Language Technology Conference ofthe
North American Chapter of the Association for
Computational Linguistics, Edmonton, Alberta,
Canada, 27 May –1 June 2003, pp. 150–157.
Mihalcea, Rada &amp; Paul Tarau (2004). Textrank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Nat-
ural Language Processing, Barcelona, Spain,
25–26 July 2004, pp. 404–411.
Nastase, Vivi (2008). Topic-driven multi-
document summarization with encyclopedic
knowledge and activation spreading. In Pro-
ceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Hon-
olulu, Hawaii, 25–27 October 2008. To appear.
Nelken, Rani &amp; Stuart M. Shieber (2006). To-
wards robust context-sensitive sentence align-
ment for monolingual corpora. In Proceedings
of the 11th Conference of the European Chapter
of the Association for Computational Linguis-
tics, Trento, Italy, 3–7 April 2006, pp. 161–168.
Otterbacher, Jahna, G¨unes¸ Erkan &amp; Dragomir
Radev (2005). Using random walks for
question-focused sentence retrieval. In Pro-
ceedings of the Human Language Technology
Conference and the 2005 Conference on Empir-
ical Methods in Natural Language Processing,
Vancouver, B.C., Canada, 6–8 October 2005,
pp. 915–922.
Ponzetto, Simone Paolo &amp; Michael Strube (2006).
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Pro-
ceedings of the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
New York, N.Y., 4–9 June 2006, pp. 192–199.
Radev, Dragomir R., Hongyan Jing &amp; Malgorzata
Budzikowska (2000). Centroid-based summa-
rization of mutliple documents: Sentence ex-
traction, utility-based evaluation, and user stud-
ies. In Proceedings of the Workshop on Au-
tomatic Summarization at ANLP/NAACL 2000,
Seattle, Wash., 30 April 2000, pp. 21–30.
Riezler, Stefan, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal &amp; Yi Liu (2007).
Statistical machine translation for query expan-
sion in answer retrieval. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Re-
public, 23–30 June 2007, pp. 464–471.
W¨uthrich, B, D. Permunetilleke, S. Leung, V. Cho,
J. Zhang &amp; W. Lam (1998). Daily prediction of
major stock indices from textual WWW data. In
In Proceedings of the 4th International Confer-
ence on Knowledge Discovery and Data Mining
- KDD-98, pp. 364–368.
</reference>
<page confidence="0.945185">
254
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818813">
<title confidence="0.998563">Extractive Summarization of Financial</title>
<author confidence="0.999271">Mihai Massimiliano Hugo</author>
<affiliation confidence="0.903163">Research gGmbH Research</affiliation>
<address confidence="0.9159015">Schloss-Wolfsbrunnenweg 33 Avinguda Diagonal 177 69118 Heidelberg, Germany 08018 Barcelona, Spain</address>
<abstract confidence="0.997852578947369">The paper presents a multi-document summarization system which builds companyspecific summaries from a collection of financial news such that the extracted sentences contain novel and relevant information about the corresponding organization. The user’s familiarity with the company’s profile is assumed. The goal of such summaries is to provide information useful for the short-term trading of the corresponding company, i.e., to facilitate the inference from news to stock price movement in the next day. We introduce a novel query (i.e., company name) expansion method and a simple unsupervized algorithm for sentence ranking. The system shows promising results in comparison with a competitive baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Courtney Wade</author>
<author>Alvaro Bolivar</author>
</authors>
<title>Retrieval and novelty detection at the sentence level.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</booktitle>
<volume>1</volume>
<pages>314--321</pages>
<location>Toronto, On.,</location>
<contexts>
<context position="11257" citStr="Allan et al., 2003" startWordPosition="1807" endWordPosition="1810">ive baseline because it has been successfully tested in a similar setting–it has been applied to multi-document query-focused summarization of news documents. Given a graph G = (5, E), where 5 is the set of all sentences from all input documents, and E is the set of edges representing normalized sentence similarities, Otterbacher et al. (2005) rank all sen248 AAPL apple music mac DAL air flight delta DVA dialysis davita esrd as in the query are stemmed and stopwords are removed from them). Relevance to the query is defined in Equation (6) which has been previously used for sentence retrieval (Allan et al., 2003): kidney inpatient X rel(s|q) _ log(tfw,s + 1) X log(tfw,q + 1) X idfw,S (6) software ipod lines schedule computer peripheral movie player desktop Table 1: Top 10 scoring words for three companies tence nodes based on the inter-sentence relations as well as the relevance to the query q. Sentence ranks are found iteratively over the set of graph nodes with the following formula: re(t�9)+(1—a)t�r_�3�() t)7 9) (2) r(s, q) _ λP The first term represents the importance of a sentence defined in respect to the query, whereas the second term infers the importance of the sentence from its relation to o</context>
</contexts>
<marker>Allan, Wade, Bolivar, 2003</marker>
<rawString>Allan, James, Courtney Wade &amp; Alvaro Bolivar (2003). Retrieval and novelty detection at the sentence level. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval Toronto, On., Canada, 28 July – 1 August 2003, pp. 314–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordi Atserias</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Massimiliano Ciaramita &amp; Giuseppe Attardi</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation,</booktitle>
<volume>1</volume>
<location>Marrakech,</location>
<marker>Atserias, Zaragoza, 2008</marker>
<rawString>Atserias, Jordi, Hugo Zaragoza, Massimiliano Ciaramita &amp; Giuseppe Attardi (2008). Semantically annotated snapshot of the English Wikipedia. In Proceedings of the 6th International Conference on Language Resources and Evaluation, Marrakech, Morocco, 26 May – 1 June 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<pages>30--1</pages>
<contexts>
<context position="13167" citStr="Brin &amp; Page, 1998" startWordPosition="2144" endWordPosition="2147">g + sfw(5)(0.5 where tfw,s is the frequency of w in sentence s, |S |is the total number of sentences in the documents from which sentences are to be extracted, and sfw is the number of sentences which contain the word w (all words in the documents as well w∈q where tfw,x stands for the number of times w appears in x, be it a sentence (s) or the query (q). If a sentence shares no words other than stopwords with the query, the relevance becomes zero. Note that without the relevance to the query part Equation 2 takes only inter-sentence similarity into account and computes the weighted PageRank (Brin &amp; Page, 1998). In defining the relevance to the query, in Equation (6), words which do not appear in too many sentences in the document collection weigh more. Indeed, if a word from the query is contained in many sentences, it should not count much. But it is also true that not all words from the query are equally important. As it has been mentioned in Section 3, words like product or offer appear in many business summaries and are equally related to any company. To penalize such words, when computing the relevance to the query, we multiply the relevance score of a given word w with the inverted document f</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Brin, Sergey &amp; Lawrence Page (1998). The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems, 30(1–7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>594--602</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="6797" citStr="Ciaramita &amp; Altun, 2006" startWordPosition="1093" endWordPosition="1097">ed of HTML tags, embedded graphics and unrelated information (e.g., ads, frames) with a set of manually devised rules. The filtering is not perfect but removes most of the noise. Each article is passed through a language processing pipeline (described in (Atserias et al., 2008)). Sentence boundaries are identified by means of simple heuristics. The text is tokenized according to Penn TreeBank style and each token lemmatized using Wordnet’s morphological functions. Part of speech tags and named entities (LOC, PER, ORG, MISC) are identified by means of a publicly available named-entity tagger5 (Ciaramita &amp; Altun, 2006, SuperSense). Apart from that, all sentences which are shorter than 5 tokens and contain neither nouns nor verbs are sorted out. We apply the latter filter as we are interested in textual information only. Numeric information contained, e.g., in tables can be easily and more reliably obtained from the indices tables available online. 3 Query Expansion In company-oriented summarization query expansion is crucial because, by default, our query contains only the symbol, that is the abbreviation of the name of the company. Unfortunately, existing query expansion techniques which utilize such know</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Ciaramita, Massimiliano &amp; Yasemin Altun (2006). Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, Sydney, Australia, 22–23 July 2006, pp. 594–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Document Understanding Conference held at the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="19937" citStr="Dang, 2005" startWordPosition="3324" endWordPosition="3325">before the ranking has taken place because often such repetitions carry important and relevant information. The redundancy filter is applied to all the systems described as they are equally prone to include repetitions. METHOD ROUGE-2 Otterbacher 0.255 (0.226 - 0.285) Query Weights 0.289 (0.254 - 0.324) Novelty Bias (simple) 0.315 (0.287 - 0.342) Novelty Bias 0.302 (0.277 - 0.329) Manual 0.472 (0.415 - 0.531) Table 2: Results of the four extraction methods and human annotators jackknife for each (query, summary) pair and computed a macro-average to make human and automatic results comparable (Dang, 2005). The scores computed on summaries produced by humans are given in the bottom line (MANUAL) and serve as upper bound and also as an indicator for the inter-annotator agreement. 6 Evaluation We randomly selected 23 company stock names, and constructed a document collection for each containing all the news provided in the Yahoo! Finance news feed for that company in a period of two days (the time period was chosen randomly). The average length of a news collection is about 600 tokens. When selecting the company names, we took care of not picking those which have only a few news articles for that</context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>Dang, Hoa Trang (2005). Overview of DUC 2005. In Proceedings of the 2005 Document Understanding Conference held at the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Vancouver, B.C., Canada, 9– 10 October 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
</authors>
<title>Using biased random walks for focused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Document Understanding Conference held at the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,,</booktitle>
<location>New York, N.Y., 8–9</location>
<contexts>
<context position="27227" citStr="Erkan, 2006" startWordPosition="4538" endWordPosition="4539">ntence relations in the document collection. In (Radev et al., 2000), the centroid of the collection, i.e., the words with the highest TF*IDF, is considered and the sentences which contain more words from the centroid are extracted. Mihalcea &amp; Tarau (2004) explore several methods developed for ranking documents in information retrieval for the single-document summarization task. Similarly, Erkan &amp; Radev (2004) apply in-degree and PageRank to build a summary from a collection of related documents. They show that their method, called LexRank, achieves good results. In (Otterbacher et al., 2005; Erkan, 2006) the ranking function of LexRank is extended to become applicable to query-focused summarization. The rank of a sentence is determined not just by its relation to other sentences in 252 the document collection but also by its relevance to the query. Relevance to the query is defined as the word-based similarity between query and sentence. Query expansion has been used for improving information retrieval (IR) or question answering (QA) systems with mixed results. One of the problems is that the queries are expanded word by word, ignoring the context and as a result the extensions often become i</context>
</contexts>
<marker>Erkan, 2006</marker>
<rawString>Erkan, G¨unes¸ (2006). Using biased random walks for focused summarization. In Proceedings of the 2006 Document Understanding Conference held at the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,, New York, N.Y., 8–9 June 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>LexRank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>22--457</pages>
<contexts>
<context position="27028" citStr="Erkan &amp; Radev (2004)" startWordPosition="4505" endWordPosition="4508">ancial markets where the news typically cover many events, only some related to the company of interest. Unsupervized summarization systems extract sentences whose relevance can be inferred from the inter-sentence relations in the document collection. In (Radev et al., 2000), the centroid of the collection, i.e., the words with the highest TF*IDF, is considered and the sentences which contain more words from the centroid are extracted. Mihalcea &amp; Tarau (2004) explore several methods developed for ranking documents in information retrieval for the single-document summarization task. Similarly, Erkan &amp; Radev (2004) apply in-degree and PageRank to build a summary from a collection of related documents. They show that their method, called LexRank, achieves good results. In (Otterbacher et al., 2005; Erkan, 2006) the ranking function of LexRank is extended to become applicable to query-focused summarization. The rank of a sentence is determined not just by its relation to other sentences in 252 the document collection but also by its relevance to the query. Relevance to the query is defined as the word-based similarity between query and sentence. Query expansion has been used for improving information retr</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Erkan, G¨unes¸ &amp; Dragomir R. Radev (2004). LexRank: Graph-based lexical centrality as salience in text summarization. Journal ofArtificial Intelligence Research, 22:457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lerman</author>
<author>Ari Gilder</author>
</authors>
<title>Mark Dredze &amp; Fernando Pereira (2008). Reading the markets: Forecasting public opinion of political candidates by news analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22st International Conference on Computational Linguistics,</booktitle>
<pages>473--480</pages>
<location>Manchester, UK,</location>
<marker>Lerman, Gilder, 2008</marker>
<rawString>Lerman, Kevin, Ari Gilder, Mark Dredze &amp; Fernando Pereira (2008). Reading the markets: Forecasting public opinion of political candidates by news analysis. In Proceedings of the 22st International Conference on Computational Linguistics, Manchester, UK, 18–22 August 2008, pp. 473–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using N-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference ofthe North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>150--157</pages>
<location>Edmonton, Alberta,</location>
<contexts>
<context position="5509" citStr="Lin &amp; Hovy, 2003" startWordPosition="879" endWordPosition="882">ences based on their relatedness to the expanded query and their overall importance (Section 4). Finally, the most relevant sentences are re-ranked based on the degree of novelty they carry (Section 5). The paper makes the following contributions. First, we present a new query expansion technique which is useful in the context of companydependent news summarization as it helps identify sentences important to the company. Second, we introduce a simple and efficient method for sentence ranking which foregrounds novel information of interest. Our system performs well in terms of the ROUGE score (Lin &amp; Hovy, 2003) compared with a competitive baseline (Section 6). 2 Data The data we work with is a collection of financial news consolidated and distributed by Yahoo! Fi2See the DUC 2007 and 2008 update tracks. nance3 from various sources4. Each story is labeled as being relevant for a company – i.e., it appears in the company’s RSS feed – if the story mentions either the company itself or the sector the company belongs to. Altogether the corpus contains 88,974 news articles from a period of about 5 months (148 days). Some articles are labeled as being relevant for several companies. The total number of (co</context>
<context position="21422" citStr="Lin &amp; Hovy, 2003" startWordPosition="3577" endWordPosition="3580">to choose the sentences which could best help them decide whether to buy, sell or retain stock for the company the following day and present them in the order of decreasing importance. The annotators compared their summaries of the first four collections and clarified the procedure before proceeding with the other ones. These four collections were then later used as a development set. All summaries – manually as well as automatically generated – were cut to the first 250 words which made the summaries 10 words shorter on average. We evaluated the performance automatically in terms of ROUGE-2 (Lin &amp; Hovy, 2003) using the parameters and following the methodology from the DUC events. The results are presented in Table 2. We also report the 95% confidence intervals in brackets. As in DUC, we used 6.1 Discussion From Table 2 follows that the modifications we applied to the baseline are sensible and indeed bring an improvement. QUERY WEIGHTS performs better than OTTERBACHER and is in turn outperformed by the algorithms biased to novel information (the two NOVELTY systems). The overlap between the confidence intervals of the baseline and the simple version of the novelty algorithm is minimal (0.002). It i</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin, Chin-Yew &amp; Eduard H. Hovy (2003). Automatic evaluation of summaries using N-gram co-occurrence statistics. In Proceedings of the Human Language Technology Conference ofthe North American Chapter of the Association for Computational Linguistics, Edmonton, Alberta, Canada, 27 May –1 June 2003, pp. 150–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>404--411</pages>
<location>Barcelona,</location>
<contexts>
<context position="26871" citStr="Mihalcea &amp; Tarau (2004)" startWordPosition="4484" endWordPosition="4487">c opinion about political candidates in such prediction markets. Our approach can be seen as a complement to both these approaches, necessary especially for financial markets where the news typically cover many events, only some related to the company of interest. Unsupervized summarization systems extract sentences whose relevance can be inferred from the inter-sentence relations in the document collection. In (Radev et al., 2000), the centroid of the collection, i.e., the words with the highest TF*IDF, is considered and the sentences which contain more words from the centroid are extracted. Mihalcea &amp; Tarau (2004) explore several methods developed for ranking documents in information retrieval for the single-document summarization task. Similarly, Erkan &amp; Radev (2004) apply in-degree and PageRank to build a summary from a collection of related documents. They show that their method, called LexRank, achieves good results. In (Otterbacher et al., 2005; Erkan, 2006) the ranking function of LexRank is extended to become applicable to query-focused summarization. The rank of a sentence is determined not just by its relation to other sentences in 252 the document collection but also by its relevance to the q</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Mihalcea, Rada &amp; Paul Tarau (2004). Textrank: Bringing order into texts. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, Barcelona, Spain, 25–26 July 2004, pp. 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
</authors>
<title>Topic-driven multidocument summarization with encyclopedic knowledge and activation spreading.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Honolulu, Hawaii,</location>
<note>To appear.</note>
<contexts>
<context position="28097" citStr="Nastase (2008)" startWordPosition="4683" endWordPosition="4684">nce to the query is defined as the word-based similarity between query and sentence. Query expansion has been used for improving information retrieval (IR) or question answering (QA) systems with mixed results. One of the problems is that the queries are expanded word by word, ignoring the context and as a result the extensions often become inadequate7. However, Riezler et al. (2007) take the entire query into account when adding new words by utilizing techniques used in statistical machine translation. Query expansion for summarization has not yet been explored as extensively as in IR or QA. Nastase (2008) uses Wikipedia and WordNet for query expansion and proposes that a concept can be expanded by adding the text of all hyperlinks from the first paragraph of the Wikipedia article about this concept. The automatic evaluation demonstrates that extracting relevant concepts from Wikipedia leads to better performance compared with WordNet: both expansion systems outperform the no-expansion version in terms of the ROUGE score. Although this method proved helpful on the DUC data, it seems less appropriate for expanding company names. For small companies there are short articles with only a few links;</context>
</contexts>
<marker>Nastase, 2008</marker>
<rawString>Nastase, Vivi (2008). Topic-driven multidocument summarization with encyclopedic knowledge and activation spreading. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Honolulu, Hawaii, 25–27 October 2008. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rani Nelken</author>
<author>Stuart M Shieber</author>
</authors>
<title>Towards robust context-sensitive sentence alignment for monolingual corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>161--168</pages>
<location>Trento,</location>
<contexts>
<context position="14036" citStr="Nelken &amp; Shieber, 2006" startWordPosition="2303" endWordPosition="2306"> also true that not all words from the query are equally important. As it has been mentioned in Section 3, words like product or offer appear in many business summaries and are equally related to any company. To penalize such words, when computing the relevance to the query, we multiply the relevance score of a given word w with the inverted document frequency of w on the corpus of business summaries Q – idfw,Q: ( |Q |� idfw,Q = log (7) qfw We also replace tfw,s with the indicator function s(w) since it has been reported to be more adequate for sentences, in particular for sentence alignment (Nelken &amp; Shieber, 2006): � 1 if s contains w s(w) = (8) 0 otherwise Thus, the modified formula we use to compute sentence ranks is as follows: rel(s|q) _ X s(w) X log(tfw,q + 1) X idfw,S X idfw,Q (9) w∈q We call these two ranking algorithms that use the formula in (2) OTTERBACHER and QUERY WEIGHTS, the difference being the way the relevance to the query is computed: (6) or (9). We use the OTTERBACHER algorithm as a baseline in the experiments reported in Section 6. destination passenger cargo atlanta fleet outpatient patient hospital disease service 249 5 Novelty Bias Apart from being related to the query, a good su</context>
</contexts>
<marker>Nelken, Shieber, 2006</marker>
<rawString>Nelken, Rani &amp; Stuart M. Shieber (2006). Towards robust context-sensitive sentence alignment for monolingual corpora. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy, 3–7 April 2006, pp. 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
</authors>
<title>G¨unes¸ Erkan &amp; Dragomir Radev</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>915--922</pages>
<location>Vancouver, B.C.,</location>
<marker>Otterbacher, 2005</marker>
<rawString>Otterbacher, Jahna, G¨unes¸ Erkan &amp; Dragomir Radev (2005). Using random walks for question-focused sentence retrieval. In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing, Vancouver, B.C., Canada, 6–8 October 2005, pp. 915–922.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>192--199</pages>
<location>New York, N.Y., 4–9</location>
<contexts>
<context position="29216" citStr="Ponzetto &amp; Strube, 2006" startWordPosition="4859" endWordPosition="4862">opriate for expanding company names. For small companies there are short articles with only a few links; the first paragraphs of the articles about larger companies often include interesting rather than relevant information. For example, the text preceding the contents box in the article about Apple Inc. (AAPL) states that “Fortune magazine named Apple the most admired company in the United States”8. The link to the article about the Fortune magazine can be hardly considered relevant for the expansion of AAPL. Wikipedia category information, which has been successfully used in some NLP tasks (Ponzetto &amp; Strube, 2006, inter alia), is too general and does not help discriminate between two companies from the same sector. Our work suggests that query expansion is needed for summarization in the financial domain. In addition to previous work, we also show that another key factor for success in this task is detecting and modeling the novelty of the target content. 7E.g., see the proceedings of TREC 9, TREC 10: http: //trec.nist.gov. 8Checked on September 17, 2008. 8 Conclusions In this paper we presented a multi-document company-oriented summarization algorithm which extracts sentences that are both relevant f</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Ponzetto, Simone Paolo &amp; Michael Strube (2006). Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, New York, N.Y., 4–9 June 2006, pp. 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Malgorzata Budzikowska</author>
</authors>
<title>Centroid-based summarization of mutliple documents: Sentence extraction, utility-based evaluation, and user studies.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization at ANLP/NAACL 2000,</booktitle>
<pages>21--30</pages>
<location>Seattle, Wash.,</location>
<contexts>
<context position="26683" citStr="Radev et al., 2000" startWordPosition="4453" endWordPosition="4456">roduced a forecasting system for prediction markets that combines news analysis with a price trend analysis model. This approach was shown to be successful for the forecasting of public opinion about political candidates in such prediction markets. Our approach can be seen as a complement to both these approaches, necessary especially for financial markets where the news typically cover many events, only some related to the company of interest. Unsupervized summarization systems extract sentences whose relevance can be inferred from the inter-sentence relations in the document collection. In (Radev et al., 2000), the centroid of the collection, i.e., the words with the highest TF*IDF, is considered and the sentences which contain more words from the centroid are extracted. Mihalcea &amp; Tarau (2004) explore several methods developed for ranking documents in information retrieval for the single-document summarization task. Similarly, Erkan &amp; Radev (2004) apply in-degree and PageRank to build a summary from a collection of related documents. They show that their method, called LexRank, achieves good results. In (Otterbacher et al., 2005; Erkan, 2006) the ranking function of LexRank is extended to become a</context>
</contexts>
<marker>Radev, Jing, Budzikowska, 2000</marker>
<rawString>Radev, Dragomir R., Hongyan Jing &amp; Malgorzata Budzikowska (2000). Centroid-based summarization of mutliple documents: Sentence extraction, utility-based evaluation, and user studies. In Proceedings of the Workshop on Automatic Summarization at ANLP/NAACL 2000, Seattle, Wash., 30 April 2000, pp. 21–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
</authors>
<title>Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal &amp; Yi Liu</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>464--471</pages>
<location>Prague, Czech Republic,</location>
<marker>Riezler, 2007</marker>
<rawString>Riezler, Stefan, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal &amp; Yi Liu (2007). Statistical machine translation for query expansion in answer retrieval. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Prague, Czech Republic, 23–30 June 2007, pp. 464–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B W¨uthrich</author>
<author>D Permunetilleke</author>
<author>S Leung</author>
<author>V Cho</author>
<author>J Zhang</author>
<author>W Lam</author>
</authors>
<title>Daily prediction of major stock indices from textual WWW data. In</title>
<date>1998</date>
<booktitle>In Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining - KDD-98,</booktitle>
<pages>364--368</pages>
<marker>W¨uthrich, Permunetilleke, Leung, Cho, Zhang, Lam, 1998</marker>
<rawString>W¨uthrich, B, D. Permunetilleke, S. Leung, V. Cho, J. Zhang &amp; W. Lam (1998). Daily prediction of major stock indices from textual WWW data. In In Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining - KDD-98, pp. 364–368.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>