<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.038693">
<title confidence="0.995469">
Modifying Beliefs in a Plan-Based Dialogue Model
</title>
<author confidence="0.982815">
Lynn Lambert
</author>
<affiliation confidence="0.9928765">
Department of Computer and Information Sciences
University of Delaware
</affiliation>
<address confidence="0.383166">
Newark, Delaware 197161
</address>
<sectionHeader confidence="0.996656" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998988857142857">
Previous models of discourse have inadequately
accounted for how beliefs change during a conversation.
This paper outlines a model of dialogue which main-
tains and updates a user&apos;s multi-level belief model as
the discourse proceeds. This belief model is used in a
plan-recognition framework to identify communicative
goals such as expressing surprise.
</bodyText>
<sectionHeader confidence="0.804662" genericHeader="method">
2 Plans, Beliefs, and Processing
</sectionHeader>
<bodyText confidence="0.920198088235294">
My plan-based model of dialogue incrementally
builds a structure of the discourse (a Dialogue Model,
or DM) using a multi-level belief model updated after
each utterance. The belief model contains the beliefs as-
cribed to the user during the course of the conversation
and how strongly each belief is held.
Researchers [1, 3, 5] have noted that discourse
understanding can be enhanced by recognizing a user&apos;s
goals, and that this recognition process requires reason-
ing about the agent&apos;s beliefs [7]. For example, in order
to recognize from utterance IS2 in the following dia-
logue that the speaker has the communicative goal of
expressing surprise at the proposition that Dr. Smith
is teaching CIS360 and not just asking if Dr. Smith is
teaching CIS420, it is necessary for the system to be
able to plausibly ascribe to IS the beliefs that 1) Dr.
Smith is teaching CIS420; 2) that this somehow implies
that Dr. Smith is not teaching CIS360; and 3) that IP
believes that Dr. Smith is teaching CIS360.
IS,: Who is teaching CIS 360?
IP,: Dr. Smith.
1S2: Dr. Smith is teaching CIS 420, isn&apos;t she?
IP2: Yes, she is. Dr. Smith is teaching two courses.
IS3: What time is CIS 360?
My model ascribes these beliefs to IS as the discourse
proceeds, aria uses the ascribed beliefs for recognizing
utterances that involve negotiation dialogues. Without
the ability to modify a belief model as a dialogue pro-
gresses, it would not be possible to plausibly ascribe
1) or 3), so it is unclear how recognizing expressions
of surprise would be accomplished in systems such as
Litman&apos;s [5] that recognize discourse goals but do not
maintain belief models. IS2 also exemplifies how people
may have levels of belief and indicate those levels in the
</bodyText>
<subsectionHeader confidence="0.788628333333333">
1This material is based upon work supported by the National
Science Foundation under Grant No. IRI-8909332. The Govern-
ment has certain rights in this material.
</subsectionHeader>
<bodyText confidence="0.998893348837209">
surface form of utterances. Here, IS uses a tag question
to indicate that he thinks that Dr. Smith is teaching
CIS420, but is not certain of it. My belief model main-
tains three levels of belief, three levels of disbelief, and
one level indicating no belief about a proposition.
My process model begins with the semantic rep-
resentation of an utterance. The effects of the surface
speech act, such as a tag question, are used to suggest
augmentations to the belief model. Plan inference rules
are used to infer actions that might motivate the utter-
ance; the belief ascription process during constraint sat-
isfaction determines whether it is reasonable to ascribe
the requisite beliefs to the agent of the action and, if
not, the inference is rejected. Focusing heuristics allow
expectations derived from the existing dialogue context
to guide the recognition process by preferring those in-
ferences that lead to the most coherent expansions of
the existing dialogue model.
The resultant DM contains a structure of the dia-
logue at every point in the discourse, including three dif-
ferent kinds of goals, each modeled on a separate level:
the domain level models domain goals such as travel-
ing by train; the problem-solving level, plan-construction
goals such as instantiating a variable in a plan; and the
discourse level, communicative goals such as express-
ing surprise. Within each of these levels, actions may
contribute to other actions on the same level; for exam-
ple, on the discourse level, providing background data,
asking a question, and answering a question all can be
part of obtaining information.2 So, actions at each level
form a tree structure in which each node represents an
action that a participant is performing and the chil-
dren of a node represent actions pursued in order to
perform the parent action. This tree structure allows
my model to capture the relationship among several ut-
terances that are all part of the same higher-level dis-
course plan, which is not possible in Litman&apos;s model
[5]. In addition, an action on one level may contribute
to, or link to, an action on an immediately higher level.
For example, discourse actions may be executed to at-
tain the knowledge needed for problem-solving actions
at the middle level.
This tripartite, plan-based model of discourse fa-
</bodyText>
<footnote confidence="0.9987556">
2The DM is really a mental model of intentions [1 which im-
plicitly captures a number of intentions that are attributed to the
participants, such as the intention that the participants follow
through with the subactions that are part of plans for actions in
the DM.
</footnote>
<page confidence="0.997752">
349
</page>
<bodyText confidence="0.999903464285714">
cilitates recognition of changing beliefs as the dialogue
progresses. Allen&apos;s representation of an Inform speech
act [1] assumed that a listener adopted the communi-
cated proposition. Clearly, listeners do not adopt every-
thing they are told (e.g., IS2 indicates that IS does not
immediately accept that Dr. Smith is teaching CIS360).
Perrault [6] assumed that a listener adopted the com-
municated proposition unless the listener had conflict-
ing beliefs, as in 1S2. Unfortunately, Perrault assumes
that people&apos;s beliefs persist so it would not be possible
for Perrault to model IS adopting IP&apos;s explanation in
IP2. I am assuming that the participants are involved
in a cooperative dialogue, so try to square away their
beliefs [4]. Thus, after every Inform action, a speaker
expects the listener either to accept any claims that the
speaker made or to initiate a negotiation dialogue.3 Ac-
ceptance can be communicated in two ways. Either the
listener can explicitly indicate acceptance (e.g., &amp;quot;oh, al-
right&amp;quot;), or the listener can implicitly convey acceptance
[2] by making an utterance which cannot be interpreted
as initiating a negotiation dialogue. Since both parties
are engaged in a cooperative dialogue in which beliefs
are squared away, this failure to initiate a negotiation di-
alogue by default indicates (implicit) acceptance of any
claims not disputed. This corresponds with a restricted
form of Perrault&apos;s default reasoning about the effects of
Inform acts [6]. An example of implicit acceptance is
considered in the next section.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="method">
3 Example
</sectionHeader>
<bodyText confidence="0.999285909090909">
Consider the dialogue model given in Section 2.
The process model infers from the first utterance that IS
is executing a high level discourse action of Obtain-Info-
Ref to determine who is teaching CIS360 and problem-
solving actions of Instantiate- Var and Build-Plan in or-
der to build a plan to take CIS360 so that IS may even-
tually execute a domain action, Take-Course, to take
CIS360. IS2 is recognized as an expression of surprise
at IP&apos;s answer since acceptance or negotiation of the
answer is expected and since the following beliefs can
be ascribed to IS: 1) as a default rule, that teachers
generally teach only one course; 2) that Dr. Smith is
already teaching CIS420 (from the tag question form);
and 3) that the combination of 1) and 2) implies that
Dr. Smith is not teaching CIS360. IP responds by try-
ing to make her answer believable and to resolve the
conflict. This is done by informing IS that his belief
about Dr. Smith teaching CIS420 is correct, but that
Dr. Smith is an exception to the default rule.
Focusing heuristics suggest explicit acceptance of
or objection to IP2 as ways to continue the current dis-
course plan. However utterance IS3, instead, pursues a
</bodyText>
<footnote confidence="0.88093">
3A third possibility exists: that the participants agree to dis-
agree about a particular point, and continue the dialogue. My
model will handle this also, but it is not preferred, and for space
reasons will not be considered further here.
</footnote>
<bodyText confidence="0.999960789473684">
completely new discourse action, Obtain-Info-Ref, un-
related to the original Obtain-Info-Ref, though still re-
lated to the problem-solving action of Instantiate-Var
in order to build a plan to take C15360. Since a new
discourse plan is being pursued, the process model in-
fers by default that IP2 has been accepted because oth-
erwise IS would have initiated a negotiation dialogue.
Since the inform action is accepted (implicitly), this ac-
tion, and the higher level actions that it contributes to,
are considered to be successfully completed, so the goals
and effects of these plans are considered to hold. Some
of the goals of these plans are that 1) IS believes that
Dr. Smith teaches both CIS360 and CI5420, and thus is
an exception to the default rule that teachers only teach
one course and 2) IS knows that Dr. Smith is the faculty
member that teaches CIS360, the answer to the original
question that IS asked. Once the process model recog-
nizes IS3 as pursuing this new Obtain-Info-Ref action,
the belief model is updated accordingly.
</bodyText>
<sectionHeader confidence="0.999587" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999348375">
Previous models of dialogue have inadequately
accounted for changing beliefs of the participants. This
paper has outlined a plan-based model of dialogue that
makes use of beliefs currently ascribed to the user, ex-
pectations derived from the focus of attention in the di-
alogue, and implicit or explicit cues from the user both
to identify communicative goals and to recognize altered
user beliefs.
</bodyText>
<sectionHeader confidence="0.999344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999964695652174">
[1] James F. Allen. A Plan-Based Approach to Speech Act
Recognition. PhD thesis, University of Toronto, Toronto,
Ontario, Canada, 1979.
[2] S. Carberry. A pragmatics-based approach to ellipsis res-
olution. Computational Linguistics, 15(2):75-96, 1989.
[3] B. Grosz and C. Sidner. Attention, intention, and
the structure of discourse. Computational Linguistics,
12(3):175-204, 1986.
[4] Aravind K. Joshi. Mutual beliefs in question-answer sys-
tems. In N. Smith, editor, Mutual Beliefs, pages 181-
197, New York, 1982. Academic Press.
[5] D. Litman and J. Allen. A plan recognition model for
subdialogues in conversation. Cognitive Science,11:163—
200, 1987.
[6] R. Perrault. An application of default logic to speech
act theory. In P. Cohen, J. Morgan, and M. Pollack,
editors, Intentions in Communication, pages 161-185.
MIT Press, Cambridge, Massachusetts, 1990.
[7] Martha Pollack. A model of plan inference that distin-
guishes between the beliefs of actors and observers. In
Proceedings of the 24th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 207-214, New
York, New York, 1986.
</reference>
<page confidence="0.997685">
350
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.753021">
<title confidence="0.999021">Modifying Beliefs in a Plan-Based Dialogue Model</title>
<author confidence="0.999946">Lynn Lambert</author>
<affiliation confidence="0.9999515">Department of Computer and Information Sciences University of Delaware</affiliation>
<address confidence="0.753847">Delaware</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>A Plan-Based Approach to Speech Act Recognition.</title>
<date>1979</date>
<tech>PhD thesis,</tech>
<institution>University of Toronto,</institution>
<location>Toronto, Ontario, Canada,</location>
<contexts>
<context position="883" citStr="[1, 3, 5]" startWordPosition="131" endWordPosition="133"> This paper outlines a model of dialogue which maintains and updates a user&apos;s multi-level belief model as the discourse proceeds. This belief model is used in a plan-recognition framework to identify communicative goals such as expressing surprise. 2 Plans, Beliefs, and Processing My plan-based model of dialogue incrementally builds a structure of the discourse (a Dialogue Model, or DM) using a multi-level belief model updated after each utterance. The belief model contains the beliefs ascribed to the user during the course of the conversation and how strongly each belief is held. Researchers [1, 3, 5] have noted that discourse understanding can be enhanced by recognizing a user&apos;s goals, and that this recognition process requires reasoning about the agent&apos;s beliefs [7]. For example, in order to recognize from utterance IS2 in the following dialogue that the speaker has the communicative goal of expressing surprise at the proposition that Dr. Smith is teaching CIS360 and not just asking if Dr. Smith is teaching CIS420, it is necessary for the system to be able to plausibly ascribe to IS the beliefs that 1) Dr. Smith is teaching CIS420; 2) that this somehow implies that Dr. Smith is not teach</context>
<context position="5126" citStr="[1]" startWordPosition="843" endWordPosition="843">ction on an immediately higher level. For example, discourse actions may be executed to attain the knowledge needed for problem-solving actions at the middle level. This tripartite, plan-based model of discourse fa2The DM is really a mental model of intentions [1 which implicitly captures a number of intentions that are attributed to the participants, such as the intention that the participants follow through with the subactions that are part of plans for actions in the DM. 349 cilitates recognition of changing beliefs as the dialogue progresses. Allen&apos;s representation of an Inform speech act [1] assumed that a listener adopted the communicated proposition. Clearly, listeners do not adopt everything they are told (e.g., IS2 indicates that IS does not immediately accept that Dr. Smith is teaching CIS360). Perrault [6] assumed that a listener adopted the communicated proposition unless the listener had conflicting beliefs, as in 1S2. Unfortunately, Perrault assumes that people&apos;s beliefs persist so it would not be possible for Perrault to model IS adopting IP&apos;s explanation in IP2. I am assuming that the participants are involved in a cooperative dialogue, so try to square away their beli</context>
</contexts>
<marker>[1]</marker>
<rawString>James F. Allen. A Plan-Based Approach to Speech Act Recognition. PhD thesis, University of Toronto, Toronto, Ontario, Canada, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Carberry</author>
</authors>
<title>A pragmatics-based approach to ellipsis resolution.</title>
<date>1989</date>
<journal>Computational Linguistics,</journal>
<pages>15--2</pages>
<contexts>
<context position="6063" citStr="[2]" startWordPosition="993" endWordPosition="993"> in 1S2. Unfortunately, Perrault assumes that people&apos;s beliefs persist so it would not be possible for Perrault to model IS adopting IP&apos;s explanation in IP2. I am assuming that the participants are involved in a cooperative dialogue, so try to square away their beliefs [4]. Thus, after every Inform action, a speaker expects the listener either to accept any claims that the speaker made or to initiate a negotiation dialogue.3 Acceptance can be communicated in two ways. Either the listener can explicitly indicate acceptance (e.g., &amp;quot;oh, alright&amp;quot;), or the listener can implicitly convey acceptance [2] by making an utterance which cannot be interpreted as initiating a negotiation dialogue. Since both parties are engaged in a cooperative dialogue in which beliefs are squared away, this failure to initiate a negotiation dialogue by default indicates (implicit) acceptance of any claims not disputed. This corresponds with a restricted form of Perrault&apos;s default reasoning about the effects of Inform acts [6]. An example of implicit acceptance is considered in the next section. 3 Example Consider the dialogue model given in Section 2. The process model infers from the first utterance that IS is e</context>
</contexts>
<marker>[2]</marker>
<rawString>S. Carberry. A pragmatics-based approach to ellipsis resolution. Computational Linguistics, 15(2):75-96, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C Sidner</author>
</authors>
<title>Attention, intention, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--3</pages>
<contexts>
<context position="883" citStr="[1, 3, 5]" startWordPosition="131" endWordPosition="133"> This paper outlines a model of dialogue which maintains and updates a user&apos;s multi-level belief model as the discourse proceeds. This belief model is used in a plan-recognition framework to identify communicative goals such as expressing surprise. 2 Plans, Beliefs, and Processing My plan-based model of dialogue incrementally builds a structure of the discourse (a Dialogue Model, or DM) using a multi-level belief model updated after each utterance. The belief model contains the beliefs ascribed to the user during the course of the conversation and how strongly each belief is held. Researchers [1, 3, 5] have noted that discourse understanding can be enhanced by recognizing a user&apos;s goals, and that this recognition process requires reasoning about the agent&apos;s beliefs [7]. For example, in order to recognize from utterance IS2 in the following dialogue that the speaker has the communicative goal of expressing surprise at the proposition that Dr. Smith is teaching CIS360 and not just asking if Dr. Smith is teaching CIS420, it is necessary for the system to be able to plausibly ascribe to IS the beliefs that 1) Dr. Smith is teaching CIS420; 2) that this somehow implies that Dr. Smith is not teach</context>
</contexts>
<marker>[3]</marker>
<rawString>B. Grosz and C. Sidner. Attention, intention, and the structure of discourse. Computational Linguistics, 12(3):175-204, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Mutual beliefs in question-answer systems.</title>
<date>1982</date>
<booktitle>Mutual Beliefs,</booktitle>
<pages>181--197</pages>
<editor>In N. Smith, editor,</editor>
<publisher>Academic Press.</publisher>
<location>New York,</location>
<contexts>
<context position="5733" citStr="[4]" startWordPosition="941" endWordPosition="941">umed that a listener adopted the communicated proposition. Clearly, listeners do not adopt everything they are told (e.g., IS2 indicates that IS does not immediately accept that Dr. Smith is teaching CIS360). Perrault [6] assumed that a listener adopted the communicated proposition unless the listener had conflicting beliefs, as in 1S2. Unfortunately, Perrault assumes that people&apos;s beliefs persist so it would not be possible for Perrault to model IS adopting IP&apos;s explanation in IP2. I am assuming that the participants are involved in a cooperative dialogue, so try to square away their beliefs [4]. Thus, after every Inform action, a speaker expects the listener either to accept any claims that the speaker made or to initiate a negotiation dialogue.3 Acceptance can be communicated in two ways. Either the listener can explicitly indicate acceptance (e.g., &amp;quot;oh, alright&amp;quot;), or the listener can implicitly convey acceptance [2] by making an utterance which cannot be interpreted as initiating a negotiation dialogue. Since both parties are engaged in a cooperative dialogue in which beliefs are squared away, this failure to initiate a negotiation dialogue by default indicates (implicit) acceptan</context>
</contexts>
<marker>[4]</marker>
<rawString>Aravind K. Joshi. Mutual beliefs in question-answer systems. In N. Smith, editor, Mutual Beliefs, pages 181-197, New York, 1982. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>J Allen</author>
</authors>
<title>A plan recognition model for subdialogues in conversation.</title>
<date>1987</date>
<journal>Cognitive Science,11:163—</journal>
<volume>200</volume>
<contexts>
<context position="883" citStr="[1, 3, 5]" startWordPosition="131" endWordPosition="133"> This paper outlines a model of dialogue which maintains and updates a user&apos;s multi-level belief model as the discourse proceeds. This belief model is used in a plan-recognition framework to identify communicative goals such as expressing surprise. 2 Plans, Beliefs, and Processing My plan-based model of dialogue incrementally builds a structure of the discourse (a Dialogue Model, or DM) using a multi-level belief model updated after each utterance. The belief model contains the beliefs ascribed to the user during the course of the conversation and how strongly each belief is held. Researchers [1, 3, 5] have noted that discourse understanding can be enhanced by recognizing a user&apos;s goals, and that this recognition process requires reasoning about the agent&apos;s beliefs [7]. For example, in order to recognize from utterance IS2 in the following dialogue that the speaker has the communicative goal of expressing surprise at the proposition that Dr. Smith is teaching CIS360 and not just asking if Dr. Smith is teaching CIS420, it is necessary for the system to be able to plausibly ascribe to IS the beliefs that 1) Dr. Smith is teaching CIS420; 2) that this somehow implies that Dr. Smith is not teach</context>
<context position="2123" citStr="[5]" startWordPosition="347" endWordPosition="347">hat Dr. Smith is teaching CIS360. IS,: Who is teaching CIS 360? IP,: Dr. Smith. 1S2: Dr. Smith is teaching CIS 420, isn&apos;t she? IP2: Yes, she is. Dr. Smith is teaching two courses. IS3: What time is CIS 360? My model ascribes these beliefs to IS as the discourse proceeds, aria uses the ascribed beliefs for recognizing utterances that involve negotiation dialogues. Without the ability to modify a belief model as a dialogue progresses, it would not be possible to plausibly ascribe 1) or 3), so it is unclear how recognizing expressions of surprise would be accomplished in systems such as Litman&apos;s [5] that recognize discourse goals but do not maintain belief models. IS2 also exemplifies how people may have levels of belief and indicate those levels in the 1This material is based upon work supported by the National Science Foundation under Grant No. IRI-8909332. The Government has certain rights in this material. surface form of utterances. Here, IS uses a tag question to indicate that he thinks that Dr. Smith is teaching CIS420, but is not certain of it. My belief model maintains three levels of belief, three levels of disbelief, and one level indicating no belief about a proposition. My p</context>
<context position="4450" citStr="[5]" startWordPosition="732" endWordPosition="732"> contribute to other actions on the same level; for example, on the discourse level, providing background data, asking a question, and answering a question all can be part of obtaining information.2 So, actions at each level form a tree structure in which each node represents an action that a participant is performing and the children of a node represent actions pursued in order to perform the parent action. This tree structure allows my model to capture the relationship among several utterances that are all part of the same higher-level discourse plan, which is not possible in Litman&apos;s model [5]. In addition, an action on one level may contribute to, or link to, an action on an immediately higher level. For example, discourse actions may be executed to attain the knowledge needed for problem-solving actions at the middle level. This tripartite, plan-based model of discourse fa2The DM is really a mental model of intentions [1 which implicitly captures a number of intentions that are attributed to the participants, such as the intention that the participants follow through with the subactions that are part of plans for actions in the DM. 349 cilitates recognition of changing beliefs as</context>
</contexts>
<marker>[5]</marker>
<rawString>D. Litman and J. Allen. A plan recognition model for subdialogues in conversation. Cognitive Science,11:163— 200, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Perrault</author>
</authors>
<title>An application of default logic to speech act theory.</title>
<date>1990</date>
<booktitle>Intentions in Communication,</booktitle>
<pages>161--185</pages>
<editor>In P. Cohen, J. Morgan, and M. Pollack, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="5351" citStr="[6]" startWordPosition="879" endWordPosition="879">really a mental model of intentions [1 which implicitly captures a number of intentions that are attributed to the participants, such as the intention that the participants follow through with the subactions that are part of plans for actions in the DM. 349 cilitates recognition of changing beliefs as the dialogue progresses. Allen&apos;s representation of an Inform speech act [1] assumed that a listener adopted the communicated proposition. Clearly, listeners do not adopt everything they are told (e.g., IS2 indicates that IS does not immediately accept that Dr. Smith is teaching CIS360). Perrault [6] assumed that a listener adopted the communicated proposition unless the listener had conflicting beliefs, as in 1S2. Unfortunately, Perrault assumes that people&apos;s beliefs persist so it would not be possible for Perrault to model IS adopting IP&apos;s explanation in IP2. I am assuming that the participants are involved in a cooperative dialogue, so try to square away their beliefs [4]. Thus, after every Inform action, a speaker expects the listener either to accept any claims that the speaker made or to initiate a negotiation dialogue.3 Acceptance can be communicated in two ways. Either the listene</context>
</contexts>
<marker>[6]</marker>
<rawString>R. Perrault. An application of default logic to speech act theory. In P. Cohen, J. Morgan, and M. Pollack, editors, Intentions in Communication, pages 161-185. MIT Press, Cambridge, Massachusetts, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Pollack</author>
</authors>
<title>A model of plan inference that distinguishes between the beliefs of actors and observers.</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>207--214</pages>
<location>New York, New York,</location>
<contexts>
<context position="1053" citStr="[7]" startWordPosition="159" endWordPosition="159">n framework to identify communicative goals such as expressing surprise. 2 Plans, Beliefs, and Processing My plan-based model of dialogue incrementally builds a structure of the discourse (a Dialogue Model, or DM) using a multi-level belief model updated after each utterance. The belief model contains the beliefs ascribed to the user during the course of the conversation and how strongly each belief is held. Researchers [1, 3, 5] have noted that discourse understanding can be enhanced by recognizing a user&apos;s goals, and that this recognition process requires reasoning about the agent&apos;s beliefs [7]. For example, in order to recognize from utterance IS2 in the following dialogue that the speaker has the communicative goal of expressing surprise at the proposition that Dr. Smith is teaching CIS360 and not just asking if Dr. Smith is teaching CIS420, it is necessary for the system to be able to plausibly ascribe to IS the beliefs that 1) Dr. Smith is teaching CIS420; 2) that this somehow implies that Dr. Smith is not teaching CIS360; and 3) that IP believes that Dr. Smith is teaching CIS360. IS,: Who is teaching CIS 360? IP,: Dr. Smith. 1S2: Dr. Smith is teaching CIS 420, isn&apos;t she? IP2: Y</context>
</contexts>
<marker>[7]</marker>
<rawString>Martha Pollack. A model of plan inference that distinguishes between the beliefs of actors and observers. In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, pages 207-214, New York, New York, 1986.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>