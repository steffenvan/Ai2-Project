<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006067">
<title confidence="0.988919">
Using Treebanking Discriminants as Parse Disambiguation Features
</title>
<author confidence="0.932222">
Md. Faisal Mahbub Chowdhury† and Yi Zhang$ and Valia Kordoni$
</author>
<affiliation confidence="0.879422">
† Dept of Computational Linguistics, Saarland University
$ Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany
</affiliation>
<email confidence="0.984264">
{chowd,yzhang,kordoni}@coli.uni-sb.de
</email>
<sectionHeader confidence="0.993584" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928105263158">
This paper presents a novel approach of in-
corporating fine-grained treebanking deci-
sions made by human annotators as dis-
criminative features for automatic parse
disambiguation. To our best knowledge,
this is the first work that exploits treebank-
ing decisions for this task. The advan-
tage of this approach is that use of human
judgements is made. The paper presents
comparative analyses of the performance
of discriminative models built using tree-
banking decisions and state-of-the-art fea-
tures. We also highlight how differently
these features scale when these models are
tested on out-of-domain data. We show
that, features extracted using treebanking
decisions are more efficient, informative
and robust compared to traditional fea-
tures.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977396226415">
State-of-the-art parse disambiguation models are
trained on treebanks, which are either fully hand-
annotated or manually disambiguated from the
parse forest produced by the parser. While most
of the hand-annotated treebanks contain only gold
trees, treebanks constructed from parser outputs
include both preferred and non-preferred analy-
ses. Some treebanking environments (such as
the SRI Cambridge TreeBanker (Carter, 1997) or
[incr tsdb()] (Oepen, 2001)) even record
the treebanking decisions (see section 2) that the
annotators take during manual annotation. These
treebanking decisions are, usually, stored in the
database/log files and used later for dynamic prop-
agation if a newer version of the grammar on the
same corpus is available (Oepen et al., 2002). But
until now, to our best knowledge, no research has
been reported on exploiting these decisions for
building a parse disambiguation model.
Previous research has adopted two approaches
to use treebanks for disambiguation models. One
approach, known as generative, uses only the gold
parse trees (Ersan and Charniak, 1995; Charniak,
2000). The other approach, known as discrimi-
native, uses both preferred trees and non-preferred
trees (Johnson et al., 1999; Toutanova et al., 2005).
In this latter approach, features such as local con-
figurations (i.e., local sub-trees), grandparents, n-
grams, etc., are extracted from all the trees and
are utilized to build the model. Neither of the ap-
proaches considers cognitive aspects of treebank-
ing, i.e. the fine-grained decision-making process
of the human annotators.
In this paper, we present our ongoing study of
using treebanking decisions for building a parse
disambiguation model. We present comparative
analyses among the features extracted using tree-
banking decisions and the state-of-the-art feature
types. We highlight how differently these features
scale when they are tested on out-of-domain data.
Our results demonstrate that features extracted us-
ing treebanking decisions are more efficient, in-
formative and robust, despite the total number of
these features being much less than that of the tra-
ditional feature types.
The rest of this paper is organised as follows
— section 2 presents some motivation along with
definition of treebanking decisions. Section 3 de-
scribes the feature extraction templates that have
been used for treebanking decisions. Section 4 ex-
plains the experimental data, results and analyses.
Section 5 concludes the paper with an outline of
our future research.
</bodyText>
<sectionHeader confidence="0.904562" genericHeader="method">
2 Treebanking decisions
</sectionHeader>
<bodyText confidence="0.999527">
One of the defining characteristics of Redwoods-
style treebanks1 (Oepen et al., 2002) is that the
candidate trees are constructed automatically by
</bodyText>
<footnote confidence="0.999535">
1More details available in http://redwoods.stanford.edu.
</footnote>
<page confidence="0.963836">
226
</page>
<note confidence="0.4270815">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 226–229,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<figure confidence="0.935336222222222">
D1 SUBJH the dog  ||barks
D2 HSPEC the  ||dog barks
D3 FRAG_NP the dog barks
D4 HSPEC the  ||dog
D5 NOUN_N_CMPND dog  ||barks
. . . . . .
D6 PLUR_NOUN_ORULE barks
D7 v_-_le barks
D8 n_-_mc_le barks
</figure>
<figureCaption confidence="0.999957">
Figure 1: Example forest and discriminants
</figureCaption>
<bodyText confidence="0.999986321428572">
the grammar, and then manually disambiguated by
human annotators. In doing so, linguistically rich
annotation is built efficiently with minimum man-
ual labor. In order to further improve the manual
disambiguation efficiency, systems like [incr
tsdb()] computes the difference between can-
didate analyses. Instead of looking at the huge
parse forest, the treebank annotators select or re-
ject the features that distinguish between different
parses, until only one parse remains. The number
of decisions for each sentence is normally around
lo92(n) where n is the total number of candidate
trees. For a sentence with 5000 candidate read-
ings, only about 12 treebanking decisions are re-
quired for a complete disambiguation. A similar
method was also proposed in (Carter, 1997).
Formally, a feature that distinguishes between
different parses is called a discriminant. For
Redwoods-style treebanks, this is usually ex-
tracted from the syntactic derivation tree of the
Head-driven Phrase Structure Grammar (HPSG)
analyses. Figure 1 shows a set of example dis-
criminants based on the two candidate trees.
A choice (acceptance or rejection, either manu-
ally annotated or inferred by the system) made on
a discriminant is called a decision. In the above
example, suppose the annotator decides to accept
the binary structure the dog  ||barks as a subject-
head construction and assigns a value yes to dis-
criminant D1, the remaining discriminants will
also receive inferred values by deduction (no for
D2, no for D3, yes for D4, etc). These decisions
are stored and used for dynamic evolution of the
treebank along with the grammar development.
Treebank decisions (especially those made by
annotators) are of particular interest to our study
of parse disambiguation. The decisions record the
fine-grained human judgements in the manual dis-
ambiguation process. This is different from the
traditional use of treebanks to build parse selec-
tion models, where a marked gold tree is picked
from the parse forest without concerning detailed
selection steps. Recent study on double annotated
treebanks (Kordoni and Zhang, 2009) shows that
annotators tend to start with the decisions with the
most certainty, and delay the “hard” decisions as
much as possible. As the decision process goes,
many of the “hard” discriminants will receive an
inferred value from the certain decisions. This
greedy approach helps to guarantee high inter-
annotator agreement. Concerning the statistical
parse selection models, the discriminative nature
of these treebanking decisions suggests that they
are highly effective features, and if properly used,
they will contribute to an efficient disambiguation
model.
</bodyText>
<sectionHeader confidence="0.978132333333333" genericHeader="method">
3 Treebanking Decisions as
Discriminative Disambiguation
Features
</sectionHeader>
<bodyText confidence="0.993334">
We use three types of feature templates for tree-
banking decisions for feature extraction. We refer
to the features extracted using these templates as
TDF (Treebanking Decision Feature) in the rest of
this paper. The feature templates are
</bodyText>
<listItem confidence="0.616405333333333">
T1: discriminant + lexical types of the yield
T2: discriminant + rule(left-child)2 + rule(right-child)
T3: instances of T2 + rule(parent) + rule(siblings)
</listItem>
<bodyText confidence="0.9268646">
TDFs of T1, T2 and T3 in combination are re-
ferred to as TDFC or TDFs with context. For
example in Figure 1, instance of T1 for the
discriminant D4 is “HSPEC3 + le_type(the)4 +
le_type(dog)&amp;quot;; instance of T2 is “HSPEC + rule(
</bodyText>
<equation confidence="0.866582">
DET) + rule(N) &amp;quot;; and instance of T3 is “HSPEC +
rule(DET ) + rule(N) + rule(S) + rule(VP)&amp;quot;.
</equation>
<bodyText confidence="0.999532833333333">
A TDF represents partial information about the
right parse tree (as most usual features). But in
some way, it also indicates that it was a point of
a decision (point of ambiguity with respect to the
underlying pre-processing grammar), hence carry-
ing some extra bit of information. TDFs allow to
</bodyText>
<footnote confidence="0.9632922">
2rule(X) represents the HPSG rule, applied on X, ex-
tracted from the corresponding derivation tree.
3HSPEC is the head-specifier rule in HPSG
4le_type(X) denotes the abstract lexical type of word X
inside the grammar.
</footnote>
<page confidence="0.994667">
227
</page>
<bodyText confidence="0.99998375">
omit certain details inside the features by encod-
ing useful purposes of relationships between lexi-
cal types of the words and their distant grandpar-
ents without considering nodes in the intermediate
levels (allowing some kind of underspecification).
In contrast, state-of-the-art feature types contain
all the nodes in the corresponding branches of
the tree. While they encode ancestor information
(through grandparenting), but they ignore siblings.
TDFs include siblings along with ancestor. Unlike
traditional features, which are generated from all
possible matches (which is huge) of feature types
followed by some frequency cut-offs, the selection
of TDFs is directly restricted by the small num-
ber of treebanking decisions themselves and ex-
haustive search is not needed. It should be noted
that, we do not use treebanking decisions made for
the parse forest of one sentence to extract features
from the parse forest of another sentence. That is
why, the number of TDFs is much smaller than
that of traditional features. This also ensures that
TDFs are highly correlated to the corresponding
constructions and corresponding sentences from
where they are extracted.
</bodyText>
<sectionHeader confidence="0.998522" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.960492">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999933647058823">
We use a collection of 8593 English sentences
from the LOGON corpus (Oepen et al., 2004) for
our experiment. 874 of them are kept as test items
and the remaining 7719 items are used for train-
ing. The sentences have an average length of 14.68
and average number of 203.26 readings per sen-
tence. The out-of-domain data are a set of 531
English Wikipedia sentences from WeScience cor-
pus (Ytrestøl et al., 2009).
Previous studies (Toutanova et al., 2005; Os-
borne and Baldridge, 2004) have reported rela-
tively high exact match accuracy with earlier ver-
sions of ERG (Flickinger, 2000) on datasets with
very short sentences. With much higher structural
ambiguities in LOGON and WeScience sentences,
the overall disambiguation accuracy drops signifi-
cantly.
</bodyText>
<subsectionHeader confidence="0.9704545">
4.2 Experimental setup and evaluation
measures
</subsectionHeader>
<bodyText confidence="0.999968888888889">
The goal of our experiments is to compare var-
ious types of features (with TDF) in terms of
efficiency, informativeness, and robustness. To
compare among the feature types, we build log-
linear training models (Johnson et al., 1999) for
parse selection (which is standard for unification-
based grammars) for TDFC, local configurations,
n-grams and active edges5. For each model, we
calculate the following evaluation metrics —
</bodyText>
<listItem confidence="0.895505823529412">
• Exact (match) accuracy: it is simply the percentage
of times that the top-ranked analysis for each test sen-
tences is identical with the gold analysis of the same
sentence.
• 5-best (match) accuracy: it is the percentage of times
that the five top-ranked analyses for each of the sen-
tences contain the gold analysis.
• Feature Hit Count (FHC): it is the total number of oc-
currences of the features (of a particular feature type)
inside all the syntactic analyses for all the test sen-
tences. So, for example, if a feature (of a particular
feature type) is observed 100 times, then these 100 oc-
currences are added to the total FHC.
• Feature Type Hit Count (FTHC): it is the total num-
ber of distinct features (of the corresponding feature
type) observed inside the syntactic analyses of all the
test sentences.
</listItem>
<bodyText confidence="0.99934375">
While exact and 5-best match measures show
relative informativeness and robustness of the fea-
ture types, FHC and FTHC provide a more com-
prehensive picture of relative efficiencies.
</bodyText>
<subsectionHeader confidence="0.99207">
4.3 Results and discussion
</subsectionHeader>
<bodyText confidence="0.999980190476191">
As we can see in Table 1, local configurations
achieve highest accuracy among the traditional
feature types. They also use higher number of fea-
tures (almost 2.7 millions). TDFC do better than
both n-grams and active edges, even with a lower
number of features. Though, local configurations
gain more accuracy than TDFC, but they do so at
a cost of 50 times higher number of features. This
indicates that features extracted using treebanking
decisions are more informative.
For out-of-domain data (Table 1), there is a big
drop of accuracy for local configurations. Active
edges and TDFC also have some accuracy drop.
Surprisingly, n-grams do better with our out-of-
domain data than in-domain, but still that accuracy
is close to that of TDFC. Note that n-grams have
8 times higher number of features than TDFC.
Hence, according to these results, TDFC are more
robust, for out-of-domain data, than local config-
urations and active edges, and almost as good as
n-grams.
</bodyText>
<footnote confidence="0.9983435">
5Active edges correspond to the branches (i.e. one daugh-
ter in turn) of the local sub-trees.
</footnote>
<page confidence="0.986205">
228
</page>
<table confidence="0.999843833333333">
Feature Total 5-best accuracy 5-best accuracy Exact accuracy Exact accuracy
template features (in-domain) (out-of-domain) (in-domain) (out-of-domain)
n-gram 438,844 68.19% 62.71% 41.30% 42.37%
local configuration 2,735,486 75.51% 64.22% 50.69% 44.44%
active edges 89,807 68.99% 61.77% 41.88% 39.92%
TDFC 53,362 70.94% 62.71% 43.59% 41.05%
</table>
<tableCaption confidence="0.998724">
Table 1: Accuracies obtained on both in-domain and out-of-domain data using n-grams (n=4), local
configurations (with grandparenting level 3), active edges and TDFC.
</tableCaption>
<table confidence="0.999940833333333">
Feature FHC FTHC Active
template features
n-gram 18,245,558 32,425 7.39%
local config. 62,060,610 357,150 13.06%
active edges 22,902,404 27,540 30.67%
TDFC 21,719,698 17,818 33.39%
</table>
<tableCaption confidence="0.981491">
Table 2: FHC and FTHC calculated for in-domain
</tableCaption>
<bodyText confidence="0.924291">
data.
The most important aspect of TDFC is that they
are more efficient than their traditional counter-
parts (Table 2). They have significantly higher
number of active features( FTHC ) than n-
</bodyText>
<subsubsectionHeader confidence="0.45197">
TotalFeature#
</subsubsectionHeader>
<bodyText confidence="0.885264">
grams and local configurations.
</bodyText>
<sectionHeader confidence="0.996936" genericHeader="discussions">
5 Future work
</sectionHeader>
<bodyText confidence="0.999995611111111">
The results of the experiments described in this pa-
per indicate a good prospect for utilizing treebank-
ing decisions, although, we think that the types of
feature templates that we are using for them are
not yet fully conveying cognitive knowledge of the
annotators, in which we are specifically interested
in. For instance, we expect to model human dis-
ambiguation process more accurately by focusing
only on human annotators’ decisions (instead of
only inferred decisions). Such a model will not
only improve the performance of the parsing sys-
tem at hand, but can also be applied interactively
in treebanking projects to achieve better annota-
tion speed (e.g., by ranking the promising discrim-
inants higher to help annotators make correct de-
cisions). Future experiments will also investigate
whether any pattern of discriminant selection by
the humans can be learnt from these decisions.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9985135">
David Carter. 1997. The treebanker: A tool for supervised
training of parsed corpora. In Proceedings of the Work-
shop on Computational Environments for Grammar De-
velopment and Linguistic Engineering, Madrid, Spain.
Eugene Charniak. 2000. A maximum entropy-based parser.
In Proceedings of the 1st Annual Meeting of the North
American Chapter of Association for Computational Lin-
guistics (NAACL 2000), pages 132–139, Seattle, USA.
Murat Ersan and Eugene Charniak. 1995. A statistical syn-
tactic disambiguation program and what it learns. pages
146–159.
Dan Flickinger. 2000. On building a more efficient grammar
by exploiting types. 6(1):15–28.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
unifcation-based grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computational Lin-
guistics (ACL 1999), pages 535–541, Maryland, USA.
Valia Kordoni and Yi Zhang. 2009. Annotating wall street
journal texts using a hand-crafted deep linguistic gram-
mar. In Proceedings of The Third Linguistic Annotation
Workshop (LAW III), Singapore.
Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christo-
pher Manning, Dan Flickinger, and Thorsten Brants.
2002. The LinGO Redwoods treebank: motivation and
preliminary applications. In Proceedings of COLING
2002: The 17th International Conference on Computa-
tional Linguistics: Project Notes, Taipei, Taiwan.
Stephan Oepen, Helge Dyvik, Jan Tore Lønning, Erik Vell-
dal, Dorothee Beermann, John Carroll, Dan Flickinger,
Lars Hellan, Janne Bondi Johannessen, Paul Meurer, Tor-
bjørn Nordgård, and Victoria Roslin. 2004. Som å kapp-
ete med trollet? towards mrs-based norwegian-english
machine translation. In Proceedings of the 10th Interna-
tional Conference on Theoretical and Methodological Is-
sues in Machine Translation, pages 11–20, MD, USA.
Stephan Oepen. 2001. [incr tsdb()] — competence and
performance laboratory. User manual. Technical report,
Computational Linguistics, Saarland University, Saar-
brücken, Germany.
Miles Osborne and Jason Baldridge. 2004. Ensemble-based
active learning for parse selection. In HLT-NAACL 2004:
Main Proceedings, pages 89–96, Boston, USA.
Kristina Toutanova, Christoper D. Manning, Dan Flickinger,
and Stephan Oepen. 2005. Stochastic HPSG parse selec-
tion using the Redwoods corpus. Journal of Research on
Language and Computation, 3(1):83–105.
Gisle Ytrestøl, Stephan Oepen, and Daniel Flickinger. 2009.
Extracting and annotating wikipedia sub-domains. In Pro-
ceedings of the 7th International Workshop on Treebanks
and Linguistic Theories, pages 185–197, Groningen, the
Netherlands.
</reference>
<page confidence="0.998866">
229
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.254077">
<title confidence="0.999936">Using Treebanking Discriminants as Parse Disambiguation Features</title>
<author confidence="0.972294">Faisal Mahbub Yi Valia</author>
<address confidence="0.451495">of Computational Linguistics, Saarland</address>
<affiliation confidence="0.798424">of Computational Linguistics, Saarland University and DFKI GmbH,</affiliation>
<email confidence="0.988817">chowd@coli.uni-sb.de</email>
<email confidence="0.988817">yzhang@coli.uni-sb.de</email>
<email confidence="0.988817">kordoni@coli.uni-sb.de</email>
<abstract confidence="0.99483635">This paper presents a novel approach of incorporating fine-grained treebanking decisions made by human annotators as discriminative features for automatic parse disambiguation. To our best knowledge, this is the first work that exploits treebanking decisions for this task. The advantage of this approach is that use of human judgements is made. The paper presents comparative analyses of the performance of discriminative models built using treebanking decisions and state-of-the-art features. We also highlight how differently these features scale when these models are tested on out-of-domain data. We show that, features extracted using treebanking decisions are more efficient, informative and robust compared to traditional features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Carter</author>
</authors>
<title>The treebanker: A tool for supervised training of parsed corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the Workshop on Computational Environments for Grammar Development and Linguistic Engineering,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="1489" citStr="Carter, 1997" startWordPosition="206" endWordPosition="207">ls are tested on out-of-domain data. We show that, features extracted using treebanking decisions are more efficient, informative and robust compared to traditional features. 1 Introduction State-of-the-art parse disambiguation models are trained on treebanks, which are either fully handannotated or manually disambiguated from the parse forest produced by the parser. While most of the hand-annotated treebanks contain only gold trees, treebanks constructed from parser outputs include both preferred and non-preferred analyses. Some treebanking environments (such as the SRI Cambridge TreeBanker (Carter, 1997) or [incr tsdb()] (Oepen, 2001)) even record the treebanking decisions (see section 2) that the annotators take during manual annotation. These treebanking decisions are, usually, stored in the database/log files and used later for dynamic propagation if a newer version of the grammar on the same corpus is available (Oepen et al., 2002). But until now, to our best knowledge, no research has been reported on exploiting these decisions for building a parse disambiguation model. Previous research has adopted two approaches to use treebanks for disambiguation models. One approach, known as generat</context>
<context position="4972" citStr="Carter, 1997" startWordPosition="739" endWordPosition="740">. In order to further improve the manual disambiguation efficiency, systems like [incr tsdb()] computes the difference between candidate analyses. Instead of looking at the huge parse forest, the treebank annotators select or reject the features that distinguish between different parses, until only one parse remains. The number of decisions for each sentence is normally around lo92(n) where n is the total number of candidate trees. For a sentence with 5000 candidate readings, only about 12 treebanking decisions are required for a complete disambiguation. A similar method was also proposed in (Carter, 1997). Formally, a feature that distinguishes between different parses is called a discriminant. For Redwoods-style treebanks, this is usually extracted from the syntactic derivation tree of the Head-driven Phrase Structure Grammar (HPSG) analyses. Figure 1 shows a set of example discriminants based on the two candidate trees. A choice (acceptance or rejection, either manually annotated or inferred by the system) made on a discriminant is called a decision. In the above example, suppose the annotator decides to accept the binary structure the dog ||barks as a subjecthead construction and assigns a </context>
</contexts>
<marker>Carter, 1997</marker>
<rawString>David Carter. 1997. The treebanker: A tool for supervised training of parsed corpora. In Proceedings of the Workshop on Computational Environments for Grammar Development and Linguistic Engineering, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum entropy-based parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Annual Meeting of the North American Chapter of Association for Computational Linguistics (NAACL</booktitle>
<pages>132--139</pages>
<location>Seattle, USA.</location>
<contexts>
<context position="2167" citStr="Charniak, 2000" startWordPosition="311" endWordPosition="312">cisions (see section 2) that the annotators take during manual annotation. These treebanking decisions are, usually, stored in the database/log files and used later for dynamic propagation if a newer version of the grammar on the same corpus is available (Oepen et al., 2002). But until now, to our best knowledge, no research has been reported on exploiting these decisions for building a parse disambiguation model. Previous research has adopted two approaches to use treebanks for disambiguation models. One approach, known as generative, uses only the gold parse trees (Ersan and Charniak, 1995; Charniak, 2000). The other approach, known as discriminative, uses both preferred trees and non-preferred trees (Johnson et al., 1999; Toutanova et al., 2005). In this latter approach, features such as local configurations (i.e., local sub-trees), grandparents, ngrams, etc., are extracted from all the trees and are utilized to build the model. Neither of the approaches considers cognitive aspects of treebanking, i.e. the fine-grained decision-making process of the human annotators. In this paper, we present our ongoing study of using treebanking decisions for building a parse disambiguation model. We present</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum entropy-based parser. In Proceedings of the 1st Annual Meeting of the North American Chapter of Association for Computational Linguistics (NAACL 2000), pages 132–139, Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murat Ersan</author>
<author>Eugene Charniak</author>
</authors>
<title>A statistical syntactic disambiguation program and what it learns.</title>
<date>1995</date>
<pages>146--159</pages>
<contexts>
<context position="2150" citStr="Ersan and Charniak, 1995" startWordPosition="307" endWordPosition="310"> record the treebanking decisions (see section 2) that the annotators take during manual annotation. These treebanking decisions are, usually, stored in the database/log files and used later for dynamic propagation if a newer version of the grammar on the same corpus is available (Oepen et al., 2002). But until now, to our best knowledge, no research has been reported on exploiting these decisions for building a parse disambiguation model. Previous research has adopted two approaches to use treebanks for disambiguation models. One approach, known as generative, uses only the gold parse trees (Ersan and Charniak, 1995; Charniak, 2000). The other approach, known as discriminative, uses both preferred trees and non-preferred trees (Johnson et al., 1999; Toutanova et al., 2005). In this latter approach, features such as local configurations (i.e., local sub-trees), grandparents, ngrams, etc., are extracted from all the trees and are utilized to build the model. Neither of the approaches considers cognitive aspects of treebanking, i.e. the fine-grained decision-making process of the human annotators. In this paper, we present our ongoing study of using treebanking decisions for building a parse disambiguation </context>
</contexts>
<marker>Ersan, Charniak, 1995</marker>
<rawString>Murat Ersan and Eugene Charniak. 1995. A statistical syntactic disambiguation program and what it learns. pages 146–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<pages>6--1</pages>
<contexts>
<context position="9927" citStr="Flickinger, 2000" startWordPosition="1526" endWordPosition="1527">y are extracted. 4 Experiment 4.1 Data We use a collection of 8593 English sentences from the LOGON corpus (Oepen et al., 2004) for our experiment. 874 of them are kept as test items and the remaining 7719 items are used for training. The sentences have an average length of 14.68 and average number of 203.26 readings per sentence. The out-of-domain data are a set of 531 English Wikipedia sentences from WeScience corpus (Ytrestøl et al., 2009). Previous studies (Toutanova et al., 2005; Osborne and Baldridge, 2004) have reported relatively high exact match accuracy with earlier versions of ERG (Flickinger, 2000) on datasets with very short sentences. With much higher structural ambiguities in LOGON and WeScience sentences, the overall disambiguation accuracy drops significantly. 4.2 Experimental setup and evaluation measures The goal of our experiments is to compare various types of features (with TDF) in terms of efficiency, informativeness, and robustness. To compare among the feature types, we build loglinear training models (Johnson et al., 1999) for parse selection (which is standard for unificationbased grammars) for TDFC, local configurations, n-grams and active edges5. For each model, we calc</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Dan Flickinger. 2000. On building a more efficient grammar by exploiting types. 6(1):15–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic unifcation-based grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>535--541</pages>
<location>Maryland, USA.</location>
<contexts>
<context position="2285" citStr="Johnson et al., 1999" startWordPosition="327" endWordPosition="330">ly, stored in the database/log files and used later for dynamic propagation if a newer version of the grammar on the same corpus is available (Oepen et al., 2002). But until now, to our best knowledge, no research has been reported on exploiting these decisions for building a parse disambiguation model. Previous research has adopted two approaches to use treebanks for disambiguation models. One approach, known as generative, uses only the gold parse trees (Ersan and Charniak, 1995; Charniak, 2000). The other approach, known as discriminative, uses both preferred trees and non-preferred trees (Johnson et al., 1999; Toutanova et al., 2005). In this latter approach, features such as local configurations (i.e., local sub-trees), grandparents, ngrams, etc., are extracted from all the trees and are utilized to build the model. Neither of the approaches considers cognitive aspects of treebanking, i.e. the fine-grained decision-making process of the human annotators. In this paper, we present our ongoing study of using treebanking decisions for building a parse disambiguation model. We present comparative analyses among the features extracted using treebanking decisions and the state-of-the-art feature types.</context>
<context position="10374" citStr="Johnson et al., 1999" startWordPosition="1591" endWordPosition="1594">09). Previous studies (Toutanova et al., 2005; Osborne and Baldridge, 2004) have reported relatively high exact match accuracy with earlier versions of ERG (Flickinger, 2000) on datasets with very short sentences. With much higher structural ambiguities in LOGON and WeScience sentences, the overall disambiguation accuracy drops significantly. 4.2 Experimental setup and evaluation measures The goal of our experiments is to compare various types of features (with TDF) in terms of efficiency, informativeness, and robustness. To compare among the feature types, we build loglinear training models (Johnson et al., 1999) for parse selection (which is standard for unificationbased grammars) for TDFC, local configurations, n-grams and active edges5. For each model, we calculate the following evaluation metrics — • Exact (match) accuracy: it is simply the percentage of times that the top-ranked analysis for each test sentences is identical with the gold analysis of the same sentence. • 5-best (match) accuracy: it is the percentage of times that the five top-ranked analyses for each of the sentences contain the gold analysis. • Feature Hit Count (FHC): it is the total number of occurrences of the features (of a p</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic unifcation-based grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL 1999), pages 535–541, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valia Kordoni</author>
<author>Yi Zhang</author>
</authors>
<title>Annotating wall street journal texts using a hand-crafted deep linguistic grammar.</title>
<date>2009</date>
<booktitle>In Proceedings of The Third Linguistic Annotation Workshop (LAW III),</booktitle>
<contexts>
<context position="6302" citStr="Kordoni and Zhang, 2009" startWordPosition="945" endWordPosition="948">no for D2, no for D3, yes for D4, etc). These decisions are stored and used for dynamic evolution of the treebank along with the grammar development. Treebank decisions (especially those made by annotators) are of particular interest to our study of parse disambiguation. The decisions record the fine-grained human judgements in the manual disambiguation process. This is different from the traditional use of treebanks to build parse selection models, where a marked gold tree is picked from the parse forest without concerning detailed selection steps. Recent study on double annotated treebanks (Kordoni and Zhang, 2009) shows that annotators tend to start with the decisions with the most certainty, and delay the “hard” decisions as much as possible. As the decision process goes, many of the “hard” discriminants will receive an inferred value from the certain decisions. This greedy approach helps to guarantee high interannotator agreement. Concerning the statistical parse selection models, the discriminative nature of these treebanking decisions suggests that they are highly effective features, and if properly used, they will contribute to an efficient disambiguation model. 3 Treebanking Decisions as Discrimi</context>
</contexts>
<marker>Kordoni, Zhang, 2009</marker>
<rawString>Valia Kordoni and Yi Zhang. 2009. Annotating wall street journal texts using a hand-crafted deep linguistic grammar. In Proceedings of The Third Linguistic Annotation Workshop (LAW III), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Kristina Toutanova</author>
<author>Stuart Shieber</author>
<author>Christopher Manning</author>
<author>Dan Flickinger</author>
<author>Thorsten Brants</author>
</authors>
<title>The LinGO Redwoods treebank: motivation and preliminary applications.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING 2002: The 17th International Conference on Computational Linguistics: Project Notes,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1827" citStr="Oepen et al., 2002" startWordPosition="258" endWordPosition="261"> parse forest produced by the parser. While most of the hand-annotated treebanks contain only gold trees, treebanks constructed from parser outputs include both preferred and non-preferred analyses. Some treebanking environments (such as the SRI Cambridge TreeBanker (Carter, 1997) or [incr tsdb()] (Oepen, 2001)) even record the treebanking decisions (see section 2) that the annotators take during manual annotation. These treebanking decisions are, usually, stored in the database/log files and used later for dynamic propagation if a newer version of the grammar on the same corpus is available (Oepen et al., 2002). But until now, to our best knowledge, no research has been reported on exploiting these decisions for building a parse disambiguation model. Previous research has adopted two approaches to use treebanks for disambiguation models. One approach, known as generative, uses only the gold parse trees (Ersan and Charniak, 1995; Charniak, 2000). The other approach, known as discriminative, uses both preferred trees and non-preferred trees (Johnson et al., 1999; Toutanova et al., 2005). In this latter approach, features such as local configurations (i.e., local sub-trees), grandparents, ngrams, etc.,</context>
<context position="3677" citStr="Oepen et al., 2002" startWordPosition="539" endWordPosition="542">e more efficient, informative and robust, despite the total number of these features being much less than that of the traditional feature types. The rest of this paper is organised as follows — section 2 presents some motivation along with definition of treebanking decisions. Section 3 describes the feature extraction templates that have been used for treebanking decisions. Section 4 explains the experimental data, results and analyses. Section 5 concludes the paper with an outline of our future research. 2 Treebanking decisions One of the defining characteristics of Redwoodsstyle treebanks1 (Oepen et al., 2002) is that the candidate trees are constructed automatically by 1More details available in http://redwoods.stanford.edu. 226 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 226–229, Paris, October 2009. c�2009 Association for Computational Linguistics D1 SUBJH the dog ||barks D2 HSPEC the ||dog barks D3 FRAG_NP the dog barks D4 HSPEC the ||dog D5 NOUN_N_CMPND dog ||barks . . . . . . D6 PLUR_NOUN_ORULE barks D7 v_-_le barks D8 n_-_mc_le barks Figure 1: Example forest and discriminants the grammar, and then manually disambiguated by human annotators. In doing</context>
</contexts>
<marker>Oepen, Toutanova, Shieber, Manning, Flickinger, Brants, 2002</marker>
<rawString>Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher Manning, Dan Flickinger, and Thorsten Brants. 2002. The LinGO Redwoods treebank: motivation and preliminary applications. In Proceedings of COLING 2002: The 17th International Conference on Computational Linguistics: Project Notes, Taipei, Taiwan.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stephan Oepen</author>
<author>Helge Dyvik</author>
<author>Jan Tore Lønning</author>
<author>Erik Velldal</author>
<author>Dorothee Beermann</author>
<author>John Carroll</author>
<author>Dan Flickinger</author>
<author>Lars Hellan</author>
<author>Janne Bondi Johannessen</author>
<author>Paul Meurer</author>
<author>Torbjørn Nordgård</author>
<author>Victoria Roslin</author>
</authors>
<title>Som å kappete med trollet? towards mrs-based norwegian-english machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>11--20</pages>
<location>MD, USA.</location>
<contexts>
<context position="9437" citStr="Oepen et al., 2004" startWordPosition="1440" endWordPosition="1443">is directly restricted by the small number of treebanking decisions themselves and exhaustive search is not needed. It should be noted that, we do not use treebanking decisions made for the parse forest of one sentence to extract features from the parse forest of another sentence. That is why, the number of TDFs is much smaller than that of traditional features. This also ensures that TDFs are highly correlated to the corresponding constructions and corresponding sentences from where they are extracted. 4 Experiment 4.1 Data We use a collection of 8593 English sentences from the LOGON corpus (Oepen et al., 2004) for our experiment. 874 of them are kept as test items and the remaining 7719 items are used for training. The sentences have an average length of 14.68 and average number of 203.26 readings per sentence. The out-of-domain data are a set of 531 English Wikipedia sentences from WeScience corpus (Ytrestøl et al., 2009). Previous studies (Toutanova et al., 2005; Osborne and Baldridge, 2004) have reported relatively high exact match accuracy with earlier versions of ERG (Flickinger, 2000) on datasets with very short sentences. With much higher structural ambiguities in LOGON and WeScience sentenc</context>
</contexts>
<marker>Oepen, Dyvik, Lønning, Velldal, Beermann, Carroll, Flickinger, Hellan, Johannessen, Meurer, Nordgård, Roslin, 2004</marker>
<rawString>Stephan Oepen, Helge Dyvik, Jan Tore Lønning, Erik Velldal, Dorothee Beermann, John Carroll, Dan Flickinger, Lars Hellan, Janne Bondi Johannessen, Paul Meurer, Torbjørn Nordgård, and Victoria Roslin. 2004. Som å kappete med trollet? towards mrs-based norwegian-english machine translation. In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation, pages 11–20, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
</authors>
<title>incr tsdb()] — competence and performance laboratory. User manual.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>Computational Linguistics, Saarland University,</institution>
<location>Saarbrücken, Germany.</location>
<contexts>
<context position="1520" citStr="Oepen, 2001" startWordPosition="211" endWordPosition="212">ata. We show that, features extracted using treebanking decisions are more efficient, informative and robust compared to traditional features. 1 Introduction State-of-the-art parse disambiguation models are trained on treebanks, which are either fully handannotated or manually disambiguated from the parse forest produced by the parser. While most of the hand-annotated treebanks contain only gold trees, treebanks constructed from parser outputs include both preferred and non-preferred analyses. Some treebanking environments (such as the SRI Cambridge TreeBanker (Carter, 1997) or [incr tsdb()] (Oepen, 2001)) even record the treebanking decisions (see section 2) that the annotators take during manual annotation. These treebanking decisions are, usually, stored in the database/log files and used later for dynamic propagation if a newer version of the grammar on the same corpus is available (Oepen et al., 2002). But until now, to our best knowledge, no research has been reported on exploiting these decisions for building a parse disambiguation model. Previous research has adopted two approaches to use treebanks for disambiguation models. One approach, known as generative, uses only the gold parse t</context>
</contexts>
<marker>Oepen, 2001</marker>
<rawString>Stephan Oepen. 2001. [incr tsdb()] — competence and performance laboratory. User manual. Technical report, Computational Linguistics, Saarland University, Saarbrücken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
<author>Jason Baldridge</author>
</authors>
<title>Ensemble-based active learning for parse selection.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Main Proceedings,</booktitle>
<pages>89--96</pages>
<location>Boston, USA.</location>
<contexts>
<context position="9828" citStr="Osborne and Baldridge, 2004" startWordPosition="1507" endWordPosition="1511"> that TDFs are highly correlated to the corresponding constructions and corresponding sentences from where they are extracted. 4 Experiment 4.1 Data We use a collection of 8593 English sentences from the LOGON corpus (Oepen et al., 2004) for our experiment. 874 of them are kept as test items and the remaining 7719 items are used for training. The sentences have an average length of 14.68 and average number of 203.26 readings per sentence. The out-of-domain data are a set of 531 English Wikipedia sentences from WeScience corpus (Ytrestøl et al., 2009). Previous studies (Toutanova et al., 2005; Osborne and Baldridge, 2004) have reported relatively high exact match accuracy with earlier versions of ERG (Flickinger, 2000) on datasets with very short sentences. With much higher structural ambiguities in LOGON and WeScience sentences, the overall disambiguation accuracy drops significantly. 4.2 Experimental setup and evaluation measures The goal of our experiments is to compare various types of features (with TDF) in terms of efficiency, informativeness, and robustness. To compare among the feature types, we build loglinear training models (Johnson et al., 1999) for parse selection (which is standard for unificatio</context>
</contexts>
<marker>Osborne, Baldridge, 2004</marker>
<rawString>Miles Osborne and Jason Baldridge. 2004. Ensemble-based active learning for parse selection. In HLT-NAACL 2004: Main Proceedings, pages 89–96, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christoper D Manning</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Stochastic HPSG parse selection using the Redwoods corpus.</title>
<date>2005</date>
<journal>Journal of Research on Language and Computation,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="2310" citStr="Toutanova et al., 2005" startWordPosition="331" endWordPosition="334">base/log files and used later for dynamic propagation if a newer version of the grammar on the same corpus is available (Oepen et al., 2002). But until now, to our best knowledge, no research has been reported on exploiting these decisions for building a parse disambiguation model. Previous research has adopted two approaches to use treebanks for disambiguation models. One approach, known as generative, uses only the gold parse trees (Ersan and Charniak, 1995; Charniak, 2000). The other approach, known as discriminative, uses both preferred trees and non-preferred trees (Johnson et al., 1999; Toutanova et al., 2005). In this latter approach, features such as local configurations (i.e., local sub-trees), grandparents, ngrams, etc., are extracted from all the trees and are utilized to build the model. Neither of the approaches considers cognitive aspects of treebanking, i.e. the fine-grained decision-making process of the human annotators. In this paper, we present our ongoing study of using treebanking decisions for building a parse disambiguation model. We present comparative analyses among the features extracted using treebanking decisions and the state-of-the-art feature types. We highlight how differe</context>
<context position="9798" citStr="Toutanova et al., 2005" startWordPosition="1503" endWordPosition="1506">tures. This also ensures that TDFs are highly correlated to the corresponding constructions and corresponding sentences from where they are extracted. 4 Experiment 4.1 Data We use a collection of 8593 English sentences from the LOGON corpus (Oepen et al., 2004) for our experiment. 874 of them are kept as test items and the remaining 7719 items are used for training. The sentences have an average length of 14.68 and average number of 203.26 readings per sentence. The out-of-domain data are a set of 531 English Wikipedia sentences from WeScience corpus (Ytrestøl et al., 2009). Previous studies (Toutanova et al., 2005; Osborne and Baldridge, 2004) have reported relatively high exact match accuracy with earlier versions of ERG (Flickinger, 2000) on datasets with very short sentences. With much higher structural ambiguities in LOGON and WeScience sentences, the overall disambiguation accuracy drops significantly. 4.2 Experimental setup and evaluation measures The goal of our experiments is to compare various types of features (with TDF) in terms of efficiency, informativeness, and robustness. To compare among the feature types, we build loglinear training models (Johnson et al., 1999) for parse selection (wh</context>
</contexts>
<marker>Toutanova, Manning, Flickinger, Oepen, 2005</marker>
<rawString>Kristina Toutanova, Christoper D. Manning, Dan Flickinger, and Stephan Oepen. 2005. Stochastic HPSG parse selection using the Redwoods corpus. Journal of Research on Language and Computation, 3(1):83–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gisle Ytrestøl</author>
<author>Stephan Oepen</author>
<author>Daniel Flickinger</author>
</authors>
<title>Extracting and annotating wikipedia sub-domains.</title>
<date>2009</date>
<booktitle>In Proceedings of the 7th International Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>185--197</pages>
<location>Groningen, the Netherlands.</location>
<contexts>
<context position="9756" citStr="Ytrestøl et al., 2009" startWordPosition="1497" endWordPosition="1500"> much smaller than that of traditional features. This also ensures that TDFs are highly correlated to the corresponding constructions and corresponding sentences from where they are extracted. 4 Experiment 4.1 Data We use a collection of 8593 English sentences from the LOGON corpus (Oepen et al., 2004) for our experiment. 874 of them are kept as test items and the remaining 7719 items are used for training. The sentences have an average length of 14.68 and average number of 203.26 readings per sentence. The out-of-domain data are a set of 531 English Wikipedia sentences from WeScience corpus (Ytrestøl et al., 2009). Previous studies (Toutanova et al., 2005; Osborne and Baldridge, 2004) have reported relatively high exact match accuracy with earlier versions of ERG (Flickinger, 2000) on datasets with very short sentences. With much higher structural ambiguities in LOGON and WeScience sentences, the overall disambiguation accuracy drops significantly. 4.2 Experimental setup and evaluation measures The goal of our experiments is to compare various types of features (with TDF) in terms of efficiency, informativeness, and robustness. To compare among the feature types, we build loglinear training models (Joh</context>
</contexts>
<marker>Ytrestøl, Oepen, Flickinger, 2009</marker>
<rawString>Gisle Ytrestøl, Stephan Oepen, and Daniel Flickinger. 2009. Extracting and annotating wikipedia sub-domains. In Proceedings of the 7th International Workshop on Treebanks and Linguistic Theories, pages 185–197, Groningen, the Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>