<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005458">
<title confidence="0.895691">
Mining Linguistically Interpreted Texts
</title>
<note confidence="0.535013">
Paulo Quaresma
Departamento de Informática,
Universidade de Évora, 7000
Évora - Portugal
</note>
<email confidence="0.944039">
{pq}@di.uevora.pt
</email>
<note confidence="0.7980706">
Cassiana Fagundes da Silva, Renata Vieira,
Fernando Santos Osório
PIPCA - Unisinos
Av. Unisinos, 950 - São Leopoldo, RS
Brasil – 93.022-000
</note>
<email confidence="0.943735">
{cassiana, renata, osorio}@exatas.unisinos.br
</email>
<sectionHeader confidence="0.992073" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999259">
This paper proposes and evaluates the use of
linguistic information in the pre-processing
phase of text mining tasks. We present several
experiments comparing our proposal for
selection of terms based on linguistic
knowledge with usual techniques applied in
the field. The results show that part of speech
information is useful for the pre-processing
phase of text categorization and clustering, as
an alternative for stop words and stemming.
</bodyText>
<sectionHeader confidence="0.99849" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974704545455">
Natural language texts can be viewed as
resources containing uniform data in such a way
that methods similar to those used in Data Base
Knowledge Extraction can be applied to them. The
adaptation of these methods to texts is known as
Text Mining (Tan, 1999). Machine learning
techniques are applied to document collections
aiming at extracting patterns that may be useful to
organize or recover information from the
collections. Tasks related to this area are text
categorization, clustering, summarization, and
information extraction. One of the first steps in
text mining tasks is the pre-processing of the
documents, as they need to be represented in a
more structured way.
Our work proposes a new technique to the pre-
processing phase of documents and we compare it
with usual pre-processing methods. We focus on
two text mining tasks, namely text categorization
and clustering. In the categorization task we
associate each document to a class from a pre-
defined set, in the clustering task the challenge is
to identify groups of similar documents without
being aware of pre-defined classes. Usually, the
pre-processing phase in these tasks are based on
the approach called bag-of-words, in which just
simple techniques are used to eliminate
uninteresting words and to reduce various
semantically related terms to the same root (stop-
words and stemming, respectively). As an
alternative, we propose the use of linguistic
information in the pre-processing phase, by
selecting words according to their category (nouns,
adjectives, proper names, verbs) and using its
canonical form. We ran a series of experiments to
evaluate this proposal over Brazilian Portuguese
texts.
This paper is organized as follows. Section 2
presents an overview of text mining. Section 3
presents the methods used for collecting the
linguistic knowledge used in the experiments. The
experiments themselves are described in Section 4.
Section 5 presents an analysis of the results and the
paper is concluded in Section 6.
</bodyText>
<sectionHeader confidence="0.965292" genericHeader="method">
2 Text Mining
</sectionHeader>
<bodyText confidence="0.999913322580645">
Text mining processes are usually divided in five
major phases: A) Document collection: consists of
the definition of the set of the documents from
which knowledge must be extracted. B) Pre-
processing: consists of a set of actions that
transform the set of documents in natural language
into a list of useful terms. C) Preparation and
selection of the data: consists in the identification
and selection of relevant terms form the pre-
processed ones. D) Knowledge Extraction: consists
of the application of machine learning techniques
to identify patterns that can classify or cluster the
documents in the collection. E) Evaluation and
interpretation of the results: consists of the
analysis of the results.
The pre-processing phase in text mining is
essential and usually very expensive and time
consuming. As texts are originally non-structured a
series of steps are required to represent them in a
format compatible with knowledge extraction
methods and tools. The usual techniques employed
in phase B are the use of a list of stop-words,
which are discarded from the original documents
and the use of stemming which reduces the words
to their root.
Having the proper tools to process Portuguese
texts, we investigate whether linguistic information
can have an impact on the results of the whole
process. In the next section we describe the tools
we used for acquiring the linguistic knowledge in
which we base our experiments.
</bodyText>
<sectionHeader confidence="0.966686" genericHeader="method">
3 Tools for acquiring linguistic knowledge
</sectionHeader>
<bodyText confidence="0.999980913043479">
The linguistic knowledge we use in the
experiments is based on the syntactic analysis
performed by the PALAVRAS parser (Bick,
2000). This Portuguese parser is robust enough to
always give an output even for incomplete or
incorrect sentences (which might be the case for
the type of documents used in text mining tasks). It
has a comparatively low percentage of errors (less
than 1% for word class and 3-4% for surface
syntax) (Bick, 2003). We also used another tool
that makes easier the extraction of features from
the analyzed texts: the Palavras Xtractor (Gasperin
et. al. 2003). This tool converts the parser output
into three XML files, containing: a) the list of all
words from the text and their identifier; b) morpho-
syntactic information for each word; c) the
sentence´s syntactic structures. Using XSL
(eXtensible Stylesheet Language)1 we can extract
specified terms from the texts, according to their
linguistic value. The resulting lists of terms
according to each combination are then passed to
phases C, D and E. The experiments are described
in detail in the next section.
</bodyText>
<sectionHeader confidence="0.998894" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.7022">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9998635">
The corpus used in the experiments is composed
by a subset of the NILC corpus (Núcleo
Interdisciplinar de Lingüística Computacional2)
containing 855 documents corresponding to
newspaper articles of Folha de São Paulo from
1994. These documents are related to five
newspaper sections: informatics, property, sports,
politics and tourism.
</bodyText>
<subsectionHeader confidence="0.996717">
4.2 Pre-processing techniques
</subsectionHeader>
<bodyText confidence="0.999726833333333">
We prepared three different versions of the
corpus (V1, V2 and V3) for 3-fold cross validation.
Each version is partitioned in different training and
testing parts, containing 2/3 and 1/3 of the
documents respectively.
For the experiments with the usual methods,
irrelevant terms (stop-words) were eliminated from
the documents, on the basis of a list of stop-words,
containing 476 terms (mainly articles, prepositions,
auxiliary verbs, pronouns, etc). The remaining
terms were stemmed according to Martin Porter’s
algorithm (Porter, 1980). Based on these
</bodyText>
<footnote confidence="0.918942">
1 Available in http://www.w3.org/Style/XSL/
2 Available in http://nilc.icmc.sc.usp.br/nilc/
</footnote>
<bodyText confidence="0.999795333333333">
techniques we generated a collection of pre-
processed documents called PD1.
To test our proposal we then pre-processed the
855 documents in a different way: we parsed all
texts of our corpus, generating the corresponding
XML files and extracted terms according to their
grammatical categories, using XSL. Based on these
techniques we generated a collection of pre-
processed documents called PD2.
</bodyText>
<subsubsectionHeader confidence="0.525488">
4.2.1 Other mining phases
</subsubsectionHeader>
<bodyText confidence="0.998387155555556">
All other text mining phases were equally
applied to both PD1 and PD2. We used relative
frequency for the selection of relevant terms. The
representation of the documents was according to
the vector space model. For the categorization task,
vectors corresponding to each class were built,
where the more frequent terms were selected. After
that, a global vector was composed. We also tested
with different numbers of terms in the global
vector (30, 60, 90, 120, 150). For the clustering
task we measured the similarity of the documents
using cosine. After calculating similarity of the
documents, the data was codified according to
format required by the machine learning tool Weka
(Witten, 2000). Weka is a collection of machine
learning algorithms for data mining tasks that
contains tools for data pre-processing,
classification, regression, clustering, association
rules, and visualization.
In this work the adopted machine learning
techniques are Decision Tree for the categorization
process and K-means for text clustering.
Decision Tree is a supervised learning algorithm
based on the recursive division of the training
examples in representative subsets, using the
metric of information gain. After the induction of a
classifying tree, it can be applied to new examples,
described with the same attributes of the training
examples.
K-means divides a group of objects in k groups
in a way that the resulting intracluster similarity is
high, but the intercluster similarity is low. The
similarity of groups is measured in respect to the
medium value of the objects in a group, which can
be seen as the center of gravity (centroid) of the
group. The parameters used to run k-means are the
default ones as suggested by the tool, seed 10 and
5 groups.
The evaluation of the results for the
categorization task is based on the classification
error, which was used to compare the results for
PD1 and PD2. For the clustering task the
evaluation of the results is given by recall and
precision, based on the generated confusion
matrices.
</bodyText>
<sectionHeader confidence="0.999884" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.946942">
5.1 Text Categorization
</subsectionHeader>
<bodyText confidence="0.999105428571429">
Table 1 shows the results for text categorization
of PD1, given by the average error rates
considering the three versions the corpus (V1, V2
and V3). We had around 20% of error for the
categorization task. We can see minor variations in
the results according to the size of the vectors. Best
results were obtained for 150 terms.
</bodyText>
<table confidence="0.987408">
Terms 30 60 90 120 150
Errors 21,64 21,99 20,47 20,35 19,77
</table>
<tableCaption confidence="0.999693">
Table 1: Average Classification Error for PD1%
</tableCaption>
<bodyText confidence="0.998684047619048">
Table 2 shows the results for different
grammatical combinations in PD2, while Figure 1
summarizes the lowest error rates found for PD1
and all groups of PD2. The group nouns and
adjectives presents the lower error rates of all
experiments (18,01). However, due to the small
size of the corpus, the improvement reported
between usual methods (18,01) and nouns-
adjectives (20,47), when considering the same
number of terms (90), are at 75-80% confidence
level only (t-test).
In general, the results show that the presence of
nouns is crucial, the worst classification errors are
based on groups that do not contain the category
nouns, and here the confidence level for the
differences reported reaches 95%. The groups
containing nouns present results comparable to
those found in the experiments based on usual
methods of pre-processing. The use of verbs, either
alone or with other grammatical groups is not an
interesting option.
</bodyText>
<table confidence="0.999497615384615">
Terms 30 60 90 120 150
Nouns 24,91 21,75 23,98 23,51 22,69
Nouns-adjec. 23,15 20,35 18,01 19,18 18,71
Nouns-adjec.- 20,82 22,92 20,94 21,05 21,17
proper names
Nouns-proper 24,09 24,56 22,80 22,45 22,80
names
Adjec.-proper 47,01 46,34 32,51 33,21 32,86
names
Verbs 63,73 62,33 57,75 58,45 55,64
Nouns-verbs 40 27,72 25,61 24,21 26,32
Nouns-verbs- 35,09 27,02 27,72 24,21 23,51
adjectives
</table>
<tableCaption confidence="0.99913">
Table 2: Average Classification Error for PD2
</tableCaption>
<bodyText confidence="0.998538">
It can be observed that usually the best results
are obtained when the documents are represented
by a larger number of terms (90, 120 and 150), for
the group nouns, however, the best results were
obtained for vectors containing just 60 terms.
</bodyText>
<figureCaption confidence="0.998387">
Figure 1: Lower error rates for PD1 and PD2
</figureCaption>
<bodyText confidence="0.999992555555556">
We looked at the terms resulting from different
selection methods and categories to check the
overlap among the groups. From PD1 to PD2
based on nouns and adjectives (the one with the
best results) we could see that we had around 50%
of different terms. That means that 50% of terms in
PD1 are terms included in the categories nouns and
adjectives and 50% of the terms selected on the
basis of stop-words and stemming are from other
grammatical categories. As adjectives added to
nouns improved the results, we checked adjectives
to figure out their significance. We found terms
such as Brazilian, electoral, multimedia, political.
Intuitively, these terms seem to be relevant for the
classes we had. Analysing the groups containing
verbs, we observed that the verbs are usually very
common or auxiliary verbs (such as to be, to have,
to say), therefore not relevant for classification.
</bodyText>
<subsectionHeader confidence="0.999284">
5.2 Text Clustering
</subsectionHeader>
<bodyText confidence="0.956212">
We tested our hypothesis through clustering
experiments for PD1 and variations of PD2. For
the experiments on clustering we used vectors
containing 150 features from V2 and we set k to 5
groups. The resulting confusion matrix for PD1 is
presented in Table 3.
Cl.0 Cl.1 Cl.2 Cl.3 Cl.4
Sp. 1 31 2 0 23
Prop. 2 0 4 0 51
Inf. 0 0 1 0 55
Pol. 0 0 2 39 16
Tour. 5 0 17 0 33
</bodyText>
<tableCaption confidence="0.996228">
Table 3: Confusion Matrix PD1 (150 terms)
</tableCaption>
<bodyText confidence="0.999512666666667">
Considering the larger group in each row and
column (highlighted in the table) as the intended
cluster for each class, the corresponding precision
is of 50,52%.
We repeated the same set of experiments for
PD2. We tested several grammatical groups, the
best result was related to nouns and proper names.
The results are shown in Tables 4. The
corresponding precision is 63,15%.
selection, since this information is provided by the
parser PALAVRAS. Other front for future work is
further tests for other languages.
</bodyText>
<table confidence="0.493391166666667">
Cl.0 Cl.1 Cl.2 Cl.3 Cl.4
Sp. 0 38 19 0 0
Prop. 11 0 44 1 1
Inf. 0 0 19 0 38
Pol. 0 1 20 36 0
Tour. 0 0 57 0 0
</table>
<tableCaption confidence="0.996974">
Table 4: Confusion Matrix PD2 (nouns + proper
</tableCaption>
<bodyText confidence="0.454813">
names, 150 terms)
</bodyText>
<sectionHeader confidence="0.999007" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999996476190476">
This paper presented a series of experiments
aiming at comparing our proposal of pre-
processing techniques based on linguistic
information with usual methods adopted for pre-
processing in text mining.
We find in the literature other alternative
proposals for the pre-processing phase of text
mining. (Gonçalves and Quaresma, 2003) use the
canonical form of the word instead stemming, for
European Portuguese. (Feldman et al, 1998)
proposes the use of compound terms as opposed to
single terms for text mining. Similarly, (Aizawa,
2001) uses morphological analysis to aid the
extraction of compound terms. Our approach
differs from those since we propose single terms
selection based on different part of speech
information.
The results show that a selection made solely on
the basis of category information produces results
at least as good as those produced by usual
methods (when the selection considers nouns and
adjectives or nouns and proper nouns) both in
categorization and clustering tasks. In the
categorization experiments we obtained the lowest
error rate for PD2 when the pre-processing phase
was based on the selection of nouns and adjectives,
18,01%. However, the second best score in the
case of categorization was achieved by the
traditional methods, 19,77%. Due to the small
corpus, further experiments are needed to verify
the statistical significance of the reported gains.
The results of the clustering experiments show a
difference in precision from 50,52% to 63,15%.
As we are planning to test our techniques with a
larger number of documents and consequently a
larger number of terms, we are considering
applying other machine-learning techniques such
as Support Vector Machines that are robust enough
to deal with a large number of terms. We are also
planning to apply more sophisticated linguistic
knowledge than just grammatical categories, as, for
instance, the use of noun phrases for terms
</bodyText>
<sectionHeader confidence="0.996501" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999434846153846">
Aizawa A., 2001. Linguistic Techniques to
Improve the Performance of Automatic Text
Categorization. Proc. of the Sixth Natural
Language Processing Pacific Rim Symposium,
pages 307-314.
Bick, E. 2000. The Parsing System PALAVRAS:
Automatic Gramatical Analysis of Porutugese in
a Constraint Grammar Framework. Århus
University. Århus: Århus University Press.
Bick, E. 2003. A Constraint Grammar Based
Question Answering System for Portuguese.
Proceedings of the 11º Portuguese Conference
on Artificial Intelligence, pages 414-418. LNAI
Springer Verlag.
Feldman R., et al. 1998. Text Mining at the Term
Level. Proc. of the Second European Symposium
on Principles of Data Mining and Knowledge
Discovery, pages 65-73. LNCS Springer.
Gasperin, C.; Vieira, R.; Goulart, R. and
Quaresma, P. 2003. Extracting XML Syntactic
Chunks from Portuguese Corpora. Proc. of the
TALN Workshop on Natural Language
Processing of Minority Languages and Small
Languages, pages 223-232. Batz-sur-Mer
France.
Gonçalves, T. and Quaresma, P. 2003. A prelimary
approach classification problem of Portuguese
juridical documents. Proceedings of the 11º
Portuguese Conference on Artificial Intelligence,
pages 435-444. LNAI Springer Verlag.
Porter, M. F. 1980. An Algorithm for Suffix
Stripping. Program, 14 no. 3, pages 130-137.
Tan, Ah-Hwee. 1999. Text mining: the state of the
art and the challenges. Proc. of the Pacific-Asia
Workshop on Knowledge Discovery from
Advanced Databases, pages 65-70, Beijing.
Witten, I. H. 2000. Data mining: Pratical Machine
Learning tools and techniques with Java
implementations. Academic Press.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.344981">
<title confidence="0.99998">Mining Linguistically Interpreted Texts</title>
<author confidence="0.999967">Paulo Quaresma</author>
<affiliation confidence="0.999843">Departamento de Informática,</affiliation>
<address confidence="0.886651">Universidade de Évora, 7000 Évora - Portugal</address>
<email confidence="0.984368">{pq}@di.uevora.pt</email>
<author confidence="0.7847845">Cassiana Fagundes da Silva</author>
<author confidence="0.7847845">Renata Vieira</author>
<author confidence="0.7847845">Fernando Santos Osório</author>
<affiliation confidence="0.759402">PIPCA - Unisinos</affiliation>
<address confidence="0.7929175">Av. Unisinos, 950 - São Leopoldo, RS Brasil – 93.022-000</address>
<email confidence="0.97144">cassiana@exatas.unisinos.br</email>
<email confidence="0.97144">renata@exatas.unisinos.br</email>
<email confidence="0.97144">osorio@exatas.unisinos.br</email>
<abstract confidence="0.999488363636364">This paper proposes and evaluates the use of linguistic information in the pre-processing phase of text mining tasks. We present several experiments comparing our proposal for selection of terms based on linguistic knowledge with usual techniques applied in the field. The results show that part of speech information is useful for the pre-processing phase of text categorization and clustering, as an alternative for stop words and stemming.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aizawa</author>
</authors>
<title>Linguistic Techniques to Improve the Performance of Automatic Text Categorization.</title>
<date>2001</date>
<booktitle>Proc. of the Sixth Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>307--314</pages>
<contexts>
<context position="13537" citStr="Aizawa, 2001" startWordPosition="2177" endWordPosition="2178">0 Table 4: Confusion Matrix PD2 (nouns + proper names, 150 terms) 6 Conclusions This paper presented a series of experiments aiming at comparing our proposal of preprocessing techniques based on linguistic information with usual methods adopted for preprocessing in text mining. We find in the literature other alternative proposals for the pre-processing phase of text mining. (Gonçalves and Quaresma, 2003) use the canonical form of the word instead stemming, for European Portuguese. (Feldman et al, 1998) proposes the use of compound terms as opposed to single terms for text mining. Similarly, (Aizawa, 2001) uses morphological analysis to aid the extraction of compound terms. Our approach differs from those since we propose single terms selection based on different part of speech information. The results show that a selection made solely on the basis of category information produces results at least as good as those produced by usual methods (when the selection considers nouns and adjectives or nouns and proper nouns) both in categorization and clustering tasks. In the categorization experiments we obtained the lowest error rate for PD2 when the pre-processing phase was based on the selection of </context>
</contexts>
<marker>Aizawa, 2001</marker>
<rawString>Aizawa A., 2001. Linguistic Techniques to Improve the Performance of Automatic Text Categorization. Proc. of the Sixth Natural Language Processing Pacific Rim Symposium, pages 307-314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bick</author>
</authors>
<title>The Parsing System PALAVRAS: Automatic Gramatical Analysis of Porutugese in a Constraint Grammar Framework. Århus University. Århus:</title>
<date>2000</date>
<publisher>Århus University Press.</publisher>
<contexts>
<context position="4413" citStr="Bick, 2000" startWordPosition="685" endWordPosition="686">in phase B are the use of a list of stop-words, which are discarded from the original documents and the use of stemming which reduces the words to their root. Having the proper tools to process Portuguese texts, we investigate whether linguistic information can have an impact on the results of the whole process. In the next section we describe the tools we used for acquiring the linguistic knowledge in which we base our experiments. 3 Tools for acquiring linguistic knowledge The linguistic knowledge we use in the experiments is based on the syntactic analysis performed by the PALAVRAS parser (Bick, 2000). This Portuguese parser is robust enough to always give an output even for incomplete or incorrect sentences (which might be the case for the type of documents used in text mining tasks). It has a comparatively low percentage of errors (less than 1% for word class and 3-4% for surface syntax) (Bick, 2003). We also used another tool that makes easier the extraction of features from the analyzed texts: the Palavras Xtractor (Gasperin et. al. 2003). This tool converts the parser output into three XML files, containing: a) the list of all words from the text and their identifier; b) morphosyntact</context>
</contexts>
<marker>Bick, 2000</marker>
<rawString>Bick, E. 2000. The Parsing System PALAVRAS: Automatic Gramatical Analysis of Porutugese in a Constraint Grammar Framework. Århus University. Århus: Århus University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bick</author>
</authors>
<title>A Constraint Grammar Based Question Answering System for Portuguese.</title>
<date>2003</date>
<booktitle>Proceedings of the 11º Portuguese Conference on Artificial Intelligence,</booktitle>
<pages>414--418</pages>
<publisher>LNAI Springer Verlag.</publisher>
<contexts>
<context position="4720" citStr="Bick, 2003" startWordPosition="738" endWordPosition="739">ocess. In the next section we describe the tools we used for acquiring the linguistic knowledge in which we base our experiments. 3 Tools for acquiring linguistic knowledge The linguistic knowledge we use in the experiments is based on the syntactic analysis performed by the PALAVRAS parser (Bick, 2000). This Portuguese parser is robust enough to always give an output even for incomplete or incorrect sentences (which might be the case for the type of documents used in text mining tasks). It has a comparatively low percentage of errors (less than 1% for word class and 3-4% for surface syntax) (Bick, 2003). We also used another tool that makes easier the extraction of features from the analyzed texts: the Palavras Xtractor (Gasperin et. al. 2003). This tool converts the parser output into three XML files, containing: a) the list of all words from the text and their identifier; b) morphosyntactic information for each word; c) the sentence´s syntactic structures. Using XSL (eXtensible Stylesheet Language)1 we can extract specified terms from the texts, according to their linguistic value. The resulting lists of terms according to each combination are then passed to phases C, D and E. The experime</context>
</contexts>
<marker>Bick, 2003</marker>
<rawString>Bick, E. 2003. A Constraint Grammar Based Question Answering System for Portuguese. Proceedings of the 11º Portuguese Conference on Artificial Intelligence, pages 414-418. LNAI Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Feldman</author>
</authors>
<title>Text Mining at the Term Level.</title>
<date>1998</date>
<booktitle>Proc. of the Second European Symposium on Principles of Data Mining and Knowledge Discovery,</booktitle>
<pages>65--73</pages>
<publisher>LNCS Springer.</publisher>
<marker>Feldman, 1998</marker>
<rawString>Feldman R., et al. 1998. Text Mining at the Term Level. Proc. of the Second European Symposium on Principles of Data Mining and Knowledge Discovery, pages 65-73. LNCS Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gasperin</author>
<author>R Vieira</author>
<author>R Goulart</author>
<author>P Quaresma</author>
</authors>
<date>2003</date>
<booktitle>Extracting XML Syntactic Chunks from Portuguese Corpora. Proc. of the TALN Workshop on Natural Language Processing of Minority Languages and Small Languages,</booktitle>
<pages>223--232</pages>
<location>Batz-sur-Mer</location>
<marker>Gasperin, Vieira, Goulart, Quaresma, 2003</marker>
<rawString>Gasperin, C.; Vieira, R.; Goulart, R. and Quaresma, P. 2003. Extracting XML Syntactic Chunks from Portuguese Corpora. Proc. of the TALN Workshop on Natural Language Processing of Minority Languages and Small Languages, pages 223-232. Batz-sur-Mer France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Gonçalves</author>
<author>P Quaresma</author>
</authors>
<title>A prelimary approach classification problem of Portuguese juridical documents.</title>
<date>2003</date>
<booktitle>Proceedings of the 11º Portuguese Conference on Artificial Intelligence,</booktitle>
<pages>435--444</pages>
<publisher>LNAI Springer Verlag.</publisher>
<contexts>
<context position="13332" citStr="Gonçalves and Quaresma, 2003" startWordPosition="2142" endWordPosition="2145">nformation is provided by the parser PALAVRAS. Other front for future work is further tests for other languages. Cl.0 Cl.1 Cl.2 Cl.3 Cl.4 Sp. 0 38 19 0 0 Prop. 11 0 44 1 1 Inf. 0 0 19 0 38 Pol. 0 1 20 36 0 Tour. 0 0 57 0 0 Table 4: Confusion Matrix PD2 (nouns + proper names, 150 terms) 6 Conclusions This paper presented a series of experiments aiming at comparing our proposal of preprocessing techniques based on linguistic information with usual methods adopted for preprocessing in text mining. We find in the literature other alternative proposals for the pre-processing phase of text mining. (Gonçalves and Quaresma, 2003) use the canonical form of the word instead stemming, for European Portuguese. (Feldman et al, 1998) proposes the use of compound terms as opposed to single terms for text mining. Similarly, (Aizawa, 2001) uses morphological analysis to aid the extraction of compound terms. Our approach differs from those since we propose single terms selection based on different part of speech information. The results show that a selection made solely on the basis of category information produces results at least as good as those produced by usual methods (when the selection considers nouns and adjectives or </context>
</contexts>
<marker>Gonçalves, Quaresma, 2003</marker>
<rawString>Gonçalves, T. and Quaresma, P. 2003. A prelimary approach classification problem of Portuguese juridical documents. Proceedings of the 11º Portuguese Conference on Artificial Intelligence, pages 435-444. LNAI Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An Algorithm for Suffix Stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<pages>130--137</pages>
<contexts>
<context position="6300" citStr="Porter, 1980" startWordPosition="976" endWordPosition="977">cs, property, sports, politics and tourism. 4.2 Pre-processing techniques We prepared three different versions of the corpus (V1, V2 and V3) for 3-fold cross validation. Each version is partitioned in different training and testing parts, containing 2/3 and 1/3 of the documents respectively. For the experiments with the usual methods, irrelevant terms (stop-words) were eliminated from the documents, on the basis of a list of stop-words, containing 476 terms (mainly articles, prepositions, auxiliary verbs, pronouns, etc). The remaining terms were stemmed according to Martin Porter’s algorithm (Porter, 1980). Based on these 1 Available in http://www.w3.org/Style/XSL/ 2 Available in http://nilc.icmc.sc.usp.br/nilc/ techniques we generated a collection of preprocessed documents called PD1. To test our proposal we then pre-processed the 855 documents in a different way: we parsed all texts of our corpus, generating the corresponding XML files and extracted terms according to their grammatical categories, using XSL. Based on these techniques we generated a collection of preprocessed documents called PD2. 4.2.1 Other mining phases All other text mining phases were equally applied to both PD1 and PD2. </context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Porter, M. F. 1980. An Algorithm for Suffix Stripping. Program, 14 no. 3, pages 130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ah-Hwee Tan</author>
</authors>
<title>Text mining: the state of the art and the challenges.</title>
<date>1999</date>
<booktitle>Proc. of the Pacific-Asia Workshop on Knowledge Discovery from Advanced Databases,</booktitle>
<pages>65--70</pages>
<location>Beijing.</location>
<contexts>
<context position="1057" citStr="Tan, 1999" startWordPosition="155" endWordPosition="156">ining tasks. We present several experiments comparing our proposal for selection of terms based on linguistic knowledge with usual techniques applied in the field. The results show that part of speech information is useful for the pre-processing phase of text categorization and clustering, as an alternative for stop words and stemming. 1 Introduction Natural language texts can be viewed as resources containing uniform data in such a way that methods similar to those used in Data Base Knowledge Extraction can be applied to them. The adaptation of these methods to texts is known as Text Mining (Tan, 1999). Machine learning techniques are applied to document collections aiming at extracting patterns that may be useful to organize or recover information from the collections. Tasks related to this area are text categorization, clustering, summarization, and information extraction. One of the first steps in text mining tasks is the pre-processing of the documents, as they need to be represented in a more structured way. Our work proposes a new technique to the preprocessing phase of documents and we compare it with usual pre-processing methods. We focus on two text mining tasks, namely text catego</context>
</contexts>
<marker>Tan, 1999</marker>
<rawString>Tan, Ah-Hwee. 1999. Text mining: the state of the art and the challenges. Proc. of the Pacific-Asia Workshop on Knowledge Discovery from Advanced Databases, pages 65-70, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
</authors>
<title>Data mining: Pratical Machine Learning tools and techniques with Java implementations.</title>
<date>2000</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="7525" citStr="Witten, 2000" startWordPosition="1164" endWordPosition="1165">tive frequency for the selection of relevant terms. The representation of the documents was according to the vector space model. For the categorization task, vectors corresponding to each class were built, where the more frequent terms were selected. After that, a global vector was composed. We also tested with different numbers of terms in the global vector (30, 60, 90, 120, 150). For the clustering task we measured the similarity of the documents using cosine. After calculating similarity of the documents, the data was codified according to format required by the machine learning tool Weka (Witten, 2000). Weka is a collection of machine learning algorithms for data mining tasks that contains tools for data pre-processing, classification, regression, clustering, association rules, and visualization. In this work the adopted machine learning techniques are Decision Tree for the categorization process and K-means for text clustering. Decision Tree is a supervised learning algorithm based on the recursive division of the training examples in representative subsets, using the metric of information gain. After the induction of a classifying tree, it can be applied to new examples, described with th</context>
</contexts>
<marker>Witten, 2000</marker>
<rawString>Witten, I. H. 2000. Data mining: Pratical Machine Learning tools and techniques with Java implementations. Academic Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>