<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000067">
<title confidence="0.9981855">
SemantiKLUE: Robust Semantic Similarity at Multiple Levels Using
Maximum Weight Matching
</title>
<author confidence="0.767341">
Thomas Proisl and Stefan Evert and Paul Greiner and Besim Kabashi
</author>
<affiliation confidence="0.555921">
Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)
</affiliation>
<address confidence="0.596858">
Department Germanistik und Komparatistik
Professur für Korpuslinguistik
Bismarckstr. 6, 91054 Erlangen, Germany
</address>
<email confidence="0.996862">
{thomas.proisl,stefan.evert,paul.greiner,besim.kabashi}@fau.de
</email>
<sectionHeader confidence="0.997361" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996525">
Being able to quantify the semantic similar-
ity between two texts is important for many
practical applications. SemantiKLUE com-
bines unsupervised and supervised tech-
niques into a robust system for measuring
semantic similarity. At the core of the sys-
tem is a word-to-word alignment of two
texts using a maximum weight matching
algorithm. The system participated in three
SemEval-2014 shared tasks and the com-
petitive results are evidence for its usability
in that broad field of application.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998010857142857">
Semantic similarity measures the semantic equiv-
alence between two texts ranging from total dif-
ference to complete semantic equivalence and is
usually encoded as a number in a closed interval,
e. g. [0,5]. Here is an example for interpreting the
numeric similarity scores taken from Agirre et al.
(2013, 33):
</bodyText>
<listItem confidence="0.966740181818182">
0. The two sentences are on different topics.
1. The two sentences are not equivalent, but are
on the same topic.
2. The two sentences are not equivalent, but
share some details.
3. The two sentences are roughly equivalent, but
some important information differs/missing.
4. The two sentences are mostly equivalent, but
some unimportant details differ.
5. The two sentences are completely equivalent,
as they mean the same thing.
</listItem>
<bodyText confidence="0.949101">
Systems capable of reliably predicting the semantic
similarity between two texts can be beneficial for a
</bodyText>
<footnote confidence="0.908021">
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.997711869565217">
broad range of NLP applications, e. g. paraphrasing,
MT evaluation, information extraction, question
answering and summarization.
A general system for semantic similarity aiming
at being applicable in such a broad scope has to
be able to adapt to the use case at hand, because
different use cases might, for example, require dif-
ferent similarity scales: For one application, two
texts dealing roughly with the same topic should
get a high similarity score, whereas for another ap-
plication being able to distinguish between subtle
differences in meaning might be important. The
three SemEval-2014 shared tasks focussing on se-
mantic similarity (cf. Sections 3, 4 and 5 for more
detailed task descriptions) provide a rich testbed
for such a general system, as the individual tasks
and subtasks have slightly different objectives.
In the remainder of this paper, we describe
SemantiKLUE, a general system for measuring se-
mantic similarity between texts that we built based
on our experience from participating in the *SEM
2013 shared task on “Semantic Textual Similarity”
(Greiner et al., 2013).
</bodyText>
<sectionHeader confidence="0.994283" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.99988275">
SemantiKLUE operates in two stages. In the first,
unsupervised stage, a number of similarity mea-
sures are computed. Those measures are the same
for all tasks and range from simple heuristics to dis-
tributional approaches to resource-heavy methods
based on WordNet and dependency structures. The
idea is to have a variety of similarity measures that
can capture small differences in meaning as well
as broad thematical similarities. In the second, su-
pervised stage, all similarity measures obtained in
this way are passed to a support vector regression
learner that is trained on the available gold standard
data in order to obtain a final semantic similarity
score. This way, the proper similarity scale for a
given task can be learned. The few remaining out-
liers in the predictions for new text pairs are cut
</bodyText>
<page confidence="0.946609">
532
</page>
<note confidence="0.98071">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 532–540,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.998684555555556">
off to fit the interval required by the task definition
([0,4] or [0,5]).
Our submissions for the individual tasks were
created using incomplete versions from different
developmental stages of the system. In the follow-
ing sections we describe the current version of the
complete system for which we also report compa-
rable results for all tasks (cf. Sections 3– 5).
The whole system is implemented in Python.
</bodyText>
<subsectionHeader confidence="0.966881">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999967875">
We use Stanford CoreNLP1 for part-of-speech tag-
ging, lemmatizing and parsing the input texts. We
utilize the CCprocessed variant of the Stanford De-
pendencies (collapsed dependencies with propaga-
tion of conjunct dependencies; de Marneffe and
Manning (2008, 13–15)) to create a graph represen-
tation of the texts using the NetworkX2 (Hagberg
et al., 2008) module. All the similarity measures
described below are computed on the basis of that
graph representation. It is important to keep in
mind that by basing all computations on the Stan-
ford Dependencies model we effectively ignore
most of the prepositions when using measures that
work on tokens.3 For some tasks, we perform some
additional task-specific preprocessing steps prior
to parsing, cf. task descriptions below.
</bodyText>
<subsectionHeader confidence="0.999129">
2.2 Simple Measures
</subsectionHeader>
<bodyText confidence="0.999175833333333">
We use four simple heuristic similarity measures
that need very little preprocessing. The first two
are word form overlap and lemma overlap between
the two texts. We take the sets of word form to-
kens/lemmatized tokens in text A and text B and
calculate the Jaccard coefficient:
</bodyText>
<equation confidence="0.7687835">
overlap = |Af1B|
|A U B|.
</equation>
<bodyText confidence="0.990624">
The third is a heuristic for the difference in text
length that was used by Gale and Church (1993) as
a similarity measure for aligning sentences:
</bodyText>
<equation confidence="0.996379">
∑N
di = bi − N 1b-
</equation>
<bodyText confidence="0.907152">
where J ai.
</bodyText>
<equation confidence="0.410743">
∑j=1 aj
</equation>
<bodyText confidence="0.915518">
For each of the N text pairs we calculate the differ-
ence di between the observed length of text B and
</bodyText>
<footnote confidence="0.98928">
1http://nlp.stanford.edu/software/corenlp.
shtml
2http://networkx.github.com/
3That is because in the CCprocessed variant of the Stanford
Dependencies most prepositions are “collapsed” into depen-
dency relations and are therefore represented as edges and not
as vertices in the graph.
</footnote>
<bodyText confidence="0.999726538461538">
the expected length of text B based on the length
of text A. By dividing that difference di by the stan-
dard deviation of all those differences, we obtain
our heuristic zi.
The fourth is a binary feature expressing whether
the two texts differ in their use of negation. We
check if one of the texts contains any of the lem-
mata no, not or none and the other doesn’t. That
feature is motivated by the comparatively large
number of sentences in the SICK dataset (Marelli
et al., 2014b) that mainly differ in their use of nega-
tion, e. g. sentence pair 42 in the training data that
has a gold similarity score of 3.4:
</bodyText>
<listItem confidence="0.99946125">
• Two people are kickboxing and spectators are
watching
• Two people are kickboxing and spectators are
not watching
</listItem>
<subsectionHeader confidence="0.9993755">
2.3 Measures Based on Distributional
Document Similarity
</subsectionHeader>
<bodyText confidence="0.999898966666667">
We obtain document similarity scores from two
large-vocabulary distributional semantic models
(DSMs).
The first model is based on a 10-billion word
Web corpus consisting of Wackypedia and ukWaC
(Baroni et al., 2009), UMBC WebBase (Han et
al., 2013), and UKCOW 2012 (Schäfer and Bild-
hauer, 2012). Target terms and feature terms are
POS-disambiguated lemmata.4 We use parame-
ters suggested by recent evaluation experiments:
co-occurrence counts in a symmetric 4-word win-
dow, the most frequent 30,000 lexical words as
features, log-likelihood scores with an additional
log-transformation, and SVD dimensionality re-
duction of L2-normalized vectors to 1000 latent
dimensions. This model provides distributional
representations for 150,000 POS-disambiguated
lemmata as target terms.
The second model was derived from the second
release of the Google Books N-Grams database
(Lin et al., 2012), using the dependency pairs pro-
vided in this version. Target and feature terms are
case-folded word forms; co-occurrence ounts are
based on direct syntactic relations. Here, the most
frequent 50,000 word forms were used as features.
All other parameters are identical to the first DSM.
This model provides distributional representations
for 250,000 word forms.
We compute bag-of-words centroid vectors for
each text as suggested by (Schütze, 1998). For each
</bodyText>
<footnote confidence="0.751978">
4e.g. can_N for the noun can
</footnote>
<equation confidence="0.453077">
zi = di
σd
</equation>
<page confidence="0.973936">
533
</page>
<figure confidence="0.9925905625">
A
woman
is
using
a
machine
made
for
sewing
A
woman
is
sewing
with
a
machine
</figure>
<bodyText confidence="0.9962714">
text pair and DSM, we calculate the cosine similar-
ity between the two centroid vectors as a measure
of their semantic similarity. We also determine the
number of unknown words in both texts according
to both DSMs as additional features.
</bodyText>
<subsectionHeader confidence="0.989231">
2.4 Alignment-based Measures
</subsectionHeader>
<bodyText confidence="0.999956333333333">
We also use features based on word-level similar-
ity. We separately compute similarities between
words using state-of-the-art WordNet similarity
measures and the two distributional semantic mod-
els described above. The words from both texts
are then aligned using those similarity scores to
maximize the similarity total. We use two types
of alignment: One-to-one alignment where some
words in the longer text remain unaligned and one-
to-many alignment where all words are aligned.
The one-to-many alignment is based on the one-to-
one alignment and aligns each previously unaligned
word in the longer text to the most similar word in
the shorter text. The discussion of the alignment
algorithm is based on the former case.
</bodyText>
<subsectionHeader confidence="0.987737">
2.4.1 Alignment via Maximum Weight
Matching
</subsectionHeader>
<bodyText confidence="0.999960136363636">
We opt for a graphical solution to the alignment
problem. The similarities between the words from
both texts can be modelled as a bipartite graph in
which every word from text A is a vertice on the
left-hand side of the graph and every word from
text B a vertex on the right-hand side. Weighted
edges connect every word from text A to every
word from text B. The weight of an edge corre-
sponds to the similarity between the two words it
connects. In order to obtain an optimal one-to-one
alignment we have to select edges in such a way
that no two edges share a common vertice and that
the sum of the edge weights is maximized. That
corresponds to the problem of finding the maxi-
mum weight matching in the graph. SemantiKLUE
utilizes the NetworkX implementation of Galil’s
(1986) algorithms for finding that maximum weight
matching.
Figure 1 visualizes the one-to-one alignment be-
tween two sentences. For the one-to-many align-
ment, the previously unaligned words are aligned
as indicated by the dashed lines.
</bodyText>
<subsectionHeader confidence="0.9615035">
2.4.2 Measures Based on Distributional
Word Similarities
</subsectionHeader>
<bodyText confidence="0.9844595">
For each of the two DSMs described in Section 2.3
we compute the best one-to-one and the best one-
</bodyText>
<figureCaption confidence="0.9815695">
Figure 1: Alignment between a sentence pair from
the SICK data set.
</figureCaption>
<bodyText confidence="0.999876631578947">
to-many alignment using the cosine similarity be-
tween two words as edge weight. For each of
those two alignments we compute the following
two similarity measures: I) the arithmetic mean of
the cosines between all the aligned words from text
A and text B and II) the arithmetic mean ignoring
identical word pairs.
In addition to those eight measures, we use the
lemma-based DSM for computing the distribution
of cosines between lemma pairs. For both align-
ments, we categorize the cosines between aligned
lemma pairs into five heuristically determined inter-
vals ([0.2,0.35), [0.35,0.5), [0.5,0.7), [0.7,0.999),
[0.999,1.0])5 and use the proportions as features.
Intuitively, the top bins correspond to links between
identical words, paradigmatically related words
and topically related words. All in all, we use a
total of 18 features computed from the DSM-based
alignments.
</bodyText>
<subsectionHeader confidence="0.794738">
2.4.3 Measures Based on WordNet
</subsectionHeader>
<bodyText confidence="0.945596384615385">
We utilize two state-of-the-art (Budanitsky and
Hirst, 2006) WordNet similarity measures for cre-
ating alignments: Leacock and Chodorow’s (1998)
normalized path length and Lin’s (1998) universal
similarity measure. For both of those similarity
measures we compute the best one-to-one and the
best one-to-many alignment. For each alignment
we compute the following two similarity measures:
I) the arithmetic mean of the similarities between
the aligned words from text A and text B and II) the
arithmetic mean ignoring identical word pairs.
5Values in the interval [0.0,0.2) are discarded as they
would be collinear with the other features.
</bodyText>
<page confidence="0.991604">
534
</page>
<bodyText confidence="0.976459324324324">
We also include the number of unknown words
in both texts according to WordNet as additional
features.
2.5 Measures Using the Dependency
Structure
We expect that the information encoded in the de-
pendency structure of the texts can be beneficial in
determining the semantic similarity between them.
Therefore, we use three heuristics for measuring
similarity on the level of syntactic dependencies.
The first simply measures the overlap of depen-
dency relation labels between the two texts (cf. Sec-
tion 2.2). The second utilizes the fact that the Stan-
ford Dependencies are organized in a hierarchy (de
Marneffe and Manning, 2008, 11–12) to compute
Leacock and Chodorow’s normalized path lengths
between individual dependency relations. That
measure for the similarity between dependency re-
lations is then used to determine the best one-to-one
alignment between dependency relations from text
A and text B and to compute the arithmetic mean
of the similarities between the aligned dependency
relations. The third heuristic gives an indication
of the quality of the one-to-one alignment and can
be used to distinguish texts that contain the same
words in different syntactic structures. It uses the
one-to-one alignment created with similarity scores
from the lemma-based DSM (cf. Section 2.4.2) to
compute the average overlap of neighbors for all
aligned word pairs. The overlap of neighbors is
determined by computing the Jaccard coefficient
of sets NA and NB. Set NA contains all words from
text B that are aligned to words from text A that
are connected to the target word via a single depen-
dency relation. NB contains all words from text B
that are connected to the word aligned to the target
word in text A via a single dependency relation.
</bodyText>
<subsectionHeader confidence="0.960402">
2.6 Experimental Features
</subsectionHeader>
<bodyText confidence="0.999956952380953">
As an experiment, we included features from a com-
mercial text clustering software that is currently
being developed by our team (Greiner and Evert, in
preparation). We used this tool – which combines
ideas from Latent Semantic Indexing and distribu-
tional semantics with multiple clustering steps – as
a black box.
We loaded all training, development and test
items for a given task into the system and applied
the clustering algorithm. However, we did not
make use of the resulting topic clusters. Instead, we
computed cosine similarities for each pair (s1,s2)
of sentences (or other textual units) based on the in-
ternal representation. In addition, we computed the
average neighbour rank of the two sentences, based
on the rank of s2 among the nearest neighbours of
s1 and vice versa.
Since these features are generated from the task
data themselves, they should adapt automatically
to the range of meaning differences present in a
given data set.
</bodyText>
<subsectionHeader confidence="0.978074">
2.7 Machine Learning
</subsectionHeader>
<bodyText confidence="0.99994425">
Using all the features described above, we have a
total of 39 individual features that measure seman-
tic similarity between two texts (cf. Sections 2.2 to
2.5) and two experimental features (cf. Section 2.6).
In order to obtain a single similarity score, we use
the scikit-learn6 (Pedregosa et al., 2011) implemen-
tation of support vector regression. In our cross-
validation experiments we got the best results with
an RBF kernel of degree 2 and a penalty C = 0.7, so
those are the parameters we use in our experiments.
The SemEval-2014 Task 1 also includes a classi-
fication subtask for which we use the same 39 + 2
features for training a support vector classifier.
Cross-validation suggests that the best parameter
setting is a polynomial kernel of degree 2 and a
penalty C = 2.5.
</bodyText>
<sectionHeader confidence="0.992583" genericHeader="method">
3 SemEval-2014 Task 1
</sectionHeader>
<subsectionHeader confidence="0.987601">
3.1 Task Description
</subsectionHeader>
<bodyText confidence="0.999551684210526">
The focus of the shared task on “Evaluation of com-
positional distributional semantic models on full
sentences through semantic relatedness and textual
entailment” (Marelli et al., 2014a) lies on the com-
positional nature of sentence semantics. By using
a specially created data set (Marelli et al., 2014b)
that tries to avoid multiword expressions and other
idiomatic features of language outside the scope of
compositional semantics, it provides a testbed for
systems implementing compositional variants of
distributional semantics. There is also an additional
subtask for detecting the entailment relation (entail-
ment, neutral, contradiction) between to sentences.
Although SemantiKLUE lacks a truly sophisti-
cated component for dealing with compositional
semantics (besides trying to incorporate the depen-
dency structure of the texts), the system takes the
seventh place in the official ranking by Pearson
correlation with a correlation coefficient of 0.780
</bodyText>
<footnote confidence="0.983103">
6http://scikit-learn.org/
</footnote>
<page confidence="0.9959">
535
</page>
<bodyText confidence="0.999853333333333">
(best of 17 systems: 0.828). In the entailment sub-
task, the system even takes the fourth place with an
accuracy of 0.823 (best of 18 systems: 0.846).
</bodyText>
<subsectionHeader confidence="0.987115">
3.2 Experiments
</subsectionHeader>
<bodyText confidence="0.998538">
The official runs we submitted for this task were
created by a work-in-progress version of Semanti-
KLUE that did not contain all the features de-
scribed above. In this section, we report on some
post-hoc experiments with the complete system us-
ing all the features as well as various subsets of
features. See Table 1 for an overview of the results.
</bodyText>
<table confidence="0.994791333333333">
Run r ρ MSE Acc.
0.780 0.736 0.403 0.823
0.782 0.738 0.398 0.823
0.798 0.754 0.373 0.820
0.793 0.748 0.383 0.817
0.763 0.713 0.432 0.793
0.801 0.757 0.367 0.823
0.729 0.670 0.484 0.746
0.708 0.636 0.515 0.715
0.676 0.667 0.561 0.754
0.660 0.568 0.585 0.567
0.576 0.565 0.688 0.614
</table>
<tableCaption confidence="0.9853015">
Table 1: Results for task 1 (Pearson’s r, Spearman’s
ρ, mean squared error and accuracy).
</tableCaption>
<bodyText confidence="0.99856352">
The whole system as described above, without
the experimental features, performs even a bit bet-
ter in the semantic similarity subtask (taking place
6) and only slightly worse in the entailment subtask
(still taking place 4) than the official submissions.
Adding the experimental features slightly improves
the results but does not lead to a better position in
the ranking.
We are particularly interested in the impact of
the resource-heavy features derived from the de-
pendency structure of the texts and from Word-
Net. If we use the complete system without the
dependency-based features (emulating the case of
a language for which we have access to a WordNet-
like resource but not to a parser), we get results
that are only marginally worse than those for the
complete system and lead to the same places in the
rankings. Additionally leaving out WordNet has a
bigger impact and results in places 9 and 8 in the
rankings.
Regarding the individual feature groups, the
DSM-alignment-based measures are the best fea-
ture group for predicting semantic similarity and
the simple heuristic measures are the best feature
group for predicting entailment.
</bodyText>
<sectionHeader confidence="0.950672" genericHeader="method">
4 SemEval-2014 Task 3
</sectionHeader>
<subsectionHeader confidence="0.99031">
4.1 Task Description
</subsectionHeader>
<bodyText confidence="0.999989214285714">
Unlike the other tasks, which focus on similar-sized
texts, the shared task on “Cross-Level Semantic
Similarity” (Jurgens et al., 2014) is about measur-
ing semantic similarity between textual units of
different lengths. It comprises four subtasks com-
paring I) paragraphs to sentences, II) sentences to
phrases, III) phrases to words and IV) words to
word senses (taken from WordNet). Due to the
nature of this task, performance in it might be es-
pecially useful as an indicator for the usefulness of
a system in the area of summarization.
SemantiKLUE takes the fourth place out of 38 in
both the official ranking by Pearson correlation and
the alternative ranking by Spearman correlation.
</bodyText>
<subsectionHeader confidence="0.993052">
4.2 Additional Preprocessing
</subsectionHeader>
<bodyText confidence="0.999960166666667">
For the official run we perform some additional pre-
processing on the data for the two subtasks on com-
paring phrases to words and words to word senses.
On the word level we combine the word with the
glosses of all its WordNet senses and on the word
sense level we replace the WordNet sense indica-
tion with its corresponding lemmata and gloss. As
our post-hoc experiments show that has a nega-
tive effect on performance in the phrase-to-word
subtask. Therefore, we skip the additional prepro-
cessing on that level for our experiments described
below.
</bodyText>
<subsectionHeader confidence="0.983916">
4.3 Experiments
</subsectionHeader>
<bodyText confidence="0.9999829">
For each of the four subtasks, we perform the
same experiments as described in Section 3.2: We
compare the official run submitted from a work-
in-progress version of SemantiKLUE with the re-
sults from the whole system; we see how the sys-
tem performs without dependency-based features
and WordNet-based features; we try out the experi-
mental features; we determine the most important
feature group for the subtask. Table 2 gives an
overview of the results.
</bodyText>
<subsectionHeader confidence="0.839015">
4.3.1 Paragraph to Sentence
</subsectionHeader>
<bodyText confidence="0.9998624">
Our submitted run takes the fifth place (ties with
another system) in the official ranking by Pearson
correlation with a correlation coefficient of 0.817
(best of 34 systems: 0.837) and seventh place in
the alternative ranking by Spearman correlation.
</bodyText>
<figure confidence="0.996708363636364">
primary run
best run
complete system
no deps
no deps, no WN
complete + experimental
only DSM alignment
only WordNet
only simple
only DSM document
only deps
</figure>
<page confidence="0.894433">
536
</page>
<table confidence="0.957140652173913">
Run
official
complete system
no deps
no deps, no WN
complete + experimental
only DSM alignment
only WordNet
only simple
only DSM document
only deps
Paragraph to sent. Sent. to phrase Phrase to word Word to sense
r p r p r p r p
0.817 0.802 0.754 0.739 0.215 0.218 0.314 0.327
0.817 0.802 0.754 0.739 0.284 0.289 0.316 0.330
0.815 0.802 0.752 0.739 0.309 0.313 0.312 0.329
0.813 0.802 0.736 0.721 0.335 0.335 0.234 0.248
0.816 0.800 0.752 0.738 0.292 0.298 0.318 0.330
0.799 0.789 0.724 0.711 0.302 0.301 0.216 0.216
0.787 0.769 0.664 0.641 0.186 0.171 0.313 0.311
0.807 0.793 0.686 0.672 0.128 0.121 0.089 0.093
0.629 0.624 0.546 0.558 0.247 0.240 0.144 0.148
0.655 0.621 0.449 0.440 0.036 0.057 −0.080 −0.076
</table>
<tableCaption confidence="0.996694">
Table 2: Results for task 3 (Pearson’s r and Spearman’s p).
</tableCaption>
<bodyText confidence="0.999613416666667">
The complete SemantiKLUE system gives identical
results. Leaving out the resource-heavy features
based on the dependency structure and WordNet
diminishes the results only very slightly, though
it still resolves the tie and puts the system on the
sixth place in the Pearson ranking. Adding the
experimental features to the complete system has a
minor negative effect.
Probably due to the length of the texts, our sim-
ple heuristic measures surpass the DSM-alignment-
based measures as the best feature group for pre-
dicting semantic similarity.
</bodyText>
<subsectionHeader confidence="0.980772">
4.3.2 Sentence to Phrase
</subsectionHeader>
<bodyText confidence="0.999981125">
In this subtask, SemantiKLUE takes the fourth
place in both the official ranking with a Pearson
correlation coefficient of 0.754 (best of 34 systems:
0.777) and in the alternative ranking by Spearman
correlation. The complete system performs iden-
tically to our submitted run and leaving out the
dependency-based features has little impact on the
results. Additionally also leaving out the WordNet-
based features has more impact on the results and
puts the system on the eighth place in the official
ranking. Just as in the paragraph-to-sentence sub-
task, adding the experimental features to the com-
plete system has a slightly negative effect.
For this subtask, the DSM-alignment-based mea-
sures are clearly the feature group that yields the
best results.
</bodyText>
<subsectionHeader confidence="0.965109">
4.3.3 Phrase to Word
</subsectionHeader>
<bodyText confidence="0.999971523809524">
For our submitted run we performed the additional
preprocessing described in Section 4.2 resulting
in the eleventh place in the official ranking with
a Pearson correlation coefficient of 0.215 (best of
22 systems: 0.415) and the 14th place in the alter-
native ranking by Spearman correlation. For our
experiments with the complete system we skip that
additional preprocessing step, i. e. we do not add
the WordNet glosses to the word, and drastically
improve the results, putting our system on the third
place in the official ranking. Even more interesting
is the observation that leaving out the resource-
heavy features further improves the results, putting
the system on the second place. In consistency
with those observations, the DSM-alignment-based
measures are not only the strongest individual fea-
ture group but also yield better results when taken
alone than the complete system.
In contrast to the first two subtasks, adding the
experimental features to the complete systems has
a slightly positive effect here.
</bodyText>
<subsectionHeader confidence="0.917663">
4.3.4 Word to Sense
</subsectionHeader>
<bodyText confidence="0.9999941875">
In the word-to-sense subtask, SemantiKLUE takes
the third place in both the official ranking with a
Pearson correlation coefficient of 0.316 (best of
20 systems: 0.381) and in the alternative rank-
ing by Spearman correlation. The complete sys-
tem performs slightly better than our submitted
run and adding the experimental features gives
another marginal improvement. Leaving out the
dependency-based features has little impact but
also leaving out the WordNet-based features sev-
erly hurts performance. The reason for that be-
haviour becomes clear when we look at the results
for the individual feature groups: the WordNet-
based measures are clearly the strongest feature
group for predicting the semantic similarity be-
tween words and word senses.
</bodyText>
<sectionHeader confidence="0.976843" genericHeader="method">
5 SemEval-2014 Task 10
</sectionHeader>
<subsectionHeader confidence="0.993771">
5.1 Task Description
</subsectionHeader>
<bodyText confidence="0.999878333333333">
The shared task on “Multilingual Semantic Textual
Similarity” (Agirre et al., 2014) is a continuation
of the SemEval-2012 and *SEM 2013 shared tasks
</bodyText>
<page confidence="0.991576">
537
</page>
<table confidence="0.999175818181818">
Run
0.349 0.643 0.733 0.773 0.855 0.640 0.694
0.432 0.638 0.660 0.736 0.810 0.659 0.676
0.464 0.672 0.657 0.771 0.836 0.690 0.700
0.457 0.675 0.636 0.764 0.834 0.690 0.694
0.426 0.653 0.617 0.719 0.780 0.636 0.654
0.466 0.674 0.673 0.772 0.849 0.687 0.706
0.475 0.706 0.711 0.788 0.852 0.715 0.727
0.465 0.700 0.699 0.781 0.848 0.722 0.722
0.448 0.722 0.677 0.752 0.791 0.706 0.697
0.475 0.711 0.715 0.795 0.864 0.721 0.733
</table>
<tableCaption confidence="0.883629">
Table 3: Results for task 10.
</tableCaption>
<table confidence="0.6132697">
best run
complete (all training data)
best overall training data
best overall, no deps
best overall, no deps, no WN
best overall + experimental
best individual training data
best individ., no deps
best individ., no deps, no WN
best individ. + experimental
</table>
<bodyText confidence="0.999155555555556">
on semantic textual similarity (Agirre et al., 2012;
Agirre et al., 2013). It comprises two subtasks:
English semantic textual similarity and Spanish
semantic textual similarity. For each subtask, there
are sentence pairs from various genres.
We only participate in the English subtask and
take the 13th place out of 38 with a weighted mean
of Pearson correlation coefficients of 0.694 (best
system: 0.761).
</bodyText>
<subsectionHeader confidence="0.960233">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.997403764705882">
From participating in the *SEM 2013 shared task
on semantic textual similarity (Greiner et al., 2013)
we already know that the composition of the train-
ing data is one of the strongest influences on system
performance in this task. As the individual data sets
are not very similar to each other, we tried to come
up with a good subset of the available training data
for each data set. In doing so, we were moderately
successful as the results in Table 3 show. Run-
ning the complete system with all of the available
training data on all test data sets results in a lower
weighted mean than our submitted run. If we stick
to using the same training data for all test data sets
and optimize the subset of the training data we use,
we achieve a slightly better result than our submit-
ted run (the optimal subset consists of the FNWN,
headlines, MSRpar, MSRvid and OnWN data sets).
Using that optimal subset of the training data and
adding the experimental features to the complete
system has a minor positive effect on the weighted
mean, with the biggest impact on the headlines and
OnWN data sets. Using the complete system with-
out the dependency-based features gives roughly
the same results but omitting all resource-heavy
features has clearly a negative impact on the re-
sults.
In another experiment we try to optimize our
strategy of finding the best subset of the training
data for each test data set. Doing that gives us a
considerably higher weighted mean than using the
same training data for every test data set, putting
our system on the eighth place. Using the complete
system, we find that the best training data subsets
for the individual test data sets are those shown in
</bodyText>
<tableCaption confidence="0.838943">
Table 4.
</tableCaption>
<table confidence="0.993155571428571">
test set training sets
deft-forum FNWN, headlines, MSRvid
deft-news FNWN, MSRpar, MSRvid
headlines FNWN, headlines, MSRpar
images FNWN, MSRpar, MSRvid
OnWN FNWN, MSRvid, OnWN
tweet-news FNWN, headlines, MSRpar, MSRvid
</table>
<tableCaption confidence="0.968544">
Table 4: Optimal subsets of training data for use
</tableCaption>
<bodyText confidence="0.952693428571429">
with the complete SemantiKLUE system.
If we add the experimental features to the com-
plete system and still optimize the training data sub-
sets, we get a small boost to the results. Leaving out
the dependency-based features does not really hurt
performance but also omitting the WordNet-based
features has a negative impact on the results.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999874">
SemantiKLUE is a robust system for predicting the
semantic similarity between two texts that can also
be used to predict entailment. The system achieves
good or very good results in three SemEval-2014
tasks representing a broad variety of semantic simi-
larity problems (cf. Table 5 for an overview of the
results of all subtasks). Our two-staged strategy of
computing several similarity measures and using
them as input for a machine learning mechanism
</bodyText>
<page confidence="0.991577">
538
</page>
<table confidence="0.9997755">
Subtask submitted run complete system winner score
score rank score rank
Task 1, similarity 0.780 7/17 0.798 6/17 0.828
Task 1, entailment 0.823 4/18 0.820 4/18 0.846
Task 3, par-2-sent 0.817 5/34 0.817 5/34 0.837
Task 3, sent-2-phr 0.754 4/34 0.754 4/34 0.777
Task 3, phr-2-word 0.215 11/22 0.284 3/22 0.415
Task 3, word-2-sense 0.314 3/20 0.316 3/20 0.381
Task 3 overall N/A 4/38 N/A 3/38 N/A
Task 10, deft-forum 0.349 20/38 0.464 12/38 0.531
Task 10, deft-news 0.643 22/37 0.672 19/37 0.785
Task 10, headlines 0.733 15/37 0.657 20/37 0.784
Task 10, images 0.773 16/37 0.771 17/37 0.834
Task 10, OnWN 0.855 3/36 0.836 7/36 0.875
Task 10, tweet-news 0.640 20/37 0.690 12/37 0.792
Task 10 overall 0.694 13/38 0.700 13/38 0.761
</table>
<tableCaption confidence="0.999335">
Table 5: Overview of results.
</tableCaption>
<bodyText confidence="0.998500043478261">
proves itself to be adaptable to the needs of the
individual tasks.
Using the maximum-weight-matching algorithm
for aligning words from both texts that have similar
distributional semantics leads to very sound fea-
tures. Even without the resource-heavy features,
the system yields competitive results. In some use
cases, those expensive features are almost negligi-
ble. Without being dependent on the availability of
resources like a dependency parser or a WordNet-
like lexical database, SemantiKLUE can easily be
adapted to other languages.
Our experimental features from the commercial
topic clustering software are useful in some cases;
in others at least they do not hurt performance.
We feel that the heuristics based on the depen-
dency structure of the texts do not exhaust all the
possibilities that dependency parsing has to offer.
In the future we would like to try out more mea-
sures based on those structures. Probably some
kind of graph edit distance incorporating the sim-
ilarities between both dependency relations and
words might turn out to be a powerful feature.
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999874020833333">
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2012. SemEval-2012 task
6: A pilot on semantic textual similarity. In First
Joint Conference on Lexical and Computational Se-
mantics, pages 385–393. ACL.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), volume 1: Proceedings of the Main
Conference and the Shared Task, pages 32–43. ACL.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014).
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide
Web: A collection of very large linguistically pro-
cessed Web-crawled corpora. Language Resources
and Evaluation, 43(3):209–226.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13–
47.
Marie-Catherine de Marneffe and Christopher D. Man-
ning, 2008. Stanford typed dependencies manual.
Stanford University.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75–102.
Zvi Galil. 1986. Efficient algorithms for finding
maximum matching in graphs. Computing Surveys,
18(1):23–38.
Paul Greiner and Stefan Evert. in preparation. The
Klugator Engine: A distributional approach to open
questions in market research.
Paul Greiner, Thomas Proisl, Stefan Evert, and Besim
Kabashi. 2013. KLUE-CORE: A regression model
of semantic textual similarity. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), volume 1: Proceedings of the Main Con-
ference and the Shared Task, pages 181–186. ACL.
Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart.
2008. Exploring network structure, dynamics, and
function using NetworkX. In Gäel Varoquaux,
</reference>
<page confidence="0.984924">
539
</page>
<reference confidence="0.999358421052632">
Travis Vaught, and Jarrod Millman, editors, Pro-
ceedings of the 7th Python in Science Conference
(SciPy2008), pages 11–15, Pasadena, CA.
Lushan Han, Abhay L. Kashyap, Tim Finin,
James Mayfield, and Johnathan Weese. 2013.
UMBC_EBIQUITY-CORE: Semantic textual
similarity systems. In Proceedings of the Second
Joint Conference on Lexical and Computational
Semantics. ACL.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 task 3:
Cross-level semantic similarity. In Proceedings of
the 8th International Workshop on Semantic Evalua-
tion (SemEval-2014).
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database,
pages 265–283. MIT Press, Cambridge, MA.
Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the Google Books Ngram
Corpus. In Proceedings of the ACL 2012 System
Demonstrations, pages 169–174, Jeju Island, Korea.
ACL.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296–304, San Francisco, CA. Morgan Kaufmann.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. SemEval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014).
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zampar-
elli. 2014b. A SICK cure for the evaluation of com-
positional distributional semantic models. In Pro-
ceedings of LREC 2014, Reykjavik. ELRA.
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Édouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.
Roland Schäfer and Felix Bildhauer. 2012. Building
large corpora from the web using a new efficient
tool chain. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Eval-
uation (LREC ’12), pages 486–493, Istanbul, Turkey.
ELRA.
Hinrich Schütze. 1998. Automatic word sense discrim-
ination. Computational Linguistics, 24(1):97–123.
</reference>
<page confidence="0.997012">
540
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.143891">
<title confidence="0.991218">SemantiKLUE: Robust Semantic Similarity at Multiple Levels Maximum Weight Matching</title>
<author confidence="0.678094">Proisl Evert Greiner Friedrich-Alexander-Universität Erlangen-Nürnberg</author>
<affiliation confidence="0.685264">Department Germanistik und</affiliation>
<note confidence="0.373429">Professur für Bismarckstr. 6, 91054 Erlangen,</note>
<email confidence="0.995424">thomas.proisl@fau.de</email>
<email confidence="0.995424">stefan.evert@fau.de</email>
<email confidence="0.995424">paul.greiner@fau.de</email>
<email confidence="0.995424">besim.kabashi@fau.de</email>
<abstract confidence="0.998644923076923">Being able to quantify the semantic similarity between two texts is important for many practical applications. SemantiKLUE combines unsupervised and supervised techniques into a robust system for measuring semantic similarity. At the core of the system is a word-to-word alignment of two texts using a maximum weight matching algorithm. The system participated in three SemEval-2014 shared tasks and the competitive results are evidence for its usability in that broad field of application.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>SemEval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>385--393</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="25809" citStr="Agirre et al., 2012" startWordPosition="4130" endWordPosition="4133">0.636 0.764 0.834 0.690 0.694 0.426 0.653 0.617 0.719 0.780 0.636 0.654 0.466 0.674 0.673 0.772 0.849 0.687 0.706 0.475 0.706 0.711 0.788 0.852 0.715 0.727 0.465 0.700 0.699 0.781 0.848 0.722 0.722 0.448 0.722 0.677 0.752 0.791 0.706 0.697 0.475 0.711 0.715 0.795 0.864 0.721 0.733 Table 3: Results for task 10. best run complete (all training data) best overall training data best overall, no deps best overall, no deps, no WN best overall + experimental best individual training data best individ., no deps best individ., no deps, no WN best individ. + experimental on semantic textual similarity (Agirre et al., 2012; Agirre et al., 2013). It comprises two subtasks: English semantic textual similarity and Spanish semantic textual similarity. For each subtask, there are sentence pairs from various genres. We only participate in the English subtask and take the 13th place out of 38 with a weighted mean of Pearson correlation coefficients of 0.694 (best system: 0.761). 5.2 Experiments From participating in the *SEM 2013 shared task on semantic textual similarity (Greiner et al., 2013) we already know that the composition of the training data is one of the strongest influences on system performance in this ta</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2012. SemEval-2012 task 6: A pilot on semantic textual similarity. In First Joint Conference on Lexical and Computational Semantics, pages 385–393. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>shared task: Semantic textual similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), volume 1: Proceedings of the Main Conference and the Shared Task,</booktitle>
<pages>32--43</pages>
<publisher>SEM</publisher>
<contexts>
<context position="1202" citStr="Agirre et al. (2013" startWordPosition="163" endWordPosition="166">tem for measuring semantic similarity. At the core of the system is a word-to-word alignment of two texts using a maximum weight matching algorithm. The system participated in three SemEval-2014 shared tasks and the competitive results are evidence for its usability in that broad field of application. 1 Introduction Semantic similarity measures the semantic equivalence between two texts ranging from total difference to complete semantic equivalence and is usually encoded as a number in a closed interval, e. g. [0,5]. Here is an example for interpreting the numeric similarity scores taken from Agirre et al. (2013, 33): 0. The two sentences are on different topics. 1. The two sentences are not equivalent, but are on the same topic. 2. The two sentences are not equivalent, but share some details. 3. The two sentences are roughly equivalent, but some important information differs/missing. 4. The two sentences are mostly equivalent, but some unimportant details differ. 5. The two sentences are completely equivalent, as they mean the same thing. Systems capable of reliably predicting the semantic similarity between two texts can be beneficial for a This work is licensed under a Creative Commons Attribution</context>
<context position="25831" citStr="Agirre et al., 2013" startWordPosition="4134" endWordPosition="4137">90 0.694 0.426 0.653 0.617 0.719 0.780 0.636 0.654 0.466 0.674 0.673 0.772 0.849 0.687 0.706 0.475 0.706 0.711 0.788 0.852 0.715 0.727 0.465 0.700 0.699 0.781 0.848 0.722 0.722 0.448 0.722 0.677 0.752 0.791 0.706 0.697 0.475 0.711 0.715 0.795 0.864 0.721 0.733 Table 3: Results for task 10. best run complete (all training data) best overall training data best overall, no deps best overall, no deps, no WN best overall + experimental best individual training data best individ., no deps best individ., no deps, no WN best individ. + experimental on semantic textual similarity (Agirre et al., 2012; Agirre et al., 2013). It comprises two subtasks: English semantic textual similarity and Spanish semantic textual similarity. For each subtask, there are sentence pairs from various genres. We only participate in the English subtask and take the 13th place out of 38 with a weighted mean of Pearson correlation coefficients of 0.694 (best system: 0.761). 5.2 Experiments From participating in the *SEM 2013 shared task on semantic textual similarity (Greiner et al., 2013) we already know that the composition of the training data is one of the strongest influences on system performance in this task. As the individual </context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), volume 1: Proceedings of the Main Conference and the Shared Task, pages 32–43. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<title>SemEval-2014 task 10: Multilingual semantic textual similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).</booktitle>
<contexts>
<context position="24978" citStr="Agirre et al., 2014" startWordPosition="3991" endWordPosition="3994">ms slightly better than our submitted run and adding the experimental features gives another marginal improvement. Leaving out the dependency-based features has little impact but also leaving out the WordNet-based features severly hurts performance. The reason for that behaviour becomes clear when we look at the results for the individual feature groups: the WordNetbased measures are clearly the strongest feature group for predicting the semantic similarity between words and word senses. 5 SemEval-2014 Task 10 5.1 Task Description The shared task on “Multilingual Semantic Textual Similarity” (Agirre et al., 2014) is a continuation of the SemEval-2012 and *SEM 2013 shared tasks 537 Run 0.349 0.643 0.733 0.773 0.855 0.640 0.694 0.432 0.638 0.660 0.736 0.810 0.659 0.676 0.464 0.672 0.657 0.771 0.836 0.690 0.700 0.457 0.675 0.636 0.764 0.834 0.690 0.694 0.426 0.653 0.617 0.719 0.780 0.636 0.654 0.466 0.674 0.673 0.772 0.849 0.687 0.706 0.475 0.706 0.711 0.788 0.852 0.715 0.727 0.465 0.700 0.699 0.781 0.848 0.722 0.722 0.448 0.722 0.677 0.752 0.791 0.706 0.697 0.475 0.711 0.715 0.795 0.864 0.721 0.733 Table 3: Results for task 10. best run complete (all training data) best overall training data best overal</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A collection of very large linguistically processed Web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="7113" citStr="Baroni et al., 2009" startWordPosition="1112" endWordPosition="1115"> motivated by the comparatively large number of sentences in the SICK dataset (Marelli et al., 2014b) that mainly differ in their use of negation, e. g. sentence pair 42 in the training data that has a gold similarity score of 3.4: • Two people are kickboxing and spectators are watching • Two people are kickboxing and spectators are not watching 2.3 Measures Based on Distributional Document Similarity We obtain document similarity scores from two large-vocabulary distributional semantic models (DSMs). The first model is based on a 10-billion word Web corpus consisting of Wackypedia and ukWaC (Baroni et al., 2009), UMBC WebBase (Han et al., 2013), and UKCOW 2012 (Schäfer and Bildhauer, 2012). Target terms and feature terms are POS-disambiguated lemmata.4 We use parameters suggested by recent evaluation experiments: co-occurrence counts in a symmetric 4-word window, the most frequent 30,000 lexical words as features, log-likelihood scores with an additional log-transformation, and SVD dimensionality reduction of L2-normalized vectors to 1000 latent dimensions. This model provides distributional representations for 150,000 POS-disambiguated lemmata as target terms. The second model was derived from the s</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: A collection of very large linguistically processed Web-crawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>47</pages>
<contexts>
<context position="11585" citStr="Budanitsky and Hirst, 2006" startWordPosition="1827" endWordPosition="1830">t measures, we use the lemma-based DSM for computing the distribution of cosines between lemma pairs. For both alignments, we categorize the cosines between aligned lemma pairs into five heuristically determined intervals ([0.2,0.35), [0.35,0.5), [0.5,0.7), [0.7,0.999), [0.999,1.0])5 and use the proportions as features. Intuitively, the top bins correspond to links between identical words, paradigmatically related words and topically related words. All in all, we use a total of 18 features computed from the DSM-based alignments. 2.4.3 Measures Based on WordNet We utilize two state-of-the-art (Budanitsky and Hirst, 2006) WordNet similarity measures for creating alignments: Leacock and Chodorow’s (1998) normalized path length and Lin’s (1998) universal similarity measure. For both of those similarity measures we compute the best one-to-one and the best one-to-many alignment. For each alignment we compute the following two similarity measures: I) the arithmetic mean of the similarities between the aligned words from text A and text B and II) the arithmetic mean ignoring identical word pairs. 5Values in the interval [0.0,0.2) are discarded as they would be collinear with the other features. 534 We also include t</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13– 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford typed dependencies manual.</title>
<date>2008</date>
<institution>Stanford University.</institution>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning, 2008. Stanford typed dependencies manual. Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="5641" citStr="Gale and Church (1993)" startWordPosition="864" endWordPosition="867">y ignore most of the prepositions when using measures that work on tokens.3 For some tasks, we perform some additional task-specific preprocessing steps prior to parsing, cf. task descriptions below. 2.2 Simple Measures We use four simple heuristic similarity measures that need very little preprocessing. The first two are word form overlap and lemma overlap between the two texts. We take the sets of word form tokens/lemmatized tokens in text A and text B and calculate the Jaccard coefficient: overlap = |Af1B| |A U B|. The third is a heuristic for the difference in text length that was used by Gale and Church (1993) as a similarity measure for aligning sentences: ∑N di = bi − N 1bwhere J ai. ∑j=1 aj For each of the N text pairs we calculate the difference di between the observed length of text B and 1http://nlp.stanford.edu/software/corenlp. shtml 2http://networkx.github.com/ 3That is because in the CCprocessed variant of the Stanford Dependencies most prepositions are “collapsed” into dependency relations and are therefore represented as edges and not as vertices in the graph. the expected length of text B based on the length of text A. By dividing that difference di by the standard deviation of all tho</context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>William A. Gale and Kenneth W. Church. 1993. A program for aligning sentences in bilingual corpora. Computational Linguistics, 19(1):75–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zvi Galil</author>
</authors>
<title>Efficient algorithms for finding maximum matching in graphs.</title>
<date>1986</date>
<journal>Computing Surveys,</journal>
<volume>18</volume>
<issue>1</issue>
<marker>Galil, 1986</marker>
<rawString>Zvi Galil. 1986. Efficient algorithms for finding maximum matching in graphs. Computing Surveys, 18(1):23–38.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Paul Greiner</author>
<author>Stefan Evert</author>
</authors>
<title>in preparation. The Klugator Engine: A distributional approach to open questions in market research.</title>
<marker>Greiner, Evert, </marker>
<rawString>Paul Greiner and Stefan Evert. in preparation. The Klugator Engine: A distributional approach to open questions in market research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Greiner</author>
<author>Thomas Proisl</author>
<author>Stefan Evert</author>
<author>Besim Kabashi</author>
</authors>
<title>KLUE-CORE: A regression model of semantic textual similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), volume 1: Proceedings of the Main Conference and the Shared Task,</booktitle>
<pages>181--186</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3047" citStr="Greiner et al., 2013" startWordPosition="446" endWordPosition="449">another application being able to distinguish between subtle differences in meaning might be important. The three SemEval-2014 shared tasks focussing on semantic similarity (cf. Sections 3, 4 and 5 for more detailed task descriptions) provide a rich testbed for such a general system, as the individual tasks and subtasks have slightly different objectives. In the remainder of this paper, we describe SemantiKLUE, a general system for measuring semantic similarity between texts that we built based on our experience from participating in the *SEM 2013 shared task on “Semantic Textual Similarity” (Greiner et al., 2013). 2 System Description SemantiKLUE operates in two stages. In the first, unsupervised stage, a number of similarity measures are computed. Those measures are the same for all tasks and range from simple heuristics to distributional approaches to resource-heavy methods based on WordNet and dependency structures. The idea is to have a variety of similarity measures that can capture small differences in meaning as well as broad thematical similarities. In the second, supervised stage, all similarity measures obtained in this way are passed to a support vector regression learner that is trained on</context>
<context position="26283" citStr="Greiner et al., 2013" startWordPosition="4203" endWordPosition="4206">training data best individ., no deps best individ., no deps, no WN best individ. + experimental on semantic textual similarity (Agirre et al., 2012; Agirre et al., 2013). It comprises two subtasks: English semantic textual similarity and Spanish semantic textual similarity. For each subtask, there are sentence pairs from various genres. We only participate in the English subtask and take the 13th place out of 38 with a weighted mean of Pearson correlation coefficients of 0.694 (best system: 0.761). 5.2 Experiments From participating in the *SEM 2013 shared task on semantic textual similarity (Greiner et al., 2013) we already know that the composition of the training data is one of the strongest influences on system performance in this task. As the individual data sets are not very similar to each other, we tried to come up with a good subset of the available training data for each data set. In doing so, we were moderately successful as the results in Table 3 show. Running the complete system with all of the available training data on all test data sets results in a lower weighted mean than our submitted run. If we stick to using the same training data for all test data sets and optimize the subset of t</context>
</contexts>
<marker>Greiner, Proisl, Evert, Kabashi, 2013</marker>
<rawString>Paul Greiner, Thomas Proisl, Stefan Evert, and Besim Kabashi. 2013. KLUE-CORE: A regression model of semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), volume 1: Proceedings of the Main Conference and the Shared Task, pages 181–186. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aric A Hagberg</author>
<author>Daniel A Schult</author>
<author>Pieter J Swart</author>
</authors>
<title>Exploring network structure, dynamics, and function using NetworkX.</title>
<date>2008</date>
<booktitle>Proceedings of the 7th Python in Science Conference (SciPy2008),</booktitle>
<pages>11--15</pages>
<editor>In Gäel Varoquaux, Travis Vaught, and Jarrod Millman, editors,</editor>
<location>Pasadena, CA.</location>
<contexts>
<context position="4798" citStr="Hagberg et al., 2008" startWordPosition="724" endWordPosition="727">m different developmental stages of the system. In the following sections we describe the current version of the complete system for which we also report comparable results for all tasks (cf. Sections 3– 5). The whole system is implemented in Python. 2.1 Preprocessing We use Stanford CoreNLP1 for part-of-speech tagging, lemmatizing and parsing the input texts. We utilize the CCprocessed variant of the Stanford Dependencies (collapsed dependencies with propagation of conjunct dependencies; de Marneffe and Manning (2008, 13–15)) to create a graph representation of the texts using the NetworkX2 (Hagberg et al., 2008) module. All the similarity measures described below are computed on the basis of that graph representation. It is important to keep in mind that by basing all computations on the Stanford Dependencies model we effectively ignore most of the prepositions when using measures that work on tokens.3 For some tasks, we perform some additional task-specific preprocessing steps prior to parsing, cf. task descriptions below. 2.2 Simple Measures We use four simple heuristic similarity measures that need very little preprocessing. The first two are word form overlap and lemma overlap between the two tex</context>
</contexts>
<marker>Hagberg, Schult, Swart, 2008</marker>
<rawString>Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. 2008. Exploring network structure, dynamics, and function using NetworkX. In Gäel Varoquaux, Travis Vaught, and Jarrod Millman, editors, Proceedings of the 7th Python in Science Conference (SciPy2008), pages 11–15, Pasadena, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay L Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Johnathan Weese</author>
</authors>
<title>UMBC_EBIQUITY-CORE: Semantic textual similarity systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics. ACL.</booktitle>
<contexts>
<context position="7146" citStr="Han et al., 2013" startWordPosition="1118" endWordPosition="1121">e number of sentences in the SICK dataset (Marelli et al., 2014b) that mainly differ in their use of negation, e. g. sentence pair 42 in the training data that has a gold similarity score of 3.4: • Two people are kickboxing and spectators are watching • Two people are kickboxing and spectators are not watching 2.3 Measures Based on Distributional Document Similarity We obtain document similarity scores from two large-vocabulary distributional semantic models (DSMs). The first model is based on a 10-billion word Web corpus consisting of Wackypedia and ukWaC (Baroni et al., 2009), UMBC WebBase (Han et al., 2013), and UKCOW 2012 (Schäfer and Bildhauer, 2012). Target terms and feature terms are POS-disambiguated lemmata.4 We use parameters suggested by recent evaluation experiments: co-occurrence counts in a symmetric 4-word window, the most frequent 30,000 lexical words as features, log-likelihood scores with an additional log-transformation, and SVD dimensionality reduction of L2-normalized vectors to 1000 latent dimensions. This model provides distributional representations for 150,000 POS-disambiguated lemmata as target terms. The second model was derived from the second release of the Google Books</context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay L. Kashyap, Tim Finin, James Mayfield, and Johnathan Weese. 2013. UMBC_EBIQUITY-CORE: Semantic textual similarity systems. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval-2014 task 3: Cross-level semantic similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).</booktitle>
<contexts>
<context position="18923" citStr="Jurgens et al., 2014" startWordPosition="3009" endWordPosition="3012">results that are only marginally worse than those for the complete system and lead to the same places in the rankings. Additionally leaving out WordNet has a bigger impact and results in places 9 and 8 in the rankings. Regarding the individual feature groups, the DSM-alignment-based measures are the best feature group for predicting semantic similarity and the simple heuristic measures are the best feature group for predicting entailment. 4 SemEval-2014 Task 3 4.1 Task Description Unlike the other tasks, which focus on similar-sized texts, the shared task on “Cross-Level Semantic Similarity” (Jurgens et al., 2014) is about measuring semantic similarity between textual units of different lengths. It comprises four subtasks comparing I) paragraphs to sentences, II) sentences to phrases, III) phrases to words and IV) words to word senses (taken from WordNet). Due to the nature of this task, performance in it might be especially useful as an indicator for the usefulness of a system in the area of summarization. SemantiKLUE takes the fourth place out of 38 in both the official ranking by Pearson correlation and the alternative ranking by Spearman correlation. 4.2 Additional Preprocessing For the official ru</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. SemEval-2014 task 3: Cross-level semantic similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database,</booktitle>
<pages>265--283</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database, pages 265–283. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri Lin</author>
<author>Jean-Baptiste Michel</author>
<author>Erez Lieberman Aiden</author>
<author>Jon Orwant</author>
<author>Will Brockman</author>
<author>Slav Petrov</author>
</authors>
<title>Syntactic annotations for the Google Books Ngram Corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>169--174</pages>
<publisher>ACL.</publisher>
<location>Jeju Island,</location>
<contexts>
<context position="7782" citStr="Lin et al., 2012" startWordPosition="1208" endWordPosition="1211">chäfer and Bildhauer, 2012). Target terms and feature terms are POS-disambiguated lemmata.4 We use parameters suggested by recent evaluation experiments: co-occurrence counts in a symmetric 4-word window, the most frequent 30,000 lexical words as features, log-likelihood scores with an additional log-transformation, and SVD dimensionality reduction of L2-normalized vectors to 1000 latent dimensions. This model provides distributional representations for 150,000 POS-disambiguated lemmata as target terms. The second model was derived from the second release of the Google Books N-Grams database (Lin et al., 2012), using the dependency pairs provided in this version. Target and feature terms are case-folded word forms; co-occurrence ounts are based on direct syntactic relations. Here, the most frequent 50,000 word forms were used as features. All other parameters are identical to the first DSM. This model provides distributional representations for 250,000 word forms. We compute bag-of-words centroid vectors for each text as suggested by (Schütze, 1998). For each 4e.g. can_N for the noun can zi = di σd 533 A woman is using a machine made for sewing A woman is sewing with a machine text pair and DSM, we</context>
</contexts>
<marker>Lin, Michel, Aiden, Orwant, Brockman, Petrov, 2012</marker>
<rawString>Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden, Jon Orwant, Will Brockman, and Slav Petrov. 2012. Syntactic annotations for the Google Books Ngram Corpus. In Proceedings of the ACL 2012 System Demonstrations, pages 169–174, Jeju Island, Korea. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA.</location>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 296–304, San Francisco, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).</booktitle>
<contexts>
<context position="6592" citStr="Marelli et al., 2014" startWordPosition="1028" endWordPosition="1031">dencies most prepositions are “collapsed” into dependency relations and are therefore represented as edges and not as vertices in the graph. the expected length of text B based on the length of text A. By dividing that difference di by the standard deviation of all those differences, we obtain our heuristic zi. The fourth is a binary feature expressing whether the two texts differ in their use of negation. We check if one of the texts contains any of the lemmata no, not or none and the other doesn’t. That feature is motivated by the comparatively large number of sentences in the SICK dataset (Marelli et al., 2014b) that mainly differ in their use of negation, e. g. sentence pair 42 in the training data that has a gold similarity score of 3.4: • Two people are kickboxing and spectators are watching • Two people are kickboxing and spectators are not watching 2.3 Measures Based on Distributional Document Similarity We obtain document similarity scores from two large-vocabulary distributional semantic models (DSMs). The first model is based on a 10-billion word Web corpus consisting of Wackypedia and ukWaC (Baroni et al., 2009), UMBC WebBase (Han et al., 2013), and UKCOW 2012 (Schäfer and Bildhauer, 2012)</context>
<context position="15914" citStr="Marelli et al., 2014" startWordPosition="2529" endWordPosition="2532">got the best results with an RBF kernel of degree 2 and a penalty C = 0.7, so those are the parameters we use in our experiments. The SemEval-2014 Task 1 also includes a classification subtask for which we use the same 39 + 2 features for training a support vector classifier. Cross-validation suggests that the best parameter setting is a polynomial kernel of degree 2 and a penalty C = 2.5. 3 SemEval-2014 Task 1 3.1 Task Description The focus of the shared task on “Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment” (Marelli et al., 2014a) lies on the compositional nature of sentence semantics. By using a specially created data set (Marelli et al., 2014b) that tries to avoid multiword expressions and other idiomatic features of language outside the scope of compositional semantics, it provides a testbed for systems implementing compositional variants of distributional semantics. There is also an additional subtask for detecting the entailment relation (entailment, neutral, contradiction) between to sentences. Although SemantiKLUE lacks a truly sophisticated component for dealing with compositional semantics (besides trying to</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014a. SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Luisa Bentivogli</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>A SICK cure for the evaluation of compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC 2014, Reykjavik. ELRA.</booktitle>
<contexts>
<context position="6592" citStr="Marelli et al., 2014" startWordPosition="1028" endWordPosition="1031">dencies most prepositions are “collapsed” into dependency relations and are therefore represented as edges and not as vertices in the graph. the expected length of text B based on the length of text A. By dividing that difference di by the standard deviation of all those differences, we obtain our heuristic zi. The fourth is a binary feature expressing whether the two texts differ in their use of negation. We check if one of the texts contains any of the lemmata no, not or none and the other doesn’t. That feature is motivated by the comparatively large number of sentences in the SICK dataset (Marelli et al., 2014b) that mainly differ in their use of negation, e. g. sentence pair 42 in the training data that has a gold similarity score of 3.4: • Two people are kickboxing and spectators are watching • Two people are kickboxing and spectators are not watching 2.3 Measures Based on Distributional Document Similarity We obtain document similarity scores from two large-vocabulary distributional semantic models (DSMs). The first model is based on a 10-billion word Web corpus consisting of Wackypedia and ukWaC (Baroni et al., 2009), UMBC WebBase (Han et al., 2013), and UKCOW 2012 (Schäfer and Bildhauer, 2012)</context>
<context position="15914" citStr="Marelli et al., 2014" startWordPosition="2529" endWordPosition="2532">got the best results with an RBF kernel of degree 2 and a penalty C = 0.7, so those are the parameters we use in our experiments. The SemEval-2014 Task 1 also includes a classification subtask for which we use the same 39 + 2 features for training a support vector classifier. Cross-validation suggests that the best parameter setting is a polynomial kernel of degree 2 and a penalty C = 2.5. 3 SemEval-2014 Task 1 3.1 Task Description The focus of the shared task on “Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment” (Marelli et al., 2014a) lies on the compositional nature of sentence semantics. By using a specially created data set (Marelli et al., 2014b) that tries to avoid multiword expressions and other idiomatic features of language outside the scope of compositional semantics, it provides a testbed for systems implementing compositional variants of distributional semantics. There is also an additional subtask for detecting the entailment relation (entailment, neutral, contradiction) between to sentences. Although SemantiKLUE lacks a truly sophisticated component for dealing with compositional semantics (besides trying to</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014b. A SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of LREC 2014, Reykjavik. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Gaël Varoquaux</author>
<author>Alexandre Gramfort</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<contexts>
<context position="15210" citStr="Pedregosa et al., 2011" startWordPosition="2410" endWordPosition="2413">we computed the average neighbour rank of the two sentences, based on the rank of s2 among the nearest neighbours of s1 and vice versa. Since these features are generated from the task data themselves, they should adapt automatically to the range of meaning differences present in a given data set. 2.7 Machine Learning Using all the features described above, we have a total of 39 individual features that measure semantic similarity between two texts (cf. Sections 2.2 to 2.5) and two experimental features (cf. Section 2.6). In order to obtain a single similarity score, we use the scikit-learn6 (Pedregosa et al., 2011) implementation of support vector regression. In our crossvalidation experiments we got the best results with an RBF kernel of degree 2 and a penalty C = 0.7, so those are the parameters we use in our experiments. The SemEval-2014 Task 1 also includes a classification subtask for which we use the same 39 + 2 features for training a support vector classifier. Cross-validation suggests that the best parameter setting is a polynomial kernel of degree 2 and a penalty C = 2.5. 3 SemEval-2014 Task 1 3.1 Task Description The focus of the shared task on “Evaluation of compositional distributional sema</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Schäfer</author>
<author>Felix Bildhauer</author>
</authors>
<title>Building large corpora from the web using a new efficient tool chain.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC ’12),</booktitle>
<pages>486--493</pages>
<location>Istanbul, Turkey. ELRA.</location>
<contexts>
<context position="7192" citStr="Schäfer and Bildhauer, 2012" startWordPosition="1125" endWordPosition="1129">ataset (Marelli et al., 2014b) that mainly differ in their use of negation, e. g. sentence pair 42 in the training data that has a gold similarity score of 3.4: • Two people are kickboxing and spectators are watching • Two people are kickboxing and spectators are not watching 2.3 Measures Based on Distributional Document Similarity We obtain document similarity scores from two large-vocabulary distributional semantic models (DSMs). The first model is based on a 10-billion word Web corpus consisting of Wackypedia and ukWaC (Baroni et al., 2009), UMBC WebBase (Han et al., 2013), and UKCOW 2012 (Schäfer and Bildhauer, 2012). Target terms and feature terms are POS-disambiguated lemmata.4 We use parameters suggested by recent evaluation experiments: co-occurrence counts in a symmetric 4-word window, the most frequent 30,000 lexical words as features, log-likelihood scores with an additional log-transformation, and SVD dimensionality reduction of L2-normalized vectors to 1000 latent dimensions. This model provides distributional representations for 150,000 POS-disambiguated lemmata as target terms. The second model was derived from the second release of the Google Books N-Grams database (Lin et al., 2012), using th</context>
</contexts>
<marker>Schäfer, Bildhauer, 2012</marker>
<rawString>Roland Schäfer and Felix Bildhauer. 2012. Building large corpora from the web using a new efficient tool chain. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC ’12), pages 486–493, Istanbul, Turkey. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="8230" citStr="Schütze, 1998" startWordPosition="1277" endWordPosition="1278">ns for 150,000 POS-disambiguated lemmata as target terms. The second model was derived from the second release of the Google Books N-Grams database (Lin et al., 2012), using the dependency pairs provided in this version. Target and feature terms are case-folded word forms; co-occurrence ounts are based on direct syntactic relations. Here, the most frequent 50,000 word forms were used as features. All other parameters are identical to the first DSM. This model provides distributional representations for 250,000 word forms. We compute bag-of-words centroid vectors for each text as suggested by (Schütze, 1998). For each 4e.g. can_N for the noun can zi = di σd 533 A woman is using a machine made for sewing A woman is sewing with a machine text pair and DSM, we calculate the cosine similarity between the two centroid vectors as a measure of their semantic similarity. We also determine the number of unknown words in both texts according to both DSMs as additional features. 2.4 Alignment-based Measures We also use features based on word-level similarity. We separately compute similarities between words using state-of-the-art WordNet similarity measures and the two distributional semantic models describ</context>
</contexts>
<marker>Schütze, 1998</marker>
<rawString>Hinrich Schütze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>