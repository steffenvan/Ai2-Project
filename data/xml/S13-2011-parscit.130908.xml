<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002509">
<title confidence="0.997184">
JU_CSE: A CRF Based Approach to Annotation of Temporal Expres-
sion, Event and Temporal Relations
</title>
<author confidence="0.974622">
Anup Kumar Kolya1, Amitava Kundu1, Asif Ekbal2, Sivaji Bandyopadhyay1
Rajdeep Gupta1
</author>
<affiliation confidence="0.9658705">
1Dept. of Computer Science &amp; Engineering 2Dept. of Computer Science &amp; Engineering
Jadavpur Univeristy IIT Patna
</affiliation>
<address confidence="0.862623">
Kolkata-700 032, India Patna-800 013, India
</address>
<email confidence="0.973857">
{anup.kolya,amitava.jucse, asif@iitp.ac.in,
rajdeepgupta20}@gmail.com sivaji_ju_cse@yahoo.com
</email>
<sectionHeader confidence="0.995176" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994438411764706">
In this paper, we present the JUCSE system,
designed for the TempEval-3 shared task. The
system extracts events and temporal infor-
mation from natural text in English. We have
participated in all the tasks of TempEval-3,
namely Task A, Task B &amp; Task C. We have
primarily utilized the Conditional Random
Field (CRF) based machine learning tech-
nique, for all the above tasks. Our system
seems to perform quite competitively in Task
A and Task B. In Task C, the system’s per-
formance is comparatively modest at the ini-
tial stages of system development. We have
incorporated various features based on differ-
ent lexical, syntactic and semantic infor-
mation, using Stanford CoreNLP and Wordnet
based tools.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999899636363636">
Temporal information extraction has been a popu-
lar and interesting research area of Natural Lan-
guage Processing (NLP) for quite some time.
Generally, a lot of events are described in a variety
of newspaper texts, stories and other important
documents where the different events described
happen at different time instants. The temporal
location and ordering of these events are either
specified or implied. Automatic identification of
time expressions and events and annotation of
temporal relations constitute an important task in
</bodyText>
<page confidence="0.990681">
64
</page>
<bodyText confidence="0.992828396825397">
text analysis. These are also important in a wide
range of NLP applications that include temporal
question answering, machine translation and doc-
ument summarization.
A lot of research in the area of temporal infor-
mation extraction has been conducted on multiple
languages, including English and several European
languages. The TimeML was first developed in
2002 in an extended workshop called TERQAS
(Time and Event Recognition for Question An-
swering Systems) and, in 2003, it was further de-
veloped in the context of the TANGO workshop
(TimeML Annotation Graphical Organizer). Since
then most of the works in this research arena have
been conducted in English. The variety of works
include TimeML (Pustejovsky et al., 2003), the
development of a temporally annotated corpus
Time-Bank (Pustejovsky et al., 2003), the temporal
evaluation challenges TempEval-1 (Verhagen et
al., 2007), TempEval-2 (Pustejovsky and Verha-
gen, 2010). In the series of Message Understanding
Conferences (MUCs) that started from 1987 and
the Sheffield Temporal Annotation scheme
(STAG) (Setzer &amp;Gaizauskas, 2000) the aim was
to identify events in news text and determine their
relationship with points on a temporal line.
In the series of TempEval evaluation exercises,
TempEval-1 was the first one where the focus was
on identification of three types of temporal rela-
tion: relation between an event and a time expres-
sion in the same sentence, relation between an
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 64–72, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
event and the document creation time, and relation
between two main events in consecutive sentences.
TempEval-2 was a follow up to TempEval-1
and consisted of six subtasks rather than three. It
added (i) identification of time expressions and
determination of values of the attributes TYPE and
VAL (ii) identification of event expressions and
determination of its attribute values. It included the
previous three relation tasks from TempEval-1 and
an additional task of annotating temporal relation
between a pair of events where one subordinates
the other.
We have participated in all three tasks of
TempEval-3- Task A, Task B and Task C. A com-
bination of CRF based machine learning and rule
based techniques has been adopted for temporal
expression extraction and determination of attrib-
ute values of the same (Task A). We have used a
CRF based technique for event extraction (Task
B), with the aid of lexical, semantic and syntactic
features. For determination of event attribute val-
ues we have used simple rule based techniques.
Automatic annotation of temporal relation between
event-time in the same sentence, event-DCT rela-
tions, mainevent-mainevent relations in consecu-
tive sentences and subevent-subevent relations in
the same sentences has been introduced as a new
task (Task-C) in the TempEval-3 exercise. We
have adopted a CRF based technique for the same
as well.
</bodyText>
<sectionHeader confidence="0.891988" genericHeader="method">
2 The JU_CSE System Approach
</sectionHeader>
<bodyText confidence="0.9990438">
The JU_CSE system for the TempEval-3 shared
task uses mainly a Conditional Random Field
(CRF) machine learning approach to achieve Task
A, Task B &amp; Task C. The workflow of our system
is illustrated in Figure 1.
</bodyText>
<sectionHeader confidence="0.763316" genericHeader="method">
2.1 Task A: Temporal Expression Identifica-
tion and Normalization
</sectionHeader>
<subsectionHeader confidence="0.977708">
Temporal Expression Identification:
</subsectionHeader>
<bodyText confidence="0.999862">
We have used CRF++ 0.571, an open source im-
plementation of the Conditional Random Field
(CRF) machine learning classifier for our experi-
ments. CRF++ templates have been used to capture
the relation between the different features in a se-
quence to identify temporal expressions. Temporal
expressions mostly appear as multi-word entities
such as “the next three days”. Therefore the use of
CRF classifier that uses context information of a
token seemed most appropriate.
Initially, all the sentences have been changed to
a vertical token-by-token level sequential structure
for temporal expressions representation by a B-I-O
encoding, using a set of mostly lexical features. In
this encoding of temporal expression, “B” indi-
cates the ‘beginning of sequence’, “I” indicates a
token inside a sequence and “O” indicates an out-
side word. We have carefully chosen the features
list based on the several entities that denote month
names, year, weekdays, various digit expressions
(day, time, AM, PM etc.) In certain temporal ex-
pression patterns (several months, last evening)
some words (several, last) act as modifiers to the
following words that represent the time expression.
Temporal expressions include time expression
modifiers, relative days, periodic temporal set,
year-eve day, month name with their short pattern
forms, season of year, time of day, decade list and
so on. We have used the POS information of each
token as a feature. We have carefully accounted for
a simple intuition revelation that most temporal
expressions contain some tokens conveying the
“time” information while others possibly convey-
ing the “quantity” of time. For example, in the ex-
pression “next three days”, “three” quantifies
“days”. Following are the different temporal ex-
pressions lists that have been utilized:
</bodyText>
<listItem confidence="0.999770714285714">
• A list of time expression modifiers: this,
mid, recent, earlier, beginning, late etc.
• A list of relative days: yesterday, tomor-
row etc.
• A list of periodic temporal set: hourly,
nightly etc.
• A list of year eve day: Christmas Day,
Valentine Day etc.
• A list of month names with their short pat-
tern forms: April, Apr. etc.
• A list of season of year: spring, winter etc.
• A list of time of day: morning, afternoon,
evening etc.
• A list of decades list: twenties, thirties etc.
</listItem>
<footnote confidence="0.974629">
1 http://crfpp.googlecode.com/svn/trunk/doc/index.html
</footnote>
<page confidence="0.999258">
65
</page>
<subsectionHeader confidence="0.889485">
Raw Text:
</subsectionHeader>
<bodyText confidence="0.955583">
For his part, Fidel Castro is the ultimate political
survivor. People have predicted his demise so
many times, and the US has tried to hasten it on
several occasions. Time and again, he endures.
</bodyText>
<equation confidence="0.912843538461538">
&lt;MAKEINSTANCE eiid=&amp;quot;ei1” eventID=&amp;quot;e1&amp;quot; pos=&amp;quot;VERB&amp;quot;
tense=&amp;quot;PRESENT&amp;quot; aspect=&amp;quot;PERFECTIVE&amp;quot; polarity=&amp;quot;POS&amp;quot; /&gt;
&lt;MAKEINSTANCE eiid=&amp;quot;ei2” eventID=&amp;quot;e2&amp;quot; pos=&amp;quot;NOUN&amp;quot;
tense=&amp;quot;PRESENT&amp;quot; aspect=&amp;quot;PERFECTIVE&amp;quot; polarity=&amp;quot;POS&amp;quot; /&gt;
&lt;MAKEINSTANCE eiid=&amp;quot;ei3” eventID=&amp;quot;e3&amp;quot; pos=&amp;quot;VERB&amp;quot;
tense=&amp;quot;PRESENT&amp;quot; aspect=&amp;quot;PERFECTIVE&amp;quot; polarity=&amp;quot;POS&amp;quot; /&gt;
.
For...OTHERS
nearly . TIMEX3
forty.... ... TIMEX3
years.... TIMEX3
.
.
</equation>
<bodyText confidence="0.713453">
Determine
Event
Class
Rule based approach to obtain tense, as-
pect, polarity, modality etc. for events
</bodyText>
<listItem confidence="0.9784294">
• Tokenize with Stanford CoreNLP
• Obtain POS tags of tokens
• Extract features from tokens
• Identify the features for event annotation and
temporal annotation separately
</listItem>
<figure confidence="0.998590757575757">
Event &amp;
Time
Features
CRF
Tag
TIMEX3
tokens
Tag EVENT
tokens
.
People...OTHERS
have OTHERS
predicted .... ... EVENT
his OTHERS
.
.
CoreNLP
for “type”
&amp; “velue”
CRF
Enlist entity pairs with features
&lt;mainevent-mainevent&gt;
&lt;event-event&gt;
&lt;event-dct&gt;
&lt;event-time&gt;
Temporal Relations:
&lt;TLINK lid=&amp;quot;l1&amp;quot; relType=&amp;quot;BEFORE&amp;quot;
eventInstanceID=&amp;quot;ei1&amp;quot; relatedTo-
Time=&amp;quot;t0&amp;quot; /&gt;
&lt;TLINK lid=&amp;quot;l2&amp;quot; relType=&amp;quot;BEFORE&amp;quot;
eventInstanceID=&amp;quot;ei2&amp;quot; relatedToEven-
tInstance=&amp;quot;ei1&amp;quot; /&gt;
Annotated Text
</figure>
<figureCaption confidence="0.941454444444444">
For his part, Fidel Castro is the ultimate political survivor.
People have &lt;EVENT class=&amp;quot;I_ACTION&amp;quot;
eid=&amp;quot;e1&amp;quot;&gt;predicted&lt;/EVENT&gt; his &lt;EVENT
class=&amp;quot;OCCURRENCE&amp;quot; eid=&amp;quot;e2&amp;quot;&gt;demise&lt;/EVENT&gt; so
many times, and the US has &lt;EVENT class=&amp;quot;I_ACTION&amp;quot;
eid=&amp;quot;e3&amp;quot;&gt;tried&lt;/EVENT&gt; to &lt;EVENT
class=&amp;quot;OCCURRENCE&amp;quot; eid=&amp;quot;e4&amp;quot;&gt;hasten&lt;/EVENT&gt; it on
several occasions.
Figure 1.The JU_CSE System Architecture
</figureCaption>
<bodyText confidence="0.995218366666667">
the documents. In this way, values of different
dates have been inferred e.g. last year, Monday,
and today.
Determination of Iormalized value and type
of Temporal Expressions:
Temporal expressions in documents are generally
defined with the type and value attributes. All the
temporal expressions can be differentiated into
three types (i) explicit (ii) relative and (iii) implicit
temporal expressions. For example, the expression
“October 1998” refers to a specific month of the
year which can be normalized without any addi-
tional information. On the other hand, the relative
expression “yesterday” can’t be normalized with-
out the knowledge of a corresponding reference
time. The reference time can either be a temporal
expression or the Document Creation Time marked
in the document. Consider the following piece of
text: “Yesterday was the 50th independence of In-
dia”. The First Independence Day of India is 15th
august 1947.” Here “Yesterday” can be normal-
ized as “15-08-1997”. It may be noted that infor-
mation such as “First Independence Day of India”
can be directly accessed from the timestamp calen-
dar, through the metadata of a document. The third
type of temporal expressions includes implicit ex-
pressions such as names of festival days, birthdays
and holidays or events. These expressions are
mapped to available calendar timeline to find out
their normalized values.
</bodyText>
<table confidence="0.99907975">
Temporal Type Value
Expression
A couple of DURATION P2Y
years
October DATE “1997-10”
Every day SET P1D
2 P.M. TIME 2013-02-01T14:00
Now DATE PRESENT_REF&amp;quot;
</table>
<tableCaption confidence="0.9498985">
Table 1: TimeML normalized type and value attributes
for temporal expressions
</tableCaption>
<bodyText confidence="0.99779325">
We have implemented a combined technique us-
ing our handcrafted rules and annotations given by
the Stanford CoreNLP tool to determine the ‘type’-
s and ‘value’-s. Four types TIME, DATE,
DURATION and SET of temporal expressions are
defined in the TimeML framework. Next, we have
evaluated the normalized value of temporal expres-
sions using Document Creation Time (DCT) from
</bodyText>
<subsectionHeader confidence="0.954959333333333">
2.2 Task B: Extraction of Event Words and
Determination of Event Attribute Values
Event Extraction
</subsectionHeader>
<bodyText confidence="0.9996819">
In our evaluation framework, we have used the
Stanford CoreNLP tool extensively to tokenize,
lemmatize, named-entity annotate and part-of-
speech tag the text portions of the input files. For
event extraction, the features have been considered
at word level, where each word has its own set of
features. The general features used to train our
CRF model are:
Morphological Features: Event words are rep-
resented mostly as verbs and nouns. The major
problem is detecting the events having non-verbal
PoS labels. Linguistically, non-verbal wordforms
are derived from verbal wordforms. Various inflec-
tional and derivational morphological rules are
involved in the process of evolving from verbal to
non-verbal wordforms. We have used a set of
handcrafted rules to identify the suffixes such as (‘-
ción’, ‘-tion’ or ‘-ion’), i.e., the morphological
markers of word token, where Person, Location
and Organization words are not considered. The
POS and lemma, in a 5-window (-2, +2), has been
used for event extraction.
Syntactic Feature: Different event words no-
tions are contained in the sentences such as: verb-
noun combinations structure, the complements of
aspectual prepositional phrases (PPs) headed by
prepositions and a particular type of complex
prepositions. These notions are captured to be used
as syntactic features for event extraction.
WordIet Feature: The RiTa Wordnet2 package
has been effectively used to extract different prop-
erties of words, such as Synonyms, Antonyms,
Hypernyms, &amp; Hyponyms, Holonyms, Meronyms,
Coordinates, &amp; Similars, Nominalizations, Verb-
Groups, &amp; Derived-terms. We have used these
Wordnet properties in the training file for the CRF
in the form of binary features for verbs and nouns
indicating if the words like “act”, ”activity”, ”phe-
nomenon” etc. occur in different relations of the
Wordnet ontology.
</bodyText>
<footnote confidence="0.959975">
2 http://www.rednoise.org/rita/wordnet/documentation/
</footnote>
<page confidence="0.998907">
67
</page>
<bodyText confidence="0.997580591836735">
Features using Semantic Roles: We use Se-
mantic Role Label (SRL) (Gildea et el, 2002; Pra-
dhan et al, 2004; Gurevich et al, 2006) to identify
different useful features for event extraction. For
each predicate in a sentence acting as event word,
semantic roles extract all constituents; determine
their arguments (agent, patient, etc.) and adjuncts
(locative, temporal, etc.). Some of the other fea-
tures like predicate, voice and verb sub-
categorization are shared by all the nodes in the
tree. In the present work, we use predicate as an
event. Semantic roles can be used to detect the
events that are nominalizations of verbs such as
agreement for agree or construction for construct.
Event nominalizations often share the same seman-
tic roles as verbs, and often replace them in written
language. Noun words, morphologically derived
from verbs, are commonly defined as deverbal
nouns. Event and result nominalizations constitute
the bulk of deverbal nouns. The first class refers to
an event/activity/process, with the nominal ex-
pressing this action (e.g., killing, destruction etc.).
Nouns in the second class describe the result or
goal of an action (e.g., agreement, consensus etc.).
Many nominals denote both the event and result
(e.g., selection). A smaller class is agent/patient
nominalizations that are usually identified by suf-
fixes such as -er, -or etc., while patient nominaliza-
tions end with -ee, -ed (e.g. employee).
Object information of Dependency Relations
(DR): We have developed handcrafted rules to
identify features for CRF training, based on the
object information present in the dependency rela-
tions of parsed sentences. Stanford Parser (de
Marneffe et al., 2006), a probabilistic lexicalized
parser containing 45 different Part-of-Speech
(PoS) tags of Penn Treebank is used to get the
parsed sentences with dependency relations. The
dependency relations are found out for the predi-
cates “dobj” so that the direct object related com-
ponents in the “dobj” predicate is considered as the
feature for the event expression. Initially the input
sentences are passed to the dependency parser3.
From the parsed output verb noun combination
direct object (dobj) dependency relations are ex-
tracted. These dobj relations basically inform us
that direct object of a VP is the noun phrase which
is the (accusative) object of the verb; the direct
object of a clause is the direct object of the VP
</bodyText>
<footnote confidence="0.564324">
3 http://nlp.stanford.edu:8080/parser/
</footnote>
<bodyText confidence="0.967632390243902">
which is the predicate of that clause. Within the
dobj relation governing verb word and dependent
noun words are acting as important features for
event identification when dependent word is not
playing any role in other dependency relation
(nsubj, prep_of, nn ,etc.) of the sentence.
In this way, we have set list of word tokens and
its features to train the recognition model. Then the
model will give to each word one of the valid la-
bels.
Determination of various Event Attribute
Values:
Values of different event attributes have been
computed as follows:
Class: Identification of the class of an event has
been done using a simple, intuitive, rule based ap-
proach. Here too, the hypernym list of an event
token from RitaWordnet has been deployed to de-
termine the class of the respective event. In this
case, OCCURRENCE has been considered the de-
fault class.
Tense, Aspect, POS: These three attributes are
the obligatory attributes of MAKEINSTANCE
tags. To determine the tense, aspect and polarity of
an event, we have used the “parse” annotator in
CoreNLP. We annotated each sentence with the
Stanford dependency relations using the above an-
notator. Thereafter various specific relations were
used to determine the tense, aspect and POS of an
event token, with another rule based approach. For
example, in the phrase “has been abducted”, the
token “been” appears as the dependent in an “aux”
relation with the event token “abducted”; and
hence the aspect “PERFECTIVE” is inferred. The
value “NONE” has been used as the default value
for both tense and aspect.
Polarity and Modality: Polarity of event tokens
are determined using Stanford dependency rela-
tions too; here the “neg” relation. To determine the
modality we search for modal words in “aux” rela-
tions with the event token.
</bodyText>
<subsectionHeader confidence="0.988823">
2.3 Task C: Temporal Relation Annotation
</subsectionHeader>
<bodyText confidence="0.999966">
We have used the gold-standard TimeBank fea-
tures for events and times for training the CRF. In
the present work, we mainly use the various com-
binations of the following features:
</bodyText>
<page confidence="0.996933">
68
</page>
<listItem confidence="0.999579111111111">
(i) Part of Speech (POS)
(ii) Event Tense
(iii) Event Aspect
(iv) Event Polarity
(v) Event Modality
(vi) Event Class
(vii) Type of temporal expression
(vii) Event Stem
(viii) Document Creation Time (DCT).
</listItem>
<bodyText confidence="0.985523928571428">
The following subsections describe how various
temporal relations are computed.
Event-DCT
We take the combined features of every event pre-
sent in the text and the DCT for this purpose.
Derived Features: We have identified different
types of context based syntactic features which are
derived from text to distinguish the different types
of temporal relations. In this task, following fea-
tures help us to identify the event-DCT relations,
specially “AFTER” temporal relations:
(i)Modal Context: Whether or not the event word
has one of the modal context words like- will,
shall, can, may, or any of their variants (might,
could, would, etc.).In the sentence: “The entire
world will [EVENT see] images of the Pope in Cu-
ba”. Here “will” context word helps us to deter-
mine event-DCT relation ‘AFTER’.
(ii)Preposition Context: Any prepositions preced-
ing an event or time expression. We consider an
example:”Children and invalids would be permit-
ted to [EVENT leave] Iraq”. Here the preposition
to helps us to determine event-DCT relation
‘AFTER’. The same principle goes for time too: in
the expressions on Friday and for nearly forty
years, the prepositions on and for governs the time.
(iii)Context word before or after temporal expres-
sion: context words like before, after, less than,
greater than etc. help us to determine event-time
temporal relation identification. Consider an ex-
ample: “After ten years of [EVENT boom] ....”
Event-Time
Derived Features: We extract all events from eve-
ry sentence. For every temporal expression in a
sentence, we pair an event in the sentence with the
former so that the temporal relation can be deter-
mined.
Similar to annotation of event-DCT relations,
here too, we have identified different types of con-
text based temporal expression features which are
derived from text to distinguish the different types
of temporal relations. In this task, the following
features help us to distinguish between event and
time relations, specially “AFTER” and “BEFORE”
temporal relations. The following features are de-
rived from text.
(i)Type of temporal expression: Represents the
temporal relationship holding between events,
times, or between an event and a time of the event.
(ii)Temporal signal: Represents temporal preposi-
tions “on” (on this coming Sunday) and slightly
contribute to the overall score of classifiers
(iii)Temporal Expression in the target sentence:
Takes the values greater than, less than, equal or
none. These values contribute to the overall score
of classifiers.
</bodyText>
<subsectionHeader confidence="0.9128445">
Mainevent-Mainevent and Subevent-
Subevent
</subsectionHeader>
<bodyText confidence="0.999152958333333">
The task demands that the main event of every sen-
tence be determined. As a heuristic decision, we
have assumed that the first event that appears in a
sentence is its main event. We pair up main events
(if present) from consecutive sentences and use
their combined features to determine their temporal
relation. For the events belonging to a single sen-
tence, we take into account the combined features
of all possible pairs of sentential events.
Derived Features: We have identified different
types of context based syntactic features which are
derived from text to distinguish the different types
of temporal relations.
(i)Relational context: If a relation holding be-
tween the previous event and the current event is
“AFTER”, the current one is in the past. This in-
formation helps us to identify the temporal relation
between the current event and successive event.
(ii)Modal Context: Whether or not the event word
has one of the context words like, will, shall, can,
may, or any of their variants (might, could, would,
etc.). The verb and auxiliaries governing the next
event play as an important feature in event-event
temporal relation identification.
</bodyText>
<page confidence="0.998428">
69
</page>
<bodyText confidence="0.99943994117647">
(iii)Ordered based context: In event-event rela-
tion identification, when EVENT-1, EVENT-2,
and EVENT-3 are linearly ordered, then we have
assigned true/false as feature value from tense and
aspect shifts in this ordered pair.
(iv) Co-reference based feature: We have used
co-referential features as derived feature using our
in-house system based on Standford CoreNLP tool,
where two event words within or outside one sen-
tence are referring to the same event, i.e. two event
words co-refer in a discourse.
(v)Event-DCT relation based feature: We have
included event-document creation times (DCT)
temporal relation types as feature of event-event
relation identification.
(ii) Preposition Context: Any prepositions before
the event or time, we consider an exam-
ple:”Children and invalids would be permitted to
[EVENT leave] Iraq”. Here the preposition to
helps us determine the event-DCT relation
‘AFTER’.
(vi) Context word before or after temporal ex-
pression: Context words like before, after, less
than, greater than help us determine event- event
temporal relations .We consider an example:”After
ten years of [EVENT boom] ....”
(vii)Stanford parser based clause boundaries
features: The two consecutive sentences are first
parsed using Stanford dependency parser and then
clause boundaries are identified. Then, considering
the prepositional context and tense verb of the
clause, temporal relations are identified where all
temporal expressions are situated in the same
clause.
</bodyText>
<sectionHeader confidence="0.999558" genericHeader="evaluation">
3 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.99990625">
For the extraction of time expressions and events
(tasks A and B), precision, recall and F1-score
have been used as evaluation metrics, using the
following formulae:
</bodyText>
<equation confidence="0.923050333333333">
precision (P) = tp/(tp + fp)
recall (R) = tp/(tp + fn)
F-measure = 2 *(P * R) / (P + R).
</equation>
<bodyText confidence="0.993055136363636">
Where, tp is the number of tokens that are part of
an extent in keys and response, fp is the number of
tokens that are part of an extent in the response but
not in the key, and fn is the number of tokens that
are part of an extent in the key but not in the re-
sponse. Additionally attribute accuracies computed
according to the following formulae have also been
reported.
Attr. Accuracy = Attr. F1 / Entity Extraction F1
Attr. R = Attr. Accuracy * Entity R
Attr. P = Attr. Accuracy * Entity P
Performance in task C is judged with the aid of the
Temporal Awareness score proposed by UzZaman
and Allen (2011)
The JU_CSE system was evaluated on the TE-3
platinum data. Table 2 reports JU_CSE’s perfor-
mance in timex extraction Task A. Under the re-
laxed match scheme, the F1-score stands at
86.38% while the strict match scheme yields a F1-
score of 75.41%. As far as TIMEX attributes are
concerned, the F1-scores are 63.81% and 73.15%
for value and type respectively.
</bodyText>
<table confidence="0.9985945">
Timex Extraction Timex Attribute
F1 P R Strict F1 Strict P Strict R Value Type Value Type
F1 F1 Accuracy Accuracy
86.38 93.28 80.43 75.49 81.51 70.29 63.81 73.15 73.87 84.68
</table>
<tableCaption confidence="0.962525">
Table 2:JU_CSE system’s TE-3 Results on Timex Task A
</tableCaption>
<table confidence="0.9997585">
Event Extraction Event Attribute
F1 P R Class Tense Aspect Class Tense Aspect
F1 F1 F1 Accuracy Accuracy Accuracy
78.57 80.85 76.41 52.65 58.58 72.09 67.01 74.56 91.75
</table>
<tableCaption confidence="0.998998">
Table 3:JU_CSE system’s TE-3 Results on Event Task B
</tableCaption>
<page confidence="0.996665">
70
</page>
<bodyText confidence="0.998602">
Table 3 reports the system’s performance in
event extraction (Task B) on TE-3 platinum da-
ta. F1-score for event extraction is 78.57%. At-
tribute F1-scores are 52.65%, 58.58% and
72.09% for class, tense and aspect respectively.
In both entities extraction tasks recall is nota-
bly lower than precision. The F1-scores for
event attributes are modest given that the attrib-
utes were computed using handcrafted rules.
However, the handcrafted approach can be treat-
ed as a good baseline to start with. Normaliza-
tion is proved to be a challenging task.
</bodyText>
<table confidence="0.99509325">
Task F1 P R
Task-ABC 24.61 19.17 34.36
Task-C 26.41 21.04 35.47
Task-C-relation-only 34.77 35.07 34.48
</table>
<tableCaption confidence="0.958781333333333">
Table 4: JU_CSE system’s TE-3 Temporal Aware-
ness results on Task ABC, TaskC-only &amp; TaskC-
relation-only
</tableCaption>
<bodyText confidence="0.998623846153846">
Table 4 presents the Temporal Awareness F1-
score for TaskABC, TaskC and TaskC-relation-
only. For TaskC-only evaluation, the event and
timex annotated data was provided and one had
to identify the TLINKs and classify the temporal
relations. In the TaskC-relation-only version the
timex and event annotations including their at-
tributes as well as TLINKs were provided save
the relation classes. Only the relation classes had
to be determined. The system yielded a temporal
awareness F1-score of 24.6% for TaskABC,
26.41% for TaskC-only and 34.77% for TaskC-
relation-only version.
</bodyText>
<sectionHeader confidence="0.999246" genericHeader="conclusions">
4 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999975730769231">
In this paper, we have presented the JU_CSE
system for the TempEval-3 shared task. Our sys-
tem in TempEval-3 may be seen upon as an im-
provement over our earlier endeavor in
TempEval-2. We have participated in all tasks of
the TempEval-3 exercise. We have incorporated
a CRF based approach in our system for all
tasks. The JU_CSE system for temporal infor-
mation extraction is currently undergoing a lot
of extensive experimentation. The one reported
in this article seemingly has a significant scope
of improvement. Preliminarily, the results yield-
ed are quite competitive and encouraging. Event
extraction and Timex extraction F1-scores at
78.58% and 86.38% encourage us to further de-
velop our CRF based scheme. We expect better
results with additional features and like to con-
tinue our experimentations with other semantic
features for the CRF classifier. Our rule-based
approach for event attribute determination how-
ever yields modest F1-scores- 52.65% &amp;
58.58% for class and tense. We intend to explore
other machine learning techniques for event at-
tribute classification. We also intend to use parse
tree based approaches for temporal relation an-
notation.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999873428571428">
This work has been partially supported by a
grant from the English to Indian language Ma-
chine Translation (EILMT) project funded by
the Department of Information and Technology
(DIT), Government of India. We would also like
to thank to Mr. Jiabul Sk. for his technical con-
tribution.
</bodyText>
<sectionHeader confidence="0.999246" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997422722222222">
A. Setzer, and R. Gaizauskas. 2000. Annotating
Events and Temporal Information in Newswire
Texts. In LREC 2000, pages 1287–1294, Athens.
D. Gildea, and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28(3):245–288.
James Pustejovsky, José Castano, Robert Ingria,
Roser Sauri, Robert Gaizauskas, Andrea Setzer,
Graham Katz, and Dragomir Radev. 2003.
TimeML: Robust specification of event and tem-
poral expressions in text. New directions in ques-
tion answering, 3: 28-34.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James
Pustejovsky. 2007. Semeval-2007 task 15:
Tempeval temporal relation identification. In Pro-
ceedings of the 4th International Workshop on
Semantic Evaluations, pages 75-80, ACL.
</reference>
<page confidence="0.980096">
71
</page>
<reference confidence="0.999538947368421">
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages
57- 62. ACL.
Olga Gurevich, Richard Crouch, Tracy H. King, and
V. de Paiva. 2006. Deverbal Nouns in Knowledge
Representation. Proceedings of FLAIRS, pages
670–675, Melbourne Beach, FL.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low Semantic Parsing using Support Vector Ma-
chine. Proceedings of HLT/NAACL-2004,
Boston, MA.
UzZaman, N. and J.F. Allen (2011), “Temporal
Evaluation.” In Proceedings of The 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies
(Short Paper), Portland, Oregon, USA.
</reference>
<page confidence="0.998722">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.459545">
<title confidence="0.994366">JU_CSE: A CRF Based Approach to Annotation of Temporal Expression, Event and Temporal Relations</title>
<author confidence="0.998832">Kumar Amitava Asif Sivaji</author>
<affiliation confidence="0.992279">of Computer Science &amp; Engineering of Computer Science &amp; Engineering</affiliation>
<address confidence="0.751735">Jadavpur Univeristy IIT Patna Kolkata-700 032, India Patna-800 013, India</address>
<email confidence="0.9777915">anup.kolya@gmail.comsivaji_ju_cse@yahoo.com</email>
<email confidence="0.9777915">amitava.jucse@gmail.comsivaji_ju_cse@yahoo.com</email>
<email confidence="0.9777915">asif@iitp.ac.in@gmail.comsivaji_ju_cse@yahoo.com</email>
<email confidence="0.9777915">rajdeepgupta20@gmail.comsivaji_ju_cse@yahoo.com</email>
<abstract confidence="0.997484833333333">In this paper, we present the JUCSE system, designed for the TempEval-3 shared task. The system extracts events and temporal information from natural text in English. We have participated in all the tasks of TempEval-3, namely Task A, Task B &amp; Task C. We have primarily utilized the Conditional Random Field (CRF) based machine learning technique, for all the above tasks. Our system seems to perform quite competitively in Task A and Task B. In Task C, the system’s performance is comparatively modest at the initial stages of system development. We have incorporated various features based on different lexical, syntactic and semantic information, using Stanford CoreNLP and Wordnet based tools.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Setzer</author>
<author>R Gaizauskas</author>
</authors>
<title>Annotating Events and Temporal Information in Newswire Texts. In LREC</title>
<date>2000</date>
<pages>1287--1294</pages>
<location>Athens.</location>
<marker>Setzer, Gaizauskas, 2000</marker>
<rawString>A. Setzer, and R. Gaizauskas. 2000. Annotating Events and Temporal Information in Newswire Texts. In LREC 2000, pages 1287–1294, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea, and D. Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>José Castano</author>
<author>Robert Ingria</author>
<author>Roser Sauri</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Graham Katz</author>
<author>Dragomir Radev</author>
</authors>
<title>TimeML: Robust specification of event and temporal expressions in text. New directions in question answering,</title>
<date>2003</date>
<volume>3</volume>
<pages>28--34</pages>
<contexts>
<context position="2411" citStr="Pustejovsky et al., 2003" startWordPosition="362" endWordPosition="365">question answering, machine translation and document summarization. A lot of research in the area of temporal information extraction has been conducted on multiple languages, including English and several European languages. The TimeML was first developed in 2002 in an extended workshop called TERQAS (Time and Event Recognition for Question Answering Systems) and, in 2003, it was further developed in the context of the TANGO workshop (TimeML Annotation Graphical Organizer). Since then most of the works in this research arena have been conducted in English. The variety of works include TimeML (Pustejovsky et al., 2003), the development of a temporally annotated corpus Time-Bank (Pustejovsky et al., 2003), the temporal evaluation challenges TempEval-1 (Verhagen et al., 2007), TempEval-2 (Pustejovsky and Verhagen, 2010). In the series of Message Understanding Conferences (MUCs) that started from 1987 and the Sheffield Temporal Annotation scheme (STAG) (Setzer &amp;Gaizauskas, 2000) the aim was to identify events in news text and determine their relationship with points on a temporal line. In the series of TempEval evaluation exercises, TempEval-1 was the first one where the focus was on identification of three ty</context>
</contexts>
<marker>Pustejovsky, Castano, Ingria, Sauri, Gaizauskas, Setzer, Katz, Radev, 2003</marker>
<rawString>James Pustejovsky, José Castano, Robert Ingria, Roser Sauri, Robert Gaizauskas, Andrea Setzer, Graham Katz, and Dragomir Radev. 2003. TimeML: Robust specification of event and temporal expressions in text. New directions in question answering, 3: 28-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Frank Schilder</author>
<author>Mark Hepple</author>
<author>Graham Katz</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2007 task 15: Tempeval temporal relation identification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>75--80</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2569" citStr="Verhagen et al., 2007" startWordPosition="383" endWordPosition="386">le languages, including English and several European languages. The TimeML was first developed in 2002 in an extended workshop called TERQAS (Time and Event Recognition for Question Answering Systems) and, in 2003, it was further developed in the context of the TANGO workshop (TimeML Annotation Graphical Organizer). Since then most of the works in this research arena have been conducted in English. The variety of works include TimeML (Pustejovsky et al., 2003), the development of a temporally annotated corpus Time-Bank (Pustejovsky et al., 2003), the temporal evaluation challenges TempEval-1 (Verhagen et al., 2007), TempEval-2 (Pustejovsky and Verhagen, 2010). In the series of Message Understanding Conferences (MUCs) that started from 1987 and the Sheffield Temporal Annotation scheme (STAG) (Setzer &amp;Gaizauskas, 2000) the aim was to identify events in news text and determine their relationship with points on a temporal line. In the series of TempEval evaluation exercises, TempEval-1 was the first one where the focus was on identification of three types of temporal relation: relation between an event and a time expression in the same sentence, relation between an Second Joint Conference on Lexical and Com</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz, and James Pustejovsky. 2007. Semeval-2007 task 15: Tempeval temporal relation identification. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 75-80, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Roser Sauri</author>
<author>Tommaso Caselli</author>
<author>James Pustejovsky</author>
</authors>
<date>2010</date>
<booktitle>Semeval-2010 task 13: Tempeval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>57--62</pages>
<publisher>ACL.</publisher>
<marker>Verhagen, Sauri, Caselli, Pustejovsky, 2010</marker>
<rawString>Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. 2010. Semeval-2010 task 13: Tempeval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 57- 62. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Gurevich</author>
<author>Richard Crouch</author>
<author>Tracy H King</author>
<author>V de Paiva</author>
</authors>
<title>Deverbal Nouns in Knowledge Representation.</title>
<date>2006</date>
<booktitle>Proceedings of FLAIRS,</booktitle>
<pages>670--675</pages>
<location>Melbourne Beach, FL.</location>
<marker>Gurevich, Crouch, King, de Paiva, 2006</marker>
<rawString>Olga Gurevich, Richard Crouch, Tracy H. King, and V. de Paiva. 2006. Deverbal Nouns in Knowledge Representation. Proceedings of FLAIRS, pages 670–675, Melbourne Beach, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Shallow Semantic Parsing using Support Vector Machine.</title>
<date>2004</date>
<booktitle>Proceedings of HLT/NAACL-2004,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="13204" citStr="Pradhan et al, 2004" startWordPosition="1985" endWordPosition="1989">ckage has been effectively used to extract different properties of words, such as Synonyms, Antonyms, Hypernyms, &amp; Hyponyms, Holonyms, Meronyms, Coordinates, &amp; Similars, Nominalizations, VerbGroups, &amp; Derived-terms. We have used these Wordnet properties in the training file for the CRF in the form of binary features for verbs and nouns indicating if the words like “act”, ”activity”, ”phenomenon” etc. occur in different relations of the Wordnet ontology. 2 http://www.rednoise.org/rita/wordnet/documentation/ 67 Features using Semantic Roles: We use Semantic Role Label (SRL) (Gildea et el, 2002; Pradhan et al, 2004; Gurevich et al, 2006) to identify different useful features for event extraction. For each predicate in a sentence acting as event word, semantic roles extract all constituents; determine their arguments (agent, patient, etc.) and adjuncts (locative, temporal, etc.). Some of the other features like predicate, voice and verb subcategorization are shared by all the nodes in the tree. In the present work, we use predicate as an event. Semantic roles can be used to detect the events that are nominalizations of verbs such as agreement for agree or construction for construct. Event nominalizations</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Daniel Jurafsky. 2004. Shallow Semantic Parsing using Support Vector Machine. Proceedings of HLT/NAACL-2004, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N UzZaman</author>
<author>J F Allen</author>
</authors>
<title>Temporal Evaluation.”</title>
<date>2011</date>
<booktitle>In Proceedings of The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (Short Paper),</booktitle>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="23828" citStr="UzZaman and Allen (2011)" startWordPosition="3685" endWordPosition="3688">(P * R) / (P + R). Where, tp is the number of tokens that are part of an extent in keys and response, fp is the number of tokens that are part of an extent in the response but not in the key, and fn is the number of tokens that are part of an extent in the key but not in the response. Additionally attribute accuracies computed according to the following formulae have also been reported. Attr. Accuracy = Attr. F1 / Entity Extraction F1 Attr. R = Attr. Accuracy * Entity R Attr. P = Attr. Accuracy * Entity P Performance in task C is judged with the aid of the Temporal Awareness score proposed by UzZaman and Allen (2011) The JU_CSE system was evaluated on the TE-3 platinum data. Table 2 reports JU_CSE’s performance in timex extraction Task A. Under the relaxed match scheme, the F1-score stands at 86.38% while the strict match scheme yields a F1- score of 75.41%. As far as TIMEX attributes are concerned, the F1-scores are 63.81% and 73.15% for value and type respectively. Timex Extraction Timex Attribute F1 P R Strict F1 Strict P Strict R Value Type Value Type F1 F1 Accuracy Accuracy 86.38 93.28 80.43 75.49 81.51 70.29 63.81 73.15 73.87 84.68 Table 2:JU_CSE system’s TE-3 Results on Timex Task A Event Extractio</context>
</contexts>
<marker>UzZaman, Allen, 2011</marker>
<rawString>UzZaman, N. and J.F. Allen (2011), “Temporal Evaluation.” In Proceedings of The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (Short Paper), Portland, Oregon, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>