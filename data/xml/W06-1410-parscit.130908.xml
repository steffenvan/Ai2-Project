<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000479">
<title confidence="0.99744">
Algorithms for Generating Referring Expressions:
Do They Do What People Do?
</title>
<author confidence="0.995981">
Jette Viethen
</author>
<affiliation confidence="0.992888">
Centre for Language Technology
Macquarie University
</affiliation>
<address confidence="0.825852">
Sydney NSW 2109
</address>
<email confidence="0.996502">
jviethen@ics.mq.edu.au
</email>
<author confidence="0.99457">
Robert Dale
</author>
<affiliation confidence="0.992717">
Centre for Language Technology
Macquarie University
</affiliation>
<address confidence="0.82654">
Sydney NSW 2109
</address>
<email confidence="0.997559">
robert.dale@mq.edu.au
</email>
<sectionHeader confidence="0.993866" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922333333334">
The natural language generation litera-
ture provides many algorithms for the
generation of referring expressions. In
this paper, we explore the question of
whether these algorithms actually produce
the kinds of expressions that people pro-
duce. We compare the output of three ex-
isting algorithms against a data set consist-
ing of human-generated referring expres-
sions, and identify a number of significant
differences between what people do and
what these algorithms do. On the basis of
these observations, we suggest some ways
forward that attempt to address these dif-
ferences.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999844274509805">
The generation of referring expressions (hence-
forth GRE) — that is, the process of working
out what properties of an entity should be used
to describe it in such a way as to distinguish
it from other entities in the context — is a re-
current theme in the natural language generation
literature. The task is discussed informally in
some of the earliest work on NLG (in particular,
see (Winograd, 1972; McDonald, 1980; Appelt,
1981)), but the first formally explicit algorithm
was introduced in (Dale, 1989); this algorithm,
often referred to as the Full Brevity (FB) algo-
rithm, has served as a starting point for many sub-
sequent GRE algorithms. To overcome its limita-
tion to one-place predicates, Dale and Haddock
(1991) introduced a constraint-based procedure
that could generate referring expressions involv-
ing relations; and as a response to the computa-
tional complexity of ‘greedy’ algorithms like FB,
Reiter and Dale (Reiter and Dale, 1992; Dale and
Reiter, 1995) introduced the psycholinguistically
motivated Incremental Algorithm (IA). In recent
years there have been a number of important ex-
tensions to the IA. The Context-Sensitive exten-
sion (Krahmer and Theune, 2002) is able to gen-
erate referring expressions for the most salient en-
tity in a context; the Boolean Expressions algo-
rithm (van Deemter, 2002) is able to derive ex-
pressions containing boolean operators, as in the
cup that does not have a handle; and the Sets
algorithm (van Deemter, 2002) extends the ba-
sic approach to references to sets, as in the red
cups. Some approaches reuse parts of other al-
gorithms: the Branch and Bound algorithm (Krah-
mer et al., 2003) uses the Full Brevity algorithm,
but is able to generate referring expressions with
both attributes and relational descriptions using a
graph-based technique. There are many other al-
gorithms described in the literature: see, for exam-
ple, (Horacek, 1997; Bateman, 1999; Stone, 2000;
Gardent, 2002). Their general aim is to produce
naturalistic referring expressions, often explicitly
by means of an attempt to follow the same kinds
of principles that we believe people might be fol-
lowing when they produce language — such as the
Gricean maxims (Grice, 1975). However, the al-
gorithms have rarely been tested against real data
from human referring expression generation.1
In this paper, we present a data set containing
human-produced referring expressions in a limited
domain. Focussing specifically on the algorithms
</bodyText>
<footnote confidence="0.937339714285714">
1The only exceptions we know of to this deficit are not
directly concerned with the kinds of properties people select,
but with phenomena such as how people group entities to-
gether (Funakoshi et al., 2004; Gatt, 2006), or with multi-
modal referring expressions where the linguistic part is not
necessarily distinguishing by itself (van der Sluis and Krah-
mer, 2004).
</footnote>
<page confidence="0.987271">
63
</page>
<note confidence="0.6947">
Proceedings of the Fourth International Natural Language Generation Conference, pages 63–70,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998297363636364">
presented in (Dale, 1989), (Dale and Haddock,
1991) and (Reiter and Dale, 1992), we explore
how well these algorithms perform in the same
context. There are significant differences between
the referring expressions produced by humans,
and those produced by the algorithms; we explore
these differences and consider what it means for
work in the generation of referring expressions.
The remainder of this paper is structured as fol-
lows. In Section 2, we introduce the data set
of human-produced referring expressions we use;
in Section 3, we introduce the representational
framework we use to model the domain underly-
ing this data; in Section 4 we introduce the three
algorithms considered in this paper; in Section 5
we discuss the results of using these algorithms
on the data that represents the model of our do-
main; in Section 6 we discuss the differences be-
tween the output of the algorithms and the human-
produced data; and in Section 7 we draw some
conclusions and suggest some steps towards ad-
dressing the issues we have identified.
</bodyText>
<sectionHeader confidence="0.957388" genericHeader="method">
2 The Data
</sectionHeader>
<bodyText confidence="0.999953222222222">
Our human-produced referring expressions are
drawn from a physical experimental setting con-
sisting of four filing cabinets, each of which is
four drawers high, located in a fairly typical aca-
demic office. The cabinets are positioned directly
next to each other, so that the drawers form a four-
by-four grid; each drawer is labelled with a num-
ber between 1 and 16 and is coloured either blue,
pink, yellow, or orange. There are four drawers of
each colour which are distributed randomly over
the grid, as shown in Figure 1.
Subjects were given a randomly generated num-
ber between 1 and 16, and asked to produce a de-
scription of the numbered drawer using any prop-
erties other than the number. There were 20 partic-
ipants in the experiment, resulting in a total of 140
referring expressions. Here are some examples of
the referring expressions produced:
</bodyText>
<listItem confidence="0.9991724">
(1) the top drawer second from the right [d3]
(2) the orange drawer on the left [dg]
(3) the orange drawer between two pink ones
[d1e]
(4) the bottom left drawer [d16]
</listItem>
<bodyText confidence="0.6366245">
Since the selection of which drawer to describe
was random, we do not have an equal number of
</bodyText>
<figureCaption confidence="0.999655">
Figure 1: The filing cabinets
</figureCaption>
<bodyText confidence="0.999980903225806">
descriptions of each drawer; in fact, the data set
ranges from two descriptions of Drawer 1 to 12 de-
scriptions of Drawer 16. One of the most obvious
things about the data set is that even the same per-
son may refer to the same entity in different ways
on different occasions, with the differences being
semantic as well as syntactic.
We are interested in comparing how algorithms
for referring expression generation differ in their
outputs from what people do; since these al-
gorithms produce distinguishing descriptions, we
therefore removed from the data set 22 descrip-
tions which were ambiguous or referred to a set of
drawers. This resulted in a total of 118 distinct re-
ferring expressions, with an average of 7.375 dis-
tinct referring expressions per drawer.
As the algorithms under scrutiny here are not
concerned with the final syntactic realisation of
the referring expression produced, we also nor-
malised the human-produced data to remove su-
perficial variations such as the distinction between
relative clauses and reduced relatives, and between
different lexical items that were synonymous in
context, such as column and cabinet.
Four absolute properties used for describing the
drawers can be identified in the natural data pro-
duced by the human participants. These are the
colour of the drawer; its row and column; and in
those cases where the drawer is situated in one of
the corners of the grid, its cornerhood.2 A number
of the natural descriptions also made use of the
</bodyText>
<footnote confidence="0.971048">
2A question we will return to below is that of how we
decide whether to view a particular property as a one-place
predicate or as a relation.
</footnote>
<page confidence="0.998569">
64
</page>
<table confidence="0.999585">
Property Count % (out of possible)
Row 95 79.66% (118)
Column 88 73.73% (118)
Colour 63 53.39% (118)
Corner 11 40.74% (27)
Relation 15 12.71% (118)
</table>
<tableCaption confidence="0.999935">
Table 1: The properties used in descriptions
</tableCaption>
<bodyText confidence="0.9998344">
following relational properties that hold between
drawers: above, below, next to, right of, left of and
between. In Table 1, Count shows the number of
descriptions using each property, and the percent-
ages show the ratio of the number of descriptions
using each property to the number of descriptions
for drawers that possess this property (hence, only
27 of the descriptions referred to corner drawers).
We have combined all uses of relations into one
row in this table to save space, since, interestingly,
their overall use is far below that of the other prop-
erties: 103 descriptions (87.3%) did not use rela-
tions.
Most algorithms in the literature aim at gen-
erating descriptions that are as short as possi-
ble, but will under certain circumstances pro-
duce redundancy. Some authors, for example
(van Deemter and Halld´orsson, 2001), have sug-
gested that human-produced descriptions are of-
ten not minimal, and this is an intuition that we
would generally agree with. However, a strong
tendency towards minimality is evident in the
human-produced data here: only 29 out of 118 de-
scriptions (24.6%) contain redundant information.
Here are a few examples:
</bodyText>
<listItem confidence="0.9942622">
• the yellow drawer in the third column from
the left second from the top [d6]
• the blue drawer in the top left corner [d1]
• the orange drawer below the two yellow
drawers [d14]
</listItem>
<bodyText confidence="0.999805">
In the first case, either the colour or column proper-
ties are redundant; in the second, colour and corner,
or only the grid information, would have been suf-
ficient; and in the third, it would have been suffi-
cient to mention one of the two yellow drawers.
</bodyText>
<sectionHeader confidence="0.988661" genericHeader="method">
3 Knowledge Representation
</sectionHeader>
<bodyText confidence="0.999714461538462">
In order to use an algorithm to generate referring
expressions in this domain, we must first decide
how to represent the domain. It turns out that this
raises some interesting questions.
We use the symbols {d1, d2 ... d161 as our
unique identifying labels for the 16 drawers.
Given some di, the goal of any given algorithm
is then to produce a distinguishing description of
that entity with respect to a context consisting of
the other 15 drawers.
As is usual, we represent the properties of the
domain in terms of attribute–value pairs. Thus we
have, for example:
</bodyText>
<listItem confidence="0.672545">
• d2: (colour, orange), (row, 1), (column, 2),
(right-of, d1), (left-of, d3), (next-to, d1), (next-to,
d3), (above, d,)
</listItem>
<bodyText confidence="0.999604888888889">
This drawer is in the top row, so it does not have a
property of the form (below, d2).
The four corner drawers additionally possess
the property (position, corner). Cornerhood can
be inferred from the row and column informa-
tion; however, we added this property explicitly
because several of the natural descriptions use the
property of cornerhood, and it seems plausible that
this is a particularly salient property in its own
right.
This raises the question of what properties
should be encoded explicitly, and which should
be inferred. Note that in the example above, we
explicitly encode relational properties that could
be computed from others, such as left-of and right-
of. Since none of the algorithms explored here
uses inference over knowledge base properties, we
opted here to ‘level the playing field’ to enable
fairer comparison between human-produced and
machine-produced descriptions.
A similar question of the role of inference arises
with regard to the transitivity of spatial relations.
For example, if d1 is above d9 and d9 is above
d16 , then it can be inferred that d1 is transitively
above d16. In a more complex domain, the imple-
mentation of this kind of knowledge might play
an important role in generating usful referring ex-
pressions. However, the uniformity of our domain
results in this inferred knowledge about transitive
relations being of little use; in fact, in most cases,
the implementation of transitive inference might
even result in the generation of unnatural descrip-
tions, such as the orange drawer (two) right of the
blue drawer for d12.
Another aspect of the representation of relations
that requires a decision is that of generalisation:
</bodyText>
<page confidence="0.997898">
65
</page>
<bodyText confidence="0.999900461538462">
next-to is a generalisation of the relations left-of and
right-of. The only algorithm of those we exam-
ine here that provides a mechanism for exploring
a generalisation hierarchy is the Incremental Al-
gorithm (Reiter and Dale, 1992), and this cannot
handle relations; so, we take the shortcut of ex-
plicitly representing the next-to relation for every
left-of and right-of relation in the knowledge base.
We then implement special-case handling that en-
sures that, if one of these facts is used, the more
general or more specific case is also deleted from
the set of properties still available for the descrip-
tion.3
</bodyText>
<sectionHeader confidence="0.99451" genericHeader="method">
4 The Algorithms
</sectionHeader>
<bodyText confidence="0.9933316">
As we have already noted above, there is a con-
siderable literature on the generation of referring
expressions, and many papers in the area provide
detailed algorithms. We focus here on the follow-
ing algorithms:
</bodyText>
<listItem confidence="0.9969538">
• The Full Brevity algorithm (Dale, 1989) at-
tempts to build a minimal distinguishing de-
scription by always selecting the most dis-
criminatory property available; see Algo-
rithm 1.
</listItem>
<bodyText confidence="0.9987174">
Let L be the set of properties to be realised in our
description; let P be the set of properties known to be
true of our intended referent r (we assume that P is
non-empty); and let C be the set of distractors (the
contrast set). The initial conditions are thus as follows:
</bodyText>
<equation confidence="0.837002333333333">
– C = {hall distractorsi};
– P = {hall properties true of ri};
– L = {}
</equation>
<bodyText confidence="0.8955175">
In order to describe the intended referent r with respect
to the contrast set C, we do the following:
</bodyText>
<listItem confidence="0.781655">
1. Check Success:
</listItem>
<table confidence="0.326796785714286">
if |C |= 0 then return L as a distinguishing
description
elseif P = ∅ then fail
else goto Step 2.
2. Choose Property:
for each pi ∈ P do:
Ci ← C ∩ {x|pi(x)}
Chosen property is pj, where Cj is the smallest set.
goto Step 3.
3. Extend Description (wrt the chosen pj):
L ← L ∪ {pj}
C ← Cj
P ← P − {pj}
goto Step 1.
</table>
<tableCaption confidence="0.124498">
Algorithm 1: The Full Brevity Algorithm
</tableCaption>
<footnote confidence="0.84539925">
3This is essentially a hack; however, there is clearly a need
for some mechanism for handling what we might think of
as equivalence classes of properties, and this is effectively a
simple approach to this question.
</footnote>
<figure confidence="0.965351983333334">
1. Check Success
if Stack is empty then return L as a DD
elseif |Cv |= 1 then pop Stack &amp; goto Step 1
elseif Pr = ∅ then fail
else goto Step 2
2. Choose Property
for each property pi ∈ Pr do
p0i ← [r\v]pi
Ni ← N ⊕ p0i
Chosen prediction is pj, where Nj contains
the smallest set Cv for v.
goto Step 3
3. Extend Description (w.r.t the chosen p)
Pr ← Pr − {p}
p ← [r\v]p
for every other constant r’ in p do
associate r0 with a new, unique variable v0
p ← [r0\v0]p
push Describe(r’,v’) onto Stack
initialise a set Pr0 of facts true of r0
N ← N ⊕ p
goto Step 1
Algorithm 2: The Relational Algorithm
MakeReferringExpression(r, C, P) L ← {}
for each member Ai of list P do
V = FindBestValue(r, Ai, BasicLevelValue(r, Ai))
if RulesOut(hAi, V i) =6 nil
then L ← L ∪ {hAi, V i}
C ← C − RulesOut(hAi, V i)
endif
ifC={}then
if htype, Xi ∈ L for some X
then return L
else return L ∪ {htype, BasicLevelValue(r,
type)i}
endif
endif
return failure
FindBestValue(r, A, initial-value)
if UserKnows(r, hA, initial-valuei) = true
then value ← initial-value
else value ← no-value
endif
if (more-specific-value ← MoreSpecificValue(r, A,
value)) =6 nil ∧
(new-value ← FindBestValue(A,
more-specific-value)) =6 nil ∧
(|RulesOut(hA, new-valuei) |&gt; |RulesOut(hA,
valuei)|)
then value ← new-value
endif
return value
RulesOut(hA, V i)
if V = no-value
then return nil
else return {x : x ∈ C ∧ UserKnows(x, hA, V i) =
false}
endif
Algorithm 3: The Incremental Algorithm
66
</figure>
<listItem confidence="0.939797">
• The relational algorithm from (Dale and Had-
dock, 1991) uses constraint satisfaction to in-
corporate relational properties while avoiding
infinite regress; see Algorithm 2.
• the Incremental Algorithm (Reiter and Dale,
1992; Dale and Reiter, 1995) considers the
available properties to be used in a descrip-
tion via a preference ordering over those
properties; see Algorithm 3.
</listItem>
<bodyText confidence="0.999719375">
For the purpose of this study, the algorithms were
implemented in Common LISP. The mechanism
described in (Dale and Reiter, 1995) to handle
generalisation hierarchies for values for the dif-
ferent properties, referred to in the algorithm here
as FindBestValue, was not implemented since, as
discussed earlier, our representation of the domain
does not make use of a hierarchy of properties.
</bodyText>
<sectionHeader confidence="0.970419" genericHeader="method">
5 The Output of the Algorithms
</sectionHeader>
<bodyText confidence="0.9999804">
Using the knowledge base described in Section 3,
we applied the algorithms from the previous sec-
tion to see whether the referring expressions they
produced were the same as, or similar to, those
produced by the human subjects. This quickly
gave rise to some situations not explicitly ad-
dressed by some of the algorithms; we discuss
these in Section 5.1 below. Section 5.2 discusses
the extent to which the behaviour of the algorithms
matched that of the human data.
</bodyText>
<subsectionHeader confidence="0.991014">
5.1 Preference Orderings
</subsectionHeader>
<bodyText confidence="0.999975731707317">
The Incremental Algorithm explicitly encodes a
preference ordering over the available properties,
in an attempt to model what appear to be semi-
conventionalised strategies for description that
people use. This also has the consequence of
avoiding a problem that faces the other two algo-
rithms: since the Full Brevity Algorithm and the
Relational Algorithm choose the most discrimina-
tory property at each step, they have to deal with
the case where several properties are equally dis-
criminatory. This turns out to be a common sit-
uation in our domain. Both algorithms implicitly
assume that the choice will be made randomly in
these cases; however, it seems to us more natural
to control this process by imposing some selection
strategy. We do this here by borrowing the idea
of preference ordering from the Incremental Algo-
rithm, and using it as a tie-breaker when multiple
properties are equally discriminatory.
Not including type information (i.e., the fact that
some di is a drawer), which has no discrimina-
tory power and therefore will never be chosen by
any of the algorithms,4 there are only four differ-
ent properties available for the Full Brevity Algo-
rithm and the Incremental Algorithm: row, column,
colour, and position. This gives us 4! = 24 different
possible preference orderings. Since some of the
human-produced descriptions use all four proper-
ties, we tested these two algorithms with all 24
preference orderings.
For the Relational Algorithm, we added the five
relations next to, left of, right of, above, and below.
This results in 9! = 362,880 possible preference
orderings; far too many to test. Since we are
primarily interested in whether the algorithm can
generate the human-produced descriptions, we re-
stricted our testing to those preference orderings
that started with a permutation of the properties
used by the participants; in addition to the 24 pref-
erence orderings above, there are 12 preference or-
derings that incorporate the relational properties.
</bodyText>
<subsectionHeader confidence="0.998986">
5.2 Coverage of the Human Data
</subsectionHeader>
<bodyText confidence="0.999978">
Overall, the Full Brevity Algorithm is able to gen-
erate 82 out of the 103 non-relational descriptions
from the natural data, providing a recall of 79.6%.
The recall score for the Incremental Algorithm is
95.1%, generating 98 of the 103 descriptions. As
these algorithms do not attempt to generate rela-
tional descriptions, the relational data is not taken
into account in evaluating the performance here.
Both algorithms are able to generate all the
non-relational minimal descriptions found in the
human-produced data. The Full Brevity Algo-
rithm unintentionally replicates the redundancy
found in nine descriptions, and the Incremental
Algorithm produces all but five of the 29 redun-
dant descriptions.
Perhaps surprisingly, the Relational Algorithm
does not generate any of the human-produced de-
scriptions. We will return to consider why this is
the case in the next section.
</bodyText>
<sectionHeader confidence="0.999635" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999285666666667">
There are two significant differences to be consid-
ered here: first, the coverage of redundant descrip-
tions by the Full Brevity and Incremental Algo-
</bodyText>
<footnote confidence="0.747273666666667">
4Consistent with much other work in the field, we as-
sume that the head noun will always be added irrespective
of whether it has any discriminatory power.
</footnote>
<page confidence="0.999389">
67
</page>
<bodyText confidence="0.99401325">
rithms; and second, the inability of the Relational
Algorithm to replicate any of the human data.
dundant descriptions which the algorithm does not
generate are as follows:
</bodyText>
<subsectionHeader confidence="0.999224">
6.1 Coverage of Redundancy
</subsectionHeader>
<bodyText confidence="0.998822340425532">
Neither the Full Brevity Algorithm nor the Incre-
mental Algorithm presumes to be able to generate
relational descriptions; however, both algorithms
are able to produce each of the minimal descrip-
tions from the set of natural data with at least one
of the preference orderings. Both also generate
several of the redundant descriptions in the nat-
ural data set, but do not capture all of the human-
generated redundancies.
The Full Brevity Algorithm has as a primary
goal the avoidance of redundant descriptions, so
it is a sign of the algorithm being consistent with
its specification that it covers fewer of the redun-
dant expressions than the Incremental Algorithm.
On the other hand, the fact that it produces any
redundant descriptions signals that the algorithm
doesn’t quite meet its specification. The cases
where the Full Brevity Algorithm produces redun-
dancy are when an entity shares with another en-
tity at least two property-values and, after choos-
ing one of these properties, the next property to
be considered is the other shared one, since it has
the same or a higher discriminatory power than all
other properties. This is a situation that was not
considered in the original algorithm; it is related
to the problem of what to do when two properties
have the same discriminatory power, as noted ear-
lier. In our domain, the situation arises for corner
drawers with the same colour (d4 and d16), and
drawers that are not in a corner but for which there
is another drawer of the same colour in each of the
same row and column (d7 and d8).
The Incremental Algorithm, on the other hand,
generates redundancy when an object shares at
least two property-values with another object and
the two shared properties are the first to be con-
sidered in the preference ordering. This is pos-
sible for corner drawers with the same colour (d4
and d16) and for drawers for which there is another
drawer of the same colour in either the same row,
the same column, or both (d5, d6, d7, d8, d10, d11,
d13, d15).
In these terms, the Incremental Algorithm is
clearly a better model of the human behaviour than
the Full Brevity Algorithm. However, we may ask
why the algorithm does not cover all the redun-
dancy found in the human descriptions. The re-
</bodyText>
<listItem confidence="0.976727125">
(5) the blue drawer in the top left corner [d1]
(6) the yellow drawer in the top right corner [d4]
(7) the pink drawer in the top of the column sec-
ond from the right [d3]
(8) the orange drawer in the bottom second from
the right [d14]
(9) the orange drawer in the bottom of the second
column from the right [d14]
</listItem>
<bodyText confidence="0.9921502">
The Incremental Algorithm stops selecting prop-
erties when a distinguishing description has been
constructed. In Example (6), for example, the
algorithm would select any of the following, de-
pending on the preference ordering used:
</bodyText>
<listItem confidence="0.996389">
(10) the yellow drawer in the corner
(11) the top left yellow drawer
(12) the drawer in the top left corner
</listItem>
<bodyText confidence="0.999992235294118">
The human subject, however, has added informa-
tion beyond what is required. This could be ex-
plained by our modelling of cornerhood: in Ex-
amples (5) and (6), one has the intuition that the
noun corner is being added simply to provide a
nominal head to the prepositional phrase in an
incrementally-constructed expression of the form
the blue drawer in the top right ..., in much
the same way as the head noun drawer is added,
whereas we have treated it as a distinct property
that adds discriminatory power. This again em-
phasises the important role the underlying repre-
sentation plays in the generation of referring ex-
pressions: if we want to emulate what people do,
then we not only need to design algorithms which
mirror their behaviour, but these algorithms have
to operate over the same kind of data.
</bodyText>
<subsectionHeader confidence="0.996711">
6.2 Relational Descriptions
</subsectionHeader>
<bodyText confidence="0.999977">
The fact that the Relational Algorithm generates
none of the human-generated descriptions is quite
disturbing. On closer examination, it transpires
that this is because, in this domain, the discrimi-
natory power of relational properties is generally
always greater than that of any other property, so
a relational property is chosen first. As noted ear-
lier, relational properties appear to be dispreferred
</bodyText>
<page confidence="0.998578">
68
</page>
<bodyText confidence="0.999974228571429">
in the human data, so the Relational Algorithm is
already disadvantaged. The relatively poor per-
formance of the algorithm is then compounded by
its insistence on continuing to use relational prop-
erties: an absolute property will only be chosen
when either the currently described drawer has no
unused relational properties left, or the number
of distractors has been reduced so much that the
discriminatory power of all remaining relational
properties is lower than that of the absolute prop-
erty, or the absolute property has the same discrim-
inatory power as the best relational one and the ab-
solute property appears before all relations in the
preference ordering.
Consequently, whereas a typical human de-
scription of drawer d2 would be the orange drawer
above the blue drawer, the Relational Algorithm
will produce the description the drawer above the
drawer above the drawer above the pink drawer.
Not only are there no descriptions of this form in
the human-produced data set, but they also sound
more like riddles someone might create to inten-
tionally make it hard for the hearer to figure out
what is meant.
There are a variety of ways in which the be-
haviour of this algorithm might be repaired. We
are currently exploring whether Krahmer et al’s
(2003) graph-based approach to GRE is able to
provide a better coverage of the data: this algo-
rithm provides the ability to make use of differ-
ent search strategies and weighting mechanisms
when adding properties to a description, and such
a mechanism might be used, for example, to coun-
terbalance the Relational Algorithm’s heavy bias
towards the relations in this domain.
</bodyText>
<sectionHeader confidence="0.997258" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979428571429">
We have noted a number of regards in which the
algorithms we have explored here do not produce
outputs that are the same as those produced by hu-
mans. Some comments on the generalisability of
these results are appropriate.
First, our results may be idiosyncratic to the
specifics of the particular domain of our experi-
ment. We would point out, however, that the do-
main is more complex, and arguably more real-
istic, than the much-simplified experimental con-
texts that have served as intuitions for earlier work
in the field; we have in mind here in particular the
experiments discussed in (Ford and Olson, 1975),
(Sonnenschein, 1985) and (Pechmann, 1989). In
the belief that the data provides a good test set
for the generation of referring expressions, we are
making the data set publicly available 5, so others
may try to develop algorithms covering the data.
A second concern is that we have only explored
the extent to which three specific algorithms are
able to cover the human data. Many of the other al-
gorithms in the literature take these as a base, and
so are unlikely to deliver significantly different re-
sults. The major exceptions here may be (a) van
Deemter’s (2002) algorithm for sets; recall that we
excluded from the human data used here 16 ref-
erences that involved sets; and, as noted above,
(b) Krahmer et al’s (2003) graph-based approach
to GRE, which may perform better than the Re-
lational Algorithm on descriptions using relations.
In future work, we intend to explore to what extent
our findings extend to other algorithms.
In conclusion, we point to two directions where
we believe further work is required.
First, as we noted early in this paper, it is clear
that there can be many different ways of refer-
ring to the same entity. Existing algorithms are
all deterministic and therefore produce exactly one
‘best’ description for each entity; but the human-
produced data clearly shows that there are many
equally valid ways of describing an entity. We
need to find some way to account for this in our
algorithms. Our intuition is that this is likely to
be best cashed out in terms of different ‘refer-
ence strategies’ that different speakers adopt in
different situations; we are reminded here of Car-
letta’s (1992) distinction between risky and cau-
tious strategies for describing objects in the Map
Task domain. More experimentation is required in
order to determine just what these strategies are:
are they, for example, characterisable as things
like ‘Produce a referring expression that is as short
as possible’ (the intuition behind the Full Brevity
Algorithm), ‘Just say what comes to mind first and
keep adding information until the description dis-
tinguishes the intended referent’ (something like
the Incremental Algorithm), or perhaps a strategy
of minimising the cognitive effort for either the
speaker or the hearer? Further psycholinguistic
experiments and data analysis are required to de-
termine the answers here.
Our second observation is that the particular re-
sults we have presented here are, ultimately, en-
</bodyText>
<footnote confidence="0.991188">
5The data set is publicly available from
http://www.ics.mq.edu.au/∼jviethen/drawers
</footnote>
<page confidence="0.999061">
69
</page>
<bodyText confidence="0.999992045454546">
tirely dependent upon the underlying representa-
tions we have used, and the decisions we have
made in choosing how to represent the properties
and relations in the domain. We believe it is im-
portant to draw attention to the fact that precisely
how we choose to represent the domain has an im-
pact on what the algorithms will do. If we are
aiming for naturalism in our algorithms for refer-
ring expression generation, then ideally we would
like our representations to mirror those used by hu-
mans; but, of course, we don’t have direct access
to what these are.
There is clearly scope for psychological exper-
imentation, perhaps along the lines initially ex-
plored by (Rosch, 1978), to determine some con-
straints here. In parallel, we are considering fur-
ther exploration into the variety of representations
that can be used, particularly with regard to the
question of which properties are considered to be
‘primitive’, and which are generated by some in-
ference mechanism; this is a much neglected as-
pect of the referring expression generation task.
</bodyText>
<sectionHeader confidence="0.999104" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999904790123457">
D. E. Appelt. 1981. Planning Natural Language Ut-
terances to Satisfy Multiple Goals. Ph.D. thesis,
Stanford University.
J. Bateman. 1999. Using aggregation for selecting
content when generating referring expressions. In
Proceedings of the 37th Meeting of the ACL, pages
127–134.
J. C. Carletta. 1992. Risk-taking and Recovery in Task-
oriented Dialogue. Ph.D. thesis, University of Edin-
burgh.
R. Dale and N. Haddock. 1991. Generating referring
expressions involving relations. In Proceedings of
the 5th Meeting of the EACL, pages 161–166, Berlin,
Germany.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19(2):233–
263.
R. Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th Meeting of the ACL, pages
68–75.
W. Ford and D. Olson. 1975. The elaboration of
the noun phrase in children’s description of objects.
Journal of Experimental Child Psychology, 19:371–
382.
K. Funakoshi, S. Watanabe, N. Kuriyama, and T. Toku-
naga. 2004. Generating referring expressions using
perceptual groups. In Proceedings of the 3rd INLG,
pages 51–60.
C. Gardent. 2002. Generating minimal definite de-
scriptions. In Proceedings of the 40th Meeting of
the ACL, pages 96–103.
A. Gatt. 2006. Structuring knowledge for reference
generation: A clustering algorithm. In Proceedings
of the 11th Meeting of the EACL.
H. P. Grice. 1975. Logic and conversation. In P. Cole
and J. Morgan, editors, Syntax and Semantics Vol-
ume 3: Speech Acts, pages 43–58. Academic Press.
H. Horacek. 1997. An algorithm for generating ref-
erential descriptions with flexible interfaces. In Pro-
ceedings of the 35th Meeting of the ACL, pages 127–
134.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In
K. van Deemter and R. Kibble, editors, Informa-
tion Sharing: Reference and Presupposition in Lan-
guage Generation and Interpretation, pages 223–
264. CSLI.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1):53–72.
D. D. McDonald. 1980. Natural Language Generation
as a Process of Decision-making Under Constraints.
Ph.D. thesis, Massachusetts Institute of Technology.
T. Pechmann. 1989. Incremental speech produc-
tion and referential overspecification. Linguistics,
27:89–110.
E. Reiter and R. Dale. 1992. A fast algorithm for the
generation of referring expressions. In Proceedings
of the 14th Meeting of the ACL, pages 232–238.
E. Rosch. 1978. Principles of categorization. In Cog-
nition and Categorization, pages 27–48. Lawrence
Erlbaum, Hillsdale, NJ.
S. Sonnenschein. 1985. The development of referen-
tial communication skills: Some situations in which
speakers give redundant messages. Journal of Psy-
cholinguistic Research, 14:489–508.
M. Stone. 2000. On identifying sets. In Proceedings
of the 1st INLG, pages 116–123.
K. van Deemter and M. M. Halld´orsson. 2001. Logi-
cal form equivalence: The case referring expressions
generation. In Proceedings of the 8th ENLG.
K. van Deemter. 2002. Generating referring expres-
sions: Boolean extensions of the incremental algo-
rithm. Computational Linguistics, 28(1):37–52.
I. van der Sluis and E. Krahmer. 2004. Evaluating
multimodal NLG using production experiments. In
Proceedings of the 4th LREC, pages 209–212, 26-28
May.
T. Winograd. 1972. Understanding Natural Language.
Academic Press.
</reference>
<page confidence="0.998476">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.101963">
<title confidence="0.5748234">Algorithms for Generating Referring Do They Do What People Do? Jette Centre for Language Macquarie</title>
<author confidence="0.991972">Sydney NSW</author>
<email confidence="0.984474">jviethen@ics.mq.edu.au</email>
<author confidence="0.902232">Robert</author>
<affiliation confidence="0.900055">Centre for Language</affiliation>
<title confidence="0.640216">Macquarie</title>
<author confidence="0.987404">Sydney NSW</author>
<email confidence="0.991349">robert.dale@mq.edu.au</email>
<abstract confidence="0.98266125">The natural language generation literature provides many algorithms for the generation of referring expressions. In this paper, we explore the question of whether these algorithms actually produce the kinds of expressions that people produce. We compare the output of three existing algorithms against a data set consisting of human-generated referring expressions, and identify a number of significant differences between what people do and what these algorithms do. On the basis of these observations, we suggest some ways forward that attempt to address these differences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D E Appelt</author>
</authors>
<title>Planning Natural Language Utterances to Satisfy Multiple Goals.</title>
<date>1981</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="1312" citStr="Appelt, 1981" startWordPosition="201" endWordPosition="202">erences between what people do and what these algorithms do. On the basis of these observations, we suggest some ways forward that attempt to address these differences. 1 Introduction The generation of referring expressions (henceforth GRE) — that is, the process of working out what properties of an entity should be used to describe it in such a way as to distinguish it from other entities in the context — is a recurrent theme in the natural language generation literature. The task is discussed informally in some of the earliest work on NLG (in particular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981)), but the first formally explicit algorithm was introduced in (Dale, 1989); this algorithm, often referred to as the Full Brevity (FB) algorithm, has served as a starting point for many subsequent GRE algorithms. To overcome its limitation to one-place predicates, Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations; and as a response to the computational complexity of ‘greedy’ algorithms like FB, Reiter and Dale (Reiter and Dale, 1992; Dale and Reiter, 1995) introduced the psycholinguistically motivated Incremental Algo</context>
</contexts>
<marker>Appelt, 1981</marker>
<rawString>D. E. Appelt. 1981. Planning Natural Language Utterances to Satisfy Multiple Goals. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bateman</author>
</authors>
<title>Using aggregation for selecting content when generating referring expressions.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Meeting of the ACL,</booktitle>
<pages>127--134</pages>
<contexts>
<context position="2783" citStr="Bateman, 1999" startWordPosition="437" endWordPosition="438">orithm (van Deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups. Some approaches reuse parts of other algorithms: the Branch and Bound algorithm (Krahmer et al., 2003) uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. There are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000; Gardent, 2002). Their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kinds of principles that we believe people might be following when they produce language — such as the Gricean maxims (Grice, 1975). However, the algorithms have rarely been tested against real data from human referring expression generation.1 In this paper, we present a data set containing human-produced referring expressions in a limited domain. Focussing specifically on the algorithms 1The only exceptions we know of to this deficit are</context>
</contexts>
<marker>Bateman, 1999</marker>
<rawString>J. Bateman. 1999. Using aggregation for selecting content when generating referring expressions. In Proceedings of the 37th Meeting of the ACL, pages 127–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Carletta</author>
</authors>
<title>Risk-taking and Recovery in Taskoriented Dialogue.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<marker>Carletta, 1992</marker>
<rawString>J. C. Carletta. 1992. Risk-taking and Recovery in Taskoriented Dialogue. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>N Haddock</author>
</authors>
<title>Generating referring expressions involving relations.</title>
<date>1991</date>
<booktitle>In Proceedings of the 5th Meeting of the EACL,</booktitle>
<pages>161--166</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="1601" citStr="Dale and Haddock (1991)" startWordPosition="246" endWordPosition="249">what properties of an entity should be used to describe it in such a way as to distinguish it from other entities in the context — is a recurrent theme in the natural language generation literature. The task is discussed informally in some of the earliest work on NLG (in particular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981)), but the first formally explicit algorithm was introduced in (Dale, 1989); this algorithm, often referred to as the Full Brevity (FB) algorithm, has served as a starting point for many subsequent GRE algorithms. To overcome its limitation to one-place predicates, Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations; and as a response to the computational complexity of ‘greedy’ algorithms like FB, Reiter and Dale (Reiter and Dale, 1992; Dale and Reiter, 1995) introduced the psycholinguistically motivated Incremental Algorithm (IA). In recent years there have been a number of important extensions to the IA. The Context-Sensitive extension (Krahmer and Theune, 2002) is able to generate referring expressions for the most salient entity in a context; the Boolean Expressions algorithm (van Deemter, 2002) is a</context>
<context position="3912" citStr="Dale and Haddock, 1991" startWordPosition="607" endWordPosition="610">. Focussing specifically on the algorithms 1The only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select, but with phenomena such as how people group entities together (Funakoshi et al., 2004; Gatt, 2006), or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der Sluis and Krahmer, 2004). 63 Proceedings of the Fourth International Natural Language Generation Conference, pages 63–70, Sydney, July 2006. c�2006 Association for Computational Linguistics presented in (Dale, 1989), (Dale and Haddock, 1991) and (Reiter and Dale, 1992), we explore how well these algorithms perform in the same context. There are significant differences between the referring expressions produced by humans, and those produced by the algorithms; we explore these differences and consider what it means for work in the generation of referring expressions. The remainder of this paper is structured as follows. In Section 2, we introduce the data set of human-produced referring expressions we use; in Section 3, we introduce the representational framework we use to model the domain underlying this data; in Section 4 we intr</context>
<context position="15399" citStr="Dale and Haddock, 1991" startWordPosition="2599" endWordPosition="2603">, BasicLevelValue(r, type)i} endif endif return failure FindBestValue(r, A, initial-value) if UserKnows(r, hA, initial-valuei) = true then value ← initial-value else value ← no-value endif if (more-specific-value ← MoreSpecificValue(r, A, value)) =6 nil ∧ (new-value ← FindBestValue(A, more-specific-value)) =6 nil ∧ (|RulesOut(hA, new-valuei) |&gt; |RulesOut(hA, valuei)|) then value ← new-value endif return value RulesOut(hA, V i) if V = no-value then return nil else return {x : x ∈ C ∧ UserKnows(x, hA, V i) = false} endif Algorithm 3: The Incremental Algorithm 66 • The relational algorithm from (Dale and Haddock, 1991) uses constraint satisfaction to incorporate relational properties while avoiding infinite regress; see Algorithm 2. • the Incremental Algorithm (Reiter and Dale, 1992; Dale and Reiter, 1995) considers the available properties to be used in a description via a preference ordering over those properties; see Algorithm 3. For the purpose of this study, the algorithms were implemented in Common LISP. The mechanism described in (Dale and Reiter, 1995) to handle generalisation hierarchies for values for the different properties, referred to in the algorithm here as FindBestValue, was not implemented</context>
</contexts>
<marker>Dale, Haddock, 1991</marker>
<rawString>R. Dale and N. Haddock. 1991. Generating referring expressions involving relations. In Proceedings of the 5th Meeting of the EACL, pages 161–166, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>E Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263</pages>
<contexts>
<context position="1849" citStr="Dale and Reiter, 1995" startWordPosition="283" endWordPosition="286">rliest work on NLG (in particular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981)), but the first formally explicit algorithm was introduced in (Dale, 1989); this algorithm, often referred to as the Full Brevity (FB) algorithm, has served as a starting point for many subsequent GRE algorithms. To overcome its limitation to one-place predicates, Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations; and as a response to the computational complexity of ‘greedy’ algorithms like FB, Reiter and Dale (Reiter and Dale, 1992; Dale and Reiter, 1995) introduced the psycholinguistically motivated Incremental Algorithm (IA). In recent years there have been a number of important extensions to the IA. The Context-Sensitive extension (Krahmer and Theune, 2002) is able to generate referring expressions for the most salient entity in a context; the Boolean Expressions algorithm (van Deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups. Some approaches reuse parts of other</context>
<context position="15590" citStr="Dale and Reiter, 1995" startWordPosition="2627" endWordPosition="2630">(more-specific-value ← MoreSpecificValue(r, A, value)) =6 nil ∧ (new-value ← FindBestValue(A, more-specific-value)) =6 nil ∧ (|RulesOut(hA, new-valuei) |&gt; |RulesOut(hA, valuei)|) then value ← new-value endif return value RulesOut(hA, V i) if V = no-value then return nil else return {x : x ∈ C ∧ UserKnows(x, hA, V i) = false} endif Algorithm 3: The Incremental Algorithm 66 • The relational algorithm from (Dale and Haddock, 1991) uses constraint satisfaction to incorporate relational properties while avoiding infinite regress; see Algorithm 2. • the Incremental Algorithm (Reiter and Dale, 1992; Dale and Reiter, 1995) considers the available properties to be used in a description via a preference ordering over those properties; see Algorithm 3. For the purpose of this study, the algorithms were implemented in Common LISP. The mechanism described in (Dale and Reiter, 1995) to handle generalisation hierarchies for values for the different properties, referred to in the algorithm here as FindBestValue, was not implemented since, as discussed earlier, our representation of the domain does not make use of a hierarchy of properties. 5 The Output of the Algorithms Using the knowledge base described in Section 3, </context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>R. Dale and E. Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 19(2):233– 263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
</authors>
<title>Cooking up referring expressions.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Meeting of the ACL,</booktitle>
<pages>68--75</pages>
<contexts>
<context position="1387" citStr="Dale, 1989" startWordPosition="212" endWordPosition="213">these observations, we suggest some ways forward that attempt to address these differences. 1 Introduction The generation of referring expressions (henceforth GRE) — that is, the process of working out what properties of an entity should be used to describe it in such a way as to distinguish it from other entities in the context — is a recurrent theme in the natural language generation literature. The task is discussed informally in some of the earliest work on NLG (in particular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981)), but the first formally explicit algorithm was introduced in (Dale, 1989); this algorithm, often referred to as the Full Brevity (FB) algorithm, has served as a starting point for many subsequent GRE algorithms. To overcome its limitation to one-place predicates, Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations; and as a response to the computational complexity of ‘greedy’ algorithms like FB, Reiter and Dale (Reiter and Dale, 1992; Dale and Reiter, 1995) introduced the psycholinguistically motivated Incremental Algorithm (IA). In recent years there have been a number of important extension</context>
<context position="3886" citStr="Dale, 1989" startWordPosition="605" endWordPosition="606">limited domain. Focussing specifically on the algorithms 1The only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select, but with phenomena such as how people group entities together (Funakoshi et al., 2004; Gatt, 2006), or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der Sluis and Krahmer, 2004). 63 Proceedings of the Fourth International Natural Language Generation Conference, pages 63–70, Sydney, July 2006. c�2006 Association for Computational Linguistics presented in (Dale, 1989), (Dale and Haddock, 1991) and (Reiter and Dale, 1992), we explore how well these algorithms perform in the same context. There are significant differences between the referring expressions produced by humans, and those produced by the algorithms; we explore these differences and consider what it means for work in the generation of referring expressions. The remainder of this paper is structured as follows. In Section 2, we introduce the data set of human-produced referring expressions we use; in Section 3, we introduce the representational framework we use to model the domain underlying this </context>
<context position="12730" citStr="Dale, 1989" startWordPosition="2095" endWordPosition="2096">we take the shortcut of explicitly representing the next-to relation for every left-of and right-of relation in the knowledge base. We then implement special-case handling that ensures that, if one of these facts is used, the more general or more specific case is also deleted from the set of properties still available for the description.3 4 The Algorithms As we have already noted above, there is a considerable literature on the generation of referring expressions, and many papers in the area provide detailed algorithms. We focus here on the following algorithms: • The Full Brevity algorithm (Dale, 1989) attempts to build a minimal distinguishing description by always selecting the most discriminatory property available; see Algorithm 1. Let L be the set of properties to be realised in our description; let P be the set of properties known to be true of our intended referent r (we assume that P is non-empty); and let C be the set of distractors (the contrast set). The initial conditions are thus as follows: – C = {hall distractorsi}; – P = {hall properties true of ri}; – L = {} In order to describe the intended referent r with respect to the contrast set C, we do the following: 1. Check Succes</context>
</contexts>
<marker>Dale, 1989</marker>
<rawString>R. Dale. 1989. Cooking up referring expressions. In Proceedings of the 27th Meeting of the ACL, pages 68–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Ford</author>
<author>D Olson</author>
</authors>
<title>The elaboration of the noun phrase in children’s description of objects.</title>
<date>1975</date>
<journal>Journal of Experimental Child Psychology,</journal>
<volume>19</volume>
<pages>382</pages>
<contexts>
<context position="26421" citStr="Ford and Olson, 1975" startWordPosition="4433" endWordPosition="4436">Future Work We have noted a number of regards in which the algorithms we have explored here do not produce outputs that are the same as those produced by humans. Some comments on the generalisability of these results are appropriate. First, our results may be idiosyncratic to the specifics of the particular domain of our experiment. We would point out, however, that the domain is more complex, and arguably more realistic, than the much-simplified experimental contexts that have served as intuitions for earlier work in the field; we have in mind here in particular the experiments discussed in (Ford and Olson, 1975), (Sonnenschein, 1985) and (Pechmann, 1989). In the belief that the data provides a good test set for the generation of referring expressions, we are making the data set publicly available 5, so others may try to develop algorithms covering the data. A second concern is that we have only explored the extent to which three specific algorithms are able to cover the human data. Many of the other algorithms in the literature take these as a base, and so are unlikely to deliver significantly different results. The major exceptions here may be (a) van Deemter’s (2002) algorithm for sets; recall that</context>
</contexts>
<marker>Ford, Olson, 1975</marker>
<rawString>W. Ford and D. Olson. 1975. The elaboration of the noun phrase in children’s description of objects. Journal of Experimental Child Psychology, 19:371– 382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Funakoshi</author>
<author>S Watanabe</author>
<author>N Kuriyama</author>
<author>T Tokunaga</author>
</authors>
<title>Generating referring expressions using perceptual groups.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd INLG,</booktitle>
<pages>51--60</pages>
<contexts>
<context position="3536" citStr="Funakoshi et al., 2004" startWordPosition="554" endWordPosition="557">attempt to follow the same kinds of principles that we believe people might be following when they produce language — such as the Gricean maxims (Grice, 1975). However, the algorithms have rarely been tested against real data from human referring expression generation.1 In this paper, we present a data set containing human-produced referring expressions in a limited domain. Focussing specifically on the algorithms 1The only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select, but with phenomena such as how people group entities together (Funakoshi et al., 2004; Gatt, 2006), or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der Sluis and Krahmer, 2004). 63 Proceedings of the Fourth International Natural Language Generation Conference, pages 63–70, Sydney, July 2006. c�2006 Association for Computational Linguistics presented in (Dale, 1989), (Dale and Haddock, 1991) and (Reiter and Dale, 1992), we explore how well these algorithms perform in the same context. There are significant differences between the referring expressions produced by humans, and those produced by the algorithms; we</context>
</contexts>
<marker>Funakoshi, Watanabe, Kuriyama, Tokunaga, 2004</marker>
<rawString>K. Funakoshi, S. Watanabe, N. Kuriyama, and T. Tokunaga. 2004. Generating referring expressions using perceptual groups. In Proceedings of the 3rd INLG, pages 51–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gardent</author>
</authors>
<title>Generating minimal definite descriptions.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="2812" citStr="Gardent, 2002" startWordPosition="441" endWordPosition="442">s able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups. Some approaches reuse parts of other algorithms: the Branch and Bound algorithm (Krahmer et al., 2003) uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. There are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000; Gardent, 2002). Their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kinds of principles that we believe people might be following when they produce language — such as the Gricean maxims (Grice, 1975). However, the algorithms have rarely been tested against real data from human referring expression generation.1 In this paper, we present a data set containing human-produced referring expressions in a limited domain. Focussing specifically on the algorithms 1The only exceptions we know of to this deficit are not directly concerned with </context>
</contexts>
<marker>Gardent, 2002</marker>
<rawString>C. Gardent. 2002. Generating minimal definite descriptions. In Proceedings of the 40th Meeting of the ACL, pages 96–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gatt</author>
</authors>
<title>Structuring knowledge for reference generation: A clustering algorithm.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Meeting of the EACL.</booktitle>
<contexts>
<context position="3549" citStr="Gatt, 2006" startWordPosition="558" endWordPosition="559">me kinds of principles that we believe people might be following when they produce language — such as the Gricean maxims (Grice, 1975). However, the algorithms have rarely been tested against real data from human referring expression generation.1 In this paper, we present a data set containing human-produced referring expressions in a limited domain. Focussing specifically on the algorithms 1The only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select, but with phenomena such as how people group entities together (Funakoshi et al., 2004; Gatt, 2006), or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der Sluis and Krahmer, 2004). 63 Proceedings of the Fourth International Natural Language Generation Conference, pages 63–70, Sydney, July 2006. c�2006 Association for Computational Linguistics presented in (Dale, 1989), (Dale and Haddock, 1991) and (Reiter and Dale, 1992), we explore how well these algorithms perform in the same context. There are significant differences between the referring expressions produced by humans, and those produced by the algorithms; we explore thes</context>
</contexts>
<marker>Gatt, 2006</marker>
<rawString>A. Gatt. 2006. Structuring knowledge for reference generation: A clustering algorithm. In Proceedings of the 11th Meeting of the EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<booktitle>Syntax and Semantics Volume 3: Speech Acts,</booktitle>
<pages>43--58</pages>
<editor>In P. Cole and J. Morgan, editors,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="3072" citStr="Grice, 1975" startWordPosition="484" endWordPosition="485">: the Branch and Bound algorithm (Krahmer et al., 2003) uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. There are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000; Gardent, 2002). Their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kinds of principles that we believe people might be following when they produce language — such as the Gricean maxims (Grice, 1975). However, the algorithms have rarely been tested against real data from human referring expression generation.1 In this paper, we present a data set containing human-produced referring expressions in a limited domain. Focussing specifically on the algorithms 1The only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select, but with phenomena such as how people group entities together (Funakoshi et al., 2004; Gatt, 2006), or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der S</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>H. P. Grice. 1975. Logic and conversation. In P. Cole and J. Morgan, editors, Syntax and Semantics Volume 3: Speech Acts, pages 43–58. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Horacek</author>
</authors>
<title>An algorithm for generating referential descriptions with flexible interfaces.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Meeting of the ACL,</booktitle>
<pages>127--134</pages>
<contexts>
<context position="2768" citStr="Horacek, 1997" startWordPosition="435" endWordPosition="436">Expressions algorithm (van Deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups. Some approaches reuse parts of other algorithms: the Branch and Bound algorithm (Krahmer et al., 2003) uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. There are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000; Gardent, 2002). Their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kinds of principles that we believe people might be following when they produce language — such as the Gricean maxims (Grice, 1975). However, the algorithms have rarely been tested against real data from human referring expression generation.1 In this paper, we present a data set containing human-produced referring expressions in a limited domain. Focussing specifically on the algorithms 1The only exceptions we know of to t</context>
</contexts>
<marker>Horacek, 1997</marker>
<rawString>H. Horacek. 1997. An algorithm for generating referential descriptions with flexible interfaces. In Proceedings of the 35th Meeting of the ACL, pages 127– 134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>M Theune</author>
</authors>
<title>Efficient contextsensitive generation of referring expressions.</title>
<date>2002</date>
<booktitle>Information Sharing: Reference and Presupposition in Language Generation and Interpretation,</booktitle>
<pages>223--264</pages>
<editor>In K. van Deemter and R. Kibble, editors,</editor>
<publisher>CSLI.</publisher>
<contexts>
<context position="2058" citStr="Krahmer and Theune, 2002" startWordPosition="313" endWordPosition="316">l Brevity (FB) algorithm, has served as a starting point for many subsequent GRE algorithms. To overcome its limitation to one-place predicates, Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations; and as a response to the computational complexity of ‘greedy’ algorithms like FB, Reiter and Dale (Reiter and Dale, 1992; Dale and Reiter, 1995) introduced the psycholinguistically motivated Incremental Algorithm (IA). In recent years there have been a number of important extensions to the IA. The Context-Sensitive extension (Krahmer and Theune, 2002) is able to generate referring expressions for the most salient entity in a context; the Boolean Expressions algorithm (van Deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups. Some approaches reuse parts of other algorithms: the Branch and Bound algorithm (Krahmer et al., 2003) uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-</context>
</contexts>
<marker>Krahmer, Theune, 2002</marker>
<rawString>E. Krahmer and M. Theune. 2002. Efficient contextsensitive generation of referring expressions. In K. van Deemter and R. Kibble, editors, Information Sharing: Reference and Presupposition in Language Generation and Interpretation, pages 223– 264. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>S van Erk</author>
<author>A Verleg</author>
</authors>
<title>Graphbased generation of referring expressions.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Krahmer, van Erk, Verleg, 2003</marker>
<rawString>E. Krahmer, S. van Erk, and A. Verleg. 2003. Graphbased generation of referring expressions. Computational Linguistics, 29(1):53–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D McDonald</author>
</authors>
<title>Natural Language Generation as a Process of Decision-making Under Constraints.</title>
<date>1980</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="1297" citStr="McDonald, 1980" startWordPosition="199" endWordPosition="200">significant differences between what people do and what these algorithms do. On the basis of these observations, we suggest some ways forward that attempt to address these differences. 1 Introduction The generation of referring expressions (henceforth GRE) — that is, the process of working out what properties of an entity should be used to describe it in such a way as to distinguish it from other entities in the context — is a recurrent theme in the natural language generation literature. The task is discussed informally in some of the earliest work on NLG (in particular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981)), but the first formally explicit algorithm was introduced in (Dale, 1989); this algorithm, often referred to as the Full Brevity (FB) algorithm, has served as a starting point for many subsequent GRE algorithms. To overcome its limitation to one-place predicates, Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations; and as a response to the computational complexity of ‘greedy’ algorithms like FB, Reiter and Dale (Reiter and Dale, 1992; Dale and Reiter, 1995) introduced the psycholinguistically motivated I</context>
</contexts>
<marker>McDonald, 1980</marker>
<rawString>D. D. McDonald. 1980. Natural Language Generation as a Process of Decision-making Under Constraints. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pechmann</author>
</authors>
<title>Incremental speech production and referential overspecification.</title>
<date>1989</date>
<journal>Linguistics,</journal>
<pages>27--89</pages>
<contexts>
<context position="26464" citStr="Pechmann, 1989" startWordPosition="4440" endWordPosition="4441">which the algorithms we have explored here do not produce outputs that are the same as those produced by humans. Some comments on the generalisability of these results are appropriate. First, our results may be idiosyncratic to the specifics of the particular domain of our experiment. We would point out, however, that the domain is more complex, and arguably more realistic, than the much-simplified experimental contexts that have served as intuitions for earlier work in the field; we have in mind here in particular the experiments discussed in (Ford and Olson, 1975), (Sonnenschein, 1985) and (Pechmann, 1989). In the belief that the data provides a good test set for the generation of referring expressions, we are making the data set publicly available 5, so others may try to develop algorithms covering the data. A second concern is that we have only explored the extent to which three specific algorithms are able to cover the human data. Many of the other algorithms in the literature take these as a base, and so are unlikely to deliver significantly different results. The major exceptions here may be (a) van Deemter’s (2002) algorithm for sets; recall that we excluded from the human data used here </context>
</contexts>
<marker>Pechmann, 1989</marker>
<rawString>T. Pechmann. 1989. Incremental speech production and referential overspecification. Linguistics, 27:89–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Dale</author>
</authors>
<title>A fast algorithm for the generation of referring expressions.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th Meeting of the ACL,</booktitle>
<pages>232--238</pages>
<contexts>
<context position="1825" citStr="Reiter and Dale, 1992" startWordPosition="279" endWordPosition="282">mally in some of the earliest work on NLG (in particular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981)), but the first formally explicit algorithm was introduced in (Dale, 1989); this algorithm, often referred to as the Full Brevity (FB) algorithm, has served as a starting point for many subsequent GRE algorithms. To overcome its limitation to one-place predicates, Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations; and as a response to the computational complexity of ‘greedy’ algorithms like FB, Reiter and Dale (Reiter and Dale, 1992; Dale and Reiter, 1995) introduced the psycholinguistically motivated Incremental Algorithm (IA). In recent years there have been a number of important extensions to the IA. The Context-Sensitive extension (Krahmer and Theune, 2002) is able to generate referring expressions for the most salient entity in a context; the Boolean Expressions algorithm (van Deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups. Some approac</context>
<context position="3940" citStr="Reiter and Dale, 1992" startWordPosition="612" endWordPosition="615">he algorithms 1The only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select, but with phenomena such as how people group entities together (Funakoshi et al., 2004; Gatt, 2006), or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der Sluis and Krahmer, 2004). 63 Proceedings of the Fourth International Natural Language Generation Conference, pages 63–70, Sydney, July 2006. c�2006 Association for Computational Linguistics presented in (Dale, 1989), (Dale and Haddock, 1991) and (Reiter and Dale, 1992), we explore how well these algorithms perform in the same context. There are significant differences between the referring expressions produced by humans, and those produced by the algorithms; we explore these differences and consider what it means for work in the generation of referring expressions. The remainder of this paper is structured as follows. In Section 2, we introduce the data set of human-produced referring expressions we use; in Section 3, we introduce the representational framework we use to model the domain underlying this data; in Section 4 we introduce the three algorithms c</context>
<context position="12079" citStr="Reiter and Dale, 1992" startWordPosition="1984" endWordPosition="1987"> domain results in this inferred knowledge about transitive relations being of little use; in fact, in most cases, the implementation of transitive inference might even result in the generation of unnatural descriptions, such as the orange drawer (two) right of the blue drawer for d12. Another aspect of the representation of relations that requires a decision is that of generalisation: 65 next-to is a generalisation of the relations left-of and right-of. The only algorithm of those we examine here that provides a mechanism for exploring a generalisation hierarchy is the Incremental Algorithm (Reiter and Dale, 1992), and this cannot handle relations; so, we take the shortcut of explicitly representing the next-to relation for every left-of and right-of relation in the knowledge base. We then implement special-case handling that ensures that, if one of these facts is used, the more general or more specific case is also deleted from the set of properties still available for the description.3 4 The Algorithms As we have already noted above, there is a considerable literature on the generation of referring expressions, and many papers in the area provide detailed algorithms. We focus here on the following al</context>
<context position="15566" citStr="Reiter and Dale, 1992" startWordPosition="2623" endWordPosition="2626">ue ← no-value endif if (more-specific-value ← MoreSpecificValue(r, A, value)) =6 nil ∧ (new-value ← FindBestValue(A, more-specific-value)) =6 nil ∧ (|RulesOut(hA, new-valuei) |&gt; |RulesOut(hA, valuei)|) then value ← new-value endif return value RulesOut(hA, V i) if V = no-value then return nil else return {x : x ∈ C ∧ UserKnows(x, hA, V i) = false} endif Algorithm 3: The Incremental Algorithm 66 • The relational algorithm from (Dale and Haddock, 1991) uses constraint satisfaction to incorporate relational properties while avoiding infinite regress; see Algorithm 2. • the Incremental Algorithm (Reiter and Dale, 1992; Dale and Reiter, 1995) considers the available properties to be used in a description via a preference ordering over those properties; see Algorithm 3. For the purpose of this study, the algorithms were implemented in Common LISP. The mechanism described in (Dale and Reiter, 1995) to handle generalisation hierarchies for values for the different properties, referred to in the algorithm here as FindBestValue, was not implemented since, as discussed earlier, our representation of the domain does not make use of a hierarchy of properties. 5 The Output of the Algorithms Using the knowledge base </context>
</contexts>
<marker>Reiter, Dale, 1992</marker>
<rawString>E. Reiter and R. Dale. 1992. A fast algorithm for the generation of referring expressions. In Proceedings of the 14th Meeting of the ACL, pages 232–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rosch</author>
</authors>
<title>Principles of categorization.</title>
<date>1978</date>
<booktitle>In Cognition and Categorization,</booktitle>
<pages>27--48</pages>
<location>Lawrence Erlbaum, Hillsdale, NJ.</location>
<marker>Rosch, 1978</marker>
<rawString>E. Rosch. 1978. Principles of categorization. In Cognition and Categorization, pages 27–48. Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sonnenschein</author>
</authors>
<title>The development of referential communication skills: Some situations in which speakers give redundant messages.</title>
<date>1985</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>14--489</pages>
<contexts>
<context position="26443" citStr="Sonnenschein, 1985" startWordPosition="4437" endWordPosition="4438">d a number of regards in which the algorithms we have explored here do not produce outputs that are the same as those produced by humans. Some comments on the generalisability of these results are appropriate. First, our results may be idiosyncratic to the specifics of the particular domain of our experiment. We would point out, however, that the domain is more complex, and arguably more realistic, than the much-simplified experimental contexts that have served as intuitions for earlier work in the field; we have in mind here in particular the experiments discussed in (Ford and Olson, 1975), (Sonnenschein, 1985) and (Pechmann, 1989). In the belief that the data provides a good test set for the generation of referring expressions, we are making the data set publicly available 5, so others may try to develop algorithms covering the data. A second concern is that we have only explored the extent to which three specific algorithms are able to cover the human data. Many of the other algorithms in the literature take these as a base, and so are unlikely to deliver significantly different results. The major exceptions here may be (a) van Deemter’s (2002) algorithm for sets; recall that we excluded from the </context>
</contexts>
<marker>Sonnenschein, 1985</marker>
<rawString>S. Sonnenschein. 1985. The development of referential communication skills: Some situations in which speakers give redundant messages. Journal of Psycholinguistic Research, 14:489–508.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
</authors>
<title>On identifying sets.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st INLG,</booktitle>
<pages>116--123</pages>
<contexts>
<context position="2796" citStr="Stone, 2000" startWordPosition="439" endWordPosition="440">mter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups. Some approaches reuse parts of other algorithms: the Branch and Bound algorithm (Krahmer et al., 2003) uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. There are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000; Gardent, 2002). Their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kinds of principles that we believe people might be following when they produce language — such as the Gricean maxims (Grice, 1975). However, the algorithms have rarely been tested against real data from human referring expression generation.1 In this paper, we present a data set containing human-produced referring expressions in a limited domain. Focussing specifically on the algorithms 1The only exceptions we know of to this deficit are not directly</context>
</contexts>
<marker>Stone, 2000</marker>
<rawString>M. Stone. 2000. On identifying sets. In Proceedings of the 1st INLG, pages 116–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
<author>M M Halld´orsson</author>
</authors>
<title>Logical form equivalence: The case referring expressions generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 8th ENLG.</booktitle>
<marker>van Deemter, Halld´orsson, 2001</marker>
<rawString>K. van Deemter and M. M. Halld´orsson. 2001. Logical form equivalence: The case referring expressions generation. In Proceedings of the 8th ENLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
</authors>
<title>Generating referring expressions: Boolean extensions of the incremental algorithm.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<marker>van Deemter, 2002</marker>
<rawString>K. van Deemter. 2002. Generating referring expressions: Boolean extensions of the incremental algorithm. Computational Linguistics, 28(1):37–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I van der Sluis</author>
<author>E Krahmer</author>
</authors>
<title>Evaluating multimodal NLG using production experiments.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th LREC,</booktitle>
<pages>209--212</pages>
<marker>van der Sluis, Krahmer, 2004</marker>
<rawString>I. van der Sluis and E. Krahmer. 2004. Evaluating multimodal NLG using production experiments. In Proceedings of the 4th LREC, pages 209–212, 26-28 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="1281" citStr="Winograd, 1972" startWordPosition="197" endWordPosition="198">ify a number of significant differences between what people do and what these algorithms do. On the basis of these observations, we suggest some ways forward that attempt to address these differences. 1 Introduction The generation of referring expressions (henceforth GRE) — that is, the process of working out what properties of an entity should be used to describe it in such a way as to distinguish it from other entities in the context — is a recurrent theme in the natural language generation literature. The task is discussed informally in some of the earliest work on NLG (in particular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981)), but the first formally explicit algorithm was introduced in (Dale, 1989); this algorithm, often referred to as the Full Brevity (FB) algorithm, has served as a starting point for many subsequent GRE algorithms. To overcome its limitation to one-place predicates, Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations; and as a response to the computational complexity of ‘greedy’ algorithms like FB, Reiter and Dale (Reiter and Dale, 1992; Dale and Reiter, 1995) introduced the psycholinguistic</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>T. Winograd. 1972. Understanding Natural Language. Academic Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>