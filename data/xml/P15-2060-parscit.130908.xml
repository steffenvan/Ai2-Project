<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003973">
<title confidence="0.998321">
Event Detection and Domain Adaptation
with Convolutional Neural Networks
</title>
<author confidence="0.996951">
Thien Huu Nguyen
</author>
<affiliation confidence="0.996348">
Computer Science Department
</affiliation>
<address confidence="0.783726">
New York University
New York, NY 10003 USA
</address>
<email confidence="0.999637">
thien@cs.nyu.edu
</email>
<sectionHeader confidence="0.993917" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977923076923">
We study the event detection problem us-
ing convolutional neural networks (CNNs)
that overcome the two fundamental limi-
tations of the traditional feature-based ap-
proaches to this task: complicated feature
engineering for rich feature sets and er-
ror propagation from the preceding stages
which generate these features. The experi-
mental results show that the CNNs outper-
form the best reported feature-based sys-
tems in the general setting as well as the
domain adaptation setting without resort-
ing to extensive external resources.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997450136363636">
We address the problem of event detection (ED):
identifying instances of specified types of events
in text. Associated with each event mention is a
phrase, the event trigger (most often a single verb
or nominalization), which evokes that event. Our
task, more precisely stated, involves identifying
event triggers and classifying them into specific
types. For instance, according to the ACE 2005
annotation guideline1, in the sentence “A police
officer was killed in New Jersey today”, an event
detection system should be able to recognize the
word “killed” as a trigger for the event “Die”. This
task is quite challenging, as the same event might
appear in the form of various trigger expressions
and an expression might represent different events
in different contexts. ED is a crucial component
in the overall task of event extraction, which also
involves event argument discovery.
Recent systems for event extraction have em-
ployed either a pipeline architecture with separate
classifiers for trigger and argument labeling (Ji and
Grishman, 2008; Gupta and Ji, 2009; Patwardhan
</bodyText>
<footnote confidence="0.9706725">
1https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/
english-events-guidelines-v5.4.3.pdf
</footnote>
<author confidence="0.786756">
Ralph Grishman
</author>
<affiliation confidence="0.624651">
Computer Science Department
New York University
</affiliation>
<address confidence="0.866728">
New York, NY 10003 USA
</address>
<email confidence="0.995041">
grishman@cs.nyu.edu
</email>
<bodyText confidence="0.998108609756098">
and Rilof, 2009; Liao and Grishman, 2011; Mc-
Closky et al., 2011; Huang and Riloff, 2012; Li
et al., 2013a) or a joint inference architecture that
performs the two subtasks at the same time to ben-
efit from their inter-dependencies (Riedel and Mc-
Callum, 2011a; Riedel and McCallum, 2011b; Li
et al., 2013b; Venugopal et al., 2014). Both ap-
proaches have coped with the ED task by elabo-
rately hand-designing a large set of features (fea-
ture engineering) and utilizing the existing super-
vised natural language processing (NLP) toolkits
and resources (i.e name tagger, parsers, gazetteers
etc) to extract these features to be fed into sta-
tistical classifiers. Although this approach has
achieved the top performance (Hong et al., 2011;
Li et al., 2013b), it suffers from at least two issues:
(i) The choice of features is a manual process
and requires linguistic intuition as well as domain
expertise, implying additional studies for new ap-
plication domains and limiting the capacity to
quickly adapt to these new domains.
(ii) The supervised NLP toolkits and resources
for feature extraction might involve errors (either
due to the imperfect nature or the performance
loss of the toolkits on new domains (Blitzer et al.,
2006; Daum´e III, 2007; McClosky et al., 2010)),
probably propagated to the final event detector.
This paper presents a convolutional neural net-
work (LeCun et al., 1988; Kalchbrenner et al.,
2014) for the ED task that automatically learns
features from sentences, and minimizes the depen-
dence on supervised toolkits and resources for fea-
tures, thus alleviating the error propagation and
improving the performance for this task. Due
to the emerging interest of the NLP community
in deep learning recently, CNNs have been stud-
ied extensively and applied effectively in vari-
ous tasks: semantic parsing (Yih et al., 2014),
search query retrieval (Shen et al., 2014), seman-
tic matching (Hu et al., 2014), sentence modeling
and classification (Kalchbrenner et al., 2014; Kim,
</bodyText>
<page confidence="0.934225">
365
</page>
<note confidence="0.381903">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 365–371,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999767">
Figure 1: Convolutional Neural Network for Event Detection.
</figureCaption>
<bodyText confidence="0.992518173913043">
2014), name tagging and semantic role labeling
(Collobert et al., 2011), relation classification and
extraction (Zeng et al., 2014; Nguyen and Grish-
man, 2015). However, to the best of our knowl-
edge, this is the first work on event detection via
CNNs so far.
First, we evaluate CNNs for ED in the general
setting and show that CNNs, though not requir-
ing complicated feature engineering, can still out-
perform the state-of-the-art feature-based meth-
ods extensively relying on the other supervised
modules and manual resources for features. Sec-
ond, we investigate CNNs in a domain adaptation
(DA) setting for ED. We demonstrate that CNNs
significantly outperform the traditional feature-
based methods with respect to generalization per-
formance across domains due to: (i) their capac-
ity to mitigate the error propagation from the pre-
processing modules for features, and (ii) the use
of word embeddings to induce a more general rep-
resentation for trigger candidates. We believe that
this is also the first research on domain adaptation
using CNNs.
</bodyText>
<sectionHeader confidence="0.984201" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.997164114285714">
We formalize the event detection problem as a
multi-class classification problem. Given a sen-
tence, for every token in that sentence, we want to
predict if the current token is an event trigger: i.e,
does it express some event in the pre-defined event
set or not (Li et al., 2013b)? The current token
along with its context in the sentence constitute
an event trigger candidate or an example in multi-
class classification terms. In order to prepare for
the CNNs, we limit the context to a fixed window
size by trimming longer sentences and padding
shorter sentences with a special token when nec-
essary. Let 2w + 1 be the fixed window size,
and x = [x−w, x−w+1, ... , x0, ... , xw−1, xw] be
some trigger candidate where the current token is
positioned in the middle of the window (token x0).
Before entering the CNNs, each token xi is trans-
formed into a real-valued vector by looking up the
following embedding tables to capture different
characteristics of the token:
- Word Embedding Table (initialized by some
pre-trained word embeddings): to capture the hid-
den semantic and syntactic properties of the tokens
(Collobert and Weston, 2008; Turian et al., 2010).
- Position Embedding Table: to embed the rel-
ative distance i of the token xi to the current token
x0. In practice, we initialize this table randomly.
- Entity Type Embedding Table: If we further
know the entity mentions and their entity types2
in the sentence, we can also capture this informa-
tion for each token by looking up the entity type
embedding table (initialized randomly) using the
entity type associated with each token. We em-
ploy the BIO annotation scheme to assign entity
type labels to each token in the trigger candidate
</bodyText>
<footnote confidence="0.9973635">
2For convenience, when mentioning entities in this paper,
we always include ACE timex and values.
</footnote>
<page confidence="0.99831">
366
</page>
<bodyText confidence="0.999845037037037">
using the heads of the entity mentions.
For each token xi, the vectors obtained from the
three look-ups above are concatenated into a sin-
gle vector xi to represent the token. As a result,
the original event trigger x is transformed into a
matrix x = [x−w, x−w+1, ... , x0,... , xw−1, xw]
of size mt x (2w + 1) (mt is the dimensionality of
the concatenated vectors of the tokens).
The matrix representation x is then passed
through a convolution layer, a max pooling layer
and a softmax at the end to perform classifica-
tion (like (Kim, 2014; Kalchbrenner et al., 2014)).
In the convolution layer, we have a set of feature
maps (filters) 1f1, f2,. .. , fn} for the convolution
operation. Each feature map fi corresponds to
some window size k and can be essentially seen
as a weight matrix of size mt x k. Figure 1 illus-
trates the proposed CNN.
The gradients are computed using back-
propagation; regularization is implemented by a
dropout (Kim, 2014; Hinton et al., 2012), and
training is done via stochastic gradient descent
with shuffled mini-batches and the AdaDelta up-
date rule (Zeiler, 2012; Kim, 2014). During the
training, we also optimize the weights of the three
embedding tables at the same time to reach an ef-
fective state (Kim, 2014).
</bodyText>
<sectionHeader confidence="0.999858" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999895">
3.1 Dataset, Hyperparameters and Resources
</subsectionHeader>
<bodyText confidence="0.979299538461538">
As the benefit of multiple window sizes in the con-
volution layer has been demonstrated in the previ-
ous work on sentence modeling (Kalchbrenner et
al., 2014; Kim, 2014), in the experiments below,
we use window sizes in the set 12, 3, 4, 51 to gen-
erate feature maps. We utilize 150 feature maps
for each window size in this set. The window size
for triggers is set to 31 while the dimensionality of
the position embeddings and entity type embed-
dings is 503.We inherit the values for the other pa-
rameters from Kim (2014), i.e, the dropout rate
p = 0.5, the mini-batch size = 50, the hyperpa-
rameter for the l2 norms = 3. Finally, we em-
ploy the pre-trained word embeddings word2vec
with 300 dimensions from Mikolov et al. (2013)
for initialization.
We evaluate the presented CNN over the ACE
2005 corpus. For comparison purposes, we uti-
lize the same test set with 40 newswire articles
3These values are chosen for their best performance on
the development data.
(672 sentences), the same development set with
30 other documents (836 sentences) and the same
training set with the remaning 529 documents
(14,849 sentences) as the previous studies on this
dataset (Ji and Grishman, 2008; Liao and Grish-
man, 2010; Li et al., 2013b). The ACE 2005 cor-
pus has 33 event subtypes that, along with one
class “None” for the non-trigger tokens, consti-
tutes a 34-class classification problem.
In order to evaluate the effectiveness of the posi-
tion embeddings and the entity type embeddings,
Table 1 reports the performance of the proposed
CNN on the development set when these embed-
dings are either included or excluded from the sys-
tems. With the large margins of performance, it is
very clear from the table that the position embed-
dings are crucial while the entity embeddings are
also very useful for CNNs on ED.
</bodyText>
<table confidence="0.999819">
Systems P R F
-Entity Types -Position 16.8 12.0 14.0
+Position 75.0 63.0 68.5
+Entity Types -Position 17.0 15.0 15.9
+Position 75.6 66.4 70.7
</table>
<tableCaption confidence="0.999953">
Table 1: Performance on the Development Set.
</tableCaption>
<bodyText confidence="0.9912242">
For the experiments below, we examine the
CNNs in two scenarios: excluding the entity type
embeddings (CNN1) and including the entity type
embeddings (CNN2). We always use position em-
beddings in these two scenarios.
</bodyText>
<subsectionHeader confidence="0.9972">
3.2 Performance Comparison
</subsectionHeader>
<bodyText confidence="0.999968">
The state-of-the-art systems for event detection on
the ACE 2005 dataset have followed the traditional
feature-based approach with rich hand-designed
feature sets, and statistical classifiers such as Max-
Ent and perceptron for structured prediction in a
joint architecture (Hong et al., 2011; Li et al.,
2013b). In this section, we compare the proposed
CNNs with these state-of-the-art systems on the
blind test set. Table 2 presents the overall per-
formance of the systems with gold-standard entity
mention and type information4.
As we can see from the table, considering the
systems that only use sentence level information,
CNN1 significantly outperforms the MaxEnt clas-
sifier as well as the joint beam search with local
features from Li et al. (2013b) (an improvement
of 1.6% in F1 score), and performs comparably
</bodyText>
<footnote confidence="0.991144">
4Entity mentions and types are used to introduce more
features into the systems.
</footnote>
<page confidence="0.966311">
367
</page>
<table confidence="0.999417">
Methods P R F
Sentence-level in Hong et al 67.6 53.5 59.7
(2011)
MaxEnt with local features in 74.5 59.1 65.9
Li et al. (2013b)
Joint beam search with local 73.7 59.3 65.7
features in Li et al. (2013b)
Joint beam search with local 73.7 62.3 67.5
and global features in Li et al.
(2013b)
Cross-entity in Hong et al. 72.9 64.3 68.3
(2011) †
CNN1: CNN without any 71.9 63.8 67.6
external features
CNN2: CNN augmented with 71.8 66.4 69.0
entity types
</table>
<tableCaption confidence="0.9960395">
Table 2: Performance with Gold-Standard Entity Mentions
and Types. † beyond sentence level.
</tableCaption>
<bodyText confidence="0.99998668">
with the joint beam search approach using both lo-
cal and global features (Li et al., 2013b). This is
remarkable since CNN1 does not require any ex-
ternal features5, in contrast to the other feature-
based systems that extensively rely on such exter-
nal features to perform well. More interestingly,
when the entity type information is incorporated
into CNN1, we obtain CNN2 that still only needs
sentence level information but achieves the state-
of-the-art performance for this task (an improve-
ment of 1.5% over the best system with only sen-
tence level information (Li et al., 2013b)).
Except for CNN1, all the systems reported in
Table 2 employ the gold-standard (perfect) entities
mentions and types from manual annotation which
might not be available in reality. Table 3 compares
the performance of CNN1 and the feature-based
systems in a more realistic setting, where entity
mentions and types are acquired from an auto-
matic high-performing name tagger and informa-
tion extraction system (Li et al., 2013b). Note that
CNN1 is eligible for this comparison as it does not
utilize any external features, thus avoiding usage
of the name tagger and the information extraction
system to identify entity mentions and types.
</bodyText>
<subsectionHeader confidence="0.981454">
3.3 Domain Adaptation Experiment
</subsectionHeader>
<bodyText confidence="0.9998322">
In this section, we aim to further compare the pro-
posed CNNs with the feature-based systems under
the domain adaptation setting for event detection.
The ultimate goal of domain adaptation re-
search is to develop techniques taking training
</bodyText>
<footnote confidence="0.98135675">
5External features are the features generated from the su-
pervised NLP modules and manual resources such as parsers,
name tagger, entity mention extractors (either automatic or
manual), gazetteers etc.
</footnote>
<table confidence="0.961585375">
Methods F
Sentence level in Ji and Grishman (2008) 59.7
MaxEnt with local features in Li et al. (2013b) 64.7
Joint beam search with local features in Li et 63.7
al. (2013b)
Joint beam search with local and global 65.6
features in Li et al. (2013b)
CNN1: CNN without any external features 67.6
</table>
<tableCaption confidence="0.993758">
Table 3: Performance with Predicted Entity Mentions and
Types.
</tableCaption>
<bodyText confidence="0.99964625">
data in some source domain and learning models
that can work well on target domains. The target
domains are supposed to be so dissimilar from the
source domain that the learning techniques would
suffer from a significant performance loss when
trained on the source domain and applied to the
target domains. To make it clear, we address the
unsupervised DA problem in this section, i.e no
training data in the target domains (Blitzer et al.,
2006; Plank and Moschitti, 2013). The fundamen-
tal reason for the performance loss of the feature-
based systems on the target domains is twofold:
</bodyText>
<listItem confidence="0.6187388">
(i) The behavioral changes of features across
domains: As domains differ, some features might
be informative in the source domain but become
less relevant in the target domains and vice versa.
(ii) The propagated errors of the pre-processing
toolkits for lower-level tasks (POS tagging, name
tagging, parsing etc) to extract features: These
pre-processing toolkits are also known to degrade
when shifted to target domains (Blitzer et al.,
2006; Daum´e III, 2007; McClosky et al., 2010),
</listItem>
<bodyText confidence="0.996001578947369">
introducing noisy features into the systems for
higher-level tasks in the target domains and even-
tually impairing the performance of these higher-
level systems on the target domains.
For ED, we postulate that CNNs are more use-
ful than the feature-based approach for DA for two
reasons. First, rather than relying on the symbolic
and concrete forms (i.e words, types etc) to con-
struct features as the traditional feature-based sys-
tems (Ji and Grishman, 2008; Li et al., 2013b)
do, CNNs automatically induce their features from
word embeddings, the general distributed repre-
sentation of words that is shared across domains.
This helps CNNs mitigate the lexical sparsity,
learn more general and effective feature represen-
tation for trigger candidates, and thus bridge the
gap between domains. Second, as CNNs mini-
mize the reliance on the supervised pre-processing
toolkits for features, they can alleviate the error
</bodyText>
<page confidence="0.995098">
368
</page>
<table confidence="0.999882">
System In-domain(bn+nw) bc cts wl
P R F P R F P R F P R F
MaxEnt 74.5 59.4 66.0 70.1 54.5 61.3 66.4 49.9 56.9 59.4 34.9 43.9
Joint beam search in Li et al. (2013b) 73.5 62.7 67.7 70.3 57.2 63.1 64.9 50.8 57.0 59.5 38.4 46.7
Joint+Local
Joint+Local+Global 72.9 63.2 67.7 68.8 57.5 62.6 64.5 52.3 57.7 56.4 38.5 45.7
CNN1 70.9 64.0 67.3 71.0 61.9 66.1† 64.0 55.0 59.1 53.2 38.4 44.6
CNN2 69.2 67.0 68.0 70.2 65.2 67.6† 68.3 58.2 62.8† 54.8 42.0 47.5
</table>
<tableCaption confidence="0.9985715">
Table 4: In-domain (first column) and Out-of-domain Performance (columns two to four). Cells marked with †designate
CNN models that significantly outperform (p &lt; 0.05) all the reported feature-based methods on the specified domain.
</tableCaption>
<bodyText confidence="0.835773">
propagation and be more robust to domain shifts.
</bodyText>
<subsectionHeader confidence="0.767898">
3.3.1 Dataset
</subsectionHeader>
<bodyText confidence="0.9999965">
We also do the experiments in this part over the
ACE 2005 dataset but focus more on the difference
between domains. The ACE 2005 corpus comes
with 6 different domains: broadcast conversation
(bc), broadcast news (bn), telephone conversation
(cts), newswire (nw), usenet (un) and webblogs
(wl). Following the common practice of domain
adaptation research on this dataset (Plank and
Moschitti, 2013; Nguyen and Grishman, 2014),
we use news (the union of bn and nw) as the
source domain and bc, cts, wl as three different
target domains. We take half of bc as the devel-
opment set and use the remaining data for testing.
We note that the distribution of event subtypes and
the vocabularies of the source and target domains
are quite different (Plank and Moschitti, 2013).
</bodyText>
<subsectionHeader confidence="0.95405">
3.3.2 Domain Adaptation Results
</subsectionHeader>
<bodyText confidence="0.999939388888889">
Table 4 presents the performance of five systems:
the MaxEnt classifier with the local features from
Li et al. (2013b) (called MaxEnt); the state-of-the-
art joint beam search systems with: (i) only local
features (called Joint+Local); and (ii) both local
and global features (called Joint+Local+Global)
in Li et al. (2013b) (the baseline systems); CNN1
and CNN2 via 5-fold cross validation. For each
system, we train a model on the training set of the
source domain and report the performance of this
model on the test set of the source domain (in-
domain performance) as well as the performance
of the model on the three target domains bc, cts
and wl (out-of-domain performance)6.
The main conclusions from the table include:
(i) The baseline systems MaxEnt, Joint+Local,
Joint+Local+Global achieve high performance on
the source domain, but degrade dramatically on
</bodyText>
<footnote confidence="0.760673333333333">
6The performance of the feature-based systems MaxEnt,
Joint+Local and Joint+Local+Global are obtained from the
actual systems in Li et al. (2013b).
</footnote>
<bodyText confidence="0.999962705882353">
the target domains due to the domain shifts. (ii)
Comparing CNN1 and the baseline systems, we
see that CNN1 performs comparably with the
baseline systems on the source domain (in-domain
performance) (as expected), substantially outper-
form the baseline systems on two of the three tar-
get domains (i.e, bc and cts), and is only less ef-
fective than the joint beam search approach on
the wl domain; (iii) Finally and most importantly,
we consistently achieve the best adaptation perfor-
mance across all the target domains with CNN2
by only introducing entity type information into
CNN1. In fact, CNN2 significantly outperforms
the feature-based systems with p &lt; 0.05 and large
margins of about 5.0% on the domains bc and cts,
clearly confirming our argument in Section 3.3 and
testifying to the benefits of CNNs on DA for ED.
</bodyText>
<sectionHeader confidence="0.999297" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999997142857143">
We present a CNN for event detection that auto-
matically learns effective feature representations
from pre-trained word embeddings, position em-
beddings as well as entity type embeddings and
reduces the error propagation. We conducted ex-
periments to compare the proposed CNN with the
state-of-the-art feature-based systems in both the
general setting and the domain adaptation setting.
The experimental results demonstrate the effec-
tiveness as well as the robustness across domains
of the CNN. In the future, our plans include: (i)
to explore the joint approaches for event extrac-
tion with CNNs; (ii) and to investigate other neural
network architectures for information extraction.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999593333333333">
We would like to thank Qi Li for providing the per-
formance of the feature-based systems on the do-
main adaptation experiments. Thank you to Yifan
He, Kai Cao, and Xiang Li for useful discussions
on the task as well as the anonymous reviewers for
their valuable feedback.
</bodyText>
<page confidence="0.998516">
369
</page>
<sectionHeader confidence="0.989679" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999796462264151">
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Corre-
spondence Learning. In Proceedings of EMNLP.
Ronan Collobert and Jason Weston. 2008. A Uni-
fied Architecture for Natural Language Processing:
Deep Neural Networks with Multitask Learning. In
Proceedings of ICML.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research
12:24932537.
Hal Daum´e III. 2007. Frustratingly Easy Domain
Adaptation. In Proceedings of ACL.
Prashant Gupta and Heng Ji. 2009. Predicting Un-
known Time Arguments Based on Cross-Event Prop-
agation. In Proceedings of ACL-IJCNLP.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012. Improving Neural Networks by
Preventing Co-Adaptation of Feature Detectors.
CoRR, abs/1207.0580.
Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
Cross-entity Inference to Improve Event Extraction.
In Proceedings of ACL.
Baotian Hu, Zhengdong Lu, Hang Li, Qingcai Chen.
2014. Convolutional Neural Network Architectures
for Matching Natural Language Sentences. In Pro-
ceedings of NIPS.
Ruihong Huang and Ellen Riloff. 2012. Modeling Tex-
tual Cohesion for Event Extraction. In Proceedings
of AAAI.
Heng Ji and Ralph Grishman. 2008. Refining Event
Extraction through Cross-Document Inference. In
Proceedings of ACL.
Nal Kalchbrenner, Edward Grefenstette and Phil Blun-
som. 2014. A Convolutional Neural Network for
Modeling Sentences. In Proceedings of ACL.
Yoon Kim. 2014. Convolutional Neural Networks for
Sentence Classification. In Proceedings of EMNLP.
Yann LeCun, L´eon Bottou, Yoshua Bengio and Patrick
Haffner. 1988. Gradient-based Learning Applied to
Document Recognition. In Proceedings of the IEEE,
86(11):22782324.
Peifeng Li, Qiaoming Zhu, and Guodong Zhou. 2013a.
Argument Inference from Relevant Event Mentions
in Chinese Argument Extraction. In Proceedings of
ACL.
Qi Li, Heng Ji, and Liang Huang. 2013b. Joint Event
Extraction via Structured Prediction with Global
Features. In Proceedings of ACL.
Shasha Liao and Ralph Grishman. 2010. Using Doc-
ument Level Cross-event Inference to Improve Event
Extraction. In Proceedings of ACL.
Shasha Liao and Ralph Grishman. 2011. Acquiring
Topic Features to Improve Event Extraction: in Pre-
selected and Balanced Collections. In Proceedings
RANLP.
David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic Domain Adaptation for Pars-
ing. In Proceedings of HLT-NAACL.
David McClosky, Mihai Surdeanu, and Chris Manning.
2011. Event Extraction as Dependency Parsing. In
Proceedings of ACL-HLT.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed Repre-
sentations of Words and Phrases and their Compo-
sitionality. In Proceedings of NIPS.
Thien Huu Nguyen and Ralph Grishman. 2014. Em-
ploying Word Representations and Regularization
for Domain Adaptation of Relation Extraction. In
Proceedings of ACL.
Thien Huu Nguyen and Ralph Grishman. 2015. Re-
lation Extraction: Perspective from Convolutional
Neural Networks. In Proceedings of the NAACL
Workshop on Vector Space Modeling for NLP
(VSM).
Siddharth Patwardhan and Ellen Rilof. 2009. A Uni-
fied Model of Phrasal and Sentential Evidence for
Information Extraction. In Proceedings of EMNLP.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding Semantic Similarity in Tree Kernels for Do-
main Adaptation ofRelation Extraction. In Proceed-
ings of ACL.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and Robust Joint Models for Biomedical Event Ex-
traction. In Proceedings of EMNLP.
Sebastian Riedel and Andrew McCallum. 2011. Ro-
bust Biomedical Event Extraction with Dual Decom-
position and Minimal Domain Adaptation. In Pro-
ceedings of the BioNLP Shared Task 2011 Work-
shop.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng and
Gr´egoire Mesnil. 2014. Learning Semantic Repre-
sentations Using Convolutional Neural Networks for
Web Search. In Proceedings of WWW.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-supervised Learning. In Proceed-
ings of ACL.
Deepak Venugopal, Chen Chen, Vibhav Gogate and
Vincent Ng. 2014. Relieving the Computational
Bottleneck: Joint Inference for Event Extraction
with High-Dimensional Features. In Proceedings of
EMNLP.
</reference>
<page confidence="0.974891">
370
</page>
<reference confidence="0.998785222222222">
Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic Parsing for Single-Relation Ques-
tion Answering. In Proceedings of ACL.
Matthew D. Zeiler. 2012. ADADELTA: An Adaptive
Learning Rate Method. CoRR, abs/1212.5701.
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou
and Jun Zhao. 2014. Relation Classification via
Convolutional Deep Neural Network. In Proceed-
ings of COLING.
</reference>
<page confidence="0.998755">
371
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950302">
<title confidence="0.99944">Event Detection and Domain with Convolutional Neural Networks</title>
<author confidence="0.999312">Thien Huu</author>
<affiliation confidence="0.998453">Computer Science</affiliation>
<address confidence="0.9932865">New York New York, NY 10003</address>
<email confidence="0.999433">thien@cs.nyu.edu</email>
<abstract confidence="0.997580285714286">We study the event detection problem using convolutional neural networks (CNNs) that overcome the two fundamental limitations of the traditional feature-based approaches to this task: complicated feature engineering for rich feature sets and error propagation from the preceding stages which generate these features. The experimental results show that the CNNs outperform the best reported feature-based systems in the general setting as well as the domain adaptation setting without resorting to extensive external resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain Adaptation with Structural Correspondence Learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3231" citStr="Blitzer et al., 2006" startWordPosition="490" endWordPosition="493">hese features to be fed into statistical classifiers. Although this approach has achieved the top performance (Hong et al., 2011; Li et al., 2013b), it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010)), probably propagated to the final event detector. This paper presents a convolutional neural network (LeCun et al., 1988; Kalchbrenner et al., 2014) for the ED task that automatically learns features from sentences, and minimizes the dependence on supervised toolkits and resources for features, thus alleviating the error propagation and improving the performance for this task. Due to the emerging interest of the NLP community in deep learning recently, CNNs have been studied extensively and applied effectively in various tasks: semantic parsing (Yih </context>
<context position="14641" citStr="Blitzer et al., 2006" startWordPosition="2389" endWordPosition="2392">am search with local and global 65.6 features in Li et al. (2013b) CNN1: CNN without any external features 67.6 Table 3: Performance with Predicted Entity Mentions and Types. data in some source domain and learning models that can work well on target domains. The target domains are supposed to be so dissimilar from the source domain that the learning techniques would suffer from a significant performance loss when trained on the source domain and applied to the target domains. To make it clear, we address the unsupervised DA problem in this section, i.e no training data in the target domains (Blitzer et al., 2006; Plank and Moschitti, 2013). The fundamental reason for the performance loss of the featurebased systems on the target domains is twofold: (i) The behavioral changes of features across domains: As domains differ, some features might be informative in the source domain but become less relevant in the target domains and vice versa. (ii) The propagated errors of the pre-processing toolkits for lower-level tasks (POS tagging, name tagging, parsing etc) to extract features: These pre-processing toolkits are also known to degrade when shifted to target domains (Blitzer et al., 2006; Daum´e III, 200</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain Adaptation with Structural Correspondence Learning. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="6524" citStr="Collobert and Weston, 2008" startWordPosition="1021" endWordPosition="1024">mming longer sentences and padding shorter sentences with a special token when necessary. Let 2w + 1 be the fixed window size, and x = [x−w, x−w+1, ... , x0, ... , xw−1, xw] be some trigger candidate where the current token is positioned in the middle of the window (token x0). Before entering the CNNs, each token xi is transformed into a real-valued vector by looking up the following embedding tables to capture different characteristics of the token: - Word Embedding Table (initialized by some pre-trained word embeddings): to capture the hidden semantic and syntactic properties of the tokens (Collobert and Weston, 2008; Turian et al., 2010). - Position Embedding Table: to embed the relative distance i of the token xi to the current token x0. In practice, we initialize this table randomly. - Entity Type Embedding Table: If we further know the entity mentions and their entity types2 in the sentence, we can also capture this information for each token by looking up the entity type embedding table (initialized randomly) using the entity type associated with each token. We employ the BIO annotation scheme to assign entity type labels to each token in the trigger candidate 2For convenience, when mentioning entiti</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural Language Processing (Almost) from Scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research</journal>
<pages>12--24932537</pages>
<contexts>
<context position="4408" citStr="Collobert et al., 2011" startWordPosition="666" endWordPosition="669">ively in various tasks: semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), semantic matching (Hu et al., 2014), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 365–371, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Convolutional Neural Network for Event Detection. 2014), name tagging and semantic role labeling (Collobert et al., 2011), relation classification and extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). However, to the best of our knowledge, this is the first work on event detection via CNNs so far. First, we evaluate CNNs for ED in the general setting and show that CNNs, though not requiring complicated feature engineering, can still outperform the state-of-the-art feature-based methods extensively relying on the other supervised modules and manual resources for features. Second, we investigate CNNs in a domain adaptation (DA) setting for ED. We demonstrate that CNNs significantly outperform the traditio</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa. 2011. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research 12:24932537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly Easy Domain Adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly Easy Domain Adaptation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prashant Gupta</author>
<author>Heng Ji</author>
</authors>
<title>Predicting Unknown Time Arguments Based on Cross-Event Propagation.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="1797" citStr="Gupta and Ji, 2009" startWordPosition="272" endWordPosition="275">er was killed in New Jersey today”, an event detection system should be able to recognize the word “killed” as a trigger for the event “Die”. This task is quite challenging, as the same event might appear in the form of various trigger expressions and an expression might represent different events in different contexts. ED is a crucial component in the overall task of event extraction, which also involves event argument discovery. Recent systems for event extraction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan 1https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5.4.3.pdf Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborate</context>
</contexts>
<marker>Gupta, Ji, 2009</marker>
<rawString>Prashant Gupta and Heng Ji. 2009. Predicting Unknown Time Arguments Based on Cross-Event Propagation. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2012</date>
<location>CoRR, abs/1207.0580.</location>
<marker>Hinton, Srivastava, 2012</marker>
<rawString>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors. CoRR, abs/1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Hong</author>
<author>Jianfeng Zhang</author>
<author>Bin Ma</author>
<author>Jian-Min Yao</author>
<author>Guodong Zhou</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Using Cross-entity Inference to Improve Event Extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2739" citStr="Hong et al., 2011" startWordPosition="409" endWordPosition="412"> joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achieved the top performance (Hong et al., 2011; Li et al., 2013b), it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010)), probably propagated to the final event detector. This paper pre</context>
<context position="11003" citStr="Hong et al., 2011" startWordPosition="1786" endWordPosition="1789">7.0 15.0 15.9 +Position 75.6 66.4 70.7 Table 1: Performance on the Development Set. For the experiments below, we examine the CNNs in two scenarios: excluding the entity type embeddings (CNN1) and including the entity type embeddings (CNN2). We always use position embeddings in these two scenarios. 3.2 Performance Comparison The state-of-the-art systems for event detection on the ACE 2005 dataset have followed the traditional feature-based approach with rich hand-designed feature sets, and statistical classifiers such as MaxEnt and perceptron for structured prediction in a joint architecture (Hong et al., 2011; Li et al., 2013b). In this section, we compare the proposed CNNs with these state-of-the-art systems on the blind test set. Table 2 presents the overall performance of the systems with gold-standard entity mention and type information4. As we can see from the table, considering the systems that only use sentence level information, CNN1 significantly outperforms the MaxEnt classifier as well as the joint beam search with local features from Li et al. (2013b) (an improvement of 1.6% in F1 score), and performs comparably 4Entity mentions and types are used to introduce more features into the sy</context>
</contexts>
<marker>Hong, Zhang, Ma, Yao, Zhou, Zhu, 2011</marker>
<rawString>Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao, Guodong Zhou, and Qiaoming Zhu. 2011. Using Cross-entity Inference to Improve Event Extraction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional Neural Network Architectures for Matching Natural Language Sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="3925" citStr="Hu et al., 2014" startWordPosition="601" endWordPosition="604">event detector. This paper presents a convolutional neural network (LeCun et al., 1988; Kalchbrenner et al., 2014) for the ED task that automatically learns features from sentences, and minimizes the dependence on supervised toolkits and resources for features, thus alleviating the error propagation and improving the performance for this task. Due to the emerging interest of the NLP community in deep learning recently, CNNs have been studied extensively and applied effectively in various tasks: semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), semantic matching (Hu et al., 2014), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 365–371, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Convolutional Neural Network for Event Detection. 2014), name tagging and semantic role labeling (Collobert et al., 2011), relation classification and extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). However, to the best of our </context>
</contexts>
<marker>Hu, Lu, Li, Chen, 2014</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, Qingcai Chen. 2014. Convolutional Neural Network Architectures for Matching Natural Language Sentences. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Modeling Textual Cohesion for Event Extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="2098" citStr="Huang and Riloff, 2012" startWordPosition="305" endWordPosition="308">s in different contexts. ED is a crucial component in the overall task of event extraction, which also involves event argument discovery. Recent systems for event extraction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan 1https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5.4.3.pdf Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achiev</context>
</contexts>
<marker>Huang, Riloff, 2012</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2012. Modeling Textual Cohesion for Event Extraction. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Refining Event Extraction through Cross-Document Inference.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1777" citStr="Ji and Grishman, 2008" startWordPosition="268" endWordPosition="271">entence “A police officer was killed in New Jersey today”, an event detection system should be able to recognize the word “killed” as a trigger for the event “Die”. This task is quite challenging, as the same event might appear in the form of various trigger expressions and an expression might represent different events in different contexts. ED is a crucial component in the overall task of event extraction, which also involves event argument discovery. Recent systems for event extraction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan 1https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5.4.3.pdf Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the </context>
<context position="9664" citStr="Ji and Grishman, 2008" startWordPosition="1568" endWordPosition="1571">i-batch size = 50, the hyperparameter for the l2 norms = 3. Finally, we employ the pre-trained word embeddings word2vec with 300 dimensions from Mikolov et al. (2013) for initialization. We evaluate the presented CNN over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles 3These values are chosen for their best performance on the development data. (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaning 529 documents (14,849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a 34-class classification problem. In order to evaluate the effectiveness of the position embeddings and the entity type embeddings, Table 1 reports the performance of the proposed CNN on the development set when these embeddings are either included or excluded from the systems. With the large margins of performance, it is very clear from the table that the position embeddings are crucial while the entity embeddings are also very usef</context>
<context position="13890" citStr="Ji and Grishman (2008)" startWordPosition="2260" endWordPosition="2263">age of the name tagger and the information extraction system to identify entity mentions and types. 3.3 Domain Adaptation Experiment In this section, we aim to further compare the proposed CNNs with the feature-based systems under the domain adaptation setting for event detection. The ultimate goal of domain adaptation research is to develop techniques taking training 5External features are the features generated from the supervised NLP modules and manual resources such as parsers, name tagger, entity mention extractors (either automatic or manual), gazetteers etc. Methods F Sentence level in Ji and Grishman (2008) 59.7 MaxEnt with local features in Li et al. (2013b) 64.7 Joint beam search with local features in Li et 63.7 al. (2013b) Joint beam search with local and global 65.6 features in Li et al. (2013b) CNN1: CNN without any external features 67.6 Table 3: Performance with Predicted Entity Mentions and Types. data in some source domain and learning models that can work well on target domains. The target domains are supposed to be so dissimilar from the source domain that the learning techniques would suffer from a significant performance loss when trained on the source domain and applied to the tar</context>
<context position="15723" citStr="Ji and Grishman, 2008" startWordPosition="2562" endWordPosition="2565">act features: These pre-processing toolkits are also known to degrade when shifted to target domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010), introducing noisy features into the systems for higher-level tasks in the target domains and eventually impairing the performance of these higherlevel systems on the target domains. For ED, we postulate that CNNs are more useful than the feature-based approach for DA for two reasons. First, rather than relying on the symbolic and concrete forms (i.e words, types etc) to construct features as the traditional feature-based systems (Ji and Grishman, 2008; Li et al., 2013b) do, CNNs automatically induce their features from word embeddings, the general distributed representation of words that is shared across domains. This helps CNNs mitigate the lexical sparsity, learn more general and effective feature representation for trigger candidates, and thus bridge the gap between domains. Second, as CNNs minimize the reliance on the supervised pre-processing toolkits for features, they can alleviate the error 368 System In-domain(bn+nw) bc cts wl P R F P R F P R F P R F MaxEnt 74.5 59.4 66.0 70.1 54.5 61.3 66.4 49.9 56.9 59.4 34.9 43.9 Joint beam sea</context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>Heng Ji and Ralph Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A Convolutional Neural Network for Modeling Sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3423" citStr="Kalchbrenner et al., 2014" startWordPosition="520" endWordPosition="523">(i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010)), probably propagated to the final event detector. This paper presents a convolutional neural network (LeCun et al., 1988; Kalchbrenner et al., 2014) for the ED task that automatically learns features from sentences, and minimizes the dependence on supervised toolkits and resources for features, thus alleviating the error propagation and improving the performance for this task. Due to the emerging interest of the NLP community in deep learning recently, CNNs have been studied extensively and applied effectively in various tasks: semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), semantic matching (Hu et al., 2014), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 365 Proceedings of the 53r</context>
<context position="7753" citStr="Kalchbrenner et al., 2014" startWordPosition="1238" endWordPosition="1241">in this paper, we always include ACE timex and values. 366 using the heads of the entity mentions. For each token xi, the vectors obtained from the three look-ups above are concatenated into a single vector xi to represent the token. As a result, the original event trigger x is transformed into a matrix x = [x−w, x−w+1, ... , x0,... , xw−1, xw] of size mt x (2w + 1) (mt is the dimensionality of the concatenated vectors of the tokens). The matrix representation x is then passed through a convolution layer, a max pooling layer and a softmax at the end to perform classification (like (Kim, 2014; Kalchbrenner et al., 2014)). In the convolution layer, we have a set of feature maps (filters) 1f1, f2,. .. , fn} for the convolution operation. Each feature map fi corresponds to some window size k and can be essentially seen as a weight matrix of size mt x k. Figure 1 illustrates the proposed CNN. The gradients are computed using backpropagation; regularization is implemented by a dropout (Kim, 2014; Hinton et al., 2012), and training is done via stochastic gradient descent with shuffled mini-batches and the AdaDelta update rule (Zeiler, 2012; Kim, 2014). During the training, we also optimize the weights of the three</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette and Phil Blunsom. 2014. A Convolutional Neural Network for Modeling Sentences. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional Neural Networks for Sentence Classification.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7725" citStr="Kim, 2014" startWordPosition="1236" endWordPosition="1237">g entities in this paper, we always include ACE timex and values. 366 using the heads of the entity mentions. For each token xi, the vectors obtained from the three look-ups above are concatenated into a single vector xi to represent the token. As a result, the original event trigger x is transformed into a matrix x = [x−w, x−w+1, ... , x0,... , xw−1, xw] of size mt x (2w + 1) (mt is the dimensionality of the concatenated vectors of the tokens). The matrix representation x is then passed through a convolution layer, a max pooling layer and a softmax at the end to perform classification (like (Kim, 2014; Kalchbrenner et al., 2014)). In the convolution layer, we have a set of feature maps (filters) 1f1, f2,. .. , fn} for the convolution operation. Each feature map fi corresponds to some window size k and can be essentially seen as a weight matrix of size mt x k. Figure 1 illustrates the proposed CNN. The gradients are computed using backpropagation; regularization is implemented by a dropout (Kim, 2014; Hinton et al., 2012), and training is done via stochastic gradient descent with shuffled mini-batches and the AdaDelta update rule (Zeiler, 2012; Kim, 2014). During the training, we also optim</context>
<context position="9003" citStr="Kim (2014)" startWordPosition="1459" endWordPosition="1460">ach an effective state (Kim, 2014). 3 Experiments 3.1 Dataset, Hyperparameters and Resources As the benefit of multiple window sizes in the convolution layer has been demonstrated in the previous work on sentence modeling (Kalchbrenner et al., 2014; Kim, 2014), in the experiments below, we use window sizes in the set 12, 3, 4, 51 to generate feature maps. We utilize 150 feature maps for each window size in this set. The window size for triggers is set to 31 while the dimensionality of the position embeddings and entity type embeddings is 503.We inherit the values for the other parameters from Kim (2014), i.e, the dropout rate p = 0.5, the mini-batch size = 50, the hyperparameter for the l2 norms = 3. Finally, we employ the pre-trained word embeddings word2vec with 300 dimensions from Mikolov et al. (2013) for initialization. We evaluate the presented CNN over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles 3These values are chosen for their best performance on the development data. (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaning 529 documents (14,849 sentences) a</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann LeCun</author>
<author>L´eon Bottou</author>
<author>Yoshua Bengio</author>
<author>Patrick Haffner</author>
</authors>
<title>Gradient-based Learning Applied to Document Recognition.</title>
<date>1988</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<pages>86--11</pages>
<contexts>
<context position="3395" citStr="LeCun et al., 1988" startWordPosition="516" endWordPosition="519">t least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010)), probably propagated to the final event detector. This paper presents a convolutional neural network (LeCun et al., 1988; Kalchbrenner et al., 2014) for the ED task that automatically learns features from sentences, and minimizes the dependence on supervised toolkits and resources for features, thus alleviating the error propagation and improving the performance for this task. Due to the emerging interest of the NLP community in deep learning recently, CNNs have been studied extensively and applied effectively in various tasks: semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), semantic matching (Hu et al., 2014), sentence modeling and classification (Kalchbrenner et al., 2014; Kim</context>
</contexts>
<marker>LeCun, Bottou, Bengio, Haffner, 1988</marker>
<rawString>Yann LeCun, L´eon Bottou, Yoshua Bengio and Patrick Haffner. 1988. Gradient-based Learning Applied to Document Recognition. In Proceedings of the IEEE, 86(11):22782324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peifeng Li</author>
<author>Qiaoming Zhu</author>
<author>Guodong Zhou</author>
</authors>
<title>Argument Inference from Relevant Event Mentions in Chinese Argument Extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2115" citStr="Li et al., 2013" startWordPosition="309" endWordPosition="312"> ED is a crucial component in the overall task of event extraction, which also involves event argument discovery. Recent systems for event extraction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan 1https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5.4.3.pdf Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achieved the top perfor</context>
<context position="5665" citStr="Li et al., 2013" startWordPosition="874" endWordPosition="877">eneralization performance across domains due to: (i) their capacity to mitigate the error propagation from the preprocessing modules for features, and (ii) the use of word embeddings to induce a more general representation for trigger candidates. We believe that this is also the first research on domain adaptation using CNNs. 2 Model We formalize the event detection problem as a multi-class classification problem. Given a sentence, for every token in that sentence, we want to predict if the current token is an event trigger: i.e, does it express some event in the pre-defined event set or not (Li et al., 2013b)? The current token along with its context in the sentence constitute an event trigger candidate or an example in multiclass classification terms. In order to prepare for the CNNs, we limit the context to a fixed window size by trimming longer sentences and padding shorter sentences with a special token when necessary. Let 2w + 1 be the fixed window size, and x = [x−w, x−w+1, ... , x0, ... , xw−1, xw] be some trigger candidate where the current token is positioned in the middle of the window (token x0). Before entering the CNNs, each token xi is transformed into a real-valued vector by looki</context>
<context position="9706" citStr="Li et al., 2013" startWordPosition="1577" endWordPosition="1580"> norms = 3. Finally, we employ the pre-trained word embeddings word2vec with 300 dimensions from Mikolov et al. (2013) for initialization. We evaluate the presented CNN over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles 3These values are chosen for their best performance on the development data. (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaning 529 documents (14,849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a 34-class classification problem. In order to evaluate the effectiveness of the position embeddings and the entity type embeddings, Table 1 reports the performance of the proposed CNN on the development set when these embeddings are either included or excluded from the systems. With the large margins of performance, it is very clear from the table that the position embeddings are crucial while the entity embeddings are also very useful for CNNs on ED. Systems P R F -Entity T</context>
<context position="11020" citStr="Li et al., 2013" startWordPosition="1790" endWordPosition="1793">tion 75.6 66.4 70.7 Table 1: Performance on the Development Set. For the experiments below, we examine the CNNs in two scenarios: excluding the entity type embeddings (CNN1) and including the entity type embeddings (CNN2). We always use position embeddings in these two scenarios. 3.2 Performance Comparison The state-of-the-art systems for event detection on the ACE 2005 dataset have followed the traditional feature-based approach with rich hand-designed feature sets, and statistical classifiers such as MaxEnt and perceptron for structured prediction in a joint architecture (Hong et al., 2011; Li et al., 2013b). In this section, we compare the proposed CNNs with these state-of-the-art systems on the blind test set. Table 2 presents the overall performance of the systems with gold-standard entity mention and type information4. As we can see from the table, considering the systems that only use sentence level information, CNN1 significantly outperforms the MaxEnt classifier as well as the joint beam search with local features from Li et al. (2013b) (an improvement of 1.6% in F1 score), and performs comparably 4Entity mentions and types are used to introduce more features into the systems. 367 Method</context>
<context position="12242" citStr="Li et al., 2013" startWordPosition="1999" endWordPosition="2002"> Sentence-level in Hong et al 67.6 53.5 59.7 (2011) MaxEnt with local features in 74.5 59.1 65.9 Li et al. (2013b) Joint beam search with local 73.7 59.3 65.7 features in Li et al. (2013b) Joint beam search with local 73.7 62.3 67.5 and global features in Li et al. (2013b) Cross-entity in Hong et al. 72.9 64.3 68.3 (2011) † CNN1: CNN without any 71.9 63.8 67.6 external features CNN2: CNN augmented with 71.8 66.4 69.0 entity types Table 2: Performance with Gold-Standard Entity Mentions and Types. † beyond sentence level. with the joint beam search approach using both local and global features (Li et al., 2013b). This is remarkable since CNN1 does not require any external features5, in contrast to the other featurebased systems that extensively rely on such external features to perform well. More interestingly, when the entity type information is incorporated into CNN1, we obtain CNN2 that still only needs sentence level information but achieves the stateof-the-art performance for this task (an improvement of 1.5% over the best system with only sentence level information (Li et al., 2013b)). Except for CNN1, all the systems reported in Table 2 employ the gold-standard (perfect) entities mentions an</context>
<context position="13941" citStr="Li et al. (2013" startWordPosition="2270" endWordPosition="2273">em to identify entity mentions and types. 3.3 Domain Adaptation Experiment In this section, we aim to further compare the proposed CNNs with the feature-based systems under the domain adaptation setting for event detection. The ultimate goal of domain adaptation research is to develop techniques taking training 5External features are the features generated from the supervised NLP modules and manual resources such as parsers, name tagger, entity mention extractors (either automatic or manual), gazetteers etc. Methods F Sentence level in Ji and Grishman (2008) 59.7 MaxEnt with local features in Li et al. (2013b) 64.7 Joint beam search with local features in Li et 63.7 al. (2013b) Joint beam search with local and global 65.6 features in Li et al. (2013b) CNN1: CNN without any external features 67.6 Table 3: Performance with Predicted Entity Mentions and Types. data in some source domain and learning models that can work well on target domains. The target domains are supposed to be so dissimilar from the source domain that the learning techniques would suffer from a significant performance loss when trained on the source domain and applied to the target domains. To make it clear, we address the unsup</context>
<context position="15740" citStr="Li et al., 2013" startWordPosition="2566" endWordPosition="2569">-processing toolkits are also known to degrade when shifted to target domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010), introducing noisy features into the systems for higher-level tasks in the target domains and eventually impairing the performance of these higherlevel systems on the target domains. For ED, we postulate that CNNs are more useful than the feature-based approach for DA for two reasons. First, rather than relying on the symbolic and concrete forms (i.e words, types etc) to construct features as the traditional feature-based systems (Ji and Grishman, 2008; Li et al., 2013b) do, CNNs automatically induce their features from word embeddings, the general distributed representation of words that is shared across domains. This helps CNNs mitigate the lexical sparsity, learn more general and effective feature representation for trigger candidates, and thus bridge the gap between domains. Second, as CNNs minimize the reliance on the supervised pre-processing toolkits for features, they can alleviate the error 368 System In-domain(bn+nw) bc cts wl P R F P R F P R F P R F MaxEnt 74.5 59.4 66.0 70.1 54.5 61.3 66.4 49.9 56.9 59.4 34.9 43.9 Joint beam search in Li et al. </context>
<context position="17843" citStr="Li et al. (2013" startWordPosition="2917" endWordPosition="2920">ng the common practice of domain adaptation research on this dataset (Plank and Moschitti, 2013; Nguyen and Grishman, 2014), we use news (the union of bn and nw) as the source domain and bc, cts, wl as three different target domains. We take half of bc as the development set and use the remaining data for testing. We note that the distribution of event subtypes and the vocabularies of the source and target domains are quite different (Plank and Moschitti, 2013). 3.3.2 Domain Adaptation Results Table 4 presents the performance of five systems: the MaxEnt classifier with the local features from Li et al. (2013b) (called MaxEnt); the state-of-theart joint beam search systems with: (i) only local features (called Joint+Local); and (ii) both local and global features (called Joint+Local+Global) in Li et al. (2013b) (the baseline systems); CNN1 and CNN2 via 5-fold cross validation. For each system, we train a model on the training set of the source domain and report the performance of this model on the test set of the source domain (indomain performance) as well as the performance of the model on the three target domains bc, cts and wl (out-of-domain performance)6. The main conclusions from the table i</context>
</contexts>
<marker>Li, Zhu, Zhou, 2013</marker>
<rawString>Peifeng Li, Qiaoming Zhu, and Guodong Zhou. 2013a. Argument Inference from Relevant Event Mentions in Chinese Argument Extraction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Liang Huang</author>
</authors>
<title>Joint Event Extraction via Structured Prediction with Global Features.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2115" citStr="Li et al., 2013" startWordPosition="309" endWordPosition="312"> ED is a crucial component in the overall task of event extraction, which also involves event argument discovery. Recent systems for event extraction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan 1https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5.4.3.pdf Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achieved the top perfor</context>
<context position="5665" citStr="Li et al., 2013" startWordPosition="874" endWordPosition="877">eneralization performance across domains due to: (i) their capacity to mitigate the error propagation from the preprocessing modules for features, and (ii) the use of word embeddings to induce a more general representation for trigger candidates. We believe that this is also the first research on domain adaptation using CNNs. 2 Model We formalize the event detection problem as a multi-class classification problem. Given a sentence, for every token in that sentence, we want to predict if the current token is an event trigger: i.e, does it express some event in the pre-defined event set or not (Li et al., 2013b)? The current token along with its context in the sentence constitute an event trigger candidate or an example in multiclass classification terms. In order to prepare for the CNNs, we limit the context to a fixed window size by trimming longer sentences and padding shorter sentences with a special token when necessary. Let 2w + 1 be the fixed window size, and x = [x−w, x−w+1, ... , x0, ... , xw−1, xw] be some trigger candidate where the current token is positioned in the middle of the window (token x0). Before entering the CNNs, each token xi is transformed into a real-valued vector by looki</context>
<context position="9706" citStr="Li et al., 2013" startWordPosition="1577" endWordPosition="1580"> norms = 3. Finally, we employ the pre-trained word embeddings word2vec with 300 dimensions from Mikolov et al. (2013) for initialization. We evaluate the presented CNN over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles 3These values are chosen for their best performance on the development data. (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaning 529 documents (14,849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a 34-class classification problem. In order to evaluate the effectiveness of the position embeddings and the entity type embeddings, Table 1 reports the performance of the proposed CNN on the development set when these embeddings are either included or excluded from the systems. With the large margins of performance, it is very clear from the table that the position embeddings are crucial while the entity embeddings are also very useful for CNNs on ED. Systems P R F -Entity T</context>
<context position="11020" citStr="Li et al., 2013" startWordPosition="1790" endWordPosition="1793">tion 75.6 66.4 70.7 Table 1: Performance on the Development Set. For the experiments below, we examine the CNNs in two scenarios: excluding the entity type embeddings (CNN1) and including the entity type embeddings (CNN2). We always use position embeddings in these two scenarios. 3.2 Performance Comparison The state-of-the-art systems for event detection on the ACE 2005 dataset have followed the traditional feature-based approach with rich hand-designed feature sets, and statistical classifiers such as MaxEnt and perceptron for structured prediction in a joint architecture (Hong et al., 2011; Li et al., 2013b). In this section, we compare the proposed CNNs with these state-of-the-art systems on the blind test set. Table 2 presents the overall performance of the systems with gold-standard entity mention and type information4. As we can see from the table, considering the systems that only use sentence level information, CNN1 significantly outperforms the MaxEnt classifier as well as the joint beam search with local features from Li et al. (2013b) (an improvement of 1.6% in F1 score), and performs comparably 4Entity mentions and types are used to introduce more features into the systems. 367 Method</context>
<context position="12242" citStr="Li et al., 2013" startWordPosition="1999" endWordPosition="2002"> Sentence-level in Hong et al 67.6 53.5 59.7 (2011) MaxEnt with local features in 74.5 59.1 65.9 Li et al. (2013b) Joint beam search with local 73.7 59.3 65.7 features in Li et al. (2013b) Joint beam search with local 73.7 62.3 67.5 and global features in Li et al. (2013b) Cross-entity in Hong et al. 72.9 64.3 68.3 (2011) † CNN1: CNN without any 71.9 63.8 67.6 external features CNN2: CNN augmented with 71.8 66.4 69.0 entity types Table 2: Performance with Gold-Standard Entity Mentions and Types. † beyond sentence level. with the joint beam search approach using both local and global features (Li et al., 2013b). This is remarkable since CNN1 does not require any external features5, in contrast to the other featurebased systems that extensively rely on such external features to perform well. More interestingly, when the entity type information is incorporated into CNN1, we obtain CNN2 that still only needs sentence level information but achieves the stateof-the-art performance for this task (an improvement of 1.5% over the best system with only sentence level information (Li et al., 2013b)). Except for CNN1, all the systems reported in Table 2 employ the gold-standard (perfect) entities mentions an</context>
<context position="13941" citStr="Li et al. (2013" startWordPosition="2270" endWordPosition="2273">em to identify entity mentions and types. 3.3 Domain Adaptation Experiment In this section, we aim to further compare the proposed CNNs with the feature-based systems under the domain adaptation setting for event detection. The ultimate goal of domain adaptation research is to develop techniques taking training 5External features are the features generated from the supervised NLP modules and manual resources such as parsers, name tagger, entity mention extractors (either automatic or manual), gazetteers etc. Methods F Sentence level in Ji and Grishman (2008) 59.7 MaxEnt with local features in Li et al. (2013b) 64.7 Joint beam search with local features in Li et 63.7 al. (2013b) Joint beam search with local and global 65.6 features in Li et al. (2013b) CNN1: CNN without any external features 67.6 Table 3: Performance with Predicted Entity Mentions and Types. data in some source domain and learning models that can work well on target domains. The target domains are supposed to be so dissimilar from the source domain that the learning techniques would suffer from a significant performance loss when trained on the source domain and applied to the target domains. To make it clear, we address the unsup</context>
<context position="15740" citStr="Li et al., 2013" startWordPosition="2566" endWordPosition="2569">-processing toolkits are also known to degrade when shifted to target domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010), introducing noisy features into the systems for higher-level tasks in the target domains and eventually impairing the performance of these higherlevel systems on the target domains. For ED, we postulate that CNNs are more useful than the feature-based approach for DA for two reasons. First, rather than relying on the symbolic and concrete forms (i.e words, types etc) to construct features as the traditional feature-based systems (Ji and Grishman, 2008; Li et al., 2013b) do, CNNs automatically induce their features from word embeddings, the general distributed representation of words that is shared across domains. This helps CNNs mitigate the lexical sparsity, learn more general and effective feature representation for trigger candidates, and thus bridge the gap between domains. Second, as CNNs minimize the reliance on the supervised pre-processing toolkits for features, they can alleviate the error 368 System In-domain(bn+nw) bc cts wl P R F P R F P R F P R F MaxEnt 74.5 59.4 66.0 70.1 54.5 61.3 66.4 49.9 56.9 59.4 34.9 43.9 Joint beam search in Li et al. </context>
<context position="17843" citStr="Li et al. (2013" startWordPosition="2917" endWordPosition="2920">ng the common practice of domain adaptation research on this dataset (Plank and Moschitti, 2013; Nguyen and Grishman, 2014), we use news (the union of bn and nw) as the source domain and bc, cts, wl as three different target domains. We take half of bc as the development set and use the remaining data for testing. We note that the distribution of event subtypes and the vocabularies of the source and target domains are quite different (Plank and Moschitti, 2013). 3.3.2 Domain Adaptation Results Table 4 presents the performance of five systems: the MaxEnt classifier with the local features from Li et al. (2013b) (called MaxEnt); the state-of-theart joint beam search systems with: (i) only local features (called Joint+Local); and (ii) both local and global features (called Joint+Local+Global) in Li et al. (2013b) (the baseline systems); CNN1 and CNN2 via 5-fold cross validation. For each system, we train a model on the training set of the source domain and report the performance of this model on the test set of the source domain (indomain performance) as well as the performance of the model on the three target domains bc, cts and wl (out-of-domain performance)6. The main conclusions from the table i</context>
</contexts>
<marker>Li, Ji, Huang, 2013</marker>
<rawString>Qi Li, Heng Ji, and Liang Huang. 2013b. Joint Event Extraction via Structured Prediction with Global Features. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Liao</author>
<author>Ralph Grishman</author>
</authors>
<title>Using Document Level Cross-event Inference to Improve Event Extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="9689" citStr="Liao and Grishman, 2010" startWordPosition="1572" endWordPosition="1576">hyperparameter for the l2 norms = 3. Finally, we employ the pre-trained word embeddings word2vec with 300 dimensions from Mikolov et al. (2013) for initialization. We evaluate the presented CNN over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles 3These values are chosen for their best performance on the development data. (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaning 529 documents (14,849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a 34-class classification problem. In order to evaluate the effectiveness of the position embeddings and the entity type embeddings, Table 1 reports the performance of the proposed CNN on the development set when these embeddings are either included or excluded from the systems. With the large margins of performance, it is very clear from the table that the position embeddings are crucial while the entity embeddings are also very useful for CNNs on ED. System</context>
</contexts>
<marker>Liao, Grishman, 2010</marker>
<rawString>Shasha Liao and Ralph Grishman. 2010. Using Document Level Cross-event Inference to Improve Event Extraction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Liao</author>
<author>Ralph Grishman</author>
</authors>
<title>Acquiring Topic Features to Improve Event Extraction: in Preselected and Balanced Collections.</title>
<date>2011</date>
<booktitle>In Proceedings RANLP.</booktitle>
<contexts>
<context position="2051" citStr="Liao and Grishman, 2011" startWordPosition="296" endWordPosition="299">nd an expression might represent different events in different contexts. ED is a crucial component in the overall task of event extraction, which also involves event argument discovery. Recent systems for event extraction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan 1https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5.4.3.pdf Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical</context>
</contexts>
<marker>Liao, Grishman, 2011</marker>
<rawString>Shasha Liao and Ralph Grishman. 2011. Acquiring Topic Features to Improve Event Extraction: in Preselected and Balanced Collections. In Proceedings RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Automatic Domain Adaptation for Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="3273" citStr="McClosky et al., 2010" startWordPosition="497" endWordPosition="500"> classifiers. Although this approach has achieved the top performance (Hong et al., 2011; Li et al., 2013b), it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010)), probably propagated to the final event detector. This paper presents a convolutional neural network (LeCun et al., 1988; Kalchbrenner et al., 2014) for the ED task that automatically learns features from sentences, and minimizes the dependence on supervised toolkits and resources for features, thus alleviating the error propagation and improving the performance for this task. Due to the emerging interest of the NLP community in deep learning recently, CNNs have been studied extensively and applied effectively in various tasks: semantic parsing (Yih et al., 2014), search query retrieval (She</context>
<context position="15266" citStr="McClosky et al., 2010" startWordPosition="2487" endWordPosition="2490">lank and Moschitti, 2013). The fundamental reason for the performance loss of the featurebased systems on the target domains is twofold: (i) The behavioral changes of features across domains: As domains differ, some features might be informative in the source domain but become less relevant in the target domains and vice versa. (ii) The propagated errors of the pre-processing toolkits for lower-level tasks (POS tagging, name tagging, parsing etc) to extract features: These pre-processing toolkits are also known to degrade when shifted to target domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010), introducing noisy features into the systems for higher-level tasks in the target domains and eventually impairing the performance of these higherlevel systems on the target domains. For ED, we postulate that CNNs are more useful than the feature-based approach for DA for two reasons. First, rather than relying on the symbolic and concrete forms (i.e words, types etc) to construct features as the traditional feature-based systems (Ji and Grishman, 2008; Li et al., 2013b) do, CNNs automatically induce their features from word embeddings, the general distributed representation of words that is </context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2010</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic Domain Adaptation for Parsing. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Chris Manning</author>
</authors>
<title>Event Extraction as Dependency Parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="2074" citStr="McClosky et al., 2011" startWordPosition="300" endWordPosition="304">present different events in different contexts. ED is a crucial component in the overall task of event extraction, which also involves event argument discovery. Recent systems for event extraction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan 1https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5.4.3.pdf Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although </context>
</contexts>
<marker>McClosky, Surdeanu, Manning, 2011</marker>
<rawString>David McClosky, Mihai Surdeanu, and Chris Manning. 2011. Event Extraction as Dependency Parsing. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<date>2013</date>
<booktitle>Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS.</booktitle>
<contexts>
<context position="9209" citStr="Mikolov et al. (2013)" startWordPosition="1495" endWordPosition="1498"> work on sentence modeling (Kalchbrenner et al., 2014; Kim, 2014), in the experiments below, we use window sizes in the set 12, 3, 4, 51 to generate feature maps. We utilize 150 feature maps for each window size in this set. The window size for triggers is set to 31 while the dimensionality of the position embeddings and entity type embeddings is 503.We inherit the values for the other parameters from Kim (2014), i.e, the dropout rate p = 0.5, the mini-batch size = 50, the hyperparameter for the l2 norms = 3. Finally, we employ the pre-trained word embeddings word2vec with 300 dimensions from Mikolov et al. (2013) for initialization. We evaluate the presented CNN over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles 3These values are chosen for their best performance on the development data. (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaning 529 documents (14,849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tok</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thien Huu Nguyen</author>
<author>Ralph Grishman</author>
</authors>
<title>Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="17351" citStr="Nguyen and Grishman, 2014" startWordPosition="2830" endWordPosition="2833">marked with †designate CNN models that significantly outperform (p &lt; 0.05) all the reported feature-based methods on the specified domain. propagation and be more robust to domain shifts. 3.3.1 Dataset We also do the experiments in this part over the ACE 2005 dataset but focus more on the difference between domains. The ACE 2005 corpus comes with 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). Following the common practice of domain adaptation research on this dataset (Plank and Moschitti, 2013; Nguyen and Grishman, 2014), we use news (the union of bn and nw) as the source domain and bc, cts, wl as three different target domains. We take half of bc as the development set and use the remaining data for testing. We note that the distribution of event subtypes and the vocabularies of the source and target domains are quite different (Plank and Moschitti, 2013). 3.3.2 Domain Adaptation Results Table 4 presents the performance of five systems: the MaxEnt classifier with the local features from Li et al. (2013b) (called MaxEnt); the state-of-theart joint beam search systems with: (i) only local features (called Join</context>
</contexts>
<marker>Nguyen, Grishman, 2014</marker>
<rawString>Thien Huu Nguyen and Ralph Grishman. 2014. Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thien Huu Nguyen</author>
<author>Ralph Grishman</author>
</authors>
<title>Relation Extraction: Perspective from Convolutional Neural Networks.</title>
<date>2015</date>
<booktitle>In Proceedings of the NAACL Workshop on Vector Space Modeling for NLP (VSM).</booktitle>
<contexts>
<context position="4495" citStr="Nguyen and Grishman, 2015" startWordPosition="678" endWordPosition="682">(Shen et al., 2014), semantic matching (Hu et al., 2014), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 365–371, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Convolutional Neural Network for Event Detection. 2014), name tagging and semantic role labeling (Collobert et al., 2011), relation classification and extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). However, to the best of our knowledge, this is the first work on event detection via CNNs so far. First, we evaluate CNNs for ED in the general setting and show that CNNs, though not requiring complicated feature engineering, can still outperform the state-of-the-art feature-based methods extensively relying on the other supervised modules and manual resources for features. Second, we investigate CNNs in a domain adaptation (DA) setting for ED. We demonstrate that CNNs significantly outperform the traditional featurebased methods with respect to generalization performance across domains due </context>
</contexts>
<marker>Nguyen, Grishman, 2015</marker>
<rawString>Thien Huu Nguyen and Ralph Grishman. 2015. Relation Extraction: Perspective from Convolutional Neural Networks. In Proceedings of the NAACL Workshop on Vector Space Modeling for NLP (VSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Rilof</author>
</authors>
<title>A Unified Model of Phrasal and Sentential Evidence for Information Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Patwardhan, Rilof, 2009</marker>
<rawString>Siddharth Patwardhan and Ellen Rilof. 2009. A Unified Model of Phrasal and Sentential Evidence for Information Extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Embedding Semantic Similarity in Tree Kernels for Domain Adaptation ofRelation Extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="14669" citStr="Plank and Moschitti, 2013" startWordPosition="2393" endWordPosition="2396">nd global 65.6 features in Li et al. (2013b) CNN1: CNN without any external features 67.6 Table 3: Performance with Predicted Entity Mentions and Types. data in some source domain and learning models that can work well on target domains. The target domains are supposed to be so dissimilar from the source domain that the learning techniques would suffer from a significant performance loss when trained on the source domain and applied to the target domains. To make it clear, we address the unsupervised DA problem in this section, i.e no training data in the target domains (Blitzer et al., 2006; Plank and Moschitti, 2013). The fundamental reason for the performance loss of the featurebased systems on the target domains is twofold: (i) The behavioral changes of features across domains: As domains differ, some features might be informative in the source domain but become less relevant in the target domains and vice versa. (ii) The propagated errors of the pre-processing toolkits for lower-level tasks (POS tagging, name tagging, parsing etc) to extract features: These pre-processing toolkits are also known to degrade when shifted to target domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010), i</context>
<context position="17323" citStr="Plank and Moschitti, 2013" startWordPosition="2826" endWordPosition="2829">olumns two to four). Cells marked with †designate CNN models that significantly outperform (p &lt; 0.05) all the reported feature-based methods on the specified domain. propagation and be more robust to domain shifts. 3.3.1 Dataset We also do the experiments in this part over the ACE 2005 dataset but focus more on the difference between domains. The ACE 2005 corpus comes with 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). Following the common practice of domain adaptation research on this dataset (Plank and Moschitti, 2013; Nguyen and Grishman, 2014), we use news (the union of bn and nw) as the source domain and bc, cts, wl as three different target domains. We take half of bc as the development set and use the remaining data for testing. We note that the distribution of event subtypes and the vocabularies of the source and target domains are quite different (Plank and Moschitti, 2013). 3.3.2 Domain Adaptation Results Table 4 presents the performance of five systems: the MaxEnt classifier with the local features from Li et al. (2013b) (called MaxEnt); the state-of-theart joint beam search systems with: (i) only</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Barbara Plank and Alessandro Moschitti. 2013. Embedding Semantic Similarity in Tree Kernels for Domain Adaptation ofRelation Extraction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Fast and Robust Joint Models for Biomedical Event Extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2267" citStr="Riedel and McCallum, 2011" startWordPosition="333" endWordPosition="337">raction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan 1https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5.4.3.pdf Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achieved the top performance (Hong et al., 2011; Li et al., 2013b), it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic</context>
</contexts>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011. Fast and Robust Joint Models for Biomedical Event Extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Robust Biomedical Event Extraction with Dual Decomposition and Minimal Domain Adaptation.</title>
<date>2011</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2011 Workshop.</booktitle>
<contexts>
<context position="2267" citStr="Riedel and McCallum, 2011" startWordPosition="333" endWordPosition="337">raction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan 1https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5.4.3.pdf Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achieved the top performance (Hong et al., 2011; Li et al., 2013b), it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic</context>
</contexts>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011. Robust Biomedical Event Extraction with Dual Decomposition and Minimal Domain Adaptation. In Proceedings of the BioNLP Shared Task 2011 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yelong Shen</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
<author>Li Deng</author>
<author>Gr´egoire Mesnil</author>
</authors>
<title>Learning Semantic Representations Using Convolutional Neural Networks for Web Search.</title>
<date>2014</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="3888" citStr="Shen et al., 2014" startWordPosition="594" endWordPosition="597">10)), probably propagated to the final event detector. This paper presents a convolutional neural network (LeCun et al., 1988; Kalchbrenner et al., 2014) for the ED task that automatically learns features from sentences, and minimizes the dependence on supervised toolkits and resources for features, thus alleviating the error propagation and improving the performance for this task. Due to the emerging interest of the NLP community in deep learning recently, CNNs have been studied extensively and applied effectively in various tasks: semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), semantic matching (Hu et al., 2014), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 365–371, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Convolutional Neural Network for Event Detection. 2014), name tagging and semantic role labeling (Collobert et al., 2011), relation classification and extraction (Zeng et al., 2014; Nguyen and Grishman</context>
</contexts>
<marker>Shen, He, Gao, Deng, Mesnil, 2014</marker>
<rawString>Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng and Gr´egoire Mesnil. 2014. Learning Semantic Representations Using Convolutional Neural Networks for Web Search. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word Representations: A Simple and General Method for Semi-supervised Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6546" citStr="Turian et al., 2010" startWordPosition="1025" endWordPosition="1028">adding shorter sentences with a special token when necessary. Let 2w + 1 be the fixed window size, and x = [x−w, x−w+1, ... , x0, ... , xw−1, xw] be some trigger candidate where the current token is positioned in the middle of the window (token x0). Before entering the CNNs, each token xi is transformed into a real-valued vector by looking up the following embedding tables to capture different characteristics of the token: - Word Embedding Table (initialized by some pre-trained word embeddings): to capture the hidden semantic and syntactic properties of the tokens (Collobert and Weston, 2008; Turian et al., 2010). - Position Embedding Table: to embed the relative distance i of the token xi to the current token x0. In practice, we initialize this table randomly. - Entity Type Embedding Table: If we further know the entity mentions and their entity types2 in the sentence, we can also capture this information for each token by looking up the entity type embedding table (initialized randomly) using the entity type associated with each token. We employ the BIO annotation scheme to assign entity type labels to each token in the trigger candidate 2For convenience, when mentioning entities in this paper, we a</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word Representations: A Simple and General Method for Semi-supervised Learning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Venugopal</author>
<author>Chen Chen</author>
<author>Vibhav Gogate</author>
<author>Vincent Ng</author>
</authors>
<title>Relieving the Computational Bottleneck: Joint Inference for Event Extraction with High-Dimensional Features.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2339" citStr="Venugopal et al., 2014" startWordPosition="346" endWordPosition="349">iers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan 1https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5.4.3.pdf Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achieved the top performance (Hong et al., 2011; Li et al., 2013b), it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for </context>
</contexts>
<marker>Venugopal, Chen, Gogate, Ng, 2014</marker>
<rawString>Deepak Venugopal, Chen Chen, Vibhav Gogate and Vincent Ng. 2014. Relieving the Computational Bottleneck: Joint Inference for Event Extraction with High-Dimensional Features. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Xiaodong He</author>
<author>Christopher Meek</author>
</authors>
<title>Semantic Parsing for Single-Relation Question Answering.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3844" citStr="Yih et al., 2014" startWordPosition="587" endWordPosition="590">2006; Daum´e III, 2007; McClosky et al., 2010)), probably propagated to the final event detector. This paper presents a convolutional neural network (LeCun et al., 1988; Kalchbrenner et al., 2014) for the ED task that automatically learns features from sentences, and minimizes the dependence on supervised toolkits and resources for features, thus alleviating the error propagation and improving the performance for this task. Due to the emerging interest of the NLP community in deep learning recently, CNNs have been studied extensively and applied effectively in various tasks: semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), semantic matching (Hu et al., 2014), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 365–371, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Convolutional Neural Network for Event Detection. 2014), name tagging and semantic role labeling (Collobert et al., 2011), relation classification and extrac</context>
</contexts>
<marker>Yih, He, Meek, 2014</marker>
<rawString>Wen-tau Yih, Xiaodong He, and Christopher Meek. 2014. Semantic Parsing for Single-Relation Question Answering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
</authors>
<title>ADADELTA: An Adaptive Learning Rate Method.</title>
<date>2012</date>
<location>CoRR, abs/1212.5701.</location>
<contexts>
<context position="8277" citStr="Zeiler, 2012" startWordPosition="1330" endWordPosition="1331">oftmax at the end to perform classification (like (Kim, 2014; Kalchbrenner et al., 2014)). In the convolution layer, we have a set of feature maps (filters) 1f1, f2,. .. , fn} for the convolution operation. Each feature map fi corresponds to some window size k and can be essentially seen as a weight matrix of size mt x k. Figure 1 illustrates the proposed CNN. The gradients are computed using backpropagation; regularization is implemented by a dropout (Kim, 2014; Hinton et al., 2012), and training is done via stochastic gradient descent with shuffled mini-batches and the AdaDelta update rule (Zeiler, 2012; Kim, 2014). During the training, we also optimize the weights of the three embedding tables at the same time to reach an effective state (Kim, 2014). 3 Experiments 3.1 Dataset, Hyperparameters and Resources As the benefit of multiple window sizes in the convolution layer has been demonstrated in the previous work on sentence modeling (Kalchbrenner et al., 2014; Kim, 2014), in the experiments below, we use window sizes in the set 12, 3, 4, 51 to generate feature maps. We utilize 150 feature maps for each window size in this set. The window size for triggers is set to 31 while the dimensionali</context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>Matthew D. Zeiler. 2012. ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daojian Zeng</author>
<author>Kang Liu</author>
<author>Siwei Lai</author>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
</authors>
<title>Relation Classification via Convolutional Deep Neural Network.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="4467" citStr="Zeng et al., 2014" startWordPosition="674" endWordPosition="677">ch query retrieval (Shen et al., 2014), semantic matching (Hu et al., 2014), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 365–371, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Convolutional Neural Network for Event Detection. 2014), name tagging and semantic role labeling (Collobert et al., 2011), relation classification and extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). However, to the best of our knowledge, this is the first work on event detection via CNNs so far. First, we evaluate CNNs for ED in the general setting and show that CNNs, though not requiring complicated feature engineering, can still outperform the state-of-the-art feature-based methods extensively relying on the other supervised modules and manual resources for features. Second, we investigate CNNs in a domain adaptation (DA) setting for ED. We demonstrate that CNNs significantly outperform the traditional featurebased methods with respect to generalization per</context>
</contexts>
<marker>Zeng, Liu, Lai, Zhou, Zhao, 2014</marker>
<rawString>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou and Jun Zhao. 2014. Relation Classification via Convolutional Deep Neural Network. In Proceedings of COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>