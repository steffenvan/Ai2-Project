<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001816">
<title confidence="0.986339">
Vector Space Model for Adaptation in Statistical Machine Translation
</title>
<author confidence="0.987735">
Boxing Chen, Roland Kuhn and George Foster
</author>
<affiliation confidence="0.935831">
National Research Council Canada
</affiliation>
<bodyText confidence="0.99210495">
first.last@nrc-cnrc.gc.ca
ular author’s or publication’s style; the word “do-
main” is often used to indicate a particular combi-
nation of all these factors. Unless there is a per-
fect match between the training data domain and
the (test) domain in which the SMT system will
be used, one can often get better performance by
adapting the system to the test domain.
Domain adaptation is an active topic in the nat-
ural language processing (NLP) research commu-
nity. Its application to SMT systems has recently
received considerable attention. Approaches that
have been tried for SMT model adaptation include
mixture models, transductive learning, data selec-
tion, instance weighting, and phrase sense disam-
biguation, etc.
Research on mixture models has considered
both linear and log-linear mixtures. Both were
studied in (Foster and Kuhn, 2007), which con-
cluded that the best approach was to combine sub-
models of the same type (for instance, several
different TMs or several different LMs) linearly,
while combining models of different types (for in-
stance, a mixture TM with a mixture LM) log-
linearly. (Koehn and Schroeder, 2007), instead,
opted for combining the sub-models directly in the
SMT log-linear framework.
In transductive learning, an MT system trained
on general domain data is used to translate in-
domain monolingual data. The resulting bilingual
sentence pairs are then used as additional train-
ing data (Ueffing et al., 2007; Chen et al., 2008;
Schwenk, 2008; Bertoldi and Federico, 2009).
Data selection approaches (Zhao et al., 2004;
Hildebrand et al., 2005; L¨u et al., 2007; Moore
and Lewis, 2010; Axelrod et al., 2011) search for
bilingual sentence pairs that are similar to the in-
domain “dev” data, then add them to the training
data.
Instance weighting approaches (Matsoukas et
</bodyText>
<note confidence="0.755992">
al., 2009; Foster et al., 2010; Huang and Xiang,
2010; Phillips and Brown, 2011; Sennrich, 2012)
</note>
<page confidence="0.935482">
1285
</page>
<sectionHeader confidence="0.828867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993934375">
This paper proposes a new approach to
domain adaptation in statistical machine
translation (SMT) based on a vector space
model (VSM). The general idea is first to
create a vector profile for the in-domain
development (“dev”) set. This profile
might, for instance, be a vector with a di-
mensionality equal to the number of train-
ing subcorpora; each entry in the vector re-
flects the contribution of a particular sub-
corpus to all the phrase pairs that can be
extracted from the dev set. Then, for
each phrase pair extracted from the train-
ing data, we create a vector with features
defined in the same way, and calculate its
similarity score with the vector represent-
ing the dev set. Thus, we obtain a de-
coding feature whose value represents the
phrase pair’s closeness to the dev. This is
a simple, computationally cheap form of
instance weighting for phrase pairs. Ex-
periments on large scale NIST evaluation
data show improvements over strong base-
lines: +1.8 BLEU on Arabic to English
and +1.4 BLEU on Chinese to English
over a non-adapted baseline, and signifi-
cant improvements in most circumstances
over baselines with linear mixture model
adaptation. An informal analysis suggests
that VSM adaptation may help in making
a good choice among words with the same
meaning, on the basis of style and genre.
</bodyText>
<sectionHeader confidence="0.986013" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9992916">
The translation models of a statistical machine
translation (SMT) system are trained on parallel
data. Usage of language and therefore the best
translation practice differs widely across genres,
topics, and dialects, and even depends on a partic-
</bodyText>
<note confidence="0.8474305">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999937142857143">
typically use a rich feature set to decide on weights
for the training data, at the sentence or phrase pair
level. For example, a sentence from a subcorpus
whose domain is far from that of the dev set would
typically receive a low weight, but sentences in
this subcorpus that appear to be of a general na-
ture might receive higher weights.
The 2012 JHU workshop on Domain Adapta-
tion for MT 1 proposed phrase sense disambigua-
tion (PSD) for translation model adaptation. In
this approach, the context of a phrase helps the
system to find the appropriate translation.
In this paper, we propose a new instance weight-
ing approach to domain adaptation based on a vec-
tor space model (VSM). As in (Foster et al., 2010),
this approach works at the level of phrase pairs.
However, the VSM approach is simpler and more
straightforward. Instead of using word-based fea-
tures and a computationally expensive training
procedure, we capture the distributional properties
of each phrase pair directly, representing it as a
vector in a space which also contains a representa-
tion of the dev set. The similarity between a given
phrase pair’s vector and the dev set vector be-
comes a feature for the decoder. It rewards phrase
pairs that are in some sense closer to those found
in the dev set, and punishes the rest. In initial ex-
periments, we tried three different similarity func-
tions: Bhattacharyya coefficient, Jensen-Shannon
divergency, and cosine measure. They all enabled
VSM adaptation to beat the non-adaptive baseline,
but Bhattacharyya similarity worked best, so we
adopted it for the remaining experiments.
The vector space used by VSM adaptation can
be defined in various ways. In the experiments
described below, we chose a definition that mea-
sures the contribution (to counts of a given phrase
pair, or to counts of all phrase pairs in the dev
set) of each training subcorpus. Thus, the vari-
ant of VSM adaptation tested here bears a super-
ficial resemblance to domain adaptation based on
mixture models for TMs, as in (Foster and Kuhn,
2007), in that both approaches rely on information
about the subcorpora from which the data origi-
nate. However, a key difference is that in this pa-
per we explicitly capture each phrase pair’s dis-
tribution across subcorpora, and compare it to the
aggregated distribution of phrase pairs in the dev
set. In mixture models, a phrase pair’s distribu-
</bodyText>
<footnote confidence="0.968108">
1http://www.clsp.jhu.edu/workshops/archive/ws-
12/groups/dasmt
</footnote>
<bodyText confidence="0.999975272727273">
tion across subcorpora is captured only implicitly,
by probabilities that reflect the prevalence of the
pair within each subcorpus. Thus, VSM adapta-
tion occurs at a much finer granularity than mix-
ture model adaptation. More fundamentally, there
is nothing about the VSM idea that obliges us to
define the vector space in terms of subcorpora.
For instance, we could cluster the words in the
source language into S clusters, and the words in
the target language into T clusters. Then, treat-
ing the dev set and each phrase pair as a pair of
bags of words (a source bag and a target bag) one
could represent each as a vector of dimension S +
T, with entries calculated from the counts associ-
ated with the S + T clusters (in a way similar to
that described for phrase pairs below). The (dev,
phrase pair) similarity would then be independent
of the subcorpora. One can think of several other
ways of defining the vector space that might yield
even better results than those reported here. Thus,
VSM adaptation is not limited to the variant of it
that we tested in our experiments.
</bodyText>
<sectionHeader confidence="0.914561" genericHeader="method">
2 Vector space model adaptation
</sectionHeader>
<bodyText confidence="0.999954357142857">
Vector space models (VSMs) have been widely
applied in many information retrieval and natural
language processing applications. For instance, to
compute the sense similarity between terms, many
researchers extract features for each term from its
context in a corpus, define a VSM and then ap-
ply similarity functions (Hindle, 1990; Lund and
Burgess, 1996; Lin, 1998; Turney, 2001).
In our experiments, we exploited the fact that
the training data come from a set of subcorpora.
For instance, the Chinese-English training data are
made up of 14 subcorpora (see section 3 below).
Suppose we have C subcorpora. The domain vec-
tor for a phrase-pair (f, e) is defined as
</bodyText>
<equation confidence="0.9993145">
V (f, e) =&lt; w1(f, e), ...wi(f, e), ..., wC(f, e) &gt;,
(1)
</equation>
<bodyText confidence="0.703854">
where wi(f, e) is a standard tf · idf weight, i.e.
</bodyText>
<equation confidence="0.973713">
wi(f, e) = tfi (f, e) · idf (f, e) . (2)
</equation>
<bodyText confidence="0.9442433">
To avoid a bias towards longer corpora, we nor-
malize the raw joint count ci(f, e) in the corpus
psi by dividing by the maximum raw count of any
1286 ase pair extracted in the corpus si. Let
max {ci (fj, ek) , (fj, ek) E si}.
(3)
The idf (f, e) is the inverse document fre-
quency: a measure of whether the phrase-pair
(f, e) is common or rare across all subcorpora. We
use the standard formula:
</bodyText>
<equation confidence="0.9984315">
idf (f, e) = logC
G (f, e)
</equation>
<bodyText confidence="0.999982111111111">
where df(f, e) is the number of subcorpora that
(f, e) appears in, and A is an empirically deter-
mined smoothing term.
For the in-domain dev set, we first run word
alignment and phrases extracting in the usual way
for the dev set, then sum the distribution of each
phrase pair (fj, ek) extracted from the dev data
across subcorpora to represent its domain informa-
tion. The dev vector is thus
</bodyText>
<equation confidence="0.995416">
V (dev) =&lt; w1(dev), ... , wC(dev) &gt;, (5)
</equation>
<bodyText confidence="0.601519">
where
</bodyText>
<equation confidence="0.859338">
cdev (fj, ek) wi(fj, ek) (6)
</equation>
<bodyText confidence="0.999723928571428">
J, K are the total numbers of source/target
phrases extracted from the dev data respectively.
cdev (fj, ek) is the joint count of phrase pair fj, ek
found in the dev set.
The vector can also be built with other features
of the phrase pair. For instance, we could replace
the raw joint count ci(f, e) in Equation 3 with the
raw marginal count of phrase pairs (f, e). There-
fore, even within the variant of VSM adaptation
we focus on in this paper, where the definition of
the vector space is based on the existence of sub-
corpora, one could utilize other definitions of the
vectors of the similarity function than those we uti-
lized in our experiments.
</bodyText>
<subsectionHeader confidence="0.95097">
2.1 Vector similarity functions
</subsectionHeader>
<bodyText confidence="0.999889">
VSM uses the similarity score between the vec-
tor representing the in-domain dev set and the vec-
tor representing each phrase pair as a decoder fea-
ture. There are many similarity functions we could
have employed for this purpose (Cha, 2007). We
tested three commonly-used functions: the Bhat-
tacharyya coefficient (BC) (Bhattacharyya, 1943;
Kazama et al., 2010), the Jensen-Shannon diver-
gence (JSD), and the cosine measure. According
to (Cha, 2007), these belong to three different fam-
ilies of similarity functions: the Fidelity family,
the Shannon’s entropy family, and the inner Prod-
uct family respectively. It was BC similarity that
yielded the best performance, and that we ended
up using in subsequent experiments.
To map the BC score onto a range from 0 to
1, we first normalize each weight in the vector by
dividing it by the sum of the weights. Thus, we get
the probability distribution of a phrase pair or the
phrase pairs in the dev data across all subcorpora:
</bodyText>
<equation confidence="0.9996795">
wi(f, e)
pi(f, e) = Ej=C (7)
j=1 wj(f,e)
wi(dev)
pi(dev) = Ej=C (8)
j=1 wj(dev)
</equation>
<bodyText confidence="0.999888636363636">
To further improve the similarity score, we ap-
ply absolute discounting smoothing when calcu-
lating the probability distributions pi(f,e). We
subtract a discounting value α from the non-zero
pi(f, e), and equally allocate the remaining proba-
bility mass to the zero probabilities. We carry out
the same smoothing for the probability distribu-
tions pi(dev). The smoothing constant α is deter-
mined empirically on held-out data.
The Bhattacharyya coefficient (BC) is defined
as follows:
</bodyText>
<equation confidence="0.987880666666667">
i=C
BC(dev; f, e) = V pi(dev) · pi(f, e) (9)
i=0
</equation>
<bodyText confidence="0.999471333333333">
The other two similarity functions we also
tested are JSD and cosine (Cos). They are defined
as follows:
</bodyText>
<equation confidence="0.996063074074074">
JSD(dev; f, e) = (10)
2pi(dev)
pi(dev)log +
pi(dev) + pi(f, e)
2pi(f, e)
pi(f, e) log I
pi (dev) + pi(f, e)
Cos(dev; f, e) = Ei pi (dev) · pi (f, e)
1287
ci (f,e)
tfi (f, e) =
+ A)
A I , (4)
j=J �
j=0
wi(dev) =
k=K
k=0
1
2[
i=C
i=1
i=C
i=1
�E �E
i p2 i (dev) i p2i (f, e)
(11)
</equation>
<table confidence="0.98662545">
corpus # segs # en tok % genres
fbis 250K 10.5M 3.7 nw
financial 90K 2.5M 0.9 fin
gale bc 79K 1.3M 0.5 bc
gale bn 75K 1.8M 0.6 bn ng
gale nw 25K 696K 0.2 nw
gale wl 24K 596K 0.2 wl
hkh 1.3M 39.5M 14.0 hans
hkl 400K 9.3M 3.3 legal
hkn 702K 16.6M 5.9 nw
isi 558K 18.0M 6.4 nw
lex&amp;ne 1.3M 2.0M 0.7 lex
other nw 146K 5.2M 1.8 nw
sinorama 282K 10.0M 3.5 nw
un 5.0M 164M 58.2 un
TOTAL 10.1M 283M 100.0 (all)
devtest
tune 1,506 161K nw wl
NIST06 1,664 189K nw bng
NIST08 1,357 164K nw wl
</table>
<tableCaption confidence="0.8390555">
Table 1: NIST Chinese-English data. In the
genres column: nw=newswire, bc=broadcast
conversation, bn=broadcast news, wl=weblog,
ng=newsgroup, un=UN proc., bng = bn &amp; ng.
</tableCaption>
<sectionHeader confidence="0.998397" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997452">
3.1 Data setting
</subsectionHeader>
<bodyText confidence="0.998724523809524">
We carried out experiments in two different set-
tings, both involving data from NIST Open MT
2012.2 The first setting is based on data from
the Chinese to English constrained track, compris-
ing about 283 million English running words. We
manually grouped the training data into 14 corpora
according to genre and origin. Table 1 summa-
rizes information about the training, development
and test sets; we show the sizes of the training sub-
corpora in number of words as a percentage of all
training data. Most training subcorpora consist of
parallel sentence pairs. The isi and lex&amp;ne cor-
pora are exceptions: the former is extracted from
comparable data, while the latter is a lexicon that
includes many named entities. The development
set (tune) was taken from the NIST 2005 evalua-
tion set, augmented with some web-genre material
reserved from other NIST corpora.
The second setting uses NIST 2012 Arabic to
English data, but excludes the UN data. There are
about 47.8 million English running words in these
</bodyText>
<footnote confidence="0.950862">
2http://www.nist.gov/itl/iad/mig/openmt12.cfm
</footnote>
<table confidence="0.998869230769231">
corpus # segs # en toks % gen
gale bc 57K 1.6M 3.3 bc
gale bn 45K 1.2M 2.5 bn
gale ng 21K 491K 1.0 ng
gale nw 17K 659K 1.4 nw
gale wl 24K 590K 1.2 wl
isi 1,124K 34.7M 72.6 nw
other nw 224K 8.7M 18.2 nw
TOTAL 1,512K 47.8M 100.0 (all)
devtest
NIST06 1,664 202K nwl
NIST08 1,360 205K nwl
NIST09 1,313 187K nwl
</table>
<tableCaption confidence="0.83773075">
Table 2: NIST Arabic-English data. In the gen
(genres) column: nw=newswire, bc=broadcast
conversation, bn=broadcast news, ng=newsgroup,
wl=weblog, nwl = nw &amp; wl.
</tableCaption>
<bodyText confidence="0.996024777777778">
training data. We manually grouped the training
data into 7 groups according to genre and origin.
Table 2 summarizes information about the train-
ing, development and test sets. Note that for this
language pair, the comparable isi data represent a
large proportion of the training data: 72% of the
English words. We use the evaluation sets from
NIST 2006, 2008, and 2009 as our development
set and two test sets, respectively.
</bodyText>
<subsectionHeader confidence="0.994911">
3.2 System
</subsectionHeader>
<bodyText confidence="0.9999818125">
Experiments were carried out with an in-house
phrase-based system similar to Moses (Koehn et
al., 2007). Each corpus was word-aligned using
IBM2, HMM, and IBM4 models, and the phrase
table was the union of phrase pairs extracted from
these separate alignments, with a length limit of
7. The translation model (TM) was smoothed in
both directions with KN smoothing (Chen et al.,
2011). We use the hierarchical lexicalized reorder-
ing model (RM) (Galley and Manning, 2008), with
a distortion limit of 7. Other features include lex-
ical weighting in both directions, word count, a
distance-based RM, a 4-gram LM trained on the
target side of the parallel data, and a 6-gram En-
glish Gigaword LM. The system was tuned with
batch lattice MIRA (Cherry and Foster, 2012).
</bodyText>
<subsectionHeader confidence="0.546812">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.994869309090909">
For the baseline, we simply concatenate all train-
ing data. We have also compared our approach
to two widely used TM domain adaptation ap-
1288
proaches. One is the log-linear combination
of TMs trained on each subcorpus (Koehn and
Schroeder, 2007), with weights of each model
tuned under minimal error rate training using
MIRA. The other is a linear combination of TMs
trained on each subcorpus, with the weights of
each model learned with an EM algorithm to max-
imize the likelihood of joint empirical phrase pair
counts for in-domain dev data. For details, refer to
(Foster and Kuhn, 2007).
The value of A and α (see Eq 4 and Section 2.1)
are determined by the performance on the dev
set of the Arabic-to-English system. For both
Arabic-to-English and Chinese-to-English exper-
iment, these values obtained on Arabic dev were
used to obtain the results below: A was set to 8,
and α was set to 0.01. (Later, we ran an exper-
iment on Chinese-to-English with A and α tuned
specifically for that language pair, but the perfor-
mance for the Chinese-English system only im-
proved by a tiny, insignificant amount).
Our metric is case-insensitive IBM BLEU (Pa-
pineni et al., 2002), which performs matching of
n-grams up to n = 4; we report BLEU scores av-
eraged across both test sets NIST06 and NIST08
for Chinese; NIST08 and NIST09 for Arabic.
Following (Koehn, 2004), we use the bootstrap-
resampling test to do significance testing. In ta-
bles 3 to 5, * and ** denote significant gains over
the baseline at p &lt; 0.05 and p &lt; 0.01 levels, re-
spectively.
We first compare the performance of differ-
ent similarity functions: cosine (COS), Jensen-
Shannon divergence (JSD) and Bhattacharyya co-
efficient (BC). The results are shown in Table 3.
All three functions obtained improvements. Both
COS and BC yield statistically significant im-
provements over the baseline, with BC performing
better than COS by a further statistically signifi-
cant margin. The Bhattacharyya coefficient is ex-
plicitly designed to measure the overlap between
the probability distributions of two statistical sam-
ples or populations, which is precisely what we are
trying to do here: we are trying to reward phrase
pairs whose distribution is similar to that of the
dev set. Thus, its superior performance in these
experiments is not unexpected.
In the next set of experiments, we compared
VSM adaptation using the BC similarity function
with the baseline which concatenates all training
data and with log-linear and linear TM mixtures
</bodyText>
<table confidence="0.9985356">
system Chinese Arabic
baseline 31.7 46.8
COS 32.3* 47.8**
JSD 32.1 47.1
BC 33.0** 48.4**
</table>
<tableCaption confidence="0.97696625">
Table 3: Comparison of different similarity func-
tions. * and ** denote significant gains over the
baseline at p &lt; 0.05 and p &lt; 0.01 levels, respec-
tively.
</tableCaption>
<table confidence="0.9996976">
system Chinese Arabic
baseline 31.7 46.8
loglinear tm 28.4 44.5
linear tm 32.7 ** 47.5 **
vsm, BC 33.0 ** 48.4 **
</table>
<tableCaption confidence="0.998974">
Table 4: Results for variants of adaptation.
</tableCaption>
<bodyText confidence="0.999932875">
whose components are based on subcorpora. Ta-
ble 4 shows that log-linear combination performs
worse than the baseline: the tuning algorithm
failed to optimize the log-linear combination even
on dev set. For Chinese, the BLEU score of the
dev set on the baseline system is 27.3, while on
the log-linear combination system, it is 24.0; for
Arabic, the BLEU score of the dev set on the base-
line system is 46.8, while on the log-linear com-
bination system, it is 45.4. We also tried adding
the global model to the loglinear combination and
it didn’t improve over the baseline for either lan-
guage pair. Linear mixture was significantly better
than the baseline at the p &lt; 0.01 level for both lan-
guage pairs. Since our approach, VSM, performed
better than the linear mixture for both pairs, it is of
course also significantly better than the baseline at
the p &lt; 0.01 level.
This raises the question: is VSM performance
significantly better than that of a linear mixture of
TMs? The answer (not shown in the table) is that
for Arabic to English, VSM performance is bet-
ter than linear mixture at the p &lt; 0.01 level. For
Chinese to English, the argument for the superi-
ority of VSM over linear mixture is less convinc-
ing: there is significance at the p &lt; 0.05 for one
of the two test sets (NIST06) but not for the other
(NIST08). At any rate, these results establish that
VSM adaptation is clearly superior to linear mix-
ture TM adaptation, for one of the two language
pairs.
In Table 4, the VSM results are based on the
</bodyText>
<page confidence="0.984942">
1289
</page>
<table confidence="0.9994668">
system Chinese Arabic
baseline 31.7 46.8
linear tm 32.7** 47.5**
vsm, joint 33.0** 48.4**
vsm, src-marginal 32.2* 47.3*
vsm, tgt-marginal 32.6** 47.6**
vsm, src+tgt (2 feat.) 32.7** 48.2**
vsm, joint+src (2 feat.) 32.9** 48.4**
vsm, joint+tgt (2 feat.) 32.9** 48.4**
vsm, joint+src+tgt (3 feat.) 33.1** 48.6**
</table>
<tableCaption confidence="0.9711165">
Table 5: Results for adaptation based on joint or
maginal counts.
</tableCaption>
<bodyText confidence="0.999907958333334">
vector of the joint counts of the phrase pair. In
the next experiment, we replace the joint counts
with the source or target marginal counts. In Ta-
ble 5, we first show the results based on source
and target marginal counts, then the results of us-
ing feature sets drawn from three decoder VSM
features: a joint count feature, a source marginal
count feature, and a target marginal count fea-
ture. For instance, the last row shows the results
when all three features are used (with their weights
tuned by MIRA). It looks as though the source and
target marginal counts contain useful information.
The best performance is obtained by combining all
three sources of information. The 3-feature ver-
sion of VSM yields +1.8 BLEU over the baseline
for Arabic to English, and +1.4 BLEU for Chinese
to English.
When we compared two sets of results in Ta-
ble 4, the joint count version of VSM and lin-
ear mixture of TMs, we found that for Arabic to
English, VSM performance is better than linear
mixture at the p &lt; 0.01 level; the Chinese to
English significance test was inconclusive (VSM
found to be superior to linear mixture at p &lt; 0.05
for NIST06 but not for NIST08). We now have
somewhat better results for the 3-feature version
of VSM shown in Table 5. How do these new re-
sults affect the VSM vs. linear mixture compari-
son? Naturally, the conclusions for Arabic don’t
change. For Chinese, 3-feature VSM is now su-
perior to linear mixture at p &lt; 0.01 on NIST06
test set, but 3-feature VSM still doesn’t have a sta-
tistically significant edge over linear mixture on
NIST08 test set. A fair summary would be that 3-
feature VSM adaptation is decisively superior to
linear mixture adaptation for Arabic to English,
and highly competitive with linear mixture adap-
tation for Chinese to English.
Our last set of experiments examined the ques-
tion: when added to a system that already has
some form of linear mixture model adaptation,
does VSM improve performance? In (Foster and
Kuhn, 2007), two kinds of linear mixture were de-
scribed: linear mixture of language models (LMs),
and linear mixture of translation models (TMs).
Some of the results reported above involved lin-
ear TM mixtures, but none of them involved lin-
ear LM mixtures. Table 6 shows the results of
different combinations of VSM and mixture mod-
els. * and ** denote significant gains over the row
no vsm at p &lt; 0.05 and p &lt; 0.01 levels, re-
spectively. This means that in the table, the base-
line within each box containing three results is the
topmost result in the box. For instance, with an
initial Chinese system that employs linear mixture
LM adaptation (lin-lm) and has a BLEU of 32.1,
adding 1-feature VSM adaptation (+vsm, joint)
improves performance to 33.1 (improvement sig-
nificant at p &lt; 0.01), while adding 3-feature VSM
instead (+vsm, 3 feat.) improves performance to
33.2 (also significant at p &lt; 0.01). For Arabic, in-
cluding either form of VSM adaptation always im-
proves performance with significance at p &lt; 0.01,
even over a system including both linear TM and
linear LM adaptation. For Chinese, adding VSM
still always yields an improvement, but the im-
provement is not significant if linear TM adapta-
tion is already in the system. These results show
that combining VSM adaptation and either or both
kinds of linear mixture adaptation never hurts per-
formance, and often improves it by a significant
amount.
</bodyText>
<subsectionHeader confidence="0.972193">
3.4 Informal Data Analysis
</subsectionHeader>
<bodyText confidence="0.999917285714286">
To get an intuition for how VSM adaptation im-
proves BLEU scores, we compared outputs from
the baseline and VSM-adapted system (“vsm,
joint” in Table 5) on the Chinese test data. We
focused on examples where the two systems had
translated the same source-language (Chinese)
phrase s differently, and where the target-language
(English) translation of s chosen by the VSM-
adapted system, tV , had a higher Bhattacharyya
score for similarity with the dev set than did the
phrase that was chosen by the baseline system, tB.
Thus, we ignored differences in the two transla-
tions that might have been due to the secondary
effects of VSM adaptation (such as a different tar-
</bodyText>
<page confidence="0.954206">
1290
</page>
<table confidence="0.999557714285714">
no-lin-adap lin-lm lin-tm lin-lm+lin-tm
no vsm 31.7 32.1 32.7 33.1
Chinese +vsm, joint 33.0** 33.1** 33.0 33.3
+vsm, 3 feat. 33.1** 33.2** 33.1 33.4
no vsm 46.8 47.0 47.5 47.7
Arabic +vsm, joint 48.4** 48.7** 48.6** 48.8**
+vsm, 3 feat. 48.6** 48.8** 48.7** 48.9**
</table>
<tableCaption confidence="0.8241505">
Table 6: Results of combining VSM and linear mixture adaptation. “lin-lm” is linear language model
adaptation, “lin-tm” is linear translation model adaptation. * and ** denote significant gains over the row
</tableCaption>
<bodyText confidence="0.994826076923077">
“no vsm” at p &lt; 0.05 and p &lt; 0.01 levels, respectively.
get phrase being preferred by the language model
in the VSM-adapted system from the one preferred
in the baseline system because of a Bhattacharyya-
mediated change in the phrase preceding it).
An interesting pattern soon emerged: the VSM-
adapted system seems to be better than the base-
line at choosing among synonyms in a way that is
appropriate to the genre or style of a text. For in-
stance, where the text to be translated is from an
informal genre such as weblog, the VSM-adapted
system will often pick an informal word where the
baseline picks a formal word with the same or sim-
ilar meaning, and vice versa where the text to be
translated is from a more formal genre. To our
surprise, we saw few examples where the VSM-
adapted system did a better job than the baseline of
choosing between two words with different mean-
ing, but we saw many examples where the VSM-
adapted system did a better job than the baseline
of choosing between two words that both have the
same meaning according to considerations of style
and genre.
Two examples are shown in Table 7. In the
first example, the first two lines show that VSM
finds that the Chinese-English phrase pair (殴打,
assaulted) has a Bhattacharyya (BC) similarity of
0.556163 to the dev set, while the phrase pair (殴
打, beat) has a BC similarity of 0.780787 to the
dev. In this situation, the VSM-adapted system
thus prefers “beat” to “assaulted” as a translation
for 殴打. The next four lines show the source
sentence (SRC), the reference (REF), the baseline
output (BSL), and the output of the VSM-adapted
system. Note that the result of VSM adaptation is
that the rather formal word “assaulted” is replaced
by its informal near-synonym “beat” in the trans-
lation of an informal weblog text.
“apprehend” might be preferable to “arrest” in
a legal text. However, it looks as though the
VSM-adapted system has learned from the dev
that among synonyms, those more characteristic
of news stories than of legal texts should be cho-
sen: it therefore picks “arrest” over its synonym
“apprehend”.
What follows is a partial list of pairs of phrases
(all single words) from our system’s outputs,
where the baseline chose the first member of a pair
and the VSM-adapted system chose the second
member of the pair to translate the same Chinese
phrase into English (because the second word
yields a better BC score for the dev set we used).
It will be seen that nearly all of the pairs involve
synonyms or near-synonyms rather than words
with radically different senses (one exception
below is “center” vs “heart”). Instead, the differ-
ences between the two words tend to be related to
genre or style: gunmen-mobsters, champion-star,
updated-latest, caricatures-cartoons, spill-leakage,
hiv-aids, inkling-clues, behaviour-actions, deceit-
trick, brazen-shameless, aristocratic-noble,
circumvent-avoid, attack-criticized, descent-born,
hasten-quickly, precipice-cliff, center-heart,
blessing-approval, imminent-approaching,
stormed-rushed, etc.
</bodyText>
<sectionHeader confidence="0.99657" genericHeader="conclusions">
4 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999958083333333">
This paper proposed a new approach to domain
adaptation in statistical machine translation, based
on vector space models (VSMs). This approach
measures the similarity between a vector repre-
senting a particular phrase pair in the phrase ta-
ble and a vector representing the dev set, yield-
ing a feature associated with that phrase pair that
will be used by the decoder. The approach is
simple, easy to implement, and computationally
cheap. For the two language pairs we looked
at, it provided a large performance improvement
over a non-adaptive baseline, and also compared
</bodyText>
<page confidence="0.950789">
1291
</page>
<table confidence="0.999789416666667">
1 phrase 殴打↔ assaulted (0.556163)
pairs 殴打↔ beat (0.780787)
SRC ...那些殴打村民的地皮流氓...
REF ... those local ruffians and hooligans who beat up villagers ...
BSL ... those who assaulted the villagers land hooligans ...
VSM ... hooligans who beat the villagers ...
2 phrase 缉拿↔ apprehend (0.286533)
pairs 缉拿↔ arrest (0.603342)
SRC ... 缉拿凶手并R将之绳之以法。
REF ... catch the killers and bring them to justice .
BSL ... apprehend the perpetrators and bring them to justice .
VSM ... arrest the perpetrators and bring them to justice.
</table>
<tableCaption confidence="0.999734">
Table 7: Examples show that VSM chooses translations according to considerations of style and genre.
</tableCaption>
<bodyText confidence="0.9995524375">
favourably with linear mixture adaptation tech-
niques.
Furthermore, VSM adaptation can be exploited
in a number of different ways, which we have only
begun to explore. In our experiments, we based
the vector space on subcorpora defined by the na-
ture of the training data. This was done purely
out of convenience: there are many, many ways to
define a vector space in this situation. An obvi-
ous and appealing one, which we intend to try in
future, is a vector space based on a bag-of-words
topic model. A feature derived from this topic-
related vector space might complement some fea-
tures derived from the subcorpora which we ex-
plored in the experiments above, and which seem
to exploit information related to genre and style.
</bodyText>
<sectionHeader confidence="0.998614" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999639658914729">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In EMNLP 2011.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of the
4th Workshop on Statistical Machine Translation,
Athens, March. WMT.
A. Bhattacharyya. 1943. On a measure of divergence
between two statistical populations defined by their
probability distributions. Bulletin of the Calcutta
Mathematical Society, 35:99–109.
Sung-Hyuk Cha. 2007. Comprehensive survey on dis-
tance/similarity measures between probability den-
sity functions. International Journal of Mathe-
matical Models ind Methods in Applied Sciences,
1(4):300–307.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for smt self-
enhancement. In ACL 2008.
Boxing Chen, Roland Kuhn, George Foster, and
Howard Johnson. 2011. Unpacking and transform-
ing feature functions: New ways to smooth phrase
tables. In MT Summit 2011.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL 2012.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
ACL Workshop on Statistical Machine Translation,
Prague, June. WMT.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Boston.
Michel Galley and C. D. Manning. 2008. A simple
and effective hierarchical phrase reordering model.
In EMNLP 2008, pages 848–856, Hawaii, October.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
EAMT Conference, Budapest, May.
Donald Hindle. 1990. Noun classification from predi-
cate.argument structures. In Proceedings of the 28th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 268–275, Pittsburgh,
PA, June. ACL.
Fei Huang and Bing Xiang. 2010. Feature-rich dis-
criminative phrase rescoring for SMT. In COLING
2010.
Jun’ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A
1292
bayesian method for robust estimation of distribu-
tional similarities. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 247–256, Uppsala, Swe-
den, July. ACL.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224–227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL 2007,
Demonstration Session.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Barcelona, Spain.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING/ACL-
98, pages 768–774, Montreal, Quebec, Canada.
Yajuan L¨u, Jin Huang, and Qun Liu. 2007. Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Pro-
ceedings of the 2007 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
Prague, Czech Republic.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods Instru-
ments and Computers, 28(2):203–208.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In ACL
2010.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311–318,
Philadelphia, July. ACL.
Aaron B. Phillips and Ralf D. Brown. 2011. Train-
ing machine translation with a second-order taylor
approximation of weighted translation instances. In
MT Summit 2011.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In IWSLT 2008.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In EACL 2012.
Peter Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Twelfth European
Conference on Machine Learning, page 491–502,
Berlin, Germany.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Prague, Czech Republic, June.
ACL.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (COLING) 2004, Geneva, Au-
gust.
</reference>
<page confidence="0.972817">
1293
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.141872">
<title confidence="0.99854">Vector Space Model for Adaptation in Statistical Machine Translation</title>
<author confidence="0.905422">Boxing Chen</author>
<author confidence="0.905422">Roland Kuhn</author>
<author confidence="0.905422">George</author>
<affiliation confidence="0.912414">National Research Council</affiliation>
<email confidence="0.63114">first.last@nrc-cnrc.gc.ca</email>
<abstract confidence="0.9743997">ular author’s or publication’s style; the word “domain” is often used to indicate a particular combination of all these factors. Unless there is a perfect match between the training data domain and the (test) domain in which the SMT system will be used, one can often get better performance by adapting the system to the test domain. Domain adaptation is an active topic in the natural language processing (NLP) research community. Its application to SMT systems has recently received considerable attention. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang,</abstract>
<address confidence="0.812828">2010; Phillips and Brown, 2011; Sennrich, 2012</address>
<date confidence="0.541421">1285</date>
<abstract confidence="0.999583848484849">This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set. Thus, we obtain a decoding feature whose value represents the phrase pair’s closeness to the dev. This is a simple, computationally cheap form of instance weighting for phrase pairs. Experiments on large scale NIST evaluation data show improvements over strong baselines: +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline, and significant improvements in most circumstances over baselines with linear mixture model adaptation. An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="1773" citStr="Axelrod et al., 2011" startWordPosition="276" endWordPosition="279"> models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance, be a vector with a dimensionality equal to the number of traini</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain adaptation for statistical machine translation with monolingual resources.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Machine Translation,</booktitle>
<publisher>WMT.</publisher>
<location>Athens,</location>
<contexts>
<context position="1638" citStr="Bertoldi and Federico, 2009" startWordPosition="253" endWordPosition="256">st approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for t</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual resources. In Proceedings of the 4th Workshop on Statistical Machine Translation, Athens, March. WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bhattacharyya</author>
</authors>
<title>On a measure of divergence between two statistical populations defined by their probability distributions.</title>
<date>1943</date>
<journal>Bulletin of the Calcutta Mathematical Society,</journal>
<pages>35--99</pages>
<contexts>
<context position="10054" citStr="Bhattacharyya, 1943" startWordPosition="1699" endWordPosition="1700">nt of VSM adaptation we focus on in this paper, where the definition of the vector space is based on the existence of subcorpora, one could utilize other definitions of the vectors of the similarity function than those we utilized in our experiments. 2.1 Vector similarity functions VSM uses the similarity score between the vector representing the in-domain dev set and the vector representing each phrase pair as a decoder feature. There are many similarity functions we could have employed for this purpose (Cha, 2007). We tested three commonly-used functions: the Bhattacharyya coefficient (BC) (Bhattacharyya, 1943; Kazama et al., 2010), the Jensen-Shannon divergence (JSD), and the cosine measure. According to (Cha, 2007), these belong to three different families of similarity functions: the Fidelity family, the Shannon’s entropy family, and the inner Product family respectively. It was BC similarity that yielded the best performance, and that we ended up using in subsequent experiments. To map the BC score onto a range from 0 to 1, we first normalize each weight in the vector by dividing it by the sum of the weights. Thus, we get the probability distribution of a phrase pair or the phrase pairs in the </context>
</contexts>
<marker>Bhattacharyya, 1943</marker>
<rawString>A. Bhattacharyya. 1943. On a measure of divergence between two statistical populations defined by their probability distributions. Bulletin of the Calcutta Mathematical Society, 35:99–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung-Hyuk Cha</author>
</authors>
<title>Comprehensive survey on distance/similarity measures between probability density functions.</title>
<date>2007</date>
<journal>International Journal of Mathematical Models ind Methods in Applied Sciences,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="9956" citStr="Cha, 2007" startWordPosition="1687" endWordPosition="1688">on 3 with the raw marginal count of phrase pairs (f, e). Therefore, even within the variant of VSM adaptation we focus on in this paper, where the definition of the vector space is based on the existence of subcorpora, one could utilize other definitions of the vectors of the similarity function than those we utilized in our experiments. 2.1 Vector similarity functions VSM uses the similarity score between the vector representing the in-domain dev set and the vector representing each phrase pair as a decoder feature. There are many similarity functions we could have employed for this purpose (Cha, 2007). We tested three commonly-used functions: the Bhattacharyya coefficient (BC) (Bhattacharyya, 1943; Kazama et al., 2010), the Jensen-Shannon divergence (JSD), and the cosine measure. According to (Cha, 2007), these belong to three different families of similarity functions: the Fidelity family, the Shannon’s entropy family, and the inner Product family respectively. It was BC similarity that yielded the best performance, and that we ended up using in subsequent experiments. To map the BC score onto a range from 0 to 1, we first normalize each weight in the vector by dividing it by the sum of t</context>
</contexts>
<marker>Cha, 2007</marker>
<rawString>Sung-Hyuk Cha. 2007. Comprehensive survey on distance/similarity measures between probability density functions. International Journal of Mathematical Models ind Methods in Applied Sciences, 1(4):300–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>Exploiting n-best hypotheses for smt selfenhancement.</title>
<date>2008</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="1593" citStr="Chen et al., 2008" startWordPosition="247" endWordPosition="250">2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general i</context>
</contexts>
<marker>Chen, Zhang, Aw, Li, 2008</marker>
<rawString>Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li. 2008. Exploiting n-best hypotheses for smt selfenhancement. In ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
<author>Howard Johnson</author>
</authors>
<title>Unpacking and transforming feature functions: New ways to smooth phrase tables.</title>
<date>2011</date>
<booktitle>In MT Summit</booktitle>
<contexts>
<context position="14695" citStr="Chen et al., 2011" startWordPosition="2522" endWordPosition="2525">ge pair, the comparable isi data represent a large proportion of the training data: 72% of the English words. We use the evaluation sets from NIST 2006, 2008, and 2009 as our development set and two test sets, respectively. 3.2 System Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with KN smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 Results For the baseline, we simply concatenate all training data. We have also compared our approach to two widely used TM domain adaptation ap1288 proaches. One is the log-linear combination of TMs trained on each s</context>
</contexts>
<marker>Chen, Kuhn, Foster, Johnson, 2011</marker>
<rawString>Boxing Chen, Roland Kuhn, George Foster, and Howard Johnson. 2011. Unpacking and transforming feature functions: New ways to smooth phrase tables. In MT Summit 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In NAACL</booktitle>
<contexts>
<context position="15072" citStr="Cherry and Foster, 2012" startWordPosition="2586" endWordPosition="2589">ing IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with KN smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 Results For the baseline, we simply concatenate all training data. We have also compared our approach to two widely used TM domain adaptation ap1288 proaches. One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. The other is a linear combination of TMs trained on each subcorpus, with the weights of each model learned with an EM algorithm to maximize the likelihood of joint empirical phrase pair counts for in-domain dev data. For details, refer to (Foster and Kuhn, 200</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In NAACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation,</booktitle>
<publisher>WMT.</publisher>
<location>Prague,</location>
<contexts>
<context position="981" citStr="Foster and Kuhn, 2007" startWordPosition="147" endWordPosition="150">omain and the (test) domain in which the SMT system will be used, one can often get better performance by adapting the system to the test domain. Domain adaptation is an active topic in the natural language processing (NLP) research community. Its application to SMT systems has recently received considerable attention. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen </context>
<context position="5829" citStr="Foster and Kuhn, 2007" startWordPosition="949" endWordPosition="952">ergency, and cosine measure. They all enabled VSM adaptation to beat the non-adaptive baseline, but Bhattacharyya similarity worked best, so we adopted it for the remaining experiments. The vector space used by VSM adaptation can be defined in various ways. In the experiments described below, we chose a definition that measures the contribution (to counts of a given phrase pair, or to counts of all phrase pairs in the dev set) of each training subcorpus. Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in (Foster and Kuhn, 2007), in that both approaches rely on information about the subcorpora from which the data originate. However, a key difference is that in this paper we explicitly capture each phrase pair’s distribution across subcorpora, and compare it to the aggregated distribution of phrase pairs in the dev set. In mixture models, a phrase pair’s distribu1http://www.clsp.jhu.edu/workshops/archive/ws12/groups/dasmt tion across subcorpora is captured only implicitly, by probabilities that reflect the prevalence of the pair within each subcorpus. Thus, VSM adaptation occurs at a much finer granularity than mixtur</context>
<context position="15674" citStr="Foster and Kuhn, 2007" startWordPosition="2688" endWordPosition="2691">ry and Foster, 2012). 3.3 Results For the baseline, we simply concatenate all training data. We have also compared our approach to two widely used TM domain adaptation ap1288 proaches. One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. The other is a linear combination of TMs trained on each subcorpus, with the weights of each model learned with an EM algorithm to maximize the likelihood of joint empirical phrase pair counts for in-domain dev data. For details, refer to (Foster and Kuhn, 2007). The value of A and α (see Eq 4 and Section 2.1) are determined by the performance on the dev set of the Arabic-to-English system. For both Arabic-to-English and Chinese-to-English experiment, these values obtained on Arabic dev were used to obtain the results below: A was set to 8, and α was set to 0.01. (Later, we ran an experiment on Chinese-to-English with A and α tuned specifically for that language pair, but the performance for the Chinese-English system only improved by a tiny, insignificant amount). Our metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matchi</context>
<context position="21831" citStr="Foster and Kuhn, 2007" startWordPosition="3755" endWordPosition="3758">bic don’t change. For Chinese, 3-feature VSM is now superior to linear mixture at p &lt; 0.01 on NIST06 test set, but 3-feature VSM still doesn’t have a statistically significant edge over linear mixture on NIST08 test set. A fair summary would be that 3- feature VSM adaptation is decisively superior to linear mixture adaptation for Arabic to English, and highly competitive with linear mixture adaptation for Chinese to English. Our last set of experiments examined the question: when added to a system that already has some form of linear mixture model adaptation, does VSM improve performance? In (Foster and Kuhn, 2007), two kinds of linear mixture were described: linear mixture of language models (LMs), and linear mixture of translation models (TMs). Some of the results reported above involved linear TM mixtures, but none of them involved linear LM mixtures. Table 6 shows the results of different combinations of VSM and mixture models. * and ** denote significant gains over the row no vsm at p &lt; 0.05 and p &lt; 0.01 levels, respectively. This means that in the table, the baseline within each box containing three results is the topmost result in the box. For instance, with an initial Chinese system that employs</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for SMT. In Proceedings of the ACL Workshop on Statistical Machine Translation, Prague, June. WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Boston.</location>
<contexts>
<context position="1965" citStr="Foster et al., 2010" startWordPosition="308" endWordPosition="311">mework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted</context>
<context position="4506" citStr="Foster et al., 2010" startWordPosition="731" endWordPosition="734">nce or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 1 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. In this paper, we propose a new instance weighting approach to domain adaptation based on a vector space model (VSM). As in (Foster et al., 2010), this approach works at the level of phrase pairs. However, the VSM approach is simpler and more straightforward. Instead of using word-based features and a computationally expensive training procedure, we capture the distributional properties of each phrase pair directly, representing it as a vector in a space which also contains a representation of the dev set. The similarity between a given phrase pair’s vector and the dev set vector becomes a feature for the decoder. It rewards phrase pairs that are in some sense closer to those found in the dev set, and punishes the rest. In initial expe</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>C D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In EMNLP 2008,</booktitle>
<pages>848--856</pages>
<location>Hawaii,</location>
<contexts>
<context position="14781" citStr="Galley and Manning, 2008" startWordPosition="2535" endWordPosition="2538">ata: 72% of the English words. We use the evaluation sets from NIST 2006, 2008, and 2009 as our development set and two test sets, respectively. 3.2 System Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with KN smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 Results For the baseline, we simply concatenate all training data. We have also compared our approach to two widely used TM domain adaptation ap1288 proaches. One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal e</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and C. D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP 2008, pages 848–856, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Adaptation of the translation model for statistical machine translation based on information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th EAMT Conference,</booktitle>
<location>Budapest,</location>
<contexts>
<context position="1709" citStr="Hildebrand et al., 2005" startWordPosition="264" endWordPosition="267"> different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance,</context>
</contexts>
<marker>Hildebrand, Eck, Vogel, Waibel, 2005</marker>
<rawString>Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Adaptation of the translation model for statistical machine translation based on information retrieval. In Proceedings of the 10th EAMT Conference, Budapest, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun classification from predicate.argument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>268--275</pages>
<publisher>ACL.</publisher>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="7667" citStr="Hindle, 1990" startWordPosition="1256" endWordPosition="1257">independent of the subcorpora. One can think of several other ways of defining the vector space that might yield even better results than those reported here. Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments. 2 Vector space model adaptation Vector space models (VSMs) have been widely applied in many information retrieval and natural language processing applications. For instance, to compute the sense similarity between terms, many researchers extract features for each term from its context in a corpus, define a VSM and then apply similarity functions (Hindle, 1990; Lund and Burgess, 1996; Lin, 1998; Turney, 2001). In our experiments, we exploited the fact that the training data come from a set of subcorpora. For instance, the Chinese-English training data are made up of 14 subcorpora (see section 3 below). Suppose we have C subcorpora. The domain vector for a phrase-pair (f, e) is defined as V (f, e) =&lt; w1(f, e), ...wi(f, e), ..., wC(f, e) &gt;, (1) where wi(f, e) is a standard tf · idf weight, i.e. wi(f, e) = tfi (f, e) · idf (f, e) . (2) To avoid a bias towards longer corpora, we normalize the raw joint count ci(f, e) in the corpus psi by dividing by th</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Donald Hindle. 1990. Noun classification from predicate.argument structures. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics (ACL), pages 268–275, Pittsburgh, PA, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Bing Xiang</author>
</authors>
<title>Feature-rich discriminative phrase rescoring for SMT.</title>
<date>2010</date>
<booktitle>In COLING</booktitle>
<contexts>
<context position="1988" citStr="Huang and Xiang, 2010" startWordPosition="312" endWordPosition="315">ve learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the training data</context>
</contexts>
<marker>Huang, Xiang, 2010</marker>
<rawString>Fei Huang and Bing Xiang. 2010. Feature-rich discriminative phrase rescoring for SMT. In COLING 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Stijn De Saeger</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
<author>Kentaro Torisawa</author>
</authors>
<date>2010</date>
<journal>A</journal>
<pages>1292</pages>
<marker>Kazama, De Saeger, Kuroda, Murata, Torisawa, 2010</marker>
<rawString>Jun’ichi Kazama, Stijn De Saeger, Kow Kuroda, Masaki Murata, and Kentaro Torisawa. 2010. A 1292</rawString>
</citation>
<citation valid="true">
<title>bayesian method for robust estimation of distributional similarities.</title>
<date></date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>247--256</pages>
<publisher>ACL.</publisher>
<location>Uppsala, Sweden,</location>
<marker></marker>
<rawString>bayesian method for robust estimation of distributional similarities. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 247–256, Uppsala, Sweden, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>224--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1266" citStr="Koehn and Schroeder, 2007" startWordPosition="195" endWordPosition="198">cently received considerable attention. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add th</context>
<context position="15331" citStr="Koehn and Schroeder, 2007" startWordPosition="2629" endWordPosition="2632">he hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 Results For the baseline, we simply concatenate all training data. We have also compared our approach to two widely used TM domain adaptation ap1288 proaches. One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. The other is a linear combination of TMs trained on each subcorpus, with the weights of each model learned with an EM algorithm to maximize the likelihood of joint empirical phrase pair counts for in-domain dev data. For details, refer to (Foster and Kuhn, 2007). The value of A and α (see Eq 4 and Section 2.1) are determined by the performance on the dev set of the Arabic-to-English system. For both Arabic-to-English and Chinese-to-English experiment, these values obtained on Arabic dev were used to obtain the res</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL 2007, Demonstration Session.</booktitle>
<contexts>
<context position="14415" citStr="Koehn et al., 2007" startWordPosition="2475" endWordPosition="2478">adcast conversation, bn=broadcast news, ng=newsgroup, wl=weblog, nwl = nw &amp; wl. training data. We manually grouped the training data into 7 groups according to genre and origin. Table 2 summarizes information about the training, development and test sets. Note that for this language pair, the comparable isi data represent a large proportion of the training data: 72% of the English words. We use the evaluation sets from NIST 2006, 2008, and 2009 as our development set and two test sets, respectively. 3.2 System Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with KN smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system wa</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL 2007, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="16438" citStr="Koehn, 2004" startWordPosition="2824" endWordPosition="2825">nglish and Chinese-to-English experiment, these values obtained on Arabic dev were used to obtain the results below: A was set to 8, and α was set to 0.01. (Later, we ran an experiment on Chinese-to-English with A and α tuned specifically for that language pair, but the performance for the Chinese-English system only improved by a tiny, insignificant amount). Our metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores averaged across both test sets NIST06 and NIST08 for Chinese; NIST08 and NIST09 for Arabic. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p &lt; 0.05 and p &lt; 0.01 levels, respectively. We first compare the performance of different similarity functions: cosine (COS), JensenShannon divergence (JSD) and Bhattacharyya coefficient (BC). The results are shown in Table 3. All three functions obtained improvements. Both COS and BC yield statistically significant improvements over the baseline, with BC performing better than COS by a further statistically significant margin. The Bhattacharyya coefficient</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL98,</booktitle>
<pages>768--774</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="7702" citStr="Lin, 1998" startWordPosition="1262" endWordPosition="1263"> think of several other ways of defining the vector space that might yield even better results than those reported here. Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments. 2 Vector space model adaptation Vector space models (VSMs) have been widely applied in many information retrieval and natural language processing applications. For instance, to compute the sense similarity between terms, many researchers extract features for each term from its context in a corpus, define a VSM and then apply similarity functions (Hindle, 1990; Lund and Burgess, 1996; Lin, 1998; Turney, 2001). In our experiments, we exploited the fact that the training data come from a set of subcorpora. For instance, the Chinese-English training data are made up of 14 subcorpora (see section 3 below). Suppose we have C subcorpora. The domain vector for a phrase-pair (f, e) is defined as V (f, e) =&lt; w1(f, e), ...wi(f, e), ..., wC(f, e) &gt;, (1) where wi(f, e) is a standard tf · idf weight, i.e. wi(f, e) = tfi (f, e) · idf (f, e) . (2) To avoid a bias towards longer corpora, we normalize the raw joint count ci(f, e) in the corpus psi by dividing by the maximum raw count of any 1286 ase</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING/ACL98, pages 768–774, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan L¨u</author>
<author>Jin Huang</author>
<author>Qun Liu</author>
</authors>
<title>Improving Statistical Machine Translation Performance by Training Data Selection and Optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Prague, Czech Republic.</location>
<marker>L¨u, Huang, Liu, 2007</marker>
<rawString>Yajuan L¨u, Jin Huang, and Qun Liu. 2007. Improving Statistical Machine Translation Performance by Training Data Selection and Optimization. In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods Instruments and Computers,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="7691" citStr="Lund and Burgess, 1996" startWordPosition="1258" endWordPosition="1261"> the subcorpora. One can think of several other ways of defining the vector space that might yield even better results than those reported here. Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments. 2 Vector space model adaptation Vector space models (VSMs) have been widely applied in many information retrieval and natural language processing applications. For instance, to compute the sense similarity between terms, many researchers extract features for each term from its context in a corpus, define a VSM and then apply similarity functions (Hindle, 1990; Lund and Burgess, 1996; Lin, 1998; Turney, 2001). In our experiments, we exploited the fact that the training data come from a set of subcorpora. For instance, the Chinese-English training data are made up of 14 subcorpora (see section 3 below). Suppose we have C subcorpora. The domain vector for a phrase-pair (f, e) is defined as V (f, e) =&lt; w1(f, e), ...wi(f, e), ..., wC(f, e) &gt;, (1) where wi(f, e) is a standard tf · idf weight, i.e. wi(f, e) = tfi (f, e) · idf (f, e) . (2) To avoid a bias towards longer corpora, we normalize the raw joint count ci(f, e) in the corpus psi by dividing by the maximum raw count of a</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>K. Lund and C. Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods Instruments and Computers, 28(2):203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="1944" citStr="Matsoukas et al., 2009" startWordPosition="304" endWordPosition="307">n the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. Then, for each </context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data. In</title>
<date>2010</date>
<booktitle>ACL</booktitle>
<contexts>
<context position="1750" citStr="Moore and Lewis, 2010" startWordPosition="272" endWordPosition="275">nearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance, be a vector with a dimensionality equal </context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<publisher>ACL.</publisher>
<location>Philadelphia,</location>
<contexts>
<context position="16251" citStr="Papineni et al., 2002" startWordPosition="2788" endWordPosition="2792">For details, refer to (Foster and Kuhn, 2007). The value of A and α (see Eq 4 and Section 2.1) are determined by the performance on the dev set of the Arabic-to-English system. For both Arabic-to-English and Chinese-to-English experiment, these values obtained on Arabic dev were used to obtain the results below: A was set to 8, and α was set to 0.01. (Later, we ran an experiment on Chinese-to-English with A and α tuned specifically for that language pair, but the performance for the Chinese-English system only improved by a tiny, insignificant amount). Our metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores averaged across both test sets NIST06 and NIST08 for Chinese; NIST08 and NIST09 for Arabic. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p &lt; 0.05 and p &lt; 0.01 levels, respectively. We first compare the performance of different similarity functions: cosine (COS), JensenShannon divergence (JSD) and Bhattacharyya coefficient (BC). The results are shown in Table 3. All three functions obtained improvements. Both</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron B Phillips</author>
<author>Ralf D Brown</author>
</authors>
<title>Training machine translation with a second-order taylor approximation of weighted translation instances.</title>
<date>2011</date>
<booktitle>In MT Summit</booktitle>
<contexts>
<context position="2014" citStr="Phillips and Brown, 2011" startWordPosition="316" endWordPosition="319">em trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the training data, we create a vector with </context>
</contexts>
<marker>Phillips, Brown, 2011</marker>
<rawString>Aaron B. Phillips and Ralf D. Brown. 2011. Training machine translation with a second-order taylor approximation of weighted translation instances. In MT Summit 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Investigations on largescale lightly-supervised training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In IWSLT</booktitle>
<contexts>
<context position="1608" citStr="Schwenk, 2008" startWordPosition="251" endWordPosition="252">ded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to</context>
</contexts>
<marker>Schwenk, 2008</marker>
<rawString>Holger Schwenk. 2008. Investigations on largescale lightly-supervised training for statistical machine translation. In IWSLT 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Perplexity minimization for translation model domain adaptation in statistical machine translation.</title>
<date>2012</date>
<booktitle>In EACL</booktitle>
<contexts>
<context position="2031" citStr="Sennrich, 2012" startWordPosition="320" endWordPosition="321">in data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the training data, we create a vector with features defined </context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine translation. In EACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the web for synonyms: Pmi-ir versus lsa on toefl.</title>
<date>2001</date>
<booktitle>In Twelfth European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="7717" citStr="Turney, 2001" startWordPosition="1264" endWordPosition="1265">everal other ways of defining the vector space that might yield even better results than those reported here. Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments. 2 Vector space model adaptation Vector space models (VSMs) have been widely applied in many information retrieval and natural language processing applications. For instance, to compute the sense similarity between terms, many researchers extract features for each term from its context in a corpus, define a VSM and then apply similarity functions (Hindle, 1990; Lund and Burgess, 1996; Lin, 1998; Turney, 2001). In our experiments, we exploited the fact that the training data come from a set of subcorpora. For instance, the Chinese-English training data are made up of 14 subcorpora (see section 3 below). Suppose we have C subcorpora. The domain vector for a phrase-pair (f, e) is defined as V (f, e) =&lt; w1(f, e), ...wi(f, e), ..., wC(f, e) &gt;, (1) where wi(f, e) is a standard tf · idf weight, i.e. wi(f, e) = tfi (f, e) · idf (f, e) . (2) To avoid a bias towards longer corpora, we normalize the raw joint count ci(f, e) in the corpus psi by dividing by the maximum raw count of any 1286 ase pair extracted</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney. 2001. Mining the web for synonyms: Pmi-ir versus lsa on toefl. In Twelfth European Conference on Machine Learning, page 491–502, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Transductive learning for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1574" citStr="Ueffing et al., 2007" startWordPosition="243" endWordPosition="246"> in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (</context>
</contexts>
<marker>Ueffing, Haffari, Sarkar, 2007</marker>
<rawString>Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar. 2007. Transductive learning for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), Prague, Czech Republic, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
</authors>
<title>Language model adaptation for statistical machine translation with structured query models.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING)</booktitle>
<location>Geneva,</location>
<contexts>
<context position="1684" citStr="Zhao et al., 2004" startWordPosition="260" endWordPosition="263">r instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Abstract This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This pro</context>
</contexts>
<marker>Zhao, Eck, Vogel, 2004</marker>
<rawString>Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Language model adaptation for statistical machine translation with structured query models. In Proceedings of the International Conference on Computational Linguistics (COLING) 2004, Geneva, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>