<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.110721">
<title confidence="0.985344">
Analysis of Syntax-Based Pronoun Resolution Methods
</title>
<author confidence="0.99814">
Joel R. Tetreault
</author>
<affiliation confidence="0.999759">
University of Rochester
Department of Computer Science
</affiliation>
<address confidence="0.935979">
Rochester, NY, 14627
</address>
<email confidence="0.99848">
tetreaul@cs.rochester.edu
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958375">
This paper presents a pronoun resolution algo-
rithm that adheres to the constraints and rules
of Centering Theory (Grosz et al., 1995) and
is an alternative to Brennan et al.&apos;s 1987 algo-
rithm. The advantages of this new model, the
Left-Right Centering Algorithm (LRC), lie in
its incremental processing of utterances and in
its low computational overhead. The algorithm
is compared with three other pronoun resolu-
tion methods: Hobbs&apos; syntax-based algorithm,
Strube&apos;s S-list approach, and the BFP Center-
ing algorithm. All four methods were imple-
mented in a system and tested on an annotated
subset of the Treebank corpus consisting of 2026
pronouns. The noteworthy results were that
Hobbs and LRC performed the best.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999392473684211">
The aim of this project is to develop a pro-
noun resolution algorithm which performs bet-
ter than the Brennan et al. 1987 algorithm&apos;
as a cognitive model while also performing well
empirically.
A revised algorithm (Left-Right Centering)
was motivated by the fact that the BFP al-
gorithm did not allow for incremental process-
ing of an utterance and hence of its pronouns,
and also by the fact that it occasionally im-
poses a high computational load, detracting
from its psycholinguistic plausibility. A sec-
ond motivation for the project is to remedy
the dearth of empirical results on pronoun res-
olution methods. Many small comparisons of
methods have been made, such as by Strube
(1998) and Walker (1989), but those usually
consist of statistics based on a small hand-
tested corpus. The problem with evaluating
</bodyText>
<footnote confidence="0.743705">
1Henceforth BFP
</footnote>
<bodyText confidence="0.999117357142857">
algorithms by hand is that it is time consum-
ing and difficult to process corpora that are
large enough to provide reliable, broadly based
statistics. By creating a system that can run
algorithms, one can easily and quickly analyze
large amounts of data and generate more reli-
able results. In this project, the new algorithm
is tested against three leading syntax-based pro-
noun resolution methods: Hobbs&apos; naive algo-
rithm (1977), S-list (Strube 1998), and BFP.
Section 2 presents the motivation and algo-
rithm for Left-Right Centering. In Section 3,
the results of the algorithms are presented and
then discussed in Section 4.
</bodyText>
<sectionHeader confidence="0.821196" genericHeader="method">
2 Left-Right Centering Algorithm
</sectionHeader>
<bodyText confidence="0.999877375">
Left-Right Centering (LRC) is a formalized
algorithm built upon centering theory&apos;s con-
straints and rules as detailed in Grosz et. al
(1995). The creation of the LRC Algorithm
is motivated by two drawbacks found in the
BFP method. The first is BFP&apos;s limitation as
a cognitive model since it makes no provision
for incremental resolution of pronouns (Kehler
1997). Psycholinguistic research support the
claim that listeners process utterances one word
at a time, so when they hear a pronoun they
will try to resolve it immediately. If new infor-
mation comes into play which makes the reso-
lution incorrect (such as a violation of binding
constraints), the listener will go back and find a
correct antecedent. This incremental resolution
problem also motivates Strube&apos;s S-list approach.
The second drawback to the BFP algorithm is
the computational explosion of generating and
filtering anchors. In utterances with two or
more pronouns and a Cf-list with several can-
didate antecedents for each pronoun, thousands
of anchors can easily be generated making for
a time consuming filtering phase. An exam-
</bodyText>
<page confidence="0.99777">
602
</page>
<bodyText confidence="0.977543541666667">
pie from the evaluation corpus illustrates this
problem (the italics in Un_i represent possible
antecedents for the pronouns (in italics) of Uri):
Un_1: Separately, the Federal Energy Regu-
latory Commission turned down for now a re-
quest by Northeast seeking approval of its possi-
ble purchase of PS of New Hampshire.
Un: Northeast said it would refile its request
and still hopes for an expedited review by the
FERC so that it could complete the purchase
by next summer if its bid is the one approved
by the bankruptcy court.
With four pronouns in Un, and eight possible
antecedents for each in Un_1, 4096 unique Cf-
lists are generated. In the cross-product phase,
9 possible Cb&apos;s are crossed with the 4096 Cf&apos;s,
generating 36864 anchors.
Given these drawbacks, we propose a revised
resolution algorithm that adheres to centering
constraints. It works by first searching for an
antecedent in the current utterance2, if one is
not found, then the previous Cf-lists (starting
with the previous utterance) are searched left-
to-right for an antecedent:
</bodyText>
<listItem confidence="0.988437875">
1. Preprocessing - from previous utterance:
Cb(Un_i) and Cf (Un_i) are available.
2. Process Utterance - parse and extract
incrementally from Un all references to dis-
course entities. For each pronoun do:
(a) Search for an antecedent intrasenten-
tially in Cf-partial(Un)3 that meets
feature and binding constraints.
</listItem>
<bodyText confidence="0.6885395">
If one is found proceed to the next pro-
noun within utterance. Else go to (b).
</bodyText>
<listItem confidence="0.998807">
(b) Search for an antecedent intersenten-
tially in Cf (Un_i) that meets feature
and binding constraints.
3. Create Cf - create Cf-list of Un by rank-
</listItem>
<bodyText confidence="0.9899816">
ing discourse entities of Un according to
grammatical function. Our implementa-
tion used a left-right breadth-first walk of
the parse tree to approximate sorting by
grammatical function.
</bodyText>
<footnote confidence="0.982843">
21n this project, a sentence is considered an utterance
3Cf-partial is a list of all processed discourse entities
in Um
</footnote>
<listItem confidence="0.918597">
4. Identify Cb - the backward-looking cen-
ter is the most highly ranked entity from
Cf (Un_i) realized in C f WO •
5. Identify Transition - with the Cb and Cf
resolved, use the criteria from (Brennan et
al., 1987) to assign the transition.
</listItem>
<bodyText confidence="0.999787666666667">
It should be noted that BFP makes use of
Centering Rule 2 (Grosz et al., 1995), LRC does
not use the transition generated or Rule 2 in
steps 4 and 5 since Rule 2&apos;s role in pronoun
resolution is not yet known (see Kehler 1997 for
a critique of its use by BFP).
Computational overhead is avoided since no
anchors or auxiliary data structures need to be
produced and filtered.
</bodyText>
<sectionHeader confidence="0.988076" genericHeader="method">
3 Evaluation of Algorithms
</sectionHeader>
<bodyText confidence="0.999943481481482">
All four algorithms were run on a 3900 utterance
subset of the Penn Treebank annotated corpus
(Marcus et al., 1993) provided by Charniak and
Ge (1998). The corpus consists of 195 different
newspaper articles. Sentences are fully brack-
eted and have labels that indicate word-class
and features. Because the S-list and BFP algo-
rithms do not allow resolution of quoted text,
all quoted expressions were removed from the
corpus, leaving 1696 pronouns (out of 2026) to
be resolved.
For analysis, the algorithms were broken up
into two classes. The &amp;quot;N&amp;quot; group consists of al-
gorithms that search intersententially through
all Cf-lists for an antecedent. The &amp;quot;1&amp;quot; group
consists of algorithms that can only search for
an antecedent in Cf (U,,_1). The results for the
&amp;quot;N&amp;quot; algorithms and &amp;quot;1&amp;quot; algorithms are depicted
in Figures 1 and 2 respectively.
For comparison, a baseline algorithm was cre-
ated which simply took the most recent NP (by
surface order) that met binding and feature con-
straints. This naive approach resolved 28.6 per-
cent of pronouns correctly. Clearly, all four per-
form better than the naive approach. The fol-
lowing section discusses the performance of each
algorithm.
</bodyText>
<sectionHeader confidence="0.999846" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.99992625">
The surprising result from this evaluation is
that the Hobbs algorithm, which uses the least
amount of information, actually performs the
best. The difference of six more pronouns right
</bodyText>
<page confidence="0.993842">
603
</page>
<table confidence="0.966272">
Algorithm Right % Right % Right Intra % Right Inter
Hobbs 1234 72.8 68.4 85.0
LRC-N 1228 72.4 67.8 85.2
Strube-N 1166 68.8 62.9 85.2
</table>
<figureCaption confidence="0.764266">
Figure 1: &amp;quot;N&amp;quot; algorithms: search all previous Cf lists
</figureCaption>
<table confidence="0.83720075">
Algorithm Right % Right % Right Intra % Right Inter
LRC-1 1208 71.2 68.4 80.7
Strube-1 1120 66.0 60.3 71.1
BFP 962 56.7 40.7 78.8
</table>
<figureCaption confidence="0.998451">
Figure 2: &amp;quot;1&amp;quot; algorithms: search C f (UT, _1) only
</figureCaption>
<bodyText confidence="0.999807775862069">
between LRC-N and Hobbs is statistically in-
significant so one may conclude that the new
centering algorithm is also a viable method.
Why do these algorithms perform better than
the others? First, both search for referents in-
trasententially and then intersentially. In this
corpus, over 71 % of all pronouns have intrasen-
tential referents, so clearly an algorithm that
favors the current utterance will perform bet-
ter. Second, both search their respective data
structures in a salience-first manner. Inter-
sententially, both examine previous utterances
in the same manner. LRC-N sorts the Cf-
list by grammatical function using a breadth-
first search and by moving prepended phrases
to a less salient position. While Hobbs&apos; algo-
rithm does not do the movement it still searches
its parse tree in a breadth-first manner thus
emulating the Cf-list search. Intrasententially,
Hobbs gets slightly more correct since it first
favors antecedents close to the pronoun before
searching the rest of the tree. LRC favors en-
tities near the head of the sentence under the
assumption they are more salient. The similar-
ities in intra- and intersentential evaluation are
reflected in the similarities in their percent right
for the respective categories.
Because the S-list approach incorporates both
semantics and syntax in its familiarity rank-
ing scheme, a shallow version which only uses
syntax is implemented in this study. Even
though several entities were incorrectly labeled,
the shallow S-list approach still performed quite
well, only 4 percent lower than Hobbs and LRC-
N.
The standing of the BFP algorithm should
not be too surprising given past studies. For
example, Strube (1997) had the S-list algorithm
performing at 91 percent correct on three New
York Times articles while the best version of
BFP performed at 81 percent. This ten per-
cent difference is reflected in the present eval-
uation as well. The main drawback for BFP
was its preference for intersentential resolution.
Also, BFP as formally defined does not have
an intrasentential processing mechanism. For
the purposes of the project, the LRC intrasen-
tential technique was used to resolve pronouns
that were unable to be resolved by the BFP (in-
tersentential) algorithm.
In additional experiments, Hobbs and LRC-
N were tested with quoted expressions included.
LRC used an approach similar to the one
proposed by Kamayema (1998) for analyzing
quoted expressions. Given this new approach,
70.4% of the 2026 pronouns were resolved cor-
rectly by LRC while Hobbs performed at 69.8%,
a difference of only 13 pronouns right.
</bodyText>
<sectionHeader confidence="0.998419" genericHeader="method">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999918">
This paper first presented a revised pronoun
resolution algorithm that adheres to the con-
straints of centering theory. It is inspired by
the need to remedy a lack of incremental pro-
cessing and computational issues with the BFP
algorithm. Second, the performance of LRC
was compared against three other leading pro-
noun resolution algorithms based solely on syn-
tax. The comparison of these algorithms is
</bodyText>
<page confidence="0.99773">
604
</page>
<bodyText confidence="0.99998">
significant in its own right because they have
not been previously compared, in computer-
encoded form, on a common corpus. Coding all
the algorithms allows one to quickly test them
all on a large corpus and eliminates human er-
ror, both shortcomings of hand evaluation.
Most noteworthy is the performance of Hobbs
and LRC. The Hobbs approach reveals that a
walk of the parse tree performs just as well as
salience based approaches. LRC performs just
as well as Hobbs, but the important point is
that it can be considered as a replacement for
the BFP algorithm not only in terms of perfor-
mance but in terms of modeling. In terms of
implementation, Hobbs is dependent on a pre-
cise parse tree for its analysis. If no parse tree
is available, Strube&apos;s S-list algorithm and LRC
prove more useful since grammatical function
can be approximated by using surface order.
</bodyText>
<sectionHeader confidence="0.99938" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.99998">
The next step is to test all four algorithms on
a novel or short stories. Statistics from the
Walker and Strube studies suggest that BFP
will perform better in these cases. Other future
work includes constructing a hybrid algorithm
of LRC and S-list in which entities are ranked
both by the familiarity scale and by grammati-
cal function. Research into how transitions and
the Cb can be used in a pronoun resolution al-
gorithm should also be examined. Strube and
Hahn (1996) developed a heuristic of ranking
transition pairs by cost to evaluate different Cf-
ranking schemes. Perhaps this heuristic could
be used to constrain the search for antecedents.
It is quite possible that hybrid algorithms (i.e.
using Hobbs for intrasentential resolution, LRC
for intersentential) may not produce any sig-
nificant improvement over the current systems.
If so, this might indicate that purely syntactic
methods cannot be pushed much farther, and
the upper limit reached can serve as a base line
for approaches that combine syntax and seman-
tics.
</bodyText>
<sectionHeader confidence="0.998357" genericHeader="conclusions">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999747833333333">
I am grateful to Barbara Grosz for aiding me
in the development of the LRC algorithm and
discussing centering issues. I am also grate-
ful to Donna Byron who was responsible for
much brainstorming, cross-checking of results,
and coding of the Hobbs algorithm. Special
thanks goes to Michael Strube, James Allen,
and Lenhart Schubert for their advice and
brainstorming. We would also like to thank
Charniak and Ge for the annotated, parsed
Treebank corpus which proved invaluable.
Partial support for the research reported in
this paper was provided by the National Sci-
ence Foundation under Grants No. IRI-90-
09018, IRI-94-04756 and CDA-94-01024 to Har-
vard University and also by the DARPA re-
search grant no. F30602-98-2-0133 to the Uni-
versity of Rochester.
</bodyText>
<sectionHeader confidence="0.998171" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999640222222222">
Susan E. Brennan, Marilyn W. Friedman, and
Carl J. Pollard. 1987. A centering approach
to pronouns. In Proceedings, 25th Annual
Meeting of the ACL, pages 155-162.
Niyu Ge, John Hale, and Eugene Charniak.
1998. A statistical approach to anaphora res-
olution. Proceedings of the Sixth Workshop
on Very Large Corpora.
Barbara J. Grosz, Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of discourse.
Computational Linguistics, 21(2):203-226.
Jerry R. Hobbs. 1977. Resolving pronoun ref-
erences. Lingua, 44:311-338.
Megumi Kameyama. 1986. Intrasentential cen-
tering: A case study. In Centering Theory in
Discourse.
Andrew Kehler. 1997. Current theories of cen-
tering for pronoun interpretation: A crit-
ical evaluation. Computational Linguistics,
23(3):467-475.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of english: The
penn treebank. Computational Lingusitics,
19(2):313-330.
Michael Strube and Udo Hahn. 1996. Func-
tional centering. In Association for Compu-
tational Lingusitics, pages 270-277.
Michael Strube. 1998. Never look back: An
alternative to centering. In Association for
Computational Lingusitics, pages 1251-1257.
Marilyn A. Walker. 1989. Evaluating discourse
processing algorithms. In Proceedings, 27th
Annual Meeting of the Association for Com-
puational Linguisitcs, pages 251-261.
</reference>
<page confidence="0.998736">
605
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.963690">
<title confidence="0.998541">Analysis of Syntax-Based Pronoun Resolution Methods</title>
<author confidence="0.999992">Joel R Tetreault</author>
<affiliation confidence="0.999888">University of Rochester Department of Computer Science</affiliation>
<address confidence="0.99987">Rochester, NY, 14627</address>
<email confidence="0.99957">tetreaul@cs.rochester.edu</email>
<abstract confidence="0.997882294117647">This paper presents a pronoun resolution algorithm that adheres to the constraints and rules of Centering Theory (Grosz et al., 1995) and is an alternative to Brennan et al.&apos;s 1987 algorithm. The advantages of this new model, the Left-Right Centering Algorithm (LRC), lie in its incremental processing of utterances and in its low computational overhead. The algorithm is compared with three other pronoun resolution methods: Hobbs&apos; syntax-based algorithm, Strube&apos;s S-list approach, and the BFP Centering algorithm. All four methods were implemented in a system and tested on an annotated subset of the Treebank corpus consisting of 2026 pronouns. The noteworthy results were that Hobbs and LRC performed the best.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Susan E Brennan</author>
<author>Marilyn W Friedman</author>
<author>Carl J Pollard</author>
</authors>
<title>A centering approach to pronouns.</title>
<date>1987</date>
<booktitle>In Proceedings, 25th Annual Meeting of the ACL,</booktitle>
<pages>155--162</pages>
<contexts>
<context position="1030" citStr="Brennan et al. 1987" startWordPosition="156" endWordPosition="159">Left-Right Centering Algorithm (LRC), lie in its incremental processing of utterances and in its low computational overhead. The algorithm is compared with three other pronoun resolution methods: Hobbs&apos; syntax-based algorithm, Strube&apos;s S-list approach, and the BFP Centering algorithm. All four methods were implemented in a system and tested on an annotated subset of the Treebank corpus consisting of 2026 pronouns. The noteworthy results were that Hobbs and LRC performed the best. 1 Introduction The aim of this project is to develop a pronoun resolution algorithm which performs better than the Brennan et al. 1987 algorithm&apos; as a cognitive model while also performing well empirically. A revised algorithm (Left-Right Centering) was motivated by the fact that the BFP algorithm did not allow for incremental processing of an utterance and hence of its pronouns, and also by the fact that it occasionally imposes a high computational load, detracting from its psycholinguistic plausibility. A second motivation for the project is to remedy the dearth of empirical results on pronoun resolution methods. Many small comparisons of methods have been made, such as by Strube (1998) and Walker (1989), but those usually</context>
<context position="5591" citStr="Brennan et al., 1987" startWordPosition="901" endWordPosition="904">ly in Cf (Un_i) that meets feature and binding constraints. 3. Create Cf - create Cf-list of Un by ranking discourse entities of Un according to grammatical function. Our implementation used a left-right breadth-first walk of the parse tree to approximate sorting by grammatical function. 21n this project, a sentence is considered an utterance 3Cf-partial is a list of all processed discourse entities in Um 4. Identify Cb - the backward-looking center is the most highly ranked entity from Cf (Un_i) realized in C f WO • 5. Identify Transition - with the Cb and Cf resolved, use the criteria from (Brennan et al., 1987) to assign the transition. It should be noted that BFP makes use of Centering Rule 2 (Grosz et al., 1995), LRC does not use the transition generated or Rule 2 in steps 4 and 5 since Rule 2&apos;s role in pronoun resolution is not yet known (see Kehler 1997 for a critique of its use by BFP). Computational overhead is avoided since no anchors or auxiliary data structures need to be produced and filtered. 3 Evaluation of Algorithms All four algorithms were run on a 3900 utterance subset of the Penn Treebank annotated corpus (Marcus et al., 1993) provided by Charniak and Ge (1998). The corpus consists </context>
</contexts>
<marker>Brennan, Friedman, Pollard, 1987</marker>
<rawString>Susan E. Brennan, Marilyn W. Friedman, and Carl J. Pollard. 1987. A centering approach to pronouns. In Proceedings, 25th Annual Meeting of the ACL, pages 155-162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
<author>John Hale</author>
<author>Eugene Charniak</author>
</authors>
<title>A statistical approach to anaphora resolution.</title>
<date>1998</date>
<booktitle>Proceedings of the Sixth Workshop on Very Large Corpora.</booktitle>
<marker>Ge, Hale, Charniak, 1998</marker>
<rawString>Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical approach to anaphora resolution. Proceedings of the Sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--2</pages>
<contexts>
<context position="5696" citStr="Grosz et al., 1995" startWordPosition="921" endWordPosition="924">discourse entities of Un according to grammatical function. Our implementation used a left-right breadth-first walk of the parse tree to approximate sorting by grammatical function. 21n this project, a sentence is considered an utterance 3Cf-partial is a list of all processed discourse entities in Um 4. Identify Cb - the backward-looking center is the most highly ranked entity from Cf (Un_i) realized in C f WO • 5. Identify Transition - with the Cb and Cf resolved, use the criteria from (Brennan et al., 1987) to assign the transition. It should be noted that BFP makes use of Centering Rule 2 (Grosz et al., 1995), LRC does not use the transition generated or Rule 2 in steps 4 and 5 since Rule 2&apos;s role in pronoun resolution is not yet known (see Kehler 1997 for a critique of its use by BFP). Computational overhead is avoided since no anchors or auxiliary data structures need to be produced and filtered. 3 Evaluation of Algorithms All four algorithms were run on a 3900 utterance subset of the Penn Treebank annotated corpus (Marcus et al., 1993) provided by Charniak and Ge (1998). The corpus consists of 195 different newspaper articles. Sentences are fully bracketed and have labels that indicate word-cla</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Resolving pronoun references.</title>
<date>1977</date>
<journal>Lingua,</journal>
<pages>44--311</pages>
<marker>Hobbs, 1977</marker>
<rawString>Jerry R. Hobbs. 1977. Resolving pronoun references. Lingua, 44:311-338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megumi Kameyama</author>
</authors>
<title>Intrasentential centering: A case study.</title>
<date>1986</date>
<booktitle>In Centering Theory in Discourse.</booktitle>
<marker>Kameyama, 1986</marker>
<rawString>Megumi Kameyama. 1986. Intrasentential centering: A case study. In Centering Theory in Discourse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kehler</author>
</authors>
<title>Current theories of centering for pronoun interpretation: A critical evaluation.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="2751" citStr="Kehler 1997" startWordPosition="437" endWordPosition="438">ive algorithm (1977), S-list (Strube 1998), and BFP. Section 2 presents the motivation and algorithm for Left-Right Centering. In Section 3, the results of the algorithms are presented and then discussed in Section 4. 2 Left-Right Centering Algorithm Left-Right Centering (LRC) is a formalized algorithm built upon centering theory&apos;s constraints and rules as detailed in Grosz et. al (1995). The creation of the LRC Algorithm is motivated by two drawbacks found in the BFP method. The first is BFP&apos;s limitation as a cognitive model since it makes no provision for incremental resolution of pronouns (Kehler 1997). Psycholinguistic research support the claim that listeners process utterances one word at a time, so when they hear a pronoun they will try to resolve it immediately. If new information comes into play which makes the resolution incorrect (such as a violation of binding constraints), the listener will go back and find a correct antecedent. This incremental resolution problem also motivates Strube&apos;s S-list approach. The second drawback to the BFP algorithm is the computational explosion of generating and filtering anchors. In utterances with two or more pronouns and a Cf-list with several can</context>
<context position="5842" citStr="Kehler 1997" startWordPosition="952" endWordPosition="953">ng by grammatical function. 21n this project, a sentence is considered an utterance 3Cf-partial is a list of all processed discourse entities in Um 4. Identify Cb - the backward-looking center is the most highly ranked entity from Cf (Un_i) realized in C f WO • 5. Identify Transition - with the Cb and Cf resolved, use the criteria from (Brennan et al., 1987) to assign the transition. It should be noted that BFP makes use of Centering Rule 2 (Grosz et al., 1995), LRC does not use the transition generated or Rule 2 in steps 4 and 5 since Rule 2&apos;s role in pronoun resolution is not yet known (see Kehler 1997 for a critique of its use by BFP). Computational overhead is avoided since no anchors or auxiliary data structures need to be produced and filtered. 3 Evaluation of Algorithms All four algorithms were run on a 3900 utterance subset of the Penn Treebank annotated corpus (Marcus et al., 1993) provided by Charniak and Ge (1998). The corpus consists of 195 different newspaper articles. Sentences are fully bracketed and have labels that indicate word-class and features. Because the S-list and BFP algorithms do not allow resolution of quoted text, all quoted expressions were removed from the corpus</context>
</contexts>
<marker>Kehler, 1997</marker>
<rawString>Andrew Kehler. 1997. Current theories of centering for pronoun interpretation: A critical evaluation. Computational Linguistics, 23(3):467-475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Lingusitics,</journal>
<pages>19--2</pages>
<contexts>
<context position="6134" citStr="Marcus et al., 1993" startWordPosition="999" endWordPosition="1002">n - with the Cb and Cf resolved, use the criteria from (Brennan et al., 1987) to assign the transition. It should be noted that BFP makes use of Centering Rule 2 (Grosz et al., 1995), LRC does not use the transition generated or Rule 2 in steps 4 and 5 since Rule 2&apos;s role in pronoun resolution is not yet known (see Kehler 1997 for a critique of its use by BFP). Computational overhead is avoided since no anchors or auxiliary data structures need to be produced and filtered. 3 Evaluation of Algorithms All four algorithms were run on a 3900 utterance subset of the Penn Treebank annotated corpus (Marcus et al., 1993) provided by Charniak and Ge (1998). The corpus consists of 195 different newspaper articles. Sentences are fully bracketed and have labels that indicate word-class and features. Because the S-list and BFP algorithms do not allow resolution of quoted text, all quoted expressions were removed from the corpus, leaving 1696 pronouns (out of 2026) to be resolved. For analysis, the algorithms were broken up into two classes. The &amp;quot;N&amp;quot; group consists of algorithms that search intersententially through all Cf-lists for an antecedent. The &amp;quot;1&amp;quot; group consists of algorithms that can only search for an ante</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Lingusitics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Udo Hahn</author>
</authors>
<title>Functional centering.</title>
<date>1996</date>
<booktitle>In Association for Computational Lingusitics,</booktitle>
<pages>270--277</pages>
<contexts>
<context position="12098" citStr="Strube and Hahn (1996)" startWordPosition="1984" endWordPosition="1987">s available, Strube&apos;s S-list algorithm and LRC prove more useful since grammatical function can be approximated by using surface order. 6 Future Work The next step is to test all four algorithms on a novel or short stories. Statistics from the Walker and Strube studies suggest that BFP will perform better in these cases. Other future work includes constructing a hybrid algorithm of LRC and S-list in which entities are ranked both by the familiarity scale and by grammatical function. Research into how transitions and the Cb can be used in a pronoun resolution algorithm should also be examined. Strube and Hahn (1996) developed a heuristic of ranking transition pairs by cost to evaluate different Cfranking schemes. Perhaps this heuristic could be used to constrain the search for antecedents. It is quite possible that hybrid algorithms (i.e. using Hobbs for intrasentential resolution, LRC for intersentential) may not produce any significant improvement over the current systems. If so, this might indicate that purely syntactic methods cannot be pushed much farther, and the upper limit reached can serve as a base line for approaches that combine syntax and semantics. 7 Acknowledgments I am grateful to Barbara</context>
</contexts>
<marker>Strube, Hahn, 1996</marker>
<rawString>Michael Strube and Udo Hahn. 1996. Functional centering. In Association for Computational Lingusitics, pages 270-277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
</authors>
<title>Never look back: An alternative to centering.</title>
<date>1998</date>
<booktitle>In Association for Computational Lingusitics,</booktitle>
<pages>1251--1257</pages>
<contexts>
<context position="1593" citStr="Strube (1998)" startWordPosition="250" endWordPosition="251">which performs better than the Brennan et al. 1987 algorithm&apos; as a cognitive model while also performing well empirically. A revised algorithm (Left-Right Centering) was motivated by the fact that the BFP algorithm did not allow for incremental processing of an utterance and hence of its pronouns, and also by the fact that it occasionally imposes a high computational load, detracting from its psycholinguistic plausibility. A second motivation for the project is to remedy the dearth of empirical results on pronoun resolution methods. Many small comparisons of methods have been made, such as by Strube (1998) and Walker (1989), but those usually consist of statistics based on a small handtested corpus. The problem with evaluating 1Henceforth BFP algorithms by hand is that it is time consuming and difficult to process corpora that are large enough to provide reliable, broadly based statistics. By creating a system that can run algorithms, one can easily and quickly analyze large amounts of data and generate more reliable results. In this project, the new algorithm is tested against three leading syntax-based pronoun resolution methods: Hobbs&apos; naive algorithm (1977), S-list (Strube 1998), and BFP. S</context>
</contexts>
<marker>Strube, 1998</marker>
<rawString>Michael Strube. 1998. Never look back: An alternative to centering. In Association for Computational Lingusitics, pages 1251-1257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
</authors>
<title>Evaluating discourse processing algorithms.</title>
<date>1989</date>
<booktitle>In Proceedings, 27th Annual Meeting of the Association for Compuational Linguisitcs,</booktitle>
<pages>251--261</pages>
<contexts>
<context position="1611" citStr="Walker (1989)" startWordPosition="253" endWordPosition="254">ter than the Brennan et al. 1987 algorithm&apos; as a cognitive model while also performing well empirically. A revised algorithm (Left-Right Centering) was motivated by the fact that the BFP algorithm did not allow for incremental processing of an utterance and hence of its pronouns, and also by the fact that it occasionally imposes a high computational load, detracting from its psycholinguistic plausibility. A second motivation for the project is to remedy the dearth of empirical results on pronoun resolution methods. Many small comparisons of methods have been made, such as by Strube (1998) and Walker (1989), but those usually consist of statistics based on a small handtested corpus. The problem with evaluating 1Henceforth BFP algorithms by hand is that it is time consuming and difficult to process corpora that are large enough to provide reliable, broadly based statistics. By creating a system that can run algorithms, one can easily and quickly analyze large amounts of data and generate more reliable results. In this project, the new algorithm is tested against three leading syntax-based pronoun resolution methods: Hobbs&apos; naive algorithm (1977), S-list (Strube 1998), and BFP. Section 2 presents </context>
</contexts>
<marker>Walker, 1989</marker>
<rawString>Marilyn A. Walker. 1989. Evaluating discourse processing algorithms. In Proceedings, 27th Annual Meeting of the Association for Compuational Linguisitcs, pages 251-261.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>