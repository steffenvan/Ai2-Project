<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<address confidence="0.315924">
LDA Based Similarity Modeling for Question Answering
</address>
<email confidence="0.978207">
dilek@icsi.berkeley.edu
</email>
<author confidence="0.968964">
Asli Celikyilmaz
</author>
<affiliation confidence="0.998381">
Computer Science Department
University of California, Berkeley
</affiliation>
<email confidence="0.992478">
asli@eecs.berkeley.edu
</email>
<note confidence="0.554239571428571">
Dilek Hakkani-Tur
International Computer
Science Institute
Berkeley, CA
Gokhan Tur
Speech Technology and
Research Laboratory
</note>
<address confidence="0.526912">
SRI International
Menlo Park, CA, USA
</address>
<email confidence="0.996797">
gokhan@speech.sri.com
</email>
<sectionHeader confidence="0.994757" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942785714286">
We present an exploration of generative mod-
eling for the question answering (QA) task to
rank candidate passages. We investigate La-
tent Dirichlet Allocation (LDA) models to ob-
tain ranking scores based on a novel similar-
ity measure between a natural language ques-
tion posed by the user and a candidate passage.
We construct two models each one introducing
deeper evaluations on latent characteristics of
passages together with given question. With
the new representation of topical structures on
QA datasets, using a limited amount of world
knowledge, we show improvements on perfor-
mance of a QA ranking system.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996753678571429">
Question Answering (QA) is a task of automatic
retrieval of an answer given a question. Typically
the question is linguistically processed and search
phrases are extracted, which are then used to retrieve
the candidate documents, passages or sentences.
A typical QA system has a pipeline structure start-
ing from extraction of candidate sentences to rank-
ing true answers. Some approaches to QA use
keyword-based techniques to locate candidate pas-
sages/sentences in the retrieved documents and then
filter based on the presence of the desired answer
type in candidate text. Ranking is then done using
syntactic features to characterize similarity to query.
In cases where simple question formulation is not
satisfactory, many advanced QA systems implement
more sophisticated syntactic, semantic and contex-
tual processing such as named-entity recognition
(Molla et al., 2006), coreference resolution (Vicedo
and Ferrandez, 2000), logical inferences (abduction
1
or entailment) (Harabagiu and Hickl, 2006) trans-
lation (Ma and McKeowon, 2009), etc., to improve
answer ranking. For instance, how questions, or spa-
tially constrained questions, etc., require such types
of deeper understanding of the question and the re-
trieved documents/passages.
Many studies on QA have focused on discrimina-
tive models to predict a function of matching fea-
tures between each question and candidate passage
(set of sentences), namely q/a pairs, e.g., (Ng et al.,
2001; Echihabi and Marcu, 2003; Harabagiu and
Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz
et al., 2009). Despite their success, they have some
room for improvement which are not usually raised,
e.g., they require hand engineered features; or cas-
cade features learnt separately from other modules
in a QA pipeline, thus propagating errors. The struc-
tures to be learned can become more complex than
the amount of training data, e.g., alignment, entail-
ment, translation, etc. In such cases, other source
of information, e.g., unlabeled examples, or human
prior knowledge, should be used to improve perfor-
mance. Generative modeling is a way of encoding
this additional information, providing a natural way
to use unlabeled data.
In this work, we present new similarity measures
to discover deeper relationship between q/a pairs
based on a probabilistic model. We investigate two
methods using Latent Dirichlet Allocation (LDA)
(Blei, 2003) in § 3, and hierarchical LDA (hLDA)
(Blei, 2009) in § 4 to discover hidden concepts. We
present ways of utilizing this information within a
discriminative classifier in § 5. With empirical ex-
periments in § 6, we analyze the effects of gener-
ative model outcome on a QA system. With the
new representation of conceptual structures on QA
</bodyText>
<note confidence="0.958644">
Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 1–9,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9991165">
datasets, using a limited amount of world knowl-
edge, we show performance improvements.
</bodyText>
<sectionHeader confidence="0.675111" genericHeader="introduction">
2 Background and Motivation
</sectionHeader>
<bodyText confidence="0.999910428571429">
Previous research have focused on improving mod-
ules of the QA pipeline such as question processing
(Huang et al., 2009), information retrieval (Clarke
et al., 2006), information extraction (Saggion and
Gaizauskas, 2006). Recent work on textual en-
tailment has shown improvements on QA results
(Harabagiu and Hickl, 2006), (Celikyilmaz et al.,
2009), when used for filtering and ranking answers.
They discover similarities between q/a pairs, where
the answer to a question should be entailed by the
text that supports the correctness of its answer.
In this paper, we present a ranking schema fo-
cusing on a new similarity modeling approach via
generative and discriminative methods to utilize best
features of both approaches. Combinations of dis-
criminative and generative methodologies have been
explored by several authors, e.g. (Bouchard and
Triggs, 2004; McCallum et al., 2006; Bishop and
Lasserre, 2007; Schmah et al., 2009), in many fields
such as natural language processing, speech recog-
nition, etc. In particular, the recent ”deep learning”
approaches (Weston et al., 2008) rely heavily on a
hybrid generative-discriminative approach: an un-
supervised generative learning phase followed by a
discriminative fine-tuning.
In an analogical way to the deep learning meth-
ods, we discover relations between the q/a pairs
based on the similarities on their latent topics dis-
covered via Bayesian probabilistic approach. We in-
vestigate different ways of discovering topic based
similarities following the fact that it is more likely
that the candidate passage entails given question and
contains true answer if they share similar topics.
Later we combine this information in different ways
into a discriminative classifier-based QA model.
The underlying mechanism of our similarity mod-
eling approach is Latent Dirichlet Allocation (LDA)
(Blei et al., 2003b). We argue that similarities can
be characterized better if we define a semantic simi-
larity measure based on hidden concepts (topics) on
top of lexico-syntactic features. We later extend our
similarity model using a hierarchical LDA (hLDA)
(Blei et al., 2003a) to discover latent topics that are
organized into hierarchies. A hierarchical structure
is particularly appealing to QA task than a flat LDA,
in that one can discover abstract and specific topics.
For example, discovering that baseball and football
are both contained in a more abstract class sports
can help to relate to a general topic of a question.
</bodyText>
<sectionHeader confidence="0.984772" genericHeader="method">
3 Similarity Modeling with LDA
</sectionHeader>
<bodyText confidence="0.9999937">
We assume that for a question posed by a user, the
document sets D are retrieved by a search engine
based on the query expanded from the question. Our
aim is to build a measure to characterize similar-
ities between a given question and each candidate
passage/sentence s ∈ D in the retrieved documents
based on similarities of their hidden topics. Thus,
we built bayesian probabilistic models on passage
level rather than document level to explicitly extract
their hidden topics. Moreover, the fact that there is
limited amount of retrieved documents D per ques-
tion (∼100 documents) makes it appealing to build
probabilistic models on passages in place of docu-
ments and define semantically coherent groups in
passages as latent concepts. Given window size n
sentences, we define a passage as s = (|D |− n) + 1
based on a n-sliding-window, where |D |is the to-
tal number of sentences in retrieved documents D.
There are 25+ sentences in documents, hence we ex-
tracted around 2500 passages for each question.
</bodyText>
<subsectionHeader confidence="0.97331">
3.1 LDA Model for Q/A System
</subsectionHeader>
<bodyText confidence="0.999993">
We briefly describe LDA (Blei et al., 2003b) model
as used in our QA system. A passage in retrieved
documents (document collection) is represented as a
mixture of fixed topics, with topic z getting weight
</bodyText>
<equation confidence="0.72642">
8(s)
</equation>
<bodyText confidence="0.879506857142857">
z in passage s and each topic is a distribution
over a finite vocabulary of words, with word w hav-
ing a probability O(z) win topic z. Placing symmet-
ric Dirichlet priors on 8(s) and O(z), with 8(s) ∼
Dirichlet(α) and O(z) ∼ Dirichlet(Q), where α
and Q are hyper-parameters to control the sparsity
of distributions, the generative model is given by:
</bodyText>
<equation confidence="0.9960804">
wi|zi, O(zi) wi ∼ Discrete(O(zi)), i = 1, ..., W
O(z) ∼ Dirichlet(Q), z = 1, ..., K
zi|8(si) ∼ Discrete(8(si)), i = 1,...,W
8(s) ∼ Dirichlet(α), s = 1, ..., 5
(1)
</equation>
<page confidence="0.906955">
2
</page>
<bodyText confidence="0.99896485">
where S is the number of passages discovered from
the document collection, K is the total number of
topics, W is the total number of words in the docu-
ment collection, and si and zi are the passage and the
topic of the ith word wi, respectively. Each word in
the vocabulary wi E V = {w1, ...wW} is assigned
to each latent topic variable zi=1,...,W of words.
After seeing the data, our goal is to calculate the
expected posterior probabilities ˆφ(zi)
wi of a word wi
in a candidate passage given a topic zi = k and ex-
pected posterior probability ˆθ(s) of topic mixings of
a given passage s, using the count matrices:
where nW K
wik is the count of wi in topic k, and nSK
sk
is the count of topic k in passage s. The LDA model
makes no attempt to account for the relation of topic
mixtures, i.e., topics are distributed flat, and each
passage is a distribution over all topics.
</bodyText>
<subsectionHeader confidence="0.9967275">
3.2 Degree of Similarity Between Q/A via
Topics from LDA:
</subsectionHeader>
<bodyText confidence="0.822509545454545">
We build a LDA model on the set of retrieved pas-
sages s along with a given question q and calculate
the degree of similarity DESLDA(q,s) between each
q/a pair based on two measures (Algorithm 1):
(1) simLDA
1 : To capture the lexical similarities
on hidden topics, we represent each s and q as
two probability distributions at each topic z =
k. Thus, we sample sparse unigram distributions
from each ˆφ(z) using the words in q and s. Each
sparse word given topic distribution is denoted as
</bodyText>
<equation confidence="0.7321184">
pq = p(wq|z,
(z) ˆφ(z)) with the set of words wq =
(w1, ..., w|q|) in q and ps = p(ws|z, ˆφ(z)) with the
set of words ws = (w1, ..., w|s|) in s, and z = 1...K
represent each topic.
</equation>
<bodyText confidence="0.9998856">
The sparse probability distributions per topic are
represented with only the words in q and s, and the
probabilities of the rest of the words in V are set
to zero. The W dimensional word probabilities is
the expected posteriors obtained from LDA model
</bodyText>
<equation confidence="0.732203">
(Eq.(2)), p(z) s= (ˆφ(z)
w1 , ..., ˆφ(z)
wjsj, 0, 0, ..) E (0,1)W,
pq = (ˆφ(z)
(z) w1 , ..., ˆφ(z)
wjqj, 0, 0,..) E (0,1)W .Given a
topic z, the similarity between p(qz) and p(z)
s is mea-
</equation>
<bodyText confidence="0.548829">
sured via transformed information radius (IR). We
</bodyText>
<subsectionHeader confidence="0.4223165">
Topic Proportions
Topic-Word Distributions
</subsectionHeader>
<figure confidence="0.906262">
disease
warming z1 predict
forecast cooling
temperature
s: “Global1 warming2 may rise3 incidence4 of malaria5.”
q: “How does global1 warming2 effect6 humans7?”
(a) Snapshot of Flat Topic Structure of passages s
for a question q on “global warming”.
Posterior Topic-Word Distributions
(b) Magnified view of word given topic and topic given passage
distributions showing s={w1,w2,w3,w4,w5} and q={w1,w2,w6,w7}
</figure>
<figureCaption confidence="0.998431571428571">
Figure 1: (a) The topic distributions of a passage s and a
question q obtained from LDA. Each topic zk is a distri-
bution over words (Most probable terms are illustrated).
(b) magnified view of (a) demonstrating sparse distribu-
tions over the vocabulary V, where only words in passage
s and question q get values. The passage-topic distribu-
tions are topic mixtures, θ(3) and θ(q), for s and q.
</figureCaption>
<bodyText confidence="0.983659">
first measure the divergence at each topic using IR
based on Kullback-Liebler (KL) divergence:
</bodyText>
<equation confidence="0.851885">
IR(pqz),ps(z))=KL(pqz)||pq ) 2ps )+KL(ps ||pqz) 2p(z) )
s
(3)
</equation>
<bodyText confidence="0.747015">
where, KL to . The divergence is
</bodyText>
<equation confidence="0.985611">
(pllq) _ �ZpZ g qi g
transformed into similarity measure (Manning and
Schutze, 1999):
W(p(z)
q , p(z)
s ) = 10−δIR(p(z)
q ,p(z)
s )1 (4)
</equation>
<bodyText confidence="0.976732285714286">
To measure the similarity between probability distri-
butions we opted for IR instead of commonly used
KL because with IR there is no problem with infinite
values since pq+ps
2 =� 0 if either pq =� 0 or ps =� 0,
and it is also symmetric, IR(p,q)=IR(q,p). The simi-
larity of q/a pairs on topic-word basis is the average
</bodyText>
<equation confidence="0.924199833333333">
w3 .
.
1In experiments S = 1 is used.
ˆφ(zi) = nw k +βθ(s) =
wi W WK x SK
+K
</equation>
<figure confidence="0.961183421875">
nwjk +Wβ Ej=1 nsj +Kα
(2)
nSK
sk +α
ps(z1)
p(z2)
s
.p(zx)
s
..
V
V
Posterior Passage- Topic Distributions
. w1 w2
w1 w2
.
. w1 w2
.
.
z
z
z1z2 .........zx
O(s)
z1z2 .........zx
O(q)
w3.....zK
w4
z1 z2
w
4
.
p(z1)
q
w7w6w5
w5
w7 w6
pq(z2)
. w5
w7 w6
V
.
...
p(zx)
q
V
V
w1w2w3..w5w6w7....
w1w2w3w4w5w6w7....
w1w2w3w4w5w6w7....
.
.
w3
w4
V
w1w2w3w4w5w6w7....
w1w2w3w4w5w6w7....
w1w2w3w4w5w6w7....
q :
s :
malaria sneeze
health
z2
...
zK
</figure>
<page confidence="0.934153">
3
</page>
<bodyText confidence="0.818979">
of transformed divergence over the entire K topics:
</bodyText>
<equation confidence="0.564126">
sim1DA(q,$) = K Ek=1 W(pqz=k),p� =k)) (5)
(2) simLDA
</equation>
<bodyText confidence="0.96426125">
2 : We introduce another measure based on
passage-topic mixing proportions in q and s to cap-
ture similarities between their topics using the trans-
formed IR in Eq.(4) as follows:
</bodyText>
<equation confidence="0.7850975">
simLDA
2 (q, s) = 10−IR(
</equation>
<bodyText confidence="0.996659666666667">
The �θ(q) and 00 are K-dimensional discrete topic
weights in question q and a passage s from Eq.(2).
In summary, simLDA
1 is a measure of lexical simi-
larity on topic-word level and simLDA
2 is a measure
of topical similarity on passage level. Together they
form the degree of similarity DESLDA(s, q) and are
combined as follows:
</bodyText>
<equation confidence="0.97903">
DESLDA(s,q)=simLDA
1 (q,s)*simLDA
2 (q, s) (7)
</equation>
<bodyText confidence="0.996189714285714">
Fig.1 shows sparse distributions obtained for sam-
ple q and s. Since the topics are not distributed hi-
erarchially, each topic distribution is over the entire
vocabulary of words in retrieved collection D. Fig.1
only shows the most probable words in a given topic.
Moreover, each s and q are represented as a discrete
probability distribution over all K topics.
</bodyText>
<construct confidence="0.344097">
Algorithm 1 Flat Topic-Based Similarity Model
</construct>
<listItem confidence="0.96855175">
1: Given a query q and candidate passages s E D
2: Build an LDA model for the retrieved passages.
3: for each passages s E D do
4: - Calculate sim1(q, s) using Eq.(5)
5: - Calculate sim2(q, s) using Eq.(6)
6: - Calculate degree of similarity between q and s:
7: DESLDA(q,s)=sim1(q, s) * sim2(q, s)
8: end for
</listItem>
<sectionHeader confidence="0.892262" genericHeader="method">
4 Similarity Modeling with hLDA
</sectionHeader>
<bodyText confidence="0.999119444444444">
Given a question, we discover hidden topic distribu-
tions using hLDA (Blei et al., 2003a). hLDA orga-
nizes topics into a tree of a fixed depth L (Fig.2.(a)),
as opposed to flat LDA. Each candidate passage s is
assigned to a path cs in the topic tree and each word
wi in s is assigned to a hidden topic zs at a level
l of cs. Each node is associated with a topic dis-
tribution over words. The Gibbs sampler (Griffiths
and Steyvers, 2004) alternates between choosing a
</bodyText>
<equation confidence="0.9976585">
γ (8)
p(pathnew, c|m, mc) = γ+m−1
</equation>
<bodyText confidence="0.99969475">
pathold and pathnew represent an existing and novel
(branch) path consecutively, mc is the number of
previous passages assigned to path c, m is the to-
tal number of passages seen so far, and γ is a hyper-
parameter, which controls the probability of creating
new paths. Based on this probability each node can
branch out a different number of child nodes propor-
tional to γ. The generative process for hLDA is:
</bodyText>
<listItem confidence="0.960724625">
(1) For each topic k E T, sample a distribution βk
Dirichlet(η).
(2) For each passage s in retrieved documents,
(a) Draw a path cs — nCRP(γ),
(b) Sample L-vector θs mixing weights from
Dirichlet distribution θs — Dir(α).
(c) For each word n, choose:
(i) a level zs,n|θs, (ii) a word ws,n |{zs,n, cs, β}
</listItem>
<bodyText confidence="0.9980015625">
Given passage s, θs is a vector of topic propor-
tions from L dimensional Dirichlet parameterized
by α (distribution over levels in the tree.) The
nth word of s is sampled by first choosing a level
zs,n = l from the discrete distribution θs with prob-
ability θs,l. Dirichlet parameter η and γ control the
size of tree effecting the number of topics. Large
values of η favor more topics (Blei et al., 2003a).
Model Learning: Gibbs sampling is a common
method to fit the hLDA models. The aim is to ob-
tain the following samples from the posterior of: (i)
the latent tree T, (ii) the level assignment z for all
words, (iii) the path assignments c for all passages
conditioned on the observed words w.
Given the assignment of words w to levels z and
assignments of passages to paths c, the expected
</bodyText>
<equation confidence="0.85248">
g(q), g(s)) (6)
</equation>
<bodyText confidence="0.999940615384616">
new path for each passage through the tree and as-
signing each word in each passage to a topic along
that path. The structure of tree is learnt along with
the topics using a nested Chinese restaurant process
(nCRP) (Blei et al., 2003a), which is used as a prior.
The nCRP is a stochastic process, which assigns
probability distributions to infinitely branching and
deep trees. nCRP specifies a distribution of words in
passages into paths in an L-level tree. Assignments
of passages to paths are sampled sequentially: The
first passage takes the initial L-level path, starting
with a single branch tree. Next, mth subsequent pas-
sage is assigned to a path drawn from distribution:
</bodyText>
<equation confidence="0.940267">
p(pathold, c|m, mc) = m�
γ+m−1
</equation>
<page confidence="0.917497">
4
</page>
<figure confidence="0.9969535">
(a) Snapshot of Hierarchical Topic Structure of (b) Magnified view of sample path c [z1,z2,z3] showing
passages s for a question q on “global warming”. s={w1,w2,w3,w4,w5} and q={w1,w2,w6,w7}
</figure>
<figureCaption confidence="0.99857175">
Figure 2: (a) A sample 3-level tree using hLDA. Each passage is associated with a path c through the hierarchy, where
each node z3 = l is associated with a distribution over terms (Most probable terms are illustrated). (b) magnified view
of a path (darker nodes) in (a). Distribution of words in given passage s and a question (q) using sub-vocabulary of
words at each level topic vl. Discrete distributions on the left are topic mixtures for each passage, pzq and pzg.
</figureCaption>
<figure confidence="0.986187578313253">
.
z2
.
.
.
.
.
Posterior Topic
Distributions
z
z1 z2 z3
p zq
pzs
Posterior Topic-Word Distributions
candidate s question q
w1
vz1
w6 w1w5w6 .... w1w5w6 ....
p(w |z1, c )
s,1 s
..
w2
w2 w7 ....
w7
w5 .... w5 ....
.
..
.
.
.
z3
..
.
.
.
.
.
.
.
.
z1 wS
p(w |z1, c )
q,1 q
vz2
s: “Global1 warming2 may rise3 incidence4 of malaria5.”
q: “How does global1 warming2 effect6 humans7?”
p(w |z3, c )
s,3 s
p(w |z3, c )
q,3 q
.wS
. .
change
zj
global
research
human
incidence
level:2
temperature
warming
disease
z2
forecast
health
predict
zK-j
...
level:3
slow
malaria
sneeze
z3
zs
zK
siberia
middle-east
starving
level:j
vz2
.
.
vz1
</figure>
<equation confidence="0.764942111111111">
p(w |z2, c )
s,2 s
vz3
w2 w7 ....
p(w |z2, c )
q,2 q
vz3
z
z1 z2 z3
</equation>
<bodyText confidence="0.999382">
posterior probability of a particular word w at a
given topic z=l of a path c=c is proportional to the
number of times w was generated by that topic:
</bodyText>
<equation confidence="0.691366">
p(w|z, c, w, ,q) a n(z=l,c=c,w=w) + (9)
</equation>
<bodyText confidence="0.999016333333333">
Similarly, posterior probability of a particular topic
z in a given passage s is proportional to number of
times z was generated by that passage:
</bodyText>
<equation confidence="0.824938">
p(z|s, z, c, α) a n(c=cc,z=l) + α (10)
</equation>
<bodyText confidence="0.999540666666667">
n(.) is the count of elements of an array satisfying
the condition. Posterior probabilities are normalized
with total counts and their hyperparameters.
</bodyText>
<subsectionHeader confidence="0.990033">
4.1 Tree-Based Similarity Model
</subsectionHeader>
<bodyText confidence="0.999265052631579">
The hLDA constructs a hierarchical tree structure
of candidate passages and given question, each of
which are represented by a path in the tree, and each
path can be shared by many passages/question. The
assumption is that passages sharing the same path
should be more similar to each other because they
share the same topics (Fig.2). Moreover, if a path
includes a question, then other passages on that path
are more likely to entail the question than passages
on the other paths. Thus, the similarity of a can-
didate passage s to a question q sharing the same
path is a measure of semantic similarity (Algorithm
2). Given a question, we build an hLDA model on
retrieved passages. Let cq be the path for a given
q. We identify the candidate passages that share the
same path with q, M = Is E D|cs = cq}. Given
path cq and M, we calculate the degree of similarity
DEShLDA(s, q) between q and s by calculating two
similarity measures:
</bodyText>
<equation confidence="0.968619">
(1) simhLDA
1 : We define two sparse (discrete) uni-
</equation>
<bodyText confidence="0.999969090909091">
gram distributions for candidate s and question q at
each node l to define lexical similarities on topic
level. The distributions are over a vocabulary of
words generated by the topic at that node, vl C
V . Note that, in hLDA the topic distributions at
each level of a path is sampled from the vocabu-
lary of passages sharing that path, contrary to LDA,
in which the topics are over entire vocabulary of
words. This enables defining a similarity measure
on specific topics. Given wq = {w1, ..., wjqj }, let
wq,l C wq be the set of words in q that are gener-
ated from topic zq at level l on path cq. The discrete
unigram distribution pql = p(wq,l|zq = l, cq, vl) rep-
resents the probability over all words vl assigned to
topic zq at level l, by sampling only for words in
wq,l. The probability of the rest of the words in vl are
set 0. Similarly, ps,l = p(ws,l|zs, cq, vl) is the proba-
bility of words ws in s extracted from the same topic
(see Fig.2.b). The word probabilities in pq,l and ps,l
are obtained using Eq. (9) and then normalized.
The similarity between pq,l and ps,l at each level
is obtained by transformed information radius:
</bodyText>
<equation confidence="0.99652">
δ-IR j(P9,j,P3,j)
WC9,1(Pq,i,ps,l) = 10 (11)
</equation>
<page confidence="0.939467">
5
</page>
<bodyText confidence="0.966771">
where the IRcq,l(pq,l, ps,l) is calculated as in Eq.(3)
this time for pq,l and ps,l (δ = 1). Finally simhLDA
</bodyText>
<equation confidence="0.974007833333333">
1 is
obtained by averaging Eq.(11) over different levels:
EL
simhLDA
1 (q, s) = 1 l�� Wcq,l(pq,l,ps,l) � l (12)
L
</equation>
<bodyText confidence="0.998332333333333">
The similarity between pq,l and ps,l is weighted by
the level l because the similarity should be rewarded
if there is a specific word overlap at child nodes.
</bodyText>
<listItem confidence="0.899337416666667">
Algorithm 2 Tree-Based Similarity Model
1: Given candidate passages s and question q.
2: Build hLDA on set of s and q to obtain tree T.
3: Find path cq on tree T and candidate passages
4: on path cq, i.e., M = Is E D|cs = cq}.
5: for candidate passage s E M do
6: Find DEShDLA(q, s) = simhLDA * simhLDA
2
7: using Eq.(12) and Eq.(13)
8: end for
9: if s E/ M, then DEShDLA(q, s)=0.
(2) simhLDA
</listItem>
<bodyText confidence="0.935023285714286">
2 : We introduce a concept-base mea-
sure based on passage-topic mixing proportions to
calculate the topical similarities between q and s.
We calculate the topic proportions of q and s, rep-
resented by pzq = p(zq|cq) and pzs = p(zs|cq) via
Eq.(10). The similarity between the distributions is
then measured with transformed IR as in Eq.(11) by:
</bodyText>
<equation confidence="0.9379775">
simhLDA
2 (q,s) = 10−IRcq(pzq,pzs) (13)
</equation>
<bodyText confidence="0.999284333333333">
In summary, simhLDA 1provides information about
the similarity between q and s based on topic-word
distributions, and simhLDA
2 is the similarity between
the weights of their topics. The two measures are
combined to calculate the degree of similarity:
</bodyText>
<equation confidence="0.538728">
DEShLDA(q,s)=simhLDA1(q,s)*sim2LDA (q, s) (14)
</equation>
<bodyText confidence="0.9966676">
Fig.2.b depicts a sample path illustrating sparse uni-
gram distributions of a q and s at each level and their
topic proportions, pzq, and pzs. The candidate pas-
sages that are not on the same path as the question
are assigned DEShLDA(s, q) = 0.
</bodyText>
<sectionHeader confidence="0.996286" genericHeader="method">
5 Discriminitive Model for QA
</sectionHeader>
<bodyText confidence="0.999404416666667">
In (Celikyilmaz et al., 2009), the QA task is posed
as a textual entailment problem using lexical and se-
mantic features to characterize similarities between
q/a pairs. A discriminative classifier is built to pre-
dict the existence of an answer in candidate sen-
tences. Although they show that semi-supervised
methods improve accuracy of their QA model un-
der limited amount of labeled data, they suggest that
with sufficient number of labeled data, supervised
methods outperform semi-supervised methods. We
argue that there is a lot to discover from unlabeled
text to help improve QA accuracy. Thus, we pro-
pose using Bayesian probabilistic models. First we
briefly present the baseline method:
Baseline: We use the supervised classifier
model presented in (Celikyilmaz et al., 2009) as
our baseline QA model. Their datasets, provided in
http://www.eecs.berkeley.edu/∼asli/asliPublish.html,
are q/a pairs from TREC task. They define each
q/a pair as a d dimensional feature vector xi E Rd
characterizing entailment information between
them. They build a support vector machine (SVM)
(Drucker et al., 1997) classifier model to predict the
entailment scores for q/a pairs.
To characterize the similarity between q/a pairs
they use: (i) features represented by similarities
between semantic components, e.g., subject, ob-
ject, verb, or named-entity types discovered in q/a
pairs, and (ii) lexical features represented by lexico-
syntactic alignments such as n-gram word overlaps
or cause and entailment relations discovered from
WordNet (Miller, 1995). For a given question q, they
rank the candidate sentences s based on predicted
entailment scores from the classifier, TE(q, s).
We extend the baseline by using the degree of
similarity between question and candidate passage
obtained from LDA, DESLDA(q, s), as well as hLDA
DEShLDA(q, s), and evaluate different models:
Model M-1: Degree of Similarity as Rank
Scores: In this model, the QA is based on a fully
generative approach in which the similarity mea-
sures of Eq.(7) in §3 and Eq.(14) in §4 are used to
obtain ranking scores. We build two separate mod-
els, M-1.1 using DESLDA(q, s), and M-1.2 using
DEShLDA(q, s) as rank scores and measure accu-
racy by re-ranking candidate passages accordingly.
Given a question, this model requires training indi-
vidual LDA and hLDA models.
</bodyText>
<listItem confidence="0.411305666666667">
Model M-2: Interpolation Between
Classifier-Based Entailment Scores and Genera-
tive Model Scores: In this model, the underlying
</listItem>
<page confidence="0.998583">
6
</page>
<bodyText confidence="0.9990125">
mechanism of QA is the discriminative method
presented in baseline. We linearly combine the
probabilistic similarity scores from generative
models, DES scores in M-1, with the baseline
scores. We build two additional models to calculate
the final rank scores; M-2.1 using:
</bodyText>
<equation confidence="0.993669666666667">
score(sjq) = a*TE(q, s)+b*DESLDA(q, s) (15)
and M-2.2 using:
score(sjq) = a*TE(q, s)+b*DEShLDA(q, s) (16)
</equation>
<bodyText confidence="0.9980446">
where 0 &lt; a &lt; 1 and 0 &lt; b &lt; 1 and a + b = 1.
We find the optimum a* and b* based on the valida-
tion experiments on training dataset. The candidate
sentences are re-ranked based on these scores.
Model M-3: Degree of Similarity as Entail-
ment Features: Another way to incorporate the la-
tent information into the discriminitive QA model
is to utilize the latent similarities as explanatory
variables in the classifier model. Particularly we
build M-3.1 by using simLDA
</bodyText>
<sectionHeader confidence="0.445818" genericHeader="method">
1 , simLDA
</sectionHeader>
<bodyText confidence="0.9894105">
2 as well as
DESLDA(q, s) as additional features for the SVM, on
top of the the existing features used in (Celikyilmaz
et al., 2009). Similarly, we build M-3.2 by using
simhLDA 1 , simhLDA 2
tional features to the SVM classifier model to predict
entailment scores. This model requires building two
new SVM classifier models with the new features.
</bodyText>
<sectionHeader confidence="0.998573" genericHeader="evaluation">
6 Experiments and Discussions
</sectionHeader>
<bodyText confidence="0.9997925625">
We demonstrate the results of our experiments on
exploration of the effect of different generative mod-
els presented in §5 on TREC QA datasets.
We performed experiments on the datasets used in
(Celikyilmaz et al., 2009). Their train dataset com-
poses of a set of 1449 questions from TREC-99-
03. For each question, the 5 top-ranked candidate
sentences are extracted from a large newswire cor-
pora (Acquaint corpus) through a search engine, i.e.,
Lucene 2. The q/a pairs are labeled as true/false de-
pending on the containment of the true answer string
in retrieved passages. Additionally, to calculate the
LDA and hLDA similarity measures for each candi-
date passage, we also extract around 100 documents
in the same fashion using Lucene and identify pas-
sages to build the probabilistic models. We calculate
</bodyText>
<footnote confidence="0.823686">
2http://lucene.apache.org/java/
</footnote>
<bodyText confidence="0.846524">
the probabilistic similarities, i.e., simLDA
</bodyText>
<equation confidence="0.9680745">
1 , simLDA
2 ,
simhLDA
1 , simhLDA
</equation>
<bodyText confidence="0.999580770833333">
2 , and the degree of similarity val-
ues, i.e., DESLDA(q, s) and DEShLDA(q, s) for
each of the 5 top-ranked candidate sentences in
training dataset at inference time. Around 7200 q/a
pairs are compiled accordingly.
The provided testing data contains a set of 202
questions from TREC2004 along with 20 candidate
sentences for each question, which are labeled as
true/false. To calculate the similarities for the 20
candidate sentences, we extract around 100 docu-
ments for each question and build LDA and hLDA
models. 4037 testing q/a pairs are compiled.
We report the retrieval performance of our mod-
els in terms of Mean Reciprocal Rank (MRR), top
1 (Top1) and top 5 prediction accuracies (Top5)
(Voorhees, 2004). We performed parameter opti-
mization during training based on prediction ac-
curacy to find the best C = {10−2,.., 102} and
F = {2−2, .., 23} for RBF kernel SVM. For the
LDA models we present the results with 10 top-
ics. In hLDA models, we use four levels for the
tree construction and set the topic Dirichlet hyper-
parameters in decreasing order of levels at q =
11.0, 0.75, 0.5, 0.251 to encourage as many terms in
the mid to low levels as the higher levels in the hi-
erarchy, for a better comparison between q/a pairs.
The nested CRP parameter -y is fixed at 1.0. We
evaluated n-sliding-window size of sentences in se-
quence, n = 11, 3, 51, to compile candidate pas-
sages for probabilistic models (Table 1). The output
scores for SVM models are normalized to [0,1].
* As our baseline (in §5), we consider supervised
classifier based QA presented in (Celikyilmaz et al.,
2009). The baseline MRR on TREC-2004 dataset is
MRR=%67.6, Top1=%58, Top5=%82.2.
* The results of the new models on testing dataset
are reported in Table 1. Incorporating the genera-
tive model output to the classifier model as input
features, i.e., M-3.1 and M-3-2, performs con-
sistently better than the rest of the models and the
baseline, where MRR result is statistically signifi-
cant based on t-test statistics (at p = 0.95 confi-
dence level). When combined with the textual en-
tailment scores, i.e., M-2.1 and M-2.2, they pro-
vide a slightly better ranking, a minor improvement
compared to the baseline. However, using the gen-
erative model outcome as sole ranking scores in
as well as DEShLDA(q, s) as addi-
</bodyText>
<page confidence="0.998808">
7
</page>
<table confidence="0.9997795">
Window-size 1-window 3-window 5-window
MRR categories MRR Top1 Top5 MRR Top1 Top5 MRR Top1 Top5
Models M-1.1 (with LDA) 42.7 30.2 64.4 42.1 30.2 64.4 42.1 30.2 64.4
M-1.1 (with hLDA) 55.8 45.5 71.0 55.8 45.5 71.0 54.9 45.5 71.0
M-2.1 (with LDA) 66.2 55.1 82.2 65.2 54.5 80.7 65.2 54.5 80.7
M-2.2 (with hLDA) 68.2 58.4 82.2 67.6 58.0 82.2 67.4 58.0 81.6
M-3.1 (with LDA) 68.0 61.0 82.2 68.0 58.1 82.2 68.2 58.1 82.2
M-3.2 (with hLDA) 68.4 63.4 82.2 68.3 61.0 82.2 68.3 61.0 82.2
</table>
<tableCaption confidence="0.938669333333333">
Table 1: The MRR results of the models presented in §5 on testing dataset (TREC 2004) using different window sizes
of candidate passages. The statistically significant model results in each corresponding MRR category are bolded.
Baseline MRR=%67.6, Top1=%58, Top5=%82.2.
</tableCaption>
<bodyText confidence="0.999316823529412">
M-1.1 and M-1.2 do not reveal as good results as
the other models, suggesting room for improvement.
* In Table 1, Top1 MRR yields better improve-
ment compared to the other two MRRs, especially
for models M-3.1 and M-3.2. This suggests that
the probabilistic model outcome rewards the can-
didate sentences containing the true answer by es-
timating higher scores and moves them up to the
higher levels of the rank.
* The analysis of different passage sizes suggest
that the 1-window size yields best results and no sig-
nificant performance improvement is observed when
window size is increased. Thus, the similarity be-
tween q/a pairs can be better explained if the candi-
date passage contains less redundant sentences.
* The fact that the similarity scores obtained from
the hLDA models are significantly better than LDA
models in Table 1 indicates an important property
of hierarchal topic models. With the hLDA specific
and generic topics can be identified on different lev-
els of the hierarchy. Two candidate passages can
be characterized with different abstract and specific
topics (Fig. 2) enabling representation of better fea-
tures to identify similarity measures between them.
Whereas in LDA, each candidate passage has a pro-
portion in each topic. Rewarding the similarities on
specific topics with the hLDA models help improve
the QA rank performance.
* In M-3.1 and M-3.2 we use probabilistic sim-
ilarities and DES as inputs to the classifier. In Table
2 we show the individual effects of these features on
the MRR testing performance along with other lexi-
cal and semantic features of the baseline. Although
the effect of each feature is comparable, the DESLDA
</bodyText>
<table confidence="0.999789">
Features M-3.1 Features M-3.1
sim1LDA 67.7 sim1hLDA 67.8
sim2LDA 67.5 sim2hLDA 68.0
DESLDA 67.9 DEShLDA 68.1
</table>
<tableCaption confidence="0.988537">
Table 2: The MRR results of the similarity measures on
testing dataset (TREC 2004) when used as input features.
</tableCaption>
<bodyText confidence="0.860537">
and DEShLDA features reveal slightly better results.
</bodyText>
<sectionHeader confidence="0.960866" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999843565217391">
In this paper we introduced a set of methods based
on Latent Dirichlet Allocation (LDA) to character-
ize the similarity between the question and the can-
didate passages, which are used as ranking scores.
The results of our experiments suggest that extract-
ing information from hidden concepts improves the
results of a classifier-based QA model.
Although unlabeled data exploration through
probabilistic graphical models can help to improve
information extraction, devising a machinery with
suitable generative models for the given natural lan-
guage task is a challenge. This work helps with
such understanding via extensive simulations and
puts forward and confirms a hypothesis explaining
the mechanisms behind the effect of unsupervised
pre-training for the final discriminant learning task.
In the future, we would like to further evaluate
the models presented in this paper for larger datasets
and for different tasks such as question paraphrase
retrieval or query expansion. Moreover, we would
like to enhance the similarities with other semantic
components extracted from questions such as ques-
tion topic and question focus.
</bodyText>
<page confidence="0.996968">
8
</page>
<sectionHeader confidence="0.993779" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999483733333334">
C. M. Bishop and J. Lasserre. Generative or dis-
criminative? getting the best of both worlds. In In
Bayesian Statistics 8, Bernardo, J. M. et al. (Eds),
Oxford University Press, 2007.
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
Hierarchical topic models and the nested chinese
restaurant process. In In Neural Information Pro-
cessing Systems [NIPS], 2003a.
D. M. Blei, A. Ng, and M. Jordan. Latent dirichlet
allocation. In Jrnl. Machine Learning Research,
3:993-1022, 2003b.
G. Bouchard and B. Triggs. The tradeoff between
generative and discriminative classifiers. In Proc.
of COMPSTAT’04, 2004.
A. Celikyilmaz, M. Thint, and Z. Huang. Graph-
based semi-supervised learning for question an-
swering. In Proc. of the ACL-2009, 2009.
C.L.A. Clarke, G. V. Cormack, R. T. Lynam, and
E. L. Terra. Question answering by passage se-
lection. In In: Advances in open domain question
answering, Strzalkowski, and Harabagiu (Eds.),
pages 259–283. Springer, 2006.
H. Drucker, C.J.C. Burger, L. Kaufman, A. Smola,
and V. Vapnik. Support vector regression ma-
chines. In NIPS 9, 1997.
A. Echihabi and D. Marcu. A noisy-channel ap-
proach to question answering. In ACL-2003,
2003.
T. Griffiths and M. Steyvers. Finding scientific top-
ics. In PNAS, 101(Supp. 1): 5228-5235, 2004.
S. Harabagiu and A. Hickl. Methods for using tex-
tual entailment in open-domain question answer-
ing. In In Proc. of ACL-2006, pages 905–912,
2006.
Z. Huang, M. Thint, and A. Celikyilmaz. Investiga-
tion of question classifier in question answering.
In In EMNLP’09, 2009.
W.-Y. Ma and K. McKeowon. Where’s the verb?
correcting machine translation during question
answering. In In ACL-IJCNLP’09, 2009.
C. Manning and H. Schutze. Foundations of statis-
tical natural language processing. In MIT Press.
Cambridge, MA, 1999.
A. McCallum, C. Pal, G. Druck, and
X. Wang. Multi-conditional learning: Gen-
erative/discriminative training for clustering and
classification. In AAAI 2006, 2006.
G.A. Miller. Wordnet: A lexical database for en-
glish. In ACM, 1995.
D. Molla, M.V. Zaanen, and D. Smith. Named en-
tity recognition for question answering. In In
ALTW2006, 2006.
H.T. Ng, J.L.P. Kwan, and Y. Xia. Question answer-
ing using a large text database: A machine learn-
ing approach. In EMNLP-2001, 2001.
H. Saggion and R. Gaizauskas. Experiments in pas-
sage selection and answer extraction for ques-
tion answering. In In: Advances in open domain
question answering, Strzalkowski, and Harabagiu
(Eds.), pages 291–302. Springer, 2006.
T. Schmah, G. E Hinton, R. Zemel, S. L. Small,
and S. Strother. Generative versus discriminative
training of rbms for classification of fmri images.
In Proc. NIPS 2009, 2009.
Dan Shen and Dietrich Klakow. Exploring correla-
tion of dependency relation paths for answer ex-
traction. In Proc. of ACL-2006, 2006.
J.L. Vicedo and A. Ferrandez. Applying anaphora
resolution to question answering and information
retrieval systems. In In LNCS, volume 1846,
pages 344–355, 2000.
Ellen M. Voorhees. Overview of trec2004 question
answering track. 2004.
J. Weston, F. Rattle, and R. Collobert. Deep learning
via semi-supervised embedding. In ICML, 2008.
</reference>
<page confidence="0.997107">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.175171">
<title confidence="0.942217">LDA Based Similarity Modeling for Question Answering</title>
<email confidence="0.983381">dilek@icsi.berkeley.edu</email>
<author confidence="0.756699">Asli</author>
<affiliation confidence="0.999913">Computer Science University of California,</affiliation>
<email confidence="0.999707">asli@eecs.berkeley.edu</email>
<author confidence="0.655732">Dilek</author>
<affiliation confidence="0.923223">International Science</affiliation>
<address confidence="0.908425">Berkeley, CA</address>
<affiliation confidence="0.83442675">Gokhan Speech Technology Research SRI</affiliation>
<address confidence="0.999358">Menlo Park, CA, USA</address>
<email confidence="0.999908">gokhan@speech.sri.com</email>
<abstract confidence="0.9996612">We present an exploration of generative modeling for the question answering (QA) task to rank candidate passages. We investigate Latent Dirichlet Allocation (LDA) models to obtain ranking scores based on a novel similarity measure between a natural language question posed by the user and a candidate passage. We construct two models each one introducing deeper evaluations on latent characteristics of passages together with given question. With the new representation of topical structures on QA datasets, using a limited amount of world knowledge, we show improvements on performance of a QA ranking system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C M Bishop</author>
<author>J Lasserre</author>
</authors>
<title>Generative or discriminative? getting the best of both worlds.</title>
<date>2007</date>
<booktitle>In In Bayesian Statistics 8,</booktitle>
<publisher>University Press,</publisher>
<location>Oxford</location>
<contexts>
<context position="4893" citStr="Bishop and Lasserre, 2007" startWordPosition="734" endWordPosition="737">n QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al., 2009), when used for filtering and ranking answers. They discover similarities between q/a pairs, where the answer to a question should be entailed by the text that supports the correctness of its answer. In this paper, we present a ranking schema focusing on a new similarity modeling approach via generative and discriminative methods to utilize best features of both approaches. Combinations of discriminative and generative methodologies have been explored by several authors, e.g. (Bouchard and Triggs, 2004; McCallum et al., 2006; Bishop and Lasserre, 2007; Schmah et al., 2009), in many fields such as natural language processing, speech recognition, etc. In particular, the recent ”deep learning” approaches (Weston et al., 2008) rely heavily on a hybrid generative-discriminative approach: an unsupervised generative learning phase followed by a discriminative fine-tuning. In an analogical way to the deep learning methods, we discover relations between the q/a pairs based on the similarities on their latent topics discovered via Bayesian probabilistic approach. We investigate different ways of discovering topic based similarities following the fac</context>
</contexts>
<marker>Bishop, Lasserre, 2007</marker>
<rawString>C. M. Bishop and J. Lasserre. Generative or discriminative? getting the best of both worlds. In In Bayesian Statistics 8, Bernardo, J. M. et al. (Eds), Oxford University Press, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>T Griffiths</author>
<author>M Jordan</author>
<author>J Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested chinese restaurant process.</title>
<date>2003</date>
<booktitle>In In Neural Information Processing Systems [NIPS],</booktitle>
<contexts>
<context position="5840" citStr="Blei et al., 2003" startWordPosition="876" endWordPosition="879">logical way to the deep learning methods, we discover relations between the q/a pairs based on the similarities on their latent topics discovered via Bayesian probabilistic approach. We investigate different ways of discovering topic based similarities following the fact that it is more likely that the candidate passage entails given question and contains true answer if they share similar topics. Later we combine this information in different ways into a discriminative classifier-based QA model. The underlying mechanism of our similarity modeling approach is Latent Dirichlet Allocation (LDA) (Blei et al., 2003b). We argue that similarities can be characterized better if we define a semantic similarity measure based on hidden concepts (topics) on top of lexico-syntactic features. We later extend our similarity model using a hierarchical LDA (hLDA) (Blei et al., 2003a) to discover latent topics that are organized into hierarchies. A hierarchical structure is particularly appealing to QA task than a flat LDA, in that one can discover abstract and specific topics. For example, discovering that baseball and football are both contained in a more abstract class sports can help to relate to a general topic</context>
<context position="7561" citStr="Blei et al., 2003" startWordPosition="1165" endWordPosition="1168">their hidden topics. Moreover, the fact that there is limited amount of retrieved documents D per question (∼100 documents) makes it appealing to build probabilistic models on passages in place of documents and define semantically coherent groups in passages as latent concepts. Given window size n sentences, we define a passage as s = (|D |− n) + 1 based on a n-sliding-window, where |D |is the total number of sentences in retrieved documents D. There are 25+ sentences in documents, hence we extracted around 2500 passages for each question. 3.1 LDA Model for Q/A System We briefly describe LDA (Blei et al., 2003b) model as used in our QA system. A passage in retrieved documents (document collection) is represented as a mixture of fixed topics, with topic z getting weight 8(s) z in passage s and each topic is a distribution over a finite vocabulary of words, with word w having a probability O(z) win topic z. Placing symmetric Dirichlet priors on 8(s) and O(z), with 8(s) ∼ Dirichlet(α) and O(z) ∼ Dirichlet(Q), where α and Q are hyper-parameters to control the sparsity of distributions, the generative model is given by: wi|zi, O(zi) wi ∼ Discrete(O(zi)), i = 1, ..., W O(z) ∼ Dirichlet(Q), z = 1, ..., K </context>
<context position="13852" citStr="Blei et al., 2003" startWordPosition="2308" endWordPosition="2311">hows the most probable words in a given topic. Moreover, each s and q are represented as a discrete probability distribution over all K topics. Algorithm 1 Flat Topic-Based Similarity Model 1: Given a query q and candidate passages s E D 2: Build an LDA model for the retrieved passages. 3: for each passages s E D do 4: - Calculate sim1(q, s) using Eq.(5) 5: - Calculate sim2(q, s) using Eq.(6) 6: - Calculate degree of similarity between q and s: 7: DESLDA(q,s)=sim1(q, s) * sim2(q, s) 8: end for 4 Similarity Modeling with hLDA Given a question, we discover hidden topic distributions using hLDA (Blei et al., 2003a). hLDA organizes topics into a tree of a fixed depth L (Fig.2.(a)), as opposed to flat LDA. Each candidate passage s is assigned to a path cs in the topic tree and each word wi in s is assigned to a hidden topic zs at a level l of cs. Each node is associated with a topic distribution over words. The Gibbs sampler (Griffiths and Steyvers, 2004) alternates between choosing a γ (8) p(pathnew, c|m, mc) = γ+m−1 pathold and pathnew represent an existing and novel (branch) path consecutively, mc is the number of previous passages assigned to path c, m is the total number of passages seen so far, an</context>
<context position="15375" citStr="Blei et al., 2003" startWordPosition="2590" endWordPosition="2593"> in retrieved documents, (a) Draw a path cs — nCRP(γ), (b) Sample L-vector θs mixing weights from Dirichlet distribution θs — Dir(α). (c) For each word n, choose: (i) a level zs,n|θs, (ii) a word ws,n |{zs,n, cs, β} Given passage s, θs is a vector of topic proportions from L dimensional Dirichlet parameterized by α (distribution over levels in the tree.) The nth word of s is sampled by first choosing a level zs,n = l from the discrete distribution θs with probability θs,l. Dirichlet parameter η and γ control the size of tree effecting the number of topics. Large values of η favor more topics (Blei et al., 2003a). Model Learning: Gibbs sampling is a common method to fit the hLDA models. The aim is to obtain the following samples from the posterior of: (i) the latent tree T, (ii) the level assignment z for all words, (iii) the path assignments c for all passages conditioned on the observed words w. Given the assignment of words w to levels z and assignments of passages to paths c, the expected g(q), g(s)) (6) new path for each passage through the tree and assigning each word in each passage to a topic along that path. The structure of tree is learnt along with the topics using a nested Chinese restau</context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2003</marker>
<rawString>D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. Hierarchical topic models and the nested chinese restaurant process. In In Neural Information Processing Systems [NIPS], 2003a.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D M Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<booktitle>In Jrnl. Machine Learning Research,</booktitle>
<pages>3--993</pages>
<marker>Blei, Ng, Jordan, </marker>
<rawString>D. M. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. In Jrnl. Machine Learning Research, 3:993-1022, 2003b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouchard</author>
<author>B Triggs</author>
</authors>
<title>The tradeoff between generative and discriminative classifiers.</title>
<date>2004</date>
<booktitle>In Proc. of COMPSTAT’04,</booktitle>
<contexts>
<context position="4843" citStr="Bouchard and Triggs, 2004" startWordPosition="726" endWordPosition="729">ork on textual entailment has shown improvements on QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al., 2009), when used for filtering and ranking answers. They discover similarities between q/a pairs, where the answer to a question should be entailed by the text that supports the correctness of its answer. In this paper, we present a ranking schema focusing on a new similarity modeling approach via generative and discriminative methods to utilize best features of both approaches. Combinations of discriminative and generative methodologies have been explored by several authors, e.g. (Bouchard and Triggs, 2004; McCallum et al., 2006; Bishop and Lasserre, 2007; Schmah et al., 2009), in many fields such as natural language processing, speech recognition, etc. In particular, the recent ”deep learning” approaches (Weston et al., 2008) rely heavily on a hybrid generative-discriminative approach: an unsupervised generative learning phase followed by a discriminative fine-tuning. In an analogical way to the deep learning methods, we discover relations between the q/a pairs based on the similarities on their latent topics discovered via Bayesian probabilistic approach. We investigate different ways of disc</context>
</contexts>
<marker>Bouchard, Triggs, 2004</marker>
<rawString>G. Bouchard and B. Triggs. The tradeoff between generative and discriminative classifiers. In Proc. of COMPSTAT’04, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Celikyilmaz</author>
<author>M Thint</author>
<author>Z Huang</author>
</authors>
<title>Graphbased semi-supervised learning for question answering.</title>
<date>2009</date>
<booktitle>In Proc. of the ACL-2009,</booktitle>
<contexts>
<context position="2547" citStr="Celikyilmaz et al., 2009" startWordPosition="370" endWordPosition="373">00), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). Despite their success, they have some room for improvement which are not usually raised, e.g., they require hand engineered features; or cascade features learnt separately from other modules in a QA pipeline, thus propagating errors. The structures to be learned can become more complex than the amount of training data, e.g., alignment, entailment, translation, etc. In such cases, other source of information, e.g., unlabeled examples, or human prior knowledge, should be used to improve performance. Generative modeling is a way of encoding this additional information, providing a natural way t</context>
<context position="4336" citStr="Celikyilmaz et al., 2009" startWordPosition="648" endWordPosition="651"> QA Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics datasets, using a limited amount of world knowledge, we show performance improvements. 2 Background and Motivation Previous research have focused on improving modules of the QA pipeline such as question processing (Huang et al., 2009), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006). Recent work on textual entailment has shown improvements on QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al., 2009), when used for filtering and ranking answers. They discover similarities between q/a pairs, where the answer to a question should be entailed by the text that supports the correctness of its answer. In this paper, we present a ranking schema focusing on a new similarity modeling approach via generative and discriminative methods to utilize best features of both approaches. Combinations of discriminative and generative methodologies have been explored by several authors, e.g. (Bouchard and Triggs, 2004; McCallum et al., 2006; Bishop and Lasserre, 2007; Schmah et al., 2009), in many fields such</context>
<context position="22329" citStr="Celikyilmaz et al., 2009" startWordPosition="3858" endWordPosition="3861">= 10−IRcq(pzq,pzs) (13) In summary, simhLDA 1provides information about the similarity between q and s based on topic-word distributions, and simhLDA 2 is the similarity between the weights of their topics. The two measures are combined to calculate the degree of similarity: DEShLDA(q,s)=simhLDA1(q,s)*sim2LDA (q, s) (14) Fig.2.b depicts a sample path illustrating sparse unigram distributions of a q and s at each level and their topic proportions, pzq, and pzs. The candidate passages that are not on the same path as the question are assigned DEShLDA(s, q) = 0. 5 Discriminitive Model for QA In (Celikyilmaz et al., 2009), the QA task is posed as a textual entailment problem using lexical and semantic features to characterize similarities between q/a pairs. A discriminative classifier is built to predict the existence of an answer in candidate sentences. Although they show that semi-supervised methods improve accuracy of their QA model under limited amount of labeled data, they suggest that with sufficient number of labeled data, supervised methods outperform semi-supervised methods. We argue that there is a lot to discover from unlabeled text to help improve QA accuracy. Thus, we propose using Bayesian probab</context>
<context position="25736" citStr="Celikyilmaz et al., 2009" startWordPosition="4402" endWordPosition="4405">EShLDA(q, s) (16) where 0 &lt; a &lt; 1 and 0 &lt; b &lt; 1 and a + b = 1. We find the optimum a* and b* based on the validation experiments on training dataset. The candidate sentences are re-ranked based on these scores. Model M-3: Degree of Similarity as Entailment Features: Another way to incorporate the latent information into the discriminitive QA model is to utilize the latent similarities as explanatory variables in the classifier model. Particularly we build M-3.1 by using simLDA 1 , simLDA 2 as well as DESLDA(q, s) as additional features for the SVM, on top of the the existing features used in (Celikyilmaz et al., 2009). Similarly, we build M-3.2 by using simhLDA 1 , simhLDA 2 tional features to the SVM classifier model to predict entailment scores. This model requires building two new SVM classifier models with the new features. 6 Experiments and Discussions We demonstrate the results of our experiments on exploration of the effect of different generative models presented in §5 on TREC QA datasets. We performed experiments on the datasets used in (Celikyilmaz et al., 2009). Their train dataset composes of a set of 1449 questions from TREC-99- 03. For each question, the 5 top-ranked candidate sentences are e</context>
<context position="28478" citStr="Celikyilmaz et al., 2009" startWordPosition="4865" endWordPosition="4868">levels for the tree construction and set the topic Dirichlet hyperparameters in decreasing order of levels at q = 11.0, 0.75, 0.5, 0.251 to encourage as many terms in the mid to low levels as the higher levels in the hierarchy, for a better comparison between q/a pairs. The nested CRP parameter -y is fixed at 1.0. We evaluated n-sliding-window size of sentences in sequence, n = 11, 3, 51, to compile candidate passages for probabilistic models (Table 1). The output scores for SVM models are normalized to [0,1]. * As our baseline (in §5), we consider supervised classifier based QA presented in (Celikyilmaz et al., 2009). The baseline MRR on TREC-2004 dataset is MRR=%67.6, Top1=%58, Top5=%82.2. * The results of the new models on testing dataset are reported in Table 1. Incorporating the generative model output to the classifier model as input features, i.e., M-3.1 and M-3-2, performs consistently better than the rest of the models and the baseline, where MRR result is statistically significant based on t-test statistics (at p = 0.95 confidence level). When combined with the textual entailment scores, i.e., M-2.1 and M-2.2, they provide a slightly better ranking, a minor improvement compared to the baseline. H</context>
</contexts>
<marker>Celikyilmaz, Thint, Huang, 2009</marker>
<rawString>A. Celikyilmaz, M. Thint, and Z. Huang. Graphbased semi-supervised learning for question answering. In Proc. of the ACL-2009, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L A Clarke</author>
<author>G V Cormack</author>
<author>R T Lynam</author>
<author>E L Terra</author>
</authors>
<title>Question answering by passage selection.</title>
<date>2006</date>
<booktitle>In In: Advances in open domain question answering, Strzalkowski, and Harabagiu (Eds.),</booktitle>
<pages>259--283</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="4153" citStr="Clarke et al., 2006" startWordPosition="622" endWordPosition="625">e classifier in § 5. With empirical experiments in § 6, we analyze the effects of generative model outcome on a QA system. With the new representation of conceptual structures on QA Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics datasets, using a limited amount of world knowledge, we show performance improvements. 2 Background and Motivation Previous research have focused on improving modules of the QA pipeline such as question processing (Huang et al., 2009), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006). Recent work on textual entailment has shown improvements on QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al., 2009), when used for filtering and ranking answers. They discover similarities between q/a pairs, where the answer to a question should be entailed by the text that supports the correctness of its answer. In this paper, we present a ranking schema focusing on a new similarity modeling approach via generative and discriminative methods to utilize best features of both approaches. Combinations of discriminative and gener</context>
</contexts>
<marker>Clarke, Cormack, Lynam, Terra, 2006</marker>
<rawString>C.L.A. Clarke, G. V. Cormack, R. T. Lynam, and E. L. Terra. Question answering by passage selection. In In: Advances in open domain question answering, Strzalkowski, and Harabagiu (Eds.), pages 259–283. Springer, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Drucker</author>
<author>C J C Burger</author>
<author>L Kaufman</author>
<author>A Smola</author>
<author>V Vapnik</author>
</authors>
<title>Support vector regression machines.</title>
<date>1997</date>
<booktitle>In NIPS 9,</booktitle>
<contexts>
<context position="23401" citStr="Drucker et al., 1997" startWordPosition="4021" endWordPosition="4024">-supervised methods. We argue that there is a lot to discover from unlabeled text to help improve QA accuracy. Thus, we propose using Bayesian probabilistic models. First we briefly present the baseline method: Baseline: We use the supervised classifier model presented in (Celikyilmaz et al., 2009) as our baseline QA model. Their datasets, provided in http://www.eecs.berkeley.edu/∼asli/asliPublish.html, are q/a pairs from TREC task. They define each q/a pair as a d dimensional feature vector xi E Rd characterizing entailment information between them. They build a support vector machine (SVM) (Drucker et al., 1997) classifier model to predict the entailment scores for q/a pairs. To characterize the similarity between q/a pairs they use: (i) features represented by similarities between semantic components, e.g., subject, object, verb, or named-entity types discovered in q/a pairs, and (ii) lexical features represented by lexicosyntactic alignments such as n-gram word overlaps or cause and entailment relations discovered from WordNet (Miller, 1995). For a given question q, they rank the candidate sentences s based on predicted entailment scores from the classifier, TE(q, s). We extend the baseline by usin</context>
</contexts>
<marker>Drucker, Burger, Kaufman, Smola, Vapnik, 1997</marker>
<rawString>H. Drucker, C.J.C. Burger, L. Kaufman, A. Smola, and V. Vapnik. Support vector regression machines. In NIPS 9, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Echihabi</author>
<author>D Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In ACL-2003,</booktitle>
<contexts>
<context position="2470" citStr="Echihabi and Marcu, 2003" startWordPosition="358" endWordPosition="361">ition (Molla et al., 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). Despite their success, they have some room for improvement which are not usually raised, e.g., they require hand engineered features; or cascade features learnt separately from other modules in a QA pipeline, thus propagating errors. The structures to be learned can become more complex than the amount of training data, e.g., alignment, entailment, translation, etc. In such cases, other source of information, e.g., unlabeled examples, or human prior knowledge, should be used to improve performance. Generative modelin</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>A. Echihabi and D. Marcu. A noisy-channel approach to question answering. In ACL-2003, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In PNAS, 101(Supp.</booktitle>
<volume>1</volume>
<pages>5228--5235</pages>
<contexts>
<context position="14199" citStr="Griffiths and Steyvers, 2004" startWordPosition="2377" endWordPosition="2380">m1(q, s) using Eq.(5) 5: - Calculate sim2(q, s) using Eq.(6) 6: - Calculate degree of similarity between q and s: 7: DESLDA(q,s)=sim1(q, s) * sim2(q, s) 8: end for 4 Similarity Modeling with hLDA Given a question, we discover hidden topic distributions using hLDA (Blei et al., 2003a). hLDA organizes topics into a tree of a fixed depth L (Fig.2.(a)), as opposed to flat LDA. Each candidate passage s is assigned to a path cs in the topic tree and each word wi in s is assigned to a hidden topic zs at a level l of cs. Each node is associated with a topic distribution over words. The Gibbs sampler (Griffiths and Steyvers, 2004) alternates between choosing a γ (8) p(pathnew, c|m, mc) = γ+m−1 pathold and pathnew represent an existing and novel (branch) path consecutively, mc is the number of previous passages assigned to path c, m is the total number of passages seen so far, and γ is a hyperparameter, which controls the probability of creating new paths. Based on this probability each node can branch out a different number of child nodes proportional to γ. The generative process for hLDA is: (1) For each topic k E T, sample a distribution βk Dirichlet(η). (2) For each passage s in retrieved documents, (a) Draw a path </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. Griffiths and M. Steyvers. Finding scientific topics. In PNAS, 101(Supp. 1): 5228-5235, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
</authors>
<title>Methods for using textual entailment in open-domain question answering. In</title>
<date>2006</date>
<booktitle>In Proc. of ACL-2006,</booktitle>
<pages>905--912</pages>
<contexts>
<context position="2001" citStr="Harabagiu and Hickl, 2006" startWordPosition="284" endWordPosition="287"> approaches to QA use keyword-based techniques to locate candidate passages/sentences in the retrieved documents and then filter based on the presence of the desired answer type in candidate text. Ranking is then done using syntactic features to characterize similarity to query. In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al., 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). Despite their success, they have some room for impro</context>
<context position="4308" citStr="Harabagiu and Hickl, 2006" startWordPosition="644" endWordPosition="647">n of conceptual structures on QA Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics datasets, using a limited amount of world knowledge, we show performance improvements. 2 Background and Motivation Previous research have focused on improving modules of the QA pipeline such as question processing (Huang et al., 2009), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006). Recent work on textual entailment has shown improvements on QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al., 2009), when used for filtering and ranking answers. They discover similarities between q/a pairs, where the answer to a question should be entailed by the text that supports the correctness of its answer. In this paper, we present a ranking schema focusing on a new similarity modeling approach via generative and discriminative methods to utilize best features of both approaches. Combinations of discriminative and generative methodologies have been explored by several authors, e.g. (Bouchard and Triggs, 2004; McCallum et al., 2006; Bishop and Lasserre, 2007; Schmah et al.</context>
</contexts>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>S. Harabagiu and A. Hickl. Methods for using textual entailment in open-domain question answering. In In Proc. of ACL-2006, pages 905–912, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Huang</author>
<author>M Thint</author>
<author>A Celikyilmaz</author>
</authors>
<title>Investigation of question classifier in question answering.</title>
<date>2009</date>
<booktitle>In In EMNLP’09,</booktitle>
<contexts>
<context position="4108" citStr="Huang et al., 2009" startWordPosition="616" endWordPosition="619">zing this information within a discriminative classifier in § 5. With empirical experiments in § 6, we analyze the effects of generative model outcome on a QA system. With the new representation of conceptual structures on QA Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics datasets, using a limited amount of world knowledge, we show performance improvements. 2 Background and Motivation Previous research have focused on improving modules of the QA pipeline such as question processing (Huang et al., 2009), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006). Recent work on textual entailment has shown improvements on QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al., 2009), when used for filtering and ranking answers. They discover similarities between q/a pairs, where the answer to a question should be entailed by the text that supports the correctness of its answer. In this paper, we present a ranking schema focusing on a new similarity modeling approach via generative and discriminative methods to utilize best features of both approac</context>
</contexts>
<marker>Huang, Thint, Celikyilmaz, 2009</marker>
<rawString>Z. Huang, M. Thint, and A. Celikyilmaz. Investigation of question classifier in question answering. In In EMNLP’09, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W-Y Ma</author>
<author>K McKeowon</author>
</authors>
<title>Where’s the verb? correcting machine translation during question answering.</title>
<date>2009</date>
<booktitle>In In ACL-IJCNLP’09,</booktitle>
<contexts>
<context position="2037" citStr="Ma and McKeowon, 2009" startWordPosition="290" endWordPosition="293">niques to locate candidate passages/sentences in the retrieved documents and then filter based on the presence of the desired answer type in candidate text. Ranking is then done using syntactic features to characterize similarity to query. In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al., 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). Despite their success, they have some room for improvement which are not usually raised,</context>
</contexts>
<marker>Ma, McKeowon, 2009</marker>
<rawString>W.-Y. Ma and K. McKeowon. Where’s the verb? correcting machine translation during question answering. In In ACL-IJCNLP’09, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundations of statistical natural language processing. In</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="11452" citStr="Manning and Schutze, 1999" startWordPosition="1861" endWordPosition="1864"> a passage s and a question q obtained from LDA. Each topic zk is a distribution over words (Most probable terms are illustrated). (b) magnified view of (a) demonstrating sparse distributions over the vocabulary V, where only words in passage s and question q get values. The passage-topic distributions are topic mixtures, θ(3) and θ(q), for s and q. first measure the divergence at each topic using IR based on Kullback-Liebler (KL) divergence: IR(pqz),ps(z))=KL(pqz)||pq ) 2ps )+KL(ps ||pqz) 2p(z) ) s (3) where, KL to . The divergence is (pllq) _ �ZpZ g qi g transformed into similarity measure (Manning and Schutze, 1999): W(p(z) q , p(z) s ) = 10−δIR(p(z) q ,p(z) s )1 (4) To measure the similarity between probability distributions we opted for IR instead of commonly used KL because with IR there is no problem with infinite values since pq+ps 2 =� 0 if either pq =� 0 or ps =� 0, and it is also symmetric, IR(p,q)=IR(q,p). The similarity of q/a pairs on topic-word basis is the average w3 . . 1In experiments S = 1 is used. ˆφ(zi) = nw k +βθ(s) = wi W WK x SK +K nwjk +Wβ Ej=1 nsj +Kα (2) nSK sk +α ps(z1) p(z2) s .p(zx) s .. V V Posterior Passage- Topic Distributions . w1 w2 w1 w2 . . w1 w2 . . z z z1z2 .........zx</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>C. Manning and H. Schutze. Foundations of statistical natural language processing. In MIT Press. Cambridge, MA, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>C Pal</author>
<author>G Druck</author>
<author>X Wang</author>
</authors>
<title>Multi-conditional learning: Generative/discriminative training for clustering and classification.</title>
<date>2006</date>
<booktitle>In AAAI</booktitle>
<contexts>
<context position="4866" citStr="McCallum et al., 2006" startWordPosition="730" endWordPosition="733">as shown improvements on QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al., 2009), when used for filtering and ranking answers. They discover similarities between q/a pairs, where the answer to a question should be entailed by the text that supports the correctness of its answer. In this paper, we present a ranking schema focusing on a new similarity modeling approach via generative and discriminative methods to utilize best features of both approaches. Combinations of discriminative and generative methodologies have been explored by several authors, e.g. (Bouchard and Triggs, 2004; McCallum et al., 2006; Bishop and Lasserre, 2007; Schmah et al., 2009), in many fields such as natural language processing, speech recognition, etc. In particular, the recent ”deep learning” approaches (Weston et al., 2008) rely heavily on a hybrid generative-discriminative approach: an unsupervised generative learning phase followed by a discriminative fine-tuning. In an analogical way to the deep learning methods, we discover relations between the q/a pairs based on the similarities on their latent topics discovered via Bayesian probabilistic approach. We investigate different ways of discovering topic based sim</context>
</contexts>
<marker>McCallum, Pal, Druck, Wang, 2006</marker>
<rawString>A. McCallum, C. Pal, G. Druck, and X. Wang. Multi-conditional learning: Generative/discriminative training for clustering and classification. In AAAI 2006, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<booktitle>In ACM,</booktitle>
<contexts>
<context position="23841" citStr="Miller, 1995" startWordPosition="4086" endWordPosition="4087"> each q/a pair as a d dimensional feature vector xi E Rd characterizing entailment information between them. They build a support vector machine (SVM) (Drucker et al., 1997) classifier model to predict the entailment scores for q/a pairs. To characterize the similarity between q/a pairs they use: (i) features represented by similarities between semantic components, e.g., subject, object, verb, or named-entity types discovered in q/a pairs, and (ii) lexical features represented by lexicosyntactic alignments such as n-gram word overlaps or cause and entailment relations discovered from WordNet (Miller, 1995). For a given question q, they rank the candidate sentences s based on predicted entailment scores from the classifier, TE(q, s). We extend the baseline by using the degree of similarity between question and candidate passage obtained from LDA, DESLDA(q, s), as well as hLDA DEShLDA(q, s), and evaluate different models: Model M-1: Degree of Similarity as Rank Scores: In this model, the QA is based on a fully generative approach in which the similarity measures of Eq.(7) in §3 and Eq.(14) in §4 are used to obtain ranking scores. We build two separate models, M-1.1 using DESLDA(q, s), and M-1.2 u</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G.A. Miller. Wordnet: A lexical database for english. In ACM, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Molla</author>
<author>M V Zaanen</author>
<author>D Smith</author>
</authors>
<title>Named entity recognition for question answering. In</title>
<date>2006</date>
<booktitle>In ALTW2006,</booktitle>
<contexts>
<context position="1872" citStr="Molla et al., 2006" startWordPosition="268" endWordPosition="271">A typical QA system has a pipeline structure starting from extraction of candidate sentences to ranking true answers. Some approaches to QA use keyword-based techniques to locate candidate passages/sentences in the retrieved documents and then filter based on the presence of the desired answer type in candidate text. Ranking is then done using syntactic features to characterize similarity to query. In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al., 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; </context>
</contexts>
<marker>Molla, Zaanen, Smith, 2006</marker>
<rawString>D. Molla, M.V. Zaanen, and D. Smith. Named entity recognition for question answering. In In ALTW2006, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>J L P Kwan</author>
<author>Y Xia</author>
</authors>
<title>Question answering using a large text database: A machine learning approach.</title>
<date>2001</date>
<booktitle>In EMNLP-2001,</booktitle>
<contexts>
<context position="2444" citStr="Ng et al., 2001" startWordPosition="354" endWordPosition="357">med-entity recognition (Molla et al., 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). Despite their success, they have some room for improvement which are not usually raised, e.g., they require hand engineered features; or cascade features learnt separately from other modules in a QA pipeline, thus propagating errors. The structures to be learned can become more complex than the amount of training data, e.g., alignment, entailment, translation, etc. In such cases, other source of information, e.g., unlabeled examples, or human prior knowledge, should be used to improve perfo</context>
</contexts>
<marker>Ng, Kwan, Xia, 2001</marker>
<rawString>H.T. Ng, J.L.P. Kwan, and Y. Xia. Question answering using a large text database: A machine learning approach. In EMNLP-2001, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>R Gaizauskas</author>
</authors>
<title>Experiments in passage selection and answer extraction for question answering. In In:</title>
<date>2006</date>
<booktitle>Advances in open domain question answering, Strzalkowski, and Harabagiu (Eds.),</booktitle>
<pages>291--302</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="4208" citStr="Saggion and Gaizauskas, 2006" startWordPosition="628" endWordPosition="631">s in § 6, we analyze the effects of generative model outcome on a QA system. With the new representation of conceptual structures on QA Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics datasets, using a limited amount of world knowledge, we show performance improvements. 2 Background and Motivation Previous research have focused on improving modules of the QA pipeline such as question processing (Huang et al., 2009), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006). Recent work on textual entailment has shown improvements on QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al., 2009), when used for filtering and ranking answers. They discover similarities between q/a pairs, where the answer to a question should be entailed by the text that supports the correctness of its answer. In this paper, we present a ranking schema focusing on a new similarity modeling approach via generative and discriminative methods to utilize best features of both approaches. Combinations of discriminative and generative methodologies have been explored by several autho</context>
</contexts>
<marker>Saggion, Gaizauskas, 2006</marker>
<rawString>H. Saggion and R. Gaizauskas. Experiments in passage selection and answer extraction for question answering. In In: Advances in open domain question answering, Strzalkowski, and Harabagiu (Eds.), pages 291–302. Springer, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Schmah</author>
<author>G E Hinton</author>
<author>R Zemel</author>
<author>S L Small</author>
<author>S Strother</author>
</authors>
<title>Generative versus discriminative training of rbms for classification of fmri images.</title>
<date>2009</date>
<booktitle>In Proc. NIPS</booktitle>
<contexts>
<context position="4915" citStr="Schmah et al., 2009" startWordPosition="738" endWordPosition="741"> Hickl, 2006), (Celikyilmaz et al., 2009), when used for filtering and ranking answers. They discover similarities between q/a pairs, where the answer to a question should be entailed by the text that supports the correctness of its answer. In this paper, we present a ranking schema focusing on a new similarity modeling approach via generative and discriminative methods to utilize best features of both approaches. Combinations of discriminative and generative methodologies have been explored by several authors, e.g. (Bouchard and Triggs, 2004; McCallum et al., 2006; Bishop and Lasserre, 2007; Schmah et al., 2009), in many fields such as natural language processing, speech recognition, etc. In particular, the recent ”deep learning” approaches (Weston et al., 2008) rely heavily on a hybrid generative-discriminative approach: an unsupervised generative learning phase followed by a discriminative fine-tuning. In an analogical way to the deep learning methods, we discover relations between the q/a pairs based on the similarities on their latent topics discovered via Bayesian probabilistic approach. We investigate different ways of discovering topic based similarities following the fact that it is more like</context>
</contexts>
<marker>Schmah, Hinton, Zemel, Small, Strother, 2009</marker>
<rawString>T. Schmah, G. E Hinton, R. Zemel, S. L. Small, and S. Strother. Generative versus discriminative training of rbms for classification of fmri images. In Proc. NIPS 2009, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Dietrich Klakow</author>
</authors>
<title>Exploring correlation of dependency relation paths for answer extraction.</title>
<date>2006</date>
<booktitle>In Proc. of ACL-2006,</booktitle>
<contexts>
<context position="2520" citStr="Shen and Klakow, 2006" startWordPosition="366" endWordPosition="369">icedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). Despite their success, they have some room for improvement which are not usually raised, e.g., they require hand engineered features; or cascade features learnt separately from other modules in a QA pipeline, thus propagating errors. The structures to be learned can become more complex than the amount of training data, e.g., alignment, entailment, translation, etc. In such cases, other source of information, e.g., unlabeled examples, or human prior knowledge, should be used to improve performance. Generative modeling is a way of encoding this additional information</context>
</contexts>
<marker>Shen, Klakow, 2006</marker>
<rawString>Dan Shen and Dietrich Klakow. Exploring correlation of dependency relation paths for answer extraction. In Proc. of ACL-2006, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Vicedo</author>
<author>A Ferrandez</author>
</authors>
<title>Applying anaphora resolution to question answering and information retrieval systems.</title>
<date>2000</date>
<booktitle>In In LNCS,</booktitle>
<volume>1846</volume>
<pages>344--355</pages>
<contexts>
<context position="1925" citStr="Vicedo and Ferrandez, 2000" startWordPosition="274" endWordPosition="277">starting from extraction of candidate sentences to ranking true answers. Some approaches to QA use keyword-based techniques to locate candidate passages/sentences in the retrieved documents and then filter based on the presence of the desired answer type in candidate text. Ranking is then done using syntactic features to characterize similarity to query. In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al., 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Cel</context>
</contexts>
<marker>Vicedo, Ferrandez, 2000</marker>
<rawString>J.L. Vicedo and A. Ferrandez. Applying anaphora resolution to question answering and information retrieval systems. In In LNCS, volume 1846, pages 344–355, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of trec2004 question answering track.</title>
<date>2004</date>
<contexts>
<context position="27606" citStr="Voorhees, 2004" startWordPosition="4708" endWordPosition="4709"> top-ranked candidate sentences in training dataset at inference time. Around 7200 q/a pairs are compiled accordingly. The provided testing data contains a set of 202 questions from TREC2004 along with 20 candidate sentences for each question, which are labeled as true/false. To calculate the similarities for the 20 candidate sentences, we extract around 100 documents for each question and build LDA and hLDA models. 4037 testing q/a pairs are compiled. We report the retrieval performance of our models in terms of Mean Reciprocal Rank (MRR), top 1 (Top1) and top 5 prediction accuracies (Top5) (Voorhees, 2004). We performed parameter optimization during training based on prediction accuracy to find the best C = {10−2,.., 102} and F = {2−2, .., 23} for RBF kernel SVM. For the LDA models we present the results with 10 topics. In hLDA models, we use four levels for the tree construction and set the topic Dirichlet hyperparameters in decreasing order of levels at q = 11.0, 0.75, 0.5, 0.251 to encourage as many terms in the mid to low levels as the higher levels in the hierarchy, for a better comparison between q/a pairs. The nested CRP parameter -y is fixed at 1.0. We evaluated n-sliding-window size of</context>
</contexts>
<marker>Voorhees, 2004</marker>
<rawString>Ellen M. Voorhees. Overview of trec2004 question answering track. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weston</author>
<author>F Rattle</author>
<author>R Collobert</author>
</authors>
<title>Deep learning via semi-supervised embedding.</title>
<date>2008</date>
<booktitle>In ICML,</booktitle>
<contexts>
<context position="5068" citStr="Weston et al., 2008" startWordPosition="761" endWordPosition="764"> a question should be entailed by the text that supports the correctness of its answer. In this paper, we present a ranking schema focusing on a new similarity modeling approach via generative and discriminative methods to utilize best features of both approaches. Combinations of discriminative and generative methodologies have been explored by several authors, e.g. (Bouchard and Triggs, 2004; McCallum et al., 2006; Bishop and Lasserre, 2007; Schmah et al., 2009), in many fields such as natural language processing, speech recognition, etc. In particular, the recent ”deep learning” approaches (Weston et al., 2008) rely heavily on a hybrid generative-discriminative approach: an unsupervised generative learning phase followed by a discriminative fine-tuning. In an analogical way to the deep learning methods, we discover relations between the q/a pairs based on the similarities on their latent topics discovered via Bayesian probabilistic approach. We investigate different ways of discovering topic based similarities following the fact that it is more likely that the candidate passage entails given question and contains true answer if they share similar topics. Later we combine this information in differen</context>
</contexts>
<marker>Weston, Rattle, Collobert, 2008</marker>
<rawString>J. Weston, F. Rattle, and R. Collobert. Deep learning via semi-supervised embedding. In ICML, 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>