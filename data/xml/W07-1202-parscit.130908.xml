<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004408">
<title confidence="0.998713">
Perceptron Training for a Wide-Coverage Lexicalized-Grammar Parser
</title>
<author confidence="0.997968">
Stephen Clark
</author>
<affiliation confidence="0.997994">
Oxford University Computing Laboratory
</affiliation>
<address confidence="0.9905765">
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
</address>
<email confidence="0.999181">
stephen.clark@comlab.ox.ac.uk
</email>
<author confidence="0.988961">
James R. Curran
</author>
<affiliation confidence="0.9924675">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.473047">
NSW 2006, Australia
</address>
<email confidence="0.997782">
james@it.usyd.edu.au
</email>
<sectionHeader confidence="0.99563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986136363636">
This paper investigates perceptron training
for a wide-coverage CCG parser and com-
pares the perceptron with a log-linear model.
The CCG parser uses a phrase-structure pars-
ing model and dynamic programming in the
form of the Viterbi algorithm to find the
highest scoring derivation. The difficulty in
using the perceptron for a phrase-structure
parsing model is the need for an efficient de-
coder. We exploit the lexicalized nature of
CCG by using a finite-state supertagger to
do much of the parsing work, resulting in
a highly efficient decoder. The perceptron
performs as well as the log-linear model; it
trains in a few hours on a single machine;
and it requires only a few hundred MB of
RAM for practical training compared to 20
GB for the log-linear model. We also inves-
tigate the order in which the training exam-
ples are presented to the online perceptron
learner, and find that order does not signifi-
cantly affect the results.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997875875">
A recent development in data-driven parsing is the
use of discriminative training methods (Riezler et
al., 2002; Taskar et al., 2004; Collins and Roark,
2004; Turian and Melamed, 2006). One popular ap-
proach is to use a log-linear parsing model and max-
imise the conditional likelihood function (Johnson
et al., 1999; Riezler et al., 2002; Clark and Curran,
2004b; Malouf and van Noord, 2004; Miyao and
</bodyText>
<page confidence="0.957434">
9
</page>
<bodyText confidence="0.999685205882353">
Tsujii, 2005). Maximising the likelihood involves
calculating feature expectations, which is computa-
tionally expensive. Dynamic programming (DP) in
the form of the inside-outside algorithm can be used
to calculate the expectations, if the features are suf-
ficiently local (Miyao and Tsujii, 2002); however,
the memory requirements can be prohibitive, es-
pecially for automatically extracted, wide-coverage
grammars. In Clark and Curran (2004b) we use clus-
ter computing resources to solve this problem.
Parsing research has also begun to adopt discrim-
inative methods from the Machine Learning litera-
ture, such as the perceptron (Freund and Schapire,
1999; Collins and Roark, 2004) and the large-
margin methods underlying Support Vector Ma-
chines (Taskar et al., 2004; McDonald, 2006).
Parser training involves decoding in an iterative pro-
cess, updating the model parameters so that the de-
coder performs better on the training data, accord-
ing to some training criterion. Hence, for efficient
training, these methods require an efficient decoder;
in fact, for methods like the perceptron, the update
procedure is so trivial that the training algorithm es-
sentially is decoding.
This paper describes a decoder for a lexicalized-
grammar parser which is efficient enough for prac-
tical discriminative training. We use a lexicalized
phrase-structure parser, the CCG parser of Clark and
Curran (2004b), together with a DP-based decoder.
The key idea is to exploit the properties of lexi-
calized grammars by using a finite-state supertag-
ger prior to parsing (Bangalore and Joshi, 1999;
Clark and Curran, 2004a). The decoder still uses
the CKY algorithm, so the worst case complexity of
</bodyText>
<note confidence="0.924257">
Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 9–16,
Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999878517857143">
the parsing is unchanged; however, by allowing the
supertagger to do much of the parsing work, the effi-
ciency of the decoder is greatly increased in practice.
We chose the perceptron for the training algo-
rithm because it has shown good performance on
other NLP tasks; in particular, Collins (2002) re-
ported good performance for a perceptron tagger
compared to a Maximum Entropy tagger. Like
Collins (2002), the decoder is the same for both the
perceptron and the log-linear parsing models; the
only change is the method for setting the weights.
The perceptron model performs as well as the log-
linear model, but is considerably easier to train.
Another contribution of this paper is to advance
wide-coverage CCG parsing. Previous discrimina-
tive models for CCG (Clark and Curran, 2004b) re-
quired cluster computing resources to train. In this
paper we reduce the memory requirements from 20
GB of RAM to only a few hundred MB, but with-
out greatly increasing the training time or reducing
parsing accuracy. This provides state-of-the-art CCG
parsing with a practical development environment.
More generally, this work provides a practical
environment for experimenting with discriminative
models for phrase-structure parsing; because the
training time for the CCG parser is relatively short
(a few hours), experiments such as comparing alter-
native feature sets can be performed. As an example,
we investigate the order in which the training exam-
ples are presented to the perceptron learner. Since
the perceptron training is an online algorithm — up-
dating the weights one training sentence at a time
— the order in which the data is processed affects
the resulting model. We consider random ordering;
presenting the shortest sentences first; and present-
ing the longest sentences first; and find that the order
does not significantly affect the final results.
We also use the random orderings to investigate
model averaging. We produced 10 different models,
by randomly permuting the data, and averaged the
weights. Again the averaging was found to have no
impact on the results, showing that the perceptron
learner — at least for this parsing task — is robust
to the order of the training examples.
The contributions of this paper are as follows.
First, we compare perceptron and log-linear parsing
models for a wide-coverage phrase-structure parser,
the first work we are aware of to do so. Second,
we provide a practical framework for developing
discriminative models for CCG, reducing the mem-
ory requirements from over 20 GB to a few hundred
MB. And third, given the significantly shorter train-
ing time compared to other discriminative parsing
models (Taskar et al., 2004), we provide a practical
framework for investigating discriminative training
methods more generally.
</bodyText>
<sectionHeader confidence="0.949579" genericHeader="introduction">
2 The CCG Parser
</sectionHeader>
<bodyText confidence="0.999963108108108">
Clark and Curran (2004b) describes the CCG parser.
The grammar used by the parser is extracted from
CCGbank, a CCG version of the Penn Treebank
(Hockenmaier, 2003). The grammar consists of 425
lexical categories, expressing subcategorisation in-
formation, plus a small number of combinatory rules
which combine the categories (Steedman, 2000). A
Maximum Entropy supertagger first assigns lexical
categories to the words in a sentence, which are
then combined by the parser using the combinatory
rules and the CKY algorithm. A log-linear model
scores the alternative parses. We use the normal-
form model, which assigns probabilities to single
derivations based on the normal-form derivations in
CCGbank. The features in the model are defined
over local parts of the derivation and include word-
word dependencies. A packed chart representation
allows efficient decoding, with the Viterbi algorithm
finding the most probable derivation.
The supertagger is a key part of the system. It
uses a log-linear model to define a distribution over
the lexical category set for each word and the previ-
ous two categories (Ratnaparkhi, 1996) and the for-
ward backward algorithm efficiently sums over all
histories to give a distibution for each word. These
distributions are then used to assign a set of lexical
categories to each word (Curran et al., 2006).
Supertagging was first defined for LTAG (Banga-
lore and Joshi, 1999), and was designed to increase
parsing speed for lexicalized grammars by allow-
ing a finite-state tagger to do some of the parsing
work. Since the elementary syntactic units in a lexi-
calized grammar — in LTAG’s case elementary trees
and in CCG’s case lexical categories – contain a sig-
nificant amount of grammatical information, com-
bining them together is easier than the parsing typi-
cally performed by phrase-structure parsers. Hence
</bodyText>
<page confidence="0.996948">
10
</page>
<bodyText confidence="0.999943321428572">
Bangalore and Joshi (1999) refer to supertagging as
almost parsing.
Supertagging has been especially successful for
CCG: Clark and Curran (2004a) demonstrates the
considerable increases in speed that can be obtained
through use of a supertagger. The supertagger in-
teracts with the parser in an adaptive fashion. Ini-
tially the supertagger assigns a small number of cat-
egories, on average, to each word in the sentence,
and the parser attempts to create a spanning analysis.
If this is not possible, the supertagger assigns more
categories, and this process continues until a span-
ning analysis is found. The number of categories as-
signed to each word is determined by a parameter Q
in the supertagger: all categories are assigned whose
forward-backward probabilities are within Q of the
highest probability category (Curran et al., 2006).
Clark and Curran (2004a) also shows how the su-
pertagger can reduce the size of the packed charts to
allow discriminative log-linear training. However,
even with the use of a supertagger, the packed charts
for the complete CCGbank require over 20 GB of
RAM. Reading the training instances into memory
one at a time and keeping a record of the relevant
feature counts would be too slow for practical de-
velopment, since the log-linear model requires hun-
dreds of iterations to converge. Hence the packed
charts need to be stored in memory. In Clark and
Curran (2004b) we use a cluster of 45 machines, to-
gether with a parallel implementation of the BFGS
training algorithm, to solve this problem.
The need for cluster computing resources presents
a barrier to the development of further CCG pars-
ing models. Hockenmaier and Steedman (2002) de-
scribe a generative model for CCG, which only re-
quires a non-iterative counting process for training,
but it is generally acknowledged that discrimina-
tive models provide greater flexibility and typically
higher performance. In this paper we propose the
perceptron algorithm as a solution. The perceptron
is an online learning algorithm, and so the param-
eters are updated one training instance at a time.
However, the key difference compared with the log-
linear training is that the perceptron converges in
many fewer iterations, and so it is practical to read
the training instances into memory one at a time.
The difficulty in using the perceptron for training
phrase-structure parsing models is the need for an
efficient decoder (since perceptron training essen-
tially is decoding). Here we exploit the lexicalized
nature of CCG by using the supertagger to restrict the
size of the charts over which Viterbi decoding is per-
formed, resulting in an extremely effcient decoder.
In fact, the decoding is so fast that we can estimate a
state-of-the-art discriminative parsing model in only
a few hours on a single machine.
</bodyText>
<sectionHeader confidence="0.957198" genericHeader="method">
3 Perceptron Training
</sectionHeader>
<bodyText confidence="0.9981922">
The parsing problem is to find a mapping from a set
of sentences x E X to a set of parses y E Y . We
assume that the mapping F is represented through a
feature vector -b(x, y) E Rd and a parameter vector
α E Rd in the following way (Collins, 2002):
</bodyText>
<equation confidence="0.997674">
F(x) = argmax -b(x, y) · α (1)
yEGEN(x)
</equation>
<bodyText confidence="0.999989133333333">
where GEN(x) denotes the set of possible parses for
sentence x and 4b(x, y) · α = EZ αZ4bZ(x, y) is the
inner product. The learning task is to set the parame-
ter values (the feature weights) using the training set
as evidence, where the training set consists of ex-
amples (xZ, yZ) for 1 G i G N. The decoder is an
algorithm which finds the argmax in (1).
In this paper, Y is the set of possible CCG deriva-
tions and GEN(x) enumerates the set of derivations
for sentence x. We use the same feature representa-
tion 4b(x, y) as in Clark and Curran (2004b), to allow
comparison with the log-linear model. The features
are defined in terms of local subtrees in the deriva-
tion, consisting of a parent category plus one or
two children. Some features are lexicalized, encod-
ing word-word dependencies. Features are integer-
valued, counting the number of times some configu-
ration occurs in a derivation.
GEN(x) is defined by the CCG grammar, plus the
supertagger, since the supertagger determines how
many lexical categories are assigned to each word
in x (through the Q parameter). Rather than try to
recreate the adaptive supertagging described in Sec-
tion 2 for training, we simply fix the the value of Q so
that GEN(x) is the set of derivations licenced by the
grammar for sentence x, given that value. Q is now
a parameter of the training process which we deter-
mine experimentally using development data. The Q
parameter can be thought of as determining the set
of incorrect derivations which the training algorithm
</bodyText>
<page confidence="0.995415">
11
</page>
<bodyText confidence="0.9941485">
uses to “discriminate against”, with a smaller value
of Q resulting in more derivations.
</bodyText>
<subsectionHeader confidence="0.997722">
3.1 Feature Forests
</subsectionHeader>
<bodyText confidence="0.999982210526316">
The same decoder is used for both training and test-
ing: the Viterbi algorithm. However, the packed
representation of GEN(x) in each case is different.
When running the parser, a lot of grammatical in-
formation is stored in order to produce linguistically
meaningful output. For training, all that is required
is a packed representation of the features on each
derivation in GEN(x) for each sentence in the train-
ing data. The feature forests described in Miyao and
Tsujii (2002) provide such a representation.
Clark and Curran (2004b) describe how a set of
CCG derivations can be represented as a feature for-
est. The feature forests are created by first building
packed charts for the training sentences, and then
extracting the feature information. Packed charts
group together equivalent chart entries. Entries are
equivalent when they interact in the same manner
with both the generation of subsequent parse struc-
ture and the numerical parse selection. In prac-
tice, this means that equivalent entries have the same
span, and form the same structures and generate the
same features in any further parsing of the sentence.
Back pointers to the daughters indicate how an indi-
vidual entry was created, so that any derivation can
be recovered from the chart.
A feature forest is essentially a packed chart with
only the feature information retained (see Miyao and
Tsujii (2002) and Clark and Curran (2004b) for the
details). Dynamic programming algorithms can be
used with the feature forests for efficient estimation.
For the log-linear parsing model in Clark and Cur-
ran (2004b), the inside-outside algorithm is used to
calculate feature expectations, which are then used
by the BFGS algorithm to optimise the likelihood
function. For the perceptron, the Viterbi algorithm
finds the features corresponding to the highest scor-
ing derivation, which are then used in a simple addi-
tive update process.
</bodyText>
<subsectionHeader confidence="0.999361">
3.2 The Perceptron Algorithm
</subsectionHeader>
<bodyText confidence="0.896292666666667">
The training algorithm initializes the parameter vec-
tor as all zeros, and updates the vector by decoding
the examples. Each feature forest is decoded with
the current parameter vector. If the output is incor-
Inputs: training examples (xi, yi)
Initialisation: set α = 0
</bodyText>
<figure confidence="0.838272666666667">
Algorithm:
for t = 1..T, i = 1..N
calculate zi = argmaxyEGEN(xi) 4b(xi, y) · α
if zi =� yi
α = α + 4b(xi, yi) − 4b(xi, zi)
Outputs: α
</figure>
<figureCaption confidence="0.999993">
Figure 1: The perceptron training algorithm
</figureCaption>
<bodyText confidence="0.89762244">
rect, the parameter vector is updated by adding the
feature vector of the correct derivation and subtract-
ing the feature vector of the decoder output. Train-
ing typically involves multiple passes over the data.
Figure 1 gives the algorithm, where N is the number
of training sentences and T is the number of itera-
tions over the data.
For all the experiments in this paper, we used the
averaged version of the perceptron. Collins (2002)
introduced the averaged perceptron, as a way of re-
ducing overfitting, and it has been shown to perform
better than the non-averaged version on a number of
tasks. The averaged parameters are defined as fol-
lows: 7s = Et=1...T,i=1...N αt,i
s /NT where αt,i sis
the value of the sth feature weight after the tth sen-
tence has been processed in the ith iteration.
A naive implementation of the averaged percep-
tron updates the accumulated weight for each fea-
ture after each example. However, the number of
features whose values change for each example is a
small proportion of the total. Hence we use the al-
gorithm described in Daume III (2006) which avoids
unnecessary calculations by only updating the accu-
mulated weight for a feature fs when αs changes.
</bodyText>
<sectionHeader confidence="0.999743" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999501363636364">
The feature forests were created as follows. First,
the value of the Q parameter for the supertagger was
fixed (for the first set of experiments at 0.004). The
supertagger was then run over the sentences in Sec-
tions 2-21 of CCGbank. We made sure that ev-
ery word was assigned the correct lexical category
among its set (we did not do this for testing). Then
the parser was run on the supertagged sentences, us-
ing the CKY algorithm and the CCG combinatory
rules. We applied the same normal-form restrictions
used in Clark and Curran (2004b): categories can
</bodyText>
<page confidence="0.99402">
12
</page>
<bodyText confidence="0.997417836734694">
only combine if they have been seen to combine in
Sections 2-21 of CCGbank, and only if they do not
violate the Eisner (1996a) normal-form constraints.
This part of the process requires a few hundred MB
of RAM to run the parser, and takes a few hours for
Sections 2-21 of CCGbank. Any further training
times or memory requirements reported do not in-
clude the resources needed to create the forests.
The feature forests are extracted from the packed
chart representation used in the parser. We only use
a feature forest for training if it contains the correct
derivation (according to CCGbank). Some forests
do not have the correct derivation, even though we
ensure the correct lexical categories are present, be-
cause the grammar used by the parser is missing
some low-frequency rules in CCGbank. The to-
tal number of forests used for the experiments was
35,370 (89% of Sections 2-21) . Only features which
occur at least twice in the training data were used,
of which there are 477,848. The complete set of
forests used to obtain the final perceptron results in
Section 4.1 require 21 GB of disk space.
The perceptron is an online algorithm, updating
the weights after each forest is processed. Each for-
est is read into memory one at a time, decoding is
performed, and the weight values are updated. Each
forest is discarded from memory after it has been
used. Constantly reading forests off disk is expen-
sive, but since the perceptron converges in so few
iterations the training times are reasonable.
In contrast, log-linear training takes hundreds of
iterations to converge, and so it would be impractical
to keep reading the forests off disk. Also, since log-
linear training uses a batch algorithm, it is more con-
venient to keep the forests in memory at all times.
In Clark and Curran (2004b) we use a cluster of 45
machines, together with a parallel implementation
of BFGS, to solve this problem, but need up to 20 GB
of RAM.
The feature forest representation, and our imple-
mentation of it, is so compact that the perceptron
training requires only 20 MB of RAM. Since the su-
pertagger has already removed much of the practical
parsing complexity, decoding one of the forests is
extremely quick, and much of the training time is
taken with continually reading the forests off disk.
However, the training time for the perceptron is still
only around 5 hours for 10 iterations.
model RAM iterations time (mins)
</bodyText>
<equation confidence="0.857372">
perceptron 20 MB 10 312
log-linear 19 GB 475 91
</equation>
<tableCaption confidence="0.912751">
Table 1: Training requirements for the perceptron
and log-linear models
Table 1 compares the training for the perceptron
</tableCaption>
<bodyText confidence="0.95891675">
and log-linear models. The perceptron was run for
10 iterations and the log-linear training was run to
convergence. The training time for 10 iterations of
the perceptron is longer than the log-linear training,
although the results in Section 4.1 show that the per-
ceptron typically converges in around 4 iterations.
The striking result in the table is the significantly
smaller memory requirement for the perceptron.
</bodyText>
<sectionHeader confidence="0.699344" genericHeader="method">
4.1 Results
</sectionHeader>
<bodyText confidence="0.999975807692308">
Table 2 gives the first set of results for the averaged
perceptron model. These were obtained using Sec-
tion 00 of CCGbank as development data. Gold-
standard POS tags from CCGbank were used for all
the experiments. The parser provides an analysis for
99.37% of the sentences in Section 00. The F-scores
are based only on the sentences for which there is
an analysis. Following Clark and Curran (2004b),
accuracy is measured using F-score over the gold-
standard predicate-argument dependencies in CCG-
bank. The table shows that the accuracy increases
initially with the number of iterations, but converges
quickly after only 4 iterations. The accuracy after
only one iteration is also surprisingly high.
Table 3 compares the accuracy of the perceptron
and log-linear models on the development data. LP
is labelled precision, LR is labelled recall, and CAT
is the lexical category accuracy. The same feature
forests were used for training the perceptron and
log-linear models, and the same parser and decoding
algorithm were used for testing, so the results for the
two models are directly comparable. The only dif-
ference in each case was the weights file used.1
The table also gives the accuracy for the percep-
tron model (after 6 iterations) when a smaller value
of the supertagger Q parameter is used during the
</bodyText>
<footnote confidence="0.6000705">
1Both of these models have parameters which have been
optimised on the development data, in the log-linear case the
Gaussian smoothing parameter and in the perceptron case the
number of training iterations.
</footnote>
<page confidence="0.995507">
13
</page>
<table confidence="0.997087">
iteration 1 2 3 4 5 6 7 8 9 10
F-score 85.87 86.28 86.33 86.49 86.46 86.51 86.47 86.52 86.53 86.54
</table>
<tableCaption confidence="0.987213">
Table 2: Accuracy on the development data for the averaged perceptron (Q = 0.004)
</tableCaption>
<table confidence="0.9992505">
model LP LR F CAT
log-linearβ_0.004 87.02 86.07 86.54 93.99
perceptronβ_0.004 87.11 85.98 86.54 94.03
perceptronβ_0.002 87.25 86.20 86.72 94.08
</table>
<tableCaption confidence="0.9300805">
Table 3: Comparison of the perceptron and log-
linear models on the development data
</tableCaption>
<bodyText confidence="0.999836090909091">
forest creation (with the number of training itera-
tions again optimised on the development data). A
smaller Q value results in larger forests, giving more
incorrect derivations for the training algorithm to
“discriminate against”. Increasing the size of the
forests is no problem for the perceptron, since the
memory requirements are so modest, but this would
cause problems for the log-linear training which is
already highly memory intensive. The table shows
that increasing the number of incorrect derivations
gives a small improvement in performance for the
perceptron.
Table 4 gives the accuracies for the two models
on the test data, Section 23 of CCGbank. Here the
coverage of the parser is 99.63%, and again the ac-
curacies are computed only for the sentences with
an analysis. The figures for the averaged perceptron
were obtained using 6 iterations, with Q = 0.002.
The perceptron slightly outperforms the log-linear
model (although we have not carried out signifi-
cance tests). We justify the use of different Q values
for the two models by arguing that the perceptron is
much more flexible in terms of the size of the train-
ing forests it can handle.
Note that the important result here is that the per-
ceptron model performs at least as well as the log-
linear model. Since the perceptron is considerably
easier to train, this is a useful finding. Also, since
the log-linear parsing model is a Conditional Ran-
dom Field (CRF), the results suggest that the percep-
tron should be compared with a CRF for other tasks
for which the CRF is considered to give state-of-the-
art results.
</bodyText>
<table confidence="0.983580333333333">
model LP LR F CAT
log-linearβ_0.004 87.39 86.51 86.95 94.07
perceptronβ_0.002 87.50 86.62 87.06 94.08
</table>
<tableCaption confidence="0.980939">
Table 4: Comparison of the perceptron and log-
linear models on the test data
</tableCaption>
<sectionHeader confidence="0.898536" genericHeader="method">
5 Order of Training Examples
</sectionHeader>
<bodyText confidence="0.999426294117647">
As an example of the flexibility of our discrimina-
tive training framework, we investigated the order in
which the training examples are presented to the on-
line perceptron learner. These experiments were par-
ticularly easy to carry out in our framework, since
the 21 GB file containing the complete set of training
forests can be sampled from directly. We stored the
position on disk of each of the forests, and selected
the forests one by one, according to some order.
The first set of experiments investigated ordering
the training examples by sentence length. Buttery
(2006) found that a psychologically motivated Cate-
gorial Grammar learning system learned faster when
the simplest linguistic examples were presented first.
Table 5 shows the results both when the shortest sen-
tences are presented first and when the longest sen-
tences are presented first. Training on the longest
sentences first provides the best performance, but is
no better than the standard ordering.
For the random ordering experiments, forests
were randomly sampled from the complete 21 GB
training file on disk, without replacement. The
new forests file was then used for the averaged-
perceptron training, and this process was repeated
9 times.
The number of iterations for each training run was
optimised in terms of the accuracy of the resulting
model on the development data. There was little
variation among the models, with the best model
scoring 86.84% F-score on the development data
and the worst scoring 86.63%. Table 6 shows that
the performance of this best model on the test data
is only slightly better than the model trained using
the CCGbank ordering.
</bodyText>
<page confidence="0.995671">
14
</page>
<table confidence="0.99990325">
iteration 1 2 3 4 5 6
Standard order 86.14 86.30 86.53 86.61 86.69 86.72
Shortest first 85.98 86.41 86.57 86.56 86.54 86.53
Longest first 86.25 86.48 86.66 86.72 86.74 86.75
</table>
<tableCaption confidence="0.974375">
Table 5: F-score of the averaged perceptron on the development data for different data orderings (Q = 0.002)
</tableCaption>
<table confidence="0.99996475">
LP LR F CAT
87.50 86.62 87.06 94.08
87.52 86.72 87.12 94.12
87.53 86.67 87.10 94.09
</table>
<tableCaption confidence="0.94131">
Table 6: Comparison of various perceptron models
on the test data
</tableCaption>
<bodyText confidence="0.9999729375">
Finally, we used the 10 models (including the
model from the original training set) to investigate
model averaging. Corston-Oliver et al. (2006) mo-
tivate model averaging for the perceptron in terms
of Bayes Point Machines. The averaged percep-
tron weights resulting from each permutation of the
training data were simply averaged to produce a new
model. Table 6 shows that the averaged model again
performs only marginally better than the original
model, and not as well as the best-performing “ran-
dom” model, which is perhaps not surprising given
the small variation among the performances of the
component models.
In summary, the perceptron learner appears highly
robust to the order of the training examples, at least
for this parsing task.
</bodyText>
<sectionHeader confidence="0.962961" genericHeader="method">
6 Comparison with Other Work
</sectionHeader>
<bodyText confidence="0.999894852941177">
Taskar et al. (2004) investigate discriminative train-
ing methods for a phrase-structure parser, and also
use dynamic programming for the decoder. The key
difference between our work and theirs is that they
are only able to train on sentences of 15 words or
less, because of the expense of the decoding.
There is work on discriminative models for de-
pendency parsing (McDonald, 2006); since there
are efficient decoding algorithms available (Eisner,
1996b), complete resources such as the Penn Tree-
bank can used for estimation, leading to accurate
parsers. There is also work on discriminative mod-
els for parse reranking (Collins and Koo, 2005). The
main drawback with this approach is that the correct
parse may get lost in the first phase.
The existing work most similar to ours is Collins
and Roark (2004). They use a beam-search decoder
as part of a phrase-structure parser to allow practical
estimation. The main difference is that we are able
to store the complete forests for training, and can
guarantee that the forest contains the correct deriva-
tion (assuming the grammar is able to generate it
given the correct lexical categories). The downside
of our approach is the restriction on the locality of
the features, to allow dynamic programming. One
possible direction for future work is to compare the
search-based approach of Collins and Roark with
our DP-based approach.
In the tagging domain, Collins (2002) compared
log-linear and perceptron training for HMM-style
tagging based on dynamic programming. Our work
could be seen as extending that of Collins since we
compare log-linear and perceptron training for a DP-
based wide-coverage parser.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999919388888889">
Investigation of discriminative training methods is
one of the most promising avenues for breaking
the current bottleneck in parsing performance. The
drawback of these methods is the need for an effi-
cient decoder. In this paper we have demonstrated
how the lexicalized nature of CCG can be used to
develop a very efficient decoder, which leads to a
practical development environment for discrimina-
tive training.
We have also provided the first comparison of a
perceptron and log-linear model for a wide-coverage
phrase-structure parser. An advantage of the percep-
tron over the log-linear model is that it is consider-
ably easier to train, requiring 1/1000th of the mem-
ory requirements and converging in only 4 iterations.
Given that the global log-linear model used here
(CRF) is thought to provide state-of-the-art perfor-
mance for many NLP tasks, it is perhaps surprising
</bodyText>
<figure confidence="0.9675275">
perceptron model
standard order
best random order
averaged
</figure>
<page confidence="0.953029">
15
</page>
<bodyText confidence="0.9999035">
that the perceptron performs as well. The evalua-
tion in this paper was based solely on CCGbank, but
we have shown in Clark and Curran (2007) that the
CCG parser gives state-of-the-art performance, out-
performing the RASP parser (Briscoe et al., 2006)
by over 5% on DepBank. This suggests the need for
more comparisons of CRFs and discriminative meth-
ods such as the perceptron for other NLP tasks.
</bodyText>
<sectionHeader confidence="0.99481" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.980711">
James Curran was funded under ARC Discovery
grants DP0453131 and DP0665973.
</bodyText>
<sectionHeader confidence="0.998736" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999672366666667">
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguistics,
25(2):237–265.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The
second release of the RASP system. In Proceedings of
the Interactive Demo Session of COLING/ACL-06, Sydney,
Austrailia.
Paula Buttery. 2006. Computational models for first language
acquisition. Technical Report UCAM-CL-TR-675, Univer-
sity of Cambridge Computer Laboratory.
Stephen Clark and James R. Curran. 2004a. The importance of
supertagging for wide-coverage CCG parsing. In Proceed-
ings of COLING-04, pages 282–288, Geneva, Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing the WSJ
using CCG and log-linear models. In Proceedings of the
42nd Meeting of the ACL, pages 104–111, Barcelona, Spain.
Stephen Clark and James R. Curran. 2007. Formalism-
independent parser evaluation with CCG and DepBank. In
Proceedings of the 45th Annual Meeting of the ACL, Prague,
Czech Republic.
Michael Collins and Terry Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational Linguistics,
31(1):25–69.
Michael Collins and Brian Roark. 2004. Incremental parsing
with the perceptron algorithm. In Proceedings of the 42nd
Meeting of the ACL, pages 111–118, Barcelona, Spain.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the 40th Meeting of
the ACL, Philadelphia, PA.
S. Corston-Oliver, A. Aue, K. Duh, and E. Ringger. 2006. Mul-
tilingual dependency parsing using bayes point machines. In
Proceedings ofHLT/NAACL-06, New York.
James R. Curran, Stephen Clark, and David Vadas. 2006.
Multi-tagging for lexicalized-grammar parsing. In Proceed-
ings of COLING/ACL-06, pages 697–704, Sydney, Aus-
trailia.
Jason Eisner. 1996a. Efficient normal-form parsing for Com-
binatory Categorial Grammar. In Proceedings of the 34th
Meeting of the ACL, pages 79–86, Santa Cruz, CA.
Jason Eisner. 1996b. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of the
16th COLING Conference, pages 340–345, Copenhagen,
Denmark.
Yoav Freund and Robert E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine Learn-
ing, 37(3):277–296.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Meeting of the ACL,
pages 335–342, Philadelphia, PA.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D. the-
sis, University of Edinburgh.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic ‘unification-
based’ grammars. In Proceedings of the 37th Meeting of the
ACL, pages 535–541, University of Maryland, MD.
Robert Malouf and Gertjan van Noord. 2004. Wide coverage
parsing with stochastic attribute value grammars. In Pro-
ceedings of the IJCNLP-04 Workshop: Beyond shallow anal-
yses - Formalisms and statistical modelingfor deep analyses,
Hainan Island, China.
Ryan McDonald. 2006. Discriminative Training and Spanning
Tree Algorithms for Dependency Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy
estimation for feature forests. In Proceedings of the Human
Language Technology Conference, San Diego, CA.
Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic dis-
ambiguation models for wide-coverage HPSG parsing. In
Proceedings of the 43rd meeting of the ACL, pages 83–90,
University of Michigan, Ann Arbor.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the EMNLP Conference,
pages 133–142, Philadelphia, PA.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell III, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-Functional
Grammar and discriminative estimation techniques. In Pro-
ceedings of the 40th Meeting of the ACL, pages 271–278,
Philadelphia, PA.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004. Max-margin parsing. In Proceedings of the EMNLP
conference, pages 1–8, Barcelona, Spain.
Joseph Turian and I. Dan Melamed. 2006. Advances in dis-
criminative parsing. In Proceedings of COLING/ACL-06,
pages 873–880, Sydney, Australia.
</reference>
<page confidence="0.998702">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.666088">
<title confidence="0.99947">Perceptron Training for a Wide-Coverage Lexicalized-Grammar Parser</title>
<author confidence="0.988673">Stephen</author>
<affiliation confidence="0.999955">Oxford University Computing</affiliation>
<address confidence="0.911482">Wolfson Building, Parks Oxford, OX1 3QD,</address>
<email confidence="0.997641">stephen.clark@comlab.ox.ac.uk</email>
<author confidence="0.999697">R James</author>
<affiliation confidence="0.997767">School of Information University of</affiliation>
<address confidence="0.835867">NSW 2006,</address>
<email confidence="0.994409">james@it.usyd.edu.au</email>
<abstract confidence="0.998697304347826">This paper investigates perceptron training a wide-coverage and compares the perceptron with a log-linear model. uses a phrase-structure parsing model and dynamic programming in the form of the Viterbi algorithm to find the highest scoring derivation. The difficulty in using the perceptron for a phrase-structure parsing model is the need for an efficient decoder. We exploit the lexicalized nature of using a finite-state supertagger to do much of the parsing work, resulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; it requires only a few hundred practical training compared to 20 the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="3239" citStr="Bangalore and Joshi, 1999" startWordPosition="499" endWordPosition="502">me training criterion. Hence, for efficient training, these methods require an efficient decoder; in fact, for methods like the perceptron, the update procedure is so trivial that the training algorithm essentially is decoding. This paper describes a decoder for a lexicalizedgrammar parser which is efficient enough for practical discriminative training. We use a lexicalized phrase-structure parser, the CCG parser of Clark and Curran (2004b), together with a DP-based decoder. The key idea is to exploit the properties of lexicalized grammars by using a finite-state supertagger prior to parsing (Bangalore and Joshi, 1999; Clark and Curran, 2004a). The decoder still uses the CKY algorithm, so the worst case complexity of Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 9–16, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics the parsing is unchanged; however, by allowing the supertagger to do much of the parsing work, the efficiency of the decoder is greatly increased in practice. We chose the perceptron for the training algorithm because it has shown good performance on other NLP tasks; in particular, Collins (2002) reported good performance for a per</context>
<context position="7701" citStr="Bangalore and Joshi, 1999" startWordPosition="1206" endWordPosition="1210">lude wordword dependencies. A packed chart representation allows efficient decoding, with the Viterbi algorithm finding the most probable derivation. The supertagger is a key part of the system. It uses a log-linear model to define a distribution over the lexical category set for each word and the previous two categories (Ratnaparkhi, 1996) and the forward backward algorithm efficiently sums over all histories to give a distibution for each word. These distributions are then used to assign a set of lexical categories to each word (Curran et al., 2006). Supertagging was first defined for LTAG (Bangalore and Joshi, 1999), and was designed to increase parsing speed for lexicalized grammars by allowing a finite-state tagger to do some of the parsing work. Since the elementary syntactic units in a lexicalized grammar — in LTAG’s case elementary trees and in CCG’s case lexical categories – contain a significant amount of grammatical information, combining them together is easier than the parsing typically performed by phrase-structure parsers. Hence 10 Bangalore and Joshi (1999) refer to supertagging as almost parsing. Supertagging has been especially successful for CCG: Clark and Curran (2004a) demonstrates the </context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the Interactive Demo Session of COLING/ACL-06,</booktitle>
<location>Sydney, Austrailia.</location>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the Interactive Demo Session of COLING/ACL-06, Sydney, Austrailia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Buttery</author>
</authors>
<title>Computational models for first language acquisition.</title>
<date>2006</date>
<tech>Technical Report UCAM-CL-TR-675,</tech>
<institution>University of Cambridge Computer Laboratory.</institution>
<contexts>
<context position="24283" citStr="Buttery (2006)" startWordPosition="4004" endWordPosition="4005">test data 5 Order of Training Examples As an example of the flexibility of our discriminative training framework, we investigated the order in which the training examples are presented to the online perceptron learner. These experiments were particularly easy to carry out in our framework, since the 21 GB file containing the complete set of training forests can be sampled from directly. We stored the position on disk of each of the forests, and selected the forests one by one, according to some order. The first set of experiments investigated ordering the training examples by sentence length. Buttery (2006) found that a psychologically motivated Categorial Grammar learning system learned faster when the simplest linguistic examples were presented first. Table 5 shows the results both when the shortest sentences are presented first and when the longest sentences are presented first. Training on the longest sentences first provides the best performance, but is no better than the standard ordering. For the random ordering experiments, forests were randomly sampled from the complete 21 GB training file on disk, without replacement. The new forests file was then used for the averagedperceptron traini</context>
</contexts>
<marker>Buttery, 2006</marker>
<rawString>Paula Buttery. 2006. Computational models for first language acquisition. Technical Report UCAM-CL-TR-675, University of Cambridge Computer Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>282--288</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1629" citStr="Clark and Curran, 2004" startWordPosition="252" endWordPosition="255"> of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and 9 Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars. In Clark and Curran (2004b) we use cluster computing resources to solve this problem. Parsing research has also begun to adopt discriminative m</context>
<context position="3056" citStr="Clark and Curran (2004" startWordPosition="469" endWordPosition="472"> McDonald, 2006). Parser training involves decoding in an iterative process, updating the model parameters so that the decoder performs better on the training data, according to some training criterion. Hence, for efficient training, these methods require an efficient decoder; in fact, for methods like the perceptron, the update procedure is so trivial that the training algorithm essentially is decoding. This paper describes a decoder for a lexicalizedgrammar parser which is efficient enough for practical discriminative training. We use a lexicalized phrase-structure parser, the CCG parser of Clark and Curran (2004b), together with a DP-based decoder. The key idea is to exploit the properties of lexicalized grammars by using a finite-state supertagger prior to parsing (Bangalore and Joshi, 1999; Clark and Curran, 2004a). The decoder still uses the CKY algorithm, so the worst case complexity of Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 9–16, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics the parsing is unchanged; however, by allowing the supertagger to do much of the parsing work, the efficiency of the decoder is greatly increased in p</context>
<context position="4288" citStr="Clark and Curran, 2004" startWordPosition="667" endWordPosition="670">e chose the perceptron for the training algorithm because it has shown good performance on other NLP tasks; in particular, Collins (2002) reported good performance for a perceptron tagger compared to a Maximum Entropy tagger. Like Collins (2002), the decoder is the same for both the perceptron and the log-linear parsing models; the only change is the method for setting the weights. The perceptron model performs as well as the loglinear model, but is considerably easier to train. Another contribution of this paper is to advance wide-coverage CCG parsing. Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. In this paper we reduce the memory requirements from 20 GB of RAM to only a few hundred MB, but without greatly increasing the training time or reducing parsing accuracy. This provides state-of-the-art CCG parsing with a practical development environment. More generally, this work provides a practical environment for experimenting with discriminative models for phrase-structure parsing; because the training time for the CCG parser is relatively short (a few hours), experiments such as comparing alternative feature sets can be performed. As an e</context>
<context position="6316" citStr="Clark and Curran (2004" startWordPosition="989" endWordPosition="992">ples. The contributions of this paper are as follows. First, we compare perceptron and log-linear parsing models for a wide-coverage phrase-structure parser, the first work we are aware of to do so. Second, we provide a practical framework for developing discriminative models for CCG, reducing the memory requirements from over 20 GB to a few hundred MB. And third, given the significantly shorter training time compared to other discriminative parsing models (Taskar et al., 2004), we provide a practical framework for investigating discriminative training methods more generally. 2 The CCG Parser Clark and Curran (2004b) describes the CCG parser. The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories, expressing subcategorisation information, plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence, which are then combined by the parser using the combinatory rules and the CKY algorithm. A log-linear model scores the alternative parses. We use the normalform model, which assigns probab</context>
<context position="8281" citStr="Clark and Curran (2004" startWordPosition="1299" endWordPosition="1302">fined for LTAG (Bangalore and Joshi, 1999), and was designed to increase parsing speed for lexicalized grammars by allowing a finite-state tagger to do some of the parsing work. Since the elementary syntactic units in a lexicalized grammar — in LTAG’s case elementary trees and in CCG’s case lexical categories – contain a significant amount of grammatical information, combining them together is easier than the parsing typically performed by phrase-structure parsers. Hence 10 Bangalore and Joshi (1999) refer to supertagging as almost parsing. Supertagging has been especially successful for CCG: Clark and Curran (2004a) demonstrates the considerable increases in speed that can be obtained through use of a supertagger. The supertagger interacts with the parser in an adaptive fashion. Initially the supertagger assigns a small number of categories, on average, to each word in the sentence, and the parser attempts to create a spanning analysis. If this is not possible, the supertagger assigns more categories, and this process continues until a spanning analysis is found. The number of categories assigned to each word is determined by a parameter Q in the supertagger: all categories are assigned whose forward-b</context>
<context position="9536" citStr="Clark and Curran (2004" startWordPosition="1508" endWordPosition="1511">Q of the highest probability category (Curran et al., 2006). Clark and Curran (2004a) also shows how the supertagger can reduce the size of the packed charts to allow discriminative log-linear training. However, even with the use of a supertagger, the packed charts for the complete CCGbank require over 20 GB of RAM. Reading the training instances into memory one at a time and keeping a record of the relevant feature counts would be too slow for practical development, since the log-linear model requires hundreds of iterations to converge. Hence the packed charts need to be stored in memory. In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. The need for cluster computing resources presents a barrier to the development of further CCG parsing models. Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. In this paper we propose the perceptron algorithm as a solution. The perceptron is an online learning</context>
<context position="11786" citStr="Clark and Curran (2004" startWordPosition="1906" endWordPosition="1909"> the following way (Collins, 2002): F(x) = argmax -b(x, y) · α (1) yEGEN(x) where GEN(x) denotes the set of possible parses for sentence x and 4b(x, y) · α = EZ αZ4bZ(x, y) is the inner product. The learning task is to set the parameter values (the feature weights) using the training set as evidence, where the training set consists of examples (xZ, yZ) for 1 G i G N. The decoder is an algorithm which finds the argmax in (1). In this paper, Y is the set of possible CCG derivations and GEN(x) enumerates the set of derivations for sentence x. We use the same feature representation 4b(x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model. The features are defined in terms of local subtrees in the derivation, consisting of a parent category plus one or two children. Some features are lexicalized, encoding word-word dependencies. Features are integervalued, counting the number of times some configuration occurs in a derivation. GEN(x) is defined by the CCG grammar, plus the supertagger, since the supertagger determines how many lexical categories are assigned to each word in x (through the Q parameter). Rather than try to recreate the adaptive supertagging described in Section 2 </context>
<context position="13388" citStr="Clark and Curran (2004" startWordPosition="2170" endWordPosition="2173">nate against”, with a smaller value of Q resulting in more derivations. 3.1 Feature Forests The same decoder is used for both training and testing: the Viterbi algorithm. However, the packed representation of GEN(x) in each case is different. When running the parser, a lot of grammatical information is stored in order to produce linguistically meaningful output. For training, all that is required is a packed representation of the features on each derivation in GEN(x) for each sentence in the training data. The feature forests described in Miyao and Tsujii (2002) provide such a representation. Clark and Curran (2004b) describe how a set of CCG derivations can be represented as a feature forest. The feature forests are created by first building packed charts for the training sentences, and then extracting the feature information. Packed charts group together equivalent chart entries. Entries are equivalent when they interact in the same manner with both the generation of subsequent parse structure and the numerical parse selection. In practice, this means that equivalent entries have the same span, and form the same structures and generate the same features in any further parsing of the sentence. Back poi</context>
<context position="16963" citStr="Clark and Curran (2004" startWordPosition="2775" endWordPosition="2778"> only updating the accumulated weight for a feature fs when αs changes. 4 Experiments The feature forests were created as follows. First, the value of the Q parameter for the supertagger was fixed (for the first set of experiments at 0.004). The supertagger was then run over the sentences in Sections 2-21 of CCGbank. We made sure that every word was assigned the correct lexical category among its set (we did not do this for testing). Then the parser was run on the supertagged sentences, using the CKY algorithm and the CCG combinatory rules. We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can 12 only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints. This part of the process requires a few hundred MB of RAM to run the parser, and takes a few hours for Sections 2-21 of CCGbank. Any further training times or memory requirements reported do not include the resources needed to create the forests. The feature forests are extracted from the packed chart representation used in the parser. We only use a feature forest for training if it contains the correct derivation (accordi</context>
<context position="18776" citStr="Clark and Curran (2004" startWordPosition="3087" endWordPosition="3090">ach forest is processed. Each forest is read into memory one at a time, decoding is performed, and the weight values are updated. Each forest is discarded from memory after it has been used. Constantly reading forests off disk is expensive, but since the perceptron converges in so few iterations the training times are reasonable. In contrast, log-linear training takes hundreds of iterations to converge, and so it would be impractical to keep reading the forests off disk. Also, since loglinear training uses a batch algorithm, it is more convenient to keep the forests in memory at all times. In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. The feature forest representation, and our implementation of it, is so compact that the perceptron training requires only 20 MB of RAM. Since the supertagger has already removed much of the practical parsing complexity, decoding one of the forests is extremely quick, and much of the training time is taken with continually reading the forests off disk. However, the training time for the perceptron is still only around 5 hours for 10 iterations. model RAM itera</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004a. The importance of supertagging for wide-coverage CCG parsing. In Proceedings of COLING-04, pages 282–288, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the ACL,</booktitle>
<pages>104--111</pages>
<location>Barcelona,</location>
<contexts>
<context position="20388" citStr="Clark and Curran (2004" startWordPosition="3357" endWordPosition="3360">ining, although the results in Section 4.1 show that the perceptron typically converges in around 4 iterations. The striking result in the table is the significantly smaller memory requirement for the perceptron. 4.1 Results Table 2 gives the first set of results for the averaged perceptron model. These were obtained using Section 00 of CCGbank as development data. Goldstandard POS tags from CCGbank were used for all the experiments. The parser provides an analysis for 99.37% of the sentences in Section 00. The F-scores are based only on the sentences for which there is an analysis. Following Clark and Curran (2004b), accuracy is measured using F-score over the goldstandard predicate-argument dependencies in CCGbank. The table shows that the accuracy increases initially with the number of iterations, but converges quickly after only 4 iterations. The accuracy after only one iteration is also surprisingly high. Table 3 compares the accuracy of the perceptron and log-linear models on the development data. LP is labelled precision, LR is labelled recall, and CAT is the lexical category accuracy. The same feature forests were used for training the perceptron and log-linear models, and the same parser and de</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004b. Parsing the WSJ using CCG and log-linear models. In Proceedings of the 42nd Meeting of the ACL, pages 104–111, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Formalismindependent parser evaluation with CCG and DepBank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Formalismindependent parser evaluation with CCG and DepBank. In Proceedings of the 45th Annual Meeting of the ACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="27198" citStr="Collins and Koo, 2005" startWordPosition="4478" endWordPosition="4481"> investigate discriminative training methods for a phrase-structure parser, and also use dynamic programming for the decoder. The key difference between our work and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding. There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). The main drawback with this approach is that the correct parse may get lost in the first phase. The existing work most similar to ours is Collins and Roark (2004). They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation. The main difference is that we are able to store the complete forests for training, and can guarantee that the forest contains the correct derivation (assuming the grammar is able to generate it given the correct lexical categories). The downside of our approach is the restriction on the locality of the features, to allow dynamic pro</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the ACL,</booktitle>
<pages>111--118</pages>
<location>Barcelona,</location>
<contexts>
<context position="1426" citStr="Collins and Roark, 2004" startWordPosition="218" endWordPosition="221">h of the parsing work, resulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and 9 Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements can be prohibitive, espe</context>
<context position="27362" citStr="Collins and Roark (2004)" startWordPosition="4508" endWordPosition="4511">and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding. There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). The main drawback with this approach is that the correct parse may get lost in the first phase. The existing work most similar to ours is Collins and Roark (2004). They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation. The main difference is that we are able to store the complete forests for training, and can guarantee that the forest contains the correct derivation (assuming the grammar is able to generate it given the correct lexical categories). The downside of our approach is the restriction on the locality of the features, to allow dynamic programming. One possible direction for future work is to compare the search-based approach of Collins and Roark with our DP-based approach. In the tagging domain, Col</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the ACL, pages 111–118, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="3803" citStr="Collins (2002)" startWordPosition="590" endWordPosition="591">tagger prior to parsing (Bangalore and Joshi, 1999; Clark and Curran, 2004a). The decoder still uses the CKY algorithm, so the worst case complexity of Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 9–16, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics the parsing is unchanged; however, by allowing the supertagger to do much of the parsing work, the efficiency of the decoder is greatly increased in practice. We chose the perceptron for the training algorithm because it has shown good performance on other NLP tasks; in particular, Collins (2002) reported good performance for a perceptron tagger compared to a Maximum Entropy tagger. Like Collins (2002), the decoder is the same for both the perceptron and the log-linear parsing models; the only change is the method for setting the weights. The perceptron model performs as well as the loglinear model, but is considerably easier to train. Another contribution of this paper is to advance wide-coverage CCG parsing. Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. In this paper we reduce the memory requirements from 20 GB of RAM</context>
<context position="11198" citStr="Collins, 2002" startWordPosition="1792" endWordPosition="1793">ecoding). Here we exploit the lexicalized nature of CCG by using the supertagger to restrict the size of the charts over which Viterbi decoding is performed, resulting in an extremely effcient decoder. In fact, the decoding is so fast that we can estimate a state-of-the-art discriminative parsing model in only a few hours on a single machine. 3 Perceptron Training The parsing problem is to find a mapping from a set of sentences x E X to a set of parses y E Y . We assume that the mapping F is represented through a feature vector -b(x, y) E Rd and a parameter vector α E Rd in the following way (Collins, 2002): F(x) = argmax -b(x, y) · α (1) yEGEN(x) where GEN(x) denotes the set of possible parses for sentence x and 4b(x, y) · α = EZ αZ4bZ(x, y) is the inner product. The learning task is to set the parameter values (the feature weights) using the training set as evidence, where the training set consists of examples (xZ, yZ) for 1 G i G N. The decoder is an algorithm which finds the argmax in (1). In this paper, Y is the set of possible CCG derivations and GEN(x) enumerates the set of derivations for sentence x. We use the same feature representation 4b(x, y) as in Clark and Curran (2004b), to allow</context>
<context position="15662" citStr="Collins (2002)" startWordPosition="2550" endWordPosition="2551">0 Algorithm: for t = 1..T, i = 1..N calculate zi = argmaxyEGEN(xi) 4b(xi, y) · α if zi =� yi α = α + 4b(xi, yi) − 4b(xi, zi) Outputs: α Figure 1: The perceptron training algorithm rect, the parameter vector is updated by adding the feature vector of the correct derivation and subtracting the feature vector of the decoder output. Training typically involves multiple passes over the data. Figure 1 gives the algorithm, where N is the number of training sentences and T is the number of iterations over the data. For all the experiments in this paper, we used the averaged version of the perceptron. Collins (2002) introduced the averaged perceptron, as a way of reducing overfitting, and it has been shown to perform better than the non-averaged version on a number of tasks. The averaged parameters are defined as follows: 7s = Et=1...T,i=1...N αt,i s /NT where αt,i sis the value of the sth feature weight after the tth sentence has been processed in the ith iteration. A naive implementation of the averaged perceptron updates the accumulated weight for each feature after each example. However, the number of features whose values change for each example is a small proportion of the total. Hence we use the a</context>
<context position="27973" citStr="Collins (2002)" startWordPosition="4608" endWordPosition="4609">04). They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation. The main difference is that we are able to store the complete forests for training, and can guarantee that the forest contains the correct derivation (assuming the grammar is able to generate it given the correct lexical categories). The downside of our approach is the restriction on the locality of the features, to allow dynamic programming. One possible direction for future work is to compare the search-based approach of Collins and Roark with our DP-based approach. In the tagging domain, Collins (2002) compared log-linear and perceptron training for HMM-style tagging based on dynamic programming. Our work could be seen as extending that of Collins since we compare log-linear and perceptron training for a DPbased wide-coverage parser. 7 Conclusion Investigation of discriminative training methods is one of the most promising avenues for breaking the current bottleneck in parsing performance. The drawback of these methods is the need for an efficient decoder. In this paper we have demonstrated how the lexicalized nature of CCG can be used to develop a very efficient decoder, which leads to a p</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 40th Meeting of the ACL, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>A Aue</author>
<author>K Duh</author>
<author>E Ringger</author>
</authors>
<title>Multilingual dependency parsing using bayes point machines.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT/NAACL-06,</booktitle>
<location>New York.</location>
<contexts>
<context position="25928" citStr="Corston-Oliver et al. (2006)" startWordPosition="4273" endWordPosition="4276">n the model trained using the CCGbank ordering. 14 iteration 1 2 3 4 5 6 Standard order 86.14 86.30 86.53 86.61 86.69 86.72 Shortest first 85.98 86.41 86.57 86.56 86.54 86.53 Longest first 86.25 86.48 86.66 86.72 86.74 86.75 Table 5: F-score of the averaged perceptron on the development data for different data orderings (Q = 0.002) LP LR F CAT 87.50 86.62 87.06 94.08 87.52 86.72 87.12 94.12 87.53 86.67 87.10 94.09 Table 6: Comparison of various perceptron models on the test data Finally, we used the 10 models (including the model from the original training set) to investigate model averaging. Corston-Oliver et al. (2006) motivate model averaging for the perceptron in terms of Bayes Point Machines. The averaged perceptron weights resulting from each permutation of the training data were simply averaged to produce a new model. Table 6 shows that the averaged model again performs only marginally better than the original model, and not as well as the best-performing “random” model, which is perhaps not surprising given the small variation among the performances of the component models. In summary, the perceptron learner appears highly robust to the order of the training examples, at least for this parsing task. 6</context>
</contexts>
<marker>Corston-Oliver, Aue, Duh, Ringger, 2006</marker>
<rawString>S. Corston-Oliver, A. Aue, K. Duh, and E. Ringger. 2006. Multilingual dependency parsing using bayes point machines. In Proceedings ofHLT/NAACL-06, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
<author>David Vadas</author>
</authors>
<title>Multi-tagging for lexicalized-grammar parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-06,</booktitle>
<pages>697--704</pages>
<location>Sydney, Austrailia.</location>
<contexts>
<context position="7632" citStr="Curran et al., 2006" startWordPosition="1196" endWordPosition="1199">he model are defined over local parts of the derivation and include wordword dependencies. A packed chart representation allows efficient decoding, with the Viterbi algorithm finding the most probable derivation. The supertagger is a key part of the system. It uses a log-linear model to define a distribution over the lexical category set for each word and the previous two categories (Ratnaparkhi, 1996) and the forward backward algorithm efficiently sums over all histories to give a distibution for each word. These distributions are then used to assign a set of lexical categories to each word (Curran et al., 2006). Supertagging was first defined for LTAG (Bangalore and Joshi, 1999), and was designed to increase parsing speed for lexicalized grammars by allowing a finite-state tagger to do some of the parsing work. Since the elementary syntactic units in a lexicalized grammar — in LTAG’s case elementary trees and in CCG’s case lexical categories – contain a significant amount of grammatical information, combining them together is easier than the parsing typically performed by phrase-structure parsers. Hence 10 Bangalore and Joshi (1999) refer to supertagging as almost parsing. Supertagging has been espe</context>
<context position="8973" citStr="Curran et al., 2006" startWordPosition="1411" endWordPosition="1414"> through use of a supertagger. The supertagger interacts with the parser in an adaptive fashion. Initially the supertagger assigns a small number of categories, on average, to each word in the sentence, and the parser attempts to create a spanning analysis. If this is not possible, the supertagger assigns more categories, and this process continues until a spanning analysis is found. The number of categories assigned to each word is determined by a parameter Q in the supertagger: all categories are assigned whose forward-backward probabilities are within Q of the highest probability category (Curran et al., 2006). Clark and Curran (2004a) also shows how the supertagger can reduce the size of the packed charts to allow discriminative log-linear training. However, even with the use of a supertagger, the packed charts for the complete CCGbank require over 20 GB of RAM. Reading the training instances into memory one at a time and keeping a record of the relevant feature counts would be too slow for practical development, since the log-linear model requires hundreds of iterations to converge. Hence the packed charts need to be stored in memory. In Clark and Curran (2004b) we use a cluster of 45 machines, t</context>
</contexts>
<marker>Curran, Clark, Vadas, 2006</marker>
<rawString>James R. Curran, Stephen Clark, and David Vadas. 2006. Multi-tagging for lexicalized-grammar parsing. In Proceedings of COLING/ACL-06, pages 697–704, Sydney, Austrailia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Efficient normal-form parsing for Combinatory Categorial Grammar.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Meeting of the ACL,</booktitle>
<pages>79--86</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="17109" citStr="Eisner (1996" startWordPosition="2804" endWordPosition="2805"> parameter for the supertagger was fixed (for the first set of experiments at 0.004). The supertagger was then run over the sentences in Sections 2-21 of CCGbank. We made sure that every word was assigned the correct lexical category among its set (we did not do this for testing). Then the parser was run on the supertagged sentences, using the CKY algorithm and the CCG combinatory rules. We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can 12 only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints. This part of the process requires a few hundred MB of RAM to run the parser, and takes a few hours for Sections 2-21 of CCGbank. Any further training times or memory requirements reported do not include the resources needed to create the forests. The feature forests are extracted from the packed chart representation used in the parser. We only use a feature forest for training if it contains the correct derivation (according to CCGbank). Some forests do not have the correct derivation, even though we ensure the correct lexical categories are present, because the gra</context>
<context position="27008" citStr="Eisner, 1996" startWordPosition="4449" endWordPosition="4450">s. In summary, the perceptron learner appears highly robust to the order of the training examples, at least for this parsing task. 6 Comparison with Other Work Taskar et al. (2004) investigate discriminative training methods for a phrase-structure parser, and also use dynamic programming for the decoder. The key difference between our work and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding. There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). The main drawback with this approach is that the correct parse may get lost in the first phase. The existing work most similar to ours is Collins and Roark (2004). They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation. The main difference is that we are able to store the complete forests for training, and can guarantee that the forest contains the correct deriva</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996a. Efficient normal-form parsing for Combinatory Categorial Grammar. In Proceedings of the 34th Meeting of the ACL, pages 79–86, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th COLING Conference,</booktitle>
<pages>340--345</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="17109" citStr="Eisner (1996" startWordPosition="2804" endWordPosition="2805"> parameter for the supertagger was fixed (for the first set of experiments at 0.004). The supertagger was then run over the sentences in Sections 2-21 of CCGbank. We made sure that every word was assigned the correct lexical category among its set (we did not do this for testing). Then the parser was run on the supertagged sentences, using the CKY algorithm and the CCG combinatory rules. We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can 12 only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints. This part of the process requires a few hundred MB of RAM to run the parser, and takes a few hours for Sections 2-21 of CCGbank. Any further training times or memory requirements reported do not include the resources needed to create the forests. The feature forests are extracted from the packed chart representation used in the parser. We only use a feature forest for training if it contains the correct derivation (according to CCGbank). Some forests do not have the correct derivation, even though we ensure the correct lexical categories are present, because the gra</context>
<context position="27008" citStr="Eisner, 1996" startWordPosition="4449" endWordPosition="4450">s. In summary, the perceptron learner appears highly robust to the order of the training examples, at least for this parsing task. 6 Comparison with Other Work Taskar et al. (2004) investigate discriminative training methods for a phrase-structure parser, and also use dynamic programming for the decoder. The key difference between our work and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding. There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). The main drawback with this approach is that the correct parse may get lost in the first phase. The existing work most similar to ours is Collins and Roark (2004). They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation. The main difference is that we are able to store the complete forests for training, and can guarantee that the forest contains the correct deriva</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996b. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th COLING Conference, pages 340–345, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="2323" citStr="Freund and Schapire, 1999" startWordPosition="355" endWordPosition="358">the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars. In Clark and Curran (2004b) we use cluster computing resources to solve this problem. Parsing research has also begun to adopt discriminative methods from the Machine Learning literature, such as the perceptron (Freund and Schapire, 1999; Collins and Roark, 2004) and the largemargin methods underlying Support Vector Machines (Taskar et al., 2004; McDonald, 2006). Parser training involves decoding in an iterative process, updating the model parameters so that the decoder performs better on the training data, according to some training criterion. Hence, for efficient training, these methods require an efficient decoder; in fact, for methods like the perceptron, the update procedure is so trivial that the training algorithm essentially is decoding. This paper describes a decoder for a lexicalizedgrammar parser which is efficient</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>335--342</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="9808" citStr="Hockenmaier and Steedman (2002)" startWordPosition="1552" endWordPosition="1555">ts for the complete CCGbank require over 20 GB of RAM. Reading the training instances into memory one at a time and keeping a record of the relevant feature counts would be too slow for practical development, since the log-linear model requires hundreds of iterations to converge. Hence the packed charts need to be stored in memory. In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. The need for cluster computing resources presents a barrier to the development of further CCG parsing models. Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. In this paper we propose the perceptron algorithm as a solution. The perceptron is an online learning algorithm, and so the parameters are updated one training instance at a time. However, the key difference compared with the loglinear training is that the perceptron converges in many fewer iterations, and so it is practical to read the training instances into memory one</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical parsing with Combinatory Categorial Grammar. In Proceedings of the 40th Meeting of the ACL, pages 335–342, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="6457" citStr="Hockenmaier, 2003" startWordPosition="1014" endWordPosition="1015">ture parser, the first work we are aware of to do so. Second, we provide a practical framework for developing discriminative models for CCG, reducing the memory requirements from over 20 GB to a few hundred MB. And third, given the significantly shorter training time compared to other discriminative parsing models (Taskar et al., 2004), we provide a practical framework for investigating discriminative training methods more generally. 2 The CCG Parser Clark and Curran (2004b) describes the CCG parser. The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories, expressing subcategorisation information, plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence, which are then combined by the parser using the combinatory rules and the CKY algorithm. A log-linear model scores the alternative parses. We use the normalform model, which assigns probabilities to single derivations based on the normal-form derivations in CCGbank. The features in the model are defined over local parts of the </context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic ‘unificationbased’ grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Meeting of the ACL,</booktitle>
<pages>535--541</pages>
<institution>University of Maryland, MD.</institution>
<contexts>
<context position="1583" citStr="Johnson et al., 1999" startWordPosition="244" endWordPosition="247">chine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and 9 Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars. In Clark and Curran (2004b) we use cluster computing resources to solve this problem. Parsing res</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic ‘unificationbased’ grammars. In Proceedings of the 37th Meeting of the ACL, pages 535–541, University of Maryland, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP-04 Workshop: Beyond</booktitle>
<location>Hainan Island, China.</location>
<marker>Malouf, van Noord, 2004</marker>
<rawString>Robert Malouf and Gertjan van Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In Proceedings of the IJCNLP-04 Workshop: Beyond shallow analyses - Formalisms and statistical modelingfor deep analyses, Hainan Island, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative Training and Spanning Tree Algorithms for Dependency Parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2450" citStr="McDonald, 2006" startWordPosition="377" endWordPosition="378"> inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars. In Clark and Curran (2004b) we use cluster computing resources to solve this problem. Parsing research has also begun to adopt discriminative methods from the Machine Learning literature, such as the perceptron (Freund and Schapire, 1999; Collins and Roark, 2004) and the largemargin methods underlying Support Vector Machines (Taskar et al., 2004; McDonald, 2006). Parser training involves decoding in an iterative process, updating the model parameters so that the decoder performs better on the training data, according to some training criterion. Hence, for efficient training, these methods require an efficient decoder; in fact, for methods like the perceptron, the update procedure is so trivial that the training algorithm essentially is decoding. This paper describes a decoder for a lexicalizedgrammar parser which is efficient enough for practical discriminative training. We use a lexicalized phrase-structure parser, the CCG parser of Clark and Curran</context>
<context position="26937" citStr="McDonald, 2006" startWordPosition="4440" endWordPosition="4441">ng given the small variation among the performances of the component models. In summary, the perceptron learner appears highly robust to the order of the training examples, at least for this parsing task. 6 Comparison with Other Work Taskar et al. (2004) investigate discriminative training methods for a phrase-structure parser, and also use dynamic programming for the decoder. The key difference between our work and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding. There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). The main drawback with this approach is that the correct parse may get lost in the first phase. The existing work most similar to ours is Collins and Roark (2004). They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation. The main difference is that we are able to store the complete forests for </context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative Training and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="1967" citStr="Miyao and Tsujii, 2002" startWordPosition="302" endWordPosition="305">aining methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and 9 Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars. In Clark and Curran (2004b) we use cluster computing resources to solve this problem. Parsing research has also begun to adopt discriminative methods from the Machine Learning literature, such as the perceptron (Freund and Schapire, 1999; Collins and Roark, 2004) and the largemargin methods underlying Support Vector Machines (Taskar et al., 2004; McDonald, 2006). Parser training involves decoding in an iterative process, updating the model parameters so that the decoder perfor</context>
<context position="13334" citStr="Miyao and Tsujii (2002)" startWordPosition="2162" endWordPosition="2165">tions which the training algorithm 11 uses to “discriminate against”, with a smaller value of Q resulting in more derivations. 3.1 Feature Forests The same decoder is used for both training and testing: the Viterbi algorithm. However, the packed representation of GEN(x) in each case is different. When running the parser, a lot of grammatical information is stored in order to produce linguistically meaningful output. For training, all that is required is a packed representation of the features on each derivation in GEN(x) for each sentence in the training data. The feature forests described in Miyao and Tsujii (2002) provide such a representation. Clark and Curran (2004b) describe how a set of CCG derivations can be represented as a feature forest. The feature forests are created by first building packed charts for the training sentences, and then extracting the feature information. Packed charts group together equivalent chart entries. Entries are equivalent when they interact in the same manner with both the generation of subsequent parse structure and the numerical parse selection. In practice, this means that equivalent entries have the same span, and form the same structures and generate the same fea</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proceedings of the Human Language Technology Conference, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd meeting of the ACL,</booktitle>
<pages>83--90</pages>
<institution>University of Michigan,</institution>
<location>Ann Arbor.</location>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the 43rd meeting of the ACL, pages 83–90, University of Michigan, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-ofspeech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>133--142</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="7417" citStr="Ratnaparkhi, 1996" startWordPosition="1161" endWordPosition="1162">he CKY algorithm. A log-linear model scores the alternative parses. We use the normalform model, which assigns probabilities to single derivations based on the normal-form derivations in CCGbank. The features in the model are defined over local parts of the derivation and include wordword dependencies. A packed chart representation allows efficient decoding, with the Viterbi algorithm finding the most probable derivation. The supertagger is a key part of the system. It uses a log-linear model to define a distribution over the lexical category set for each word and the previous two categories (Ratnaparkhi, 1996) and the forward backward algorithm efficiently sums over all histories to give a distibution for each word. These distributions are then used to assign a set of lexical categories to each word (Curran et al., 2006). Supertagging was first defined for LTAG (Bangalore and Joshi, 1999), and was designed to increase parsing speed for lexicalized grammars by allowing a finite-state tagger to do some of the parsing work. Since the elementary syntactic units in a lexicalized grammar — in LTAG’s case elementary trees and in CCG’s case lexical categories – contain a significant amount of grammatical i</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy part-ofspeech tagger. In Proceedings of the EMNLP Conference, pages 133–142, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>271--278</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1380" citStr="Riezler et al., 2002" startWordPosition="210" endWordPosition="213"> using a finite-state supertagger to do much of the parsing work, resulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and 9 Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, th</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell III, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proceedings of the 40th Meeting of the ACL, pages 271–278, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6635" citStr="Steedman, 2000" startWordPosition="1038" endWordPosition="1039">er 20 GB to a few hundred MB. And third, given the significantly shorter training time compared to other discriminative parsing models (Taskar et al., 2004), we provide a practical framework for investigating discriminative training methods more generally. 2 The CCG Parser Clark and Curran (2004b) describes the CCG parser. The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories, expressing subcategorisation information, plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence, which are then combined by the parser using the combinatory rules and the CKY algorithm. A log-linear model scores the alternative parses. We use the normalform model, which assigns probabilities to single derivations based on the normal-form derivations in CCGbank. The features in the model are defined over local parts of the derivation and include wordword dependencies. A packed chart representation allows efficient decoding, with the Viterbi algorithm finding the most probable derivation. The supert</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the EMNLP conference,</booktitle>
<pages>1--8</pages>
<location>Barcelona,</location>
<contexts>
<context position="1401" citStr="Taskar et al., 2004" startWordPosition="214" endWordPosition="217">supertagger to do much of the parsing work, resulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and 9 Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements</context>
<context position="6176" citStr="Taskar et al., 2004" startWordPosition="969" endWordPosition="972"> impact on the results, showing that the perceptron learner — at least for this parsing task — is robust to the order of the training examples. The contributions of this paper are as follows. First, we compare perceptron and log-linear parsing models for a wide-coverage phrase-structure parser, the first work we are aware of to do so. Second, we provide a practical framework for developing discriminative models for CCG, reducing the memory requirements from over 20 GB to a few hundred MB. And third, given the significantly shorter training time compared to other discriminative parsing models (Taskar et al., 2004), we provide a practical framework for investigating discriminative training methods more generally. 2 The CCG Parser Clark and Curran (2004b) describes the CCG parser. The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories, expressing subcategorisation information, plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence, which are then combined by the parser using the </context>
<context position="26576" citStr="Taskar et al. (2004)" startWordPosition="4379" endWordPosition="4382">r the perceptron in terms of Bayes Point Machines. The averaged perceptron weights resulting from each permutation of the training data were simply averaged to produce a new model. Table 6 shows that the averaged model again performs only marginally better than the original model, and not as well as the best-performing “random” model, which is perhaps not surprising given the small variation among the performances of the component models. In summary, the perceptron learner appears highly robust to the order of the training examples, at least for this parsing task. 6 Comparison with Other Work Taskar et al. (2004) investigate discriminative training methods for a phrase-structure parser, and also use dynamic programming for the decoder. The key difference between our work and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding. There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers. There is also work on discriminative models for parse reranking (</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. 2004. Max-margin parsing. In Proceedings of the EMNLP conference, pages 1–8, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>I Dan Melamed</author>
</authors>
<title>Advances in discriminative parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-06,</booktitle>
<pages>873--880</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1453" citStr="Turian and Melamed, 2006" startWordPosition="222" endWordPosition="225">sulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and 9 Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements can be prohibitive, especially for automatically ex</context>
</contexts>
<marker>Turian, Melamed, 2006</marker>
<rawString>Joseph Turian and I. Dan Melamed. 2006. Advances in discriminative parsing. In Proceedings of COLING/ACL-06, pages 873–880, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>