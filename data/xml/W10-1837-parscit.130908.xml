<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001690">
<title confidence="0.855847">
PackPlay: Mining semantic data in collaborative games
</title>
<note confidence="0.9261075">
Nathan Green Paul Breimyer Vinay Kumar Nagiza F. Samatova
NC State University NC State University NC State University Oak Ridge National Lab
890 Oval Drive 890 Oval Drive 890 Oval Drive 1 Bethel Valley Rd
Raleigh, NC 27695 Raleigh, NC 27695 Raleigh, NC 27695 Oak Ridge, TN 37831
</note>
<sectionHeader confidence="0.976592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999910576923077">
Building training data is labor-intensive
and presents a major obstacle to advanc-
ing machine learning technologies such as
machine translators, named entity recog-
nizers (NER), part-of-speech taggers, etc.
Training data are often specialized for a
particular language or Natural Language
Processing (NLP) task. Knowledge cap-
tured by a specific set of training data is
not easily transferable, even to the same
NLP task in another language. Emerging
technologies, such as social networks and
serious games, offer a unique opportunity
to change how we construct training data.
While collaborative games have been used
in information retrieval, it is an open is-
sue whether users can contribute accurate
annotations in a collaborative game con-
text for a problem that requires an exact
answer, such as games that would create
named entity recognition training data. We
present PackPlay, a collaborative game
framework that empirically shows players’
ability to mimic annotation accuracy and
thoroughness seen in gold standard anno-
tated corpora.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939423076923">
Annotated corpora are sets of structured text
used in Natural Language Processing (NLP) that
contain supplemental knowledge, such as tagged
parts-of-speech, semantic concepts assigned to
phrases, or semantic relationships between these
concepts. Machine Learning (ML) is a subfield of
Artificial Intelligence that studies how computers
can obtain knowledge and create predictive mod-
els. These models require annotated corpora to
learn rules and patterns. However, these anno-
tated corpora must be manually curated for each
domain or task, which is labor intensive and te-
dious (Scannell, 2007), thereby creating a bot-
tleneck for advancing ML and NLP prediction
tools. Furthermore, knowledge captured by a spe-
cific annotated corpus is often not transferable to
another task, even to the same NLP task in an-
other language. Domain and language specific
corpora are useful for many language technol-
ogy applications, including named entity recogni-
tion (NER), machine translation, spelling correc-
tion, and machine-readable dictionaries. The An
Cr´ubad´an Project, for example, has succeeded in
creating corpora for more than 400 of the world’s
6000+ languages by Web crawling. With a few ex-
ceptions, most of the 400+ corpora, however, lack
any linguistic annotations due to the limitations of
annotation tools (Rayson et al., 2006).
Despite the many documented advantages of
annotated data over raw data (Granger and
Rayson, 1998; Mair, 2005), there is a dearth of
annotated corpora in many domains. The ma-
jority of previous corpus annotation efforts re-
lied on manual annotation by domain experts,
automated prediction tagging systems, and hy-
brid semi-automatic systems that used both ap-
proaches. While yielding high quality and enor-
mously valuable corpora, manually annotating
corpora can be prohibitively costly and time con-
suming. For example, the GENIA corpus contains
9,372 sentences, curated by five part-time annota-
tors, one senior coordinator, and one junior coor-
dinator over 1.5 years (Kim et al., 2008). Semi-
automatic approaches decrease human effort but
often introduce significant error, while still requir-
ing human interaction.
The Web can help facilitate semi-automatic ap-
proaches by connecting distributed human users
at a previously unfathomable scale and presents
an opportunity to expand annotation efforts to
countless users using Human Computation, the
concept of outsourcing certain computational
</bodyText>
<page confidence="0.965317">
227
</page>
<note confidence="0.955962">
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 227–234,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99973670212766">
processes to humans, generally to solve prob-
lems that are intractable or difficult for comput-
ers. This concept is demonstrated in our previ-
ous work, WebBANC (Green et al., 2009) and
BioDEAL (Breimyer et al., 2009), which allows
users to annotate Web documents through a Web
browser plugin for the purposes of creating lin-
guistically and biologically tagged annotated cor-
pora and with micro-tasking via Mechanical Turk,
which allows for a low cost option for manual la-
bor tasks (Snow et al., 2008; Kittur et al., 2008).
While the Web and Human Computation may
be a powerful tandem for generating data and
solving difficult problems, in order to succeed,
users must be motivated to participate. Humans
have been fascinated with games for centuries
and play them for many reasons, including for
entertainment, honing skills, and gaining knowl-
edge (FAS Summit, 2006). Every year, a large
amount of hours are spent playing online computer
games. The games range form simple card and
word games to more complex 3-D world games.
One such site for word, puzzle, and card games is
Pogo.com1. According to protrackr,2 Pogo has al-
most 6 million unique visitors a day. Alexa.com3
shows that the average user is on the site for 11
minutes at a time. When the average time spent on
the site is propagated to each user, the combined
time is equal to more than 45,000 days of human
time. Arguably if, the games on Pogo were used
to harvest useful data, various fields of Computer
Science research could be advanced.
There has been a recent trend to leverage hu-
man’s fascination in game playing to solve diffi-
cult problems through Human Computation. Two
such games include ESP and Google’s Image La-
beler (Ahn and Dabbish, 2004), in which play-
ers annotate images in a cooperative environment
to correctly match image tags with their partner.
Semantic annotation has also been addressed in
the game Phrase Detectives (Chamberlain et al.,
2009), which has the goal of creating large scale
training data for anaphora resolution. These types
of games are part of a larger, serious games, initia-
tive (Annetta, 2008).
This paper introduces the Web-enabled collabo-
rative game framework, PackPlay, and investigates
</bodyText>
<footnote confidence="0.9975">
1Pogo. http://www.pogo.com/
2Protrackr.com site information and statistis-
tics.http://www.protrackr.com/
3Alexa: The Web Information Company.
http://www.alexa.com/
</footnote>
<bodyText confidence="0.999740095238095">
how collaborative online gaming can affect anno-
tation throughput and annotation accuracy. There
are two main questions for such systems: first,
will overall throughput increase compared to tra-
ditional methods of annotating, such as the man-
ual construction of the Genia Corpus? Second,
how accurate are the collective annotations? A
successful human computation environment, such
as PackPlay, would represent a paradigm shift in
the way annotated corpora are created. However,
adoption of such a framework cannot be expected
until these questions are answered. We address
both of these questions in multiple games in our
PackPlay system through evaluation of the collec-
tive players’ annotations with precision and recall
to judge accuracy of players’ annotations and the
number of games played to judge throughput. We
show improvements in both areas over traditional
annotation methods and show accuracy compara-
ble to expert prediction systems that could be used
for semi-supervised annotation.
</bodyText>
<sectionHeader confidence="0.993572" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999982">
We empirically show casual game players’ abil-
ity to accurately and throughly annotate corpora
by conducting experiments following the process
described in Section 2.1 with 8 players using
the PackPlay System. The testers annotate the
datasets described in Section 2.2 and results are
analyzed using the equations in Section 2.3.
</bodyText>
<subsectionHeader confidence="0.884109">
2.1 PackPlay Process Flow
</subsectionHeader>
<bodyText confidence="0.999903631578947">
Figure 1 shows the average PackPlay process flow
that a player will follow for a multi-player game.
Assuming the player is registered, the player will
always start by logging in and selecting the game
he or she wants to play. Once in the game screen,
the system will try to pair the player with another
player who is waiting. After a set time limit,
the game will automatically pair the user with a
PlayerBot. It is important to note that the player
will not know that his or her partner is a Player-
Bot.
Once paired, a game can start. In most games, a
question will be sampled from our database. How
this sampling takes place is up to the individual
game. Once sampled, the question will be dis-
played to one player or all players, depending on
whether the game is synchronous or asynchronous
(see definitions in Sections 3.1.2 and 3.2.2). Once
the question is displayed, two things can happen.
</bodyText>
<page confidence="0.996502">
228
</page>
<figureCaption confidence="0.999726">
Figure 1: User process flow for PackPlay games.
</figureCaption>
<bodyText confidence="0.999527222222222">
First, the timer can run out; this timer is set by each
game individually. Second, the player may answer
the question and move on to the next question. Af-
ter either one of those two options, a new question
will be sampled. This cycle continues until the
game session is over. This is usually determined
by the game, as each game can set the number of
questions in a session, or by a player quiting the
game.
</bodyText>
<subsectionHeader confidence="0.998668">
2.2 Data Sources
</subsectionHeader>
<bodyText confidence="0.99999525">
To compare named entity results, PackPlay uses
sentences and annotations from CoNLL 2003, a
“gold” standard corpus (Tjong et al., 2003). We
use the CoNLL 2003 corpus since it has been cu-
rated by experts and the PackPlay system can com-
pare our players’ annotations vs those of 16 sub-
mitted predictive models, also refered to as the
CoNLL average, in the 2003 conference on nat-
ural language learning. This paper will refer to the
training corpus as the CoNLL corpus, and we se-
lected it for our evaluation due to its widespread
adoption as a benchmark corpus.
</bodyText>
<subsectionHeader confidence="0.997248">
2.3 Metrics
</subsectionHeader>
<bodyText confidence="0.9999682">
To measure how thoroughly and accurately our
players annotate the data, we calculate both recall
(Equation 1) and precision (Equation 2), in which
α is the set of words annotated in PackPlay and β
is the set of words in the base CoNLL corpus.
</bodyText>
<equation confidence="0.98828">
Recall = |α ∩ β  |(1)
|β|
Precision = |α ∩ β  |(2)
|α|
</equation>
<bodyText confidence="0.999895125">
Each game module in the PackPlay system has
its own scoring module, which is intended to im-
prove the players’ precision. For this reason, scor-
ing is handled on a per game level. Each game has
its own leader board as well. The leader board is
used to motivate the players to continue playing
the PackPlay games. This is intended to improve
recall for annotations in the system.
</bodyText>
<sectionHeader confidence="0.999169" genericHeader="method">
3 Games
</sectionHeader>
<subsectionHeader confidence="0.99541">
3.1 Entity Discovery
3.1.1 Game description
</subsectionHeader>
<bodyText confidence="0.999986208333333">
Named entities are a foundational part of many
NLP systems from information extraction sys-
tems to machine translation systems. The abil-
ity to detect an entity is an application area called
Named Entity Recognition (NER). The most com-
mon named entity categories are Person (Per), Lo-
cation (Loc), and Organization (Org). The ability
to extract these entities may be used in everyday
work, such as extracting defendants, cities, and
companies from court briefings, or it may be used
for critical systems in national defense, such as
monitoring communications for people and loca-
tions of interest.
To help with the creation of more NER systems,
Entity Discovery (see Figure 2), a game for an-
notating sentences with supplied entities was cre-
ated. The goal of the game is to pair players with
each other and allow them to annotate sentences
together. While this annotation task could be done
by one person, it is a very time consuming activ-
ity. By creating a game, we hope that players will
be more likely to annotate for fun and will anno-
tate correctly and completely in order to receive a
higher score in the PackPlay system.
</bodyText>
<subsectionHeader confidence="0.908876">
3.1.2 Implementation
</subsectionHeader>
<bodyText confidence="0.9995051">
Entity Discovery is implemented as a synchronous
two-player game. A synchronous game is one in
which both players have the same task in the game,
in this case, to annotate a sentence. To have a base
comparison point, all players are asked to annotate
a random set of 60 sentences to start, for which we
have the correct answers. This way we will be able
to assess the trustworthiness score in future itera-
tions. After the pretest, the players will be shown
sentences randomly sampled with replacement.
</bodyText>
<figure confidence="0.998672333333333">
Start Screen w/ leader
board
Annotate
Display
Question
Timer Done?
expired? No
Has
Partner?
Select
Game
Login
Yes
No
Yes
Yes
Yes
Wait for Partner
for X seconds
Pair with Bot
No
Has
Partner?
More
Questions?
Yes
No
No
End
Game
</figure>
<page confidence="0.785774">
229
</page>
<figureCaption confidence="0.999886">
Figure 2: Screenshot of a player annotating the Person entity Jimi Hendrix
</figureCaption>
<bodyText confidence="0.99993845">
In Entity Discovery, we made a design decision
to keep a player’s partner anonymous. This should
help reduce cheating, such as agreeing to select
the same word over and over, and it should reduce
the ability for a player to only play with his or
her friends, which might enhance their ability to
cheat by using other communication systems such
as instant messaging or a cell phone. Since Pack-
Play is still in the experimental stages, players may
not always be available. For this reason, we have
implemented a PlayerBot system. The PlayerBot
will mimic another player by selecting previously
annotated phrases for a given sentence from the
database. From the human players’ point of view,
nothing seems different.
Players are asked to annotate, or tag, as many
entities as they can find in a sentence. Players are
also told at the beginning of the game that they are
paired with another user. Their goal is to annotate
the same things as their partner. Our assumption
is that if the game is a single player game then the
players may just annotate the most obvious enti-
ties for gaining more points. By having the player
to try to guess at what their partner may anno-
tate we hope to get better overall coverage of enti-
ties. We try to minimize the errors, which guess-
ing might produce, in a second game, Name That
Entity (Section 3.2).
To annotate a sentence, the player simply high-
lights a word or phrase and clicks on a relevant
entity. For instance in Entity Discovery, a player
can annotate the phrase “Jimi Hendrix” as a Per-
son entity. From this point on, the player is free
to annotate more phrases in the sentence. When
the player completes annotating a sentence, the
player hits “Next Problem.” The system then waits
for the player’s partner to hit “Next Problem” as
well. When both players finish annotating, the
game points will be calculated and a new question
will be sampled for the players.
</bodyText>
<page confidence="0.966174">
230
</page>
<table confidence="0.998915333333333">
Statistic Total Mean
# of games 29 3.62
# of annotations 291 40.85
</table>
<tableCaption confidence="0.889382">
Table 1: Statistics returned from our user study for
</tableCaption>
<figure confidence="0.438286">
the game Entity Discovery
</figure>
<figureCaption confidence="0.987369">
Figure 3: Screenshot of what the player sees at the
end of the Entity Discovery game
</figureCaption>
<subsectionHeader confidence="0.938313">
3.1.3 Scoring
</subsectionHeader>
<bodyText confidence="0.999889722222222">
Scoring can be done in a variety of ways, each hav-
ing an impact on players’ performance and enjoy-
ment. For Entity Discovery, we decided to give
each user a flat score of 100 points for every an-
swer that matched their partner. At the end of
each game session, the player will see what an-
swers matched with their partner. For instance, if
both players tagged “Jimi Hendrix” as a Person,
they will both receive 100 points. We do not show
the players their matched scores after each sen-
tence, since this might bias the user to tag more
or less depending on what their partner does. Fig-
ure 3 shows a typical scoring screen at the end
of a game; in Figure 3, the players matched 4
phrases, totaling 400 points. It is important to note
that at this stage we do not distinguish between
correct and incorrect annotations, just whether the
two players agree.
</bodyText>
<subsectionHeader confidence="0.687993">
3.1.4 User Case Study Methodology
</subsectionHeader>
<bodyText confidence="0.999975186046511">
To examine Entity Discovery as a collaborative
game toward the creation of an annotated corpus,
we conducted a user experiment to collect sam-
ple data on a known data set. Over a short time,
8 players were asked to play both Entity Discov-
ery and Name That Entity. In PackPlay, through-
put can be estimated, since each game has a de-
fined time limit, defined as the average number
of entities annotated per question times the num-
ber of users times the average number of ques-
tions seen by a user. Unlike other systems such as
Mechanical Turk (Snow et al., 2008; Kittur et al.,
2008), BioDeal (Breimyer et al., 2009), or Web-
BANC (Green et al., 2009), in PackPlay we define
the speed at which a user annotates.
Each game in Entity Discovery consists of 10
sentences from the CoNLL corpus. These sen-
tences are not guaranteed to have a named en-
tity within them. The users in the study were not
informed of the entity content as to not bias the
experiment and falsely raise our precision scores.
With only 8 players, we obtained 291 annotations,
which averaged to about 40 annotations per user.
This study was not done over a long period of time,
so each user only played, on average, 3.6 games.
Two players were asked to intentionally anno-
tate poorly. The goal of using poor annotators
was to simulate real world players, who may just
click answers to ruin the game or who are clue-
less to what a named entity is. This information
can be used in later research to help automatically
detect “bad” annotators using anomaly detection
techniques.
PackPlay also stores information not used in
this study, such as time stamps for each question
answered. This information will be incorporated
into future experiment analysis to see if we can
further improve our annotated corpora based on
the order and time spent forming an annotation.
For instance, the first annotation in a sentence may
have a higher probability than the last annotation.
It is possible that if a user answers too fast, the
answer is likely an error.
</bodyText>
<subsectionHeader confidence="0.791904">
3.1.5 Output Quality
</subsectionHeader>
<bodyText confidence="0.999981777777778">
Every player completes part of a 60 sentence
pretest in which we know the answers. For each
game, the questions are sampled without replace-
ment but this does not carry over after a game.
For instance, if a player finishes game 1, he or she
will never see the same question twice. For game
two, no question within the game will be repeated,
however, the player might see a question he or she
answered in game 1. Because of this, each user
will not see all 60 questions, but we will have a
good sample to judge whether a user is accurate
or not. The ability to repeat a question in different
games allows us, in future research, to test play-
ers using intra-annotator agreement statistics. This
tests how well a player agrees with himself or her-
self. From this set of 60 questions we have calcu-
lated each player’s recall and precision scores.
As Table 2 shows, the recall scores for Entity
</bodyText>
<page confidence="0.999193">
231
</page>
<tableCaption confidence="0.9826095">
Table 2: Recall and precision for Entity Discovery
annotations of CoNLL data.
</tableCaption>
<table confidence="0.999071666666667">
Per Loc Org Avg CoNLL
Avg
Recall 0.94 0.95 0.85 0.9 0.82
(All Data)
Precision 0.47 0.70 0.53 0.62 0.83
(All Data)
</table>
<bodyText confidence="0.997368121951219">
Discovery in this experiment were 0.94, 0.95, and
0.85 for Person, Location, and Organization, re-
spectively. The overall average was 0.9, which
beats out the CoNLL average, an average of 16
expert systems, for recall. Entity Discovery’s num-
bers are similar to the pattern seen in the CoNLL
predictive systems for Person, Location and Or-
ganization, in which Organization was the lowest
and Person was the highest. The precision num-
bers were quite lower, with an average of 0.62.
When examining the data, most of the precision
errors occurred because of word phrase boundary
issues with the annotation and also players often
are unsure whether to include titles such as Presi-
dent, Mr., or Dr. There were also quite a few errors
where players annotated concepts as People such
as “The Judge” or “The scorekeeper.” While this is
incorrect for named entity recognition, it might be
of interest to a co-reference resolution corpus. The
precision numbers are likely low because of our
untrained players and because some of the players
were told to intentionally annotate entities incor-
rectly. To improve on these numbers, we applied
a coverage requirement and majority voting. The
coverage requirement requires that more than one
player has annotated a given phrase for the an-
notation to be included in the corpus. Majority
voting indicates that the phrase is only included
if 50% or more of the playerss who annotated a
phrase, agreed on the specific entity assigned to
the phrase.
As Table 3 shows, both majority voting and
coverage requirements improve precision by more
than 10%. When combined, they improve the
overall precision to 0.88, a 26% improvement.
This is an improvement to the expert CoNLL sys-
tems score of 0.83. The majority voting likely
removed the annotations from our purposefully
“bad” annotators.
For future work, as the number of players in-
creases, we will have to increase our coverage re-
</bodyText>
<tableCaption confidence="0.95507">
Table 3: Precision for Entity Discovery annota-
tions of CoNLL data with filtering
</tableCaption>
<table confidence="0.998299">
Per Loc Org Avg
Precision 0.56 0.79 0.65 0.72
(Majority Voting)
Precision 0.69 0.83 0.63 0.73
(Coverage Req.)
Precision 0.90 0.95 0.81 0.88
(Majority Voting +
Coverage Req.)
</table>
<bodyText confidence="0.997152571428571">
quirement to match. This ratio has not been deter-
mined and will need to be tested. A more success-
ful way to detect errors in our annotations may be
to create a separate game to verify given answers.
To initially test this concept we have made and set
up an experiment with a game, called Name That
Entity.
</bodyText>
<subsectionHeader confidence="0.9988945">
3.2 Name That Entity
3.2.1 Game Description
</subsectionHeader>
<bodyText confidence="0.999956">
Name That Entity is another game with a focus
on named entities. Name That Entity was created
to show that game mechanics and the creation of
further games would enhance the value of an an-
notated corpus. In the case of Name That Entity,
we have created a multiple choice game in which
the player will select the entity that best represents
the highlighted word or phrase. Unlike Entity Dis-
covery, this allows us to focus the annotation ef-
fort on particular words or phrases. Once again,
this is modeled as a two-player game but the play-
ers are not playing simultaneously. The goal for
the player is to select the same entity type for the
highlighted word that their partner selects. In this
game, speed is of the essence since each question
will ask for one entity as opposed to Entity Discov-
ery, which was open ended to how many entities
might exist in a sentence.
</bodyText>
<subsectionHeader confidence="0.91367">
3.2.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999819125">
As described above, Name That Entity appears to
be a two-player synchronous game. The player
is under the assumption that he or she must once
again match his or her partner’s choice. What the
player does not know is that the multi-player is
simulated in this case. The player is replaced with
a PlayerBot which chooses annotations from the
Entity Discovery game. This, in essence, creates
</bodyText>
<page confidence="0.985131">
232
</page>
<bodyText confidence="0.999928136363636">
an asynchronous game, in which one player has
the task of finding entities and the other player has
the task of verifying entities. This gives us a fur-
ther mechanism to check the validity of entities an-
notated by the Entity Discovery game.
As with Entity Discovery, the player’s partner is
anonymous. This anonymity allows us to keep the
asynchronous structure hidden, as well as judge a
new metric, intra-annotator agreement, not tested
in the previous game. Since it is possible that a
player in PackPlay may have a question sampled
that was previously annotated in the Entity Dis-
covery game by the same player, we can use intra-
annotator agreement. While well-known inter-
annotator statistics, such as Cohen’s Kappa, evalu-
ate one annotator versus the other annotator, intra-
annotator statistics allow us to judge an annota-
tor versus himself or herself to test for consis-
tency (Artstein and Poesio, 2008). In the Pack-
Play framework this allows us to detect playerss
who are randomly guessing and are therefore not
consistent with themselves.
</bodyText>
<subsectionHeader confidence="0.930696">
3.2.3 Scoring
</subsectionHeader>
<bodyText confidence="0.999993958333333">
Since entity coverage of a sentence is not an is-
sue in the multiple choice game, we made use of
a different scoring system that would reward first
instincts. While the Entity Discovery game has a
set score for every answer, Name That Entity has a
sliding scale. For each question, the max score is
100 points, as the time ticks away the user receives
fewer points. The points remaining are indicated
to the user via a timing bar at the bottom of the
screen.
When the player completes a game, he or she
is allowed to view the results for that game. Un-
like the Entity Discovery game, we display to the
player what entity his or her partner chooses on
the question in which they both did not match.
This gives us a quick and simple form of annotator
training, since a player with no experience may not
be familiar with a particular entity. This was seen
with the players’ ability to detect an Organization
entity. We expect that when a player sees what
his or her partner annotates a phrase as, the player,
is, in effect, being trained. However, displaying
this at the end should not have any affects toward
cheating since their partners are anonymous.
</bodyText>
<sectionHeader confidence="0.366503" genericHeader="method">
3.2.4 User Case Study Methodology
</sectionHeader>
<bodyText confidence="0.998320125">
Of the 8 players who participated in the Entity Dis-
covery study, 7 also played Name That Entity dur-
ing their game sessions. We did not inform the
players, but the questions asked in Name That En-
tity were the same answers that the players gave in
the experiment in Section 3.1.4. The basic anno-
tation numbers from our small user study can be
seen in Table 4.
</bodyText>
<tableCaption confidence="0.8115645">
Table 4: Statistics returned from our user study for
the game Name That Entity
</tableCaption>
<table confidence="0.998172">
Statistic Total Mean
# of games 20 2.85
# of annotations 195 27.85
</table>
<subsubsectionHeader confidence="0.468739">
3.2.5 Output Quality
</subsubsectionHeader>
<bodyText confidence="0.999395142857143">
As Name That Entity is not intended to be a solo
mechanism to generate annotations, but instead
a way to verify existing annotations, we did not
assess the recall and precision of the game. In-
stead we are looking at the number of annota-
tions, unique annotations, and conflicting annota-
tions generated by our players in this game.
</bodyText>
<tableCaption confidence="0.957606">
Table 5: Types of annotations generated by Name
</tableCaption>
<table confidence="0.813190333333333">
That Entity
Error Count
Annotations 195
Unique Annotations 141
Conflicts 38
Unique Conflicts 35
</table>
<bodyText confidence="0.999766722222222">
In Table 5, unique annotations refer to annota-
tions verified by only one user. Of the 195 total
verified annotation, 38 had conflicting answers. In
the majority of the cases the players marked these
conflicts as “None of the Above,” indicating that
the annotated phrase from Entity Discovery was
incorrect. For instance, many players made the
mistake in Entity Discovery of marking phrases
such as “German,” “English,” and “French” as Lo-
cation entities when they are, in fact, just adjec-
tives. In Name That Entity, the majority of players
corrected each other and marked these as “None
of the Above.”
The main use of this game will be to incorporate
it as an accuracy check for players based on these
conflicting annotation. This accuracy check will
be used in future work to deal with user confidence
scores and conflict resolution.
</bodyText>
<page confidence="0.997794">
233
</page>
<sectionHeader confidence="0.998966" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999954473684211">
Annotated corpora generation presents a major ob-
stacle to advancing modern Natural Language Pro-
cessing technologies. In this paper we introduced
the PackPlay framework, which aims to leverage
a distributed web user community in a collabora-
tive game to build semantically-rich annotated cor-
pora from players annotations. PackPlay is shown
to have high precision and recall numbers when
compared to expert systems in the area of named
entity recognition. These annotated corpora were
generated from two collaborative games in Pack-
Play, Entity Discovery and Name That Entity. The
two games combined let us exploit the benefits of
both synchronous and asynchronous gameplay as
mechanisms to verify the quality of our annotated
corpora. Future work should combine the play-
ers output with a player confidence score based
on conflict resolution algorithms, using both inter-
and intra-annotator metrics.
</bodyText>
<sectionHeader confidence="0.999048" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998712133333334">
Luis von Ahn and Laura Dabbish. 2004 Labeling im-
ages with a computer game. ACM, pages 319-326,
Vienna, Austria.
Leonard A. Annetta. 2008 Serious Educational
Games: From Theory to Practice. Sense Publishers.
Ron Artstein and Massimo Poesio. 2008 Inter-
coder agreement for computational linguistics.
Computational Linguistics, Vol. 34, Issue 4, pages
555-596.
Maged N. Kamel Boulos and Steve Wheeler. 2007.
The emerging web 2.0 social software: an enabling
suite of sociable technologies in health and health
care education. Health information and libraries
journal, Vol. 24, pages 223.
Paul Breimyer, Nathan Green, Vinay Kumar, and Na-
giza F. Samatova. 2009. BioDEAL: community
generation of biological annotations. BMC Medical
Informatics and Decision Making, Vol. 9, pages
Suppl+1.
Jon Chamberlain, Udo Kruschwitz, and Massimo Poe-
sio. 2009. Constructing an anaphorically anno-
tated corpus with non-experts: assessing the qual-
ity of collaborative annotations. People’s Web ’09:
Proceedings of the 2009 Workshop on The People’s
Web Meets NLP, pages 57-62.
FAS Summit on educational games: Harnessing the
power of video games for learning (report), 2006.
Sylviane Granger and Paul Rayson. 1998. Learner
English on Computer. Longman, London, and New
Yorks pp. 119-131.
Nathan Green, Paul Breimyer, Vinay Kumar, and Na-
giza F. Samatova. 2009. WebBANC: Build-
ing Semantically-Rich Annotated Corpora from
Web User Annotations of Minority Languages.
Proceedings of the 17th Nordic Conference of
Computational Linguistics (NODALIDA), Vol. 4,
pages 48-56, Odense, Denmark.
Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9:10.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk
CHI ’08: Proceeding of the twenty-sixth annual
SIGCHI conference on Human factors in computing
systems, pages 453-456, Florence, Italy.
Ravi Kumar, Jasmine Novak, and Andrew Tomkins.
2006 Structure and evolution of online social net-
works. KDD ’06: Proceedings of the 12th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 611-617, New
York, NY.
C. Mair. 2005. The corpus-based study of language
change in progress: The extra value of tagged cor-
pora. The AAACL/ICAME Conference, Ann Arbor,
MI.
Paul Rayson, James Walkerdine,William H. Fletcher,
and Adam Kilgarriff. 2006. Annotated web as cor-
pus The 2nd International Workshop on Web as
Corpus (EACL06), Trento, Italy.
Kevin P. Scannell. 2007. The Crbadn Project:
Corpus building for under-resourced languages.
Proceedings of the 3rd Web as Corpus Workshop
Louvain-la-Neuve, Belgium.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast – but is it
good?: evaluating non-expert annotations for nat-
ural language tasks EMNLP ’08: Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 254–263, Honolulu,
Hawaii.
Erik F. Tjong, Kim Sang and Fien De Meul-
der 2003 Introduction to the conll-2003 shared
task: language-independent named entity recogni-
tion. Association for Computational Linguistics,
pages 142-147, Edmonton, Canada.
</reference>
<page confidence="0.998378">
234
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.792004">
<title confidence="0.999914">PackPlay: Mining semantic data in collaborative games</title>
<author confidence="0.999916">Nathan Green Paul Breimyer Vinay Kumar Nagiza F Samatova</author>
<affiliation confidence="0.997116">NC State University NC State University NC State University Oak Ridge National Lab</affiliation>
<address confidence="0.99476">890 Oval Drive 890 Oval Drive 890 Oval Drive 1 Bethel Valley Rd Raleigh, NC 27695 Raleigh, NC 27695 Raleigh, NC 27695 Oak Ridge, TN 37831</address>
<abstract confidence="0.991026">Building training data is labor-intensive and presents a major obstacle to advancing machine learning technologies such as machine translators, named entity recognizers (NER), part-of-speech taggers, etc. Training data are often specialized for a particular language or Natural Language Processing (NLP) task. Knowledge captured by a specific set of training data is not easily transferable, even to the same NLP task in another language. Emerging technologies, such as social networks and serious games, offer a unique opportunity to change how we construct training data. While collaborative games have been used in information retrieval, it is an open issue whether users can contribute accurate annotations in a collaborative game context for a problem that requires an exact answer, such as games that would create named entity recognition training data. We a collaborative game framework that empirically shows players’ ability to mimic annotation accuracy and thoroughness seen in gold standard annotated corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<pages>319--326</pages>
<publisher>ACM,</publisher>
<location>Vienna, Austria.</location>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2004 Labeling images with a computer game. ACM, pages 319-326, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard A Annetta</author>
</authors>
<title>Serious Educational Games: From Theory to Practice.</title>
<date>2008</date>
<publisher>Sense Publishers.</publisher>
<contexts>
<context position="6068" citStr="Annetta, 2008" startWordPosition="949" endWordPosition="950">ch could be advanced. There has been a recent trend to leverage human’s fascination in game playing to solve difficult problems through Human Computation. Two such games include ESP and Google’s Image Labeler (Ahn and Dabbish, 2004), in which players annotate images in a cooperative environment to correctly match image tags with their partner. Semantic annotation has also been addressed in the game Phrase Detectives (Chamberlain et al., 2009), which has the goal of creating large scale training data for anaphora resolution. These types of games are part of a larger, serious games, initiative (Annetta, 2008). This paper introduces the Web-enabled collaborative game framework, PackPlay, and investigates 1Pogo. http://www.pogo.com/ 2Protrackr.com site information and statististics.http://www.protrackr.com/ 3Alexa: The Web Information Company. http://www.alexa.com/ how collaborative online gaming can affect annotation throughput and annotation accuracy. There are two main questions for such systems: first, will overall throughput increase compared to traditional methods of annotating, such as the manual construction of the Genia Corpus? Second, how accurate are the collective annotations? A successf</context>
</contexts>
<marker>Annetta, 2008</marker>
<rawString>Leonard A. Annetta. 2008 Serious Educational Games: From Theory to Practice. Sense Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Intercoder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<pages>555--596</pages>
<contexts>
<context position="23140" citStr="Artstein and Poesio, 2008" startWordPosition="3892" endWordPosition="3895">s partner is anonymous. This anonymity allows us to keep the asynchronous structure hidden, as well as judge a new metric, intra-annotator agreement, not tested in the previous game. Since it is possible that a player in PackPlay may have a question sampled that was previously annotated in the Entity Discovery game by the same player, we can use intraannotator agreement. While well-known interannotator statistics, such as Cohen’s Kappa, evaluate one annotator versus the other annotator, intraannotator statistics allow us to judge an annotator versus himself or herself to test for consistency (Artstein and Poesio, 2008). In the PackPlay framework this allows us to detect playerss who are randomly guessing and are therefore not consistent with themselves. 3.2.3 Scoring Since entity coverage of a sentence is not an issue in the multiple choice game, we made use of a different scoring system that would reward first instincts. While the Entity Discovery game has a set score for every answer, Name That Entity has a sliding scale. For each question, the max score is 100 points, as the time ticks away the user receives fewer points. The points remaining are indicated to the user via a timing bar at the bottom of th</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008 Intercoder agreement for computational linguistics. Computational Linguistics, Vol. 34, Issue 4, pages 555-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maged N Kamel Boulos</author>
<author>Steve Wheeler</author>
</authors>
<title>The emerging web 2.0 social software: an enabling suite of sociable technologies in health and health care education. Health information and libraries journal,</title>
<date>2007</date>
<volume>24</volume>
<pages>223</pages>
<marker>Boulos, Wheeler, 2007</marker>
<rawString>Maged N. Kamel Boulos and Steve Wheeler. 2007. The emerging web 2.0 social software: an enabling suite of sociable technologies in health and health care education. Health information and libraries journal, Vol. 24, pages 223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Breimyer</author>
<author>Nathan Green</author>
<author>Vinay Kumar</author>
<author>Nagiza F Samatova</author>
</authors>
<title>BioDEAL: community generation of biological annotations.</title>
<date>2009</date>
<journal>BMC Medical Informatics and Decision Making,</journal>
<volume>9</volume>
<pages>1</pages>
<contexts>
<context position="4189" citStr="Breimyer et al., 2009" startWordPosition="630" endWordPosition="633">atic approaches by connecting distributed human users at a previously unfathomable scale and presents an opportunity to expand annotation efforts to countless users using Human Computation, the concept of outsourcing certain computational 227 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 227–234, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics processes to humans, generally to solve problems that are intractable or difficult for computers. This concept is demonstrated in our previous work, WebBANC (Green et al., 2009) and BioDEAL (Breimyer et al., 2009), which allows users to annotate Web documents through a Web browser plugin for the purposes of creating linguistically and biologically tagged annotated corpora and with micro-tasking via Mechanical Turk, which allows for a low cost option for manual labor tasks (Snow et al., 2008; Kittur et al., 2008). While the Web and Human Computation may be a powerful tandem for generating data and solving difficult problems, in order to succeed, users must be motivated to participate. Humans have been fascinated with games for centuries and play them for many reasons, including for entertainment, honing</context>
<context position="15949" citStr="Breimyer et al., 2009" startWordPosition="2650" endWordPosition="2653">Methodology To examine Entity Discovery as a collaborative game toward the creation of an annotated corpus, we conducted a user experiment to collect sample data on a known data set. Over a short time, 8 players were asked to play both Entity Discovery and Name That Entity. In PackPlay, throughput can be estimated, since each game has a defined time limit, defined as the average number of entities annotated per question times the number of users times the average number of questions seen by a user. Unlike other systems such as Mechanical Turk (Snow et al., 2008; Kittur et al., 2008), BioDeal (Breimyer et al., 2009), or WebBANC (Green et al., 2009), in PackPlay we define the speed at which a user annotates. Each game in Entity Discovery consists of 10 sentences from the CoNLL corpus. These sentences are not guaranteed to have a named entity within them. The users in the study were not informed of the entity content as to not bias the experiment and falsely raise our precision scores. With only 8 players, we obtained 291 annotations, which averaged to about 40 annotations per user. This study was not done over a long period of time, so each user only played, on average, 3.6 games. Two players were asked t</context>
</contexts>
<marker>Breimyer, Green, Kumar, Samatova, 2009</marker>
<rawString>Paul Breimyer, Nathan Green, Vinay Kumar, and Nagiza F. Samatova. 2009. BioDEAL: community generation of biological annotations. BMC Medical Informatics and Decision Making, Vol. 9, pages Suppl+1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Chamberlain</author>
<author>Udo Kruschwitz</author>
<author>Massimo Poesio</author>
</authors>
<title>Constructing an anaphorically annotated corpus with non-experts: assessing the quality of collaborative annotations. People’s Web ’09:</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Workshop on The People’s Web Meets NLP,</booktitle>
<pages>57--62</pages>
<contexts>
<context position="5900" citStr="Chamberlain et al., 2009" startWordPosition="919" endWordPosition="922">user, the combined time is equal to more than 45,000 days of human time. Arguably if, the games on Pogo were used to harvest useful data, various fields of Computer Science research could be advanced. There has been a recent trend to leverage human’s fascination in game playing to solve difficult problems through Human Computation. Two such games include ESP and Google’s Image Labeler (Ahn and Dabbish, 2004), in which players annotate images in a cooperative environment to correctly match image tags with their partner. Semantic annotation has also been addressed in the game Phrase Detectives (Chamberlain et al., 2009), which has the goal of creating large scale training data for anaphora resolution. These types of games are part of a larger, serious games, initiative (Annetta, 2008). This paper introduces the Web-enabled collaborative game framework, PackPlay, and investigates 1Pogo. http://www.pogo.com/ 2Protrackr.com site information and statististics.http://www.protrackr.com/ 3Alexa: The Web Information Company. http://www.alexa.com/ how collaborative online gaming can affect annotation throughput and annotation accuracy. There are two main questions for such systems: first, will overall throughput incr</context>
</contexts>
<marker>Chamberlain, Kruschwitz, Poesio, 2009</marker>
<rawString>Jon Chamberlain, Udo Kruschwitz, and Massimo Poesio. 2009. Constructing an anaphorically annotated corpus with non-experts: assessing the quality of collaborative annotations. People’s Web ’09: Proceedings of the 2009 Workshop on The People’s Web Meets NLP, pages 57-62.</rawString>
</citation>
<citation valid="true">
<title>FAS Summit on educational games: Harnessing the power of video games for learning (report),</title>
<date>2006</date>
<marker>2006</marker>
<rawString>FAS Summit on educational games: Harnessing the power of video games for learning (report), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
<author>Paul Rayson</author>
</authors>
<date>1998</date>
<journal>Learner English on Computer. Longman, London, and New Yorks</journal>
<pages>119--131</pages>
<contexts>
<context position="2817" citStr="Granger and Rayson, 1998" startWordPosition="426" endWordPosition="429"> in another language. Domain and language specific corpora are useful for many language technology applications, including named entity recognition (NER), machine translation, spelling correction, and machine-readable dictionaries. The An Cr´ubad´an Project, for example, has succeeded in creating corpora for more than 400 of the world’s 6000+ languages by Web crawling. With a few exceptions, most of the 400+ corpora, however, lack any linguistic annotations due to the limitations of annotation tools (Rayson et al., 2006). Despite the many documented advantages of annotated data over raw data (Granger and Rayson, 1998; Mair, 2005), there is a dearth of annotated corpora in many domains. The majority of previous corpus annotation efforts relied on manual annotation by domain experts, automated prediction tagging systems, and hybrid semi-automatic systems that used both approaches. While yielding high quality and enormously valuable corpora, manually annotating corpora can be prohibitively costly and time consuming. For example, the GENIA corpus contains 9,372 sentences, curated by five part-time annotators, one senior coordinator, and one junior coordinator over 1.5 years (Kim et al., 2008). Semiautomatic a</context>
</contexts>
<marker>Granger, Rayson, 1998</marker>
<rawString>Sylviane Granger and Paul Rayson. 1998. Learner English on Computer. Longman, London, and New Yorks pp. 119-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Green</author>
<author>Paul Breimyer</author>
<author>Vinay Kumar</author>
<author>Nagiza F Samatova</author>
</authors>
<title>WebBANC: Building Semantically-Rich Annotated Corpora from Web User Annotations of Minority Languages.</title>
<date>2009</date>
<booktitle>Proceedings of the 17th Nordic Conference of Computational Linguistics (NODALIDA),</booktitle>
<volume>4</volume>
<pages>48--56</pages>
<location>Odense, Denmark.</location>
<contexts>
<context position="4153" citStr="Green et al., 2009" startWordPosition="624" endWordPosition="627">eb can help facilitate semi-automatic approaches by connecting distributed human users at a previously unfathomable scale and presents an opportunity to expand annotation efforts to countless users using Human Computation, the concept of outsourcing certain computational 227 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 227–234, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics processes to humans, generally to solve problems that are intractable or difficult for computers. This concept is demonstrated in our previous work, WebBANC (Green et al., 2009) and BioDEAL (Breimyer et al., 2009), which allows users to annotate Web documents through a Web browser plugin for the purposes of creating linguistically and biologically tagged annotated corpora and with micro-tasking via Mechanical Turk, which allows for a low cost option for manual labor tasks (Snow et al., 2008; Kittur et al., 2008). While the Web and Human Computation may be a powerful tandem for generating data and solving difficult problems, in order to succeed, users must be motivated to participate. Humans have been fascinated with games for centuries and play them for many reasons,</context>
<context position="15982" citStr="Green et al., 2009" startWordPosition="2657" endWordPosition="2660">ery as a collaborative game toward the creation of an annotated corpus, we conducted a user experiment to collect sample data on a known data set. Over a short time, 8 players were asked to play both Entity Discovery and Name That Entity. In PackPlay, throughput can be estimated, since each game has a defined time limit, defined as the average number of entities annotated per question times the number of users times the average number of questions seen by a user. Unlike other systems such as Mechanical Turk (Snow et al., 2008; Kittur et al., 2008), BioDeal (Breimyer et al., 2009), or WebBANC (Green et al., 2009), in PackPlay we define the speed at which a user annotates. Each game in Entity Discovery consists of 10 sentences from the CoNLL corpus. These sentences are not guaranteed to have a named entity within them. The users in the study were not informed of the entity content as to not bias the experiment and falsely raise our precision scores. With only 8 players, we obtained 291 annotations, which averaged to about 40 annotations per user. This study was not done over a long period of time, so each user only played, on average, 3.6 games. Two players were asked to intentionally annotate poorly. </context>
</contexts>
<marker>Green, Breimyer, Kumar, Samatova, 2009</marker>
<rawString>Nathan Green, Paul Breimyer, Vinay Kumar, and Nagiza F. Samatova. 2009. WebBANC: Building Semantically-Rich Annotated Corpora from Web User Annotations of Minority Languages. Proceedings of the 17th Nordic Conference of Computational Linguistics (NODALIDA), Vol. 4, pages 48-56, Odense, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus annotation for mining biomedical events from literature.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<pages>9--10</pages>
<contexts>
<context position="3400" citStr="Kim et al., 2008" startWordPosition="517" endWordPosition="520">raw data (Granger and Rayson, 1998; Mair, 2005), there is a dearth of annotated corpora in many domains. The majority of previous corpus annotation efforts relied on manual annotation by domain experts, automated prediction tagging systems, and hybrid semi-automatic systems that used both approaches. While yielding high quality and enormously valuable corpora, manually annotating corpora can be prohibitively costly and time consuming. For example, the GENIA corpus contains 9,372 sentences, curated by five part-time annotators, one senior coordinator, and one junior coordinator over 1.5 years (Kim et al., 2008). Semiautomatic approaches decrease human effort but often introduce significant error, while still requiring human interaction. The Web can help facilitate semi-automatic approaches by connecting distributed human users at a previously unfathomable scale and presents an opportunity to expand annotation efforts to countless users using Human Computation, the concept of outsourcing certain computational 227 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 227–234, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics processes to humans, gen</context>
</contexts>
<marker>Kim, Ohta, Tsujii, 2008</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii. 2008. Corpus annotation for mining biomedical events from literature. BMC Bioinformatics, 9:10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniket Kittur</author>
<author>Ed H Chi</author>
<author>Bongwon Suh</author>
</authors>
<date>2008</date>
<booktitle>Crowdsourcing user studies with Mechanical Turk CHI ’08: Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>453--456</pages>
<location>Florence, Italy.</location>
<contexts>
<context position="4493" citStr="Kittur et al., 2008" startWordPosition="681" endWordPosition="684">010, pages 227–234, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics processes to humans, generally to solve problems that are intractable or difficult for computers. This concept is demonstrated in our previous work, WebBANC (Green et al., 2009) and BioDEAL (Breimyer et al., 2009), which allows users to annotate Web documents through a Web browser plugin for the purposes of creating linguistically and biologically tagged annotated corpora and with micro-tasking via Mechanical Turk, which allows for a low cost option for manual labor tasks (Snow et al., 2008; Kittur et al., 2008). While the Web and Human Computation may be a powerful tandem for generating data and solving difficult problems, in order to succeed, users must be motivated to participate. Humans have been fascinated with games for centuries and play them for many reasons, including for entertainment, honing skills, and gaining knowledge (FAS Summit, 2006). Every year, a large amount of hours are spent playing online computer games. The games range form simple card and word games to more complex 3-D world games. One such site for word, puzzle, and card games is Pogo.com1. According to protrackr,2 Pogo has </context>
<context position="15916" citStr="Kittur et al., 2008" startWordPosition="2645" endWordPosition="2648">s agree. 3.1.4 User Case Study Methodology To examine Entity Discovery as a collaborative game toward the creation of an annotated corpus, we conducted a user experiment to collect sample data on a known data set. Over a short time, 8 players were asked to play both Entity Discovery and Name That Entity. In PackPlay, throughput can be estimated, since each game has a defined time limit, defined as the average number of entities annotated per question times the number of users times the average number of questions seen by a user. Unlike other systems such as Mechanical Turk (Snow et al., 2008; Kittur et al., 2008), BioDeal (Breimyer et al., 2009), or WebBANC (Green et al., 2009), in PackPlay we define the speed at which a user annotates. Each game in Entity Discovery consists of 10 sentences from the CoNLL corpus. These sentences are not guaranteed to have a named entity within them. The users in the study were not informed of the entity content as to not bias the experiment and falsely raise our precision scores. With only 8 players, we obtained 291 annotations, which averaged to about 40 annotations per user. This study was not done over a long period of time, so each user only played, on average, 3.</context>
</contexts>
<marker>Kittur, Chi, Suh, 2008</marker>
<rawString>Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008. Crowdsourcing user studies with Mechanical Turk CHI ’08: Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems, pages 453-456, Florence, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Kumar</author>
<author>Jasmine Novak</author>
<author>Andrew Tomkins</author>
</authors>
<title>Structure and evolution of online social networks.</title>
<date>2006</date>
<booktitle>KDD ’06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>611--617</pages>
<location>New York, NY.</location>
<marker>Kumar, Novak, Tomkins, 2006</marker>
<rawString>Ravi Kumar, Jasmine Novak, and Andrew Tomkins. 2006 Structure and evolution of online social networks. KDD ’06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 611-617, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mair</author>
</authors>
<title>The corpus-based study of language change in progress: The extra value of tagged corpora.</title>
<date>2005</date>
<booktitle>The AAACL/ICAME Conference,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2830" citStr="Mair, 2005" startWordPosition="430" endWordPosition="431">in and language specific corpora are useful for many language technology applications, including named entity recognition (NER), machine translation, spelling correction, and machine-readable dictionaries. The An Cr´ubad´an Project, for example, has succeeded in creating corpora for more than 400 of the world’s 6000+ languages by Web crawling. With a few exceptions, most of the 400+ corpora, however, lack any linguistic annotations due to the limitations of annotation tools (Rayson et al., 2006). Despite the many documented advantages of annotated data over raw data (Granger and Rayson, 1998; Mair, 2005), there is a dearth of annotated corpora in many domains. The majority of previous corpus annotation efforts relied on manual annotation by domain experts, automated prediction tagging systems, and hybrid semi-automatic systems that used both approaches. While yielding high quality and enormously valuable corpora, manually annotating corpora can be prohibitively costly and time consuming. For example, the GENIA corpus contains 9,372 sentences, curated by five part-time annotators, one senior coordinator, and one junior coordinator over 1.5 years (Kim et al., 2008). Semiautomatic approaches dec</context>
</contexts>
<marker>Mair, 2005</marker>
<rawString>C. Mair. 2005. The corpus-based study of language change in progress: The extra value of tagged corpora. The AAACL/ICAME Conference, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Rayson</author>
<author>James Walkerdine</author>
<author>William H Fletcher</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Annotated web as corpus</title>
<date>2006</date>
<booktitle>The 2nd International Workshop on Web as Corpus (EACL06),</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="2719" citStr="Rayson et al., 2006" startWordPosition="411" endWordPosition="414">specific annotated corpus is often not transferable to another task, even to the same NLP task in another language. Domain and language specific corpora are useful for many language technology applications, including named entity recognition (NER), machine translation, spelling correction, and machine-readable dictionaries. The An Cr´ubad´an Project, for example, has succeeded in creating corpora for more than 400 of the world’s 6000+ languages by Web crawling. With a few exceptions, most of the 400+ corpora, however, lack any linguistic annotations due to the limitations of annotation tools (Rayson et al., 2006). Despite the many documented advantages of annotated data over raw data (Granger and Rayson, 1998; Mair, 2005), there is a dearth of annotated corpora in many domains. The majority of previous corpus annotation efforts relied on manual annotation by domain experts, automated prediction tagging systems, and hybrid semi-automatic systems that used both approaches. While yielding high quality and enormously valuable corpora, manually annotating corpora can be prohibitively costly and time consuming. For example, the GENIA corpus contains 9,372 sentences, curated by five part-time annotators, one</context>
</contexts>
<marker>Rayson, Walkerdine, Fletcher, Kilgarriff, 2006</marker>
<rawString>Paul Rayson, James Walkerdine,William H. Fletcher, and Adam Kilgarriff. 2006. Annotated web as corpus The 2nd International Workshop on Web as Corpus (EACL06), Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Scannell</author>
</authors>
<title>The Crbadn Project: Corpus building for under-resourced languages.</title>
<date>2007</date>
<booktitle>Proceedings of the 3rd Web as Corpus Workshop Louvain-la-Neuve,</booktitle>
<location>Belgium.</location>
<contexts>
<context position="1987" citStr="Scannell, 2007" startWordPosition="299" endWordPosition="300">corpora. 1 Introduction Annotated corpora are sets of structured text used in Natural Language Processing (NLP) that contain supplemental knowledge, such as tagged parts-of-speech, semantic concepts assigned to phrases, or semantic relationships between these concepts. Machine Learning (ML) is a subfield of Artificial Intelligence that studies how computers can obtain knowledge and create predictive models. These models require annotated corpora to learn rules and patterns. However, these annotated corpora must be manually curated for each domain or task, which is labor intensive and tedious (Scannell, 2007), thereby creating a bottleneck for advancing ML and NLP prediction tools. Furthermore, knowledge captured by a specific annotated corpus is often not transferable to another task, even to the same NLP task in another language. Domain and language specific corpora are useful for many language technology applications, including named entity recognition (NER), machine translation, spelling correction, and machine-readable dictionaries. The An Cr´ubad´an Project, for example, has succeeded in creating corpora for more than 400 of the world’s 6000+ languages by Web crawling. With a few exceptions,</context>
</contexts>
<marker>Scannell, 2007</marker>
<rawString>Kevin P. Scannell. 2007. The Crbadn Project: Corpus building for under-resourced languages. Proceedings of the 3rd Web as Corpus Workshop Louvain-la-Neuve, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast – but is it good?: evaluating non-expert annotations for natural language tasks</title>
<date>2008</date>
<booktitle>EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>254--263</pages>
<location>Honolulu, Hawaii.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast – but is it good?: evaluating non-expert annotations for natural language tasks EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 254–263, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong</author>
</authors>
<title>Kim Sang and Fien De Meulder</title>
<date>2003</date>
<pages>142--147</pages>
<location>Edmonton, Canada.</location>
<marker>Tjong, 2003</marker>
<rawString>Erik F. Tjong, Kim Sang and Fien De Meulder 2003 Introduction to the conll-2003 shared task: language-independent named entity recognition. Association for Computational Linguistics, pages 142-147, Edmonton, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>