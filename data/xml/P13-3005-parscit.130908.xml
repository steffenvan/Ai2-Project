<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002989">
<title confidence="0.996817">
Survey on parsing three dependency representations for English
</title>
<author confidence="0.992583">
Angelina Ivanova Stephan Oepen Lilja Øvrelid
</author>
<affiliation confidence="0.998296">
University of Oslo, Department of Informatics
</affiliation>
<email confidence="0.992366">
{angelii|oe|liljao}@ifi.uio.no
</email>
<sectionHeader confidence="0.997334" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998976615384615">
In this paper we focus on practical is-
sues of data representation for dependency
parsing. We carry out an experimental
comparison of (a) three syntactic depen-
dency schemes; (b) three data-driven de-
pendency parsers; and (c) the influence of
two different approaches to lexical cate-
gory disambiguation (aka tagging) prior to
parsing. Comparing parsing accuracies in
various setups, we study the interactions
of these three aspects and analyze which
configurations are easier to learn for a de-
pendency parser.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999851018181818">
Dependency parsing is one of the mainstream re-
search areas in natural language processing. De-
pendency representations are useful for a number
of NLP applications, for example, machine trans-
lation (Ding and Palmer, 2005), information ex-
traction (Yakushiji et al., 2006), analysis of ty-
pologically diverse languages (Bunt et al., 2010)
and parser stacking (Øvrelid et al., 2009). There
were several shared tasks organized on depen-
dency parsing (CoNLL 2006–2007) and labeled
dependencies (CoNLL 2008–2009) and there were
a number of attempts to compare various depen-
dencies intrinsically, e.g. (Miyao et al., 2007), and
extrinsically, e.g. (Wu et al., 2012).
In this paper we focus on practical issues of data
representation for dependency parsing. The cen-
tral aspects of our discussion are (a) three depen-
dency formats: two ‘classic’ representations for
dependency parsing, namely, Stanford Basic (SB)
and CoNLL Syntactic Dependencies (CD), and
bilexical dependencies from the HPSG English
Resource Grammar (ERG), so-called DELPH-IN
Syntactic Derivation Tree (DT), proposed recently
by Ivanova et al. (2012); (b) three state-of-the art
statistical parsers: Malt (Nivre et al., 2007), MST
(McDonald et al., 2005) and the parser of Bohnet
and Nivre (2012); (c) two approaches to word-
category disambiguation, e.g. exploiting common
PTB tags and using supertags (i.e. specialized
ERG lexical types).
We parse the formats and compare accuracies
in all configurations in order to determine how
parsers, dependency representations and grammat-
ical tagging methods interact with each other in
application to automatic syntactic analysis.
SB and CD are derived automatically from
phrase structures of Penn Treebank to accommo-
date the needs of fast and accurate dependency
parsing, whereas DT is rooted in the formal gram-
mar theory HPSG and is independent from any
specific treebank. For DT we gain more expres-
sivity from the underlying linguistic theory, which
challenges parsing with statistical tools. The struc-
tural analysis of the schemes in Ivanova et al.
(2012) leads to the hypothesis that CD and DT
are more similar to each other than SB to DT.
We recompute similarities on a larger treebank and
check whether parsing results reflect them.
The paper has the following structure: an
overview of related work is presented in Sec-
tion 2; treebanks, tagsets, dependency schemes
and parsers used in the experiments are introduced
in Section 3; analysis of parsing results is dis-
cussed in Section 4; conclusions and future work
are outlined in Section 5.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9997733">
Schwartz et al. (2012) investigate which depen-
dency representations of several syntactic struc-
tures are easier to parse with supervised ver-
sions of the Klein and Manning (2004) parser,
ClearParser (Choi and Nicolov, 2009), MST
Parser, Malt and the Easy First Non-directional
parser (Goldberg and Elhadad, 2010). The results
imply that all parsers consistently perform better
when (a) coordination has one of the conjuncts as
the head rather than the coordinating conjunction;
</bodyText>
<page confidence="0.998853">
31
</page>
<note confidence="0.569885">
Proceedings of the ACL Student Research Workshop, pages 31–37,
</note>
<page confidence="0.368803">
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</page>
<figure confidence="0.361621">
A , B and C A , B and C A, B and C
</figure>
<figureCaption confidence="0.99812">
Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats
</figureCaption>
<bodyText confidence="0.960407727272727">
(b) the noun phrase is headed by the noun rather
than by determiner; (c) prepositions or subordinat-
ing conjunctions, rather than their NP or clause ar-
guments, serve as the head in prepositional phrase
or subordinated clauses. Therefore we can expect
(a) Malt and MST to have fewer errors on coor-
dination structures parsing SB and CD than pars-
ing DT, because SB and CD choose the first con-
junct as the head and DT chooses the coordinating
conjunction as the head; (b,c) no significant dif-
ferences for the errors on noun and prepositional
phrases, because all three schemes have the noun
as the head of the noun phrase and the preposition
as the head of the prepositional phrase.
Miwa et al. (2010) present intristic and extris-
tic (event-extraction task) evaluation of six parsers
(GDep, Bikel, Stanford, Charniak-Johnson, C&amp;C
and Enju parser) on three dependency formats
(Stanford Dependencies, CoNLL-X, and Enju
PAS). Intristic evaluation results show that all
parsers have the highest accuracies with the
CoNLL-X format.
</bodyText>
<sectionHeader confidence="0.982248" genericHeader="method">
3 Data and software
</sectionHeader>
<subsectionHeader confidence="0.987012">
3.1 Treebanks
</subsectionHeader>
<bodyText confidence="0.999869066666667">
For the experiments in this paper we used the Penn
Treebank (Marcus et al., 1993) and the Deep-
Bank (Flickinger et al., 2012). The latter is com-
prised of roughly 82% of the sentences of the first
16 sections of the Penn Treebank annotated with
full HPSG analyses from the English Resource
Grammar (ERG). The DeepBank annotations are
created on top of the raw text of the PTB. Due to
imperfections of the automatic tokenization, there
are some token mismatches between DeepBank
and PTB. We had to filter out such sentences to
have consistent number of tokens in the DT, SB
and CD formats. For our experiments we had
available a training set of 22209 sentences and a
test set of 1759 sentences (from Section 15).
</bodyText>
<subsectionHeader confidence="0.998657">
3.2 Parsers
</subsectionHeader>
<bodyText confidence="0.985846833333333">
In the experiments described in Section 4 we used
parsers that adopt different approaches and imple-
ment various algorithms.
Malt (Nivre et al., 2007): transition-based de-
pendency parser with local learning and greedy
search.
MST (McDonald et al., 2005): graph-based
dependency parser with global near-exhaustive
search.
Bohnet and Nivre (2012) parser: transition-
based dependency parser with joint tagger that im-
plements global learning and beam search.
</bodyText>
<subsectionHeader confidence="0.997655">
3.3 Dependency schemes
</subsectionHeader>
<bodyText confidence="0.999685380952381">
In this work we extract DeepBank data in the form
of bilexical syntactic dependencies, DELPH-IN
Syntactic Derivation Tree (DT) format. We ob-
tain the exact same sentences in Stanford Basic
(SB) format from the automatic conversion of the
PTB with the Stanford parser (de Marneffe et al.,
2006) and in the CoNLL Syntactic Dependencies
(CD) representation using the LTH Constituent-
to-Dependency Conversion Tool for Penn-style
Treebanks (Johansson and Nugues, 2007).
SB and CD represent the way to convert PTB
to bilexical dependencies; in contrast, DT is
grounded in linguistic theory and captures deci-
sions taken in the grammar. Figure 1 demonstrates
the differences between the formats on the coor-
dination structure. According to Schwartz et al.
(2012), analysis of coordination in SB and CD is
easier for a statistical parser to learn; however, as
we will see in section 4.3, DT has more expressive
power distinguishing structural ambiguities illus-
trated by the classic example old men and women.
</bodyText>
<subsectionHeader confidence="0.979238">
3.4 Part-of-speech tags
</subsectionHeader>
<bodyText confidence="0.9999825">
We experimented with two tag sets: PTB tags and
lexical types of the ERG grammar - supertags.
PTB tags determine the part of speech (PoS)
and some morphological features, such as num-
ber for nouns, degree of comparison for adjectives
and adverbs, tense and agreement with person and
number of subject for verbs, etc.
Supertags are composed of part-of-speech, va-
lency in the form of an ordered sequence of
complements, and annotations that encompass
category-internal subdivisions, e.g. mass vs. count
vs. proper nouns, intersective vs. scopal adverbs,
</bodyText>
<page confidence="0.996039">
32
</page>
<bodyText confidence="0.999716583333333">
or referential vs. expletive pronouns. Example of
a supertag: v np is le (verb “is” that takes noun
phrase as a complement).
There are 48 tags in the PTB tagset and 1091
supertags in the set of lexical types of the ERG.
The state-of-the-art accuracy of PoS-tagging on
in-domain test data using gold-standard tokeniza-
tion is roughly 97% for the PTB tagset and ap-
proximately 95% for the ERG supertags (Ytrestøl,
2011). Supertagging for the ERG grammar is an
ongoing research effort and an off-the-shelf su-
pertagger for the ERG is not currently available.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999402333333333">
In this section we give a detailed analysis of pars-
ing into SB, CD and DT dependencies with Malt,
MST and the Bohnet and Nivre (2012) parser.
</bodyText>
<subsectionHeader confidence="0.991085">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999893888888889">
For Malt and MST we perform the experiments
on gold PoS tags, whereas the Bohnet and Nivre
(2012) parser predicts PoS tags during testing.
Prior to each experiment with Malt, we used
MaltOptimizer to obtain settings and a feature
model; for MST we exploited default configura-
tion; for the Bohnet and Nivre (2012) parser we
set the beam parameter to 80 and otherwise em-
ployed the default setup.
With regards to evaluation metrics we use la-
belled attachment score (LAS), unlabeled attach-
ment score (UAS) and label accuracy (LACC) ex-
cluding punctuation. Our results cannot be di-
rectly compared to the state-of-the-art scores on
the Penn Treebank because we train on sections
0-13 and test on section 15 of WSJ. Also our re-
sults are not strictly inter-comparable because the
setups we are using are different.
</bodyText>
<subsectionHeader confidence="0.96858">
4.2 Discussion
</subsectionHeader>
<bodyText confidence="0.9996336">
The results that we are going to analyze are pre-
sented in Tables 1 and 2. Statistical significance
was assessed using Dan Bikel’s parsing evaluation
comparator1 at the 0.001 significance level. We
inspect three different aspects in the interpretation
of these results: parser, dependency format and
tagset. Below we will look at these three angles
in detail.
From the parser perspective Malt and MST are
not very different in the traditional setup with gold
</bodyText>
<footnote confidence="0.7797175">
1http://nextens.uvt.nl/depparse-wiki/
SoftwarePage#scoring
</footnote>
<bodyText confidence="0.998299274509804">
PTB tags (Table 1, Gold PTB tags). The Bohnet
and Nivre (2012) parser outperforms Malt on CD
and DT and MST on SB, CD and DT with PTB
tags even though it does not receive gold PTB tags
during test phase but predicts them (Table 2, Pre-
dicted PTB tags). This is explained by the fact that
the Bohnet and Nivre (2012) parser implements a
novel approach to parsing: beam-search algorithm
with global structure learning.
MST “loses” more than Malt when parsing SB
with gold supertags (Table 1, Gold supertags).
This parser exploits context features “POS tag of
each intervening word between head and depen-
dent” (McDonald et al., 2006). Due to the far
larger size of the supertag set compared to the PTB
tagset, such features are sparse and have low fre-
quencies. This leads to the lower scores of pars-
ing accuracy for MST. For the Bohnet and Nivre
(2012) parser the complexity of supertag predic-
tion has significant negative influence on the at-
tachment and labeling accuracies (Table 2, Pre-
dicted supertags). The addition of gold PTB tags
as a feature lifts the performance of the Bohnet
and Nivre (2012) parser to the level of perfor-
mance of Malt and MST on CD with gold su-
pertags and Malt on SB with gold supertags (com-
pare Table 2, Predicted supertags + gold PTB, and
Table 1, Gold supertags).
Both Malt and MST benefit slightly from the
combination of gold PTB tags and gold supertags
(Table 1, Gold PTB tags + gold supertags). For
the Bohnet and Nivre (2012) parser we also ob-
serve small rise of accuracy when gold supertags
are provided as a feature for prediction of PTB
tags (compare Predicted PTB tags and Predicted
PTB tags + gold supertags sections of Table 2).
The parsers have different running times: it
takes minutes to run an experiment with Malt,
about 2 hours with MST and up to a day with the
Bohnet and Nivre (2012) parser.
From the point of view of the dependency for-
mat, SB has the highest LACC and CD is first-rate
on UAS for all three parsers in most of the con-
figurations (Tables 1 and 2). This means that SB
is easier to label and CD is easier to parse struc-
turally. DT appears to be a more difficult target
format because it is both hard to label and attach
in most configurations. It is not an unexpected re-
sult, since SB and CD are both derived from PTB
phrase-structure trees and are oriented to ease de-
pendency parsing task. DT is not custom-designed
</bodyText>
<page confidence="0.995425">
33
</page>
<table confidence="0.984717736842105">
Gold PTB tags Predicted PTB tags
LAS MST UAS MST LACC MST LAS UAS LACC
Malt Malt Malt Bohnet and Nivre (2012)
SB 89.21 88.59 90.95 90.88 93.58 92.79 SB 89.56 92.36 93.30
CD 88.74 88.72 91.89 92.01 91.29 91.34 CD 89.77 93.01 92.10
DT 85.97 86.36 89.22 90.01 88.73 89.22 DT 88.26 91.63 90.72
Gold supertags Predicted supertags
LAS MST UAS MST LACC MST LAS UAS LACC
Malt Malt Malt Bohnet and Nivre (2012)
SB 87.76 85.25 90.63 88.56 92.38 90.29 SB 85.41 89.38 90.17
CD 88.22 87.27 91.17 90.41 91.30 90.74 CD 86.73 90.73 89.72
DT 89.92 89.58 90.96 90.56 92.50 92.64 DT 85.76 89.50 88.56
Gold PTB tags + gold supertags Pred. PTB tags + gold supertags
LAS MST UAS MST LACC MST LAS UAS LACC
Malt Malt Malt Bohnet and Nivre (2012)
SB 90.321 89.431 91.901 91.842 94.481 93.261 SB 90.32 93.01 93.85
CD 89.591 89.372 92.431 92.772 92.321 92.072 CD 90.55 93.56 92.79
DT 90.691 91.192 91.831 92.332 93.101 93.692 DT 91.51 92.99 93.88
Pred. supertags + gold PTB
</table>
<tableCaption confidence="0.92114">
Table 1: Parsing results of Malt and MST on
</tableCaption>
<figureCaption confidence="0.939811888888889">
Stanford Basic (SB), CoNLL Syntactic De-
pendencies (CD) and DELPH-IN Syntactic
Derivation Tree (DT) formats. Punctuation is
excluded from the scoring. Gold PTB tags:
Malt and MST are trained and tested on gold
PTB tags. Gold supertags: Malt and MST
are trained and tested on gold supertags. Gold
PTB tags + gold supertags: Malt and MST are
trained on gold PTB tags and gold supertags.
</figureCaption>
<bodyText confidence="0.890310166666667">
1 denotes a feature model in which gold PTB
tags function as PoS and gold supertags act
as additional features (in CPOSTAG field); 2
stands for the feature model which exploits
gold supertags as PoS and uses gold PTB tags
as extra features (in CPOSTAG field).
</bodyText>
<table confidence="0.998649">
LAS UAS LACC
Bohnet and Nivre (2012)
SB 87.20 90.07 91.81
CD 87.79 91.47 90.62
DT 86.31 89.80 89.17
</table>
<tableCaption confidence="0.980649">
Table 2: Parsing results of the Bohnet
</tableCaption>
<bodyText confidence="0.994399933333334">
and Nivre (2012) parser on Stanford Ba-
sic (SB), CoNLL Syntactic Dependencies
(CD) and DELPH-IN Syntactic Deriva-
tion Tree (DT) formats. Parser is trained
on gold-standard data. Punctuation is ex-
cluded from the scoring. Predicted PTB:
parser predicts PTB tags during the test
phase. Predicted supertags: parser pre-
dicts supertags during the test phase. Pre-
dicted PTB + gold supertags: parser re-
ceives gold supertags as feature and pre-
dicts PTB tags during the test phase. Pre-
dicted supertags + gold PTB: parser re-
ceives PTB tags as feature and predicts
supertags during test phase.
</bodyText>
<page confidence="0.99793">
34
</page>
<bodyText confidence="0.999939117647059">
to dependency parsing and is independent from
parsing questions in this sense. Unlike SB and
CD, it is linguistically informed by the underlying,
full-fledged HPSG grammar.
The Jaccard similarity on our training set is 0.57
for SB and CD, 0.564 for CD and DT, and 0.388
for SB and DT. These similarity values show that
CD and DT are structurally closer to each other
than SB and DT. Contrary to our expectations, the
accuracy scores of parsers do not suggest that CD
and DT are particularly similar to each other in
terms of parsing.
Inspecting the aspect of tagset we conclude that
traditional PTB tags are compatible with SB and
CD but do not fit the DT scheme well, while ERG
supertags are specific to the ERG framework and
do not seem to be appropriate for SB and CD. Nei-
ther of these findings seem surprising, as PTB tags
were developed as part of the treebank from which
CD and SB are derived; whereas ERG supertags
are closely related to the HPSG syntactic struc-
tures captured in DT. PTB tags were designed to
simplify PoS-tagging whereas supertags were de-
veloped to capture information that is required to
analyze syntax of HPSG.
For each PTB tag we collected corresponding
supertags from the gold-standard training set. For
open word classes such as nouns, adjectives, ad-
verbs and verbs the relation between PTB tags
and supertags is many-to-many. Unique one-to-
many correspondence holds only for possessive
wh-pronoun and punctuation.
Thus, supertags do not provide extra level of
detalization for PTB tags, but PTB tags and su-
pertags are complementary. As discussed in sec-
tion 3.4, they contain bits of information that are
different. For this reason their combination re-
sults in slight increase of accuracy for all three
parsers on all dependency formats (Table 1, Gold
PTB tags + gold supertags, and Table 2, Predicted
PTB + gold supertags and Predicted supertags +
gold PTB). The Bohnet and Nivre (2012) parser
predicts supertags with an average accuracy of
89.73% which is significantly lower than state-of-
the-art 95% (Ytrestøl, 2011).
When we consider punctuation in the evalua-
tion, all scores raise significantly for DT and at
the same time decrease for SB and CD for all three
parsers. This is explained by the fact that punctu-
ation in DT is always attached to the nearest token
which is easy to learn for a statistical parser.
</bodyText>
<subsectionHeader confidence="0.998068">
4.3 Error analysis
</subsectionHeader>
<bodyText confidence="0.996636291666667">
Using the CoNLL-07 evaluation script2 on our test
set, for each parser we obtained the error rate dis-
tribution over CPOSTAG on SB, CD and DT.
VBP, VBZ and VBG. VBP (verb, non-3rd
person singular present), VBZ (verb, 3rd per-
son singular present) and VBG (verb, gerund or
present participle) are the PTB tags that have error
rates in 10 highest error rates list for each parser
(Malt, MST and the Bohnet and Nivre (2012)
parser) with each dependency format (SB, CD
and DT) and with each PoS tag set (PTB PoS
and supertags) when PTB tags are included as
CPOSTAG feature. We automatically collected all
sentences that contain 1) attachment errors, 2) la-
bel errors, 3) attachment and label errors for VBP,
VBZ and VBG made by Malt parser on DT format
with PTB PoS. For each of these three lexical cat-
egories we manually analyzed a random sample
of sentences with errors and their corresponding
gold-standard versions.
In many cases such errors are related to the root
of the sentence when the verb is either treated as
complement or adjunct instead of having a root
status or vice versa. Errors with these groups of
verbs mostly occur in the complex sentences that
contain several verbs. Sentences with coordina-
tion are particularly difficult for the correct attach-
ment and labeling of the VBP (see Figure 2 for an
example).
Coordination. The error rate of Malt, MST and
the Bohnet and Nivre (2012) parser for the coor-
dination is not so high for SB and CD ( 1% and
2% correspondingly with MaltParser, PTB tags)
whereas for DT the error rate on the CPOSTAGS
is especially high (26% with MaltParser, PTB
tags). It means that there are many errors on
incoming dependency arcs for coordinating con-
junctions when parsing DT. On outgoing arcs
parsers also make more mistakes on DT than on
SB and CD. This is related to the difference in
choice of annotation principle (see Figure 1). As
it was shown in (Schwartz et al., 2012), it is harder
to parse coordination headed by coordinating con-
junction.
Although the approach used in DT is harder for
parser to learn, it has some advantages: using SB
and CD annotations, we cannot distinguish the two
cases illustrated with the sentences (a) and (b):
</bodyText>
<footnote confidence="0.981256">
2http://nextens.uvt.nl/depparse-wiki/
SoftwarePage#scoring
</footnote>
<page confidence="0.996469">
35
</page>
<note confidence="0.374741">
VBP VBD VBD
The figures show that spending rose 0.1 % in the third quarter &lt;... &gt; and was up 3.8 % from a year ago .
</note>
<figureCaption confidence="0.9956875">
Figure 2: The gold-standard (in green above the sentence) and the incorrect Malt’s (in red below the
sentence) analyses of the utterance from the DeepBank in DT format with PTB PoS tags
</figureCaption>
<figure confidence="0.9944197">
root
HD-CMP
MRK-NH
SB-HD
VP-VP
Cl-CL
SP-HD
HD-CMP
root
MRK-NH
</figure>
<bodyText confidence="0.961419764705883">
a) The fight is putting a tight squeeze on prof-
its of many, threatening to drive the small-
est ones out of business and straining rela-
tions between the national fast-food chains
and their franchisees.
b) Proceeds from the sale will be used for re-
modelling and reforbishing projects, as well
as for the planned MGM Grand hotel/casino
and theme park.
In the sentence a) “the national fast-food” refers
only to the conjunct “chains”, while in the sen-
tence b) “the planned” refers to both conjuncts and
“MGM Grand” refers only to the first conjunct.
The Bohnet and Nivre (2012) parser succeeds in
finding the correct conjucts (shown in bold font)
on DT and makes mistakes on SB and CD in some
difficult cases like the following ones:
</bodyText>
<listItem confidence="0.945450714285714">
a) &lt;... &gt; investors hoard gold and help under-
pin its price &lt;... &gt;
b) Then take the expected return and subtract
one standard deviation.
CD and SB wrongly suggest “gold” and “help” to
be conjoined in the first sentence and “return” and
“deviation” in the second.
</listItem>
<sectionHeader confidence="0.926634" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999973475">
In this survey we gave a comparative experi-
mental overview of (i) parsing three dependency
schemes, viz., Stanford Basic (SB), CoNLL Syn-
tactic Dependencies (CD) and DELPH-IN Syn-
tactic Derivation Tree (DT), (ii) with three lead-
ing dependency parsers, viz., Malt, MST and the
Bohnet and Nivre (2012) parser (iii) exploiting
two different tagsets, viz., PTB tags and supertags.
From the parser perspective, the Bohnet and
Nivre (2012) parser performs better than Malt and
MST not only on conventional formats but also on
the new representation, although this parser solves
a harder task than Malt and MST.
From the dependency format perspective, DT
appeares to be a more difficult target dependency
representation than SB and CD. This suggests that
the expressivity that we gain from the grammar
theory (e.g. for coordination) is harder to learn
with state-of-the-art dependency parsers. CD and
DT are structurally closer to each other than SB
and DT; however, we did not observe sound evi-
dence of a correlation between structural similar-
ity of CD and DT and their parsing accuracies
Regarding the tagset aspect, it is natural that
PTB tags are good for SB and CD, whereas the
more fine-grained set of supertags fits DT bet-
ter. PTB tags and supertags are complementary,
and for all three parsers we observe slight benefits
from being supplied with both types of tags.
As future work we would like to run more ex-
periments with predicted supertags. In the absence
of a specialized supertagger, we can follow the
pipeline of (Ytrestøl, 2011) who reached the state-
of-the-art supertagging accuracy of 95%.
Another area of our interest is an extrinsic eval-
uation of SB, CD and DT, e.g. applied to semantic
role labeling and question-answering in order to
find out if the usage of the DT format grounded
in the computational grammar theory is beneficial
for such tasks.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999957428571428">
The authors would like to thank Rebecca Dridan,
Joakim Nivre, Bernd Bohnet, Gertjan van Noord
and Jelke Bloem for interesting discussions and
the two anonymous reviewers for comments on
the work. Experimentation was made possible
through access to the high-performance comput-
ing resources at the University of Oslo.
</bodyText>
<page confidence="0.997526">
36
</page>
<sectionHeader confidence="0.995966" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999442666666667">
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In
EMNLP-CoNLL, pages 1455–1465. ACL.
Harry Bunt, Paola Merlo, and Joakim Nivre, editors.
2010. Trends in Parsing Technology. Springer Ver-
lag, Stanford.
Jinho D Choi and Nicolas Nicolov. 2009. K-best, lo-
cally pruned, transition-based dependency parsing
using robust risk minimization. Recent Advances in
Natural Language Processing V, pages 205–216.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure trees. In
LREC.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL’05), pages 541–548, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
Daniel Flickinger, Yi Zhang, and Valia Kordoni. 2012.
DeepBank: a Dynamically Annotated Treebank of
the Wall Street Journal. In Proceedings of the
Eleventh International Workshop on Treebanks and
Linguistic Theories, pages 85–96. Edies Colibri.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ’10, pages 742–750, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Angelina Ivanova, Stephan Oepen, Lilja Øvrelid, and
Dan Flickinger. 2012. Who did what to whom?
a contrastive study of syntacto-semantic dependen-
cies. In Proceedings of the Sixth Linguistic Annota-
tion Workshop, pages 2–11, Jeju, Republic of Korea,
July. Association for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA 2007, pages
105–112, Tartu, Estonia, May 25-26.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: mod-
els of dependency and constituency. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, ACL ’04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313–330, June.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, HLT ’05, pages 523–530, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ’06, pages 216–220,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun ichi Tsujii. 2010. Evaluating dependency repre-
sentations for event extraction. In Chu-Ren Huang
and Dan Jurafsky, editors, COLING, pages 779–787.
Tsinghua University Press.
Yusuke Miyao, Kenji Sagae, and Jun’ichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Ann Copestake, editor, Pro-
ceedings of the GEAF 2007 Workshop, CSLI Studies
in Computational Linguistics Online, page 21 pages.
CSLI Publications.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Lilja Øvrelid, Jonas Kuhn, and Kathrin Spreyer. 2009.
Cross-framework parser stacking for data-driven de-
pendency parsing. TAL, 50(3):109–138.
Roy Schwartz, Omri Abend, and Ari Rappoport. 2012.
Learnability-based syntactic annotation design. In
Proc. of the 24th International Conference on Com-
putational Linguistics (Coling 2012), Mumbai, In-
dia, December. Coling 2012 Organizing Committee.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2012. A Compara-
tive Study of Target Dependency Structures for Sta-
tistical Machine Translation. In ACL (2), pages 100–
104. The Association for Computer Linguistics.
Akane Yakushiji, Yusuke Miyao, Tomoko Ohta, Yuka
Tateisi, and Jun’ichi Tsujii. 2006. Automatic con-
struction of predicate-argument structure patterns
for biomedical information extraction. In Proceed-
ings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’06,
pages 284–292, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Gisle Ytrestøl. 2011. Cuteforce: deep deterministic
HPSG parsing. In Proceedings of the 12th Interna-
tional Conference on Parsing Technologies, IWPT
’11, pages 186–197, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.999611">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.837973">
<title confidence="0.993711">Survey on parsing three dependency representations for English</title>
<author confidence="0.995685">Angelina Ivanova Stephan Oepen Lilja Øvrelid</author>
<affiliation confidence="0.952265">University of Oslo, Department of</affiliation>
<abstract confidence="0.991769714285714">In this paper we focus on practical issues of data representation for dependency parsing. We carry out an experimental comparison of (a) three syntactic dependency schemes; (b) three data-driven dependency parsers; and (c) the influence of two different approaches to lexical category disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>1455--1465</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1967" citStr="Bohnet and Nivre (2012)" startWordPosition="288" endWordPosition="291">ly, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012); (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007), MST (McDonald et al., 2005) and the parser of Bohnet and Nivre (2012); (c) two approaches to wordcategory disambiguation, e.g. exploiting common PTB tags and using supertags (i.e. specialized ERG lexical types). We parse the formats and compare accuracies in all configurations in order to determine how parsers, dependency representations and grammatical tagging methods interact with each other in application to automatic syntactic analysis. SB and CD are derived automatically from phrase structures of Penn Treebank to accommodate the needs of fast and accurate dependency parsing, whereas DT is rooted in the formal grammar theory HPSG and is independent from any</context>
<context position="6155" citStr="Bohnet and Nivre (2012)" startWordPosition="971" endWordPosition="974">token mismatches between DeepBank and PTB. We had to filter out such sentences to have consistent number of tokens in the DT, SB and CD formats. For our experiments we had available a training set of 22209 sentences and a test set of 1759 sentences (from Section 15). 3.2 Parsers In the experiments described in Section 4 we used parsers that adopt different approaches and implement various algorithms. Malt (Nivre et al., 2007): transition-based dependency parser with local learning and greedy search. MST (McDonald et al., 2005): graph-based dependency parser with global near-exhaustive search. Bohnet and Nivre (2012) parser: transitionbased dependency parser with joint tagger that implements global learning and beam search. 3.3 Dependency schemes In this work we extract DeepBank data in the form of bilexical syntactic dependencies, DELPH-IN Syntactic Derivation Tree (DT) format. We obtain the exact same sentences in Stanford Basic (SB) format from the automatic conversion of the PTB with the Stanford parser (de Marneffe et al., 2006) and in the CoNLL Syntactic Dependencies (CD) representation using the LTH Constituentto-Dependency Conversion Tool for Penn-style Treebanks (Johansson and Nugues, 2007). SB a</context>
<context position="8563" citStr="Bohnet and Nivre (2012)" startWordPosition="1361" endWordPosition="1364">b “is” that takes noun phrase as a complement). There are 48 tags in the PTB tagset and 1091 supertags in the set of lexical types of the ERG. The state-of-the-art accuracy of PoS-tagging on in-domain test data using gold-standard tokenization is roughly 97% for the PTB tagset and approximately 95% for the ERG supertags (Ytrestøl, 2011). Supertagging for the ERG grammar is an ongoing research effort and an off-the-shelf supertagger for the ERG is not currently available. 4 Experiments In this section we give a detailed analysis of parsing into SB, CD and DT dependencies with Malt, MST and the Bohnet and Nivre (2012) parser. 4.1 Setup For Malt and MST we perform the experiments on gold PoS tags, whereas the Bohnet and Nivre (2012) parser predicts PoS tags during testing. Prior to each experiment with Malt, we used MaltOptimizer to obtain settings and a feature model; for MST we exploited default configuration; for the Bohnet and Nivre (2012) parser we set the beam parameter to 80 and otherwise employed the default setup. With regards to evaluation metrics we use labelled attachment score (LAS), unlabeled attachment score (UAS) and label accuracy (LACC) excluding punctuation. Our results cannot be directly</context>
<context position="9982" citStr="Bohnet and Nivre (2012)" startWordPosition="1590" endWordPosition="1593"> we are using are different. 4.2 Discussion The results that we are going to analyze are presented in Tables 1 and 2. Statistical significance was assessed using Dan Bikel’s parsing evaluation comparator1 at the 0.001 significance level. We inspect three different aspects in the interpretation of these results: parser, dependency format and tagset. Below we will look at these three angles in detail. From the parser perspective Malt and MST are not very different in the traditional setup with gold 1http://nextens.uvt.nl/depparse-wiki/ SoftwarePage#scoring PTB tags (Table 1, Gold PTB tags). The Bohnet and Nivre (2012) parser outperforms Malt on CD and DT and MST on SB, CD and DT with PTB tags even though it does not receive gold PTB tags during test phase but predicts them (Table 2, Predicted PTB tags). This is explained by the fact that the Bohnet and Nivre (2012) parser implements a novel approach to parsing: beam-search algorithm with global structure learning. MST “loses” more than Malt when parsing SB with gold supertags (Table 1, Gold supertags). This parser exploits context features “POS tag of each intervening word between head and dependent” (McDonald et al., 2006). Due to the far larger size of t</context>
<context position="11377" citStr="Bohnet and Nivre (2012)" startWordPosition="1839" endWordPosition="1842">d Nivre (2012) parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies (Table 2, Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the Bohnet and Nivre (2012) parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2, Predicted supertags + gold PTB, and Table 1, Gold supertags). Both Malt and MST benefit slightly from the combination of gold PTB tags and gold supertags (Table 1, Gold PTB tags + gold supertags). For the Bohnet and Nivre (2012) parser we also observe small rise of accuracy when gold supertags are provided as a feature for prediction of PTB tags (compare Predicted PTB tags and Predicted PTB tags + gold supertags sections of Table 2). The parsers have different running times: it takes minutes to run an experiment with Malt, about 2 hours with MST and up to a day with the Bohnet and Nivre (2012) parser. From the point of view of the dependency format, SB has the highest LACC and CD is first-rate on UAS for all three parsers in most of the configurations (Tables 1 and 2). This means that SB is easier to label and CD is </context>
<context position="12700" citStr="Bohnet and Nivre (2012)" startWordPosition="2087" endWordPosition="2090">d to label and attach in most configurations. It is not an unexpected result, since SB and CD are both derived from PTB phrase-structure trees and are oriented to ease dependency parsing task. DT is not custom-designed 33 Gold PTB tags Predicted PTB tags LAS MST UAS MST LACC MST LAS UAS LACC Malt Malt Malt Bohnet and Nivre (2012) SB 89.21 88.59 90.95 90.88 93.58 92.79 SB 89.56 92.36 93.30 CD 88.74 88.72 91.89 92.01 91.29 91.34 CD 89.77 93.01 92.10 DT 85.97 86.36 89.22 90.01 88.73 89.22 DT 88.26 91.63 90.72 Gold supertags Predicted supertags LAS MST UAS MST LACC MST LAS UAS LACC Malt Malt Malt Bohnet and Nivre (2012) SB 87.76 85.25 90.63 88.56 92.38 90.29 SB 85.41 89.38 90.17 CD 88.22 87.27 91.17 90.41 91.30 90.74 CD 86.73 90.73 89.72 DT 89.92 89.58 90.96 90.56 92.50 92.64 DT 85.76 89.50 88.56 Gold PTB tags + gold supertags Pred. PTB tags + gold supertags LAS MST UAS MST LACC MST LAS UAS LACC Malt Malt Malt Bohnet and Nivre (2012) SB 90.321 89.431 91.901 91.842 94.481 93.261 SB 90.32 93.01 93.85 CD 89.591 89.372 92.431 92.772 92.321 92.072 CD 90.55 93.56 92.79 DT 90.691 91.192 91.831 92.332 93.101 93.692 DT 91.51 92.99 93.88 Pred. supertags + gold PTB Table 1: Parsing results of Malt and MST on Stanford B</context>
<context position="13970" citStr="Bohnet and Nivre (2012)" startWordPosition="2313" endWordPosition="2316">d DELPH-IN Syntactic Derivation Tree (DT) formats. Punctuation is excluded from the scoring. Gold PTB tags: Malt and MST are trained and tested on gold PTB tags. Gold supertags: Malt and MST are trained and tested on gold supertags. Gold PTB tags + gold supertags: Malt and MST are trained on gold PTB tags and gold supertags. 1 denotes a feature model in which gold PTB tags function as PoS and gold supertags act as additional features (in CPOSTAG field); 2 stands for the feature model which exploits gold supertags as PoS and uses gold PTB tags as extra features (in CPOSTAG field). LAS UAS LACC Bohnet and Nivre (2012) SB 87.20 90.07 91.81 CD 87.79 91.47 90.62 DT 86.31 89.80 89.17 Table 2: Parsing results of the Bohnet and Nivre (2012) parser on Stanford Basic (SB), CoNLL Syntactic Dependencies (CD) and DELPH-IN Syntactic Derivation Tree (DT) formats. Parser is trained on gold-standard data. Punctuation is excluded from the scoring. Predicted PTB: parser predicts PTB tags during the test phase. Predicted supertags: parser predicts supertags during the test phase. Predicted PTB + gold supertags: parser receives gold supertags as feature and predicts PTB tags during the test phase. Predicted supertags + gold </context>
<context position="16572" citStr="Bohnet and Nivre (2012)" startWordPosition="2755" endWordPosition="2758">bs the relation between PTB tags and supertags is many-to-many. Unique one-tomany correspondence holds only for possessive wh-pronoun and punctuation. Thus, supertags do not provide extra level of detalization for PTB tags, but PTB tags and supertags are complementary. As discussed in section 3.4, they contain bits of information that are different. For this reason their combination results in slight increase of accuracy for all three parsers on all dependency formats (Table 1, Gold PTB tags + gold supertags, and Table 2, Predicted PTB + gold supertags and Predicted supertags + gold PTB). The Bohnet and Nivre (2012) parser predicts supertags with an average accuracy of 89.73% which is significantly lower than state-ofthe-art 95% (Ytrestøl, 2011). When we consider punctuation in the evaluation, all scores raise significantly for DT and at the same time decrease for SB and CD for all three parsers. This is explained by the fact that punctuation in DT is always attached to the nearest token which is easy to learn for a statistical parser. 4.3 Error analysis Using the CoNLL-07 evaluation script2 on our test set, for each parser we obtained the error rate distribution over CPOSTAG on SB, CD and DT. VBP, VBZ a</context>
<context position="18413" citStr="Bohnet and Nivre (2012)" startWordPosition="3075" endWordPosition="3078">f these three lexical categories we manually analyzed a random sample of sentences with errors and their corresponding gold-standard versions. In many cases such errors are related to the root of the sentence when the verb is either treated as complement or adjunct instead of having a root status or vice versa. Errors with these groups of verbs mostly occur in the complex sentences that contain several verbs. Sentences with coordination are particularly difficult for the correct attachment and labeling of the VBP (see Figure 2 for an example). Coordination. The error rate of Malt, MST and the Bohnet and Nivre (2012) parser for the coordination is not so high for SB and CD ( 1% and 2% correspondingly with MaltParser, PTB tags) whereas for DT the error rate on the CPOSTAGS is especially high (26% with MaltParser, PTB tags). It means that there are many errors on incoming dependency arcs for coordinating conjunctions when parsing DT. On outgoing arcs parsers also make more mistakes on DT than on SB and CD. This is related to the difference in choice of annotation principle (see Figure 1). As it was shown in (Schwartz et al., 2012), it is harder to parse coordination headed by coordinating conjunction. Altho</context>
<context position="20204" citStr="Bohnet and Nivre (2012)" startWordPosition="3387" endWordPosition="3390"> MRK-NH SB-HD VP-VP Cl-CL SP-HD HD-CMP root MRK-NH a) The fight is putting a tight squeeze on profits of many, threatening to drive the smallest ones out of business and straining relations between the national fast-food chains and their franchisees. b) Proceeds from the sale will be used for remodelling and reforbishing projects, as well as for the planned MGM Grand hotel/casino and theme park. In the sentence a) “the national fast-food” refers only to the conjunct “chains”, while in the sentence b) “the planned” refers to both conjuncts and “MGM Grand” refers only to the first conjunct. The Bohnet and Nivre (2012) parser succeeds in finding the correct conjucts (shown in bold font) on DT and makes mistakes on SB and CD in some difficult cases like the following ones: a) &lt;... &gt; investors hoard gold and help underpin its price &lt;... &gt; b) Then take the expected return and subtract one standard deviation. CD and SB wrongly suggest “gold” and “help” to be conjoined in the first sentence and “return” and “deviation” in the second. 5 Conclusions and future work In this survey we gave a comparative experimental overview of (i) parsing three dependency schemes, viz., Stanford Basic (SB), CoNLL Syntactic Dependen</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In EMNLP-CoNLL, pages 1455–1465. ACL.</rawString>
</citation>
<citation valid="true">
<date>2010</date>
<booktitle>Trends in Parsing Technology.</booktitle>
<editor>Harry Bunt, Paola Merlo, and Joakim Nivre, editors.</editor>
<publisher>Springer Verlag, Stanford.</publisher>
<contexts>
<context position="4730" citStr="(2010)" startWordPosition="746" endWordPosition="746">ubordinating conjunctions, rather than their NP or clause arguments, serve as the head in prepositional phrase or subordinated clauses. Therefore we can expect (a) Malt and MST to have fewer errors on coordination structures parsing SB and CD than parsing DT, because SB and CD choose the first conjunct as the head and DT chooses the coordinating conjunction as the head; (b,c) no significant differences for the errors on noun and prepositional phrases, because all three schemes have the noun as the head of the noun phrase and the preposition as the head of the prepositional phrase. Miwa et al. (2010) present intristic and extristic (event-extraction task) evaluation of six parsers (GDep, Bikel, Stanford, Charniak-Johnson, C&amp;C and Enju parser) on three dependency formats (Stanford Dependencies, CoNLL-X, and Enju PAS). Intristic evaluation results show that all parsers have the highest accuracies with the CoNLL-X format. 3 Data and software 3.1 Treebanks For the experiments in this paper we used the Penn Treebank (Marcus et al., 1993) and the DeepBank (Flickinger et al., 2012). The latter is comprised of roughly 82% of the sentences of the first 16 sections of the Penn Treebank annotated wi</context>
</contexts>
<marker>2010</marker>
<rawString>Harry Bunt, Paola Merlo, and Joakim Nivre, editors. 2010. Trends in Parsing Technology. Springer Verlag, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Nicolas Nicolov</author>
</authors>
<title>K-best, locally pruned, transition-based dependency parsing using robust risk minimization. Recent Advances in Natural Language Processing V,</title>
<date>2009</date>
<pages>205--216</pages>
<contexts>
<context position="3496" citStr="Choi and Nicolov, 2009" startWordPosition="532" endWordPosition="535">ilarities on a larger treebank and check whether parsing results reflect them. The paper has the following structure: an overview of related work is presented in Section 2; treebanks, tagsets, dependency schemes and parsers used in the experiments are introduced in Section 3; analysis of parsing results is discussed in Section 4; conclusions and future work are outlined in Section 5. 2 Related work Schwartz et al. (2012) investigate which dependency representations of several syntactic structures are easier to parse with supervised versions of the Klein and Manning (2004) parser, ClearParser (Choi and Nicolov, 2009), MST Parser, Malt and the Easy First Non-directional parser (Goldberg and Elhadad, 2010). The results imply that all parsers consistently perform better when (a) coordination has one of the conjuncts as the head rather than the coordinating conjunction; 31 Proceedings of the ACL Student Research Workshop, pages 31–37, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics A , B and C A , B and C A, B and C Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats (b) the noun phrase is headed by the noun rather than by deter</context>
</contexts>
<marker>Choi, Nicolov, 2009</marker>
<rawString>Jinho D Choi and Nicolas Nicolov. 2009. K-best, locally pruned, transition-based dependency parsing using robust risk minimization. Recent Advances in Natural Language Processing V, pages 205–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure trees.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure trees. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>541--548</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="935" citStr="Ding and Palmer, 2005" startWordPosition="133" endWordPosition="136">comparison of (a) three syntactic dependency schemes; (b) three data-driven dependency parsers; and (c) the influence of two different approaches to lexical category disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser. 1 Introduction Dependency parsing is one of the mainstream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005), information extraction (Yakushiji et al., 2006), analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (Øvrelid et al., 2009). There were several shared tasks organized on dependency parsing (CoNLL 2006–2007) and labeled dependencies (CoNLL 2008–2009) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: t</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 541–548, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Flickinger</author>
<author>Yi Zhang</author>
<author>Valia Kordoni</author>
</authors>
<title>DeepBank: a Dynamically Annotated Treebank of the Wall Street Journal.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eleventh International Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>85--96</pages>
<contexts>
<context position="5214" citStr="Flickinger et al., 2012" startWordPosition="816" endWordPosition="819">l three schemes have the noun as the head of the noun phrase and the preposition as the head of the prepositional phrase. Miwa et al. (2010) present intristic and extristic (event-extraction task) evaluation of six parsers (GDep, Bikel, Stanford, Charniak-Johnson, C&amp;C and Enju parser) on three dependency formats (Stanford Dependencies, CoNLL-X, and Enju PAS). Intristic evaluation results show that all parsers have the highest accuracies with the CoNLL-X format. 3 Data and software 3.1 Treebanks For the experiments in this paper we used the Penn Treebank (Marcus et al., 1993) and the DeepBank (Flickinger et al., 2012). The latter is comprised of roughly 82% of the sentences of the first 16 sections of the Penn Treebank annotated with full HPSG analyses from the English Resource Grammar (ERG). The DeepBank annotations are created on top of the raw text of the PTB. Due to imperfections of the automatic tokenization, there are some token mismatches between DeepBank and PTB. We had to filter out such sentences to have consistent number of tokens in the DT, SB and CD formats. For our experiments we had available a training set of 22209 sentences and a test set of 1759 sentences (from Section 15). 3.2 Parsers In</context>
</contexts>
<marker>Flickinger, Zhang, Kordoni, 2012</marker>
<rawString>Daniel Flickinger, Yi Zhang, and Valia Kordoni. 2012. DeepBank: a Dynamically Annotated Treebank of the Wall Street Journal. In Proceedings of the Eleventh International Workshop on Treebanks and Linguistic Theories, pages 85–96. Edies Colibri.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>742--750</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3585" citStr="Goldberg and Elhadad, 2010" startWordPosition="545" endWordPosition="548">per has the following structure: an overview of related work is presented in Section 2; treebanks, tagsets, dependency schemes and parsers used in the experiments are introduced in Section 3; analysis of parsing results is discussed in Section 4; conclusions and future work are outlined in Section 5. 2 Related work Schwartz et al. (2012) investigate which dependency representations of several syntactic structures are easier to parse with supervised versions of the Klein and Manning (2004) parser, ClearParser (Choi and Nicolov, 2009), MST Parser, Malt and the Easy First Non-directional parser (Goldberg and Elhadad, 2010). The results imply that all parsers consistently perform better when (a) coordination has one of the conjuncts as the head rather than the coordinating conjunction; 31 Proceedings of the ACL Student Research Workshop, pages 31–37, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics A , B and C A , B and C A, B and C Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats (b) the noun phrase is headed by the noun rather than by determiner; (c) prepositions or subordinating conjunctions, rather than their NP or clause arg</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 742–750, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelina Ivanova</author>
<author>Stephan Oepen</author>
<author>Lilja Øvrelid</author>
<author>Dan Flickinger</author>
</authors>
<title>Who did what to whom? a contrastive study of syntacto-semantic dependencies.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth Linguistic Annotation Workshop,</booktitle>
<pages>2--11</pages>
<institution>Korea, July. Association for Computational Linguistics.</institution>
<location>Jeju, Republic of</location>
<contexts>
<context position="1821" citStr="Ivanova et al. (2012)" startWordPosition="264" endWordPosition="267"> (CoNLL 2008–2009) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012); (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007), MST (McDonald et al., 2005) and the parser of Bohnet and Nivre (2012); (c) two approaches to wordcategory disambiguation, e.g. exploiting common PTB tags and using supertags (i.e. specialized ERG lexical types). We parse the formats and compare accuracies in all configurations in order to determine how parsers, dependency representations and grammatical tagging methods interact with each other in application to automatic syntactic analysis. SB and CD are derived automatically from phrase structures of Penn Treebank to</context>
</contexts>
<marker>Ivanova, Oepen, Øvrelid, Flickinger, 2012</marker>
<rawString>Angelina Ivanova, Stephan Oepen, Lilja Øvrelid, and Dan Flickinger. 2012. Who did what to whom? a contrastive study of syntacto-semantic dependencies. In Proceedings of the Sixth Linguistic Annotation Workshop, pages 2–11, Jeju, Republic of Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA</booktitle>
<pages>105--112</pages>
<location>Tartu, Estonia,</location>
<contexts>
<context position="6749" citStr="Johansson and Nugues, 2007" startWordPosition="1060" endWordPosition="1063">e search. Bohnet and Nivre (2012) parser: transitionbased dependency parser with joint tagger that implements global learning and beam search. 3.3 Dependency schemes In this work we extract DeepBank data in the form of bilexical syntactic dependencies, DELPH-IN Syntactic Derivation Tree (DT) format. We obtain the exact same sentences in Stanford Basic (SB) format from the automatic conversion of the PTB with the Stanford parser (de Marneffe et al., 2006) and in the CoNLL Syntactic Dependencies (CD) representation using the LTH Constituentto-Dependency Conversion Tool for Penn-style Treebanks (Johansson and Nugues, 2007). SB and CD represent the way to convert PTB to bilexical dependencies; in contrast, DT is grounded in linguistic theory and captures decisions taken in the grammar. Figure 1 demonstrates the differences between the formats on the coordination structure. According to Schwartz et al. (2012), analysis of coordination in SB and CD is easier for a statistical parser to learn; however, as we will see in section 4.3, DT has more expressive power distinguishing structural ambiguities illustrated by the classic example old men and women. 3.4 Part-of-speech tags We experimented with two tag sets: PTB t</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proceedings of NODALIDA 2007, pages 105–112, Tartu, Estonia, May 25-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3451" citStr="Klein and Manning (2004)" startWordPosition="526" endWordPosition="529"> to each other than SB to DT. We recompute similarities on a larger treebank and check whether parsing results reflect them. The paper has the following structure: an overview of related work is presented in Section 2; treebanks, tagsets, dependency schemes and parsers used in the experiments are introduced in Section 3; analysis of parsing results is discussed in Section 4; conclusions and future work are outlined in Section 5. 2 Related work Schwartz et al. (2012) investigate which dependency representations of several syntactic structures are easier to parse with supervised versions of the Klein and Manning (2004) parser, ClearParser (Choi and Nicolov, 2009), MST Parser, Malt and the Easy First Non-directional parser (Goldberg and Elhadad, 2010). The results imply that all parsers consistently perform better when (a) coordination has one of the conjuncts as the head rather than the coordinating conjunction; 31 Proceedings of the ACL Student Research Workshop, pages 31–37, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics A , B and C A , B and C A, B and C Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats (b) the noun phra</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpus-based induction of syntactic structure: models of dependency and constituency. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5171" citStr="Marcus et al., 1993" startWordPosition="808" endWordPosition="811">n and prepositional phrases, because all three schemes have the noun as the head of the noun phrase and the preposition as the head of the prepositional phrase. Miwa et al. (2010) present intristic and extristic (event-extraction task) evaluation of six parsers (GDep, Bikel, Stanford, Charniak-Johnson, C&amp;C and Enju parser) on three dependency formats (Stanford Dependencies, CoNLL-X, and Enju PAS). Intristic evaluation results show that all parsers have the highest accuracies with the CoNLL-X format. 3 Data and software 3.1 Treebanks For the experiments in this paper we used the Penn Treebank (Marcus et al., 1993) and the DeepBank (Flickinger et al., 2012). The latter is comprised of roughly 82% of the sentences of the first 16 sections of the Penn Treebank annotated with full HPSG analyses from the English Resource Grammar (ERG). The DeepBank annotations are created on top of the raw text of the PTB. Due to imperfections of the automatic tokenization, there are some token mismatches between DeepBank and PTB. We had to filter out such sentences to have consistent number of tokens in the DT, SB and CD formats. For our experiments we had available a training set of 22209 sentences and a test set of 1759 </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 523–530, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a twostage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>216--220</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10549" citStr="McDonald et al., 2006" startWordPosition="1688" endWordPosition="1691">s (Table 1, Gold PTB tags). The Bohnet and Nivre (2012) parser outperforms Malt on CD and DT and MST on SB, CD and DT with PTB tags even though it does not receive gold PTB tags during test phase but predicts them (Table 2, Predicted PTB tags). This is explained by the fact that the Bohnet and Nivre (2012) parser implements a novel approach to parsing: beam-search algorithm with global structure learning. MST “loses” more than Malt when parsing SB with gold supertags (Table 1, Gold supertags). This parser exploits context features “POS tag of each intervening word between head and dependent” (McDonald et al., 2006). Due to the far larger size of the supertag set compared to the PTB tagset, such features are sparse and have low frequencies. This leads to the lower scores of parsing accuracy for MST. For the Bohnet and Nivre (2012) parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies (Table 2, Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the Bohnet and Nivre (2012) parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2, </context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a twostage discriminative parser. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 216–220, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
</authors>
<title>Sampo Pyysalo, Tadayoshi Hara, and Jun ichi Tsujii.</title>
<date>2010</date>
<booktitle>In Chu-Ren Huang</booktitle>
<pages>779--787</pages>
<editor>and Dan Jurafsky, editors, COLING,</editor>
<publisher>Tsinghua University Press.</publisher>
<marker>Miwa, 2010</marker>
<rawString>Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and Jun ichi Tsujii. 2010. Evaluating dependency representations for event extraction. In Chu-Ren Huang and Dan Jurafsky, editors, COLING, pages 779–787. Tsinghua University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Towards framework-independent evaluation of deep linguistic parsers.</title>
<date>2007</date>
<booktitle>Proceedings of the GEAF 2007 Workshop, CSLI Studies in Computational Linguistics Online,</booktitle>
<pages>21</pages>
<editor>In Ann Copestake, editor,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="1327" citStr="Miyao et al., 2007" startWordPosition="192" endWordPosition="195">duction Dependency parsing is one of the mainstream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005), information extraction (Yakushiji et al., 2006), analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (Øvrelid et al., 2009). There were several shared tasks organized on dependency parsing (CoNLL 2006–2007) and labeled dependencies (CoNLL 2008–2009) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012); (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007), MST (McDonald et al., 2005) a</context>
</contexts>
<marker>Miyao, Sagae, Tsujii, 2007</marker>
<rawString>Yusuke Miyao, Kenji Sagae, and Jun’ichi Tsujii. 2007. Towards framework-independent evaluation of deep linguistic parsers. In Ann Copestake, editor, Proceedings of the GEAF 2007 Workshop, CSLI Studies in Computational Linguistics Online, page 21 pages. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="1896" citStr="Nivre et al., 2007" startWordPosition="275" endWordPosition="278">endencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012); (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007), MST (McDonald et al., 2005) and the parser of Bohnet and Nivre (2012); (c) two approaches to wordcategory disambiguation, e.g. exploiting common PTB tags and using supertags (i.e. specialized ERG lexical types). We parse the formats and compare accuracies in all configurations in order to determine how parsers, dependency representations and grammatical tagging methods interact with each other in application to automatic syntactic analysis. SB and CD are derived automatically from phrase structures of Penn Treebank to accommodate the needs of fast and accurate dependency parsing, whereas DT </context>
<context position="5961" citStr="Nivre et al., 2007" startWordPosition="945" endWordPosition="948"> analyses from the English Resource Grammar (ERG). The DeepBank annotations are created on top of the raw text of the PTB. Due to imperfections of the automatic tokenization, there are some token mismatches between DeepBank and PTB. We had to filter out such sentences to have consistent number of tokens in the DT, SB and CD formats. For our experiments we had available a training set of 22209 sentences and a test set of 1759 sentences (from Section 15). 3.2 Parsers In the experiments described in Section 4 we used parsers that adopt different approaches and implement various algorithms. Malt (Nivre et al., 2007): transition-based dependency parser with local learning and greedy search. MST (McDonald et al., 2005): graph-based dependency parser with global near-exhaustive search. Bohnet and Nivre (2012) parser: transitionbased dependency parser with joint tagger that implements global learning and beam search. 3.3 Dependency schemes In this work we extract DeepBank data in the form of bilexical syntactic dependencies, DELPH-IN Syntactic Derivation Tree (DT) format. We obtain the exact same sentences in Stanford Basic (SB) format from the automatic conversion of the PTB with the Stanford parser (de Mar</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lilja Øvrelid</author>
<author>Jonas Kuhn</author>
<author>Kathrin Spreyer</author>
</authors>
<title>Cross-framework parser stacking for data-driven dependency parsing.</title>
<date>2009</date>
<journal>TAL,</journal>
<volume>50</volume>
<issue>3</issue>
<contexts>
<context position="1092" citStr="Øvrelid et al., 2009" startWordPosition="157" endWordPosition="160">ategory disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser. 1 Introduction Dependency parsing is one of the mainstream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005), information extraction (Yakushiji et al., 2006), analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (Øvrelid et al., 2009). There were several shared tasks organized on dependency parsing (CoNLL 2006–2007) and labeled dependencies (CoNLL 2008–2009) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the H</context>
</contexts>
<marker>Øvrelid, Kuhn, Spreyer, 2009</marker>
<rawString>Lilja Øvrelid, Jonas Kuhn, and Kathrin Spreyer. 2009. Cross-framework parser stacking for data-driven dependency parsing. TAL, 50(3):109–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Omri Abend</author>
<author>Ari Rappoport</author>
</authors>
<title>Learnability-based syntactic annotation design.</title>
<date>2012</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proc. of the 24th International Conference on Computational Linguistics (Coling 2012),</booktitle>
<location>Mumbai, India,</location>
<contexts>
<context position="3297" citStr="Schwartz et al. (2012)" startWordPosition="502" endWordPosition="505"> parsing with statistical tools. The structural analysis of the schemes in Ivanova et al. (2012) leads to the hypothesis that CD and DT are more similar to each other than SB to DT. We recompute similarities on a larger treebank and check whether parsing results reflect them. The paper has the following structure: an overview of related work is presented in Section 2; treebanks, tagsets, dependency schemes and parsers used in the experiments are introduced in Section 3; analysis of parsing results is discussed in Section 4; conclusions and future work are outlined in Section 5. 2 Related work Schwartz et al. (2012) investigate which dependency representations of several syntactic structures are easier to parse with supervised versions of the Klein and Manning (2004) parser, ClearParser (Choi and Nicolov, 2009), MST Parser, Malt and the Easy First Non-directional parser (Goldberg and Elhadad, 2010). The results imply that all parsers consistently perform better when (a) coordination has one of the conjuncts as the head rather than the coordinating conjunction; 31 Proceedings of the ACL Student Research Workshop, pages 31–37, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguisti</context>
<context position="7039" citStr="Schwartz et al. (2012)" startWordPosition="1107" endWordPosition="1110">rmat. We obtain the exact same sentences in Stanford Basic (SB) format from the automatic conversion of the PTB with the Stanford parser (de Marneffe et al., 2006) and in the CoNLL Syntactic Dependencies (CD) representation using the LTH Constituentto-Dependency Conversion Tool for Penn-style Treebanks (Johansson and Nugues, 2007). SB and CD represent the way to convert PTB to bilexical dependencies; in contrast, DT is grounded in linguistic theory and captures decisions taken in the grammar. Figure 1 demonstrates the differences between the formats on the coordination structure. According to Schwartz et al. (2012), analysis of coordination in SB and CD is easier for a statistical parser to learn; however, as we will see in section 4.3, DT has more expressive power distinguishing structural ambiguities illustrated by the classic example old men and women. 3.4 Part-of-speech tags We experimented with two tag sets: PTB tags and lexical types of the ERG grammar - supertags. PTB tags determine the part of speech (PoS) and some morphological features, such as number for nouns, degree of comparison for adjectives and adverbs, tense and agreement with person and number of subject for verbs, etc. Supertags are </context>
<context position="18935" citStr="Schwartz et al., 2012" startWordPosition="3170" endWordPosition="3173"> Figure 2 for an example). Coordination. The error rate of Malt, MST and the Bohnet and Nivre (2012) parser for the coordination is not so high for SB and CD ( 1% and 2% correspondingly with MaltParser, PTB tags) whereas for DT the error rate on the CPOSTAGS is especially high (26% with MaltParser, PTB tags). It means that there are many errors on incoming dependency arcs for coordinating conjunctions when parsing DT. On outgoing arcs parsers also make more mistakes on DT than on SB and CD. This is related to the difference in choice of annotation principle (see Figure 1). As it was shown in (Schwartz et al., 2012), it is harder to parse coordination headed by coordinating conjunction. Although the approach used in DT is harder for parser to learn, it has some advantages: using SB and CD annotations, we cannot distinguish the two cases illustrated with the sentences (a) and (b): 2http://nextens.uvt.nl/depparse-wiki/ SoftwarePage#scoring 35 VBP VBD VBD The figures show that spending rose 0.1 % in the third quarter &lt;... &gt; and was up 3.8 % from a year ago . Figure 2: The gold-standard (in green above the sentence) and the incorrect Malt’s (in red below the sentence) analyses of the utterance from the DeepB</context>
</contexts>
<marker>Schwartz, Abend, Rappoport, 2012</marker>
<rawString>Roy Schwartz, Omri Abend, and Ari Rappoport. 2012. Learnability-based syntactic annotation design. In Proc. of the 24th International Conference on Computational Linguistics (Coling 2012), Mumbai, India, December. Coling 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Katsuhito Sudoh</author>
<author>Kevin Duh</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<title>A Comparative Study of Target Dependency Structures for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In ACL (2),</booktitle>
<pages>100--104</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="1370" citStr="Wu et al., 2012" startWordPosition="199" endWordPosition="202">tream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005), information extraction (Yakushiji et al., 2006), analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (Øvrelid et al., 2009). There were several shared tasks organized on dependency parsing (CoNLL 2006–2007) and labeled dependencies (CoNLL 2008–2009) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012); (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007), MST (McDonald et al., 2005) and the parser of Bohnet and Nivre (2012); (</context>
</contexts>
<marker>Wu, Sudoh, Duh, Tsukada, Nagata, 2012</marker>
<rawString>Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2012. A Comparative Study of Target Dependency Structures for Statistical Machine Translation. In ACL (2), pages 100– 104. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akane Yakushiji</author>
<author>Yusuke Miyao</author>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Automatic construction of predicate-argument structure patterns for biomedical information extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>284--292</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="984" citStr="Yakushiji et al., 2006" startWordPosition="140" endWordPosition="143">emes; (b) three data-driven dependency parsers; and (c) the influence of two different approaches to lexical category disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser. 1 Introduction Dependency parsing is one of the mainstream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005), information extraction (Yakushiji et al., 2006), analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (Øvrelid et al., 2009). There were several shared tasks organized on dependency parsing (CoNLL 2006–2007) and labeled dependencies (CoNLL 2008–2009) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsi</context>
</contexts>
<marker>Yakushiji, Miyao, Ohta, Tateisi, Tsujii, 2006</marker>
<rawString>Akane Yakushiji, Yusuke Miyao, Tomoko Ohta, Yuka Tateisi, and Jun’ichi Tsujii. 2006. Automatic construction of predicate-argument structure patterns for biomedical information extraction. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 284–292, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gisle Ytrestøl</author>
</authors>
<title>Cuteforce: deep deterministic HPSG parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies, IWPT ’11,</booktitle>
<pages>186--197</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8278" citStr="Ytrestøl, 2011" startWordPosition="1313" endWordPosition="1314">ch, valency in the form of an ordered sequence of complements, and annotations that encompass category-internal subdivisions, e.g. mass vs. count vs. proper nouns, intersective vs. scopal adverbs, 32 or referential vs. expletive pronouns. Example of a supertag: v np is le (verb “is” that takes noun phrase as a complement). There are 48 tags in the PTB tagset and 1091 supertags in the set of lexical types of the ERG. The state-of-the-art accuracy of PoS-tagging on in-domain test data using gold-standard tokenization is roughly 97% for the PTB tagset and approximately 95% for the ERG supertags (Ytrestøl, 2011). Supertagging for the ERG grammar is an ongoing research effort and an off-the-shelf supertagger for the ERG is not currently available. 4 Experiments In this section we give a detailed analysis of parsing into SB, CD and DT dependencies with Malt, MST and the Bohnet and Nivre (2012) parser. 4.1 Setup For Malt and MST we perform the experiments on gold PoS tags, whereas the Bohnet and Nivre (2012) parser predicts PoS tags during testing. Prior to each experiment with Malt, we used MaltOptimizer to obtain settings and a feature model; for MST we exploited default configuration; for the Bohnet </context>
<context position="16704" citStr="Ytrestøl, 2011" startWordPosition="2776" endWordPosition="2777">nctuation. Thus, supertags do not provide extra level of detalization for PTB tags, but PTB tags and supertags are complementary. As discussed in section 3.4, they contain bits of information that are different. For this reason their combination results in slight increase of accuracy for all three parsers on all dependency formats (Table 1, Gold PTB tags + gold supertags, and Table 2, Predicted PTB + gold supertags and Predicted supertags + gold PTB). The Bohnet and Nivre (2012) parser predicts supertags with an average accuracy of 89.73% which is significantly lower than state-ofthe-art 95% (Ytrestøl, 2011). When we consider punctuation in the evaluation, all scores raise significantly for DT and at the same time decrease for SB and CD for all three parsers. This is explained by the fact that punctuation in DT is always attached to the nearest token which is easy to learn for a statistical parser. 4.3 Error analysis Using the CoNLL-07 evaluation script2 on our test set, for each parser we obtained the error rate distribution over CPOSTAG on SB, CD and DT. VBP, VBZ and VBG. VBP (verb, non-3rd person singular present), VBZ (verb, 3rd person singular present) and VBG (verb, gerund or present partic</context>
<context position="22188" citStr="Ytrestøl, 2011" startWordPosition="3723" endWordPosition="3724">o each other than SB and DT; however, we did not observe sound evidence of a correlation between structural similarity of CD and DT and their parsing accuracies Regarding the tagset aspect, it is natural that PTB tags are good for SB and CD, whereas the more fine-grained set of supertags fits DT better. PTB tags and supertags are complementary, and for all three parsers we observe slight benefits from being supplied with both types of tags. As future work we would like to run more experiments with predicted supertags. In the absence of a specialized supertagger, we can follow the pipeline of (Ytrestøl, 2011) who reached the stateof-the-art supertagging accuracy of 95%. Another area of our interest is an extrinsic evaluation of SB, CD and DT, e.g. applied to semantic role labeling and question-answering in order to find out if the usage of the DT format grounded in the computational grammar theory is beneficial for such tasks. Acknowledgments The authors would like to thank Rebecca Dridan, Joakim Nivre, Bernd Bohnet, Gertjan van Noord and Jelke Bloem for interesting discussions and the two anonymous reviewers for comments on the work. Experimentation was made possible through access to the high-pe</context>
</contexts>
<marker>Ytrestøl, 2011</marker>
<rawString>Gisle Ytrestøl. 2011. Cuteforce: deep deterministic HPSG parsing. In Proceedings of the 12th International Conference on Parsing Technologies, IWPT ’11, pages 186–197, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>