<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999348">
Extracting Noun Phrases from Large-Scale Texts:
A Hybrid Approach and Its Automatic Evaluation
</title>
<author confidence="0.99074">
Kuang-hua Chen and Hsin-Hsi Chen
</author>
<affiliation confidence="0.908627666666667">
Department of Computer Science and Information Engineering
National Taiwan University
Taipei, Taiwan, R.O.C.
</affiliation>
<email confidence="0.983106">
Internet: hh_chen@csie.ntu.edu.tw
</email>
<sectionHeader confidence="0.99297" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999698083333333">
To acquire noun phrases from running texts is useful for
many applications, such as word grouping, terminology
indexing, etc. The reported literatures adopt pure
probabilistic approach, or pure rule-based noun phrases
grammar to tackle this problem. In this paper, we apply
a probabilistic chunker to deciding the implicit
boundaries of constituents and utilize the linguistic
knowledge to extract the noun phrases by a finite state
mechanism. The test texts are SUSANNE Corpus and
the results are evaluated by comparing the parse field of
SUSANNE Corpus automatically. The results of this
preliminary experiment are encouraging.
</bodyText>
<sectionHeader confidence="0.997026" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9929476">
From the cognitive point of view, human being must
recognize, learn and understand the entities or concepts
(concrete or abstract) in the texts for natural language
comprehension. These entities or concepts are usually
described by noun phrases. The evidences from the
language learning of children also show the belief (Snow
and Ferguson, 1977). Therefore, if we can grasp the
noun phases of the texts, we will understand the texts to
some extent. This consideration is also captured by
theories of discourse analysis, such as Discourse
Representation Theory (Kamp, 1981).
Traditionally, to make out the noun phrases in a text
means to parse the text and to resolve the attachment
relations among the constituents. However, parsing the
text completely is very difficult, since various
ambiguities cannot be resolved solely by syntactic or
semantic information. Do we really need to fully parse
the texts in every application? Some researchers apply
shallow or partial parsers (Smadja, 1991; Hindle, 1990)
to acquiring specific patterns from texts. These tell us
that it is not necessary to completely parse the texts for
some applications.
This paper will propose a probabilistic partial parser
and incorporate linguistic knowledge to extract noun
phrases. The partial parser is motivated by an intuition
(Abney, 1991):
(1) When we read a sentence, we read it chunk by
chunk.
Abney uses two level grammar rules to implement the
parser through pure LR parsing technique. The first
level grammar rule takes care of the chunking process.
The second level grammar rule tackles the attachment
problems among chunks. Historically, our statistics-
based partial parser is called chunker. The chunker
receives tagged texts and outputs a linear chunk
sequences. We assign a syntactic head and a semantic
head to each chunk. Then, we extract the plausible
maximal noun phrases according to the information of
syntactic head and semantic head, and a finite state
mechanism with only 8 states.
Section 2 will give a brief review of the works for the
acquisition of noun phrases. Section 3 will describe the
language model for chunker. Section 4 will specify how
to apply linguistic knowledge to assigning heads to each
chunk. Section 5 will list the experimental results of
chunker. Following Section 5, Section 6 will give the
performance of our work on the retrieval of noun phrases.
The possible extensions of the proposed work will be
discussed in Section 7. Section 8 will conclude the
remarks.
</bodyText>
<sectionHeader confidence="0.989445" genericHeader="introduction">
2. Previous Works
</sectionHeader>
<bodyText confidence="0.999866909090909">
Church (1988) proposes a part of speech tagger and a
simple noun phrase extractor. His noun phrase extractor
brackets the noun phrases of input tagged texts according
to two probability matrices: one is starting noun phrase
matrix; the other is ending noun phrase matrix. The
methodology is a simple version of Garside and Leech&apos;s
probabilistic parser (1985). Church lists a sample text in
the Appendix of his paper to show the performance of his
work. It demonstrates only 5 out of 248 noun phrases are
omitted. Because the tested text is too small to assess the
results, the experiment for large volume of texts is needed.
</bodyText>
<page confidence="0.99557">
234
</page>
<bodyText confidence="0.999966545454546">
Bourigault (1992) reports a tool, LEX7&apos;ER, for
extracting terminologies from texts. LEXTER triggers
two-stage processing: 1) analysis (by identification of
frontiers), which extracts the maximal-length noun
phrase: 2) parsing (the maximal-length noun phrases),
which, furthermore, acquires the terminology embedded
in the noun phrases. Bourigault declares the LEXTER
extracts 95% maximal-length noun phrases, that is,
43500 out of 46000 from test corpus. The result is
validated by an expert. However, the precision is not
reported in the Boruigault&apos;s paper.
Voutilainen (1993) announces NProol for acquisition
of maximal-length noun phrases. NPtool applies two
finite state mechanisms (one is NP-hostile; the other is
NP-friendly) to the task. The two mechanisms produce
two NP sets and any NP candidate with at least one
occurrence in both sets will be labeled as the &amp;quot;ok&amp;quot; NP.
The reported recall is 98.5-100% and the precision is 95-
98% validated manually by some 20000 words. But from
the sample text listed in Appendix of his paper, the recall
is about 85% and we can find some inconsistencies
among these extracted noun phrases.
</bodyText>
<sectionHeader confidence="0.988094" genericHeader="method">
3. Language Model
</sectionHeader>
<bodyText confidence="0.975235888888889">
Parsing can be viewed as optimizing. Suppose an n-
word sentence. w1. v,.....w (including punctuation
marks), the parsing task is to find a parsing tree T, such
that P(71w1, w.„ iiin) has the maximal probability. We
define T here to be a sequence of chunks, c1, c2, ...,Cm
and each c, (0 &lt;j &lt; in) contains one or more words wj
(0 &lt;j n). For example, the sentence &amp;quot;parsing can be
viewed as optimization.&amp;quot; consists of 7 words. Its one
possible parsing result under our demand is:
</bodyText>
<listItem confidence="0.545022">
(2) [parsing] [can be viewed] [as optimization] [.]
</listItem>
<equation confidence="0.876722">
1 c2 C3 C4
Now, the parsing task is to find the best chunk sequence,
C*, such that
(3) = argmaxP(C, I )
The Ci is one possible chunk sequence, c1, c2, cm‘,
</equation>
<bodyText confidence="0.917170909090909">
where in, is the number of chunks of the possible chunk
sequence. To chunk raw text without other information
is very difficult, since the word patterns are many
millions. Therefore, we apply a tagger to preprocessing
the raw texts and give each word a unique part of speech.
That is, for an n-word sentence, w1, w2. ....w,,, (including
punctuation marks), we assign part of speeches t1,
tn to the respective words. Now the real working model
is:
( 4 ) C* = arginaxP(C, I t,&amp;quot;)
Using bi-gram language model, we then reduce P(Cilti,
</bodyText>
<equation confidence="0.8731024">
t2, tn) as (5),
(5) P(Cat)= P,(c;It;)
1-1 t)x P,(cklt)
7=-•-
=----&amp;quot;nPAcklck,)x I(c)
</equation>
<bodyText confidence="0.769637333333333">
where P1( • ) denotes the probability for the i&apos;th chunk
sequence and co denotes the beginning mark of a
sentence. Following (5), formula (4) becomes
</bodyText>
<equation confidence="0.936884">
(6) argInaxP(C, It; )
argpaxn p (ck Ick,) x p (ck )
))1
</equation>
<bodyText confidence="0.9971296">
In order to make the expression (6) match the intuition of
human being, namely, 1) the scoring metrics are all
positive, 2) large value means high score, and 3) the
scores are between 0 and 1, we define a score function
S(P( • )) shown as (7).
</bodyText>
<equation confidence="0.86965975">
(7) S(P( • ))=0 when P( • )= 0;
S(P( • ))= 1.0/(1.0+ABS(log(P( • )))) o/w.
We then rewrite (6) as (8).
(8) C* = argmaxP( )
argmaxn p (ck ) x p (ck )
k=1
= argmaxE [log( P, (c, Ick_, )) + log(P, ))]
= argmaxELS(Fjci Ici_3)+S(P, (ck
</equation>
<bodyText confidence="0.999523714285714">
The final language model is to find a chunk sequence C*,
which satisfies the expression (8).
Dynamic programming shown in (9) is used to find
the best chunk sequence. The score[i] denotes the score
of position i. The words between position prep] and
position i form the best chunk from the viewpoint of
position i. The dscore(cf) is the score of the probability
</bodyText>
<page confidence="0.996342">
235
</page>
<bodyText confidence="0.91546775">
P(c1) and the cscore(clici-i) is the score of the probability
/3(cIlcr-i). These scores are collected from the training
corpus. SUSANNE corpus (Sampson, 1993; Sampson,
1994). The details will be touched on in Section 5.
</bodyText>
<listItem confidence="0.99287175">
(9) Algorithm
input : word sequence wi, w2, wn, and
the corresponding POS sequence ti, t2, tn
output : a sequence of chunks ci, c2,
1. score[0] = 0;
pre[0] = 0;
2. for (i = i&lt;n+1; i++) do 3 and 4;
3. j*=maxarg(score[pre[j]l+dscore(ci)+cscore(cjicj-i));
</listItem>
<bodyText confidence="0.926165">
where ci = ti;
</bodyText>
<equation confidence="0.9701615">
cj-i = tpreW+ 1, , tj;
4. score[i]=score[prep*]]+dscore(ci*)+cseore(ci*Ici*-1);
pre(i] = j*:
5. for (i=n; i&gt;0; i=pre[i]) do
</equation>
<bodyText confidence="0.767033">
output the word Wpre[i]+1, Wi to form a chunk;
</bodyText>
<sectionHeader confidence="0.935629" genericHeader="method">
4. Linguistic Knowledge
</sectionHeader>
<bodyText confidence="0.990248666666667">
In order to assign a head to each chunk, we first define
priorities of POSes. X&apos;-theory (Sells, 1985) has defined
the X&apos;-equivalences shown as Table 1.
</bodyText>
<tableCaption confidence="0.996188">
Table 1. X&apos;-Equivalences
</tableCaption>
<table confidence="0.965419">
X X&apos; X&amp;quot;
N N&apos; NP
V V&apos; VP
A A&apos; AP
P P&apos; PP
INFL S (I&apos;) S (IP)
</table>
<bodyText confidence="0.965138">
Table 1 defines five different phrasal structures and the
hierarchical structures. The heads of these phrasal
structures are the first level of X&apos;-Equivalences, that is, X.
The other grammatical constituents function as the
specifiers or modifiers, that is, they are accompanying
words not core words. Following this line, we define the
primary priority of POS listed in Table 1.
</bodyText>
<listItem confidence="0.876416">
(10) Primary POS priority&apos; : V&gt;N&gt; A&gt;P
</listItem>
<bodyText confidence="0.967923666666667">
In order to extract the exact head, we further define
Secondary POS priority among the 134 POSes defined in
LOB corpus (Johansson, 1986).
</bodyText>
<listItem confidence="0.7772694">
(11) Secondary POS priority is a linear
precedence relationship within the primary
priorities for coarse POSes
1 We do not consider the INFL. since our model will not touch on this
structure.
</listItem>
<bodyText confidence="0.99194475">
For example, LOB corpus defines four kinds of verbial
words under the coarse POS V: VB*, DO*, BE* and
HV*2. The secondary priority within the coarse POS V
is:
</bodyText>
<listItem confidence="0.943371">
(12) VB* &gt; FiV* &gt; DO* &gt; BE*
</listItem>
<bodyText confidence="0.984357666666667">
Furthermore, we define the semantic head and the
syntactic head (Abney, 1991).
(13) Semantic head is the head of a phrase
according to the semantic usage; but
syntactic head is the head based on the
grammatical relations.
Both the syntactic head and the semantic head are useful
in extracting noun phrases. For example, if the semantic
head of a chunk is the noun and the syntactic one is the
preposition, it would be a prepositional phrase.
Therefore, it can be connected to the previous noun
chunk to form a new noun phrase. In some case, we will
find some chunks contain only one word, called one-
word chunks. They maybe contain a conjunction, e.g.,
that. Therefore, the syntactic head and the semantic
head of one-word chunks are the word itself.
Following these definitions, we extract the noun
phrases by procedure (14):
</bodyText>
<listItem confidence="0.959531636363636">
(14) (a) Tag the input sentences.
(b) Partition the tagged sentences into
chunks by using a probabilistic partial
parser.
(c) Decide the syntactic head and the
semantic head of each chunk.
(d) According to the syntactic and the
semantic heads, extract noun phrase
from these chunks and connect as
many noun phrases as possible by a
finite state mechanism.
</listItem>
<figureCaption confidence="0.998937">
Figure 1. The Noun Phrases Extraction Procedure
</figureCaption>
<bodyText confidence="0.6711158">
Figure 1 shows the procedure. The input raw texts will
be assigned POSes to each word and then pipelined into
2 Asterisk * denotes wildcard. Therefore, VB* represents VB (verb,
base form), VBD (verb, preterite), VBG (present participle), VBN (past
participle) and VBZ (3rd singular form of verb).
</bodyText>
<figure confidence="0.99724">
NP TRACTOR
NP set
tagged
CD texts CID
TT&apos;
TAG-MAPPER)
Tr
SUSANNE
CORPUS
EVALUATION
NP set
recall &amp;
precision
raw
texts
chunked
texts
</figure>
<page confidence="0.993417">
236
</page>
<bodyText confidence="0.99925">
a chunker. The tag sets of LOB and SUSANNE are
different. Since the tag set of SUSANNE corpus is
subsumed by the tag set of LOB corpus, a TAG-
MAPPER is used to map tags of SUSANNE corpus to
those of LOB corpus. The chunker will output a
sequence of chunks. Finally, a finite state NP-
TRACTOR will extract NPs. Figure 2 shows the finite
state mechanism used in our work.
</bodyText>
<figureCaption confidence="0.993889">
Figure 2. The Finite State Machine for Noun Phrases
</figureCaption>
<bodyText confidence="0.984712375">
The symbols in Figure 2 are tags of LOB corpus. N*
denotes nous; P* denotes pronouns; ,1* denotes adjectives;
A* denotes quantifiers. qualifiers and determiners; IN
denotes prepositions; CD* denotes cardinals; OD*
denotes ordinals, and NR* denotes adverbial nouns.
Asterisk * denotes a wildcard. For convenience, some
constraints, such as syntactic and semantic head
checking, are not shown in Figure 2.
</bodyText>
<sectionHeader confidence="0.94464" genericHeader="method">
5. First Stage of Experiments
</sectionHeader>
<bodyText confidence="0.977342266666667">
Following the procedures depicted in Figure 1, we
should train a chunker firstly. This is done by using the
SUSANNE Corpus (Sampson, 1993; Sampson, 1994) as
the training texts. The SUSANNE Corpus is a modified
and condensed version of Brown Corpus (Francis and
Kucera, 1979). It only contains the 1/10 of Brown
Corpus, but involves more information than Brown
Corpus. The Corpus consists of four kinds of texts: 1) A:
press reportage; 2) G: belles letters, biography, memoirs;
3) J: learned writing; and 4) N: adventure and Western
fiction. The Categories of A, G. J and N are named from
respective categories of the Brown Corpus. Each
Category consists of 16 files and each file contains about
2000 words.
The following shows a snapshot of SUSANNE Corpus.
</bodyText>
<footnote confidence="0.740446444444445">
G010010a - YB &lt;minbrk&gt; [Oh.Oh]
001:0010b - JJ NORTHERN northern [0[S[Np:s.
G01:0010c - NN2 liberals liberal .Nps]
(101:0010d - VBR are be [Vab.Vab]
G01:0010e - AT the the [Np:e.
G01:0010f - JB chief chief
G010010g - NN2 supporters supporter
G010010h - 10 of of
G010010i - JJ civil civil 1Np•
</footnote>
<table confidence="0.9997046">
G01:00101 - NN2 rights right .Np]
G01:0020a - CC and and !Poi.
G01:00206 - IO of of
G01:0020c - NN1u integration integration .Po+]PolNp:e15]
G01:0020d - YF +
</table>
<tableCaption confidence="0.9999075">
Table 2 lists basic statistics of SUSANNE Corpus.
Table 2. The Overview of SUSANNE Corpus
</tableCaption>
<table confidence="0.999566833333333">
Categories Files Paragraphs Sentences Words
A 16 767 1445 37180
G 16 280 1554 37583
J 16 197 1353 36554
N 16 723 2568 38736
Total 64 1967 6920 150053
</table>
<bodyText confidence="0.993340631578947">
In order to avoid the errors introduced by tagger, the
SUSANNE corpus is used as the training and testing
texts. Note the tags of SUSANNE corpus are mapped to
LOB corpus. The 3/4 of texts of each categories of
SUSANNE Corpus are both for training the chunker and
testing the chunker (inside test). The rest texts are only
for testing (outside test). Every tree structure contained
in the parse field is extracted to form a potential chunk
grammar and the adjacent tree structures are also
extracted to form a potential context chunk grammar.
After the training process, total 10937 chunk grammar
rules associated with different scores and 37198 context
chunk grammar rules are extracted. These chunk
grammar rules are used in the chunking process.
Table 3 lists the time taken for processing SUSANNE
corpus. This experiment is executed on the Sun Sparc
10, model 30 workstation. T denotes time, W word, C
chunk, and S sentence. Therefore, T/W means the time
taken to process a word on average.
</bodyText>
<tableCaption confidence="0.997238">
Table 3. The Processing Time
</tableCaption>
<table confidence="0.99921">
T/W TIC T/S
0.00295 0.0071 0.0758
G 0.00283 0.0069 0.0685
J 0.00275 0.0073 0.0743
N 0.00309 0.0066 0.0467
Av. li 0.00291 0.0070 0.0663
</table>
<bodyText confidence="0.9904177">
According to Table 3, to process a word needs 0.00291
seconds on average. To process all SUSANNE corpus
needs about 436 seconds, or 7.27 minutes.
In order to evaluate the performance of our chunker,
we compare the results of our chunker with the
denotation made by the SUSANNE Corpus. This
comparison is based on the following criterion:
(15) The content of each chunk should be
dominated by one non-terminal node in
SUSANNE parse field.
</bodyText>
<page confidence="0.982372">
237
</page>
<bodyText confidence="0.998969666666667">
This criterion is based on an observation that each non-
terminal node has a chance to dominate a chunk.
Table 4 is the experimental results of testing the
SUSANNE Corpus according to the specified criterion.
As usual, the symbol C denotes chunk and S denotes
sentence.
</bodyText>
<tableCaption confidence="0.994306">
Table 4 Experimental Results
</tableCaption>
<table confidence="0.999689347826087">
TEST OUTSIDE TEST INSIDE TEST
Cat.
C S C S
A sof correct 4866 380 10480 1022
# of incorrect 40 14 84 29
total # 4906 394 10564 1051
correct rate 0.99 0.96 0.99 0.97
G # of correct 4748 355 10293 1130
# of incorrect 153 32 133 37
total # 4901 387 10426 1167
correct rate 0.97 0.92 0.99 0.97
J 4 of correct 4335 283 9193 1032
tt of incorrect 170 15 88 23
total # 4505 298 9281 1055
correct rate 0.96 0.95 0.99 0.98
N 4 of correct 5163 536 12717 1906
# of incorrect 79 42 172 84
total # 5242 578 12889 1990
correct rate 0,98 0.93 0.99 0.96
Av. Sof correct 19112 1554 42683 5090
# of incorrect 442 103 477 173
total # 19554 1657 43160 5263
correct rate 0.98 0.94 0.99 0.97
</table>
<bodyText confidence="0.993062625">
Table 4 shows the chunker has more than 98% chunk
correct rate and 94% sentence correct rate in outside test,
and 99% chunk correct rate and 97% sentence correct
rate in inside test. Note that once a chunk is mischopped,
the sentence is also mischopped. Therefore, sentence
correct rate is always less than chunk correct rate.
Figure 3 gives a direct view of the correct rate of this
chunker.
</bodyText>
<figure confidence="0.6251565">
95
9
ttc
&apos;honk t9n4e Chunk Stnlence
</figure>
<subsectionHeader confidence="0.612654">
Outside Test Inside Test
</subsectionHeader>
<bodyText confidence="0.619226">
Figiire 3 The Correct Rate of Experiments
</bodyText>
<sectionHeader confidence="0.700001" genericHeader="method">
6. Acquisition of Noun Phrases
</sectionHeader>
<bodyText confidence="0.999809666666667">
We employ the SUSANNE Corpus as test corpus. Since
the SUSANNE Corpus is a parsed corpus, we may use it
as criteria for evaluation. The volume of test texts is
around 150,000 words including punctuation marks.
The time needed from inputting texts of SUSANNE
Corpus to outputting the extracted noun phrases is listed
in Table 5. Comparing with Table 3, the time of
combining chunks to form the candidate noun phrases is
not significant.
</bodyText>
<tableCaption confidence="0.976613">
Table 5. Time for Acquisition of Noun Phrases
</tableCaption>
<table confidence="0.998346833333333">
Words Time (sec.) Time/Word
A 37180 112.32 0.00302
G 37583 108.80 0.00289
J 36554 103.04 0.00282
N 38736 122.72 0.00317
Total Q 150053 446.88 0.00298
</table>
<bodyText confidence="0.955083333333333">
The evaluation is based on two metrics: precision and
recall. Precision means the correct rate of what the
system gets. Recall indicates the extent to which the real
noun phrases retrieved from texts against the real noun
phrases contained in the texts. Table 6 describes how to
calculate these metrics.
</bodyText>
<tableCaption confidence="0.84168">
Table 6. Contingency Table for Evaluation
</tableCaption>
<table confidence="0.67135925">
SUSANNE
NP non-NP
System NP a b
llnon-NP C
</table>
<bodyText confidence="0.9831404">
The rows of &amp;quot;System&amp;quot; indicate our NP-TRACTOR thinks
the candidate as an NP or not an NP; the columns of
&amp;quot;SUSANNE&amp;quot; indicate SUSANNE Corpus takes the
candidate as an NP or not an NP. Following Table 6, we
will calculate precision and recall shown as (16).
</bodyText>
<equation confidence="0.9861035">
(16) Precision = a/(a+b) * 100%
Recall = a/(a+c) * 100%
</equation>
<bodyText confidence="0.999723583333333">
To calculate the precision and the recall based on the
parse field of SUSANNE Corpus is not so
straightforward at the first glance. For example. (17)3
itself is a noun phrse but it contains four noun phrases.
A tool for extracting noun phrases should output what
kind of and how many noun phrases, when it processes
the texts like (17). Three kinds of noun phrases
(maximal noun phrases, minimal noun phrases and
ordinary noun phrases) are defined first. Maximal noun
phrases are those noun phrases which are not contained
in other noun phrases. In contrast, minimal noun
phrases do not contain any other noun phrases.
</bodyText>
<footnote confidence="0.8992865">
3 This example is taken from N06:0280d-N06:0290d, Susanne Corpus
(N06 means file N06, 0280 and 0290 are the original line numbers in
Brown Corpus. Recall that the Susanne Corpus is a modified and reduced
version of Brown Corpus).
</footnote>
<page confidence="0.997147">
238
</page>
<bodyText confidence="0.9859435">
Apparently, a noun phrase may be both a maximal noun
phrase and a minimal noun phrase. Ordinary noun
phrases are noun phrases with no restrictions. Take (17)
as an example. It has three minimal noun phrases, one
maximal noun phrases and five ordinary noun phrases.
In general, a noun-phrase extractor forms the front end
of other applications, e.g., acquisition of verb
subcategorization frames. Under this consideration, it is
not appropriate to taking (17) as a whole to form a noun
phrase. Our system will extract two noun phrases from
(17), &amp;quot;a black badge of frayed respectability&amp;quot; and &amp;quot;his
neck&amp;quot;.
(17) [Ha black badge] of [frayed respectability]]
that ought never to have left [his neck]]
We calculate the numbers of maximal noun phrases,
minimal noun phrases and ordinary noun phrases
denoted in SUSANNE Corpus, respectively and compare
these numbers with the number of noun phrases
extracted by our system.
Table 7 lists the number of ordinary noun phrases
(NP), maximal noun phrases (MNP), minimal noun
phrases (mNP) in SUSANNE Corpus. MmNP denotes
the maximal noun phrases which are also the minimal
noun phrases. On average, a maximal noun phrase
subsumes 1.61 ordinary noun phrases and 1.09 minimal
noun phrases.
</bodyText>
<tableCaption confidence="0.996984">
Table 7. The Number of Noun Phrases in Corpus
</tableCaption>
<table confidence="0.99831875">
NP MNP mNP MmNP NP mNP
A
MNP MNP
A 10063 5614 6503 3207 1.79 1.16
G 9221 5451 6143 3226 1.69 1.13
J 8696 4568 5200 2241 1.90 1.14
N 9851 7895 7908 5993 1.25 1.00
Total 37831 23528 25754 14667 1.61 1.09
</table>
<bodyText confidence="0.9988865">
To calculate the precision. we examine the extracted
noun phrases (ENP) and judge the correctness by the
SUSANNE Corpus. The CNP denotes the correct
ordinary noun phrases. CMNP the correct maximal noun
phrases. CmNP correct minimal noun phrases and
CMmNP the correct maximal noun phrases which are
also the minimal noun phrases. The results are itemized
in Table 8. The average precision is 95%.
</bodyText>
<tableCaption confidence="0.999261">
Table 8. Precision of Our System
</tableCaption>
<table confidence="0.987938285714286">
ENP CNP CMNP CmNP CMmNP I Precision
A 8011 7660 3709 4348 3047 0.96
G 7431 6943 3626 4366 3028 0.93
J 6457 5958 2701 3134 2005 0.92
N 8861 8559 6319 6637 5808 0.97
,
Total 30760 29120 16355 18485 _ 13888 0.95
</table>
<bodyText confidence="0.99983375">
Here, the computation of recall is ambiguous to some
extent. Comparing columns CMNP and CmNP in Table
8 with columns MNP and mNP in Table 7. 70% of MNP
and 72% of mNP in SUSANNE Corpus are extracted. In
addition, 95% of MmNP is extracted by our system. It
means the recall for extracting noun phrases that exist
independently in SUSANNE Corpus is 95%. What types
of noun phrases are extracted are heavily dependent on
what applications we will follow. We will discuss this
point in Section 7. Therefore, the real number of the
applicable noun phrases in the Corpus is not known.
The number should be between the number of NPs and
that of MNPs. In the original design for NP-TRACTOR,
a maximal noun phrase which contains clauses or
prepositional phrases with prepositions other than &amp;quot;of&apos; is
not considered as an extracted unit. As the result, the
number of such kinds of applicable noun phrases (ANPs)
form the basis to calculate recall. These numbers are
listed in Table 9 and the corresponding recalls are also
shown.
</bodyText>
<tableCaption confidence="0.998869">
Table 9. The limitation of Values for Recall
</tableCaption>
<table confidence="0.999465166666667">
ANP CNP I Recall
A 7873 7660 0.97
G 7199 6943 0.96
J 6278 5958 0.95
N 8793 8559 0.97
Av. 30143 29120 0.96
</table>
<bodyText confidence="0.9994832">
The automatic validation of the experimenta results
gives us an estimated recall. Appendix provides a
sample text and the extracted noun phrases. Interested
readers could examine the sample text and calculate
recall and precision for a comparison.
</bodyText>
<sectionHeader confidence="0.808735" genericHeader="method">
7. Applications
</sectionHeader>
<bodyText confidence="0.999899352941176">
Identification of noun phrases in texts is useful for many
applications. Anaphora resolution (Hirst, 1981) is to
resolve the relationship of the noun phrases, namely,
what the antecedent of a noun phrase is. The extracted
noun phrases can form the set of possible candidates (or
universal in the terminology of discourse representation
theory). For acquisition of verb subcategorization frames,
to bracket the noun phrases in the texts is indispensable.
It can help us to find the boundary of the subject, the
object and the prepositional phrase. We would use the
acquired noun phrases for an application of adjective
grouping. The extracted noun phrases may contain
adjectives which pre-modify the head noun. We then
utilize the similarity of head nouns to group the adjectives.
In addition, we may give the head noun a semantic tag,
such as Roget&apos;s Thesaurus provides, and then analyze the
adjectives. To automatically produce the index of a book,
</bodyText>
<page confidence="0.995435">
239
</page>
<bodyText confidence="0.999943307692308">
we would extract the noun phrases contained in the book,
calculate the inverse document frequency (IDF) and their
term frequency (TF) (Salton, 1991), and screen out the
implausible terms.
These applications also have impacts on identifying
noun phrases. For applications like anaphora resolution
and acquisition of verb subcategorization frames, the
maximal noun phrases are not suitable. For applications
like grouping adjectives and automatic book indexing,
some kinds of maximal noun phrases, such as noun
phrases postmodified by &amp;quot;of&apos; prepositional phrases, are
suitable: but some are not, e.g., noun phrases modified by
relative clauses.
</bodyText>
<sectionHeader confidence="0.830071" genericHeader="conclusions">
8. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999688586956522">
The difficulty of this work is how to extract the real
maximal noun phrases. If we cannot decide the
prepositional phrase &amp;quot;over a husband eyes&amp;quot; is licensed by
the verb &amp;quot;pull&amp;quot;, we will not know &amp;quot;the wool&amp;quot; and &amp;quot;a
husband eyes&amp;quot; are two noun phrases or form a noun
pharse combined by the preposition &amp;quot;over&amp;quot;.
(18) to pull the wool over a husband eyes
to sell the books of my uncle
In contrast, the noun phrase &amp;quot;the books of my uncle&amp;quot; is
so called maximal noun phrase in current context. As
the result, we conclude that if we do not resolve PP-
attachment problem (Hindle and Rooth, 1993), to the
expected extent, we will not extract the maximal noun
phrases. In our work, the probabilistic chunker decides
the implicit boundaries between words and the NP-
TRACTOR connects the adjacent noun chunks. When a
noun chunk is followed by a preposition chunk, we do
not connect the two chunks except the preposition chunk
is led by &amp;quot;of&apos; preposition.
Comparing with other works, our results are
evaluated by a parsed corpus automatically and show the
high precision. Although we do not point out the exact
recall, we provide estimated values. The testing scale is
large enough (about 150,000 words). In contrast,
Church (1988) tests a text and extracts the simple noun
phrases only. Bourigault&apos;s work (1992) is evaluated
manually, and dose not report the precision. Hence, the
real performance is not known. The work executed by
Voutilainen (1993) is more complex than our work. The
input text first is morphologizied, then parsed by
constraint grammar, analyzed by two different noun
phrases grammar and finally extracted by the
occurrences. Like other works, Voutilainen&apos;s work is
also evaluated manually.
In this paper, we propose a language model to chunk
texts. The simple but effective chunker could be seen as
a linear structure parser, and could be applied to many
applications. A method is presented to extract the noun
phrases. Most importantly, the relations of maximal
noun phrases, minimal noun phrases, ordinary noun
phrases and applicable noun phrases are distinguished in
this work. Their impacts on the subsequent applications
are also addressed. In addition, automatic evaluation
provides a fair basis and does not involve human costs.
The experimental results show that this parser is a useful
tool for further research on large volume of real texts.
</bodyText>
<sectionHeader confidence="0.994008" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.991271">
We are grateful to Dr. Geoffrey Sampson for his kindly
providing SUSANNE Corpus and the details of tag set to
us.
</bodyText>
<sectionHeader confidence="0.998229" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999420611111111">
Abney, Steven (1991), &amp;quot;Parsing by Chunks,&amp;quot; in
Principle-Based Parsing, Berwick, Abney and
Tenny (Eds.), Kluwer Academic Publishers, pp.
257-278.
Bourigault, Didier (1992), &amp;quot;Surface Grammatical
Analysis for the Extraction of Terminological Noun
Phrases,&amp;quot; Proceedings of the 15th International
Conference on Computational Linguistics,
COLING-92, Vol. III, Nantes, France, pp. 977-981.
Church, Kenneth (1988), &amp;quot;A Stochastic Parts Program
and Noun Phrase Parser for Unrestricted Text,&amp;quot;
Proceedings of Second Conference on Applied
Natural Language Processing, pp. 136-143.
Francis, N. and Kucera, H. (1979), Manual of
Information to Accompany a Standard Sample of
Presentday Edited American English, for Use with
Digital Computers, Department of Linguistics,
Brown University, Providence, R. I., U.S.A.,
original ed. 1964, revised 1971, revised and
augmented 1979.
Garside, Roger and Leech, Geoffrey (1985), &amp;quot;A
Probabilistic Parser,&amp;quot; Proceedings of Second
Conference of the European Chapter of the ACL.
pp. 166-170.
Hindle, Donald (1990), &amp;quot;Noun Classification from
Predicate-Argument Structures,&amp;quot; Proceedings of
28th Annual Meeting of ACL, pp. 268-275.
Hindle, Donald and Rooth, Mats (1993), &amp;quot;Structural
Ambiguity and Lexical Relations,&amp;quot; Computational
Linguistics, 19(1), pp. 103-120.
Hirst, G. (1981), Anaphora in Natural Language
Understanding: a Survey, Lecture Notes 119.
Springer-Verlag.
Johansson, Stig (1986), The Tagged LOB Corpus:
Users&apos; Manual, Bergen: Norwegian Computing
Centre for the Humanities.
</reference>
<page confidence="0.948439">
240
</page>
<reference confidence="0.999439347826087">
Kamp. H. (1981), &amp;quot;A Theory of Truth and Semantic
Representation,&amp;quot; Formal Methods in the Study of
Language, Vol. 1, (J. Groenendijk, T. Janssen, and
M. Stokhof Eds.), Mathema-tische Centrum.
Salton, G. (1991), &amp;quot;Developments in Automatic Text
Retrieval,&amp;quot; Science, Vol. 253, pp. 974-979.
Sampson, Geoffrey (1993), &amp;quot;The SUSANNE Corpus,&amp;quot;
ICAME Journal, No. 17, pp. 125-127.
Sampson. Geoffrey (1994), English for the Computer,
Oxford University Press.
Sells, Peter (1985), Lectures on Contemporary Syntactic
Theories, Lecture Notes, No. 3, CSLI.
Smadja, Frank (1991), Extracting Collocations from
Text, An Application: Language Generation, Ph.D.
Dissertation. Columbia University, 1991.
Snow. C.E. and Ferguson, C.A. (Eds.) (1977), Talking
to Children: Language Input and Acquisition,
Cambridge. Cambridge University Press.
Voutilainen, Atro (1993), &amp;quot;NPtool, a Detector of
English Noun Phrases.&amp;quot; Proceedings of the
Workshop on Very Large Corpora: Academic and
Industrial Perspectives, Ohio State University,
Columbus, Ohio, USA, pp. 48-57.
</reference>
<sectionHeader confidence="0.986294" genericHeader="references">
Appendix
</sectionHeader>
<bodyText confidence="0.997089857142857">
For demonstration, we list a sample text quoted from
Ni 8:0010a-N18 :0250e, SUSANNE Corpus. The
extracted noun phrases are bracketed. We could compute
the precision and the recall from the text as a reference
and compare the gap with the experimental results
itemized in Section 6. In actual, the result shows that the
system has high precision and recall for the text.
</bodyText>
<reference confidence="0.91532171875">
I &amp;quot;Foo _QL many_AP people_NNS I think VB that_CS [ the ATI
primary JJ purpose NN of IN a_AT higher JJR education_NN ] is BEZ
to TO help_ VB I you_PP2 make VB [ a_AT living_NN ] +;_; thi; DT
is BEZ not_XNOT so RB +„ for_CS [ education_NN I offers_VBZ
I all ABN kinds NNS of IN dividends_NNS ] +,_, including_IN
how WRB to_TO pull_VB [ the ATI wool NN ] over [ a_AT
husband _NN eyes NNS I while_ CS [ you_PP2 ] are_BER having_I-VG
I an AT affair NN I with_IN [ his_PP$ wife NN I . If CS [ it PP3 ]
were BED not_XNOT for IN [ an AT old JJ professor_NPT I
who WPR made VBD [ me_PP 10 ] read VB [ th-e_ATI classics_NN ]
1 PPI A 1 would_MD have_HV been—BEN stymied_VBN on IN
what_ WDT to_TO do DO +„ and_CC now_RN [ I PP-1A ]
understand _VB why_WRB [ they_PP3AS ] are_BER [ classics NN I+; ;
those_ DTS who WPR wrote VBD [ them_PP3OS I knew VBD
I people_NNS 1 and_CC what WDT made_VBD [ people_NNS I
tick_VB [ I PP1A ] worked VBD for_IN [ my_PP$ Uncle_NPT ]
( _( [ +an_AT Uncle NPT by_IN marriage_NN I so_RB [ you PP2 I
will MD not XNOT— think VB this_DT has_HVZ [ a_AT mild JJ
undercurrent NN of_IN incest NN +)_) who WPR ran VBD
I one_CDI (:)--f IN those DTS antique JJ shops_NNS ] in_IN [ New_JJ
Orleans_NP ] Vieux &amp;AV Carre_&amp;FW +„ [ the_ATI old JJ French JJ
Quarter NPL I [ The_ATI arrangement NN I [ I PPlA I had_HVD
with_IN [ himPP30 ] was BEDZ to_TO work VB [ four CD
hours NRS I [ a_AT day_NR I . . [ The ATI rest_NN of IN the ATI
time__NR It I_PPI A I devoted_VBD to_IN painting_VBG or CC to IN
those DTS [ other JJB activities_NNS I [ a_AT young_JJ and_CC—
healthy_ll man_NN just_RB out_IN of IN [ college_NN ] finds_BZ
interesting_JJ . I I_PPI A I had_HVD I a_AT one-room JJ studio_ NN 1
which_WDTR overlooked_VBD I an_AT ancient JJ courtyard_ NN I
filled_VBN with_IN [ flowers NNS and_CC plants_ NNS ]
blooming_VBG everlastingly_RB in_IN I the ATI southern JJ
sun_NN I ._. I I_PPI A ] had_HVD come_VBN to_ IN [ New JJ
</reference>
<equation confidence="0.923902894736842">
Orleans_NP ] [ two_CD years_NRS ] earlier_RBR after_IN
[ graduating_VBG college_NN I partly_RB because CS [ 1_PP I A ]
loved_VBD [ the_ATI city_NPL I and_CC partly RB because _CS
there_EX was_BEDZ quite_QL [ a_AT noted_JJ art_NN colony NN I
there_RN When_CS [ my_PP$ Uncle_NPT ] offered_VBD
[ me_PPIO ] [ a_AT part-time _JJ job_NN I which_WDTR would_MD
take_VB [ care_NN ] of IN [ my_PP$ normal_JJ expenses NNS ]
and_CC give_VB [ me_PPIO II time_NR ]to_TO paint_VB [ I_PP1A
accepted_VBD [ The_ATI arrangement_NN I tumed_VBD out_RP
to_TO be_BE excellent_B [ I_PP1 A ] loved_VBD [ the ATI
city_NPL I and_CC [ I_PPI A I particularly_RB loved_VBD [ the_ATI
gaiety_NN and_CC spirit_NN I of IN [ Mardi_NR Gras_NR I ._.
[ I_PP1 A I had_HVD seen_VBN (two CD of IN them_PP3OS I and_CC
[ we_PPIAS ] would_MD soon RB be_BE in_IN another_DT city-
wide JJ +,_, [ joyous_JJ celebration_NN with_IN romance_NN I in_IN
[ the_ATI air_NN ] +;_; and_CC +,_, when_CS [ you_PP2 I took_VBD
[ a_AT walk_NPL ] [ you_PP2 ] never_RB knew_VBD what_WDT
[ adventure_NN or_CC pair_NN of IN sparkling_JJ eyes_NNS I
were_BED waiting_VBG around_IN [ the_ATI next_OD comer_NPL I ._.
</equation>
<page confidence="0.993631">
241
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.665007">
<title confidence="0.9998205">Extracting Noun Phrases from Large-Scale Texts: A Hybrid Approach and Its Automatic Evaluation</title>
<author confidence="0.999745">Kuang-hua Chen</author>
<author confidence="0.999745">Hsin-Hsi Chen</author>
<affiliation confidence="0.999868">Department of Computer Science and Information Engineering National Taiwan University</affiliation>
<address confidence="0.97084">Taiwan, R.O.C.</address>
<email confidence="0.675356">Internet:hh_chen@csie.ntu.edu.tw</email>
<abstract confidence="0.999767">To acquire noun phrases from running texts is useful for many applications, such as word grouping, terminology reported literatures adopt pure probabilistic approach, or pure rule-based noun phrases grammar to tackle this problem. In this paper, we apply a probabilistic chunker to deciding the implicit boundaries of constituents and utilize the linguistic knowledge to extract the noun phrases by a finite state mechanism. The test texts are SUSANNE Corpus and the results are evaluated by comparing the parse field of SUSANNE Corpus automatically. The results of this preliminary experiment are encouraging.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Parsing by Chunks,&amp;quot; in Principle-Based Parsing,</title>
<date>1991</date>
<pages>257--278</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Berwick, Abney</location>
<contexts>
<context position="2245" citStr="Abney, 1991" startWordPosition="332" endWordPosition="333">ituents. However, parsing the text completely is very difficult, since various ambiguities cannot be resolved solely by syntactic or semantic information. Do we really need to fully parse the texts in every application? Some researchers apply shallow or partial parsers (Smadja, 1991; Hindle, 1990) to acquiring specific patterns from texts. These tell us that it is not necessary to completely parse the texts for some applications. This paper will propose a probabilistic partial parser and incorporate linguistic knowledge to extract noun phrases. The partial parser is motivated by an intuition (Abney, 1991): (1) When we read a sentence, we read it chunk by chunk. Abney uses two level grammar rules to implement the parser through pure LR parsing technique. The first level grammar rule takes care of the chunking process. The second level grammar rule tackles the attachment problems among chunks. Historically, our statisticsbased partial parser is called chunker. The chunker receives tagged texts and outputs a linear chunk sequences. We assign a syntactic head and a semantic head to each chunk. Then, we extract the plausible maximal noun phrases according to the information of syntactic head and se</context>
<context position="9491" citStr="Abney, 1991" startWordPosition="1577" endWordPosition="1578">ority&apos; : V&gt;N&gt; A&gt;P In order to extract the exact head, we further define Secondary POS priority among the 134 POSes defined in LOB corpus (Johansson, 1986). (11) Secondary POS priority is a linear precedence relationship within the primary priorities for coarse POSes 1 We do not consider the INFL. since our model will not touch on this structure. For example, LOB corpus defines four kinds of verbial words under the coarse POS V: VB*, DO*, BE* and HV*2. The secondary priority within the coarse POS V is: (12) VB* &gt; FiV* &gt; DO* &gt; BE* Furthermore, we define the semantic head and the syntactic head (Abney, 1991). (13) Semantic head is the head of a phrase according to the semantic usage; but syntactic head is the head based on the grammatical relations. Both the syntactic head and the semantic head are useful in extracting noun phrases. For example, if the semantic head of a chunk is the noun and the syntactic one is the preposition, it would be a prepositional phrase. Therefore, it can be connected to the previous noun chunk to form a new noun phrase. In some case, we will find some chunks contain only one word, called oneword chunks. They maybe contain a conjunction, e.g., that. Therefore, the synt</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Abney, Steven (1991), &amp;quot;Parsing by Chunks,&amp;quot; in Principle-Based Parsing, Berwick, Abney and Tenny (Eds.), Kluwer Academic Publishers, pp. 257-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Didier Bourigault</author>
</authors>
<title>Surface Grammatical Analysis for the Extraction of Terminological Noun Phrases,&amp;quot;</title>
<date>1992</date>
<booktitle>Proceedings of the 15th International Conference on Computational Linguistics, COLING-92, Vol. III,</booktitle>
<pages>977--981</pages>
<location>Nantes, France,</location>
<contexts>
<context position="4077" citStr="Bourigault (1992)" startWordPosition="638" endWordPosition="639"> of speech tagger and a simple noun phrase extractor. His noun phrase extractor brackets the noun phrases of input tagged texts according to two probability matrices: one is starting noun phrase matrix; the other is ending noun phrase matrix. The methodology is a simple version of Garside and Leech&apos;s probabilistic parser (1985). Church lists a sample text in the Appendix of his paper to show the performance of his work. It demonstrates only 5 out of 248 noun phrases are omitted. Because the tested text is too small to assess the results, the experiment for large volume of texts is needed. 234 Bourigault (1992) reports a tool, LEX7&apos;ER, for extracting terminologies from texts. LEXTER triggers two-stage processing: 1) analysis (by identification of frontiers), which extracts the maximal-length noun phrase: 2) parsing (the maximal-length noun phrases), which, furthermore, acquires the terminology embedded in the noun phrases. Bourigault declares the LEXTER extracts 95% maximal-length noun phrases, that is, 43500 out of 46000 from test corpus. The result is validated by an expert. However, the precision is not reported in the Boruigault&apos;s paper. Voutilainen (1993) announces NProol for acquisition of max</context>
</contexts>
<marker>Bourigault, 1992</marker>
<rawString>Bourigault, Didier (1992), &amp;quot;Surface Grammatical Analysis for the Extraction of Terminological Noun Phrases,&amp;quot; Proceedings of the 15th International Conference on Computational Linguistics, COLING-92, Vol. III, Nantes, France, pp. 977-981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text,&amp;quot;</title>
<date>1988</date>
<booktitle>Proceedings of Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="3444" citStr="Church (1988)" startWordPosition="531" endWordPosition="532">ic head and semantic head, and a finite state mechanism with only 8 states. Section 2 will give a brief review of the works for the acquisition of noun phrases. Section 3 will describe the language model for chunker. Section 4 will specify how to apply linguistic knowledge to assigning heads to each chunk. Section 5 will list the experimental results of chunker. Following Section 5, Section 6 will give the performance of our work on the retrieval of noun phrases. The possible extensions of the proposed work will be discussed in Section 7. Section 8 will conclude the remarks. 2. Previous Works Church (1988) proposes a part of speech tagger and a simple noun phrase extractor. His noun phrase extractor brackets the noun phrases of input tagged texts according to two probability matrices: one is starting noun phrase matrix; the other is ending noun phrase matrix. The methodology is a simple version of Garside and Leech&apos;s probabilistic parser (1985). Church lists a sample text in the Appendix of his paper to show the performance of his work. It demonstrates only 5 out of 248 noun phrases are omitted. Because the tested text is too small to assess the results, the experiment for large volume of texts</context>
<context position="25027" citStr="Church (1988)" startWordPosition="4241" endWordPosition="4242">d extent, we will not extract the maximal noun phrases. In our work, the probabilistic chunker decides the implicit boundaries between words and the NPTRACTOR connects the adjacent noun chunks. When a noun chunk is followed by a preposition chunk, we do not connect the two chunks except the preposition chunk is led by &amp;quot;of&apos; preposition. Comparing with other works, our results are evaluated by a parsed corpus automatically and show the high precision. Although we do not point out the exact recall, we provide estimated values. The testing scale is large enough (about 150,000 words). In contrast, Church (1988) tests a text and extracts the simple noun phrases only. Bourigault&apos;s work (1992) is evaluated manually, and dose not report the precision. Hence, the real performance is not known. The work executed by Voutilainen (1993) is more complex than our work. The input text first is morphologizied, then parsed by constraint grammar, analyzed by two different noun phrases grammar and finally extracted by the occurrences. Like other works, Voutilainen&apos;s work is also evaluated manually. In this paper, we propose a language model to chunk texts. The simple but effective chunker could be seen as a linear </context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, Kenneth (1988), &amp;quot;A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text,&amp;quot; Proceedings of Second Conference on Applied Natural Language Processing, pp. 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Francis</author>
<author>H Kucera</author>
</authors>
<title>Manual of Information to Accompany a Standard Sample of Presentday Edited American English, for Use with Digital Computers,</title>
<date>1979</date>
<editor>Providence, R. I., U.S.A., original ed.</editor>
<institution>Department of Linguistics, Brown University,</institution>
<contexts>
<context position="12206" citStr="Francis and Kucera, 1979" startWordPosition="2033" endWordPosition="2036">s; ,1* denotes adjectives; A* denotes quantifiers. qualifiers and determiners; IN denotes prepositions; CD* denotes cardinals; OD* denotes ordinals, and NR* denotes adverbial nouns. Asterisk * denotes a wildcard. For convenience, some constraints, such as syntactic and semantic head checking, are not shown in Figure 2. 5. First Stage of Experiments Following the procedures depicted in Figure 1, we should train a chunker firstly. This is done by using the SUSANNE Corpus (Sampson, 1993; Sampson, 1994) as the training texts. The SUSANNE Corpus is a modified and condensed version of Brown Corpus (Francis and Kucera, 1979). It only contains the 1/10 of Brown Corpus, but involves more information than Brown Corpus. The Corpus consists of four kinds of texts: 1) A: press reportage; 2) G: belles letters, biography, memoirs; 3) J: learned writing; and 4) N: adventure and Western fiction. The Categories of A, G. J and N are named from respective categories of the Brown Corpus. Each Category consists of 16 files and each file contains about 2000 words. The following shows a snapshot of SUSANNE Corpus. G010010a - YB &lt;minbrk&gt; [Oh.Oh] 001:0010b - JJ NORTHERN northern [0[S[Np:s. G01:0010c - NN2 liberals liberal .Nps] (10</context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>Francis, N. and Kucera, H. (1979), Manual of Information to Accompany a Standard Sample of Presentday Edited American English, for Use with Digital Computers, Department of Linguistics, Brown University, Providence, R. I., U.S.A., original ed. 1964, revised 1971, revised and augmented 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Garside</author>
<author>Geoffrey Leech</author>
</authors>
<title>A Probabilistic Parser,&amp;quot;</title>
<date>1985</date>
<booktitle>Proceedings of Second Conference of the European Chapter of the ACL.</booktitle>
<pages>166--170</pages>
<marker>Garside, Leech, 1985</marker>
<rawString>Garside, Roger and Leech, Geoffrey (1985), &amp;quot;A Probabilistic Parser,&amp;quot; Proceedings of Second Conference of the European Chapter of the ACL. pp. 166-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun Classification from Predicate-Argument Structures,&amp;quot;</title>
<date>1990</date>
<booktitle>Proceedings of 28th Annual Meeting of ACL,</booktitle>
<pages>268--275</pages>
<contexts>
<context position="1931" citStr="Hindle, 1990" startWordPosition="284" endWordPosition="285"> the texts, we will understand the texts to some extent. This consideration is also captured by theories of discourse analysis, such as Discourse Representation Theory (Kamp, 1981). Traditionally, to make out the noun phrases in a text means to parse the text and to resolve the attachment relations among the constituents. However, parsing the text completely is very difficult, since various ambiguities cannot be resolved solely by syntactic or semantic information. Do we really need to fully parse the texts in every application? Some researchers apply shallow or partial parsers (Smadja, 1991; Hindle, 1990) to acquiring specific patterns from texts. These tell us that it is not necessary to completely parse the texts for some applications. This paper will propose a probabilistic partial parser and incorporate linguistic knowledge to extract noun phrases. The partial parser is motivated by an intuition (Abney, 1991): (1) When we read a sentence, we read it chunk by chunk. Abney uses two level grammar rules to implement the parser through pure LR parsing technique. The first level grammar rule takes care of the chunking process. The second level grammar rule tackles the attachment problems among c</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle, Donald (1990), &amp;quot;Noun Classification from Predicate-Argument Structures,&amp;quot; Proceedings of 28th Annual Meeting of ACL, pp. 268-275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural Ambiguity and Lexical Relations,&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>103--120</pages>
<contexts>
<context position="24398" citStr="Hindle and Rooth, 1993" startWordPosition="4136" endWordPosition="4139">by relative clauses. 8. Concluding Remarks The difficulty of this work is how to extract the real maximal noun phrases. If we cannot decide the prepositional phrase &amp;quot;over a husband eyes&amp;quot; is licensed by the verb &amp;quot;pull&amp;quot;, we will not know &amp;quot;the wool&amp;quot; and &amp;quot;a husband eyes&amp;quot; are two noun phrases or form a noun pharse combined by the preposition &amp;quot;over&amp;quot;. (18) to pull the wool over a husband eyes to sell the books of my uncle In contrast, the noun phrase &amp;quot;the books of my uncle&amp;quot; is so called maximal noun phrase in current context. As the result, we conclude that if we do not resolve PPattachment problem (Hindle and Rooth, 1993), to the expected extent, we will not extract the maximal noun phrases. In our work, the probabilistic chunker decides the implicit boundaries between words and the NPTRACTOR connects the adjacent noun chunks. When a noun chunk is followed by a preposition chunk, we do not connect the two chunks except the preposition chunk is led by &amp;quot;of&apos; preposition. Comparing with other works, our results are evaluated by a parsed corpus automatically and show the high precision. Although we do not point out the exact recall, we provide estimated values. The testing scale is large enough (about 150,000 words</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Hindle, Donald and Rooth, Mats (1993), &amp;quot;Structural Ambiguity and Lexical Relations,&amp;quot; Computational Linguistics, 19(1), pp. 103-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
</authors>
<title>Anaphora in Natural Language Understanding: a Survey,</title>
<date>1981</date>
<journal>Lecture Notes</journal>
<volume>119</volume>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="22306" citStr="Hirst, 1981" startWordPosition="3796" endWordPosition="3797"> recall. These numbers are listed in Table 9 and the corresponding recalls are also shown. Table 9. The limitation of Values for Recall ANP CNP I Recall A 7873 7660 0.97 G 7199 6943 0.96 J 6278 5958 0.95 N 8793 8559 0.97 Av. 30143 29120 0.96 The automatic validation of the experimenta results gives us an estimated recall. Appendix provides a sample text and the extracted noun phrases. Interested readers could examine the sample text and calculate recall and precision for a comparison. 7. Applications Identification of noun phrases in texts is useful for many applications. Anaphora resolution (Hirst, 1981) is to resolve the relationship of the noun phrases, namely, what the antecedent of a noun phrase is. The extracted noun phrases can form the set of possible candidates (or universal in the terminology of discourse representation theory). For acquisition of verb subcategorization frames, to bracket the noun phrases in the texts is indispensable. It can help us to find the boundary of the subject, the object and the prepositional phrase. We would use the acquired noun phrases for an application of adjective grouping. The extracted noun phrases may contain adjectives which pre-modify the head no</context>
</contexts>
<marker>Hirst, 1981</marker>
<rawString>Hirst, G. (1981), Anaphora in Natural Language Understanding: a Survey, Lecture Notes 119. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stig Johansson</author>
</authors>
<title>The Tagged LOB Corpus: Users&apos; Manual, Bergen: Norwegian Computing Centre for the Humanities.</title>
<date>1986</date>
<contexts>
<context position="9033" citStr="Johansson, 1986" startWordPosition="1496" endWordPosition="1497">es X X&apos; X&amp;quot; N N&apos; NP V V&apos; VP A A&apos; AP P P&apos; PP INFL S (I&apos;) S (IP) Table 1 defines five different phrasal structures and the hierarchical structures. The heads of these phrasal structures are the first level of X&apos;-Equivalences, that is, X. The other grammatical constituents function as the specifiers or modifiers, that is, they are accompanying words not core words. Following this line, we define the primary priority of POS listed in Table 1. (10) Primary POS priority&apos; : V&gt;N&gt; A&gt;P In order to extract the exact head, we further define Secondary POS priority among the 134 POSes defined in LOB corpus (Johansson, 1986). (11) Secondary POS priority is a linear precedence relationship within the primary priorities for coarse POSes 1 We do not consider the INFL. since our model will not touch on this structure. For example, LOB corpus defines four kinds of verbial words under the coarse POS V: VB*, DO*, BE* and HV*2. The secondary priority within the coarse POS V is: (12) VB* &gt; FiV* &gt; DO* &gt; BE* Furthermore, we define the semantic head and the syntactic head (Abney, 1991). (13) Semantic head is the head of a phrase according to the semantic usage; but syntactic head is the head based on the grammatical relation</context>
</contexts>
<marker>Johansson, 1986</marker>
<rawString>Johansson, Stig (1986), The Tagged LOB Corpus: Users&apos; Manual, Bergen: Norwegian Computing Centre for the Humanities.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H</author>
</authors>
<title>A Theory of Truth and Semantic Representation,&amp;quot; Formal Methods in the</title>
<date>1981</date>
<journal>Study of Language,</journal>
<volume>1</volume>
<institution>Mathema-tische Centrum.</institution>
<marker>H, 1981</marker>
<rawString>Kamp. H. (1981), &amp;quot;A Theory of Truth and Semantic Representation,&amp;quot; Formal Methods in the Study of Language, Vol. 1, (J. Groenendijk, T. Janssen, and M. Stokhof Eds.), Mathema-tische Centrum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Developments in Automatic Text Retrieval,&amp;quot;</title>
<date>1991</date>
<journal>Science,</journal>
<volume>253</volume>
<pages>974--979</pages>
<contexts>
<context position="23302" citStr="Salton, 1991" startWordPosition="3957" endWordPosition="3958"> of the subject, the object and the prepositional phrase. We would use the acquired noun phrases for an application of adjective grouping. The extracted noun phrases may contain adjectives which pre-modify the head noun. We then utilize the similarity of head nouns to group the adjectives. In addition, we may give the head noun a semantic tag, such as Roget&apos;s Thesaurus provides, and then analyze the adjectives. To automatically produce the index of a book, 239 we would extract the noun phrases contained in the book, calculate the inverse document frequency (IDF) and their term frequency (TF) (Salton, 1991), and screen out the implausible terms. These applications also have impacts on identifying noun phrases. For applications like anaphora resolution and acquisition of verb subcategorization frames, the maximal noun phrases are not suitable. For applications like grouping adjectives and automatic book indexing, some kinds of maximal noun phrases, such as noun phrases postmodified by &amp;quot;of&apos; prepositional phrases, are suitable: but some are not, e.g., noun phrases modified by relative clauses. 8. Concluding Remarks The difficulty of this work is how to extract the real maximal noun phrases. If we c</context>
</contexts>
<marker>Salton, 1991</marker>
<rawString>Salton, G. (1991), &amp;quot;Developments in Automatic Text Retrieval,&amp;quot; Science, Vol. 253, pp. 974-979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
</authors>
<title>The SUSANNE Corpus,&amp;quot;</title>
<date>1993</date>
<journal>ICAME Journal,</journal>
<volume>17</volume>
<pages>125--127</pages>
<contexts>
<context position="7717" citStr="Sampson, 1993" startWordPosition="1270" endWordPosition="1271">p (ck ) k=1 = argmaxE [log( P, (c, Ick_, )) + log(P, ))] = argmaxELS(Fjci Ici_3)+S(P, (ck The final language model is to find a chunk sequence C*, which satisfies the expression (8). Dynamic programming shown in (9) is used to find the best chunk sequence. The score[i] denotes the score of position i. The words between position prep] and position i form the best chunk from the viewpoint of position i. The dscore(cf) is the score of the probability 235 P(c1) and the cscore(clici-i) is the score of the probability /3(cIlcr-i). These scores are collected from the training corpus. SUSANNE corpus (Sampson, 1993; Sampson, 1994). The details will be touched on in Section 5. (9) Algorithm input : word sequence wi, w2, wn, and the corresponding POS sequence ti, t2, tn output : a sequence of chunks ci, c2, 1. score[0] = 0; pre[0] = 0; 2. for (i = i&lt;n+1; i++) do 3 and 4; 3. j*=maxarg(score[pre[j]l+dscore(ci)+cscore(cjicj-i)); where ci = ti; cj-i = tpreW+ 1, , tj; 4. score[i]=score[prep*]]+dscore(ci*)+cseore(ci*Ici*-1); pre(i] = j*: 5. for (i=n; i&gt;0; i=pre[i]) do output the word Wpre[i]+1, Wi to form a chunk; 4. Linguistic Knowledge In order to assign a head to each chunk, we first define priorities of POS</context>
<context position="12069" citStr="Sampson, 1993" startWordPosition="2013" endWordPosition="2014">The Finite State Machine for Noun Phrases The symbols in Figure 2 are tags of LOB corpus. N* denotes nous; P* denotes pronouns; ,1* denotes adjectives; A* denotes quantifiers. qualifiers and determiners; IN denotes prepositions; CD* denotes cardinals; OD* denotes ordinals, and NR* denotes adverbial nouns. Asterisk * denotes a wildcard. For convenience, some constraints, such as syntactic and semantic head checking, are not shown in Figure 2. 5. First Stage of Experiments Following the procedures depicted in Figure 1, we should train a chunker firstly. This is done by using the SUSANNE Corpus (Sampson, 1993; Sampson, 1994) as the training texts. The SUSANNE Corpus is a modified and condensed version of Brown Corpus (Francis and Kucera, 1979). It only contains the 1/10 of Brown Corpus, but involves more information than Brown Corpus. The Corpus consists of four kinds of texts: 1) A: press reportage; 2) G: belles letters, biography, memoirs; 3) J: learned writing; and 4) N: adventure and Western fiction. The Categories of A, G. J and N are named from respective categories of the Brown Corpus. Each Category consists of 16 files and each file contains about 2000 words. The following shows a snapshot</context>
</contexts>
<marker>Sampson, 1993</marker>
<rawString>Sampson, Geoffrey (1993), &amp;quot;The SUSANNE Corpus,&amp;quot; ICAME Journal, No. 17, pp. 125-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey</author>
</authors>
<title>English for the Computer,</title>
<date>1994</date>
<publisher>University Press.</publisher>
<location>Oxford</location>
<marker>Geoffrey, 1994</marker>
<rawString>Sampson. Geoffrey (1994), English for the Computer, Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Sells</author>
</authors>
<date>1985</date>
<booktitle>Lectures on Contemporary Syntactic Theories, Lecture Notes, No. 3, CSLI.</booktitle>
<contexts>
<context position="8344" citStr="Sells, 1985" startWordPosition="1376" endWordPosition="1377">). The details will be touched on in Section 5. (9) Algorithm input : word sequence wi, w2, wn, and the corresponding POS sequence ti, t2, tn output : a sequence of chunks ci, c2, 1. score[0] = 0; pre[0] = 0; 2. for (i = i&lt;n+1; i++) do 3 and 4; 3. j*=maxarg(score[pre[j]l+dscore(ci)+cscore(cjicj-i)); where ci = ti; cj-i = tpreW+ 1, , tj; 4. score[i]=score[prep*]]+dscore(ci*)+cseore(ci*Ici*-1); pre(i] = j*: 5. for (i=n; i&gt;0; i=pre[i]) do output the word Wpre[i]+1, Wi to form a chunk; 4. Linguistic Knowledge In order to assign a head to each chunk, we first define priorities of POSes. X&apos;-theory (Sells, 1985) has defined the X&apos;-equivalences shown as Table 1. Table 1. X&apos;-Equivalences X X&apos; X&amp;quot; N N&apos; NP V V&apos; VP A A&apos; AP P P&apos; PP INFL S (I&apos;) S (IP) Table 1 defines five different phrasal structures and the hierarchical structures. The heads of these phrasal structures are the first level of X&apos;-Equivalences, that is, X. The other grammatical constituents function as the specifiers or modifiers, that is, they are accompanying words not core words. Following this line, we define the primary priority of POS listed in Table 1. (10) Primary POS priority&apos; : V&gt;N&gt; A&gt;P In order to extract the exact head, we further </context>
</contexts>
<marker>Sells, 1985</marker>
<rawString>Sells, Peter (1985), Lectures on Contemporary Syntactic Theories, Lecture Notes, No. 3, CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>Extracting Collocations from Text, An Application: Language Generation,</title>
<date>1991</date>
<institution>Ph.D. Dissertation. Columbia University,</institution>
<contexts>
<context position="1916" citStr="Smadja, 1991" startWordPosition="282" endWordPosition="283">noun phases of the texts, we will understand the texts to some extent. This consideration is also captured by theories of discourse analysis, such as Discourse Representation Theory (Kamp, 1981). Traditionally, to make out the noun phrases in a text means to parse the text and to resolve the attachment relations among the constituents. However, parsing the text completely is very difficult, since various ambiguities cannot be resolved solely by syntactic or semantic information. Do we really need to fully parse the texts in every application? Some researchers apply shallow or partial parsers (Smadja, 1991; Hindle, 1990) to acquiring specific patterns from texts. These tell us that it is not necessary to completely parse the texts for some applications. This paper will propose a probabilistic partial parser and incorporate linguistic knowledge to extract noun phrases. The partial parser is motivated by an intuition (Abney, 1991): (1) When we read a sentence, we read it chunk by chunk. Abney uses two level grammar rules to implement the parser through pure LR parsing technique. The first level grammar rule takes care of the chunking process. The second level grammar rule tackles the attachment p</context>
</contexts>
<marker>Smadja, 1991</marker>
<rawString>Smadja, Frank (1991), Extracting Collocations from Text, An Application: Language Generation, Ph.D. Dissertation. Columbia University, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E</author>
<author>C A Ferguson</author>
</authors>
<title>Talking to Children: Language Input and Acquisition, Cambridge.</title>
<date>1977</date>
<publisher>Cambridge University Press.</publisher>
<marker>E, Ferguson, 1977</marker>
<rawString>Snow. C.E. and Ferguson, C.A. (Eds.) (1977), Talking to Children: Language Input and Acquisition, Cambridge. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
</authors>
<title>NPtool, a Detector of English Noun Phrases.&amp;quot;</title>
<date>1993</date>
<booktitle>Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives,</booktitle>
<pages>48--57</pages>
<institution>State University,</institution>
<location>Ohio</location>
<contexts>
<context position="4637" citStr="Voutilainen (1993)" startWordPosition="716" endWordPosition="717">for large volume of texts is needed. 234 Bourigault (1992) reports a tool, LEX7&apos;ER, for extracting terminologies from texts. LEXTER triggers two-stage processing: 1) analysis (by identification of frontiers), which extracts the maximal-length noun phrase: 2) parsing (the maximal-length noun phrases), which, furthermore, acquires the terminology embedded in the noun phrases. Bourigault declares the LEXTER extracts 95% maximal-length noun phrases, that is, 43500 out of 46000 from test corpus. The result is validated by an expert. However, the precision is not reported in the Boruigault&apos;s paper. Voutilainen (1993) announces NProol for acquisition of maximal-length noun phrases. NPtool applies two finite state mechanisms (one is NP-hostile; the other is NP-friendly) to the task. The two mechanisms produce two NP sets and any NP candidate with at least one occurrence in both sets will be labeled as the &amp;quot;ok&amp;quot; NP. The reported recall is 98.5-100% and the precision is 95- 98% validated manually by some 20000 words. But from the sample text listed in Appendix of his paper, the recall is about 85% and we can find some inconsistencies among these extracted noun phrases. 3. Language Model Parsing can be viewed a</context>
<context position="25248" citStr="Voutilainen (1993)" startWordPosition="4276" endWordPosition="4277">ollowed by a preposition chunk, we do not connect the two chunks except the preposition chunk is led by &amp;quot;of&apos; preposition. Comparing with other works, our results are evaluated by a parsed corpus automatically and show the high precision. Although we do not point out the exact recall, we provide estimated values. The testing scale is large enough (about 150,000 words). In contrast, Church (1988) tests a text and extracts the simple noun phrases only. Bourigault&apos;s work (1992) is evaluated manually, and dose not report the precision. Hence, the real performance is not known. The work executed by Voutilainen (1993) is more complex than our work. The input text first is morphologizied, then parsed by constraint grammar, analyzed by two different noun phrases grammar and finally extracted by the occurrences. Like other works, Voutilainen&apos;s work is also evaluated manually. In this paper, we propose a language model to chunk texts. The simple but effective chunker could be seen as a linear structure parser, and could be applied to many applications. A method is presented to extract the noun phrases. Most importantly, the relations of maximal noun phrases, minimal noun phrases, ordinary noun phrases and appl</context>
</contexts>
<marker>Voutilainen, 1993</marker>
<rawString>Voutilainen, Atro (1993), &amp;quot;NPtool, a Detector of English Noun Phrases.&amp;quot; Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, Ohio State University, Columbus, Ohio, USA, pp. 48-57.</rawString>
</citation>
<citation valid="false">
<authors>
<author>I</author>
</authors>
<title>Foo _QL many_AP people_NNS I think</title>
<journal>JJ shops_NNS ] in_IN [ New_JJ Orleans_NP ] Vieux &amp;AV Carre_&amp;FW +„ [ the_ATI old JJ French JJ Quarter NPL I [ The_ATI arrangement NN I [ I PPlA I had_HVD with_IN [</journal>
<booktitle>VB that_CS [ the ATI primary JJ purpose NN of IN a_AT higher JJR education_NN ] is BEZ to TO help_ VB I you_PP2 make VB [ a_AT</booktitle>
<marker>I, </marker>
<rawString>I &amp;quot;Foo _QL many_AP people_NNS I think VB that_CS [ the ATI primary JJ purpose NN of IN a_AT higher JJR education_NN ] is BEZ to TO help_ VB I you_PP2 make VB [ a_AT living_NN ] +;_; thi; DT is BEZ not_XNOT so RB +„ for_CS [ education_NN I offers_VBZ I all ABN kinds NNS of IN dividends_NNS ] +,_, including_IN how WRB to_TO pull_VB [ the ATI wool NN ] over [ a_AT husband _NN eyes NNS I while_ CS [ you_PP2 ] are_BER having_I-VG I an AT affair NN I with_IN [ his_PP$ wife NN I . If CS [ it PP3 ] were BED not_XNOT for IN [ an AT old JJ professor_NPT I who WPR made VBD [ me_PP 10 ] read VB [ th-e_ATI classics_NN ] 1 PPI A 1 would_MD have_HV been—BEN stymied_VBN on IN what_ WDT to_TO do DO +„ and_CC now_RN [ I PP-1A ] understand _VB why_WRB [ they_PP3AS ] are_BER [ classics NN I+; ; those_ DTS who WPR wrote VBD [ them_PP3OS I knew VBD I people_NNS 1 and_CC what WDT made_VBD [ people_NNS I tick_VB [ I PP1A ] worked VBD for_IN [ my_PP$ Uncle_NPT ] ( _( [ +an_AT Uncle NPT by_IN marriage_NN I so_RB [ you PP2 I will MD not XNOT— think VB this_DT has_HVZ [ a_AT mild JJ undercurrent NN of_IN incest NN +)_) who WPR ran VBD I one_CDI (:)--f IN those DTS antique JJ shops_NNS ] in_IN [ New_JJ Orleans_NP ] Vieux &amp;AV Carre_&amp;FW +„ [ the_ATI old JJ French JJ Quarter NPL I [ The_ATI arrangement NN I [ I PPlA I had_HVD with_IN [ himPP30 ] was BEDZ to_TO work VB [ four CD hours NRS I [ a_AT day_NR I . . [ The ATI rest_NN of IN the ATI time__NR It I_PPI A I devoted_VBD to_IN painting_VBG or CC to IN those DTS [ other JJB activities_NNS I [ a_AT young_JJ and_CC— healthy_ll man_NN just_RB out_IN of IN [ college_NN ] finds_BZ interesting_JJ . I I_PPI A I had_HVD I a_AT one-room JJ studio_ NN 1 which_WDTR overlooked_VBD I an_AT ancient JJ courtyard_ NN I filled_VBN with_IN [ flowers NNS and_CC plants_ NNS ] blooming_VBG everlastingly_RB in_IN I the ATI southern JJ sun_NN I ._. I I_PPI A ] had_HVD come_VBN to_ IN [ New JJ</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>