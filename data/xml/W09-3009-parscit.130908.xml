<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.254482">
<title confidence="0.998772">
Explorations in Automatic Image Annotation using Textual Features
</title>
<author confidence="0.991129">
Chee Wee Leong
</author>
<affiliation confidence="0.9941255">
Computer Science &amp; Engineering
University of North Texas
</affiliation>
<email confidence="0.991153">
cheeweeleong@my.unt.edu
</email>
<author confidence="0.990032">
Rada Mihalcea
</author>
<affiliation confidence="0.995107">
Computer Science &amp; Engineering
University of North Texas
</affiliation>
<email confidence="0.996007">
rada@cs.unt.edu
</email>
<sectionHeader confidence="0.993845" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999244">
In this paper, we report our work on
automatic image annotation by combining
several textual features drawn from the
text surrounding the image. Evaluation of
our system is performed on a dataset of
images and texts collected from the web.
We report our findings through compar-
ative evaluation with two gold standard
collections of manual annotations on the
same dataset.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983538461539">
Despite the usefulness of images in expressing
ideas, machine understanding of the meaning of
an image remains a daunting task for comput-
ers, as the interplay between the different visual
components of an image does not conform to any
fixed pattern that allows for formal reasoning of
its semantics. Often, the machine interpretation of
the concepts present in an image, known as auto-
matic image annotation, can only be inferred by
its accompanying text or co-occurrence informa-
tion drawn from a large corpus of texts and im-
ages (Li and Wang, 2008; Barnard and Forsyth,
2001). Not surprisingly, humans have the innate
ability to perform this task reliably, but given a
large database of images, manual annotation is
both labor-intensive and time-consuming.
Our work centers around the question : Pro-
vided an image with its associated text, can we
use the text to reliably extract keywords that rel-
evantly describe the image ? Note that we are not
concerned with the generation of keywords for an
image, but rather their extraction from the related
text. Our goal eventually is to automate this task
by leveraging on texts which are naturally occur-
ring with images. In all our experiments, we only
consider the use of nouns as annotation keywords.
</bodyText>
<sectionHeader confidence="0.999641" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999968464285715">
Although automatic image annotation is a popu-
lar task in computer vision and image processing,
there are only a few efforts that leverage on the
multitude of resources available for natural lan-
guage processing to derive robust linguistic based
image annotation models. Most of the work has
posed the annotation task as a classification prob-
lem, such as (Li and Wang, 2008), where images
are annotated using semantic labels associated to
a semantic class.
The most recent work on image annotation us-
ing linguistic features (Feng and Lapata, 2008)
involves implementing an extended version of
the continuous relevance model that is proposed
in (Jeon et al., 2003). The basic idea underlying
their work is to perform annotation of a test im-
age by using keywords shared by similar training
images. Evaluation of their system performance
is based on a dataset collected from the news do-
main (BBC). Unlike them, in this paper, we at-
tempt to perform image annotation on datasets
from unrestricted domains. We are also interested
in extending the work pursued in (Deschacht and
Moens, 2007), where visualness and salience are
proposed as important textual features for discov-
ering named entities present in an image, by ex-
tracting other textual features that can further im-
prove existing image annotation models.
</bodyText>
<sectionHeader confidence="0.98165" genericHeader="method">
3 Data Sets
</sectionHeader>
<bodyText confidence="0.999879142857143">
We use 180 images collected from the Web, from
pages that have a single image within a specified
size range (width and height of 275 to 1000 pix-
els). 110 images are used for development, while
the remaining 70 are used for test. We create two
different gold standards. The first, termed as Intu-
itive annotation standard (G5intuition), presents a
user with the image in the absence of its associated
text, and asks the user for the 5 most relevant anno-
tations. The second, called Contextual annotation
standard (G5context), provides the user with a list
of candidates1 for annotation, with the user free to
choose any of the candidates deemed relevant to
describe the image. The user, however, is not con-
</bodyText>
<footnote confidence="0.697221">
1Union of candidates proposed by all systems participat-
ing in the evaluation, including the baseline system
</footnote>
<page confidence="0.989766">
56
</page>
<note confidence="0.95769">
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 56–59,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999650523809524">
strained to choose any candidate word, nor is she
obligated to choose a specified number of candi-
dates. For each image I in the evaluation set, we
invited five users to perform the annotation task
per gold standard. The agreement is 7.78% for
GSintuition and 22.27% for GScontext, where we
consider an annotation that is proposed by three or
more users as one that is being agreed upon. The
union of their inputs forms the set GSintuition(I)
and GScontext(I) respectively. We do not consider
image captions for use as a gold standard here due
to their absence in many of the images – a ran-
dom sampling of 15 images reveals that 7 of them
lack captions. Contrary to their use as a proxy for
annotation keywords in (Feng and Lapata, 2008;
Deschacht and Moens, 2007), where evaluation is
performed on datasets gleaned from authoritative
news websites, most captions in our dataset are
not guaranteed to be noise free. However, they are
used as part of the text for generating annotations
where they exist.
</bodyText>
<sectionHeader confidence="0.992569" genericHeader="method">
4 Automatic Image Annotation
</sectionHeader>
<bodyText confidence="0.999992777777778">
We approach the task of automatic image anno-
tation using four methods. Due to the orthogo-
nal nature in their search for keywords, the out-
put for each method is generated separately and
later combined in an unsupervised setting. How-
ever, all four methods perform their discrimination
of words by drawing information exclusively from
the text associated to the image, using no image
visual features in the process.
</bodyText>
<subsectionHeader confidence="0.991467">
4.1 Semantic Cloud (Sem)
</subsectionHeader>
<bodyText confidence="0.999992884615384">
Every text describes at least one topic that can be
semantically represented by a collection of words.
Intuitively, there exists several “clouds” of seman-
tically similar words that form several, possibly
overlapping, sets of topics. Our task is to se-
lect the dominant topic put forward in the text,
with the assumption that such a topic is being
represented by the largest set of words. We use
an adapted version of the K-means clustering ap-
proach, which attempts to find natural “clusters”
of words in the text by grouping words with a com-
mon centroid. Each centroid is the semantic cen-
ter of the group of words and the distance between
each centroid and the words are approximated by
ESA (Gabrilovich and Markovitch, 2007). Fur-
ther, we perform our experiments with the follow-
ing assumptions : (1) To maximize recall, we as-
sume that there are only two topics in every text.
(2) Every word or collocation in the text must be
classified under one of these two topics, but not
both. In cases, where there is a tie, the classi-
fication is chosen randomly. For each dominant
cluster extracted, we rank the words in decreasing
order of their ESA distance to the centroid. To-
gether, they represent the gist of the topic and are
used as a set of candidates for labeling the image.
</bodyText>
<subsectionHeader confidence="0.970929">
4.2 Lexical Distance (Lex)
</subsectionHeader>
<bodyText confidence="0.999798230769231">
Words that are lexically close to the picture in the
document are generally well-suited for annotat-
ing the image. The assumption is drawn from the
observation that the caption of an image is usu-
ally located close to the image itself. For images
without captions, we consider words surrounding
the image as possible candidates for annotation.
Whenever a word appears multiple times within
the text, its occurrence closest to the image is used
to calculate the lexical distance. To discriminate
against general words, we weigh the Lexical Dis-
tance Score (LDS) for each word by its tf * idf
score as in the equation shown below :
</bodyText>
<equation confidence="0.997462">
LDS(Wi) = t f* idf(Wi)/LS(Wi) (1)
</equation>
<bodyText confidence="0.999728">
where LS(Wi) is the minimum lexical distance of
Wi to the image, and idf is calculated using counts
from the British National Corpus.
</bodyText>
<subsectionHeader confidence="0.996974">
4.3 Saliency (Sal)
</subsectionHeader>
<bodyText confidence="0.999847">
To our knowledge, all word similarity metrics pro-
vide a symmetric score between a pair of words
w1 and w2 to indicate their semantic similarity.
Intuitively, this is not always the case. In psy-
cholinguistics terms, uttering w1 may bring into
mind w2, while the appearance of w2 without any
contextual clues may not associate with w1 at all.
Thus, the degree of similarity of w1 with respect
to w2 should be separated from that of w2 with
respect to w1. We use a directional measure of
similarity:
</bodyText>
<equation confidence="0.963316333333333">
Cij
DSim(wi, wj) = ∗ Sim(wi, wj) (2)
Ci
</equation>
<bodyText confidence="0.9999654375">
where Cij is the count of articles in Wikipedia
containing words wi and wj, Ci is the count of ar-
ticles containing words wi, and Sim(wi, wj) is the
cosine similarity of the ESA vectors representing
the two words. The directional weight (Cij/Ci)
amounts to the degree of association of wi with re-
spect to wj. Using the directional inferential sim-
ilarity scores as directed edges and distinct words
as vertices, we obtain a graph for each text. The
directed edges denotes the idea of “recommenda-
tion” where we say w1 recommends w2 if and only
if there is a directed edge from w1 to w2, with
the weight of the recommendation being the di-
rectional similarity score. By employing the graph
iteration algorithm proposed in (Mihalcea and Ta-
rau, 2004), we can compute the rank of a vertex in
</bodyText>
<page confidence="0.996666">
57
</page>
<bodyText confidence="0.9996918">
the entire graph. The output generated is a sorted
list of words in decreasing order of their ranks,
which serves as a list of candidates for annotating
the image. Note that the top-ranked word must in-
fer some or all of the words in the text.
</bodyText>
<tableCaption confidence="0.901485">
Table 1: An image annotation example
</tableCaption>
<bodyText confidence="0.996174566666667">
Sem symptoms, treatment, medical treat-
ment, medical care, sore throat, fluids,
cough, tonsils, strep throat, swab
Lex strep throat, cotton swab, lymph nodes,
rheumatic fever, swab, strep, fever, sore
throat, lab, scarlet fever
Sal strep, swab, nemours, teens, ginger ale,
grapefruit juice, sore, antibiotics, kids,
fever
Pic throat, runny nose, strep throat, sore
throat, hand washing, orange juice, 24
hours, medical care, beverages, lymph
nodes
Combined treatment, cough, tonsils, swab, fluids,
strep throat
Doc Title strep throat
tf *idf strep, throat, antibiotics, symptoms,
child, swab, fever, treatment, teens,
nemours
GScontext medical care, medical treatment, doc-
tor, cotton swab, treatment, tonsils, sore
throat, swab, throat, sore, sample, symp-
toms, throat, cough, medication, bacte-
ria, lab, scarlet fever, strep throat, teens,
culture, kids, child, streptococcus, doctor,
strep
GSintuition tongue, depressor, exam, eyes, cartoon,
doctor, health, child, tonsils, fingers, hair,
mouth, dentist, sample, cloth, curly, tip,
examine
</bodyText>
<subsectionHeader confidence="0.993097">
4.4 Picturable Cues (Pic)
</subsectionHeader>
<bodyText confidence="0.999971928571429">
Some words are more picturable than others. For
instance, it is easy to find a picture that describes
the word banana than another word paradigm.
Clearly, picturable words in the associated text of
an image are natural candidates for labeling it. Un-
like the work in (Deschacht and Moens, 2007),
we employ a corpus-based approach to compute
word to word similarity. We collect a list of 200
manually-annotated words2 that are deemed to be
picturable by humans. We use this list of words
as our set of seed words, 5seed. We then iterate a
bootstrapping process where each word in the text
is compared to every word in the set of seed words,
and any word having a maximum ESA score of
</bodyText>
<footnote confidence="0.95805">
2http://simple.wikipedia.org/wiki/Wikipedia:
Basic English picture wordlist
</footnote>
<bodyText confidence="0.999965">
greater than 0.95 is added to 5seed. Similarly, the
maximum ESA score of each word over all 5seed
words is recorded. This is the picturability score
of the word.
</bodyText>
<sectionHeader confidence="0.975678" genericHeader="method">
5 Experiments and Evaluations
</sectionHeader>
<bodyText confidence="0.999926433333333">
We investigate the performance of each of the four
annotation methods individually, followed by a
combined approach using all of them. In the in-
dividual setting, we simply obtain the set of candi-
dates proposed by each method as possible anno-
tation keywords for the image. In the unsupervised
combined setting, only the labels proposed by all
individual methods are selected, and listed in re-
verse order of their combined rankings.
We allow each system to produce a re-ranked
list of top k words to be the final annotations for a
given image. A system can discretionary generate
less (but not more) than k words that is appropri-
ate to its confidence level. Similar to (Feng and
Lapata, 2008), we evaluate our systems using pre-
cision, recall and F-measure for k=10, k=15 and
k=20 words.
For comparison, we also implemented two
baselines systems: tf * idf and Doc Title, which
simply takes all the words in the title of the
web page and uses them as annotation labels for
the image. In the absence of a document title,
we use the first sentence in the document. The
results for G5intuition and G5context are tabu-
lated in Tables 2 and 3 respectively. We fur-
ther illustrate our results with an annotation ex-
ample (an image taken from a webpage discussing
strep throat among teens) in Table 1. Words in
bold matches G5context while those underlined
matches G5intuition.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.992119235294118">
As observed, the system implementing the Se-
mantic Cloud method significantly outperforms
the rest of the systems in terms of recall and F-
measure using the gold standard G5intuition. The
unsupervised combined system yields the high-
est precision at 16.26% (at k=10,15,20) but at
a low recall of 1.52%. Surprisingly, the base-
line system using tf *idf performs relatively well
across all the experiments using the gold stan-
dard G5intuition, outperforming two of our pro-
posed methods Salience (Sal) and Picturability
Cues (Pic) consistently for all k values. The other
baseline, Doc Title, records the highest precision
at 16.33% at k=10 with a low recall of 3.81%. For
k=15 and k=20, the F-measure scored 6.31 and
6.29 respectively, both lower than that scored by
tf * idf.
</bodyText>
<page confidence="0.999">
58
</page>
<tableCaption confidence="0.960799">
Table 2: Results for Automatic Image Annotation for GSintuition. In both Tables 2 and 3, statistically
significant results are marked with ∗(measured against Doc Title, p&lt;0.05, paired t-test), ×(measured
against tf*idf, p&lt;0.1, paired t-test), †(measured against tf*idf, p&lt;0.05, paired t-test).
</tableCaption>
<table confidence="0.9937703">
GSintuition
k=10 k=15 k=20
P R F P R F P R F
Sem 11.71 6.25* 8.15 11.31 8.91*&amp;quot; 9.97*† 10.36 9.45*&amp;quot; 9.88*†
Lex 9.00 4.80 6.26 7.33 5.86 6.51 7.14 7.62 7.37
Sal 4.57 2.43 3.17 6.28 5.03 5.59 6.38 6.78 6.57
Pic 7.14 3.81 4.97 6.09 4.87 5.41 5.64 6.02 5.82
Combined 16.26 1.52 2.78 16.26† 1.52 2.78 16.26† 1.52 2.78
Doc Title 16.33 3.81 6.18 15.56 3.96 6.31 15.33 3.96 6.29
tf *idf 9.71 5.18 6.76 8.28 6.63 7.36 7.14 7.62 7.37
</table>
<tableCaption confidence="0.949275">
Table 3: Results for Automatic Image Annotation for GScontext
</tableCaption>
<table confidence="0.9866652">
GScontext
k=10 k=15 k=20
P R F P R F P R F
Sem 71.57 26.20*† 38.36*† 68.00 37.34*† 48.21*† 64.56 47.17*† 54.51*†
Lex 61.00 22.23 32.59 58.95 32.37 41.79 56.92 41.68 48.12
Sal 46.42 16.99 24.88 51.14 28.08 36.25 54.59 39.80 46.04
Pic 51.71 21.12 29.99 56.85 31.22 40.31 56.35 41.26 47.64
Combined 75.60*† 4.86 9.13 75.60*† 4.86 9.13 75.60*† 4.86 9.13
Doc Title 32.67 5.23 9.02 32.33 5.64 9.60 32.15 5.70 9.68
tf *idf 55.85 20.44 29.93 54.19 29.75 38.41 49.07 35.93 41.48
</table>
<bodyText confidence="0.999330516129032">
When performing evaluations using the gold
standard GScontext, significantly higher precision,
recall and F-measure values are scored by all the
systems, including both baselines. This is perhaps
due to the availability of candidates that suggests
a form of cued recall, rather than free recall, as
is the case with GSintuitive. The user is able to
annotate an image with higher accuracy e.g. la-
belling a Chihuahua as a Chihuahua instead of a
dog. Again, the Semantic Cloud method contin-
ues to outperform all the other systems in terms of
recall and F-measure consistently for k=10, k=15
and k=20 words. A similar trend as observed us-
ing the gold standard of GSintuition is seen here,
where again our combined system favors precision
over recall at all values of k.
A possible explanation for the poor perfor-
mance of the Saliency method is perhaps due to
over-specific words that infer all other words in the
text, yet unknown to the knowledge of most hu-
man annotators. For instance, the word Mussolini,
referring to the dictator Benito Mussolini, was not
selected as an annotation for an image showing
scenes of World War II depicting the Axis troops,
though it suggests the concepts of war, World War
II and so on. The Pic method is also not perform-
ing as well as expected under the two gold anno-
tation standards, mainly due to the fact that it fo-
cuses on selecting picturable nouns but not nec-
essarily those that are semantically linked to the
image itself.
</bodyText>
<sectionHeader confidence="0.999683" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.99997025">
The use of the semantic cloud method to generate
automatic annotations is promising. Future work
will consider using additional semantic resources
such as ontological information and ency-
clopaedic knowledge to enhance existing models.
We are also interested to pursue human knowledge
modeling to account for the differences in annota-
tors in order create a more objective gold standard.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997506434782609">
Kobus Barnard and David Forsyth. 2001. Learning the se-
mantics of words and pictures. In Proceedings ofInterna-
tional Conference on Computer Vision.
Koen Deschacht and Marie-Francine Moens. 2007. Text
analysis for automatic image annotation. In Proceedings
of the Association for Computational Linguisticd.
Yansong Feng and Mirella Lapata. 2008. Automatic image
annotation using auxiliary text information. In Proceed-
ings of the Association for Computational Linguistics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Comput-
ing semantic relatedness using wikipedia-based explicit
semantic analysis. In International Joint Conferences on
Artificial Intelligence.
J Jeon, V Lavrenko, and R Manmatha. 2003. Automatic im-
age annotation and retrieval using cross-media relevance
models. In Proceedings of the ACM SIGIR Conference on
Research and Development in Information Retrieval.
Jia Li and James Wang. 2008. Real-time computerized an-
notation of pictures. In Proceedings ofInternational Con-
ference on Computer Vision.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing
order into texts. In in Proceedings of Empirical Methods
in Natural Language Processing.
</reference>
<page confidence="0.999236">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.939441">
<title confidence="0.998958">Explorations in Automatic Image Annotation using Textual Features</title>
<author confidence="0.993366">Chee Wee</author>
<affiliation confidence="0.999893">Computer Science &amp; University of North</affiliation>
<email confidence="0.998556">cheeweeleong@my.unt.edu</email>
<author confidence="0.962024">Rada</author>
<affiliation confidence="0.999921">Computer Science &amp; University of North</affiliation>
<email confidence="0.999642">rada@cs.unt.edu</email>
<abstract confidence="0.998708090909091">In this paper, we report our work on automatic image annotation by combining several textual features drawn from the text surrounding the image. Evaluation of our system is performed on a dataset of images and texts collected from the web. We report our findings through comparative evaluation with two gold standard collections of manual annotations on the same dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kobus Barnard</author>
<author>David Forsyth</author>
</authors>
<title>Learning the semantics of words and pictures.</title>
<date>2001</date>
<booktitle>In Proceedings ofInternational Conference on Computer Vision.</booktitle>
<contexts>
<context position="1220" citStr="Barnard and Forsyth, 2001" startWordPosition="186" endWordPosition="189">al annotations on the same dataset. 1 Introduction Despite the usefulness of images in expressing ideas, machine understanding of the meaning of an image remains a daunting task for computers, as the interplay between the different visual components of an image does not conform to any fixed pattern that allows for formal reasoning of its semantics. Often, the machine interpretation of the concepts present in an image, known as automatic image annotation, can only be inferred by its accompanying text or co-occurrence information drawn from a large corpus of texts and images (Li and Wang, 2008; Barnard and Forsyth, 2001). Not surprisingly, humans have the innate ability to perform this task reliably, but given a large database of images, manual annotation is both labor-intensive and time-consuming. Our work centers around the question : Provided an image with its associated text, can we use the text to reliably extract keywords that relevantly describe the image ? Note that we are not concerned with the generation of keywords for an image, but rather their extraction from the related text. Our goal eventually is to automate this task by leveraging on texts which are naturally occurring with images. In all our</context>
</contexts>
<marker>Barnard, Forsyth, 2001</marker>
<rawString>Kobus Barnard and David Forsyth. 2001. Learning the semantics of words and pictures. In Proceedings ofInternational Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Text analysis for automatic image annotation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguisticd.</booktitle>
<contexts>
<context position="2989" citStr="Deschacht and Moens, 2007" startWordPosition="479" endWordPosition="482">he most recent work on image annotation using linguistic features (Feng and Lapata, 2008) involves implementing an extended version of the continuous relevance model that is proposed in (Jeon et al., 2003). The basic idea underlying their work is to perform annotation of a test image by using keywords shared by similar training images. Evaluation of their system performance is based on a dataset collected from the news domain (BBC). Unlike them, in this paper, we attempt to perform image annotation on datasets from unrestricted domains. We are also interested in extending the work pursued in (Deschacht and Moens, 2007), where visualness and salience are proposed as important textual features for discovering named entities present in an image, by extracting other textual features that can further improve existing image annotation models. 3 Data Sets We use 180 images collected from the Web, from pages that have a single image within a specified size range (width and height of 275 to 1000 pixels). 110 images are used for development, while the remaining 70 are used for test. We create two different gold standards. The first, termed as Intuitive annotation standard (G5intuition), presents a user with the image</context>
<context position="4945" citStr="Deschacht and Moens, 2007" startWordPosition="810" endWordPosition="813">n set, we invited five users to perform the annotation task per gold standard. The agreement is 7.78% for GSintuition and 22.27% for GScontext, where we consider an annotation that is proposed by three or more users as one that is being agreed upon. The union of their inputs forms the set GSintuition(I) and GScontext(I) respectively. We do not consider image captions for use as a gold standard here due to their absence in many of the images – a random sampling of 15 images reveals that 7 of them lack captions. Contrary to their use as a proxy for annotation keywords in (Feng and Lapata, 2008; Deschacht and Moens, 2007), where evaluation is performed on datasets gleaned from authoritative news websites, most captions in our dataset are not guaranteed to be noise free. However, they are used as part of the text for generating annotations where they exist. 4 Automatic Image Annotation We approach the task of automatic image annotation using four methods. Due to the orthogonal nature in their search for keywords, the output for each method is generated separately and later combined in an unsupervised setting. However, all four methods perform their discrimination of words by drawing information exclusively from</context>
<context position="10728" citStr="Deschacht and Moens, 2007" startWordPosition="1780" endWordPosition="1783">at, sore, sample, symptoms, throat, cough, medication, bacteria, lab, scarlet fever, strep throat, teens, culture, kids, child, streptococcus, doctor, strep GSintuition tongue, depressor, exam, eyes, cartoon, doctor, health, child, tonsils, fingers, hair, mouth, dentist, sample, cloth, curly, tip, examine 4.4 Picturable Cues (Pic) Some words are more picturable than others. For instance, it is easy to find a picture that describes the word banana than another word paradigm. Clearly, picturable words in the associated text of an image are natural candidates for labeling it. Unlike the work in (Deschacht and Moens, 2007), we employ a corpus-based approach to compute word to word similarity. We collect a list of 200 manually-annotated words2 that are deemed to be picturable by humans. We use this list of words as our set of seed words, 5seed. We then iterate a bootstrapping process where each word in the text is compared to every word in the set of seed words, and any word having a maximum ESA score of 2http://simple.wikipedia.org/wiki/Wikipedia: Basic English picture wordlist greater than 0.95 is added to 5seed. Similarly, the maximum ESA score of each word over all 5seed words is recorded. This is the pictur</context>
</contexts>
<marker>Deschacht, Moens, 2007</marker>
<rawString>Koen Deschacht and Marie-Francine Moens. 2007. Text analysis for automatic image annotation. In Proceedings of the Association for Computational Linguisticd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic image annotation using auxiliary text information.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2452" citStr="Feng and Lapata, 2008" startWordPosition="390" endWordPosition="393">ts, we only consider the use of nouns as annotation keywords. 2 Related Work Although automatic image annotation is a popular task in computer vision and image processing, there are only a few efforts that leverage on the multitude of resources available for natural language processing to derive robust linguistic based image annotation models. Most of the work has posed the annotation task as a classification problem, such as (Li and Wang, 2008), where images are annotated using semantic labels associated to a semantic class. The most recent work on image annotation using linguistic features (Feng and Lapata, 2008) involves implementing an extended version of the continuous relevance model that is proposed in (Jeon et al., 2003). The basic idea underlying their work is to perform annotation of a test image by using keywords shared by similar training images. Evaluation of their system performance is based on a dataset collected from the news domain (BBC). Unlike them, in this paper, we attempt to perform image annotation on datasets from unrestricted domains. We are also interested in extending the work pursued in (Deschacht and Moens, 2007), where visualness and salience are proposed as important textu</context>
<context position="4917" citStr="Feng and Lapata, 2008" startWordPosition="806" endWordPosition="809">mage I in the evaluation set, we invited five users to perform the annotation task per gold standard. The agreement is 7.78% for GSintuition and 22.27% for GScontext, where we consider an annotation that is proposed by three or more users as one that is being agreed upon. The union of their inputs forms the set GSintuition(I) and GScontext(I) respectively. We do not consider image captions for use as a gold standard here due to their absence in many of the images – a random sampling of 15 images reveals that 7 of them lack captions. Contrary to their use as a proxy for annotation keywords in (Feng and Lapata, 2008; Deschacht and Moens, 2007), where evaluation is performed on datasets gleaned from authoritative news websites, most captions in our dataset are not guaranteed to be noise free. However, they are used as part of the text for generating annotations where they exist. 4 Automatic Image Annotation We approach the task of automatic image annotation using four methods. Due to the orthogonal nature in their search for keywords, the output for each method is generated separately and later combined in an unsupervised setting. However, all four methods perform their discrimination of words by drawing </context>
<context position="12075" citStr="Feng and Lapata, 2008" startWordPosition="2009" endWordPosition="2012">s individually, followed by a combined approach using all of them. In the individual setting, we simply obtain the set of candidates proposed by each method as possible annotation keywords for the image. In the unsupervised combined setting, only the labels proposed by all individual methods are selected, and listed in reverse order of their combined rankings. We allow each system to produce a re-ranked list of top k words to be the final annotations for a given image. A system can discretionary generate less (but not more) than k words that is appropriate to its confidence level. Similar to (Feng and Lapata, 2008), we evaluate our systems using precision, recall and F-measure for k=10, k=15 and k=20 words. For comparison, we also implemented two baselines systems: tf * idf and Doc Title, which simply takes all the words in the title of the web page and uses them as annotation labels for the image. In the absence of a document title, we use the first sentence in the document. The results for G5intuition and G5context are tabulated in Tables 2 and 3 respectively. We further illustrate our results with an annotation example (an image taken from a webpage discussing strep throat among teens) in Table 1. Wo</context>
</contexts>
<marker>Feng, Lapata, 2008</marker>
<rawString>Yansong Feng and Mirella Lapata. 2008. Automatic image annotation using auxiliary text information. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In International Joint Conferences on Artificial Intelligence.</booktitle>
<contexts>
<context position="6376" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1048" endWordPosition="1051">n of words. Intuitively, there exists several “clouds” of semantically similar words that form several, possibly overlapping, sets of topics. Our task is to select the dominant topic put forward in the text, with the assumption that such a topic is being represented by the largest set of words. We use an adapted version of the K-means clustering approach, which attempts to find natural “clusters” of words in the text by grouping words with a common centroid. Each centroid is the semantic center of the group of words and the distance between each centroid and the words are approximated by ESA (Gabrilovich and Markovitch, 2007). Further, we perform our experiments with the following assumptions : (1) To maximize recall, we assume that there are only two topics in every text. (2) Every word or collocation in the text must be classified under one of these two topics, but not both. In cases, where there is a tie, the classification is chosen randomly. For each dominant cluster extracted, we rank the words in decreasing order of their ESA distance to the centroid. Together, they represent the gist of the topic and are used as a set of candidates for labeling the image. 4.2 Lexical Distance (Lex) Words that are lexically</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In International Joint Conferences on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>V Lavrenko</author>
<author>R Manmatha</author>
</authors>
<title>Automatic image annotation and retrieval using cross-media relevance models.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="2568" citStr="Jeon et al., 2003" startWordPosition="408" endWordPosition="411">pular task in computer vision and image processing, there are only a few efforts that leverage on the multitude of resources available for natural language processing to derive robust linguistic based image annotation models. Most of the work has posed the annotation task as a classification problem, such as (Li and Wang, 2008), where images are annotated using semantic labels associated to a semantic class. The most recent work on image annotation using linguistic features (Feng and Lapata, 2008) involves implementing an extended version of the continuous relevance model that is proposed in (Jeon et al., 2003). The basic idea underlying their work is to perform annotation of a test image by using keywords shared by similar training images. Evaluation of their system performance is based on a dataset collected from the news domain (BBC). Unlike them, in this paper, we attempt to perform image annotation on datasets from unrestricted domains. We are also interested in extending the work pursued in (Deschacht and Moens, 2007), where visualness and salience are proposed as important textual features for discovering named entities present in an image, by extracting other textual features that can furthe</context>
</contexts>
<marker>Jeon, Lavrenko, Manmatha, 2003</marker>
<rawString>J Jeon, V Lavrenko, and R Manmatha. 2003. Automatic image annotation and retrieval using cross-media relevance models. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Li</author>
<author>James Wang</author>
</authors>
<title>Real-time computerized annotation of pictures.</title>
<date>2008</date>
<booktitle>In Proceedings ofInternational Conference on Computer Vision.</booktitle>
<contexts>
<context position="1192" citStr="Li and Wang, 2008" startWordPosition="182" endWordPosition="185">collections of manual annotations on the same dataset. 1 Introduction Despite the usefulness of images in expressing ideas, machine understanding of the meaning of an image remains a daunting task for computers, as the interplay between the different visual components of an image does not conform to any fixed pattern that allows for formal reasoning of its semantics. Often, the machine interpretation of the concepts present in an image, known as automatic image annotation, can only be inferred by its accompanying text or co-occurrence information drawn from a large corpus of texts and images (Li and Wang, 2008; Barnard and Forsyth, 2001). Not surprisingly, humans have the innate ability to perform this task reliably, but given a large database of images, manual annotation is both labor-intensive and time-consuming. Our work centers around the question : Provided an image with its associated text, can we use the text to reliably extract keywords that relevantly describe the image ? Note that we are not concerned with the generation of keywords for an image, but rather their extraction from the related text. Our goal eventually is to automate this task by leveraging on texts which are naturally occur</context>
</contexts>
<marker>Li, Wang, 2008</marker>
<rawString>Jia Li and James Wang. 2008. Real-time computerized annotation of pictures. In Proceedings ofInternational Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In in Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9046" citStr="Mihalcea and Tarau, 2004" startWordPosition="1519" endWordPosition="1523">ds wi, and Sim(wi, wj) is the cosine similarity of the ESA vectors representing the two words. The directional weight (Cij/Ci) amounts to the degree of association of wi with respect to wj. Using the directional inferential similarity scores as directed edges and distinct words as vertices, we obtain a graph for each text. The directed edges denotes the idea of “recommendation” where we say w1 recommends w2 if and only if there is a directed edge from w1 to w2, with the weight of the recommendation being the directional similarity score. By employing the graph iteration algorithm proposed in (Mihalcea and Tarau, 2004), we can compute the rank of a vertex in 57 the entire graph. The output generated is a sorted list of words in decreasing order of their ranks, which serves as a list of candidates for annotating the image. Note that the top-ranked word must infer some or all of the words in the text. Table 1: An image annotation example Sem symptoms, treatment, medical treatment, medical care, sore throat, fluids, cough, tonsils, strep throat, swab Lex strep throat, cotton swab, lymph nodes, rheumatic fever, swab, strep, fever, sore throat, lab, scarlet fever Sal strep, swab, nemours, teens, ginger ale, grap</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In in Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>