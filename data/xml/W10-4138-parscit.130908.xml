<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.282579">
<title confidence="0.9982825">
Term Contributed Boundary Tagging by Conditional Random
Fields for SIGHAN 2010 Chinese Word Segmentation Bakeoff
</title>
<author confidence="0.999686">
Tian-Jian Jiang $ Shih-Hung Liu*1 Cheng-Lung Sung*1 Wen-Lian Hsu $
</author>
<affiliation confidence="0.991433666666667">
Department of *Department of $Institute of
Computer Science Electrical Engineering nformation Science
National Tsing-Hua University National Taiwan University Academia Sinica
</affiliation>
<email confidence="0.997332">
ftmjiang,journey,clsung,hsul@iis.sinica.edu.tw
</email>
<sectionHeader confidence="0.997365" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999616875">
This paper presents a Chinese word
segmentation system submitted to the
closed training evaluations of CIPS-
SIGHAN-2010 bakeoff. The system uses
a conditional random field model with
one simple feature called term contri-
buted boundaries (TCB) in addition to
the &amp;quot;B I&amp;quot; character-based tagging ap-
proach. TCB can be extracted from unla-
beled corpora automatically, and seg-
mentation variations of different do-
mains are expected to be reflected impli-
citly. The experiment result shows that
TCB does improve &amp;quot;B I&amp;quot; tagging domain-
independently about 1% of the F1 meas-
ure score.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999775444444444">
The CIPS-SIGHAN-2010 bakeoff task of Chi-
nese word segmentation is focused on cross-
domain texts. The design of data set is challeng-
ing particularly. The domain-specific training
corpora remain unlabeled, and two of the test
corpora keep domains unknown before releasing,
therefore it is not easy to apply ordinary machine
learning approaches, especially for the closed
training evaluations.
</bodyText>
<sectionHeader confidence="0.999661" genericHeader="introduction">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.8727385">
2.1 The `BI&amp;quot; Character-Based Tagging of
Conditional Random Field as Baseline
</subsectionHeader>
<bodyText confidence="0.999863952380952">
The character-based &amp;quot;OB I&amp;quot; tagging of
Conditional Random Field (Lafferty et al., 2001)
has been widely used in Chinese word
segmentation recently (Xue and Shen, 2003;
Peng and McCallum, 2004; Tseng et al., 2005).
Under the scheme, each character of a word is
labeled as `B&apos; if it is the first character of a
multiple-character word, or ` I&apos; otherwise. If the
character is a single-character word itself, &amp;quot;O&amp;quot;
will be its label. As Table 1 shows, the lost of
performance is about 1% by replacing &amp;quot;O&amp;quot; with
&amp;quot;B&amp;quot; for character-based CRF tagging on the
dataset of CIPS-SIGHAN-2010 bakeoff task of
Chinese word segmentation, thus we choose
&amp;quot;B I&amp;quot; as our baseline for simplicity, with this 1%
lost bearing in mind. In tables of this paper, SC
stands for Simplified Chinese and TC represents
for Traditional Chinese. Test corpora of SC and
TC are divided into four domains, where suffix
A, B, C and D attached, for texts of literature,
computer, medicine and finance, respectively.
</bodyText>
<table confidence="0.999873294117647">
R P F OOV
SC-A OBI 0.906 0.916 0.911 0.539
B I 0.896 0.907 0.901 0.508
SC-B OBI 0.868 0.797 0.831 0.410
B I 0.850 0.763 0.805 0.327
SC-C OBI 0.897 0.897 0.897 0.590
B I 0.888 0.886 0.887 0.551
SC-D OBI 0.900 0.903 0.901 0.472
B I 0.888 0.891 0.890 0.419
TC-A OBI 0.873 0.898 0.886 0.727
B I 0.856 0.884 0.870 0.674
TC-B OBI 0.906 0.932 0.919 0.578
B I 0.894 0.920 0.907 0.551
TC-C OBI 0.902 0.923 0.913 0.722
B I 0.891 0.914 0.902 0.674
TC-D OBI 0.924 0.934 0.929 0.765
B I 0.908 0.922 0.915 0.722
</table>
<tableCaption confidence="0.993689">
Table 1. OBI vs. B I; where the lost of F &gt; 1%,
</tableCaption>
<bodyText confidence="0.985635">
such as SC-B, is caused by incorrect English
segments that will be discussed in the section 4.
</bodyText>
<subsectionHeader confidence="0.995304">
2.2 Term Contributed Boundary
</subsectionHeader>
<bodyText confidence="0.999731386363636">
The word boundary and the word frequency are
the standard notions of frequency in corpus-
based natural language processing, but they lack
the correct information about the actual boun-
dary and frequency of a phrase&apos;s occurrence.
The distortion of phrase boundaries and frequen-
cies was first observed in the Vodis Corpus
when the bigram &amp;quot;RAIL ENQUIRIES&amp;quot; and tri-
gram &amp;quot;BRIT ISH RAIL ENQUIRIES&amp;quot; were ex-
amined and reported by O&apos;Boyle (1993). Both of
them occur 73 times, which is a large number for
such a small corpus. &amp;quot;ENQUIRIES&amp;quot; follows
&amp;quot;RAIL&amp;quot; with a very high probability when it is
preceded by &amp;quot;BRITISH.&amp;quot; However, when
&amp;quot;RAIL&amp;quot; is preceded by words other than &amp;quot;BRIT-
ISH,&amp;quot; &amp;quot;ENQUIRIES&amp;quot; does not occur, but words
like &amp;quot;TICKET&amp;quot; or &amp;quot;JOURNEY&amp;quot; may. Thus, the
bigram &amp;quot;RAIL ENQUIRIES&amp;quot; gives a misleading
probability that &amp;quot;RAIL&amp;quot; is followed by &amp;quot;EN-
QUIRIES&amp;quot; irrespective of what precedes it. This
problem happens not only with word-token cor-
pora but also with corpora in which all the com-
pounds are tagged as units since overlapping N-
grams still appear, therefore corresponding solu-
tions such as those of Zhang et al. (2006) were
proposed.
We uses suffix array algorithm to calculate ex-
act boundaries of phrase and their frequencies
(Sung et al., 2008), called term contributed
boundaries (TCB) and term contributed fre-
quencies (TCF), respectively, to analogize simi-
larities and differences with the term frequencies
(TF). For example, in Vodis Corpus, the original
TF of the term &amp;quot;RAIL ENQUIRIES&amp;quot; is 73.
However, the actual TCF of &amp;quot;RAIL ENQUI-
RIES&amp;quot; is 0, since all of the frequency values are
contributed by the term &amp;quot;BRIT ISH RAIL EN
QUIRIES&amp;quot;. In this case, we can see that `BRIT-
ISH RAIL ENQUIRIES&apos; is really a more fre-
quent term in the corpus, where &amp;quot;RAIL EN-
QUIRIES&amp;quot; is not. Hence the TCB of &amp;quot;BRITISH
RAIL ENQUIRIES&amp;quot; is ready for CRF tagging as
&amp;quot;BRITISH/TB RAIL/TB ENQUIRIES/TI,&amp;quot; for
example.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.995946727272727">
Besides submitted results, there are several
different experiments that we have done. The
configuration is about the trade-off between data
sparseness and domain fitness. For the sake of
OOV issue, TCBs from all the training and test
corpora are included in the configuration of
submitted results. For potentially better consis-
tency to different types of text, TCBs from the
training corpora and/or test corpora are grouped
by corresponding domains of test corpora. Table
2 and Table 3 provide the details, where the
baseline is the character-based &amp;quot;B I&amp;quot; tagging, and
others are &amp;quot;B I&amp;quot; with additional different TCB
configurations: TCBall stands for the submitted
results; TCBa, TCBb, TCBta, TCBtb, TCBtc,
TCBtd represents TCB extracted from the train-
ing corpus A, B, and the test corpus A, B, C, D,
respectively. Table 2 indicates that F1 measure
scores can be improved by TCB about 1%, do-
main-independently. Table 3 gives a hint of the
major contribution of performance is from TCB
of each test corpus.
</bodyText>
<table confidence="0.999947882352941">
R P F OOV
SC-A B I 0.896 0.907 0.901 0.508
TCBall 0.917 0.921 0.919 0.699
SC-B B I 0.850 0.763 0.805 0.327
TCBall 0.876 0.799 0.836 0.456
SC-C B I 0.888 0.886 0.887 0.551
TCBall 0.900 0.896 0.898 0.699
SC-D B I 0.888 0.891 0.890 0.419
TCBall 0.910 0.906 0.908 0.562
TC-A B I 0.856 0.884 0.870 0.674
TCBall 0.871 0.891 0.881 0.670
TC-B B I 0.894 0.920 0.907 0.551
TCBall 0.913 0.917 0.915 0.663
TC-C B I 0.891 0.914 0.902 0.674
TCBall 0.900 0.915 0.908 0.668
TC-D B I 0.908 0.922 0.915 0.722
TCBall 0.929 0.922 0.925 0.732
</table>
<tableCaption confidence="0.998291">
Table 2. Baseline vs. Submitted Results
</tableCaption>
<table confidence="0.999596">
F OOV
TC-A TCBta 0.889 0.706
TCBa 0.888 0.690
TCBta + TCBa 0.889 0.710
TCBall 0.881 0.670
TC-B TCBtb 0.911 0.636
TCBb 0.921 0.696
TCBtb + TCBb 0.912 0.641
TCBall 0.915 0.663
TC-C TCBtc 0.918 0.705
TCBall 0.908 0.668
TC-D TCBtd 0.927 0.717
TCBall 0.925 0.732
</table>
<tableCaption confidence="0.960839">
Table 3b. Traditional Chinese Domain-specific
TCB vs. TCBall
</tableCaption>
<sectionHeader confidence="0.999277" genericHeader="method">
4 Error Analysis
</sectionHeader>
<bodyText confidence="0.999634222222222">
The most significant type of error in our results
is unintentionally segmented English words. Ra-
ther than developing another set of tag for Eng-
lish alphabets, we applies post-processing to fix
this problem under the restriction of closed train-
ing by using only alphanumeric character infor-
mation. Table 4 compares F1 measure score of
the Simplified Chinese experiment results before
and after the post-processing.
</bodyText>
<table confidence="0.99913195">
F1 measure score
before after
SC-A OBI 0.911 0.918
B I 0.901 0.908
TCBta 0.918 0.920
TCBta + TCBa 0.917 0.920
TCBall 0.919 0.921
SC-B OBI 0.831 0.920
B I 0.805 0.910
TCBtb 0.832 0.917
TCBtb + TCBb 0.830 0.916
TCBall 0.836 0.916
SC-C OBI 0.897 0.904
B I 0.887 0.896
TCBtc 0.897 0.901
TCBall 0.898 0.902
SC-D OBI 0.901 0.919
B I 0.890 0.908
TCBtd 0.905 0.915
TCBall 0.908 0.918
</table>
<tableCaption confidence="0.9174525">
Table 4. F1 measure scores before and after
English Problem Fixed
</tableCaption>
<bodyText confidence="0.999967714285714">
The major difference between gold standards
of the Simplified Chinese corpora and the Tradi-
tional Chinese corpora is about non-Chinese
characters. All of the alphanumeric and the
punctuation sequences are separated from Chi-
nese sequences in the Simplified Chinese corpo-
ra, but can be part of the Chinese word segments
in the Traditional Chinese corpora. For example,
a phrase `服用 / simvastatin / ( / statins 類 / n / 一 /
a / )&apos; (`/&apos; represents the word boundary) from
the domain C of the test data cannot be either
recognized by `B I&apos; and/or TCB tagging ap-
proaches, or post-processed. This is the reason
why Table 4 does not come along with Tradi-
tional Chinese experiment results.
Some errors are due to inconsistencies in the
gold standard of non-Chinese character, For ex-
ample, in the Traditional Chinese corpora, some
percentage digits are separated from their per-
centage signs, meanwhile those percentage signs
are connected to parentheses right next to them.
</bodyText>
<sectionHeader confidence="0.998764" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<table confidence="0.999478461538462">
F OOV
SC-A TCBta 0.918 0.690
TCBa 0.917 0.679
TCBta + TCBa 0.917 0.690
TCBall 0.919 0.699
SC-B TCBtb 0.832 0.465
TCBb 0.828 0.453
TCBtb + TCBb 0.830 0.459
TCBall 0.836 0.456
SC-C TCBtc 0.897 0.618
TCBall 0.898 0.699
SC-D TCBtd 0.905 0.557
TCBall 0.910 0.562
</table>
<tableCaption confidence="0.9482735">
Table 3a. Simplified Chinese Domain-specific
TCB vs. TCBall
</tableCaption>
<bodyText confidence="0.995797545454545">
This paper introduces a simple CRF feature
called term contributed boundaries (TCB) for
Chinese word segmentation. The experiment
result shows that it can improve the basic `B I&amp;quot;
tagging scheme about 1% of the F1 measure
score, domain-independently.
Further tagging scheme for non-Chinese cha-
racters are desired for recognizing some sophis-
ticated gold standard of Chinese word segmenta-
tion that concatenates alphanumeric characters
to Chinese characters.
</bodyText>
<sectionHeader confidence="0.963814" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.6123395">
The CRF model used in this paper is developed based
on CRF++, http://crfpp.sourceforge.net/
Term Contributed Boundaries used in this paper are
extracted by YASA, http://yasa.newzilla.org/
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999754472222222">
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: proba-
bilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference of Machine Learning, 591-598.
Peter O&apos;Boyle. 1993. A Study of an N-Gram Lan-
guage Model for Speech Recognition. PhD thesis.
Queen&apos;s University Belfast.
Fuchun Peng and Andrew McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. In Proceedings of Interna-
tional Conference of Computational linguistics,
562-568, Geneva, Switzerland.
Cheng-Lung Sung, Hsu-Chun Yen, and Wen-Lian
Hsu. 2008. Compute the Term Contributed Fre-
quency. In Proceedings of the 2008 Eighth Inter-
national Conference on Intelligent Systems Design
and Applications, 325-328, Washington, D.C.,
USA.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Da-
niel Jurafsky, and Christopher Manning. 2005. A
conditional random field word segmenter for Sig-
han bakeoff 2005. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language
Processing, Jeju, Korea.
Nianwen Xue and Libin Shen. 2003. Chinese word-
segmentation as LMR tagging. In Proceedings of
the Second SIGHAN Workshop on Chinese Lan-
guage Processing.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumi-
ta. 2006. Subword-based tagging by conditional
random fields for Chinese word segmentation. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, 193-
196, New York, USA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.527876">
<title confidence="0.996189">Term Contributed Boundary Tagging by Conditional Random</title>
<author confidence="0.8172925">Fields for SIGHAN Chinese Word Segmentation Bakeoff Jiang Cheng-Lung Wen-Lian Hsu</author>
<affiliation confidence="0.964897666666667">of of of Computer Science Electrical Engineering nformation Science National Tsing-Hua University National Taiwan University Academia Sinica</affiliation>
<email confidence="0.949341">ftmjiang,journey,clsung,hsul@iis.sinica.edu.tw</email>
<abstract confidence="0.998648705882353">This paper presents a Chinese word segmentation system submitted to the closed training evaluations of CIPS- SIGHAN-2010 bakeoff. The system uses a conditional random field model with simple feature called contriboundaries in addition to the &amp;quot;B I&amp;quot; character-based tagging approach. TCB can be extracted from unlabeled corpora automatically, and segmentation variations of different domains are expected to be reflected implicitly. The experiment result shows that TCB does improve &amp;quot;B I&amp;quot; tagging domainindependently about 1% of the F1 measure score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proceedings of International Conference of Machine Learning,</booktitle>
<pages>591--598</pages>
<contexts>
<context position="1561" citStr="Lafferty et al., 2001" startWordPosition="220" endWordPosition="223">ently about 1% of the F1 measure score. 1 Introduction The CIPS-SIGHAN-2010 bakeoff task of Chinese word segmentation is focused on crossdomain texts. The design of data set is challenging particularly. The domain-specific training corpora remain unlabeled, and two of the test corpora keep domains unknown before releasing, therefore it is not easy to apply ordinary machine learning approaches, especially for the closed training evaluations. 2 Methodology 2.1 The `BI&amp;quot; Character-Based Tagging of Conditional Random Field as Baseline The character-based &amp;quot;OB I&amp;quot; tagging of Conditional Random Field (Lafferty et al., 2001) has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as `B&apos; if it is the first character of a multiple-character word, or ` I&apos; otherwise. If the character is a single-character word itself, &amp;quot;O&amp;quot; will be its label. As Table 1 shows, the lost of performance is about 1% by replacing &amp;quot;O&amp;quot; with &amp;quot;B&amp;quot; for character-based CRF tagging on the dataset of CIPS-SIGHAN-2010 bakeoff task of Chinese word segmentation, thus we choose &amp;quot;B I&amp;quot; as our baseline for simplicity, with this 1% los</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In Proceedings of International Conference of Machine Learning, 591-598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter O&apos;Boyle</author>
</authors>
<title>A Study of an N-Gram Language Model for Speech Recognition. PhD thesis.</title>
<date>1993</date>
<institution>Queen&apos;s University Belfast.</institution>
<contexts>
<context position="3550" citStr="O&apos;Boyle (1993)" startWordPosition="579" endWordPosition="580">5 0.722 Table 1. OBI vs. B I; where the lost of F &gt; 1%, such as SC-B, is caused by incorrect English segments that will be discussed in the section 4. 2.2 Term Contributed Boundary The word boundary and the word frequency are the standard notions of frequency in corpusbased natural language processing, but they lack the correct information about the actual boundary and frequency of a phrase&apos;s occurrence. The distortion of phrase boundaries and frequencies was first observed in the Vodis Corpus when the bigram &amp;quot;RAIL ENQUIRIES&amp;quot; and trigram &amp;quot;BRIT ISH RAIL ENQUIRIES&amp;quot; were examined and reported by O&apos;Boyle (1993). Both of them occur 73 times, which is a large number for such a small corpus. &amp;quot;ENQUIRIES&amp;quot; follows &amp;quot;RAIL&amp;quot; with a very high probability when it is preceded by &amp;quot;BRITISH.&amp;quot; However, when &amp;quot;RAIL&amp;quot; is preceded by words other than &amp;quot;BRITISH,&amp;quot; &amp;quot;ENQUIRIES&amp;quot; does not occur, but words like &amp;quot;TICKET&amp;quot; or &amp;quot;JOURNEY&amp;quot; may. Thus, the bigram &amp;quot;RAIL ENQUIRIES&amp;quot; gives a misleading probability that &amp;quot;RAIL&amp;quot; is followed by &amp;quot;ENQUIRIES&amp;quot; irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping Ngrams still appe</context>
</contexts>
<marker>O&apos;Boyle, 1993</marker>
<rawString>Peter O&apos;Boyle. 1993. A Study of an N-Gram Language Model for Speech Recognition. PhD thesis. Queen&apos;s University Belfast.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of International Conference of Computational linguistics,</booktitle>
<pages>562--568</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1665" citStr="Peng and McCallum, 2004" startWordPosition="237" endWordPosition="240">d segmentation is focused on crossdomain texts. The design of data set is challenging particularly. The domain-specific training corpora remain unlabeled, and two of the test corpora keep domains unknown before releasing, therefore it is not easy to apply ordinary machine learning approaches, especially for the closed training evaluations. 2 Methodology 2.1 The `BI&amp;quot; Character-Based Tagging of Conditional Random Field as Baseline The character-based &amp;quot;OB I&amp;quot; tagging of Conditional Random Field (Lafferty et al., 2001) has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as `B&apos; if it is the first character of a multiple-character word, or ` I&apos; otherwise. If the character is a single-character word itself, &amp;quot;O&amp;quot; will be its label. As Table 1 shows, the lost of performance is about 1% by replacing &amp;quot;O&amp;quot; with &amp;quot;B&amp;quot; for character-based CRF tagging on the dataset of CIPS-SIGHAN-2010 bakeoff task of Chinese word segmentation, thus we choose &amp;quot;B I&amp;quot; as our baseline for simplicity, with this 1% lost bearing in mind. In tables of this paper, SC stands for Simplified Chinese and TC represents for Tradi</context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>Fuchun Peng and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of International Conference of Computational linguistics, 562-568, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheng-Lung Sung</author>
<author>Hsu-Chun Yen</author>
<author>Wen-Lian Hsu</author>
</authors>
<title>Compute the Term Contributed Frequency.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Eighth International Conference on Intelligent Systems Design and Applications,</booktitle>
<pages>325--328</pages>
<location>Washington, D.C., USA.</location>
<contexts>
<context position="4352" citStr="Sung et al., 2008" startWordPosition="712" endWordPosition="715">&amp;quot;RAIL&amp;quot; is preceded by words other than &amp;quot;BRITISH,&amp;quot; &amp;quot;ENQUIRIES&amp;quot; does not occur, but words like &amp;quot;TICKET&amp;quot; or &amp;quot;JOURNEY&amp;quot; may. Thus, the bigram &amp;quot;RAIL ENQUIRIES&amp;quot; gives a misleading probability that &amp;quot;RAIL&amp;quot; is followed by &amp;quot;ENQUIRIES&amp;quot; irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping Ngrams still appear, therefore corresponding solutions such as those of Zhang et al. (2006) were proposed. We uses suffix array algorithm to calculate exact boundaries of phrase and their frequencies (Sung et al., 2008), called term contributed boundaries (TCB) and term contributed frequencies (TCF), respectively, to analogize similarities and differences with the term frequencies (TF). For example, in Vodis Corpus, the original TF of the term &amp;quot;RAIL ENQUIRIES&amp;quot; is 73. However, the actual TCF of &amp;quot;RAIL ENQUIRIES&amp;quot; is 0, since all of the frequency values are contributed by the term &amp;quot;BRIT ISH RAIL EN QUIRIES&amp;quot;. In this case, we can see that `BRITISH RAIL ENQUIRIES&apos; is really a more frequent term in the corpus, where &amp;quot;RAIL ENQUIRIES&amp;quot; is not. Hence the TCB of &amp;quot;BRITISH RAIL ENQUIRIES&amp;quot; is ready for CRF tagging as &amp;quot;BRIT</context>
</contexts>
<marker>Sung, Yen, Hsu, 2008</marker>
<rawString>Cheng-Lung Sung, Hsu-Chun Yen, and Wen-Lian Hsu. 2008. Compute the Term Contributed Frequency. In Proceedings of the 2008 Eighth International Conference on Intelligent Systems Design and Applications, 325-328, Washington, D.C., USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for Sighan bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>Jeju,</location>
<contexts>
<context position="1686" citStr="Tseng et al., 2005" startWordPosition="241" endWordPosition="244"> on crossdomain texts. The design of data set is challenging particularly. The domain-specific training corpora remain unlabeled, and two of the test corpora keep domains unknown before releasing, therefore it is not easy to apply ordinary machine learning approaches, especially for the closed training evaluations. 2 Methodology 2.1 The `BI&amp;quot; Character-Based Tagging of Conditional Random Field as Baseline The character-based &amp;quot;OB I&amp;quot; tagging of Conditional Random Field (Lafferty et al., 2001) has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as `B&apos; if it is the first character of a multiple-character word, or ` I&apos; otherwise. If the character is a single-character word itself, &amp;quot;O&amp;quot; will be its label. As Table 1 shows, the lost of performance is about 1% by replacing &amp;quot;O&amp;quot; with &amp;quot;B&amp;quot; for character-based CRF tagging on the dataset of CIPS-SIGHAN-2010 bakeoff task of Chinese word segmentation, thus we choose &amp;quot;B I&amp;quot; as our baseline for simplicity, with this 1% lost bearing in mind. In tables of this paper, SC stands for Simplified Chinese and TC represents for Traditional Chinese. Test </context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for Sighan bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese wordsegmentation as LMR tagging.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="1640" citStr="Xue and Shen, 2003" startWordPosition="233" endWordPosition="236"> task of Chinese word segmentation is focused on crossdomain texts. The design of data set is challenging particularly. The domain-specific training corpora remain unlabeled, and two of the test corpora keep domains unknown before releasing, therefore it is not easy to apply ordinary machine learning approaches, especially for the closed training evaluations. 2 Methodology 2.1 The `BI&amp;quot; Character-Based Tagging of Conditional Random Field as Baseline The character-based &amp;quot;OB I&amp;quot; tagging of Conditional Random Field (Lafferty et al., 2001) has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as `B&apos; if it is the first character of a multiple-character word, or ` I&apos; otherwise. If the character is a single-character word itself, &amp;quot;O&amp;quot; will be its label. As Table 1 shows, the lost of performance is about 1% by replacing &amp;quot;O&amp;quot; with &amp;quot;B&amp;quot; for character-based CRF tagging on the dataset of CIPS-SIGHAN-2010 bakeoff task of Chinese word segmentation, thus we choose &amp;quot;B I&amp;quot; as our baseline for simplicity, with this 1% lost bearing in mind. In tables of this paper, SC stands for Simplified Chinese an</context>
</contexts>
<marker>Xue, Shen, 2003</marker>
<rawString>Nianwen Xue and Libin Shen. 2003. Chinese wordsegmentation as LMR tagging. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Genichiro Kikui</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Subword-based tagging by conditional random fields for Chinese word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>193--196</pages>
<location>New York, USA.</location>
<contexts>
<context position="4224" citStr="Zhang et al. (2006)" startWordPosition="691" endWordPosition="694">for such a small corpus. &amp;quot;ENQUIRIES&amp;quot; follows &amp;quot;RAIL&amp;quot; with a very high probability when it is preceded by &amp;quot;BRITISH.&amp;quot; However, when &amp;quot;RAIL&amp;quot; is preceded by words other than &amp;quot;BRITISH,&amp;quot; &amp;quot;ENQUIRIES&amp;quot; does not occur, but words like &amp;quot;TICKET&amp;quot; or &amp;quot;JOURNEY&amp;quot; may. Thus, the bigram &amp;quot;RAIL ENQUIRIES&amp;quot; gives a misleading probability that &amp;quot;RAIL&amp;quot; is followed by &amp;quot;ENQUIRIES&amp;quot; irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping Ngrams still appear, therefore corresponding solutions such as those of Zhang et al. (2006) were proposed. We uses suffix array algorithm to calculate exact boundaries of phrase and their frequencies (Sung et al., 2008), called term contributed boundaries (TCB) and term contributed frequencies (TCF), respectively, to analogize similarities and differences with the term frequencies (TF). For example, in Vodis Corpus, the original TF of the term &amp;quot;RAIL ENQUIRIES&amp;quot; is 73. However, the actual TCF of &amp;quot;RAIL ENQUIRIES&amp;quot; is 0, since all of the frequency values are contributed by the term &amp;quot;BRIT ISH RAIL EN QUIRIES&amp;quot;. In this case, we can see that `BRITISH RAIL ENQUIRIES&apos; is really a more frequen</context>
</contexts>
<marker>Zhang, Kikui, Sumita, 2006</marker>
<rawString>Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. 2006. Subword-based tagging by conditional random fields for Chinese word segmentation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, 193-196, New York, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>