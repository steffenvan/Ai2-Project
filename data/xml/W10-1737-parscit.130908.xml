<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002379">
<title confidence="0.994073">
Aiding Pronoun Translation with Co-Reference Resolution
</title>
<author confidence="0.986937">
Ronan Le Nagard and Philipp Koehn
</author>
<affiliation confidence="0.9322405">
University of Edinburgh
Edinburgh, United Kingdom
</affiliation>
<email confidence="0.990781">
s@678231@sms.ed.ac.uk, pkoehn@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993714" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999830333333333">
We propose a method to improve the trans-
lation of pronouns by resolving their co-
reference to prior mentions. We report re-
sults using two different co-reference res-
olution methods and point to remaining
challenges.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.963951945454546">
While machine translation research has made
great progress over the last years, including the in-
creasing exploitation of linguistic annotation, the
problems are mainly framed as the translation of
isolated sentences. This restriction of the task ig-
nores several discourse-level problems, such as the
translation of pronouns.
Pronouns typically refer to earlier mention of
entities, and the nature of these entities may matter
for translation. A glaring case is the translation of
the English it and they into languages with gram-
matical gender (as for instance, most European
languages). If it refers to an object that has a male
grammatical gender in the target language, then its
translation is a male pronoun (e.g., il in French),
while referring to a female object requires a fe-
male pronoun (e.g., elle in French).
Figure 1 illustrates the problem. Given a pair of
sentence such as
The window is open. It is blue.
the translation of it cannot be determined given
only the sentence it occurs in. It is essential that
we connect it to the entity the window in the pre-
vious sentence.
Making such a connection between references
to the same entity is called co-reference resolu-
tion, or anaphora resolution.1 While this problem
1In the context of pronouns, anaphora resolution and co-
reference resolution are identical, but they differ in other con-
texts.
has motivated significant research in the field of
natural language processing, the integration of co-
reference resolution methods into machine transla-
tion has been lacking. The recent wave of work on
statistical machine translation has essentially not
moved beyond sentence-level and has not touched
co-reference resolution.
Our approach to aiding pronoun translation with
co-reference resolution can be outlined as follows.
On both training and test data, we identify the
anaphoric noun of each occurrence of it and they
on the source side (English). We then identify
the noun’s translation into the target language (in
our experiments, French), and identify the target
noun’s grammatical gender. Based on that gender,
we replace it with it-masculine, it-feminine or it-
neutral (ditto for they). We train a statistical ma-
chine translation system with a thusly annotated
corpus and apply it to the annotated test sentences.
Our experiments show some degree of suc-
cess of the method, but also highlight that current
co-reference resolution methods (we implemented
Hobbs and Lappin/Laess) have not yet achieved
sufficient performance to significantly reduce the
number of errors in pronoun translation.
</bodyText>
<sectionHeader confidence="0.999939" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.998534">
2.1 Co-Reference and Machine Translation
</subsectionHeader>
<bodyText confidence="0.99991875">
The problem of anaphora resolution applied to ma-
chine translation has not been treated much in the
literature. Although some papers refer to the prob-
lem, their content is mostly concerned with the
problem of anaphora resolution and speak very lit-
tle about the integration of such an algorithm in the
bigger theme of machine translation.
Mitkov et al. [1995] deplore the lack of study
of the question and try to address it with the im-
plementation of an anaphora resolution model and
its integration into the CAT2 translation system
[Sharp, 1988], a transfer system that uses an ab-
</bodyText>
<page confidence="0.951596">
252
</page>
<note confidence="0.8487705">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 252–261,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
The window is open. It is blue. La fenˆetre est ouverte. Elle est bleue. CORRECT
The window is open. It is black. La fenˆetre est ouverte. Il est noir. WRONG
The oven is open. It is new. Le four est ouverte. Elle est neuve. WRONG
The door is open. It is new. La porte est ouverte. Elle est neuve. CORRECT
</note>
<figureCaption confidence="0.998573">
Figure 1: Translation errors due to lack of co-reference resolution (created with Google Translate).
</figureCaption>
<bodyText confidence="0.987156181818182">
stract intermediate representation. The anaphora
resolution step adds additional features to the in-
termediate representation.
Leass and Schwall [1991] present a list of rules
to be implemented directly into the machine trans-
lation system. These rules seem to work mostly
like a dictionary and are checked in a priority or-
der. They state what should be the translation of
a pronoun in each special case. Being specific to
the problem of translating anaphors into Korean,
these are of little interest to our current work.
</bodyText>
<subsectionHeader confidence="0.997513">
2.2 Co-Reference : Syntactic Method
</subsectionHeader>
<bodyText confidence="0.999973861538462">
The first work on the resolution of pronouns was
done in the 1970s, largely based on a syntactic ap-
proach. This work was based on empirical data
and observations about natural languages. For ex-
ample, Winograd [1972] uses the notion of co-
reference chains when stating that if a single pro-
noun is used several times in a sentence or a group
of adjunct sentences, all instances of this pronoun
should refer to the same entity.
Others have also stated that antecedents of a
pronoun should be found in one of the n sen-
tences preceding the pronouns, where n should
be small [Klapholz and Lockman, 1975]. Hobbs
[1978] showed that this number was close to one,
although no actual limit could be really imposed.
In work by both Hobbs [1978] and Winograd
[1972], the resolution of pronouns also involves a
syntactic study of the parse tree of sentences. The
order with which candidate antecedents are prior-
itized is similar in both studies. They first look for
the antecedent to be a subject, then the direct ob-
ject of a noun and finally an indirect object. Only
thereafter previous sentences are checked for an
antecedent, in no particular order, although the left
to right order seems to be preferred in the literature
as it implicitly preserves the order just mentioned.
Winograd uses focus values of noun phrases in
sentences to choose the appropriate antecedent.
Hobbs also refers to the work by Charniak
[1972] and Wilks [1975] for the problem of
anaphora resolution. However, they do not offer a
complete solution to the problem. For this reason
Hobbs [1978] is often considered to be the most
comprehensive early syntactic study of the prob-
lem, and as such, often used as a baseline to evalu-
ate anaphora resolution methods. We use his work
and comment on it in a later section.
Another approach to anaphora resolution is
based on the centering theory first proposed by
Grosz et al. [1995]. Brennan et al. [1987] propose
an algorithm for pronoun resolution based on cen-
tering theory. Once again, the entities are ranked
according to their grammatical role, where subject
is more salient than existential constructs, which
are more salient than direct and indirect objects.
Walker [1998] further improves the theory of cen-
tering theory for anaphora resolution, proposing
the idea of cache model to replace the stack model
described originally.
Another syntactic approach to the problem of
co-reference resolution is the use of weighted
features by Lappin and Leass [1994] which we
present in more details in a further section. This al-
gorithm is based on two modules, a syntactic filter
followed by a system of salience weighting. The
algorithm gathers all potential noun phrase an-
tecedents of a pronoun from the current and close
previous sentences. The syntactic filter then filters
out the ones that are unlikely to be antecedents, ac-
cording to different rules, including general agree-
ment rules. The remaining candidate noun phrases
are weighted according to salience factors. The
authors demonstrate a higher success rate with
their algorithm (86%) than with their implemen-
tation of the Hobbs algorithm (82%).
</bodyText>
<subsectionHeader confidence="0.999546">
2.3 Co-Reference : Statistical Approach
</subsectionHeader>
<bodyText confidence="0.999866">
Machine Learning has also been applied to the
problem of anaphora resolution. Ng [2005] gives
a survey of the research carried out in this area.
The work by Aone and Bennett [1995] is among
the first in this field. It applies machine learning to
anaphora resolution on Japanese text. The authors
use a set of 66 features, related to both the referent
itself and to the relation between the referent and
</bodyText>
<page confidence="0.997161">
253
</page>
<bodyText confidence="0.999967666666667">
its antecedent. They include ”lexical (e.g. cate-
gory), syntactic (e.g. grammatical role), semantic
(e.g. semantic class), and positional (e.g. distance
between anaphor and antecedent)” information.
Ge et al. [1998] also present a statistical algo-
rithm based on the study of statistical data in a
large corpus and the application of a naive Bayes
model. The authors report an accuracy rate of
82.9%, or 84.2% with the addition of statistical
data on gender categorization of words.
In more recent work, Kehler et al. [2004] show
a move towards the use of common-sense knowl-
edge to help the resolution of anaphors. They use
referring probabilities taken from a large anno-
tated corpus as a knowledge base.
</bodyText>
<subsectionHeader confidence="0.999405">
2.4 Shared Tasks and Evaluation
</subsectionHeader>
<bodyText confidence="0.999991172413793">
Although a fairly large amount of research has
been done in the field, it is often reported [Mitkov
et al., 1995] that there does not yet exist a method
to resolve pronouns which is entirely satisfactory
and effective. Different kinds of texts (novel,
newspaper,...) pose problems [Hobbs, 1978] and
the field is also victim of lack of standardization.
Algorithms are evaluated on different texts and
large annotated corpora with co-reference infor-
mation is lacking to check results. A response to
these problems came with the creation of shared
tasks, such as the MUC [Grishman and Sund-
heim, 1996] which included a co-reference sub-
task [Chinchor and Hirschmann, 1997] and led to
the creation of the MUC-6 and MUC-7 corpora.
There are other annotation efforts worth men-
tioning, such as the ARRAU corpus [Poesio and
Artstein, 2008] which include texts from various
sources and deals with previous problems in an-
notation such as anaphora ambiguity and anno-
tation of information on agreement, grammatical
function and reference. The Anaphoric Bank and
the Phrase Detectives are both part of the Anawiki
project [Poesio et al., 2008] and also promise the
creation of a standardized corpus. The first one al-
lows for the sharing of annotated corpora. The sec-
ond is a collaborative effort to annotate large cor-
pora through the Web. In its first year of use, the
system saw the resolution of 700,000 pronouns.
</bodyText>
<sectionHeader confidence="0.986434" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.9996676">
The method has two main aspects: the application
of co-reference to annotate pronouns and the sub-
sequent integration into statistical machine trans-
lation. We begin our description with the latter
aspect.
</bodyText>
<subsectionHeader confidence="0.997963">
3.1 Integration into Machine Translation
</subsectionHeader>
<bodyText confidence="0.999982361702128">
English pronouns such as it (and they) do not
have a unique French translation, but rather sev-
eral words are potential translations. Note that for
simplicity we comment here on the pronoun it, but
the same conclusions can be drawn from the study
of the plural pronoun they.
In most cases, the translation ambiguity can-
not be resolved in the context of a single sentence
because the pronoun refers to an antecedent in a
previous sentence. Statistical machine translation
focuses on single sentences and therefore cannot
deal with antecedents in previous sentences. Our
approach does not fundamentally change the sta-
tistical machine translation approach, but treats the
necessary pronoun classification as a external task.
Hence, the pronoun it is annotated, resulting
in the three different surface forms presented to
the translation system: it-neutral, it-feminine, it-
masculine. These therefore encode the gender in-
formation of the pronoun and each of them will
be match to its corresponding French translation
in the translation table.
An interesting point to note is the fact that these
pronouns only encode gender information about
the pronouns and omit number and person infor-
mation. This has two reasons.
Firstly, study of the lexical translation table for
the baseline system shows that the probability of
having the singular pronoun it translated into the
plural pronouns ils and elles is 10 times smaller
than the one for the singular/singular translation
pair. This means that the number of times a sin-
gular pronoun in English translates into a plural
pronoun in French is negligible.
The other reason to omit the cases when a sin-
gular pronoun is translated into a plural pronoun is
due to the performance of our algorithm. Indeed,
the detection of number information in the algo-
rithm is not good enough and returns many false
results which would reduce the performance of the
final system. Also, adding the number agreement
to the pronoun would mean a high segmentation
between all the different possibilities, which we
assumed would result in worse performance of the
translation system.
Once we have created a way to tag the pronouns
with gender information, the system needs to learn
</bodyText>
<page confidence="0.993302">
254
</page>
<figure confidence="0.337712">
② training: word alignment, test: translation mapping
</figure>
<figureCaption confidence="0.9770405">
Figure 2: Overview of the process to annotate pronouns: The word it is connected to the antecedent
window which was translated as fenˆetre, a feminine noun. Thus, the pronoun is annotated as it-feminine.
</figureCaption>
<figure confidence="0.987348875">
① co-reference resolution
It is blue.
④ annotation
It-feminine is blue.
The window is open.
La fenetre est ouverte.
③ lexical resources
FEMININE
</figure>
<bodyText confidence="0.999625828571429">
the new probabilities that link the source language
pronoun to the target language pronouns. That is
all instances of it in the training data, which can
be found at any position in the corpus sentences,
should be replaced by one of its three declension.
However, it is important to stress that the gender
information that should be encoded in the English
corpus is the one which corresponds to the gender
of the French translation of the antecedent.
In order to find the correct gender information
for the pronoun, we execute the co-reference reso-
lution algorithm on the English text which returns
the antecedent of the pronoun (more on this in the
next section). Note that we are not interested in the
English gender of the antecedent, but in gender of
its translation.
Thus, we need to detect the French translation
of the English antecedent. For the training data,
we rely on the word alignment that is produced as
a by-product of the training of a statistical machine
translation system. For the test data, we rely on
the implicit word mapping performed during the
translation process.
Note that this requires in practice the translation
of all preceding sentences before we can annotate
the current sentence. To avoid this practical bur-
den in our experiments, we simply use the map-
ping in the baseline translation. The performance
of the sentence alignment (88
Once the French word is obtained, it is used as
the input of a module which returns the gender of
the entity in French. This is then used to replace
the original pronoun with the new gendered pro-
noun.
The entire process is illustrated in Figure 2.
</bodyText>
<subsectionHeader confidence="0.999318">
3.2 The Hobbs Algorithm
</subsectionHeader>
<bodyText confidence="0.999993727272727">
The Hobbs algorithm is considered to be the base-
line algorithm for co-reference resolution. The al-
gorithm uses the syntactic parse tree of the sen-
tences as input.
The algorithm traverses the parse tree and se-
lects appropriate candidate referents to the pro-
noun. It goes up sentence nodes and checks all
NP nodes encountered for agreement with the pro-
noun. The order in which the algorithm traverses
the tree ensures that some priorities are respected,
to make sure the most probable antecedent is re-
turned first. By doing this, the algorithm tends
to enforces some of the constraints that apply to
co-reference [Jurafsky et al., 2000]. The recency
constraint is enforced thanks to the order in which
the algorithm traverses the sentences and both the
binding and grammatical role constraints are en-
forced by the use of the syntactic tree and Part-Of-
Speech tags on the words.
Because the algorithm only uses the parse tree
of the sentences, the semantic meaning of words
is completely omitted in the process of select-
ing candidate antecedents and no knowledge is
required except for the implicit knowledge con-
tained within agreement features.
As mentioned earlier, the Hobbs algorithm goes
up the tree from the given pronoun to the top of the
tree and stops at each sentence or noun node on its
way. In each of these nodes, it performs breadth
first search of the sub tree and returns any noun
phrase node encountered as a potential antecedent.
If the antecedent is genuine (according to gender,
number, and person agreement), it is returned.
</bodyText>
<page confidence="0.990133">
255
</page>
<bodyText confidence="0.999406363636364">
In case no antecedent was found in the current
sentence, the algorithm goes back up in the text,
looking at each sentence separately, in a left-to-
right breadth first fashion. This ensures that the
subject/object/indirect object priorities and hierar-
chy are respected. Again, if a candidate NP has
matching agreement features, it is returned as the
antecedent of the pronoun. Otherwise the algo-
rithm goes one sentence higher.
The original algorithm uses limited knowledge
because it assumes that:
</bodyText>
<listItem confidence="0.999984333333333">
• Dates do not move.
• Places do not move.
• Large fixed objects don’t move.
</listItem>
<bodyText confidence="0.990115">
This add limited semantic restrictions for the an-
tecedent chosen. Indeed, if the pronoun is fol-
lowed by a motion verb, the antecedent could not
be a date, a place or a large fixed object. However,
as Hobbs states himself, those constraints help lit-
tle since they do not apply in most cases.
</bodyText>
<subsectionHeader confidence="0.999501">
3.3 The Lappin and Leass Algorithm
</subsectionHeader>
<bodyText confidence="0.9999830125">
Lappin and Leass [1994] proposed an anaphora
resolution algorithm for third person pronouns and
lexical anaphors. It is based on slot grammar and
uses syntax combined with a system of weights
to select the appropriate antecedent of a pronoun.
The implementation of the algorithm we deal with
here is fairly different from the one presented in
the original paper, and is largely inspired from the
JavaRAP implementation [Qiu et al., 2004].
The first important variation was mentioned ear-
lier and concerns the application of co-reference
resolution to machine translation. We concen-
trate in this work on the resolution of third per-
son pronouns, and we omit reflexive pronouns (it-
self, themselves) (referred to as lexical anaphora in
some works).
Another variation comes from the use of the
Collins parser [Collins, 2003]. Although work on
the original algorithm uses McCord’s Slot Gram-
mar parser [McCord, 1990], work on JavaRAP
shows that rules can be created to simulate the cat-
egories and predicates used in slot grammar. Also,
Preiss [2002] evaluates the use of different parsers
for the Lappin and Leass algorithm, showing that
performance of the algorithm is not related to the
performance of the parser itself. The JavaRAP im-
plementation uses a Charniak parser, which per-
forms worse than the Collins parser in Preiss’ re-
search.
For these reasons and in order to allow for reuse
of the code used previously in the implementation
of the Hobbs algorithm, the input to the Lappin
and Leass algorithm is text parsed with the Collins
parser.
It should be noted that the Lappin and Le-
ass algorithm (also called RAP for Resolution of
Anaphora Procedure) has been used in the original
research for the application of machine translation.
The algorithm processes sentence by sentence,
keeping in memory the information regarding the
last four sentences. In the first step of the algo-
rithm, all noun phrases (NPs) are extracted and
classified. Definite and indefinite NPs are sep-
arated, and pleonastic pronouns are segregated
from other pronouns.
The notion of salience is very important in
RAP, as it allows the algorithm to choose between
competing NPs. All candidate NPs are given a
”salience weighting”, which represents the impor-
tance and visibility of the phrase in the sentence,
and in relation to the pronoun that is being re-
solved.
Salience weighting is based on the syntactic
form of the sentence and the value for an NP is
calculated through the contribution, or not, of dif-
ferent salience factors, to which weights are asso-
ciated. This calculation ensures that different im-
portance will be given to a subject noun phrase in
a sentence, and a noun phrase that is embedded in
another or that represents the indirect object of a
verb.
There are a number of salience factors such
as sentence recency, subject emphasis, existential
emphasis, accusative emphasis, etc. Each factor is
associated with a predefined weight.
Once the weight of each candidate has been cal-
culated, the algorithm uses syntactic information
to filter out the noun phrases that the pronoun is
unlikely to refer to. This includes agreement and
other checks.
The list of candidate NPs obtained after this
processing is then cleared of all NPs that fall un-
der a given threshold. The original algorithm then
deals with singular and plural pronouns in differ-
ent ways. The JavaRAP implementation however
does not use these differences and we refer the
reader to Lappin and Leass’ paper for further in-
formation.
Finally, the candidate NPs mentioned in the pre-
vious list are ranked according to their salience
</bodyText>
<page confidence="0.995313">
256
</page>
<bodyText confidence="0.999184285714286">
weights and the highest scoring one is returned as
the antecedent of the pronoun. In case several NPs
have the same salience weight, the one closest to
the pronoun is returned.
results similar to the implementation of a machine
learning technique, this method seemed appropri-
ate for our purpose.
</bodyText>
<subsectionHeader confidence="0.987386">
3.4 Pleonastic It
</subsectionHeader>
<bodyText confidence="0.9993162">
English makes an extensive use of the pronoun it
in a pleonastic fashion. That is, many times, it is
considered to be structural and does not refer to
any entity previously mentioned. The following
are examples of pleonastic uses of it:
</bodyText>
<listItem confidence="0.999643333333333">
• It is raining.
• It seems important that I see him.
• The session is opened, it was announced.
</listItem>
<bodyText confidence="0.999379027777778">
Being able to discriminate the use of a struc-
tural it from the use of a referential use of it is
very important for the success of the co-reference
algorithm. Indeed, resolving a pleonastic it will
be a waste of time for the algorithm, and more im-
portantly, it will increase the chance of errors and
will result in poorer performances. Moreover, the
pleonastic it is most times translated masculine in
French, meaning any other resolution by the algo-
rithm will yield errors.
In the past, the importance given to the detec-
tion of the pleonastic use of it has varied from au-
thor to author. As an example, Rush et al. [1971],
in their work on automatic summarization, only
mentioned the problem. Others formed a set of
rules to detect them, such as Liddy et al. [1987]
with 142 rules, or Lappin and Leass [1994] who
propose a very restricted set of rules for the detec-
tion of the structural it.
Paice and Husk [1987] carried out extensive re-
search on the topic and their paper defines various
categories for the pronoun it as well as proposing
a set of rules that allow to differentiate when the
pronoun it is used as a relational pronoun or as a
pleonastic pronoun.
Their method categorise words according to the
presence of given words around the pronoun it.
They distinguish constructs such as it VERB STA-
TUS to TASK; construct expressing doubt contain-
ing words such as whether, if, how; parenthetical
it such as it seems, it was said. The original arti-
cle identifies seven categories for pleonastic pro-
nouns.
Since their own results showed a success rate
of 92.2% on a test section of the LOBC corpus
and the implementation of their technique yields
</bodyText>
<sectionHeader confidence="0.999178" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999871414634147">
In this section, we comment on the tools used for
the implementation of the algorithms, as well as
support tools and corpora.
The implementation of both of the algorithms
was done using the Python programming lan-
guage, which was chosen for its simplicity in pro-
cessing text files and because it is the language in
which the Natural Language Toolkit is developed.
The Natural Language Toolkit (NLTK) is a suite
of Python modules used for research into natural
language processing. We mostly used its Tree and
ParentedTree modules which enable the represen-
tation of parse trees into tree structures. NLTK
also includes a naive Bayes classifier, which we
used in association with the names corpus in order
to classify proper names into gender categories ac-
cording to a set of features. We also use NLTK for
its named entity capacities, in order to find ani-
macity information of entities.
English sentences were annotated with the MX-
POST Part of Speech tagger and the Collins syn-
tactic parser.
The Lefff lexicon, introduced by Sagot et al.
[2006] was used to get agreement features of
French words. It contains over 80,000 French
words,2 along with gender and number informa-
tion.
We used the open source Moses toolkit [Koehn
et al., 2007] and trained standard phrase-based
translation models.
As training data, we used the Europarl corpus
[Koehn, 2005], a commonly used parallel corpus
in statistical machine translation research. While
there are also commonly used Europarl test sets,
these do not contain sentences in sequence for
complete documents. Instead, we used as test set
the proceedings from October 5, 2000 - a set of
1742 sentences from the held-out portion of the
corpus. We translated the test set both with a base-
line system and a system trained on the annotated
training data and tested on an annotated test set.
</bodyText>
<footnote confidence="0.999884">
2The original version version of the lexicon is available
from http://www.labri.fr/perso/clement/lefff/.
</footnote>
<page confidence="0.984145">
257
</page>
<table confidence="0.899907222222222">
Word Count
English singular he 17,181
she 4,575
it 214,047
French singular il 187,921
elle 45,682
English plural they 54,776
French plural ils 32,350
elles 16,238
</table>
<tableCaption confidence="0.9801245">
Table 1: Number of sentences in the training cor-
pus containing third person personal pronouns.
</tableCaption>
<table confidence="0.9997452">
correct annotation 33/59 56%
correct translation:
annotated 40/59 68%
correctly annotated 27/33 82%
baseline 41/59 69%
</table>
<tableCaption confidence="0.979312">
Table 4: Translation Results: On a manually ex-
</tableCaption>
<figureCaption confidence="0.9255194">
amined portion of the test set, only 33 of 59 pro-
nouns are labeled correctly. The translation results
of our method does not differ significantly from
the baseline. Most of the correctly annotated pro-
nouns are translated correctly.
</figureCaption>
<table confidence="0.8472685">
Truth Method
Pleonastic Referential
Pleonastic 42 20
Referential 19 98
</table>
<tableCaption confidence="0.9949">
Table 2: Detection of pleonastic pronouns
</tableCaption>
<sectionHeader confidence="0.999789" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.999935">
5.1 Corpus Statistics for Pronouns
</subsectionHeader>
<bodyText confidence="0.999971">
Personal pronouns are among the most frequent
words in text. In the training corpus of 1,393,452
sentences, about a 6th contain third person per-
sonal pronouns. See Table 1 for detailed statistics.
The English pronoun it is much more frequent
than he or she. For both languages, the masculine
forms are more frequent than the feminine forms.
There are then a total of 233,603 sentences con-
taining a third person pronoun in French, and
235,803 sentences containing a third person pro-
noun in English. This means that over 2,000
of those pronouns in English do not have equiv-
alent in French. Similarly for plural: A total
of 48,588 sentences contain a plural pronoun in
French, against 54,776 in English. That shows that
over 6,000 of the English ones are not translated
into French.
</bodyText>
<subsectionHeader confidence="0.999951">
5.2 Detection of the Pleonastic it
</subsectionHeader>
<bodyText confidence="0.9999894">
We checked, how well our method for pleonas-
tic it detection works on a section of the test set.
We achieved both recall and precision of 83% for
the categorization of the referential it. For details,
please see Table 2.
</bodyText>
<subsectionHeader confidence="0.997732">
5.3 Translation Probabilities
</subsectionHeader>
<bodyText confidence="0.999973785714286">
Let us now examine the translation probabilities
for the annotated and un-annotated pronouns. De-
tails are given in Table 3.
In the baseline system, both it and they have
a strong translation preference for the masculine
over the feminine form of the French pronoun.
It translates with probability 0.307 to il and with
probability 0.090 to elle. The rest of the probabil-
ity mass is taken up by the NULL token, punctua-
tion, and a long tail of unlikely choices.
For both the Hobbs and the Lappin/Laess algo-
rithm, the probability distribution is shifted to the
desired French pronoun. The shift is strongest for
the masculine marked they, which prefers the mas-
culine ils with 0.431 over the feminine elles with
0.053 (numbers for Hobbs, Lappin/Laess numbers
are 0.435 and 0.054, respectively).
Feminine marked pronouns now slightly prefer
feminine French forms, overcoming the original
bias. The neutrally marked pronouns shift slightly
in favor of masculine translations.
The pronoun they-neutral appears in 12,424
sentences in the corpus, which all represent failed
resolution of the co-reference. Indeed, French
does not have neutral gender and the plural third
person pronoun is never pleonastic. These results
therefore show that a lot of noise is added to the
system.
</bodyText>
<subsectionHeader confidence="0.998939">
5.4 Translation Results
</subsectionHeader>
<bodyText confidence="0.999429909090909">
The BLEU scores for our method is almost iden-
tical to the baseline performance. This is not sur-
prising, since we only expect to change the transla-
tion of a small number of words (however, impor-
tant words for understanding the meaning of the
text).
A better evaluation metric is the number of cor-
rectly translated pronouns. This requires manual
inspection of the translation results. Results are
given in Table 4.
While the shift of the translation probabilities
</bodyText>
<page confidence="0.988536">
258
</page>
<note confidence="0.477391">
Unannotated Hobbs Lappin and Laess
</note>
<table confidence="0.999662903225807">
English French p
it-neutral il 0.369
it-neutral elle 0.065
it-masculine il 0.230
it-masculine elle 0.060
it-feminine il 0.144
it-feminine elle 0.168
they-neutral ils 0.344
they-neutral elles 0.102
they-masc. ils 0.435
they-masc. elles 0.053
they-feminine ils 0.208
they-feminine elles 0.259
English French p
it-neutral il 0.372
it-neutral elle 0.064
it-masculine il 0.211
it-masculine elle 0.051
it-feminine il 0.142
it-feminine elle 0.156
they-neutral ils 0.354
they-neutral elles 0.090
they-masc. ils 0.431
they-masc. elles 0.054
they-feminine ils 0.207
they-feminine elles 0.255
English French p
it il 0.307
it elle 0.090
they ils 0.341
they elles 0.130
</table>
<tableCaption confidence="0.788435">
Table 3: Translation probabilities. The probabilities of gender-marked pronouns are shifted to the
corresponding gender in the two cases the text was annotated with the co-reference resolution methods
mentionned earlier.
</tableCaption>
<bodyText confidence="0.99976125">
suggests that we are moving the translation of pro-
nouns in the right direction, this is not reflected by
the sample of pronoun translations we inspected.
In fact, the performance for our method is almost
identical to the baseline (68% and 69%, respec-
tively).
One cause for this is the poor performance
of the co-reference resolution method, which la-
bels only 56% of pronouns correctly. On this
sub-sample of correctly annotated pronouns, we
achieve 82% correct translations. However, the
baseline method also performs well on this subset.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999762">
We presented a method to aid pronoun transla-
tion for statistical machine translation by using co-
reference resolution. This is to our knowledge the
first such work.
While our method works in principle, the re-
sults are not yet convincing. The main problem
is the low performance of the co-reference resolu-
tion algorithm we used. The method works well
when the co-reference resolution algorithm pro-
vides correct results.
Future work should concentrate on better co-
reference algorithms. The context of machine
translation also provides an interesting testbed for
such algorithms, since it offers standard test sets
for many language pairs.
</bodyText>
<sectionHeader confidence="0.999083" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99942">
This work was supported by the EuroMatrixPlus
project funded by the European Commission (7th
Framework Programme).
</bodyText>
<sectionHeader confidence="0.984566" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.61715325">
C. Aone and S.W. Bennett. Evaluating automated
and manual acquisition of anaphora resolution
strategies. In Proceedings of the 33rd annual
meeting on Association for Computational Lin-
guistics, pages 122–129. Association for Com-
putational Linguistics Morristown, NJ, USA,
1995.
S. E. Brennan, M. W. Friedman, and C. J. Pollard.
A centering approach to pronouns. In Proceed-
ings of the 25th annual meeting on Association
for Computational Linguistics, pages 155–162,
1987.
</bodyText>
<reference confidence="0.982546833333333">
E. Charniak. Toward a model of children’s story
comprehension. MIT, 1972.
N. Chinchor and L. Hirschmann. MUC-7 corefer-
ence task definition, version 3.0. In Proceedings
of MUC, volume 7, 1997.
M. Collins. Head-driven statistical models for nat-
ural language parsing. Computational Linguis-
tics, 29(4):589–637, 2003.
N. Ge, J. Hale, and E. Charniak. A statistical ap-
proach to anaphora resolution. In Proceedings
of the Sixth Workshop on Very Large Corpora,
pages 161–170, 1998.
</reference>
<page confidence="0.990405">
259
</page>
<reference confidence="0.995506775510204">
R. Grishman and B. Sundheim. Message un-
derstanding conference-6: A brief history. In
Proceedings of the 16th conference on Com-
putational Linguistics-Volume 1, pages 466–
471. Association for Computational Linguistics
Morristown, NJ, USA, 1996.
B. J. Grosz, S. Weinstein, and A. K. Joshi. Center-
ing: A framework for modeling the local coher-
ence of discourse. Computational Linguistics,
21(2):203–225, 1995.
J. R. Hobbs. Resolving Pronoun References. Lin-
gua, 44:339–352, 1978.
D. Jurafsky, J. H. Martin, A. Kehler, K. Van-
der Linden, and N. Ward. Speech and language
processing. Prentice Hall New York, 2000.
A. Kehler, D. Appelt, L. Taylor, and A. Simma.
The (non) utility of predicate-argument fre-
quencies for pronoun interpretation. In Proc. of
HLT-NAACL, volume 4, pages 289–296, 2004.
D. Klapholz and A. Lockman. Contextual refer-
ence resolution. American Journal of Compu-
tational Linguistics, microfiche 36, 1975.
Philipp Koehn. Europarl: A parallel corpus for
statistical machine translation. In Proceedings
of the Tenth Machine Translation Summit (MT
Summit X), Phuket, Thailand, September 2005.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Christopher J.
Dyer, Ondˇrej Bojar, Alexandra Constantin,
and Evan Herbst. Moses: Open source toolkit
for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the
Association for Computational Linguistics
Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177–180, Prague,
Czech Republic, June 2007. Association for
Computational Linguistics.
S. Lappin and H.J. Leass. An algorithm for
pronominal anaphora resolution. Computa-
tional Linguistics, 20(4):561, 1994.
Herbert Leass and Ulrike Schwall. An Anaphora
Resolution Procedure for Machine Translation.
Technical Report Report 172, IBM Germany
Science Center, Institute for Knowledge Based
Systems, 1991.
E. Liddy, S. Bonzi, J. Katzer, and E. Oddy. A
study of discourse anaphora in scientific ab-
stracts. Journal of the American Society for In-
formation Science, 38(4):255–261, 1987.
Michael C. McCord. Slot grammar: A system
for simpler construction of practical natural lan-
guage grammars. In Proceedings of the In-
ternational Symposium on Natural Language
and Logic, pages 118–145, London, UK, 1990.
Springer-Verlag. ISBN 3-540-53082-7.
R. Mitkov, S. K. Choi, and R. Sharp. Anaphora
resolution in Machine Translation. In Proceed-
ings of the Sixth International Conference on
Theoretical and Methodological Issues in Ma-
chine Translation, TMI’95, 1995.
V. Ng. Machine learning for coreference resolu-
tion: From local classification to global rank-
ing. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics,
page 164. Association for Computational Lin-
guistics, 2005.
C. D. Paice and G. D. Husk. Towards the auto-
matic recognition of anaphoric features in En-
glish text: the impersonal pronoun. Computer
Speech &amp; Language, 2(2):109–132, 1987.
M. Poesio and R. Artstein. Anaphoric annotation
in the ARRAU corpus. In Proceedings of the In-
ternational Conference on Language Resources
and Evaluation (LREC), 2008.
M. Poesio, U. Kruschwitz, and J. Chamberlain.
ANAWIKI: Creating anaphorically annotated
resources through Web cooperation. In Pro-
ceedings of the International Conference on
Language Resources and Evaluation (LREC),
volume 8. Citeseer, 2008.
Judita Preiss. Choosing a parser for anaphora reso-
lution. In 4th Discourse Anaphora and Anaphor
Resolution Colloquium (DAARC), pages 175–
180. Edi c˜oes Colibri, 2002.
Long Qiu, Min yen Kan, and Tat seng Chua.
A public reference implementation of the rap
anaphora resolution algorithm. In Proceedings
of the International Conference on Language
Resources and Evaluation (LREC), pages 291–
294, 2004.
J. E. Rush, R. Salvador, and A. Zamora. Auto-
matic abstracting and indexing. II. Production
of indicative abstracts by application of contex-
tual inference and syntactic coherence criteria.
Journal of the American Society for Information
Science and Technology, 22(4):260–274, 1971.
</reference>
<page confidence="0.954658">
260
</page>
<reference confidence="0.999621315789474">
B. Sagot, L. Cl´ement, E. V. de La Clergerie, and
P. Boullier. The Lefff 2 syntactic lexicon for
French: architecture, acquisition, use. In Pro-
ceedings of the International Conference on
Language Resources and Evaluation (LREC),
2006.
Randall Sharp. CAT2 – implementing a formal-
ism for multi-lingual MT. In 2nd International
Conference on Theoretical and Methodological
Issues in Machine Translation of Natural Lan-
guage, pages 3–6, 1988.
M. A. Walker. Centering, anaphora resolution, and
discourse structure. Centering theory in dis-
course, pages 401–435, 1998.
Y. Wilks. A preferential, pattern-seeking, seman-
tics for natural language inference. Words and
Intelligence I, pages 83–102, 1975.
T. Winograd. Understanding natural language.
Cognitive Psychology, 3(1):1–191, 1972.
</reference>
<page confidence="0.997825">
261
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.708784">
<title confidence="0.999904">Aiding Pronoun Translation with Co-Reference Resolution</title>
<author confidence="0.997322">Ronan Le_Nagard</author>
<author confidence="0.997322">Philipp</author>
<affiliation confidence="0.905967">University of Edinburgh, United</affiliation>
<email confidence="0.871152">s@678231@sms.ed.ac.uk,pkoehn@inf.ed.ac.uk</email>
<abstract confidence="0.995103571428571">We propose a method to improve the translation of pronouns by resolving their coreference to prior mentions. We report results using two different co-reference resolution methods and point to remaining challenges.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Toward a model of children’s story comprehension.</title>
<date>1972</date>
<publisher>MIT,</publisher>
<marker>Charniak, 1972</marker>
<rawString>E. Charniak. Toward a model of children’s story comprehension. MIT, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
<author>L Hirschmann</author>
</authors>
<title>MUC-7 coreference task definition, version 3.0.</title>
<date>1997</date>
<booktitle>In Proceedings of MUC,</booktitle>
<volume>7</volume>
<contexts>
<context position="9720" citStr="Chinchor and Hirschmann, 1997" startWordPosition="1575" endWordPosition="1578">earch has been done in the field, it is often reported [Mitkov et al., 1995] that there does not yet exist a method to resolve pronouns which is entirely satisfactory and effective. Different kinds of texts (novel, newspaper,...) pose problems [Hobbs, 1978] and the field is also victim of lack of standardization. Algorithms are evaluated on different texts and large annotated corpora with co-reference information is lacking to check results. A response to these problems came with the creation of shared tasks, such as the MUC [Grishman and Sundheim, 1996] which included a co-reference subtask [Chinchor and Hirschmann, 1997] and led to the creation of the MUC-6 and MUC-7 corpora. There are other annotation efforts worth mentioning, such as the ARRAU corpus [Poesio and Artstein, 2008] which include texts from various sources and deals with previous problems in annotation such as anaphora ambiguity and annotation of information on agreement, grammatical function and reference. The Anaphoric Bank and the Phrase Detectives are both part of the Anawiki project [Poesio et al., 2008] and also promise the creation of a standardized corpus. The first one allows for the sharing of annotated corpora. The second is a collab</context>
</contexts>
<marker>Chinchor, Hirschmann, 1997</marker>
<rawString>N. Chinchor and L. Hirschmann. MUC-7 coreference task definition, version 3.0. In Proceedings of MUC, volume 7, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="18213" citStr="Collins, 2003" startWordPosition="2986" endWordPosition="2987">e appropriate antecedent of a pronoun. The implementation of the algorithm we deal with here is fairly different from the one presented in the original paper, and is largely inspired from the JavaRAP implementation [Qiu et al., 2004]. The first important variation was mentioned earlier and concerns the application of co-reference resolution to machine translation. We concentrate in this work on the resolution of third person pronouns, and we omit reflexive pronouns (itself, themselves) (referred to as lexical anaphora in some works). Another variation comes from the use of the Collins parser [Collins, 2003]. Although work on the original algorithm uses McCord’s Slot Grammar parser [McCord, 1990], work on JavaRAP shows that rules can be created to simulate the categories and predicates used in slot grammar. Also, Preiss [2002] evaluates the use of different parsers for the Lappin and Leass algorithm, showing that performance of the algorithm is not related to the performance of the parser itself. The JavaRAP implementation uses a Charniak parser, which performs worse than the Collins parser in Preiss’ research. For these reasons and in order to allow for reuse of the code used previously in the </context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ge</author>
<author>J Hale</author>
<author>E Charniak</author>
</authors>
<title>A statistical approach to anaphora resolution.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>161--170</pages>
<marker>Ge, Hale, Charniak, 1998</marker>
<rawString>N. Ge, J. Hale, and E. Charniak. A statistical approach to anaphora resolution. In Proceedings of the Sixth Workshop on Very Large Corpora, pages 161–170, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>B Sundheim</author>
</authors>
<title>Message understanding conference-6: A brief history.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational Linguistics-Volume 1,</booktitle>
<pages>466--471</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics</institution>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="9650" citStr="Grishman and Sundheim, 1996" startWordPosition="1564" endWordPosition="1568">.4 Shared Tasks and Evaluation Although a fairly large amount of research has been done in the field, it is often reported [Mitkov et al., 1995] that there does not yet exist a method to resolve pronouns which is entirely satisfactory and effective. Different kinds of texts (novel, newspaper,...) pose problems [Hobbs, 1978] and the field is also victim of lack of standardization. Algorithms are evaluated on different texts and large annotated corpora with co-reference information is lacking to check results. A response to these problems came with the creation of shared tasks, such as the MUC [Grishman and Sundheim, 1996] which included a co-reference subtask [Chinchor and Hirschmann, 1997] and led to the creation of the MUC-6 and MUC-7 corpora. There are other annotation efforts worth mentioning, such as the ARRAU corpus [Poesio and Artstein, 2008] which include texts from various sources and deals with previous problems in annotation such as anaphora ambiguity and annotation of information on agreement, grammatical function and reference. The Anaphoric Bank and the Phrase Detectives are both part of the Anawiki project [Poesio et al., 2008] and also promise the creation of a standardized corpus. The first o</context>
</contexts>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>R. Grishman and B. Sundheim. Message understanding conference-6: A brief history. In Proceedings of the 16th conference on Computational Linguistics-Volume 1, pages 466– 471. Association for Computational Linguistics Morristown, NJ, USA, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>S Weinstein</author>
<author>A K Joshi</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<marker>Grosz, Weinstein, Joshi, 1995</marker>
<rawString>B. J. Grosz, S. Weinstein, and A. K. Joshi. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Resolving Pronoun References.</title>
<date>1978</date>
<location>Lingua, 44:339–352,</location>
<contexts>
<context position="9347" citStr="Hobbs, 1978" startWordPosition="1517" endWordPosition="1518">addition of statistical data on gender categorization of words. In more recent work, Kehler et al. [2004] show a move towards the use of common-sense knowledge to help the resolution of anaphors. They use referring probabilities taken from a large annotated corpus as a knowledge base. 2.4 Shared Tasks and Evaluation Although a fairly large amount of research has been done in the field, it is often reported [Mitkov et al., 1995] that there does not yet exist a method to resolve pronouns which is entirely satisfactory and effective. Different kinds of texts (novel, newspaper,...) pose problems [Hobbs, 1978] and the field is also victim of lack of standardization. Algorithms are evaluated on different texts and large annotated corpora with co-reference information is lacking to check results. A response to these problems came with the creation of shared tasks, such as the MUC [Grishman and Sundheim, 1996] which included a co-reference subtask [Chinchor and Hirschmann, 1997] and led to the creation of the MUC-6 and MUC-7 corpora. There are other annotation efforts worth mentioning, such as the ARRAU corpus [Poesio and Artstein, 2008] which include texts from various sources and deals with previou</context>
</contexts>
<marker>Hobbs, 1978</marker>
<rawString>J. R. Hobbs. Resolving Pronoun References. Lingua, 44:339–352, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
<author>A Kehler</author>
<author>K Vander Linden</author>
<author>N Ward</author>
</authors>
<title>Speech and language processing.</title>
<date>2000</date>
<publisher>Prentice Hall</publisher>
<location>New York,</location>
<contexts>
<context position="15584" citStr="Jurafsky et al., 2000" startWordPosition="2545" endWordPosition="2548">gorithm is considered to be the baseline algorithm for co-reference resolution. The algorithm uses the syntactic parse tree of the sentences as input. The algorithm traverses the parse tree and selects appropriate candidate referents to the pronoun. It goes up sentence nodes and checks all NP nodes encountered for agreement with the pronoun. The order in which the algorithm traverses the tree ensures that some priorities are respected, to make sure the most probable antecedent is returned first. By doing this, the algorithm tends to enforces some of the constraints that apply to co-reference [Jurafsky et al., 2000]. The recency constraint is enforced thanks to the order in which the algorithm traverses the sentences and both the binding and grammatical role constraints are enforced by the use of the syntactic tree and Part-OfSpeech tags on the words. Because the algorithm only uses the parse tree of the sentences, the semantic meaning of words is completely omitted in the process of selecting candidate antecedents and no knowledge is required except for the implicit knowledge contained within agreement features. As mentioned earlier, the Hobbs algorithm goes up the tree from the given pronoun to the to</context>
</contexts>
<marker>Jurafsky, Martin, Kehler, Linden, Ward, 2000</marker>
<rawString>D. Jurafsky, J. H. Martin, A. Kehler, K. Vander Linden, and N. Ward. Speech and language processing. Prentice Hall New York, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
<author>D Appelt</author>
<author>L Taylor</author>
<author>A Simma</author>
</authors>
<title>The (non) utility of predicate-argument frequencies for pronoun interpretation.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<volume>4</volume>
<pages>289--296</pages>
<marker>Kehler, Appelt, Taylor, Simma, 2004</marker>
<rawString>A. Kehler, D. Appelt, L. Taylor, and A. Simma. The (non) utility of predicate-argument frequencies for pronoun interpretation. In Proc. of HLT-NAACL, volume 4, pages 289–296, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klapholz</author>
<author>A Lockman</author>
</authors>
<title>Contextual reference resolution.</title>
<date>1975</date>
<journal>American Journal of Computational Linguistics, microfiche</journal>
<volume>36</volume>
<contexts>
<context position="5357" citStr="Klapholz and Lockman, 1975" startWordPosition="859" endWordPosition="862">o-Reference : Syntactic Method The first work on the resolution of pronouns was done in the 1970s, largely based on a syntactic approach. This work was based on empirical data and observations about natural languages. For example, Winograd [1972] uses the notion of coreference chains when stating that if a single pronoun is used several times in a sentence or a group of adjunct sentences, all instances of this pronoun should refer to the same entity. Others have also stated that antecedents of a pronoun should be found in one of the n sentences preceding the pronouns, where n should be small [Klapholz and Lockman, 1975]. Hobbs [1978] showed that this number was close to one, although no actual limit could be really imposed. In work by both Hobbs [1978] and Winograd [1972], the resolution of pronouns also involves a syntactic study of the parse tree of sentences. The order with which candidate antecedents are prioritized is similar in both studies. They first look for the antecedent to be a subject, then the direct object of a noun and finally an indirect object. Only thereafter previous sentences are checked for an antecedent, in no particular order, although the left to right order seems to be preferred in</context>
</contexts>
<marker>Klapholz, Lockman, 1975</marker>
<rawString>D. Klapholz and A. Lockman. Contextual reference resolution. American Journal of Computational Linguistics, microfiche 36, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth Machine Translation Summit (MT Summit X),</booktitle>
<location>Phuket, Thailand,</location>
<contexts>
<context position="24636" citStr="Koehn, 2005" startWordPosition="4079" endWordPosition="4080">der categories according to a set of features. We also use NLTK for its named entity capacities, in order to find animacity information of entities. English sentences were annotated with the MXPOST Part of Speech tagger and the Collins syntactic parser. The Lefff lexicon, introduced by Sagot et al. [2006] was used to get agreement features of French words. It contains over 80,000 French words,2 along with gender and number information. We used the open source Moses toolkit [Koehn et al., 2007] and trained standard phrase-based translation models. As training data, we used the Europarl corpus [Koehn, 2005], a commonly used parallel corpus in statistical machine translation research. While there are also commonly used Europarl test sets, these do not contain sentences in sequence for complete documents. Instead, we used as test set the proceedings from October 5, 2000 - a set of 1742 sentences from the held-out portion of the corpus. We translated the test set both with a baseline system and a system trained on the annotated training data and tested on an annotated test set. 2The original version version of the lexicon is available from http://www.labri.fr/perso/clement/lefff/. 257 Word Count E</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Tenth Machine Translation Summit (MT Summit X), Phuket, Thailand, September 2005.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Christopher J Dyer</author>
</authors>
<title>Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="24522" citStr="Koehn et al., 2007" startWordPosition="4061" endWordPosition="4064">s a naive Bayes classifier, which we used in association with the names corpus in order to classify proper names into gender categories according to a set of features. We also use NLTK for its named entity capacities, in order to find animacity information of entities. English sentences were annotated with the MXPOST Part of Speech tagger and the Collins syntactic parser. The Lefff lexicon, introduced by Sagot et al. [2006] was used to get agreement features of French words. It contains over 80,000 French words,2 along with gender and number information. We used the open source Moses toolkit [Koehn et al., 2007] and trained standard phrase-based translation models. As training data, we used the Europarl corpus [Koehn, 2005], a commonly used parallel corpus in statistical machine translation research. While there are also commonly used Europarl test sets, these do not contain sentences in sequence for complete documents. Instead, we used as test set the proceedings from October 5, 2000 - a set of 1742 sentences from the held-out portion of the corpus. We translated the test set both with a baseline system and a system trained on the annotated training data and tested on an annotated test set. 2The or</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Christopher J. Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June 2007. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lappin</author>
<author>H J Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<marker>Lappin, Leass, 1994</marker>
<rawString>S. Lappin and H.J. Leass. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):561, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Leass</author>
<author>Ulrike Schwall</author>
</authors>
<title>An Anaphora Resolution Procedure for Machine Translation.</title>
<date>1991</date>
<tech>Technical Report Report 172,</tech>
<institution>IBM Germany Science Center, Institute for Knowledge Based Systems,</institution>
<marker>Leass, Schwall, 1991</marker>
<rawString>Herbert Leass and Ulrike Schwall. An Anaphora Resolution Procedure for Machine Translation. Technical Report Report 172, IBM Germany Science Center, Institute for Knowledge Based Systems, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Liddy</author>
<author>S Bonzi</author>
<author>J Katzer</author>
<author>E Oddy</author>
</authors>
<title>A study of discourse anaphora in scientific abstracts.</title>
<date>1987</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>38</volume>
<issue>4</issue>
<marker>Liddy, Bonzi, Katzer, Oddy, 1987</marker>
<rawString>E. Liddy, S. Bonzi, J. Katzer, and E. Oddy. A study of discourse anaphora in scientific abstracts. Journal of the American Society for Information Science, 38(4):255–261, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C McCord</author>
</authors>
<title>Slot grammar: A system for simpler construction of practical natural language grammars.</title>
<date>1990</date>
<journal>Springer-Verlag. ISBN</journal>
<booktitle>In Proceedings of the International Symposium on Natural Language and Logic,</booktitle>
<pages>118--145</pages>
<location>London, UK,</location>
<contexts>
<context position="18303" citStr="McCord, 1990" startWordPosition="3000" endWordPosition="3001">e is fairly different from the one presented in the original paper, and is largely inspired from the JavaRAP implementation [Qiu et al., 2004]. The first important variation was mentioned earlier and concerns the application of co-reference resolution to machine translation. We concentrate in this work on the resolution of third person pronouns, and we omit reflexive pronouns (itself, themselves) (referred to as lexical anaphora in some works). Another variation comes from the use of the Collins parser [Collins, 2003]. Although work on the original algorithm uses McCord’s Slot Grammar parser [McCord, 1990], work on JavaRAP shows that rules can be created to simulate the categories and predicates used in slot grammar. Also, Preiss [2002] evaluates the use of different parsers for the Lappin and Leass algorithm, showing that performance of the algorithm is not related to the performance of the parser itself. The JavaRAP implementation uses a Charniak parser, which performs worse than the Collins parser in Preiss’ research. For these reasons and in order to allow for reuse of the code used previously in the implementation of the Hobbs algorithm, the input to the Lappin and Leass algorithm is text</context>
</contexts>
<marker>McCord, 1990</marker>
<rawString>Michael C. McCord. Slot grammar: A system for simpler construction of practical natural language grammars. In Proceedings of the International Symposium on Natural Language and Logic, pages 118–145, London, UK, 1990. Springer-Verlag. ISBN 3-540-53082-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitkov</author>
<author>S K Choi</author>
<author>R Sharp</author>
</authors>
<title>Anaphora resolution in Machine Translation.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation, TMI’95,</booktitle>
<contexts>
<context position="9166" citStr="Mitkov et al., 1995" startWordPosition="1488" endWordPosition="1491">a statistical algorithm based on the study of statistical data in a large corpus and the application of a naive Bayes model. The authors report an accuracy rate of 82.9%, or 84.2% with the addition of statistical data on gender categorization of words. In more recent work, Kehler et al. [2004] show a move towards the use of common-sense knowledge to help the resolution of anaphors. They use referring probabilities taken from a large annotated corpus as a knowledge base. 2.4 Shared Tasks and Evaluation Although a fairly large amount of research has been done in the field, it is often reported [Mitkov et al., 1995] that there does not yet exist a method to resolve pronouns which is entirely satisfactory and effective. Different kinds of texts (novel, newspaper,...) pose problems [Hobbs, 1978] and the field is also victim of lack of standardization. Algorithms are evaluated on different texts and large annotated corpora with co-reference information is lacking to check results. A response to these problems came with the creation of shared tasks, such as the MUC [Grishman and Sundheim, 1996] which included a co-reference subtask [Chinchor and Hirschmann, 1997] and led to the creation of the MUC-6 and MUC</context>
</contexts>
<marker>Mitkov, Choi, Sharp, 1995</marker>
<rawString>R. Mitkov, S. K. Choi, and R. Sharp. Anaphora resolution in Machine Translation. In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation, TMI’95, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Machine learning for coreference resolution: From local classification to global ranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>164</pages>
<marker>Ng, 2005</marker>
<rawString>V. Ng. Machine learning for coreference resolution: From local classification to global ranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, page 164. Association for Computational Linguistics, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Paice</author>
<author>G D Husk</author>
</authors>
<title>Towards the automatic recognition of anaphoric features in English text: the impersonal pronoun.</title>
<date>1987</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>2</volume>
<issue>2</issue>
<marker>Paice, Husk, 1987</marker>
<rawString>C. D. Paice and G. D. Husk. Towards the automatic recognition of anaphoric features in English text: the impersonal pronoun. Computer Speech &amp; Language, 2(2):109–132, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>R Artstein</author>
</authors>
<title>Anaphoric annotation in the ARRAU corpus.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<contexts>
<context position="9882" citStr="Poesio and Artstein, 2008" startWordPosition="1603" endWordPosition="1606"> and effective. Different kinds of texts (novel, newspaper,...) pose problems [Hobbs, 1978] and the field is also victim of lack of standardization. Algorithms are evaluated on different texts and large annotated corpora with co-reference information is lacking to check results. A response to these problems came with the creation of shared tasks, such as the MUC [Grishman and Sundheim, 1996] which included a co-reference subtask [Chinchor and Hirschmann, 1997] and led to the creation of the MUC-6 and MUC-7 corpora. There are other annotation efforts worth mentioning, such as the ARRAU corpus [Poesio and Artstein, 2008] which include texts from various sources and deals with previous problems in annotation such as anaphora ambiguity and annotation of information on agreement, grammatical function and reference. The Anaphoric Bank and the Phrase Detectives are both part of the Anawiki project [Poesio et al., 2008] and also promise the creation of a standardized corpus. The first one allows for the sharing of annotated corpora. The second is a collaborative effort to annotate large corpora through the Web. In its first year of use, the system saw the resolution of 700,000 pronouns. 3 Method The method has two</context>
</contexts>
<marker>Poesio, Artstein, 2008</marker>
<rawString>M. Poesio and R. Artstein. Anaphoric annotation in the ARRAU corpus. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>U Kruschwitz</author>
<author>J Chamberlain</author>
</authors>
<title>ANAWIKI: Creating anaphorically annotated resources through Web cooperation.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<volume>8</volume>
<contexts>
<context position="10181" citStr="Poesio et al., 2008" startWordPosition="1650" endWordPosition="1653">ems came with the creation of shared tasks, such as the MUC [Grishman and Sundheim, 1996] which included a co-reference subtask [Chinchor and Hirschmann, 1997] and led to the creation of the MUC-6 and MUC-7 corpora. There are other annotation efforts worth mentioning, such as the ARRAU corpus [Poesio and Artstein, 2008] which include texts from various sources and deals with previous problems in annotation such as anaphora ambiguity and annotation of information on agreement, grammatical function and reference. The Anaphoric Bank and the Phrase Detectives are both part of the Anawiki project [Poesio et al., 2008] and also promise the creation of a standardized corpus. The first one allows for the sharing of annotated corpora. The second is a collaborative effort to annotate large corpora through the Web. In its first year of use, the system saw the resolution of 700,000 pronouns. 3 Method The method has two main aspects: the application of co-reference to annotate pronouns and the subsequent integration into statistical machine translation. We begin our description with the latter aspect. 3.1 Integration into Machine Translation English pronouns such as it (and they) do not have a unique French trans</context>
</contexts>
<marker>Poesio, Kruschwitz, Chamberlain, 2008</marker>
<rawString>M. Poesio, U. Kruschwitz, and J. Chamberlain. ANAWIKI: Creating anaphorically annotated resources through Web cooperation. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), volume 8. Citeseer, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judita Preiss</author>
</authors>
<title>Choosing a parser for anaphora resolution.</title>
<date>2002</date>
<booktitle>In 4th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC), pages 175– 180. Edi c˜oes Colibri,</booktitle>
<marker>Preiss, 2002</marker>
<rawString>Judita Preiss. Choosing a parser for anaphora resolution. In 4th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC), pages 175– 180. Edi c˜oes Colibri, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Qiu</author>
<author>Min yen Kan</author>
<author>Tat seng Chua</author>
</authors>
<title>A public reference implementation of the rap anaphora resolution algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>291--294</pages>
<contexts>
<context position="17832" citStr="Qiu et al., 2004" startWordPosition="2924" endWordPosition="2927"> a date, a place or a large fixed object. However, as Hobbs states himself, those constraints help little since they do not apply in most cases. 3.3 The Lappin and Leass Algorithm Lappin and Leass [1994] proposed an anaphora resolution algorithm for third person pronouns and lexical anaphors. It is based on slot grammar and uses syntax combined with a system of weights to select the appropriate antecedent of a pronoun. The implementation of the algorithm we deal with here is fairly different from the one presented in the original paper, and is largely inspired from the JavaRAP implementation [Qiu et al., 2004]. The first important variation was mentioned earlier and concerns the application of co-reference resolution to machine translation. We concentrate in this work on the resolution of third person pronouns, and we omit reflexive pronouns (itself, themselves) (referred to as lexical anaphora in some works). Another variation comes from the use of the Collins parser [Collins, 2003]. Although work on the original algorithm uses McCord’s Slot Grammar parser [McCord, 1990], work on JavaRAP shows that rules can be created to simulate the categories and predicates used in slot grammar. Also, Preiss [</context>
</contexts>
<marker>Qiu, Kan, Chua, 2004</marker>
<rawString>Long Qiu, Min yen Kan, and Tat seng Chua. A public reference implementation of the rap anaphora resolution algorithm. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), pages 291– 294, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Rush</author>
<author>R Salvador</author>
<author>A Zamora</author>
</authors>
<title>Automatic abstracting and indexing. II. Production of indicative abstracts by application of contextual inference and syntactic coherence criteria.</title>
<date>1971</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>22</volume>
<issue>4</issue>
<marker>Rush, Salvador, Zamora, 1971</marker>
<rawString>J. E. Rush, R. Salvador, and A. Zamora. Automatic abstracting and indexing. II. Production of indicative abstracts by application of contextual inference and syntactic coherence criteria. Journal of the American Society for Information Science and Technology, 22(4):260–274, 1971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sagot</author>
<author>L Cl´ement</author>
<author>E V de La Clergerie</author>
<author>P Boullier</author>
</authors>
<title>The Lefff 2 syntactic lexicon for French: architecture, acquisition, use.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<marker>Sagot, Cl´ement, Clergerie, Boullier, 2006</marker>
<rawString>B. Sagot, L. Cl´ement, E. V. de La Clergerie, and P. Boullier. The Lefff 2 syntactic lexicon for French: architecture, acquisition, use. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randall Sharp</author>
</authors>
<title>CAT2 – implementing a formalism for multi-lingual MT.</title>
<date>1988</date>
<booktitle>In 2nd International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Language,</booktitle>
<pages>3--6</pages>
<contexts>
<context position="3572" citStr="Sharp, 1988" startWordPosition="559" endWordPosition="560">ranslation. 2 Related Work 2.1 Co-Reference and Machine Translation The problem of anaphora resolution applied to machine translation has not been treated much in the literature. Although some papers refer to the problem, their content is mostly concerned with the problem of anaphora resolution and speak very little about the integration of such an algorithm in the bigger theme of machine translation. Mitkov et al. [1995] deplore the lack of study of the question and try to address it with the implementation of an anaphora resolution model and its integration into the CAT2 translation system [Sharp, 1988], a transfer system that uses an ab252 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 252–261, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics The window is open. It is blue. La fenˆetre est ouverte. Elle est bleue. CORRECT The window is open. It is black. La fenˆetre est ouverte. Il est noir. WRONG The oven is open. It is new. Le four est ouverte. Elle est neuve. WRONG The door is open. It is new. La porte est ouverte. Elle est neuve. CORRECT Figure 1: Translation errors due to lack of co-reference resolution</context>
</contexts>
<marker>Sharp, 1988</marker>
<rawString>Randall Sharp. CAT2 – implementing a formalism for multi-lingual MT. In 2nd International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Language, pages 3–6, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
</authors>
<title>Centering, anaphora resolution, and discourse structure. Centering theory in discourse,</title>
<date>1998</date>
<pages>401--435</pages>
<marker>Walker, 1998</marker>
<rawString>M. A. Walker. Centering, anaphora resolution, and discourse structure. Centering theory in discourse, pages 401–435, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>A preferential, pattern-seeking, semantics for natural language inference. Words and Intelligence I,</title>
<date>1975</date>
<pages>83--102</pages>
<marker>Wilks, 1975</marker>
<rawString>Y. Wilks. A preferential, pattern-seeking, semantics for natural language inference. Words and Intelligence I, pages 83–102, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Understanding natural language.</title>
<date>1972</date>
<journal>Cognitive Psychology,</journal>
<volume>3</volume>
<issue>1</issue>
<marker>Winograd, 1972</marker>
<rawString>T. Winograd. Understanding natural language. Cognitive Psychology, 3(1):1–191, 1972.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>