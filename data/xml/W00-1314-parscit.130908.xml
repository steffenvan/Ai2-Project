<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016099">
<title confidence="0.9982575">
Word Alignment of English-Chinese Bilingual Corpus
Based on Chunks
</title>
<author confidence="0.994731">
Sun Le, Jin Youbing, Du Lin, Sun Yufang
</author>
<affiliation confidence="0.980786666666667">
Chinese Information Processing Center
Institute of Software
Chinese Academy of Sciences
</affiliation>
<address confidence="0.6100705">
Beijing 100080
P. R. China
</address>
<email confidence="0.909491">
lesun, ybjin, yfsun, ldu@sonata.iscas.ac.cn
</email>
<sectionHeader confidence="0.996374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930461538462">
In this paper, a method for the word alignment
of English-Chinese corpus based on chunks is
proposed. The chunks of English sentences are
identified firstly. Then the chunk boundaries of
Chinese sentences are predicted by the
translations of English chunks and heuristic
information. The ambiguities of Chinese chunk
boundaries are resolved by the coterminous
words in English chunks. With the chunk
aligned bilingual corpus, a translation relation
probability is proposed to align words. Finally,
we evaluate our system by real corpus and
present the experiment results.
</bodyText>
<keyword confidence="0.9611355">
Key Words: Word Alignment, Chunk Alignment,
Bilingual Corpus, Lexicon Extraction
</keyword>
<sectionHeader confidence="0.999852" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999834338983051">
With the easier access to bilingual corpora, there
is a tendency in NLP community to process and
refine the bilingual corpora, which can serve as
the knowledge base in support of many NLP
applications, such as automatic or human-aid
translation, multilingual terminology and
lexicography, multilingual information retrieval
system, etc.
Different NLP applications need different
bilingual corpora, which are aligned at different
level. They can be divided by the nature of the
segment to section level, paragraph level,
sentence level, phrase level, word level, byte
level, etc.
As for our applications, we choose the chunk
level to do alignment based on following
considerations. Firstly, our applications, which
include an example-based machine translation
system, a computer aid translation system and a
multilingual information retrieval system, need
the alignment below the sentence level, on
which we can acquire bilingual word and phrase
dictionaries and other useful translation
information. Secondly, the word level alignment
between English and Chinese language is
difficult to deal with. There are no cognate
words. The change in Chinese word order and
word POS always produce many null and
mistake correspondences. Next, we observe the
phenomenon that when we translate the English
sentence to Chinese sentence, all the words in
one English chunk tend to be translated as one
block of Chinese words which are coterminous.
The word orders within these blocks tend to
keep with the English chunk, also. So there are
stronger boundaries between chunks than
between words when we translate texts. Finally,
as we all known, chunk has been assigned
syntactic structure (Steven Abney, 1991), which
comprises a connected sub-graph of the
sentence&apos;s parse tree. So it&apos;s possible to align
sentence structure and obtain translation
grammars based on chunks by parsing.
Many researchers have studied the text
alignment problem and a number of quite
encouraging results have been reported to
different level alignments. With
sentence-aligned corpus ready in hand, we focus
our attention on the intra-sentence alignment
between the sentence pairs. In this paper, a
method for the word alignment of
English-Chinese corpus based on chunks is
proposed. The chunks of English sentences are
identified firstly. Then the chunk boundaries of
Chinese sentences are predicted by the bilingual
lexicon and synonymy Chinese dictionary and
heuristic information. The ambiguities of
Chinese chunk boundaries are resolved by the
coterminous words in English chunks. With the
</bodyText>
<page confidence="0.995999">
110
</page>
<bodyText confidence="0.999934">
chunk aligned bilingual corpus, a translation
relation probability is proposed to align words.
Although this paper is related to
English-Chinese word alignment, the idea can
be used to any other language bilingual corpora.
In the following sections, we first present a brief
review of related work in word alignment. Then
discuss our alignment algorithm based on
chunks in detail. Following this is an analysis of
our experimental results. Finally, we close our
paper with a discussion of future work.
</bodyText>
<sectionHeader confidence="0.999869" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998130708333334">
There are basically two kinds of approaches on
word alignment: the statistical-based approaches
(Brown et. al., 1990; Gale &amp; Church, 1991;
Dagan et. al. 1993; Chang, 1994), and the
lexicon-based approaches (Ker &amp; Chang, 1997;
Wang et. al., 1999).
Several translation models based on word
alignment are built by Brown et al. (1990) in
order to implement the English-French
statistical machine translation. The probabilities,
such as translation probability, fertility
probability, distortion probability, are estimated
by EM algorithm. The ,C2 measure is used by
Gale &amp; Church (1991) to align partial words.
Dagan (1993) uses an improved Brown model to
align the words for texts including OCR noise.
They first align word partially by character
string matching. Then use the translation model
to align words. Chang (1994) uses the POS
probability rather than translation probability in
Brown model to align the English-Chinese POS
tagged corpus. Ker &amp; Chang (1997) propose an
approach to align Chinese English corpus based
on semantic class. There are two semantic
classes are used in their model. One is the
semantic class of Longman lexicon of
contemporary English, the other is synonymy
Chinese dictionary. The semantic class rules of
translation between Chinese and English are
extracted from large-scale training corpus. Then
Chinese and English words are aligned by these
rules. Wang (1999) also uses the lexicons to
align the Chinese English bilingual corpus. His
model is based on bilingual lexicon, sense
similarity and location distortion probability.
The statistical-based approaches need complex
training and are sensitive to training data. It&apos;s a
pity that almost no linguistic knowledge is used
in these approaches. The lexicon-based
approaches seem simplify the word alignment
problem and can&apos;t obtain much translation
information above word level. To combine these
two approaches in a better way is the direction
in near future. In this paper we proposed a
method to align the bilingual corpus base on
chunks. The linguistic knowledge such as POS
tag and Chunk tag are used in a simply
statistical model.
</bodyText>
<sectionHeader confidence="0.919324" genericHeader="method">
3 Alignment Algorithm
</sectionHeader>
<subsectionHeader confidence="0.985297">
3.1 Outline of Algorithm
</subsectionHeader>
<bodyText confidence="0.998150179487179">
For our procedure in this paper, the bilingual
corpus has been aligned at the sentence level,
and the English language texts have been tagged
with POS tag, and the Chinese language texts
have been segmented and tagged with POS tag.
We have available a bilingual lexicon which
lists typical translation for many of the words in
the corpus. We have available a synonymy
Chinese dictionary, also. We identify the chunks
of English sentences and then predict the chunk
boundaries of Chinese sentences from the
translation of every English chunks and
heuristic information by use of the bilingual
lexicon. The ambiguities of Chinese chunk
boundaries are resolved by the coterminous
words in English chunks. After produce the
word candidate sets by statistical method, we
calculate the translation relation probability
between every word pair and select the best
alignment forms. The detail algorithm for word
alignment is given in table 1.
Step 1: According to the definition of Chunk in
English, separate the English sentence into
a few chunks and labeled with order
number from left to right.
Step 2: Try to fmd the Chinese translation of
every English chunk created in step 1 by
bilingual dictionary and synonymy Chinese
dictionary. If the Chinese translation is find,
then label the Chinese words with the same
number used for the English chunk in step
1.
Step 3: Disambiguate the multi-label Chinese
words by the translation location of
coterminous words within the same English
chunk.
Step 4: Separate the Chinese sentence into a few
chunks by heuristic information.
Step 5: Save all the alignment at chunk level in
</bodyText>
<page confidence="0.993797">
111
</page>
<bodyText confidence="0.998307333333333">
whole corpus as a base for word alignment.
Step 6: Produce the word candidate sets by
statistical method.
Step 7: Calculate the translation relation
probability between every word and it&apos;s
candidate translation words.
Step 8: Select the best translation by comparing
the total TRP value in different alignment
forms.
</bodyText>
<tableCaption confidence="0.987532">
Table 1. Outline of Alignment Algorithm
</tableCaption>
<subsectionHeader confidence="0.992744">
3.2 Chunk Identifying of English
Sentence
</subsectionHeader>
<bodyText confidence="0.999951454545455">
Following Steven Abney (1991), there are two
separate stages in chunking parser, which is the
chunker and the attacher. The chunker converts
a stream of words into a stream of chunks, and
the attacher converts the stream of chunks into a
stream of sentences. So only the chunker is
needed in this paper. It&apos;s a non-deterministic
version of a LR parser. For detail about chunker
and the used grammars, please see Abney
(1991). Then the chunks in one sentence are
labeled with order number from left to right.
</bodyText>
<subsectionHeader confidence="0.99947">
3.3 Chunk Boundary Prediction of
Chinese Sentence
</subsectionHeader>
<bodyText confidence="0.999778934782608">
We observe the phenomenon that when we
translate the English sentence to Chinese
sentence, all the words in one English chunk
tend to be translated as one block of Chinese
words that are coterminous. The word orders
within these blocks tend to keep with the
English chunk, also. There are three examples in
figure 1. The first sentence pair is chosen from
an example sentence of Abney (1991). The
second sentence pair is from a computer
handbook. In these sentence pair all English
chunks can find the exactly Chinese Chunk. In
the third sentence pair only one English chunk
can&apos;t find the exactly Chinese chunk for this
sentence is chosen from a story and the
translation is not literally.
In order to find the Chinese translation of every
English chunk, we use the bilingual dictionary
and synonymy Chinese dictionary to implement
the matching. If the Chinese translation of any
words within the English chunk is found, then
label the Chinese word with the same number
used for labeling the English chunk.
If there are Chinese words, which are labeled
simultaneously by two or more number of
English chunks, we use the number of nearby
Chinese words to disambiguate. For example, in
figure 2, the first Chinese word MEP may be
correspondent to the English chunk 5 or 7. We
have known that the words in one English chunk
tend to be translated as one block of Chinese
words that are coterminous. So it&apos;s easy to
decide the first Chinese word t5ti fifi is
correspondent to the English chunk 7, the second
Chinese word IS.ci RJ is correspondent to the
English chunk 5. By the same way, we can find
the correct translations of Chinese word P,M=
and is English chunk 6 and chunk 8
respectively. In Step 4 of figure 2, the Chinese
words with the same label number are bracketed
with in one chunk. Finally, we separate the
Chinese sentence into a few chunks by heuristic
information based on POS tag (especially the
preposition, conjunction, and auxiliary words)
and the grammatical knowledge-base of
contemporary Chinese (Yu sin wen, 1998).
</bodyText>
<figure confidence="0.980159444444444">
[The bald [was sitting] [on his suitcase].
ON A] [ElE] [ft IYAn&apos;±]
[To acce s [detailed information] [ about SC
supportserv_isssL[_click] [o
&amp;quot;Su ore&apos;].
[SCO .141JRA-M] Ei
[I gathered] [from what they said] ,[that an elder sister] [of his] [ was coming] [to stay with them],[ and
that she ex ed] [ that evenin
UA4t1tnAitr41] [40)][-- AR ][*][f;flaffil_VE-V,
</figure>
<figureCaption confidence="0.999578">
Figure 1. Three Examples of Chunk Alignment
</figureCaption>
<page confidence="0.995125">
112
</page>
<bodyText confidence="0.97632375">
Step 1 English chunks with order number
[This product 1] [is designed 2] for [low-cost 3], [turnkey solutions 4] and [mission-critical
applications 5] that [require 6] [a central application host 7] and [ do not require 8] [networking 9].
Step 2 Label the translation of English chunk with it&apos;s order number
</bodyText>
<figure confidence="0.347011666666667">
(1) NI (1) A t ,RX(6/5) -e(7) 4,&apos;C(7) )W (5/7) ±V1(7) ThiT (8) (6/8) IR
IJ 1147 (9) &apos;a (3) A* (3) A 17CAtin !WA (4) (4) 2 &apos;f.t (5) 11* AID:0 (5/7) W
In (2)
Step 3 Disambiguate the multi-label Chinese words
1,(1) ?` rfil (1) ):2EI RX(6) -it(7) t11(7) T(8) x (8) UM(9)
&apos;1(3) A* (3). TITAITIM ig-siA(4) 4(4) ilft(5) q* 041(5) -R-m-n(2).
Step 4. Separate the Chinese sentence into a few chunks
a (0] A t [R(6)] [—ft r=1:1&apos;b IVO .±411. (7)] W CT M (8)] UM In (9)1 ME A
* (3)1. [-p% ig.‘1A •-A(4)] St [Alit 414 5,1141(5)J W [ait In (2)]
</figure>
<figureCaption confidence="0.999298">
Figure 2. An Example for Chunk Alignment Algorithm from Step 1 to 4
</figureCaption>
<subsectionHeader confidence="0.9920975">
3.4 Calculation of Translation Relation
Probability for Words
</subsectionHeader>
<bodyText confidence="0.9999266">
With the alignments at chunk level of whole
corpus, we propose a Translation Relation
Probability (TRP) to implement the word
alignment. The translation Relation probability
of words are given by following equation:
</bodyText>
<equation confidence="0.906464">
2
P
= ec
er fe fe
</equation>
<bodyText confidence="0.9984575">
Where fe is the frequency of English word in
whole corpus; fe is the frequency of Chinese
Word in whole corpus; fee is calculated by
follow equation:
(2)
Where LAV is the average words number of all
English chunks and all Chinese chunks which
are related to the English word in whole Corpus;
Le, is the word number of the English chunk in
which the English candidate words co-occur
with the Chinese words; Lei is the word number
of the Chinese chunk in which the English
candidate words co-occur with the Chinese
words; N is the total number of chunks in which
the English word co-occur with the Chinese
word; 13ce is the penalty value to indicate the
POS change between the English word and the
Chinese word.
By this equation we connect the chunk length
and POS change with the co-occurrence
frequency. The less the chunk length, the higher
the translation relation probability. For example,
the chunk pair, which is composed by one
English word and two Chinese words, is more
reliable than the chunk pair, which is composed
by four English words and four Chinese words.
An example is given in figure 3. There are 5
possible alignment forms in our consideration
for this chunk, which includes three English
words and three Chinese words. Then calculate
the total TRP value for every possible alignment
word pairs in each alignment form by equation
(1). After we get the total TRP value for each
alignment form, we choose the biggest one.
</bodyText>
<figure confidence="0.941240352941176">
ln( 2LAV ) in(L, Av )
Lei + Lei
X
13cc
ln(LAv )
(1)
fee =El
floppy disk drive
A
k;
A
floppy disk drive floppy disk drive
\
VA Eft VA 4E-0 fg
floppy disk drive floppy disk drive
IN
Vfit Eit V Eft
</figure>
<figureCaption confidence="0.999585">
Figure 3. The Possible Word Alignment Forms in One Chunk
</figureCaption>
<page confidence="0.998186">
113
</page>
<sectionHeader confidence="0.966486" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.863229">
4.1 System Architecture
</subsectionHeader>
<figure confidence="0.999050833333333">
Retrieval Results
User&apos;s Languages
Inquiry
Tagged
English
corpus
Chunk
Identifying
Chunk
Tagged
Sentence
Chunk
Tagged
Sentence
Source Text
Example Based Machine
Translation System
Target Text
Computer Aid
Translation System
Segmented and
Rule for Part-of-Speech tagged Chinese
Corpus
Word Dictionary
Grammar Rule for
Chunk Constructing
Heuristic information for
Chunk Constructing
Multilingual Information
Retrieval System
</figure>
<figureCaption confidence="0.999554">
Figure 4. System Architecture
</figureCaption>
<page confidence="0.994645">
114
</page>
<subsectionHeader confidence="0.995442">
4.2 Experiment Results
</subsectionHeader>
<bodyText confidence="0.999933823529412">
We tested our system with an English-Chinese
bilingual corpus, which is part of a computer
handbook (Sco Unix handbook). There are
about 2246 English sentence and 2169 Chinese
sentence in this computer handbook after filter
noisy figures and tables. Finally we extracted
14,214 chunk pairs from the corpus. The
accuracy for automatic chunk alignment is
85.7%. The accuracy for word alignment based
on correctly aligned chunk pairs is 93.6%. The
errors mainly due to the following reasons:
Chinese segmentation error, stop words noise,
POS tag error. The parameter 13ec we used in
equation (2) should be chosen from the training
corpus. In table 2, the total TRP values of
example in figure 3 are showed. The alignment
form D is the best.
</bodyText>
<table confidence="0.976384454545455">
(floppy I V4) 0.9444 X 1/3 Total TRP of A =0.3792
(disk 1 Eft) 0.0212 X 1/3
(drivel ii) 0.1722 X 1/3
(floppy 1 Vii. EM) 0.2857 X 1/2 Total TRP of B =0.3194
(disk drive 1 ) 0.1765 X 1/2
(floppy I U ) 0.9444 X 1/2 Total TRP of C =0.6485
(disk drive I EiVi 4) 0.3529 X 1/2
(floppy disk I Vg) 0.8333 X 1/2 Total TRP of D =0.8640
(drive I E VI ) 0.8947 X 1/2
(floppy disk 1 VA 4E43) 0.3429 X 1/2 Total TRP of E =0.2576
(drive I 5) 0.1722 X 1/2
</table>
<tableCaption confidence="0.999627">
Table 2. Total TRP Value for Example in Figure 3
</tableCaption>
<sectionHeader confidence="0.997844" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99997028125">
With the more and more bilingual corpora, there
is a tendency in NLP community to process and
refine the bilingual corpora, which can serve as
the knowledge base in support of many NLP
applications. In this paper, a method for the
word alignment of English-Chinese corpus
based on chunks is presented. After identified
the chunks of English sentences, we predict the
chunk boundaries of Chinese sentences by the
bilingual lexicon, synonymy Chinese dictionary
and heuristic information. The ambiguities of
Chinese chunk boundaries are resolved by the
coterminous words in English chunks. After
produce the word candidate sets by statistical
method, we calculate the translation relation
probability between every word pair and select
the best alignment forms. We evaluate our
system by real corpus and present the results.
Although the results we got are quite promising
to bilingual English Chinese text, there are still
much to do in near future. The corpus we use in
our experiment is a relative small corpus about
computer handbook, in which the terms are
translated with high consistency. We should
extend our method to the large corpus of other
domains without lost much accuracy. To
increase the correct rate of Chinese word
segmentation is important for our word
alignment. To extract the corresponding syntax
information of English Chinese bilingual corpus
by shallow parsing is a direction for future work,
also.
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99968775">
This research was, funded by Natural Science
Foundation of China (Grant No. 69983009).
The authors would like to thank the anonymous
reviewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9989382">
Abney, Steven, 1991. Parsing by Chunks. In: Robert
Berwick, Steven Abney and Carol Tenny (eds.),
Pringciple-Based Parsing, Kluwer Academic
Publishers
Brown, P. F., Della Pietra, S. A., Della Pietra, V., J.,
and Mercer, R. L., 1993. The Mathematics of
Statistical Machine Translation: Parameter
Estimation. In .Computational Linguistics, 19(2),
pp.263-311.
Chang, J. S., and Chen, M. H. C. 1994 Using Partial
</reference>
<page confidence="0.989822">
115
</page>
<reference confidence="0.996076450980392">
Aligned Parallel Text and Part-of-speech
Information in Word Alignment. In Proceedings of
the First Conference of the Association for
Machine Translation in the Americas(AMTA94),
pp 16-23
Dagan, I. and Church, K. W. 1994 Termight:
Identifying and Translating. Technical
terminology. In Proceedings of EACL
Fung, P., and Church, K. W., 1994. K-vec: A New
Approach for Aligning Parallel Texts. In
Proceedings of the 15th International Conference
on Computational Linguistics (COLING94),
Japan, pp. 1096-1102,
Gale, W. A., and Church, K. W., 1991. A Program for
Aligning Sentences in Bilingual Corpora. In
Proceedings of the 29th Annual Meeting of the
Association for Computational Linguistics
(ACL91), pp. 177-184
Kay, M., and Roscheisen M., 1993. Text-Translation
Alignment. Computational Linguistics,
19/1,pp.121
Ker, M. and Chang, J. S. 1997 A Class-Based
Approach to Word Alignment. Computational
Linguistics,23(2),pp 313-343
Langlais, Ph., Simard , M., Veronis, J., Armstong , S.,
Bonhomme, P., Debili, F., Isabelle, P., Souissi ,
and Theron, P., 1998. Arcade: A cooperative
research project on parallel text alignment
evaluation. In First International Conference on
Language Resources and Evaluation, Granada,
Spain.
Melamed, I. D. 1996. Automatic Detection of
Omissions in Translations. In Proceedings of the
16th International Conference on Computational
Linguistics, Copenhagen, Denmark
Sun, Le, Du, Lin, Sun, Yufang, Jin, Youbin 1999
Sentence Alignment of English-Chinese Complex
Bilingual Corpora. Proceeding of the workshop
MAL99, 135-139
Wang, Bin, Liu, Qun, and Zhang, Xiang, 1999 An
Automatic Chinese-English Word Alignment
System. Proceedings of ICMI99, pp100-104,
Hong Kong
Wu, Daikai.and Xia, Xuanyin. 1995. Large-Scale
Automatic Extraction of an English-Chinese
translation Lexicon. Machine Translation,
9:3-4,285-313
Yu, Shiwen, Thu, Xuefeng, Wang, Hui, Zhang
Yunyun, 1998 The Grammatical Knowledge-base
of Contemporary Chinese: A complete
Specification. Tsinghua University Publishers
</reference>
<page confidence="0.999029">
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.421096">
<title confidence="0.9983415">Word Alignment of English-Chinese Bilingual Corpus Based on Chunks</title>
<author confidence="0.996086">Sun Le</author>
<author confidence="0.996086">Jin Youbing</author>
<author confidence="0.996086">Du Lin</author>
<author confidence="0.996086">Sun</author>
<affiliation confidence="0.8889365">Chinese Information Processing Institute of</affiliation>
<title confidence="0.7203185">Chinese Academy of Beijing</title>
<author confidence="0.993401">P R China</author>
<email confidence="0.994181">lesun,ybjin,yfsun,ldu@sonata.iscas.ac.cn</email>
<abstract confidence="0.999957357142857">In this paper, a method for the word alignment of English-Chinese corpus based on chunks is proposed. The chunks of English sentences are identified firstly. Then the chunk boundaries of Chinese sentences are predicted by the translations of English chunks and heuristic information. The ambiguities of Chinese chunk boundaries are resolved by the coterminous words in English chunks. With the chunk aligned bilingual corpus, a translation relation probability is proposed to align words. Finally, we evaluate our system by real corpus and present the experiment results.</abstract>
<keyword confidence="0.996321">Key Words: Word Alignment, Chunk Alignment,</keyword>
<intro confidence="0.874421">Bilingual Corpus, Lexicon Extraction</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Parsing by Chunks.</title>
<date>1991</date>
<editor>In: Robert Berwick, Steven Abney and Carol Tenny (eds.), Pringciple-Based Parsing,</editor>
<publisher>Kluwer Academic Publishers</publisher>
<contexts>
<context position="2638" citStr="Abney, 1991" startWordPosition="392" endWordPosition="393">t to deal with. There are no cognate words. The change in Chinese word order and word POS always produce many null and mistake correspondences. Next, we observe the phenomenon that when we translate the English sentence to Chinese sentence, all the words in one English chunk tend to be translated as one block of Chinese words which are coterminous. The word orders within these blocks tend to keep with the English chunk, also. So there are stronger boundaries between chunks than between words when we translate texts. Finally, as we all known, chunk has been assigned syntactic structure (Steven Abney, 1991), which comprises a connected sub-graph of the sentence&apos;s parse tree. So it&apos;s possible to align sentence structure and obtain translation grammars based on chunks by parsing. Many researchers have studied the text alignment problem and a number of quite encouraging results have been reported to different level alignments. With sentence-aligned corpus ready in hand, we focus our attention on the intra-sentence alignment between the sentence pairs. In this paper, a method for the word alignment of English-Chinese corpus based on chunks is proposed. The chunks of English sentences are identified </context>
<context position="8239" citStr="Abney (1991)" startWordPosition="1269" endWordPosition="1270">coterminous words within the same English chunk. Step 4: Separate the Chinese sentence into a few chunks by heuristic information. Step 5: Save all the alignment at chunk level in 111 whole corpus as a base for word alignment. Step 6: Produce the word candidate sets by statistical method. Step 7: Calculate the translation relation probability between every word and it&apos;s candidate translation words. Step 8: Select the best translation by comparing the total TRP value in different alignment forms. Table 1. Outline of Alignment Algorithm 3.2 Chunk Identifying of English Sentence Following Steven Abney (1991), there are two separate stages in chunking parser, which is the chunker and the attacher. The chunker converts a stream of words into a stream of chunks, and the attacher converts the stream of chunks into a stream of sentences. So only the chunker is needed in this paper. It&apos;s a non-deterministic version of a LR parser. For detail about chunker and the used grammars, please see Abney (1991). Then the chunks in one sentence are labeled with order number from left to right. 3.3 Chunk Boundary Prediction of Chinese Sentence We observe the phenomenon that when we translate the English sentence t</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Abney, Steven, 1991. Parsing by Chunks. In: Robert Berwick, Steven Abney and Carol Tenny (eds.), Pringciple-Based Parsing, Kluwer Academic Publishers</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>Della Pietra</author>
<author>S A</author>
<author>Della Pietra</author>
<author>J V</author>
<author>R L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>In .Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>Brown, Pietra, A, Pietra, V, Mercer, 1993</marker>
<rawString>Brown, P. F., Della Pietra, S. A., Della Pietra, V., J., and Mercer, R. L., 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. In .Computational Linguistics, 19(2), pp.263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Chang</author>
<author>M H C Chen</author>
</authors>
<title>Using Partial Aligned Parallel Text and Part-of-speech Information in Word Alignment.</title>
<date>1994</date>
<booktitle>In Proceedings of the First Conference of the Association for Machine Translation in the Americas(AMTA94),</booktitle>
<pages>16--23</pages>
<marker>Chang, Chen, 1994</marker>
<rawString>Chang, J. S., and Chen, M. H. C. 1994 Using Partial Aligned Parallel Text and Part-of-speech Information in Word Alignment. In Proceedings of the First Conference of the Association for Machine Translation in the Americas(AMTA94), pp 16-23</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>K W Church</author>
</authors>
<title>Termight: Identifying and Translating. Technical terminology.</title>
<date>1994</date>
<booktitle>In Proceedings of EACL</booktitle>
<marker>Dagan, Church, 1994</marker>
<rawString>Dagan, I. and Church, K. W. 1994 Termight: Identifying and Translating. Technical terminology. In Proceedings of EACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>K W Church</author>
</authors>
<title>K-vec: A New Approach for Aligning Parallel Texts.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING94), Japan,</booktitle>
<pages>1096--1102</pages>
<marker>Fung, Church, 1994</marker>
<rawString>Fung, P., and Church, K. W., 1994. K-vec: A New Approach for Aligning Parallel Texts. In Proceedings of the 15th International Conference on Computational Linguistics (COLING94), Japan, pp. 1096-1102,</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>A Program for Aligning Sentences in Bilingual Corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (ACL91),</booktitle>
<pages>177--184</pages>
<contexts>
<context position="4160" citStr="Gale &amp; Church, 1991" startWordPosition="622" endWordPosition="625">ranslation relation probability is proposed to align words. Although this paper is related to English-Chinese word alignment, the idea can be used to any other language bilingual corpora. In the following sections, we first present a brief review of related work in word alignment. Then discuss our alignment algorithm based on chunks in detail. Following this is an analysis of our experimental results. Finally, we close our paper with a discussion of future work. 2 Related Work There are basically two kinds of approaches on word alignment: the statistical-based approaches (Brown et. al., 1990; Gale &amp; Church, 1991; Dagan et. al. 1993; Chang, 1994), and the lexicon-based approaches (Ker &amp; Chang, 1997; Wang et. al., 1999). Several translation models based on word alignment are built by Brown et al. (1990) in order to implement the English-French statistical machine translation. The probabilities, such as translation probability, fertility probability, distortion probability, are estimated by EM algorithm. The ,C2 measure is used by Gale &amp; Church (1991) to align partial words. Dagan (1993) uses an improved Brown model to align the words for texts including OCR noise. They first align word partially by cha</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>Gale, W. A., and Church, K. W., 1991. A Program for Aligning Sentences in Bilingual Corpora. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (ACL91), pp. 177-184</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
<author>M Roscheisen</author>
</authors>
<title>Text-Translation Alignment. Computational Linguistics,</title>
<date>1993</date>
<marker>Kay, Roscheisen, 1993</marker>
<rawString>Kay, M., and Roscheisen M., 1993. Text-Translation Alignment. Computational Linguistics, 19/1,pp.121</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ker</author>
<author>J S Chang</author>
</authors>
<title>A Class-Based Approach to Word Alignment.</title>
<date>1997</date>
<journal>Computational</journal>
<volume>23</volume>
<issue>2</issue>
<pages>313--343</pages>
<contexts>
<context position="4247" citStr="Ker &amp; Chang, 1997" startWordPosition="636" endWordPosition="639">d to English-Chinese word alignment, the idea can be used to any other language bilingual corpora. In the following sections, we first present a brief review of related work in word alignment. Then discuss our alignment algorithm based on chunks in detail. Following this is an analysis of our experimental results. Finally, we close our paper with a discussion of future work. 2 Related Work There are basically two kinds of approaches on word alignment: the statistical-based approaches (Brown et. al., 1990; Gale &amp; Church, 1991; Dagan et. al. 1993; Chang, 1994), and the lexicon-based approaches (Ker &amp; Chang, 1997; Wang et. al., 1999). Several translation models based on word alignment are built by Brown et al. (1990) in order to implement the English-French statistical machine translation. The probabilities, such as translation probability, fertility probability, distortion probability, are estimated by EM algorithm. The ,C2 measure is used by Gale &amp; Church (1991) to align partial words. Dagan (1993) uses an improved Brown model to align the words for texts including OCR noise. They first align word partially by character string matching. Then use the translation model to align words. Chang (1994) use</context>
</contexts>
<marker>Ker, Chang, 1997</marker>
<rawString>Ker, M. and Chang, J. S. 1997 A Class-Based Approach to Word Alignment. Computational Linguistics,23(2),pp 313-343</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bonhomme</author>
<author>P Debili</author>
<author>F Isabelle</author>
<author>P Souissi</author>
</authors>
<title>Arcade: A cooperative research project on parallel text alignment evaluation.</title>
<date>1998</date>
<booktitle>In First International Conference on Language Resources and Evaluation,</booktitle>
<location>Granada,</location>
<marker>Bonhomme, Debili, Isabelle, Souissi, 1998</marker>
<rawString>Langlais, Ph., Simard , M., Veronis, J., Armstong , S., Bonhomme, P., Debili, F., Isabelle, P., Souissi , and Theron, P., 1998. Arcade: A cooperative research project on parallel text alignment evaluation. In First International Conference on Language Resources and Evaluation, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Automatic Detection of Omissions in Translations.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<location>Copenhagen, Denmark</location>
<marker>Melamed, 1996</marker>
<rawString>Melamed, I. D. 1996. Automatic Detection of Omissions in Translations. In Proceedings of the 16th International Conference on Computational Linguistics, Copenhagen, Denmark</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Sun</author>
<author>Lin Du</author>
</authors>
<date>1999</date>
<booktitle>Sentence Alignment of English-Chinese Complex Bilingual Corpora. Proceeding of the workshop MAL99,</booktitle>
<pages>135--139</pages>
<location>Sun, Yufang, Jin, Youbin</location>
<marker>Sun, Du, 1999</marker>
<rawString>Sun, Le, Du, Lin, Sun, Yufang, Jin, Youbin 1999 Sentence Alignment of English-Chinese Complex Bilingual Corpora. Proceeding of the workshop MAL99, 135-139</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Wang</author>
<author>Qun Liu</author>
<author>Xiang Zhang</author>
</authors>
<title>An Automatic Chinese-English Word Alignment System.</title>
<date>1999</date>
<booktitle>Proceedings of ICMI99,</booktitle>
<pages>100--104</pages>
<location>Hong Kong</location>
<marker>Wang, Liu, Zhang, 1999</marker>
<rawString>Wang, Bin, Liu, Qun, and Zhang, Xiang, 1999 An Automatic Chinese-English Word Alignment System. Proceedings of ICMI99, pp100-104, Hong Kong</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daikai and Xia Wu</author>
<author>Xuanyin</author>
</authors>
<title>Large-Scale Automatic Extraction of an English-Chinese translation Lexicon.</title>
<date>1995</date>
<journal>Machine Translation,</journal>
<pages>9--3</pages>
<marker>Wu, Xuanyin, 1995</marker>
<rawString>Wu, Daikai.and Xia, Xuanyin. 1995. Large-Scale Automatic Extraction of an English-Chinese translation Lexicon. Machine Translation, 9:3-4,285-313</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwen Yu</author>
<author>Xuefeng Thu</author>
<author>Hui Wang</author>
<author>Zhang Yunyun</author>
</authors>
<title>The Grammatical Knowledge-base of Contemporary Chinese: A complete Specification.</title>
<date>1998</date>
<publisher>Tsinghua University Publishers</publisher>
<marker>Yu, Thu, Wang, Yunyun, 1998</marker>
<rawString>Yu, Shiwen, Thu, Xuefeng, Wang, Hui, Zhang Yunyun, 1998 The Grammatical Knowledge-base of Contemporary Chinese: A complete Specification. Tsinghua University Publishers</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>