<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000065">
<title confidence="0.839453">
Modelling selectional preferences in a lexical hierarchy
</title>
<author confidence="0.898411">
Diarmuid O´ S´eaghdha
</author>
<affiliation confidence="0.9656555">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.651724">
Cambridge, UK
</address>
<email confidence="0.98734">
do242@cam.ac.uk
</email>
<author confidence="0.9885">
Anna Korhonen
</author>
<affiliation confidence="0.992943">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.655239">
Cambridge, UK
</address>
<email confidence="0.994677">
Anna.Korhonen@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.996596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994441875">
This paper describes Bayesian selectional
preference models that incorporate knowledge
from a lexical hierarchy such as WordNet. In-
spired by previous work on modelling with
WordNet, these approaches are based either on
“cutting” the hierarchy at an appropriate level
of generalisation or on a “walking” model that
selects a path from the root to a leaf. In
an evaluation comparing against human plau-
sibility judgements, we show that the mod-
els presented here outperform previously pro-
posed comparable WordNet-based models, are
competitive with state-of-the-art selectional
preference models and are particularly well-
suited to estimating plausibility for items that
were not seen in training.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942959183673">
The concept of selectional preference captures the
intuitive fact that predicates in language have a bet-
ter semantic “fit” for certain arguments than oth-
ers. For example, the direct object argument slot
of the verb eat is more plausibly filled by a type
of food (I ate a pizza) than by a type of vehicle (I
ate a car), while the subject slot of the verb laugh
is more plausibly filled by a person than by a veg-
etable. Human language users’ knowledge about
selectional preferences has been implicated in anal-
yses of metaphor processing (Wilks, 1978) and in
psycholinguistic studies of comprehension (Rayner
et al., 2004). In Natural Language Processing, au-
tomatically acquired preference models have been
shown to aid a number of tasks, including semantic
role labelling (Zapirain et al., 2009), parsing (Zhou
et al., 2011) and lexical disambiguation (Thater et
al., 2010; O´ S´eaghdha and Korhonen, 2011).
It is tempting to assume that with a large enough
corpus, preference learning reduces to a simple lan-
guage modelling task that can be solved by counting
predicate-argument co-occurrences. Indeed, Keller
and Lapata (2003) show that relatively good perfor-
mance at plausibility estimation can be attained by
submitting queries to a Web search engine. How-
ever, there are many scenarios where this approach
is insufficient: for languages and language domains
where Web-scale data is unavailable, for predicate
types (e.g., inference rules or semantic roles) that
cannot be retrieved by keyword search and for ap-
plications where accurate models of rarer words are
required. O´ S´eaghdha (2010) shows that the Web-
based approach is reliably outperformed by more
complex models trained on smaller corpora for less
frequent predicate-argument combinations. Models
that induce a level of semantic representation, such
as probabilistic latent variable models, have a further
advantage in that they can provide rich structured in-
formation for downstream tasks such as lexical dis-
ambiguation ( O´ S´eaghdha and Korhonen, 2011) and
semantic relation mining (Yao et al., 2011).
Recent research has investigated the potential
of Bayesian probabilistic models such as Latent
Dirichlet Allocation (LDA) for modelling selec-
tional preferences ( O´ S´eaghdha, 2010; Ritter et al.,
2010; Reisinger and Mooney, 2011). These mod-
els are flexible and robust, yielding superior perfor-
mance compared to previous approaches. In this
paper we present a preliminary study of analogous
</bodyText>
<page confidence="0.961516">
170
</page>
<note confidence="0.9725765">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 170–179,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999788">
models that make use of a lexical hierarchy (in our
case the WordNet hierarchy). We describe two broad
classes of probabilistic models over WordNet and
how they can be implemented in a Bayesian frame-
work. The two main potential advantages of in-
corporating WordNet information are: (a) improved
predictions about rare and out-of-vocabulary argu-
ments; (b) the ability to perform syntactic word
sense disambiguation with a principled probabilistic
model and without the need for an additional step
that heuristically maps latent variables onto Word-
Net senses. Focussing here on (a), we demon-
strate that our models attain better performance than
previously-proposed WordNet-based methods on a
plausibility estimation task and are particularly well-
suited to estimating plausibility for arguments that
were not seen in training and for which LDA cannot
make useful predictions.
</bodyText>
<sectionHeader confidence="0.9289" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.9999618">
The WordNet lexical hierarchy (Fellbaum, 1998)
is one of the most-used resources in NLP, finding
many applications in both the definition of tasks (e.g.
the SENSEVAL/SemEval word sense disambigua-
tion tasks) and in the construction of systems. The
idea of using WordNet to define selectional prefer-
ences was first implemented by Resnik (1993), who
proposed a measure of associational strength be-
tween a semantic class s and a predicate p corre-
sponding to a relation type r:
</bodyText>
<equation confidence="0.989244">
1 A(s,p,r) = ηP(s|p,r)lo92 PP(IIr)) (1)
</equation>
<bodyText confidence="0.9999727">
where η is a normalisation term. This measure cap-
tures the degree to which the probability of seeing
s given the predicate p differs from the prior proba-
bility of s. Given that we are often interested in the
preference of p for a word w rather than a class and
words generally map onto multiple classes, Resnik
suggests calculating A(s, p, r) for all classes that
could potentially be expressed by w and predicting
the maximal value.
Cut-based models assume that modelling the se-
lectional preference of a predicate involves finding
the right “level of generalisation” in the WordNet
hierarchy. For example, the direct object slot of
the verb eat can be associated with the subhierarchy
rooted at the synset food#n#1, as all hyponyms of
that synset are assumed to be edible and the imme-
diate hypernym of the synset, substance#n#1, is too
general given that many substances are rarely eaten.1
This leads to the notion of “cutting” the hierarchy at
one or more positions (Li and Abe, 1998). The mod-
elling task then becomes that of finding the cuts that
are maximally general without overgeneralising. Li
and Abe (1998) propose a model in which the appro-
priate cut c is selected according to the Minimum
Description Length principle; this principle explic-
itly accounts for the trade-off between generalisa-
tion and accuracy by minimising a sum of model de-
scription length and data description length. The
probability of a predicate p taking as its argument
an synset s is modelled as:
</bodyText>
<equation confidence="0.999846">
Pla(s|p,r) = P(s|cs,p,r)P(c|p) (2)
</equation>
<bodyText confidence="0.999896">
where cs,p,r is the portion of the cut learned for p
that dominates s. The distribution P(s|cs,p,r) is held
to be uniform over all synsets dominated by cs,p,r,
while P(c|p) is given by a maximum likelihood es-
timate.
Clark and Weir (2002) present a model that, while
not explicitly described as cut-based, likewise seeks
to find the right level of generalisation for an obser-
vation. In this case, the hypernym at which to “cut”
is chosen by a chi-squared test: if the aggregate pref-
erence of p for classes in the subhierarchy rooted at c
differs significantly from the individual preferences
of p for the immediate children of c, the hierarchy is
cut below c. The probability of p taking a synset s
as its argument is given by:
</bodyText>
<equation confidence="0.998926333333333">
P(p|cs,p,r, r)P (s|r)
P(p |r)P(s, |r) (3 )
Es,ES P(p |cs,,p,r, r) P(p|r)
</equation>
<bodyText confidence="0.995566375">
where cs,p,r is the root node of the subhierarchy con-
taining s that was selected for p.
An alternative approach to modelling with Word-
Net uses its hierarchical structure to define a Markov
model with transitions from senses to senses and
from senses to words. The intuition here is that each
observation is generated by a “walk” from the root
of the hierarchy to a leaf node and emitting the word
</bodyText>
<footnote confidence="0.9900935">
1In this paper we use WordNet version 3.0, except where
stated otherwise.
</footnote>
<equation confidence="0.963728">
Pcw(s|p, r) =
</equation>
<page confidence="0.98441">
171
</page>
<bodyText confidence="0.999933638297873">
corresponding to the leaf. Abney and Light (1999)
proposed such a model for selectional preferences,
trained via EM, but failed to achieve competitive
performance on a pseudodisambiguation task.
The models described above have subsequently
been used in many different studies. For exam-
ple: McCarthy and Carroll (2003) use Li and Abe’s
method in a word sense disambiguation setting;
Schulte im Walde et al. (2008) use their MDL ap-
proach as part of a system for syntactic and seman-
tic subcategorisation frame learning; Shutova (2010)
deploys Resnik’s method for metaphor interpreta-
tion. Brockmann and Lapata (2003) report a com-
parative evaluation in which the methods of Resnik
and Clark and Weir outpeform Li and Abe’s method
on a plausibility estimation task.
Much recent work on preference learning has fo-
cused on purely distributional methods that do not
use a predefined hierarchy but learn to make general-
isations about predicates and arguments from corpus
observations alone. These methods can be vector-
based (Erk et al., 2010; Thater et al., 2010), dis-
criminative (Bergsma et al., 2008) or probabilistic
( O´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger
and Mooney, 2011). In the probabilistic category,
Bayesian models based on the “topic modelling”
framework (Blei et al., 2003b) have been shown to
achieve state-of-the-art performance in a number of
evaluation settings; the models considered in this pa-
per are also related to this framework.
In machine learning, researchers have proposed
a variety of topic modelling methods where the la-
tent variables are arranged in a hierarchical structure
(Blei et al., 2003a; Mimno et al., 2007). In con-
trast to the present work, these models use a rel-
atively shallow hierarchy (e.g., 3 levels) and any
hierarchy node can in principle emit any vocabu-
lary item; they thus provide a poor match for our
goal of modelling over WordNet. Boyd-Graber et
al. (2007) describe a topic model that is directly in-
fluenced by Abney and Light’s Markov model ap-
proach; this model (LDAWN) is described further in
Section 3.3 below. Reisinger and Pas¸ca (2009) in-
vestigate Bayesian methods for attaching attributes
harvested from the Web at an appropriate level in
the WordNet hierarchy; this task is related in spirit
to the preference learning task.
</bodyText>
<sectionHeader confidence="0.868889" genericHeader="method">
3 Probabilistic modelling over WordNet
</sectionHeader>
<subsectionHeader confidence="0.982777">
3.1 Notation
</subsectionHeader>
<bodyText confidence="0.9999985">
We assume that we have a lexical hierarchy in the
form of a directed acyclic graph G = (5, E) where
each node (or synset) s E 5 is associated with a
set of words Wn belonging to a large vocabulary V .
Each edge e E E leads from a node n to its children
(or hyponyms) Chn. As G is a DAG, a node may
have more than one parent but there are no cycles.
The ultimate goal is to learn a distribution over the
argument vocabulary V for each predicate p in a set
P, through observing predicate-argument pairs. A
predicate is understood to correspond to a pairing of
a lexical item v and a relation type r, for example
p = (eat, direct object). The list of observations
for a predicate p is denoted by Observations(p).
</bodyText>
<subsectionHeader confidence="0.992187">
3.2 Cut-based models
</subsectionHeader>
<bodyText confidence="0.861718">
Model 1 Generative story for WN-CUT
for cut c E {1 ... JCJ} do
</bodyText>
<equation confidence="0.907125625">
4bc — Multinomial(&amp;)
end for
for predicate p E {1... JPJ} do
Op — Dirichlet(α)
for argument instance i E Observations(p)
do
ci — Multinomial(Op)
wi — Multinomial(4bcz)
</equation>
<bodyText confidence="0.931149411764706">
end for
end for
The first model we consider, WN-CUT, is directly
inspired by Li and Abe’s model (2). Each predicate
p is associated with a distribution over “cuts”, i.e.,
complete subgraphs of G rooted at a single node
and containing all nodes dominated by the root. It
follows that the number of possible cuts is the same
as the number of synsets. Each cut c is associated
with a non-uniform distribution over the set of words
Wc that can be generated by the synsets contained
in c. As well as the use of a non-uniform emis-
sion distribution and the placing of Dirichlet priors
on the multinomial over cuts, a significant differ-
ence from Li and Abe’s model is that overlapping
cuts are permitted (indeed, every cut has non-zero
probability given a predicate). For example, the
</bodyText>
<page confidence="0.989198">
172
</page>
<bodyText confidence="0.9999042">
model may learn that the direct object slot of eat
gives high probability to the cut rooted at food#n#1
but also that the cut rooted at substance#n#1 has
non-negligible probability, higher than that assigned
to phenomenon#n#1. It follows that the estimated
probability of p taking argument w takes into ac-
count all possible cuts, weighted by their probability
given p.
The generative story for WN-CUT is given in Al-
gorithm 1; this describes the assumptions made by
the model about how a corpus of observations is gen-
erated. The probability of predicate p taking argu-
ment w is defined as (4); an empirical posterior esti-
mate of this quantity can be computed from a Gibbs
sampling state via (5):
</bodyText>
<equation confidence="0.9986754">
X
P(w|p) = P(c|p)P(w|c) (4)
c
fwc + β (5)
f·c +  |Wc|β
</equation>
<bodyText confidence="0.986259535714286">
where fcw is the number of observations contain-
ing argument w that have been assigned cut c, fcp
is the number of observations containing predicate
p that have been assigned cut c and fc·, f·p are the
marginal counts for cut c and predicate p, respec-
tively. The two terms that are multiplied in (4) play
complementary roles analogous to those of the two
description lengths in Li and Abe’s MDL formula-
tion; P(c|p) will prefer to reuse more general cuts,
while P(w|c) will prefer more specific cuts with a
smaller associated argument vocabulary.
As the number of words |Wc |that can be emitted
by a cut |c |varies according to the size of the sub-
hierarchy under the cut, the proportion of probability
mass accorded to the likelihood and the prior in (5)
is not constant. An alternative formulation is to keep
the distribution of mass between likelihood and prior
constant but vary the value of the individual βc pa-
rameters according to cut size. Experiments suggest
that this alternative does not differ in performance.
The second cut-based model, WN-CUT-TOPICS,
extends WN-CUT by adding two extra layers of la-
tent variables. Firstly, the choice of cut is condi-
tional on a “topic” variable z rather than directly
conditioned on the predicate; when the topic vocab-
ulary Z is much smaller than the cut vocabulary C,
this has the effect of clustering the cuts. Secondly,
Model 2 Generative story for WN-CUT-TOPICS
</bodyText>
<equation confidence="0.912895235294118">
for topic z E 11 ... |Z|} do
Ψz — Dirichlet(α)
end for
for cut c E 11 ... |C|} do
Φc — Dirichlet(γc)
end for
for synset s E 11 ... |S|} do
Ξs — Dirichlet(βs)
end for
for predicate p E 11 ... |P|} do
θp — Dirichlet(κ)
for argument instance i E Observations(p)
do
zi — Multinomial(θp)
ci — Multinomial(Ψz)
si — Multinomial(Φc)
wi — Multinomial(Ξs)
</equation>
<listItem confidence="0.57128">
end for
end for
</listItem>
<bodyText confidence="0.999497090909091">
instead of immediately drawing a word once a cut
has been chosen, the model first draws a synset s
and then draws a word from the vocabulary Ws asso-
ciated with that synset. This has two advantages; it
directly disambiguates each observation to a specific
synset rather than to a region of the hierarchy and it
should also improve plausibility predictions for rare
synonyms of common arguments. The generative
story for WN-CUT-TOPICS is given in Algorithm 2;
the distribution over arguments for p is given in (6)
and the corresponding posterior estimate in (7):
</bodyText>
<equation confidence="0.999545888888889">
X X X
P(w|p) = P(z|p) P(c|z) P(s|c)P(w|s)
z c s
(6)
fzp + κz Xfcz + α
f·p + P f·z + |C|αX
z1 κz1 c
fws + β (7)
f·c + |Sc|γf·s + |Ws|β
</equation>
<bodyText confidence="0.991695">
As before, fzp, fcz, fsc and fws are the re-
spective co-occurrence counts of topics/predicates,
cuts/topics, synsets/cuts and words/synsets in the
sampling state and f·p, f·z, f·c and f·s are the cor-
responding marginal counts.
</bodyText>
<figure confidence="0.918059428571429">
Xoc fcp + α
c f·p + |C|α
Xoc
z
X
s
fsc + γ
</figure>
<page confidence="0.993601">
173
</page>
<bodyText confidence="0.999852346153846">
Since WN-CUT and WN-CUT-TOPICS are con-
structed from multinomials with Dirichlet priors,
it is relatively straightforward to train them by
collapsed Gibbs sampling (Griffiths and Steyvers,
2004), an iterative method whereby each latent vari-
able in the model is stochastically updated accord-
ing to the distribution given by conditioning on the
current latent variable assignments of all other to-
kens. In the case of WN-CUT, this amounts to up-
dating the cut assignment ci for each token in turn.
For WN-CUT-TOPICS there are three variables to
update; ci and si must be updated simultaneously,
but zi can be updated independently for the bene-
fit of efficiency. Although WordNet contains 82,115
noun synsets, updates for ci and si can be computed
very efficiently, as there are typically few possible
synsets for a given word type and few possible cuts
for a given synset (the maximum synset depth is 19).
The hyperparameters for the various Dirichlet pri-
ors are also reestimated in the course of learning; the
values of these hyperparameters control the degree
of sparsity preferred by the model. The “top-level”
hyperparameters a in WN-CUT and K in WN-CUT-
TOPICS are estimated using a fixed-point iteration
proposed by Wallach (2008); the other hyperparam-
eters are learned by slice sampling (Neal, 2003).
</bodyText>
<subsectionHeader confidence="0.97797">
3.3 Walk-based models
</subsectionHeader>
<bodyText confidence="0.99981015">
Abney and Light (1999) proposed an approach to
selectional preference learning in which arguments
are generated for predicates by following a path
A = (l1, ... ,l|λ|) from the root of the hierarchy to a
leaf node and emitting the corresponding word. The
path is chosen according to a Markov model with
transition probabilities specific to each predicate. In
this model, each leaf node is associated with a sin-
gle word; the synsets associated with that word are
the immediate parent nodes of the leaf. Abney and
Light found that their model did not match the per-
formance of Resnik’s (1993) simpler method. We
have had a similar lack of success with a Bayesian
version of this model, which we do not describe fur-
ther here.
Boyd-Graber et al. (2007) describe a related topic
model, LDAWN, for word sense disambiguation
that adds an intermediate layer of latent variables
Z on which the Markov model parameters are con-
ditioned. In their application, each document in a
</bodyText>
<figure confidence="0.914852294117647">
Model 3 Generative story for LDAWN
for topic z ∈ {1 ... |Z|} do
for synset s ∈ {1 ... |5|} do
Draw transition probabilities Ψz,s ∼
Dirichlet(uas)
end for
end for
for predicate p ∈ {1... |P|} do
Bp ∼ Dirichlet(K)
for argument instance i ∈ Observations(p)
do
zi ∼ Multinomial(Bp)
Create a path starting at the root synset A0:
while not at a leaf node do
At+1 ∼ Multinomial(Ψzi,λt)
end while
Emit the word at the leaf as wi
</figure>
<tableCaption confidence="0.278292">
end for
end for
</tableCaption>
<bodyText confidence="0.9996694">
corpus is associated with a distribution over topics
and each topic is associated with a distribution over
paths. The clustering effect of the topic layer allows
the documents to “share” information and hence al-
leviate problems due to sparsity. By analogy to Ab-
ney and Light, it is a short and intuitive step to ap-
ply LDAWN to selectional preference learning. The
generative story for LDAWN is given in Algorithm
3; the probability model for P(w|p) is defined by (8)
and the posterior estimate is (9):
</bodyText>
<equation confidence="0.979339">
X X
P(w|p) = P(z|p)
z λ
fzp + Kz X
1 [A → w] ×
f·p + Pz, Kz,
λ
</equation>
<bodyText confidence="0.999774111111111">
where 1[A → w] = 1 when the path A leads to leaf
node w and has value 0 otherwise. Following Boyd-
Graber et al. the Dirichlet priors on the transition
probabilities are parameterised by the product of a
strength parameter a and a distribution as, the latter
being fixed according to relative corpus frequencies
to “guide” the model towards more fruitful paths.
Gibbs sampling updates for LDAWN are given in
Boyd-Graber et al. (2007). As before, we reestimate
</bodyText>
<equation confidence="0.999847">
1[A → w]P(A|z) (8)
X∝
z
(9)
fz,li→· + a
|λ|−1
Y
i=1
fz,li→li+1 + Qali→li+1
</equation>
<page confidence="0.991872">
174
</page>
<bodyText confidence="0.966325">
SEEN:
staff morale 0.4889
team morale 0.5945
issue morale 0.0595
UNSEEN:
pupil morale 0.4318
minute morale -0.0352
snow morale -0.2748
</bodyText>
<tableCaption confidence="0.997478">
Table 1: Extract from the noun-noun section of Keller and
Lapata’s (2003) dataset, with human plausibility scores
</tableCaption>
<bodyText confidence="0.998886666666667">
the hyperparameters during learning; K is estimated
by Wallach’s fixed-point iteration and a is estimated
by slice sampling.
</bodyText>
<sectionHeader confidence="0.999894" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998344">
4.1 Experimental procedure
</subsectionHeader>
<bodyText confidence="0.990784351351351">
We evaluate our methods by comparing their predic-
tions to human judgements of predicate-argument
plausibility. This is a standard approach to se-
lectional preference evaluation (Keller and Lapata,
2003; Brockmann and Lapata, 2003; O´ S´eaghdha,
2010) and arguably yields a better appraisal of a
model’s intrinsic semantic quality than other eval-
uations such as pseudo-disambiguation or held-out
likelihood prediction.2 We use a set of plau-
sibility judgements collected by Keller and Lap-
ata (2003). This dataset comprises 180 predicate-
argument combinations for each of three syntactic
relations: verb-object, noun-noun modification and
adjective-noun modification. The data for each re-
lation is divided into a “seen” portion containing
90 combinations that were observed in the British
National Corpus and an “unseen” portion contain-
ing 90 combinations that do not appear (though
the predicates and arguments do appear separately).
Plausibility judgements were elicited from a large
group of human subjects, then normalised and log-
transformed. Table 1 gives a representative illus-
tration of the data. Following the evaluation in O´
S´eaghdha (2010), with which we wish to compare,
we use Pearson r and Spearman p correlation coef-
ficients as performance measures.
All models were trained on the 90-million word
2For a related argument in the context of topic model evalu-
ation, see Chang et al. (2009).
written component of the British National Cor-
pus,3 lemmatised, POS-tagged and parsed with the
RASP toolkit (Briscoe et al., 2006). We removed
predicates occurring with just one argument type
and all tokens containing non-alphabetic characters.
The resulting datasets consist of 3,587,172 verb-
object observations (7,954 predicate types, 80,107
argument types), 3,732,470 noun-noun observations
(68,303 predicate types, 105,425 argument types)
and 3,843,346 adjective-noun observations (29,975
predicate types, 62,595 argument types).
All the Bayesian models were trained by Gibbs
sampling, as outlined above. For each model we run
three sampling chains for 1,000 iterations and aver-
age the plausibility predictions for each to produce a
final prediction P(w1p) for each predicate-argument
item. As the evaluation demands an estimate of the
joint probability P(w, p) we multiply the predicted
P(w1p) by a predicate probability P(pIr) estimated
from relative corpus frequencies. In training we use
a burn-in period of 200 iterations, after which hyper-
parameters are reestimated and P(plr) predictions
are sampled every 50 iterations. All probability es-
timates are log-transformed to match the gold stan-
dard judgements.
In order to compare against previously proposed
selectional preference approaches based on Word-
Net we also reimplemented the methods that per-
formed best in the evaluation of Brockmann and
Lapata (2003): Resnik (1993) and Clark and Weir
(2002). For Resnik’s model we used WordNet 2.1
rather than WordNet 3.0 as the former has multi-
ple roots, a property that turns out to be necessary
for good performance. Clark and Weir’s method
requires that the user specify a significance thresh-
old a to be used in deciding where to cut; to give
it the best possible chance we tested with a range
of values (0.05, 0.3, 0.6, 0.9) and report results for
the best-performing setting, which consistently was
a = 0.9. One can also use different statistical hy-
pothesis tests; again we choose the test giving the
best results, which was Pearson’s chi-squared test.
As this method produces a probability estimate con-
ditioned on the predicate p we multiply by a MLE
estimate of P(plr) and log-transform the result.
</bodyText>
<footnote confidence="0.966068">
3http://www.natcorp.ox.ac.uk/
</footnote>
<page confidence="0.991302">
175
</page>
<table confidence="0.97693625">
eat food#n#1, aliment#n#1, entity#n#1, solid#n#1, food#n#2
drink fluid#n#1, liquid#n#1, entity#n#1, alcohol#n#1, beverage#n#1
appoint individual#n#1, entity#n#1, chief#n#1, being#n#2, expert#n#1
publish abstract entity#n#1, piece of writing#n#1, communication#n#2, publication#n#1
</table>
<tableCaption confidence="0.838535">
Table 2: Most probable cuts learned by WN-CUT for the object argument of selected verbs
</tableCaption>
<table confidence="0.999964333333333">
Verb-object Noun-noun Adjective-noun
Seen Unseen Seen Unseen Seen Unseen
r p r p r p r p r p r p
WN-CUT .593 .582 .514 .571 .550 .584 .564 .590 .561 .618 .453 .439
WN-CUT-100 .500 .529 .575 .630 .619 .639 .662 .706 .537 .510 .464 .431
WN-CUT-200 .538 .546 .557 .608 .595 .632 .639 .669 .585 .587 .435 .431
LDAWN-100 .497 .538 .558 .594 .605 .619 .635 .633 .549 .545 .459 .462
LDAWN-200 .546 .562 .508 .548 .610 .654 .526 .568 .578 .583 .453 .450
Resnik .384 .473 .469 .470 .242 .187 .152 .037 .309 .388 .311 .280
Clark/Weir .489 .546 .312 .365 .441 .521 .543 .576 .440 .476 .271 .242
BNC (MLE) .620 .614 .196 .222 .544 .604 .114 .125 .543 .622 .135 .102
LDA .504 .541 .558 .603 .615 .641 .636 .666 .594 .558 .468 .459
</table>
<tableCaption confidence="0.991958">
Table 3: Results (Pearson r and Spearman p correlations) on Keller and Lapata’s (2003) plausibility data; underlining
denotes the best-performing WordNet-based model, boldface denotes the overall best performance
</tableCaption>
<subsectionHeader confidence="0.797178">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999810955555556">
Table 2 demonstrates the top cuts learned by the
WN-CUT model from the verb-object training data
for a selection of verbs. Table 3 gives quanti-
tative results for the WordNet-based models un-
der consideration, as well as results reported by O´
S´eaghdha (2010) for a purely distributional LDA
model with 100 topics and a Maximum Likelihood
Estimate model learned from the BNC. In general,
the Bayesian WordNet-based models outperform the
models of Resnik and Clark and Weir, and are com-
petitive with the state-of-the-art LDA results. To
test the statistical significance of performance differ-
ences we use the test proposed by Meng et al. (1992)
for comparing correlated correlations, i.e., correla-
tion scores with a shared gold standard. The dif-
ferences between Bayesian WordNet models are not
significant (p &gt; 0.05, two-tailed) for any dataset or
evaluation measure. However, all Bayesian mod-
els improve significantly over Resnik’s and Clark
and Weir’s models for multiple conditions. Perhaps
surprisingly, the relatively simple WN-CUT model
scores the greatest number of significant improve-
ments over both Resnik (7 out of 12 conditions)
and Clark and Weir (8 out of 12), though the other
Bayesian models do follow close behind. This may
suggest that the incorporation of WordNet structure
into the model in itself provides much of the cluster-
ing benefit provided by an additional layer of “topic”
latent variables.4
In order to test the ability of the WordNet-based
models to make predictions about arguments that
are absent from the training vocabulary, we created
an artificial out-of-vocabulary dataset by removing
each of the Keller and Lapata argument words from
the input corpus and retraining. An LDA selectional
preference model will completely fail here, but we
hope that the WordNet models can still make rela-
tively accurate predictions by leveraging the addi-
tional lexical knowledge provided by the hierarchy.
For example, if one knows that a tomatillo is classed
as a vegetable in WordNet, one can predict a rel-
atively high probability that it can be eaten, even
though the word tomatillo does not appear in the
BNC.
As a baseline we use a BNC-trained model that
</bodyText>
<footnote confidence="0.69405625">
4An alternative hypothesis is that samplers for the more
complex models take longer to “mix”. We have run some exper-
iments with 5,000 iterations but did not observe an improvement
in performance.
</footnote>
<page confidence="0.989725">
176
</page>
<table confidence="0.999912818181818">
Verb-object Noun-noun Adjective-noun
Seen Unseen Seen Unseen Seen Unseen
r p r p r p r p r p r p
WN-CUT .334 .326 .518 .569 .252 .212 .254 .274 .451 .397 .471 .458
WN-CUT-100 .308 .357 .459 .489 .223 .207 .126 .074 .285 .264 .234 .226
WN-CUT-200 .273 .321 .452 .482 .192 .174 .115 .053 .266 .212 .220 .214
LDAWN-100 .223 .235 .410 .391 .259 .220 .132 .138 .016 .037 .264 .254
LDAWN-200 .291 .285 .392 .379 .240 .163 .118 .131 .041 .078 .209 .212
Resnik .203 .341 .472 .497 .054 -.054 .184 .089 .353 .393 .333 .365
Clark/Weir .222 .287 .201 .235 .225 .162 .279 .304 .313 .202 .190 .148
BNC .206 .224 .276 .240 .256 .240 .223 .225 .088 .103 .220 .231
</table>
<tableCaption confidence="0.999374">
Table 4: Forced-OOV results (Pearson r and Spearman p correlations) on Keller and Lapata’s (2003) plausibility data
</tableCaption>
<bodyText confidence="0.999682535714286">
predicts P(w, p) proportional to the MLE predicate
probability P(p); a distributional LDA model will
make essentially the same prediction. Clark and
Weir’s method does not have full coverage; if no
sense s of an argument appears in the data then
P(slp) is zero for all senses and the resulting pre-
diction is zero, which cannot be log-transformed.
To sidestep this issue, unseen senses are assigned a
pseudofrequency of 0.1. Results for this “forced-
OOV” task are presented in Table 4. WN-CUT
proves the most adept at generalising to unseen ar-
guments, attaining the best performance on 7 of 12
dataset/evaluation conditions and a statistically sig-
nificant improvement over the baseline on 6. We ob-
serve that estimating the plausibility of unseen ar-
guments for noun-noun modifiers is particularly dif-
ficult. One obvious explanation is that the training
data for this relation has fewer tokens per predi-
cate, making it more difficult to learn their prefer-
ences. A second, more hypothetical, explanation is
that the ontological structure of WordNet is a rela-
tively poor fit for the preferences of nominal modi-
fiers; it is well-known that almost any pair of nouns
can combine to produce a minimally plausible noun-
noun compound (Downing, 1977) and it may be that
this behaviour is ill-suited by the assumption that
preferences are sparse distributions over regions of
WordNet.
</bodyText>
<sectionHeader confidence="0.999187" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99953196">
In this paper we have presented a range of
Bayesian selectional preference models that incor-
porate knowledge about the structure of a lexical hi-
erarchy. One motivation for this work was to test
the hypothesis that such knowledge can be helpful
in constructing robust models that can handle rare
and unseen arguments. To this end we have re-
ported a plausibility-based evaluation in which our
models outperform previously proposed WordNet-
based preference models and make sensible predic-
tions for out-of-vocabulary items. A second motiva-
tion, which we intend to explore in future work, is
to apply our models in the context of a word sense
disambiguation task. Previous studies have demon-
strated the effectiveness of distributional Bayesian
selectional preference models for predicting lexical
substitutes ( O´ S´eaghdha and Korhonen, 2011) but
these models lack a principled way to map a word
onto its most likely WordNet sense. The methods
presented in this paper offer a promising solution to
this issue. Another potential research direction is in-
tegration of semantic relation extraction algorithms
with WordNet or other lexical resources, along the
lines of Pennacchiotti and Pantel (2006) and Van
Durme et al. (2009).
</bodyText>
<sectionHeader confidence="0.994208" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9995">
The work in this paper was funded by the EP-
SRC (UK) grant EP/G051070/1, EU grant 7FP-ITC-
248064 and the Royal Society, (UK).
</bodyText>
<sectionHeader confidence="0.995568" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.91028275">
Steven Abney and Marc Light. 1999. Hiding a semantic
hierarchy in a Markov model. In Proceedings of the
ACL-99 Workshop on Unsupervised Learning in NLP,
College Park, MD.
</reference>
<page confidence="0.989779">
177
</page>
<reference confidence="0.999802710280374">
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preferences
from unlabeled text. In Proceedings of EMNLP-08,
Honolulu, HI.
David M. Blei, Thomas L. Griffiths, Michael I. Jordan,
and Joshua B. Tenenbaum. 2003a. Hierarchical topic
models and the nested Chinese Restaurant Process. In
Proceedings of NIPS-03, Vancouver, BC.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003b. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In Pro-
ceedings of EMNLP-CoNLL-07, Prague, Czech Re-
public.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the ACL-06 Interactive Presentation Sessions,
Sydney, Australia.
Carsten Brockmann and Mirella Lapata. 2003. Evalu-
ating and combining approaches to selectional pref-
erence acquisition. In Proceedings of EACL-03, Bu-
dapest, Hungary.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of NIPS-09, Vancouver, BC.
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2), 187–206.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810–842.
Katrin Erk, Sebastian Pad´o, and Ulrike Pad´o. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4):723–763.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl. 1):5228–5235.
Frank Keller and Mirella Lapata. 2003. Using the Web to
obtain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459–484.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217–244.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically
acquired selectional preferences. Computational Lin-
guistics, 29(4):639–654.
Xiao-Li Meng, Robert Rosenthal, and Donald B. Rubin.
1992. Comparing correlated correlation coefficients.
Psychological Bulletin, 111(1):172–175.
David Mimno, Wei Li, and Andrew McCallum. 2007.
Mixtures of hierarchical topics with Pachinko alloca-
tion. In Proceedings of ICML-07, Corvallis, OR.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31(3):705–767.
Diarmuid O´ S´eaghdha and Anna Korhonen. 2011. Prob-
abilistic models of similarity in syntactic context. In
Proceedings of EMNLP-11, Edinburgh, UK.
Diarmuid O´ S´eaghdha. 2010. Latent variable models
of selectional preference. In Proceedings of ACL-10,
Uppsala, Sweden.
Marco Pennacchiotti and Patrick Pantel. 2006. Ontolo-
gizing semantic relations. In Proceedings of COLING-
ACL-06, Sydney, Australia.
Keith Rayner, Tessa Warren, Barbara J. Juhasz, and Si-
mon P. Liversedge. 2004. The effect of plausibil-
ity on eye movements in reading. Journal of Experi-
mental Psychology: Learning Memory and Cognition,
30(6):1290–1301.
Joseph Reisinger and Raymond Mooney. 2011. Cross-
cutting models of lexical semantics. In Proceedings of
EMNLP-11, Edinburgh, UK.
Joseph Reisinger and Marius Pas¸ca. 2009. Latent vari-
able models of concept-attribute attachment. In Pro-
ceedings of ACL-IJCNLP-09, Suntec, Singapore.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent Dirichlet allocation method for selectional prefer-
ences. In Proceedings ACL-10, Uppsala, Sweden.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible, and Helmut Schmid. 2008. Combining EM
training and the MDL principle for an automatic verb
classification incorporating selectional preferences. In
Proceedings of ACL-08:HLT, Columbus, OH.
Ekaterina Shutova. 2010. Automatic metaphor inter-
pretation as a paraphrasing task. In Proceedings of
NAACL-HLT-10, Los Angeles, CA.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of ACL-10, Uppsala, Sweden.
Benjamin Van Durme, Philip Michalak, and Lenhart K.
Schubert. 2009. Deriving generalized knowledge
from corpora using WordNet abstraction. In Proceed-
ings of EACL-09, Athens, Greece.
Hanna Wallach. 2008. Structured Topic Models for Lan-
guage. Ph.D. thesis, University of Cambridge.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11:197–225.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
</reference>
<page confidence="0.980144">
178
</page>
<reference confidence="0.9988329">
generative models. In Proceedings of EMNLP-11, Ed-
inburgh, UK.
Be˜nat Zapirain, Eneko Agirre, and Llu´ıs M`arquez. 2009.
Generalizing over lexical features: Selectional prefer-
ences for semantic role classification. In Proceedings
of ACL-IJCNLP-09, Singapore.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting web-derived selectional preference to im-
prove statistical dependency parsing. In Proceedings
of ACL-11, Portland, OR.
</reference>
<page confidence="0.99881">
179
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.257489">
<title confidence="0.992069">Modelling selectional preferences in a lexical hierarchy</title>
<author confidence="0.544152">O´</author>
<affiliation confidence="0.9472315">Computer University of</affiliation>
<address confidence="0.821622">Cambridge,</address>
<email confidence="0.991865">do242@cam.ac.uk</email>
<author confidence="0.702391">Anna</author>
<affiliation confidence="0.9827915">Computer University of</affiliation>
<address confidence="0.877646">Cambridge,</address>
<email confidence="0.985487">Anna.Korhonen@cl.cam.ac.uk</email>
<abstract confidence="0.999651294117647">This paper describes Bayesian selectional preference models that incorporate knowledge from a lexical hierarchy such as WordNet. Inspired by previous work on modelling with WordNet, these approaches are based either on “cutting” the hierarchy at an appropriate level of generalisation or on a “walking” model that selects a path from the root to a leaf. In an evaluation comparing against human plausibility judgements, we show that the models presented here outperform previously proposed comparable WordNet-based models, are competitive with state-of-the-art selectional preference models and are particularly wellsuited to estimating plausibility for items that were not seen in training.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>Marc Light</author>
</authors>
<title>Hiding a semantic hierarchy in a Markov model.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL-99 Workshop on Unsupervised Learning in NLP,</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="7868" citStr="Abney and Light (1999)" startWordPosition="1250" endWordPosition="1253"> given by: P(p|cs,p,r, r)P (s|r) P(p |r)P(s, |r) (3 ) Es,ES P(p |cs,,p,r, r) P(p|r) where cs,p,r is the root node of the subhierarchy containing s that was selected for p. An alternative approach to modelling with WordNet uses its hierarchical structure to define a Markov model with transitions from senses to senses and from senses to words. The intuition here is that each observation is generated by a “walk” from the root of the hierarchy to a leaf node and emitting the word 1In this paper we use WordNet version 3.0, except where stated otherwise. Pcw(s|p, r) = 171 corresponding to the leaf. Abney and Light (1999) proposed such a model for selectional preferences, trained via EM, but failed to achieve competitive performance on a pseudodisambiguation task. The models described above have subsequently been used in many different studies. For example: McCarthy and Carroll (2003) use Li and Abe’s method in a word sense disambiguation setting; Schulte im Walde et al. (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in w</context>
<context position="16733" citStr="Abney and Light (1999)" startWordPosition="2785" endWordPosition="2788">si can be computed very efficiently, as there are typically few possible synsets for a given word type and few possible cuts for a given synset (the maximum synset depth is 19). The hyperparameters for the various Dirichlet priors are also reestimated in the course of learning; the values of these hyperparameters control the degree of sparsity preferred by the model. The “top-level” hyperparameters a in WN-CUT and K in WN-CUTTOPICS are estimated using a fixed-point iteration proposed by Wallach (2008); the other hyperparameters are learned by slice sampling (Neal, 2003). 3.3 Walk-based models Abney and Light (1999) proposed an approach to selectional preference learning in which arguments are generated for predicates by following a path A = (l1, ... ,l|λ|) from the root of the hierarchy to a leaf node and emitting the corresponding word. The path is chosen according to a Markov model with transition probabilities specific to each predicate. In this model, each leaf node is associated with a single word; the synsets associated with that word are the immediate parent nodes of the leaf. Abney and Light found that their model did not match the performance of Resnik’s (1993) simpler method. We have had a sim</context>
</contexts>
<marker>Abney, Light, 1999</marker>
<rawString>Steven Abney and Marc Light. 1999. Hiding a semantic hierarchy in a Markov model. In Proceedings of the ACL-99 Workshop on Unsupervised Learning in NLP, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Discriminative learning of selectional preferences from unlabeled text.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP-08,</booktitle>
<location>Honolulu, HI.</location>
<contexts>
<context position="8911" citStr="Bergsma et al., 2008" startWordPosition="1416" endWordPosition="1419">nd semantic subcategorisation frame learning; Shutova (2010) deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in which the methods of Resnik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone. These methods can be vectorbased (Erk et al., 2010; Thater et al., 2010), discriminative (Bergsma et al., 2008) or probabilistic ( O´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger and Mooney, 2011). In the probabilistic category, Bayesian models based on the “topic modelling” framework (Blei et al., 2003b) have been shown to achieve state-of-the-art performance in a number of evaluation settings; the models considered in this paper are also related to this framework. In machine learning, researchers have proposed a variety of topic modelling methods where the latent variables are arranged in a hierarchical structure (Blei et al., 2003a; Mimno et al., 2007). In contrast to the present work, these mode</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>Shane Bergsma, Dekang Lin, and Randy Goebel. 2008. Discriminative learning of selectional preferences from unlabeled text. In Proceedings of EMNLP-08, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Thomas L Griffiths</author>
<author>Michael I Jordan</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested Chinese Restaurant Process.</title>
<date>2003</date>
<booktitle>In Proceedings of NIPS-03,</booktitle>
<location>Vancouver, BC.</location>
<contexts>
<context position="9107" citStr="Blei et al., 2003" startWordPosition="1446" endWordPosition="1449">snik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone. These methods can be vectorbased (Erk et al., 2010; Thater et al., 2010), discriminative (Bergsma et al., 2008) or probabilistic ( O´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger and Mooney, 2011). In the probabilistic category, Bayesian models based on the “topic modelling” framework (Blei et al., 2003b) have been shown to achieve state-of-the-art performance in a number of evaluation settings; the models considered in this paper are also related to this framework. In machine learning, researchers have proposed a variety of topic modelling methods where the latent variables are arranged in a hierarchical structure (Blei et al., 2003a; Mimno et al., 2007). In contrast to the present work, these models use a relatively shallow hierarchy (e.g., 3 levels) and any hierarchy node can in principle emit any vocabulary item; they thus provide a poor match for our goal of modelling over WordNet. Boyd</context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2003</marker>
<rawString>David M. Blei, Thomas L. Griffiths, Michael I. Jordan, and Joshua B. Tenenbaum. 2003a. Hierarchical topic models and the nested Chinese Restaurant Process. In Proceedings of NIPS-03, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="9107" citStr="Blei et al., 2003" startWordPosition="1446" endWordPosition="1449">snik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone. These methods can be vectorbased (Erk et al., 2010; Thater et al., 2010), discriminative (Bergsma et al., 2008) or probabilistic ( O´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger and Mooney, 2011). In the probabilistic category, Bayesian models based on the “topic modelling” framework (Blei et al., 2003b) have been shown to achieve state-of-the-art performance in a number of evaluation settings; the models considered in this paper are also related to this framework. In machine learning, researchers have proposed a variety of topic modelling methods where the latent variables are arranged in a hierarchical structure (Blei et al., 2003a; Mimno et al., 2007). In contrast to the present work, these models use a relatively shallow hierarchy (e.g., 3 levels) and any hierarchy node can in principle emit any vocabulary item; they thus provide a poor match for our goal of modelling over WordNet. Boyd</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003b. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL-07,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9728" citStr="Boyd-Graber et al. (2007)" startWordPosition="1550" endWordPosition="1553">2003b) have been shown to achieve state-of-the-art performance in a number of evaluation settings; the models considered in this paper are also related to this framework. In machine learning, researchers have proposed a variety of topic modelling methods where the latent variables are arranged in a hierarchical structure (Blei et al., 2003a; Mimno et al., 2007). In contrast to the present work, these models use a relatively shallow hierarchy (e.g., 3 levels) and any hierarchy node can in principle emit any vocabulary item; they thus provide a poor match for our goal of modelling over WordNet. Boyd-Graber et al. (2007) describe a topic model that is directly influenced by Abney and Light’s Markov model approach; this model (LDAWN) is described further in Section 3.3 below. Reisinger and Pas¸ca (2009) investigate Bayesian methods for attaching attributes harvested from the Web at an appropriate level in the WordNet hierarchy; this task is related in spirit to the preference learning task. 3 Probabilistic modelling over WordNet 3.1 Notation We assume that we have a lexical hierarchy in the form of a directed acyclic graph G = (5, E) where each node (or synset) s E 5 is associated with a set of words Wn belong</context>
<context position="17457" citStr="Boyd-Graber et al. (2007)" startWordPosition="2911" endWordPosition="2914">cates by following a path A = (l1, ... ,l|λ|) from the root of the hierarchy to a leaf node and emitting the corresponding word. The path is chosen according to a Markov model with transition probabilities specific to each predicate. In this model, each leaf node is associated with a single word; the synsets associated with that word are the immediate parent nodes of the leaf. Abney and Light found that their model did not match the performance of Resnik’s (1993) simpler method. We have had a similar lack of success with a Bayesian version of this model, which we do not describe further here. Boyd-Graber et al. (2007) describe a related topic model, LDAWN, for word sense disambiguation that adds an intermediate layer of latent variables Z on which the Markov model parameters are conditioned. In their application, each document in a Model 3 Generative story for LDAWN for topic z ∈ {1 ... |Z|} do for synset s ∈ {1 ... |5|} do Draw transition probabilities Ψz,s ∼ Dirichlet(uas) end for end for for predicate p ∈ {1... |P|} do Bp ∼ Dirichlet(K) for argument instance i ∈ Observations(p) do zi ∼ Multinomial(Bp) Create a path starting at the root synset A0: while not at a leaf node do At+1 ∼ Multinomial(Ψzi,λt) en</context>
<context position="19109" citStr="Boyd-Graber et al. (2007)" startWordPosition="3213" endWordPosition="3216">ive story for LDAWN is given in Algorithm 3; the probability model for P(w|p) is defined by (8) and the posterior estimate is (9): X X P(w|p) = P(z|p) z λ fzp + Kz X 1 [A → w] × f·p + Pz, Kz, λ where 1[A → w] = 1 when the path A leads to leaf node w and has value 0 otherwise. Following BoydGraber et al. the Dirichlet priors on the transition probabilities are parameterised by the product of a strength parameter a and a distribution as, the latter being fixed according to relative corpus frequencies to “guide” the model towards more fruitful paths. Gibbs sampling updates for LDAWN are given in Boyd-Graber et al. (2007). As before, we reestimate 1[A → w]P(A|z) (8) X∝ z (9) fz,li→· + a |λ|−1 Y i=1 fz,li→li+1 + Qali→li+1 174 SEEN: staff morale 0.4889 team morale 0.5945 issue morale 0.0595 UNSEEN: pupil morale 0.4318 minute morale -0.0352 snow morale -0.2748 Table 1: Extract from the noun-noun section of Keller and Lapata’s (2003) dataset, with human plausibility scores the hyperparameters during learning; K is estimated by Wallach’s fixed-point iteration and a is estimated by slice sampling. 4 Experiments 4.1 Experimental procedure We evaluate our methods by comparing their predictions to human judgements of p</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In Proceedings of EMNLP-CoNLL-07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL-06 Interactive Presentation Sessions,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="21158" citStr="Briscoe et al., 2006" startWordPosition="3527" endWordPosition="3530">do appear separately). Plausibility judgements were elicited from a large group of human subjects, then normalised and logtransformed. Table 1 gives a representative illustration of the data. Following the evaluation in O´ S´eaghdha (2010), with which we wish to compare, we use Pearson r and Spearman p correlation coefficients as performance measures. All models were trained on the 90-million word 2For a related argument in the context of topic model evaluation, see Chang et al. (2009). written component of the British National Corpus,3 lemmatised, POS-tagged and parsed with the RASP toolkit (Briscoe et al., 2006). We removed predicates occurring with just one argument type and all tokens containing non-alphabetic characters. The resulting datasets consist of 3,587,172 verbobject observations (7,954 predicate types, 80,107 argument types), 3,732,470 noun-noun observations (68,303 predicate types, 105,425 argument types) and 3,843,346 adjective-noun observations (29,975 predicate types, 62,595 argument types). All the Bayesian models were trained by Gibbs sampling, as outlined above. For each model we run three sampling chains for 1,000 iterations and average the plausibility predictions for each to pro</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the ACL-06 Interactive Presentation Sessions, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carsten Brockmann</author>
<author>Mirella Lapata</author>
</authors>
<title>Evaluating and combining approaches to selectional preference acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL-03,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="8431" citStr="Brockmann and Lapata (2003)" startWordPosition="1337" endWordPosition="1340">(s|p, r) = 171 corresponding to the leaf. Abney and Light (1999) proposed such a model for selectional preferences, trained via EM, but failed to achieve competitive performance on a pseudodisambiguation task. The models described above have subsequently been used in many different studies. For example: McCarthy and Carroll (2003) use Li and Abe’s method in a word sense disambiguation setting; Schulte im Walde et al. (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in which the methods of Resnik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone. These methods can be vectorbased (Erk et al., 2010; Thater et al., 2010), discriminative (Bergsma et al., 2008) or probabilistic ( O´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger and Mooney, 2011). In the probabilistic category,</context>
<context position="19858" citStr="Brockmann and Lapata, 2003" startWordPosition="3327" endWordPosition="3330">e 0.4889 team morale 0.5945 issue morale 0.0595 UNSEEN: pupil morale 0.4318 minute morale -0.0352 snow morale -0.2748 Table 1: Extract from the noun-noun section of Keller and Lapata’s (2003) dataset, with human plausibility scores the hyperparameters during learning; K is estimated by Wallach’s fixed-point iteration and a is estimated by slice sampling. 4 Experiments 4.1 Experimental procedure We evaluate our methods by comparing their predictions to human judgements of predicate-argument plausibility. This is a standard approach to selectional preference evaluation (Keller and Lapata, 2003; Brockmann and Lapata, 2003; O´ S´eaghdha, 2010) and arguably yields a better appraisal of a model’s intrinsic semantic quality than other evaluations such as pseudo-disambiguation or held-out likelihood prediction.2 We use a set of plausibility judgements collected by Keller and Lapata (2003). This dataset comprises 180 predicateargument combinations for each of three syntactic relations: verb-object, noun-noun modification and adjective-noun modification. The data for each relation is divided into a “seen” portion containing 90 combinations that were observed in the British National Corpus and an “unseen” portion cont</context>
<context position="22446" citStr="Brockmann and Lapata (2003)" startWordPosition="3712" endWordPosition="3715">em. As the evaluation demands an estimate of the joint probability P(w, p) we multiply the predicted P(w1p) by a predicate probability P(pIr) estimated from relative corpus frequencies. In training we use a burn-in period of 200 iterations, after which hyperparameters are reestimated and P(plr) predictions are sampled every 50 iterations. All probability estimates are log-transformed to match the gold standard judgements. In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of Brockmann and Lapata (2003): Resnik (1993) and Clark and Weir (2002). For Resnik’s model we used WordNet 2.1 rather than WordNet 3.0 as the former has multiple roots, a property that turns out to be necessary for good performance. Clark and Weir’s method requires that the user specify a significance threshold a to be used in deciding where to cut; to give it the best possible chance we tested with a range of values (0.05, 0.3, 0.6, 0.9) and report results for the best-performing setting, which consistently was a = 0.9. One can also use different statistical hypothesis tests; again we choose the test giving the best resu</context>
</contexts>
<marker>Brockmann, Lapata, 2003</marker>
<rawString>Carsten Brockmann and Mirella Lapata. 2003. Evaluating and combining approaches to selectional preference acquisition. In Proceedings of EACL-03, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS-09,</booktitle>
<location>Vancouver, BC.</location>
<contexts>
<context position="21027" citStr="Chang et al. (2009)" startWordPosition="3507" endWordPosition="3510">itish National Corpus and an “unseen” portion containing 90 combinations that do not appear (though the predicates and arguments do appear separately). Plausibility judgements were elicited from a large group of human subjects, then normalised and logtransformed. Table 1 gives a representative illustration of the data. Following the evaluation in O´ S´eaghdha (2010), with which we wish to compare, we use Pearson r and Spearman p correlation coefficients as performance measures. All models were trained on the 90-million word 2For a related argument in the context of topic model evaluation, see Chang et al. (2009). written component of the British National Corpus,3 lemmatised, POS-tagged and parsed with the RASP toolkit (Briscoe et al., 2006). We removed predicates occurring with just one argument type and all tokens containing non-alphabetic characters. The resulting datasets consist of 3,587,172 verbobject observations (7,954 predicate types, 80,107 argument types), 3,732,470 noun-noun observations (68,303 predicate types, 105,425 argument types) and 3,843,346 adjective-noun observations (29,975 predicate types, 62,595 argument types). All the Bayesian models were trained by Gibbs sampling, as outlin</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In Proceedings of NIPS-09, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<pages>187--206</pages>
<contexts>
<context position="6767" citStr="Clark and Weir (2002)" startWordPosition="1055" endWordPosition="1058"> in which the appropriate cut c is selected according to the Minimum Description Length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising a sum of model description length and data description length. The probability of a predicate p taking as its argument an synset s is modelled as: Pla(s|p,r) = P(s|cs,p,r)P(c|p) (2) where cs,p,r is the portion of the cut learned for p that dominates s. The distribution P(s|cs,p,r) is held to be uniform over all synsets dominated by cs,p,r, while P(c|p) is given by a maximum likelihood estimate. Clark and Weir (2002) present a model that, while not explicitly described as cut-based, likewise seeks to find the right level of generalisation for an observation. In this case, the hypernym at which to “cut” is chosen by a chi-squared test: if the aggregate preference of p for classes in the subhierarchy rooted at c differs significantly from the individual preferences of p for the immediate children of c, the hierarchy is cut below c. The probability of p taking a synset s as its argument is given by: P(p|cs,p,r, r)P (s|r) P(p |r)P(s, |r) (3 ) Es,ES P(p |cs,,p,r, r) P(p|r) where cs,p,r is the root node of the </context>
<context position="22487" citStr="Clark and Weir (2002)" startWordPosition="3719" endWordPosition="3722">e joint probability P(w, p) we multiply the predicted P(w1p) by a predicate probability P(pIr) estimated from relative corpus frequencies. In training we use a burn-in period of 200 iterations, after which hyperparameters are reestimated and P(plr) predictions are sampled every 50 iterations. All probability estimates are log-transformed to match the gold standard judgements. In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of Brockmann and Lapata (2003): Resnik (1993) and Clark and Weir (2002). For Resnik’s model we used WordNet 2.1 rather than WordNet 3.0 as the former has multiple roots, a property that turns out to be necessary for good performance. Clark and Weir’s method requires that the user specify a significance threshold a to be used in deciding where to cut; to give it the best possible chance we tested with a range of values (0.05, 0.3, 0.6, 0.9) and report results for the best-performing setting, which consistently was a = 0.9. One can also use different statistical hypothesis tests; again we choose the test giving the best results, which was Pearson’s chi-squared test</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Stephen Clark and David Weir. 2002. Class-based probability estimation using a semantic hierarchy. Computational Linguistics, 28(2), 187–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela Downing</author>
</authors>
<title>On the creation and use of English compound nouns.</title>
<date>1977</date>
<journal>Language,</journal>
<volume>53</volume>
<issue>4</issue>
<contexts>
<context position="28960" citStr="Downing, 1977" startWordPosition="4792" endWordPosition="4793">and a statistically significant improvement over the baseline on 6. We observe that estimating the plausibility of unseen arguments for noun-noun modifiers is particularly difficult. One obvious explanation is that the training data for this relation has fewer tokens per predicate, making it more difficult to learn their preferences. A second, more hypothetical, explanation is that the ontological structure of WordNet is a relatively poor fit for the preferences of nominal modifiers; it is well-known that almost any pair of nouns can combine to produce a minimally plausible nounnoun compound (Downing, 1977) and it may be that this behaviour is ill-suited by the assumption that preferences are sparse distributions over regions of WordNet. 5 Conclusion In this paper we have presented a range of Bayesian selectional preference models that incorporate knowledge about the structure of a lexical hierarchy. One motivation for this work was to test the hypothesis that such knowledge can be helpful in constructing robust models that can handle rare and unseen arguments. To this end we have reported a plausibility-based evaluation in which our models outperform previously proposed WordNetbased preference </context>
</contexts>
<marker>Downing, 1977</marker>
<rawString>Pamela Downing. 1977. On the creation and use of English compound nouns. Language, 53(4):810–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
<author>Ulrike Pad´o</author>
</authors>
<title>A flexible, corpus-driven model of regular and inverse selectional preferences.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<marker>Erk, Pad´o, Pad´o, 2010</marker>
<rawString>Katrin Erk, Sebastian Pad´o, and Ulrike Pad´o. 2010. A flexible, corpus-driven model of regular and inverse selectional preferences. Computational Linguistics, 36(4):723–763.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6130" citStr="(1998)" startWordPosition="951" endWordPosition="951">icate involves finding the right “level of generalisation” in the WordNet hierarchy. For example, the direct object slot of the verb eat can be associated with the subhierarchy rooted at the synset food#n#1, as all hyponyms of that synset are assumed to be edible and the immediate hypernym of the synset, substance#n#1, is too general given that many substances are rarely eaten.1 This leads to the notion of “cutting” the hierarchy at one or more positions (Li and Abe, 1998). The modelling task then becomes that of finding the cuts that are maximally general without overgeneralising. Li and Abe (1998) propose a model in which the appropriate cut c is selected according to the Minimum Description Length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising a sum of model description length and data description length. The probability of a predicate p taking as its argument an synset s is modelled as: Pla(s|p,r) = P(s|cs,p,r)P(c|p) (2) where cs,p,r is the portion of the cut learned for p that dominates s. The distribution P(s|cs,p,r) is held to be uniform over all synsets dominated by cs,p,r, while P(c|p) is given by a maximum likel</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="15581" citStr="Griffiths and Steyvers, 2004" startWordPosition="2595" endWordPosition="2598">g posterior estimate in (7): X X X P(w|p) = P(z|p) P(c|z) P(s|c)P(w|s) z c s (6) fzp + κz Xfcz + α f·p + P f·z + |C|αX z1 κz1 c fws + β (7) f·c + |Sc|γf·s + |Ws|β As before, fzp, fcz, fsc and fws are the respective co-occurrence counts of topics/predicates, cuts/topics, synsets/cuts and words/synsets in the sampling state and f·p, f·z, f·c and f·s are the corresponding marginal counts. Xoc fcp + α c f·p + |C|α Xoc z X s fsc + γ 173 Since WN-CUT and WN-CUT-TOPICS are constructed from multinomials with Dirichlet priors, it is relatively straightforward to train them by collapsed Gibbs sampling (Griffiths and Steyvers, 2004), an iterative method whereby each latent variable in the model is stochastically updated according to the distribution given by conditioning on the current latent variable assignments of all other tokens. In the case of WN-CUT, this amounts to updating the cut assignment ci for each token in turn. For WN-CUT-TOPICS there are three variables to update; ci and si must be updated simultaneously, but zi can be updated independently for the benefit of efficiency. Although WordNet contains 82,115 noun synsets, updates for ci and si can be computed very efficiently, as there are typically few possib</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the Web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="2094" citStr="Keller and Lapata (2003)" startWordPosition="314" endWordPosition="317"> analyses of metaphor processing (Wilks, 1978) and in psycholinguistic studies of comprehension (Rayner et al., 2004). In Natural Language Processing, automatically acquired preference models have been shown to aid a number of tasks, including semantic role labelling (Zapirain et al., 2009), parsing (Zhou et al., 2011) and lexical disambiguation (Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011). It is tempting to assume that with a large enough corpus, preference learning reduces to a simple language modelling task that can be solved by counting predicate-argument co-occurrences. Indeed, Keller and Lapata (2003) show that relatively good performance at plausibility estimation can be attained by submitting queries to a Web search engine. However, there are many scenarios where this approach is insufficient: for languages and language domains where Web-scale data is unavailable, for predicate types (e.g., inference rules or semantic roles) that cannot be retrieved by keyword search and for applications where accurate models of rarer words are required. O´ S´eaghdha (2010) shows that the Webbased approach is reliably outperformed by more complex models trained on smaller corpora for less frequent predic</context>
<context position="19830" citStr="Keller and Lapata, 2003" startWordPosition="3323" endWordPosition="3326">i+1 174 SEEN: staff morale 0.4889 team morale 0.5945 issue morale 0.0595 UNSEEN: pupil morale 0.4318 minute morale -0.0352 snow morale -0.2748 Table 1: Extract from the noun-noun section of Keller and Lapata’s (2003) dataset, with human plausibility scores the hyperparameters during learning; K is estimated by Wallach’s fixed-point iteration and a is estimated by slice sampling. 4 Experiments 4.1 Experimental procedure We evaluate our methods by comparing their predictions to human judgements of predicate-argument plausibility. This is a standard approach to selectional preference evaluation (Keller and Lapata, 2003; Brockmann and Lapata, 2003; O´ S´eaghdha, 2010) and arguably yields a better appraisal of a model’s intrinsic semantic quality than other evaluations such as pseudo-disambiguation or held-out likelihood prediction.2 We use a set of plausibility judgements collected by Keller and Lapata (2003). This dataset comprises 180 predicateargument combinations for each of three syntactic relations: verb-object, noun-noun modification and adjective-noun modification. The data for each relation is divided into a “seen” portion containing 90 combinations that were observed in the British National Corpus </context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>Frank Keller and Mirella Lapata. 2003. Using the Web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="6001" citStr="Li and Abe, 1998" startWordPosition="927" endWordPosition="930"> potentially be expressed by w and predicting the maximal value. Cut-based models assume that modelling the selectional preference of a predicate involves finding the right “level of generalisation” in the WordNet hierarchy. For example, the direct object slot of the verb eat can be associated with the subhierarchy rooted at the synset food#n#1, as all hyponyms of that synset are assumed to be edible and the immediate hypernym of the synset, substance#n#1, is too general given that many substances are rarely eaten.1 This leads to the notion of “cutting” the hierarchy at one or more positions (Li and Abe, 1998). The modelling task then becomes that of finding the cuts that are maximally general without overgeneralising. Li and Abe (1998) propose a model in which the appropriate cut c is selected according to the Minimum Description Length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising a sum of model description length and data description length. The probability of a predicate p taking as its argument an synset s is modelled as: Pla(s|p,r) = P(s|cs,p,r)P(c|p) (2) where cs,p,r is the portion of the cut learned for p that dominates s. </context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 24(2):217–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Disambiguating nouns, verbs and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="8136" citStr="McCarthy and Carroll (2003)" startWordPosition="1289" endWordPosition="1292"> a Markov model with transitions from senses to senses and from senses to words. The intuition here is that each observation is generated by a “walk” from the root of the hierarchy to a leaf node and emitting the word 1In this paper we use WordNet version 3.0, except where stated otherwise. Pcw(s|p, r) = 171 corresponding to the leaf. Abney and Light (1999) proposed such a model for selectional preferences, trained via EM, but failed to achieve competitive performance on a pseudodisambiguation task. The models described above have subsequently been used in many different studies. For example: McCarthy and Carroll (2003) use Li and Abe’s method in a word sense disambiguation setting; Schulte im Walde et al. (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in which the methods of Resnik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>Diana McCarthy and John Carroll. 2003. Disambiguating nouns, verbs and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4):639–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao-Li Meng</author>
<author>Robert Rosenthal</author>
<author>Donald B Rubin</author>
</authors>
<title>Comparing correlated correlation coefficients.</title>
<date>1992</date>
<journal>Psychological Bulletin,</journal>
<volume>111</volume>
<issue>1</issue>
<contexts>
<context position="25225" citStr="Meng et al. (1992)" startWordPosition="4166" endWordPosition="4169"> top cuts learned by the WN-CUT model from the verb-object training data for a selection of verbs. Table 3 gives quantitative results for the WordNet-based models under consideration, as well as results reported by O´ S´eaghdha (2010) for a purely distributional LDA model with 100 topics and a Maximum Likelihood Estimate model learned from the BNC. In general, the Bayesian WordNet-based models outperform the models of Resnik and Clark and Weir, and are competitive with the state-of-the-art LDA results. To test the statistical significance of performance differences we use the test proposed by Meng et al. (1992) for comparing correlated correlations, i.e., correlation scores with a shared gold standard. The differences between Bayesian WordNet models are not significant (p &gt; 0.05, two-tailed) for any dataset or evaluation measure. However, all Bayesian models improve significantly over Resnik’s and Clark and Weir’s models for multiple conditions. Perhaps surprisingly, the relatively simple WN-CUT model scores the greatest number of significant improvements over both Resnik (7 out of 12 conditions) and Clark and Weir (8 out of 12), though the other Bayesian models do follow close behind. This may sugg</context>
</contexts>
<marker>Meng, Rosenthal, Rubin, 1992</marker>
<rawString>Xiao-Li Meng, Robert Rosenthal, and Donald B. Rubin. 1992. Comparing correlated correlation coefficients. Psychological Bulletin, 111(1):172–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Mixtures of hierarchical topics with Pachinko allocation.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML-07,</booktitle>
<location>Corvallis, OR.</location>
<contexts>
<context position="9466" citStr="Mimno et al., 2007" startWordPosition="1503" endWordPosition="1506">010; Thater et al., 2010), discriminative (Bergsma et al., 2008) or probabilistic ( O´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger and Mooney, 2011). In the probabilistic category, Bayesian models based on the “topic modelling” framework (Blei et al., 2003b) have been shown to achieve state-of-the-art performance in a number of evaluation settings; the models considered in this paper are also related to this framework. In machine learning, researchers have proposed a variety of topic modelling methods where the latent variables are arranged in a hierarchical structure (Blei et al., 2003a; Mimno et al., 2007). In contrast to the present work, these models use a relatively shallow hierarchy (e.g., 3 levels) and any hierarchy node can in principle emit any vocabulary item; they thus provide a poor match for our goal of modelling over WordNet. Boyd-Graber et al. (2007) describe a topic model that is directly influenced by Abney and Light’s Markov model approach; this model (LDAWN) is described further in Section 3.3 below. Reisinger and Pas¸ca (2009) investigate Bayesian methods for attaching attributes harvested from the Web at an appropriate level in the WordNet hierarchy; this task is related in s</context>
</contexts>
<marker>Mimno, Li, McCallum, 2007</marker>
<rawString>David Mimno, Wei Li, and Andrew McCallum. 2007. Mixtures of hierarchical topics with Pachinko allocation. In Proceedings of ICML-07, Corvallis, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="16687" citStr="Neal, 2003" startWordPosition="2780" endWordPosition="2781">5 noun synsets, updates for ci and si can be computed very efficiently, as there are typically few possible synsets for a given word type and few possible cuts for a given synset (the maximum synset depth is 19). The hyperparameters for the various Dirichlet priors are also reestimated in the course of learning; the values of these hyperparameters control the degree of sparsity preferred by the model. The “top-level” hyperparameters a in WN-CUT and K in WN-CUTTOPICS are estimated using a fixed-point iteration proposed by Wallach (2008); the other hyperparameters are learned by slice sampling (Neal, 2003). 3.3 Walk-based models Abney and Light (1999) proposed an approach to selectional preference learning in which arguments are generated for predicates by following a path A = (l1, ... ,l|λ|) from the root of the hierarchy to a leaf node and emitting the corresponding word. The path is chosen according to a Markov model with transition probabilities specific to each predicate. In this model, each leaf node is associated with a single word; the synsets associated with that word are the immediate parent nodes of the leaf. Abney and Light found that their model did not match the performance of Res</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford M. Neal. 2003. Slice sampling. Annals of Statistics, 31(3):705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
<author>Anna Korhonen</author>
</authors>
<title>Probabilistic models of similarity in syntactic context.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP-11,</booktitle>
<location>Edinburgh, UK.</location>
<marker>S´eaghdha, Korhonen, 2011</marker>
<rawString>Diarmuid O´ S´eaghdha and Anna Korhonen. 2011. Probabilistic models of similarity in syntactic context. In Proceedings of EMNLP-11, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL-10,</booktitle>
<location>Uppsala,</location>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings of ACL-10, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Patrick Pantel</author>
</authors>
<title>Ontologizing semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of COLINGACL-06,</booktitle>
<location>Sydney, Australia.</location>
<marker>Pennacchiotti, Pantel, 2006</marker>
<rawString>Marco Pennacchiotti and Patrick Pantel. 2006. Ontologizing semantic relations. In Proceedings of COLINGACL-06, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Rayner</author>
<author>Tessa Warren</author>
<author>Barbara J Juhasz</author>
<author>Simon P Liversedge</author>
</authors>
<title>The effect of plausibility on eye movements in reading.</title>
<date>2004</date>
<journal>Journal of Experimental Psychology: Learning Memory and Cognition,</journal>
<volume>30</volume>
<issue>6</issue>
<contexts>
<context position="1587" citStr="Rayner et al., 2004" startWordPosition="237" endWordPosition="240"> The concept of selectional preference captures the intuitive fact that predicates in language have a better semantic “fit” for certain arguments than others. For example, the direct object argument slot of the verb eat is more plausibly filled by a type of food (I ate a pizza) than by a type of vehicle (I ate a car), while the subject slot of the verb laugh is more plausibly filled by a person than by a vegetable. Human language users’ knowledge about selectional preferences has been implicated in analyses of metaphor processing (Wilks, 1978) and in psycholinguistic studies of comprehension (Rayner et al., 2004). In Natural Language Processing, automatically acquired preference models have been shown to aid a number of tasks, including semantic role labelling (Zapirain et al., 2009), parsing (Zhou et al., 2011) and lexical disambiguation (Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011). It is tempting to assume that with a large enough corpus, preference learning reduces to a simple language modelling task that can be solved by counting predicate-argument co-occurrences. Indeed, Keller and Lapata (2003) show that relatively good performance at plausibility estimation can be attained by submitti</context>
</contexts>
<marker>Rayner, Warren, Juhasz, Liversedge, 2004</marker>
<rawString>Keith Rayner, Tessa Warren, Barbara J. Juhasz, and Simon P. Liversedge. 2004. The effect of plausibility on eye movements in reading. Journal of Experimental Psychology: Learning Memory and Cognition, 30(6):1290–1301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond Mooney</author>
</authors>
<title>Crosscutting models of lexical semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP-11,</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="3266" citStr="Reisinger and Mooney, 2011" startWordPosition="490" endWordPosition="493">els trained on smaller corpora for less frequent predicate-argument combinations. Models that induce a level of semantic representation, such as probabilistic latent variable models, have a further advantage in that they can provide rich structured information for downstream tasks such as lexical disambiguation ( O´ S´eaghdha and Korhonen, 2011) and semantic relation mining (Yao et al., 2011). Recent research has investigated the potential of Bayesian probabilistic models such as Latent Dirichlet Allocation (LDA) for modelling selectional preferences ( O´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger and Mooney, 2011). These models are flexible and robust, yielding superior performance compared to previous approaches. In this paper we present a preliminary study of analogous 170 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 170–179, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics models that make use of a lexical hierarchy (in our case the WordNet hierarchy). We describe two broad classes of probabilistic models over WordNet and how they can be implemented in a Bayesian framework. The two main potential advantages of incorporating WordNet in</context>
<context position="8999" citStr="Reisinger and Mooney, 2011" startWordPosition="1430" endWordPosition="1433">od for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in which the methods of Resnik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone. These methods can be vectorbased (Erk et al., 2010; Thater et al., 2010), discriminative (Bergsma et al., 2008) or probabilistic ( O´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger and Mooney, 2011). In the probabilistic category, Bayesian models based on the “topic modelling” framework (Blei et al., 2003b) have been shown to achieve state-of-the-art performance in a number of evaluation settings; the models considered in this paper are also related to this framework. In machine learning, researchers have proposed a variety of topic modelling methods where the latent variables are arranged in a hierarchical structure (Blei et al., 2003a; Mimno et al., 2007). In contrast to the present work, these models use a relatively shallow hierarchy (e.g., 3 levels) and any hierarchy node can in pri</context>
</contexts>
<marker>Reisinger, Mooney, 2011</marker>
<rawString>Joseph Reisinger and Raymond Mooney. 2011. Crosscutting models of lexical semantics. In Proceedings of EMNLP-11, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Marius Pas¸ca</author>
</authors>
<title>Latent variable models of concept-attribute attachment.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP-09,</booktitle>
<location>Suntec, Singapore.</location>
<marker>Reisinger, Pas¸ca, 2009</marker>
<rawString>Joseph Reisinger and Marius Pas¸ca. 2009. Latent variable models of concept-attribute attachment. In Proceedings of ACL-IJCNLP-09, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4843" citStr="Resnik (1993)" startWordPosition="729" endWordPosition="730">rmance than previously-proposed WordNet-based methods on a plausibility estimation task and are particularly wellsuited to estimating plausibility for arguments that were not seen in training and for which LDA cannot make useful predictions. 2 Background and Related Work The WordNet lexical hierarchy (Fellbaum, 1998) is one of the most-used resources in NLP, finding many applications in both the definition of tasks (e.g. the SENSEVAL/SemEval word sense disambiguation tasks) and in the construction of systems. The idea of using WordNet to define selectional preferences was first implemented by Resnik (1993), who proposed a measure of associational strength between a semantic class s and a predicate p corresponding to a relation type r: 1 A(s,p,r) = ηP(s|p,r)lo92 PP(IIr)) (1) where η is a normalisation term. This measure captures the degree to which the probability of seeing s given the predicate p differs from the prior probability of s. Given that we are often interested in the preference of p for a word w rather than a class and words generally map onto multiple classes, Resnik suggests calculating A(s, p, r) for all classes that could potentially be expressed by w and predicting the maximal v</context>
<context position="22461" citStr="Resnik (1993)" startWordPosition="3716" endWordPosition="3717"> an estimate of the joint probability P(w, p) we multiply the predicted P(w1p) by a predicate probability P(pIr) estimated from relative corpus frequencies. In training we use a burn-in period of 200 iterations, after which hyperparameters are reestimated and P(plr) predictions are sampled every 50 iterations. All probability estimates are log-transformed to match the gold standard judgements. In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of Brockmann and Lapata (2003): Resnik (1993) and Clark and Weir (2002). For Resnik’s model we used WordNet 2.1 rather than WordNet 3.0 as the former has multiple roots, a property that turns out to be necessary for good performance. Clark and Weir’s method requires that the user specify a significance threshold a to be used in deciding where to cut; to give it the best possible chance we tested with a range of values (0.05, 0.3, 0.6, 0.9) and report results for the best-performing setting, which consistently was a = 0.9. One can also use different statistical hypothesis tests; again we choose the test giving the best results, which was </context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Philip Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent Dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings ACL-10,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="3237" citStr="Ritter et al., 2010" startWordPosition="486" endWordPosition="489">d by more complex models trained on smaller corpora for less frequent predicate-argument combinations. Models that induce a level of semantic representation, such as probabilistic latent variable models, have a further advantage in that they can provide rich structured information for downstream tasks such as lexical disambiguation ( O´ S´eaghdha and Korhonen, 2011) and semantic relation mining (Yao et al., 2011). Recent research has investigated the potential of Bayesian probabilistic models such as Latent Dirichlet Allocation (LDA) for modelling selectional preferences ( O´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger and Mooney, 2011). These models are flexible and robust, yielding superior performance compared to previous approaches. In this paper we present a preliminary study of analogous 170 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 170–179, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics models that make use of a lexical hierarchy (in our case the WordNet hierarchy). We describe two broad classes of probabilistic models over WordNet and how they can be implemented in a Bayesian framework. The two main potential advantage</context>
<context position="8970" citStr="Ritter et al., 2010" startWordPosition="1426" endWordPosition="1429">deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in which the methods of Resnik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone. These methods can be vectorbased (Erk et al., 2010; Thater et al., 2010), discriminative (Bergsma et al., 2008) or probabilistic ( O´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger and Mooney, 2011). In the probabilistic category, Bayesian models based on the “topic modelling” framework (Blei et al., 2003b) have been shown to achieve state-of-the-art performance in a number of evaluation settings; the models considered in this paper are also related to this framework. In machine learning, researchers have proposed a variety of topic modelling methods where the latent variables are arranged in a hierarchical structure (Blei et al., 2003a; Mimno et al., 2007). In contrast to the present work, these models use a relatively shallow hierarchy (e.g., 3 levels) and </context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent Dirichlet allocation method for selectional preferences. In Proceedings ACL-10, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Christian Hying</author>
<author>Christian Scheible</author>
<author>Helmut Schmid</author>
</authors>
<title>Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08:HLT,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="8231" citStr="Walde et al. (2008)" startWordPosition="1306" endWordPosition="1309">that each observation is generated by a “walk” from the root of the hierarchy to a leaf node and emitting the word 1In this paper we use WordNet version 3.0, except where stated otherwise. Pcw(s|p, r) = 171 corresponding to the leaf. Abney and Light (1999) proposed such a model for selectional preferences, trained via EM, but failed to achieve competitive performance on a pseudodisambiguation task. The models described above have subsequently been used in many different studies. For example: McCarthy and Carroll (2003) use Li and Abe’s method in a word sense disambiguation setting; Schulte im Walde et al. (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in which the methods of Resnik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone. These methods can be vectorbase</context>
</contexts>
<marker>Walde, Hying, Scheible, Schmid, 2008</marker>
<rawString>Sabine Schulte im Walde, Christian Hying, Christian Scheible, and Helmut Schmid. 2008. Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences. In Proceedings of ACL-08:HLT, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Automatic metaphor interpretation as a paraphrasing task.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT-10,</booktitle>
<location>Los Angeles, CA.</location>
<contexts>
<context position="8350" citStr="Shutova (2010)" startWordPosition="1328" endWordPosition="1329">paper we use WordNet version 3.0, except where stated otherwise. Pcw(s|p, r) = 171 corresponding to the leaf. Abney and Light (1999) proposed such a model for selectional preferences, trained via EM, but failed to achieve competitive performance on a pseudodisambiguation task. The models described above have subsequently been used in many different studies. For example: McCarthy and Carroll (2003) use Li and Abe’s method in a word sense disambiguation setting; Schulte im Walde et al. (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in which the methods of Resnik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone. These methods can be vectorbased (Erk et al., 2010; Thater et al., 2010), discriminative (Bergsma et al., 2008) or probabilistic ( O´ S´eaghdha, 2010;</context>
</contexts>
<marker>Shutova, 2010</marker>
<rawString>Ekaterina Shutova. 2010. Automatic metaphor interpretation as a paraphrasing task. In Proceedings of NAACL-HLT-10, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL-10,</booktitle>
<location>Uppsala,</location>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of ACL-10, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Philip Michalak</author>
<author>Lenhart K Schubert</author>
</authors>
<title>Deriving generalized knowledge from corpora using WordNet abstraction.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL-09,</booktitle>
<location>Athens, Greece.</location>
<marker>Van Durme, Michalak, Schubert, 2009</marker>
<rawString>Benjamin Van Durme, Philip Michalak, and Lenhart K. Schubert. 2009. Deriving generalized knowledge from corpora using WordNet abstraction. In Proceedings of EACL-09, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
</authors>
<title>Structured Topic Models for Language.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="16617" citStr="Wallach (2008)" startWordPosition="2769" endWordPosition="2770">ependently for the benefit of efficiency. Although WordNet contains 82,115 noun synsets, updates for ci and si can be computed very efficiently, as there are typically few possible synsets for a given word type and few possible cuts for a given synset (the maximum synset depth is 19). The hyperparameters for the various Dirichlet priors are also reestimated in the course of learning; the values of these hyperparameters control the degree of sparsity preferred by the model. The “top-level” hyperparameters a in WN-CUT and K in WN-CUTTOPICS are estimated using a fixed-point iteration proposed by Wallach (2008); the other hyperparameters are learned by slice sampling (Neal, 2003). 3.3 Walk-based models Abney and Light (1999) proposed an approach to selectional preference learning in which arguments are generated for predicates by following a path A = (l1, ... ,l|λ|) from the root of the hierarchy to a leaf node and emitting the corresponding word. The path is chosen according to a Markov model with transition probabilities specific to each predicate. In this model, each leaf node is associated with a single word; the synsets associated with that word are the immediate parent nodes of the leaf. Abney</context>
</contexts>
<marker>Wallach, 2008</marker>
<rawString>Hanna Wallach. 2008. Structured Topic Models for Language. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>Making preferences more active.</title>
<date>1978</date>
<journal>Artificial Intelligence,</journal>
<pages>11--197</pages>
<contexts>
<context position="1516" citStr="Wilks, 1978" startWordPosition="229" endWordPosition="230">bility for items that were not seen in training. 1 Introduction The concept of selectional preference captures the intuitive fact that predicates in language have a better semantic “fit” for certain arguments than others. For example, the direct object argument slot of the verb eat is more plausibly filled by a type of food (I ate a pizza) than by a type of vehicle (I ate a car), while the subject slot of the verb laugh is more plausibly filled by a person than by a vegetable. Human language users’ knowledge about selectional preferences has been implicated in analyses of metaphor processing (Wilks, 1978) and in psycholinguistic studies of comprehension (Rayner et al., 2004). In Natural Language Processing, automatically acquired preference models have been shown to aid a number of tasks, including semantic role labelling (Zapirain et al., 2009), parsing (Zhou et al., 2011) and lexical disambiguation (Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011). It is tempting to assume that with a large enough corpus, preference learning reduces to a simple language modelling task that can be solved by counting predicate-argument co-occurrences. Indeed, Keller and Lapata (2003) show that relatively </context>
</contexts>
<marker>Wilks, 1978</marker>
<rawString>Yorick Wilks. 1978. Making preferences more active. Artificial Intelligence, 11:197–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Aria Haghighi</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Structured relation discovery using generative models.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP-11,</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="3034" citStr="Yao et al., 2011" startWordPosition="457" endWordPosition="460">ic roles) that cannot be retrieved by keyword search and for applications where accurate models of rarer words are required. O´ S´eaghdha (2010) shows that the Webbased approach is reliably outperformed by more complex models trained on smaller corpora for less frequent predicate-argument combinations. Models that induce a level of semantic representation, such as probabilistic latent variable models, have a further advantage in that they can provide rich structured information for downstream tasks such as lexical disambiguation ( O´ S´eaghdha and Korhonen, 2011) and semantic relation mining (Yao et al., 2011). Recent research has investigated the potential of Bayesian probabilistic models such as Latent Dirichlet Allocation (LDA) for modelling selectional preferences ( O´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger and Mooney, 2011). These models are flexible and robust, yielding superior performance compared to previous approaches. In this paper we present a preliminary study of analogous 170 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 170–179, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics models that make use of a lexical </context>
</contexts>
<marker>Yao, Haghighi, Riedel, McCallum, 2011</marker>
<rawString>Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew McCallum. 2011. Structured relation discovery using generative models. In Proceedings of EMNLP-11, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Be˜nat Zapirain</author>
<author>Eneko Agirre</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Generalizing over lexical features: Selectional preferences for semantic role classification.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP-09,</booktitle>
<marker>Zapirain, Agirre, M`arquez, 2009</marker>
<rawString>Be˜nat Zapirain, Eneko Agirre, and Llu´ıs M`arquez. 2009. Generalizing over lexical features: Selectional preferences for semantic role classification. In Proceedings of ACL-IJCNLP-09, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
<author>Kang Liu</author>
<author>Li Cai</author>
</authors>
<title>Exploiting web-derived selectional preference to improve statistical dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-11,</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="1790" citStr="Zhou et al., 2011" startWordPosition="268" endWordPosition="271">f the verb eat is more plausibly filled by a type of food (I ate a pizza) than by a type of vehicle (I ate a car), while the subject slot of the verb laugh is more plausibly filled by a person than by a vegetable. Human language users’ knowledge about selectional preferences has been implicated in analyses of metaphor processing (Wilks, 1978) and in psycholinguistic studies of comprehension (Rayner et al., 2004). In Natural Language Processing, automatically acquired preference models have been shown to aid a number of tasks, including semantic role labelling (Zapirain et al., 2009), parsing (Zhou et al., 2011) and lexical disambiguation (Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011). It is tempting to assume that with a large enough corpus, preference learning reduces to a simple language modelling task that can be solved by counting predicate-argument co-occurrences. Indeed, Keller and Lapata (2003) show that relatively good performance at plausibility estimation can be attained by submitting queries to a Web search engine. However, there are many scenarios where this approach is insufficient: for languages and language domains where Web-scale data is unavailable, for predicate types (e.g.</context>
</contexts>
<marker>Zhou, Zhao, Liu, Cai, 2011</marker>
<rawString>Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve statistical dependency parsing. In Proceedings of ACL-11, Portland, OR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>