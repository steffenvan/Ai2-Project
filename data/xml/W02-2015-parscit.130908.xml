<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001280">
<title confidence="0.903351">
Distinguishing Easy and Hard Instances
</title>
<author confidence="0.99452">
Yuval Krymolowski
</author>
<affiliation confidence="0.800124333333333">
Department of Computer Science
Bar-Ilan University
52900 Ramat Gan, Israel
</affiliation>
<sectionHeader confidence="0.874331" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999873352941177">
Error analysis is a key step in developing sta-
tistical parsers. In doing this, we manually dis-
cover typical cases by examining parser output.
In this paper we argue that the process can be
speeded up by considering the output from an
ensemble of parsers. We do this by resampling
small proportions (10% and up) from the train-
ing data, and exploiting the high diversity of the
resulting parsers - resulting from the sparseness
of natural-language data. Varying the sample
size, we can trace the gradual learning of each
instance and classify instances into a few types.
This division helps in distinguishing instances
which are hard for the system, from instances
which may be learned in principle. We suggest
that such analysis can yield a qualitative ap-
proach to evaluation of statistical parsers.
</bodyText>
<sectionHeader confidence="0.996297" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999587239583334">
The task of parsing can be viewed as a detection
task, where the goal of the parser is to detect
instances of structures witnin the sentence. Ac-
cordingly, recall and precision are used in mea-
suring the parser performance.
When we say &amp;quot;Recall is 80%&amp;quot;, do we mean:
i. in the given test set, the chance of a single
instance to be detected is 80%? or
ii. in an arbitrary sample, 80% of the in-
stances will be detected?
In general, we have a single test data set at
hand. Assuming it is representative, we adopt
the second interpretation and conjecture that
the recall carries to other data sets. There
still remains, however, the question from the
first interpretation: what are the chances of an
instance to be detected? Or in more general
terms: how easy or hard each instance is?
Some instances in natural language (NL) data
are very easy to detect, or to classify correctly.
For example, words with only one possible part-
of-speech (POS) are easy to tag, prepositional
phrases that include &amp;quot;of&amp;quot; are easy to attach, and
NPs with a very common structure are easy to
detect. On the other hand, some instances may
be very hard to detect with statistical methods
due to the inherent sparseness of NL data.
When analyzing the errors made in a single
run of a supervised parser, we can discover prob-
lematic cases by observing recurring errors. But
results from a single run show a partial picture,
because they are sensitive to peculiarities in the
training set. Such results may reflect failures or
successes that are not due to properties of the
model, but to a random balance of positive and
negative evidence for certain types of instances.
If we knew, for a certain learning system,
which types of instances are easy and which
are hard to detect, and how &amp;quot;easy&amp;quot; or &amp;quot;hard&amp;quot;
the individual instances are, this would provide
us with means to qualitatively analyze the sys-
tem&apos;s performance. This, in turn, would enable
us to compare systems that are similar accord-
ing to recall and precision. In such cases, it is
possible that the systems differ in their ability to
handle certain types of instances in a way which
recall and precision do not reflect. Such a com-
parison would be qualitative in nature, unlike
statistical significance tests such as McNemar&apos;s
test whose goal is different.
In this paper, we propose a method which al-
lows to study the easiness and hardness of test
instances. The method relies on training the
model on samples from the training data, there-
by creating multiple parsers, and recording the
instances detected each time.
We use the number of parsers that detected
the instance as a measure of easiness. Due to
the Zipfian nature of NL data, training samples
are likely to differ in low-count instances. As
each parser uses the features that were present
in its training set, the parsers will differ with
respect to the set of features they use. Each
parser will provide a different viewpoint of the
target structures. It is expected that easy in-
stances will be detected by all or most of the
parsers, while hard instances — missed by most
of the parsers. Moreover, as we increase the
sample size, we can trace the learning process
at the instance level and characterize instances
according to the variation of their easiness.
The idea of using an ensemble of supervised
systems, trained on different samples, for mak-
ing observations regarding individual instances
is common in the frameworks of classifier com-
bination and selective sampling. The boosting
approach (Freund and Schapire, 1995) creates a
classifier ensemble by training a system on sam-
ples drawn with preference to hard instances.
Skalak (1997) suggests a more elaborated tax-
onomy of easiness and hardness levels, derived
from leave-one-out results. He uses this infor-
mation in order to remove uncharacteristic in-
stances from the training set.
Query by committee (Seung et al., 1992) ap-
proaches select training instances for which the
disagreement between classifiers is the highest.
Abe and Mamitsuka (1998), proposes to obtain
the classifier collection by sampling from the
training set with a uniform distribution (as in
bagging (Breiman, 1996)) or with preference to
hard instances as in boosting.
We propose to use the ensemble of supervised
parsers for error analysis. We demonstrate our
method with a statistical memory-based shallow
parsing algorithm and the task of NP detection.
</bodyText>
<sectionHeader confidence="0.951848" genericHeader="method">
2 Learning Algorithm
</sectionHeader>
<bodyText confidence="0.999960230769231">
The experiments were carried out using the
memory-based sequence learning algorithm pre-
sented by Dagan and Krymolowski (2001, here-
after MBSL). The system considers POS se-
quences at the beginning or end of target in-
stances, NPs in our case. For each POS se-
quence, it records the number of times it ap-
pears in the beginning or end of an NP, as well
as anywhere else in the corpus. For composi-
tional NPs, the embedded NPs are treated in
the same way as POS. This provides an addi-
tional level of abstraction. Figure 1 presents
sample training data.
The input for parsing is a POS sequence, rep-
resenting a tagged sentence. MBSL tests each
subsequence, with its context, as a candidate
for being an NP instance. Single words are test-
ed first, then two-word subsequences and so on
in increasing order of length. This allows the
algorithm to use embedded NPs when testing
composite ones.
If a subseqeuence corresponds to a real in-
stance, then ideally, the entire POS string with
its context would appear a number of times in
the training data as an instance, and not in oth-
er structures. For example, the sequence&apos;
</bodyText>
<equation confidence="0.261676">
&amp;quot;VBZ DT NNS IN NNP .&amp;quot;
</equation>
<bodyText confidence="0.866076285714286">
appears once in Figure 1, third sentence, as
an NP instance with one word in each side:
&amp;quot;VBZ [Ni, DT NNS IN NNP NP] .
In the more general case, MBSL tries to re-
construct the POS sequence from prefixes and
suffixes of NPs in the training data, which we
term tiles. For example, the NP
</bodyText>
<equation confidence="0.380333">
&amp;quot;[NP DT JJ NN NNS IN NNP NP]
</equation>
<bodyText confidence="0.999854666666667">
has support for the prefix tile &amp;quot; [Np DT JJ NN&amp;quot;
from the two first NPs in sentences 2 and 3,
and for the suffix tiles &amp;quot;IN NNP Np]&apos; &amp;quot;NNS IN
NNP NO&amp;quot; from the first NP in sentence 2 and
the second NP in sentence 3 respectively.
The algorithm takes negative evidence into
account as well. For example, the prefix tile
[Np JJ&amp;quot; appears in sentence 1, but &amp;quot;JJ&amp;quot; ap-
pears twice in other positions, that is, not as
the first word in an NP. These two appear-
ances constitute a negative evidence for this tile,
which overshadows the single positive appear-
ance, therefore MBSL will not use this tile.
Each NP candidate is given a score, the algo-
rithm uses this score in disambiguation. MBSL
considers up to a specified number c of context
words, before and after an NP, in tiles. Here we
use c = 2.
</bodyText>
<sectionHeader confidence="0.995203" genericHeader="method">
3 Easiness
</sectionHeader>
<bodyText confidence="0.727858714285714">
Let T denote the set of NPs in the test data,
and S a set of n training-set samples used for
1-We use the Penn Treebank set of POS
tags: DT=determiner, JJ=adjective, RB=adverb,
VBD=verb in past tense, VBN=passive verb, VB=verb,
IN=preposition, NN=singular noun, NNS=plural noun,
and CC=coordinating conjunction.
</bodyText>
<listItem confidence="0.972827">
1. [NP NNS NP] [vp VBZ RB [NP JJ NNS NP] vP] •
2. [NP DT JJ NN IN NNP NP] [vP VBZ [Np DT NN NI)] vP]
3 • [NP DT JJ NN NN NP] [vP VBZ [NP DT NNS IN NNP NI)] VP]
</listItem>
<figureCaption confidence="0.993601">
Figure 1: An example of training data for MBSL
</figureCaption>
<bodyText confidence="0.97092004">
training a supervised parser. We refer to the
parser trained on the ith sample as &amp;quot;parser i&amp;quot;.
Each instance a E T can be characterized by a
bit-vector
vs(a) = (vi(a),... , vri(a)) ,
where
11 a was detected by parser i
vi(a) = a was not detected by parser i .
The vector vs(a) is the detection profile of a
according to the set of samples S.
We extract from the detection profile the pro-
portion
number of &apos;1&apos;s in vs (a)
easinesss (a) =
which is the probability of detecting a by one of
the parsers in the ensemble. For easy and hard
instances, easinesss(a) will be close to 1 and 0
respectively. The easiness does not depend only
on the instance, but also on the training sam-
ples. We will discuss this issue further in the
experiment section. In this paper we restrict
our study to NPs marked in an annotated cor-
pus, all of these NPs are therefore correct. In
the general case, an instance can have a high
easiness but still not be correct.
</bodyText>
<sectionHeader confidence="0.9015385" genericHeader="method">
4 Experiments
4.1 Data
</sectionHeader>
<bodyText confidence="0.99971175">
The training data used in our experiments con-
sisted of Penn Treebank WSJ (Marcus et al.,
1993) Sections 15-18, with Section 20 as test.
These data sets were used by Ramshaw and
Marcus (1995) and CoNLL-2000 shared task
(Tjong Kim Sang and Buchholz, 2000) and have
become a common testbed for shallow parsing
tasks. Table 1 shows the number of sentences
and NPs in the training and test data. We
counted compositional NPs separately, as their
structure is more complicated than that of base
NPs.
</bodyText>
<table confidence="0.999741285714286">
Training Test
Dataset WSJ 15-18 WSJ 20
Sentences 8936 2012
NPs: 50860 11401
Base 18472 4398
Compositional 69332 15799
Total
</table>
<tableCaption confidence="0.997864">
Table 1: Training and test data
</tableCaption>
<table confidence="0.999975666666667">
Psamp 10% 25% 50% 80% 95%
Avg. rec.% 81 83 84 85 85
Easiness: proportion in test data
0 3 4 5 8 10
0-0.1 9 9 10 12 14
0.1-0.2 3 2 2 1 1
0.2-0.3 2 2 1 1 0
0.3-0.4 2 2 1 1 0
0.4-0.5 2 2 1 0 0
0.5-0.6 2 2 2 1 0
0.6-0.7 2 2 1 1 0
0.7-0.8 3 2 2 2 0
0.8-0.9 5 3 2 2 1
0.9-1 70 74 78 81 84
1 52 61 67 74 78
</table>
<tableCaption confidence="0.988491">
Table 2: A summary of five resampling exper-
</tableCaption>
<bodyText confidence="0.92150475">
iments: sample size, average recall, and distri-
bution of easiness. Instances with easiness of
0 and 1 are counted within the corresponding
ranges as well as separately.
</bodyText>
<subsectionHeader confidence="0.975157">
4.2 Estimating Easiness
</subsectionHeader>
<bodyText confidence="0.999871181818182">
We conducted a few resampling experiments,
each with a different sampling proportion
10% &lt; Psamp &lt; 99.75% of the training data.
1000 samples were taken in each experiment,
resulting in an ensemble of n = 1000 parsers.
Table 2 presents a summary of five resam-
pling experiments. For reference, the recall of
the parser trained on the complete data is 85%.
We see that the average recall rises as samples
grow.
As Psamp is increased, there is more overlap
between individual training samples. For exam-
ple, two samples of 90% each will share at least
80% of the training data, while at the other ex-
treme, two 10% samples are typically disjoint.
This effect results in more similar parsers when
the samples are larger. We therefore see an in-
crease in the number of instances with extreme
easiness values near 0 and 1 for larger Psamp.
Ultimately, with the full training sample, each
instance is either detected or missed, therefore,
for each a E T
</bodyText>
<equation confidence="0.988185">
lim easinesss(a) = 0 or 1.
Psamp—)-100%
</equation>
<bodyText confidence="0.9790795">
where S represents a set of training samples
drew with proportion Psamp.
When the training samples are small, only
very easy instances will have easiness 1, but
instances with easiness 0 may not necessarily
be hard. The converse happens when the sam-
ples are large: hard instances will have lower
easiness values but instances with easiness 1
need not be very easy. In order to get a de-
tailed view, we examined, for each a E T, how
its easiness varies with the sample size.
Figure 2 shows four patterns of easiness
change. The curves can be viewed as &amp;quot;learn-
ing curves&amp;quot; for individual instances. As table
3 shows, most of the instances start with easi-
ness values above 0.5 for small samples, which
increase as the sample size grows. The high-
er the initial easiness, the faster it gets to the
value of 1. As for the instances with initial easi-
ness below 0.5 — they end up mostly undetected
by the parser trained on the full training set.
Such a behaviour coincides with our usual view
of learning.
This is not, however, the whole story. Some
instances start with very low easiness and yet
end with a value of 1 (&amp;quot;learned&amp;quot; instances),
while others start with high easiness but even-
tually are not detected (&amp;quot;forgotten&amp;quot; instances).
We explain both cases by a borderline phe-
nomenon for which there is little negative ev-
idence in the first case, and little positive evi-
dence in the second. For example, the construc-
tion &amp;quot;NP IN NP&amp;quot; may or may not be a composi-
tional noun. When most such constructions in
the training sample are not NPs, incidental bal-
ance of features in a small sample can still result
in detecting such instances, whereas larger sam-
ples are less noisy and therefore less susceptible
to this effect.
Training sample size (% from training set)
</bodyText>
<figureCaption confidence="0.994425">
Figure 2: Four patterns of the easiness change
with sampling proportion Psamp.
</figureCaption>
<table confidence="0.999298857142857">
Easiness Full Sample
Psamp = 10%
Missed Detected
0-50% 13% 5%
Hard Learned
50-100% 1.5% 80.5%
Forgotten Easy
</table>
<tableCaption confidence="0.9945">
Table 3: Frequencies of instances exhibiting the
four types of easiness change
</tableCaption>
<subsectionHeader confidence="0.99841">
4.3 Easiness and Bagging
</subsectionHeader>
<bodyText confidence="0.999868928571429">
Taking small samples has the advantage of pro-
ducing parsers which disagree more with one
another, and can provide finer distinctions be-
tween instances. In memory-based learning,
where it is important to keep all the available
evidence from the training data (Daelemans et
al., 1999), small samples have an advantage in
requiring less space - with a tradeoff in perfor-
mance.
In order to study whether we can still get a
good performance with small samples, we ran
bagging (Breiman, 1996) experiments. We used
Psamp = 10% and tried a range of thresholds.
For each threshold 0, we selected the instances
</bodyText>
<figure confidence="0.991316923076923">
= Instances
- Learned
00
20 40 60 80 100 20 40 60 80
20 40 60 80 100 20
40 60 80
100
Easy Instances
Hard Instances
Instances Forgotten
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
</figure>
<bodyText confidence="0.999892">
with easiness greater than 0, and calculated the
recall, precision, and Fo with ,i3 = 1. The re-
sults are presented in Table 4 along with the
performance of the parser trained on the full
training set. As we see, it is possible to achieve
a performance similar to that of the full model
by bagging parsers trained on small samples of
the training set.
As we use a single data set and a single
method for this work, it is hard to say whether
this holds for other tasks as well. This result
may indicate that a small training material is
sufficient for analyzing the particular test set
(as also implied by the high recall values for
low psamp in table 2), we leave that for future
research. In particular, while thresholds of 30%
and 40% yielded recall and precision similar to
those obtained by training on the full data set,
it is not possible to say at present whether this
is due to the task, the system, or the data sets
in use.
</bodyText>
<table confidence="0.999661">
Threshold Recall Precision Fo
20% 88.3 65.5 75.2
30% 86.1 70.1 77.2
40% 84.0 73.4 78.3
50% 81.9 76.5 79.1
60% 79.5 79.0 79.3
70% 77.1 81.4 79.2
Full 85.0 71.4 77.6
</table>
<tableCaption confidence="0.9842975">
Table 4: Results of bagging experiment with
10% samples
</tableCaption>
<sectionHeader confidence="0.996538" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999982565789474">
Analyzing the results of a single run of a super-
vised parser, we can find frequent errors. This
information is, however, limited to that run and
does not always reflect why an instance was
not detected. Possible reasons can be that the
system cannot find supporting evidence in the
training data, or due to a random balance of
supporting and contradicting evidence for that
instance. Distinguishing between errors of these
types and, more generally, tracing the learning
or &amp;quot;forgetting&amp;quot; patterns of instances, are im-
portant steps in analyzing the performance of a
supervised parser. Combined with a clustering
approach (Krymolowski and Marx, 2002), we
may be able to group together instances with
similar behaviour and structure, and speed up
the process of error analysis.
When a model is probabilistic (e.g., DOP
(Scha et al., 1999)), we can intuitively observe
that instances that get a high probability are
easy while those with low probability are hard.
For models represented as a separator in an ab-
stract numeric feature space (e.g., SNoW (Roth,
1998)), the distance from the separator can be
an indication of easiness. Assuming the sepa-
rator fluctuates within a bounded area of space
for different training samples, the easy instances
are those within a safe distance from that area,
while the hard ones are more sensitive to errors
resulting from noise in the training samples.
In this work, we proposed a method for esti-
mating the easiness of instances which is suit-
able even for a non-probabilistic model or model
which is not represented in an abstract numeric
feature space. The method relies on generating
an ensemble of parsers by resampling from the
training data. We studied the effect of sample
size on the distribution of easiness in a test sam-
ple, and presented learning curves for individual
instances. The curves can help in finding easy
instances that are being learned in the common
way, as well as instances affected by noise, or
for which little evidence does exist in the data
although the full model misses them.
In using training samples, we took advantage
of the Zipfian distribution of natural-language
data. This yields samples that differ in the low-
count instances they contain which, in turn, in-
creases the difference between parsers trained
on different samples. Given a sampling propor-
tion, we sampled randomly from the data. It
might be possible to increase the diversity a-
mong parsers by sampling chunks of sentences.
As the style within a chunk is more uniform
than within a collection of texts, this could fo-
cus each parser on a smaller number of instance
types. This will sharpen the coverage differ-
ences between parsers (in expense of the cover-
age of each one).
As further work, we plan to investigate the
relation between easiness and concepts like
typicality (Zhang, 1992) and class prediction
strength (e.g. Hoste and Daelemans (2000)).
We also plan to study the extent to which easi-
ness depends on the feature set used by the su-
pervised parser, and compare a number of sys-
tems in order to find instances which are hard
or easy for most of the methods (cf. Pedersen
(2002) for the word-sense disambiguation task).
We hope this would provide a means for qualita-
tive comparison between systems. Further yet,
we hope this would contribute to a more focused
use of the individual learning methods, possibly
in combination with hand-coded rules, saving
learning effort for the cases where it is more
needed.
</bodyText>
<sectionHeader confidence="0.992594" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99995575">
The author thanks Zvika Marx for very helpful
discussions, as well as Adam Kilgarriff, Miles
Osborne, and the anonymous reviewers for their
interesting and important comments.
</bodyText>
<sectionHeader confidence="0.997806" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998973902777778">
N. Abe and H. Mamitsuka. 1998. Query learn-
ing strategies using boosting and bagging. In
Proceedings of ICML &apos;98, pages 1-10.
L. Breiman. 1996. Bagging predictors. Ma-
chine Learning, 24:123-140.
Walter Daelemans, Antal van den Bosch, and
Jakub Zavrel. 1999. Forgetting exception-
s is harmful in language learning. Machine
Learning, 34:11-44.
I. Dagan and Y. Krymolowski. 2001. Com-
positional memory-based partial parsing.
In R. Bod, R. Scha, and K. Sima&apos;an,
editors, Data-Oriented Parsing, chapter
11.6. CSLI Publications, Stanford. to appear,
http://turing.wins.uva.n1/—rens/dopbook.html.
Y. Freund and R. E. Schapire. 1995. A decision-
theoretic generalization of on-line learning
and an application to boosting. In Pro-
ceedings of the 2&apos;d European Conference on
Computational Learning Theory, pages 23-
37. Springer-Verlag.
Véronique Hoste and Walter Daelemans. 2000.
Comparing bagging and boosting for natural
language processing tasks: a typicality ap-
proach. In Ad Feelders, editor, Proceedings
of Benelearn 2000, pages 101-108.
Yuval Krymolowski and Zvika Marx. 2002.
Clustering the space of phrases identified by
an ensemble of supervised shallow parsers. In
Proceedings of ICML &apos;02 Workshop on Text
Learning (TextML-2002), Sydney, Australia,
July.
M. P. Marcus, B. Santorini, and
M. Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313-330, June.
Ted Pedersen. 2002. Assessing system agree-
ment and instance difficulty in the lexical
sample tasks of senseval-2. In Proceedings of
the Workshop on Word Sense Disambigua-
tion: Recent Successes and Future Directions,
July.
L. A. Ramshaw and M. P. Marcus. 1995. Tex-
t chunking using transformation-based learn-
ing. In Proceedings of the Third Workshop on
Very Large Corpora.
D. Roth. 1998. Learning to resolve natural lan-
guage ambiguities: A unified approach. In
proc. of the Fifteenth National Conference on
Artificial Intelligence, pages 806-813, Menlo
Park, CA, USA, July. AAAI Press.
Remko Scha, Rens Bod, and Khalil Sima&apos;an.
1999. A memory-based model of syntactic
analysis: Data-oriented parsing. Journal of
Experimental and Theoretical Al, 11:409-440.
H. Sebastian Seung, Manfred Opper, and Haim
Sompolinsky. 1992. Query by committee. In
Proceedings of the ACM Workshop on Com-
putational Learning Theory, Pittsburgh, PA.
David Skalak. 1997. Prototype Selection
for Composite Nearest Neighbor Classifier-
s. Ph.D. thesis, University of Massachusetts,
Amherst, Massachusetts.
Erik. F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the con11-2000 shared
task: Chunking. In Proceedings of CoNLL-
2000 and LLL-2000, pages 127-132, Lisbon,
Portugal.
J. Zhang. 1992. Selecting typical instances in
instance-based learning. In Proceedings of
ICML &apos;92, pages 470-479.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.422991">
<title confidence="0.9957">Distinguishing Easy and Hard Instances</title>
<author confidence="0.862279">Yuval</author>
<affiliation confidence="0.7648255">Department of Computer Bar-Ilan</affiliation>
<address confidence="0.991822">52900 Ramat Gan, Israel</address>
<abstract confidence="0.995228">Error analysis is a key step in developing statistical parsers. In doing this, we manually discover typical cases by examining parser output. In this paper we argue that the process can be speeded up by considering the output from an ensemble of parsers. We do this by resampling small proportions (10% and up) from the training data, and exploiting the high diversity of the resulting parsers resulting from the sparseness of natural-language data. Varying the sample size, we can trace the gradual learning of each instance and classify instances into a few types. This division helps in distinguishing instances which are hard for the system, from instances which may be learned in principle. We suggest that such analysis can yield a qualitative approach to evaluation of statistical parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Abe</author>
<author>H Mamitsuka</author>
</authors>
<title>Query learning strategies using boosting and bagging.</title>
<date>1998</date>
<booktitle>In Proceedings of ICML &apos;98,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="4946" citStr="Abe and Mamitsuka (1998)" startWordPosition="828" endWordPosition="831">idual instances is common in the frameworks of classifier combination and selective sampling. The boosting approach (Freund and Schapire, 1995) creates a classifier ensemble by training a system on samples drawn with preference to hard instances. Skalak (1997) suggests a more elaborated taxonomy of easiness and hardness levels, derived from leave-one-out results. He uses this information in order to remove uncharacteristic instances from the training set. Query by committee (Seung et al., 1992) approaches select training instances for which the disagreement between classifiers is the highest. Abe and Mamitsuka (1998), proposes to obtain the classifier collection by sampling from the training set with a uniform distribution (as in bagging (Breiman, 1996)) or with preference to hard instances as in boosting. We propose to use the ensemble of supervised parsers for error analysis. We demonstrate our method with a statistical memory-based shallow parsing algorithm and the task of NP detection. 2 Learning Algorithm The experiments were carried out using the memory-based sequence learning algorithm presented by Dagan and Krymolowski (2001, hereafter MBSL). The system considers POS sequences at the beginning or </context>
</contexts>
<marker>Abe, Mamitsuka, 1998</marker>
<rawString>N. Abe and H. Mamitsuka. 1998. Query learning strategies using boosting and bagging. In Proceedings of ICML &apos;98, pages 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<pages>24--123</pages>
<contexts>
<context position="5085" citStr="Breiman, 1996" startWordPosition="851" endWordPosition="852">es a classifier ensemble by training a system on samples drawn with preference to hard instances. Skalak (1997) suggests a more elaborated taxonomy of easiness and hardness levels, derived from leave-one-out results. He uses this information in order to remove uncharacteristic instances from the training set. Query by committee (Seung et al., 1992) approaches select training instances for which the disagreement between classifiers is the highest. Abe and Mamitsuka (1998), proposes to obtain the classifier collection by sampling from the training set with a uniform distribution (as in bagging (Breiman, 1996)) or with preference to hard instances as in boosting. We propose to use the ensemble of supervised parsers for error analysis. We demonstrate our method with a statistical memory-based shallow parsing algorithm and the task of NP detection. 2 Learning Algorithm The experiments were carried out using the memory-based sequence learning algorithm presented by Dagan and Krymolowski (2001, hereafter MBSL). The system considers POS sequences at the beginning or end of target instances, NPs in our case. For each POS sequence, it records the number of times it appears in the beginning or end of an NP</context>
<context position="13765" citStr="Breiman, 1996" startWordPosition="2446" endWordPosition="2447">5% 80.5% Forgotten Easy Table 3: Frequencies of instances exhibiting the four types of easiness change 4.3 Easiness and Bagging Taking small samples has the advantage of producing parsers which disagree more with one another, and can provide finer distinctions between instances. In memory-based learning, where it is important to keep all the available evidence from the training data (Daelemans et al., 1999), small samples have an advantage in requiring less space - with a tradeoff in performance. In order to study whether we can still get a good performance with small samples, we ran bagging (Breiman, 1996) experiments. We used Psamp = 10% and tried a range of thresholds. For each threshold 0, we selected the instances = Instances - Learned 00 20 40 60 80 100 20 40 60 80 20 40 60 80 100 20 40 60 80 100 Easy Instances Hard Instances Instances Forgotten 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 with easiness greater than 0, and calculated the recall, precision, and Fo with ,i3 = 1. The results are presented in Table 4 along with the performance of the parser trained on the full training set. As we see, it is possible to achieve a performance similar to that of the full model </context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>L. Breiman. 1996. Bagging predictors. Machine Learning, 24:123-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
<author>Jakub Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--11</pages>
<marker>Daelemans, van den Bosch, Zavrel, 1999</marker>
<rawString>Walter Daelemans, Antal van den Bosch, and Jakub Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, 34:11-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>Y Krymolowski</author>
</authors>
<title>Compositional memory-based partial parsing. In</title>
<date>2001</date>
<editor>R. Bod, R. Scha, and K. Sima&apos;an, editors, Data-Oriented Parsing, chapter 11.6. CSLI Publications, Stanford.</editor>
<note>to appear, http://turing.wins.uva.n1/—rens/dopbook.html.</note>
<contexts>
<context position="5472" citStr="Dagan and Krymolowski (2001" startWordPosition="908" endWordPosition="911">ing instances for which the disagreement between classifiers is the highest. Abe and Mamitsuka (1998), proposes to obtain the classifier collection by sampling from the training set with a uniform distribution (as in bagging (Breiman, 1996)) or with preference to hard instances as in boosting. We propose to use the ensemble of supervised parsers for error analysis. We demonstrate our method with a statistical memory-based shallow parsing algorithm and the task of NP detection. 2 Learning Algorithm The experiments were carried out using the memory-based sequence learning algorithm presented by Dagan and Krymolowski (2001, hereafter MBSL). The system considers POS sequences at the beginning or end of target instances, NPs in our case. For each POS sequence, it records the number of times it appears in the beginning or end of an NP, as well as anywhere else in the corpus. For compositional NPs, the embedded NPs are treated in the same way as POS. This provides an additional level of abstraction. Figure 1 presents sample training data. The input for parsing is a POS sequence, representing a tagged sentence. MBSL tests each subsequence, with its context, as a candidate for being an NP instance. Single words are t</context>
</contexts>
<marker>Dagan, Krymolowski, 2001</marker>
<rawString>I. Dagan and Y. Krymolowski. 2001. Compositional memory-based partial parsing. In R. Bod, R. Scha, and K. Sima&apos;an, editors, Data-Oriented Parsing, chapter 11.6. CSLI Publications, Stanford. to appear, http://turing.wins.uva.n1/—rens/dopbook.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>A decisiontheoretic generalization of on-line learning and an application to boosting.</title>
<date>1995</date>
<booktitle>In Proceedings of the 2&apos;d European Conference on Computational Learning Theory,</booktitle>
<pages>23--37</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="4465" citStr="Freund and Schapire, 1995" startWordPosition="753" endWordPosition="756">vide a different viewpoint of the target structures. It is expected that easy instances will be detected by all or most of the parsers, while hard instances — missed by most of the parsers. Moreover, as we increase the sample size, we can trace the learning process at the instance level and characterize instances according to the variation of their easiness. The idea of using an ensemble of supervised systems, trained on different samples, for making observations regarding individual instances is common in the frameworks of classifier combination and selective sampling. The boosting approach (Freund and Schapire, 1995) creates a classifier ensemble by training a system on samples drawn with preference to hard instances. Skalak (1997) suggests a more elaborated taxonomy of easiness and hardness levels, derived from leave-one-out results. He uses this information in order to remove uncharacteristic instances from the training set. Query by committee (Seung et al., 1992) approaches select training instances for which the disagreement between classifiers is the highest. Abe and Mamitsuka (1998), proposes to obtain the classifier collection by sampling from the training set with a uniform distribution (as in bag</context>
</contexts>
<marker>Freund, Schapire, 1995</marker>
<rawString>Y. Freund and R. E. Schapire. 1995. A decisiontheoretic generalization of on-line learning and an application to boosting. In Proceedings of the 2&apos;d European Conference on Computational Learning Theory, pages 23-37. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Véronique Hoste</author>
<author>Walter Daelemans</author>
</authors>
<title>Comparing bagging and boosting for natural language processing tasks: a typicality approach.</title>
<date>2000</date>
<booktitle>Proceedings of Benelearn</booktitle>
<pages>101--108</pages>
<editor>In Ad Feelders, editor,</editor>
<contexts>
<context position="18123" citStr="Hoste and Daelemans (2000)" startWordPosition="3205" endWordPosition="3208">etween parsers trained on different samples. Given a sampling proportion, we sampled randomly from the data. It might be possible to increase the diversity among parsers by sampling chunks of sentences. As the style within a chunk is more uniform than within a collection of texts, this could focus each parser on a smaller number of instance types. This will sharpen the coverage differences between parsers (in expense of the coverage of each one). As further work, we plan to investigate the relation between easiness and concepts like typicality (Zhang, 1992) and class prediction strength (e.g. Hoste and Daelemans (2000)). We also plan to study the extent to which easiness depends on the feature set used by the supervised parser, and compare a number of systems in order to find instances which are hard or easy for most of the methods (cf. Pedersen (2002) for the word-sense disambiguation task). We hope this would provide a means for qualitative comparison between systems. Further yet, we hope this would contribute to a more focused use of the individual learning methods, possibly in combination with hand-coded rules, saving learning effort for the cases where it is more needed. Acknowledgements The author tha</context>
</contexts>
<marker>Hoste, Daelemans, 2000</marker>
<rawString>Véronique Hoste and Walter Daelemans. 2000. Comparing bagging and boosting for natural language processing tasks: a typicality approach. In Ad Feelders, editor, Proceedings of Benelearn 2000, pages 101-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Krymolowski</author>
<author>Zvika Marx</author>
</authors>
<title>Clustering the space of phrases identified by an ensemble of supervised shallow parsers.</title>
<date>2002</date>
<booktitle>In Proceedings of ICML &apos;02 Workshop on Text Learning (TextML-2002),</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="15891" citStr="Krymolowski and Marx, 2002" startWordPosition="2831" endWordPosition="2834">s of a single run of a supervised parser, we can find frequent errors. This information is, however, limited to that run and does not always reflect why an instance was not detected. Possible reasons can be that the system cannot find supporting evidence in the training data, or due to a random balance of supporting and contradicting evidence for that instance. Distinguishing between errors of these types and, more generally, tracing the learning or &amp;quot;forgetting&amp;quot; patterns of instances, are important steps in analyzing the performance of a supervised parser. Combined with a clustering approach (Krymolowski and Marx, 2002), we may be able to group together instances with similar behaviour and structure, and speed up the process of error analysis. When a model is probabilistic (e.g., DOP (Scha et al., 1999)), we can intuitively observe that instances that get a high probability are easy while those with low probability are hard. For models represented as a separator in an abstract numeric feature space (e.g., SNoW (Roth, 1998)), the distance from the separator can be an indication of easiness. Assuming the separator fluctuates within a bounded area of space for different training samples, the easy instances are </context>
</contexts>
<marker>Krymolowski, Marx, 2002</marker>
<rawString>Yuval Krymolowski and Zvika Marx. 2002. Clustering the space of phrases identified by an ensemble of supervised shallow parsers. In Proceedings of ICML &apos;02 Workshop on Text Learning (TextML-2002), Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="9161" citStr="Marcus et al., 1993" startWordPosition="1611" endWordPosition="1614">ich is the probability of detecting a by one of the parsers in the ensemble. For easy and hard instances, easinesss(a) will be close to 1 and 0 respectively. The easiness does not depend only on the instance, but also on the training samples. We will discuss this issue further in the experiment section. In this paper we restrict our study to NPs marked in an annotated corpus, all of these NPs are therefore correct. In the general case, an instance can have a high easiness but still not be correct. 4 Experiments 4.1 Data The training data used in our experiments consisted of Penn Treebank WSJ (Marcus et al., 1993) Sections 15-18, with Section 20 as test. These data sets were used by Ramshaw and Marcus (1995) and CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000) and have become a common testbed for shallow parsing tasks. Table 1 shows the number of sentences and NPs in the training and test data. We counted compositional NPs separately, as their structure is more complicated than that of base NPs. Training Test Dataset WSJ 15-18 WSJ 20 Sentences 8936 2012 NPs: 50860 11401 Base 18472 4398 Compositional 69332 15799 Total Table 1: Training and test data Psamp 10% 25% 50% 80% 95% Avg. rec.% 81 83 8</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Assessing system agreement and instance difficulty in the lexical sample tasks of senseval-2.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<marker>Pedersen, 2002</marker>
<rawString>Ted Pedersen. 2002. Assessing system agreement and instance difficulty in the lexical sample tasks of senseval-2. In Proceedings of the Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="9257" citStr="Ramshaw and Marcus (1995)" startWordPosition="1628" endWordPosition="1631">ard instances, easinesss(a) will be close to 1 and 0 respectively. The easiness does not depend only on the instance, but also on the training samples. We will discuss this issue further in the experiment section. In this paper we restrict our study to NPs marked in an annotated corpus, all of these NPs are therefore correct. In the general case, an instance can have a high easiness but still not be correct. 4 Experiments 4.1 Data The training data used in our experiments consisted of Penn Treebank WSJ (Marcus et al., 1993) Sections 15-18, with Section 20 as test. These data sets were used by Ramshaw and Marcus (1995) and CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000) and have become a common testbed for shallow parsing tasks. Table 1 shows the number of sentences and NPs in the training and test data. We counted compositional NPs separately, as their structure is more complicated than that of base NPs. Training Test Dataset WSJ 15-18 WSJ 20 Sentences 8936 2012 NPs: 50860 11401 Base 18472 4398 Compositional 69332 15799 Total Table 1: Training and test data Psamp 10% 25% 50% 80% 95% Avg. rec.% 81 83 84 85 85 Easiness: proportion in test data 0 3 4 5 8 10 0-0.1 9 9 10 12 14 0.1-0.2 3 2 2 1 1 0.2-</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: A unified approach.</title>
<date>1998</date>
<booktitle>In proc. of the Fifteenth National Conference on Artificial Intelligence,</booktitle>
<pages>806--813</pages>
<publisher>AAAI Press.</publisher>
<location>Menlo Park, CA, USA,</location>
<contexts>
<context position="16302" citStr="Roth, 1998" startWordPosition="2902" endWordPosition="2903">y, tracing the learning or &amp;quot;forgetting&amp;quot; patterns of instances, are important steps in analyzing the performance of a supervised parser. Combined with a clustering approach (Krymolowski and Marx, 2002), we may be able to group together instances with similar behaviour and structure, and speed up the process of error analysis. When a model is probabilistic (e.g., DOP (Scha et al., 1999)), we can intuitively observe that instances that get a high probability are easy while those with low probability are hard. For models represented as a separator in an abstract numeric feature space (e.g., SNoW (Roth, 1998)), the distance from the separator can be an indication of easiness. Assuming the separator fluctuates within a bounded area of space for different training samples, the easy instances are those within a safe distance from that area, while the hard ones are more sensitive to errors resulting from noise in the training samples. In this work, we proposed a method for estimating the easiness of instances which is suitable even for a non-probabilistic model or model which is not represented in an abstract numeric feature space. The method relies on generating an ensemble of parsers by resampling f</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In proc. of the Fifteenth National Conference on Artificial Intelligence, pages 806-813, Menlo Park, CA, USA, July. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remko Scha</author>
<author>Rens Bod</author>
<author>Khalil Sima&apos;an</author>
</authors>
<title>A memory-based model of syntactic analysis: Data-oriented parsing.</title>
<date>1999</date>
<journal>Journal of Experimental and Theoretical Al,</journal>
<pages>11--409</pages>
<contexts>
<context position="16078" citStr="Scha et al., 1999" startWordPosition="2863" endWordPosition="2866">ons can be that the system cannot find supporting evidence in the training data, or due to a random balance of supporting and contradicting evidence for that instance. Distinguishing between errors of these types and, more generally, tracing the learning or &amp;quot;forgetting&amp;quot; patterns of instances, are important steps in analyzing the performance of a supervised parser. Combined with a clustering approach (Krymolowski and Marx, 2002), we may be able to group together instances with similar behaviour and structure, and speed up the process of error analysis. When a model is probabilistic (e.g., DOP (Scha et al., 1999)), we can intuitively observe that instances that get a high probability are easy while those with low probability are hard. For models represented as a separator in an abstract numeric feature space (e.g., SNoW (Roth, 1998)), the distance from the separator can be an indication of easiness. Assuming the separator fluctuates within a bounded area of space for different training samples, the easy instances are those within a safe distance from that area, while the hard ones are more sensitive to errors resulting from noise in the training samples. In this work, we proposed a method for estimati</context>
</contexts>
<marker>Scha, Bod, Sima&apos;an, 1999</marker>
<rawString>Remko Scha, Rens Bod, and Khalil Sima&apos;an. 1999. A memory-based model of syntactic analysis: Data-oriented parsing. Journal of Experimental and Theoretical Al, 11:409-440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sebastian Seung</author>
<author>Manfred Opper</author>
<author>Haim Sompolinsky</author>
</authors>
<title>Query by committee.</title>
<date>1992</date>
<booktitle>In Proceedings of the ACM Workshop on Computational Learning Theory,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="4821" citStr="Seung et al., 1992" startWordPosition="810" endWordPosition="813">e idea of using an ensemble of supervised systems, trained on different samples, for making observations regarding individual instances is common in the frameworks of classifier combination and selective sampling. The boosting approach (Freund and Schapire, 1995) creates a classifier ensemble by training a system on samples drawn with preference to hard instances. Skalak (1997) suggests a more elaborated taxonomy of easiness and hardness levels, derived from leave-one-out results. He uses this information in order to remove uncharacteristic instances from the training set. Query by committee (Seung et al., 1992) approaches select training instances for which the disagreement between classifiers is the highest. Abe and Mamitsuka (1998), proposes to obtain the classifier collection by sampling from the training set with a uniform distribution (as in bagging (Breiman, 1996)) or with preference to hard instances as in boosting. We propose to use the ensemble of supervised parsers for error analysis. We demonstrate our method with a statistical memory-based shallow parsing algorithm and the task of NP detection. 2 Learning Algorithm The experiments were carried out using the memory-based sequence learning</context>
</contexts>
<marker>Seung, Opper, Sompolinsky, 1992</marker>
<rawString>H. Sebastian Seung, Manfred Opper, and Haim Sompolinsky. 1992. Query by committee. In Proceedings of the ACM Workshop on Computational Learning Theory, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Skalak</author>
</authors>
<title>Prototype Selection for Composite Nearest Neighbor Classifiers.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts,</institution>
<location>Amherst, Massachusetts.</location>
<contexts>
<context position="4582" citStr="Skalak (1997)" startWordPosition="774" endWordPosition="775">sers, while hard instances — missed by most of the parsers. Moreover, as we increase the sample size, we can trace the learning process at the instance level and characterize instances according to the variation of their easiness. The idea of using an ensemble of supervised systems, trained on different samples, for making observations regarding individual instances is common in the frameworks of classifier combination and selective sampling. The boosting approach (Freund and Schapire, 1995) creates a classifier ensemble by training a system on samples drawn with preference to hard instances. Skalak (1997) suggests a more elaborated taxonomy of easiness and hardness levels, derived from leave-one-out results. He uses this information in order to remove uncharacteristic instances from the training set. Query by committee (Seung et al., 1992) approaches select training instances for which the disagreement between classifiers is the highest. Abe and Mamitsuka (1998), proposes to obtain the classifier collection by sampling from the training set with a uniform distribution (as in bagging (Breiman, 1996)) or with preference to hard instances as in boosting. We propose to use the ensemble of supervis</context>
</contexts>
<marker>Skalak, 1997</marker>
<rawString>David Skalak. 1997. Prototype Selection for Composite Nearest Neighbor Classifiers. Ph.D. thesis, University of Massachusetts, Amherst, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the con11-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL2000 and LLL-2000,</booktitle>
<pages>127--132</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="9320" citStr="Sang and Buchholz, 2000" startWordPosition="1638" endWordPosition="1641">y. The easiness does not depend only on the instance, but also on the training samples. We will discuss this issue further in the experiment section. In this paper we restrict our study to NPs marked in an annotated corpus, all of these NPs are therefore correct. In the general case, an instance can have a high easiness but still not be correct. 4 Experiments 4.1 Data The training data used in our experiments consisted of Penn Treebank WSJ (Marcus et al., 1993) Sections 15-18, with Section 20 as test. These data sets were used by Ramshaw and Marcus (1995) and CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000) and have become a common testbed for shallow parsing tasks. Table 1 shows the number of sentences and NPs in the training and test data. We counted compositional NPs separately, as their structure is more complicated than that of base NPs. Training Test Dataset WSJ 15-18 WSJ 20 Sentences 8936 2012 NPs: 50860 11401 Base 18472 4398 Compositional 69332 15799 Total Table 1: Training and test data Psamp 10% 25% 50% 80% 95% Avg. rec.% 81 83 84 85 85 Easiness: proportion in test data 0 3 4 5 8 10 0-0.1 9 9 10 12 14 0.1-0.2 3 2 2 1 1 0.2-0.3 2 2 1 1 0 0.3-0.4 2 2 1 1 0 0.4-0.5 2 2 1 0 0 0.5-0.6 2 2 2</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik. F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the con11-2000 shared task: Chunking. In Proceedings of CoNLL2000 and LLL-2000, pages 127-132, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhang</author>
</authors>
<title>Selecting typical instances in instance-based learning.</title>
<date>1992</date>
<booktitle>In Proceedings of ICML &apos;92,</booktitle>
<pages>470--479</pages>
<contexts>
<context position="18060" citStr="Zhang, 1992" startWordPosition="3198" endWordPosition="3199">ontain which, in turn, increases the difference between parsers trained on different samples. Given a sampling proportion, we sampled randomly from the data. It might be possible to increase the diversity among parsers by sampling chunks of sentences. As the style within a chunk is more uniform than within a collection of texts, this could focus each parser on a smaller number of instance types. This will sharpen the coverage differences between parsers (in expense of the coverage of each one). As further work, we plan to investigate the relation between easiness and concepts like typicality (Zhang, 1992) and class prediction strength (e.g. Hoste and Daelemans (2000)). We also plan to study the extent to which easiness depends on the feature set used by the supervised parser, and compare a number of systems in order to find instances which are hard or easy for most of the methods (cf. Pedersen (2002) for the word-sense disambiguation task). We hope this would provide a means for qualitative comparison between systems. Further yet, we hope this would contribute to a more focused use of the individual learning methods, possibly in combination with hand-coded rules, saving learning effort for the</context>
</contexts>
<marker>Zhang, 1992</marker>
<rawString>J. Zhang. 1992. Selecting typical instances in instance-based learning. In Proceedings of ICML &apos;92, pages 470-479.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>