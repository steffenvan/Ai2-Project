<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000091">
<title confidence="0.510817">
Similarity metrics for aligning children&apos;s articulation data
</title>
<author confidence="0.487979">
Harold L. SOMERS
</author>
<bodyText confidence="0.97756425">
Centre for Computational Linguistics
UMIST, PO Box 88,
Manchester M60 1QD, England
haro 1 d@c c 1 . umi s t . ac . uk
</bodyText>
<sectionHeader confidence="0.975263" genericHeader="abstract">
1. Background
</sectionHeader>
<bodyText confidence="0.9999808">
This paper concerns the implementation and
testing of similarity metrics for the alignment of
phonetic segments in transcriptions of children&apos;s
(mis)articulations with the adult model. This has
an obvious application in the development of
software to assist speech and language clinicians
to assess clients and to plan therapy. This paper
will give some of the background to this general
problem, but will focus on the computational
and linguistic aspect of the alignment problem.
</bodyText>
<subsectionHeader confidence="0.996021">
1.1. Articulation testing
</subsectionHeader>
<bodyText confidence="0.999780975609756">
It is well known that a child&apos;s acquisition of
phonology is gradual, and can be charted
according to the appearance of phonetic
distinctions (e.g. stops vs. fricatives), the dis-
appearance of childish mispronunciations,
especially due to assimilation ([gog] for dog),
and the ability to articulate particular phonetic
configurations (e.g. consonant clusters).
Whether screening whole populations of
children, or assessing individual referrals, the
articulation test is an important tool for the
speech clinician.
A child&apos;s articulatory development is usually
described with reference to an adult model, and
in terms of deviations from it: a number of
phonological &amp;quot;processes&amp;quot; can be identified, and
their significance with respect to the
chronological age of the child assessed. Often
processes interact, e.g. when spoon is
pronounced [mull] we have consonant-cluster
reduction and assimilation.
The problem for this paper is to align the
segments in the transcription of the child&apos;s
articulation with the target model pronunci-
ation. The task is complicated by the need to
identify cases of &amp;quot;metathesis&amp;quot;, where the
corresponding sounds have been reordered (e.g.
remember .— [flurernba]) and &amp;quot;merges&amp;quot;, a special
case of consonant-cluster reduction where the
resulting segment has some of the features of
both elements in the original cluster (e.g. sleep
It would be appropriate here to review the
software currently available to speech clinicians,
but lack of space prevents us from doing so (see
Somers, forthcoming). Suffice it to say that
software does exist, but is mainly for
grammatical and lexical analysis. Of the tiny
number of programs which specifically address
the problem of articulation testing, none, as far
as one can tell, involve automatic alignment of
the data.
</bodyText>
<subsectionHeader confidence="0.977773">
1.2. Segment alignment
</subsectionHeader>
<bodyText confidence="0.999945185185185">
In a recent paper, Covington (1996) described
an algorithm for aligning historical cognates.
The present author was struck by the possibility
of using this technique for the child-language
application, a task for which a somewhat similar
algorithm had been developed some years ago
(Somers 1978, 1979). In both algorithms, the
phonetic segments are interpreted as bundles of
phonetic features, and the algorithms include a
simple similarity metric for comparing the
segments pairwise. The algorithms differ
somewhat in the way the search space is
reduced, but the results are quite comparable
(Somers, forthcoming).
Coincidentally, a recent article by Connolly
(1997) has suggested a number of ways of
quantifying the similarity or difference between
two individual phones, on the basis of per-
ceptual and articulatory differences. Connolly&apos;s
metric is also feature-based, but differs from the
others mentioned in its complexity. In particular,
the features can be differentially weighted for
salience, and, additionally, not all the features
are simple Booleans. In the second part of his
article, Connolly introduces a distance measure
for comparing sequences of phones, based on
the Levenshtein distance well-known in the
</bodyText>
<page confidence="0.987742">
1227
</page>
<bodyText confidence="0.999940066666667">
spell-checking, speech-processing and corpus-
alignment literatures (inter alia). Again, this
metric can be weighted, to allow substitutions to
be valued differentially (on the basis of the
individual phone distance measure as described
in the first part), and to deal with merges and
metathesis.
Although his methods are clearly com-
putational in nature, Connolly reports (personal
communication) that he has not yet implemented
them. In this paper, we describe a simple imple-
mentation and adaptation of Connolly&apos;s metrics,
and a brief critical evaluation of their per-
formance on some child language data (both real
and artificial).
</bodyText>
<sectionHeader confidence="0.986836" genericHeader="keywords">
2. The alignment algorithms
</sectionHeader>
<bodyText confidence="0.99975675">
We have implemented three versions of an
alignment algorithm, utilising different segment
similarity measures, but the same sequence
measure.
</bodyText>
<subsectionHeader confidence="0.999769">
2.1. Coding the input
</subsectionHeader>
<bodyText confidence="0.999995166666667">
Before we consider the algorithms themselves,
however, it is appropriate to mention briefly the
issue of transcription. On the one hand,
children&apos;s articulations can include a much
wider variety of phones than those which are
found in the target system; in addition, certain
secondary phonetic features may be particularly
important in the description of the child&apos;s
articulation (e.g. spreading, laryngealization). So
the transcriptions need to be &amp;quot;narrow&amp;quot;. On the
other hand, speech clinicians nevertheless tend
to use a &amp;quot;contrastive&amp;quot; transcription, essentially
phonemic except where the child&apos;s articulation
differs from the target: so normal allophonic
variation will not necessarily be reflected in the
transcription. Any program that is to be used for
the analysis of articulation data will need an
appropriate coding scheme which allows a
narrow transcription in a fairly transparent
notation. Some software offers phonetic
transcription schemes based on the ASCII
character set (e.g. Perry 1995). Alternatively, it
seems quite feasible to allow the transcriptions
to be input using a standard word-processor and
a phonetic font, and to interpret the symbols
accordingly. For a commercial implementation
it would be better to follow the standard
proposed by the IPA (Esling &amp; Gaylord 1993),
which has been approved by the ISO, and
included in the Unicode definitions.
</bodyText>
<subsectionHeader confidence="0.964865">
2.2. Internal representation
</subsectionHeader>
<bodyText confidence="0.9999605">
Representing the phonetic segments as bundles
of features is an obvious technique, and one
which is widely adopted. In the algorithm
reported in Somers (1979) — henceforth CAT
— phones are represented as bundles of binary
articulatory features. Some primary features also
serve as secondary features where appropriate
(e.g. dark &apos;I&apos; is marked as VEL(ar)), but there are
also explicit secondary features, e.g.
ASP(iration).
Connolly (1997) suggests two alternative
feature representations. The first is based on
perceptual features, which, he claims, are more
significant than articulatory features &amp;quot;from the
point of view of communicative dysfunction&amp;quot;
(p.276). On the other hand, he admits that using
perceptual features can be problematic, unless
&amp;quot;we are prepared to accept a relatively unrefined
quantification method&amp;quot; (p.277). Connolly rejects
a number of perceptual feature schemes for
consonants in favour of one proposed by Line
(1987), which identifies two perceptual features
or axes, &amp;quot;friction strength&amp;quot; (FS) and &amp;quot;pitch&amp;quot; (P),
and divides the consonant phones into six
groups, differentiated by their score on each of
these axes, as shown in Figure I.
Henceforth we will refer to this scheme as
&amp;quot;FS/P&amp;quot;. In fact, there are a number of drawbacks
and shortcomings in Connolly&apos;s scheme for our
purposes, notably the absence of many non-
English phones (all non-pulmonics, uvulars,
retroflexes, trills and taps), and there is no
indication how to handle secondary features
typically needed to transcribe children&apos;s
articulations accurately. We have tried to rectify
the first shortcoming in our implementation, but
it is not obvious how to deal with the second.
Connolly&apos;s alternative feature representation
is based on articulatory features, adapted from
Ladefoged&apos;s (1971) system, though unlike the
features used in the CAT scheme, some of
the features are not binary. Figure 2 shows the
feature scheme for consonants, which we have
adapted slightly, in detail. We will refer to this
</bodyText>
<page confidence="0.989168">
1228
</page>
<figureCaption confidence="0.992922">
Figure 1. Perceptual feature-based representation (FS/P) of consonants from Connolly (1997:279f)
</figureCaption>
<bodyText confidence="0.968545666666667">
Members
bilabial plosives; labial and alveolar nasals
glottal obstruents; central and lateral approximants;
palatal and velar nasals
alveolar plosives; labial and dental fricatives; voiceless
nasals
velar and palatal obstruents
palato-alveolar and lateral fricatives
alveolar fricatives and affricates
</bodyText>
<figureCaption confidence="0.988526">
Figure 2. Articulatory feature scheme (Lad) for consonants, adapted from Connolly (1997:282P.
</figureCaption>
<bodyText confidence="0.675476875">
(a) non-binary features with explanations of the values:
glottalic: 1 (ejective), 0.5 (pulmonic), 0 (implosive)
voice: 1 (glottal stop), 0.8 (laryngealized), 0.6 (voiced), 0.2 (murmur), 0 (voiceless)
place (i.e. passive articulator): 1 (labial), 0.9 (dental), 0.85 (alveolar), 0.8 (post-alveolar), 0.75 (pre-
palatal), 0.7 (palatal), 0.6 (velar), 0.5 (uvular), 0.3 (pharyngeal), 0 (glottal)
constrictor: 1 (labial), 0.9 (dental), 0.85 (apical), 0.75 (laminal), 0.6 (dorsal), 0.3 (radical), 0 (glottal)
stop: 1 (stop), 0.95 (affricate), 0.9 (fricative), 0 (approximant)
length: 1 (long), 0.5 (half-long)
</bodyText>
<figure confidence="0.964597333333333">
(b) binary features:
velaric (for clicks), aspirated, nasal, lateral, trill, tap, retroflex, rounded, syllabic, unreleased, grooved
Group Friction-strength Pitch
1 0.0 0.0
2 0.0 0.4
3 0.4 0.3
4 0.5 0.8
5 0.8 0.9
6 1.0 1.0
</figure>
<bodyText confidence="0.9981414">
scheme as &amp;quot;Lad&amp;quot;. Again, some features or
feature values needed to be added, notably a
value of &amp;quot;stop&amp;quot; for affricates.
Let us now consider the similarity metrics
based on these three schemes.
</bodyText>
<subsectionHeader confidence="0.7602365">
2.3. Similarity metrics for individual
phones
</subsectionHeader>
<bodyText confidence="0.999945028571429">
The similarity (or distance) metric is the key to
the alignment algorithm. In the case of CAT, the
distance measure is quite simply a count of the
binary features for which the polarity differs. So
for example, when comparing the articulation
[d] with a target of [st], the [s] and [d] differ in
terms of three features (VOICE, STOP and FRIC)
while [t] and [d] differ in only one (VOICE): so
[d] is more similar to [t] than to [s].
In FS/P, the two features are weighted to
reflect the greater importance of FS over P, the
former being valued double the latter. To
calculate the similarity of two phones we add the
difference in their FS scores to half the
difference in their P scores. If the two phones
are in the same group, the score is set at 0.05
(unless they are identical, in which case it is 0).
Thus, to take our [st]-4d] example again, since
[s] is in group 6, and [t] and [d] both in group 3,
[t]-[d] scores 0.05, [s]-[d] 0.95.
The similarity metric based on the Lad
scheme is simpler, in that all the features are
equally weighted. The Lad score is the simply
sum of the score differences for all the features.
For our example of [st]-4d], the [t]-[d]
difference is only in one feature, &amp;quot;voice&amp;quot;, with
values 0 and 0.6 respectively, while the [s]-[d]
difference has the 0.6 voice difference plus a
difference of 0.1 in the &amp;quot;stop&amp;quot; feature ([d] scores
I, [s] scores 0.9).
All three metrics agree that [d] is more
similar to [t] than to [s], as we might hope and
expect. As we will see below, the different
feature schemes do not always give the same
result however.
</bodyText>
<subsectionHeader confidence="0.999087">
2.4. Sequence comparison
</subsectionHeader>
<bodyText confidence="0.999551375">
Connolly&apos;s proposed algorithm for aligning
sequences of phones is based on the Levenshtein
distance. He calls it a &amp;quot;weighted&amp;quot; Levenshtein
distance, because the algorithm would have to
take into account the similarity scores between
individual segments when deciding in cases of
combined substitution and deletion (e.g. our [st]
-&gt; [d] example) which segment to mark as
</bodyText>
<page confidence="0.977894">
1229
</page>
<bodyText confidence="0.9998941">
inserted or deleted. Connolly suggests (p.291)
that substitutions should always be preferred
over insertions and deletions, and this
assumption was also built into the algorithm we
originally developed in Somers (1979).
However, this does not always give the correct
solution: for example, if the sequence [skr] (e.g.
in scrape) was realised as [[ski, we would prefer
the alignment in (la) with one insertion and one
deletion, to that in ( lb) with only substitutions.
</bodyText>
<equation confidence="0.609943">
(1) a. - s k r b. s k r
Ss k- s k
</equation>
<bodyText confidence="0.99999046875">
The algorithm would also have to be adjusted to
allow for metathesis, though Connolly suggests
that merges do not present a special problem
because they can always be treated as a
substitution plus an omission (p.292) — again
we disagree with this approach and will
illustrate the problem below.
For these reasons we have not used a
Levenshtein distance algorithm for our new
implementation of the alignment task. As
described in Somers (forthcoming), the original
alignment algorithm in CAT relied on a single
predetermined anchor point, and then
exhaustively compared all possible alignments
either side of the anchor, though only when the
number of segments differed.
We now prefer a more general recursive
algorithm in which we identify in the two
strings a suitable anchor, then split the strings
around the two anchor points, and repeat the
process with each half string until one (or both)
is (are) reduced to the empty string. The
algorithm is given in Figure 3. Step 2 is the key
to the algorithm, and is primed to look first for
identical phones, else vowels, else the phones
are compared pairwise exhaustively. If there is a
choice of &amp;quot;best match&amp;quot;, we prefer values of i and
j that are similar, and near the middle of the
string. Although the algorithm is looking for the
best match, it is also looking for possible
merges, which will be identified when there is
no single best match.
</bodyText>
<subsectionHeader confidence="0.934042">
2.5. Identifying metathesis
</subsectionHeader>
<bodyText confidence="0.993830666666667">
It is difficult to incorporate a test for
metathesis directly into the above algorithm, and
it is better to make a second pass looking for this
</bodyText>
<figureCaption confidence="0.997614">
Figure 3. The alignment algorithm.
</figureCaption>
<bodyText confidence="0.993186">
Let X and Y be the strings to be aligned, of
length m and n, where each X[i], Y[/],
</bodyText>
<listItem confidence="0.991987090909091">
1 is a bundle of features.
1. If X=[] and Y=[], then stop; else if X=[]
(Y=[]) then mark all segments in Y (X) as
&amp;quot;inserted&amp;quot; (&amp;quot;omitted&amp;quot; ) and stop; else
continue.
2. Find the best matching X[i] and Y[/], and
mark these as &amp;quot;aligned&amp;quot;.
3. Take the substring X[1]..X[i-1] and the
substring Y[1]..Y[j-1] and repeat from step
1; and similarly with the substrings
X[i+1]..X[m], and Y[/+1]..Y[n].
</listItem>
<bodyText confidence="0.999826473684211">
phenomenon explicitly. For our purposes it is
reasonable to focus on consonants. Metathesis
can occur either with contiguous phones, e.g.
[desk] -4 [deks], or with phones either side of a
vowel, e.g. [enfant] -4 [efilant]. In addition, one
or both of the phones may have undergone some
other phonological processes, e.g. [enfant] -4
[eprlant], where the [f] and [1] have been
exchanged, but the [f] realised as a [p].
The algorithm described above will analyse
metatheses in one of two ways, depending on
various other factors. One analysis will simply
align the phones with each other. To recognise
this as a case of metathesis, we need to see if the
crossing alignment gives a better score. The
other analysis will align one or other of the
identical phones, and mark the others as
omitted/inserted. The second pass looks out for
both these situations.
</bodyText>
<sectionHeader confidence="0.976599" genericHeader="introduction">
3. Evaluation
</sectionHeader>
<bodyText confidence="0.998928333333333">
In this section we consider how the algorithm
deals with some data, both real and simulated.
We want (a) to see if the algorithm as described
gets alignments that correspond to the alignment
favoured by a human; and (b) to compare the
different feature systems that have been
proposed.
For many of the examples we have used,
there is no problem, and nothing to choose
between the systems. These are cases of simple
omission (e.g. spoon.-[pun]), insertion (Everton
[evatAnt]), substitution (feather [bey]), and
</bodyText>
<page confidence="0.971685">
1230
</page>
<bodyText confidence="0.915917608695652">
[Evatiult]), substitution (feather -4 [bey]), and
various combinations of those processes
(Christmas-4[gixmax], aeroplane—+[wejabein]).
Cases of inserted vowels (e.g. spoon-4[supun])
were analysed correctly when the inserted vowel
was different from the main vowel. So for
example chimney -4 [tfunim] caused difficulty,
with the alignment (2a) preferred over (2b).
(2)a. tfimn b.tfim—ni
tfim— mi tfim mm
Differences between the feature systems
show up when the alignment combines
substitutions and omissions, and the &amp;quot;best
match&amp;quot; comes into play. Vocalisation of
syllabics (e.g. bottle [bot:I] [bn?uw]) caused
problems, with the syllabic [t] aligning with [u]
in the CAT system, [?] in FS/P, and [w] in Lad.
In other cases where the systems gave
different results, the FS/P system most often
gave inappropriate alignments. For example,
monkey [mAijki] —&gt; [mAn?i] was correctly aligned
as in (3a) by the other two systems, but as (3b)
with FS/P.
</bodyText>
<equation confidence="0.881816">
(3)a.mnijki b.mn—oki
mAn7i mAn
</equation>
<bodyText confidence="0.99626962962963">
For teeth [ti0]—&gt;[?isx], FS/P aligned the [x] with
the [0] while the other systems got the more
likely [0]—*[s] alignment. Similarly, the Lad and
CAT systems labelled the [1] as omitted in
bridge [b.ud3]-4gix], while FS/P aligned it with
[g].
When identifying merges on the other hand,
only CAT had any success, in sleep [slip]--[iip]
(but not when the [I] is not marked as voiceless).
In analysing [fl]—&gt;[b], CAT suggests a merge,
FS/P marks the [f] as omitted, Lad the [I]. In
principle, the FS/P system offers most scope for
identifying merges, as it only recognises six
different classes of consonant phone, while the
Lad system is too fine-grained: indeed, we were
unable to find (or simulate) any plausible case
which Lad would analyse as a merge.
Against that it should also be noted that such
analyses cannot be carried out totally in
isolation. For example, compare the case where
[4] is only used when [sl] is expected to the one
where [s] is generally realised as [4]: we might
want to analyse only the former case as a merge,
the latter as a substitution plus omission. It
should be remembered that the alignment task is
only the first step of the analysis of the child&apos;s
phonetic system.
</bodyText>
<sectionHeader confidence="0.993163" genericHeader="method">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.999950555555556">
Because of its poor performance with many
alignments, we must reject the FS/P system.
This is not a great surprise: a feature system
based on perceptual differences seems
intuitively questionable for an articulation
analysis task. There does not seem much to
choose between Lad and CAT, though the former
gives a more subtle scoring system, which might
be useful for screening children. On the other
hand, it never identifies merges, even in highly
plausible cases, so the system using simpler
binary articulatory features may be the best
solution.
Whichever system is used, it seems that an
acceptable level of success can be achieved with
the algorithm described here, and it could form
the basis of software for the automatic analysis
of children&apos;s articulation data.
</bodyText>
<sectionHeader confidence="0.99768" genericHeader="method">
5. References
</sectionHeader>
<reference confidence="0.99725844">
Connolly, John H. (1997) Quantifying target—
realization differences. Clinical Linguistics &amp;
Phonetics 11:267-298.
Covington, Michael A. (1996) An algorithm to align
words for historical comparison. Computational
Linguistics 22:481-496.
Esling, John H. &amp; Harry Gaylord (1993) Computer
codes for phonetic symbols. Journal of the
International Phonetic Association 23:83-97.
Ladefoged, P. (1971) Preliminaries to Linguistic
Phonetics. Chicago: University of Chicago Press.
Line, Pippa (1987) An Investigation of Auditory
Distance. M.Phil. dissertation, De Montfort
University, Leicester.
Perry, Cecyle K. (1995) Review of Phonological
Deviation Analysis by Computer (PDAC). Child
Language Teaching and Therapy 11:331-340.
Somers, H.L. (1978) Computerised Articulation
Testing. M.A. thesis, Manchester University.
Somers, H.L. (1979) Using the computer to analyse
articulation test data. British Journal of Disorders
of Communication 14:231-240.
Somers, H.L. (forthcoming) Aligning phonetic
segments for children&apos;s articulation assessment. To
appear in Computational Linguistics.
</reference>
<page confidence="0.995528">
1231
</page>
<bodyText confidence="0.900576307692308">
Similarity metrics for aligning
children&apos;s articulation data
An important step in the automatic analysis of
child-language articulation data is to align the
transcriptions of children&apos;s (mis)articulations
with adult models. The problems underlying
this task are discussed and a number of
algorithms are presented and compared. These
are based on various similarity or distance
measures for individual phonetic segments,
considering perceptual and articulatory
features, which may be weighted to reflect
salience, and on sequence comparison.
</bodyText>
<figure confidence="0.703322444444444">
Teto)5Elifo)--37ttb1-1-6
ofrAtisitm
EARIVII, toyttatAo5-•
IV C.St tt I&amp;quot; 6 Z:&amp;quot;.. L&amp;quot; Cs ;Fitt
iz
VrbfftqfrMIzilit7611.1EA
Uplrit61:-.6f-c V, LN&lt; 097)1,19
XAffittVIT{d5,-0060
flt-40314A150flitittAbOliSAlzdtg
UT;lllitt-t.rctaYr6,) /1M-41StL
Rfar.010o7k X/AffittrAtiti,
TO60 fitYX1t,
rtifun Lo:5, tze
lAbt LNOT, 7)1.19 X.Lx0M
#R1U3NtS.O0 tAllArD Ltantsnia
0)**(1. 7)9X111TUAffil,
1ztft**Ittc.65M1445(07)
Atz—to NUMblA. tifcc,
</figure>
<sectionHeader confidence="0.995655" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9640859375">
Thanks to Joe Somers for providing some of the
example data; and to Marie-Jo Proulx and Ayako
Matsuo who helped with the abstracts.
Une comparaison de quelques
mesures de ressemblance pour
l&apos;analyse comparative des
transcriptions d&apos;articulation
infantile
En ce qui concerne l&apos;analyse des transcriptions
d&apos;articulation infantile, il est tres important
d&apos;identifier les correspondences entre les
articulations de l&apos;enfant, parfois fausses, et celles de
l&apos;adulte percues en tant que modele. Nous decrivons
l&apos;automatisation de cette Cache, et presentons
quelques algorithmes dont nous faisons une
comparaison evaluative. Les algorithmes se basent
sur certaines mesures de ressemblance (ou distance)
phonetique entre les segments individuels qui
considerent les traits perceptuels et articulatoires,
ceux qui peuvent porter des poids scion leur
saillance. Ii s&apos;agit aussi d&apos;une comparaison de
sequences.
Les erreurs d&apos;articulation sont parfois de simples
substitutions d&apos;un son par un autre, ou des insertions
ou omissions, qui sont faciles a analyser. Les
problemes decoulent surtout des &amp;quot;metatheses&amp;quot; (par
ex. elephant s&apos;exprime [efeld]), surtout oü il y a aussi
une substitution (par ex. [epela] pour elephant), et
des &amp;quot;fusions&amp;quot; (par ex. crayon [kRej5] -4 [xej5]) oü le
[x] rassemble egalement au [k] et au [R].
Les trois mesures de ressemblance utilisent les
traits phonetiques: un systeme de simples traits
</bodyText>
<reference confidence="0.822890789473684">
articulatoires binaires (TAB) elabore par le present
auteur; un systeme de traits perceptuels (&amp;quot;force de
friction&amp;quot; et &amp;quot;ton&amp;quot; FF/T) elabore par Connolly
(1997); et un systeme de traits articulatoires non-
binaires base sur Ladefoged (1971). Pour beaucoup
d&apos;exemples, les trois systemes ont trouve la meme
solution. LA ou ils different, le systeme FF/1 est
moms performant. Entre les deux autres, le systeme
le plus simple (TAB) semble aussi etre le plus
robuste. Pour la comparaison des sequences, un seul
algorithme est present&amp; Ii fonctionne tres bien, sauf
quand ii s&apos;agit d&apos;une voyelle identique inseree (par
ex. [kRej5] --+ [keRej5]).
Parmi les logiciels commercialises destines aux
orthophonistes actuellement disponibles, aucun ne
comprend d&apos;analyse automatique des articulations,
celle-ci etant consider&amp; &amp;quot;trop difficile&amp;quot;. Le present
travail suggere qu&apos;un tel logiciel est au contraire tout
a fait concevable.
</reference>
<page confidence="0.99308">
1232
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000043">
<title confidence="0.998635">Similarity metrics for aligning children&apos;s articulation data</title>
<author confidence="0.999654">Harold L SOMERS</author>
<affiliation confidence="0.998286">Centre for Computational Linguistics</affiliation>
<address confidence="0.854466333333333">UMIST, PO Box 88, Manchester M60 1QD, England haro 1 d@c c 1 . umi s t . ac . uk</address>
<abstract confidence="0.997941">1. Background This paper concerns the implementation and testing of similarity metrics for the alignment of phonetic segments in transcriptions of children&apos;s (mis)articulations with the adult model. This has an obvious application in the development of software to assist speech and language clinicians to assess clients and to plan therapy. This paper will give some of the background to this general problem, but will focus on the computational and linguistic aspect of the alignment problem. 1.1. Articulation testing It is well known that a child&apos;s acquisition of phonology is gradual, and can be charted according to the appearance of phonetic distinctions (e.g. stops vs. fricatives), the disappearance of childish mispronunciations, due to assimilation ([gog] for and the ability to articulate particular phonetic configurations (e.g. consonant clusters). Whether screening whole populations of children, or assessing individual referrals, the articulation test is an important tool for the speech clinician. A child&apos;s articulatory development is usually described with reference to an adult model, and in terms of deviations from it: a number of phonological &amp;quot;processes&amp;quot; can be identified, and their significance with respect to the chronological age of the child assessed. Often interact, e.g. when is pronounced [mull] we have consonant-cluster reduction and assimilation. The problem for this paper is to align the segments in the transcription of the child&apos;s articulation with the target model pronunciation. The task is complicated by the need to identify cases of &amp;quot;metathesis&amp;quot;, where the corresponding sounds have been reordered (e.g. .— and &amp;quot;merges&amp;quot;, a special case of consonant-cluster reduction where the resulting segment has some of the features of elements in the original cluster (e.g. It would be appropriate here to review the software currently available to speech clinicians, but lack of space prevents us from doing so (see Somers, forthcoming). Suffice it to say that but is mainly for grammatical and lexical analysis. Of the tiny number of programs which specifically address the problem of articulation testing, none, as far one can tell, involve of the data. 1.2. Segment alignment In a recent paper, Covington (1996) described an algorithm for aligning historical cognates. The present author was struck by the possibility of using this technique for the child-language application, a task for which a somewhat similar algorithm had been developed some years ago (Somers 1978, 1979). In both algorithms, the phonetic segments are interpreted as bundles of phonetic features, and the algorithms include a metric comparing the segments pairwise. The algorithms differ somewhat in the way the search space is reduced, but the results are quite comparable (Somers, forthcoming). Coincidentally, a recent article by Connolly (1997) has suggested a number of ways of quantifying the similarity or difference between two individual phones, on the basis of perceptual and articulatory differences. Connolly&apos;s metric is also feature-based, but differs from the others mentioned in its complexity. In particular, the features can be differentially weighted for salience, and, additionally, not all the features are simple Booleans. In the second part of his article, Connolly introduces a distance measure comparing phones, based on the Levenshtein distance well-known in the 1227 spell-checking, speech-processing and corpusliteratures alia). this metric can be weighted, to allow substitutions to be valued differentially (on the basis of the individual phone distance measure as described in the first part), and to deal with merges and metathesis. Although his methods are clearly computational in nature, Connolly reports (personal communication) that he has not yet implemented them. In this paper, we describe a simple implementation and adaptation of Connolly&apos;s metrics, and a brief critical evaluation of their performance on some child language data (both real and artificial). 2. The alignment algorithms We have implemented three versions of an alignment algorithm, utilising different segment similarity measures, but the same sequence measure. 2.1. Coding the input Before we consider the algorithms themselves, however, it is appropriate to mention briefly the issue of transcription. On the one hand, children&apos;s articulations can include a much wider variety of phones than those which are found in the target system; in addition, certain secondary phonetic features may be particularly important in the description of the child&apos;s articulation (e.g. spreading, laryngealization). So the transcriptions need to be &amp;quot;narrow&amp;quot;. On the other hand, speech clinicians nevertheless tend to use a &amp;quot;contrastive&amp;quot; transcription, essentially phonemic except where the child&apos;s articulation differs from the target: so normal allophonic variation will not necessarily be reflected in the transcription. Any program that is to be used for the analysis of articulation data will need an appropriate coding scheme which allows a narrow transcription in a fairly transparent notation. Some software offers phonetic transcription schemes based on the ASCII character set (e.g. Perry 1995). Alternatively, it seems quite feasible to allow the transcriptions to be input using a standard word-processor and a phonetic font, and to interpret the symbols accordingly. For a commercial implementation it would be better to follow the standard proposed by the IPA (Esling &amp; Gaylord 1993), which has been approved by the ISO, and included in the Unicode definitions. representation Representing the phonetic segments as bundles of features is an obvious technique, and one which is widely adopted. In the algorithm reported in Somers (1979) — henceforth CAT — phones are represented as bundles of binary articulatory features. Some primary features also serve as secondary features where appropriate (e.g. dark &apos;I&apos; is marked as VEL(ar)), but there are also explicit secondary features, e.g. ASP(iration). Connolly (1997) suggests two alternative feature representations. The first is based on which, he claims, are more significant than articulatory features &amp;quot;from the point of view of communicative dysfunction&amp;quot; (p.276). On the other hand, he admits that using perceptual features can be problematic, unless &amp;quot;we are prepared to accept a relatively unrefined quantification method&amp;quot; (p.277). Connolly rejects a number of perceptual feature schemes for consonants in favour of one proposed by Line (1987), which identifies two perceptual features or axes, &amp;quot;friction strength&amp;quot; (FS) and &amp;quot;pitch&amp;quot; (P), and divides the consonant phones into six groups, differentiated by their score on each of these axes, as shown in Figure I. Henceforth we will refer to this scheme as &amp;quot;FS/P&amp;quot;. In fact, there are a number of drawbacks and shortcomings in Connolly&apos;s scheme for our purposes, notably the absence of many non- English phones (all non-pulmonics, uvulars, retroflexes, trills and taps), and there is no indication how to handle secondary features typically needed to transcribe children&apos;s articulations accurately. We have tried to rectify the first shortcoming in our implementation, but it is not obvious how to deal with the second. Connolly&apos;s alternative feature representation based on adapted from Ladefoged&apos;s (1971) system, though unlike the features used in the CAT scheme, some of the features are not binary. Figure 2 shows the feature scheme for consonants, which we have adapted slightly, in detail. We will refer to this 1228 Figure 1. Perceptual feature-based representation (FS/P) of consonants from Connolly (1997:279f) Members bilabial plosives; labial and alveolar nasals glottal obstruents; central and lateral approximants; palatal and velar nasals alveolar plosives; labial and dental fricatives; voiceless nasals velar and palatal obstruents palato-alveolar and lateral fricatives alveolar fricatives and affricates Figure 2. Articulatory feature scheme (Lad) for consonants, adapted from Connolly (1997:282P.</abstract>
<note confidence="0.888716166666667">(a) non-binary features with explanations of the values: glottalic: 1 (ejective), 0.5 (pulmonic), 0 (implosive) voice: 1 (glottal stop), 0.8 (laryngealized), 0.6 (voiced), 0.2 (murmur), 0 (voiceless) (i.e. passive articulator): 1 (labial), 0.9 (dental), 0.85 (alveolar), 0.8 (post-alveolar), 0.75 (prepalatal), 0.7 (palatal), 0.6 (velar), 0.5 (uvular), 0.3 (pharyngeal), 0 (glottal) constrictor: 1 (labial), 0.9 (dental), 0.85 (apical), 0.75 (laminal), 0.6 (dorsal), 0.3 (radical), 0 (glottal) stop: 1 (stop), 0.95 (affricate), 0.9 (fricative), 0 (approximant) length: 1 (long), 0.5 (half-long) (b) binary features: velaric (for clicks), aspirated, nasal, lateral, trill, tap, retroflex, rounded, syllabic, unreleased, grooved Group Friction-strength Pitch 1 0.0 0.0</note>
<phone confidence="0.4927795">2 0.0 0.4 3 0.4 0.3</phone>
<abstract confidence="0.987397533632288">4 0.5 0.8 5 0.8 0.9 6 1.0 1.0 scheme as &amp;quot;Lad&amp;quot;. Again, some features or feature values needed to be added, notably a value of &amp;quot;stop&amp;quot; for affricates. Let us now consider the similarity metrics based on these three schemes. 2.3. Similarity metrics for individual phones The similarity (or distance) metric is the key to the alignment algorithm. In the case of CAT, the distance measure is quite simply a count of the binary features for which the polarity differs. So for example, when comparing the articulation with a target of [st], the [d] differ in terms of three features (VOICE, STOP and FRIC) while [t] and [d] differ in only one (VOICE): so [d] is more similar to [t] than to [s]. In FS/P, the two features are weighted to reflect the greater importance of FS over P, the former being valued double the latter. To calculate the similarity of two phones we add the difference in their FS scores to half the difference in their P scores. If the two phones are in the same group, the score is set at 0.05 (unless they are identical, in which case it is 0). Thus, to take our [st]-4d] example again, since [s] is in group 6, and [t] and [d] both in group 3, [t]-[d] scores 0.05, [s]-[d] 0.95. The similarity metric based on the Lad scheme is simpler, in that all the features are equally weighted. The Lad score is the simply sum of the score differences for all the features. For our example of [st]-4d], the [t]-[d] difference is only in one feature, &amp;quot;voice&amp;quot;, with values 0 and 0.6 respectively, while the [s]-[d] difference has the 0.6 voice difference plus a difference of 0.1 in the &amp;quot;stop&amp;quot; feature ([d] scores I, [s] scores 0.9). All three metrics agree that [d] is more similar to [t] than to [s], as we might hope and expect. As we will see below, the different feature schemes do not always give the same result however. 2.4. Sequence comparison Connolly&apos;s proposed algorithm for aligning sequences of phones is based on the Levenshtein distance. He calls it a &amp;quot;weighted&amp;quot; Levenshtein distance, because the algorithm would have to take into account the similarity scores between individual segments when deciding in cases of combined substitution and deletion (e.g. our [st] -&gt; [d] example) which segment to mark as 1229 inserted or deleted. Connolly suggests (p.291) that substitutions should always be preferred over insertions and deletions, and this assumption was also built into the algorithm we originally developed in Somers (1979). However, this does not always give the correct solution: for example, if the sequence [skr] (e.g. realised as [[ski, we would prefer the alignment in (la) with one insertion and one deletion, to that in ( lb) with only substitutions. (1) a. s k r b. s k r Ss ks k The algorithm would also have to be adjusted to allow for metathesis, though Connolly suggests that merges do not present a special problem because they can always be treated as a substitution plus an omission (p.292) — again we disagree with this approach and will illustrate the problem below. For these reasons we have not used a Levenshtein distance algorithm for our new implementation of the alignment task. As described in Somers (forthcoming), the original alignment algorithm in CAT relied on a single predetermined anchor point, and then exhaustively compared all possible alignments either side of the anchor, though only when the number of segments differed. We now prefer a more general recursive algorithm in which we identify in the two strings a suitable anchor, then split the strings around the two anchor points, and repeat the process with each half string until one (or both) is (are) reduced to the empty string. The algorithm is given in Figure 3. Step 2 is the key to the algorithm, and is primed to look first for identical phones, else vowels, else the phones are compared pairwise exhaustively. If there is a of &amp;quot;best match&amp;quot;, we prefer values of are similar, and near the middle of the string. Although the algorithm is looking for the best match, it is also looking for possible merges, which will be identified when there is no single best match. 2.5. Identifying metathesis It is difficult to incorporate a test for metathesis directly into the above algorithm, and it is better to make a second pass looking for this Figure 3. The alignment algorithm. Let X and Y be the strings to be aligned, of each X[i], Y[/], 1 is a bundle of features. 1. If X=[] and Y=[], then stop; else if X=[] (Y=[]) then mark all segments in Y (X) as &amp;quot;inserted&amp;quot; (&amp;quot;omitted&amp;quot; ) and stop; else continue. 2. Find the best matching X[i] and Y[/], and mark these as &amp;quot;aligned&amp;quot;. 3. Take the substring X[1]..X[i-1] and the substring Y[1]..Y[j-1] and repeat from step 1; and similarly with the substrings X[i+1]..X[m], and Y[/+1]..Y[n]. phenomenon explicitly. For our purposes it is reasonable to focus on consonants. Metathesis can occur either with contiguous phones, e.g. [desk] -4 [deks], or with phones either side of a vowel, e.g. [enfant] -4 [efilant]. In addition, one or both of the phones may have undergone some other phonological processes, e.g. [enfant] -4 [eprlant], where the [f] and [1] have been exchanged, but the [f] realised as a [p]. The algorithm described above will analyse metatheses in one of two ways, depending on various other factors. One analysis will simply align the phones with each other. To recognise this as a case of metathesis, we need to see if the crossing alignment gives a better score. The other analysis will align one or other of the identical phones, and mark the others as omitted/inserted. The second pass looks out for both these situations. 3. Evaluation In this section we consider how the algorithm deals with some data, both real and simulated. We want (a) to see if the algorithm as described gets alignments that correspond to the alignment favoured by a human; and (b) to compare the different feature systems that have been proposed. For many of the examples we have used, there is no problem, and nothing to choose between the systems. These are cases of simple (e.g. substitution and 1230 substitution -4 and various combinations of those processes (Christmas-4[gixmax], aeroplane—+[wejabein]). Cases of inserted vowels (e.g. spoon-4[supun]) were analysed correctly when the inserted vowel was different from the main vowel. So for -4 caused difficulty, with the alignment (2a) preferred over (2b). (2)a. tfimn b.tfim—ni tfim— mi tfim mm Differences between the feature systems show up when the alignment combines substitutions and omissions, and the &amp;quot;best match&amp;quot; comes into play. Vocalisation of (e.g. [bot:I] caused problems, with the syllabic [t] aligning with [u] [?] in FS/P, and [w] in Lad. In other cases where the systems gave different results, the FS/P system most often gave inappropriate alignments. For example, —&gt; [mAn?i] was correctly aligned as in (3a) by the other two systems, but as (3b) with FS/P. mAn7i mAn [ti0]—&gt;[?isx], aligned the [x] the [0] while the other systems got the more [0]—*[s] alignment. Similarly, the Lad CAT systems labelled the [1] as omitted in while FS/P aligned it with [g]. When identifying merges on the other hand, CAT had any success, in (but not when the [I] is not marked as voiceless). In analysing [fl]—&gt;[b], CAT suggests a merge, FS/P marks the [f] as omitted, Lad the [I]. In principle, the FS/P system offers most scope for identifying merges, as it only recognises six different classes of consonant phone, while the Lad system is too fine-grained: indeed, we were unable to find (or simulate) any plausible case which Lad would analyse as a merge. Against that it should also be noted that such analyses cannot be carried out totally in isolation. For example, compare the case where only used when [sl] is expected to the one where [s] is generally realised as [4]: we might analyse only the former as a merge, the latter as a substitution plus omission. It should be remembered that the alignment task is only the first step of the analysis of the child&apos;s phonetic system. 4. Conclusion Because of its poor performance with many alignments, we must reject the FS/P system. This is not a great surprise: a feature system based on perceptual differences seems questionable for an analysis task. There does not seem much to between Lad and the former gives a more subtle scoring system, which might be useful for screening children. On the other hand, it never identifies merges, even in highly plausible cases, so the system using simpler binary articulatory features may be the best solution. Whichever system is used, it seems that an acceptable level of success can be achieved with the algorithm described here, and it could form the basis of software for the automatic analysis of children&apos;s articulation data. 5. References Connolly, John H. (1997) Quantifying target— differences. Linguistics &amp;</abstract>
<note confidence="0.9527254">Phonetics 11:267-298. Covington, Michael A. (1996) An algorithm to align for historical comparison. Esling, John H. &amp; Harry Gaylord (1993) Computer for phonetic symbols. of the</note>
<title confidence="0.956046">Phonetic Association</title>
<author confidence="0.939366">P to Linguistic</author>
<affiliation confidence="0.974063">University of Chicago Press.</affiliation>
<note confidence="0.718890777777778">Line, Pippa (1987) An Investigation of Auditory Distance. M.Phil. dissertation, De Montfort University, Leicester. Perry, Cecyle K. (1995) Review of Phonological Analysis by Computer (PDAC). Language Teaching and Therapy 11:331-340. Somers, H.L. (1978) Computerised Articulation Testing. M.A. thesis, Manchester University. Somers, H.L. (1979) Using the computer to analyse</note>
<abstract confidence="0.964462625">test data. Journal of Disorders Communication Somers, H.L. (forthcoming) Aligning phonetic segments for children&apos;s articulation assessment. To in Linguistics. 1231 Similarity metrics for aligning children&apos;s articulation data An important step in the automatic analysis of child-language articulation data is to align the transcriptions of children&apos;s (mis)articulations with adult models. The problems underlying this task are discussed and a number of algorithms are presented and compared. These are based on various similarity or distance measures for individual phonetic segments, considering perceptual and articulatory features, which may be weighted to reflect salience, and on sequence comparison. ofrAtisitm tt I&amp;quot; 6 Z:&amp;quot;.. L&amp;quot; ;Fitt iz V, LN&lt; TO60 fitYX1t, LNOT, 7)1.19 tAllArD Ltantsnia 1ztft**Ittc.65M1445(07) tifcc, Acknowledgements Thanks to Joe Somers for providing some of the example data; and to Marie-Jo Proulx and Ayako Matsuo who helped with the abstracts. Une comparaison de quelques mesures de ressemblance pour l&apos;analyse comparative des transcriptions d&apos;articulation infantile En ce qui concerne l&apos;analyse des transcriptions d&apos;articulation infantile, il est tres important d&apos;identifier les correspondences entre les articulations de l&apos;enfant, parfois fausses, et celles de l&apos;adulte percues en tant que modele. Nous decrivons l&apos;automatisation de cette Cache, et presentons quelques algorithmes dont nous faisons une comparaison evaluative. Les algorithmes se basent sur certaines mesures de ressemblance (ou distance) phonetique entre les segments individuels qui considerent les traits perceptuels et articulatoires, ceux qui peuvent porter des poids scion leur saillance. Ii s&apos;agit aussi d&apos;une comparaison de sequences. Les erreurs d&apos;articulation sont parfois de simples substitutions d&apos;un son par un autre, ou des insertions ou omissions, qui sont faciles a analyser. Les problemes decoulent surtout des &amp;quot;metatheses&amp;quot; (par [efeld]), surtout oü il y a aussi substitution (par ex. [epela] pour &amp;quot;fusions&amp;quot; (par ex. -4 [xej5]) oü le [x] rassemble egalement au [k] et au [R]. Les trois mesures de ressemblance utilisent les traits phonetiques: un systeme de simples traits articulatoires binaires (TAB) elabore par le present de traits perceptuels (&amp;quot;force de friction&amp;quot; et &amp;quot;ton&amp;quot; FF/T) elabore par Connolly et un systeme de traits articulatoires nonsur Ladefoged (1971). Pour beaucoup trois systemes ont trouve la meme different, le systeme FF/1 est performant. les deux autres, le systeme plus simple (TAB) aussi etre le plus Pour la comparaison sequences, un seul est present&amp; Ii tres bien, sauf ii s&apos;agit d&apos;une identique inseree (par ex. [kRej5] --+ [keRej5]). les commercialises destines aux actuellement aucun ne d&apos;analyse des articulations, etant consider&amp; difficile&amp;quot;. Le present suggere qu&apos;un logiciel est au contraire tout a fait concevable.</abstract>
<intro confidence="0.611921">1232</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John H Connolly</author>
</authors>
<title>Quantifying target— realization differences.</title>
<date>1997</date>
<journal>Clinical Linguistics &amp; Phonetics</journal>
<pages>11--267</pages>
<contexts>
<context position="3186" citStr="Connolly (1997)" startWordPosition="482" endWordPosition="483">orithm for aligning historical cognates. The present author was struck by the possibility of using this technique for the child-language application, a task for which a somewhat similar algorithm had been developed some years ago (Somers 1978, 1979). In both algorithms, the phonetic segments are interpreted as bundles of phonetic features, and the algorithms include a simple similarity metric for comparing the segments pairwise. The algorithms differ somewhat in the way the search space is reduced, but the results are quite comparable (Somers, forthcoming). Coincidentally, a recent article by Connolly (1997) has suggested a number of ways of quantifying the similarity or difference between two individual phones, on the basis of perceptual and articulatory differences. Connolly&apos;s metric is also feature-based, but differs from the others mentioned in its complexity. In particular, the features can be differentially weighted for salience, and, additionally, not all the features are simple Booleans. In the second part of his article, Connolly introduces a distance measure for comparing sequences of phones, based on the Levenshtein distance well-known in the 1227 spell-checking, speech-processing and </context>
<context position="6413" citStr="Connolly (1997)" startWordPosition="963" endWordPosition="964">er to follow the standard proposed by the IPA (Esling &amp; Gaylord 1993), which has been approved by the ISO, and included in the Unicode definitions. 2.2. Internal representation Representing the phonetic segments as bundles of features is an obvious technique, and one which is widely adopted. In the algorithm reported in Somers (1979) — henceforth CAT — phones are represented as bundles of binary articulatory features. Some primary features also serve as secondary features where appropriate (e.g. dark &apos;I&apos; is marked as VEL(ar)), but there are also explicit secondary features, e.g. ASP(iration). Connolly (1997) suggests two alternative feature representations. The first is based on perceptual features, which, he claims, are more significant than articulatory features &amp;quot;from the point of view of communicative dysfunction&amp;quot; (p.276). On the other hand, he admits that using perceptual features can be problematic, unless &amp;quot;we are prepared to accept a relatively unrefined quantification method&amp;quot; (p.277). Connolly rejects a number of perceptual feature schemes for consonants in favour of one proposed by Line (1987), which identifies two perceptual features or axes, &amp;quot;friction strength&amp;quot; (FS) and &amp;quot;pitch&amp;quot; (P), and</context>
<context position="8057" citStr="Connolly (1997" startWordPosition="1212" endWordPosition="1213">tures typically needed to transcribe children&apos;s articulations accurately. We have tried to rectify the first shortcoming in our implementation, but it is not obvious how to deal with the second. Connolly&apos;s alternative feature representation is based on articulatory features, adapted from Ladefoged&apos;s (1971) system, though unlike the features used in the CAT scheme, some of the features are not binary. Figure 2 shows the feature scheme for consonants, which we have adapted slightly, in detail. We will refer to this 1228 Figure 1. Perceptual feature-based representation (FS/P) of consonants from Connolly (1997:279f) Members bilabial plosives; labial and alveolar nasals glottal obstruents; central and lateral approximants; palatal and velar nasals alveolar plosives; labial and dental fricatives; voiceless nasals velar and palatal obstruents palato-alveolar and lateral fricatives alveolar fricatives and affricates Figure 2. Articulatory feature scheme (Lad) for consonants, adapted from Connolly (1997:282P. (a) non-binary features with explanations of the values: glottalic: 1 (ejective), 0.5 (pulmonic), 0 (implosive) voice: 1 (glottal stop), 0.8 (laryngealized), 0.6 (voiced), 0.2 (murmur), 0 (voiceles</context>
</contexts>
<marker>Connolly, 1997</marker>
<rawString>Connolly, John H. (1997) Quantifying target— realization differences. Clinical Linguistics &amp; Phonetics 11:267-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>An algorithm to align words for historical comparison.</title>
<date>1996</date>
<journal>Computational Linguistics</journal>
<pages>22--481</pages>
<contexts>
<context position="2554" citStr="Covington (1996)" startWordPosition="389" endWordPosition="390">sonant-cluster reduction where the resulting segment has some of the features of both elements in the original cluster (e.g. sleep It would be appropriate here to review the software currently available to speech clinicians, but lack of space prevents us from doing so (see Somers, forthcoming). Suffice it to say that software does exist, but is mainly for grammatical and lexical analysis. Of the tiny number of programs which specifically address the problem of articulation testing, none, as far as one can tell, involve automatic alignment of the data. 1.2. Segment alignment In a recent paper, Covington (1996) described an algorithm for aligning historical cognates. The present author was struck by the possibility of using this technique for the child-language application, a task for which a somewhat similar algorithm had been developed some years ago (Somers 1978, 1979). In both algorithms, the phonetic segments are interpreted as bundles of phonetic features, and the algorithms include a simple similarity metric for comparing the segments pairwise. The algorithms differ somewhat in the way the search space is reduced, but the results are quite comparable (Somers, forthcoming). Coincidentally, a r</context>
</contexts>
<marker>Covington, 1996</marker>
<rawString>Covington, Michael A. (1996) An algorithm to align words for historical comparison. Computational Linguistics 22:481-496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John H Esling</author>
<author>Harry Gaylord</author>
</authors>
<title>Computer codes for phonetic symbols.</title>
<date>1993</date>
<journal>Journal of the International Phonetic Association</journal>
<pages>23--83</pages>
<contexts>
<context position="5867" citStr="Esling &amp; Gaylord 1993" startWordPosition="879" endWordPosition="882">t necessarily be reflected in the transcription. Any program that is to be used for the analysis of articulation data will need an appropriate coding scheme which allows a narrow transcription in a fairly transparent notation. Some software offers phonetic transcription schemes based on the ASCII character set (e.g. Perry 1995). Alternatively, it seems quite feasible to allow the transcriptions to be input using a standard word-processor and a phonetic font, and to interpret the symbols accordingly. For a commercial implementation it would be better to follow the standard proposed by the IPA (Esling &amp; Gaylord 1993), which has been approved by the ISO, and included in the Unicode definitions. 2.2. Internal representation Representing the phonetic segments as bundles of features is an obvious technique, and one which is widely adopted. In the algorithm reported in Somers (1979) — henceforth CAT — phones are represented as bundles of binary articulatory features. Some primary features also serve as secondary features where appropriate (e.g. dark &apos;I&apos; is marked as VEL(ar)), but there are also explicit secondary features, e.g. ASP(iration). Connolly (1997) suggests two alternative feature representations. The</context>
</contexts>
<marker>Esling, Gaylord, 1993</marker>
<rawString>Esling, John H. &amp; Harry Gaylord (1993) Computer codes for phonetic symbols. Journal of the International Phonetic Association 23:83-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ladefoged</author>
</authors>
<title>Preliminaries to Linguistic Phonetics.</title>
<date>1971</date>
<publisher>University of Chicago Press.</publisher>
<location>Chicago:</location>
<marker>Ladefoged, 1971</marker>
<rawString>Ladefoged, P. (1971) Preliminaries to Linguistic Phonetics. Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pippa Line</author>
</authors>
<title>An Investigation of Auditory Distance.</title>
<date>1987</date>
<institution>De Montfort University,</institution>
<location>Leicester.</location>
<note>M.Phil. dissertation,</note>
<contexts>
<context position="6916" citStr="Line (1987)" startWordPosition="1036" endWordPosition="1037">I&apos; is marked as VEL(ar)), but there are also explicit secondary features, e.g. ASP(iration). Connolly (1997) suggests two alternative feature representations. The first is based on perceptual features, which, he claims, are more significant than articulatory features &amp;quot;from the point of view of communicative dysfunction&amp;quot; (p.276). On the other hand, he admits that using perceptual features can be problematic, unless &amp;quot;we are prepared to accept a relatively unrefined quantification method&amp;quot; (p.277). Connolly rejects a number of perceptual feature schemes for consonants in favour of one proposed by Line (1987), which identifies two perceptual features or axes, &amp;quot;friction strength&amp;quot; (FS) and &amp;quot;pitch&amp;quot; (P), and divides the consonant phones into six groups, differentiated by their score on each of these axes, as shown in Figure I. Henceforth we will refer to this scheme as &amp;quot;FS/P&amp;quot;. In fact, there are a number of drawbacks and shortcomings in Connolly&apos;s scheme for our purposes, notably the absence of many nonEnglish phones (all non-pulmonics, uvulars, retroflexes, trills and taps), and there is no indication how to handle secondary features typically needed to transcribe children&apos;s articulations accurately.</context>
</contexts>
<marker>Line, 1987</marker>
<rawString>Line, Pippa (1987) An Investigation of Auditory Distance. M.Phil. dissertation, De Montfort University, Leicester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecyle K Perry</author>
</authors>
<title>Review of Phonological Deviation Analysis by Computer (PDAC). Child Language Teaching and Therapy</title>
<date>1995</date>
<pages>11--331</pages>
<contexts>
<context position="5574" citStr="Perry 1995" startWordPosition="835" endWordPosition="836">ing, laryngealization). So the transcriptions need to be &amp;quot;narrow&amp;quot;. On the other hand, speech clinicians nevertheless tend to use a &amp;quot;contrastive&amp;quot; transcription, essentially phonemic except where the child&apos;s articulation differs from the target: so normal allophonic variation will not necessarily be reflected in the transcription. Any program that is to be used for the analysis of articulation data will need an appropriate coding scheme which allows a narrow transcription in a fairly transparent notation. Some software offers phonetic transcription schemes based on the ASCII character set (e.g. Perry 1995). Alternatively, it seems quite feasible to allow the transcriptions to be input using a standard word-processor and a phonetic font, and to interpret the symbols accordingly. For a commercial implementation it would be better to follow the standard proposed by the IPA (Esling &amp; Gaylord 1993), which has been approved by the ISO, and included in the Unicode definitions. 2.2. Internal representation Representing the phonetic segments as bundles of features is an obvious technique, and one which is widely adopted. In the algorithm reported in Somers (1979) — henceforth CAT — phones are represente</context>
</contexts>
<marker>Perry, 1995</marker>
<rawString>Perry, Cecyle K. (1995) Review of Phonological Deviation Analysis by Computer (PDAC). Child Language Teaching and Therapy 11:331-340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H L Somers</author>
</authors>
<title>Computerised Articulation Testing.</title>
<date>1978</date>
<tech>M.A. thesis,</tech>
<institution>Manchester University.</institution>
<contexts>
<context position="2813" citStr="Somers 1978" startWordPosition="428" endWordPosition="429">g so (see Somers, forthcoming). Suffice it to say that software does exist, but is mainly for grammatical and lexical analysis. Of the tiny number of programs which specifically address the problem of articulation testing, none, as far as one can tell, involve automatic alignment of the data. 1.2. Segment alignment In a recent paper, Covington (1996) described an algorithm for aligning historical cognates. The present author was struck by the possibility of using this technique for the child-language application, a task for which a somewhat similar algorithm had been developed some years ago (Somers 1978, 1979). In both algorithms, the phonetic segments are interpreted as bundles of phonetic features, and the algorithms include a simple similarity metric for comparing the segments pairwise. The algorithms differ somewhat in the way the search space is reduced, but the results are quite comparable (Somers, forthcoming). Coincidentally, a recent article by Connolly (1997) has suggested a number of ways of quantifying the similarity or difference between two individual phones, on the basis of perceptual and articulatory differences. Connolly&apos;s metric is also feature-based, but differs from the o</context>
</contexts>
<marker>Somers, 1978</marker>
<rawString>Somers, H.L. (1978) Computerised Articulation Testing. M.A. thesis, Manchester University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H L Somers</author>
</authors>
<title>Using the computer to analyse articulation test data.</title>
<date>1979</date>
<journal>British Journal of Disorders of Communication</journal>
<pages>14--231</pages>
<contexts>
<context position="6133" citStr="Somers (1979)" startWordPosition="922" endWordPosition="923">mes based on the ASCII character set (e.g. Perry 1995). Alternatively, it seems quite feasible to allow the transcriptions to be input using a standard word-processor and a phonetic font, and to interpret the symbols accordingly. For a commercial implementation it would be better to follow the standard proposed by the IPA (Esling &amp; Gaylord 1993), which has been approved by the ISO, and included in the Unicode definitions. 2.2. Internal representation Representing the phonetic segments as bundles of features is an obvious technique, and one which is widely adopted. In the algorithm reported in Somers (1979) — henceforth CAT — phones are represented as bundles of binary articulatory features. Some primary features also serve as secondary features where appropriate (e.g. dark &apos;I&apos; is marked as VEL(ar)), but there are also explicit secondary features, e.g. ASP(iration). Connolly (1997) suggests two alternative feature representations. The first is based on perceptual features, which, he claims, are more significant than articulatory features &amp;quot;from the point of view of communicative dysfunction&amp;quot; (p.276). On the other hand, he admits that using perceptual features can be problematic, unless &amp;quot;we are pr</context>
<context position="11713" citStr="Somers (1979)" startWordPosition="1799" endWordPosition="1800"> Sequence comparison Connolly&apos;s proposed algorithm for aligning sequences of phones is based on the Levenshtein distance. He calls it a &amp;quot;weighted&amp;quot; Levenshtein distance, because the algorithm would have to take into account the similarity scores between individual segments when deciding in cases of combined substitution and deletion (e.g. our [st] -&gt; [d] example) which segment to mark as 1229 inserted or deleted. Connolly suggests (p.291) that substitutions should always be preferred over insertions and deletions, and this assumption was also built into the algorithm we originally developed in Somers (1979). However, this does not always give the correct solution: for example, if the sequence [skr] (e.g. in scrape) was realised as [[ski, we would prefer the alignment in (la) with one insertion and one deletion, to that in ( lb) with only substitutions. (1) a. - s k r b. s k r Ss k- s k The algorithm would also have to be adjusted to allow for metathesis, though Connolly suggests that merges do not present a special problem because they can always be treated as a substitution plus an omission (p.292) — again we disagree with this approach and will illustrate the problem below. For these reasons w</context>
</contexts>
<marker>Somers, 1979</marker>
<rawString>Somers, H.L. (1979) Using the computer to analyse articulation test data. British Journal of Disorders of Communication 14:231-240.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H L Somers</author>
</authors>
<title>(forthcoming) Aligning phonetic segments for children&apos;s articulation assessment. To appear in Computational Linguistics. articulatoires binaires (TAB) elabore par le present auteur; un systeme de traits perceptuels (&amp;quot;force de friction&amp;quot; et &amp;quot;ton&amp;quot; FF/T) elabore par Connolly</title>
<date>1997</date>
<note>[kRej5] --+ [keRej5]).</note>
<marker>Somers, 1997</marker>
<rawString>Somers, H.L. (forthcoming) Aligning phonetic segments for children&apos;s articulation assessment. To appear in Computational Linguistics. articulatoires binaires (TAB) elabore par le present auteur; un systeme de traits perceptuels (&amp;quot;force de friction&amp;quot; et &amp;quot;ton&amp;quot; FF/T) elabore par Connolly (1997); et un systeme de traits articulatoires nonbinaires base sur Ladefoged (1971). Pour beaucoup d&apos;exemples, les trois systemes ont trouve la meme solution. LA ou ils different, le systeme FF/1 est moms performant. Entre les deux autres, le systeme le plus simple (TAB) semble aussi etre le plus robuste. Pour la comparaison des sequences, un seul algorithme est present&amp; Ii fonctionne tres bien, sauf quand ii s&apos;agit d&apos;une voyelle identique inseree (par ex. [kRej5] --+ [keRej5]).</rawString>
</citation>
<citation valid="false">
<title>Parmi les logiciels commercialises destines aux orthophonistes actuellement disponibles, aucun ne comprend d&apos;analyse automatique des articulations, celle-ci etant consider&amp; &amp;quot;trop difficile&amp;quot;. Le present travail suggere qu&apos;un tel logiciel est au contraire tout a fait concevable.</title>
<marker></marker>
<rawString>Parmi les logiciels commercialises destines aux orthophonistes actuellement disponibles, aucun ne comprend d&apos;analyse automatique des articulations, celle-ci etant consider&amp; &amp;quot;trop difficile&amp;quot;. Le present travail suggere qu&apos;un tel logiciel est au contraire tout a fait concevable.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>