<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.046752">
<title confidence="0.987294">
Syntax Augmented Machine Translation via Chart Parsing
</title>
<author confidence="0.990896">
Andreas Zollmann and Ashish Venugopal
</author>
<affiliation confidence="0.984926">
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.999397">
{zollmann,ashishv}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99964412">
We present translation results on the
shared task ”Exploiting Parallel Texts for
Statistical Machine Translation” gener-
ated by a chart parsing decoder operating
on phrase tables augmented and general-
ized with target language syntactic cate-
gories. We use a target language parser
to generate parse trees for each sentence
on the target side of the bilingual train-
ing corpus, matching them with phrase
table lattices built for the corresponding
source sentence. Considering phrases that
correspond to syntactic categories in the
parse trees we develop techniques to aug-
ment (declare a syntactically motivated
category for a phrase pair) and general-
ize (form mixed terminal and nonterminal
phrases) the phrase table into a synchro-
nous bilingual grammar. We present re-
sults on the French-to-English task for this
workshop, representing significant im-
provements over the workshop’s baseline
system. Our translation system is avail-
able open-source under the GNU General
Public License.
</bodyText>
<sectionHeader confidence="0.999628" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999565">
Recent work in machine translation has evolved
from the traditional word (Brown et al., 1993) and
phrase based (Koehn et al., 2003a) models to in-
clude hierarchical phrase models (Chiang, 2005) and
bilingual synchronous grammars (Melamed, 2004).
These advances are motivated by the desire to in-
tegrate richer knowledge sources within the transla-
tion process with the explicit goal of producing more
fluent translations in the target language. The hi-
erarchical translation operations introduced in these
methods call for extensions to the traditional beam
decoder (Koehn et al., 2003a). In this work we
introduce techniques to generate syntactically mo-
tivated generalized phrases and discuss issues in
chart parser based decoding in the statistical ma-
chine translation environment.
(Chiang, 2005) generates synchronous context-
free grammar (SynCFG) rules from an existing
phrase translation table. These rules can be viewed
as phrase pairs with mixed lexical and non-terminal
entries, where non-terminal entries (occurring as
pairs in the source and target side) represent place-
holders for inserting additional phrases pairs (which
again may contain nonterminals) at decoding time.
While (Chiang, 2005) uses only two nonterminal
symbols in his grammar, we introduce multiple syn-
tactic categories, taking advantage of a target lan-
guage parser for this information. While (Yamada
and Knight, 2002) represent syntactical information
in the decoding process through a series of transfor-
mation operations, we operate directly at the phrase
level. In addition to the benefits that come from
a more structured hierarchical rule set, we believe
that these restrictions serve as a syntax driven lan-
guage model that can guide the decoding process,
as n-gram context based language models do in tra-
ditional decoding. In the following sections, we
describe our phrase annotation and generalization
process followed by the design and pruning deci-
sions in our chart parser. We give results on the
French-English Europarl data and conclude with
prospects for future work.
</bodyText>
<page confidence="0.957254">
138
</page>
<note confidence="0.4782535">
Proceedings of the Workshop on Statistical Machine Translation, pages 138–141,
New York City, June 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.946895" genericHeader="method">
2 Rule Generation
</sectionHeader>
<subsectionHeader confidence="0.931305">
X -&gt; rep
</subsectionHeader>
<bodyText confidence="0.997389714285714">
We start with phrase translations on the parallel
training data using the techniques and implementa-
tion described in (Koehn et al., 2003a). This phrase
table provides the purely lexical entries in the final
hierarchical rule set that will be used in decoding.
We then use Charniak’s parser (Charniak, 2000) to
generate the most likely parse tree for each Eng-
lish target sentence in the training corpus. Next,
we determine all phrase pairs in the phrase table
whose source and target side occur in each respec-
tive source and target sentence pair defining the
scope of the initial rules in our SynCFG.
Annotation If the target side of any of these ini-
tial rules correspond to a syntactic category C of the
target side parse tree, we label the phrase pair with
that syntactic category. This label corresponds to the
left-hand side of our synchronous grammar. Phrase
pairs that do not correspond to a span in the parse
tree are given a default category &apos;X&apos;, and can still
play a role in the decoding process. In work done af-
ter submission to the 2006 data track, we assign such
phrases an extended category of the form C1 + C2,
C1/C2, or C2\C1, indicating that the phrase pair’s
target side spans two adjacent syntactic categories
(e.g., she went: NP+V), a partial syntactic cate-
gory C1 missing a C2 to the right (e.g., the great:
NP/NN), or a partial C1 missing a C2 to the left (e.g.,
great wall: DT\NP), respectively.
Generalization In order to mitigate the effects
of sparse data when working with phrase and n-
gram models we would like to generate generalized
phrases, which include non-terminal symbols that
can be filled with other phrases. Therefore, after
annotating the initial rules from the current train-
ing sentence pair, we adhere to (Chiang, 2005) to
recursively generalize each existing rule; however,
we abstract on a per-sentence basis. The grammar
extracted from this evaluation’s training data con-
tains 75 nonterminals in our standard system, and
4000 nonterminals in the extended-category system.
Figure 1 illustrates the annotation and generalization
process.
</bodyText>
<figure confidence="0.687079">
S -&gt; [NP (N n
</figure>
<figureCaption confidence="0.96775">
Figure 1: Selected annotated and generalized (dotted arc)
rules for the first sentence ofEuroparl.
</figureCaption>
<sectionHeader confidence="0.992557" genericHeader="method">
3 Scoring
</sectionHeader>
<bodyText confidence="0.999966166666667">
We employ a log-linear model to assign costs to the
SynCFG. Given a source sentence f, the preferred
translation output is determined by computing the
lowest-cost derivation (combination of hierarchical
and glue rules) yielding f as its source side, where
the cost of a derivation R1 o · · · o Rn with respective
feature vectors v1, ... , vn E Rm is given by
Here, λ1, ... , λm are the parameters of the log-
linear model, which we optimize on a held-out por-
tion of the training set (2005 development data) us-
ing minimum-error-rate training (Och, 2003). We
use the following features for our rules:
</bodyText>
<listItem confidence="0.998864363636364">
• source- and target-conditioned neg-log lexical
weights as described in (Koehn et al., 2003b)
• neg-log relative frequencies: left-hand-
side-conditioned, target-phrase-conditioned,
source-phrase-conditioned
• Counters: n.o. rule applications, n.o. target
words
• Flags: IsPurelyLexical (i.e., contains only ter-
minals), IsPurelyAbstract (i.e., contains only
nonterminals), IsXRule (i.e., non-syntactical
span), IsGlueRule
</listItem>
<equation confidence="0.8144445">
N-&gt;reprise/resum
repris
λi
�vj�i .
�m
i=1
n
j=1
</equation>
<page confidence="0.983114">
139
</page>
<listItem confidence="0.9905605">
• Penalties: rareness penalty exp(1 −
RuleFrequency); unbalancedness penalty
jMeanTargetSourceRatio * ‘n.o. source words’−
‘n.o. target words’�
</listItem>
<sectionHeader confidence="0.980738" genericHeader="method">
4 Parsing
</sectionHeader>
<bodyText confidence="0.999884489361702">
Our SynCFG rules are equivalent to a probabilistic
context-free grammar and decoding is therefore an
application of chart parsing. Instead of the common
method of converting the CFG grammar into Chom-
sky Normal Form and applying a CKY algorithm
to produce the most likely parse for a given source
sentence, we avoided the explosion of the rule set
caused by the introduction of new non-terminals in
the conversion process and implemented a variant
of the CKY+ algorithm as described in (J.Earley,
1970).
Each cell of the parsing process in (J.Earley,
1970) contains a set of hypergraph nodes (Huang
and Chiang, 2005). A hypergraph node is an equiv-
alence class of complete hypotheses (derivations)
with identical production results (left-hand sides of
the corresponding applied rules). Complete hy-
potheses point directly to nodes in their backwards
star, and the cost of the complete hypothesis is cal-
culated with respect to each back pointer node’s best
cost.
This structure affords efficient parsing with mini-
mal pruning (we use a single parameter to restrict the
number of hierarchical rules applied), but sacrifices
effective management of unique language model
states contributing to significant search errors dur-
ing parsing. At initial submission time we simply
re-scored a K-Best list extracted after first best pars-
ing using the lazy retrieval process in (Huang and
Chiang, 2005).
Post-submission After our workshop submission,
we modified the K-Best list extraction process to in-
tegrate an n-gram language model during K-Best ex-
traction. Instead of expanding each derivation (com-
plete hypothesis) in a breadth-first fashion, we ex-
pand only a single back pointer, and score this new
derivation with its translation model scores and a
language model cost estimate, consisting of an ac-
curate component, based on the words translated so
far, and an estimate based on each remaining (not
expanded) back pointer’s top scoring hypothesis.
To improve the diversity of the final K-Best list,
we keep track of partially expanded hypotheses that
have generated identical target words and refer to the
same hypergraph nodes. Any arising twin hypothe-
sis is immediately removed from the K-Best extrac-
tion beam during the expansion process.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.998471166666667">
We present results that compare our system against
the baseline Pharaoh implementation (Koehn et al.,
2003a) and MER training scripts provided for this
workshop. Our results represent work done before
the submission due date as well as after with the fol-
lowing generalized phrase systems.
</bodyText>
<listItem confidence="0.999081625">
• Baseline - Pharaoh with phrases extracted from
IBM Model 4 training with maximum phrase
length 7 and extraction method ‘diag-growth-
final’ (Koehn et al., 2003a)
• Lex - Phrase-decoder simulation: using only
the initial lexical rules from the phrase table,
all with LHS X, the Glue rule, and a binary
reordering rule with its own reordering-feature
• XCat - All nonterminals merged into a single
X nonterminal: simulation of the system Hiero
(Chiang, 2005).
• Syn - Syntactic extraction using the Penn Tree-
bank parse categories as nonterminals; rules
containing up to 4 nonterminal abstraction
sites.
• SynExt - Syntactic extraction using the
</listItem>
<bodyText confidence="0.911290214285714">
extended-category scheme, but with rules only
containing up to 2 nonterminal abstraction
sites.
We also explored the impact of longer initial
phrases by training another phrase table with phrases
up to length 12. Our results are presented in Ta-
ble 1. While our submission time system (Syn using
LM for rescoring only) shows no improvement over
the baseline, we clearly see the impact of integrating
the language model into the K-Best list extraction
process. Our final system shows at statistically sig-
nificant improvement over the baseline (0.78 BLEU
points is the 95 confidence level). We also see a
trend towards improving translation quality as we
</bodyText>
<page confidence="0.990158">
140
</page>
<table confidence="0.9982132">
System Dev: w/o LM Dev: LM-rescoring Test: LM-r. Dev: integrated LM Test: int. LM
Baseline - max. phr. length 7 – – – 31.11 30.61
Lex - max. phrase length 7 27.94 29.39 29.95 28.96 29.12
XCat - max. phrase length 7 27.56 30.27 29.81 30.89 31.01
Syn - max. phrase length 7 29.20 30.95 30.58 31.52 31.31
SynExt - max. phrase length 7 – – – 31.73 31.41
Baseline - max. phr. length 12 – – – 31.16 30.90
Lex - max. phr. length 12 – – – 29.30 29.51
XCat - max. phr. length 12 – – – 30.79 30.59
SynExt - max. phr. length 12 – – – 31.07 31.76
</table>
<tableCaption confidence="0.968644">
Table 1: Translation results (IBM BLEU) for each system on the Fr-En ’06 Shared Task ‘Development Set’ (used for MER
parameter tuning) and ’06 ‘Development Test Set’ (identical to last year’s Shared Task’s test set). The system submitted for
evaluation is highlighted in bold.
</tableCaption>
<bodyText confidence="0.999479">
employ richer extraction techniques. The relatively
poor performance of Lex with LM in K-Best com-
pared to the baseline shows that we are still making
search errors during parsing despite tighter integra-
tion of the language model.
We also ran an experiment with CMU’s phrase-
based decoder (Vogel et al., 2003) using the length-
7 phrase table. While its development-set score was
only 31.01, the decoder achieved 31.42 on the test
set, placing it at the same level as our extended-
category system for that phrase table.
</bodyText>
<sectionHeader confidence="0.999603" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999927533333333">
In this work we applied syntax based resources
(the target language parser) to annotate and gener-
alize phrase translation tables extracted via exist-
ing phrase extraction techniques. Our work reaf-
firms the feasibility of parsing approaches to ma-
chine translation in a large data setting, and il-
lustrates the impact of adding syntactic categories
to drive and constrain the structured search space.
While no improvements were available at submis-
sion time, our subsequent performance highlights
the importance of tight integration of n-gram lan-
guage modeling within the syntax driven parsing en-
vironment. Our translation system is available open-
source under the GNU General Public License at:
www.cs.cmu.edu/˜zollmann/samt
</bodyText>
<sectionHeader confidence="0.999328" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999626095238095">
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263–311.
Eugene Charniak. 2000. A maximum entropy-inspired
parser. In Proceedings of the North American Associ-
ation for Computational Linguistics (HLT/NAACL).
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of the As-
sociation for Computational Linguistics.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the 9th International Work-
shop on Parsing Technologies.
J.Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the Assocation for Com-
puting Machinery, 13(2):94–102.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003a. Pharaoh: A beam search decoder for phrase-
base statistical machine translation models. In Pro-
ceedings of the Sixth Confernence of the Association
for Machine Translation in the Americas, Edomonton,
Canada, May 27-June 1.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003b. Statistical phrase-based translation. In
Proceedings of the Human Language Technology
and North American Association for Computational
Linguistics Conference (HLT/NAACL), Edomonton,
Canada, May 27-June 1.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In ACL, pages 653–660.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the Associ-
ation for Computational Linguistics, Sapporo, Japan,
July 6-7.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venogupal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical translation system. In Pro-
ceedings ofMT Summit IX, New Orleans, LA, Septem-
ber.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In Proc. of the Association
for Computational Linguistics.
</reference>
<page confidence="0.99825">
141
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.365536">
<title confidence="0.999657">Syntax Augmented Machine Translation via Chart Parsing</title>
<author confidence="0.932885">Zollmann</author>
<affiliation confidence="0.9943265">School of Computer Carnegie Mellon</affiliation>
<abstract confidence="0.99628196">We present translation results on the shared task ”Exploiting Parallel Texts for Statistical Machine Translation” generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop’s baseline system. Our translation system is available open-source under the GNU General</abstract>
<intro confidence="0.410591">Public License.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1274" citStr="Brown et al., 1993" startWordPosition="182" endWordPosition="185">ng phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop’s baseline system. Our translation system is available open-source under the GNU General Public License. 1 Introduction Recent work in machine translation has evolved from the traditional word (Brown et al., 1993) and phrase based (Koehn et al., 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). These advances are motivated by the desire to integrate richer knowledge sources within the translation process with the explicit goal of producing more fluent translations in the target language. The hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder (Koehn et al., 2003a). In this work we introduce techniques to generate syntactically motivated generalized phrases and discuss iss</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics (HLT/NAACL).</booktitle>
<contexts>
<context position="3726" citStr="Charniak, 2000" startWordPosition="556" endWordPosition="557">sions in our chart parser. We give results on the French-English Europarl data and conclude with prospects for future work. 138 Proceedings of the Workshop on Statistical Machine Translation, pages 138–141, New York City, June 2006. c�2006 Association for Computational Linguistics 2 Rule Generation X -&gt; rep We start with phrase translations on the parallel training data using the techniques and implementation described in (Koehn et al., 2003a). This phrase table provides the purely lexical entries in the final hierarchical rule set that will be used in decoding. We then use Charniak’s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus. Next, we determine all phrase pairs in the phrase table whose source and target side occur in each respective source and target sentence pair defining the scope of the initial rules in our SynCFG. Annotation If the target side of any of these initial rules correspond to a syntactic category C of the target side parse tree, we label the phrase pair with that syntactic category. This label corresponds to the left-hand side of our synchronous grammar. Phrase pairs that do not correspond to a span in t</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum entropy-inspired parser. In Proceedings of the North American Association for Computational Linguistics (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1373" citStr="Chiang, 2005" startWordPosition="200" endWordPosition="201">eclare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop’s baseline system. Our translation system is available open-source under the GNU General Public License. 1 Introduction Recent work in machine translation has evolved from the traditional word (Brown et al., 1993) and phrase based (Koehn et al., 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). These advances are motivated by the desire to integrate richer knowledge sources within the translation process with the explicit goal of producing more fluent translations in the target language. The hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder (Koehn et al., 2003a). In this work we introduce techniques to generate syntactically motivated generalized phrases and discuss issues in chart parser based decoding in the statistical machine translation environment. (Chiang, 200</context>
<context position="5177" citStr="Chiang, 2005" startWordPosition="809" endWordPosition="810">icating that the phrase pair’s target side spans two adjacent syntactic categories (e.g., she went: NP+V), a partial syntactic category C1 missing a C2 to the right (e.g., the great: NP/NN), or a partial C1 missing a C2 to the left (e.g., great wall: DT\NP), respectively. Generalization In order to mitigate the effects of sparse data when working with phrase and ngram models we would like to generate generalized phrases, which include non-terminal symbols that can be filled with other phrases. Therefore, after annotating the initial rules from the current training sentence pair, we adhere to (Chiang, 2005) to recursively generalize each existing rule; however, we abstract on a per-sentence basis. The grammar extracted from this evaluation’s training data contains 75 nonterminals in our standard system, and 4000 nonterminals in the extended-category system. Figure 1 illustrates the annotation and generalization process. S -&gt; [NP (N n Figure 1: Selected annotated and generalized (dotted arc) rules for the first sentence ofEuroparl. 3 Scoring We employ a log-linear model to assign costs to the SynCFG. Given a source sentence f, the preferred translation output is determined by computing the lowest</context>
<context position="7459" citStr="Chiang, 2005" startWordPosition="1159" endWordPosition="1160">r SynCFG rules are equivalent to a probabilistic context-free grammar and decoding is therefore an application of chart parsing. Instead of the common method of converting the CFG grammar into Chomsky Normal Form and applying a CKY algorithm to produce the most likely parse for a given source sentence, we avoided the explosion of the rule set caused by the introduction of new non-terminals in the conversion process and implemented a variant of the CKY+ algorithm as described in (J.Earley, 1970). Each cell of the parsing process in (J.Earley, 1970) contains a set of hypergraph nodes (Huang and Chiang, 2005). A hypergraph node is an equivalence class of complete hypotheses (derivations) with identical production results (left-hand sides of the corresponding applied rules). Complete hypotheses point directly to nodes in their backwards star, and the cost of the complete hypothesis is calculated with respect to each back pointer node’s best cost. This structure affords efficient parsing with minimal pruning (we use a single parameter to restrict the number of hierarchical rules applied), but sacrifices effective management of unique language model states contributing to significant search errors du</context>
<context position="9832" citStr="Chiang, 2005" startWordPosition="1532" endWordPosition="1533">s provided for this workshop. Our results represent work done before the submission due date as well as after with the following generalized phrase systems. • Baseline - Pharaoh with phrases extracted from IBM Model 4 training with maximum phrase length 7 and extraction method ‘diag-growthfinal’ (Koehn et al., 2003a) • Lex - Phrase-decoder simulation: using only the initial lexical rules from the phrase table, all with LHS X, the Glue rule, and a binary reordering rule with its own reordering-feature • XCat - All nonterminals merged into a single X nonterminal: simulation of the system Hiero (Chiang, 2005). • Syn - Syntactic extraction using the Penn Treebank parse categories as nonterminals; rules containing up to 4 nonterminal abstraction sites. • SynExt - Syntactic extraction using the extended-category scheme, but with rules only containing up to 2 nonterminal abstraction sites. We also explored the impact of longer initial phrases by training another phrase table with phrases up to length 12. Our results are presented in Table 1. While our submission time system (Syn using LM for rescoring only) shows no improvement over the baseline, we clearly see the impact of integrating the language m</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="7459" citStr="Huang and Chiang, 2005" startWordPosition="1157" endWordPosition="1160">Parsing Our SynCFG rules are equivalent to a probabilistic context-free grammar and decoding is therefore an application of chart parsing. Instead of the common method of converting the CFG grammar into Chomsky Normal Form and applying a CKY algorithm to produce the most likely parse for a given source sentence, we avoided the explosion of the rule set caused by the introduction of new non-terminals in the conversion process and implemented a variant of the CKY+ algorithm as described in (J.Earley, 1970). Each cell of the parsing process in (J.Earley, 1970) contains a set of hypergraph nodes (Huang and Chiang, 2005). A hypergraph node is an equivalence class of complete hypotheses (derivations) with identical production results (left-hand sides of the corresponding applied rules). Complete hypotheses point directly to nodes in their backwards star, and the cost of the complete hypothesis is calculated with respect to each back pointer node’s best cost. This structure affords efficient parsing with minimal pruning (we use a single parameter to restrict the number of hierarchical rules applied), but sacrifices effective management of unique language model states contributing to significant search errors du</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the 9th International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Assocation for Computing Machinery,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="7345" citStr="Earley, 1970" startWordPosition="1140" endWordPosition="1141">Frequency); unbalancedness penalty jMeanTargetSourceRatio * ‘n.o. source words’− ‘n.o. target words’� 4 Parsing Our SynCFG rules are equivalent to a probabilistic context-free grammar and decoding is therefore an application of chart parsing. Instead of the common method of converting the CFG grammar into Chomsky Normal Form and applying a CKY algorithm to produce the most likely parse for a given source sentence, we avoided the explosion of the rule set caused by the introduction of new non-terminals in the conversion process and implemented a variant of the CKY+ algorithm as described in (J.Earley, 1970). Each cell of the parsing process in (J.Earley, 1970) contains a set of hypergraph nodes (Huang and Chiang, 2005). A hypergraph node is an equivalence class of complete hypotheses (derivations) with identical production results (left-hand sides of the corresponding applied rules). Complete hypotheses point directly to nodes in their backwards star, and the cost of the complete hypothesis is calculated with respect to each back pointer node’s best cost. This structure affords efficient parsing with minimal pruning (we use a single parameter to restrict the number of hierarchical rules applied)</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>J.Earley. 1970. An efficient context-free parsing algorithm. Communications of the Assocation for Computing Machinery, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Pharaoh: A beam search decoder for phrasebase statistical machine translation models.</title>
<date>2003</date>
<booktitle>In Proceedings of the Sixth Confernence of the Association for Machine Translation in the Americas,</booktitle>
<location>Edomonton, Canada,</location>
<contexts>
<context position="1311" citStr="Koehn et al., 2003" startWordPosition="189" endWordPosition="192">c categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop’s baseline system. Our translation system is available open-source under the GNU General Public License. 1 Introduction Recent work in machine translation has evolved from the traditional word (Brown et al., 1993) and phrase based (Koehn et al., 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). These advances are motivated by the desire to integrate richer knowledge sources within the translation process with the explicit goal of producing more fluent translations in the target language. The hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder (Koehn et al., 2003a). In this work we introduce techniques to generate syntactically motivated generalized phrases and discuss issues in chart parser based decoding in</context>
<context position="3556" citStr="Koehn et al., 2003" startWordPosition="527" endWordPosition="530">d language models do in traditional decoding. In the following sections, we describe our phrase annotation and generalization process followed by the design and pruning decisions in our chart parser. We give results on the French-English Europarl data and conclude with prospects for future work. 138 Proceedings of the Workshop on Statistical Machine Translation, pages 138–141, New York City, June 2006. c�2006 Association for Computational Linguistics 2 Rule Generation X -&gt; rep We start with phrase translations on the parallel training data using the techniques and implementation described in (Koehn et al., 2003a). This phrase table provides the purely lexical entries in the final hierarchical rule set that will be used in decoding. We then use Charniak’s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus. Next, we determine all phrase pairs in the phrase table whose source and target side occur in each respective source and target sentence pair defining the scope of the initial rules in our SynCFG. Annotation If the target side of any of these initial rules correspond to a syntactic category C of the target side parse tree, we label</context>
<context position="6309" citStr="Koehn et al., 2003" startWordPosition="993" endWordPosition="996">rce sentence f, the preferred translation output is determined by computing the lowest-cost derivation (combination of hierarchical and glue rules) yielding f as its source side, where the cost of a derivation R1 o · · · o Rn with respective feature vectors v1, ... , vn E Rm is given by Here, λ1, ... , λm are the parameters of the loglinear model, which we optimize on a held-out portion of the training set (2005 development data) using minimum-error-rate training (Och, 2003). We use the following features for our rules: • source- and target-conditioned neg-log lexical weights as described in (Koehn et al., 2003b) • neg-log relative frequencies: left-handside-conditioned, target-phrase-conditioned, source-phrase-conditioned • Counters: n.o. rule applications, n.o. target words • Flags: IsPurelyLexical (i.e., contains only terminals), IsPurelyAbstract (i.e., contains only nonterminals), IsXRule (i.e., non-syntactical span), IsGlueRule N-&gt;reprise/resum repris λi �vj�i . �m i=1 n j=1 139 • Penalties: rareness penalty exp(1 − RuleFrequency); unbalancedness penalty jMeanTargetSourceRatio * ‘n.o. source words’− ‘n.o. target words’� 4 Parsing Our SynCFG rules are equivalent to a probabilistic context-free g</context>
<context position="9193" citStr="Koehn et al., 2003" startWordPosition="1426" endWordPosition="1429">es and a language model cost estimate, consisting of an accurate component, based on the words translated so far, and an estimate based on each remaining (not expanded) back pointer’s top scoring hypothesis. To improve the diversity of the final K-Best list, we keep track of partially expanded hypotheses that have generated identical target words and refer to the same hypergraph nodes. Any arising twin hypothesis is immediately removed from the K-Best extraction beam during the expansion process. 5 Results We present results that compare our system against the baseline Pharaoh implementation (Koehn et al., 2003a) and MER training scripts provided for this workshop. Our results represent work done before the submission due date as well as after with the following generalized phrase systems. • Baseline - Pharaoh with phrases extracted from IBM Model 4 training with maximum phrase length 7 and extraction method ‘diag-growthfinal’ (Koehn et al., 2003a) • Lex - Phrase-decoder simulation: using only the initial lexical rules from the phrase table, all with LHS X, the Glue rule, and a binary reordering rule with its own reordering-feature • XCat - All nonterminals merged into a single X nonterminal: simula</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003a. Pharaoh: A beam search decoder for phrasebase statistical machine translation models. In Proceedings of the Sixth Confernence of the Association for Machine Translation in the Americas, Edomonton, Canada, May 27-June 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL),</booktitle>
<location>Edomonton, Canada,</location>
<contexts>
<context position="1311" citStr="Koehn et al., 2003" startWordPosition="189" endWordPosition="192">c categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop’s baseline system. Our translation system is available open-source under the GNU General Public License. 1 Introduction Recent work in machine translation has evolved from the traditional word (Brown et al., 1993) and phrase based (Koehn et al., 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). These advances are motivated by the desire to integrate richer knowledge sources within the translation process with the explicit goal of producing more fluent translations in the target language. The hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder (Koehn et al., 2003a). In this work we introduce techniques to generate syntactically motivated generalized phrases and discuss issues in chart parser based decoding in</context>
<context position="3556" citStr="Koehn et al., 2003" startWordPosition="527" endWordPosition="530">d language models do in traditional decoding. In the following sections, we describe our phrase annotation and generalization process followed by the design and pruning decisions in our chart parser. We give results on the French-English Europarl data and conclude with prospects for future work. 138 Proceedings of the Workshop on Statistical Machine Translation, pages 138–141, New York City, June 2006. c�2006 Association for Computational Linguistics 2 Rule Generation X -&gt; rep We start with phrase translations on the parallel training data using the techniques and implementation described in (Koehn et al., 2003a). This phrase table provides the purely lexical entries in the final hierarchical rule set that will be used in decoding. We then use Charniak’s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus. Next, we determine all phrase pairs in the phrase table whose source and target side occur in each respective source and target sentence pair defining the scope of the initial rules in our SynCFG. Annotation If the target side of any of these initial rules correspond to a syntactic category C of the target side parse tree, we label</context>
<context position="6309" citStr="Koehn et al., 2003" startWordPosition="993" endWordPosition="996">rce sentence f, the preferred translation output is determined by computing the lowest-cost derivation (combination of hierarchical and glue rules) yielding f as its source side, where the cost of a derivation R1 o · · · o Rn with respective feature vectors v1, ... , vn E Rm is given by Here, λ1, ... , λm are the parameters of the loglinear model, which we optimize on a held-out portion of the training set (2005 development data) using minimum-error-rate training (Och, 2003). We use the following features for our rules: • source- and target-conditioned neg-log lexical weights as described in (Koehn et al., 2003b) • neg-log relative frequencies: left-handside-conditioned, target-phrase-conditioned, source-phrase-conditioned • Counters: n.o. rule applications, n.o. target words • Flags: IsPurelyLexical (i.e., contains only terminals), IsPurelyAbstract (i.e., contains only nonterminals), IsXRule (i.e., non-syntactical span), IsGlueRule N-&gt;reprise/resum repris λi �vj�i . �m i=1 n j=1 139 • Penalties: rareness penalty exp(1 − RuleFrequency); unbalancedness penalty jMeanTargetSourceRatio * ‘n.o. source words’− ‘n.o. target words’� 4 Parsing Our SynCFG rules are equivalent to a probabilistic context-free g</context>
<context position="9193" citStr="Koehn et al., 2003" startWordPosition="1426" endWordPosition="1429">es and a language model cost estimate, consisting of an accurate component, based on the words translated so far, and an estimate based on each remaining (not expanded) back pointer’s top scoring hypothesis. To improve the diversity of the final K-Best list, we keep track of partially expanded hypotheses that have generated identical target words and refer to the same hypergraph nodes. Any arising twin hypothesis is immediately removed from the K-Best extraction beam during the expansion process. 5 Results We present results that compare our system against the baseline Pharaoh implementation (Koehn et al., 2003a) and MER training scripts provided for this workshop. Our results represent work done before the submission due date as well as after with the following generalized phrase systems. • Baseline - Pharaoh with phrases extracted from IBM Model 4 training with maximum phrase length 7 and extraction method ‘diag-growthfinal’ (Koehn et al., 2003a) • Lex - Phrase-decoder simulation: using only the initial lexical rules from the phrase table, all with LHS X, the Glue rule, and a binary reordering rule with its own reordering-feature • XCat - All nonterminals merged into a single X nonterminal: simula</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003b. Statistical phrase-based translation. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL), Edomonton, Canada, May 27-June 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>653--660</pages>
<contexts>
<context position="1424" citStr="Melamed, 2004" startWordPosition="206" endWordPosition="207">rase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop’s baseline system. Our translation system is available open-source under the GNU General Public License. 1 Introduction Recent work in machine translation has evolved from the traditional word (Brown et al., 1993) and phrase based (Koehn et al., 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). These advances are motivated by the desire to integrate richer knowledge sources within the translation process with the explicit goal of producing more fluent translations in the target language. The hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder (Koehn et al., 2003a). In this work we introduce techniques to generate syntactically motivated generalized phrases and discuss issues in chart parser based decoding in the statistical machine translation environment. (Chiang, 2005) generates synchronous contextfree grammar (SynCF</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>I. Dan Melamed. 2004. Statistical machine translation by parsing. In ACL, pages 653–660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the Association for Computational Linguistics,</booktitle>
<pages>6--7</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="6170" citStr="Och, 2003" startWordPosition="973" endWordPosition="974">ted arc) rules for the first sentence ofEuroparl. 3 Scoring We employ a log-linear model to assign costs to the SynCFG. Given a source sentence f, the preferred translation output is determined by computing the lowest-cost derivation (combination of hierarchical and glue rules) yielding f as its source side, where the cost of a derivation R1 o · · · o Rn with respective feature vectors v1, ... , vn E Rm is given by Here, λ1, ... , λm are the parameters of the loglinear model, which we optimize on a held-out portion of the training set (2005 development data) using minimum-error-rate training (Och, 2003). We use the following features for our rules: • source- and target-conditioned neg-log lexical weights as described in (Koehn et al., 2003b) • neg-log relative frequencies: left-handside-conditioned, target-phrase-conditioned, source-phrase-conditioned • Counters: n.o. rule applications, n.o. target words • Flags: IsPurelyLexical (i.e., contains only terminals), IsPurelyAbstract (i.e., contains only nonterminals), IsXRule (i.e., non-syntactical span), IsGlueRule N-&gt;reprise/resum repris λi �vj�i . �m i=1 n j=1 139 • Penalties: rareness penalty exp(1 − RuleFrequency); unbalancedness penalty jMe</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the Association for Computational Linguistics, Sapporo, Japan, July 6-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Ying Zhang</author>
<author>Fei Huang</author>
<author>Alicia Tribble</author>
<author>Ashish Venogupal</author>
<author>Bing Zhao</author>
<author>Alex Waibel</author>
</authors>
<title>The CMU statistical translation system.</title>
<date>2003</date>
<booktitle>In Proceedings ofMT Summit IX,</booktitle>
<location>New Orleans, LA,</location>
<contexts>
<context position="11794" citStr="Vogel et al., 2003" startWordPosition="1873" endWordPosition="1876">. phr. length 12 – – – 31.07 31.76 Table 1: Translation results (IBM BLEU) for each system on the Fr-En ’06 Shared Task ‘Development Set’ (used for MER parameter tuning) and ’06 ‘Development Test Set’ (identical to last year’s Shared Task’s test set). The system submitted for evaluation is highlighted in bold. employ richer extraction techniques. The relatively poor performance of Lex with LM in K-Best compared to the baseline shows that we are still making search errors during parsing despite tighter integration of the language model. We also ran an experiment with CMU’s phrasebased decoder (Vogel et al., 2003) using the length7 phrase table. While its development-set score was only 31.01, the decoder achieved 31.42 on the test set, placing it at the same level as our extendedcategory system for that phrase table. 6 Conclusions In this work we applied syntax based resources (the target language parser) to annotate and generalize phrase translation tables extracted via existing phrase extraction techniques. Our work reaffirms the feasibility of parsing approaches to machine translation in a large data setting, and illustrates the impact of adding syntactic categories to drive and constrain the struct</context>
</contexts>
<marker>Vogel, Zhang, Huang, Tribble, Venogupal, Zhao, Waibel, 2003</marker>
<rawString>Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble, Ashish Venogupal, Bing Zhao, and Alex Waibel. 2003. The CMU statistical translation system. In Proceedings ofMT Summit IX, New Orleans, LA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A decoder for syntax-based statistical mt.</title>
<date>2002</date>
<booktitle>In Proc. of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2573" citStr="Yamada and Knight, 2002" startWordPosition="374" endWordPosition="377">nvironment. (Chiang, 2005) generates synchronous contextfree grammar (SynCFG) rules from an existing phrase translation table. These rules can be viewed as phrase pairs with mixed lexical and non-terminal entries, where non-terminal entries (occurring as pairs in the source and target side) represent placeholders for inserting additional phrases pairs (which again may contain nonterminals) at decoding time. While (Chiang, 2005) uses only two nonterminal symbols in his grammar, we introduce multiple syntactic categories, taking advantage of a target language parser for this information. While (Yamada and Knight, 2002) represent syntactical information in the decoding process through a series of transformation operations, we operate directly at the phrase level. In addition to the benefits that come from a more structured hierarchical rule set, we believe that these restrictions serve as a syntax driven language model that can guide the decoding process, as n-gram context based language models do in traditional decoding. In the following sections, we describe our phrase annotation and generalization process followed by the design and pruning decisions in our chart parser. We give results on the French-Engli</context>
</contexts>
<marker>Yamada, Knight, 2002</marker>
<rawString>Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical mt. In Proc. of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>