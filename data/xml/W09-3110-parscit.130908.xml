<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000201">
<title confidence="0.967672">
Compilation of Specialized Comparable Corpora in French and Japanese
</title>
<author confidence="0.828918">
Lorraine Goeuriot, Emmanuel Morin and Béatrice Daille
</author>
<note confidence="0.4599305">
LINA - Université de Nantes
France
</note>
<email confidence="0.626278">
firstname.lastname@univ-nantes.fr
</email>
<sectionHeader confidence="0.982335" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999879">
We present in this paper the development
of a specialized comparable corpora com-
pilation tool, for which quality would be
close to a manually compiled corpus. The
comparability is based on three levels: do-
main, topic and type of discourse. Domain
and topic can be filtered with the keywords
used through web search. But the detec-
tion of the type of discourse needs a wide
linguistic analysis. The first step of our
work is to automate the detection of the
type of discourse that can be found in a
scientific domain (science and popular sci-
ence) in French and Japanese languages.
First, a contrastive stylistic analysis of the
two types of discourse is done on both lan-
guages. This analysis leads to the creation
of a reusable, generic and robust typology.
Machine learning algorithms are then ap-
plied to the typology, using shallow pars-
ing. We obtain good results, with an av-
erage precision of 80% and an average re-
call of 70% that demonstrate the efficiency
of this typology. This classification tool
is then inserted in a corpus compilation
tool which is a text collection treatment
chain realized through IBM UTMA system.
Starting from two specialized web docu-
ments collection in French and Japanese,
this tool creates the corresponding corpus.
</bodyText>
<sectionHeader confidence="0.999334" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894238095238">
Comparable corpora are sets of texts in differ-
ent languages, that are not translations, but share
some characteristics (Bowker and Pearson, 2002).
They represent useful resources from which are
extracted multilingual terminologies (Déjean et
al., 2002) or multilingual lexicons (Fung and Yee,
1998). Comparable corpora are also used in
contrastive multilingual studies framework (Peters
and Picchi, 1997), they constitute a precious re-
source for translators (Laviosa, 1998) and teachers
(Zanettin, 1998), as they provide a way to observe
languages in use.
Their compilation is easier than parallel corpora
compilation, because translated resources are rare
and there is a lack of resources when the languages
involved do not include English. Furthermore, the
amount of multilingual documents available on the
Web ensures the possibility of automatically com-
piling them. Nevertheless, this task can not be
summarized to a simple collection of documents
sharing vocabulary. It is necessary to respect the
common characteristics of texts in corpora, es-
tablished before the compilation, according to the
corpus finality (McEnery and Xiao, 2007). Many
works are about compilation of corpora from the
Web (Baroni and Kilgarriff, 2006) but none, in our
knowledge, focuses on compilation of compara-
ble corpora, which has to satisfy many constraints.
We fix three comparability levels: domain, topic
and type of discourse. Our goal is to automate
recognition of these comparability levels in docu-
ments, in order to include them into a corpus. We
work on Web documents on specialized scientific
domains in French and Japanese languages. As
document topics can be filtered with keywords in
the Web search (Chakrabarti et al., 1999), we fo-
cus in this paper on automatic recognition of types
of discourse that can be found in scientific docu-
ments: science and popular science. This classi-
fication tool is then inserted in a specialized com-
parable corpora compilation tool, which is devel-
opped through the Unstructured Information Man-
</bodyText>
<page confidence="0.9871">
55
</page>
<note confidence="0.998134">
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 55–63,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999255888888889">
agement Architecture (UIMA) (Ferrucci and Lally,
2004).
This paper is structured as follows. After an in-
troduction of related works in section 2, stylistic
analysis of our corpus will be presented in sec-
tion 3. This analysis will lead to the creation of
a typology of scientific and popular science dis-
course type in specifialized domains. The appli-
cation of learning algorithms to the typology will
be described in section 4, and the results will be
presented in section 5. We will show that our ty-
pology, based on linguistically motivated features,
can characterize science and popular science dis-
courses in French and Japanese documents, and
that the use of our three comparablility levels can
improve corpora comparability. Finally, we de-
scribe the development of the corpus compilation
tool.
</bodyText>
<sectionHeader confidence="0.987677" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999771933333333">
“A comparable corpus can be defined as a corpus
containing components that are collected using
the same sampling frame and similar balance and
representativeness” (McEnery and Xiao, 2007, p.
20). Comparability is ensured using character-
istics which can refer to the text creation con-
text (period, author...), or to the text itself (topic,
genre...). The choice of the common characteris-
tics, which define the content of corpora, affects
the degree of comparability, notion used to quan-
tify how two corpora can be comparable. The
choice of these characteristics depends on the fi-
nality of the corpus. Among papers on comparable
corpora, we distinguish two types of works, which
induces different choices:
</bodyText>
<listItem confidence="0.9967362">
• General language works, where texts of cor-
pora usually share a domain and a period.
Fung and Yee (1998) used a corpus composed
of newspaper in English and Chinese on a
specific period to extract words translations,
using IR and NLP methods. Rapp (1999)
used a English / German corpus, composed of
documents coming from newspapers as well
as scientific papers to study alignment meth-
ods and bilingual lexicon extraction from
non-parallel corpora (which can be consid-
ered as comparable);
• Specialized language works, where choice of
criteria is various. Déjean et al. (2002) used a
corpus composed of scientific abstracts from
</listItem>
<bodyText confidence="0.997751882352941">
Medline, a medical portal, in English and
German. Thus they used documents sharing a
domain and a genre to extract bilingual termi-
nology. Chiao (2002) used a corpus of docu-
ments of medical domain on a specific topic
to work on the extraction of specialized ter-
minologies.
In general language works, documents of compa-
rable corpora often share characteristics like do-
main or topic. As they are usually extracted from
newspapers, it is important to limit them to a cer-
tain period to guarantee their comparability.
In specialized corpora, first levels of compara-
bility can be achieved with the domain and the
topic. Moreover, several communicative settings
appear in specialized language (Bowker and Pear-
son, 2002): expert-expert, expert-initiate, relative
expert to the uninitiated, teacher-pupil. Malrieu
and Rastier (2002) specify several levels of tex-
tual classification, each of which corresponding to
a certain granularity. The first level is discourse,
defined as a set of utterances from a enunciator
characterized by a global topical unit (Ducrot and
Todorov, 1972). The second level is genre, de-
fined as text categories distinguished by matured
speakers. For example, to literary discourse corre-
spond several genres: drama, poetry, prose... In-
spired by these communicative settings and tex-
tual categories, we choose to distinguish two com-
municative settings or type of discourse in spe-
cialized domains: science (texts written by ex-
perts to experts) and popular science (texts written
to non-experts, by experts, semi-experts or non-
experts). This comparability level, the type of dis-
course, reflects the context of production or usage
of the documents, and guarantees a lexical homo-
geneity in corpora (Bowker and Pearson, 2002, p.
27). Furthermore, Morin et al. (2007) proved that
comparable corpora sharing a topic and a type of
discourse are well adapted for multilingual termi-
nologies extraction.
Our goal is to create a tool to compile compa-
rable corpora in French and Japanese which docu-
ments are extracted from the Web. We investigate
automatic categorization of documents according
to their type of discourse. This categorization is
based on a typology of elements characterizing
these types of discourse. To this end, we carry
out a stylistic and contrastive analysis (Karlgren,
1998). This analysis aims to highlight linguis-
tically motivated features through several dimen-
</bodyText>
<page confidence="0.994538">
56
</page>
<bodyText confidence="0.99866975">
sions (structural, modal and lexical), whose com-
bination characterizes scientific or popular science
discourse. A specialized comparable corpus can
be compiled from a single type of discourse docu-
ment collection through several steps. Last part of
this paper focuses on the automation of these steps
using the IBM Unstructured Information Manage-
ment Architecture (UIMA).
</bodyText>
<subsectionHeader confidence="0.652439">
3 Analysis of Types of Discourse
</subsectionHeader>
<bodyText confidence="0.999970444444444">
The recognition of types of discourse is based
on a stylistic analysis adapted from a deductive
and contrastive method, which purpose is to raise
discriminant and linguistically motivated features
characterizing these two types of discourse. Main
difficulty here is to find relevant features which fit
every language involved. These features, gathered
in a typology, will be used to adapt machine learn-
ing algorithms to compilation of corpora. This
typology thus needs to be robust, generic and
reusable in other languages and domains. Gener-
icity is ensured by a broad typology composed of
features covering a wide range of documents char-
acteristics, while robustness is guaranteed with
operational (computable) features and treatment
adaptable to Web documents as well as texts.
Sinclair (1996) distinguishes two levels of anal-
ysis in his report on text typologies: external level,
characterizing the context of creation of the docu-
ment; and internal level, corresponding to linguis-
tic characteristics of document. Because our cor-
pora are composed of documents extracted from
the Web, we consider external level features as
all the features related to the creation of docu-
ments and their structure (non-linguistic features)
and call them structural features. Stylistic analy-
sis raises several granularity levels among linguis-
tic characteristics of the texts. We thus distinguish
two levels in the internal dimension. Firstly, in
order to distinguish between scientific and pop-
ular science documents, we need to consider the
speaker in his speech: the modality. Secondly, sci-
entific discourse can be characterized by vocabu-
lary, word length and other lexical features. There-
fore our typology is based on three analysis levels:
structural, modal and lexical.
</bodyText>
<subsectionHeader confidence="0.998309">
3.1 Structural Dimension
</subsectionHeader>
<bodyText confidence="0.999698666666667">
When documents are extracted from the Web, the
structure and the context of creation of the doc-
uments should be considered. In the framework
</bodyText>
<table confidence="0.999641785714286">
Feature French Japanese
URL pattern x
Document’s format x x
Meta tags x x
Title tag x x
Pages layout x x
Pages background x x
Images x x
Links x x
Paragraphs x x
Item lists x x
Number of sentences x x
Typography x x
Document’s length x x
</table>
<tableCaption confidence="0.999508">
Table 1: Structural dimension features
</tableCaption>
<bodyText confidence="0.99993775">
of Web documents classification, several elements
bring useful information: pictures, videos and
other multimedia contents (Asirvatham and Ravi,
2001); meta-information, title and HTML struc-
ture (Riboni, 2002). While those information are
not often used in comparable corpora, they can be
used to classify them. Table 1 shows structural
features.
</bodyText>
<subsectionHeader confidence="0.999353">
3.2 Modal Dimension
</subsectionHeader>
<bodyText confidence="0.99900024">
The degree of specialization required by the recip-
ient or reader is characterized by the relation built
in the utterance between the speaker or author and
the recipient or reader1. The tone and linguistic
elements in texts define this relation. The modal-
isation is an interpretation of the author’s attitude
toward the content of his/her assertion. Modali-
sation is characterized by many textual markers:
verbs, adverbs, politeness forms, etc. Presence of
the speaker and his position towards his speech
are quite different in scientific and popular science
discourse. Thus we think modalisation markers
can be relevant. For example, the speaker directly
speaks to the reader in some popular science doc-
uments: “By eating well, you’ll also help to pre-
vent diabetes problems that can occur later in life,
like heart disease”. Whereas a scientific document
would have a neutral tone: “Obesity plays a cen-
tral role in the insulin resistance syndrome, which
includes hyperinsulinemia, [... ] and an increased
risk of atherosclerotic cardiovascular disease”.
Most of the modal theories are language de-
pendent, and use description phenomena that are
specific to each language. Conversely, the theory
exposed in (Charaudeau, 1992) is rather indepen-
</bodyText>
<footnote confidence="0.983857">
1Since we work on a scientific domain, we will consider
the speaker as the author of texts, and the recipient as the
reader.
</footnote>
<page confidence="0.99859">
57
</page>
<bodyText confidence="0.99990895">
dent of the language and operational for French
and Japanese (Ishimaru, 2006). According to Cha-
raudeau (1992, p.572), modalisation clarifies the
position of the speaker with respect to his reader,
to himself and to his speech. Modalisation is com-
posed of locutive acts, particular positions of the
author in his speech, and each locutive act is char-
acterized by modalities. We kept in his theory two
locutive acts involving the author:
Allocutive act: the author gets the reader in-
volved in the speech (ex.: “You have to do
this.”);
Elocutive act: the author is involved in his own
speech, he reveals his position regarding his
speech (ex.: “I would like to do this.”).
Each of these acts are then divided into several
modalities. These modalities are presented in ta-
ble 2 with English examples. Some of the modali-
ties are not used in a language or another, because
they are not frequent or too ambiguous.
</bodyText>
<subsectionHeader confidence="0.998673">
3.3 Lexical Dimension
</subsectionHeader>
<bodyText confidence="0.999964235294118">
Biber (1988) uses lexical information to observe
variations between texts, especially between gen-
res and types of texts. Karlgren (1998) also use
lexical information to characterize text genres, and
use them to observe stylistic variations among
texts. Thus, we assume that lexical information
is relevant in the distinction between science and
popular science discourse. Firstly, because a spe-
cialized vocabulary is a principal characteristic of
specialized domain texts (Bowker and Pearson,
2002, p. 26). Secondly, because scientific docu-
ments contain more complex lexical units, nomi-
nal compounds or nominal sentences than popular
science documents (Sager, 1990).
Table 3 presents the lexical dimension features.
Note that these features show a higher language
dependency than other dimension features.
</bodyText>
<sectionHeader confidence="0.997026" genericHeader="method">
4 Automatic Classification by Type of
Discourse
</sectionHeader>
<bodyText confidence="0.99749875">
The process of documents classification can be di-
vided into three steps: document indexing, classi-
fier learning and classifier evaluation (Sebastiani,
2002). Document indexing consists in building
a compact representation of documents that can
be interpreted by a classifier. In our case, each
document di is represented as a vector of fea-
tures weight: di* = {w1i, ... , wni} where n is the
</bodyText>
<table confidence="0.999781230769231">
Feature French Japanese
Specialized vocabulary x x
Numerals x x
Units of measurement x x
Words length x
Bibliography x x
Bibliographic quotes x x
Punctuation x x
Sentences end x
Brackets x x
Other alphabets (latin, x
hiragana, katakana)
Symbols x
</table>
<tableCaption confidence="0.761115">
Table 3: Lexical dimension features
</tableCaption>
<table confidence="0.9998505">
Dimension Method
Structural Pattern matching
Modal Lexical and lexico-syntactic patterns
Lexical Lexical patterns
</table>
<tableCaption confidence="0.999434">
Table 4: Markers detection methods
</tableCaption>
<bodyText confidence="0.999706166666667">
number of features of the typology and wig is the
weight of the jth feature in the ith document. Each
feature weight is normalized, dividing the weight
by the total. Documents indexing is characterized
by our typology (section 3) and features imple-
mentation.
</bodyText>
<subsectionHeader confidence="0.845468">
4.1 Features Implementation
</subsectionHeader>
<bodyText confidence="0.9999585">
In order to get a fast classification system, we priv-
ileged for the implementation of our typology fea-
tures shallow parsing such as lexical markers and
lexico-syntactic patterns (method for each dimen-
sion is detailed in table 4).
Structural Features We used 12 structural fea-
tures introduced in section 3.1. Most of these fea-
tures are achieved through pattern matching. For
example, URL patterns can determine is the docu-
ment belongs to websites such as hospital (http:
//www.chu-***.fr) or universities websites
(http://www.univ-***.fr), etc. As for
paragraphs, images, links, etc., one simple search
of HTML tags was made.
Modal Features Locutor presence markers in
a text can be implicit or ambiguous. We fo-
cused here on simple markers of his presence in
order to avoid noise in our results (high preci-
sion but weak recall). Thus we don’t recognize
all modal markers in a text but those recognized
are correct. There are pronouns which are spe-
cific to the speech act: for instance, for the eloc-
utive act, the French pronouns je (I) and nous
(we), and the Japanese pronouns t (I), &amp;quot; (we)
</bodyText>
<page confidence="0.994987">
58
</page>
<table confidence="0.999970142857143">
Feature Example French Japanese
Allocutive modality
Allocutive personal pronouns You x x
Injunction modality Don’t do this x x
Authorization modality You can do this x x
Judgement modality Congratulations for doing it! x x
Suggestion modality You should do this x
Interrogation modality When do you arrive? x
Interjection modality How are you, Sir? x
Request modality Please, do this x
Elocutive modality
Elocutive personal I, we x x
Noticing modality We notice that he left x x
Knowledge modality I know that he left x x
Opinion modality I think he left x x
Will modality I would like him to leave x x
Promise modality I promise to be here x x
Declaration modality I affirm he left x x
Appreciation modality I like this x
Commitment modality We have to do this x
Possibility modality I can inform them
</table>
<tableCaption confidence="0.990976">
Table 2: Modal dimension features
</tableCaption>
<bodyText confidence="0.99962453125">
and ft,(r (we). The modalities are also com-
puted with lexical markers. For example, the
modality of knowledge can be detected in French
with verbs like savoir, connaître (know), and in
Japanese with the verb VK (know), with po-
lite form and with neutral form
�#&amp;�K.
Lexical Features Some of our lexical criteria
are specific to the scientific documents, like bib-
liographies and bibliographic quotations, special-
ized vocabulary or the measurement units. To
measure the terminological density (proportion of
specialized vocabulary in the text) in French, we
evaluate terms with stems of Greek-Latin (Namer
and Baud, 2007) and suffix characters of rela-
tional adjectives that are particularly frequent in
scientific domains (Daille, 2000). We listed about
50 stems such as inter-, auto- or nano-, and the
10 relational suffixes such such as -ique or -al.
For Japanese, we listed prefix characteristics of
names of disease or symptoms (Yc_W&apos;K (congen-
ital), jgIY�&apos;K(hereditary), etc.). These stems can
be found in both type of discourse, but not in the
same proportions. Specialized terms are used in
both type of discourse in different ways. For ex-
ample, the term “ovarectomie” (ovarectomy) can
be frequent in a scientific document and used once
in a popular science documents to explain it and
then replaced by “ablation des ovaires” (ovary ab-
lation). Sentences end are specific ending particles
used in japanese, for example the particle l7&apos; is of-
ten used at the end of an interrogative sentence.
</bodyText>
<subsectionHeader confidence="0.993676">
4.2 Learning Algorithms
</subsectionHeader>
<bodyText confidence="0.999891111111111">
Classifier learning is a process which observes fea-
tures weight of documents classified in a class
c or c and determine characteristics that a new
document should have to be classified in one of
these two classes 2. Given a document indexing,
there are some well-known algorithms that can
achieve this process (neural network, Bayes clas-
sifiers, SVM, etc.) of which Sebastiani (2002) car-
ried out a research about the assemblage and com-
parison. Applied to a Reuters newswires corpus,
these techniques showed variable performances in
the usage level of supervised or unsupervised ap-
proaches, of the size of the corpus, of the number
of categories, etc. We decided to use SVMlight
(Joachims, 2002) and C4.5 (Quinlan, 1993), since
both of them seem to be the most appropriate to
our data (small corpora, binary classification, less
than 100 features).
</bodyText>
<sectionHeader confidence="0.999463" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999908222222222">
In this section, we describe the two comparable
corpora used and present the two experiments car-
ried out with each of them. The first compara-
ble corpus is used to train the classifier in order
to learn a classification model based on our typol-
ogy (i.e. training task). The second comparable
corpus is used to evaluate the impact of the clas-
sification model when applied on new documents
(i.e. evaluation task).
</bodyText>
<footnote confidence="0.994313">
2This is the binary case. See (Sebastiani, 2002) for other
cases.
</footnote>
<page confidence="0.998703">
59
</page>
<subsectionHeader confidence="0.97314">
5.1 Comparable Corpora
</subsectionHeader>
<bodyText confidence="0.999395545454545">
The corpora used in our experiments are both
composed of French and Japanese documents har-
vested from the Web. The documents were taken
from the medical domain, within the topic of di-
abetes and nutrition for training task, and breast
cancer for the evaluation task. Document harvest-
ing was carried out with a domain-based search
and a manual selection. Documents topic is fil-
tered using keywords reflecting the specialized
domain: for example alimentation, diabète and
obésité 3 for French part and WrA and ��% 4
for the Japanese part of the training task corpus.
Those keywords are directly related to the topic or
they can be synonyms (found on thesaurus) or se-
mantically linked terms (found in Web documents
collected). Then the documents were manually se-
lected by native speakers of each language who are
not domain specialists, and classified with respect
to their type of discourse: science (SC) or pop-
ular science (PS). Manual classification is based
on the following heuristics, to decide their type of
discourse:
</bodyText>
<listItem confidence="0.971933666666667">
• A scientific document is written by special-
ists to specialists.
• We distinguish two levels of popular science:
</listItem>
<bodyText confidence="0.988949555555556">
texts written by specialists for the general
public and texts written by the general pub-
lic for the general public. Without distinction
of these last two levels, we privileged doc-
uments written by specialists, assuming that
they may be richer in content and vocabulary
(for example advices from a doctor would be
richer and longer than forum discussions).
Our manual classification is based on the two
previous heuristics, and endorsed by several em-
pirical elements: website’s origin, vocabulary
used, etc. The classification of ambiguous docu-
ments has been validated by linguists. A few doc-
uments for which it was difficult to decide on the
type of discourse, such as those written by peo-
ple whose specialist status was not clear, were not
retained.
We thus created two comparable corpora:
</bodyText>
<listItem confidence="0.851378428571429">
• [DIAB_CP] related to the topic of diabetes
and nutrition and used to train the classifier.
3nutrition, diabetes, and obesity
4diabetes and overweight
• [BC_CP] related to the topic of breast cancer
and used to evaluate the effectiveness of the
classifier.
</listItem>
<bodyText confidence="0.9989515">
Table 5 shows the main features of each compa-
rable corpora: the number of documents, and the
number of words5 for each language and each type
of discourse.
</bodyText>
<table confidence="0.999930444444444">
# docs # words
FR SC 65 425,781
[DIAB_CP] PS 183 267,885
JP SC 119 234,857
PS 419 572,430
FR SC 50 443,741
[BC_CP] PS 42 71,980
JP SC 48 211,122
PS 51 123,277
</table>
<tableCaption confidence="0.99918">
Table 5: Basic data on each comparable corpora
</tableCaption>
<subsectionHeader confidence="0.590741">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.849012821428572">
We present in this section two classification tasks:
• the first one consists in training and test-
ing classifiers with [DIAB_CP], using N-fold
cross validation method that consists in divid-
ing the corpus into n sub-samples of the same
size (we fix N = 5). Results are for 5 parti-
tioning on average;
• the second one consists in testing on [BC_CP]
the best classifier learned on [DIAB_CP], in
order to evaluate its impact on new docu-
ments.
Tables 6 and 7 show results of these two tasks.
On both table we present precision and recall
metrics with the two learning systems used. On
table 6, we can see that the results concerning
the French documents are quite satisfactory alto-
gether, with a recall on average of 87%, and a pre-
cision on average of 90% as for the classifier C4.5
(more than 215 documents are well classified from
248 French documents of [DIAB_CP]). The re-
sults of the classification in Japanese are also good
with the classifier C.4.5. More than 90% of doc-
uments are correctly classified, and the precision
reaches on average 80%. Some of the lower results
can be explained, especially in Japanese by the
high range of document genres in the corpus (re-
search papers, newspapers, scientific magazines,
recipes, job offers, forum discussions... ).
</bodyText>
<footnote confidence="0.9994865">
5For Japanese, the number of words is the number of oc-
currences recognized by ChaSen (Matsumoto et al., 1999)
</footnote>
<page confidence="0.99308">
60
</page>
<table confidence="0.999242875">
French Rec. Japanese Rec.
Prec. Prec.
SC 1.00 0.36 0.70 0.65
lvms 0.80 1,00 0.72 0.80
PS
45c. SC 0.89 0.80 0.76 0.99
PS 0.91 0.94 0.95
0.96
</table>
<tableCaption confidence="0.993317">
Table 6: Precision and recall for each language,
each classifier, on [DIAB_CP]
</tableCaption>
<bodyText confidence="0.9571894">
Table 7 shows results on [BC_CP]. In general,
we note a decrease of the results with [BC_CP],
although results are still satisfactory. French doc-
uments are well classified whatever the classifier
is, with a precision higher than 75% and a recall
higher than 75%, which represent more than 70
well classified documents on 92. Japanese docu-
ments are well classified too, with 76% precision
and 77% recall on average, with 23 documents
wrong classified on 99. This classification model
is effective when it is applied to a different medi-
cal topic. This classification model seems efficient
to recognize scientific discourse from popular sci-
ence one in French and Japanese documents on a
particular topic.
</bodyText>
<table confidence="0.999373875">
French Rec. Japanese Rec.
Prec. Prec.
SC 0.92 0.53 0.90 0.61
lvms 0.64 0.95 0.66 0.98
PS
SC 0.70 0.92 0.76 0.70
45c. 0.87 0.56 0.75 0.80
PS
</table>
<tableCaption confidence="0.98154">
Table 7: Precision and recall for each language,
each classifier, on [BC_CP]
</tableCaption>
<sectionHeader confidence="0.9126515" genericHeader="method">
6 Comparable Corpora Compilation
Tool
</sectionHeader>
<bodyText confidence="0.9657875">
Compilation of a corpus, whatever type it is, is
composed of several steps.
</bodyText>
<listItem confidence="0.749144">
1. Corpus Specifications: they must be defined
</listItem>
<bodyText confidence="0.9254793">
by the creator or user of the corpus. It in-
cludes decisions on its type, languages in-
volved, resources from which are extracted
documents, its size, etc. In the case of spe-
cialized comparable corpora, specifications
concern languages involved, size, resources
and documents domain, theme and type of
discourse. This step depends on the applica-
tive goals of the corpus and has to be done
carefully.
</bodyText>
<listItem confidence="0.9663222">
2. Documents Selection and Collection:
according to the resource, size and other
corpus criteria chosen during the first step,
documents are collected.
3. Documents Normalization and Annotation:
cleaning and linguistic treatments are applied
to documents in order to convert them into
raw texts and annotated texts.
4. Corpus Documentation: compilation of a
corpus that can be used in a durable way
</listItem>
<bodyText confidence="0.999418620689655">
must include this step. Documentation
of the corpus includes information about
the compilation (creator, date, method,
resources, etc.) and information about the
corpus documents. Text Encoding Initiative
(TEI) standard has been created in order to
conserve in an uniformed way this kind of
information in a corpus 6.
A corpus quality highly depends on the first two
steps. Moreover, these steps are directly linked to
the creator use of the corpus. The first step must
be realized by the user to create an relevant corpus.
Although second step can be computerizable (Ro-
gelio Nazar and Cabré, 2008), we choose to keep
it manual in order to guarantee corpus quality. We
decided to work on a system which realizes the
last steps, i.e. normalization, annotation and docu-
mentation, starting from a collection of documents
selected by a user.
Our tool has been developed on Unstructured
Information Management Architecture (UIMA)
that has been created by IBM Research Divi-
sion (Ferrucci and Lally, 2004). Unstructured
data (texts, images, etc.) collections can be eas-
ily treated on this platform and many libraries are
available. Our tool starts with a web documents or
texts collection and is composed of several com-
ponents realizing each part of the creation of the
corpus:
</bodyText>
<listItem confidence="0.835120166666667">
1. the collection is loaded and documents are
converted to texts (with conversion tools
from pdf or html to text mainly);
2. all texts are cleaned and normalized (noise
from the conversion is cleaned, all texts are
converted into the same encoding, etc.);
</listItem>
<footnote confidence="0.963362">
6http://www.tei-c.org/index.xml
</footnote>
<page confidence="0.998315">
61
</page>
<bodyText confidence="0.945598346153846">
3. a pre-syntactic treatment is applied on texts
(segmentation mainly) to prepare them for
the following step;
4. morphologic and morpho-syntactic tagging
tools are applied on the texts (Brill tagger
(Brill, 1994) and Flemm lemmer (Namer,
2000) for French texts, Chasen (Matsumoto
et al., 1999) for Japanese);
5. texts are classified according to their type
of discourse: we use here the most efficient
SVMlight classifier. In fact, two corpus are
created, on for each type of discourse, then
the user can choose one of them. A vecto-
rial representation of each document is com-
puted, then these vectors are classified with
the classifier selected.
6. documentation is produced for the corpus, a
certain amount of information are included
and they can be easily completed by the user.
In reality, this tool is more a compilation assis-
tant than a compilator. It facilitates the compila-
tion task: the user is in charge of the most im-
portant part of the compilation, but the technical
part (treatment of each document) is realized by
the system. This guarantee a high quality in the
corpus.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99998597368421">
This article has described a first attempt of com-
piling smart comparable corpora. The quality is
close to a manually collected corpus, and the high
degree of comparability is guaranteed by a com-
mon domain and topic, but also by a same type of
discourse. In order to detect automatically some of
the comparability levels, we carried out a stylistic
and contrastive analysis and elaborated a typology
for the characterization of scientific and popular
science types of discourse on the Web. This typol-
ogy is based on three aspects of Web documents:
the structural aspect, the modal aspect and lexi-
cal aspect. From the modality part, this distinction
is operational even on linguistically distant lan-
guages, as we proved by the validation on French
and Japanese. Our typology, implemented using
SVMlight and C4.5 learning algorithms brought
satisfactory results of classification, not only on
the training corpus but also on an evaluation cor-
pus, since we obtained a precision on average of
80% and a recall of 70%. This classifier has then
been included into a tool to assist specialized com-
parable corpora compilation. Starting from a Web
documents collection selected by the user, this
tool realizes cleaning, normalization and linguis-
tic treatment of each document and “physically”
creates the corpus.
This tool is a first attempt and can be improved.
In a first time, we would like to assist the selection
and collection of documents, which could be real-
ized through the tool. Moreover, we would like to
investigate needs of comparable corpora users in
order to adapt our tool. Finally, others languages
could be added to the system, which represents a
quite time-consuming task: a classifier would have
to be created so all the linguistic analysis and clas-
sification tasks would have to be done again for
other languages.
</bodyText>
<sectionHeader confidence="0.978706" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999483">
This research program has been funded by the
French National Research Agency (ANR) through
the C-mantic project (ANR-07-MDCO-002-01)
2008-2010. We thank Yukie Nakao for the
japanese corpus and linguistic resources.
</bodyText>
<sectionHeader confidence="0.998086" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998958086956522">
Arul Prakash Asirvatham and Kranthi Kumar Ravi.
2001. Web page classification based on document
structure. IEEE National Convention.
Marco Baroni and Adam Kilgarriff. 2006. Large
linguistically-processed web corpora for multiple
languages. In EACL’06, pages 87–90. The Associa-
tion for Computer Linguistics.
Douglas Biber. 1988. Variation across Speech and
Writing. Cambridge University Press.
Lynne Bowker and Jennifer Pearson. 2002. Working
with Specialized Language: A Practical Guide to
Using Corpora. London/New York, Routeledge.
Eric Brill. 1994. Some advances in transformation-
based part of speech tagging. In Proceedings of the
12th National Conference on Arti�cial Intelligence
(AAAI’94), pages 722–727, Seattle, WA, USA.
Soumen Chakrabarti, Martin van den Berg, and Byron
Dom. 1999. Focused crawling: a new approach
to topic-specific Web resource discovery. Computer
Networks (Amsterdam, Netherlands: 1999), 31(11–
16):1623–1640.
Patrick Charaudeau. 1992. Grammaire du sens et de
l’expression. Hachette.
</reference>
<page confidence="0.980872">
62
</page>
<reference confidence="0.9994146875">
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In COLING’02,
pages 1208–1212, Tapei, Taiwan.
Béatrice Daille. 2000. Morphological rule induction
for terminology acquisition. In COLING’00, pages
215–221, Sarrbrucken, Germany.
Hervé Déjean, Éric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In COLING’02.
Oswald Ducrot and Tzvetan Todorov. 1972. Diction-
naire encyclopédique des sciences du langage. Édi-
tions du Seuil.
David Ferrucci and Adam Lally. 2004. Uima: An
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10:327–348.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, com-
parable texts. In Christian Boitet and Pete White-
lock, editors, COLING’98, volume 1, pages 414–
420, Montreal, Quebec, Canada.
Kumiko Ishimaru. 2006. Comparative study on the
discourse of advertisement in France and Japan:
beauty products. Ph.D. thesis, Osaka University,
Japan.
Thorsten Joachims. 2002. Learning to Classify Text
using Support Vector Machines. Kluwer Academic
Publishers.
Jussi Karlgren, 1998. Natural Language Information
Retrieval, chapter Stylistic Experiments in Informa-
tion Retrieval. Tomek, Kluwer.
Sarah Laviosa. 1998. Corpus-based approaches to
contrastive linguistics and translation studies. Meta,
43(4):474–479.
Denise Malrieu and Francois Rastier. 2002. Genres et
variations morphosyntaxiques. Traitement Automa-
tique des Langues (TAL), 42(2):548–577.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
and Yoshitaka Hirano. 1999. Japanese Morpho-
logical Analysis System ChaSen 2.0 Users Manual.
Technical report, Nara Institute of Science and Tech-
nology (NAIST).
Anthony McEnery and Zhonghua Xiao. 2007. Par-
allel and comparable corpora: What is happening?
In Gunilla Anderman and Margaret Rogers, editors,
Incorporating Corpora: The Linguist and the Trans-
lator. Clevedon: Multilingual Matters.
Fiammetta Namer and Robert Baud. 2007. Defin-
ing and relating biomedical terms: Towards a cross-
language morphosemantics-based system. Interna-
tional Journal of Medical Informatics, 76(2-3):226–
233.
Fiametta Namer. 2000. Flemm : Un analyseur flexion-
nel du français à base de règles. Traitement Automa-
tique des Langues (TAL), 41(2):523–548.
Carol Peters and Eugenio Picchi. 1997. Using lin-
guistic tools and resources in cross-language re-
trieval. In David Hull and Doug Oard, editors,
Cross-Language Text and Speech Retrieval. Papers
from the 1997 AAAI Spring Symposium, Technical
Report SS-97-05, pages 179–188.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Fran-
cisco, CA, USA.
Reinhard Rapp. 1999. Automatic Identification of
Word Translations from Unrelated English and Ger-
man Corpora. In ACL’99, pages 519–526, College
Park, Maryland, USA.
Daniele Riboni. 2002. Feature selection for web
page classification. In Hassan Shafazand and A Min
Tjoa, editors, Proceedings of the 1st EurAsian Con-
ference on Advances in Information and Communi-
cation Technology (EURASIA-ICT), pages 473–478,
Shiraz, Iran. Springer.
Jorge Vivaldi Rogelio Nazar and Teresa Cabré. 2008.
A suite to compile and analyze an lsp corpus. In
Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odjik, Stelios Piperidis,
and Daniel Tapias, editors, Proceedings of the Sixth
International Language Resources and Evaluation
(LREC’08), Marrakech, Morocco, may. European
Language Resources Association (ELRA).
J. C. Sager. 1990. A Pratical Course in Terminology
Processing. John Benjamins, Amsterdam.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing Sur-
veys, 34(1):1–47.
John Sinclair. 1996. Preliminary recommendations on
text typology. Technical report, EAGLES (Expert
Advisory Group on Language Engineering Stan-
dards).
Federico Zanettin. 1998. Bilingual comparable
corpora and the training of translators. Meta,
43(4):616–630.
</reference>
<page confidence="0.999457">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.797948">
<title confidence="0.99094">Compilation of Specialized Comparable Corpora in French and Japanese</title>
<author confidence="0.923263">Lorraine Goeuriot</author>
<author confidence="0.923263">Emmanuel Morin</author>
<author confidence="0.923263">Béatrice LINA</author>
<email confidence="0.974107">firstname.lastname@univ-nantes.fr</email>
<abstract confidence="0.998583193548387">We present in this paper the development of a specialized comparable corpora compilation tool, for which quality would be close to a manually compiled corpus. The comparability is based on three levels: domain, topic and type of discourse. Domain and topic can be filtered with the keywords used through web search. But the detection of the type of discourse needs a wide linguistic analysis. The first step of our work is to automate the detection of the type of discourse that can be found in a scientific domain (science and popular science) in French and Japanese languages. First, a contrastive stylistic analysis of the two types of discourse is done on both languages. This analysis leads to the creation of a reusable, generic and robust typology. Machine learning algorithms are then applied to the typology, using shallow parsing. We obtain good results, with an average precision of 80% and an average recall of 70% that demonstrate the efficiency of this typology. This classification tool is then inserted in a corpus compilation tool which is a text collection treatment realized through IBM Starting from two specialized web documents collection in French and Japanese, this tool creates the corresponding corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arul Prakash Asirvatham</author>
<author>Kranthi Kumar Ravi</author>
</authors>
<title>Web page classification based on document structure.</title>
<date>2001</date>
<journal>IEEE National Convention.</journal>
<contexts>
<context position="10933" citStr="Asirvatham and Ravi, 2001" startWordPosition="1717" endWordPosition="1720">: structural, modal and lexical. 3.1 Structural Dimension When documents are extracted from the Web, the structure and the context of creation of the documents should be considered. In the framework Feature French Japanese URL pattern x Document’s format x x Meta tags x x Title tag x x Pages layout x x Pages background x x Images x x Links x x Paragraphs x x Item lists x x Number of sentences x x Typography x x Document’s length x x Table 1: Structural dimension features of Web documents classification, several elements bring useful information: pictures, videos and other multimedia contents (Asirvatham and Ravi, 2001); meta-information, title and HTML structure (Riboni, 2002). While those information are not often used in comparable corpora, they can be used to classify them. Table 1 shows structural features. 3.2 Modal Dimension The degree of specialization required by the recipient or reader is characterized by the relation built in the utterance between the speaker or author and the recipient or reader1. The tone and linguistic elements in texts define this relation. The modalisation is an interpretation of the author’s attitude toward the content of his/her assertion. Modalisation is characterized by m</context>
</contexts>
<marker>Asirvatham, Ravi, 2001</marker>
<rawString>Arul Prakash Asirvatham and Kranthi Kumar Ravi. 2001. Web page classification based on document structure. IEEE National Convention.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Large linguistically-processed web corpora for multiple languages. In</title>
<date>2006</date>
<booktitle>EACL’06,</booktitle>
<pages>87--90</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="2692" citStr="Baroni and Kilgarriff, 2006" startWordPosition="415" endWordPosition="418">rpora compilation, because translated resources are rare and there is a lack of resources when the languages involved do not include English. Furthermore, the amount of multilingual documents available on the Web ensures the possibility of automatically compiling them. Nevertheless, this task can not be summarized to a simple collection of documents sharing vocabulary. It is necessary to respect the common characteristics of texts in corpora, established before the compilation, according to the corpus finality (McEnery and Xiao, 2007). Many works are about compilation of corpora from the Web (Baroni and Kilgarriff, 2006) but none, in our knowledge, focuses on compilation of comparable corpora, which has to satisfy many constraints. We fix three comparability levels: domain, topic and type of discourse. Our goal is to automate recognition of these comparability levels in documents, in order to include them into a corpus. We work on Web documents on specialized scientific domains in French and Japanese languages. As document topics can be filtered with keywords in the Web search (Chakrabarti et al., 1999), we focus in this paper on automatic recognition of types of discourse that can be found in scientific docu</context>
</contexts>
<marker>Baroni, Kilgarriff, 2006</marker>
<rawString>Marco Baroni and Adam Kilgarriff. 2006. Large linguistically-processed web corpora for multiple languages. In EACL’06, pages 87–90. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>Variation across Speech and Writing.</title>
<date>1988</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="13458" citStr="Biber (1988)" startWordPosition="2129" endWordPosition="2130"> locutive act is characterized by modalities. We kept in his theory two locutive acts involving the author: Allocutive act: the author gets the reader involved in the speech (ex.: “You have to do this.”); Elocutive act: the author is involved in his own speech, he reveals his position regarding his speech (ex.: “I would like to do this.”). Each of these acts are then divided into several modalities. These modalities are presented in table 2 with English examples. Some of the modalities are not used in a language or another, because they are not frequent or too ambiguous. 3.3 Lexical Dimension Biber (1988) uses lexical information to observe variations between texts, especially between genres and types of texts. Karlgren (1998) also use lexical information to characterize text genres, and use them to observe stylistic variations among texts. Thus, we assume that lexical information is relevant in the distinction between science and popular science discourse. Firstly, because a specialized vocabulary is a principal characteristic of specialized domain texts (Bowker and Pearson, 2002, p. 26). Secondly, because scientific documents contain more complex lexical units, nominal compounds or nominal s</context>
</contexts>
<marker>Biber, 1988</marker>
<rawString>Douglas Biber. 1988. Variation across Speech and Writing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynne Bowker</author>
<author>Jennifer Pearson</author>
</authors>
<title>Working with Specialized Language: A Practical Guide to Using Corpora.</title>
<date>2002</date>
<location>London/New York, Routeledge.</location>
<contexts>
<context position="1609" citStr="Bowker and Pearson, 2002" startWordPosition="255" endWordPosition="258"> are then applied to the typology, using shallow parsing. We obtain good results, with an average precision of 80% and an average recall of 70% that demonstrate the efficiency of this typology. This classification tool is then inserted in a corpus compilation tool which is a text collection treatment chain realized through IBM UTMA system. Starting from two specialized web documents collection in French and Japanese, this tool creates the corresponding corpus. 1 Introduction Comparable corpora are sets of texts in different languages, that are not translations, but share some characteristics (Bowker and Pearson, 2002). They represent useful resources from which are extracted multilingual terminologies (Déjean et al., 2002) or multilingual lexicons (Fung and Yee, 1998). Comparable corpora are also used in contrastive multilingual studies framework (Peters and Picchi, 1997), they constitute a precious resource for translators (Laviosa, 1998) and teachers (Zanettin, 1998), as they provide a way to observe languages in use. Their compilation is easier than parallel corpora compilation, because translated resources are rare and there is a lack of resources when the languages involved do not include English. Fur</context>
<context position="6499" citStr="Bowker and Pearson, 2002" startWordPosition="1029" endWordPosition="1033">omain and a genre to extract bilingual terminology. Chiao (2002) used a corpus of documents of medical domain on a specific topic to work on the extraction of specialized terminologies. In general language works, documents of comparable corpora often share characteristics like domain or topic. As they are usually extracted from newspapers, it is important to limit them to a certain period to guarantee their comparability. In specialized corpora, first levels of comparability can be achieved with the domain and the topic. Moreover, several communicative settings appear in specialized language (Bowker and Pearson, 2002): expert-expert, expert-initiate, relative expert to the uninitiated, teacher-pupil. Malrieu and Rastier (2002) specify several levels of textual classification, each of which corresponding to a certain granularity. The first level is discourse, defined as a set of utterances from a enunciator characterized by a global topical unit (Ducrot and Todorov, 1972). The second level is genre, defined as text categories distinguished by matured speakers. For example, to literary discourse correspond several genres: drama, poetry, prose... Inspired by these communicative settings and textual categories</context>
<context position="13943" citStr="Bowker and Pearson, 2002" startWordPosition="2197" endWordPosition="2200">of the modalities are not used in a language or another, because they are not frequent or too ambiguous. 3.3 Lexical Dimension Biber (1988) uses lexical information to observe variations between texts, especially between genres and types of texts. Karlgren (1998) also use lexical information to characterize text genres, and use them to observe stylistic variations among texts. Thus, we assume that lexical information is relevant in the distinction between science and popular science discourse. Firstly, because a specialized vocabulary is a principal characteristic of specialized domain texts (Bowker and Pearson, 2002, p. 26). Secondly, because scientific documents contain more complex lexical units, nominal compounds or nominal sentences than popular science documents (Sager, 1990). Table 3 presents the lexical dimension features. Note that these features show a higher language dependency than other dimension features. 4 Automatic Classification by Type of Discourse The process of documents classification can be divided into three steps: document indexing, classifier learning and classifier evaluation (Sebastiani, 2002). Document indexing consists in building a compact representation of documents that can</context>
</contexts>
<marker>Bowker, Pearson, 2002</marker>
<rawString>Lynne Bowker and Jennifer Pearson. 2002. Working with Specialized Language: A Practical Guide to Using Corpora. London/New York, Routeledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Some advances in transformationbased part of speech tagging.</title>
<date>1994</date>
<booktitle>In Proceedings of the 12th National Conference on Arti�cial Intelligence (AAAI’94),</booktitle>
<pages>722--727</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="28026" citStr="Brill, 1994" startWordPosition="4523" endWordPosition="4524">r texts collection and is composed of several components realizing each part of the creation of the corpus: 1. the collection is loaded and documents are converted to texts (with conversion tools from pdf or html to text mainly); 2. all texts are cleaned and normalized (noise from the conversion is cleaned, all texts are converted into the same encoding, etc.); 6http://www.tei-c.org/index.xml 61 3. a pre-syntactic treatment is applied on texts (segmentation mainly) to prepare them for the following step; 4. morphologic and morpho-syntactic tagging tools are applied on the texts (Brill tagger (Brill, 1994) and Flemm lemmer (Namer, 2000) for French texts, Chasen (Matsumoto et al., 1999) for Japanese); 5. texts are classified according to their type of discourse: we use here the most efficient SVMlight classifier. In fact, two corpus are created, on for each type of discourse, then the user can choose one of them. A vectorial representation of each document is computed, then these vectors are classified with the classifier selected. 6. documentation is produced for the corpus, a certain amount of information are included and they can be easily completed by the user. In reality, this tool is more </context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>Eric Brill. 1994. Some advances in transformationbased part of speech tagging. In Proceedings of the 12th National Conference on Arti�cial Intelligence (AAAI’94), pages 722–727, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soumen Chakrabarti</author>
<author>Martin van den Berg</author>
<author>Byron Dom</author>
</authors>
<title>Focused crawling: a new approach to topic-specific Web resource discovery. Computer Networks</title>
<date>1999</date>
<pages>31--11</pages>
<location>Amsterdam, Netherlands:</location>
<marker>Chakrabarti, van den Berg, Dom, 1999</marker>
<rawString>Soumen Chakrabarti, Martin van den Berg, and Byron Dom. 1999. Focused crawling: a new approach to topic-specific Web resource discovery. Computer Networks (Amsterdam, Netherlands: 1999), 31(11– 16):1623–1640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Charaudeau</author>
</authors>
<title>Grammaire du sens et de l’expression.</title>
<date>1992</date>
<publisher>Hachette.</publisher>
<contexts>
<context position="12372" citStr="Charaudeau, 1992" startWordPosition="1942" endWordPosition="1943">n be relevant. For example, the speaker directly speaks to the reader in some popular science documents: “By eating well, you’ll also help to prevent diabetes problems that can occur later in life, like heart disease”. Whereas a scientific document would have a neutral tone: “Obesity plays a central role in the insulin resistance syndrome, which includes hyperinsulinemia, [... ] and an increased risk of atherosclerotic cardiovascular disease”. Most of the modal theories are language dependent, and use description phenomena that are specific to each language. Conversely, the theory exposed in (Charaudeau, 1992) is rather indepen1Since we work on a scientific domain, we will consider the speaker as the author of texts, and the recipient as the reader. 57 dent of the language and operational for French and Japanese (Ishimaru, 2006). According to Charaudeau (1992, p.572), modalisation clarifies the position of the speaker with respect to his reader, to himself and to his speech. Modalisation is composed of locutive acts, particular positions of the author in his speech, and each locutive act is characterized by modalities. We kept in his theory two locutive acts involving the author: Allocutive act: th</context>
</contexts>
<marker>Charaudeau, 1992</marker>
<rawString>Patrick Charaudeau. 1992. Grammaire du sens et de l’expression. Hachette.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Looking for candidate translational equivalents in specialized, comparable corpora.</title>
<date>2002</date>
<booktitle>In COLING’02,</booktitle>
<pages>1208--1212</pages>
<location>Tapei, Taiwan.</location>
<marker>Chiao, Zweigenbaum, 2002</marker>
<rawString>Yun-Chuang Chiao and Pierre Zweigenbaum. 2002. Looking for candidate translational equivalents in specialized, comparable corpora. In COLING’02, pages 1208–1212, Tapei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Béatrice Daille</author>
</authors>
<title>Morphological rule induction for terminology acquisition.</title>
<date>2000</date>
<booktitle>In COLING’00,</booktitle>
<pages>215--221</pages>
<location>Sarrbrucken, Germany.</location>
<contexts>
<context position="18073" citStr="Daille, 2000" startWordPosition="2870" endWordPosition="2871">ected in French with verbs like savoir, connaître (know), and in Japanese with the verb VK (know), with polite form and with neutral form �#&amp;�K. Lexical Features Some of our lexical criteria are specific to the scientific documents, like bibliographies and bibliographic quotations, specialized vocabulary or the measurement units. To measure the terminological density (proportion of specialized vocabulary in the text) in French, we evaluate terms with stems of Greek-Latin (Namer and Baud, 2007) and suffix characters of relational adjectives that are particularly frequent in scientific domains (Daille, 2000). We listed about 50 stems such as inter-, auto- or nano-, and the 10 relational suffixes such such as -ique or -al. For Japanese, we listed prefix characteristics of names of disease or symptoms (Yc_W&apos;K (congenital), jgIY�&apos;K(hereditary), etc.). These stems can be found in both type of discourse, but not in the same proportions. Specialized terms are used in both type of discourse in different ways. For example, the term “ovarectomie” (ovarectomy) can be frequent in a scientific document and used once in a popular science documents to explain it and then replaced by “ablation des ovaires” (ova</context>
</contexts>
<marker>Daille, 2000</marker>
<rawString>Béatrice Daille. 2000. Morphological rule induction for terminology acquisition. In COLING’00, pages 215–221, Sarrbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hervé Déjean</author>
<author>Éric Gaussier</author>
<author>Fatia Sadat</author>
</authors>
<title>An approach based on multilingual thesauri and model combination for bilingual lexicon extraction.</title>
<date>2002</date>
<booktitle>In COLING’02.</booktitle>
<contexts>
<context position="1716" citStr="Déjean et al., 2002" startWordPosition="269" endWordPosition="272">0% and an average recall of 70% that demonstrate the efficiency of this typology. This classification tool is then inserted in a corpus compilation tool which is a text collection treatment chain realized through IBM UTMA system. Starting from two specialized web documents collection in French and Japanese, this tool creates the corresponding corpus. 1 Introduction Comparable corpora are sets of texts in different languages, that are not translations, but share some characteristics (Bowker and Pearson, 2002). They represent useful resources from which are extracted multilingual terminologies (Déjean et al., 2002) or multilingual lexicons (Fung and Yee, 1998). Comparable corpora are also used in contrastive multilingual studies framework (Peters and Picchi, 1997), they constitute a precious resource for translators (Laviosa, 1998) and teachers (Zanettin, 1998), as they provide a way to observe languages in use. Their compilation is easier than parallel corpora compilation, because translated resources are rare and there is a lack of resources when the languages involved do not include English. Furthermore, the amount of multilingual documents available on the Web ensures the possibility of automaticall</context>
<context position="5735" citStr="Déjean et al. (2002)" startWordPosition="906" endWordPosition="909">s of works, which induces different choices: • General language works, where texts of corpora usually share a domain and a period. Fung and Yee (1998) used a corpus composed of newspaper in English and Chinese on a specific period to extract words translations, using IR and NLP methods. Rapp (1999) used a English / German corpus, composed of documents coming from newspapers as well as scientific papers to study alignment methods and bilingual lexicon extraction from non-parallel corpora (which can be considered as comparable); • Specialized language works, where choice of criteria is various. Déjean et al. (2002) used a corpus composed of scientific abstracts from Medline, a medical portal, in English and German. Thus they used documents sharing a domain and a genre to extract bilingual terminology. Chiao (2002) used a corpus of documents of medical domain on a specific topic to work on the extraction of specialized terminologies. In general language works, documents of comparable corpora often share characteristics like domain or topic. As they are usually extracted from newspapers, it is important to limit them to a certain period to guarantee their comparability. In specialized corpora, first level</context>
</contexts>
<marker>Déjean, Gaussier, Sadat, 2002</marker>
<rawString>Hervé Déjean, Éric Gaussier, and Fatia Sadat. 2002. An approach based on multilingual thesauri and model combination for bilingual lexicon extraction. In COLING’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oswald Ducrot</author>
<author>Tzvetan Todorov</author>
</authors>
<title>Dictionnaire encyclopédique des sciences du langage. Éditions du Seuil.</title>
<date>1972</date>
<contexts>
<context position="6859" citStr="Ducrot and Todorov, 1972" startWordPosition="1081" endWordPosition="1084"> limit them to a certain period to guarantee their comparability. In specialized corpora, first levels of comparability can be achieved with the domain and the topic. Moreover, several communicative settings appear in specialized language (Bowker and Pearson, 2002): expert-expert, expert-initiate, relative expert to the uninitiated, teacher-pupil. Malrieu and Rastier (2002) specify several levels of textual classification, each of which corresponding to a certain granularity. The first level is discourse, defined as a set of utterances from a enunciator characterized by a global topical unit (Ducrot and Todorov, 1972). The second level is genre, defined as text categories distinguished by matured speakers. For example, to literary discourse correspond several genres: drama, poetry, prose... Inspired by these communicative settings and textual categories, we choose to distinguish two communicative settings or type of discourse in specialized domains: science (texts written by experts to experts) and popular science (texts written to non-experts, by experts, semi-experts or nonexperts). This comparability level, the type of discourse, reflects the context of production or usage of the documents, and guarante</context>
</contexts>
<marker>Ducrot, Todorov, 1972</marker>
<rawString>Oswald Ducrot and Tzvetan Todorov. 1972. Dictionnaire encyclopédique des sciences du langage. Éditions du Seuil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Adam Lally</author>
</authors>
<title>Uima: An architectural approach to unstructured information processing in the corporate research environment.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<pages>10--327</pages>
<contexts>
<context position="3700" citStr="Ferrucci and Lally, 2004" startWordPosition="576" endWordPosition="579">nese languages. As document topics can be filtered with keywords in the Web search (Chakrabarti et al., 1999), we focus in this paper on automatic recognition of types of discourse that can be found in scientific documents: science and popular science. This classification tool is then inserted in a specialized comparable corpora compilation tool, which is developped through the Unstructured Information Man55 Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 55–63, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP agement Architecture (UIMA) (Ferrucci and Lally, 2004). This paper is structured as follows. After an introduction of related works in section 2, stylistic analysis of our corpus will be presented in section 3. This analysis will lead to the creation of a typology of scientific and popular science discourse type in specifialized domains. The application of learning algorithms to the typology will be described in section 4, and the results will be presented in section 5. We will show that our typology, based on linguistically motivated features, can characterize science and popular science discourses in French and Japanese documents, and that the </context>
<context position="27249" citStr="Ferrucci and Lally, 2004" startWordPosition="4399" endWordPosition="4402"> Moreover, these steps are directly linked to the creator use of the corpus. The first step must be realized by the user to create an relevant corpus. Although second step can be computerizable (Rogelio Nazar and Cabré, 2008), we choose to keep it manual in order to guarantee corpus quality. We decided to work on a system which realizes the last steps, i.e. normalization, annotation and documentation, starting from a collection of documents selected by a user. Our tool has been developed on Unstructured Information Management Architecture (UIMA) that has been created by IBM Research Division (Ferrucci and Lally, 2004). Unstructured data (texts, images, etc.) collections can be easily treated on this platform and many libraries are available. Our tool starts with a web documents or texts collection and is composed of several components realizing each part of the creation of the corpus: 1. the collection is loaded and documents are converted to texts (with conversion tools from pdf or html to text mainly); 2. all texts are cleaned and normalized (noise from the conversion is cleaned, all texts are converted into the same encoding, etc.); 6http://www.tei-c.org/index.xml 61 3. a pre-syntactic treatment is appl</context>
</contexts>
<marker>Ferrucci, Lally, 2004</marker>
<rawString>David Ferrucci and Adam Lally. 2004. Uima: An architectural approach to unstructured information processing in the corporate research environment. Natural Language Engineering, 10:327–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An ir approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<volume>1</volume>
<pages>414--420</pages>
<editor>In Christian Boitet and Pete Whitelock, editors, COLING’98,</editor>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="1762" citStr="Fung and Yee, 1998" startWordPosition="276" endWordPosition="279">e the efficiency of this typology. This classification tool is then inserted in a corpus compilation tool which is a text collection treatment chain realized through IBM UTMA system. Starting from two specialized web documents collection in French and Japanese, this tool creates the corresponding corpus. 1 Introduction Comparable corpora are sets of texts in different languages, that are not translations, but share some characteristics (Bowker and Pearson, 2002). They represent useful resources from which are extracted multilingual terminologies (Déjean et al., 2002) or multilingual lexicons (Fung and Yee, 1998). Comparable corpora are also used in contrastive multilingual studies framework (Peters and Picchi, 1997), they constitute a precious resource for translators (Laviosa, 1998) and teachers (Zanettin, 1998), as they provide a way to observe languages in use. Their compilation is easier than parallel corpora compilation, because translated resources are rare and there is a lack of resources when the languages involved do not include English. Furthermore, the amount of multilingual documents available on the Web ensures the possibility of automatically compiling them. Nevertheless, this task can </context>
<context position="5265" citStr="Fung and Yee (1998)" startWordPosition="831" endWordPosition="834"> 20). Comparability is ensured using characteristics which can refer to the text creation context (period, author...), or to the text itself (topic, genre...). The choice of the common characteristics, which define the content of corpora, affects the degree of comparability, notion used to quantify how two corpora can be comparable. The choice of these characteristics depends on the finality of the corpus. Among papers on comparable corpora, we distinguish two types of works, which induces different choices: • General language works, where texts of corpora usually share a domain and a period. Fung and Yee (1998) used a corpus composed of newspaper in English and Chinese on a specific period to extract words translations, using IR and NLP methods. Rapp (1999) used a English / German corpus, composed of documents coming from newspapers as well as scientific papers to study alignment methods and bilingual lexicon extraction from non-parallel corpora (which can be considered as comparable); • Specialized language works, where choice of criteria is various. Déjean et al. (2002) used a corpus composed of scientific abstracts from Medline, a medical portal, in English and German. Thus they used documents sh</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An ir approach for translating new words from nonparallel, comparable texts. In Christian Boitet and Pete Whitelock, editors, COLING’98, volume 1, pages 414– 420, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kumiko Ishimaru</author>
</authors>
<title>Comparative study on the discourse of advertisement in France and Japan: beauty products.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Osaka University,</institution>
<contexts>
<context position="12595" citStr="Ishimaru, 2006" startWordPosition="1981" endWordPosition="1982">s a scientific document would have a neutral tone: “Obesity plays a central role in the insulin resistance syndrome, which includes hyperinsulinemia, [... ] and an increased risk of atherosclerotic cardiovascular disease”. Most of the modal theories are language dependent, and use description phenomena that are specific to each language. Conversely, the theory exposed in (Charaudeau, 1992) is rather indepen1Since we work on a scientific domain, we will consider the speaker as the author of texts, and the recipient as the reader. 57 dent of the language and operational for French and Japanese (Ishimaru, 2006). According to Charaudeau (1992, p.572), modalisation clarifies the position of the speaker with respect to his reader, to himself and to his speech. Modalisation is composed of locutive acts, particular positions of the author in his speech, and each locutive act is characterized by modalities. We kept in his theory two locutive acts involving the author: Allocutive act: the author gets the reader involved in the speech (ex.: “You have to do this.”); Elocutive act: the author is involved in his own speech, he reveals his position regarding his speech (ex.: “I would like to do this.”). Each of</context>
</contexts>
<marker>Ishimaru, 2006</marker>
<rawString>Kumiko Ishimaru. 2006. Comparative study on the discourse of advertisement in France and Japan: beauty products. Ph.D. thesis, Osaka University, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Learning to Classify Text using Support Vector Machines.</title>
<date>2002</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="19551" citStr="Joachims, 2002" startWordPosition="3114" endWordPosition="3115">assified in a class c or c and determine characteristics that a new document should have to be classified in one of these two classes 2. Given a document indexing, there are some well-known algorithms that can achieve this process (neural network, Bayes classifiers, SVM, etc.) of which Sebastiani (2002) carried out a research about the assemblage and comparison. Applied to a Reuters newswires corpus, these techniques showed variable performances in the usage level of supervised or unsupervised approaches, of the size of the corpus, of the number of categories, etc. We decided to use SVMlight (Joachims, 2002) and C4.5 (Quinlan, 1993), since both of them seem to be the most appropriate to our data (small corpora, binary classification, less than 100 features). 5 Experiments In this section, we describe the two comparable corpora used and present the two experiments carried out with each of them. The first comparable corpus is used to train the classifier in order to learn a classification model based on our typology (i.e. training task). The second comparable corpus is used to evaluate the impact of the classification model when applied on new documents (i.e. evaluation task). 2This is the binary c</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Learning to Classify Text using Support Vector Machines. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jussi Karlgren</author>
</authors>
<title>Natural Language Information Retrieval, chapter Stylistic Experiments in Information Retrieval.</title>
<date>1998</date>
<publisher>Tomek, Kluwer.</publisher>
<contexts>
<context position="8086" citStr="Karlgren, 1998" startWordPosition="1276" endWordPosition="1277"> homogeneity in corpora (Bowker and Pearson, 2002, p. 27). Furthermore, Morin et al. (2007) proved that comparable corpora sharing a topic and a type of discourse are well adapted for multilingual terminologies extraction. Our goal is to create a tool to compile comparable corpora in French and Japanese which documents are extracted from the Web. We investigate automatic categorization of documents according to their type of discourse. This categorization is based on a typology of elements characterizing these types of discourse. To this end, we carry out a stylistic and contrastive analysis (Karlgren, 1998). This analysis aims to highlight linguistically motivated features through several dimen56 sions (structural, modal and lexical), whose combination characterizes scientific or popular science discourse. A specialized comparable corpus can be compiled from a single type of discourse document collection through several steps. Last part of this paper focuses on the automation of these steps using the IBM Unstructured Information Management Architecture (UIMA). 3 Analysis of Types of Discourse The recognition of types of discourse is based on a stylistic analysis adapted from a deductive and cont</context>
<context position="13582" citStr="Karlgren (1998)" startWordPosition="2147" endWordPosition="2148">t: the author gets the reader involved in the speech (ex.: “You have to do this.”); Elocutive act: the author is involved in his own speech, he reveals his position regarding his speech (ex.: “I would like to do this.”). Each of these acts are then divided into several modalities. These modalities are presented in table 2 with English examples. Some of the modalities are not used in a language or another, because they are not frequent or too ambiguous. 3.3 Lexical Dimension Biber (1988) uses lexical information to observe variations between texts, especially between genres and types of texts. Karlgren (1998) also use lexical information to characterize text genres, and use them to observe stylistic variations among texts. Thus, we assume that lexical information is relevant in the distinction between science and popular science discourse. Firstly, because a specialized vocabulary is a principal characteristic of specialized domain texts (Bowker and Pearson, 2002, p. 26). Secondly, because scientific documents contain more complex lexical units, nominal compounds or nominal sentences than popular science documents (Sager, 1990). Table 3 presents the lexical dimension features. Note that these feat</context>
</contexts>
<marker>Karlgren, 1998</marker>
<rawString>Jussi Karlgren, 1998. Natural Language Information Retrieval, chapter Stylistic Experiments in Information Retrieval. Tomek, Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Laviosa</author>
</authors>
<title>Corpus-based approaches to contrastive linguistics and translation studies.</title>
<date>1998</date>
<journal>Meta,</journal>
<volume>43</volume>
<issue>4</issue>
<contexts>
<context position="1937" citStr="Laviosa, 1998" startWordPosition="302" endWordPosition="303">m. Starting from two specialized web documents collection in French and Japanese, this tool creates the corresponding corpus. 1 Introduction Comparable corpora are sets of texts in different languages, that are not translations, but share some characteristics (Bowker and Pearson, 2002). They represent useful resources from which are extracted multilingual terminologies (Déjean et al., 2002) or multilingual lexicons (Fung and Yee, 1998). Comparable corpora are also used in contrastive multilingual studies framework (Peters and Picchi, 1997), they constitute a precious resource for translators (Laviosa, 1998) and teachers (Zanettin, 1998), as they provide a way to observe languages in use. Their compilation is easier than parallel corpora compilation, because translated resources are rare and there is a lack of resources when the languages involved do not include English. Furthermore, the amount of multilingual documents available on the Web ensures the possibility of automatically compiling them. Nevertheless, this task can not be summarized to a simple collection of documents sharing vocabulary. It is necessary to respect the common characteristics of texts in corpora, established before the com</context>
</contexts>
<marker>Laviosa, 1998</marker>
<rawString>Sarah Laviosa. 1998. Corpus-based approaches to contrastive linguistics and translation studies. Meta, 43(4):474–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denise Malrieu</author>
<author>Francois Rastier</author>
</authors>
<title>Genres et variations morphosyntaxiques.</title>
<date>2002</date>
<booktitle>Traitement Automatique des Langues (TAL),</booktitle>
<pages>42--2</pages>
<contexts>
<context position="6610" citStr="Malrieu and Rastier (2002)" startWordPosition="1042" endWordPosition="1045">on a specific topic to work on the extraction of specialized terminologies. In general language works, documents of comparable corpora often share characteristics like domain or topic. As they are usually extracted from newspapers, it is important to limit them to a certain period to guarantee their comparability. In specialized corpora, first levels of comparability can be achieved with the domain and the topic. Moreover, several communicative settings appear in specialized language (Bowker and Pearson, 2002): expert-expert, expert-initiate, relative expert to the uninitiated, teacher-pupil. Malrieu and Rastier (2002) specify several levels of textual classification, each of which corresponding to a certain granularity. The first level is discourse, defined as a set of utterances from a enunciator characterized by a global topical unit (Ducrot and Todorov, 1972). The second level is genre, defined as text categories distinguished by matured speakers. For example, to literary discourse correspond several genres: drama, poetry, prose... Inspired by these communicative settings and textual categories, we choose to distinguish two communicative settings or type of discourse in specialized domains: science (tex</context>
</contexts>
<marker>Malrieu, Rastier, 2002</marker>
<rawString>Denise Malrieu and Francois Rastier. 2002. Genres et variations morphosyntaxiques. Traitement Automatique des Langues (TAL), 42(2):548–577.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Akira Kitauchi</author>
<author>Tatsuo Yamashita</author>
<author>Yoshitaka Hirano</author>
</authors>
<date>1999</date>
<booktitle>Japanese Morphological Analysis System ChaSen 2.0 Users Manual. Technical report, Nara Institute of Science and Technology (NAIST).</booktitle>
<contexts>
<context position="24151" citStr="Matsumoto et al., 1999" startWordPosition="3891" endWordPosition="3894">90% as for the classifier C4.5 (more than 215 documents are well classified from 248 French documents of [DIAB_CP]). The results of the classification in Japanese are also good with the classifier C.4.5. More than 90% of documents are correctly classified, and the precision reaches on average 80%. Some of the lower results can be explained, especially in Japanese by the high range of document genres in the corpus (research papers, newspapers, scientific magazines, recipes, job offers, forum discussions... ). 5For Japanese, the number of words is the number of occurrences recognized by ChaSen (Matsumoto et al., 1999) 60 French Rec. Japanese Rec. Prec. Prec. SC 1.00 0.36 0.70 0.65 lvms 0.80 1,00 0.72 0.80 PS 45c. SC 0.89 0.80 0.76 0.99 PS 0.91 0.94 0.95 0.96 Table 6: Precision and recall for each language, each classifier, on [DIAB_CP] Table 7 shows results on [BC_CP]. In general, we note a decrease of the results with [BC_CP], although results are still satisfactory. French documents are well classified whatever the classifier is, with a precision higher than 75% and a recall higher than 75%, which represent more than 70 well classified documents on 92. Japanese documents are well classified too, with 76%</context>
<context position="28107" citStr="Matsumoto et al., 1999" startWordPosition="4534" endWordPosition="4537">h part of the creation of the corpus: 1. the collection is loaded and documents are converted to texts (with conversion tools from pdf or html to text mainly); 2. all texts are cleaned and normalized (noise from the conversion is cleaned, all texts are converted into the same encoding, etc.); 6http://www.tei-c.org/index.xml 61 3. a pre-syntactic treatment is applied on texts (segmentation mainly) to prepare them for the following step; 4. morphologic and morpho-syntactic tagging tools are applied on the texts (Brill tagger (Brill, 1994) and Flemm lemmer (Namer, 2000) for French texts, Chasen (Matsumoto et al., 1999) for Japanese); 5. texts are classified according to their type of discourse: we use here the most efficient SVMlight classifier. In fact, two corpus are created, on for each type of discourse, then the user can choose one of them. A vectorial representation of each document is computed, then these vectors are classified with the classifier selected. 6. documentation is produced for the corpus, a certain amount of information are included and they can be easily completed by the user. In reality, this tool is more a compilation assistant than a compilator. It facilitates the compilation task: t</context>
</contexts>
<marker>Matsumoto, Kitauchi, Yamashita, Hirano, 1999</marker>
<rawString>Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, and Yoshitaka Hirano. 1999. Japanese Morphological Analysis System ChaSen 2.0 Users Manual. Technical report, Nara Institute of Science and Technology (NAIST).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony McEnery</author>
<author>Zhonghua Xiao</author>
</authors>
<title>Parallel and comparable corpora: What is happening?</title>
<date>2007</date>
<booktitle>In Gunilla Anderman</booktitle>
<editor>and Margaret Rogers, editors,</editor>
<location>Clevedon: Multilingual Matters.</location>
<contexts>
<context position="2604" citStr="McEnery and Xiao, 2007" startWordPosition="401" endWordPosition="404">ide a way to observe languages in use. Their compilation is easier than parallel corpora compilation, because translated resources are rare and there is a lack of resources when the languages involved do not include English. Furthermore, the amount of multilingual documents available on the Web ensures the possibility of automatically compiling them. Nevertheless, this task can not be summarized to a simple collection of documents sharing vocabulary. It is necessary to respect the common characteristics of texts in corpora, established before the compilation, according to the corpus finality (McEnery and Xiao, 2007). Many works are about compilation of corpora from the Web (Baroni and Kilgarriff, 2006) but none, in our knowledge, focuses on compilation of comparable corpora, which has to satisfy many constraints. We fix three comparability levels: domain, topic and type of discourse. Our goal is to automate recognition of these comparability levels in documents, in order to include them into a corpus. We work on Web documents on specialized scientific domains in French and Japanese languages. As document topics can be filtered with keywords in the Web search (Chakrabarti et al., 1999), we focus in this p</context>
<context position="4642" citStr="McEnery and Xiao, 2007" startWordPosition="728" endWordPosition="731">thms to the typology will be described in section 4, and the results will be presented in section 5. We will show that our typology, based on linguistically motivated features, can characterize science and popular science discourses in French and Japanese documents, and that the use of our three comparablility levels can improve corpora comparability. Finally, we describe the development of the corpus compilation tool. 2 Background “A comparable corpus can be defined as a corpus containing components that are collected using the same sampling frame and similar balance and representativeness” (McEnery and Xiao, 2007, p. 20). Comparability is ensured using characteristics which can refer to the text creation context (period, author...), or to the text itself (topic, genre...). The choice of the common characteristics, which define the content of corpora, affects the degree of comparability, notion used to quantify how two corpora can be comparable. The choice of these characteristics depends on the finality of the corpus. Among papers on comparable corpora, we distinguish two types of works, which induces different choices: • General language works, where texts of corpora usually share a domain and a peri</context>
</contexts>
<marker>McEnery, Xiao, 2007</marker>
<rawString>Anthony McEnery and Zhonghua Xiao. 2007. Parallel and comparable corpora: What is happening? In Gunilla Anderman and Margaret Rogers, editors, Incorporating Corpora: The Linguist and the Translator. Clevedon: Multilingual Matters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fiammetta Namer</author>
<author>Robert Baud</author>
</authors>
<title>Defining and relating biomedical terms: Towards a crosslanguage morphosemantics-based system.</title>
<date>2007</date>
<journal>International Journal of Medical Informatics,</journal>
<pages>76--2</pages>
<contexts>
<context position="17958" citStr="Namer and Baud, 2007" startWordPosition="2852" endWordPosition="2855">es and ft,(r (we). The modalities are also computed with lexical markers. For example, the modality of knowledge can be detected in French with verbs like savoir, connaître (know), and in Japanese with the verb VK (know), with polite form and with neutral form �#&amp;�K. Lexical Features Some of our lexical criteria are specific to the scientific documents, like bibliographies and bibliographic quotations, specialized vocabulary or the measurement units. To measure the terminological density (proportion of specialized vocabulary in the text) in French, we evaluate terms with stems of Greek-Latin (Namer and Baud, 2007) and suffix characters of relational adjectives that are particularly frequent in scientific domains (Daille, 2000). We listed about 50 stems such as inter-, auto- or nano-, and the 10 relational suffixes such such as -ique or -al. For Japanese, we listed prefix characteristics of names of disease or symptoms (Yc_W&apos;K (congenital), jgIY�&apos;K(hereditary), etc.). These stems can be found in both type of discourse, but not in the same proportions. Specialized terms are used in both type of discourse in different ways. For example, the term “ovarectomie” (ovarectomy) can be frequent in a scientific d</context>
</contexts>
<marker>Namer, Baud, 2007</marker>
<rawString>Fiammetta Namer and Robert Baud. 2007. Defining and relating biomedical terms: Towards a crosslanguage morphosemantics-based system. International Journal of Medical Informatics, 76(2-3):226– 233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fiametta Namer</author>
</authors>
<title>Flemm : Un analyseur flexionnel du français à base de règles.</title>
<date>2000</date>
<booktitle>Traitement Automatique des Langues (TAL),</booktitle>
<pages>41--2</pages>
<contexts>
<context position="28057" citStr="Namer, 2000" startWordPosition="4528" endWordPosition="4529">sed of several components realizing each part of the creation of the corpus: 1. the collection is loaded and documents are converted to texts (with conversion tools from pdf or html to text mainly); 2. all texts are cleaned and normalized (noise from the conversion is cleaned, all texts are converted into the same encoding, etc.); 6http://www.tei-c.org/index.xml 61 3. a pre-syntactic treatment is applied on texts (segmentation mainly) to prepare them for the following step; 4. morphologic and morpho-syntactic tagging tools are applied on the texts (Brill tagger (Brill, 1994) and Flemm lemmer (Namer, 2000) for French texts, Chasen (Matsumoto et al., 1999) for Japanese); 5. texts are classified according to their type of discourse: we use here the most efficient SVMlight classifier. In fact, two corpus are created, on for each type of discourse, then the user can choose one of them. A vectorial representation of each document is computed, then these vectors are classified with the classifier selected. 6. documentation is produced for the corpus, a certain amount of information are included and they can be easily completed by the user. In reality, this tool is more a compilation assistant than a </context>
</contexts>
<marker>Namer, 2000</marker>
<rawString>Fiametta Namer. 2000. Flemm : Un analyseur flexionnel du français à base de règles. Traitement Automatique des Langues (TAL), 41(2):523–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Peters</author>
<author>Eugenio Picchi</author>
</authors>
<title>Using linguistic tools and resources in cross-language retrieval.</title>
<date>1997</date>
<booktitle>In David Hull and Doug Oard, editors, Cross-Language Text and Speech Retrieval. Papers from the</booktitle>
<tech>Technical Report SS-97-05,</tech>
<pages>179--188</pages>
<contexts>
<context position="1868" citStr="Peters and Picchi, 1997" startWordPosition="290" endWordPosition="293">tool which is a text collection treatment chain realized through IBM UTMA system. Starting from two specialized web documents collection in French and Japanese, this tool creates the corresponding corpus. 1 Introduction Comparable corpora are sets of texts in different languages, that are not translations, but share some characteristics (Bowker and Pearson, 2002). They represent useful resources from which are extracted multilingual terminologies (Déjean et al., 2002) or multilingual lexicons (Fung and Yee, 1998). Comparable corpora are also used in contrastive multilingual studies framework (Peters and Picchi, 1997), they constitute a precious resource for translators (Laviosa, 1998) and teachers (Zanettin, 1998), as they provide a way to observe languages in use. Their compilation is easier than parallel corpora compilation, because translated resources are rare and there is a lack of resources when the languages involved do not include English. Furthermore, the amount of multilingual documents available on the Web ensures the possibility of automatically compiling them. Nevertheless, this task can not be summarized to a simple collection of documents sharing vocabulary. It is necessary to respect the c</context>
</contexts>
<marker>Peters, Picchi, 1997</marker>
<rawString>Carol Peters and Eugenio Picchi. 1997. Using linguistic tools and resources in cross-language retrieval. In David Hull and Doug Oard, editors, Cross-Language Text and Speech Retrieval. Papers from the 1997 AAAI Spring Symposium, Technical Report SS-97-05, pages 179–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="19576" citStr="Quinlan, 1993" startWordPosition="3118" endWordPosition="3119"> and determine characteristics that a new document should have to be classified in one of these two classes 2. Given a document indexing, there are some well-known algorithms that can achieve this process (neural network, Bayes classifiers, SVM, etc.) of which Sebastiani (2002) carried out a research about the assemblage and comparison. Applied to a Reuters newswires corpus, these techniques showed variable performances in the usage level of supervised or unsupervised approaches, of the size of the corpus, of the number of categories, etc. We decided to use SVMlight (Joachims, 2002) and C4.5 (Quinlan, 1993), since both of them seem to be the most appropriate to our data (small corpora, binary classification, less than 100 features). 5 Experiments In this section, we describe the two comparable corpora used and present the two experiments carried out with each of them. The first comparable corpus is used to train the classifier in order to learn a classification model based on our typology (i.e. training task). The second comparable corpus is used to evaluate the impact of the classification model when applied on new documents (i.e. evaluation task). 2This is the binary case. See (Sebastiani, 200</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic Identification of Word Translations from Unrelated English and German Corpora. In</title>
<date>1999</date>
<booktitle>ACL’99,</booktitle>
<pages>519--526</pages>
<location>College Park, Maryland, USA.</location>
<contexts>
<context position="5414" citStr="Rapp (1999)" startWordPosition="858" endWordPosition="859">). The choice of the common characteristics, which define the content of corpora, affects the degree of comparability, notion used to quantify how two corpora can be comparable. The choice of these characteristics depends on the finality of the corpus. Among papers on comparable corpora, we distinguish two types of works, which induces different choices: • General language works, where texts of corpora usually share a domain and a period. Fung and Yee (1998) used a corpus composed of newspaper in English and Chinese on a specific period to extract words translations, using IR and NLP methods. Rapp (1999) used a English / German corpus, composed of documents coming from newspapers as well as scientific papers to study alignment methods and bilingual lexicon extraction from non-parallel corpora (which can be considered as comparable); • Specialized language works, where choice of criteria is various. Déjean et al. (2002) used a corpus composed of scientific abstracts from Medline, a medical portal, in English and German. Thus they used documents sharing a domain and a genre to extract bilingual terminology. Chiao (2002) used a corpus of documents of medical domain on a specific topic to work on</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic Identification of Word Translations from Unrelated English and German Corpora. In ACL’99, pages 519–526, College Park, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Riboni</author>
</authors>
<title>Feature selection for web page classification. In</title>
<date>2002</date>
<booktitle>Proceedings of the 1st EurAsian Conference on Advances in Information and Communication Technology (EURASIA-ICT),</booktitle>
<pages>473--478</pages>
<editor>Hassan Shafazand and A Min Tjoa, editors,</editor>
<publisher>Shiraz, Iran. Springer.</publisher>
<contexts>
<context position="10992" citStr="Riboni, 2002" startWordPosition="1727" endWordPosition="1728"> are extracted from the Web, the structure and the context of creation of the documents should be considered. In the framework Feature French Japanese URL pattern x Document’s format x x Meta tags x x Title tag x x Pages layout x x Pages background x x Images x x Links x x Paragraphs x x Item lists x x Number of sentences x x Typography x x Document’s length x x Table 1: Structural dimension features of Web documents classification, several elements bring useful information: pictures, videos and other multimedia contents (Asirvatham and Ravi, 2001); meta-information, title and HTML structure (Riboni, 2002). While those information are not often used in comparable corpora, they can be used to classify them. Table 1 shows structural features. 3.2 Modal Dimension The degree of specialization required by the recipient or reader is characterized by the relation built in the utterance between the speaker or author and the recipient or reader1. The tone and linguistic elements in texts define this relation. The modalisation is an interpretation of the author’s attitude toward the content of his/her assertion. Modalisation is characterized by many textual markers: verbs, adverbs, politeness forms, etc.</context>
</contexts>
<marker>Riboni, 2002</marker>
<rawString>Daniele Riboni. 2002. Feature selection for web page classification. In Hassan Shafazand and A Min Tjoa, editors, Proceedings of the 1st EurAsian Conference on Advances in Information and Communication Technology (EURASIA-ICT), pages 473–478, Shiraz, Iran. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Vivaldi Rogelio Nazar</author>
<author>Teresa Cabré</author>
</authors>
<title>A suite to compile and analyze an lsp corpus.</title>
<date>2008</date>
<booktitle>Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<editor>In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors,</editor>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="26849" citStr="Nazar and Cabré, 2008" startWordPosition="4335" endWordPosition="4338">e used in a durable way must include this step. Documentation of the corpus includes information about the compilation (creator, date, method, resources, etc.) and information about the corpus documents. Text Encoding Initiative (TEI) standard has been created in order to conserve in an uniformed way this kind of information in a corpus 6. A corpus quality highly depends on the first two steps. Moreover, these steps are directly linked to the creator use of the corpus. The first step must be realized by the user to create an relevant corpus. Although second step can be computerizable (Rogelio Nazar and Cabré, 2008), we choose to keep it manual in order to guarantee corpus quality. We decided to work on a system which realizes the last steps, i.e. normalization, annotation and documentation, starting from a collection of documents selected by a user. Our tool has been developed on Unstructured Information Management Architecture (UIMA) that has been created by IBM Research Division (Ferrucci and Lally, 2004). Unstructured data (texts, images, etc.) collections can be easily treated on this platform and many libraries are available. Our tool starts with a web documents or texts collection and is composed </context>
</contexts>
<marker>Nazar, Cabré, 2008</marker>
<rawString>Jorge Vivaldi Rogelio Nazar and Teresa Cabré. 2008. A suite to compile and analyze an lsp corpus. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors, Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Sager</author>
</authors>
<title>A Pratical Course</title>
<date>1990</date>
<booktitle>in Terminology Processing. John Benjamins,</booktitle>
<location>Amsterdam.</location>
<contexts>
<context position="14111" citStr="Sager, 1990" startWordPosition="2223" endWordPosition="2224">ations between texts, especially between genres and types of texts. Karlgren (1998) also use lexical information to characterize text genres, and use them to observe stylistic variations among texts. Thus, we assume that lexical information is relevant in the distinction between science and popular science discourse. Firstly, because a specialized vocabulary is a principal characteristic of specialized domain texts (Bowker and Pearson, 2002, p. 26). Secondly, because scientific documents contain more complex lexical units, nominal compounds or nominal sentences than popular science documents (Sager, 1990). Table 3 presents the lexical dimension features. Note that these features show a higher language dependency than other dimension features. 4 Automatic Classification by Type of Discourse The process of documents classification can be divided into three steps: document indexing, classifier learning and classifier evaluation (Sebastiani, 2002). Document indexing consists in building a compact representation of documents that can be interpreted by a classifier. In our case, each document di is represented as a vector of features weight: di* = {w1i, ... , wni} where n is the Feature French Japan</context>
</contexts>
<marker>Sager, 1990</marker>
<rawString>J. C. Sager. 1990. A Pratical Course in Terminology Processing. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="14456" citStr="Sebastiani, 2002" startWordPosition="2272" endWordPosition="2273">ecialized vocabulary is a principal characteristic of specialized domain texts (Bowker and Pearson, 2002, p. 26). Secondly, because scientific documents contain more complex lexical units, nominal compounds or nominal sentences than popular science documents (Sager, 1990). Table 3 presents the lexical dimension features. Note that these features show a higher language dependency than other dimension features. 4 Automatic Classification by Type of Discourse The process of documents classification can be divided into three steps: document indexing, classifier learning and classifier evaluation (Sebastiani, 2002). Document indexing consists in building a compact representation of documents that can be interpreted by a classifier. In our case, each document di is represented as a vector of features weight: di* = {w1i, ... , wni} where n is the Feature French Japanese Specialized vocabulary x x Numerals x x Units of measurement x x Words length x Bibliography x x Bibliographic quotes x x Punctuation x x Sentences end x Brackets x x Other alphabets (latin, x hiragana, katakana) Symbols x Table 3: Lexical dimension features Dimension Method Structural Pattern matching Modal Lexical and lexico-syntactic pa</context>
<context position="19240" citStr="Sebastiani (2002)" startWordPosition="3063" endWordPosition="3064">t and then replaced by “ablation des ovaires” (ovary ablation). Sentences end are specific ending particles used in japanese, for example the particle l7&apos; is often used at the end of an interrogative sentence. 4.2 Learning Algorithms Classifier learning is a process which observes features weight of documents classified in a class c or c and determine characteristics that a new document should have to be classified in one of these two classes 2. Given a document indexing, there are some well-known algorithms that can achieve this process (neural network, Bayes classifiers, SVM, etc.) of which Sebastiani (2002) carried out a research about the assemblage and comparison. Applied to a Reuters newswires corpus, these techniques showed variable performances in the usage level of supervised or unsupervised approaches, of the size of the corpus, of the number of categories, etc. We decided to use SVMlight (Joachims, 2002) and C4.5 (Quinlan, 1993), since both of them seem to be the most appropriate to our data (small corpora, binary classification, less than 100 features). 5 Experiments In this section, we describe the two comparable corpora used and present the two experiments carried out with each of the</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
</authors>
<title>Preliminary recommendations on text typology.</title>
<date>1996</date>
<journal>EAGLES (Expert Advisory Group on Language Engineering Standards).</journal>
<tech>Technical report,</tech>
<contexts>
<context position="9377" citStr="Sinclair (1996)" startWordPosition="1470" endWordPosition="1471">tivated features characterizing these two types of discourse. Main difficulty here is to find relevant features which fit every language involved. These features, gathered in a typology, will be used to adapt machine learning algorithms to compilation of corpora. This typology thus needs to be robust, generic and reusable in other languages and domains. Genericity is ensured by a broad typology composed of features covering a wide range of documents characteristics, while robustness is guaranteed with operational (computable) features and treatment adaptable to Web documents as well as texts. Sinclair (1996) distinguishes two levels of analysis in his report on text typologies: external level, characterizing the context of creation of the document; and internal level, corresponding to linguistic characteristics of document. Because our corpora are composed of documents extracted from the Web, we consider external level features as all the features related to the creation of documents and their structure (non-linguistic features) and call them structural features. Stylistic analysis raises several granularity levels among linguistic characteristics of the texts. We thus distinguish two levels in t</context>
</contexts>
<marker>Sinclair, 1996</marker>
<rawString>John Sinclair. 1996. Preliminary recommendations on text typology. Technical report, EAGLES (Expert Advisory Group on Language Engineering Standards).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Federico Zanettin</author>
</authors>
<title>Bilingual comparable corpora and the training of translators.</title>
<date>1998</date>
<journal>Meta,</journal>
<volume>43</volume>
<issue>4</issue>
<contexts>
<context position="1967" citStr="Zanettin, 1998" startWordPosition="306" endWordPosition="307">zed web documents collection in French and Japanese, this tool creates the corresponding corpus. 1 Introduction Comparable corpora are sets of texts in different languages, that are not translations, but share some characteristics (Bowker and Pearson, 2002). They represent useful resources from which are extracted multilingual terminologies (Déjean et al., 2002) or multilingual lexicons (Fung and Yee, 1998). Comparable corpora are also used in contrastive multilingual studies framework (Peters and Picchi, 1997), they constitute a precious resource for translators (Laviosa, 1998) and teachers (Zanettin, 1998), as they provide a way to observe languages in use. Their compilation is easier than parallel corpora compilation, because translated resources are rare and there is a lack of resources when the languages involved do not include English. Furthermore, the amount of multilingual documents available on the Web ensures the possibility of automatically compiling them. Nevertheless, this task can not be summarized to a simple collection of documents sharing vocabulary. It is necessary to respect the common characteristics of texts in corpora, established before the compilation, according to the cor</context>
</contexts>
<marker>Zanettin, 1998</marker>
<rawString>Federico Zanettin. 1998. Bilingual comparable corpora and the training of translators. Meta, 43(4):616–630.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>