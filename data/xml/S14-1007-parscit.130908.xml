<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.086909">
<title confidence="0.925776">
Identifying semantic relations in a specialized corpus through
distributional analysis of a cooccurrence tensor
</title>
<author confidence="0.720034">
Gabriel Bernier-Colborne
</author>
<affiliation confidence="0.335760666666667">
OLST (Universit´e de Montr´eal)
C.P. 6128, succ. Centre-Ville
Montr´eal (Qu´ebec) Canada H3C 3J7
</affiliation>
<email confidence="0.984481">
gabriel.bernier-colborne@umontreal.ca
</email>
<sectionHeader confidence="0.993411" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866555555556">
We describe a method of encoding cooc-
currence information in a three-way tensor
from which HAL-style word space mod-
els can be derived. We use these models to
identify semantic relations in a specialized
corpus. Results suggest that the tensor-
based methods we propose are more ro-
bust than the basic HAL model in some
respects.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99945912">
Word space models such as LSA (Landauer and
Dumais, 1997) and HAL (Lund et al., 1995) have
been shown to identify semantic relations from
corpus data quite effectively. However, the per-
formance of such models depends on the parame-
ters used to construct the word space. In the case
of HAL, parameters such as the size of the con-
text window can have a significant impact on the
ability of the model to identify semantic relations
and on the types of relations (e.g. paradigmatic or
syntagmatic) captured.
In this paper, we describe a method of encoding
cooccurrence information which employs a three-
way tensor instead of a matrix. Because the ten-
sor explicitly encodes the distance between a tar-
get word and the context words that co-occur with
it, it allows us to extract matrices corresponding to
HAL models with different context windows with-
out repeatedly processing the whole corpus, but it
also allows us to experiment with different kinds
of word spaces. We describe one method whereby
features are selected in different slices of the ten-
sor corresponding to different distances between
the target and context words, and another which
uses SVD for dimensionality reduction. Models
</bodyText>
<footnote confidence="0.90574875">
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999558142857143">
are evaluated and compared on reference data ex-
tracted from a specialized dictionary of the envi-
ronment domain, as our target application is the
identification of lexico-semantic relations in spe-
cialized corpora. Preliminary results suggest the
tensor-based methods are more robust than the ba-
sic HAL model in some respects.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99991709375">
The tensor encoding method we describe is based
on the Hyperspace Analogue to Language, or
HAL, model (Lund et al., 1995; Lund and
Burgess, 1996), which has been shown to be par-
ticularly effective at modeling paradigmatic rela-
tions such as synonymy. In the HAL model, word
order is taken into account insofar as the word vec-
tors it produces contain information about both the
cooccurrents that precede a word and those that
follow it. In recent years, there have been several
proposals that aim to add word order information
to models that rely mainly on word context infor-
mation (Jones and Mewhort, 2007; Sahlgren et al.,
2008), including models based on multi-way ten-
sors. Symonds et al. (2011) proposed an efficient
tensor encoding method which builds on unstruc-
tured word space models (i.e. models based on
simple cooccurrence rather than syntactic struc-
ture) by adding order information. The method we
describe differs in that it explicitly encodes the dis-
tance between a target word and its cooccurrents.
Multi-way tensors have been used to construct
different kinds of word space models in recent
years. Turney (2007) used a word-word-pattern
tensor to model semantic similarity, Van de Cruys
(2009) used a tensor containing corpus-derived
subject-verb-object triples to model selectional
preferences, and Baroni and Lenci (2010) pro-
posed a general, tensor-based framework for struc-
tured word space models. The tensor encoding
method we describe differs in that it is based on
an unstructured word space model, HAL.
</bodyText>
<page confidence="0.988211">
57
</page>
<note confidence="0.846115">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 57–62,
Dublin, Ireland, August 23-24 2014.
</note>
<sectionHeader confidence="0.992981" genericHeader="method">
3 HAL
</sectionHeader>
<bodyText confidence="0.999989418604651">
The HAL model employs a sliding context win-
dow to compute a word-word cooccurrence ma-
trix, which we will note A, in which value aij is
based on the number of times context word wj ap-
pears within the context window of target word wi.
Thus, words that share cooccurrents will be closer
in word space. If equal weight is given to all con-
text words in the window, regardless of distance,
we call the context window rectangular. In the
original HAL model, the values added to A are
inversely proportional to the distance between the
target word and context word in a given context.
In this case, the context window is triangular.
In the HAL model, the cooccurrence matrix is
computed by considering only the context words
that occur before the target word. Once the ma-
trix has been computed, row vector ai: contains
cooccurrence information about words preceding
wi, and column vector a:i contains information
about those that follow it. The row vector and
column vector of each target word are concate-
nated, such that the resulting word vectors con-
tain information about both left-cooccurrents and
right-cooccurrents. We call this type of context
window directional, following (Sahlgren, 2006),
as opposed to a symetric context window, in which
cooccurrence counts in the left and right contexts
are summed. In our experiment, we only use one
type of context window (directional and rectangu-
lar), but models corresponding to different types
of context windows can be derived from the cooc-
currence tensor we describe in section 4.
Once the values in A have been computed, they
can be weighted using schemes such as TF-ITF
(Lavelli et al., 2004) and Positive Pointwise Mu-
tual Information (PPMI), which we use here as
it has been shown to be particularly effective by
Bullinaria and Levy (2007). Finally, a distance or
similarity measure is used to compare word vec-
tors. Lund and Burgess (1996) use Minkowski
distances. We will use the cosine similarity, as
did Sch¨utze (1992) in a model similar to HAL and
which directly influenced its development.
</bodyText>
<sectionHeader confidence="0.990481" genericHeader="method">
4 The Cooccurrence Tensor
</sectionHeader>
<bodyText confidence="0.999993903846154">
In the following description of the cooccurrence
tensor, we follow the notational guidelines of
(Kolda, 2006), as in (Turney, 2007; Baroni and
Lenci, 2010). Let W be the vocabulary1, which
we index by i to refer to a target word and by j for
context words. Furthermore, let P, indexed by k,
be a set of positions, relative to a target word wi,
in which a context word wj can co-occur with wi.
In other words, this is the signed distance between
wj and wi, in number of words. For instance, in
the sentence “a dog bit the mailman”, we would
say that “dog” co-occurs with “bit” in position −1.
If we only consider the words directly adjacent to
a target word, then P = {−1, +1}. If the tensor
encoding method is used to generate HAL-style
cooccurrence matrices corresponding to different
context windows, then P would include all posi-
tions in the largest window under consideration.
In a cooccurrence matrix A, aij contains the
frequency at which word wj co-occurs with word
wi in a fixed context window. Rather than comput-
ing matrices using fixed-size context windows, we
can construct a cooccurrence tensor X, a labeled
three-way tensor in which values xijk indicate the
frequency at which word wj co-occurs with word
wi in position pk. Table 1 illustrates a cooccur-
rence tensor for the sentence “dogs bite mailmen”
using a context window of 1 (P = {−1, +1}), in
the form of a nested table.
In tensor X, xi:k denotes the row vector of wi
at position pk, x:jk denotes the column vector of
word wj at position pk and xij: denotes the tube
vector indicating the frequency at which wj co-
occurs with wi in each of the positions in P.
HAL-style cooccurrence matrices correspond-
ing to different context windows can be extracted
from the tensor by summing and concatenating
various slices of the tensor. A frontal slice X::k
represents a I × J cooccurrence matrix for po-
sition pk. A cooccurrence matrix corresponding
to a symetric context window of size n can be
extracted by summing the slices X::k for pk ∈
{−n, −n + 1, ... , n}. For a directional window,
we first sum the slices for pk ∈ {−n, ... , −1},
then sum the slices for pk ∈ {1,... , n}, then con-
catenate the 2 resulting matrices horizontally.
Thus, summing and concatenating slices allows
us to extract HAL-style cooccurrence matrices. A
different kind of model can also be obtained by
concatenating slices of the tensor. For instance, if
we concatenate X::k for pk ∈ {−2, −1, +1, +2}
horizontally, we obtain a matrix containing a vec-
</bodyText>
<footnote confidence="0.994852">
1We assume that the target and context words are the same
set, but this need not be the case.
</footnote>
<page confidence="0.998338">
58
</page>
<bodyText confidence="0.8919064">
j=]:dog j=2:bite j=3:mailman
k=1: −1 k=2: +1 k=1: −1 k=2: +1 k=1: −1 k=2: +1
i=]:dog 0 0 0 1 0 0
i=2:bite 1 0 0 0 0 1
i=3:mailman 0 0 1 0 0 0
</bodyText>
<tableCaption confidence="0.995661">
Table 1: A 3 x 3 x 2 cooccurrence tensor.
</tableCaption>
<bodyText confidence="0.999958">
tor of length 4J (instead of the 2J-length vectors
of the HAL model) for each target word, which
encodes cooccurrence information about 4 specific
positions relative to that word. We will refer to this
method as the tensor slicing method. Note that if
P = {−1, 11 the resulting matrix is identical to a
HAL model with context size 1
As the size of the resulting vectors is KJ, this
method can result in very high-dimensional word
vectors. In the original HAL model, Lund et
al. (1995) reduced the dimensionality of the vec-
tors through feature selection, by keeping only the
features that have the highest variance. Sch¨utze
(1992), on the other hand, used truncated SVD for
this purpose. Both techniques can be used with the
tensor slicing method. In our experiment, SVD
was applied to the matrices obtained by concate-
nating tensor slices horizontally2. As for feature
selection, a fixed number of features (those with
the highest variance) were selected from each slice
of the tensor, and these reduced slices were then
concatenated.
It must be acknowledged that this tensor encod-
ing method is not efficient in terms of memory.
However, this was not a major issue in our exper-
imental setting, as the size of the vocabulary was
small (5K words), and we limited the number of
positions in P to 10. Also, a sparse tensor was
used to reduce memory consumption.
</bodyText>
<sectionHeader confidence="0.999784" genericHeader="method">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.998986">
5.1 Corpus and Preprocessing
</subsectionHeader>
<bodyText confidence="0.993929666666666">
In this experiment, we used the PANACEA En-
vironment English monolingual corpus, which is
2We also tried concatenating slices vertically (thus ob-
taining a matrix where rows correspond to &lt;target word,
position&gt; tuples and columns correspond to context words)
before applying SVD, then concatenating all row vectors cor-
responding to the same target word, but we will not report
the results here for lack of space. Concatenating slices hor-
izontally performed better and seems more intuitive, and the
size of the resulting vectors is not dependent on the number
of positions in P.
freely distributed by ELDA for research purposes3
(Catalog Reference ELRA-W0063). This corpus
contains 28071 documents (-50 million tokens)
dealing with different aspects of the environment
domain, harvested from web sites using a focused
crawler. The corpus was converted from XML to
raw text, various string normalization operations
were then applied, and the corpus was lemmatized
using TreeTagger (Schmid, 1994). The vocabu-
lary (W) was selected based on word frequency:
we used the 5000 most frequent words in the cor-
pus, excluding stop words and strings containing
non-alphabetic characters. During computation of
the cooccurrence tensor, OOV words were ignored
(rather than deleted), and the context window was
allowed to span sentence boundaries.
</bodyText>
<subsectionHeader confidence="0.997281">
5.2 Evaluation Data
</subsectionHeader>
<bodyText confidence="0.999972105263158">
Models were evaluated using reference data ex-
tracted from DiCoEnviro4, a specialized dictio-
nary of the environment. This dictionary de-
scribes the meaning and behaviour of terms of
the environment domain as well as the lexico-
semantic relations between these terms. Of the
various relations encoded in the dictionary, we
focused on a subset of three paradigmatic rela-
tions: near-synonyms (terms that have similar
meanings), antonyms (opposite meanings), and
hyponyms (kinds of). 446 pairs containing a head-
word and a related term were extracted from the
dictionary. We then filtered out the pairs that con-
tained at least one OOV term, and were left with
374 pairs containing two paradigmatically-related,
single-word terms. About two thirds (246) of these
examples were used for parameter selection, and
the rest were set aside for a final comparison of
the highest-scoring models.
</bodyText>
<footnote confidence="0.9986426">
3http://catalog.elra.info/product_
info.php?products_id=1184
4http://olst.ling.umontreal.ca/
cgi-bin/dicoenviro/search_enviro.cgi
(under construction).
</footnote>
<page confidence="0.998698">
59
</page>
<subsectionHeader confidence="0.988713">
5.3 Automatic Evaluation
</subsectionHeader>
<bodyText confidence="0.9999867">
Each model was automatically evaluated on the
reference data as follows. For each &lt;headword,
related term&gt; pair in the training set, we computed
the cosine similarity between the headword and
all other words in the vocabulary, then observed
the rank of the related term in the sorted list of
neighbours. The score used to compare models
is recall at k (R@k), which is the percentage of
cases where the related term is among the k near-
est neighbours of the headword. It should be noted
that a score of 100% is not always possible in this
setting (depending on the value of k), as some
headwords have more than 1 related term in the
reference data. Nonetheless, since most (∼70%)
have 1 or 2 related terms, R@k for some small
value of k (we use k = 10) should be a good indica-
tor of accuracy. A measure that explicitly accounts
for the fact that different terms have different num-
bers of related terms (e.g. R-precision) would be a
good alternative.
</bodyText>
<subsectionHeader confidence="0.998108">
5.4 Models Tested
</subsectionHeader>
<bodyText confidence="0.999218863636363">
We compared HAL and the tensor slicing
method using either feature selection or SVD5,
as explained in section 4. We will refer to
each of these models as HALSEL, TNSRSEL,
HALSVD and TNSRSVD. Context sizes ranged
from 1 to 5 words. For feature selection,
the number of features could take values in
{1000, 2000,..., 10000}, 10000 being the max-
imum number of features in a HAL model us-
ing a vocabulary of 5000 words. In the case
of TNSRSEL, to determine the number of fea-
tures selected per slice, we took each value in
{1000, 2000,..., 10000}, divided it by K (the
number of positions in P), and rounded down.
This way, once the slices are concatenated, the
total number of features is equal to (or slightly
less than) that of one of the HALSEL mod-
els, allowing for a straightforward comparison.
When SVD was used instead of feature selection,
the number of components could take values in
{100, 200, ...,1000}. In all cases, word vectors
were weighted using PPMI and normalized6.
</bodyText>
<footnote confidence="0.961178625">
5We used the SVD implementation (ARPACK solver)
provided in the scikit-learn toolkit (Pedregosa et al., 2011).
6For HALSEL and TNSRSEL, we apply PPMI weighting
after feature selection. In the case of TNSRSEL, we wanted
to avoid weighting each slice of the tensor separately. We
decided to apply weighting after feature selection in the case
of HALSEL as well in order to enable a more straightforward
comparison. We should also note that, in our experiments
</footnote>
<table confidence="0.999569818181818">
absorb extreme precipitation
emit severe rainfall
sequester intense snowfall
convert harsh temperature
produce catastrophic rain
accumulate unusual evaporation
store seasonal runoff
radiate mild moisture
consume cold snow
remove dramatic weather
reflect increase deposition
</table>
<tableCaption confidence="0.9719675">
Table 2: 10 nearest neighbours of 3 environmental
terms using the HALSEL model.
</tableCaption>
<sectionHeader confidence="0.999522" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999973689655172">
Table 2 illustrates the kinds of relations identified
by the basic HALSEL model. It shows the 10 near-
est neighbours of the verb absorb, the adjective
extreme and the noun precipitation. If we com-
pare these results with the paradigmatic relations
encoded in DiCoEnviro, we see that, in the case
of absorb, 3 of its neighbours are encoded in the
dictionary, and all 3 are antonyms or terms having
opposite meanings: emit, radiate, and reflect. As
for extreme, the top 2 neighbours are both encoded
in the dictionary as near-synonyms. Finally, rain
and snow are both encoded as kinds of precipita-
tion. Most of the other neighbours shown here are
also paradigmatically related to the query terms.
Thus, HAL seems quite capable of identifying the
three types of paradigmatic relations we hoped to
identify.
Table 3 shows the best R@10 achieved by each
model on the training set, which was used to tune
the context size and number of features or compo-
nents, and their scores on the test set, which was
only used to compare the best models. In the case
of HALSEL, the best model has a context window
size of 1 and uses 9K out of 10K available features.
As for TNSRSEL, the best model had a context size
of 2 (P = {−2, −1, +1, +2}) and 10000 features
(2500 per slice). It performed only slightly better
on the training set, however it beat the HAL model
with a wider margin on the test set.
</bodyText>
<footnote confidence="0.9100075">
using HAL, PPMI weighting performed better when applied
after feature selection, especially for low numbers of features.
</footnote>
<page confidence="0.989663">
60
</page>
<figure confidence="0.998265571428571">
(a) (b)
10
1
2
3
4
5
</figure>
<figureCaption confidence="0.9983625">
Figure 1: HAL vs. tensor slicing method using SVD for dimensionality reduction. R@10 is plotted
against number of components. Models are identical when context size is 1. (a) HALSVD (b) TNSRSVD
</figureCaption>
<table confidence="0.9995438">
Model Train Test
HALSEL 60.57 57.03
TNSRSEL 60.98 60.94
HALSVD 59.76 56.25
TNSRSVD 60.57 60.16
</table>
<tableCaption confidence="0.999441">
Table 3: R@10 (%) of best models.
</tableCaption>
<bodyText confidence="0.999984208333333">
The best HALSVD model used a 1-word window
and 1000 components, whereas the best TNSRSVD
model had a context size of 2 and 800 components.
Again, the tensor-based model slightly edged out
the HAL model on the training set, but performed
considerably better on the test set.
Further analysis of the results indeed suggests
that the tensor slicing method is more robust in
some respects than the basic HAL model. Fig-
ure 1 compares the performance of HALSVD and
TNSRSVD on the training set, taking into account
context size and number of components. It shows
that the HAL model is quite sensitive to context
size, narrower context performing better in this
task. The tensor-based method reduces this gap in
performance between context sizes, the gain being
greater for larger context sizes. Furthermore, us-
ing the tensor-based method with a slightly wider
context (2) raises R@10 for most values of the
number of components. Results obtained with
HALSEL and TNSRSEL follow the same trend, the
tensor-based method being more robust with re-
spect to context size. For lack of space, we only
show the plot comparing HALSVD and TNSRSVD.
</bodyText>
<sectionHeader confidence="0.989279" genericHeader="conclusions">
7 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999949666666667">
The work presented in this paper is still in its ex-
ploratory phase. The tensor slicing method we
described has only been evaluated on one corpus
and one set of reference data. Experiments would
need to be carried out on common word space
evaluation tasks in order to compare its perfor-
mance to that of HAL and other word space mod-
els. However, our results suggest that the tensor-
based methods are more robust than the basic HAL
model to a certain extent, and can improve accu-
racy. This could prove especially useful in settings
where no reference data are available for parame-
ter tuning.
Various possibilities offered by the cooccur-
rence tensor remain to be explored, such as
weighting the number of features selected per
slice using some function of the distance between
words, extracting matrices from the tensor by ap-
plying various functions to the tube vectors corre-
sponding to each word pair, and applying weight-
ing functions that have been generalized to higher-
order tensors (Van de Cruys, 2011) or tensor de-
composition methods such as those described in
(Turney, 2007).
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99586925">
We would like to thank the anonymous reviewers
for their helpful and thorough comments. Funding
was provided by the Social Sciences and Humani-
ties Research Council of Canada.
</bodyText>
<page confidence="0.998985">
61
</page>
<sectionHeader confidence="0.990268" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999394743589744">
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673–721.
John A Bullinaria and Joseph P Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39(3):510–526.
Michael N Jones and Douglas JK Mewhort. 2007.
Representing word meaning and order information
in a composite holographic lexicon. Psychological
Review, 114(1):1–37.
Tamara Gibson Kolda. 2006. Multilinear operators
for higher-order decompositions. Technical Report
SAND2006-2081, Sandia National Laboratories.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to Plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211.
Alberto Lavelli, Fabrizio Sebastiani, and Roberto
Zanoli. 2004. Distributional term representations:
An experimental comparison. In Proceedings of
the 13th ACM International Conference on Informa-
tion and Knowledge Management, pages 615–624.
ACM.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, &amp; Computers, 28(2):203–208.
Kevin Lund, Curt Burgess, and Ruth Ann Atchley.
1995. Semantic and associative priming in high-
dimensional semantic space. In Proceedings of the
17th Annual Conference of the Cognitive Science
Society, volume 17, pages 660–665.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode order
in word space. In Proceedings of the 30th Annual
Conference of the Cognitive Science Society, pages
1300–1305. Cognitive Science Society.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Hinrich Sch¨utze. 1992. Dimensions of meaning. In
Proceedings of the 1992 ACM/IEEE Conference on
Supercomputing (Supercomputing’92), pages 787–
796. IEEE Computer Society Press.
Michael Symonds, Peter D Bruza, Laurianne Sitbon,
and Ian Turner. 2011. Modelling word meaning us-
ing efficient tensor representations. In Proceedings
of 25th Pacific Asia Conference on Language, Infor-
mation and Computation.
Peter Turney. 2007. Empirical evaluation of four
tensor decomposition algorithms. Technical Report
ERB-1152, National Research Council of Canada,
Ottawa.
Tim Van de Cruys. 2009. A non-negative tensor fac-
torization model for selectional preference induc-
tion. In Proceedings of the Workshop on Geomet-
rical Models of Natural Language Semantics, pages
83–90. ACL.
Tim Van de Cruys. 2011. Two multivariate general-
izations of pointwise mutual information. In Pro-
ceedings of the Workshop on Distributional Seman-
tics and Compositionality, pages 16–20. ACL.
</reference>
<page confidence="0.999178">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.247186">
<title confidence="0.774631">Identifying semantic relations in a specialized corpus distributional analysis of a cooccurrence tensor</title>
<author confidence="0.721489">Gabriel</author>
<note confidence="0.800052333333333">OLST (Universit´e de C.P. 6128, succ. Montr´eal (Qu´ebec) Canada H3C</note>
<email confidence="0.951758">gabriel.bernier-colborne@umontreal.ca</email>
<abstract confidence="0.994925">We describe a method of encoding cooccurrence information in a three-way tensor from which HAL-style word space models can be derived. We use these models to identify semantic relations in a specialized corpus. Results suggest that the tensorbased methods we propose are more robust than the basic HAL model in some respects.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="3697" citStr="Baroni and Lenci (2010)" startWordPosition="575" endWordPosition="578">cient tensor encoding method which builds on unstructured word space models (i.e. models based on simple cooccurrence rather than syntactic structure) by adding order information. The method we describe differs in that it explicitly encodes the distance between a target word and its cooccurrents. Multi-way tensors have been used to construct different kinds of word space models in recent years. Turney (2007) used a word-word-pattern tensor to model semantic similarity, Van de Cruys (2009) used a tensor containing corpus-derived subject-verb-object triples to model selectional preferences, and Baroni and Lenci (2010) proposed a general, tensor-based framework for structured word space models. The tensor encoding method we describe differs in that it is based on an unstructured word space model, HAL. 57 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 57–62, Dublin, Ireland, August 23-24 2014. 3 HAL The HAL model employs a sliding context window to compute a word-word cooccurrence matrix, which we will note A, in which value aij is based on the number of times context word wj appears within the context window of target word wi. Thus, words that share coocc</context>
<context position="6261" citStr="Baroni and Lenci, 2010" startWordPosition="1001" endWordPosition="1004">such as TF-ITF (Lavelli et al., 2004) and Positive Pointwise Mutual Information (PPMI), which we use here as it has been shown to be particularly effective by Bullinaria and Levy (2007). Finally, a distance or similarity measure is used to compare word vectors. Lund and Burgess (1996) use Minkowski distances. We will use the cosine similarity, as did Sch¨utze (1992) in a model similar to HAL and which directly influenced its development. 4 The Cooccurrence Tensor In the following description of the cooccurrence tensor, we follow the notational guidelines of (Kolda, 2006), as in (Turney, 2007; Baroni and Lenci, 2010). Let W be the vocabulary1, which we index by i to refer to a target word and by j for context words. Furthermore, let P, indexed by k, be a set of positions, relative to a target word wi, in which a context word wj can co-occur with wi. In other words, this is the signed distance between wj and wi, in number of words. For instance, in the sentence “a dog bit the mailman”, we would say that “dog” co-occurs with “bit” in position −1. If we only consider the words directly adjacent to a target word, then P = {−1, +1}. If the tensor encoding method is used to generate HAL-style cooccurrence matri</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="5823" citStr="Bullinaria and Levy (2007)" startWordPosition="931" endWordPosition="934">, following (Sahlgren, 2006), as opposed to a symetric context window, in which cooccurrence counts in the left and right contexts are summed. In our experiment, we only use one type of context window (directional and rectangular), but models corresponding to different types of context windows can be derived from the cooccurrence tensor we describe in section 4. Once the values in A have been computed, they can be weighted using schemes such as TF-ITF (Lavelli et al., 2004) and Positive Pointwise Mutual Information (PPMI), which we use here as it has been shown to be particularly effective by Bullinaria and Levy (2007). Finally, a distance or similarity measure is used to compare word vectors. Lund and Burgess (1996) use Minkowski distances. We will use the cosine similarity, as did Sch¨utze (1992) in a model similar to HAL and which directly influenced its development. 4 The Cooccurrence Tensor In the following description of the cooccurrence tensor, we follow the notational guidelines of (Kolda, 2006), as in (Turney, 2007; Baroni and Lenci, 2010). Let W be the vocabulary1, which we index by i to refer to a target word and by j for context words. Furthermore, let P, indexed by k, be a set of positions, rel</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A Bullinaria and Joseph P Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39(3):510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael N Jones</author>
<author>Douglas JK Mewhort</author>
</authors>
<title>Representing word meaning and order information in a composite holographic lexicon.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>1</issue>
<contexts>
<context position="2965" citStr="Jones and Mewhort, 2007" startWordPosition="465" endWordPosition="468">ts. 2 Related Work The tensor encoding method we describe is based on the Hyperspace Analogue to Language, or HAL, model (Lund et al., 1995; Lund and Burgess, 1996), which has been shown to be particularly effective at modeling paradigmatic relations such as synonymy. In the HAL model, word order is taken into account insofar as the word vectors it produces contain information about both the cooccurrents that precede a word and those that follow it. In recent years, there have been several proposals that aim to add word order information to models that rely mainly on word context information (Jones and Mewhort, 2007; Sahlgren et al., 2008), including models based on multi-way tensors. Symonds et al. (2011) proposed an efficient tensor encoding method which builds on unstructured word space models (i.e. models based on simple cooccurrence rather than syntactic structure) by adding order information. The method we describe differs in that it explicitly encodes the distance between a target word and its cooccurrents. Multi-way tensors have been used to construct different kinds of word space models in recent years. Turney (2007) used a word-word-pattern tensor to model semantic similarity, Van de Cruys (200</context>
</contexts>
<marker>Jones, Mewhort, 2007</marker>
<rawString>Michael N Jones and Douglas JK Mewhort. 2007. Representing word meaning and order information in a composite holographic lexicon. Psychological Review, 114(1):1–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Gibson Kolda</author>
</authors>
<title>Multilinear operators for higher-order decompositions.</title>
<date>2006</date>
<tech>Technical Report SAND2006-2081,</tech>
<institution>Sandia National Laboratories.</institution>
<contexts>
<context position="6215" citStr="Kolda, 2006" startWordPosition="995" endWordPosition="996">they can be weighted using schemes such as TF-ITF (Lavelli et al., 2004) and Positive Pointwise Mutual Information (PPMI), which we use here as it has been shown to be particularly effective by Bullinaria and Levy (2007). Finally, a distance or similarity measure is used to compare word vectors. Lund and Burgess (1996) use Minkowski distances. We will use the cosine similarity, as did Sch¨utze (1992) in a model similar to HAL and which directly influenced its development. 4 The Cooccurrence Tensor In the following description of the cooccurrence tensor, we follow the notational guidelines of (Kolda, 2006), as in (Turney, 2007; Baroni and Lenci, 2010). Let W be the vocabulary1, which we index by i to refer to a target word and by j for context words. Furthermore, let P, indexed by k, be a set of positions, relative to a target word wi, in which a context word wj can co-occur with wi. In other words, this is the signed distance between wj and wi, in number of words. For instance, in the sentence “a dog bit the mailman”, we would say that “dog” co-occurs with “bit” in position −1. If we only consider the words directly adjacent to a target word, then P = {−1, +1}. If the tensor encoding method is</context>
</contexts>
<marker>Kolda, 2006</marker>
<rawString>Tamara Gibson Kolda. 2006. Multilinear operators for higher-order decompositions. Technical Report SAND2006-2081, Sandia National Laboratories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="679" citStr="Landauer and Dumais, 1997" startWordPosition="95" endWordPosition="98">s through distributional analysis of a cooccurrence tensor Gabriel Bernier-Colborne OLST (Universit´e de Montr´eal) C.P. 6128, succ. Centre-Ville Montr´eal (Qu´ebec) Canada H3C 3J7 gabriel.bernier-colborne@umontreal.ca Abstract We describe a method of encoding cooccurrence information in a three-way tensor from which HAL-style word space models can be derived. We use these models to identify semantic relations in a specialized corpus. Results suggest that the tensorbased methods we propose are more robust than the basic HAL model in some respects. 1 Introduction Word space models such as LSA (Landauer and Dumais, 1997) and HAL (Lund et al., 1995) have been shown to identify semantic relations from corpus data quite effectively. However, the performance of such models depends on the parameters used to construct the word space. In the case of HAL, parameters such as the size of the context window can have a significant impact on the ability of the model to identify semantic relations and on the types of relations (e.g. paradigmatic or syntagmatic) captured. In this paper, we describe a method of encoding cooccurrence information which employs a threeway tensor instead of a matrix. Because the tensor explicitl</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Lavelli</author>
<author>Fabrizio Sebastiani</author>
<author>Roberto Zanoli</author>
</authors>
<title>Distributional term representations: An experimental comparison.</title>
<date>2004</date>
<booktitle>In Proceedings of the 13th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>615--624</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5675" citStr="Lavelli et al., 2004" startWordPosition="906" endWordPosition="909">resulting word vectors contain information about both left-cooccurrents and right-cooccurrents. We call this type of context window directional, following (Sahlgren, 2006), as opposed to a symetric context window, in which cooccurrence counts in the left and right contexts are summed. In our experiment, we only use one type of context window (directional and rectangular), but models corresponding to different types of context windows can be derived from the cooccurrence tensor we describe in section 4. Once the values in A have been computed, they can be weighted using schemes such as TF-ITF (Lavelli et al., 2004) and Positive Pointwise Mutual Information (PPMI), which we use here as it has been shown to be particularly effective by Bullinaria and Levy (2007). Finally, a distance or similarity measure is used to compare word vectors. Lund and Burgess (1996) use Minkowski distances. We will use the cosine similarity, as did Sch¨utze (1992) in a model similar to HAL and which directly influenced its development. 4 The Cooccurrence Tensor In the following description of the cooccurrence tensor, we follow the notational guidelines of (Kolda, 2006), as in (Turney, 2007; Baroni and Lenci, 2010). Let W be the</context>
</contexts>
<marker>Lavelli, Sebastiani, Zanoli, 2004</marker>
<rawString>Alberto Lavelli, Fabrizio Sebastiani, and Roberto Zanoli. 2004. Distributional term representations: An experimental comparison. In Proceedings of the 13th ACM International Conference on Information and Knowledge Management, pages 615–624. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="2506" citStr="Lund and Burgess, 1996" startWordPosition="386" endWordPosition="389">l Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ are evaluated and compared on reference data extracted from a specialized dictionary of the environment domain, as our target application is the identification of lexico-semantic relations in specialized corpora. Preliminary results suggest the tensor-based methods are more robust than the basic HAL model in some respects. 2 Related Work The tensor encoding method we describe is based on the Hyperspace Analogue to Language, or HAL, model (Lund et al., 1995; Lund and Burgess, 1996), which has been shown to be particularly effective at modeling paradigmatic relations such as synonymy. In the HAL model, word order is taken into account insofar as the word vectors it produces contain information about both the cooccurrents that precede a word and those that follow it. In recent years, there have been several proposals that aim to add word order information to models that rely mainly on word context information (Jones and Mewhort, 2007; Sahlgren et al., 2008), including models based on multi-way tensors. Symonds et al. (2011) proposed an efficient tensor encoding method whi</context>
<context position="5923" citStr="Lund and Burgess (1996)" startWordPosition="948" endWordPosition="951">the left and right contexts are summed. In our experiment, we only use one type of context window (directional and rectangular), but models corresponding to different types of context windows can be derived from the cooccurrence tensor we describe in section 4. Once the values in A have been computed, they can be weighted using schemes such as TF-ITF (Lavelli et al., 2004) and Positive Pointwise Mutual Information (PPMI), which we use here as it has been shown to be particularly effective by Bullinaria and Levy (2007). Finally, a distance or similarity measure is used to compare word vectors. Lund and Burgess (1996) use Minkowski distances. We will use the cosine similarity, as did Sch¨utze (1992) in a model similar to HAL and which directly influenced its development. 4 The Cooccurrence Tensor In the following description of the cooccurrence tensor, we follow the notational guidelines of (Kolda, 2006), as in (Turney, 2007; Baroni and Lenci, 2010). Let W be the vocabulary1, which we index by i to refer to a target word and by j for context words. Furthermore, let P, indexed by k, be a set of positions, relative to a target word wi, in which a context word wj can co-occur with wi. In other words, this is </context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, &amp; Computers, 28(2):203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
<author>Ruth Ann Atchley</author>
</authors>
<title>Semantic and associative priming in highdimensional semantic space.</title>
<date>1995</date>
<booktitle>In Proceedings of the 17th Annual Conference of the Cognitive Science Society,</booktitle>
<volume>17</volume>
<pages>660--665</pages>
<contexts>
<context position="707" citStr="Lund et al., 1995" startWordPosition="101" endWordPosition="104"> a cooccurrence tensor Gabriel Bernier-Colborne OLST (Universit´e de Montr´eal) C.P. 6128, succ. Centre-Ville Montr´eal (Qu´ebec) Canada H3C 3J7 gabriel.bernier-colborne@umontreal.ca Abstract We describe a method of encoding cooccurrence information in a three-way tensor from which HAL-style word space models can be derived. We use these models to identify semantic relations in a specialized corpus. Results suggest that the tensorbased methods we propose are more robust than the basic HAL model in some respects. 1 Introduction Word space models such as LSA (Landauer and Dumais, 1997) and HAL (Lund et al., 1995) have been shown to identify semantic relations from corpus data quite effectively. However, the performance of such models depends on the parameters used to construct the word space. In the case of HAL, parameters such as the size of the context window can have a significant impact on the ability of the model to identify semantic relations and on the types of relations (e.g. paradigmatic or syntagmatic) captured. In this paper, we describe a method of encoding cooccurrence information which employs a threeway tensor instead of a matrix. Because the tensor explicitly encodes the distance betwe</context>
<context position="2481" citStr="Lund et al., 1995" startWordPosition="382" endWordPosition="385">on 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ are evaluated and compared on reference data extracted from a specialized dictionary of the environment domain, as our target application is the identification of lexico-semantic relations in specialized corpora. Preliminary results suggest the tensor-based methods are more robust than the basic HAL model in some respects. 2 Related Work The tensor encoding method we describe is based on the Hyperspace Analogue to Language, or HAL, model (Lund et al., 1995; Lund and Burgess, 1996), which has been shown to be particularly effective at modeling paradigmatic relations such as synonymy. In the HAL model, word order is taken into account insofar as the word vectors it produces contain information about both the cooccurrents that precede a word and those that follow it. In recent years, there have been several proposals that aim to add word order information to models that rely mainly on word context information (Jones and Mewhort, 2007; Sahlgren et al., 2008), including models based on multi-way tensors. Symonds et al. (2011) proposed an efficient t</context>
<context position="9359" citStr="Lund et al. (1995)" startWordPosition="1575" endWordPosition="1578">−1 k=2: +1 k=1: −1 k=2: +1 i=]:dog 0 0 0 1 0 0 i=2:bite 1 0 0 0 0 1 i=3:mailman 0 0 1 0 0 0 Table 1: A 3 x 3 x 2 cooccurrence tensor. tor of length 4J (instead of the 2J-length vectors of the HAL model) for each target word, which encodes cooccurrence information about 4 specific positions relative to that word. We will refer to this method as the tensor slicing method. Note that if P = {−1, 11 the resulting matrix is identical to a HAL model with context size 1 As the size of the resulting vectors is KJ, this method can result in very high-dimensional word vectors. In the original HAL model, Lund et al. (1995) reduced the dimensionality of the vectors through feature selection, by keeping only the features that have the highest variance. Sch¨utze (1992), on the other hand, used truncated SVD for this purpose. Both techniques can be used with the tensor slicing method. In our experiment, SVD was applied to the matrices obtained by concatenating tensor slices horizontally2. As for feature selection, a fixed number of features (those with the highest variance) were selected from each slice of the tensor, and these reduced slices were then concatenated. It must be acknowledged that this tensor encoding</context>
</contexts>
<marker>Lund, Burgess, Atchley, 1995</marker>
<rawString>Kevin Lund, Curt Burgess, and Ruth Ann Atchley. 1995. Semantic and associative priming in highdimensional semantic space. In Proceedings of the 17th Annual Conference of the Cognitive Science Society, volume 17, pages 660–665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="14740" citStr="Pedregosa et al., 2011" startWordPosition="2445" endWordPosition="2448"> features selected per slice, we took each value in {1000, 2000,..., 10000}, divided it by K (the number of positions in P), and rounded down. This way, once the slices are concatenated, the total number of features is equal to (or slightly less than) that of one of the HALSEL models, allowing for a straightforward comparison. When SVD was used instead of feature selection, the number of components could take values in {100, 200, ...,1000}. In all cases, word vectors were weighted using PPMI and normalized6. 5We used the SVD implementation (ARPACK solver) provided in the scikit-learn toolkit (Pedregosa et al., 2011). 6For HALSEL and TNSRSEL, we apply PPMI weighting after feature selection. In the case of TNSRSEL, we wanted to avoid weighting each slice of the tensor separately. We decided to apply weighting after feature selection in the case of HALSEL as well in order to enable a more straightforward comparison. We should also note that, in our experiments absorb extreme precipitation emit severe rainfall sequester intense snowfall convert harsh temperature produce catastrophic rain accumulate unusual evaporation store seasonal runoff radiate mild moisture consume cold snow remove dramatic weather refle</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
<author>Anders Holst</author>
<author>Pentti Kanerva</author>
</authors>
<title>Permutations as a means to encode order in word space.</title>
<date>2008</date>
<booktitle>In Proceedings of the 30th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1300--1305</pages>
<publisher>Cognitive Science Society.</publisher>
<contexts>
<context position="2989" citStr="Sahlgren et al., 2008" startWordPosition="469" endWordPosition="472">nsor encoding method we describe is based on the Hyperspace Analogue to Language, or HAL, model (Lund et al., 1995; Lund and Burgess, 1996), which has been shown to be particularly effective at modeling paradigmatic relations such as synonymy. In the HAL model, word order is taken into account insofar as the word vectors it produces contain information about both the cooccurrents that precede a word and those that follow it. In recent years, there have been several proposals that aim to add word order information to models that rely mainly on word context information (Jones and Mewhort, 2007; Sahlgren et al., 2008), including models based on multi-way tensors. Symonds et al. (2011) proposed an efficient tensor encoding method which builds on unstructured word space models (i.e. models based on simple cooccurrence rather than syntactic structure) by adding order information. The method we describe differs in that it explicitly encodes the distance between a target word and its cooccurrents. Multi-way tensors have been used to construct different kinds of word space models in recent years. Turney (2007) used a word-word-pattern tensor to model semantic similarity, Van de Cruys (2009) used a tensor contain</context>
</contexts>
<marker>Sahlgren, Holst, Kanerva, 2008</marker>
<rawString>Magnus Sahlgren, Anders Holst, and Pentti Kanerva. 2008. Permutations as a means to encode order in word space. In Proceedings of the 30th Annual Conference of the Cognitive Science Society, pages 1300–1305. Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Stockholm University.</institution>
<contexts>
<context position="5225" citStr="Sahlgren, 2006" startWordPosition="831" endWordPosition="832">t. In this case, the context window is triangular. In the HAL model, the cooccurrence matrix is computed by considering only the context words that occur before the target word. Once the matrix has been computed, row vector ai: contains cooccurrence information about words preceding wi, and column vector a:i contains information about those that follow it. The row vector and column vector of each target word are concatenated, such that the resulting word vectors contain information about both left-cooccurrents and right-cooccurrents. We call this type of context window directional, following (Sahlgren, 2006), as opposed to a symetric context window, in which cooccurrence counts in the left and right contexts are summed. In our experiment, we only use one type of context window (directional and rectangular), but models corresponding to different types of context windows can be derived from the cooccurrence tensor we describe in section 4. Once the values in A have been computed, they can be weighted using schemes such as TF-ITF (Lavelli et al., 2004) and Positive Pointwise Mutual Information (PPMI), which we use here as it has been shown to be particularly effective by Bullinaria and Levy (2007). </context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces. Ph.D. thesis, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="11267" citStr="Schmid, 1994" startWordPosition="1882" endWordPosition="1883"> results here for lack of space. Concatenating slices horizontally performed better and seems more intuitive, and the size of the resulting vectors is not dependent on the number of positions in P. freely distributed by ELDA for research purposes3 (Catalog Reference ELRA-W0063). This corpus contains 28071 documents (-50 million tokens) dealing with different aspects of the environment domain, harvested from web sites using a focused crawler. The corpus was converted from XML to raw text, various string normalization operations were then applied, and the corpus was lemmatized using TreeTagger (Schmid, 1994). The vocabulary (W) was selected based on word frequency: we used the 5000 most frequent words in the corpus, excluding stop words and strings containing non-alphabetic characters. During computation of the cooccurrence tensor, OOV words were ignored (rather than deleted), and the context window was allowed to span sentence boundaries. 5.2 Evaluation Data Models were evaluated using reference data extracted from DiCoEnviro4, a specialized dictionary of the environment. This dictionary describes the meaning and behaviour of terms of the environment domain as well as the lexicosemantic relation</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Dimensions of meaning.</title>
<date>1992</date>
<booktitle>In Proceedings of the 1992 ACM/IEEE Conference on Supercomputing (Supercomputing’92),</booktitle>
<pages>787--796</pages>
<publisher>IEEE Computer Society Press.</publisher>
<marker>Sch¨utze, 1992</marker>
<rawString>Hinrich Sch¨utze. 1992. Dimensions of meaning. In Proceedings of the 1992 ACM/IEEE Conference on Supercomputing (Supercomputing’92), pages 787– 796. IEEE Computer Society Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Symonds</author>
<author>Peter D Bruza</author>
<author>Laurianne Sitbon</author>
<author>Ian Turner</author>
</authors>
<title>Modelling word meaning using efficient tensor representations.</title>
<date>2011</date>
<booktitle>In Proceedings of 25th Pacific Asia Conference on Language, Information and Computation.</booktitle>
<contexts>
<context position="3057" citStr="Symonds et al. (2011)" startWordPosition="480" endWordPosition="483">to Language, or HAL, model (Lund et al., 1995; Lund and Burgess, 1996), which has been shown to be particularly effective at modeling paradigmatic relations such as synonymy. In the HAL model, word order is taken into account insofar as the word vectors it produces contain information about both the cooccurrents that precede a word and those that follow it. In recent years, there have been several proposals that aim to add word order information to models that rely mainly on word context information (Jones and Mewhort, 2007; Sahlgren et al., 2008), including models based on multi-way tensors. Symonds et al. (2011) proposed an efficient tensor encoding method which builds on unstructured word space models (i.e. models based on simple cooccurrence rather than syntactic structure) by adding order information. The method we describe differs in that it explicitly encodes the distance between a target word and its cooccurrents. Multi-way tensors have been used to construct different kinds of word space models in recent years. Turney (2007) used a word-word-pattern tensor to model semantic similarity, Van de Cruys (2009) used a tensor containing corpus-derived subject-verb-object triples to model selectional </context>
</contexts>
<marker>Symonds, Bruza, Sitbon, Turner, 2011</marker>
<rawString>Michael Symonds, Peter D Bruza, Laurianne Sitbon, and Ian Turner. 2011. Modelling word meaning using efficient tensor representations. In Proceedings of 25th Pacific Asia Conference on Language, Information and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Empirical evaluation of four tensor decomposition algorithms.</title>
<date>2007</date>
<tech>Technical Report ERB-1152,</tech>
<institution>National Research Council of Canada,</institution>
<location>Ottawa.</location>
<contexts>
<context position="3485" citStr="Turney (2007)" startWordPosition="549" endWordPosition="550">r information to models that rely mainly on word context information (Jones and Mewhort, 2007; Sahlgren et al., 2008), including models based on multi-way tensors. Symonds et al. (2011) proposed an efficient tensor encoding method which builds on unstructured word space models (i.e. models based on simple cooccurrence rather than syntactic structure) by adding order information. The method we describe differs in that it explicitly encodes the distance between a target word and its cooccurrents. Multi-way tensors have been used to construct different kinds of word space models in recent years. Turney (2007) used a word-word-pattern tensor to model semantic similarity, Van de Cruys (2009) used a tensor containing corpus-derived subject-verb-object triples to model selectional preferences, and Baroni and Lenci (2010) proposed a general, tensor-based framework for structured word space models. The tensor encoding method we describe differs in that it is based on an unstructured word space model, HAL. 57 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 57–62, Dublin, Ireland, August 23-24 2014. 3 HAL The HAL model employs a sliding context window to</context>
<context position="6236" citStr="Turney, 2007" startWordPosition="999" endWordPosition="1000">using schemes such as TF-ITF (Lavelli et al., 2004) and Positive Pointwise Mutual Information (PPMI), which we use here as it has been shown to be particularly effective by Bullinaria and Levy (2007). Finally, a distance or similarity measure is used to compare word vectors. Lund and Burgess (1996) use Minkowski distances. We will use the cosine similarity, as did Sch¨utze (1992) in a model similar to HAL and which directly influenced its development. 4 The Cooccurrence Tensor In the following description of the cooccurrence tensor, we follow the notational guidelines of (Kolda, 2006), as in (Turney, 2007; Baroni and Lenci, 2010). Let W be the vocabulary1, which we index by i to refer to a target word and by j for context words. Furthermore, let P, indexed by k, be a set of positions, relative to a target word wi, in which a context word wj can co-occur with wi. In other words, this is the signed distance between wj and wi, in number of words. For instance, in the sentence “a dog bit the mailman”, we would say that “dog” co-occurs with “bit” in position −1. If we only consider the words directly adjacent to a target word, then P = {−1, +1}. If the tensor encoding method is used to generate HAL</context>
</contexts>
<marker>Turney, 2007</marker>
<rawString>Peter Turney. 2007. Empirical evaluation of four tensor decomposition algorithms. Technical Report ERB-1152, National Research Council of Canada, Ottawa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>A non-negative tensor factorization model for selectional preference induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>83--90</pages>
<publisher>ACL.</publisher>
<marker>Van de Cruys, 2009</marker>
<rawString>Tim Van de Cruys. 2009. A non-negative tensor factorization model for selectional preference induction. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 83–90. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>Two multivariate generalizations of pointwise mutual information.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Distributional Semantics and Compositionality,</booktitle>
<pages>16--20</pages>
<publisher>ACL.</publisher>
<marker>Van de Cruys, 2011</marker>
<rawString>Tim Van de Cruys. 2011. Two multivariate generalizations of pointwise mutual information. In Proceedings of the Workshop on Distributional Semantics and Compositionality, pages 16–20. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>