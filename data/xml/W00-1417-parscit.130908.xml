<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999023">
Content Aggregation in Natural Language Hypertext
Summarization of OLAP and Data Mining Discoveries
</title>
<author confidence="0.965145">
Jacques Robin
</author>
<affiliation confidence="0.8647695">
Universidade Federal de Pernambuco (UFPE)
Centro de Informatica (On)
</affiliation>
<address confidence="0.810589">
Caixa Postal 7851
50732-970 — Recife, Brazil
</address>
<email confidence="0.988219">
jr@di.ufpe.br
</email>
<author confidence="0.562346">
Eloi L. Favero
</author>
<affiliation confidence="0.485958">
Universidade Federal do Para (UFPA)
Departamento de Informatica (DI)
</affiliation>
<address confidence="0.7237685">
Campus do .Cruarna
66075-900 — Belem, Para.
</address>
<email confidence="0.960476">
ellf@di.ufpe.br
</email>
<sectionHeader confidence="0.994475" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994007866666667">
We present a new approach to paratactic
content aggregation in the context of
generating hypertext summaries of OLAP
and data mining discoveries. Two key
properties make this approach innovative
and interesting: (1) it encapsulates
aggregation inside the sentence planning
component, and (2) it relies on a domain
independent algorithm working on a data
structure that abstracts from lexical and
syntactic knowledge.
1 Research context: hypertext
executive summary generation for
intelligent decision-support
In this paper, we present a new approach to
content aggregation in Natural Nanguage
Generation (NLG). This approach has been
developed for the NLG system HYSSOP
(HYpertext Summary System of On-line
analytical Processing) which summarizes OLAP
(On-Line Analytical Processing) and Data
Mining discoveries into an hypertext report.
HYSSOP is itself part of the Intelligent
Decision-Support System (IDSS) MATRIKS
(Multidimensional Analysis and Textual
Reporting for Insight Knowledge Search), which
aims to provide a comprehensive knowledge
discovery environment through seamless
integration of data warehousing, OLAP, data
mining, expert system and NLG technologies.
</bodyText>
<sectionHeader confidence="0.528227" genericHeader="method">
1.1 The MATRIKS intelligent decision-
support system
</sectionHeader>
<bodyText confidence="0.998461">
The architecture of MATRIKS is given in Fig.
I. It extends previous cutting-edge environments
for Knowledge Discovery in Databases (KDD)
such as DBMiner (Han et al. 1997) by the
integration of:
0 a data warehouse hypercube exploration
expert system allowing automation and
expertise legacy of dimensional data
warehouse exploration strategies developed
by human data analyst using OLAP queries
and data mining tools;
an hypertext executive summary generator
reporting data hypercube exploration insights
in the most concise and familiar way: a few
web pages of natural language.
These two extensions allow an IDSS to be used
directly by decision makers without constant
mediation of a data analyst.
1.2 The HYSSOP natural language
hypertext summary generator
To our knowledge, the development of
HYSSOP is pioneer work in coupling OLAP
and data mining with natural language
generation, Fig. 2. We view such coupling as a
synergetic fit with tremendous potential for a
wide range of practical applications. In a
nutshell&apos;, while NLG is the only technology able
to completely fulfill the reporting needs of
&apos; See Favero (2000) for further justification for this
view, as well as for details on the motivation and
technology underlying MATRIKS.
</bodyText>
<page confidence="0.996869">
124
</page>
<bodyText confidence="0.951149857142857">
OLAP and data mining, these two technologies
are reciprocally the only ones able to completely
fulfill the content determination needs of a key
NLG application sub-class: textual
summarization of quantitative data.
in most cases, it is its very unexpectedness
that makes a fact interesting to report;
</bodyText>
<figure confidence="0.910238666666667">
Data warehouse
hypercube exploration
expert system
</figure>
<figureCaption confidence="0.981301">
Fig. I — The architecture of MATRIKS
</figureCaption>
<bodyText confidence="0.998105090909091">
Generators that summarize large amount of
quantitative data by a short natural language text
(such as ANA (Kukich 1988), GOSSIP
(Carcagno and lordanskaja 1993), PLANDoc
(McKeown, Kukich and Shaw 1994) among
others) generally perform content determination
by relying on a fixed set of domain-dependent
heuristic rules. Such an approach suffers from
two severe limitations that prevent it from
reporting the most interesting content from an
underlying database:
</bodyText>
<listItem confidence="0.989396">
• it does not scale up for analytical contexts
</listItem>
<bodyText confidence="0.910256666666667">
with high dimensionality and which take into
account the historical.—evolution of data
through time; such complex context would
require a combinatorially explosive number
of summary content determination heuristic
rules;
</bodyText>
<listItem confidence="0.9413945">
• it can only select facts whose class have been
thought ahead by the rule base author, while
</listItem>
<subsectionHeader confidence="0.494878">
NL executive summary:
Linked Web pages
</subsectionHeader>
<figureCaption confidence="0.813808">
Fig. 2 — The architecture of HYSSOP
</figureCaption>
<bodyText confidence="0.943720380952381">
OLAP and data mining are the two technologies
that emerged to tackle precisely these two
issues: for OLAP, efficient search in a high
dimensionality, historical data search space, and
for data mining, automatic discovery in such
spaces, of hitherto unsuspected regularities or
singularities. In the MATRIKS architecture,
heuristic rules are not used to define content
worth reporting in a data warehouse executive
summary. Instead, they are used to guide the
process of searching the warehouse for
unexpected facts using OLAP and data mining
operators.
A data warehouse hypercube exploration expert
system encapsulates such rules in its knowledge
base to perform content determination. An
example output of such expert system, and input
to HYSSOP, is given in Fig. 3: the data cells
selected for inclusion in the output textual
summary are passed along with their OLAP
context and the data mining annotations that
</bodyText>
<figureCaption confidence="0.541376">
justify their relevance. One output generated by
HYSSOP from this input is given in Fig. 4. and
Fig. 5.
</figureCaption>
<figure confidence="0.999096217391304">
Decision
maker
HYSSOP
NL hypertext
5.11111,11ary
generator
Data W erehouse
h ye ercube
exploration
E apart S
Data
analyst
Hypertext.]
planner
.Linear
discourse
planner
.1
Sentence
planner
Lexlcalizer
Grammatical&amp;quot;
reallzer
</figure>
<page confidence="0.996082">
325
</page>
<table confidence="0.977141052631579">
OLAP context Data mining annotations
Dimensions Meas. Discovery roll up context (an%) Drilldown
I Cell product place time A exception prod place time place
3 2 4 nation
1c Birch Beer nation - - Nov -10 low
2c Jolt Cola nation Aug +6 low 0 3 -7 nation
3c Birch Beer nation Jun -12 low 2 5 3 nation
4c Birch Beer nation Sep +42 high -2 1 1 nation
5c Cola central Aug -30 low 7 -5 -1 region
region -
6c Diet Soda . east . . Aug +40 ,low -5.- 7 .. 43-
7c Diet Soda east Sep -33 medium -1 0 7 region
Bc Diet Soda east Jul -40 high -1 5 8 region
9c Diet Soda south Jul +19 low 1 -1 -11 region
region
10c Diet Soda west Aug -17 low 2 4 1
11c Cola Colorado Sep -32 medium -2 2 2 state
12c Cola Colorado Jul -40 medium -1 4 0 state
13c Cola Wisconsin Jul -11 low _ 0 3 7 state
</table>
<figure confidence="0.930033529411765">
Fig. 3 — An example input of HYSSOP, derived from an example retailing database taken from,
(Sarawagi, Agrawal and Megiddo, 1998). The part inside the hold sub-frame is the input to the
sentence planner
Last year, the most atypical sales variations from one month to the next occurred for
O Birch Beer with a 42% national increase from September to October
O Diet Soda with a 40% decrease in the Eastern region from July to August
At the next level of idiosyncrasy came:
• Cola&apos;s Colorado sales, falling 40% from July to August and then a further 32% from September to October;
• again Diet Soda Eastern sales, falling 33% from September to October.
Less aberrant but still notably atypical were:
O again nationwide Birch Beer sales&apos; -12% from June to July and -10% from November to December
• Cola&apos;s 11% fail from July to August in the Central region and 30% dive in Wisconsin from August to
September;
a Diet Soda sales&apos; 19% increase in the Southern region from July to August, followed by its two opposite
regional variations from August to September, +10% in the East but -17% in the West;
O national Jolt Cola sales&apos; +6% from August to September.
To know what makes one of these variations unusual in the context of this year&apos;s sales, click on it.
</figure>
<figureCaption confidence="0.999189">
Fig. 4— Example of HYSSOP front-page output
</figureCaption>
<bodyText confidence="0.959103">
The 40% decrease in Diet Soda sales was very atypical mostly due to the combination of the two following facts:
</bodyText>
<listItem confidence="0.998806666666667">
• across the rest of the regions, the July to August average variation for that product was 9% increase;
• over the rest of the year, the average monthly decrease in Eastern sales for that product was only 7%.&apos;&apos;
• across the rest of the product line, the Eastern sales variations from July to August was 2%
</listItem>
<figureCaption confidence="0.635637">
Fig. 5 — Example of HYSSOP follow-up page output (behind the 40% from page anchor link)
</figureCaption>
<bodyText confidence="0.999973125">
The architecture of HYSSOP is given in Fig. 2.
HYSSOP is entirely implemented in LIFE (Alt-
Kaci and Lincoln.1989), a language that extends
Prolog with functional programming, arityless
feature structure unification and hierarchical
type constraint inheritance. For content
realization, HYSSOP relies on feature structure
unification. Lexicalization is inspired from the
approach described in (Elhadad. McKeown and
Robin 1997), while surface syntactic realization
follows the approach described in (Favero and
Robin .200 Ob ); HYS SOP-- makes two innovative
contributions to NLG research: one to hypertext
content planning presented in (Favero and
Robin 2000a) and one to content aggregation
presented in the rest of this paper.
</bodyText>
<sectionHeader confidence="0.89959" genericHeader="method">
2 Research focus: content aggregation
</sectionHeader>
<page confidence="0.993515">
126
</page>
<bodyText confidence="0.992143774193548">
in natural language generation
Natural language generation system is
traditionally decomposed in the following
subtasks: content determination, discourse-level
content organization, sentence-level content
organization, lexical content realization and
grammatical content realization. The first three
. subtasks toget bera.re Jolt= referred to As zontent
planning, and the last two together as linguistic
realization. This separation is now fairly
standard and most implementations encapsulate
each task in a separate module (Robin 1995),
(Reiter 1994).
Another generation subtask that has recently
received much attention is content aggregation.
However, there is still no consensus on the exact
scope of aggregation and on its precise relation
with the five standard generation tasks listed
above. To avoid ambiguity, we define
aggregation here as: grouping several content
units, sharing various semantic features, inside
a single linguistic structure, in such a way that
the shared features are maximally factored out
and minimally repeated in the generated text.
Defined as above, aggregation is essentially a
key subtask of sentence planning. As such,
aggregation choices are constrained by
discourse planning decisions and they in turn
constrain lexical choices.
In HYSSOP, aggregation is carried out by the
sentence planner in three steps:
</bodyText>
<listItem confidence="0.9949015">
1. content factorization, which is performed on
a tabular data structure called a Factorization
Matrix (FM) ;
2. generation from the FM of a discourse tree
representing the hypertext plan to pass down
to the lexicalizer;
3. top-down traversal of the discourse tree to
detect content units with shared features
occurring in non-adjacent sentences and
annotate them as anaphora.
</listItem>
<bodyText confidence="0.99761">
Such annotations are then used by the
lexicalizer to choose the appropriate cue word to
insert near or in place of the anaphoric item.
</bodyText>
<subsectionHeader confidence="0.996692">
2.1 Content factorization in Ervssor
</subsectionHeader>
<bodyText confidence="0.851955630434783">
The key properties of the factorization matrix
that sets it apart from previously proposed data
structures on which to perform aggregation are
that:
▪ it fully abstracts from lexical and syntactic
information;
• fo.cuses.. on two _types.: of ...informati on . kept
separate in most generators, (1) the semantic
features of each sentence constituent
(generally represented only before
lexicalization), and (2) the linear precedence
constraints between them (generally
represented only late during syntactic
realization);
• it visually captures the interaction between
the two, which underlies the factorization
phenomenon at the core of aggregation.
In HYSSOP, the sentence planner receives as
input from the discourse planner an FM
representing the yet unaggregated content to be
conveyed, together with an ordered list of
candidate semantic dimensions to consider for
outermost factoring. The pseudo-code of
HYSSOP&apos;s aggregation algorithm is given in
Fig. 10. We now illustrate this algorithm on the
input example FM that appears inside the bold
sub-frame of the overall HYSSOP input given in
Fig. 3. For this example, we assume that the
discourse planner directive is to factor out first
the exception dimension, followed by the
product dimension, i.e., FactoringStrategy —
[except,product]. This example illustrates the
mixed initiative choice of the aggregation
strategy: part of it is dictated by the discourse
planner to ensure that aggregation will not
adversely affect the high-level textual
organization that it carefully planned.
The remaining part, in our example factoring
along the place and time dimensions, is left to
the initiative .of the.sentence planner. The. first
step of HYSSOP&apos;s aggregation algorithm is to
shift the priority dimension D of the factoring
strategy to the second leftmost column of the
FM. The second step is to sort the FM rows in
(increasing or decreasing) order of their D cell
values. The third step is to horizontally slice the
</bodyText>
<page confidence="0.989029">
127
</page>
<bodyText confidence="0.99961875">
FM into row groups with identical D cell values. itself prefigures the output text linguistic
The fourth step is to merge these identical cells constituent structure) after the left to right and
and annotate the merged cell with the number of narrowing embedding of sub-matrices inside the
cells that it replaced. The FM resulting from FM.
these four first steps on the input FM inside the
bold sub-frame of Fig. 3 using exception as
The fifth step consistsofrectirsively calling the
entire aggregation algorithm inside each row
group on the sub-FM to the right of D, using the
remaining dimensions of the factoring strategy.
Let us now follow one such recursive call: the
one on the sub-FM inside a bold sub-frame in
Fig. 6 to the right of the exception column in the
third row group. The result of the first four
aggregation steps of this recursive call is given
in Fig. 7. This time it is the product dimension
that has been left-shifted and that provided the
basis for row sorting, row grouping and cell
merging. Further recursive calls are now
triggered. These calls are different from the
preceding ones, however, in that at this point all
the input constraints provided by the discourse
planner have already been satisfied. It is thus
now up to the sentence planner to choose along
which dimension to perform the next
factorization step. In the current
implementation, the column with the lowest
number of distinct values is always chosen. In
our example, this translates as factoring along
the time dimension for some row groups and
along the space dimension for the others. The
result of the recursive aggregation call on the
sub-FM inside the bold frame of Fig. 7 is given
in Fig. 8. In this case, factoring occurred along
the time dimension. The fully aggregated FM
resulting from all the recursive cans is given in
Fig. 9. Note how the left to right embedding of
its cells reflects exactly the left to right
embedding of the phrases in the natural
language summary of Fig. 4 generated from it.
</bodyText>
<subsectionHeader confidence="0.952936">
2.2 Cue word generation in HYSSOP
</subsectionHeader>
<bodyText confidence="0.9936126">
Once content factorization is completed, the
sentence planner builds in two passes the
discourse tree that the lexicalizer expects as
input. In the first pass, the sentence planner
patterns the recursive structure of the tree (that
</bodyText>
<table confidence="0.998216105263158">
except product place tint A
a
high Birch Beer nation Sep +42
*2
Diet Soda east. Jul -40
*3 &apos; --Inert -, ,..vDietSkida- .. iNiSt &apos; - - qSept-- -33 -L-
i&apos; &apos;
Cola Cobra. Sep -32
I Cola Cobra. Jul -40
low Birch Beer nation Nov -10
*8
Cola
Jolt Cola nation Auq +6
Birch Beer nation Jun -12
Cola central Aug -30
Diet Soda east Aug +10
Diet Soda south Jul +19
Diet Soda west Aug -17
Wiscon Jul -11
</table>
<figureCaption confidence="0.453428">
Fig. 6 — Left shift, row grouping and cell
merging along the exception dimension
</figureCaption>
<table confidence="0.979108166666667">
product place time A
Birch Beer nation Nov -10
*2 nation Jun -12
Cola central Aug -30
*2 Wisconsin Jul -11
Diet Soda east Aug +10
.3 south Jul +19
west Aug -17
Jolt Cola nation Aug +6
Fig. 7— Recursion along the product dimension
Cell
9c
6c
10c
1111:1121111111117223= A
Jul south +19
Aug east +10
*2
west
Fig. 8 — Recursion along the time dimension
excep product &apos; place X time&apos;
t time X place
high Birch Beer nation Sep +42
&amp;quot;2 Diet Soda east Jul -40
med. Cola Colorad Sep -32
o
&apos;2 &apos;2 Jul -40 ,
Diet Soda east Sep -33
low Birch Beer nation Nov -10
&amp;quot;8 &amp;quot;2 12. Jun 12
Cola central Aug -30
*2 VViscon Jul -11
Diet Soda Jul south +19
Aug east +10
.3 *2 west -17
Jolt Cola nation Aug +6
</table>
<figure confidence="0.9386928125">
cell
factoring dimension is given in Fig. 6.
40
8c
11c
12c
1 c
2c
3c
5c
6c
9c
10c
13c
Cell
1c
30
5c
13c
6c
9c
10c
2c
cell
Ac
8c
11c
12c
7c
lc
3c
50
</figure>
<footnote confidence="0.8633892">
13c
9c
6c
10c
2c
</footnote>
<page confidence="0.988737">
128
</page>
<figureCaption confidence="0.8778025">
Fig. 9 — Final, fully aggregated FM after all
recursive calls
</figureCaption>
<bodyText confidence="0.997685">
In the second pass, the sentence planner
traverses this initial discourse tree to enrich it
with anaphoric annotations that the lexicalizer
needs to generated cue words such as &amp;quot;again&amp;quot;,
&amp;quot;both&amp;quot;, &amp;quot;neither&amp;quot;, &amp;quot;except&amp;quot; etc. Planning cue
words can be considered -part of- aggregation
since it makes the aggregation structures explicit
to the reader and prevents ambiguities that may
otherwise be introduced by aggressive content
factorization. A fragment of the sentence
planner output discourse tree built form &amp;quot; the
aggregated FM of Fig. 9 is given in Fig. 12. The
discourse tree spans horizontally with its root to
the left of the feature structure and its leaves to
the right. Note in Fig. 12 the cue word directive:
fanaph=loccur--rd, repeated—fproduct, region//I.
It indicates that this is the second mention in the
text of a content unit with product .-- .&amp;quot;Birch
Beee-andlTegion&apos;— -nation. &apos;The-I exi cal izer uses
this annotation to generate the cue word &amp;quot;again&amp;quot;
before the second reference to &amp;quot;nationwide
</bodyText>
<figure confidence="0.775272695652174">
Birch Beer sales&amp;quot;.
• factor(Matrix,FactoringStrategy)
variables: Matrix = a factorization matrix
FactoringStrategy = a list of pairs (Dimension, Order) where Dimension E dimensions(Matrix)
and Order E (increasing,decreasing)
RowGroups = list of sub-matrices of Matrix
begin
if FactoringStrategy = emptyList
then FactoringStrategy &lt;- buildFactoringStrategy(matrix) ;
(Diml,Order1) &lt;- first(FactoringStrategy) ;
RemainingFactoringStrategy &lt;- rest(FactoringStrategy) ;
Matrix &lt;- leftShiftColumn(MatnX,Diml);
Matrix &lt;- sortRows(Matrix,Diml,Orderl) ;
RowGroups &lt;- horizSlice(Matrix,Diml);
for each RowGroup in RowGroups do:
RowGroup &lt;- mergeCells(RowGroup,Diml) ;
(LeftSubMatrix,RighSubMatrix) &lt;- cut(RowGroup,Dim1) ;
FactoredRightSubMatrix &lt;- factor(RightSubMatrix, RemainingFactoringStrategy)
RowGroup &lt;- paste(LeftSubMatrix,FactoredRightSubMatrix,Dim1)
Matrix update(Matrix,RowGroup);
endfor;
return Matrix ;
end.
</figure>
<listItem confidence="0.999517857142857">
• buildFactoringStrategy(Matrix): returns inside a list a pair (Dim,increasing) where Dim is the matrix&apos;s dimension (i.e.,
column) with the lowest number of distinct values.
• leftShiftColumn (Matrix,Dim1): moves Dim1 to the second leftmost column next to the cell id column,
• sortRows(Matrix,Diml,Order): sorts the Matrix&apos;s rows in order of their Diml cell value; Order specifies whether the order
should be increasing or decreasing.
• horizSlice(Matrix,Dim1): horizontally slices the Matrix into row groups with equal value along Diml.
• mergeCells(RowGroup,Diml): merges (by definition equal valued) cells of Dim1 in RowGroup.
</listItem>
<footnote confidence="0.6628706">
cut(RowGroup,Dim1): cuts RowGroup into two sub-matrices, one to the left of Diml (including Diml) and the other to the
right of Diml
paste(LeftSubMatrix,FactoredRightSubMatrix,Dim1): pastes together left and right sub-matrices.
update(Matrix,RowGroup): identifies the rows Ru of Matrix whose cell iris match those of RowGroup RG and substitute
those Ruby R0 inside Matrix
</footnote>
<figureCaption confidence="0.783819">
Fig. JO— HYSSOP &apos;.c aggregation algorithm
</figureCaption>
<bodyText confidence="0.947908583333333">
A special class of aggregation-related cue
phrases involves not only the sentence planner
and the lexicalizer but also the discourse
planner. One discourse strategy option that
HYSSOP implements is to precede each
aggregation group by a cue phrase explicitly
mentioning the group&apos;s cardinal. An example
summary front page generated using such a
strategy is given in Fig. 11. The count annotation
in the cell merging function of HYSSOP&apos;s
aggregation algorithm are computed for that
purpose. While the decision to use an explicit
</bodyText>
<page confidence="0.997148">
129
</page>
<bodyText confidence="0.9868436">
count discourse strategy lies within the discourse planner and their realization as cue phrases are
planner, the counts are computed by the sentence carried out by the lexicalizer.
Last year, there were 13 exceptions in the beverage product line.
The most striking was Birch Beer&apos;s 42% national fall from Sep to Oct.
The remaining exceptions clustered around four products were:
</bodyText>
<listItem confidence="0.958406083333333">
• Again, Birch Beer&apos;s sales accounting for other two national exceptions, both decreasing mild values:
1. a 12% from Jun to Jul;
2. a 10% from Nov to Dec;
▪ Cola&apos;s sales accountinglarfounexceptions: _
1. two medium in Colorado, a 40% from Jul to Aug arid a 32% from Aug to Sep;
2. two mild, a 11% in Wisconsin from Jul to Aug and a 30% in Central region from Aug to Sep;
O Diet Soda accounting for 5 exceptions:
1. one strong, a 40% slump in Eastern region from Jul to Aug;
2. one medium, a 33% slump in Eastern region from Sep to Oct;
3. three mild: two increasing, a 10 % in Eastern region from Aug to Sep and a 19% in Southern region
from Jul to Aug; and one falling, a 17% in Western region from Aug to Sep:
* Finally, Jolt Cola&apos;s sales accounting for one mild exception, a 6% national fall from Aug to Sep.
</listItem>
<figureCaption confidence="0.343203">
Fig. ii HYSSOP&apos;s front page output using discourse strategy with explicit counts
</figureCaption>
<table confidence="0.887904277777778">
cat = aggr, level =1, ngroup =2, nmsg =2
common Exceptionallity = high %%The most atypical sales variations from one moth to the next occurred
for
distinct = IIcat =msg. attr =[product =Birch beer&amp;quot;, time =9, place =nation, var=+42]
%%Birch Beer with a 42% national increase from Sept to Oct
cat =msg, attr qproduct =&amp;quot;Diet Soda&amp;quot;, time =7, place =east, var=-401
%%Diet Soda with a 413% decrease in the Eastern region from Jul to Aug
cat =aggr, level=1, ngroup=2, nmsg=3
common exceptionallity = medium %%At next level of idiosyncrasy came:
cat =aggr, level =2. ngroup =2, nmsg=2.
common 1 product=Cola, place=Colorado %VD Cola&apos;s sales
distinct =
I distinct I I cat=msg, attr=flime=7, var =4D] °A% falling 40% from Jun to Jul
I cal=msg, attr=ftime=9 var =-32 %% and then a further 32 from Sep to Oct
cat =msg, attr .,[product =&amp;quot;Dief Soda&amp;quot;, time =9, place =east var=-33
anaph foccurr =2nd, repeated-product, place]
VA again Diet Soda Eastern sales, falling 33% from Sep to Oct
cat =aggr, %% Less aberrant but still notably atypical were:
</table>
<figureCaption confidence="0.411755">
Fig. 12 — Fragment of LIFE feature structure representing the discourse tree output of the sentence
planner and input to the lexicalizer.
</figureCaption>
<sectionHeader confidence="0.993734" genericHeader="method">
3 Related work in content aggregation
</sectionHeader>
<bodyText confidence="0.982202230769231">
The main previous works on content
aggregation are due to:
O (Dalianis 1995, 1996), whose ASTROGEN
system generates natural language
paraphrases of formal software specification
for validation purposes;
6 (Huang and Fiedler 1997), whose PROVERB
system generates natural language
mathematical proofs from a theorem prover
reasoning trace;
o (Robin and McKeown, 1996), whose
STREAK system generates basketball game
summaries from a semantic network
</bodyText>
<page confidence="0.989511">
130
</page>
<bodyText confidence="0.999897571428571">
representing the key game statistics and their
historical context;
* (Shaw 1998), whose CASPER discourse and
sentence planner has been used both in the
PLANDoc system that generates
telecommunication equipment installation
plan documentation from an expert system
trace and the MAGIC system that generates
ICU patient status - briefs -.froin medieg
measurements.
In this section, we briefly compare these
research efforts with ours along four
dimensions: (1) the definition of aggregation
and the scope of the aggregation task
implemented in the generator, (2) the type of
representation the generator takes as input and
the type of output text that it produces, (3) the
generator&apos;s architecture and the localization of
the aggregation task within it, and (4) the data
structures and algorithms used to implement
aggregation.
</bodyText>
<subsectionHeader confidence="0.999824">
3.1 Definition of the aggregation task
</subsectionHeader>
<bodyText confidence="0.9998704">
The definition of aggregation that we gave at the
beginning of previous section is similar to those
provided by Dalianis and Huang, although it
focuses on common feature factorization to
insure aggregation remains a proper subset of
sentence planning. By viewing aggregation only
as a process of combining clauses, Shaw&apos;s
definition is more restrictive. In our view,
aggregation is best handled prior to commit to
specific syntactic categories and the same
abstract process, such the algorithm of Fig. 10,
can be used to aggregate content units inside
linguistic constituents of any syntactic category
(clause, nominal, prepositional phrases,
adjectival phrases, etc.). In terms of aggregation
task coverage, HYSSOP focuses on paratactic
forms of aggregation. In contrast, ASTROGEN,
CASPER, PROVERB and STREAK also
perform hypotactic and paradigmatic
aggregation.
</bodyText>
<subsectionHeader confidence="0.9991245">
3.2 Input representation and generated
output text
</subsectionHeader>
<bodyText confidence="0.999944043478261">
A second characteristic that sets HYSSOP apart
from other generators performing aggregation is
the nature of its input: a set of data cells
extracted from a dimensional data warehouse
hypercube. In contrast, the other systems all take
as input either a semantic .network extracted
from a knowledge base or a pre-linguistic
representation of the text to generate such as
Meteer&apos;s text structure (Meteer 1992) or
Jackendoffs semantic structure (Jackendoff
1985). Such natural language processing
—oriented inputs tend ld&apos;slinplify the overall -text
generation task and hide important issues that
come up in real life applications for which raw
data is often the only available input. In terms of
output, HYSSOP differs from most other
systems in that it generates hypertext instead of
linear text. It thus tackles the content
aggregation problem in a particularly demanding
application requiring the generator to
simultaneously start from raw data, produce
hypertext output and enforce conciseness
constraints.
</bodyText>
<subsectionHeader confidence="0.938773">
3.3 Generation architecture and
aggregation localization
</subsectionHeader>
<bodyText confidence="0.999982666666667">
While its overall architecture is a conventional
pipeline, HYSSOP is unique in encapsulating all
aggregation processing in the sentence planner
and carrying it out entirely on a deep semantic
representation. In contrast, most other systems
distribute aggregation over several processing
components and across several levels of internal
representations: deep semantic, thematic and
even surface syntactic for some of them.
</bodyText>
<subsectionHeader confidence="0.9975255">
3.4 Data structures and algorithms for
aggregation
</subsectionHeader>
<bodyText confidence="0.99997725">
All previous approaches to aggregations relied
on rules that included some domain-specific
semantic or lexical information. In contrast, the
aggregation algorithm used by HYSSOP is
.domain independent since it relies only on (1)
generic matrix row and column shuffling
operations, and (2) on a generic similarity
,-Trleasure between-arbitrary data cells.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99917825">
We presented a new approach to content
aggregation in the context of a very challenging
and practical generation application:
summarizing OLAP and data mining discoveries
</bodyText>
<page confidence="0.995233">
131
</page>
<bodyText confidence="0.9995954">
as a few linked web pages of fluent and concise
natural language. We believe that the key
contribution to our work is to show the
feasibility to perform effective paratactic
aggregation:
</bodyText>
<listItem confidence="0.941908333333333">
• encapsulated within a single generation
component (the sentence planner)
• using a domain-independent algorithm and a
</listItem>
<bodyText confidence="0.933101916666666">
simple data strueture,&amp;quot; the fktoriiation
matrix, that captures the key structural and
ordering constraints on paratactic
aggregation while completely abstracting
from domain semantic idiosyncrasies as well
as from lexical and syntactic details.
This is a first success towards the development
of a plug-in content aggregation component for
text generation, reusable across application
domains. In future work, we intend to
empirically evaluate the summaries generated by
HYSSOP.
</bodyText>
<sectionHeader confidence="0.998071" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998884135135135">
Ait-Kaci H. and Lincoln P. (1989) LIFE — A natural
language for natural language. TA . Informations,
30(1-2):37-67, Association pour le Traitement
Automatique des Langues, Paris France.
Carcagno D. and lordanskaja L. (1993) Content
determination and text structuring; two interrelated
processes. In H Horacek (ed.) New concepts in
NLG: Planning, realisation and systems. London:
Pinter Publishers, pp 10-26.
Dalianis H. (1995) Aggregation, Formal specification
and Natural Language Generation. In Proc. of the
NLDB.95 First International Workshop on the
application of NL to Databases, 135-149,
Versailles, France
Dalianis H. (1996) Aggregation as a subtask of text
and sentence planning. In Proc. of Florida Al
Research symposium, FLAI RS-96, Florida, pp 1-5.
Elhadad M., McKeown K. and Robin J. (1997)
Floating constraints in lexical choice.
Computational Linguistics, 23(2).
Favero E. L. (2000). Generating- hypertext summaries
of data mining discoveries in multidimensional
databases. PhD Thesis. Centro de Informatica,
UFPE, Recife, Brazil.
Favero E. L. and Robin J. (2000a). Using OLAP and
data mining for content planning in natural
language generation. Accepted for publication in
Proc. of SInternational. Conference on
Applications of Natural Language to Information
Systems, NLDB&apos;2000, 28-30 June, Versailles
France.
Favero E. L. and Robin J. (2000b). Implementing
Functional Unification Grammars for Text
Generation as Featured Definite Clause Grammars.
Submitted to Natural Language Engineering.
_DIActiner, Attp.://d1xcs.sfaLedu/Dalvtiner/
index.html
Huang G. and Fiedler A (1996) Paraphrasing and
aggregation argumentative text using text structure.
In Proc. of the 8th International NLG Workshop,
pages 21-3, Sussex, UK.
Jackendoff R. (1985) Semantics and Cognition. MIT
Press, Cambridge, MA, June 15-17.
Kukich K. (1988) Fluency in Natural Language
Reports in Natural Language Generation Systems,
McDonald, D. &amp; Bloc, L. (Eds.), Springer-Verlag.
McKeown K., Kukich, K. and Shaw J. (1994)
Practical issues in automatic document generation.
In Proc. of ANLP&apos;94, pages 7-14, Stuttgart, Oct.
Meteer M. (1992) Expressibility and the problem of
efficient text planning. Communication in Artificial
Intelligence. Pinter Publisher Limited, London.
Reiter E. (1994) Has a Consensus NL Generation
Architecture Appeared, and is it
Psycholinguistically Plausible? In Proc of the
Seventh International Workshop on Natural
Language Generation (INLGW-1994), pages 163-
170. Kennebunkport, Maine, USA.
Robin J. (1995) Revision-based generation of natural
language summaries providing historical
background: corpus-based analysis, design,
implementation and evaluation. Ph.D. Thesis.
CUCS-034-94, Columbia University, Computer
Science Department , New York, USA. 357p.
Robin J. and McKeown K. (1996) Empirically
designing and evaluating a new revision-based
model for summary generation. Artificial
Intelligence, 85(1-2). 57p.
Sarawagi S. Agrawal R and Megiddo N. (1998)
Discovery-driven exploration of MDDB data
cubes. In Proc. Int. Conf. of Extending Database
Technology (EDIT &apos;98), March.
Shaw J. (1998) Segregatory coordination and ellipsis
in text generation. In Proc. of the 17th COLING &apos;98.
</reference>
<page confidence="0.997711">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.999413">Content Aggregation in Natural Language Hypertext Summarization of OLAP and Data Mining Discoveries</title>
<author confidence="0.964579">Jacques</author>
<affiliation confidence="0.996301">Universidade Federal de Pernambuco Centro de Informatica</affiliation>
<address confidence="0.787214">Caixa Postal 50732-970 — Recife,</address>
<email confidence="0.995577">jr@di.ufpe.br</email>
<author confidence="0.997821">Eloi L Favero</author>
<affiliation confidence="0.9947535">Universidade Federal do Para (UFPA) Departamento de Informatica (DI)</affiliation>
<address confidence="0.992541">Campus do .Cruarna 66075-900 — Belem, Para.</address>
<email confidence="0.998402">ellf@di.ufpe.br</email>
<abstract confidence="0.989110718475074">We present a new approach to paratactic content aggregation in the context of generating hypertext summaries of OLAP and data mining discoveries. Two key properties make this approach innovative and interesting: (1) it encapsulates aggregation inside the sentence planning component, and (2) it relies on a domain independent algorithm working on a data structure that abstracts from lexical and syntactic knowledge. 1 Research context: hypertext executive summary generation for intelligent decision-support In this paper, we present a new approach to content aggregation in Natural Nanguage Generation (NLG). This approach has been developed for the NLG system HYSSOP (HYpertext Summary System of On-line analytical Processing) which summarizes OLAP (On-Line Analytical Processing) and Data Mining discoveries into an hypertext report. HYSSOP is itself part of the Intelligent Decision-Support System (IDSS) MATRIKS (Multidimensional Analysis and Textual Reporting for Insight Knowledge Search), which aims to provide a comprehensive knowledge discovery environment through seamless integration of data warehousing, OLAP, data mining, expert system and NLG technologies. The intelligent decisionsupport system The architecture of MATRIKS is given in Fig. I. It extends previous cutting-edge environments for Knowledge Discovery in Databases (KDD) such as DBMiner (Han et al. 1997) by the integration of: a warehouse hypercube exploration system automation and expertise legacy of dimensional data warehouse exploration strategies developed by human data analyst using OLAP queries and data mining tools; executive summary generator reporting data hypercube exploration insights in the most concise and familiar way: a few web pages of natural language. These two extensions allow an IDSS to be used by decision makers constant mediation of a data analyst. 1.2 The HYSSOP natural language hypertext summary generator To our knowledge, the development of HYSSOP is pioneer work in coupling OLAP and data mining with natural language generation, Fig. 2. We view such coupling as a synergetic fit with tremendous potential for a wide range of practical applications. In a nutshell&apos;, while NLG is the only technology able to completely fulfill the reporting needs of See Favero (2000) for further justification for view, as well as for details on the motivation and technology underlying MATRIKS. 124 OLAP and data mining, these two technologies are reciprocally the only ones able to completely fulfill the content determination needs of a key NLG application sub-class: summarization of quantitative data. most cases, it is unexpectedness that makes a fact interesting to report; Data hypercube expert system Fig. I — The architecture of MATRIKS that summarize large amount quantitative data by a short natural language text as ANA (Kukich (Carcagno and lordanskaja 1993), PLANDoc (McKeown, Kukich and Shaw 1994) among others) generally perform content determination by relying on a fixed set of domain-dependent heuristic rules. Such an approach suffers from two severe limitations that prevent it from reporting the most interesting content from an underlying database: • it does not scale up for analytical contexts with high dimensionality and which take into account the historical.—evolution of data through time; such complex context would require a combinatorially explosive number of summary content determination heuristic rules; • it can only select facts whose class have been thought ahead by the rule base author, while summary: Linked Web pages Fig. 2 — The architecture of HYSSOP OLAP and data mining are the two technologies that emerged to tackle precisely these two for efficient search in a high dimensionality, historical data search space, and for data mining, automatic discovery in such spaces, of hitherto unsuspected regularities or In the MATRIKS rules are used to define content worth reporting in a data warehouse executive summary. Instead, they are used to guide the process of searching the warehouse for unexpected facts using OLAP and data mining operators. A data warehouse hypercube exploration expert system encapsulates such rules in its knowledge base to perform content determination. An example output of such expert system, and input to HYSSOP, is given in Fig. 3: the data cells for inclusion in output textual summary are passed along with their OLAP context and the data mining annotations that justify their relevance. One output generated by HYSSOP from this input is given in Fig. 4. and Fig. 5. Decision maker HYSSOP NL hypertext 5.11111,11ary generator Data W h ye exploration E apart S Data analyst planner discourse planner .1 Sentence planner Lexlcalizer reallzer 325 OLAP context Data mining annotations Dimensions Meas. Discovery roll up context (an%) Drilldown product place time A exception place time place nation 3 2 4 1c Birch Beer nation - - Nov -10 low 2c Jolt Cola nation Aug +6 low 0 3 -7 nation 3c Birch Beer nation Jun -12 low 2 5 3 nation 4c Birch Beer nation Sep +42 high -2 1 1 nation 5c Cola central Aug -30 low 7 -5 -1 region region - 6c Diet Soda . east . . Aug +40 ,low -5.- 7 .. 43- 7c Diet Soda east Sep -33 medium -1 0 7 region Bc Diet Soda east Jul -40 high -1 5 8 region 9c Diet Soda south Jul +19 low 1 region region 10c Diet Soda west Aug low 2 4 1 11c Cola Colorado Sep -32 medium -2 2 2 state 12c Cola Colorado Jul -40 medium -1 4 0 state 13c Cola Wisconsin Jul -11 low _ 0 3 7 state Fig. 3 — An example input of HYSSOP, derived from an example retailing database taken from, (Sarawagi, Agrawal and Megiddo, 1998). The part inside the hold sub-frame is the input to the sentence planner year, the most atypical sales variations from one month to the next Birch a 42%national increase from September to October O Diet Soda with a 40% decrease in the Eastern region from July to August At the next level of idiosyncrasy came: Cola&apos;s Colorado sales, falling 40%from July to August and then a further 32%from September to October; • again Diet Soda Eastern sales, falling 33% from September to October. Less aberrant but still notably atypical were: again nationwide Birch Beer sales&apos; -12%from June to July and -10%from November to December Cola&apos;s 11% fail from July to August in the and 30% dive in Wisconsin from August to September; a Diet Soda sales&apos; 19% increase in the Southern region from July to August, followed by its two opposite variations from August to September, +10%in the East but -17%in the West; national Jolt sales&apos; August to September. know what makes one of these variations unusual in the context of this year&apos;s sales, click of HYSSOP output 40% decrease in Diet was mostly due to the combination of the two following across rest of the regions, the July to August average variation for that product was 9% increase; over the rest of the year, the Eastern sales product was only 7%.&apos;&apos; across the rest product line, the Eastern sales variations from July to August was 2% 5 — Example of HYSSOP follow-up page output (behind the 40% from page anchor architecture of HYSSOP is Fig. 2. HYSSOP is entirely implemented in LIFE (Alt- Kaci and Lincoln.1989), a language that extends Prolog with functional programming, arityless feature structure unification and hierarchical type constraint inheritance. For content realization, HYSSOP relies on feature structure unification. Lexicalization is inspired from the approach described in (Elhadad. McKeown and Robin 1997), while surface syntactic realization follows the approach described in (Favero and .200 Ob ); HYS SOP-two contributions to NLG research: one to hypertext content planning presented in (Favero and 2000a) and one to content presented in the rest of this paper. 2 Research focus: content aggregation 126 in natural language generation Natural language generation system is traditionally decomposed in the following subtasks: content determination, discourse-level content organization, sentence-level content organization, lexical content realization and grammatical content realization. The first three . subtasks toget bera.re Jolt= referred to As zontent planning, and the last two together as linguistic This separation is standard and most implementations encapsulate each task in a separate module (Robin 1995), (Reiter 1994). Another generation subtask that has recently received much attention is content aggregation. However, there is still no consensus on the exact scope of aggregation and on its precise relation with the five standard generation tasks listed above. To avoid ambiguity, we define here as: several content units, sharing various semantic features, inside a single linguistic structure, in such a way that the shared features are maximally factored out and minimally repeated in the generated text. Defined as above, aggregation is essentially a key subtask of sentence planning. As such, choices constrained by discourse planning decisions and they in turn constrain lexical choices. In HYSSOP, aggregation is carried out by the sentence planner in three steps: 1. content factorization, which is performed on tabular data structure called a ; 2. generation from the FM of a discourse tree representing the hypertext plan to pass down to the lexicalizer; 3. top-down traversal of the discourse tree to content with shared features occurring in non-adjacent sentences and annotate them as anaphora. annotations then used by the lexicalizer to choose the appropriate cue word to insert near or in place of the anaphoric item. factorization in The key properties of the factorization matrix that sets it apart from previously proposed data structures on which to perform aggregation are that: ▪ it fully abstracts from lexical and syntactic information; • fo.cuses.. on two _types.: of ...informati on . kept separate in most generators, (1) the semantic features of each sentence constituent (generally represented only before lexicalization), and (2) the linear precedence constraints between them (generally represented only late during syntactic realization); • it visually captures the interaction between the two, which underlies the factorization phenomenon at the core of aggregation. In HYSSOP, the sentence planner receives as input from the discourse planner an FM representing the yet unaggregated content to be conveyed, together with an ordered list of candidate semantic dimensions to consider for outermost factoring. The pseudo-code of HYSSOP&apos;s aggregation algorithm is given in now illustrate this algorithm on the input example FM that appears inside the bold sub-frame of the overall HYSSOP input given in 3. this example, we assume that discourse planner directive is to factor out first the exception dimension, followed by the dimension, FactoringStrategy — example illustrates the mixed initiative choice of the aggregation strategy: part of it is dictated by the discourse planner to ensure that aggregation will not adversely affect the high-level textual organization that it carefully planned. The remaining part, in our example factoring along the place and time dimensions, is left to initiative the.sentence planner. The. first step of HYSSOP&apos;s aggregation algorithm is to shift the priority dimension D of the factoring strategy to the second leftmost column of the FM. The second step is to sort the FM rows in (increasing or decreasing) order of their D cell values. The third step is to horizontally slice the 127 FM into row groups with identical D cell values. itself prefigures the output text linguistic The fourth step is to merge these identical cells constituent structure) after the left to right and and annotate the merged cell with the number of narrowing embedding of sub-matrices inside the cells that it replaced. The FM resulting from FM. these four first steps on the input FM inside the sub-frame of Fig. 3 using The fifth step consistsofrectirsively calling the entire aggregation algorithm inside each row group on the sub-FM to the right of D, using the remaining dimensions of the factoring strategy. Let us now follow one such recursive call: the one on the sub-FM inside a bold sub-frame in 6 to the right of the in the third row group. The result of the first four aggregation steps of this recursive call is given in Fig. 7. This time it is the product dimension that has been left-shifted and that provided the basis for row sorting, row grouping and cell merging. Further recursive calls are now triggered. These calls are different from the preceding ones, however, in that at this point all the input constraints provided by the discourse planner have already been satisfied. It is thus now up to the sentence planner to choose along which dimension to perform the next factorization step. In the implementation, the column with the lowest number of distinct values is always chosen. In our example, this translates as factoring along the time dimension for some row groups and along the space dimension for the others. The result of the recursive aggregation call on the sub-FM inside the bold frame of Fig. 7 is given in Fig. 8. In this case, factoring occurred along the time dimension. The fully aggregated FM resulting from all the recursive cans is given in Fig. 9. Note how the left to right embedding of its cells reflects exactly the left to right embedding of the phrases in the natural language summary of Fig. 4 generated from it. 2.2 Cue word generation in HYSSOP Once content factorization is completed, the sentence planner builds in two passes the discourse tree that the lexicalizer expects as input. In the first pass, the sentence planner patterns the recursive structure of the tree (that except product place tint a A</abstract>
<note confidence="0.946546592592593">high Birch Beer nation Sep +42 *2 Diet Soda east. Jul -40 *3 -, iNiSt - -L- Cola Cobra. Sep -32 I Cola Cobra. Jul -40 low Birch Beer nation Nov -10 *8 Cola Jolt Cola nation Auq +6 Birch Beer nation Jun -12 Cola central Aug -30 Diet Soda east Aug +10 Diet Soda south Jul +19 Diet Soda west Aug -17 Wiscon Jul -11 Fig. 6 — Left shift, row grouping and cell merging along the exception dimension product place time A Birch Beer nation Nov -10 *2 nation Jun -12 Cola central Aug -30 *2 Wisconsin Jul -11 Diet Soda east Aug +10 .3 south Jul +19 west Aug -17 Jolt Cola nation Aug +6</note>
<abstract confidence="0.623404533333333">along the product dimension 9c 10c 1111:1121111111117223= A Jul south +19 Aug east +10 *2 west Fig. 8 — Recursion along the time dimension excep t product &apos; place X time&apos; time X place high Birch Beer nation Sep +42 &amp;quot;2 Diet Soda east Jul -40 med. Cola Colorad Sep -32 o &apos;2 &apos;2 Jul -40 ,</abstract>
<note confidence="0.867236234042553">Diet Soda east Sep -33 low Birch Beer nation Nov -10 &amp;quot;8 &amp;quot;2 Jun 12 Cola central Aug -30 *2 VViscon Jul -11 Diet Soda Jul south +19 Aug east +10 .3 *2 west -17 Jolt Cola nation Aug +6 cell factoring dimension is given in Fig. 6. 40 8c 11c 12c 1 c 2c 3c 5c 6c 9c 10c 13c Cell 1c 30 5c 13c 6c 9c 10c 2c cell Ac 8c 11c 12c 7c lc 3c 50 13c 9c 6c 10c 2c 128</note>
<abstract confidence="0.989666658119658">Fig. 9 — Final, fully aggregated FM after all recursive calls In the second pass, the sentence planner traverses this initial discourse tree to enrich it with anaphoric annotations that the lexicalizer needs to generated cue words such as &amp;quot;again&amp;quot;, &amp;quot;both&amp;quot;, &amp;quot;neither&amp;quot;, &amp;quot;except&amp;quot; etc. Planning cue words can be considered -part ofaggregation since it makes the aggregation structures explicit to the reader and prevents ambiguities that may otherwise be introduced by aggressive content factorization. A fragment of the sentence planner output discourse tree built form &amp;quot; the aggregated FM of Fig. 9 is given in Fig. 12. The discourse tree spans horizontally with its root to the left of the feature structure and its leaves to the right. Note in Fig. 12 the cue word directive: region//I. It indicates that this is the second mention in the of a content unit with .-- .&amp;quot;Birch exi cal izer uses annotation to generate the cue word the second reference to Birch Beer sales&amp;quot;. • factor(Matrix,FactoringStrategy) variables: Matrix = a factorization matrix FactoringStrategy = a list of pairs (Dimension, Order) where Dimension E dimensions(Matrix) Order RowGroups = list of sub-matrices of Matrix begin if FactoringStrategy = emptyList then FactoringStrategy &lt;buildFactoringStrategy(matrix) ; (Diml,Order1) &lt;first(FactoringStrategy) ; RemainingFactoringStrategy &lt;rest(FactoringStrategy) ; Matrix &lt;leftShiftColumn(MatnX,Diml); Matrix &lt;sortRows(Matrix,Diml,Orderl) ; RowGroups &lt;horizSlice(Matrix,Diml); for each RowGroup in RowGroups do: RowGroup &lt;mergeCells(RowGroup,Diml) ; (LeftSubMatrix,RighSubMatrix) &lt;cut(RowGroup,Dim1) ; FactoredRightSubMatrix &lt;factor(RightSubMatrix, RemainingFactoringStrategy) endfor; return Matrix ; end. buildFactoringStrategy(Matrix): inside a list a pair (Dim,increasing) where Dim is the matrix&apos;s dimension (i.e., column) with the lowest number of distinct values. leftShiftColumn (Matrix,Dim1): Dim1 to second leftmost next to the cell id column, sortRows(Matrix,Diml,Order): the Matrix&apos;s rows in order their Diml value; Order specifies whether the order should be increasing or decreasing. horizSlice(Matrix,Dim1): slices the Matrix into row groups with equal value along Diml. • mergeCells(RowGroup,Diml): merges (by definition equal valued) cells of Dim1 in RowGroup. RowGroup into two sub-matrices, one to the left of Diml (including Diml) and the other to the right of Diml together left and right identifies the rows Ru of Matrix whose cell iris match those of RowGroup and substitute those Ruby R0 inside Matrix Fig. JO— HYSSOP &apos;.c aggregation algorithm A special class of aggregation-related cue phrases involves not only the sentence planner and the lexicalizer but also the discourse planner. One discourse strategy option that HYSSOP implements is to precede each aggregation group by a cue phrase explicitly mentioning the group&apos;s cardinal. An example front page such a strategy is given in Fig. 11. The count annotation in the cell merging function of HYSSOP&apos;s aggregation algorithm are computed for that purpose. While the decision to use an explicit 129 count discourse strategy lies within the discourse planner and their realization as cue phrases are planner, the counts are computed by the sentence carried out by the lexicalizer. Last year, there were 13 exceptions in the beverage product line. The most striking was Birch Beer&apos;s 42% national fall from Sep to Oct. The remaining exceptions clustered around four products were: • Again, Birch Beer&apos;s sales accounting for other two national exceptions, both decreasing mild values: a Jun to Jul; 2. a 10% from Nov to Dec; ▪ Cola&apos;s sales accountinglarfounexceptions: _ 1. two medium in Colorado, a 40% from Jul to Aug arid a 32% from Aug to Sep; 2. two mild, a 11% in Wisconsin from Jul to Aug and a 30% in Central region from Aug to Sep; O Diet Soda accounting for 5 exceptions: 1. one strong, a 40% slump in Eastern region from Jul to Aug; 2. one medium, a 33% slump in Eastern region from Sep to Oct; 3. three mild: two increasing, a 10 % in Eastern region from Aug to Sep and a 19% in Southern region from Jul to Aug; and one falling, a 17% in Western region from Aug to Sep: * Finally, Jolt Cola&apos;s sales accounting for one mild exception, a 6% national fall from Aug to Sep. HYSSOP&apos;s front page output using discourse strategy with counts = level =1, ngroup =2, nmsg =2 Exceptionallity = high sales variations from one moth to the next for distinct = =msg. attr =[product =Birch beer&amp;quot;, time =9, place =nation, var=+42] %%Birch Beer with a 42% national increase from Sept to Oct qproduct =&amp;quot;Diet Soda&amp;quot;, =7, =east, Soda with a 413% decrease in the Eastern region from Jul cat =aggr, level=1, ngroup=2, nmsg=3 exceptionallity = medium of idiosyncrasy came: cat =aggr, level =2. ngroup =2, nmsg=2. 1 product=Cola, place=Colorado %VD sales distinct = distinctI cat=msg, attr=flime=7, var =4D] °A% 40% from Jun to Jul cal=msg, attr=ftime=9 var =-32 %% then a further Sep =msg, attr Soda&amp;quot;, =9, =east foccurr =2nd, place] Eastern sales, falling 33% from Sep to Oct cat =aggr, %% Less aberrant but still notably atypical were: 12 — Fragment of structure representing the discourse tree output of the sentence planner and input to the lexicalizer. 3 Related work in content aggregation The main previous works on content aggregation are due to: O (Dalianis 1995, 1996), whose ASTROGEN system generates natural paraphrases of formal software specification for validation purposes; 6(Huang and Fiedler 1997), whose PROVERB system generates natural mathematical proofs from a theorem prover reasoning trace; and McKeown, 1996), whose STREAK system generates basketball game summaries from a semantic network 130 representing the key game statistics and their historical context; * (Shaw 1998), whose CASPER discourse and sentence planner has been used both in the PLANDoc system that generates telecommunication equipment installation plan documentation from an expert system trace and the MAGIC system that generates patient status briefs medieg measurements. In this section, we briefly compare these research efforts with ours along four dimensions: (1) the definition of aggregation and the scope of the aggregation task implemented in the generator, (2) the type of representation the generator takes as input and the type of output text that it produces, (3) the generator&apos;s architecture and the localization of the aggregation task within it, and (4) the data structures and algorithms used to implement aggregation. 3.1 Definition of the aggregation task The definition of aggregation that we gave at the beginning of previous section is similar to those provided by Dalianis and Huang, although it on common feature aggregation remains a of sentence planning. By viewing aggregation only a process of combining definition is more restrictive. In our view, aggregation is best handled prior to commit to specific syntactic categories and the same abstract process, such the algorithm of Fig. 10, can be used to aggregate content units inside linguistic constituents of any syntactic category (clause, nominal, prepositional phrases, adjectival phrases, etc.). In terms of aggregation task coverage, HYSSOP focuses on paratactic forms of aggregation. In contrast, ASTROGEN, CASPER, PROVERB and STREAK also perform hypotactic and aggregation. 3.2 Input representation and generated output text A second characteristic that sets HYSSOP apart from other generators performing aggregation is the nature of its input: a set of data cells extracted from a dimensional data warehouse hypercube. In contrast, the other systems all take as input either a semantic .network extracted from a knowledge base or a pre-linguistic representation of the text to generate such as Meteer&apos;s text structure (Meteer 1992) or Jackendoffs semantic structure (Jackendoff 1985). Such natural language processing inputs tend ld&apos;slinplify the overall generation task and hide important issues that come up in real life applications for which raw data is often the only available input. In terms of output, HYSSOP differs from most other systems in that it generates hypertext instead of linear text. It thus tackles the content aggregation problem in a particularly demanding application requiring the generator to simultaneously start from raw data, produce hypertext output and enforce conciseness constraints. 3.3 Generation architecture and aggregation localization While its overall architecture is a conventional pipeline, HYSSOP is unique in encapsulating all aggregation processing in the sentence planner and carrying it out entirely on a deep semantic representation. In contrast, most other systems distribute aggregation over several processing components and across several levels of internal representations: deep semantic, thematic and even surface syntactic for some of them. 3.4 Data structures and algorithms for aggregation All previous approaches to aggregations relied on rules that included some domain-specific semantic or lexical information. In contrast, the aggregation algorithm used by HYSSOP is .domain independent since it relies only on (1) generic matrix row and column shuffling operations, and (2) on a generic similarity data cells. 4 Conclusion We presented a new approach to content aggregation in the context of a very challenging and practical generation summarizing OLAP and data mining discoveries 131 as a few linked web pages of fluent and concise natural language. We believe that the key contribution to our work is to show the feasibility to perform effective paratactic aggregation: • encapsulated within a single generation component (the sentence planner) • using a domain-independent algorithm and a simple data strueture,&amp;quot; the fktoriiation that the key structural and ordering constraints on aggregation while completely abstracting from domain semantic idiosyncrasies as well as from lexical and syntactic details. This is a first success towards the development of a plug-in content aggregation component for text generation, reusable across application domains. In future work, we intend to empirically evaluate the summaries generated by HYSSOP.</abstract>
<note confidence="0.676556214285714">References Ait-Kaci H. and Lincoln P. (1989) LIFE — A natural for natural language. . Informations, 30(1-2):37-67, Association pour le Traitement Automatique des Langues, Paris France. Carcagno D. and lordanskaja L. (1993) Content determination and text structuring; two interrelated In H Horacek (ed.) concepts in Planning, realisation and systems. Pinter Publishers, pp 10-26. Dalianis H. (1995) Aggregation, Formal specification Natural Language Generation. Proc. of the First International Workshop on the of NL to Databases,</note>
<address confidence="0.77712">Versailles, France</address>
<abstract confidence="0.818517875">Dalianis H. (1996) Aggregation as a subtask of text sentence planning. Proc. of Florida Al symposium, RS-96, Florida, pp 1-5. Elhadad M., McKeown K. and Robin J. (1997) Floating constraints in lexical Linguistics, E. L. (2000). hypertext summaries of data mining discoveries in multidimensional Thesis. Centro de Informatica, UFPE, Recife, Brazil. E. L. and Robin Using OLAP and data mining for content planning in natural language generation. Accepted for publication in Proc. of SInternational. Conference on Applications of Natural Language to Information June, France. L. Robin Text Generation as Featured Definite Clause Grammars. to Language Engineering. index.html Huang G. and Fiedler A (1996) Paraphrasing and aggregation argumentative text using text structure.</abstract>
<note confidence="0.934578944444444">In Proc. of the 8th International NLG Workshop, pages 21-3, Sussex, UK. R. (1985) and Cognition. Press, Cambridge, MA, June 15-17. Kukich K. (1988) Fluency in Natural Language in Language Generation Systems, McDonald, D. &amp; Bloc, L. (Eds.), Springer-Verlag. K., and Shaw J. (1994) issues in automatic Proc. of ANLP&apos;94, 7-14, Stuttgart, Oct. Meteer M. (1992) Expressibility and the problem of text planning. in Artificial Publisher Reiter E. (1994) Has a Consensus NL Generation Architecture Appeared, and is Plausible? Proc of the Seventh International Workshop on Natural Generation (INLGW-1994), 163-</note>
<address confidence="0.560029">170. Kennebunkport, Maine, USA.</address>
<abstract confidence="0.89813675">J. (1995) generation of natural language summaries providing historical background: corpus-based analysis, design, and evaluation. Thesis.</abstract>
<note confidence="0.771154153846154">CUCS-034-94, Columbia University, Computer Science Department , New York, USA. 357p. Robin J. and McKeown K. (1996) Empirically designing and evaluating a new revision-based for summary generation. 57p. Sarawagi S. Agrawal R and Megiddo N. (1998) Discovery-driven exploration of MDDB data Proc. Int. Conf. of Extending Database (EDIT &apos;98), Shaw J. (1998) Segregatory coordination and ellipsis text generation. Proc. of the COLING &apos;98. 132</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Ait-Kaci</author>
<author>P Lincoln</author>
</authors>
<title>LIFE — A natural language for natural language.</title>
<date>1989</date>
<journal>TA . Informations,</journal>
<pages>30--1</pages>
<location>Paris</location>
<marker>Ait-Kaci, Lincoln, 1989</marker>
<rawString>Ait-Kaci H. and Lincoln P. (1989) LIFE — A natural language for natural language. TA . Informations, 30(1-2):37-67, Association pour le Traitement Automatique des Langues, Paris France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Carcagno</author>
<author>L lordanskaja</author>
</authors>
<title>Content determination and text structuring; two interrelated processes.</title>
<date>1993</date>
<booktitle>In H Horacek (ed.) New concepts in NLG: Planning, realisation and systems.</booktitle>
<pages>10--26</pages>
<publisher>Pinter Publishers,</publisher>
<location>London:</location>
<contexts>
<context position="3413" citStr="Carcagno and lordanskaja 1993" startWordPosition="494" endWordPosition="497">, as well as for details on the motivation and technology underlying MATRIKS. 124 OLAP and data mining, these two technologies are reciprocally the only ones able to completely fulfill the content determination needs of a key NLG application sub-class: textual summarization of quantitative data. in most cases, it is its very unexpectedness that makes a fact interesting to report; Data warehouse hypercube exploration expert system Fig. I — The architecture of MATRIKS Generators that summarize large amount of quantitative data by a short natural language text (such as ANA (Kukich 1988), GOSSIP (Carcagno and lordanskaja 1993), PLANDoc (McKeown, Kukich and Shaw 1994) among others) generally perform content determination by relying on a fixed set of domain-dependent heuristic rules. Such an approach suffers from two severe limitations that prevent it from reporting the most interesting content from an underlying database: • it does not scale up for analytical contexts with high dimensionality and which take into account the historical.—evolution of data through time; such complex context would require a combinatorially explosive number of summary content determination heuristic rules; • it can only select facts whos</context>
</contexts>
<marker>Carcagno, lordanskaja, 1993</marker>
<rawString>Carcagno D. and lordanskaja L. (1993) Content determination and text structuring; two interrelated processes. In H Horacek (ed.) New concepts in NLG: Planning, realisation and systems. London: Pinter Publishers, pp 10-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dalianis</author>
</authors>
<title>Aggregation, Formal specification and Natural Language Generation.</title>
<date>1995</date>
<booktitle>In Proc. of the NLDB.95 First International Workshop on the application of NL to Databases,</booktitle>
<pages>135--149</pages>
<location>Versailles, France</location>
<contexts>
<context position="22315" citStr="Dalianis 1995" startWordPosition="3591" endWordPosition="3592">=flime=7, var =4D] °A% falling 40% from Jun to Jul I cal=msg, attr=ftime=9 var =-32 %% and then a further 32 from Sep to Oct cat =msg, attr .,[product =&amp;quot;Dief Soda&amp;quot;, time =9, place =east var=-33 anaph foccurr =2nd, repeated-product, place] VA again Diet Soda Eastern sales, falling 33% from Sep to Oct cat =aggr, %% Less aberrant but still notably atypical were: Fig. 12 — Fragment of LIFE feature structure representing the discourse tree output of the sentence planner and input to the lexicalizer. 3 Related work in content aggregation The main previous works on content aggregation are due to: O (Dalianis 1995, 1996), whose ASTROGEN system generates natural language paraphrases of formal software specification for validation purposes; 6 (Huang and Fiedler 1997), whose PROVERB system generates natural language mathematical proofs from a theorem prover reasoning trace; o (Robin and McKeown, 1996), whose STREAK system generates basketball game summaries from a semantic network 130 representing the key game statistics and their historical context; * (Shaw 1998), whose CASPER discourse and sentence planner has been used both in the PLANDoc system that generates telecommunication equipment installation p</context>
</contexts>
<marker>Dalianis, 1995</marker>
<rawString>Dalianis H. (1995) Aggregation, Formal specification and Natural Language Generation. In Proc. of the NLDB.95 First International Workshop on the application of NL to Databases, 135-149, Versailles, France</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dalianis</author>
</authors>
<title>Aggregation as a subtask of text and sentence planning.</title>
<date>1996</date>
<booktitle>In Proc. of Florida Al Research symposium, FLAI RS-96,</booktitle>
<pages>1--5</pages>
<location>Florida,</location>
<marker>Dalianis, 1996</marker>
<rawString>Dalianis H. (1996) Aggregation as a subtask of text and sentence planning. In Proc. of Florida Al Research symposium, FLAI RS-96, Florida, pp 1-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
<author>K McKeown</author>
<author>J Robin</author>
</authors>
<title>Floating constraints in lexical choice.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<marker>Elhadad, McKeown, Robin, 1997</marker>
<rawString>Elhadad M., McKeown K. and Robin J. (1997) Floating constraints in lexical choice. Computational Linguistics, 23(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E L Favero</author>
</authors>
<title>Generating- hypertext summaries of data mining discoveries in multidimensional databases. PhD Thesis. Centro de Informatica,</title>
<date>2000</date>
<location>UFPE, Recife, Brazil.</location>
<contexts>
<context position="2743" citStr="Favero (2000)" startWordPosition="394" endWordPosition="395">most concise and familiar way: a few web pages of natural language. These two extensions allow an IDSS to be used directly by decision makers without constant mediation of a data analyst. 1.2 The HYSSOP natural language hypertext summary generator To our knowledge, the development of HYSSOP is pioneer work in coupling OLAP and data mining with natural language generation, Fig. 2. We view such coupling as a synergetic fit with tremendous potential for a wide range of practical applications. In a nutshell&apos;, while NLG is the only technology able to completely fulfill the reporting needs of &apos; See Favero (2000) for further justification for this view, as well as for details on the motivation and technology underlying MATRIKS. 124 OLAP and data mining, these two technologies are reciprocally the only ones able to completely fulfill the content determination needs of a key NLG application sub-class: textual summarization of quantitative data. in most cases, it is its very unexpectedness that makes a fact interesting to report; Data warehouse hypercube exploration expert system Fig. I — The architecture of MATRIKS Generators that summarize large amount of quantitative data by a short natural language t</context>
</contexts>
<marker>Favero, 2000</marker>
<rawString>Favero E. L. (2000). Generating- hypertext summaries of data mining discoveries in multidimensional databases. PhD Thesis. Centro de Informatica, UFPE, Recife, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E L Favero</author>
<author>J Robin</author>
</authors>
<title>Using OLAP and data mining for content planning in natural language generation. Accepted for publication in</title>
<date>2000</date>
<booktitle>Proc. of SInternational. Conference on Applications of Natural Language to Information Systems,</booktitle>
<volume>2000</volume>
<pages>28--30</pages>
<location>Versailles</location>
<contexts>
<context position="8623" citStr="Favero and Robin 2000" startWordPosition="1384" endWordPosition="1387"> in Fig. 2. HYSSOP is entirely implemented in LIFE (AltKaci and Lincoln.1989), a language that extends Prolog with functional programming, arityless feature structure unification and hierarchical type constraint inheritance. For content realization, HYSSOP relies on feature structure unification. Lexicalization is inspired from the approach described in (Elhadad. McKeown and Robin 1997), while surface syntactic realization follows the approach described in (Favero and Robin .200 Ob ); HYS SOP-- makes two innovative contributions to NLG research: one to hypertext content planning presented in (Favero and Robin 2000a) and one to content aggregation presented in the rest of this paper. 2 Research focus: content aggregation 126 in natural language generation Natural language generation system is traditionally decomposed in the following subtasks: content determination, discourse-level content organization, sentence-level content organization, lexical content realization and grammatical content realization. The first three . subtasks toget bera.re Jolt= referred to As zontent planning, and the last two together as linguistic realization. This separation is now fairly standard and most implementations encaps</context>
</contexts>
<marker>Favero, Robin, 2000</marker>
<rawString>Favero E. L. and Robin J. (2000a). Using OLAP and data mining for content planning in natural language generation. Accepted for publication in Proc. of SInternational. Conference on Applications of Natural Language to Information Systems, NLDB&apos;2000, 28-30 June, Versailles France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E L Favero</author>
<author>J Robin</author>
</authors>
<title>Implementing Functional Unification Grammars for Text Generation as Featured Definite Clause Grammars. Submitted to Natural Language Engineering. _DIActiner,</title>
<date>2000</date>
<booktitle>Attp.://d1xcs.sfaLedu/Dalvtiner/ index.html</booktitle>
<contexts>
<context position="8623" citStr="Favero and Robin 2000" startWordPosition="1384" endWordPosition="1387"> in Fig. 2. HYSSOP is entirely implemented in LIFE (AltKaci and Lincoln.1989), a language that extends Prolog with functional programming, arityless feature structure unification and hierarchical type constraint inheritance. For content realization, HYSSOP relies on feature structure unification. Lexicalization is inspired from the approach described in (Elhadad. McKeown and Robin 1997), while surface syntactic realization follows the approach described in (Favero and Robin .200 Ob ); HYS SOP-- makes two innovative contributions to NLG research: one to hypertext content planning presented in (Favero and Robin 2000a) and one to content aggregation presented in the rest of this paper. 2 Research focus: content aggregation 126 in natural language generation Natural language generation system is traditionally decomposed in the following subtasks: content determination, discourse-level content organization, sentence-level content organization, lexical content realization and grammatical content realization. The first three . subtasks toget bera.re Jolt= referred to As zontent planning, and the last two together as linguistic realization. This separation is now fairly standard and most implementations encaps</context>
</contexts>
<marker>Favero, Robin, 2000</marker>
<rawString>Favero E. L. and Robin J. (2000b). Implementing Functional Unification Grammars for Text Generation as Featured Definite Clause Grammars. Submitted to Natural Language Engineering. _DIActiner, Attp.://d1xcs.sfaLedu/Dalvtiner/ index.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Huang</author>
<author>A Fiedler</author>
</authors>
<title>Paraphrasing and aggregation argumentative text using text structure.</title>
<date>1996</date>
<booktitle>In Proc. of the 8th International NLG Workshop,</booktitle>
<pages>21--3</pages>
<location>Sussex, UK.</location>
<marker>Huang, Fiedler, 1996</marker>
<rawString>Huang G. and Fiedler A (1996) Paraphrasing and aggregation argumentative text using text structure. In Proc. of the 8th International NLG Workshop, pages 21-3, Sussex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>Semantics and Cognition.</title>
<date>1985</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="24930" citStr="Jackendoff 1985" startWordPosition="3976" endWordPosition="3977">. In contrast, ASTROGEN, CASPER, PROVERB and STREAK also perform hypotactic and paradigmatic aggregation. 3.2 Input representation and generated output text A second characteristic that sets HYSSOP apart from other generators performing aggregation is the nature of its input: a set of data cells extracted from a dimensional data warehouse hypercube. In contrast, the other systems all take as input either a semantic .network extracted from a knowledge base or a pre-linguistic representation of the text to generate such as Meteer&apos;s text structure (Meteer 1992) or Jackendoffs semantic structure (Jackendoff 1985). Such natural language processing —oriented inputs tend ld&apos;slinplify the overall -text generation task and hide important issues that come up in real life applications for which raw data is often the only available input. In terms of output, HYSSOP differs from most other systems in that it generates hypertext instead of linear text. It thus tackles the content aggregation problem in a particularly demanding application requiring the generator to simultaneously start from raw data, produce hypertext output and enforce conciseness constraints. 3.3 Generation architecture and aggregation locali</context>
</contexts>
<marker>Jackendoff, 1985</marker>
<rawString>Jackendoff R. (1985) Semantics and Cognition. MIT Press, Cambridge, MA, June 15-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Fluency in Natural Language Reports in</title>
<date>1988</date>
<journal>Natural Language Generation Systems, McDonald, D. &amp; Bloc,</journal>
<editor>L. (Eds.),</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="3373" citStr="Kukich 1988" startWordPosition="491" endWordPosition="492">fication for this view, as well as for details on the motivation and technology underlying MATRIKS. 124 OLAP and data mining, these two technologies are reciprocally the only ones able to completely fulfill the content determination needs of a key NLG application sub-class: textual summarization of quantitative data. in most cases, it is its very unexpectedness that makes a fact interesting to report; Data warehouse hypercube exploration expert system Fig. I — The architecture of MATRIKS Generators that summarize large amount of quantitative data by a short natural language text (such as ANA (Kukich 1988), GOSSIP (Carcagno and lordanskaja 1993), PLANDoc (McKeown, Kukich and Shaw 1994) among others) generally perform content determination by relying on a fixed set of domain-dependent heuristic rules. Such an approach suffers from two severe limitations that prevent it from reporting the most interesting content from an underlying database: • it does not scale up for analytical contexts with high dimensionality and which take into account the historical.—evolution of data through time; such complex context would require a combinatorially explosive number of summary content determination heuristi</context>
</contexts>
<marker>Kukich, 1988</marker>
<rawString>Kukich K. (1988) Fluency in Natural Language Reports in Natural Language Generation Systems, McDonald, D. &amp; Bloc, L. (Eds.), Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>K Kukich</author>
<author>J Shaw</author>
</authors>
<title>Practical issues in automatic document generation.</title>
<date>1994</date>
<booktitle>In Proc. of ANLP&apos;94,</booktitle>
<pages>7--14</pages>
<location>Stuttgart,</location>
<marker>McKeown, Kukich, Shaw, 1994</marker>
<rawString>McKeown K., Kukich, K. and Shaw J. (1994) Practical issues in automatic document generation. In Proc. of ANLP&apos;94, pages 7-14, Stuttgart, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meteer</author>
</authors>
<title>Expressibility and the problem of efficient text planning.</title>
<date>1992</date>
<booktitle>Communication in Artificial Intelligence.</booktitle>
<publisher>Pinter Publisher Limited,</publisher>
<location>London.</location>
<contexts>
<context position="24878" citStr="Meteer 1992" startWordPosition="3970" endWordPosition="3971">YSSOP focuses on paratactic forms of aggregation. In contrast, ASTROGEN, CASPER, PROVERB and STREAK also perform hypotactic and paradigmatic aggregation. 3.2 Input representation and generated output text A second characteristic that sets HYSSOP apart from other generators performing aggregation is the nature of its input: a set of data cells extracted from a dimensional data warehouse hypercube. In contrast, the other systems all take as input either a semantic .network extracted from a knowledge base or a pre-linguistic representation of the text to generate such as Meteer&apos;s text structure (Meteer 1992) or Jackendoffs semantic structure (Jackendoff 1985). Such natural language processing —oriented inputs tend ld&apos;slinplify the overall -text generation task and hide important issues that come up in real life applications for which raw data is often the only available input. In terms of output, HYSSOP differs from most other systems in that it generates hypertext instead of linear text. It thus tackles the content aggregation problem in a particularly demanding application requiring the generator to simultaneously start from raw data, produce hypertext output and enforce conciseness constraints</context>
</contexts>
<marker>Meteer, 1992</marker>
<rawString>Meteer M. (1992) Expressibility and the problem of efficient text planning. Communication in Artificial Intelligence. Pinter Publisher Limited, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
</authors>
<title>Has a Consensus NL Generation Architecture Appeared, and is it Psycholinguistically Plausible?</title>
<date>1994</date>
<booktitle>In Proc of the Seventh International Workshop on Natural Language Generation (INLGW-1994),</booktitle>
<pages>163--170</pages>
<location>Kennebunkport, Maine, USA.</location>
<contexts>
<context position="9287" citStr="Reiter 1994" startWordPosition="1475" endWordPosition="1476">est of this paper. 2 Research focus: content aggregation 126 in natural language generation Natural language generation system is traditionally decomposed in the following subtasks: content determination, discourse-level content organization, sentence-level content organization, lexical content realization and grammatical content realization. The first three . subtasks toget bera.re Jolt= referred to As zontent planning, and the last two together as linguistic realization. This separation is now fairly standard and most implementations encapsulate each task in a separate module (Robin 1995), (Reiter 1994). Another generation subtask that has recently received much attention is content aggregation. However, there is still no consensus on the exact scope of aggregation and on its precise relation with the five standard generation tasks listed above. To avoid ambiguity, we define aggregation here as: grouping several content units, sharing various semantic features, inside a single linguistic structure, in such a way that the shared features are maximally factored out and minimally repeated in the generated text. Defined as above, aggregation is essentially a key subtask of sentence planning. As </context>
</contexts>
<marker>Reiter, 1994</marker>
<rawString>Reiter E. (1994) Has a Consensus NL Generation Architecture Appeared, and is it Psycholinguistically Plausible? In Proc of the Seventh International Workshop on Natural Language Generation (INLGW-1994), pages 163-170. Kennebunkport, Maine, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robin</author>
</authors>
<title>Revision-based generation of natural language summaries providing historical background: corpus-based analysis, design, implementation and evaluation.</title>
<date>1995</date>
<tech>Ph.D. Thesis. CUCS-034-94,</tech>
<institution>Columbia University, Computer Science Department ,</institution>
<location>New York, USA. 357p.</location>
<contexts>
<context position="9272" citStr="Robin 1995" startWordPosition="1473" endWordPosition="1474">ented in the rest of this paper. 2 Research focus: content aggregation 126 in natural language generation Natural language generation system is traditionally decomposed in the following subtasks: content determination, discourse-level content organization, sentence-level content organization, lexical content realization and grammatical content realization. The first three . subtasks toget bera.re Jolt= referred to As zontent planning, and the last two together as linguistic realization. This separation is now fairly standard and most implementations encapsulate each task in a separate module (Robin 1995), (Reiter 1994). Another generation subtask that has recently received much attention is content aggregation. However, there is still no consensus on the exact scope of aggregation and on its precise relation with the five standard generation tasks listed above. To avoid ambiguity, we define aggregation here as: grouping several content units, sharing various semantic features, inside a single linguistic structure, in such a way that the shared features are maximally factored out and minimally repeated in the generated text. Defined as above, aggregation is essentially a key subtask of sentenc</context>
</contexts>
<marker>Robin, 1995</marker>
<rawString>Robin J. (1995) Revision-based generation of natural language summaries providing historical background: corpus-based analysis, design, implementation and evaluation. Ph.D. Thesis. CUCS-034-94, Columbia University, Computer Science Department , New York, USA. 357p.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robin</author>
<author>K McKeown</author>
</authors>
<title>Empirically designing and evaluating a new revision-based model for summary generation.</title>
<date>1996</date>
<journal>Artificial Intelligence,</journal>
<volume>85</volume>
<issue>1</issue>
<pages>57</pages>
<contexts>
<context position="22605" citStr="Robin and McKeown, 1996" startWordPosition="3628" endWordPosition="3631">g 33% from Sep to Oct cat =aggr, %% Less aberrant but still notably atypical were: Fig. 12 — Fragment of LIFE feature structure representing the discourse tree output of the sentence planner and input to the lexicalizer. 3 Related work in content aggregation The main previous works on content aggregation are due to: O (Dalianis 1995, 1996), whose ASTROGEN system generates natural language paraphrases of formal software specification for validation purposes; 6 (Huang and Fiedler 1997), whose PROVERB system generates natural language mathematical proofs from a theorem prover reasoning trace; o (Robin and McKeown, 1996), whose STREAK system generates basketball game summaries from a semantic network 130 representing the key game statistics and their historical context; * (Shaw 1998), whose CASPER discourse and sentence planner has been used both in the PLANDoc system that generates telecommunication equipment installation plan documentation from an expert system trace and the MAGIC system that generates ICU patient status - briefs -.froin medieg measurements. In this section, we briefly compare these research efforts with ours along four dimensions: (1) the definition of aggregation and the scope of the aggr</context>
</contexts>
<marker>Robin, McKeown, 1996</marker>
<rawString>Robin J. and McKeown K. (1996) Empirically designing and evaluating a new revision-based model for summary generation. Artificial Intelligence, 85(1-2). 57p.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarawagi S Agrawal R</author>
<author>N Megiddo</author>
</authors>
<title>Discovery-driven exploration of MDDB data cubes.</title>
<date>1998</date>
<booktitle>In Proc. Int. Conf. of Extending Database Technology (EDIT &apos;98),</booktitle>
<marker>R, Megiddo, 1998</marker>
<rawString>Sarawagi S. Agrawal R and Megiddo N. (1998) Discovery-driven exploration of MDDB data cubes. In Proc. Int. Conf. of Extending Database Technology (EDIT &apos;98), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shaw</author>
</authors>
<title>Segregatory coordination and ellipsis in text generation.</title>
<date>1998</date>
<booktitle>In Proc. of the 17th COLING &apos;98.</booktitle>
<contexts>
<context position="22771" citStr="Shaw 1998" startWordPosition="3654" endWordPosition="3655">ce planner and input to the lexicalizer. 3 Related work in content aggregation The main previous works on content aggregation are due to: O (Dalianis 1995, 1996), whose ASTROGEN system generates natural language paraphrases of formal software specification for validation purposes; 6 (Huang and Fiedler 1997), whose PROVERB system generates natural language mathematical proofs from a theorem prover reasoning trace; o (Robin and McKeown, 1996), whose STREAK system generates basketball game summaries from a semantic network 130 representing the key game statistics and their historical context; * (Shaw 1998), whose CASPER discourse and sentence planner has been used both in the PLANDoc system that generates telecommunication equipment installation plan documentation from an expert system trace and the MAGIC system that generates ICU patient status - briefs -.froin medieg measurements. In this section, we briefly compare these research efforts with ours along four dimensions: (1) the definition of aggregation and the scope of the aggregation task implemented in the generator, (2) the type of representation the generator takes as input and the type of output text that it produces, (3) the generator</context>
</contexts>
<marker>Shaw, 1998</marker>
<rawString>Shaw J. (1998) Segregatory coordination and ellipsis in text generation. In Proc. of the 17th COLING &apos;98.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>