<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.033347">
<title confidence="0.958905">
EHU-ALM: Similarity-Feature Based Approach for Student Response
Analysis
</title>
<author confidence="0.984517">
Itziar Aldabe, Montse Maritxalar Oier Lopez de Lacalle
</author>
<affiliation confidence="0.9932275">
IXA NLP Group University of Edinburgh
University of Basque Country (UPV-EHU) IKERBASQUE,
</affiliation>
<email confidence="0.8328975">
itziar.aldabe@ehu.es Basque Foundation for Science
montse.maritxalar@ehu.es oier.lopezdelacalle@gmail.com
</email>
<sectionHeader confidence="0.995571" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.990610875">
We present a 5-way supervised system based
on syntactic-semantic similarity features. The
model deploys: Text overlap measures,
WordNet-based lexical similarities, graph-
based similarities, corpus-based similarities,
syntactic structure overlap and predicate-
argument overlap measures. These measures
are applied to question, reference answer and
student answer triplets. We take into account
the negation in the syntactic and predicate-
argument overlap measures. Our system uses
the domain-specific data as one dataset to
build a robust system. The results show that
our system is above the median and mean on
all the evaluation scenarios of the SemEval-
2013 task #7.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99857735">
In this paper we describe our participation with a
feature-based supervised system to the SemEval-
2013 task #7: The Joint Student Response Analy-
sis and 8th Recognizing Textual Entailment Chal-
lenge (Dzikovska et al., 2013). The goal of our
participation is to build a generic system that is
robust enough across domains and scenarios. A
domain-specific system requires new training ex-
amples when shifting to a new domain. However,
domain-specific data is difficult to obtain and creat-
ing new resources is expensive.
We seek robustness by mixing the instances from
BEETLE and SCIENTSBANK. We show our strategy
is suitable to build a generic system that performs
competitively on any domain in the 5-way task.
The paper proceeds as follows. Section 2 de-
scribes the system presenting the learning features
and the runs. In Section 3 we show the optimiza-
tion details, followed by the results (Section 4) and
a preliminary error analysis (Section 5).
</bodyText>
<sectionHeader confidence="0.962694" genericHeader="method">
2 System description
</sectionHeader>
<bodyText confidence="0.999890789473684">
Our system aims for robustness using the domain-
specific training data as one dataset. Therefore,
we do not differentiate between examples from the
given domains (BEETLE and SCIENTSBANK) when
training the system. In contrast, our approach dintin-
guishes between new questions (unseen answer vs.
unseen question) as well as question types (how,
what and why) by means of simple heuristics.
The runs are organized according to different sys-
tem designs. Although all the runs use the same fea-
ture set, we split the training set to build more spe-
cialized classifiers. Training examples are grouped
depending on: i) the answer is unseen; ii) the ques-
tion is unseen; and iii) the question type (i.e. what,
how, why). Each run defines a framework to explore
the different ways to approach the problem. While
the first run is the simplest and is the most generic
in nature, the third tries to split the task into simpler
problems and creates more specialized classifiers.
</bodyText>
<subsectionHeader confidence="0.996238">
2.1 Similarity learning features
</subsectionHeader>
<bodyText confidence="0.999834666666667">
Our model is based on various text similarity fea-
tures. Almost all of the measures are computed be-
tween question, reference answer and student an-
swer triplets. The measures based on syntactic struc-
ture and predicate-argument overlaps are only ap-
plied to the student and reference answer pairs. In
</bodyText>
<page confidence="0.933983">
580
</page>
<bodyText confidence="0.999079765957447">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 580–584, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
total, we defined 30 features which can be grouped
as follows:
resulting vectors associated with each text in the
latent space.
Text overlap measures The similarity of two texts
is computed based on the number of overlapping
words. We obtain the similarity of two texts based
on the F-Measure, the Dice Coefficient, The Cosine,
and the Lesk measures. For that, we use the imple-
mentation available in the Text::Similarity package1.
WordNet-based lexical similarities All the simi-
larity metrics based on WordNet (Miller, 1995) fol-
low the methodology proposed in (Mihalcea et al.,
2006). For each open-class word in one of the in-
put texts, we obtain the maximun semantic similar-
ity or relatedness value matching the same open-
class words in the other input text. The values of
each matching are summed up and normalized by
the length of the two input texts as explained in
(Mihalcea et al., 2006). We compute the measures
of Resnik, Lin, Jiang-Conrath, Leacock-Chodorow,
Wu-Palmer, Banerjee-Pedersen, and Patwardhan-
Pedersen provided in the WordNet::Similarity pack-
age (Patwardhan et al., 2003).
Graph-based similarities The similarity of two
texts is based on a graph-based representation
(Agirre and Soroa, 2009) of WordNet. The method
is a two-step process: first the personalized PageR-
ank over WordNet is computed for each text. This
produces a probability distribution over WordNet.
Then, the probability distributions are encoded as
vectors and the cosine similarity between those vec-
tors is calculated.
Corpus-based similarities We compute two
corpus-based similarity measures: Latent Semantic
Analysis (Deerwester et al., 1990) and Latent
Dirichlet Allocation (Blei et al., 2003). We estimate
100 dimensions for LSA and 50 topics for LDA.
Both models are obtained from a subset of the En-
glish Wikipedia following the hierarchy of science
categories. We started with a small set of categories
and recovered the articles below the sub-hierarchy.
We only went 3 levels down to avoid noisy articles
as the category system is rather flat. The similarity
of two texts is the cosine similarity between the
</bodyText>
<footnote confidence="0.970684">
1http://www.d.umn.edu/ tpederse/text-similarity.html
</footnote>
<bodyText confidence="0.99988324">
Syntactic structure overlap The role of syntax is
studied by the use of graph subsumption based on
the approach proposed in (McCarthy et al., 2008).
The text is mapped into a graph with nodes rep-
resenting words and links indicating syntactic de-
pendencies between them. The similarity of two
texts is computed based on the overlap of the syn-
tactic structures. Negation is handled explicitly in
the graph.
Predicate-argument overlap The similarity of
two texts is computed by analyzing the overlap of
the predicates and their associated semantic argu-
ments. The system looks for verbal and nominal
predicates. The similarity is also based on the ap-
proach proposed in (McCarthy et al., 2008). The
graph is represented with words as nodes and the
semantic role of arguments as links. First, the ver-
bal propositions and their arguments are automat-
ically obtained (Bj¨orkelund et al., 2009) as repre-
sented in PropBank (Palmer et al., 2005). Second,
a generalization of the predicates is obtained based
on VerbNet (Kipper, 2005) and NomBank (Meyers
et al., 2004). Finally, the similarity of two texts
is computed based on the overlap of the predicate-
argument relations.
</bodyText>
<subsectionHeader confidence="0.999777">
2.2 Architecture of the runs
</subsectionHeader>
<bodyText confidence="0.999828666666667">
Generic Framework RUN1 This is the simplest
framework for the assessment of student answers.
The system relies on a single classifier, which has
been optimized on the unseen question scenario.
The scenario is simulated by splitting the training
set so that each question and its answers are in the
same fold.
Unseen Framework RUN2 This framework relies
on two classifiers. The first is tuned on an unseen
answer scenario and the second is prepared for the
question scenario (cf. RUN1). In order to build the
unseen answer classifier, we split the training set so
that answers to the same question can occur in dif-
ferent folders. In test time, the instance is classified
depending on whether it is an unseen answer or an
</bodyText>
<page confidence="0.983019">
581
</page>
<table confidence="0.999636">
BEETLE SCIENTSBANK OVERALL
Uns-answ Uns-qst All Uns-answ Uns-qst Uns-dom All All
RUN1 0.499 (6) 0.352 (7) 0.404 0.396 (7) 0.283 (4) 0.345 (3) 0.348 0.406
RUN2 0.526 (4) 0.352 (7) 0.413 0.418 (6) 0.283 (4) 0.345 (3) 0.350 0.414
RUN3 0.502 (5) 0.370 (6) 0.415 0.424 (5) 0.260 (8) 0.337 (5) 0.340 0.403
LOWEST 0.170 0.173 - 0.089 0.095 0.121 - -
BEST 0.619 0.552 - 0.478 0.307 0.380 - -
MEAN 0.435 0.343 - 0.341 0.240 0.267 - -
MEDIAN 0.437 0.326 - 0.376 0.259 0.268 - -
</table>
<tableCaption confidence="0.9018818">
Table 1: 5-way results of the runs in F1 macro-average on BEETLE and SCIENTSBANK domains across different
scenarios. Along with the runs, the LOWEST and the BEST system in each scenario are shown. The MEAN and
MEDIAN of the dataset are also presented. Finally, the OVERALL results are showed summing up both domains. Uns-
answ refers to unseen answers scenario, Uns-qst stands for unseen question, Uns-dom unseen domain and All refers
to the sum of all scenarios. The run results are presented together with the ranked position in the task.
</tableCaption>
<bodyText confidence="0.975231545454546">
unseen question2.
Question-type Framework RUN3 The run con-
sists of a set of question-type expert classifiers. We
divided the training set based on whether an instance
reflected a what, how or why question. We then par-
titioned each question type into unseen answer and
unseen question scenarios. In total, the framework
deploys 6 classifiers, i.e. a test instance is classified
according to the question type and scenario. We set
heuristics to automatically distinguish the instance
type.
</bodyText>
<sectionHeader confidence="0.983167" genericHeader="method">
3 Optimization on training set
</sectionHeader>
<bodyText confidence="0.999731176470588">
We set a heuristic to create the training instances.
For each student answer, if the matching reference
answer is indicated in it, we create a triplet with the
question, the student answer, and the matching ref-
erence answer. If there is no matching answer, the
reference answer is randomly selected giving pref-
erence to the best reference answers.
Once we have a training set, we split it into dif-
ferent ways to simulate the scenarios described in
Section 2.2. All the models are optimized using 10-
fold cross-validation of the pertaining training set.
For the classifiers in RUN1 and RUN2 we used 8910
training instances. For RUN3 the instances were di-
vided as follows: 1235 instances for how questions,
3089 for what questions and 4589 for why ques-
tions. In total, we obtained 8 models which were
distributed through the runs.
</bodyText>
<footnote confidence="0.760682">
2We treat unseen-domain instances as unseen-question in-
stances.
</footnote>
<bodyText confidence="0.999887285714286">
Our approach uses Support Vector Ma-
chine (Chang and Lin, 2011) to build the classifiers.
As the number of features is not high, we used the
gaussian kernel in order to solve the non-linear
problem. The main parameters of the kernel (-y and
C) were tuned using grid search over the parameter
in the cross-validation setting. We focused on
optimizing the F1 macro average of the classifier
in order to avoid a bias towards the major classes.
Each of the 8 classifiers were tuned independently.
The triplets of question, student answer and ref-
erence answer of the test instances were always cre-
ated selecting the first reference answer of the given
set of answers.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999919176470588">
A total of 8 teams participated in the 5-way task,
submitting a total of 16 system runs (Dzikovska et
al., 2013). Table 1 shows the performance obtained
by our systems across domains and different scenar-
ios. Our three runs ranked differently based on the
evaluation scenario: beetle-uns-answ (6,4,5 rank for
RUN1, RUN2, RUN3, respectively); beetle-uns-qst
(7,7,6); scientsbank-uns-answ (7,6,5); scientsbank-
uns-qst (4,4,8) and scientsbank-uns-dom (3,3,5). We
also evaluated our runs on the entire domain (All
columns) and on the whole test set (OVERALL).
The results show we built robust systems. Despite
being below the best system of each evaluation sce-
nario, the results show that the runs are competitive.
All our runs are above the median and outperform
the average results on each evaluation. Overall, the
results attained in SCIENTSBANK are lower than in
</bodyText>
<page confidence="0.99484">
582
</page>
<bodyText confidence="0.999558307692308">
BEETLE. This might be due to the questions and
answers being longer in SCIENTSBANK, making it
difficult to obtain good patterns.
As regards our runs, there is no significant overall
difference. While RUN3 performs better in BEETLE
unseen question and SCIENTSBANK unseen answer,
in the rest of scenarios RUN2 outperforms the rest
of the runs. As expected, RUN2 outperforms RUN1
in the unseen answer scenario since the former has
a module specializing in unseen answers. However,
although RUN3 is an ensemble of six classifiers, it is
not the best run. This is probably because the train-
ing sets are not big enough.
</bodyText>
<table confidence="0.980472666666667">
Unseen framework (RUN2)
Prec Rec F1
correct 0.552 0.677 0.608
partially correct 0.324 0.323 0.323
contradictory 0.239 0.121 0.160
irrelevant 0.472 0.377 0.419
non domain 0.415 0.849 0.557
Macro average 0.400 0.469 0.414
Micro average 0.443 0.464 0.446
</table>
<tableCaption confidence="0.999703">
Table 2: results of the RUN2 system on a entire test set.
</tableCaption>
<bodyText confidence="0.99839425">
Table 2 shows the detailed results of the RUN2
system on the entire test set. It is noticeable the
low results obtained on the contradictory class. This
might be because the defined features are not able
to model negation properly and do not deal with
antonymy. Surprisingly, the non domain class is not
the most problematic, even if the system was trained
on a low number of instances.
</bodyText>
<sectionHeader confidence="0.937133" genericHeader="conclusions">
5 Preliminary Error Analysis
</sectionHeader>
<bodyText confidence="0.9670872">
We conducted a preliminary error analysis and stud-
ied some of the misclassified test instances to detect
some problematic issues and to define improvements
to our approach.
Example 5.1 Sam and Jasmine were sitting on a
park bench eating their lunches. A mosquito landed
on Sam’s arm and Sam began slapping at it. When
he did that, he knocked Jasmine’s soda into her lap,
causing her to jump up. What was Sam’s response?
R: Sam’s response was to slap the mosquito.
</bodyText>
<footnote confidence="0.35786">
S1: Sam’s response was to say sorry
S2: To smack the bee.
</footnote>
<bodyText confidence="0.984358789473684">
Some of the detected errors suggest that our use
of syntax and lexical overlap is not sufficient to iden-
tify the correct class. Our system marks the student
answer S1 from Example 5.13 as correct. The ref-
erence answer and the student answer share a great
number of words and the dependency trees are al-
most identical, but not the meanings. In addition, the
question contains additional information that may
require other types of features to correctly classify
the instance.
The predicate-argument overlap feature tries to
generalize the predicate information to find similar-
ities between verbs with the same meaning. How-
ever, our system does not always work in a correct
way. The verb smack in the student answer S2 and
the verb slap in the reference answer mean the same.
Our system classifies the answer incorrectly. If we
look at PropBank and VerbNet, we find that there
is not mapping between PropBank and VerbNet for
these particular verbs.
Example 5.2 Why do you think the other terminals
are being held in a different electrical state than that
of the negative terminal?
R: Terminals 4, 5 and 6 are not connected to the
negative battery terminal
S1: They are connected to the positive battery ter-
minal
We consider the negation as part of the syntac-
tic and predicate-argument overlap measures. How-
ever, our system does not characterize the similar-
ity between not connected to the negative and con-
nected to the positive (Example 5.2). This type of
examples suggest that the system needs to model the
negation and antonyms with additional features.
In the future, further error analysis will be car-
ried out to design features to better model the prob-
lem. We also anticipate creating a specialized fea-
ture space for each question type.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99778175">
This research was partially funded by the Ber2Tek
project (IE12-333), the SKaTeR project (TIN2012-
38584-C06-02) and the NewsReader project (FP7-
ICT-2011-8-316404).
</bodyText>
<footnote confidence="0.965022">
3R refers to the reference answer and S1 and S2 to student
answers.
</footnote>
<page confidence="0.998205">
583
</page>
<sectionHeader confidence="0.989055" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99673">
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th conference of the European chapter of
the Association for Computational Linguistics (EACL-
2009), Athens, Greece.
Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of The Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL-2009),
pages 43–48.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1–27:27, May.
Scott Deerwester, Susan Dumais, Goerge Furnas,
Thomas Landauer, and Richard Harshman. 1990. In-
dexing by Latent Semantic Analysis. Journal of the
American Society for Information Science, 41(6):391–
407.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, University of
Pennsylvania.
Philip M. McCarthy, Vasile Rus, Scott A. Crossley,
Arthur C. Graesser, and Danielle S. McNamara. 2008.
Assessing forward-, reverse-, and average-entailment
indices on natural language input from the intelligent
tutoring system, iSTART. In D. Wilson and G. Sut-
cliffe, editors, Proceedings of the 21st International
Florida Artificial Intelligence Research Society Con-
ference, pages 201–206, Menlo Park, CA: The AAAI
Press.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The nombank project: An interim
report. In A. Meyers, editor, HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24–31,
Boston, Massachusetts, USA, May 2 - May 7. Associ-
ation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings the
American Association forArtificial Intelligence (AAAI
2006), Boston.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38(11):39–41.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic role. Computational Linguistics, 31(1):71–
106.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2003. Using measures of semantic related-
ness for word sense disambiguation. In Proceedings
of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics.
</reference>
<page confidence="0.998585">
584
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.194224">
<title confidence="0.9939065">EHU-ALM: Similarity-Feature Based Approach for Student Response Analysis</title>
<author confidence="0.922224">Itziar Aldabe</author>
<author confidence="0.922224">Montse Maritxalar Oier Lopez de_Lacalle</author>
<affiliation confidence="0.845776333333333">IXA NLP Group University of Edinburgh University of Basque Country (UPV-EHU) IKERBASQUE, Foundation for</affiliation>
<email confidence="0.71683">montse.maritxalar@ehu.esoier.lopezdelacalle@gmail.com</email>
<abstract confidence="0.966632117647059">We present a 5-way supervised system based on syntactic-semantic similarity features. The model deploys: Text overlap measures, WordNet-based lexical similarities, graphbased similarities, corpus-based similarities, syntactic structure overlap and predicateargument overlap measures. These measures are applied to question, reference answer and student answer triplets. We take into account the negation in the syntactic and predicateargument overlap measures. Our system uses the domain-specific data as one dataset to build a robust system. The results show that our system is above the median and mean on all the evaluation scenarios of the SemEval- 2013 task #7.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing pagerank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th conference of the European chapter of the Association for Computational Linguistics (EACL2009),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="4733" citStr="Agirre and Soroa, 2009" startWordPosition="723" endWordPosition="726">or each open-class word in one of the input texts, we obtain the maximun semantic similarity or relatedness value matching the same openclass words in the other input text. The values of each matching are summed up and normalized by the length of the two input texts as explained in (Mihalcea et al., 2006). We compute the measures of Resnik, Lin, Jiang-Conrath, Leacock-Chodorow, Wu-Palmer, Banerjee-Pedersen, and PatwardhanPedersen provided in the WordNet::Similarity package (Patwardhan et al., 2003). Graph-based similarities The similarity of two texts is based on a graph-based representation (Agirre and Soroa, 2009) of WordNet. The method is a two-step process: first the personalized PageRank over WordNet is computed for each text. This produces a probability distribution over WordNet. Then, the probability distributions are encoded as vectors and the cosine similarity between those vectors is calculated. Corpus-based similarities We compute two corpus-based similarity measures: Latent Semantic Analysis (Deerwester et al., 1990) and Latent Dirichlet Allocation (Blei et al., 2003). We estimate 100 dimensions for LSA and 50 topics for LDA. Both models are obtained from a subset of the English Wikipedia fol</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings of the 12th conference of the European chapter of the Association for Computational Linguistics (EACL2009), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>Multilingual semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of The Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>43--48</pages>
<marker>Bj¨orkelund, Hafdell, Nugues, 2009</marker>
<rawString>Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues. 2009. Multilingual semantic role labeling. In Proceedings of The Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="5206" citStr="Blei et al., 2003" startWordPosition="791" endWordPosition="794">(Patwardhan et al., 2003). Graph-based similarities The similarity of two texts is based on a graph-based representation (Agirre and Soroa, 2009) of WordNet. The method is a two-step process: first the personalized PageRank over WordNet is computed for each text. This produces a probability distribution over WordNet. Then, the probability distributions are encoded as vectors and the cosine similarity between those vectors is calculated. Corpus-based similarities We compute two corpus-based similarity measures: Latent Semantic Analysis (Deerwester et al., 1990) and Latent Dirichlet Allocation (Blei et al., 2003). We estimate 100 dimensions for LSA and 50 topics for LDA. Both models are obtained from a subset of the English Wikipedia following the hierarchy of science categories. We started with a small set of categories and recovered the articles below the sub-hierarchy. We only went 3 levels down to avoid noisy articles as the category system is rather flat. The similarity of two texts is the cosine similarity between the 1http://www.d.umn.edu/ tpederse/text-similarity.html Syntactic structure overlap The role of syntax is studied by the use of graph subsumption based on the approach proposed in (Mc</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Trans. Intell. Syst. Technol.,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="10076" citStr="Chang and Lin, 2011" startWordPosition="1604" endWordPosition="1607">ce answers. Once we have a training set, we split it into different ways to simulate the scenarios described in Section 2.2. All the models are optimized using 10- fold cross-validation of the pertaining training set. For the classifiers in RUN1 and RUN2 we used 8910 training instances. For RUN3 the instances were divided as follows: 1235 instances for how questions, 3089 for what questions and 4589 for why questions. In total, we obtained 8 models which were distributed through the runs. 2We treat unseen-domain instances as unseen-question instances. Our approach uses Support Vector Machine (Chang and Lin, 2011) to build the classifiers. As the number of features is not high, we used the gaussian kernel in order to solve the non-linear problem. The main parameters of the kernel (-y and C) were tuned using grid search over the parameter in the cross-validation setting. We focused on optimizing the F1 macro average of the classifier in order to avoid a bias towards the major classes. Each of the 8 classifiers were tuned independently. The triplets of question, student answer and reference answer of the test instances were always created selecting the first reference answer of the given set of answers. </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: A library for support vector machines. ACM Trans. Intell. Syst. Technol., 2(3):27:1–27:27, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>Goerge Furnas</author>
<author>Thomas Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>407</pages>
<contexts>
<context position="5154" citStr="Deerwester et al., 1990" startWordPosition="783" endWordPosition="786">rdhanPedersen provided in the WordNet::Similarity package (Patwardhan et al., 2003). Graph-based similarities The similarity of two texts is based on a graph-based representation (Agirre and Soroa, 2009) of WordNet. The method is a two-step process: first the personalized PageRank over WordNet is computed for each text. This produces a probability distribution over WordNet. Then, the probability distributions are encoded as vectors and the cosine similarity between those vectors is calculated. Corpus-based similarities We compute two corpus-based similarity measures: Latent Semantic Analysis (Deerwester et al., 1990) and Latent Dirichlet Allocation (Blei et al., 2003). We estimate 100 dimensions for LSA and 50 topics for LDA. Both models are obtained from a subset of the English Wikipedia following the hierarchy of science categories. We started with a small set of categories and recovered the articles below the sub-hierarchy. We only went 3 levels down to avoid noisy articles as the category system is rather flat. The similarity of two texts is the cosine similarity between the 1http://www.d.umn.edu/ tpederse/text-similarity.html Syntactic structure overlap The role of syntax is studied by the use of gra</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan Dumais, Goerge Furnas, Thomas Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391– 407.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Rodney Nielsen</author>
<author>Chris Brew</author>
<author>Claudia Leacock</author>
<author>Danilo Giampiccolo</author>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="1232" citStr="Dzikovska et al., 2013" startWordPosition="165" endWordPosition="168">sures. These measures are applied to question, reference answer and student answer triplets. We take into account the negation in the syntactic and predicateargument overlap measures. Our system uses the domain-specific data as one dataset to build a robust system. The results show that our system is above the median and mean on all the evaluation scenarios of the SemEval2013 task #7. 1 Introduction In this paper we describe our participation with a feature-based supervised system to the SemEval2013 task #7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge (Dzikovska et al., 2013). The goal of our participation is to build a generic system that is robust enough across domains and scenarios. A domain-specific system requires new training examples when shifting to a new domain. However, domain-specific data is difficult to obtain and creating new resources is expensive. We seek robustness by mixing the instances from BEETLE and SCIENTSBANK. We show our strategy is suitable to build a generic system that performs competitively on any domain in the 5-way task. The paper proceeds as follows. Section 2 describes the system presenting the learning features and the runs. In Se</context>
<context position="10798" citStr="Dzikovska et al., 2013" startWordPosition="1730" endWordPosition="1733">der to solve the non-linear problem. The main parameters of the kernel (-y and C) were tuned using grid search over the parameter in the cross-validation setting. We focused on optimizing the F1 macro average of the classifier in order to avoid a bias towards the major classes. Each of the 8 classifiers were tuned independently. The triplets of question, student answer and reference answer of the test instances were always created selecting the first reference answer of the given set of answers. 4 Results A total of 8 teams participated in the 5-way task, submitting a total of 16 system runs (Dzikovska et al., 2013). Table 1 shows the performance obtained by our systems across domains and different scenarios. Our three runs ranked differently based on the evaluation scenario: beetle-uns-answ (6,4,5 rank for RUN1, RUN2, RUN3, respectively); beetle-uns-qst (7,7,6); scientsbank-uns-answ (7,6,5); scientsbankuns-qst (4,4,8) and scientsbank-uns-dom (3,3,5). We also evaluated our runs on the entire domain (All columns) and on the whole test set (OVERALL). The results show we built robust systems. Despite being below the best system of each evaluation scenario, the results show that the runs are competitive. All</context>
</contexts>
<marker>Dzikovska, Nielsen, Brew, Leacock, Giampiccolo, Bentivogli, Clark, Dagan, Dang, 2013</marker>
<rawString>Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew, Claudia Leacock, Danilo Giampiccolo, Luisa Bentivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang. 2013. Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge. In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics, Atlanta, Georgia, USA, 13-14 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
</authors>
<title>VerbNet: A broad-coverage, comprehensive verb lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="6699" citStr="Kipper, 2005" startWordPosition="1034" endWordPosition="1035">dicate-argument overlap The similarity of two texts is computed by analyzing the overlap of the predicates and their associated semantic arguments. The system looks for verbal and nominal predicates. The similarity is also based on the approach proposed in (McCarthy et al., 2008). The graph is represented with words as nodes and the semantic role of arguments as links. First, the verbal propositions and their arguments are automatically obtained (Bj¨orkelund et al., 2009) as represented in PropBank (Palmer et al., 2005). Second, a generalization of the predicates is obtained based on VerbNet (Kipper, 2005) and NomBank (Meyers et al., 2004). Finally, the similarity of two texts is computed based on the overlap of the predicateargument relations. 2.2 Architecture of the runs Generic Framework RUN1 This is the simplest framework for the assessment of student answers. The system relies on a single classifier, which has been optimized on the unseen question scenario. The scenario is simulated by splitting the training set so that each question and its answers are in the same fold. Unseen Framework RUN2 This framework relies on two classifiers. The first is tuned on an unseen answer scenario and the </context>
</contexts>
<marker>Kipper, 2005</marker>
<rawString>Karin Kipper. 2005. VerbNet: A broad-coverage, comprehensive verb lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philip M McCarthy</author>
<author>Vasile Rus</author>
<author>Scott A Crossley</author>
<author>Arthur C Graesser</author>
<author>Danielle S McNamara</author>
</authors>
<title>Assessing forward-, reverse-, and average-entailment indices on natural language input from the intelligent tutoring system, iSTART.</title>
<date>2008</date>
<booktitle>Proceedings of the 21st International Florida Artificial Intelligence Research Society Conference,</booktitle>
<pages>201--206</pages>
<editor>In D. Wilson and G. Sutcliffe, editors,</editor>
<publisher>The AAAI Press.</publisher>
<location>Menlo Park, CA:</location>
<contexts>
<context position="5826" citStr="McCarthy et al., 2008" startWordPosition="890" endWordPosition="893">3). We estimate 100 dimensions for LSA and 50 topics for LDA. Both models are obtained from a subset of the English Wikipedia following the hierarchy of science categories. We started with a small set of categories and recovered the articles below the sub-hierarchy. We only went 3 levels down to avoid noisy articles as the category system is rather flat. The similarity of two texts is the cosine similarity between the 1http://www.d.umn.edu/ tpederse/text-similarity.html Syntactic structure overlap The role of syntax is studied by the use of graph subsumption based on the approach proposed in (McCarthy et al., 2008). The text is mapped into a graph with nodes representing words and links indicating syntactic dependencies between them. The similarity of two texts is computed based on the overlap of the syntactic structures. Negation is handled explicitly in the graph. Predicate-argument overlap The similarity of two texts is computed by analyzing the overlap of the predicates and their associated semantic arguments. The system looks for verbal and nominal predicates. The similarity is also based on the approach proposed in (McCarthy et al., 2008). The graph is represented with words as nodes and the seman</context>
</contexts>
<marker>McCarthy, Rus, Crossley, Graesser, McNamara, 2008</marker>
<rawString>Philip M. McCarthy, Vasile Rus, Scott A. Crossley, Arthur C. Graesser, and Danielle S. McNamara. 2008. Assessing forward-, reverse-, and average-entailment indices on natural language input from the intelligent tutoring system, iSTART. In D. Wilson and G. Sutcliffe, editors, Proceedings of the 21st International Florida Artificial Intelligence Research Society Conference, pages 201–206, Menlo Park, CA: The AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grishman</author>
</authors>
<title>The nombank project: An interim report.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<volume>2</volume>
<pages>24--31</pages>
<editor>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph</editor>
<location>Boston, Massachusetts, USA,</location>
<marker>Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The nombank project: An interim report. In A. Meyers, editor, HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation, pages 24–31, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings the American Association forArtificial Intelligence (AAAI 2006),</booktitle>
<location>Boston.</location>
<contexts>
<context position="4107" citStr="Mihalcea et al., 2006" startWordPosition="625" endWordPosition="628">ociation for Computational Linguistics total, we defined 30 features which can be grouped as follows: resulting vectors associated with each text in the latent space. Text overlap measures The similarity of two texts is computed based on the number of overlapping words. We obtain the similarity of two texts based on the F-Measure, the Dice Coefficient, The Cosine, and the Lesk measures. For that, we use the implementation available in the Text::Similarity package1. WordNet-based lexical similarities All the similarity metrics based on WordNet (Miller, 1995) follow the methodology proposed in (Mihalcea et al., 2006). For each open-class word in one of the input texts, we obtain the maximun semantic similarity or relatedness value matching the same openclass words in the other input text. The values of each matching are summed up and normalized by the length of the two input texts as explained in (Mihalcea et al., 2006). We compute the measures of Resnik, Lin, Jiang-Conrath, Leacock-Chodorow, Wu-Palmer, Banerjee-Pedersen, and PatwardhanPedersen provided in the WordNet::Similarity package (Patwardhan et al., 2003). Graph-based similarities The similarity of two texts is based on a graph-based representatio</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings the American Association forArtificial Intelligence (AAAI 2006), Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="4048" citStr="Miller, 1995" startWordPosition="617" endWordPosition="618">84, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics total, we defined 30 features which can be grouped as follows: resulting vectors associated with each text in the latent space. Text overlap measures The similarity of two texts is computed based on the number of overlapping words. We obtain the similarity of two texts based on the F-Measure, the Dice Coefficient, The Cosine, and the Lesk measures. For that, we use the implementation available in the Text::Similarity package1. WordNet-based lexical similarities All the similarity metrics based on WordNet (Miller, 1995) follow the methodology proposed in (Mihalcea et al., 2006). For each open-class word in one of the input texts, we obtain the maximun semantic similarity or relatedness value matching the same openclass words in the other input text. The values of each matching are summed up and normalized by the length of the two input texts as explained in (Mihalcea et al., 2006). We compute the measures of Resnik, Lin, Jiang-Conrath, Leacock-Chodorow, Wu-Palmer, Banerjee-Pedersen, and PatwardhanPedersen provided in the WordNet::Similarity package (Patwardhan et al., 2003). Graph-based similarities The simi</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: A corpus annotated with semantic role.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>106</pages>
<contexts>
<context position="6611" citStr="Palmer et al., 2005" startWordPosition="1019" endWordPosition="1022">ed on the overlap of the syntactic structures. Negation is handled explicitly in the graph. Predicate-argument overlap The similarity of two texts is computed by analyzing the overlap of the predicates and their associated semantic arguments. The system looks for verbal and nominal predicates. The similarity is also based on the approach proposed in (McCarthy et al., 2008). The graph is represented with words as nodes and the semantic role of arguments as links. First, the verbal propositions and their arguments are automatically obtained (Bj¨orkelund et al., 2009) as represented in PropBank (Palmer et al., 2005). Second, a generalization of the predicates is obtained based on VerbNet (Kipper, 2005) and NomBank (Meyers et al., 2004). Finally, the similarity of two texts is computed based on the overlap of the predicateargument relations. 2.2 Architecture of the runs Generic Framework RUN1 This is the simplest framework for the assessment of student answers. The system relies on a single classifier, which has been optimized on the unseen question scenario. The scenario is simulated by splitting the training set so that each question and its answers are in the same fold. Unseen Framework RUN2 This frame</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: A corpus annotated with semantic role. Computational Linguistics, 31(1):71– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Using measures of semantic relatedness for word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="4613" citStr="Patwardhan et al., 2003" startWordPosition="706" endWordPosition="709"> All the similarity metrics based on WordNet (Miller, 1995) follow the methodology proposed in (Mihalcea et al., 2006). For each open-class word in one of the input texts, we obtain the maximun semantic similarity or relatedness value matching the same openclass words in the other input text. The values of each matching are summed up and normalized by the length of the two input texts as explained in (Mihalcea et al., 2006). We compute the measures of Resnik, Lin, Jiang-Conrath, Leacock-Chodorow, Wu-Palmer, Banerjee-Pedersen, and PatwardhanPedersen provided in the WordNet::Similarity package (Patwardhan et al., 2003). Graph-based similarities The similarity of two texts is based on a graph-based representation (Agirre and Soroa, 2009) of WordNet. The method is a two-step process: first the personalized PageRank over WordNet is computed for each text. This produces a probability distribution over WordNet. Then, the probability distributions are encoded as vectors and the cosine similarity between those vectors is calculated. Corpus-based similarities We compute two corpus-based similarity measures: Latent Semantic Analysis (Deerwester et al., 1990) and Latent Dirichlet Allocation (Blei et al., 2003). We es</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>