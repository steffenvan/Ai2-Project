<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001174">
<title confidence="0.9934115">
Nonparametric Learning of Phonological Constraints in Optimality
Theory
</title>
<author confidence="0.981701">
Gabriel Doyle Klinton Bicknell Roger Levy
</author>
<affiliation confidence="0.976254">
Department of Linguistics Department of Linguistics Department of Linguistics
</affiliation>
<address confidence="0.8075925">
UC San Diego Northwestern University UC San Diego
La Jolla, CA, USA 92093 Evanston, IL, USA 60208 La Jolla, CA, USA 92093
</address>
<email confidence="0.998837">
gdoyle@ucsd.edu kbicknell@northwestern.edu rlevy@ucsd.edu
</email>
<sectionHeader confidence="0.993895" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999995714285714">
We present a method to jointly learn fea-
tures and weights directly from distri-
butional data in a log-linear framework.
Specifically, we propose a non-parametric
Bayesian model for learning phonologi-
cal markedness constraints directly from
the distribution of input-output mappings
in an Optimality Theory (OT) setting. The
model uses an Indian Buffet Process prior
to learn the feature values used in the log-
linear method, and is the first algorithm
for learning phonological constraints with-
out presupposing constraint structure. The
model learns a system of constraints that
explains observed data as well as the
phonologically-grounded constraints of a
standard analysis, with a violation struc-
ture corresponding to the standard con-
straints. These results suggest an alterna-
tive data-driven source for constraints in-
stead of a fully innate constraint set.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995794">
Many aspects of human cognition involve the in-
teraction of constraints that push a decision-maker
toward different options, whether in something so
trivial as choosing a movie or so important as
a fight-or-flight response. These constraint-driven
decisions can be modeled with a log-linear system.
In these models, a set of constraints is weighted
and their violations are used to determine a prob-
ability distribution over outcomes. But where do
these constraints come from?
We consider this question by examining the
dominant framework in modern phonology, Opti-
mality Theory (Prince and Smolensky, 1993, OT),
implemented in a log-linear framework, MaxEnt
OT (Goldwater and Johnson, 2003), with output
forms’ probabilities based on a weighted sum of
constraint violations. OT analyses generally as-
sume that the constraints are innate and univer-
sal, both to obviate the problem of learning con-
straints’ identities and to limit the set of possible
languages.
We propose a new approach: to learn con-
straints with limited innate phonological knowl-
edge by identifying sets of constraint violations
that explain the observed distributional data, in-
stead of selecting constraints from an innate set
of constraint definitions. Because the constraints
are identified as sets of violations, this also per-
mits constraints specific to a given language to
be learned. This method, which we call IBPOT,
uses an Indian Buffet Process (IBP) prior to define
the space of possible constraint violation matri-
ces, and uses Bayesian reasoning to identify con-
straint matrices likely to have generated the ob-
served data. In identifying constraints solely by
their extensional violation profiles, this method
does not directly identify the intensional defini-
tions of the identified constraints, but to the extent
that the resulting violation profiles are phonologi-
cally interpretable, we may conclude that the data
themselves guide constraint identification. We test
IBPOT on tongue-root vowel harmony in Wolof, a
West African language.
The set of constraints learned by the model sat-
isfy two major goals: they explain the data as well
as the standard phonological analysis, and their vi-
olation structures correspond to the standard con-
straints. This suggests an alternative data-driven
genesis for constraints, rather than the traditional
assumption of fully innate constraints.
</bodyText>
<sectionHeader confidence="0.97449" genericHeader="introduction">
2 Phonology and Optimality Theory
</sectionHeader>
<subsectionHeader confidence="0.962314">
2.1 OT structure
</subsectionHeader>
<bodyText confidence="0.99899975">
Optimality Theory has been used for constraint-
based analysis of many areas of language, but we
focus on its most successful application: phonol-
ogy. We consider an OT analysis of the mappings
</bodyText>
<page confidence="0.962985">
1094
</page>
<note confidence="0.8303845">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1094–1103,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99991628125">
between underlying forms and their phonological
manifestations – i.e., mappings between forms in
the mental lexicon and the actual vocalized forms
of the words.1
Stated generally, an OT system takes some in-
put, generates a set of candidate outputs, deter-
mines what constraints each output violates, and
then selects a candidate output with a relatively
unobjectionable violation profile. To do this, an
OT system contains four major components: a
generator GEN, which generates candidate out-
put forms for the input; a set of constraints CON,
which penalize candidates; a evaluation method
EVAL, which selects an winning candidate; and
H, a language-particular weighting of constraints
that EVAL uses to determine the winning candi-
date. Previous OT work has focused on identifying
the appropriate formulation of EVAL and the val-
ues and acquisition of H, while taking GEN and
CON as given. Here, we expand the learning task
by proposing an acquisition method for CON.
To learn CON, we propose a data-driven
markedness constraint learning system that avoids
both innateness and tractability issues. Unlike pre-
vious OT learning methods, which assume known
constraint definitions and only learn the relative
strength of these constraints, the IBPOT learns
constraint violation profiles and weights for them
simultaneously. The constraints are derived from
sets of violations that effectively explain the ob-
served data, rather than being selected from a pre-
existing set of possible constraints.
</bodyText>
<subsectionHeader confidence="0.994452">
2.2 OT as a weighted-constraint method
</subsectionHeader>
<bodyText confidence="0.999349">
Although all OT systems share the same core
structure, different choices of EVAL lead to dif-
ferent behaviors. In IBPOT, we use the log-
linear EVAL developed by Goldwater and John-
son (2003) in their MaxEnt OT system. MEOT
extends traditional OT to account for variation
(cases in which multiple candidates can be the
winner), as well as gradient/probabilistic produc-
tions (Anttila, 1997) and other constraint interac-
tions (e.g., cumulativity) that traditional OT can-
not handle (Keller, 2000). MEOT also is motivated
by the general MaxEnt framework, whereas most
other OT formulations are ad hoc constructions
specific to phonology.
In MEOT, each constraint Ci is associated with
</bodyText>
<footnote confidence="0.984840666666667">
1Although phonology is usually framed in terms of sound,
sign languages also have components that serve equivalent
roles in the physical realization of signs (Stokoe, 1960).
</footnote>
<bodyText confidence="0.996773">
a weight wi &lt; 0. (Weights are always negative
in OT; a constraint violation can never make a
candidate more likely to win.) For a given input-
candidate pair (x, y), fi(y, x) is the number of vi-
olations of constraint Ci by the pair. As a maxi-
mum entropy model, the probability of y given x
is proportional to the exponential of the weighted
sum of violations, Ei wifi(y, x). If y(x) is the
set of all output candidates for the input x, then
the probability of y as the winning output is:
</bodyText>
<equation confidence="0.999571">
p(y |x) = exp (Ei wifi(y, x))(1)
Ez∈Y(x) exp(Ei wifi(z, x))
</equation>
<bodyText confidence="0.999449166666666">
This formulation represents a probabilistic
extension of the traditional formulation of
OT (Prince and Smolensky, 1993). Traditionally,
constraints form a strict hierarchy, where a single
violation of a high-ranked constraint is worse than
any number of violations of lower-ranked con-
straints. Traditional OT is also deterministic, with
the optimal candidate always selected. In MEOT,
the constraint weights define hierarchies of vary-
ing strictness, and some probability is assigned to
all candidates. If constraints’ weights are close to-
gether, multiple violations of lower-weighted con-
straints can reduce a candidate’s probability below
that of a competitor with a single high-weight vio-
lation. As the distance between weights in MEOT
increases, the probability of a suboptimal candi-
date being chosen approaches zero; thus the tradi-
tional formulation is a limit case of MEOT.
</bodyText>
<subsectionHeader confidence="0.991613">
2.3 OT in practice
</subsectionHeader>
<bodyText confidence="0.999874466666667">
Figure 1 shows tableaux, a visualization for
OT, applied in Wolof (Archangeli and Pulley-
blank, 1994; Boersma, 1999). We are interested
in four Wolof constraints that combine to induce
vowel harmony: *r, PARSE[rtr], HARMONY, and
PARSE[atr]. The meaning of these constraints will
be discussed in Sect. 4.1; for now, we will only
consider their violation profiles. Each column rep-
resents a constraint, with weights decreasing left-
to-right. Each tableau looks at a single input form,
noted in the top-left cell: ete, EtE, rte, or itE.
Each row is a candidate output form. A black
cell indicates that the candidate, or input-candidate
pair, violates the constraint in that column.2 A
white cell indicates no violation. Grey stripes are
</bodyText>
<footnote confidence="0.62452475">
2In general, a constraint can be violated multiple times
by a given candidate, but we will be using binary constraints
(violated or not) in this work. See Sect. 5.2 for further discus-
sion.
</footnote>
<page confidence="0.970599">
1095
</page>
<table confidence="0.9202438">
ete &apos;I Parse(rtr) Harmony Parse(atr) Score
ete 0
ɛte -24
etɛ -24
ɛtɛ -8
ɛtɛ &apos;I Parse(rtr) Harmony Parse(atr) Score
ete -32
ɛte -48
etɛ -48
ɛtɛ 0
ɪte &apos;I Parse(rtr) Harmony Parse(atr) Score
ite -32
ɪte -80
itɛ -56
ɪtɛ -72
</table>
<figure confidence="0.988141692307692">
&apos;I
Parse(rtr) Harmony
Parse(atr)
Score
-120
-32
-16
-72
itɛ
ite
ɪte
itɛ
ɪtɛ
</figure>
<figureCaption confidence="0.66698375">
Figure 1: Tableaux for the Wolof input forms ete, ctc, rte, and itc. Black indicates violation, white no
violation. Scores are calculated for a MaxEnt OT system with constraint weights of -64, -32, -16, and -8,
approximating a traditional hierarchical OT design. Values of grey-striped cells have negligible effects
on the distribution (see Sect. 4.3).
</figureCaption>
<bodyText confidence="0.997870892857143">
overlaid on cells whose value will have a negligi-
ble impact on the distribution due to the values of
higher-ranked constraint.
Constraints fall into two categories, faithful-
ness and markedness, which differ in what infor-
mation they use to assign violations. Faithfulness
constraints penalize mismatches between the in-
put and output, while markedness constraints con-
sider only the output. Faithfulness violations in-
clude phoneme additions or deletions between the
input and output; markedness violations include
penalizing specific phonemes in the output form,
regardless of whether the phoneme is present in
the input.
In MaxEnt OT, each constraint has a weight,
and the candidates’ scores are the sums of the
weights of violated constraints. In the ete tableau
at top left, output ete has no violations, and there-
fore a score of zero. Outputs cte and etc vio-
late both HARMONY (weight 16) and PARSE[atr]
(weight 8), so their scores are 24. Output ctc vi-
olates PARSE[atr], and has score 8. Thus the log-
probability of output ctc is 1/8 that of ete, and the
log-probability of disharmonious cte and etc are
each 1/24 that of ete. As the ratio between scores
increases, the log-probability ratios can become
arbitrarily close to zero, approximating the deter-
ministic situation of traditional OT.
</bodyText>
<subsectionHeader confidence="0.998779">
2.4 Learning Constraints
</subsectionHeader>
<bodyText confidence="0.99997380952381">
Choosing a winning candidate presumes that a
set of constraints CON is available, but where do
these constraints come from? The standard as-
sumption within OT is that CON is innate and
universal. But in the absence of direct evidence
of innate constraints, we should prefer a method
that can derive the constraints from cognitively-
general learning over one that assumes they are
pre-specified. Learning appropriate model features
has been an important idea in the development of
constraint-based models (Della Pietra et al., 1997).
The innateness assumption can induce tractabil-
ity issues as well. The strictest formulation of in-
nateness posits that virtually all constraints are
shared across all languages, even when there is
no evidence for the constraint in a particular lan-
guage (Tesar and Smolensky, 2000). Strict uni-
versality is undermined by the extremely large
set of constraints it must weight, as well as
the possible existence of language-particular con-
straints (Smith, 2004).
A looser version of universality supposes that
constraints are built compositionally from a set
of constraint templates or primitives or phono-
logical features (Hayes, 1999; Smith, 2004; Id-
sardi, 2006; Riggle, 2009). This version allows
language-particular constraints, but it comes with
a computational cost, as the learner must be able
to generate and evaluate possible constraints while
learning the language’s phonology. Even with rel-
atively simple constraint templates, such as the
phonological constraint learner of Hayes and Wil-
son (2008), the number of possible constraints ex-
pands exponentially. Depending on the specific
formulation of the constraints, the constraint iden-
tification problem may even be NP-hard (Idsardi,
2006; Heinz et al., 2009). Our approach of casting
the learning problem as one of identifying viola-
tion profiles is an attempt to determine the amount
that can be learned about the active constraints in a
paradigm without hypothesizing intensional con-
straint definitions. The violation profile informa-
</bodyText>
<page confidence="0.966557">
1096
</page>
<bodyText confidence="0.999986833333333">
tion used by our model could then be used to nar-
row the search space for intensional constraints,
either by performing post-hoc analysis of the con-
straints identified by our model or by combining
intensional constraint search into the learning pro-
cess. We discuss each of these possibilities in Sec-
tion 5.2.
Innateness is less of a concern for faithfulness
than markedness constraints. Faithfulness viola-
tions are determined by the changes between an
input form and a candidate, yielding an indepen-
dent motivation for a universal set of faithfulness
constraints (McCarthy, 2008). Some markedness
constraints can also be motivated in a universal
manner (Hayes, 1999), but many markedness con-
straints lack such grounding.3 As such, it is un-
clear where a universal set of markedness con-
straints would come from.
</bodyText>
<sectionHeader confidence="0.999369" genericHeader="method">
3 The IBPOT Model
</sectionHeader>
<subsectionHeader confidence="0.992012">
3.1 Structure
</subsectionHeader>
<bodyText confidence="0.9998626875">
The IBPOT model defines a generative process for
mappings between input and output forms based
on three latent variables: the constraint violation
matrices F (faithfulness) and M (markedness),
and the weight vector w. The cells of the violation
matrices correspond to the number of violations of
a constraint by a given input-output mapping. Fijk
is the number of violations of faithfulness con-
straint Fk by input-output pair type (xi, yj); Mjl is
the number of violations of markedness constraint
M·l by output candidate yj. Note that M is shared
across inputs, as Mjl has the same value for all
input-output pairs with output yj. The weight vec-
tor w provides weight for both F and M. Proba-
bilities of output forms are given by a log-linear
function:
</bodyText>
<equation confidence="0.9487545">
p(yj|xi) = (2)
exp (Ek wkFijk + El wlMjl)
E exp (Ek wkFizk + El wlMzl)
yzEY(xi)
</equation>
<bodyText confidence="0.992901344827586">
Note that this is the same structure as Eq. 1
but with faithfulness and markedness constraints
listed separately. As discussed in Sect. 2.4, we as-
sume that F is known as part of the output of GEN
(Riggle, 2009). The goal of the IBPOT model is to
3McCarthy (2008, §4.8) gives examples of “ad hoc” in-
tersegmental constraints. Even well-known constraint types,
such as generalized alignment, can have disputed structures
(Hyde, 2012).
learn the markedness matrix M and weights w for
both the markedness and faithfulness constraints.
As for M, we need a non-parametric prior, as
there is no inherent limit to the number of marked-
ness constraints a language will use. We use the
Indian Buffet Process (Griffiths and Ghahramani,
2005), which defines a proper probability distri-
bution over binary feature matrices with an un-
bounded number of columns. The IBP can be
thought of as representing the set of dishes that
diners eat at an infinite buffet table. Each diner
(i.e., output form) first draws dishes (i.e., con-
straint violations) with probability proportional
to the number of previous diners who drew it:
p(Mjl = 1|{Mzl}z&lt;j) = nl/j. After choosing
from the previously taken dishes, the diner can
try additional dishes that no previous diner has
had. The number of new dishes that the j-th cus-
tomer draws follows a Poisson(α/j) distribution.
The complete specification of the model is then:
</bodyText>
<equation confidence="0.999247">
M ∼ IBP(α); Y(xi) = Gen(xi)
w ∼ −F(1,1); y|xi ∼ LogLin(M, F, w, Y(xi))
</equation>
<subsectionHeader confidence="0.823412">
3.2 Inference
</subsectionHeader>
<bodyText confidence="0.999911076923077">
To perform inference in this model, we adopt a
common Markov chain Monte Carlo estimation
procedure for IBPs (G¨or¨ur et al., 2006; Navarro
and Griffiths, 2007). We alternate approximate
Gibbs sampling over the constraint matrix M,
using the IBP prior, with a Metropolis-Hastings
method to sample constraint weights w.
We initialize the model with a randomly-drawn
markedness violation matrix M and weight vector
w. To learn, we iterate through the output forms
yj; for each, we split M_j· into “represented” con-
straints (those that are violated by at least one
output form other than yj) and “non-represented”
constraints (those violated only by yj). For each
represented constraint M·l, we re-sample the value
for the cell Mjl. All non-represented constraints
are removed, and we propose new constraints, vi-
olated only by yj, to replace them. After each it-
eration through M, we use Metropolis-Hastings to
update the weight vector w.
Represented constraint sampling We begin by
resampling Mjl for all represented constraints
M·l, conditioned on the rest of the violations
(M_(jl), F) and the weights w. This is the sam-
pling counterpart of drawing existing features in
the IBP generative process. By Bayes’ Rule, the
</bodyText>
<page confidence="0.97948">
1097
</page>
<bodyText confidence="0.991039666666667">
posterior probability of a violation is propor-
tional to product of the likelihood p(Y |Mjl =
1, M_jl, F, w) from Eq. 2 and the IBP prior prob-
ability p(Mjl = 1|M_jl) = n_jl/n, where n_jl
is the number of outputs other than yj that violate
constraint M·l.
Non-represented constraint sampling After
sampling the represented constraints for yj, we
consider the addition of new constraints that are
violated only by yj. This is the sampling coun-
terpart to the Poisson draw for new features in
the IBP generative process. Ideally, this would
draw new constraints from the infinite feature ma-
trix; however, this requires marginalizing the like-
lihood over possible weights, and we lack an ap-
propriate conjugate prior for doing so. We approx-
imate the infinite matrix with a truncated Bernoulli
draw over unrepresented constraints (G¨or¨ur et al.,
2006). We consider in each sample at most K*
new constraints, with weights based on the auxil-
iary vector w*. This approximation retains the un-
bounded feature set of the IBP, as repeated sam-
pling can add more and more constraints without
limit.
The auxiliary vector w* contains the weights
of all the constraints that have been removed in
the previous step. If the number of constraints
removed is less than K*, w* is filled out with
draws from the prior distribution over weights. We
then consider adding any subset of these new con-
straints to M, each of which would be violated
only by yj. Let M* represent a (possibly empty)
set of constraints paired with a subset of w*. The
posterior probability of drawing M* from the trun-
cated Bernoulli distribution is the product of the
prior probability of M* \ NY + α) and the like-
</bodyText>
<equation confidence="0.935585">
K
lihood p(Y |M*, w*, M, w, F), including the new
constraints M*.
</equation>
<bodyText confidence="0.9868846">
Weight sampling After sampling through
all candidates, we use Metropolis-Hastings
to estimate new weights for both con-
straint matrices. Our proposal distribution is
Gamma(wk&apos;/η,η/wk), with mean wk and
mode wk −η(for wk &gt; 1). Unlike Gibbs
wk
sampling on the constraints, which occurs only on
markedness constraints, weights are sampled for
both markedness and faithfulness features.
</bodyText>
<sectionHeader confidence="0.999578" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.998958">
4.1 Wolof vowel harmony
</subsectionHeader>
<bodyText confidence="0.938117454545455">
We test the model by learning the markedness con-
straints driving Wolof vowel harmony (Archangeli
and Pulleyblank, 1994). Vowel harmony in gen-
eral refers to a phonological phenomenon wherein
the vowels of a word share certain features in the
output form even if they do not share them in the
input. In the case of Wolof, harmony encourages
forms that have consistent tongue root positions.
The Wolof vowel system has two relevant fea-
tures, tongue root position and vowel height. The
tongue root can either be advanced (ATR) or re-
tracted (RTR), and the body of the tongue can be in
the high, middle, or low part of the mouth. These
features define six vowels:
high mid low
ATR i e a
RTR i e a
We test IBPOT on the harmony system provided
in the Praat program (Boersma, 1999), previ-
ously used as a test case by Goldwater and John-
son (2003) for MEOT learning with known con-
straints. This system has four constraints:4
</bodyText>
<listItem confidence="0.998851111111111">
• Markedness:
– *i: do not have i (high RTR vowel)
– HARMONY: do not have RTR and ATR
vowels in the same word
• Faithfulness:
– PARSE[rtr]: do not change RTR input to
ATR output
– PARSE[atr]: do not change ATR input to
RTR output
</listItem>
<bodyText confidence="0.999836909090909">
These constraints define the phonological stan-
dard that we will compare IBPOT to, with a rank-
ing from strongest to weakest of *i &gt;&gt; PARSE[rtr]
&gt;&gt; HARMONY &gt;&gt; PARSE[atr]. Under this rank-
ing, Wolof harmony is achieved by changing a
disharmonious ATR to an RTR, unless this cre-
ates an i vowel. We see this in Figure 1, where
three of the four winners are harmonic, but with
input ite, harmony would require violating one
of the two higher-ranked constraints. As in previ-
ous MEOT work, all Wolof candidates are faithful
</bodyText>
<footnote confidence="0.999026333333333">
4The version in Praat includes a fifth constraint, but its
value never affects the choice of output in our data and is
omitted in this analysis.
</footnote>
<page confidence="0.997416">
1098
</page>
<bodyText confidence="0.999980552631579">
with respect to vowel height, either because height
changes are not considered by GEN, or because
of a high-ranked faithfulness constraint blocking
height changes.5
The Wolof constraints provide an interesting
testing ground for the model, because it is a small
set of constraints to be learned, but contains the
HARMONY constraint, which can be violated by
non-adjacent segments. Non-adjacent constraints
are difficult for string-based approaches because
of the exponential number of possible relation-
ships across non-adjacent segments. However, the
Wolof results show that by learning violations di-
rectly, IBPOT does not encounter problems with
non-adjacent constraints.
The Wolof data has 36 input forms, each of the
form V1tV2, where V1 and V2 are vowels that agree
in height. Each input form has four candidate out-
puts, with one output always winning. The outputs
appear for multiple inputs, as shown in Figure 1.
The candidate outputs are the four combinations
of tongue-roots for the given vowel heights; the
inputs and candidates are known to the learner.
We generate simulated data by observing 1000 in-
stances of the winning output for each input.6 The
model must learn the markedness constraints *I
and HARMONY, as well as the weights for all four
constraints.
We make a small modification to the constraints
for the test data: all constraints are limited to bi-
nary values. For constraints that can be violated
multiple times by an output (e.g., *I twice by ItI),
we use only a single violation. This is necessary in
the current model definition because the IBP pro-
duces a prior over binary matrices. We generate
the simulated data using only single violations of
each constraint by each output form. Overcoming
the binarity restriction is discussed in Sect. 5.2.
</bodyText>
<subsectionHeader confidence="0.975311">
4.2 Experiment Design
</subsectionHeader>
<bodyText confidence="0.852900375">
We run the model for 10000 iterations, using de-
terministic annealing through the first 2500 it-
5In the present experiment, we assume that GEN does not
generate candidates with unfaithful vowel heights. If unfaith-
ful vowel heights were allowed by GEN, these unfaithful can-
didates would incur a violation approximately as strong as *I,
as neither unfaithful-height candidates nor I candidates are at-
tested in the Wolof data.
</bodyText>
<footnote confidence="0.8474995">
6Since data, matrix, and weight likelihoods all shape the
learned constraints, there must be enough data for the model
to avoid settling for a simple matrix that poorly explains the
data. This represents a similar training set size to previous
work (Goldwater and Johnson, 2003; Boersma and Hayes,
2001).
</footnote>
<bodyText confidence="0.999962363636364">
erations. The model is initialized with a ran-
dom markedness matrix drawn from the IBP and
weights from the exponential prior. We ran ver-
sions of the model with parameter settings be-
tween 0.01 and 1 for α, 0.05 and 0.5 for q, and
2 and 5 for K*. All these produced quantitatively
similar results; we report values for α = 1, q =
0.5, and K* = 5, which provides the least bias
toward small constraint sets.
To establish performance for the phonological
standard, we use the IBPOT learner to find con-
straint weights but do not update M. The resultant
learner is essentially MaxEnt OT with the weights
estimated through Metropolis sampling instead of
gradient ascent. This is done so that the IBPOT
weights and phonological standard weights are
learned by the same process and can be compared.
We use the same parameters for this baseline as
for the IBPOT tests. The results in this section are
based on nine runs each of IBPOT and MEOT; ten
MEOT runs were performed but one failed to con-
verge and was removed from analysis.
</bodyText>
<subsectionHeader confidence="0.815227">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999974555555556">
A successful set of learned constraints will satisfy
two criteria: achieving good data likelihood (no
worse than the phonological-standard constraints)
and acquiring constraint violation profiles that are
phonologically interpretable. We find that both of
these criteria are met by IBPOT on Wolof.
Likelihood comparison First, we calculate the
joint probability of the data and model given the
priors, p(Y, M, wIF, α), which is proportional to
the product of three terms: the data likelihood
p(YIM, F, w), the markedness matrix probabil-
ity p(M|α), and the weight probability p(w). We
present both the mean and MAP values for these
over the final 1000 iterations of each run. Results
are shown in Table 1.
All eight differences are significant according
to t-tests over the nine runs. In all cases but mean
M, the IBPOT method has a better log-probability.
The most important differences are those in the
data probabilities, as the matrix and weight prob-
abilities are reflective primarily of the choice of
prior. By both measures, the IBPOT constraints
explain the observed data better than the phono-
logically standard constraints.
Interestingly, the mean M probability is lower
for IBPOT than for the phonological standard.
Though the phonologically standard constraints
</bodyText>
<page confidence="0.989395">
1099
</page>
<table confidence="0.999515333333333">
MAP Mean
IBPOT PS IBPOT PS
Data -1.52 -3.94 -5.48 -9.23
M -51.7 -53.3 -54.7 -53.3
w -44.2 -71.1 -50.6 -78.1
Joint -97.4 -128.4 -110.6 -140.6
</table>
<tableCaption confidence="0.998399">
Table 1: Data, markedness matrix, weight vec-
</tableCaption>
<bodyText confidence="0.993998906976744">
tor, and joint log-probabilities for the IBPOT and
the phonological standard constraints. MAP and
mean estimates over the final 1000 iterations for
each run. All IBPOT/PS differences are significant
(p &lt; .005 for MAP M; p &lt; .001 for others).
exist independently of the IBP prior, they fit the
prior better than the average IBPOT constraints do.
This shows that the IBP’s prior preferences can be
overcome in order to have constraints that better
explain the data.
Constraint comparison Our second criterion
is the acquisition of meaningful constraints,
that is, ones whose violation profiles have
phonologically-grounded explanations. IBPOT
learns the same number of markedness constraints
as the phonological standard (two); over the final
1000 iterations of the model runs, 99.2% of the it-
erations had two markedness constraints, and the
rest had three.
Turning to the form of these constraints, Figure
2 shows violation profiles from the last iteration
of a representative IBPOT run.7 Because vowel
heights must be faithful between input and out-
put, the Wolof data is divided into nine separate
paradigms, each containing the four candidates
(ATR/RTR × ATR/RTR) for the vowel heights in
the input.
The violations on a given output form only
affect probabilities within its paradigm. As a
result, learned constraints are consistent within
paradigms, but across paradigms, the same con-
straint may serve different purposes.
For instance, the strongest learned markedness
constraint, shown as M1 in Figure 2, has the same
violations as the top-ranked constraint that ac-
tively distinguishes between candidates in each
paradigm. For the five paradigms with at least
one high vowel (the top row and left column),
M1 has the same violations as *I, as *I penal-
izes some but not all of the candidates. In the
7Specifically, from the run with the median joint posterior.
other four paradigms, *I penalizes none of the
candidates, and the IBPOT learner has no rea-
son to learn it. Instead, it learns that M1 has
the same violations as HARMONY, which is the
highest-weighted constraint that distinguishes be-
tween candidates in these paradigms. Thus in the
high-vowel paradigms, M1 serves as *I, while in
the low/mid-vowel paradigms, it serves as HAR-
MONY.
The lower-weighted M2 is defined noisily, as
the higher-ranked M1 makes some values of M2
inconsequential. Consider the top-left paradigm of
Figure 2, the high-high input, in which only one
candidate does not violate M1 (*I). Because M1
has a much higher weight than M2, a violation of
M2 has a negligible effect on a candidate’s prob-
ability.8 In such cells, the constraint’s value is in-
fluenced more by the prior than by the data. These
inconsequential cells are overlaid with grey stripes
in Figure 2.
The meaning of M2, then, depends only on the
consequential cells. In the high-vowel paradigms,
M2 matches HARMONY, and the learned and stan-
dard constraints agree on all consequential viola-
tions, despite being essentially at chance on the in-
distinguishable violations (58%). On the non-high
paradigms, the meaning of M2 is unclear, as HAR-
MONY is handled by M1 and *I is unviolated. In
all four paradigms, the model learns that the RTR-
RTR candidate violates M2 and the ATR-ATR can-
didate does not; this appears to be the model’s at-
tempt to reinforce a pattern in the lowest-ranked
faithfulness constraint (PARSE[atr]), which the
ATR-ATR candidate never violates.
Thus, while the IBPOT constraints are not
identical to the phonologically standard ones,
they reflect a version of the standard constraints
that is consistent with the IBPOT framework.9
In paradigms where each markedness constraint
distinguishes candidates, the learned constraints
match the standard constraints. In paradigms
where only one constraint distinguishes candi-
dates, the top learned constraint matches it and the
second learned constraint exhibits a pattern con-
sistent with a low-ranked faithfulness constraint.
</bodyText>
<footnote confidence="0.997138428571429">
8Given the learned weights in Fig. 2, if the losinf candi-
date violates M1, its probability changes from 10− 2 when
the preferred candidate does not violate M2 to 10−8 when it
does.
9In fact, it appears this constraint organization is favored
by IBPOT as it allows for lower weights, hence the large dif-
ference in w log-probability in Table 1.
</footnote>
<page confidence="0.98104">
1100
</page>
<figure confidence="0.99939">
ete
ɛte
etɛ
etə
ɛtə
eta
ɛta
ɛtɛ
eti
ɛti
etɪ
ɛtɪ
sɪ
Phono. Std.
Harmony
M1
Learned
M2
lo
mid
lo
hi
lo
lo
əte
ətɛ
ətə
əta
ate
atɛ
atə
ata
əti
ətɪ
ati
atɪ
sɪ
Phono. Std.
Harmony
M1
Learned
M2
mid
hi
mid
mid
mid
lo
Phono. Std. Learned
M1
M2
Harmony
hi
mid
hi
lo
ite
ɪte
itɛ
ɪtɛ
itə
ɪtə
ita
ɪta
ɪtɪ
sɪ
iti
hi
hi
ɪti
itɪ
</figure>
<figureCaption confidence="0.997476">
Figure 2: Phonologically standard (*i, HARMONY) and learned (M1,M2) constraint violation profiles for
</figureCaption>
<bodyText confidence="0.755826">
the output forms. Learned weights for the standard constraints are -32.8 and -15.3; for M1 and M2, they
are -26.5 and -8.4. Black indicates violation, white no violation. Grey stripes indicate cells whose values
have negligible effects on the probability distribution.
</bodyText>
<sectionHeader confidence="0.998331" genericHeader="evaluation">
5 Discussion and Future Work
</sectionHeader>
<subsectionHeader confidence="0.935685">
5.1 Relation to phonotactic learning
</subsectionHeader>
<bodyText confidence="0.999971473684211">
Our primary finding from IBPOT is that it is possi-
ble to identify constraints that are both effective at
explaining the data and representative of theorized
phonologically-grounded constraints, given only
input-output mappings and faithfulness violations.
Furthermore, these constraints are successfully ac-
quired without any knowledge of the phonological
structure of the data beyond the faithfulness vio-
lation profiles. The model’s ability to infer con-
straint violation profiles without theoretical con-
straint structure provides an alternative solution to
the problems of the traditionally innate and univer-
sal OT constraint set.
As it jointly learns constraints and weights,
the IBPOT model calls to mind Hayes and
Wilson’s (2008) joint phonotactic learner. Their
learner also jointly learns weights and constraints,
but directly selects its constraints from a composi-
tional grammar of constraint definitions. This lim-
its their learner in practice by the rapid explosion
in the number of constraints as the maximum con-
straint definition size grows. By directly learning
violation profiles, the IBPOT model avoids this ex-
plosion, and the violation profiles can be automat-
ically parsed to identify the constraint definitions
that are consistent with the learned profile. The
inference method of the two models is different
as well; the phonotactic learner selects constraints
greedily, whereas the sampling on M in IBPOT
asymptotically approaches the posterior.
The two learners also address related but dif-
ferent phonological problems. The phonotactic
learner considers phonotactic problems, in which
only output matters. The constraints learned by
Hayes and Wilson’s learner are essentially OT
markedness constraints, but their learner does not
have to account for varied inputs or effects of faith-
fulness constraints.
</bodyText>
<subsectionHeader confidence="0.998614">
5.2 Extending the learning model
</subsectionHeader>
<bodyText confidence="0.999986666666667">
IBPOT, as proposed here, learns constraints based
on binary violation profiles, defined extensionally.
A complete model of constraint acquisition should
provide intensional definitions that are phonolog-
ically grounded and cover potentially non-binary
constraints. We discuss how to extend the model
toward these goals.
IBPOT currently learns extensional constraints,
defined by which candidates do or do not violate
the constraint. Intensional definitions are needed
to extend constraints to unseen forms. Post hoc vi-
olation profile analysis, as in Sect. 4.3, provides
a first step toward this goal. Such analysis can
be integrated into the learning process using the
Rational Rules model (Goodman et al., 2008) to
identify likely constraint definitions composition-
ally. Alternately, phonological knowledge could
be integrated into a joint constraint learning pro-
cess in the form of a naturalness bias on the con-
straint weights or a phonologically-motivated re-
placement for the IBP prior.
The results presented here use binary con-
straints, where each candidate violates each con-
straint only once, a result of the IBP’s restriction
to binary matrices. Non-binarity can be handled
by using the binary matrix M to indicate whether
a candidate violates a constraint, with a second
</bodyText>
<page confidence="0.9726">
1101
</page>
<bodyText confidence="0.999986523809524">
distribution determining the number of violations.
Alternately, a binary matrix can directly capture
non-binary constraints; Frank and Satta (1998)
converted existing non-binary constraints into a
binary OT system by representing non-binary con-
straints as a set of equally-weighted overlapping
constraints, each accounting for one violation. The
non-binary harmony constraint, for instance, be-
comes a set {*(at least one disharmony), *(at least
two disharmonies), etc.}.
Lastly, the Wolof vowel harmony problem pro-
vides a test case with overlaps in the candidate sets
for different inputs. This candidate overlap helps
the model find appropriate constraint structures.
Analyzing other phenomena may require the iden-
tification of appropriate abstractions to find this
same structural overlap. English regular plurals,
for instance, fall into broad categories depending
on the features of the stem-final phoneme. IBPOT
learning in such settings may require learning an
appropriate abstraction as well.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999996636363636">
A central assumption of Optimality Theory has
been the existence of a fixed inventory of uni-
versal markedness constraints innately available to
the learner, an assumption by arguments regarding
the computational complexity of constraint iden-
tification. However, our results show for the first
time that nonparametric, data-driven learning can
identify sparse constraint inventories that both ac-
curately predict the data and are phonologically
meaningful, providing a serious alternative to the
strong nativist view of the OT constraint inventory.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998858857142857">
We wish to thank Eric Bakovi´c, Emily Mor-
gan, Mark Mysl´ın, the UCSD Computational Psy-
cholinguistics Lab, the Phon Company, and the re-
viewers for their discussions and feedback on this
work. This research was supported by NSF award
IIS-0830535 and an Alfred P. Sloan Foundation
Research Fellowship to RL.
</bodyText>
<sectionHeader confidence="0.999427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999675280701754">
Arto Anttila. 1997. Variation in Finnish phonology
and morphology. Ph.D. thesis, Stanford U.
Diana Archangeli and Douglas Pulleyblank. 1994.
Grounded phonology. MIT Press.
Paul Boersma. 1999. Empirical tests of the Gradual
Learning Algorithm. Linguistic Inquiry, 32:45–86.
Paul Boersma and Bruce Hayes. 2001. Optimality-
theoretic learning in the Praat program. In Proceed-
ings of the Institute of Phonetic Sciences of the Uni-
versity of Amsterdam.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 19:380–393.
Robert Frank and Giorgio Satta. 1998. Optimality the-
ory and the generative complexity of constraint vio-
lability. Computational Linguistics, 24:307–315.
Sharon Goldwater and Mark Johnson. 2003. Learning
OT constraint rankings using a Maximum Entropy
model. In Proceedings of the Workshop on Variation
within Optimality Theory.
Noah Goodman, Joshua Tenebaum, Jacob Feldman,
and Tom Griffiths. 2008. A rational analysis of rule-
based concept learning. Cognitive Science, 32:108–
154.
Dilan G¨or¨ur, Frank J¨akel, and Carl Rasmussen. 2006.
A choice model with infinitely many latent features.
In Proceedings of the 23rd International Conference
on Machine Learning.
Thomas Griffiths and Zoubin Ghahramani. 2005. Infi-
nite latent feature models and the Indian buffet pro-
cess. Technical Report 2005-001, Gatsby Computa-
tional Neuroscience Unit.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learn-
ing. Linguistic Inquiry, 39:379–440.
Bruce Hayes. 1999. Phonetically driven phonology:
the role of optimality theory and inductive ground-
ing. In Darnell et al, editor, Formalism and Func-
tionalism in Linguistics, vol. 1. Benjamins.
Jeffrey Heinz, Gregory Kobele, and Jason Riggle.
2009. Evaluating the complexity of Optimality The-
ory. Linguistic Inquiry.
Brett Hyde. 2012. Alignment constraints. Natural
Language and Linguistic Theory, 30:789–836.
William Idsardi. 2006. A simple proof that Optimal-
ity Theory is computationally intractable. Linguistic
Inquiry, 37:271–275.
Frank Keller. 2000. Gradience in grammar: Ex-
perimental and computational aspects of degrees of
grammaticality. Ph.D. thesis, U. of Edinburgh.
John McCarthy. 2008. Doing Optimality Theory.
Blackwell.
Daniel Navarro and Tom Griffiths. 2007. A nonpara-
metric Bayesian method for inferring features from
similarity judgments. In Advances in Neural Infor-
mation Processing Systems 19.
</reference>
<page confidence="0.943479">
1102
</page>
<reference confidence="0.999654692307692">
Alan Prince and Paul Smolensky. 1993. Optimality
theory: Constraint interaction in generative gram-
mar. Technical report, Rutgers Center for Cognitive
Science.
Jason Riggle. 2009. Generating contenders. Rutgers
Optimality Archive, 1044.
Jennifer Smith. 2004. Making constraints composi-
tional: toward a compositional model of Con. Lin-
gua, 114:1433–1464.
William Stokoe. 1960. Sign Language Structure. Lin-
stok Press.
Bruce Tesar and Paul Smolensky. 2000. Learnability
in Optimality Theory. MIT Press.
</reference>
<page confidence="0.985221">
1103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938133">
<title confidence="0.988234">Nonparametric Learning of Phonological Constraints in Optimality Theory</title>
<author confidence="0.999959">Gabriel Doyle Klinton Bicknell Roger Levy</author>
<affiliation confidence="0.999966">Department of Linguistics Department of Linguistics Department of Linguistics UC San Diego Northwestern University UC San Diego</affiliation>
<address confidence="0.998754">La Jolla, CA, USA 92093 Evanston, IL, USA 60208 La Jolla, CA, USA</address>
<email confidence="0.99982">gdoyle@ucsd.edukbicknell@northwestern.edurlevy@ucsd.edu</email>
<abstract confidence="0.998263363636364">We present a method to jointly learn features and weights directly from distributional data in a log-linear framework. Specifically, we propose a non-parametric Bayesian model for learning phonological markedness constraints directly from the distribution of input-output mappings in an Optimality Theory (OT) setting. The model uses an Indian Buffet Process prior to learn the feature values used in the loglinear method, and is the first algorithm for learning phonological constraints without presupposing constraint structure. The model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis, with a violation structure corresponding to the standard constraints. These results suggest an alternative data-driven source for constraints instead of a fully innate constraint set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arto Anttila</author>
</authors>
<title>Variation in Finnish phonology and morphology.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford U.</institution>
<contexts>
<context position="5969" citStr="Anttila, 1997" startWordPosition="901" endWordPosition="902">em simultaneously. The constraints are derived from sets of violations that effectively explain the observed data, rather than being selected from a preexisting set of possible constraints. 2.2 OT as a weighted-constraint method Although all OT systems share the same core structure, different choices of EVAL lead to different behaviors. In IBPOT, we use the loglinear EVAL developed by Goldwater and Johnson (2003) in their MaxEnt OT system. MEOT extends traditional OT to account for variation (cases in which multiple candidates can be the winner), as well as gradient/probabilistic productions (Anttila, 1997) and other constraint interactions (e.g., cumulativity) that traditional OT cannot handle (Keller, 2000). MEOT also is motivated by the general MaxEnt framework, whereas most other OT formulations are ad hoc constructions specific to phonology. In MEOT, each constraint Ci is associated with 1Although phonology is usually framed in terms of sound, sign languages also have components that serve equivalent roles in the physical realization of signs (Stokoe, 1960). a weight wi &lt; 0. (Weights are always negative in OT; a constraint violation can never make a candidate more likely to win.) For a give</context>
</contexts>
<marker>Anttila, 1997</marker>
<rawString>Arto Anttila. 1997. Variation in Finnish phonology and morphology. Ph.D. thesis, Stanford U.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Archangeli</author>
<author>Douglas Pulleyblank</author>
</authors>
<title>Grounded phonology.</title>
<date>1994</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7977" citStr="Archangeli and Pulleyblank, 1994" startWordPosition="1223" endWordPosition="1227">s selected. In MEOT, the constraint weights define hierarchies of varying strictness, and some probability is assigned to all candidates. If constraints’ weights are close together, multiple violations of lower-weighted constraints can reduce a candidate’s probability below that of a competitor with a single high-weight violation. As the distance between weights in MEOT increases, the probability of a suboptimal candidate being chosen approaches zero; thus the traditional formulation is a limit case of MEOT. 2.3 OT in practice Figure 1 shows tableaux, a visualization for OT, applied in Wolof (Archangeli and Pulleyblank, 1994; Boersma, 1999). We are interested in four Wolof constraints that combine to induce vowel harmony: *r, PARSE[rtr], HARMONY, and PARSE[atr]. The meaning of these constraints will be discussed in Sect. 4.1; for now, we will only consider their violation profiles. Each column represents a constraint, with weights decreasing leftto-right. Each tableau looks at a single input form, noted in the top-left cell: ete, EtE, rte, or itE. Each row is a candidate output form. A black cell indicates that the candidate, or input-candidate pair, violates the constraint in that column.2 A white cell indicates</context>
<context position="19429" citStr="Archangeli and Pulleyblank, 1994" startWordPosition="3095" endWordPosition="3098"> of M* \ NY + α) and the likeK lihood p(Y |M*, w*, M, w, F), including the new constraints M*. Weight sampling After sampling through all candidates, we use Metropolis-Hastings to estimate new weights for both constraint matrices. Our proposal distribution is Gamma(wk&apos;/η,η/wk), with mean wk and mode wk −η(for wk &gt; 1). Unlike Gibbs wk sampling on the constraints, which occurs only on markedness constraints, weights are sampled for both markedness and faithfulness features. 4 Experiment 4.1 Wolof vowel harmony We test the model by learning the markedness constraints driving Wolof vowel harmony (Archangeli and Pulleyblank, 1994). Vowel harmony in general refers to a phonological phenomenon wherein the vowels of a word share certain features in the output form even if they do not share them in the input. In the case of Wolof, harmony encourages forms that have consistent tongue root positions. The Wolof vowel system has two relevant features, tongue root position and vowel height. The tongue root can either be advanced (ATR) or retracted (RTR), and the body of the tongue can be in the high, middle, or low part of the mouth. These features define six vowels: high mid low ATR i e a RTR i e a We test IBPOT on the harmony</context>
</contexts>
<marker>Archangeli, Pulleyblank, 1994</marker>
<rawString>Diana Archangeli and Douglas Pulleyblank. 1994. Grounded phonology. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Boersma</author>
</authors>
<title>Empirical tests of the Gradual Learning Algorithm. Linguistic Inquiry,</title>
<date>1999</date>
<pages>32--45</pages>
<contexts>
<context position="7993" citStr="Boersma, 1999" startWordPosition="1228" endWordPosition="1229">t weights define hierarchies of varying strictness, and some probability is assigned to all candidates. If constraints’ weights are close together, multiple violations of lower-weighted constraints can reduce a candidate’s probability below that of a competitor with a single high-weight violation. As the distance between weights in MEOT increases, the probability of a suboptimal candidate being chosen approaches zero; thus the traditional formulation is a limit case of MEOT. 2.3 OT in practice Figure 1 shows tableaux, a visualization for OT, applied in Wolof (Archangeli and Pulleyblank, 1994; Boersma, 1999). We are interested in four Wolof constraints that combine to induce vowel harmony: *r, PARSE[rtr], HARMONY, and PARSE[atr]. The meaning of these constraints will be discussed in Sect. 4.1; for now, we will only consider their violation profiles. Each column represents a constraint, with weights decreasing leftto-right. Each tableau looks at a single input form, noted in the top-left cell: ete, EtE, rte, or itE. Each row is a candidate output form. A black cell indicates that the candidate, or input-candidate pair, violates the constraint in that column.2 A white cell indicates no violation. G</context>
<context position="20082" citStr="Boersma, 1999" startWordPosition="3219" endWordPosition="3220">phonological phenomenon wherein the vowels of a word share certain features in the output form even if they do not share them in the input. In the case of Wolof, harmony encourages forms that have consistent tongue root positions. The Wolof vowel system has two relevant features, tongue root position and vowel height. The tongue root can either be advanced (ATR) or retracted (RTR), and the body of the tongue can be in the high, middle, or low part of the mouth. These features define six vowels: high mid low ATR i e a RTR i e a We test IBPOT on the harmony system provided in the Praat program (Boersma, 1999), previously used as a test case by Goldwater and Johnson (2003) for MEOT learning with known constraints. This system has four constraints:4 • Markedness: – *i: do not have i (high RTR vowel) – HARMONY: do not have RTR and ATR vowels in the same word • Faithfulness: – PARSE[rtr]: do not change RTR input to ATR output – PARSE[atr]: do not change ATR input to RTR output These constraints define the phonological standard that we will compare IBPOT to, with a ranking from strongest to weakest of *i &gt;&gt; PARSE[rtr] &gt;&gt; HARMONY &gt;&gt; PARSE[atr]. Under this ranking, Wolof harmony is achieved by changing a</context>
</contexts>
<marker>Boersma, 1999</marker>
<rawString>Paul Boersma. 1999. Empirical tests of the Gradual Learning Algorithm. Linguistic Inquiry, 32:45–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Boersma</author>
<author>Bruce Hayes</author>
</authors>
<title>Optimalitytheoretic learning in the Praat program.</title>
<date>2001</date>
<booktitle>In Proceedings of the Institute of Phonetic Sciences of the</booktitle>
<institution>University of Amsterdam.</institution>
<contexts>
<context position="23640" citStr="Boersma and Hayes, 2001" startWordPosition="3811" endWordPosition="3814"> present experiment, we assume that GEN does not generate candidates with unfaithful vowel heights. If unfaithful vowel heights were allowed by GEN, these unfaithful candidates would incur a violation approximately as strong as *I, as neither unfaithful-height candidates nor I candidates are attested in the Wolof data. 6Since data, matrix, and weight likelihoods all shape the learned constraints, there must be enough data for the model to avoid settling for a simple matrix that poorly explains the data. This represents a similar training set size to previous work (Goldwater and Johnson, 2003; Boersma and Hayes, 2001). erations. The model is initialized with a random markedness matrix drawn from the IBP and weights from the exponential prior. We ran versions of the model with parameter settings between 0.01 and 1 for α, 0.05 and 0.5 for q, and 2 and 5 for K*. All these produced quantitatively similar results; we report values for α = 1, q = 0.5, and K* = 5, which provides the least bias toward small constraint sets. To establish performance for the phonological standard, we use the IBPOT learner to find constraint weights but do not update M. The resultant learner is essentially MaxEnt OT with the weights </context>
</contexts>
<marker>Boersma, Hayes, 2001</marker>
<rawString>Paul Boersma and Bruce Hayes. 2001. Optimalitytheoretic learning in the Praat program. In Proceedings of the Institute of Phonetic Sciences of the University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--380</pages>
<contexts>
<context position="11298" citStr="Pietra et al., 1997" startWordPosition="1766" endWordPosition="1769">ero, approximating the deterministic situation of traditional OT. 2.4 Learning Constraints Choosing a winning candidate presumes that a set of constraints CON is available, but where do these constraints come from? The standard assumption within OT is that CON is innate and universal. But in the absence of direct evidence of innate constraints, we should prefer a method that can derive the constraints from cognitivelygeneral learning over one that assumes they are pre-specified. Learning appropriate model features has been an important idea in the development of constraint-based models (Della Pietra et al., 1997). The innateness assumption can induce tractability issues as well. The strictest formulation of innateness posits that virtually all constraints are shared across all languages, even when there is no evidence for the constraint in a particular language (Tesar and Smolensky, 2000). Strict universality is undermined by the extremely large set of constraints it must weight, as well as the possible existence of language-particular constraints (Smith, 2004). A looser version of universality supposes that constraints are built compositionally from a set of constraint templates or primitives or phon</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19:380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frank</author>
<author>Giorgio Satta</author>
</authors>
<title>Optimality theory and the generative complexity of constraint violability.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--307</pages>
<contexts>
<context position="34451" citStr="Frank and Satta (1998)" startWordPosition="5541" endWordPosition="5544">could be integrated into a joint constraint learning process in the form of a naturalness bias on the constraint weights or a phonologically-motivated replacement for the IBP prior. The results presented here use binary constraints, where each candidate violates each constraint only once, a result of the IBP’s restriction to binary matrices. Non-binarity can be handled by using the binary matrix M to indicate whether a candidate violates a constraint, with a second 1101 distribution determining the number of violations. Alternately, a binary matrix can directly capture non-binary constraints; Frank and Satta (1998) converted existing non-binary constraints into a binary OT system by representing non-binary constraints as a set of equally-weighted overlapping constraints, each accounting for one violation. The non-binary harmony constraint, for instance, becomes a set {*(at least one disharmony), *(at least two disharmonies), etc.}. Lastly, the Wolof vowel harmony problem provides a test case with overlaps in the candidate sets for different inputs. This candidate overlap helps the model find appropriate constraint structures. Analyzing other phenomena may require the identification of appropriate abstra</context>
</contexts>
<marker>Frank, Satta, 1998</marker>
<rawString>Robert Frank and Giorgio Satta. 1998. Optimality theory and the generative complexity of constraint violability. Computational Linguistics, 24:307–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Learning OT constraint rankings using a Maximum Entropy model.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Variation within Optimality Theory.</booktitle>
<contexts>
<context position="1943" citStr="Goldwater and Johnson, 2003" startWordPosition="280" endWordPosition="283">onstraints that push a decision-maker toward different options, whether in something so trivial as choosing a movie or so important as a fight-or-flight response. These constraint-driven decisions can be modeled with a log-linear system. In these models, a set of constraints is weighted and their violations are used to determine a probability distribution over outcomes. But where do these constraints come from? We consider this question by examining the dominant framework in modern phonology, Optimality Theory (Prince and Smolensky, 1993, OT), implemented in a log-linear framework, MaxEnt OT (Goldwater and Johnson, 2003), with output forms’ probabilities based on a weighted sum of constraint violations. OT analyses generally assume that the constraints are innate and universal, both to obviate the problem of learning constraints’ identities and to limit the set of possible languages. We propose a new approach: to learn constraints with limited innate phonological knowledge by identifying sets of constraint violations that explain the observed distributional data, instead of selecting constraints from an innate set of constraint definitions. Because the constraints are identified as sets of violations, this al</context>
<context position="5771" citStr="Goldwater and Johnson (2003)" startWordPosition="868" endWordPosition="872"> issues. Unlike previous OT learning methods, which assume known constraint definitions and only learn the relative strength of these constraints, the IBPOT learns constraint violation profiles and weights for them simultaneously. The constraints are derived from sets of violations that effectively explain the observed data, rather than being selected from a preexisting set of possible constraints. 2.2 OT as a weighted-constraint method Although all OT systems share the same core structure, different choices of EVAL lead to different behaviors. In IBPOT, we use the loglinear EVAL developed by Goldwater and Johnson (2003) in their MaxEnt OT system. MEOT extends traditional OT to account for variation (cases in which multiple candidates can be the winner), as well as gradient/probabilistic productions (Anttila, 1997) and other constraint interactions (e.g., cumulativity) that traditional OT cannot handle (Keller, 2000). MEOT also is motivated by the general MaxEnt framework, whereas most other OT formulations are ad hoc constructions specific to phonology. In MEOT, each constraint Ci is associated with 1Although phonology is usually framed in terms of sound, sign languages also have components that serve equiva</context>
<context position="20146" citStr="Goldwater and Johnson (2003)" startWordPosition="3229" endWordPosition="3233">rd share certain features in the output form even if they do not share them in the input. In the case of Wolof, harmony encourages forms that have consistent tongue root positions. The Wolof vowel system has two relevant features, tongue root position and vowel height. The tongue root can either be advanced (ATR) or retracted (RTR), and the body of the tongue can be in the high, middle, or low part of the mouth. These features define six vowels: high mid low ATR i e a RTR i e a We test IBPOT on the harmony system provided in the Praat program (Boersma, 1999), previously used as a test case by Goldwater and Johnson (2003) for MEOT learning with known constraints. This system has four constraints:4 • Markedness: – *i: do not have i (high RTR vowel) – HARMONY: do not have RTR and ATR vowels in the same word • Faithfulness: – PARSE[rtr]: do not change RTR input to ATR output – PARSE[atr]: do not change ATR input to RTR output These constraints define the phonological standard that we will compare IBPOT to, with a ranking from strongest to weakest of *i &gt;&gt; PARSE[rtr] &gt;&gt; HARMONY &gt;&gt; PARSE[atr]. Under this ranking, Wolof harmony is achieved by changing a disharmonious ATR to an RTR, unless this creates an i vowel. We</context>
<context position="23614" citStr="Goldwater and Johnson, 2003" startWordPosition="3807" endWordPosition="3810">ough the first 2500 it5In the present experiment, we assume that GEN does not generate candidates with unfaithful vowel heights. If unfaithful vowel heights were allowed by GEN, these unfaithful candidates would incur a violation approximately as strong as *I, as neither unfaithful-height candidates nor I candidates are attested in the Wolof data. 6Since data, matrix, and weight likelihoods all shape the learned constraints, there must be enough data for the model to avoid settling for a simple matrix that poorly explains the data. This represents a similar training set size to previous work (Goldwater and Johnson, 2003; Boersma and Hayes, 2001). erations. The model is initialized with a random markedness matrix drawn from the IBP and weights from the exponential prior. We ran versions of the model with parameter settings between 0.01 and 1 for α, 0.05 and 0.5 for q, and 2 and 5 for K*. All these produced quantitatively similar results; we report values for α = 1, q = 0.5, and K* = 5, which provides the least bias toward small constraint sets. To establish performance for the phonological standard, we use the IBPOT learner to find constraint weights but do not update M. The resultant learner is essentially M</context>
</contexts>
<marker>Goldwater, Johnson, 2003</marker>
<rawString>Sharon Goldwater and Mark Johnson. 2003. Learning OT constraint rankings using a Maximum Entropy model. In Proceedings of the Workshop on Variation within Optimality Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah Goodman</author>
<author>Joshua Tenebaum</author>
<author>Jacob Feldman</author>
<author>Tom Griffiths</author>
</authors>
<title>A rational analysis of rulebased concept learning.</title>
<date>2008</date>
<journal>Cognitive Science,</journal>
<volume>32</volume>
<pages>154</pages>
<contexts>
<context position="33733" citStr="Goodman et al., 2008" startWordPosition="5433" endWordPosition="5436">tensionally. A complete model of constraint acquisition should provide intensional definitions that are phonologically grounded and cover potentially non-binary constraints. We discuss how to extend the model toward these goals. IBPOT currently learns extensional constraints, defined by which candidates do or do not violate the constraint. Intensional definitions are needed to extend constraints to unseen forms. Post hoc violation profile analysis, as in Sect. 4.3, provides a first step toward this goal. Such analysis can be integrated into the learning process using the Rational Rules model (Goodman et al., 2008) to identify likely constraint definitions compositionally. Alternately, phonological knowledge could be integrated into a joint constraint learning process in the form of a naturalness bias on the constraint weights or a phonologically-motivated replacement for the IBP prior. The results presented here use binary constraints, where each candidate violates each constraint only once, a result of the IBP’s restriction to binary matrices. Non-binarity can be handled by using the binary matrix M to indicate whether a candidate violates a constraint, with a second 1101 distribution determining the </context>
</contexts>
<marker>Goodman, Tenebaum, Feldman, Griffiths, 2008</marker>
<rawString>Noah Goodman, Joshua Tenebaum, Jacob Feldman, and Tom Griffiths. 2008. A rational analysis of rulebased concept learning. Cognitive Science, 32:108– 154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilan G¨or¨ur</author>
<author>Frank J¨akel</author>
<author>Carl Rasmussen</author>
</authors>
<title>A choice model with infinitely many latent features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning.</booktitle>
<marker>G¨or¨ur, J¨akel, Rasmussen, 2006</marker>
<rawString>Dilan G¨or¨ur, Frank J¨akel, and Carl Rasmussen. 2006. A choice model with infinitely many latent features. In Proceedings of the 23rd International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Griffiths</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Infinite latent feature models and the Indian buffet process.</title>
<date>2005</date>
<tech>Technical Report 2005-001,</tech>
<institution>Gatsby Computational Neuroscience Unit.</institution>
<contexts>
<context position="15194" citStr="Griffiths and Ghahramani, 2005" startWordPosition="2393" endWordPosition="2396">sted separately. As discussed in Sect. 2.4, we assume that F is known as part of the output of GEN (Riggle, 2009). The goal of the IBPOT model is to 3McCarthy (2008, §4.8) gives examples of “ad hoc” intersegmental constraints. Even well-known constraint types, such as generalized alignment, can have disputed structures (Hyde, 2012). learn the markedness matrix M and weights w for both the markedness and faithfulness constraints. As for M, we need a non-parametric prior, as there is no inherent limit to the number of markedness constraints a language will use. We use the Indian Buffet Process (Griffiths and Ghahramani, 2005), which defines a proper probability distribution over binary feature matrices with an unbounded number of columns. The IBP can be thought of as representing the set of dishes that diners eat at an infinite buffet table. Each diner (i.e., output form) first draws dishes (i.e., constraint violations) with probability proportional to the number of previous diners who drew it: p(Mjl = 1|{Mzl}z&lt;j) = nl/j. After choosing from the previously taken dishes, the diner can try additional dishes that no previous diner has had. The number of new dishes that the j-th customer draws follows a Poisson(α/j) d</context>
</contexts>
<marker>Griffiths, Ghahramani, 2005</marker>
<rawString>Thomas Griffiths and Zoubin Ghahramani. 2005. Infinite latent feature models and the Indian buffet process. Technical Report 2005-001, Gatsby Computational Neuroscience Unit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Hayes</author>
<author>Colin Wilson</author>
</authors>
<title>A maximum entropy model of phonotactics and phonotactic learning. Linguistic Inquiry,</title>
<date>2008</date>
<pages>39--379</pages>
<contexts>
<context position="12301" citStr="Hayes and Wilson (2008)" startWordPosition="1916" endWordPosition="1920">ll as the possible existence of language-particular constraints (Smith, 2004). A looser version of universality supposes that constraints are built compositionally from a set of constraint templates or primitives or phonological features (Hayes, 1999; Smith, 2004; Idsardi, 2006; Riggle, 2009). This version allows language-particular constraints, but it comes with a computational cost, as the learner must be able to generate and evaluate possible constraints while learning the language’s phonology. Even with relatively simple constraint templates, such as the phonological constraint learner of Hayes and Wilson (2008), the number of possible constraints expands exponentially. Depending on the specific formulation of the constraints, the constraint identification problem may even be NP-hard (Idsardi, 2006; Heinz et al., 2009). Our approach of casting the learning problem as one of identifying violation profiles is an attempt to determine the amount that can be learned about the active constraints in a paradigm without hypothesizing intensional constraint definitions. The violation profile informa1096 tion used by our model could then be used to narrow the search space for intensional constraints, either by </context>
</contexts>
<marker>Hayes, Wilson, 2008</marker>
<rawString>Bruce Hayes and Colin Wilson. 2008. A maximum entropy model of phonotactics and phonotactic learning. Linguistic Inquiry, 39:379–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Hayes</author>
</authors>
<title>Phonetically driven phonology: the role of optimality theory and inductive grounding.</title>
<date>1999</date>
<booktitle>Formalism and Functionalism in Linguistics,</booktitle>
<volume>1</volume>
<editor>In Darnell et al, editor,</editor>
<publisher>Benjamins.</publisher>
<contexts>
<context position="11928" citStr="Hayes, 1999" startWordPosition="1864" endWordPosition="1865">sumption can induce tractability issues as well. The strictest formulation of innateness posits that virtually all constraints are shared across all languages, even when there is no evidence for the constraint in a particular language (Tesar and Smolensky, 2000). Strict universality is undermined by the extremely large set of constraints it must weight, as well as the possible existence of language-particular constraints (Smith, 2004). A looser version of universality supposes that constraints are built compositionally from a set of constraint templates or primitives or phonological features (Hayes, 1999; Smith, 2004; Idsardi, 2006; Riggle, 2009). This version allows language-particular constraints, but it comes with a computational cost, as the learner must be able to generate and evaluate possible constraints while learning the language’s phonology. Even with relatively simple constraint templates, such as the phonological constraint learner of Hayes and Wilson (2008), the number of possible constraints expands exponentially. Depending on the specific formulation of the constraints, the constraint identification problem may even be NP-hard (Idsardi, 2006; Heinz et al., 2009). Our approach o</context>
<context position="13458" citStr="Hayes, 1999" startWordPosition="2098" endWordPosition="2099">he search space for intensional constraints, either by performing post-hoc analysis of the constraints identified by our model or by combining intensional constraint search into the learning process. We discuss each of these possibilities in Section 5.2. Innateness is less of a concern for faithfulness than markedness constraints. Faithfulness violations are determined by the changes between an input form and a candidate, yielding an independent motivation for a universal set of faithfulness constraints (McCarthy, 2008). Some markedness constraints can also be motivated in a universal manner (Hayes, 1999), but many markedness constraints lack such grounding.3 As such, it is unclear where a universal set of markedness constraints would come from. 3 The IBPOT Model 3.1 Structure The IBPOT model defines a generative process for mappings between input and output forms based on three latent variables: the constraint violation matrices F (faithfulness) and M (markedness), and the weight vector w. The cells of the violation matrices correspond to the number of violations of a constraint by a given input-output mapping. Fijk is the number of violations of faithfulness constraint Fk by input-output pai</context>
</contexts>
<marker>Hayes, 1999</marker>
<rawString>Bruce Hayes. 1999. Phonetically driven phonology: the role of optimality theory and inductive grounding. In Darnell et al, editor, Formalism and Functionalism in Linguistics, vol. 1. Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Heinz</author>
<author>Gregory Kobele</author>
<author>Jason Riggle</author>
</authors>
<title>Evaluating the complexity of Optimality Theory. Linguistic Inquiry.</title>
<date>2009</date>
<contexts>
<context position="12512" citStr="Heinz et al., 2009" startWordPosition="1948" endWordPosition="1951"> phonological features (Hayes, 1999; Smith, 2004; Idsardi, 2006; Riggle, 2009). This version allows language-particular constraints, but it comes with a computational cost, as the learner must be able to generate and evaluate possible constraints while learning the language’s phonology. Even with relatively simple constraint templates, such as the phonological constraint learner of Hayes and Wilson (2008), the number of possible constraints expands exponentially. Depending on the specific formulation of the constraints, the constraint identification problem may even be NP-hard (Idsardi, 2006; Heinz et al., 2009). Our approach of casting the learning problem as one of identifying violation profiles is an attempt to determine the amount that can be learned about the active constraints in a paradigm without hypothesizing intensional constraint definitions. The violation profile informa1096 tion used by our model could then be used to narrow the search space for intensional constraints, either by performing post-hoc analysis of the constraints identified by our model or by combining intensional constraint search into the learning process. We discuss each of these possibilities in Section 5.2. Innateness </context>
</contexts>
<marker>Heinz, Kobele, Riggle, 2009</marker>
<rawString>Jeffrey Heinz, Gregory Kobele, and Jason Riggle. 2009. Evaluating the complexity of Optimality Theory. Linguistic Inquiry.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett Hyde</author>
</authors>
<title>Alignment constraints. Natural Language and Linguistic Theory,</title>
<date>2012</date>
<pages>30--789</pages>
<contexts>
<context position="14896" citStr="Hyde, 2012" startWordPosition="2345" endWordPosition="2346"> provides weight for both F and M. Probabilities of output forms are given by a log-linear function: p(yj|xi) = (2) exp (Ek wkFijk + El wlMjl) E exp (Ek wkFizk + El wlMzl) yzEY(xi) Note that this is the same structure as Eq. 1 but with faithfulness and markedness constraints listed separately. As discussed in Sect. 2.4, we assume that F is known as part of the output of GEN (Riggle, 2009). The goal of the IBPOT model is to 3McCarthy (2008, §4.8) gives examples of “ad hoc” intersegmental constraints. Even well-known constraint types, such as generalized alignment, can have disputed structures (Hyde, 2012). learn the markedness matrix M and weights w for both the markedness and faithfulness constraints. As for M, we need a non-parametric prior, as there is no inherent limit to the number of markedness constraints a language will use. We use the Indian Buffet Process (Griffiths and Ghahramani, 2005), which defines a proper probability distribution over binary feature matrices with an unbounded number of columns. The IBP can be thought of as representing the set of dishes that diners eat at an infinite buffet table. Each diner (i.e., output form) first draws dishes (i.e., constraint violations) w</context>
</contexts>
<marker>Hyde, 2012</marker>
<rawString>Brett Hyde. 2012. Alignment constraints. Natural Language and Linguistic Theory, 30:789–836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Idsardi</author>
</authors>
<title>A simple proof that Optimality Theory is computationally intractable. Linguistic Inquiry,</title>
<date>2006</date>
<pages>37--271</pages>
<contexts>
<context position="11956" citStr="Idsardi, 2006" startWordPosition="1868" endWordPosition="1870">bility issues as well. The strictest formulation of innateness posits that virtually all constraints are shared across all languages, even when there is no evidence for the constraint in a particular language (Tesar and Smolensky, 2000). Strict universality is undermined by the extremely large set of constraints it must weight, as well as the possible existence of language-particular constraints (Smith, 2004). A looser version of universality supposes that constraints are built compositionally from a set of constraint templates or primitives or phonological features (Hayes, 1999; Smith, 2004; Idsardi, 2006; Riggle, 2009). This version allows language-particular constraints, but it comes with a computational cost, as the learner must be able to generate and evaluate possible constraints while learning the language’s phonology. Even with relatively simple constraint templates, such as the phonological constraint learner of Hayes and Wilson (2008), the number of possible constraints expands exponentially. Depending on the specific formulation of the constraints, the constraint identification problem may even be NP-hard (Idsardi, 2006; Heinz et al., 2009). Our approach of casting the learning probl</context>
</contexts>
<marker>Idsardi, 2006</marker>
<rawString>William Idsardi. 2006. A simple proof that Optimality Theory is computationally intractable. Linguistic Inquiry, 37:271–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
</authors>
<title>Gradience in grammar: Experimental and computational aspects of degrees of grammaticality.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>U. of Edinburgh.</institution>
<contexts>
<context position="6073" citStr="Keller, 2000" startWordPosition="916" endWordPosition="917">ved data, rather than being selected from a preexisting set of possible constraints. 2.2 OT as a weighted-constraint method Although all OT systems share the same core structure, different choices of EVAL lead to different behaviors. In IBPOT, we use the loglinear EVAL developed by Goldwater and Johnson (2003) in their MaxEnt OT system. MEOT extends traditional OT to account for variation (cases in which multiple candidates can be the winner), as well as gradient/probabilistic productions (Anttila, 1997) and other constraint interactions (e.g., cumulativity) that traditional OT cannot handle (Keller, 2000). MEOT also is motivated by the general MaxEnt framework, whereas most other OT formulations are ad hoc constructions specific to phonology. In MEOT, each constraint Ci is associated with 1Although phonology is usually framed in terms of sound, sign languages also have components that serve equivalent roles in the physical realization of signs (Stokoe, 1960). a weight wi &lt; 0. (Weights are always negative in OT; a constraint violation can never make a candidate more likely to win.) For a given inputcandidate pair (x, y), fi(y, x) is the number of violations of constraint Ci by the pair. As a ma</context>
</contexts>
<marker>Keller, 2000</marker>
<rawString>Frank Keller. 2000. Gradience in grammar: Experimental and computational aspects of degrees of grammaticality. Ph.D. thesis, U. of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John McCarthy</author>
</authors>
<title>Doing Optimality Theory.</title>
<date>2008</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="13371" citStr="McCarthy, 2008" startWordPosition="2085" endWordPosition="2086">s. The violation profile informa1096 tion used by our model could then be used to narrow the search space for intensional constraints, either by performing post-hoc analysis of the constraints identified by our model or by combining intensional constraint search into the learning process. We discuss each of these possibilities in Section 5.2. Innateness is less of a concern for faithfulness than markedness constraints. Faithfulness violations are determined by the changes between an input form and a candidate, yielding an independent motivation for a universal set of faithfulness constraints (McCarthy, 2008). Some markedness constraints can also be motivated in a universal manner (Hayes, 1999), but many markedness constraints lack such grounding.3 As such, it is unclear where a universal set of markedness constraints would come from. 3 The IBPOT Model 3.1 Structure The IBPOT model defines a generative process for mappings between input and output forms based on three latent variables: the constraint violation matrices F (faithfulness) and M (markedness), and the weight vector w. The cells of the violation matrices correspond to the number of violations of a constraint by a given input-output mapp</context>
<context position="14727" citStr="McCarthy (2008" startWordPosition="2322" endWordPosition="2323">kedness constraint M·l by output candidate yj. Note that M is shared across inputs, as Mjl has the same value for all input-output pairs with output yj. The weight vector w provides weight for both F and M. Probabilities of output forms are given by a log-linear function: p(yj|xi) = (2) exp (Ek wkFijk + El wlMjl) E exp (Ek wkFizk + El wlMzl) yzEY(xi) Note that this is the same structure as Eq. 1 but with faithfulness and markedness constraints listed separately. As discussed in Sect. 2.4, we assume that F is known as part of the output of GEN (Riggle, 2009). The goal of the IBPOT model is to 3McCarthy (2008, §4.8) gives examples of “ad hoc” intersegmental constraints. Even well-known constraint types, such as generalized alignment, can have disputed structures (Hyde, 2012). learn the markedness matrix M and weights w for both the markedness and faithfulness constraints. As for M, we need a non-parametric prior, as there is no inherent limit to the number of markedness constraints a language will use. We use the Indian Buffet Process (Griffiths and Ghahramani, 2005), which defines a proper probability distribution over binary feature matrices with an unbounded number of columns. The IBP can be th</context>
</contexts>
<marker>McCarthy, 2008</marker>
<rawString>John McCarthy. 2008. Doing Optimality Theory. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Navarro</author>
<author>Tom Griffiths</author>
</authors>
<title>A nonparametric Bayesian method for inferring features from similarity judgments.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 19.</booktitle>
<contexts>
<context position="16101" citStr="Navarro and Griffiths, 2007" startWordPosition="2545" endWordPosition="2548">onstraint violations) with probability proportional to the number of previous diners who drew it: p(Mjl = 1|{Mzl}z&lt;j) = nl/j. After choosing from the previously taken dishes, the diner can try additional dishes that no previous diner has had. The number of new dishes that the j-th customer draws follows a Poisson(α/j) distribution. The complete specification of the model is then: M ∼ IBP(α); Y(xi) = Gen(xi) w ∼ −F(1,1); y|xi ∼ LogLin(M, F, w, Y(xi)) 3.2 Inference To perform inference in this model, we adopt a common Markov chain Monte Carlo estimation procedure for IBPs (G¨or¨ur et al., 2006; Navarro and Griffiths, 2007). We alternate approximate Gibbs sampling over the constraint matrix M, using the IBP prior, with a Metropolis-Hastings method to sample constraint weights w. We initialize the model with a randomly-drawn markedness violation matrix M and weight vector w. To learn, we iterate through the output forms yj; for each, we split M_j· into “represented” constraints (those that are violated by at least one output form other than yj) and “non-represented” constraints (those violated only by yj). For each represented constraint M·l, we re-sample the value for the cell Mjl. All non-represented constraint</context>
</contexts>
<marker>Navarro, Griffiths, 2007</marker>
<rawString>Daniel Navarro and Tom Griffiths. 2007. A nonparametric Bayesian method for inferring features from similarity judgments. In Advances in Neural Information Processing Systems 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Prince</author>
<author>Paul Smolensky</author>
</authors>
<title>Optimality theory: Constraint interaction in generative grammar.</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>Rutgers Center for Cognitive Science.</institution>
<contexts>
<context position="1858" citStr="Prince and Smolensky, 1993" startWordPosition="268" endWordPosition="271">nt set. 1 Introduction Many aspects of human cognition involve the interaction of constraints that push a decision-maker toward different options, whether in something so trivial as choosing a movie or so important as a fight-or-flight response. These constraint-driven decisions can be modeled with a log-linear system. In these models, a set of constraints is weighted and their violations are used to determine a probability distribution over outcomes. But where do these constraints come from? We consider this question by examining the dominant framework in modern phonology, Optimality Theory (Prince and Smolensky, 1993, OT), implemented in a log-linear framework, MaxEnt OT (Goldwater and Johnson, 2003), with output forms’ probabilities based on a weighted sum of constraint violations. OT analyses generally assume that the constraints are innate and universal, both to obviate the problem of learning constraints’ identities and to limit the set of possible languages. We propose a new approach: to learn constraints with limited innate phonological knowledge by identifying sets of constraint violations that explain the observed distributional data, instead of selecting constraints from an innate set of constrai</context>
<context position="7100" citStr="Prince and Smolensky, 1993" startWordPosition="1090" endWordPosition="1093">egative in OT; a constraint violation can never make a candidate more likely to win.) For a given inputcandidate pair (x, y), fi(y, x) is the number of violations of constraint Ci by the pair. As a maximum entropy model, the probability of y given x is proportional to the exponential of the weighted sum of violations, Ei wifi(y, x). If y(x) is the set of all output candidates for the input x, then the probability of y as the winning output is: p(y |x) = exp (Ei wifi(y, x))(1) Ez∈Y(x) exp(Ei wifi(z, x)) This formulation represents a probabilistic extension of the traditional formulation of OT (Prince and Smolensky, 1993). Traditionally, constraints form a strict hierarchy, where a single violation of a high-ranked constraint is worse than any number of violations of lower-ranked constraints. Traditional OT is also deterministic, with the optimal candidate always selected. In MEOT, the constraint weights define hierarchies of varying strictness, and some probability is assigned to all candidates. If constraints’ weights are close together, multiple violations of lower-weighted constraints can reduce a candidate’s probability below that of a competitor with a single high-weight violation. As the distance betwee</context>
</contexts>
<marker>Prince, Smolensky, 1993</marker>
<rawString>Alan Prince and Paul Smolensky. 1993. Optimality theory: Constraint interaction in generative grammar. Technical report, Rutgers Center for Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riggle</author>
</authors>
<title>Generating contenders.</title>
<date>2009</date>
<journal>Rutgers Optimality Archive,</journal>
<pages>1044</pages>
<contexts>
<context position="11971" citStr="Riggle, 2009" startWordPosition="1871" endWordPosition="1872">s well. The strictest formulation of innateness posits that virtually all constraints are shared across all languages, even when there is no evidence for the constraint in a particular language (Tesar and Smolensky, 2000). Strict universality is undermined by the extremely large set of constraints it must weight, as well as the possible existence of language-particular constraints (Smith, 2004). A looser version of universality supposes that constraints are built compositionally from a set of constraint templates or primitives or phonological features (Hayes, 1999; Smith, 2004; Idsardi, 2006; Riggle, 2009). This version allows language-particular constraints, but it comes with a computational cost, as the learner must be able to generate and evaluate possible constraints while learning the language’s phonology. Even with relatively simple constraint templates, such as the phonological constraint learner of Hayes and Wilson (2008), the number of possible constraints expands exponentially. Depending on the specific formulation of the constraints, the constraint identification problem may even be NP-hard (Idsardi, 2006; Heinz et al., 2009). Our approach of casting the learning problem as one of id</context>
<context position="14676" citStr="Riggle, 2009" startWordPosition="2312" endWordPosition="2313">e (xi, yj); Mjl is the number of violations of markedness constraint M·l by output candidate yj. Note that M is shared across inputs, as Mjl has the same value for all input-output pairs with output yj. The weight vector w provides weight for both F and M. Probabilities of output forms are given by a log-linear function: p(yj|xi) = (2) exp (Ek wkFijk + El wlMjl) E exp (Ek wkFizk + El wlMzl) yzEY(xi) Note that this is the same structure as Eq. 1 but with faithfulness and markedness constraints listed separately. As discussed in Sect. 2.4, we assume that F is known as part of the output of GEN (Riggle, 2009). The goal of the IBPOT model is to 3McCarthy (2008, §4.8) gives examples of “ad hoc” intersegmental constraints. Even well-known constraint types, such as generalized alignment, can have disputed structures (Hyde, 2012). learn the markedness matrix M and weights w for both the markedness and faithfulness constraints. As for M, we need a non-parametric prior, as there is no inherent limit to the number of markedness constraints a language will use. We use the Indian Buffet Process (Griffiths and Ghahramani, 2005), which defines a proper probability distribution over binary feature matrices wit</context>
</contexts>
<marker>Riggle, 2009</marker>
<rawString>Jason Riggle. 2009. Generating contenders. Rutgers Optimality Archive, 1044.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Smith</author>
</authors>
<title>Making constraints compositional: toward a compositional model of Con. Lingua,</title>
<date>2004</date>
<pages>114--1433</pages>
<contexts>
<context position="11755" citStr="Smith, 2004" startWordPosition="1839" endWordPosition="1840">are pre-specified. Learning appropriate model features has been an important idea in the development of constraint-based models (Della Pietra et al., 1997). The innateness assumption can induce tractability issues as well. The strictest formulation of innateness posits that virtually all constraints are shared across all languages, even when there is no evidence for the constraint in a particular language (Tesar and Smolensky, 2000). Strict universality is undermined by the extremely large set of constraints it must weight, as well as the possible existence of language-particular constraints (Smith, 2004). A looser version of universality supposes that constraints are built compositionally from a set of constraint templates or primitives or phonological features (Hayes, 1999; Smith, 2004; Idsardi, 2006; Riggle, 2009). This version allows language-particular constraints, but it comes with a computational cost, as the learner must be able to generate and evaluate possible constraints while learning the language’s phonology. Even with relatively simple constraint templates, such as the phonological constraint learner of Hayes and Wilson (2008), the number of possible constraints expands exponenti</context>
</contexts>
<marker>Smith, 2004</marker>
<rawString>Jennifer Smith. 2004. Making constraints compositional: toward a compositional model of Con. Lingua, 114:1433–1464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Stokoe</author>
</authors>
<title>Sign Language Structure.</title>
<date>1960</date>
<publisher>Linstok Press.</publisher>
<contexts>
<context position="6433" citStr="Stokoe, 1960" startWordPosition="971" endWordPosition="972">nal OT to account for variation (cases in which multiple candidates can be the winner), as well as gradient/probabilistic productions (Anttila, 1997) and other constraint interactions (e.g., cumulativity) that traditional OT cannot handle (Keller, 2000). MEOT also is motivated by the general MaxEnt framework, whereas most other OT formulations are ad hoc constructions specific to phonology. In MEOT, each constraint Ci is associated with 1Although phonology is usually framed in terms of sound, sign languages also have components that serve equivalent roles in the physical realization of signs (Stokoe, 1960). a weight wi &lt; 0. (Weights are always negative in OT; a constraint violation can never make a candidate more likely to win.) For a given inputcandidate pair (x, y), fi(y, x) is the number of violations of constraint Ci by the pair. As a maximum entropy model, the probability of y given x is proportional to the exponential of the weighted sum of violations, Ei wifi(y, x). If y(x) is the set of all output candidates for the input x, then the probability of y as the winning output is: p(y |x) = exp (Ei wifi(y, x))(1) Ez∈Y(x) exp(Ei wifi(z, x)) This formulation represents a probabilistic extensio</context>
</contexts>
<marker>Stokoe, 1960</marker>
<rawString>William Stokoe. 1960. Sign Language Structure. Linstok Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Tesar</author>
<author>Paul Smolensky</author>
</authors>
<title>Learnability in Optimality Theory.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11579" citStr="Tesar and Smolensky, 2000" startWordPosition="1810" endWordPosition="1813">universal. But in the absence of direct evidence of innate constraints, we should prefer a method that can derive the constraints from cognitivelygeneral learning over one that assumes they are pre-specified. Learning appropriate model features has been an important idea in the development of constraint-based models (Della Pietra et al., 1997). The innateness assumption can induce tractability issues as well. The strictest formulation of innateness posits that virtually all constraints are shared across all languages, even when there is no evidence for the constraint in a particular language (Tesar and Smolensky, 2000). Strict universality is undermined by the extremely large set of constraints it must weight, as well as the possible existence of language-particular constraints (Smith, 2004). A looser version of universality supposes that constraints are built compositionally from a set of constraint templates or primitives or phonological features (Hayes, 1999; Smith, 2004; Idsardi, 2006; Riggle, 2009). This version allows language-particular constraints, but it comes with a computational cost, as the learner must be able to generate and evaluate possible constraints while learning the language’s phonology</context>
</contexts>
<marker>Tesar, Smolensky, 2000</marker>
<rawString>Bruce Tesar and Paul Smolensky. 2000. Learnability in Optimality Theory. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>