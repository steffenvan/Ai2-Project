<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.943395">
Efficient Deep Processing of Japanese
</title>
<note confidence="0.740922">
Melanie SIEGEL
DFKI GmbH
Stuhlsatzenhausweg 3
66123 Saarbr√ºcken, Germany
</note>
<email confidence="0.912144">
siegel@dfki.de
</email>
<address confidence="0.558015">
Emily M. BENDER
CSLI Stanford
220 Panama Street
Stanford, CA, 94305-4115, USA
</address>
<email confidence="0.992476">
bender@csli.stanford.edu
</email>
<sectionHeader confidence="0.993698" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999889181818182">
We present a broad coverage Japanese
grammar written in the HPSG formalism
with MRS semantics. The grammar is
created for use in real world applications,
such that robustness and performance issues
play an important role. It is connected to a
POS tagging and word segmentation tool.
This grammar is being developed in a
multilingual context, requiring MRS
structures that are easily comparable across
languages.
</bodyText>
<sectionHeader confidence="0.959606" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999926547619048">
Natural language processing technology has
recently reached a point where applications that
rely on deep linguistic processing are becoming
feasible. Such applications (e.g. message
extraction systems, machine translation and
dialogue understanding systems) require natural
language understanding, or at least an
approximation thereof. This, in turn, requires
rich and highly precise information as the output
of a parse. However, if the technology is to
meet the demands of real-world applications,
this must not come at the cost of robustness.
Robustness requires not only wide coverage by
the grammar (in both syntax and semantics), but
also large and extensible lexica as well as
interfaces to preprocessing systems for named
entity recognition, non-linguistic structures such
as addresses, etc. Furthermore, applications
built on deep NLP technology should be
extensible to multiple languages. This requires
flexible yet well-defined output structures that
can be adapted to grammars of many different
languages. Finally, for use in real-world
applications, NLP systems meeting the above
desiderata must also be efficient.
In this paper, we describe the development of
a broad coverage grammar for Japanese that is
used in an automatic email response application.
The grammar is based on work done in the
Verbmobil project (Siegel 2000) on machine
translation of spoken dialogues in the domain of
travel planning. It has since been greatly
extended to accommodate written Japanese and
new domains.
The grammar is couched in the theoretical
framework of Head-Driven Phrase Structure
Grammar (HPSG) (Pollard &amp; Sag 1994), with
semantic representations in Minimal Recursion
Semantics (MRS) (Copestake et al. 2001).
HPSG is well suited to the task of multilingual
development of broad coverage grammars: It is
flexible enough (analyses can be shared across
languages but also tailored as necessary), and
has a rich theoretical literature from which to
draw analyzes and inspiration. The
characteristic type hierarchy of HPSG also
facilitates the development of grammars that are
easy to extend. MRS is a flat semantic
formalism that works well with typed feature
structures and is flexible in that it provides
structures that are under-specified for scopal
information. These structures give compact
representations of ambiguities that are often
irrelevant to the task at hand.
HPSG and MRS have the further advantage
that there are practical and useful open-source
tools for writing, testing, and efficiently
processing grammars written in these
formalisms. The tools we are using in this
project include the LKB system (Copestake
2002) for grammar development, [incr tsdb()]
(Oepen &amp; Carroll 2000) for testing the grammar
and tracking changes, and PET (Callmeier
2000), a very efficient HPSG parser, for
processing. We also use the ChaSen tokenizer
and POS tagger (Asahara &amp; Matsumoto 2000).
While couched within the same general
framework (BPSG), our approach differs from
that of Kanayama et al (2000). The work
described there achieves impressive coverage
(83.7% on the EDR corpus of newspaper text)
with an underspecified grammar consisting of a
small number of lexical entries, lexical types
associated with parts of speech, and six
underspecified grammar rules. In contrast, our
grammar is much larger in terms of the number
of lexical entries, the number of grammar rules,
and the constraints on both,1 and takes
correspondingly more effort to bring up to that
level of coverage. The higher level of detail
allows us to output precise semantic
representations as well as to use syntactic,
semantic and lexical information to reduce
ambiguity and rank parses.
</bodyText>
<sectionHeader confidence="0.972959" genericHeader="method">
1 Japanese HPSG Syntax
</sectionHeader>
<bodyText confidence="0.956750517241379">
The fundamental notion of an BPSG is the sign.
A sign is a complex feature structure
representing information of different linguistic
levels of a phrase or lexical item. The attribute-
value matrix of a sign in the Japanese BPSG is
quite similar to a sign in the LinGO English
Resource Grammar (henceforth ERG)
(Flickinger 2000), with information about the
orthographical realization of the lexical sign in
PHON, syntactic and semantic information in
SYNSEM, information about the lexical status in
LEX, nonlocal information in NONLOC, head
information that goes up the tree in HEAD and
information about subcategorization in SUBCAT.
The grammar implementation is based on a
system of types. There are 900 lexical types that
define the syntactic, semantic and pragmatic
properties of the Japanese words, and 188 types
that define the properties of phrases and lexical
rules. The grammar includes 50 lexical rules for
inflectional and derivational morphology and 47
phrase structure rules. The lexicon contains 5100
stem entries. As the grammar is developed for
use in applications, it treats a wide range of
1 We do also make use of generic lexical entries for
certain parts of speech as a means of extending our
lexicon. See section 3 below.
basic constructions of Japanese. Only some of
these phenomena can be described here.
</bodyText>
<subsectionHeader confidence="0.99884">
1.1 Subcategorization
</subsectionHeader>
<bodyText confidence="0.999572222222222">
The structure of SUBCAT is different from the
ERG SUBCAT structure. This is due to
differences in subcategorization between
Japanese and English. A fundamental difference
is the fact that, in Japanese, verbal arguments are
frequently omitted. For example, arguments that
refer to the speaker, addressee, and other
arguments that can be inferred from context are
often omitted in spoken language. Additionally,
optional verbal arguments can scramble. On the
other hand, some arguments are not only
obligatory, but must also be realized adjacent to
the selecting head.
To account for this, our subcategorization
contains the attributes SAT and VAL. The SAT
value encodes whether a verbal argument is
already saturated (such that it cannot be
saturated again), optional or adjacent. VAL
contains the agreement information for the
argument. When an argument is realized, its
SAT value on the mother node is specified as sat
and its SYNSEM is unified with its VAL value on
the subcategorizing head. The VAL value on the
mother is none. Adjacency must be checked in
every rule that combines heads and arguments or
adjuncts. This is the principle of adjacency,
stated as follows:
In a headed phrase, the SUBCAT.SAT value
on the non-head daughter must not contain
any adjacent arguments. In a head-
complement structure, the SUBCAT.SAT
value of the head daughter must not contain
any adjacent arguments besides the non-
head daughter. In a head-adjunct structure,
the SUBCAT.SAT value of the head daughter
must not contain any adjacent arguments.
</bodyText>
<subsectionHeader confidence="0.969063">
1.2 Verbal inflection
</subsectionHeader>
<bodyText confidence="0.996256475">
Japanese verb stems combine with endings that
provide information about honorification, tense,
aspect, voice and mode. Inflectional rules for the
different types of stems prepare the verb stems
for combination with the verbal endings. For
example, the verb stem yomu must be inflected
to yon to combine with the past tense ending da.
Morphological features constrain the
combination of stem and ending. In the above
example, the inflectional rule changes the mu
character to the n character and assigns the value
nd-morph to the morphological feature
RMORPH-BIND-TYPE. The ending da selects
for a verbal stem with this value.
Endings can be combined with other endings,
as in -sase-rare-mashi-ta (causative-potential-
honorific-past), but not arbitrarily:
*-sase-mashi-rare-ta
*-sase-ta-mashi-rare
-sase-ta
-rare-mashi-ta
This is accounted for with two kinds of rules
which realize mutually selected elements. In the
combination of stem and ending, the verb stem
selects for the verbal ending via the head feature
SPEC. In the case of the combination of two
verbal endings, the first ending selects for the
second one via the head feature MARK. In both
cases, the right element subcategorizes for the
left one via SUBCAT.VAL.SPR. Using this
mechanism, it is possible to control the sequence
of verbal endings: Verb stems select verbal
endings via SPEC and take no SPR, derivational
morphemes (like causative or potential) select
tense endings or other derivational morphemes
via MARK and subcategorize for verb stems
and/or verb endings via SPR (sase takes only
verb stems), and tense endings take verb stems
or endings as SPR and take no MARK or SPEC
(as they occur at the end of the sequence).
</bodyText>
<subsectionHeader confidence="0.99476">
1.3 Complex Predicates
</subsectionHeader>
<bodyText confidence="0.999987">
A special treatment is needed for Japanese
verbal noun + light verb constructions. In these
cases, a word that combines the qualities of a
noun with those of a verb occurs in a
construction with a verb that has only marginal
semantic information. The syntactic, semantic
and pragmatic information on the complex is a
combination of the information of the two.
Consider example 1. The verbal noun
benkyou contains subcategorization information
(transitive), as well as semantic information (the
benkyou-relation and its semantic arguments).
The light verb shi-ta supplies tense information
(past). Pragmatic information can be supplied by
both parts of the construction, as in the formal
form o-benkyou shi-mashi-ta. The rule that
licenses this type of combination is the vn-light-
rule, a subtype of the head-marker-rule.
</bodyText>
<equation confidence="0.8486295">
Example 1:
Benkyou shi-ta.
</equation>
<bodyText confidence="0.998448214285714">
study do-past
&apos;Someone has studied.&apos;
Japanese auxiliaries combine with verbs and
provide either aspectual or perspective
information or information about honorification.
In a verb-auxiliary construction, the information
about subcategorization is a combination of the
SUBCAT information of verb and auxiliary,
depending on the type of auxiliary. The rule
responsible for the information combination in
these cases is the head-specifier-rule. We have
three basic types of auxiliaries. The first type is
aspect auxiliaries. These are treated as raising
verbs, and include such elements as iru (roughly,
progressive) and aru (roughly, perfective), as
can be seen in example 2. The other two classes
of auxiliaries provide information about
perspective or the point of view from which a
situation is being described. Both classes of
auxiliaries add a ni (dative) marked argument to
the argument structure of the whole predicate.
The classes differ in how they relate their
arguments to the arguments of the verb. One
class (including kureru &apos;give&apos;; see example 3) are
treated as subject control verbs. The other class
(including morau &apos;receive&apos;, see example 4)
establishes a control relation between the ni-
marked argument and the embedded subject.
</bodyText>
<figure confidence="0.6973447">
Example 2:
Keeki wo tabe-te iru.
cake ACC eat progressive
&apos;Someone is eating cake.&apos;
Example 3:
Sensei wa watashi ni hon wo
teacher TOP I DAT book ACC
katte kure-ta.
buy give-past
&apos;The teacher bought me a book.&apos;
</figure>
<figureCaption confidence="0.123296">
Example 4:
</figureCaption>
<bodyText confidence="0.5901522">
Watashi ga sensei ni hon wo
I NOM teacher DAT book ACC
katte morat-ta.
buy get-past
&apos;The teacher bought me a book.&apos;
</bodyText>
<subsectionHeader confidence="0.988497">
1.4 Particles in a type hierarchy
</subsectionHeader>
<bodyText confidence="0.9999864">
The careful treatment of Japanese particles is
essential, because they are the most frequently
occurring words and have various central
functions in the grammar. It is difficult, because
one particle can fulfill more than one function
and they can co-occur, but not arbitrarily. The
Japanese grammar thus contains a type hierarchy
of 44 types for particles. See Siegel (1999) for a
more detailed description of relevant phenomena
and solutions.
</bodyText>
<subsectionHeader confidence="0.993415">
1.5 Numeral Expressions
</subsectionHeader>
<bodyText confidence="0.9999508125">
Number names, such as sen kyuu hyaku juu
&apos;1910&apos; constitute a notable exception to the
general head-final pattern of Japanese phrases.
We found Smith&apos;s (1999) head-medial analysis
of English number names to be directly
applicable to the Japanese system as well
(Bender 2002). This analysis was easily
incorporated into the grammar, despite the
oddity of head positioning, because the type
hierarchy of HPSG is well suited to express the
partial generalizations that permeate natural
language.
On the other hand, number names in
Japanese contrast sharply with number names in
English in that they are rarely used without a
numeral classifier.
</bodyText>
<equation confidence="0.51919025">
Example 5:
Juu *(hiki no) neko ga ki-ta.
ten CL GEN cat NOM arrive-past
&apos;Ten cats arrived.&apos;
</equation>
<bodyText confidence="0.999865333333333">
The grammar provides for &apos;true&apos; numeral
classifiers like hon, ko, and hiki, as well as
formatives like en &apos;yen&apos; and do &apos;degree&apos; which
combine with number names just like numeral
classifiers do, but never serve as numeral
classifiers for other nouns. In addition, there are
a few non-branching rules that allow bare
number names to surface as numeral classifier
phrases with specific semantic constraints.
</bodyText>
<subsectionHeader confidence="0.955299">
1.6 Pragmatic information
</subsectionHeader>
<bodyText confidence="0.999922714285714">
Spoken language and email correspondence both
encode references to the social relation of the
dialogue partners. Utterances can express social
distance between addressee and speaker and
third persons. Honorifics can even express
respect towards inanimates. Pragmatic
information is treated in the CONTEXT layer of
the complex signs. Honorific information is
given in the CONTEXT.BACKGROUND and
linked to addressee and speaker anchors.
The expression of empathy or in-group vs.
out-group is quite prevalent in Japanese. One
means of expressing empathy is the perspective
auxiliaries discussed above. For example, two
auxiliaries meaning roughly &apos;give&apos; (ageru and
kureru) contrast in where they place the
empathy. In the case of ageru, it is with the
giver. In the case of kureru, it is with the
recipient. We model this within the sign by
positing a feature EMPATHY within CONTEXT
and linking it to the relevant arguments&apos; indices.
</bodyText>
<sectionHeader confidence="0.986886" genericHeader="method">
2 Japanese MRS Semantics
</sectionHeader>
<bodyText confidence="0.984779105263158">
In the multilingual context in which this
grammar has been developed, a high premium is
placed on parallel and consistent semantic
representations between grammars for different
languages. Ensuring this parallelism enables the
reuse of the same downstream technology, no
matter which language is used as input.
Integrating MRS representations parallel to
those used in the ERG into the Japanese
grammar took approximately 3 months. Of
course, semantic work is on-going, as every new
construction treated needs to be given a suitable
semantic representation. For the most part,
semantic representations developed for English
were straightforwardly applicable to Japanese.
This section provides a brief overview of those
cases where the Japanese constructions we
encountered led to innovations in the semantic
representations and/or the correspondence
between syntactic and semantic structures. Due
to space limitations, we discuss these analyses in
general terms and omit technical details.
2.l Nominalization and Verbal Nouns
Nominalization is of course attested in English
and across languages. However, it is much more
prevalent in Japanese than in English, primarily
because of verbal nouns. As noted in Section
1.3 above, a verbal noun like benkyou &apos;study&apos; can
appear in syntactic contexts requiring nouns, or,
in combination with a light verb, in contexts
requiring verbs. One possible analysis would
provide two separate lexical entries, one with
nominal and one with verbal semantics.
However, this would not only be redundant
(missing the systematic relationship between
these uses of verbal nouns) but would also
contradict the intuition that even in its nominal
use, the arguments of benkyou are still present.
</bodyText>
<equation confidence="0.364745">
Example 6:
</equation>
<bodyText confidence="0.980526833333333">
Nihongo no benkyou wo hajimeru.
Japanese GEN study ACC begin
&apos;Someone begins the study of Japanese.&apos;
In order to capture this intuition, we opted for an
analysis that essentially treats verbal nouns as
underlyingly verbal. The nominal uses are
produced by a lexical rule which nominalizes the
verbal nouns. The semantic effect of this rule is
to provide a nominal relation which introduces a
variable which can in turn be bound by
quantifiers. The nominal relation subordinates
the original verbal relation supplied by the
verbal noun. The rule is lexical as we have not
yet found any cases where the verb&apos;s arguments
are clearly filled by phrases in the syntax. If
they do appear, it is with genitive marking (e.g.,
nihongo no in the example above). In order to
reduce ambiguity, we leave the relationship
between these genitive marked NPs and the
nominalized verbal noun underspecified. There
is nothing in the syntax to disambiguate these
cases, and we find that they are better left to
downstream processing, where there may be
access to world knowledge.
</bodyText>
<subsectionHeader confidence="0.996505">
2.2 Numeral Classifiers
</subsectionHeader>
<bodyText confidence="0.999987423076923">
As noted in Section1.5, the internal syntax of
number names is surprisingly parallel between
English and Japanese, but their external syntax
differs dramatically. English number names can
appear directly as modifiers of NPs and are
treated semantically as adjectives in the ERG.
Japanese number names can only modify nouns
in combination with numeral classifiers. In
addition, numeral classifier phrases can appear
in NP positions (akin to partitives in English).
Finally, some numeral-classifier-like elements
do not serve the modifier function but can only
head phrases that fill NP positions.
This constellation of facts required the
following innovations: a representation of
numbers that doesn&apos;t treat them as adjectives (in
MRS terms, a feature structure without the ARG
feature), a representation of the semantic
contribution of numeral classifiers (a relation
between numbers and the nouns they modify,
this time with an ARG feature), and a set of
rules for promoting numeral classifier phrases to
NPs that contribute the appropriate nominal
semantics (underspecified in the case of ordinary
numeral classifiers or specific in the case of
words like en &apos;yen&apos;).
</bodyText>
<subsectionHeader confidence="0.999068">
2.3 Relative Clauses and Adjectives
</subsectionHeader>
<bodyText confidence="0.999976888888889">
The primary issue in the analysis of relative
clauses and adjectives is the possibility of
extreme ambiguity, due to several intersecting
factors: Japanese has rampant pro-drop and
does not have any relative pronouns. In
addition, a head noun modified by a relative
clause need not correspond to any gap in the
relative clause, as shown by examples like the
following (Matsumoto 1997):
</bodyText>
<equation confidence="0.5462645">
Example 7:
atama ga yoku naru hon
</equation>
<bodyText confidence="0.994294">
head NOM better become book
&apos;a book that makes one smarter&apos;
Therefore, if we were to posit an attributive
adjective + noun construction (distinct from the
relative clause + noun possibility) we would
have systematic ambiguities for NPs like akai
hon (&apos;red book&apos;), ambiguities which could never
be resolved based on information in the
sentence. Instead, we have opted for a relative
clause analysis of any adjective + noun
combination in which the adjective could
potentially be used predicatively. Furthermore,
because of gapless relative clauses like the one
cited above, we have opted for a non-extraction
analysis of relative clauses.2
Nonetheless, the well-formedness constraints
on MRS representations require that there be
2 There is in fact some linguistic evidence for
extraction in some relative clauses in Japanese (see
e.g., Baldwin 2001). However, we saw no practical
need to allow for this possibility in our grammar, and
particularly not one that would justify the increase in
ambiguity. There is also evidence that some
adjectives are true attributives and cannot be used
predicatively (Yamakido 2000). These are handled by
a separate adjective + noun rule restricted to just
these cases.
some relationship between the head noun and
the relative clause. We picked the topic relation
for this purpose (following Kuno 1973). The
topic relation is introduced into the semantics by
the relative clause rule. As with main clause
topics (which we also give a non-extraction
analysis), we rely on downstream anaphora
resolution to refine the relationship.
</bodyText>
<subsectionHeader confidence="0.987057">
2.4 Summary
</subsectionHeader>
<bodyText confidence="0.999883818181818">
For the most part, semantic representations and
the syntax-semantic interface already worked
out in the ERG were directly applicable to the
Japanese grammar. In those cases where
Japanese presented problems not yet
encountered (or at least not yet tackled) in
English, it was fairly straightforward to work out
suitable MRS representations and means of
building them up. Both of these points illustrate
the cross-linguistic validity and practical utility
of MRS representations.
</bodyText>
<sectionHeader confidence="0.889534" genericHeader="method">
3 Integration of a Morphological
Analyzer
</sectionHeader>
<bodyText confidence="0.9999325">
As Japanese written text does not have word
segmentation, a preprocessing system is
required. We integrated ChaSen (Asahara &amp;
Matsumoto 2000), a tool that provides word
segmentation as well as POS tags and
morphological information such as verbal
inflection. As the lexical coverage of ChaSen is
higher than that of the HPSG lexicon, default
part-of-speech entries are inserted into the
lexicon. These are triggered by the part-of-
speech information given by ChaSen, if there is
no existing entry in the lexicon. These specific
default entries assign a type to the word that
contains features typical to its part-of-speech. It
is therefore possible to restrict the lexicon to
those cases where the lexical information
contains more than the typical information for a
certain part-of-speech. This default mechanism
is often used for different kinds of names and
&apos;ordinary&apos; nouns, but also for adverbs,
interjections and verbal nouns (where we
assume a default transitive valence pattern).3
</bodyText>
<footnote confidence="0.617507666666667">
3 Kanayama et al. (2000) use a similar mechanism for
most words. They report only 105 grammar-inherent
lexical entries.
</footnote>
<bodyText confidence="0.999741157894737">
The ChaSen lexicon is extended with a domain-
specific lexicon, containing, among others,
names in the domain of banking.
For verbs and adjectives, ChaSen gives
information about stems and inflection that is
used in a similar way. The inflection type is
translated to an HPSG type. These types interact
with the inflectional rules in the grammar such
that the default entries are inflected just as
&apos;known&apos; words would be.
In addition to the preprocessing done by
ChaSen, an additional (shallow) preprocessing
tool recognizes numbers, date expressions,
addresses, email addresses, URLs, telephone
numbers and currency expressions. The output
of the preprocessing tool replaces these
expressions in the string with placeholders. The
placeholders are parsed by the grammar using
special placeholder lexical entries.
</bodyText>
<sectionHeader confidence="0.939113" genericHeader="method">
4 Robustness and Performance Issues
</sectionHeader>
<bodyText confidence="0.999914909090909">
The grammar is aimed at working with real-
world data, rather than at experimenting with
linguistic examples. Therefore, robustness and
performance issues play an important role.
While grammar development is carried out in
the LKB (Copestake 2002), processing (both in
the application domain and for the purposes of
running test suites) is done with the highly
efficient PET parser (Callmeier 2000). Figures 1
and 2 show the performance of PET parsing of
hand-made and real data, respectively.
</bodyText>
<footnote confidence="0.33145325">
Fig.1 Performance parsing banking data, generated
by [incr tsdb()]
Fig.2 Performance parsing document request data,
generated by [incr tsdb()]
</footnote>
<bodyText confidence="0.999738">
One characteristic of real-world data is the
variety of punctuation marks that occur and the
potential for ambiguity that they bring. In our
grammar, certain punctuation marks are given
lexical entries and processed by grammar rules.
Take, for example, quotation marks. Ignoring
them (as done in most development-oriented
grammars and smaller grammars), leads to a
significant loss of structural information:
</bodyText>
<figure confidence="0.952831407407408">
Phenomenon items etasks filter edges first total tcpu gc space
# √ò % √ò √ò (s) √ò (s) √ò (s) √ò (s) √ò (kb)
Total
742
946
95.7
303
0.06
0.11
0.11
0
833
Phenomenon items etasks filter edges first total tcpu tgc space
# √ò % √ò √ò (s) √ò (s) √ò (s) √ò (s) √ò (kb)
Total
316
2020
96.5
616
0.23
0.26
0.26
0
1819
Example 8:
&amp;quot;Botan wo osu&amp;quot; to it-ta
button ACC push COMPL say-past
</figure>
<bodyText confidence="0.989413696428571">
&apos;Someone said: ‚Äúpush the button. &amp;quot;‚Äô
The formative to is actually ambiguous between
a complementizer and a conjunction. Since the
phrase before to is a complete sentence, this
string is ambiguous if one ignores the quotation
marks. With the quotation marks, however, only
the complementizer to is possible. Given the
high degree of ambiguity inherent in broad-
coverage grammars, we have found it extremely
useful to parse punctuation rather than ignore it.
The domains we have been working on (like
many others) contain many date and number
expressions. While a shallow tool recognizes
general structures, the grammar contains rules
and types to process these.
Phenomena occurring in semi-spontaneous
language (email correspondence), such as
interjections (e.g. maa &apos;well&apos;), contracted verb
forms (e.g. tabe-chatta &lt; tabete-shimatta
&apos;(someone) ate it all up&apos;), fragmentary sentences
(e.g. bangou: 1265 &apos;number: 1265&apos;) and NP
fragments (e.g. bangou? &apos;number?&apos;) must be
covered as well as the &apos;ordinary&apos; complete
sentences found in more carefully edited text.
Our grammar includes types, lexical entries, and
grammar rules for dealing with such phenomena.
Perhaps the most important performance
issue for broad coverage grammars is ambiguity.
At one point in the development of this
grammar, the average number of readings
doubled in two months of work. We currently
have two strategies for addressing this problem:
First, we include a mechanism into the grammar
rules that chooses left-branching rules in cases
of compounds, genitive modification and
conjuncts, as we don‚Äôt have enough lexical-
semantic information represented to choose the
right dependencies in these cases.4 Secondly, we
use a mechanism for hand-coding reading
preferences among rules and lexical entries.
4Consider, for example, genitive modification: The
semantic relationship between modifier and modifiee
is dependent on their semantic properties: toukyou no
kaigi - &apos;the meeting in Tokyo&apos;, watashi no hon - &apos;my
book&apos;. More lexical-semantic information is needed
to choose the correct parse in more complex
structures, such as in watashi no toukyou no imooto ‚Äì
‚ÄòMy sister in Tokyo‚Äô.
Restrictions like head-complement preferred to
head-adjunct are quite obvious. Others require
domain-specific mechanisms that shall be
subject of further work. Stochastic
disambiguation methods being developed for the
ERG by the Redwoods project at Stanford
University (Oepen et al. 2002) should be
applicable to this grammar as well.
</bodyText>
<sectionHeader confidence="0.999038" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999857111111111">
The grammar currently covers 93.4% of
constructed examples for the banking domain
(747 sentences) and 78.2% of realistic email
correspondence data (316 sentences), concerning
requests for documents. During three months of
work, the coverage in the banking domain
increased 48.49%. The coverage of the
document request data increased 51.43% in the
following two weeks.
</bodyText>
<table confidence="0.9848684">
Phenomenon total positive word lexical parser total overall
items items string items analyses results coverage
# # % √ò √ò # %
Total 747 747 101 75.24 6.54 698 93.4
Fig.3 Coverage of banking data, generated by
[incr tsdb()]
Phenomenon total positive word lexical parser total overall
items items string items analyses results coverage
# # % √ò √ò # %
Total 316 316 1.00 83.90 39.91 247 78.2
</table>
<tableCaption confidence="0.689426">
Fig.4 Coverage of document request data, generated
by [incr tsdb()]
</tableCaption>
<bodyText confidence="0.999974166666667">
We applied the grammar to unseen data in one
of the covered domains, namely the FAQ site of
a Japanese bank. The coverage was 61%. 91.2%
of the parses output were associated with all
well-formed MRSs. That means that we could
get correct MRSs in 55.61% of all sentences.
</bodyText>
<sectionHeader confidence="0.916188" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999982516129032">
We described a broad coverage Japanese
grammar, based on HPSG theory. It encodes
syntactic, semantic, and pragmatic information.
The grammar system is connected to a
morphological analysis system and uses default
entries for words unknown to the HPSG lexicon.
Some basic constructions of the Japanese
grammar were described. As the grammar is
aimed at working in applications with real-world
data, performance and robustness issues are
important.
The grammar is being developed in a
multilingual context, where much value is
placed on parallel and consistent semantic
representations. The development of this
grammar constitutes an important test of the
cross-linguistic validity of the MRS formalism.
The evaluation shows that the grammar is at
a stage where domain adaptation is possible in a
reasonable amount of time. Thus, it is a
powerful resource for linguistic applications for
Japanese.
In future work, this grammar could be further
adapted to another domain, such as the EDR
newspaper corpus (including a headline
grammar). As each new domain is approached,
we anticipate that the adaptation will become
easier as resources from earlier domains are
reused. Initial evaluation of the grammar on
new domains and the growth curve of grammar
coverage should bear this out.
</bodyText>
<sectionHeader confidence="0.999099" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999870858974359">
Asahara, Masayuki and Yuji Matsumoto (2000).
Extended Models and Tools for High-performance
Part-of-speech Tagger. In Proceedings of the 18th
International Conference on Computational
Linguistics, Coling 2000, 21-27. Saarbr√ºcken,
Germany.
Baldwin, Timothy (2001). Making Lexical Sense of
Japanese-English Machine Translation: A
Disambiguation Extravaganza. PhD thesis, Tokyo
Institute of Technology.
Bender, Emily M. (2002). Number Names in
Japanese: A Head-Medial Construction in a Head-
Final Language. Paper presented at the 76th
annual meeting of the LSA, San Francisco.
Callmeier, Ulrich (2000). PET ‚Äî a platform for
experimentation with efficient HPSG processing
techniques. Journal of Natural Language
Engineering, Special Issue on Efficient Processing
with HPSG: Methods, Systems, Evaluation, pages
99-108.
Copestake, Ann (2002). Implementing Typed
Feature-Structure Grammars. Stanford: CSLI.
Copestake, Ann, Alex Lascarides, and Dan Flickinger
(2001). An Algebra for Semantic Construction in
Constraint-based Grammars. Proceedings of the
39th Annual Meeting of the Association for
Computational Linguistics (ACL 2001), Toulouse,
France.
Flickinger, Dan (2000). On Building a More
Efficient Grammar by Exploiting Types. Natural
Language Engineering 6(1) (Special Issue on
Efficient Processing with HPSG), pages 15-28.
Kanayama, Hiroshi, Kentaro Torisawa, Yutaka
Mitsuishi and Jun‚Äôichi Tsujii (2000). A Hybrid
Japanese Parser with Hand-crafted Grammar and
Statistics. In Proceedings of the 18th International
Conference on Computational Linguistics, Coling
2000. Saarbr√ºcken, Germany.
Kuno, Susumu (1973). The Structure of the Japanese
Language. Cambridge, MA: The MIT Press.
Matsumoto, Yoshiko (1997). Noun-Modifying
Constructions in Japanese: A Frame Semantic
Approach. John Benjamins.
Oepen, Stephan and John Carroll (2000).
Performance Profiling for Parser Engineering.
Journal of Natural Language Engineering, Special
Issue on Efficient Processing with HPSG: Methods,
Systems, Evaluation, pages 81-97.
Oepen, Stephan, Kristina Toutanova, Stuart Shieber,
Chris Manning, Dan Flickinger and Thorsten
Brants (2002). The LinGO Redwoods Treebank.
Motivation and Preliminary Applications. In
Proceedings of the 19th International Conference
on Computational Linguistics, Coling 2002. Tapei,
Taiwan. .
Pollard, Carl and Ivan A. Sag (1994). Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Siegel, Melanie (1999). The Syntactic Processing of
Particles in Japanese Spoken Language. In: Wang,
Jhing-Fa and Wu, Chung-Hsien (eds.):
Proceedings of the 13th Pacific Asia Conference
on Language, Information and Computation,
Taipei 1999.
Siegel, Melanie (2000). HPSG Analysis of Japanese.
In: W. Wahlster (ed.): Verbmobil: Foundations of
Speech-to-Speech Translation. Springer Verlag.
Smith, Jeffrey D. (1999). English number names in
HPSG. In Gert Webelhuth, Andreas Kathol, and
Jean-Pierre Koenig (eds.), Lexical and
Constructional Aspects of Linguistic Explanation.
Stanford: CSLI. 145-160.
Yamakido, Hiroko (2000). Japanese attributive
adjectives are not (all) relative clauses. In Roger
Billerey and Brook Danielle Lillehaugen (eds.),
WCCFL 19: Proceedings of the 19th West Coast
Conference on Formal Linguistics. Somerville,
MA: Cascadilla Press. 588-602.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.401322">
<title confidence="0.741101">Efficient Deep Processing of Japanese Melanie DFKI Stuhlsatzenhausweg</title>
<address confidence="0.977462">66123 Saarbr√ºcken,</address>
<email confidence="0.992675">siegel@dfki.de</email>
<author confidence="0.996021">M Emily</author>
<affiliation confidence="0.841903">CSLI</affiliation>
<address confidence="0.9979585">220 Panama Stanford, CA, 94305-4115,</address>
<email confidence="0.999867">bender@csli.stanford.edu</email>
<abstract confidence="0.995405">We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics. The grammar is created for use in real world applications, such that robustness and performance issues play an important role. It is connected to a POS tagging and word segmentation tool. This grammar is being developed in a multilingual context, requiring MRS structures that are easily comparable across languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extended Models and Tools for High-performance Part-of-speech Tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics, Coling</booktitle>
<pages>21--27</pages>
<location>Saarbr√ºcken, Germany.</location>
<contexts>
<context position="3555" citStr="Asahara &amp; Matsumoto 2000" startWordPosition="524" endWordPosition="527">n. These structures give compact representations of ambiguities that are often irrelevant to the task at hand. HPSG and MRS have the further advantage that there are practical and useful open-source tools for writing, testing, and efficiently processing grammars written in these formalisms. The tools we are using in this project include the LKB system (Copestake 2002) for grammar development, [incr tsdb()] (Oepen &amp; Carroll 2000) for testing the grammar and tracking changes, and PET (Callmeier 2000), a very efficient HPSG parser, for processing. We also use the ChaSen tokenizer and POS tagger (Asahara &amp; Matsumoto 2000). While couched within the same general framework (BPSG), our approach differs from that of Kanayama et al (2000). The work described there achieves impressive coverage (83.7% on the EDR corpus of newspaper text) with an underspecified grammar consisting of a small number of lexical entries, lexical types associated with parts of speech, and six underspecified grammar rules. In contrast, our grammar is much larger in terms of the number of lexical entries, the number of grammar rules, and the constraints on both,1 and takes correspondingly more effort to bring up to that level of coverage. The</context>
<context position="20706" citStr="Asahara &amp; Matsumoto 2000" startWordPosition="3202" endWordPosition="3205">ations and the syntax-semantic interface already worked out in the ERG were directly applicable to the Japanese grammar. In those cases where Japanese presented problems not yet encountered (or at least not yet tackled) in English, it was fairly straightforward to work out suitable MRS representations and means of building them up. Both of these points illustrate the cross-linguistic validity and practical utility of MRS representations. 3 Integration of a Morphological Analyzer As Japanese written text does not have word segmentation, a preprocessing system is required. We integrated ChaSen (Asahara &amp; Matsumoto 2000), a tool that provides word segmentation as well as POS tags and morphological information such as verbal inflection. As the lexical coverage of ChaSen is higher than that of the HPSG lexicon, default part-of-speech entries are inserted into the lexicon. These are triggered by the part-ofspeech information given by ChaSen, if there is no existing entry in the lexicon. These specific default entries assign a type to the word that contains features typical to its part-of-speech. It is therefore possible to restrict the lexicon to those cases where the lexical information contains more than the t</context>
</contexts>
<marker>Asahara, Matsumoto, 2000</marker>
<rawString>Asahara, Masayuki and Yuji Matsumoto (2000). Extended Models and Tools for High-performance Part-of-speech Tagger. In Proceedings of the 18th International Conference on Computational Linguistics, Coling 2000, 21-27. Saarbr√ºcken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>Making Lexical Sense of Japanese-English Machine Translation: A Disambiguation Extravaganza.</title>
<date>2001</date>
<tech>PhD thesis,</tech>
<institution>Tokyo Institute of Technology.</institution>
<contexts>
<context position="19317" citStr="Baldwin 2001" startWordPosition="2991" endWordPosition="2992">like akai hon (&apos;red book&apos;), ambiguities which could never be resolved based on information in the sentence. Instead, we have opted for a relative clause analysis of any adjective + noun combination in which the adjective could potentially be used predicatively. Furthermore, because of gapless relative clauses like the one cited above, we have opted for a non-extraction analysis of relative clauses.2 Nonetheless, the well-formedness constraints on MRS representations require that there be 2 There is in fact some linguistic evidence for extraction in some relative clauses in Japanese (see e.g., Baldwin 2001). However, we saw no practical need to allow for this possibility in our grammar, and particularly not one that would justify the increase in ambiguity. There is also evidence that some adjectives are true attributives and cannot be used predicatively (Yamakido 2000). These are handled by a separate adjective + noun rule restricted to just these cases. some relationship between the head noun and the relative clause. We picked the topic relation for this purpose (following Kuno 1973). The topic relation is introduced into the semantics by the relative clause rule. As with main clause topics (wh</context>
</contexts>
<marker>Baldwin, 2001</marker>
<rawString>Baldwin, Timothy (2001). Making Lexical Sense of Japanese-English Machine Translation: A Disambiguation Extravaganza. PhD thesis, Tokyo Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
</authors>
<title>Number Names in Japanese: A Head-Medial Construction in a HeadFinal Language. Paper presented at the 76th annual meeting of the LSA,</title>
<date>2002</date>
<location>San Francisco.</location>
<contexts>
<context position="12181" citStr="Bender 2002" startWordPosition="1886" endWordPosition="1887">ntral functions in the grammar. It is difficult, because one particle can fulfill more than one function and they can co-occur, but not arbitrarily. The Japanese grammar thus contains a type hierarchy of 44 types for particles. See Siegel (1999) for a more detailed description of relevant phenomena and solutions. 1.5 Numeral Expressions Number names, such as sen kyuu hyaku juu &apos;1910&apos; constitute a notable exception to the general head-final pattern of Japanese phrases. We found Smith&apos;s (1999) head-medial analysis of English number names to be directly applicable to the Japanese system as well (Bender 2002). This analysis was easily incorporated into the grammar, despite the oddity of head positioning, because the type hierarchy of HPSG is well suited to express the partial generalizations that permeate natural language. On the other hand, number names in Japanese contrast sharply with number names in English in that they are rarely used without a numeral classifier. Example 5: Juu *(hiki no) neko ga ki-ta. ten CL GEN cat NOM arrive-past &apos;Ten cats arrived.&apos; The grammar provides for &apos;true&apos; numeral classifiers like hon, ko, and hiki, as well as formatives like en &apos;yen&apos; and do &apos;degree&apos; which combin</context>
</contexts>
<marker>Bender, 2002</marker>
<rawString>Bender, Emily M. (2002). Number Names in Japanese: A Head-Medial Construction in a HeadFinal Language. Paper presented at the 76th annual meeting of the LSA, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Callmeier</author>
</authors>
<title>PET ‚Äî a platform for experimentation with efficient HPSG processing techniques.</title>
<date>2000</date>
<journal>Journal of Natural Language Engineering, Special</journal>
<booktitle>Issue on Efficient Processing with HPSG: Methods, Systems, Evaluation,</booktitle>
<pages>99--108</pages>
<contexts>
<context position="3433" citStr="Callmeier 2000" startWordPosition="506" endWordPosition="507">feature structures and is flexible in that it provides structures that are under-specified for scopal information. These structures give compact representations of ambiguities that are often irrelevant to the task at hand. HPSG and MRS have the further advantage that there are practical and useful open-source tools for writing, testing, and efficiently processing grammars written in these formalisms. The tools we are using in this project include the LKB system (Copestake 2002) for grammar development, [incr tsdb()] (Oepen &amp; Carroll 2000) for testing the grammar and tracking changes, and PET (Callmeier 2000), a very efficient HPSG parser, for processing. We also use the ChaSen tokenizer and POS tagger (Asahara &amp; Matsumoto 2000). While couched within the same general framework (BPSG), our approach differs from that of Kanayama et al (2000). The work described there achieves impressive coverage (83.7% on the EDR corpus of newspaper text) with an underspecified grammar consisting of a small number of lexical entries, lexical types associated with parts of speech, and six underspecified grammar rules. In contrast, our grammar is much larger in terms of the number of lexical entries, the number of gra</context>
<context position="22916" citStr="Callmeier 2000" startWordPosition="3542" endWordPosition="3543">The output of the preprocessing tool replaces these expressions in the string with placeholders. The placeholders are parsed by the grammar using special placeholder lexical entries. 4 Robustness and Performance Issues The grammar is aimed at working with realworld data, rather than at experimenting with linguistic examples. Therefore, robustness and performance issues play an important role. While grammar development is carried out in the LKB (Copestake 2002), processing (both in the application domain and for the purposes of running test suites) is done with the highly efficient PET parser (Callmeier 2000). Figures 1 and 2 show the performance of PET parsing of hand-made and real data, respectively. Fig.1 Performance parsing banking data, generated by [incr tsdb()] Fig.2 Performance parsing document request data, generated by [incr tsdb()] One characteristic of real-world data is the variety of punctuation marks that occur and the potential for ambiguity that they bring. In our grammar, certain punctuation marks are given lexical entries and processed by grammar rules. Take, for example, quotation marks. Ignoring them (as done in most development-oriented grammars and smaller grammars), leads t</context>
</contexts>
<marker>Callmeier, 2000</marker>
<rawString>Callmeier, Ulrich (2000). PET ‚Äî a platform for experimentation with efficient HPSG processing techniques. Journal of Natural Language Engineering, Special Issue on Efficient Processing with HPSG: Methods, Systems, Evaluation, pages 99-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Copestake</author>
</authors>
<title>Implementing Typed Feature-Structure Grammars.</title>
<date>2002</date>
<publisher>CSLI.</publisher>
<location>Ann</location>
<contexts>
<context position="3300" citStr="Copestake 2002" startWordPosition="486" endWordPosition="487">SG also facilitates the development of grammars that are easy to extend. MRS is a flat semantic formalism that works well with typed feature structures and is flexible in that it provides structures that are under-specified for scopal information. These structures give compact representations of ambiguities that are often irrelevant to the task at hand. HPSG and MRS have the further advantage that there are practical and useful open-source tools for writing, testing, and efficiently processing grammars written in these formalisms. The tools we are using in this project include the LKB system (Copestake 2002) for grammar development, [incr tsdb()] (Oepen &amp; Carroll 2000) for testing the grammar and tracking changes, and PET (Callmeier 2000), a very efficient HPSG parser, for processing. We also use the ChaSen tokenizer and POS tagger (Asahara &amp; Matsumoto 2000). While couched within the same general framework (BPSG), our approach differs from that of Kanayama et al (2000). The work described there achieves impressive coverage (83.7% on the EDR corpus of newspaper text) with an underspecified grammar consisting of a small number of lexical entries, lexical types associated with parts of speech, and s</context>
<context position="22765" citStr="Copestake 2002" startWordPosition="3518" endWordPosition="3519">ional (shallow) preprocessing tool recognizes numbers, date expressions, addresses, email addresses, URLs, telephone numbers and currency expressions. The output of the preprocessing tool replaces these expressions in the string with placeholders. The placeholders are parsed by the grammar using special placeholder lexical entries. 4 Robustness and Performance Issues The grammar is aimed at working with realworld data, rather than at experimenting with linguistic examples. Therefore, robustness and performance issues play an important role. While grammar development is carried out in the LKB (Copestake 2002), processing (both in the application domain and for the purposes of running test suites) is done with the highly efficient PET parser (Callmeier 2000). Figures 1 and 2 show the performance of PET parsing of hand-made and real data, respectively. Fig.1 Performance parsing banking data, generated by [incr tsdb()] Fig.2 Performance parsing document request data, generated by [incr tsdb()] One characteristic of real-world data is the variety of punctuation marks that occur and the potential for ambiguity that they bring. In our grammar, certain punctuation marks are given lexical entries and proc</context>
</contexts>
<marker>Copestake, 2002</marker>
<rawString>Copestake, Ann (2002). Implementing Typed Feature-Structure Grammars. Stanford: CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Alex Lascarides</author>
<author>Dan Flickinger</author>
</authors>
<title>An Algebra for Semantic Construction in Constraint-based Grammars.</title>
<date>2001</date>
<booktitle>Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="2377" citStr="Copestake et al. 2001" startWordPosition="342" endWordPosition="345"> desiderata must also be efficient. In this paper, we describe the development of a broad coverage grammar for Japanese that is used in an automatic email response application. The grammar is based on work done in the Verbmobil project (Siegel 2000) on machine translation of spoken dialogues in the domain of travel planning. It has since been greatly extended to accommodate written Japanese and new domains. The grammar is couched in the theoretical framework of Head-Driven Phrase Structure Grammar (HPSG) (Pollard &amp; Sag 1994), with semantic representations in Minimal Recursion Semantics (MRS) (Copestake et al. 2001). HPSG is well suited to the task of multilingual development of broad coverage grammars: It is flexible enough (analyses can be shared across languages but also tailored as necessary), and has a rich theoretical literature from which to draw analyzes and inspiration. The characteristic type hierarchy of HPSG also facilitates the development of grammars that are easy to extend. MRS is a flat semantic formalism that works well with typed feature structures and is flexible in that it provides structures that are under-specified for scopal information. These structures give compact representation</context>
</contexts>
<marker>Copestake, Lascarides, Flickinger, 2001</marker>
<rawString>Copestake, Ann, Alex Lascarides, and Dan Flickinger (2001). An Algebra for Semantic Construction in Constraint-based Grammars. Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL 2001), Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On Building a More Efficient Grammar by Exploiting Types.</title>
<date>2000</date>
<journal>Natural Language Engineering</journal>
<booktitle>(Special Issue on Efficient Processing with HPSG),</booktitle>
<volume>6</volume>
<issue>1</issue>
<pages>15--28</pages>
<contexts>
<context position="4680" citStr="Flickinger 2000" startWordPosition="708" endWordPosition="709"> on both,1 and takes correspondingly more effort to bring up to that level of coverage. The higher level of detail allows us to output precise semantic representations as well as to use syntactic, semantic and lexical information to reduce ambiguity and rank parses. 1 Japanese HPSG Syntax The fundamental notion of an BPSG is the sign. A sign is a complex feature structure representing information of different linguistic levels of a phrase or lexical item. The attributevalue matrix of a sign in the Japanese BPSG is quite similar to a sign in the LinGO English Resource Grammar (henceforth ERG) (Flickinger 2000), with information about the orthographical realization of the lexical sign in PHON, syntactic and semantic information in SYNSEM, information about the lexical status in LEX, nonlocal information in NONLOC, head information that goes up the tree in HEAD and information about subcategorization in SUBCAT. The grammar implementation is based on a system of types. There are 900 lexical types that define the syntactic, semantic and pragmatic properties of the Japanese words, and 188 types that define the properties of phrases and lexical rules. The grammar includes 50 lexical rules for inflectiona</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Flickinger, Dan (2000). On Building a More Efficient Grammar by Exploiting Types. Natural Language Engineering 6(1) (Special Issue on Efficient Processing with HPSG), pages 15-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
</authors>
<title>Kentaro Torisawa, Yutaka Mitsuishi and Jun‚Äôichi Tsujii</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics, Coling</booktitle>
<location>Saarbr√ºcken, Germany.</location>
<marker>Kanayama, 2000</marker>
<rawString>Kanayama, Hiroshi, Kentaro Torisawa, Yutaka Mitsuishi and Jun‚Äôichi Tsujii (2000). A Hybrid Japanese Parser with Hand-crafted Grammar and Statistics. In Proceedings of the 18th International Conference on Computational Linguistics, Coling 2000. Saarbr√ºcken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susumu Kuno</author>
</authors>
<title>The Structure of the Japanese Language.</title>
<date>1973</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="19804" citStr="Kuno 1973" startWordPosition="3069" endWordPosition="3070">be 2 There is in fact some linguistic evidence for extraction in some relative clauses in Japanese (see e.g., Baldwin 2001). However, we saw no practical need to allow for this possibility in our grammar, and particularly not one that would justify the increase in ambiguity. There is also evidence that some adjectives are true attributives and cannot be used predicatively (Yamakido 2000). These are handled by a separate adjective + noun rule restricted to just these cases. some relationship between the head noun and the relative clause. We picked the topic relation for this purpose (following Kuno 1973). The topic relation is introduced into the semantics by the relative clause rule. As with main clause topics (which we also give a non-extraction analysis), we rely on downstream anaphora resolution to refine the relationship. 2.4 Summary For the most part, semantic representations and the syntax-semantic interface already worked out in the ERG were directly applicable to the Japanese grammar. In those cases where Japanese presented problems not yet encountered (or at least not yet tackled) in English, it was fairly straightforward to work out suitable MRS representations and means of buildin</context>
</contexts>
<marker>Kuno, 1973</marker>
<rawString>Kuno, Susumu (1973). The Structure of the Japanese Language. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshiko Matsumoto</author>
</authors>
<title>Noun-Modifying Constructions in Japanese: A Frame Semantic Approach.</title>
<date>1997</date>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="18432" citStr="Matsumoto 1997" startWordPosition="2853" endWordPosition="2854">g numeral classifier phrases to NPs that contribute the appropriate nominal semantics (underspecified in the case of ordinary numeral classifiers or specific in the case of words like en &apos;yen&apos;). 2.3 Relative Clauses and Adjectives The primary issue in the analysis of relative clauses and adjectives is the possibility of extreme ambiguity, due to several intersecting factors: Japanese has rampant pro-drop and does not have any relative pronouns. In addition, a head noun modified by a relative clause need not correspond to any gap in the relative clause, as shown by examples like the following (Matsumoto 1997): Example 7: atama ga yoku naru hon head NOM better become book &apos;a book that makes one smarter&apos; Therefore, if we were to posit an attributive adjective + noun construction (distinct from the relative clause + noun possibility) we would have systematic ambiguities for NPs like akai hon (&apos;red book&apos;), ambiguities which could never be resolved based on information in the sentence. Instead, we have opted for a relative clause analysis of any adjective + noun combination in which the adjective could potentially be used predicatively. Furthermore, because of gapless relative clauses like the one cite</context>
</contexts>
<marker>Matsumoto, 1997</marker>
<rawString>Matsumoto, Yoshiko (1997). Noun-Modifying Constructions in Japanese: A Frame Semantic Approach. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>John Carroll</author>
</authors>
<title>Performance Profiling for Parser Engineering.</title>
<date>2000</date>
<journal>Journal of Natural Language Engineering, Special</journal>
<booktitle>Issue on Efficient Processing with HPSG: Methods, Systems, Evaluation,</booktitle>
<pages>81--97</pages>
<contexts>
<context position="3362" citStr="Oepen &amp; Carroll 2000" startWordPosition="493" endWordPosition="496"> easy to extend. MRS is a flat semantic formalism that works well with typed feature structures and is flexible in that it provides structures that are under-specified for scopal information. These structures give compact representations of ambiguities that are often irrelevant to the task at hand. HPSG and MRS have the further advantage that there are practical and useful open-source tools for writing, testing, and efficiently processing grammars written in these formalisms. The tools we are using in this project include the LKB system (Copestake 2002) for grammar development, [incr tsdb()] (Oepen &amp; Carroll 2000) for testing the grammar and tracking changes, and PET (Callmeier 2000), a very efficient HPSG parser, for processing. We also use the ChaSen tokenizer and POS tagger (Asahara &amp; Matsumoto 2000). While couched within the same general framework (BPSG), our approach differs from that of Kanayama et al (2000). The work described there achieves impressive coverage (83.7% on the EDR corpus of newspaper text) with an underspecified grammar consisting of a small number of lexical entries, lexical types associated with parts of speech, and six underspecified grammar rules. In contrast, our grammar is m</context>
</contexts>
<marker>Oepen, Carroll, 2000</marker>
<rawString>Oepen, Stephan and John Carroll (2000). Performance Profiling for Parser Engineering. Journal of Natural Language Engineering, Special Issue on Efficient Processing with HPSG: Methods, Systems, Evaluation, pages 81-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Kristina Toutanova</author>
<author>Stuart Shieber</author>
<author>Chris Manning</author>
<author>Dan Flickinger</author>
<author>Thorsten Brants</author>
</authors>
<title>The LinGO Redwoods Treebank. Motivation and Preliminary Applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics, Coling</booktitle>
<location>Tapei, Taiwan. .</location>
<contexts>
<context position="26365" citStr="Oepen et al. 2002" startWordPosition="4078" endWordPosition="4081">semantic relationship between modifier and modifiee is dependent on their semantic properties: toukyou no kaigi - &apos;the meeting in Tokyo&apos;, watashi no hon - &apos;my book&apos;. More lexical-semantic information is needed to choose the correct parse in more complex structures, such as in watashi no toukyou no imooto ‚Äì ‚ÄòMy sister in Tokyo‚Äô. Restrictions like head-complement preferred to head-adjunct are quite obvious. Others require domain-specific mechanisms that shall be subject of further work. Stochastic disambiguation methods being developed for the ERG by the Redwoods project at Stanford University (Oepen et al. 2002) should be applicable to this grammar as well. 5 Evaluation The grammar currently covers 93.4% of constructed examples for the banking domain (747 sentences) and 78.2% of realistic email correspondence data (316 sentences), concerning requests for documents. During three months of work, the coverage in the banking domain increased 48.49%. The coverage of the document request data increased 51.43% in the following two weeks. Phenomenon total positive word lexical parser total overall items items string items analyses results coverage # # % √ò √ò # % Total 747 747 101 75.24 6.54 698 93.4 Fig.3 Cov</context>
</contexts>
<marker>Oepen, Toutanova, Shieber, Manning, Flickinger, Brants, 2002</marker>
<rawString>Oepen, Stephan, Kristina Toutanova, Stuart Shieber, Chris Manning, Dan Flickinger and Thorsten Brants (2002). The LinGO Redwoods Treebank. Motivation and Preliminary Applications. In Proceedings of the 19th International Conference on Computational Linguistics, Coling 2002. Tapei, Taiwan. .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="2285" citStr="Pollard &amp; Sag 1994" startWordPosition="330" endWordPosition="333">ent languages. Finally, for use in real-world applications, NLP systems meeting the above desiderata must also be efficient. In this paper, we describe the development of a broad coverage grammar for Japanese that is used in an automatic email response application. The grammar is based on work done in the Verbmobil project (Siegel 2000) on machine translation of spoken dialogues in the domain of travel planning. It has since been greatly extended to accommodate written Japanese and new domains. The grammar is couched in the theoretical framework of Head-Driven Phrase Structure Grammar (HPSG) (Pollard &amp; Sag 1994), with semantic representations in Minimal Recursion Semantics (MRS) (Copestake et al. 2001). HPSG is well suited to the task of multilingual development of broad coverage grammars: It is flexible enough (analyses can be shared across languages but also tailored as necessary), and has a rich theoretical literature from which to draw analyzes and inspiration. The characteristic type hierarchy of HPSG also facilitates the development of grammars that are easy to extend. MRS is a flat semantic formalism that works well with typed feature structures and is flexible in that it provides structures t</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, Carl and Ivan A. Sag (1994). Head-Driven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Siegel</author>
</authors>
<title>The Syntactic Processing of Particles in Japanese Spoken Language.</title>
<date>1999</date>
<booktitle>Proceedings of the 13th Pacific Asia Conference on Language, Information and Computation,</booktitle>
<editor>In: Wang, Jhing-Fa and Wu, Chung-Hsien (eds.):</editor>
<location>Taipei</location>
<contexts>
<context position="11814" citStr="Siegel (1999)" startWordPosition="1830" endWordPosition="1831"> I DAT book ACC katte kure-ta. buy give-past &apos;The teacher bought me a book.&apos; Example 4: Watashi ga sensei ni hon wo I NOM teacher DAT book ACC katte morat-ta. buy get-past &apos;The teacher bought me a book.&apos; 1.4 Particles in a type hierarchy The careful treatment of Japanese particles is essential, because they are the most frequently occurring words and have various central functions in the grammar. It is difficult, because one particle can fulfill more than one function and they can co-occur, but not arbitrarily. The Japanese grammar thus contains a type hierarchy of 44 types for particles. See Siegel (1999) for a more detailed description of relevant phenomena and solutions. 1.5 Numeral Expressions Number names, such as sen kyuu hyaku juu &apos;1910&apos; constitute a notable exception to the general head-final pattern of Japanese phrases. We found Smith&apos;s (1999) head-medial analysis of English number names to be directly applicable to the Japanese system as well (Bender 2002). This analysis was easily incorporated into the grammar, despite the oddity of head positioning, because the type hierarchy of HPSG is well suited to express the partial generalizations that permeate natural language. On the other h</context>
</contexts>
<marker>Siegel, 1999</marker>
<rawString>Siegel, Melanie (1999). The Syntactic Processing of Particles in Japanese Spoken Language. In: Wang, Jhing-Fa and Wu, Chung-Hsien (eds.): Proceedings of the 13th Pacific Asia Conference on Language, Information and Computation, Taipei 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Siegel</author>
</authors>
<title>HPSG Analysis of Japanese. In:</title>
<date>2000</date>
<editor>W. Wahlster (ed.):</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="2004" citStr="Siegel 2000" startWordPosition="289" endWordPosition="290"> entity recognition, non-linguistic structures such as addresses, etc. Furthermore, applications built on deep NLP technology should be extensible to multiple languages. This requires flexible yet well-defined output structures that can be adapted to grammars of many different languages. Finally, for use in real-world applications, NLP systems meeting the above desiderata must also be efficient. In this paper, we describe the development of a broad coverage grammar for Japanese that is used in an automatic email response application. The grammar is based on work done in the Verbmobil project (Siegel 2000) on machine translation of spoken dialogues in the domain of travel planning. It has since been greatly extended to accommodate written Japanese and new domains. The grammar is couched in the theoretical framework of Head-Driven Phrase Structure Grammar (HPSG) (Pollard &amp; Sag 1994), with semantic representations in Minimal Recursion Semantics (MRS) (Copestake et al. 2001). HPSG is well suited to the task of multilingual development of broad coverage grammars: It is flexible enough (analyses can be shared across languages but also tailored as necessary), and has a rich theoretical literature fro</context>
</contexts>
<marker>Siegel, 2000</marker>
<rawString>Siegel, Melanie (2000). HPSG Analysis of Japanese. In: W. Wahlster (ed.): Verbmobil: Foundations of Speech-to-Speech Translation. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey D Smith</author>
</authors>
<title>English number names</title>
<date>1999</date>
<booktitle>Lexical and Constructional Aspects of Linguistic Explanation. Stanford: CSLI.</booktitle>
<pages>145--160</pages>
<editor>in HPSG. In Gert Webelhuth, Andreas Kathol, and Jean-Pierre Koenig (eds.),</editor>
<marker>Smith, 1999</marker>
<rawString>Smith, Jeffrey D. (1999). English number names in HPSG. In Gert Webelhuth, Andreas Kathol, and Jean-Pierre Koenig (eds.), Lexical and Constructional Aspects of Linguistic Explanation. Stanford: CSLI. 145-160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Yamakido</author>
</authors>
<title>Japanese attributive adjectives are not (all) relative clauses.</title>
<date>2000</date>
<booktitle>WCCFL 19: Proceedings of the 19th West Coast Conference on Formal Linguistics.</booktitle>
<pages>588--602</pages>
<editor>In Roger Billerey and Brook Danielle Lillehaugen (eds.),</editor>
<publisher>Cascadilla Press.</publisher>
<location>Somerville, MA:</location>
<contexts>
<context position="19584" citStr="Yamakido 2000" startWordPosition="3033" endWordPosition="3034">hermore, because of gapless relative clauses like the one cited above, we have opted for a non-extraction analysis of relative clauses.2 Nonetheless, the well-formedness constraints on MRS representations require that there be 2 There is in fact some linguistic evidence for extraction in some relative clauses in Japanese (see e.g., Baldwin 2001). However, we saw no practical need to allow for this possibility in our grammar, and particularly not one that would justify the increase in ambiguity. There is also evidence that some adjectives are true attributives and cannot be used predicatively (Yamakido 2000). These are handled by a separate adjective + noun rule restricted to just these cases. some relationship between the head noun and the relative clause. We picked the topic relation for this purpose (following Kuno 1973). The topic relation is introduced into the semantics by the relative clause rule. As with main clause topics (which we also give a non-extraction analysis), we rely on downstream anaphora resolution to refine the relationship. 2.4 Summary For the most part, semantic representations and the syntax-semantic interface already worked out in the ERG were directly applicable to the </context>
</contexts>
<marker>Yamakido, 2000</marker>
<rawString>Yamakido, Hiroko (2000). Japanese attributive adjectives are not (all) relative clauses. In Roger Billerey and Brook Danielle Lillehaugen (eds.), WCCFL 19: Proceedings of the 19th West Coast Conference on Formal Linguistics. Somerville, MA: Cascadilla Press. 588-602.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>