<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.145879">
<title confidence="0.977265">
Criteria for the Manual Grouping of Verb Senses
</title>
<author confidence="0.954866">
Cecily Jill Duffield, Jena D. Hwang, Susan Windisch Brown,
Dmitriy Dligach, Sarah E.Vieweg, Jenny Davis, Martha Palmer
</author>
<affiliation confidence="0.998393">
Departments of Linguistics and Computer Science
University of Colorado
</affiliation>
<address confidence="0.7371095">
Boulder, C0 80039-0295, USA
{cecily.duffield, hwangd, susan.brown, dmitry.dligach,
</address>
<email confidence="0.986122">
sarah.vieweg, jennifer.davis, martha.palmer}@colorado.edu
</email>
<sectionHeader confidence="0.995652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999921818181818">
In this paper, we argue that clustering
WordNet senses into more coarse-grained
groupings results in higher inter-annotator
agreement and increased system
performance. Clustering of verb senses
involves examining syntactic and semantic
features of verbs and arguments on a case-
by-case basis rather than applying a strict
methodology. Determining appropriate
criteria for clustering is based primarily on
the needs of annotators.
</bodyText>
<sectionHeader confidence="0.994007" genericHeader="keywords">
1 Credits
</sectionHeader>
<bodyText confidence="0.940682">
We gratefully acknowledge the support of the
National Science Foundation Grant NSF-0415923,
Word Sense Disambiguation, and the GALE
program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-
0022, a subcontract from the BBN-AGILE Team.
</bodyText>
<sectionHeader confidence="0.998851" genericHeader="introduction">
2 Introduction
</sectionHeader>
<bodyText confidence="0.999797975">
Word sense ambiguity poses significant obstacles
to accurate and efficient information extraction and
automatic translation. Successful disambiguation
of polysemous words in NLP applications depends
on determining an appropriate level of granularity
of sense distinctions, perhaps more so for
distinguishing between multiple senses of verbs
than for any other grammatical category. WordNet,
an important and widely used lexical resource, uses
fine-grained distinctions that provide subtle
information about the particular usages of various
lexical items (Felbaum, 1998). When used as a
resource for annotation of various genres of text,
this fine level of granularity has not been
conducive to high rates of inter-annotator
agreement (ITA) or high automatic tagging
performance. Annotation of verb senses as
described by coarse-grained Proposition Bank
framesets may result in higher ITA scores, but the
blurring of distinctions between verb senses with
similar argument structures may fail to alleviate
the problems posed by ambiguity. Our goal in this
project is to create verb sense distinctions at a
middle level of granularity that allow us to capture
as much information as possible from a lexical
item while still attaining high ITA scores and high
system performance in automatic sense
disambiguation. We have demonstrated that clear
sense distinctions improve annotator productivity
and accuracy. System performance typically lags
around 10% behind ITA rates. ITA scores of at
least 90% for a majority of our sense-groupings
result in the expected corresponding improvement
in system performance. Training on this new data,
Chen et al., (2006) report 86.7% accuracy for verbs
using a smoothed maximum entropy model and
rich linguistic features. (Also Semeval071) They
also report state-of-the-art performance on fine-
grained senses, but the results are more than 16%
lower. We begin by describing the overall process.
</bodyText>
<sectionHeader confidence="0.714175" genericHeader="method">
3 The Grouping and Annotation Process
</sectionHeader>
<bodyText confidence="0.99747">
The process for building our database with the
appropriate level of verb sense distinctions
</bodyText>
<footnote confidence="0.931437">
1 Task 17, http://nlp.cs.swarthmore.edu/semeval/.
</footnote>
<page confidence="0.995495">
49
</page>
<bodyText confidence="0.997810631578948">
Proceedings of the Linguistic Annotation Workshop, pages 49–52,
Prague, June 2007. c�2007 Association for Computational Linguistics
involves two steps: sense grouping and annotation
(Figure 1). During our sense grouping process,
linguists (henceforth, “groupers”) cluster fine-
grained sense distinctions listed in WordNet 2.1
into more coarse-grained groupings. These rough
clusters of WordNet entries are based on speaker
intuition. Other resources, including PropBank,
VerbNet (based on Levin’s verb classes (Levin,
1993)), and online dictionaries are consulted in
further refining the distinctions between senses
(Palmer, et. al., 2005, Kipper et al., 2006). To aid
annotators in understanding the distinctions, sense
groupings are ordered according to saliency and
frequency. Detailed information, including
syntactic frames and semantic features, is provided
as commentary for the groupings. We also provide
the annotators with simple example sentences from
WordNet as well as syntactically complex and
ambiguous attested usages from Google search
results. These examples are intended to guide
annotators faced with similar challenges in the data
to be tagged.
Completed verb sense groupings are sent
through sample-annotation and tagged by two
annotators. Groupings that receive an ITA score of
90% or above are then used to annotate all
instances of that verb in our corpora in actual-
annotation. Groupings that receive less than 90%
ITA scores are regrouped (Hovy et al., 2006).
Revisions are made based on a second grouper’s
evaluation of the original grouping, as well as
patterns of annotator disagreement. Verb groupings
receiving ITA scores of 85% or above are sent
through actual-annotation. Verbs scoring below
85% are regrouped by a third grouper, and in some
cases, by the entire grouping team. It is sometimes
impossible to get ITA scores over 85% for high
frequency verbs that also have high entropy. These
have to be carefully adjudicated to produce a gold
standard. Revised verbs are then evaluated and
either deemed ready for actual-annotation or are
sent for a third and final round of sample-
annotation. Verbs subject to the re-annotation
process are tagged by different annotators. Data
from actual-annotation is examined by an
adjudicator who resolves remaining disagreements
between annotators. The adjudicated data is then
used as the gold standard for automatic annotation.
The final versions of the sense groupings are
mapped to VerbNet and FrameNet and linked to
the Omega Ontology (Philpot et al., 2005).
Verbs are selected based on frequency of
appearance in the WSJ corpus. As the most
frequent verbs are also the most polysemous, the
number of sense distinctions per verb as well as the
number of instances to be tagged decreases as the
project continues. The 740 most frequent verbs in
the WSJ corpus were grouped in order of
frequency. They have an average polysemy of 7
senses in WordNet; our sense groups have reduced
the polysemy to 3.75 senses. Of these, 307 verb
groupings have undergone regrouping to some
extent. A total of 670 verbs have completed actual-
annotation and adjudication. The next 660 verbs
have been divided into rough semantic domains
based on VerbNet classes, and grouping will
proceed according to these semantic domains
rather than by verb frequency. As groupers create
sense groupings for new verbs, old verb sense
groupings in the same semantic domain are
consulted. This organization allows for more
consistent grouping methodologies, as well as
more efficiency in integrating our sense groupings
into the Ontology.
</bodyText>
<figureCaption confidence="0.998941">
Figure 1: The grouping and annotation process.
</figureCaption>
<page confidence="0.979354">
50
</page>
<sectionHeader confidence="0.992302" genericHeader="method">
4 Grouping Methodology
</sectionHeader>
<bodyText confidence="0.999940861111111">
Various criteria are considered when
disambiguating senses and creating sense
groupings for the verbs, including frequent lexical
usages and collocations, syntactic features and
alternations, and semantic features, similarly to
Senseval2 (Palmer, et. al. 2006). Because these
criteria do not apply uniformly to every verb,
groupers take various approaches when creating
sense groupings. Groupers recognize that there are
many alternate ways to cluster senses at this level
of granularity; each grouping represents only one
possible clustering as a middle ground between
PropBank and WordNet senses for each verb. Our
highest priority is to then create clear distinctions
among sense groupings that will be easily
understood by the annotators and consequently
result in high ITA scores. Initial clustering is based
on groupers’ intuitions of the most salient
categories. Many verb groupings, such as that for
the verb kill, provide little detailed syntactic or
semantic analysis and yet have received high ITA
scores. The success of these intuitive sense
groupings is not due to lack of polysemy; kill has
15 WordNet senses and 2 multi-word expressions
clustered into 9 sense groupings, yet it received
94% ITA in first round sample-annotation.
While annotators have little trouble tagging text
with verb senses that fall neatly into intuitive
categories, many verbs have fine-grained WordNet
senses that fall on a continuum between two
distinct lexical usages. In such cases, syntactic and
semantic aspects of the verb and its arguments help
groupers cluster senses in such a way that
annotators can make consistent decisions in
tagging the text.
Syntactic criteria: Annotators have found
syntactic frames, such as those defining VerbNet
classes, to be useful in understanding boundaries
between sense groupings. For example, split was
originally grouped with consideration for the units
resulting from a splitting event (i.e. whether a
whole unit had been split into incomplete portions
of the whole, or into smaller, but complete,
individual units.) This grouping proved difficult
for annotators to distinguish, with an ITA of 42%.
Using the causative/inchoative alternation for
verbs in the “break-45.1” class to regroup resulted
in higher consistency among annotators, increasing
the ITA score to 95%.
Semantic criteria: When senses of a verb have
similar syntactic frames, and usages fall along a
continuum between these senses, semantic features
of the arguments, or less often, of the verb itself,
can clarify these senses and help groupers draw
clear distinctions between them. Argument features
that are considered when creating sense groupings
include [+/-attribute], [+/-patient], and [+/-
locative]. It is most common for groupers to mark
these features on nominal arguments, but a
prepositional phrase may also be described in
semantic terms. Semantic features of the verb that
are considered include aspectual features, as
illustrated by the use of [+/-punctual] in sense
groupings for make (Figure 2). However, it may be
argued that this feature is unnecessary for
annotators to be able to distinguish between the
sense groupings, as the prepositional phrase in
sense 9 is a more salient feature for annotators.
Other features of the verb that were used earlier
in the project include concrete/abstract,
continuative, stative, and others. However, these
features proved less useful than those
</bodyText>
<table confidence="0.721234545454545">
Sense Description and Commentary WordNet 2.1 Examples
group senses
8 Attain or reach something desired make 13, 22, - He made the basketball team.
NP1[+agent] MAKE[+punctual] 38 - We barely made the plane.
NP2[desired goal, destination, state] - I made the opening act in plenty of time.
This sense implies the goal has been met. - Can you believe it? We made it!
Includes: MAKE IT
9 Move toward or away from a location make 30, 37 - As the enemy approached our town, we made for the
NP1[+agent] MAKE[-punctual] make off 1 hills.
(pronoun+way) PP/INFP make way 1 - He made his way carefully across the icy parking lot.
- They made off with the jewels.
</table>
<figureCaption confidence="0.9037675">
Figure 2: Sense groupings 8 and 9 for “make.” Senses are distinguished in part by aspectual features
marked on the verb.
</figureCaption>
<page confidence="0.99573">
51
</page>
<bodyText confidence="0.99996947826087">
described above, and annotators not familiar
with linguistic theory found them to be
confusing. Therefore, they are now rarely used
to label sense groupings. Such concepts, when
used, are more likely to be described in prose
commentary for the sake of the annotators.
Certain compositional features of verbs have
also proven to be confusing for annotators. In
several cases, attempts to distinguish sense
groupings based on manner and path have
resulted in increased annotator disagreement. In
the first attempt at grouping roll, syntactic and
semantic information, as well as prose
commentary, was presented to help annotators
distinguish the manner and path sense
groupings. Despite this, the admissibility of
certain prepositions in both senses (“The baby
rolled over,” vs “She rolled over to the wall,”)
may have blurred the distinction. In two rounds
of sample-annotation, the greatest number of
disagreements occurred with respect to these
two senses for roll, which were then merged in
the final version of the sense groupings.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999975074074074">
Building on results in grouping fine-grained
WordNet senses into more coarse-grained senses
that led to improved inter-annotator agreement
(ITA) and system performance (Palmer et al.,
2004; Palmer et al., 2007), we have developed a
process for rapid sense inventory creation and
annotation of verbs that also provides critical
links between the grouped word senses and the
ontology (Philpot et al., 2005). This process is
based on recognizing that sense distinctions can
be represented by linguists in a hierarchical
structure, that is rooted in very coarse-grained
distinctions which become increasingly fine-
grained until reaching WordNet (or similar)
senses at the leaves. Sets of senses under
specific nodes of the tree are grouped together
into single entries, along with the syntactic and
semantic criteria for their groupings, to be
presented to the annotators. Criteria are applied
on a case-by-case basis, considering syntactic
and semantic features as consistently as possible
when grouping verbs in similar semantic
domains as defined by VerbNet. By using this
approach when creating sense groupings, we are
able to provide annotators with clear and reliable
descriptions of senses, resulting in improved
accuracy and performance.
</bodyText>
<sectionHeader confidence="0.990347" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871575">
Chen, J., A. Schein, L. Ungar and M. Palmer.
2006. An Empirical Study of the Behavior of
Word Sense Disambiguation. Proceedings of
HLT-NAACL 2006. New York, NY.
Fellbaum, C. (ed.) 1998. WordNet: An On-line
Lexical Database and Some of its
Applications. MIT Press, Cambridge, MA.
Kipper, K., A. Korhonen, N. Ryant, and M.
Palmer. 2006. Extensive Classifications of
English Verbs. Proceedings of the 12th
EURALEX International Congress. Turin,
Italy.
Levin, B. 1993. English Verb Classes and
Alternations. The University of Chicago
Press, Chicago, IL.
OntoNotes, 2006. Hovy, E.H., M. Marcus, M.
Palmer, S. Pradhan, L. Ramshaw, and R.
Weischedel. 2006. OntoNotes: The 90%
Solution. Short paper. Proceedings of HLT-
NAACL 2006. New York, NY.
Palmer, M., O. Babko-Malaya, and H.T. Dang.
2004. Different Sense Granularities for
Different Applications. Proceedings of the
2nd Workshop on Scalable Natural Language
Understanding Systems (HLT-NAACL 2004).
Boston, MA.
Palmer, M., Dang, H.T., and Fellbaum, C.,
Making Fine-grained and Coarse-grained
sense distinctions, both manually and
automatically, Journal of Natural Language
Engineering (to appear, 2007).
Palmer, M., Gildea, D., Kingsbury, P., The
Proposition Bank: A Corpus Annotated with
Semantic Roles, Computational Linguistics
Journal, 31:1, 2005.
Philpot, A., E.H. Hovy, and P. Pantel. 2005. The
Omega Ontology. Proceedings of the
ONTOLEX Workshop at the International
Conference on Natural Language Processing
(IJCNLP). Jeju Island, Korea.
</reference>
<page confidence="0.998858">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.314345">
<title confidence="0.998337">Criteria for the Manual Grouping of Verb Senses</title>
<author confidence="0.9764815">Cecily Jill Duffield</author>
<author confidence="0.9764815">Jena D Hwang</author>
<author confidence="0.9764815">Susan Windisch Dmitriy Dligach</author>
<author confidence="0.9764815">Sarah E Vieweg</author>
<author confidence="0.9764815">Jenny Davis</author>
<author confidence="0.9764815">Martha</author>
<affiliation confidence="0.999544">Departments of Linguistics and Computer University of</affiliation>
<address confidence="0.989928">Boulder, C0 80039-0295,</address>
<email confidence="0.9452205">cecily.duffield@colorado.edu</email>
<email confidence="0.9452205">hwangd@colorado.edu</email>
<email confidence="0.9452205">susan.brown@colorado.edu</email>
<email confidence="0.9452205">sarah.vieweg@colorado.edu</email>
<email confidence="0.9452205">jennifer.davis@colorado.edu</email>
<email confidence="0.9452205">martha.palmer@colorado.edu</email>
<abstract confidence="0.998715">In this paper, we argue that clustering WordNet senses into more coarse-grained groupings results in higher inter-annotator agreement and increased system performance. Clustering of verb senses involves examining syntactic and semantic features of verbs and arguments on a caseby-case basis rather than applying a strict methodology. Determining appropriate criteria for clustering is based primarily on the needs of annotators.</abstract>
<note confidence="0.769069142857143">1 Credits We gratefully acknowledge the support of the National Science Foundation Grant NSF-0415923, Word Sense Disambiguation, and the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C- 0022, a subcontract from the BBN-AGILE Team.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>A Schein</author>
<author>L Ungar</author>
<author>M Palmer</author>
</authors>
<title>An Empirical Study of the Behavior of Word Sense Disambiguation.</title>
<date>2006</date>
<booktitle>Proceedings of HLT-NAACL</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="2753" citStr="Chen et al., (2006)" startWordPosition="384" endWordPosition="387">ty. Our goal in this project is to create verb sense distinctions at a middle level of granularity that allow us to capture as much information as possible from a lexical item while still attaining high ITA scores and high system performance in automatic sense disambiguation. We have demonstrated that clear sense distinctions improve annotator productivity and accuracy. System performance typically lags around 10% behind ITA rates. ITA scores of at least 90% for a majority of our sense-groupings result in the expected corresponding improvement in system performance. Training on this new data, Chen et al., (2006) report 86.7% accuracy for verbs using a smoothed maximum entropy model and rich linguistic features. (Also Semeval071) They also report state-of-the-art performance on finegrained senses, but the results are more than 16% lower. We begin by describing the overall process. 3 The Grouping and Annotation Process The process for building our database with the appropriate level of verb sense distinctions 1 Task 17, http://nlp.cs.swarthmore.edu/semeval/. 49 Proceedings of the Linguistic Annotation Workshop, pages 49–52, Prague, June 2007. c�2007 Association for Computational Linguistics involves tw</context>
</contexts>
<marker>Chen, Schein, Ungar, Palmer, 2006</marker>
<rawString>Chen, J., A. Schein, L. Ungar and M. Palmer. 2006. An Empirical Study of the Behavior of Word Sense Disambiguation. Proceedings of HLT-NAACL 2006. New York, NY.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An On-line Lexical Database and Some of its Applications.</title>
<date>1998</date>
<editor>Fellbaum, C. (ed.)</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Fellbaum, C. (ed.) 1998. WordNet: An On-line Lexical Database and Some of its Applications. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>A Korhonen</author>
<author>N Ryant</author>
<author>M Palmer</author>
</authors>
<title>Extensive Classifications of English Verbs.</title>
<date>2006</date>
<booktitle>Proceedings of the 12th EURALEX International Congress.</booktitle>
<location>Turin, Italy.</location>
<contexts>
<context position="3869" citStr="Kipper et al., 2006" startWordPosition="539" endWordPosition="542">n Workshop, pages 49–52, Prague, June 2007. c�2007 Association for Computational Linguistics involves two steps: sense grouping and annotation (Figure 1). During our sense grouping process, linguists (henceforth, “groupers”) cluster finegrained sense distinctions listed in WordNet 2.1 into more coarse-grained groupings. These rough clusters of WordNet entries are based on speaker intuition. Other resources, including PropBank, VerbNet (based on Levin’s verb classes (Levin, 1993)), and online dictionaries are consulted in further refining the distinctions between senses (Palmer, et. al., 2005, Kipper et al., 2006). To aid annotators in understanding the distinctions, sense groupings are ordered according to saliency and frequency. Detailed information, including syntactic frames and semantic features, is provided as commentary for the groupings. We also provide the annotators with simple example sentences from WordNet as well as syntactically complex and ambiguous attested usages from Google search results. These examples are intended to guide annotators faced with similar challenges in the data to be tagged. Completed verb sense groupings are sent through sample-annotation and tagged by two annotators</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2006</marker>
<rawString>Kipper, K., A. Korhonen, N. Ryant, and M. Palmer. 2006. Extensive Classifications of English Verbs. Proceedings of the 12th EURALEX International Congress. Turin, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>The University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="3732" citStr="Levin, 1993" startWordPosition="521" endWordPosition="522">ate level of verb sense distinctions 1 Task 17, http://nlp.cs.swarthmore.edu/semeval/. 49 Proceedings of the Linguistic Annotation Workshop, pages 49–52, Prague, June 2007. c�2007 Association for Computational Linguistics involves two steps: sense grouping and annotation (Figure 1). During our sense grouping process, linguists (henceforth, “groupers”) cluster finegrained sense distinctions listed in WordNet 2.1 into more coarse-grained groupings. These rough clusters of WordNet entries are based on speaker intuition. Other resources, including PropBank, VerbNet (based on Levin’s verb classes (Levin, 1993)), and online dictionaries are consulted in further refining the distinctions between senses (Palmer, et. al., 2005, Kipper et al., 2006). To aid annotators in understanding the distinctions, sense groupings are ordered according to saliency and frequency. Detailed information, including syntactic frames and semantic features, is provided as commentary for the groupings. We also provide the annotators with simple example sentences from WordNet as well as syntactically complex and ambiguous attested usages from Google search results. These examples are intended to guide annotators faced with si</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, B. 1993. English Verb Classes and Alternations. The University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>OntoNotes</author>
</authors>
<title>OntoNotes: The 90% Solution. Short paper.</title>
<date>2006</date>
<booktitle>Proceedings of HLTNAACL</booktitle>
<location>New York, NY.</location>
<marker>OntoNotes, 2006</marker>
<rawString>OntoNotes, 2006. Hovy, E.H., M. Marcus, M. Palmer, S. Pradhan, L. Ramshaw, and R. Weischedel. 2006. OntoNotes: The 90% Solution. Short paper. Proceedings of HLTNAACL 2006. New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>O Babko-Malaya</author>
<author>H T Dang</author>
</authors>
<title>Different Sense Granularities for Different Applications.</title>
<date>2004</date>
<booktitle>Proceedings of the 2nd Workshop on Scalable Natural Language Understanding Systems (HLT-NAACL 2004).</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="12280" citStr="Palmer et al., 2004" startWordPosition="1853" endWordPosition="1856">annotators distinguish the manner and path sense groupings. Despite this, the admissibility of certain prepositions in both senses (“The baby rolled over,” vs “She rolled over to the wall,”) may have blurred the distinction. In two rounds of sample-annotation, the greatest number of disagreements occurred with respect to these two senses for roll, which were then merged in the final version of the sense groupings. 5 Conclusion Building on results in grouping fine-grained WordNet senses into more coarse-grained senses that led to improved inter-annotator agreement (ITA) and system performance (Palmer et al., 2004; Palmer et al., 2007), we have developed a process for rapid sense inventory creation and annotation of verbs that also provides critical links between the grouped word senses and the ontology (Philpot et al., 2005). This process is based on recognizing that sense distinctions can be represented by linguists in a hierarchical structure, that is rooted in very coarse-grained distinctions which become increasingly finegrained until reaching WordNet (or similar) senses at the leaves. Sets of senses under specific nodes of the tree are grouped together into single entries, along with the syntacti</context>
</contexts>
<marker>Palmer, Babko-Malaya, Dang, 2004</marker>
<rawString>Palmer, M., O. Babko-Malaya, and H.T. Dang. 2004. Different Sense Granularities for Different Applications. Proceedings of the 2nd Workshop on Scalable Natural Language Understanding Systems (HLT-NAACL 2004). Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>H T Dang</author>
<author>C Fellbaum</author>
</authors>
<title>Making Fine-grained and Coarse-grained sense distinctions, both manually and automatically,</title>
<date>2007</date>
<journal>Journal of Natural Language Engineering</journal>
<note>to appear,</note>
<contexts>
<context position="12302" citStr="Palmer et al., 2007" startWordPosition="1857" endWordPosition="1860">h the manner and path sense groupings. Despite this, the admissibility of certain prepositions in both senses (“The baby rolled over,” vs “She rolled over to the wall,”) may have blurred the distinction. In two rounds of sample-annotation, the greatest number of disagreements occurred with respect to these two senses for roll, which were then merged in the final version of the sense groupings. 5 Conclusion Building on results in grouping fine-grained WordNet senses into more coarse-grained senses that led to improved inter-annotator agreement (ITA) and system performance (Palmer et al., 2004; Palmer et al., 2007), we have developed a process for rapid sense inventory creation and annotation of verbs that also provides critical links between the grouped word senses and the ontology (Philpot et al., 2005). This process is based on recognizing that sense distinctions can be represented by linguists in a hierarchical structure, that is rooted in very coarse-grained distinctions which become increasingly finegrained until reaching WordNet (or similar) senses at the leaves. Sets of senses under specific nodes of the tree are grouped together into single entries, along with the syntactic and semantic criteri</context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2007</marker>
<rawString>Palmer, M., Dang, H.T., and Fellbaum, C., Making Fine-grained and Coarse-grained sense distinctions, both manually and automatically, Journal of Natural Language Engineering (to appear, 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: A Corpus Annotated with Semantic Roles,</title>
<date>2005</date>
<journal>Computational Linguistics Journal,</journal>
<volume>31</volume>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., Gildea, D., Kingsbury, P., The Proposition Bank: A Corpus Annotated with Semantic Roles, Computational Linguistics Journal, 31:1, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Philpot</author>
<author>E H Hovy</author>
<author>P Pantel</author>
</authors>
<title>The Omega Ontology.</title>
<date>2005</date>
<booktitle>Proceedings of the ONTOLEX Workshop at the International Conference on Natural Language Processing (IJCNLP). Jeju Island,</booktitle>
<contexts>
<context position="5742" citStr="Philpot et al., 2005" startWordPosition="830" endWordPosition="833">tropy. These have to be carefully adjudicated to produce a gold standard. Revised verbs are then evaluated and either deemed ready for actual-annotation or are sent for a third and final round of sampleannotation. Verbs subject to the re-annotation process are tagged by different annotators. Data from actual-annotation is examined by an adjudicator who resolves remaining disagreements between annotators. The adjudicated data is then used as the gold standard for automatic annotation. The final versions of the sense groupings are mapped to VerbNet and FrameNet and linked to the Omega Ontology (Philpot et al., 2005). Verbs are selected based on frequency of appearance in the WSJ corpus. As the most frequent verbs are also the most polysemous, the number of sense distinctions per verb as well as the number of instances to be tagged decreases as the project continues. The 740 most frequent verbs in the WSJ corpus were grouped in order of frequency. They have an average polysemy of 7 senses in WordNet; our sense groups have reduced the polysemy to 3.75 senses. Of these, 307 verb groupings have undergone regrouping to some extent. A total of 670 verbs have completed actualannotation and adjudication. The nex</context>
<context position="12496" citStr="Philpot et al., 2005" startWordPosition="1888" endWordPosition="1891">stinction. In two rounds of sample-annotation, the greatest number of disagreements occurred with respect to these two senses for roll, which were then merged in the final version of the sense groupings. 5 Conclusion Building on results in grouping fine-grained WordNet senses into more coarse-grained senses that led to improved inter-annotator agreement (ITA) and system performance (Palmer et al., 2004; Palmer et al., 2007), we have developed a process for rapid sense inventory creation and annotation of verbs that also provides critical links between the grouped word senses and the ontology (Philpot et al., 2005). This process is based on recognizing that sense distinctions can be represented by linguists in a hierarchical structure, that is rooted in very coarse-grained distinctions which become increasingly finegrained until reaching WordNet (or similar) senses at the leaves. Sets of senses under specific nodes of the tree are grouped together into single entries, along with the syntactic and semantic criteria for their groupings, to be presented to the annotators. Criteria are applied on a case-by-case basis, considering syntactic and semantic features as consistently as possible when grouping verb</context>
</contexts>
<marker>Philpot, Hovy, Pantel, 2005</marker>
<rawString>Philpot, A., E.H. Hovy, and P. Pantel. 2005. The Omega Ontology. Proceedings of the ONTOLEX Workshop at the International Conference on Natural Language Processing (IJCNLP). Jeju Island, Korea.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>