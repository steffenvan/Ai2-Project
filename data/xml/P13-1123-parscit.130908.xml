<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998552">
Conditional Random Fields for Responsive Surface Realisation using
Global Features
</title>
<author confidence="0.998998">
Nina Dethlefs, Helen Hastie, Heriberto Cuay´ahuitl and Oliver Lemon
</author>
<affiliation confidence="0.978911">
Mathematical and Computer Sciences
Heriot-Watt University, Edinburgh
</affiliation>
<email confidence="0.984505">
n.s.dethlefs  |h.hastie  |h.cuayahuitl  |o.lemon@hw.ac.uk
</email>
<sectionHeader confidence="0.998463" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999797272727273">
Surface realisers in spoken dialogue sys-
tems need to be more responsive than con-
ventional surface realisers. They need to
be sensitive to the utterance context as well
as robust to partial or changing generator
inputs. We formulate surface realisation as
a sequence labelling task and combine the
use of conditional random fields (CRFs)
with semantic trees. Due to their extended
notion of context, CRFs are able to take
the global utterance context into account
and are less constrained by local features
than other realisers. This leads to more
natural and less repetitive surface realisa-
tion. It also allows generation from partial
and modified inputs and is therefore ap-
plicable to incremental surface realisation.
Results from a human rating study confirm
that users are sensitive to this extended no-
tion of context and assign ratings that are
significantly higher (up to 14%) than those
for taking only local context into account.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957120000001">
Surface realisation typically aims to produce out-
put that is grammatically well-formed, natural and
cohesive. Cohesion can be characterised by lexical
or syntactic cues such as repetitions, substitutions,
ellipses, or connectives. In automatic language
generation, such properties can sometimes be dif-
ficult to model, because they require rich context-
awareness that keeps track of all (or much) of what
was generated before, i.e. a growing generation
history. In text generation, cohesion can span over
the entire text. In interactive settings such as gen-
eration within a spoken dialogue system (SDS), a
challenge is often to keep track of cohesion over
several utterances. In addition, since interactions
are dynamic, generator inputs from the dialogue
manager can sometimes be partial or subject to
subsequent modification. This has been addressed
by work on incremental processing (Schlangen
and Skantze, 2009). Since dialogue acts are passed
on to the generation module as soon as possible,
this can sometimes lead to incomplete generator
inputs (because the user is still speaking), or in-
puts that are subject to later modification (because
of an initial ASR mis-recognition).
In this paper, we propose to formulate surface
realisation as a sequence labelling task. We use
conditional random fields (Lafferty et al., 2001;
Sutton and McCallum, 2006), which are suitable
for modelling rich contexts, in combination with
semantic trees for rich linguistic information. This
combination is able to keep track of dependen-
cies between syntactic, semantic and lexical fea-
tures across multiple utterances. Our model can
be trained from minimally labelled data, which re-
duces development time and may (in the future)
facilitate an application to new domains.
The domain used in this paper is a pedestrian
walking around a city looking for information and
recommendations for local restaurants from an
SDS. We describe here the module for surface re-
alisation. Our main hypothesis is that the use of
global context in a CRF with semantic trees can
lead to surface realisations that are better phrased,
more natural and less repetitive than taking only
local features into account. Results from a human
rating study confirm this hypothesis. In addition,
we compare our system with alternative surface
realisation methods from the literature, namely, a
rank and boost approach and n-grams.
Finally, we argue that our approach lends itself
</bodyText>
<page confidence="0.944232">
1254
</page>
<note confidence="0.913587">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1254–1263,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999363833333333">
to surface realisation within incremental systems,
because CRFs are able to model context across
full as well as partial generator inputs which may
undergo modifications during generation. As a
demonstration, we apply our model to incremen-
tal surface realisation in a proof-of-concept study.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999978575">
Our approach is most closely related to Lu et
al. (2009) who also use CRFs to find the best
surface realisation from a semantic tree. They
conclude from an automatic evaluation that using
CRF-based generation which takes long-range de-
pendencies into account outperforms several base-
lines. However, Lu et al.’s generator does not take
context beyond the current utterance into account
and is thus restricted to local features. Further-
more, their model is not able to modify generation
results on the fly due to new or updated inputs.
In terms of surface realisation from graphical
models (and within the context of SDSs), our ap-
proach is also related to work by Georgila et al.
(2002) and Dethlefs and Cuay´ahuitl (2011b), who
use HMMs, Dethlefs and Cuay´ahuitl (2011a) who
use Bayes Nets, and Mairesse et al. (2010) who
use Dynamic Bayes Nets within an Active Learn-
ing framework. The last approach is also con-
cerned with generating restaurant recommenda-
tions within an SDS. Specifically, their system op-
timises its performance online, during the interac-
tion, by asking users to provide it with new textual
descriptions of concepts, for which it is unsure of
the best realisation. In contrast to these related
approaches, we use undirected graphical models
which are useful when the natural directionality
between the input variables is unknown.
In terms of surface realisation for SDSs, Oh and
Rudnicky (2000) present foundational work in us-
ing an n-gram-based system. They train a surface
realiser based on a domain-dependent language
model and use an overgeneration and ranking ap-
proach. Candidate utterances are ranked accord-
ing to a penalty function which penalises too long
or short utterances, repetitious utterances and ut-
terances which either contain more or less infor-
mation than required by the dialogue act. While
their approach is fast to execute, it has the dis-
advantage of not being able to model long-range
dependencies. They show that humans rank their
output equivalently to template-based generation.
Further, our approach is related to the SPaRKy
sentence generator (Walker et al., 2007). SPaRKy
was also developed for the domain of restaurant
recommendations and was shown to be equivalent
to or better than a carefully designed template-
based generator which had received high human
ratings in the past (Stent et al., 2002). It generates
sentences in two steps. First, it produces a ran-
domised set of alternative realisations, which are
then ranked according to a mapping from sentence
plans to predicted human ratings using a boosting
algorithm. As in our approach, SPaRKy distin-
guishes local and global features. Local features
take only information of the current tree node into
account, including its parents, siblings and chil-
dren, while global features take information of the
entire utterance into account. While SPaRKy is
shown to reach high output quality in compari-
son to a template-based baseline, the authors ac-
knowledge that generation with SPaRKy is rather
slow when applied in a real-time SDS. This could
present a problem in incremental settings, where
generation speed is of particular importance.
The SPaRKy system is also used by Rieser et
al. (2011), who focus on information presentation
strategies for restaurant recommendations, sum-
maries or comparisons within an SDS. Their sur-
face realiser is informed by the highest ranked
SPaRKy outputs for a particular information pre-
sentation strategy and will constitute one of our
baselines in the evaluation.
More work on trainable realisation for SDSs
generally includes Bulyko and Ostendorf (2002)
who use finite state transducers, Nakatsu and
White (2006) who use supervised learning, Varges
(2006) who uses chart generation, and Konstas
and Lapata (2012) who use weighted hypergraphs,
among others.
</bodyText>
<sectionHeader confidence="0.996341" genericHeader="method">
3 Cohesion across Utterances
</sectionHeader>
<subsectionHeader confidence="0.999925">
3.1 Tree-based Semantic Representations
</subsectionHeader>
<bodyText confidence="0.999966272727273">
The restaurant recommendations we generate can
include any of the attributes shown in Table 1.
It is then the task of the surface realiser to find
the best realisation, including whether to present
them in one or several sentences. This often is
a sentence planning decision, but in our approach
it is handled using CRF-based surface realisation.
The semantic forms underlying surface realisation
can be produced in many ways. In our case, they
are produced by a reinforcement learning agent
which orders semantic attributes in the tree ac-
</bodyText>
<page confidence="0.994942">
1255
</page>
<figureCaption confidence="0.93618875">
Figure 1: Architecture of our SDS with a focus on
the NLG components. While the user is speaking,
the dialogue manager sends dialogue acts to the
NLG module, which uses reinforcement learning
to order semantic attributes and produce a seman-
tic tree (see Dethlefs et al. (2012b)). This paper fo-
cuses on surface realisation from these trees using
a CRF as shown in the surface realisation module.
</figureCaption>
<figure confidence="0.695368454545455">
Slot Example
ADDRESS The venue’s address is ...
AREA It is located in ...
FOOD The restaurant serves ... cuisine.
NAME The restaurant’s name is ...
PHONE The venue’s phone number is ...
POSTCODE The postcode is ...
QUALITY This is a ... venue.
PRICE It is located in the ... price range.
SIGNATURE The venue specialises in ...
VENUE This venue is a ...
</figure>
<tableCaption confidence="0.886903">
Table 1: Semantic slots required for our domain
</tableCaption>
<bodyText confidence="0.969789166666667">
along with example realisations. Attributes can be
combined in all possible ways during generation.
cording to their confidence in the dialogue. This
is because SDSs can often have uncertainties with
regard to the user’s actual desired attribute values
due to speech recognition inaccuracies. We there-
fore model all semantic slots as probability distri-
butions, such as inform(food=Indian, 0.6) or in-
form(food=Italian, 0.4) and apply reinforcement
learning to finding the optimal sequence for pre-
sentation. Please see Dethlefs et al. (2012b) for
details. Here, we simply assume that a semantic
form has been produced by a previous processing
module.
As shown in the architecture diagram in Fig-
ure 1, a CRF surface realiser takes a semantic
tree as input. We represent these as context-free
trees which can be defined formally as 4-tuples
</bodyText>
<figure confidence="0.501026">
Lexical Syntactic Semantic
features features features
</figure>
<figureCaption confidence="0.969797">
Figure 2: (a) Graphical representation of a linear-
</figureCaption>
<bodyText confidence="0.960050958333333">
chain Conditional Random Field (CRF), where
empty nodes correspond to the labelled sequence,
shaded nodes to linguistic observations, and dark
squares to feature functions between states and ob-
servations; (b) Example semantic trees that are up-
dated at each time step in order to provide linguis-
tic features to the CRF (only one possible surface
realisation is shown and parse categories are omit-
ted for brevity); (c) Finite state machine of phrases
(labels) for this example.
{5, T, N, H}, where 5 is a start symbol, typically
the root node of the tree; T = {t0, t1, t2 ... t|T|}
is a set of terminal symbols, corresponding to sin-
gle phrases; N = {n0, n1, n2 ... n|N|} is a set of
non-terminal symbols corresponding to semantic
categories, and H = {h0, h1, h2 ... h|H|} is a set
of production rules of the form n → α, where
n E N, α E T U N. The production rules rep-
resent alternatives at each branching node where
the CRF is consulted for the best available expan-
sion from the subset of possible ones. All nodes
in the tree are annotated with a semantic concept
(obtained from the semantic form) as well as their
parse category.
</bodyText>
<subsectionHeader confidence="0.999699">
3.2 Conditional Random Fields for
Phrase-Based Surface Realisation
</subsectionHeader>
<bodyText confidence="0.9998958">
The main idea of our approach is to treat surface
realisation as a sequence labelling task in which a
sequence of semantic inputs needs to be labelled
with appropriate surface realisations. The task is
therefore to find a mapping between (observed)
</bodyText>
<figure confidence="0.999499631578947">
Interaction
Manager
Micro-turn dialogue
act, inform(food=Thai)
Timing and Ordering
semantics of
user utterance
intervening modules
Semantic tree
speech
String of words
(synthesised)
User
Surface Realisation
The Beluga is a great Italian restaurant
y0 y1 y2
inform(
venue=
Restaurant)
root
inform(
name=
Beluga)
root
root
inform(
type=
Italian)
The Beluga
restaurant
is a great Italian
The
Beluga
restaurant
is a great
Italian
other
phrases
</figure>
<page confidence="0.975393">
1256
</page>
<bodyText confidence="0.996626">
lexical, syntactic and semantic features and a (hid-
den) best surface realisation.
We use the linear-chain Conditional Random
Field (CRF) model for statistical phrase-based sur-
face realisation, see Figure 2 (a). This probabilis-
tic model defines the posterior probability of la-
bels (surface realisation phrases) y={y1, ... , y|y|}
given features x={x1, ... , x|x|} (informed by a se-
mantic tree, see Figure 2 (b)), as
</bodyText>
<equation confidence="0.997800333333333">
1 T K
P(y|x) = Z(x) � exp �BkΦk(yt, yt−1, xt)
t=1 k=1
</equation>
<bodyText confidence="0.689271769230769">
where
is a normalisation factor over all pos-
sible realisations (i.e. labellings) of
such that the
sum of all terms is one. The parameters Bk are
weights corresponding to feature functions
which are real values describing the label state y
at time t based on the previous label state
and
features
For example: from Figure 2 (c),
might have the value
= 1.0 for the transition
</bodyText>
<figure confidence="0.852441761904762">
from
to
a great
and 0.0
elsewhere. The parameters Bk are set to maximise
the conditional likelihood of phrase sequences in
the training data set. They are estimated using the
gradient ascent algorithm.
After training, labels can
Z(x)
x
Φk(.),
yt−1
xt.
Φk
Φk
“TheBeluga”
“is
Italian”,
be predicted for new
sequences of observations. The most likely phrase
</figure>
<figureCaption confidence="0.347098">
sequence is expressed as
</figureCaption>
<bodyText confidence="0.775097111111111">
= arg max
∗
y
We use the Mallet
(McCallum, 2002) for
parameter learning an
package1
d inference.
for each tree node.
</bodyText>
<footnote confidence="0.949641">
1http://mallet.cs.umass.edu/
2http://nlp.stanford.edu/software/
lex-parser.shtml
</footnote>
<bodyText confidence="0.8537375">
y
which is computed using the Viterbi algorithm.
</bodyText>
<subsectionHeader confidence="0.999868">
3.3 Feature Selection and Training
</subsectionHeader>
<bodyText confidence="0.9996364">
The following features define the generation con-
text used during training of the CRF. The genera-
tion context includes everything that has been gen-
erated for the current utterance so far. All features
can be obtained from a semantic input tree.
</bodyText>
<listItem confidence="0.999244">
• Lexical items of parents and siblings,
• Semantic types in expansion,
• Semantic types of parents and siblings,
• Parse category of expansion,
• Parse categories of parents and siblings.
</listItem>
<bodyText confidence="0.986100410714286">
We use the StanfordParser2 (Marneffe et al., 2006)
to obtain the parse category
The semantics for each node are derived from the
input dialogue acts (these are listed in Table 1) and
are associated with nodes. The lexical items are
present in the generation context and are mapped
to semantic tree nodes.
As an example, for generating an utterance (la-
bel sequence) such as The Beluga is a great restau-
rant. It is located in the city centre., each gen-
eration step needs to take the features of the en-
tire generation history into account. This includes
all individual lexical items generated, the seman-
tic types used and the parse categories for each
tree node involved. For the first constituent, The
Beluga, this corresponds to the features
BE-
GIN NAME} indicating the beginning of a sentence
(where empty features are omitted), the beginning
of a new generation context and the next semantic
slot required. For the second constituent, is a great
restaurant, the features are {THE BELUGA NAME
NP VENUE}, i.e. including the generation history
(with lexical items and parse category added for
the first constituent) and the semantics of the next
required slot, VENUE. In this way, a sequence of
surface form constituents is generated correspond-
ing to latent states in the CRF.
Since global utterance features capture the full
generation context (i.e. beyond the current ut-
terance), we are also able to model phenomena
such as co-references and pronouns. This is useful
for longer restaurant recommendations which may
span over more than one utterance. If the genera-
tion history already contains a semantic attribute,
e.g. the restaurant name, the CRF may afterwards
choose a pronoun, e.g. it, which has a higher like-
lihood than using the proper name again. Simi-
larly, the CRF may decide to realise a new attribute
as constituents of different order, such as a sen-
tence or PP, depending on the length, number and
parse categories of previously generated output. In
this way, our approach implicitly treats sentence
planning decisions such as the distribution of con-
tent over a set of messages in the same way as (or
as part of) surface realisation. A further capabil-
ity of our surface realiser is that it can generate
complete phrases from full as well as partial dia-
logue acts. This is useful in interactive contexts,
where we need as much robustness as possible. A
demonstration of this is given in Section 5 in an
application to incremental surface realisation.
To train the CRF, we used a data set of 552
restaurant recommendations fr
{ˆ
om the website The
</bodyText>
<equation confidence="0.486988">
P(y|x),
</equation>
<page confidence="0.886053">
1257
</page>
<bodyText confidence="0.99291575">
List.3 The data contains recommendations such as
Located in the city centre, Beluga is a stylish yet
laid-back restaurant with a smart menu of modern
European cuisine.
</bodyText>
<subsectionHeader confidence="0.873101">
3.4 Grammar Induction
</subsectionHeader>
<bodyText confidence="0.999996857142857">
The grammar g of surface realisation candidates
is obtained through an automatic grammar induc-
tion algorithm which can be run on unlabelled
data and requires only minimal human interven-
tion. This grammar defines the surface realisa-
tion space for the CRFs. We provide the human
corpus of restaurant recommendations from Sec-
tion 3.3 as input to grammar induction. The al-
gorithm is shown in Algorithm 1. It first identi-
fies all semantic attributes of interest in an utter-
ance, in our case those specified in Table 1, and re-
places them by a variable. These attributes include
food types, such as Mexican, Chinese, particular
parts of town, prices, etc. About 45% of them can
be identified based on heuristics. The remainder
needs to be hand-annotated at the moment, which
includes mainly attributes like restaurant names or
quality attributes, such as delicate, exquisite, etc.
Subsequently, all utterances are parsed using the
Stanford parser to obtain constituents and are inte-
grated into the grammar under construction. The
non-terminal symbols are named after the auto-
matically annotated semantic attributes contained
in their expansion, e.g. NAME QUALITY → The
$name$ is of $quality$ quality. In this way, each
non-terminal symbol has a semantic representa-
tion and an associated parse category. In total, our
induced grammar contains more than 800 rules.
</bodyText>
<sectionHeader confidence="0.99944" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999988384615385">
To evaluate our approach, we focus on a sub-
jective human rating study which aims to deter-
mine whether CRF-based surface realisation that
takes the full generation context into account,
called CRF (global), is perceived better by human
judges than one that uses a CRF but just takes local
context into account, called CRF (local). While
CRF (global) uses features from the entire genera-
tion history, CRF (local) uses only features from
the current tree branch. We assume that cohe-
sion can be identified by untrained judges as natu-
ral, well-phrased and non-repetitive surface forms.
To examine differences in methodology between
</bodyText>
<footnote confidence="0.934551">
3http://www.list.co.uk
</footnote>
<construct confidence="0.486753">
Algorithm 1 Grammar Induction.
</construct>
<listItem confidence="0.847093514285714">
1: function FINDGRAMMAR(utterances u, semantic at-
tributes a) return grammar
2: for each utterance u do
3: if u contains a semantic attribute from a, such as
venue, cuisine, etc. then
4: Find and replace the attribute by its semantic
variable, e.g. $venue$.
5: end if
6: Parse the sentence and induce a set of rules α →
0, where α is a semantic variable and 0 is its parse.
7: Traverse the parse tree in a top-down, depth-first
search and
8: if expansion 0 exists then
9: continue
10: else if non-terminal α exists then
11: add new expansion 0 to α.
12: else write new rule α → 0.
13: end if
14: Write grammar.
15: end for
16: end function
CRFs and other state-of-the-art methods, we also
compare our system to two other baselines:
• CLASSiC corresponds to the system re-
ported in Rieser et al. (2011),4 which gen-
erates restaurant recommendations based on
the SPaRKy system (Walker et al., 2007), and
has received high ratings in the past. SPaRKy
uses global utterance features.
• n-grams represents a simple 5-gram baseline
that is similar to Oh and Rudnicky (2000)’s
system. We will sample from the most likely
slot realisations that do not contain a repeti-
tion and include exactly the required slot val-
ues. Local context only is taken into account.
</listItem>
<subsectionHeader confidence="0.99599">
4.1 Human Rating Study
</subsectionHeader>
<bodyText confidence="0.999976923076923">
We carried out a user rating study on the Crowd-
Flower crowd sourcing platform.5 Each partici-
pant was shown part of a real human-system dia-
logue that emerged as part of the CLASSiC project
evaluation (Rieser et al., 2011). All dialogues
and data are freely available from http://www.
classic-project.org. Each dialogue contained
two variations for one of the utterances. These
variations were generated from two out of the four
systems described above. The order that these
were presented to the participant was counterbal-
anced. Table 2 gives an example of a dialogue seg-
ment presented to the participants.
</bodyText>
<footnote confidence="0.985357">
4In Rieser et al. (2011), this system is referred to as the
TIP system, which generates summaries, comparisons or rec-
ommendations for restaurants. For the present study, we com-
</footnote>
<page confidence="0.995652">
1258
</page>
<bodyText confidence="0.964797">
SYS Thank you for calling the Cambridge Information
system. Your call will be recorded for research pur-
poses. You may ask for information about a place
to eat, such as a restaurant, a pub, or a cafe. How
may I help you?
USR I want to find an American restaurant which is in
the very expensive area.
</bodyText>
<figure confidence="0.889798090909091">
SYS The restaurant Gourmet Burger is an outstanding,
A expensive restaurant located in the central area.
SYS Gourmet Burger is a smart and welcoming restau-
B rant. Gourmet Burger provides an expensive dining
experience with great food and friendly service. If
you’re looking for a central meal at an expensive
price.
USR What is the address and phone number?
SYS Gourmet Burger is on Regent Street and its phone
number is 01223 312598.
USR Thank you. Good bye.
</figure>
<tableCaption confidence="0.817662">
Table 2: Example dialogue for participants to
compare alternative outputs in italics, USR=user,
SYS A=CRF (global), SYS B=CRF(local).
</tableCaption>
<table confidence="0.9998052">
System Natural Phrasing Repetit.
CRF global 3.65 3.64 3.65
CRF local 3.10* 3.19* 3.13*
CLASSiC 3.53* 3.59 3.48*
n-grams 3.01* 3.09* 3.32*
</table>
<tableCaption confidence="0.8567615">
Table 3: Subjective user ratings. Significance with
CRF (global) at p&lt;0.05 is indicated as *.
</tableCaption>
<bodyText confidence="0.999727">
44 participants gave a total of 1,830 ratings of
utterances produced across the four systems. Flu-
ent speakers of English only were requested and
the participants were from the United States. They
were asked to rate each utterance on a 5 point Lik-
ert scale in response to the following questions
(where 5 corresponds to totally agree and 1 cor-
responds to totally disagree):
</bodyText>
<listItem confidence="0.90765125">
• The utterance was natural, i.e. it could have
been produced by a human. (Natural)
• The utterance was phrased well. (Phrasing)
• The utterance was repetitive. (Repetitive)
</listItem>
<sectionHeader confidence="0.660126" genericHeader="method">
4.2 Results
</sectionHeader>
<bodyText confidence="0.9998136">
We can see from Table 3 that across all the cate-
gories, the CRF (global) gets the highest overall
ratings. This difference is significant for all cat-
egories compared with CRF (local) and n-grams
(using a 1-sided Mann Whitney U-test, p &lt; 0.001).
</bodyText>
<footnote confidence="0.9099435">
pare only with the subset of recommendations.
5http://www.crowdflower.com
</footnote>
<bodyText confidence="0.99994654">
Possibly this is because the local context taken
into account by both systems was not enough to
ensure cohesion across surface phrases. It is not
possible, e.g., to cover co-references within a lo-
cal context only or discourse markers that refer be-
yond the current utterance. This can lead to short
and repetitive phrases, such as Make your way to
Gourmet Burger. The food quality is outstanding.
The prices are expensive. generated by the n-gram
baseline.
The CLASSiC baseline, based on SPaRKy, was
the most competitive system in our comparison.
None-the-less CRF (global) is rated higher across
categories and significantly so for Natural (p &lt;
0.05) and Repetitive (p &lt; 0.005). For Phrasing,
there is a trend but not a significant difference (p
&lt; 0.16). All comparisons are based on a 1-sided
Mann Whitney U-test. A qualitative comparison
between the CRF (global) and CLASSiC outputs
showed the following. CLASSiC utterances tend
to be longer and contain more sentences than CRF
(global) utterances. While CRF (global) often de-
cides to aggregate attributes into one sentence,
such as the Beluga is an outstanding restaurant
in the city centre, CLASSiC tends to rely more on
individual messages, such as The Beluga is an out-
standing restaurant. It is located in the city cen-
tre. A possible reason is that while CRF (global)
is able to take features beyond an utterance into
account, CLASSiC/SPaRKy is restricted to global
features of the current utterance.
We can further compare our results with Rieser
et al. (2011) and Mairesse et al. (2010) who also
generate restaurant recommendations and asked
similar questions to participants as we did. Rieser
et al. (2011)’s system received an average rating
of 3.586 in terms of Phrasing which compares to
our 3.64. This difference is not significant, and
in line with the user ratings we observed for the
CLASSiC system above (3.59). Mairesse et al.
(2010) achieved an average score of 4.05 in terms
of Natural in comparison to our 3.65. This differ-
ence is significant at p&lt;0.05. Possibly their better
performance is due to the data set being more “in
domain” than ours. They collected data from hu-
mans that was written specifically for the task that
the system was tested on. In contrast, our system
was trained on freely available data that was writ-
ten by professional restaurant reviewers. Unfortu-
nately, we cannot compare across other categories,
</bodyText>
<footnote confidence="0.934411">
6This was rescaled from a 1-6 scale.
</footnote>
<page confidence="0.983803">
1259
</page>
<table confidence="0.8041624">
USR1 I’m looking for a nice restaurant in the centre.
SYS1 inform(area=centre [0.21, food=Thai [0.31)
inform(name=Bangkok [0.31)
So you’re looking for a Thai ...
USR2 [barges in1 No, I’m looking for a restaurant
with good quality food.
SYS2 inform(quality=good [0.61, name=Beluga [0.61)
Oh sorry, so a nice restaurant located ...
USR3 [barges in1 ... in the city centre.
SYS3 inform(area=centre [0.81)
</table>
<tableCaption confidence="0.609417833333333">
Table 4: Example dialogue where the dialogue
manager needs to send incremental updates to the
NLG. Incremental surface realisation from seman-
tic trees for this dialogue is shown in Figure 3.
because the authors tested only for Phrasing and
Natural, respectively.
</tableCaption>
<sectionHeader confidence="0.999266" genericHeader="method">
5 Incremental Surface Realisation
</sectionHeader>
<bodyText confidence="0.997626448275862">
Recent years have seen increased interest in
incremental dialogue processing (Skantze and
Schlangen, 2009; Schlangen and Skantze, 2009).
The main characteristic of incremental architec-
tures is that instead of waiting for the end of a user
turn, they begin to process the input stream as soon
as possible, updating their processing hypotheses
as more information becomes available. From a
dialogue perspective, they can be said to work on
partial rather than full dialogue acts.
With respect to surface realisation, incremen-
tal NLG systems have predominantly relied on
pre-defined templates (Purver and Otsuka, 2003;
Skantze and Hjalmarsson, 2010; Dethlefs et al.,
2012a), which limits the flexibility and quality of
output generation. Buschmeier et al. (2012) have
presented a system which systematically takes
the user’s acoustic understanding problems into
account by pausing, repeating or re-phrasing if
necessary. Their approach is based on SPUD
(Stone et al., 2003), a constraint satisfaction-based
NLG architecture and marks important progress
towards more flexible incremental surface realisa-
tion. However, given the human labour involved in
constraint specification, cohesion is often limited
to a local context. Especially for long utterances
or such that are separated by user turns, this may
lead to surface form increments that are not well
connected and lack cohesion.
</bodyText>
<subsectionHeader confidence="0.998794">
5.1 Application to Incremental SR
</subsectionHeader>
<bodyText confidence="0.989589078431373">
This section will discuss a proof-of-concept appli-
cation of our approach to incremental surface re-
alisation. Table 4 shows an example dialogue be-
tween a user and system that contains a number
of incremental phenomena that require hypothe-
sis updates, system corrections and user barge-
ins. Incremental surface realisation for this dia-
logue is shown in Figure 3, where processing steps
are indicated as bold-face numbers and are trig-
gered by partial dialogue acts that are sent from
the dialogue manager, such as inform(area=centre
[0.21). The numbers in square brackets indicate
the system’s confidence in the attribute-value pair.
Once a dialogue act is observed by the NLG sys-
tem, a reinforcement learning agent determines the
order of attributes and produces a semantic tree, as
described in Section 3.1. Since the semantic forms
are constructed incrementally, new tree nodes can
be attached to and deleted from an existing tree,
depending on what kind of update is required.
In the dialogue in Table 4, the user first asks
for a nice restaurant in the centre. The dialogue
manager constructs a first attribute-value slot, in-
form(area=centre [0.21, ...), and passes it on to
NLG.7 In Figure 3, we can observe the corre-
sponding NLG action, a first tree is created with
just a root node and a node representing the area
slot (step 1). In a second step, the semantically
annotated node gets expanded into a surface form
that is chosen from a set of candidates (shown in
curly brackets). The CRF is responsible for this
last step. Since there is no preceding utterance, the
best surface form is chosen based on the semantics
alone. Active tree nodes, i.e. those currently under
generation, are indicated as asterisks in Figure 3.
Currently inactive nodes are shown as circles.
Step 3 then further expands the current tree
adding a node for the food type and the name of
a restaurant that the dialogue manager had passed.
We see here that attributes can either be primitive
or complex. Primitive attributes contain a single
semantic type, such as area, whereas complex at-
tributes contain multiple types, such as food, name
and need to be decomposed in a later processing
step (see steps 4 and 6). Step 5 again uses the CRF
7Note here that the information passed on to the NLG is
distinct from the dialogue manager’s own actions. In the ex-
ample, the NLG is asked to generate a recommendation, but
the dialogue manager actually decides to clarify the user’s
preferences due to low confidence. This scenario is an exam-
ple of generator inputs that may get revised afterwards.
</bodyText>
<page confidence="0.938309">
1260
</page>
<figure confidence="0.976255344262295">
root
root
* *
(6) inform
(food=Thai)
Bangkok)
(3) inform(food=Thai
name=Bangkok)
*
inform(area=
centre)
inform(food=Thai,
name=Bangkok)
inform(area=
centre)
Right in the city centre,
*
Right in the city centre,
(4) inform(name=
root
*
(1) inform
(area=centre)*
(2) Right in the city centre,
{located in $area$, if
you&apos;re looking to eat
in $area$, in $area$, ...}
(5) Bangkok
{the $name$,
it is called $name$, ...}
inform(quality=nice,
name=Beluga)
root
inform(area=
centre)
Right in the city centre,
(8) inform(name=
Beluga)
(10) inform(quality=
very good)
*
*
*
root
inform(name=
Bangkok)
*
*
inform
(food=Thai)
Right in the city centre,
*
Bangkok
(7) inform(quality=very
good, name=Beluga)
*
inform(area=
centre)
(9) the Beluga (11) is of very good quality.
{$name$, the venue {is a $quality$ venue, if you want $quality$
called $name$, ...} food, $quality$, a $quality$ place ...}
</figure>
<figureCaption confidence="0.99272175">
Figure 3: Example of incremental surface realisation, where each generation step is indicated by a num-
ber. Active generation nodes are shown as asterisks and deletions are shown as crossed out. Lexical and
semantic features are associated with their respective nodes. Syntactic information in the form of parse
categories are also taken into account for surface realisation, but have been omitted in this figure.
</figureCaption>
<bodyText confidence="0.999978333333334">
to obtain the next surface realisation that connects
with the previous one (so that a sequence of real-
isation “labels” appears: Right in the city centre
and Bangkok). It takes the full generation context
into account to ensure a globally optimal choice.
This is important, because the local context would
otherwise be restricted to a partial dialogue act,
which can be much smaller than a full dialogue
act and thus lead to short, repetitive sentences.
The dialogue continues as the system implicitly
confirms the user’s preferred restaurant (SYS1).
At this point, we encounter a user barge-in correct-
ing the desired choice. As a consequence, the dia-
logue manager needs to update its initial hypothe-
ses and communicate this to NLG. Here, the last
three tree nodes need to be deleted from the tree
because the information is no longer valid. This
update and the deletion is shown in step 7. After-
wards, the dialogue continues and NLG involves
mainly expanding the current tree into a full se-
quence of surface realisations for partial dialogue
acts which come together into a full utterance.
This example illustrates three incremental pro-
cessing steps: expansions, updates and deletions.
Expansions are the most frequent operation. They
add new partial dialogue acts to the semantic tree.
They also consult the CRF for the best surface
realisation. Since CRFs are not restricted by the
Markov condition, they are less constrained by lo-
cal context than other models and can take non-
local dependencies into account. For our applica-
tion, the maximal context is 9 semantic attributes
(for a surface form that uses all possible 10 at-
tributes). While their extended context aware-
ness can often make CRFs slow to train, they are
fast at execution and therefore very applicable to
the incremental scenario. For applications involv-
ing longer-spanning alternatives, such as texts or
paragraphs, the context of the CRF would likely
have to be constrained. Updates are triggered by
the hypothesis updates of the dialogue manager.
Whenever a new attribute comes in, it is checked
against the generator’s existing knowledge. If it
is inconsistent with previous knowledge, an up-
date is triggered and often followed by a deletion.
Whenever generated output needs to be modified,
old expansions and surface forms are deleted first,
before new ones can be expanded in their place.
</bodyText>
<subsectionHeader confidence="0.995993">
5.2 Updates and Processing Speed Results
</subsectionHeader>
<bodyText confidence="0.999454">
Since fast responses are crucial in incremental sys-
tems, we measured the average time our system
took for a surface realisation. The time is 100ms
on a MacBook Intel Core 2.6 Duo with 8GB in
</bodyText>
<page confidence="0.967007">
1261
</page>
<bodyText confidence="0.999475769230769">
RAM. This is slightly better than other incremen-
tal systems (Skantze and Schlangen, 2009) and
much faster than state-of-the-art non-incremental
systems such as SPaRKy (Walker et al., 2007).
In addition, we measured the number of neces-
sary generation updates in comparison to a non-
incremental setting. Since updates take effect di-
rectly on partial dialogue acts, rather than the full
generated utterance, we require around 50% less
updates as if generating from scratch for every
changed input hypothesis. A qualitative analysis
of the generated outputs showed that the quality is
comparable to the non-incremental case.
</bodyText>
<sectionHeader confidence="0.998838" genericHeader="conclusions">
6 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.999987361111111">
We have presented a novel technique for surface
realisation that treats generation as a sequence la-
belling task by combining a CRF with tree-based
semantic representations. An essential property
of interactive surface realisers is to keep track of
the utterance context including dependencies be-
tween linguistic features to generate cohesive ut-
terances. We have argued that CRFs are well
suited for this task because they are not restricted
by independence assumptions. In a human rating
study, we confirmed that judges rated our output
as better phrased, more natural and less repetitive
than systems that just take local features into ac-
count. This also holds for a comparison with state-
of-the-art rank and boost or n-gram approaches.
Keeping track of the global context is also impor-
tant for incremental systems since generator inputs
can be incomplete or subject to modification. In a
proof-of-concept study, we have argued that our
approach is applicable to incremental surface real-
isation. This was supported by preliminary results
on the speed, number of updates and quality dur-
ing generation. As future work, we plan to test
our model in a task-based setting using an end-to-
end SDS in an incremental and non-incremental
setting. This study will contain additional evalu-
ation categories, such as the understandability or
informativeness of system utterances. In addition,
we may compare different sequence labelling al-
gorithms for surface realisation (Nguyen and Guo,
2007) or segmented CRFs (Sarawagi and Cohen,
2005) and apply our method to more complex sur-
face realisation domains such as text generation or
summarisation. Finally, we would like to explore
methods for unsupervised data labelling so as to
facilitate portability across domains further.
</bodyText>
<sectionHeader confidence="0.995577" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999935333333334">
The research leading to this work was funded by
the EC FP7 programme FP7/2011-14 under grant
agreement no. 287615 (PARLANCE).
</bodyText>
<sectionHeader confidence="0.986333" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.974080595744681">
Ivan Bulyko and Mari Ostendorf. 2002. Efficient in-
tegrated response generation from multiple targets
using weighted finite state transducers. Computer
Speech and Language, 16:533–550.
Hendrik Buschmeier, Timo Baumann, Benjamin
Dosch, Stefan Kopp, and David Schlangen. 2012.
Incremental Language Generation and Incremental
Speech Synthesis. In Proceedings of the 13th An-
nual SigDial Meeting on Discourse and Dialogue
(SIGdial), Seoul, South Korea.
Nina Dethlefs and Heriberto Cuay´ahuitl. 2011a. Com-
bining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Genera-
tion in Situated Dialogue. In Proceedings ofthe 13th
European Workshop on Natural Language Genera-
tion (ENLG), Nancy, France.
Nina Dethlefs and Heriberto Cuay´ahuitl. 2011b.
Hierarchical Reinforcement Learning and Hidden
Markov Models for Task-Oriented Natural Lan-
guage Generation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), Portland, Oregon, USA.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012a. Optimising Incremental Dialogue
Decisions Using Information Density for Interac-
tive Systems. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-CoNLL), Jeju, South Korea.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012b. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of the International Con-
ference on Natural Language Generation (INLG),
Chicago, Illinois, USA.
Kallirroi Georgila, Nikos Fakotakis, and George
Kokkinakis. 2002. Stochastic Language Modelling
for Recognition and Generation in Dialogue Sys-
tems. TAL (Traitement automatique des langues)
Journal, 43(3):129–154.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text Generation via Discriminative Reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociationfor Computational Linguistics, pages 369–
378, Jeju Island, Korea.
John D. Lafferty, Andrew McCallum, and Fer-
nando C.N. Pereira. 2001. Conditional Random
</reference>
<page confidence="0.831225">
1262
</page>
<reference confidence="0.999861527472527">
Fields: Probabilistic Models for Segmenting and La-
beling Sequence Data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing (ICML), pages 282–289.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009.
Natural Language Generation with Tree Conditional
Random Fields. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), Singapore.
Franc¸ois Mairesse, Filip Jurˇciˇcek, Simon Keizer,
Blaise Thomson, Kai Yu, and Steve Young. 2010.
Phrase-Based Statistical Language Generation Us-
ing Graphical Models and Active Learning. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL), Uppsala,
Sweden.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC),
Genoa, Italy.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Crystal Nakatsu and Michael White. 2006. Learning
to Say It Well: Reranking Realizations by Predicted
Synthesis Quality. In In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (COLING-ACL) 2006, pages 1113–1120,
Sydney, Australia.
Nam Nguyen and Yunsong Guo. 2007. Comparisons
of Sequence Labeling Algorithms and Extensions.
In Proceedings of the International Conference on
Machine Learning (ICML), Corvallis, OR, USA.
Alice Oh and Alexander Rudnicky. 2000. Stochas-
tic Language Generation for Spoken Dialogue Sys-
tems. In Proceedings of the ANLP/NAACL Work-
shop on Conversational Systems, pages 27–32, Seat-
tle, Washington, USA.
Matthew Purver and Masayuki Otsuka. 2003. In-
cremental Generation by Incremental Parsing. In
In Proceedings of the 6th UK Special-Interesting
Group for Computational Linguistics (CLUK) Col-
loquium.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th Euro-
pean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Sunita Sarawagi and William Cohen. 2005. Semi-
Markov Conditional Random Fields for Information
Extraction. Advances in Neural Information Pro-
cessing.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, Athens, Greece.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards Incremental Speech Generation in Dialogue
Systems. In Proceedings of the 11th Annual SigDial
Meeting on Discourse and Dialogue, Tokyo, Japan.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental Dialogue Processing in a Micro-Domain. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, Athens, Greece.
Amanda Stent, Marilyn Walker, Steve Whittaker, and
Preetam Maloor. 2002. User-tailored Generation
for Spoken Dialogue: An Experiment. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing.
Matthew Stone, Christine Doran, Bonnie Webber, To-
nia Bleam, and Martha Palmer. 2003. Microplan-
ning with Communicative Intentions: The SPUD
System. Computational Intelligence, 19:311–381.
Charles Sutton and Andrew McCallum. 2006. Intro-
duction to Conditional Random Fields for Relational
Learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
Sebastian Varges. 2006. Overgeneration and Ranking
for Spoken Dialogue Systems. In Proceedings of the
Fourth International Natural Language Generation
Conference (INLG), Sydney, Australia.
Marilyn Walker, Amanda Stent, Franc¸ois Mairesse,
and Rashmi Prasad. 2007. Individual and Do-
main Adaptation in Sentence Planning for Dia-
logue. Journal of Artificial Intelligence Research,
30(1):413–456.
</reference>
<page confidence="0.901794">
1263
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.564857">
<title confidence="0.9990185">Conditional Random Fields for Responsive Surface Realisation Global Features</title>
<author confidence="0.990286">Nina Dethlefs</author>
<author confidence="0.990286">Helen Hastie</author>
<author confidence="0.990286">Heriberto Cuay´ahuitl</author>
<author confidence="0.990286">Oliver</author>
<affiliation confidence="0.8047855">Mathematical and Computer Heriot-Watt University,</affiliation>
<email confidence="0.950551">n.s.dethlefs|h.hastie|h.cuayahuitl|o.lemon@hw.ac.uk</email>
<abstract confidence="0.998973956521739">Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields (CRFs) with semantic trees. Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. This leads to more natural and less repetitive surface realisation. It also allows generation from partial and modified inputs and is therefore applicable to incremental surface realisation. Results from a human rating study confirm that users are sensitive to this extended notion of context and assign ratings that are higher (up to than those for taking only local context into account.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ivan Bulyko</author>
<author>Mari Ostendorf</author>
</authors>
<title>Efficient integrated response generation from multiple targets using weighted finite state transducers. Computer Speech and Language,</title>
<date>2002</date>
<pages>16--533</pages>
<contexts>
<context position="7754" citStr="Bulyko and Ostendorf (2002)" startWordPosition="1198" endWordPosition="1201">generation with SPaRKy is rather slow when applied in a real-time SDS. This could present a problem in incremental settings, where generation speed is of particular importance. The SPaRKy system is also used by Rieser et al. (2011), who focus on information presentation strategies for restaurant recommendations, summaries or comparisons within an SDS. Their surface realiser is informed by the highest ranked SPaRKy outputs for a particular information presentation strategy and will constitute one of our baselines in the evaluation. More work on trainable realisation for SDSs generally includes Bulyko and Ostendorf (2002) who use finite state transducers, Nakatsu and White (2006) who use supervised learning, Varges (2006) who uses chart generation, and Konstas and Lapata (2012) who use weighted hypergraphs, among others. 3 Cohesion across Utterances 3.1 Tree-based Semantic Representations The restaurant recommendations we generate can include any of the attributes shown in Table 1. It is then the task of the surface realiser to find the best realisation, including whether to present them in one or several sentences. This often is a sentence planning decision, but in our approach it is handled using CRF-based s</context>
</contexts>
<marker>Bulyko, Ostendorf, 2002</marker>
<rawString>Ivan Bulyko and Mari Ostendorf. 2002. Efficient integrated response generation from multiple targets using weighted finite state transducers. Computer Speech and Language, 16:533–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendrik Buschmeier</author>
<author>Timo Baumann</author>
<author>Benjamin Dosch</author>
<author>Stefan Kopp</author>
<author>David Schlangen</author>
</authors>
<title>Incremental Language Generation and Incremental Speech Synthesis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual SigDial Meeting on Discourse and Dialogue (SIGdial),</booktitle>
<location>Seoul, South</location>
<contexts>
<context position="26906" citStr="Buschmeier et al. (2012)" startWordPosition="4349" endWordPosition="4352">. The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and quality of output generation. Buschmeier et al. (2012) have presented a system which systematically takes the user’s acoustic understanding problems into account by pausing, repeating or re-phrasing if necessary. Their approach is based on SPUD (Stone et al., 2003), a constraint satisfaction-based NLG architecture and marks important progress towards more flexible incremental surface realisation. However, given the human labour involved in constraint specification, cohesion is often limited to a local context. Especially for long utterances or such that are separated by user turns, this may lead to surface form increments that are not well connec</context>
</contexts>
<marker>Buschmeier, Baumann, Dosch, Kopp, Schlangen, 2012</marker>
<rawString>Hendrik Buschmeier, Timo Baumann, Benjamin Dosch, Stefan Kopp, and David Schlangen. 2012. Incremental Language Generation and Incremental Speech Synthesis. In Proceedings of the 13th Annual SigDial Meeting on Discourse and Dialogue (SIGdial), Seoul, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Dethlefs</author>
<author>Heriberto Cuay´ahuitl</author>
</authors>
<title>Combining Hierarchical Reinforcement Learning and Bayesian Networks for Natural Language Generation in Situated Dialogue.</title>
<date>2011</date>
<booktitle>In Proceedings ofthe 13th European Workshop on Natural Language Generation (ENLG),</booktitle>
<location>Nancy, France.</location>
<marker>Dethlefs, Cuay´ahuitl, 2011</marker>
<rawString>Nina Dethlefs and Heriberto Cuay´ahuitl. 2011a. Combining Hierarchical Reinforcement Learning and Bayesian Networks for Natural Language Generation in Situated Dialogue. In Proceedings ofthe 13th European Workshop on Natural Language Generation (ENLG), Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Dethlefs</author>
<author>Heriberto Cuay´ahuitl</author>
</authors>
<title>Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT),</booktitle>
<location>Portland, Oregon, USA.</location>
<marker>Dethlefs, Cuay´ahuitl, 2011</marker>
<rawString>Nina Dethlefs and Heriberto Cuay´ahuitl. 2011b. Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT), Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Dethlefs</author>
<author>Helen Hastie</author>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-CoNLL),</booktitle>
<location>Jeju, South</location>
<contexts>
<context position="8846" citStr="Dethlefs et al. (2012" startWordPosition="1372" endWordPosition="1375">them in one or several sentences. This often is a sentence planning decision, but in our approach it is handled using CRF-based surface realisation. The semantic forms underlying surface realisation can be produced in many ways. In our case, they are produced by a reinforcement learning agent which orders semantic attributes in the tree ac1255 Figure 1: Architecture of our SDS with a focus on the NLG components. While the user is speaking, the dialogue manager sends dialogue acts to the NLG module, which uses reinforcement learning to order semantic attributes and produce a semantic tree (see Dethlefs et al. (2012b)). This paper focuses on surface realisation from these trees using a CRF as shown in the surface realisation module. Slot Example ADDRESS The venue’s address is ... AREA It is located in ... FOOD The restaurant serves ... cuisine. NAME The restaurant’s name is ... PHONE The venue’s phone number is ... POSTCODE The postcode is ... QUALITY This is a ... venue. PRICE It is located in the ... price range. SIGNATURE The venue specialises in ... VENUE This venue is a ... Table 1: Semantic slots required for our domain along with example realisations. Attributes can be combined in all possible way</context>
<context position="26815" citStr="Dethlefs et al., 2012" startWordPosition="4336" endWordPosition="4339">cremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and quality of output generation. Buschmeier et al. (2012) have presented a system which systematically takes the user’s acoustic understanding problems into account by pausing, repeating or re-phrasing if necessary. Their approach is based on SPUD (Stone et al., 2003), a constraint satisfaction-based NLG architecture and marks important progress towards more flexible incremental surface realisation. However, given the human labour involved in constraint specification, cohesion is often limited to a local context. Especially for long utterances or such that are</context>
</contexts>
<marker>Dethlefs, Hastie, Rieser, Lemon, 2012</marker>
<rawString>Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver Lemon. 2012a. Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-CoNLL), Jeju, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Dethlefs</author>
<author>Helen Hastie</author>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Optimising Incremental Generation for Spoken Dialogue Systems: Reducing the Need for Fillers.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Natural Language Generation (INLG),</booktitle>
<location>Chicago, Illinois, USA.</location>
<contexts>
<context position="8846" citStr="Dethlefs et al. (2012" startWordPosition="1372" endWordPosition="1375">them in one or several sentences. This often is a sentence planning decision, but in our approach it is handled using CRF-based surface realisation. The semantic forms underlying surface realisation can be produced in many ways. In our case, they are produced by a reinforcement learning agent which orders semantic attributes in the tree ac1255 Figure 1: Architecture of our SDS with a focus on the NLG components. While the user is speaking, the dialogue manager sends dialogue acts to the NLG module, which uses reinforcement learning to order semantic attributes and produce a semantic tree (see Dethlefs et al. (2012b)). This paper focuses on surface realisation from these trees using a CRF as shown in the surface realisation module. Slot Example ADDRESS The venue’s address is ... AREA It is located in ... FOOD The restaurant serves ... cuisine. NAME The restaurant’s name is ... PHONE The venue’s phone number is ... POSTCODE The postcode is ... QUALITY This is a ... venue. PRICE It is located in the ... price range. SIGNATURE The venue specialises in ... VENUE This venue is a ... Table 1: Semantic slots required for our domain along with example realisations. Attributes can be combined in all possible way</context>
<context position="26815" citStr="Dethlefs et al., 2012" startWordPosition="4336" endWordPosition="4339">cremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and quality of output generation. Buschmeier et al. (2012) have presented a system which systematically takes the user’s acoustic understanding problems into account by pausing, repeating or re-phrasing if necessary. Their approach is based on SPUD (Stone et al., 2003), a constraint satisfaction-based NLG architecture and marks important progress towards more flexible incremental surface realisation. However, given the human labour involved in constraint specification, cohesion is often limited to a local context. Especially for long utterances or such that are</context>
</contexts>
<marker>Dethlefs, Hastie, Rieser, Lemon, 2012</marker>
<rawString>Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver Lemon. 2012b. Optimising Incremental Generation for Spoken Dialogue Systems: Reducing the Need for Fillers. In Proceedings of the International Conference on Natural Language Generation (INLG), Chicago, Illinois, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>Nikos Fakotakis</author>
<author>George Kokkinakis</author>
</authors>
<title>Stochastic Language Modelling for Recognition and Generation in Dialogue Systems. TAL (Traitement automatique des langues)</title>
<date>2002</date>
<journal>Journal,</journal>
<volume>43</volume>
<issue>3</issue>
<contexts>
<context position="4852" citStr="Georgila et al. (2002)" startWordPosition="742" endWordPosition="745">also use CRFs to find the best surface realisation from a semantic tree. They conclude from an automatic evaluation that using CRF-based generation which takes long-range dependencies into account outperforms several baselines. However, Lu et al.’s generator does not take context beyond the current utterance into account and is thus restricted to local features. Furthermore, their model is not able to modify generation results on the fly due to new or updated inputs. In terms of surface realisation from graphical models (and within the context of SDSs), our approach is also related to work by Georgila et al. (2002) and Dethlefs and Cuay´ahuitl (2011b), who use HMMs, Dethlefs and Cuay´ahuitl (2011a) who use Bayes Nets, and Mairesse et al. (2010) who use Dynamic Bayes Nets within an Active Learning framework. The last approach is also concerned with generating restaurant recommendations within an SDS. Specifically, their system optimises its performance online, during the interaction, by asking users to provide it with new textual descriptions of concepts, for which it is unsure of the best realisation. In contrast to these related approaches, we use undirected graphical models which are useful when the n</context>
</contexts>
<marker>Georgila, Fakotakis, Kokkinakis, 2002</marker>
<rawString>Kallirroi Georgila, Nikos Fakotakis, and George Kokkinakis. 2002. Stochastic Language Modelling for Recognition and Generation in Dialogue Systems. TAL (Traitement automatique des langues) Journal, 43(3):129–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Konstas</author>
<author>Mirella Lapata</author>
</authors>
<title>Conceptto-text Generation via Discriminative Reranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics,</booktitle>
<pages>369--378</pages>
<location>Jeju Island,</location>
<contexts>
<context position="7913" citStr="Konstas and Lapata (2012)" startWordPosition="1222" endWordPosition="1225">lar importance. The SPaRKy system is also used by Rieser et al. (2011), who focus on information presentation strategies for restaurant recommendations, summaries or comparisons within an SDS. Their surface realiser is informed by the highest ranked SPaRKy outputs for a particular information presentation strategy and will constitute one of our baselines in the evaluation. More work on trainable realisation for SDSs generally includes Bulyko and Ostendorf (2002) who use finite state transducers, Nakatsu and White (2006) who use supervised learning, Varges (2006) who uses chart generation, and Konstas and Lapata (2012) who use weighted hypergraphs, among others. 3 Cohesion across Utterances 3.1 Tree-based Semantic Representations The restaurant recommendations we generate can include any of the attributes shown in Table 1. It is then the task of the surface realiser to find the best realisation, including whether to present them in one or several sentences. This often is a sentence planning decision, but in our approach it is handled using CRF-based surface realisation. The semantic forms underlying surface realisation can be produced in many ways. In our case, they are produced by a reinforcement learning </context>
</contexts>
<marker>Konstas, Lapata, 2012</marker>
<rawString>Ioannis Konstas and Mirella Lapata. 2012. Conceptto-text Generation via Discriminative Reranking. In Proceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics, pages 369– 378, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning (ICML),</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2562" citStr="Lafferty et al., 2001" startWordPosition="382" endWordPosition="385"> are dynamic, generator inputs from the dialogue manager can sometimes be partial or subject to subsequent modification. This has been addressed by work on incremental processing (Schlangen and Skantze, 2009). Since dialogue acts are passed on to the generation module as soon as possible, this can sometimes lead to incomplete generator inputs (because the user is still speaking), or inputs that are subject to later modification (because of an initial ASR mis-recognition). In this paper, we propose to formulate surface realisation as a sequence labelling task. We use conditional random fields (Lafferty et al., 2001; Sutton and McCallum, 2006), which are suitable for modelling rich contexts, in combination with semantic trees for rich linguistic information. This combination is able to keep track of dependencies between syntactic, semantic and lexical features across multiple utterances. Our model can be trained from minimally labelled data, which reduces development time and may (in the future) facilitate an application to new domains. The domain used in this paper is a pedestrian walking around a city looking for information and recommendations for local restaurants from an SDS. We describe here the mo</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C.N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning (ICML), pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
</authors>
<title>Natural Language Generation with Tree Conditional Random Fields.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="4225" citStr="Lu et al. (2009)" startWordPosition="638" endWordPosition="641"> n-grams. Finally, we argue that our approach lends itself 1254 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1254–1263, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics to surface realisation within incremental systems, because CRFs are able to model context across full as well as partial generator inputs which may undergo modifications during generation. As a demonstration, we apply our model to incremental surface realisation in a proof-of-concept study. 2 Related Work Our approach is most closely related to Lu et al. (2009) who also use CRFs to find the best surface realisation from a semantic tree. They conclude from an automatic evaluation that using CRF-based generation which takes long-range dependencies into account outperforms several baselines. However, Lu et al.’s generator does not take context beyond the current utterance into account and is thus restricted to local features. Furthermore, their model is not able to modify generation results on the fly due to new or updated inputs. In terms of surface realisation from graphical models (and within the context of SDSs), our approach is also related to wor</context>
</contexts>
<marker>Lu, Ng, Lee, 2009</marker>
<rawString>Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Natural Language Generation with Tree Conditional Random Fields. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Mairesse</author>
<author>Filip Jurˇciˇcek</author>
<author>Simon Keizer</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<location>Uppsala,</location>
<marker>Mairesse, Jurˇciˇcek, Keizer, Thomson, Yu, Young, 2010</marker>
<rawString>Franc¸ois Mairesse, Filip Jurˇciˇcek, Simon Keizer, Blaise Thomson, Kai Yu, and Steve Young. 2010. Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning. In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics (ACL), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Genoa, Italy.</location>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="13470" citStr="McCallum, 2002" startWordPosition="2149" endWordPosition="2150"> which are real values describing the label state y at time t based on the previous label state and features For example: from Figure 2 (c), might have the value = 1.0 for the transition from to a great and 0.0 elsewhere. The parameters Bk are set to maximise the conditional likelihood of phrase sequences in the training data set. They are estimated using the gradient ascent algorithm. After training, labels can Z(x) x Φk(.), yt−1 xt. Φk Φk “TheBeluga” “is Italian”, be predicted for new sequences of observations. The most likely phrase sequence is expressed as = arg max ∗ y We use the Mallet (McCallum, 2002) for parameter learning an package1 d inference. for each tree node. 1http://mallet.cs.umass.edu/ 2http://nlp.stanford.edu/software/ lex-parser.shtml y which is computed using the Viterbi algorithm. 3.3 Feature Selection and Training The following features define the generation context used during training of the CRF. The generation context includes everything that has been generated for the current utterance so far. All features can be obtained from a semantic input tree. • Lexical items of parents and siblings, • Semantic types in expansion, • Semantic types of parents and siblings, • Parse </context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Crystal Nakatsu</author>
<author>Michael White</author>
</authors>
<title>Learning to Say It Well: Reranking Realizations by Predicted Synthesis Quality. In</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (COLING-ACL)</booktitle>
<pages>1113--1120</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="7813" citStr="Nakatsu and White (2006)" startWordPosition="1207" endWordPosition="1210">ime SDS. This could present a problem in incremental settings, where generation speed is of particular importance. The SPaRKy system is also used by Rieser et al. (2011), who focus on information presentation strategies for restaurant recommendations, summaries or comparisons within an SDS. Their surface realiser is informed by the highest ranked SPaRKy outputs for a particular information presentation strategy and will constitute one of our baselines in the evaluation. More work on trainable realisation for SDSs generally includes Bulyko and Ostendorf (2002) who use finite state transducers, Nakatsu and White (2006) who use supervised learning, Varges (2006) who uses chart generation, and Konstas and Lapata (2012) who use weighted hypergraphs, among others. 3 Cohesion across Utterances 3.1 Tree-based Semantic Representations The restaurant recommendations we generate can include any of the attributes shown in Table 1. It is then the task of the surface realiser to find the best realisation, including whether to present them in one or several sentences. This often is a sentence planning decision, but in our approach it is handled using CRF-based surface realisation. The semantic forms underlying surface r</context>
</contexts>
<marker>Nakatsu, White, 2006</marker>
<rawString>Crystal Nakatsu and Michael White. 2006. Learning to Say It Well: Reranking Realizations by Predicted Synthesis Quality. In In Proceedings of the Annual Meeting of the Association for Computational Linguistics (COLING-ACL) 2006, pages 1113–1120, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nam Nguyen</author>
<author>Yunsong Guo</author>
</authors>
<title>Comparisons of Sequence Labeling Algorithms and Extensions.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<location>Corvallis, OR, USA.</location>
<marker>Nguyen, Guo, 2007</marker>
<rawString>Nam Nguyen and Yunsong Guo. 2007. Comparisons of Sequence Labeling Algorithms and Extensions. In Proceedings of the International Conference on Machine Learning (ICML), Corvallis, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Oh</author>
<author>Alexander Rudnicky</author>
</authors>
<title>Stochastic Language Generation for Spoken Dialogue Systems.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP/NAACL Workshop on Conversational Systems,</booktitle>
<pages>27--32</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="5578" citStr="Oh and Rudnicky (2000)" startWordPosition="857" endWordPosition="860">, and Mairesse et al. (2010) who use Dynamic Bayes Nets within an Active Learning framework. The last approach is also concerned with generating restaurant recommendations within an SDS. Specifically, their system optimises its performance online, during the interaction, by asking users to provide it with new textual descriptions of concepts, for which it is unsure of the best realisation. In contrast to these related approaches, we use undirected graphical models which are useful when the natural directionality between the input variables is unknown. In terms of surface realisation for SDSs, Oh and Rudnicky (2000) present foundational work in using an n-gram-based system. They train a surface realiser based on a domain-dependent language model and use an overgeneration and ranking approach. Candidate utterances are ranked according to a penalty function which penalises too long or short utterances, repetitious utterances and utterances which either contain more or less information than required by the dialogue act. While their approach is fast to execute, it has the disadvantage of not being able to model long-range dependencies. They show that humans rank their output equivalently to template-based ge</context>
<context position="20011" citStr="Oh and Rudnicky (2000)" startWordPosition="3226" endWordPosition="3229">f expansion 0 exists then 9: continue 10: else if non-terminal α exists then 11: add new expansion 0 to α. 12: else write new rule α → 0. 13: end if 14: Write grammar. 15: end for 16: end function CRFs and other state-of-the-art methods, we also compare our system to two other baselines: • CLASSiC corresponds to the system reported in Rieser et al. (2011),4 which generates restaurant recommendations based on the SPaRKy system (Walker et al., 2007), and has received high ratings in the past. SPaRKy uses global utterance features. • n-grams represents a simple 5-gram baseline that is similar to Oh and Rudnicky (2000)’s system. We will sample from the most likely slot realisations that do not contain a repetition and include exactly the required slot values. Local context only is taken into account. 4.1 Human Rating Study We carried out a user rating study on the CrowdFlower crowd sourcing platform.5 Each participant was shown part of a real human-system dialogue that emerged as part of the CLASSiC project evaluation (Rieser et al., 2011). All dialogues and data are freely available from http://www. classic-project.org. Each dialogue contained two variations for one of the utterances. These variations were</context>
</contexts>
<marker>Oh, Rudnicky, 2000</marker>
<rawString>Alice Oh and Alexander Rudnicky. 2000. Stochastic Language Generation for Spoken Dialogue Systems. In Proceedings of the ANLP/NAACL Workshop on Conversational Systems, pages 27–32, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Masayuki Otsuka</author>
</authors>
<title>Incremental Generation by Incremental Parsing. In</title>
<date>2003</date>
<booktitle>In Proceedings of the 6th UK Special-Interesting Group for Computational Linguistics (CLUK) Colloquium.</booktitle>
<contexts>
<context position="26761" citStr="Purver and Otsuka, 2003" startWordPosition="4328" endWordPosition="4331">lisation Recent years have seen increased interest in incremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and quality of output generation. Buschmeier et al. (2012) have presented a system which systematically takes the user’s acoustic understanding problems into account by pausing, repeating or re-phrasing if necessary. Their approach is based on SPUD (Stone et al., 2003), a constraint satisfaction-based NLG architecture and marks important progress towards more flexible incremental surface realisation. However, given the human labour involved in constraint specification, cohesion is often limited to a local co</context>
</contexts>
<marker>Purver, Otsuka, 2003</marker>
<rawString>Matthew Purver and Masayuki Otsuka. 2003. Incremental Generation by Incremental Parsing. In In Proceedings of the 6th UK Special-Interesting Group for Computational Linguistics (CLUK) Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Simon Keizer</author>
<author>Xingkun Liu</author>
<author>Oliver Lemon</author>
</authors>
<title>Adaptive Information Presentation for Spoken Dialogue Systems: Evaluation with Human Subjects.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG),</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="7358" citStr="Rieser et al. (2011)" startWordPosition="1140" endWordPosition="1143"> in our approach, SPaRKy distinguishes local and global features. Local features take only information of the current tree node into account, including its parents, siblings and children, while global features take information of the entire utterance into account. While SPaRKy is shown to reach high output quality in comparison to a template-based baseline, the authors acknowledge that generation with SPaRKy is rather slow when applied in a real-time SDS. This could present a problem in incremental settings, where generation speed is of particular importance. The SPaRKy system is also used by Rieser et al. (2011), who focus on information presentation strategies for restaurant recommendations, summaries or comparisons within an SDS. Their surface realiser is informed by the highest ranked SPaRKy outputs for a particular information presentation strategy and will constitute one of our baselines in the evaluation. More work on trainable realisation for SDSs generally includes Bulyko and Ostendorf (2002) who use finite state transducers, Nakatsu and White (2006) who use supervised learning, Varges (2006) who uses chart generation, and Konstas and Lapata (2012) who use weighted hypergraphs, among others. </context>
<context position="19746" citStr="Rieser et al. (2011)" startWordPosition="3184" endWordPosition="3187">hen 4: Find and replace the attribute by its semantic variable, e.g. $venue$. 5: end if 6: Parse the sentence and induce a set of rules α → 0, where α is a semantic variable and 0 is its parse. 7: Traverse the parse tree in a top-down, depth-first search and 8: if expansion 0 exists then 9: continue 10: else if non-terminal α exists then 11: add new expansion 0 to α. 12: else write new rule α → 0. 13: end if 14: Write grammar. 15: end for 16: end function CRFs and other state-of-the-art methods, we also compare our system to two other baselines: • CLASSiC corresponds to the system reported in Rieser et al. (2011),4 which generates restaurant recommendations based on the SPaRKy system (Walker et al., 2007), and has received high ratings in the past. SPaRKy uses global utterance features. • n-grams represents a simple 5-gram baseline that is similar to Oh and Rudnicky (2000)’s system. We will sample from the most likely slot realisations that do not contain a repetition and include exactly the required slot values. Local context only is taken into account. 4.1 Human Rating Study We carried out a user rating study on the CrowdFlower crowd sourcing platform.5 Each participant was shown part of a real huma</context>
<context position="24527" citStr="Rieser et al. (2011)" startWordPosition="3975" endWordPosition="3978">ollowing. CLASSiC utterances tend to be longer and contain more sentences than CRF (global) utterances. While CRF (global) often decides to aggregate attributes into one sentence, such as the Beluga is an outstanding restaurant in the city centre, CLASSiC tends to rely more on individual messages, such as The Beluga is an outstanding restaurant. It is located in the city centre. A possible reason is that while CRF (global) is able to take features beyond an utterance into account, CLASSiC/SPaRKy is restricted to global features of the current utterance. We can further compare our results with Rieser et al. (2011) and Mairesse et al. (2010) who also generate restaurant recommendations and asked similar questions to participants as we did. Rieser et al. (2011)’s system received an average rating of 3.586 in terms of Phrasing which compares to our 3.64. This difference is not significant, and in line with the user ratings we observed for the CLASSiC system above (3.59). Mairesse et al. (2010) achieved an average score of 4.05 in terms of Natural in comparison to our 3.65. This difference is significant at p&lt;0.05. Possibly their better performance is due to the data set being more “in domain” than ours. T</context>
</contexts>
<marker>Rieser, Keizer, Liu, Lemon, 2011</marker>
<rawString>Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver Lemon. 2011. Adaptive Information Presentation for Spoken Dialogue Systems: Evaluation with Human Subjects. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William Cohen</author>
</authors>
<title>SemiMarkov Conditional Random Fields for Information Extraction.</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing.</booktitle>
<marker>Sarawagi, Cohen, 2005</marker>
<rawString>Sunita Sarawagi and William Cohen. 2005. SemiMarkov Conditional Random Fields for Information Extraction. Advances in Neural Information Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Gabriel Skantze</author>
</authors>
<title>A General, Abstract Model of Incremental Dialogue Processing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="2149" citStr="Schlangen and Skantze, 2009" startWordPosition="316" endWordPosition="319">sometimes be difficult to model, because they require rich contextawareness that keeps track of all (or much) of what was generated before, i.e. a growing generation history. In text generation, cohesion can span over the entire text. In interactive settings such as generation within a spoken dialogue system (SDS), a challenge is often to keep track of cohesion over several utterances. In addition, since interactions are dynamic, generator inputs from the dialogue manager can sometimes be partial or subject to subsequent modification. This has been addressed by work on incremental processing (Schlangen and Skantze, 2009). Since dialogue acts are passed on to the generation module as soon as possible, this can sometimes lead to incomplete generator inputs (because the user is still speaking), or inputs that are subject to later modification (because of an initial ASR mis-recognition). In this paper, we propose to formulate surface realisation as a sequence labelling task. We use conditional random fields (Lafferty et al., 2001; Sutton and McCallum, 2006), which are suitable for modelling rich contexts, in combination with semantic trees for rich linguistic information. This combination is able to keep track of</context>
<context position="26282" citStr="Schlangen and Skantze, 2009" startWordPosition="4253" endWordPosition="4256">urant with good quality food. SYS2 inform(quality=good [0.61, name=Beluga [0.61) Oh sorry, so a nice restaurant located ... USR3 [barges in1 ... in the city centre. SYS3 inform(area=centre [0.81) Table 4: Example dialogue where the dialogue manager needs to send incremental updates to the NLG. Incremental surface realisation from semantic trees for this dialogue is shown in Figure 3. because the authors tested only for Phrasing and Natural, respectively. 5 Incremental Surface Realisation Recent years have seen increased interest in incremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and quality of output generation. </context>
</contexts>
<marker>Schlangen, Skantze, 2009</marker>
<rawString>David Schlangen and Gabriel Skantze. 2009. A General, Abstract Model of Incremental Dialogue Processing. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
<author>Anna Hjalmarsson</author>
</authors>
<title>Towards Incremental Speech Generation in Dialogue Systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual SigDial Meeting on Discourse and Dialogue,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="26792" citStr="Skantze and Hjalmarsson, 2010" startWordPosition="4332" endWordPosition="4335">e seen increased interest in incremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and quality of output generation. Buschmeier et al. (2012) have presented a system which systematically takes the user’s acoustic understanding problems into account by pausing, repeating or re-phrasing if necessary. Their approach is based on SPUD (Stone et al., 2003), a constraint satisfaction-based NLG architecture and marks important progress towards more flexible incremental surface realisation. However, given the human labour involved in constraint specification, cohesion is often limited to a local context. Especially for long utte</context>
</contexts>
<marker>Skantze, Hjalmarsson, 2010</marker>
<rawString>Gabriel Skantze and Anna Hjalmarsson. 2010. Towards Incremental Speech Generation in Dialogue Systems. In Proceedings of the 11th Annual SigDial Meeting on Discourse and Dialogue, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
<author>David Schlangen</author>
</authors>
<title>Incremental Dialogue Processing in a Micro-Domain.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="26252" citStr="Skantze and Schlangen, 2009" startWordPosition="4249" endWordPosition="4252">1 No, I’m looking for a restaurant with good quality food. SYS2 inform(quality=good [0.61, name=Beluga [0.61) Oh sorry, so a nice restaurant located ... USR3 [barges in1 ... in the city centre. SYS3 inform(area=centre [0.81) Table 4: Example dialogue where the dialogue manager needs to send incremental updates to the NLG. Incremental surface realisation from semantic trees for this dialogue is shown in Figure 3. because the authors tested only for Phrasing and Natural, respectively. 5 Incremental Surface Realisation Recent years have seen increased interest in incremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and </context>
<context position="34120" citStr="Skantze and Schlangen, 2009" startWordPosition="5518" endWordPosition="5521">ecked against the generator’s existing knowledge. If it is inconsistent with previous knowledge, an update is triggered and often followed by a deletion. Whenever generated output needs to be modified, old expansions and surface forms are deleted first, before new ones can be expanded in their place. 5.2 Updates and Processing Speed Results Since fast responses are crucial in incremental systems, we measured the average time our system took for a surface realisation. The time is 100ms on a MacBook Intel Core 2.6 Duo with 8GB in 1261 RAM. This is slightly better than other incremental systems (Skantze and Schlangen, 2009) and much faster than state-of-the-art non-incremental systems such as SPaRKy (Walker et al., 2007). In addition, we measured the number of necessary generation updates in comparison to a nonincremental setting. Since updates take effect directly on partial dialogue acts, rather than the full generated utterance, we require around 50% less updates as if generating from scratch for every changed input hypothesis. A qualitative analysis of the generated outputs showed that the quality is comparable to the non-incremental case. 6 Conclusion and Future Directions We have presented a novel techniqu</context>
</contexts>
<marker>Skantze, Schlangen, 2009</marker>
<rawString>Gabriel Skantze and David Schlangen. 2009. Incremental Dialogue Processing in a Micro-Domain. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Stent</author>
<author>Marilyn Walker</author>
<author>Steve Whittaker</author>
<author>Preetam Maloor</author>
</authors>
<title>User-tailored Generation for Spoken Dialogue: An Experiment.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="6512" citStr="Stent et al., 2002" startWordPosition="1005" endWordPosition="1008">nd utterances which either contain more or less information than required by the dialogue act. While their approach is fast to execute, it has the disadvantage of not being able to model long-range dependencies. They show that humans rank their output equivalently to template-based generation. Further, our approach is related to the SPaRKy sentence generator (Walker et al., 2007). SPaRKy was also developed for the domain of restaurant recommendations and was shown to be equivalent to or better than a carefully designed templatebased generator which had received high human ratings in the past (Stent et al., 2002). It generates sentences in two steps. First, it produces a randomised set of alternative realisations, which are then ranked according to a mapping from sentence plans to predicted human ratings using a boosting algorithm. As in our approach, SPaRKy distinguishes local and global features. Local features take only information of the current tree node into account, including its parents, siblings and children, while global features take information of the entire utterance into account. While SPaRKy is shown to reach high output quality in comparison to a template-based baseline, the authors ac</context>
</contexts>
<marker>Stent, Walker, Whittaker, Maloor, 2002</marker>
<rawString>Amanda Stent, Marilyn Walker, Steve Whittaker, and Preetam Maloor. 2002. User-tailored Generation for Spoken Dialogue: An Experiment. In Proceedings of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Christine Doran</author>
<author>Bonnie Webber</author>
<author>Tonia Bleam</author>
<author>Martha Palmer</author>
</authors>
<date>2003</date>
<booktitle>Microplanning with Communicative Intentions: The SPUD System. Computational Intelligence,</booktitle>
<pages>19--311</pages>
<contexts>
<context position="27117" citStr="Stone et al., 2003" startWordPosition="4380" endWordPosition="4383">nformation becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and quality of output generation. Buschmeier et al. (2012) have presented a system which systematically takes the user’s acoustic understanding problems into account by pausing, repeating or re-phrasing if necessary. Their approach is based on SPUD (Stone et al., 2003), a constraint satisfaction-based NLG architecture and marks important progress towards more flexible incremental surface realisation. However, given the human labour involved in constraint specification, cohesion is often limited to a local context. Especially for long utterances or such that are separated by user turns, this may lead to surface form increments that are not well connected and lack cohesion. 5.1 Application to Incremental SR This section will discuss a proof-of-concept application of our approach to incremental surface realisation. Table 4 shows an example dialogue between a u</context>
</contexts>
<marker>Stone, Doran, Webber, Bleam, Palmer, 2003</marker>
<rawString>Matthew Stone, Christine Doran, Bonnie Webber, Tonia Bleam, and Martha Palmer. 2003. Microplanning with Communicative Intentions: The SPUD System. Computational Intelligence, 19:311–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Introduction to Conditional Random Fields for Relational Learning.</title>
<date>2006</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2590" citStr="Sutton and McCallum, 2006" startWordPosition="386" endWordPosition="389"> inputs from the dialogue manager can sometimes be partial or subject to subsequent modification. This has been addressed by work on incremental processing (Schlangen and Skantze, 2009). Since dialogue acts are passed on to the generation module as soon as possible, this can sometimes lead to incomplete generator inputs (because the user is still speaking), or inputs that are subject to later modification (because of an initial ASR mis-recognition). In this paper, we propose to formulate surface realisation as a sequence labelling task. We use conditional random fields (Lafferty et al., 2001; Sutton and McCallum, 2006), which are suitable for modelling rich contexts, in combination with semantic trees for rich linguistic information. This combination is able to keep track of dependencies between syntactic, semantic and lexical features across multiple utterances. Our model can be trained from minimally labelled data, which reduces development time and may (in the future) facilitate an application to new domains. The domain used in this paper is a pedestrian walking around a city looking for information and recommendations for local restaurants from an SDS. We describe here the module for surface realisation</context>
</contexts>
<marker>Sutton, McCallum, 2006</marker>
<rawString>Charles Sutton and Andrew McCallum. 2006. Introduction to Conditional Random Fields for Relational Learning. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Varges</author>
</authors>
<title>Overgeneration and Ranking for Spoken Dialogue Systems.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fourth International Natural Language Generation Conference (INLG),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="7856" citStr="Varges (2006)" startWordPosition="1215" endWordPosition="1216">ettings, where generation speed is of particular importance. The SPaRKy system is also used by Rieser et al. (2011), who focus on information presentation strategies for restaurant recommendations, summaries or comparisons within an SDS. Their surface realiser is informed by the highest ranked SPaRKy outputs for a particular information presentation strategy and will constitute one of our baselines in the evaluation. More work on trainable realisation for SDSs generally includes Bulyko and Ostendorf (2002) who use finite state transducers, Nakatsu and White (2006) who use supervised learning, Varges (2006) who uses chart generation, and Konstas and Lapata (2012) who use weighted hypergraphs, among others. 3 Cohesion across Utterances 3.1 Tree-based Semantic Representations The restaurant recommendations we generate can include any of the attributes shown in Table 1. It is then the task of the surface realiser to find the best realisation, including whether to present them in one or several sentences. This often is a sentence planning decision, but in our approach it is handled using CRF-based surface realisation. The semantic forms underlying surface realisation can be produced in many ways. In</context>
</contexts>
<marker>Varges, 2006</marker>
<rawString>Sebastian Varges. 2006. Overgeneration and Ranking for Spoken Dialogue Systems. In Proceedings of the Fourth International Natural Language Generation Conference (INLG), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Amanda Stent</author>
<author>Franc¸ois Mairesse</author>
<author>Rashmi Prasad</author>
</authors>
<title>Individual and Domain Adaptation in Sentence Planning for Dialogue.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="6275" citStr="Walker et al., 2007" startWordPosition="966" endWordPosition="969">ace realiser based on a domain-dependent language model and use an overgeneration and ranking approach. Candidate utterances are ranked according to a penalty function which penalises too long or short utterances, repetitious utterances and utterances which either contain more or less information than required by the dialogue act. While their approach is fast to execute, it has the disadvantage of not being able to model long-range dependencies. They show that humans rank their output equivalently to template-based generation. Further, our approach is related to the SPaRKy sentence generator (Walker et al., 2007). SPaRKy was also developed for the domain of restaurant recommendations and was shown to be equivalent to or better than a carefully designed templatebased generator which had received high human ratings in the past (Stent et al., 2002). It generates sentences in two steps. First, it produces a randomised set of alternative realisations, which are then ranked according to a mapping from sentence plans to predicted human ratings using a boosting algorithm. As in our approach, SPaRKy distinguishes local and global features. Local features take only information of the current tree node into acco</context>
<context position="19840" citStr="Walker et al., 2007" startWordPosition="3198" endWordPosition="3201">se the sentence and induce a set of rules α → 0, where α is a semantic variable and 0 is its parse. 7: Traverse the parse tree in a top-down, depth-first search and 8: if expansion 0 exists then 9: continue 10: else if non-terminal α exists then 11: add new expansion 0 to α. 12: else write new rule α → 0. 13: end if 14: Write grammar. 15: end for 16: end function CRFs and other state-of-the-art methods, we also compare our system to two other baselines: • CLASSiC corresponds to the system reported in Rieser et al. (2011),4 which generates restaurant recommendations based on the SPaRKy system (Walker et al., 2007), and has received high ratings in the past. SPaRKy uses global utterance features. • n-grams represents a simple 5-gram baseline that is similar to Oh and Rudnicky (2000)’s system. We will sample from the most likely slot realisations that do not contain a repetition and include exactly the required slot values. Local context only is taken into account. 4.1 Human Rating Study We carried out a user rating study on the CrowdFlower crowd sourcing platform.5 Each participant was shown part of a real human-system dialogue that emerged as part of the CLASSiC project evaluation (Rieser et al., 2011)</context>
<context position="34219" citStr="Walker et al., 2007" startWordPosition="5532" endWordPosition="5535">is triggered and often followed by a deletion. Whenever generated output needs to be modified, old expansions and surface forms are deleted first, before new ones can be expanded in their place. 5.2 Updates and Processing Speed Results Since fast responses are crucial in incremental systems, we measured the average time our system took for a surface realisation. The time is 100ms on a MacBook Intel Core 2.6 Duo with 8GB in 1261 RAM. This is slightly better than other incremental systems (Skantze and Schlangen, 2009) and much faster than state-of-the-art non-incremental systems such as SPaRKy (Walker et al., 2007). In addition, we measured the number of necessary generation updates in comparison to a nonincremental setting. Since updates take effect directly on partial dialogue acts, rather than the full generated utterance, we require around 50% less updates as if generating from scratch for every changed input hypothesis. A qualitative analysis of the generated outputs showed that the quality is comparable to the non-incremental case. 6 Conclusion and Future Directions We have presented a novel technique for surface realisation that treats generation as a sequence labelling task by combining a CRF wi</context>
</contexts>
<marker>Walker, Stent, Mairesse, Prasad, 2007</marker>
<rawString>Marilyn Walker, Amanda Stent, Franc¸ois Mairesse, and Rashmi Prasad. 2007. Individual and Domain Adaptation in Sentence Planning for Dialogue. Journal of Artificial Intelligence Research, 30(1):413–456.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>