<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000046">
<title confidence="0.980243">
The Spanish Resource Grammar: pre-processing strategy and lexical acquisi-
tion
</title>
<author confidence="0.847269">
Montserrat Marimon, Núria Bel, Sergio Espeja, Natalia Seghezzi
</author>
<affiliation confidence="0.533385">
IULA - Universitat Pompeu Fabra
</affiliation>
<address confidence="0.317683">
Pl. de la Mercè, 10-12
08002-Barcelona
</address>
<email confidence="0.994141">
{montserrat.marimon,nuria.bel,sergio.espeja,natalia.seghezzi}@upf.edu
</email>
<sectionHeader confidence="0.993692" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999848875">
This paper describes work on the develop-
ment of an open-source HPSG grammar for
Spanish implemented within the LKB sys-
tem. Following a brief description of the
main features of the grammar, we present
our approach for pre-processing and on-
going research on automatic lexical acqui-
sition.1
</bodyText>
<sectionHeader confidence="0.998803" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999098263157895">
In this paper we describe the development of the
Spanish Resource Grammar (SRG), an open-
source2 medium-coverage grammar for Spanish.
The grammar is grounded in the theoretical
framework of HPSG (Head-driven Phrase Struc-
ture Grammar; Pollard and Sag, 1994) and uses
Minimal Recursion Semantics (MRS) for the se-
mantic representation (Copestake et al, 2006). The
SRG is implemented within the Linguistic Knowl-
edge Building (LKB) system (Copestake, 2002),
based on the basic components of the grammar
Matrix, an open–source starter-kit for the devel-
opment of HPSG grammars developed as part of
the LinGO consortium’s multilingual grammar
engineering (Bender et al., 2002).
The SRG is part of the DELPH-IN open-source
repository of linguistic resources and tools for
writing (the LKB system), testing (The [incr
tsbd()]; Oepen and Carroll, 2000) and efficiently
</bodyText>
<footnote confidence="0.927767166666667">
1 This research was supported by the Spanish Ministerio de
Educación y Ciencia: Project AAILE (HUM2004-05111-C02-
01), Ramon y Cajal, Juan de la Cierva programmes and PTA-
CTE/1370/2003 with Fondo Social Europeo.
2 The Spanish Resource Grammar may be downloaded from:
http://www.upf.edu/pdi/iula/montserrat.marimon/.
</footnote>
<bodyText confidence="0.999901971428572">
processing HPSG grammars (the PET system;
Callmeier, 2000). Further linguistic resources that
are available in the DELPH-IN repository include
broad-coverage grammars for English, German and
Japanese as well as smaller grammars for French,
Korean, Modern Greek, Norwegian and
Portuguese .3
The SRG has a full coverage of closed word
classes and it contains about 50,000 lexical entries
for open classes (roughtly: 6,600 verbs, 28,000
nouns, 11,200 adjectives and 4,000 adverbs).
These lexical entries are organized into a type
hierachy of about 400 leaf types (defined by a type
hierarchy of around 5,500 types). The grammar
also has 40 lexical rules to perform valence
changing operations on lexical items and 84
structure rules to combine words and phrases into
larger constituents and to compositionally build up
the semantic representation.
We have been developing the SRG since
January 2005. The range of linguistic phenomena
that the grammar handles includes almost all types
of subcategorization structures, valence
alternations, subordinate clauses, raising and
control, determination, null-subjects and
impersonal constructions, compound tenses,
modification, passive constructions, comparatives
and superlatives, cliticization, relative and
interrogative clauses and sentential adjuncts,
among others.
Together with the linguistic resources (grammar
and lexicon) we provide a set of controlled hand-
constructed test suites. The construction of the test
suites plays a major role in the development of the
SRG, since test suites provide a fine-grained diag-
</bodyText>
<footnote confidence="0.857139">
3 See http://www.delph-in.net/.
</footnote>
<page confidence="0.96148">
105
</page>
<note confidence="0.931531">
Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 105–111,
Prague, Czech Republic, June, 2007. @2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999853465116279">
nosis of grammar performance and they allow us to
compare the SRG with other DELPH-IN gram-
mars. In building the test suites we aimed at (a)
testing specific phenomena in isolation or in con-
trolled interaction, (b) providing test cases which
show systematic and exhaustive variations over
each phenomenon, including infrequent phenom-
ena and variations, (c) avoiding irrelevant variation
(i.e. different instances of the same lexical type), (d)
avoiding ambiguity, and (e) including negative or
ungrammatical data. We have about 500 test cases
which are distributed by linguistic phenomena (we
have 17 files). Each test case includes a short lin-
guistic annotation describing the phenomenon and
the number of expected results when more than
one analysis cannot be avoided (e.g. testing op-
tionality).
Test suites are not the only source of data we
have used for testing the SRG. Hand-constructed
sentences were complemented by real corpus cases
from: (a) the Spanish questions from the Question
Answering track at CLEF (CLEF-2003, CLEF-
2004, CLEF-2005 and CLEF-2006), and (b) the
general sub-corpus of the Corpus Tècnic de
l’IULA (IULA’s Technical Corpus; Cabré and
Bach, 2004); this sub-corpus includes newspaper
articles and it has been set up for contrastive
studies. CLEF cases include short queries showing
little interaction of phenomena and an average of
9.2 words; newspaper articles show a high level of
syntactic complexity and interaction of phenomena,
sentences are a bit longer, ranging up to 35 words.
We are currently shifting to much more varied
corpus data of the Corpus Tècnic de l’IULA, which
includes specialized corpus of written text in the
areas of computer science, environment, law,
medicine and economics, collected from several
sources, such as legal texts, textbooks, research
reports, user manuals, ... In these texts sentence
length may range up to 70 words.
The rest of the paper describes the pre-
processing strategy we have adopted and on our
on-going research on lexical acquisition.
</bodyText>
<sectionHeader confidence="0.600395" genericHeader="method">
2 Pre-processing in the SRG
</sectionHeader>
<bodyText confidence="0.9940331">
Following previous experiments within the
Advanced Linguistic Engineering Platform (ALEP)
platform (Marimon, 2002), we have integrated a
shallow processing tool, the FreeLing tool, as a
pre-processing module of the grammar.
The FreeLing tool is an open-source4 language
analysis tool suite (Atserias et al., 2006) perfoming
the following functionalities (though
disambiguation, named entity classification and the
last three functionalities have not been integrated):
</bodyText>
<listItem confidence="0.999953583333333">
• Text tokenization (including MWU and
contraction splitting).
• Sentence splitting.
• Morpho-syntactic analysis and
disambiguation.
• Named entity detection and classification.
• Date/number/currency/ratios/physical
magnitude (speed, weight, temperature,
density, etc.) recognition.
• Chart-based shallow parsing.
• WordNet-based sense annotation.
• Dependency parsing.
</listItem>
<bodyText confidence="0.998887933333333">
FreeLing also includes a guesser to deal with
words which are not found in the lexicon by
computing the probability of each possible PoS tag
given the longest observed termination string for
that word. Smoothing using probabilities of shorter
termination strings is also performed. Details can
be found in Brants (2000) and Samuelson (1993).
Our system integrates the FreeLing tool by
means of the LKB Simple PreProcessor Protocol
(SPPP; http://wiki.delph-in.net/moin/LkbSppp),
which assumes that a preprocessor runs as an
external process to the LKB system, and uses the
LKB inflectional rule component to convert the
PoS tags delivered by the FreeLing tool into partial
descriptions of feature structures.
</bodyText>
<subsectionHeader confidence="0.968474">
2.1 The integration of PoS tags
</subsectionHeader>
<bodyText confidence="0.99997975">
The integration of the morpho-syntactic analysis in
the LKB system using the SPPP protocol means
defining inflectional rules that propagate the mor-
pho-syntactic information associated to full-forms,
in the form of PoS tags, to the morpho-syntactic
features of the lexical items. (1) shows the rule
propagating the tag AQMS (adjective qualitative
masculine singular) delivered by FreeLing. Note
</bodyText>
<footnote confidence="0.7189415">
4 The FreeLing tool may be downloaded from
http://www.garraf.epsevg.upc.es/freeling/.
</footnote>
<page confidence="0.995981">
106
</page>
<bodyText confidence="0.999908">
that we use the tag as the rule identifier (i.e. the
name of the inflectional rule in the LKB).
</bodyText>
<equation confidence="0.9895152">
(1) aqms :=
%suffix ()
[SYNSEM.LOCAL[CAT adj,
AGR.PNG[PN 3sg,
GEN masc]]]
</equation>
<bodyText confidence="0.999981642857143">
In Spanish, when the verb is in non-finite form,
such as infinitive or gerund, or it is in the impera-
tive, clitics5 take the form of enclitics. That is, they
are attached to the verb forming a unique word,
e.g. hacerlo (hacer+lo; to do it), gustarle (gus-
tar+le; to like to him). FreeLing does not split
verbs and pronouns, but uses complex tags that
append the tags of each word. Thus, the form ha-
cerlo gets the tag VMN+PP3MSA (verb main in-
finitive + personal pronoun 3rd masculine singular
accusative). In order to deal with these complex
tags, the SRG includes a series of rules that build
up the same type of linguistic structure as that one
built up with the structure rules attaching affixes to
the left of verbal heads. Since the application of
these rules is based on the tag delivered by FreeL-
ing, they are included in the set of inflectional rules
and they are applied after the set of rules dealing
with complement cliticization.
Apart from avoiding the implementation of in-
flectional rules for such a highly inflected lan-
guage, the integration of the morpho-syntactic
analysis tags will allow us to implement default
lexical entries (i.e. lexical entry templates that are
activated when the system cannot find a particular
lexical entry to apply) on the basis of the category
encoded to the lexical tag delivered by FreeLing,
for virtually unlimited lexical coverage. 6
</bodyText>
<subsectionHeader confidence="0.990779">
2.2 The integration of multiword expressions
</subsectionHeader>
<bodyText confidence="0.901289090909091">
All multiword expressions in FreeLing are stored
in a file. The format of the file is one multiword
per line, having three fields each: form, lemma and
PoS.7 (2) shows two examples of multiword fixed
5 Actually, Spanish weak pronouns are considered pronominal
affixes rather than pronominal clitics.
6 The use of underspecified default lexical entries in a
highly lexicalized grammar, however, may increase
ambiguity and overgeneration (Marimon and Bel,
2004).
7 FreeLing only handles continuous multiword expres-
sions.
expressions; i.e. the ones that are fully lexicalized
and never show morpho-syntactic variation, a
través de (through) and a buenas horas (finally).
(2) a_través_de a_través_de SPS00
a_buenas_horas a_buenas_horas RG
The multiword form field may admit lemmas in
angle brackets, meaning that any form with that
lemma will be a valid component for the multi-
word. Tags are specified directly or as a reference
to the tag of some of the multiword components.
</bodyText>
<listItem confidence="0.972490375">
(3) builds a multiword with both singular and plu-
ral forms (apartado(s) de correos (P.O Box)). The
tag of the multiform is that of its first form ($1)
which starts with NC and takes the values for
number depending on whether the form is singular
or plural.
(3) &lt;apartado&gt;_de_correos apar-
tado_de _correos \$1:NC
</listItem>
<bodyText confidence="0.98649088">
Both fixed expressions and semi-fixed expres-
sions are integrated by means of the inflectional
rules that we have described in the previous sub-
section and they are treated in the grammar as
word complex with a single part of speech.
2.3 The integration of messy details and
named entities
FreeLing identifies, classifies and, when appropri-
ate, normalizes special text constructions that may
be considered peripheral to the lexicon, such as
dates, numbers, currencies, ratios, physical magni-
tudes, etc. FreeLing also identifies and classifies
named entities (i.e. proper names); however, we do
not activate the classification functionality, since
high performance of that functionality is only
achieved with PoS disambiguated contexts.
To integrate these messy details and named enti-
ties into the grammar, we require special inflec-
tional rules and lexical entry templates for each
text construction tag delivered by FreeLing. Some
of these tags are: W for dates, Z for numbers, Zm
for currencies, ... In order to define one single en-
try for each text construct, we identify the tag and
the STEM feature. (4) shows the lexical entry for
dates.8
</bodyText>
<footnote confidence="0.755485">
8 Each lexical entry in the SRG consists of a unique identifier,
a lexical type, an orthography and a semantic relation.
</footnote>
<page confidence="0.965461">
107
</page>
<equation confidence="0.68948625">
(4)
date := date_le &amp;
[STEM &lt;”w”&gt;,
SYNSEM.LKEY.KEYREL.PRED time_n_rel]
</equation>
<bodyText confidence="0.999552">
The integration of these messy details allows us
to release the analysis process from certain tasks
that may be reliably dealt with by shallow external
components.
</bodyText>
<sectionHeader confidence="0.986739" genericHeader="method">
3 Automatic Lexical Acquisition
</sectionHeader>
<bodyText confidence="0.999962108108108">
We have investigated Machine Learning (ML)
methods applied to the acquisition of the informa-
tion contained in the lexicon of the SRG.
ML applied to lexical acquisition is a very active
area of work linked to deep linguistic analysis due
to the central role that lexical information has in
lexicalized grammars and the costs of hand-
crafting them. Korhonen (2002), Carroll and Fang
(2004), Baldwin (2005), Blunsom and Baldwin
(2006), and Zhang and Kordoni (2006) are just a
few examples of reported research work on deep
lexical acquisition.
The most successful systems of lexical acquisi-
tion are based on the linguistic idea that the con-
texts where words occur are associated to particu-
lar lexical types. Although the methods are differ-
ent, most of the systems work upon the syntactic
information on words as collected from a corpus,
and they develop different techniques to decide
whether this information is relevant for type as-
signment or it is noise, especially when there are
just a few examples. In the LKB grammatical
framework, lexical types are defined as a combina-
tion of grammatical features. For our research, we
have looked at these morpho-syntactically moti-
vated features that can help in discriminating the
different types that we will ultimately use to clas-
sify words. Thus, words are assigned a number of
grammatical features, the ones that define the lexi-
cal types.
Table 1 and Table 2 show the syntactic features
that we use to characterize 6 types of adjectives
and 7 types of nouns in Spanish, respectively.9 As
can be observed, adjectives are cross-classified
according to their syntactic position within the NP,
i.e. (preN(ominal)) vs postN(ominal), the possibil-
ity of co-occurring in predicative constructions
</bodyText>
<page confidence="0.515341">
9 The SRG has 35 types for nouns and 44 types for adjectives.
</page>
<bodyText confidence="0.999322384615385">
(pred) and being modified by degree adverbs (G),
and their subcategorization frame (pcomp);
whereas lexical types for nouns are basically de-
fined on the basis of the mass/countable distinction
and valence information. Thus, an adjective like
bonito (nice), belonging to the type a_qual_intr,
may be found both in pre-nominal and post-
nominal position or in predicative constructions, it
may also be modified by degree adverbs, this type
of adjectives, however, does not take comple-
ments. Nouns belonging to the type n_intr_count,
like muchacha (girl), are countable intransitive
nouns.
</bodyText>
<tableCaption confidence="0.973822125">
TYPE/SF preN postN pred G pcomp
a adv int yes no no no no
a_adv_event yes yes no no no
a_rel_nonpred no yes no no no
a_rel_pred no yes yes no no
a qual intr yes yes yes yes no
a qual trans yes yes yes yes yes
Table 1. Some adjectival types of the SRG
</tableCaption>
<bodyText confidence="0.98444">
TYPE/SF mass count intr trans pcomp
n intr mass yes no yes no no
n_intr_count no yes yes no no
n_intr_cnt- yes yes yes no no
mss
n_trans_mass yes no no yes no
n trans count no yes no yes no
n_ppde_pcom no yes no yes yes
p_count
n_ppde_pcom yes no no yes yes
p_mss
</bodyText>
<tableCaption confidence="0.974721">
Table 2. Some nominal types of the SRG
</tableCaption>
<bodyText confidence="0.9999912">
We have investigated two methods to automati-
cally acquire such linguistic information for Span-
ish nouns and adjectives: a Bayesian model and a
decision tree. The aim of working with these two
methods was to compare their performance taking
into account that while the decision tree gets the
information from previously annotated data, the
Bayesian method learns it from the linguistic ty-
pology as defined by the grammar. These methods
are described in the following subsections.
</bodyText>
<subsectionHeader confidence="0.995069">
3.1 A Bayesian model for lexical acquisition
</subsectionHeader>
<bodyText confidence="0.999961">
We have used a Bayesian model of inductive learn-
ing for assigning grammatical features to words
occurring in a corpus. Given a hypothesis space
(the linguistic features of words according to its
lexical type) and one or more occurrences of the
</bodyText>
<page confidence="0.998384">
108
</page>
<bodyText confidence="0.999843454545454">
word to classify, the learner evaluates all hypothe-
ses for word features and values by computing
their posterior probabilities, proportional to the
product of prior probabilities and likelihood.
In order to obtain the likelihood, grammatical
features are related to the expected contexts where
their instances might appear. The linguistic typol-
ogy provides likelihood information that is the
learner’s expectation about which contexts are
likely to be observed given a particular hypothesis
of a word type. This likelihood is used as a substi-
tute of the computations made by observing di-
rectly the data, which is what a supervised machine
learning method does. As said, our aim was to
compare these two strategies.
The decision on a particular word is determined
by averaging the predictions of all hypothesis
weighted by their posterior probabilities. More
technically, for each syntactic feature {sf1, sf2, ...,
sfn} of the set SF (Syntactic Features) represented
in the lexical typology, we define the goal of our
system to be the assignment of a value, {no, yes},
that maximizes the result of a function f: a→ SF,
where a is the collection of its occurrences (σ =
{v1, v2, ..., vz}), each being a n-dimensional vector.
The decision on value assignment is achieved by
considering every occurrence as a cumulative evi-
dence in favour or against of having each syntactic
feature. Thus, our function Z’(SF, a), shown in (5),
will assess how much relevant information is got
from all the vectors. A further function, shown in
(8), will decide on the maximal value in order to
assign sfi,x.
</bodyText>
<equation confidence="0.611537">
z
(5) Z&apos;(sf x,σ)=∑P(sf,x  |vj
j )
</equation>
<bodyText confidence="0.998998428571429">
To assess P(sfi,x|vj), we use (6), which is the ap-
plication of Bayes Rule for solving the estimation
of the probability of a vector conditioned to a par-
ticular feature and value.
For solving (6), the prior P(sfi,x) is computed on
the basis of a lexical typology too, assuming that
what is more frequent in the typology will corre-
spondingly be more frequent in the data. For com-
puting the likelihood P(vj|sfi,x), as each vector is
made of m components, that is, the linguistic cues
vz = {lc1, lc2, ..., lcm}, we proceed as in (7) on the
basis of P(lcl|sfi,x); i.e. the likelihood of finding the
word in a particular context given a particular syn-
tactic feature.
</bodyText>
<equation confidence="0.867765230769231">
m
(7) P(vj  |sf ,x) = n Plcl  |sfx
,
Finally Z, as in (8), is the function that assigns
the syntactic features to σ .10
 |σ) &gt; Z&apos; (sf. |σ)
z, x = no
Z
 |) &apos; (
σ &gt; Z sf  |)
σ
i x yes
, =
</equation>
<bodyText confidence="0.999992454545454">
For computing the likelihood, we count on the
conditional probabilities of the correlations be-
tween features as defined in the typology. We use
these correlations to infer the expectation of ob-
serving the linguistic cues associated to particular
syntactic features, and to make it to be conditional
to a particular feature and value. However, linguis-
tic cues and syntactic features are in two different
dimensions; syntactic features are properties of
lexical items, while linguistic cues show the char-
acteristics of actual occurrences. As we assume
that each syntactic feature must have at least one
corresponding linguistic cue, we must tune the
probability to acknowledge the factors that affect
linguistic cues. For such a tuning, we have consid-
ered the following two issues: (i) to include in the
assessments the known uncertainty of the linguistic
cues that can be present in the occurrence or not;
and (ii) to create a dummy variable to deal with the
fact that, while syntactic features in the typology
are independent from one another, evidences in
text are not so.
We have also observed that the information that
can be gathered by looking at all word occurrences
as a complex unit have a conclusive value. Take
for instance the case of prepositions. The observa-
tion of a given prepositions in different occur-
rences of the same word is a conclusive evidence
for considering it a bound preposition. In order to
take this into account, we have devised a function
that acts as a dynamic weighting module. The
function app_lc(sfi, a) returns the number of con-
texts where the cue is found. In the case that in a
</bodyText>
<footnote confidence="0.625084">
10 In the theoretical case of having the same probability
for yes and for no, Z is undefined.
</footnote>
<figure confidence="0.99178596">
P(v |sf
j i ,x
k P(vj  |sf k)P(sf,k
(6) P sf |vj ) =
( i ,x
) P ( sf i x )
,
)
)
(8)
Z
&apos; (sf
i
,
x=yes
=
,
x=no
&apos; (sf
i
⎧⎩Z
yes⎫ ⎪⎬
⎪⎭
→
→ no
</figure>
<page confidence="0.995282">
109
</page>
<bodyText confidence="0.997260666666667">
particular signature there is no context with such a
lc, it returns ‘1’. Thus, app_lc is used to reinforce
this conclusive evidence in (5), which is now (9).
</bodyText>
<figure confidence="0.950135285714286">
(9)
⎞ )
Z&apos;(sf x=yes,6) = ⎜ EP(sf x=yes  |vp⎠⎟* app _lc(sfi, 6
j
z
Z&apos; (sfi x=no , 6) = E P(sfi&amp;quot;=no  |v j)
j
</figure>
<subsectionHeader confidence="0.997272">
3.2 A Decision tree
</subsectionHeader>
<bodyText confidence="0.999206363636364">
Linguistic motivated features have also been
evaluated using a C4.5 Decision Tree (DT) classi-
fier (Quinlan, 1993) in the Weka implementation
(Witten and Frank, 2005). These features corre-
spond to the expected contexts for the different
nominal and adjectival lexical types.
We have trained the DT with all the vectors of
the word occurrences that we had in the different
gold-standards, using their encoding for the super-
vised experiment in a 10-fold cross-validation test-
ing (Bel et al. 2007).
</bodyText>
<subsectionHeader confidence="0.994106">
3.3 Evaluation and Results
</subsectionHeader>
<bodyText confidence="0.999947521739131">
For the evaluation, we have applied both methods
to the lexical acquisition of nouns and adjectives.
We have worked with a PoS tagged corpus of
1,091,314 words. Datasets of 496 adjectives and
289 nouns were selected among the ones that had
occurrences in the corpus. Some manual selection
had to be done in order to have all possible types
represented but still it roughly corresponds to the
distribution of features in the existing lexicon.
We evaluated by comparing with Gold-
standard files; i.e. the manually encoded lexicon of
the SRG. The usual accuracy measures as type
precision (percentage of feature values correctly
assigned to all values assigned) and type recall
(percentage of correct feature values found in the
dictionary) have been used. F1 is the usual score
combining precision and recall.
Table 3 shows the results in terms of F1 score
for the different methods and PoS for feature as-
signment. From these data, we concluded that the
probabilistic information inferred from the lexical
typology defined in our grammar is a good source
of knowledge for lexical acquisition.
</bodyText>
<table confidence="0.965178333333333">
PoS noun adj
Z 0.88 0.87
DT 0.89 0.9
</table>
<tableCaption confidence="0.984896666666667">
Table 3. F1 for different methods and PoS.
Table 4 shows more details of the results compar-
ing between DT and Z for Spanish adjectives.
</tableCaption>
<table confidence="0.999856">
SF = no SF = yes
Z DT Z DT
prep a 0.98 0.97 0.72 0.44
prep en 0.98 0.99 0.27 0
prep con 0.99 0.99 0.60 0
prep para 0.98 0.99 0.51 0.53
prep de 0.88 0.97 0.34 0.42
postN 0 0 0.99 0.99
preN 0.75 0.83 0.44 0.80
Pred 0.50 0.41 0.59 0.82
G 0.85 0.80 0.75 0.72
Sent 0.97 0.97 0.55 0.44
</table>
<tableCaption confidence="0.994957">
Table 4. F1 for Spanish adjectival features.
</tableCaption>
<bodyText confidence="0.999405857142857">
Finally, Table 5 shows the results for 50 Spanish
nouns with only one occurrence in the corpus.
These results show that grammatical features can
be used for lexical acquisition of low frequency
lexical items, providing a good hypothesis for en-
suring grammar robustness and adding no over-
generation to parsing results.
</bodyText>
<table confidence="0.995099285714286">
DT Z
prec. rec. F prec. rec. F
MASS 0.50 0.16 0.25 0.66 0.25 0.36
COUNT 0.97 1.00 0.98 1.00 0.96 0.98
TRANS 0.75 0.46 0.57 0.68 0.73 0.71
INTRANS 0.85 0.95 0.89 0.89 0.76 0.82
PCOMP 0 0 0 0.14 0.20 0.16
</table>
<tableCaption confidence="0.972282">
Table 5. Results of 50 unseen nouns with a sin-
gle occurrence.
</tableCaption>
<sectionHeader confidence="0.999523" genericHeader="method">
4 Future Work
</sectionHeader>
<bodyText confidence="0.999950571428571">
We have presented work on the development of an
HPSG grammar for Spanish; in particular, we have
described our approach for pre-processing and on-
going research on automatic lexical acquisition.
Besides extending the coverage of the SRG and
continuing research on lexical acquisition, the spe-
cific aims of our future work on the SRG are:
</bodyText>
<listItem confidence="0.973665">
• Treebank development.
</listItem>
<page confidence="0.983553">
110
</page>
<bodyText confidence="0.957964117647059">
• To extend the shallow/deep architecture
and integrate the structures generated by
partial parsing, to provide robust techniques
for infrequent structural constructions. The
coverage of these linguistic structures by
means of structure rules would increase both
processing time and ambiguity.
• To use ML methods for disambiguation;
i.e. for ranking possible parsings according
to relevant linguistic features, thus enabling
the setting of a threshold to select the n-best
analyses.
• The development of error mining tech-
niques (van Noord, 2004) to identify errone-
ous and incomplete information in the lin-
guistic resources which cause the grammar
to fail.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999806207792208">
J. Atserias, B. Casas, E. Comelles, M. González, L. Pa-
dró and M. Padró. 2006. FreeLing 1.3: Syntactic and
semantic services in an open-source NLP library. 5th
International Conference on Language Resources
and Evaluation. Genoa, Italy.
T. Baldwin. 2005. Bootstrapping Deep Lexical Re-
sources: Resources for Courses, ACL-SIGLEX 2005.
Workshop on Deep Lexical Acquisition. Ann Arbor,
Michigan.
N. Bel, S. Espeja, M. Marimon. 2007. Automatic Ac-
quisition of Grammatical Types for Nouns. Human
Language Technologies: The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics. Rochester, NY, USA,
E.M. Bender, D. Flickinger and S. Oepen. 2002. The
grammar Matrix. An open-source starter-kit for the
rapid development of cress-linguistically consistent
broad-coverage precision grammar. Workshop on
Grammar Engineering and Evaluation, 19th Interna-
tional Conference on Computational Linguistics.
Taipei, Taiwan.
P. Blunsom and T. Baldwin. 2006. Multilingual Deep
Lexical Acquisition for HPSGs via Supertagging.
Conference on Empirical Methods in Natural Lan-
guage Processing. Sydney, Australia.
T. Brants. 2000. TnT: A statistical part-of-speech tag-
ger. 6th Conference on Applied Natural Language
Processing. Seattle, USA.
T. Cabré and C. Bach, 2004. El corpus tècnic de
l’IULA: corpus textual especializado plurilingüe.
Panacea, V. 16, pages 173-176.
U. Callmeier. 2000. Pet – a platform for experimenta-
tion with efficient HPSG processing. Journal of
Natural Language Engineering 6(1): Special Issue
on Efficient Processing with HPSG: Methods, Sys-
tem, Evaluation, pages 99-108.
A. Copestake, D. Flickinger, C. Pollard and I.A. Sag.
2006. Minimal Recursion Semantics: An Introduc-
tion. Research on Language and Computation
3.4:281-332.
A. Copestake. 2002. Implementing Typed Features
Structure Grammars. CSLI Publications.
A. Korhonen. 2002. ‘Subcategorization acquisition’. As
Technical Report UCAM-CL-TR-530, University of
Cambridge, UK.
M. Marimon. 2002. Integrating Shallow Linguistic
Processing into a Unification-based Spanish Gram-
mar. 9th International Conference on Computational
Linguistics. Taipei, Taiwan.
M. Marimon and N. Bel. 2004. Lexical Entry Templates
for Robust Deep Parsing. 4th International Confer-
ence on Language Resources and Evaluation. Lis-
bon, Portugal.
S. Oepen and J. Carroll. 2000. Performance Profiling for
Parser Engineering. Journal of Natural Language
Engineering 6(1): Special Issue on Efficient Process-
ing with HPSG: Methods, System, Evaluation, pages
81-97.
C.J. Pollard and I.A. Sag. 1994. Head-driven Phrase
Structure Grammar. The University of Chicago
Press, Chicago.
R.J. Quinlan 1993. C4.5: Programs for Machine Learn-
ing. Series in Machine Learning. Morgan Kaufman,
San Mateo, CA.
C. Samuelson. 1993. Morphological tagging based en-
tirely on Bayesian inference. 9th Nordic Conference
on Computational Linguistics. Stockholm, Sweden.
I.H. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan
Kaufmann, San Francisco.
G. van Noord. 2004. Error mining for wide-coverage
grammar engineering. 42th Annual Meeting of the
ACL. Barcelona, Spain.
Y. Zhang and V. Kordoni. 2006. Automated deep lexi-
cal acquisition for robust open text processing. 5th
International Conference on Language Resources
and Evaluation. Genoa, Italy.
</reference>
<page confidence="0.998785">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.458145">
<title confidence="0.980262">Spanish Resource Grammar: pre-processing strategy and lexical acquisition</title>
<author confidence="0.954235">Montserrat Marimon</author>
<author confidence="0.954235">Núria Bel</author>
<author confidence="0.954235">Sergio Espeja</author>
<author confidence="0.954235">Natalia</author>
<affiliation confidence="0.939085">IULA - Universitat Pompeu Pl. de la Mercè,</affiliation>
<address confidence="0.829844">08002-Barcelona</address>
<email confidence="0.999653">montserrat.marimon@upf.edu</email>
<email confidence="0.999653">nuria.bel@upf.edu</email>
<email confidence="0.999653">sergio.espeja@upf.edu</email>
<email confidence="0.999653">natalia.seghezzi@upf.edu</email>
<abstract confidence="0.949847125">This paper describes work on the development of an open-source HPSG grammar for Spanish implemented within the LKB system. Following a brief description of the main features of the grammar, we present our approach for pre-processing and ongoing research on automatic lexical acqui-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Atserias</author>
<author>B Casas</author>
<author>E Comelles</author>
<author>M González</author>
<author>L Padró</author>
<author>M Padró</author>
</authors>
<title>FreeLing 1.3: Syntactic and semantic services</title>
<date>2006</date>
<booktitle>in an open-source NLP library. 5th International Conference on Language Resources and Evaluation.</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="5887" citStr="Atserias et al., 2006" startWordPosition="863" endWordPosition="866">ected from several sources, such as legal texts, textbooks, research reports, user manuals, ... In these texts sentence length may range up to 70 words. The rest of the paper describes the preprocessing strategy we have adopted and on our on-going research on lexical acquisition. 2 Pre-processing in the SRG Following previous experiments within the Advanced Linguistic Engineering Platform (ALEP) platform (Marimon, 2002), we have integrated a shallow processing tool, the FreeLing tool, as a pre-processing module of the grammar. The FreeLing tool is an open-source4 language analysis tool suite (Atserias et al., 2006) perfoming the following functionalities (though disambiguation, named entity classification and the last three functionalities have not been integrated): • Text tokenization (including MWU and contraction splitting). • Sentence splitting. • Morpho-syntactic analysis and disambiguation. • Named entity detection and classification. • Date/number/currency/ratios/physical magnitude (speed, weight, temperature, density, etc.) recognition. • Chart-based shallow parsing. • WordNet-based sense annotation. • Dependency parsing. FreeLing also includes a guesser to deal with words which are not found in</context>
</contexts>
<marker>Atserias, Casas, Comelles, González, Padró, Padró, 2006</marker>
<rawString>J. Atserias, B. Casas, E. Comelles, M. González, L. Padró and M. Padró. 2006. FreeLing 1.3: Syntactic and semantic services in an open-source NLP library. 5th International Conference on Language Resources and Evaluation. Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Baldwin</author>
</authors>
<title>Bootstrapping Deep Lexical Resources: Resources for Courses,</title>
<date>2005</date>
<booktitle>ACL-SIGLEX 2005. Workshop on Deep Lexical Acquisition.</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="12453" citStr="Baldwin (2005)" startWordPosition="1890" endWordPosition="1891">RED time_n_rel] The integration of these messy details allows us to release the analysis process from certain tasks that may be reliably dealt with by shallow external components. 3 Automatic Lexical Acquisition We have investigated Machine Learning (ML) methods applied to the acquisition of the information contained in the lexicon of the SRG. ML applied to lexical acquisition is a very active area of work linked to deep linguistic analysis due to the central role that lexical information has in lexicalized grammars and the costs of handcrafting them. Korhonen (2002), Carroll and Fang (2004), Baldwin (2005), Blunsom and Baldwin (2006), and Zhang and Kordoni (2006) are just a few examples of reported research work on deep lexical acquisition. The most successful systems of lexical acquisition are based on the linguistic idea that the contexts where words occur are associated to particular lexical types. Although the methods are different, most of the systems work upon the syntactic information on words as collected from a corpus, and they develop different techniques to decide whether this information is relevant for type assignment or it is noise, especially when there are just a few examples. I</context>
</contexts>
<marker>Baldwin, 2005</marker>
<rawString>T. Baldwin. 2005. Bootstrapping Deep Lexical Resources: Resources for Courses, ACL-SIGLEX 2005. Workshop on Deep Lexical Acquisition. Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bel</author>
<author>S Espeja</author>
<author>M Marimon</author>
</authors>
<title>Automatic Acquisition of Grammatical Types for Nouns. Human Language Technologies:</title>
<date>2007</date>
<booktitle>The Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<location>Rochester, NY, USA,</location>
<contexts>
<context position="20851" citStr="Bel et al. 2007" startWordPosition="3358" endWordPosition="3361">) ⎞ ) Z&apos;(sf x=yes,6) = ⎜ EP(sf x=yes |vp⎠⎟* app _lc(sfi, 6 j z Z&apos; (sfi x=no , 6) = E P(sfi&amp;quot;=no |v j) j 3.2 A Decision tree Linguistic motivated features have also been evaluated using a C4.5 Decision Tree (DT) classifier (Quinlan, 1993) in the Weka implementation (Witten and Frank, 2005). These features correspond to the expected contexts for the different nominal and adjectival lexical types. We have trained the DT with all the vectors of the word occurrences that we had in the different gold-standards, using their encoding for the supervised experiment in a 10-fold cross-validation testing (Bel et al. 2007). 3.3 Evaluation and Results For the evaluation, we have applied both methods to the lexical acquisition of nouns and adjectives. We have worked with a PoS tagged corpus of 1,091,314 words. Datasets of 496 adjectives and 289 nouns were selected among the ones that had occurrences in the corpus. Some manual selection had to be done in order to have all possible types represented but still it roughly corresponds to the distribution of features in the existing lexicon. We evaluated by comparing with Goldstandard files; i.e. the manually encoded lexicon of the SRG. The usual accuracy measures as t</context>
</contexts>
<marker>Bel, Espeja, Marimon, 2007</marker>
<rawString>N. Bel, S. Espeja, M. Marimon. 2007. Automatic Acquisition of Grammatical Types for Nouns. Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics. Rochester, NY, USA,</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Bender</author>
<author>D Flickinger</author>
<author>S Oepen</author>
</authors>
<title>The grammar Matrix. An open-source starter-kit for the rapid development of cress-linguistically consistent broad-coverage precision grammar.</title>
<date>2002</date>
<booktitle>Workshop on Grammar Engineering and Evaluation, 19th International Conference on Computational Linguistics.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1262" citStr="Bender et al., 2002" startWordPosition="177" endWordPosition="180">e Spanish Resource Grammar (SRG), an opensource2 medium-coverage grammar for Spanish. The grammar is grounded in the theoretical framework of HPSG (Head-driven Phrase Structure Grammar; Pollard and Sag, 1994) and uses Minimal Recursion Semantics (MRS) for the semantic representation (Copestake et al, 2006). The SRG is implemented within the Linguistic Knowledge Building (LKB) system (Copestake, 2002), based on the basic components of the grammar Matrix, an open–source starter-kit for the development of HPSG grammars developed as part of the LinGO consortium’s multilingual grammar engineering (Bender et al., 2002). The SRG is part of the DELPH-IN open-source repository of linguistic resources and tools for writing (the LKB system), testing (The [incr tsbd()]; Oepen and Carroll, 2000) and efficiently 1 This research was supported by the Spanish Ministerio de Educación y Ciencia: Project AAILE (HUM2004-05111-C02- 01), Ramon y Cajal, Juan de la Cierva programmes and PTACTE/1370/2003 with Fondo Social Europeo. 2 The Spanish Resource Grammar may be downloaded from: http://www.upf.edu/pdi/iula/montserrat.marimon/. processing HPSG grammars (the PET system; Callmeier, 2000). Further linguistic resources that a</context>
</contexts>
<marker>Bender, Flickinger, Oepen, 2002</marker>
<rawString>E.M. Bender, D. Flickinger and S. Oepen. 2002. The grammar Matrix. An open-source starter-kit for the rapid development of cress-linguistically consistent broad-coverage precision grammar. Workshop on Grammar Engineering and Evaluation, 19th International Conference on Computational Linguistics. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Baldwin</author>
</authors>
<title>Multilingual Deep Lexical Acquisition for HPSGs via Supertagging.</title>
<date>2006</date>
<booktitle>Conference on Empirical Methods in Natural Language Processing.</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="12481" citStr="Blunsom and Baldwin (2006)" startWordPosition="1892" endWordPosition="1895">The integration of these messy details allows us to release the analysis process from certain tasks that may be reliably dealt with by shallow external components. 3 Automatic Lexical Acquisition We have investigated Machine Learning (ML) methods applied to the acquisition of the information contained in the lexicon of the SRG. ML applied to lexical acquisition is a very active area of work linked to deep linguistic analysis due to the central role that lexical information has in lexicalized grammars and the costs of handcrafting them. Korhonen (2002), Carroll and Fang (2004), Baldwin (2005), Blunsom and Baldwin (2006), and Zhang and Kordoni (2006) are just a few examples of reported research work on deep lexical acquisition. The most successful systems of lexical acquisition are based on the linguistic idea that the contexts where words occur are associated to particular lexical types. Although the methods are different, most of the systems work upon the syntactic information on words as collected from a corpus, and they develop different techniques to decide whether this information is relevant for type assignment or it is noise, especially when there are just a few examples. In the LKB grammatical framew</context>
</contexts>
<marker>Blunsom, Baldwin, 2006</marker>
<rawString>P. Blunsom and T. Baldwin. 2006. Multilingual Deep Lexical Acquisition for HPSGs via Supertagging. Conference on Empirical Methods in Natural Language Processing. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT: A statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>6th Conference on Applied Natural Language Processing.</booktitle>
<location>Seattle, USA.</location>
<contexts>
<context position="6732" citStr="Brants (2000)" startWordPosition="976" endWordPosition="977">itting. • Morpho-syntactic analysis and disambiguation. • Named entity detection and classification. • Date/number/currency/ratios/physical magnitude (speed, weight, temperature, density, etc.) recognition. • Chart-based shallow parsing. • WordNet-based sense annotation. • Dependency parsing. FreeLing also includes a guesser to deal with words which are not found in the lexicon by computing the probability of each possible PoS tag given the longest observed termination string for that word. Smoothing using probabilities of shorter termination strings is also performed. Details can be found in Brants (2000) and Samuelson (1993). Our system integrates the FreeLing tool by means of the LKB Simple PreProcessor Protocol (SPPP; http://wiki.delph-in.net/moin/LkbSppp), which assumes that a preprocessor runs as an external process to the LKB system, and uses the LKB inflectional rule component to convert the PoS tags delivered by the FreeLing tool into partial descriptions of feature structures. 2.1 The integration of PoS tags The integration of the morpho-syntactic analysis in the LKB system using the SPPP protocol means defining inflectional rules that propagate the morpho-syntactic information associ</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT: A statistical part-of-speech tagger. 6th Conference on Applied Natural Language Processing. Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cabré</author>
<author>C Bach</author>
</authors>
<title>El corpus tècnic de l’IULA: corpus textual especializado plurilingüe.</title>
<date>2004</date>
<journal>Panacea, V.</journal>
<volume>16</volume>
<pages>173--176</pages>
<contexts>
<context position="4706" citStr="Cabré and Bach, 2004" startWordPosition="681" endWordPosition="684"> distributed by linguistic phenomena (we have 17 files). Each test case includes a short linguistic annotation describing the phenomenon and the number of expected results when more than one analysis cannot be avoided (e.g. testing optionality). Test suites are not the only source of data we have used for testing the SRG. Hand-constructed sentences were complemented by real corpus cases from: (a) the Spanish questions from the Question Answering track at CLEF (CLEF-2003, CLEF2004, CLEF-2005 and CLEF-2006), and (b) the general sub-corpus of the Corpus Tècnic de l’IULA (IULA’s Technical Corpus; Cabré and Bach, 2004); this sub-corpus includes newspaper articles and it has been set up for contrastive studies. CLEF cases include short queries showing little interaction of phenomena and an average of 9.2 words; newspaper articles show a high level of syntactic complexity and interaction of phenomena, sentences are a bit longer, ranging up to 35 words. We are currently shifting to much more varied corpus data of the Corpus Tècnic de l’IULA, which includes specialized corpus of written text in the areas of computer science, environment, law, medicine and economics, collected from several sources, such as legal</context>
</contexts>
<marker>Cabré, Bach, 2004</marker>
<rawString>T. Cabré and C. Bach, 2004. El corpus tècnic de l’IULA: corpus textual especializado plurilingüe. Panacea, V. 16, pages 173-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Callmeier</author>
</authors>
<title>Pet – a platform for experimentation with efficient HPSG processing.</title>
<date>2000</date>
<journal>Journal of Natural Language Engineering</journal>
<booktitle>Special Issue on Efficient Processing with HPSG: Methods, System, Evaluation,</booktitle>
<volume>6</volume>
<issue>1</issue>
<pages>99--108</pages>
<contexts>
<context position="1825" citStr="Callmeier, 2000" startWordPosition="258" endWordPosition="259">ultilingual grammar engineering (Bender et al., 2002). The SRG is part of the DELPH-IN open-source repository of linguistic resources and tools for writing (the LKB system), testing (The [incr tsbd()]; Oepen and Carroll, 2000) and efficiently 1 This research was supported by the Spanish Ministerio de Educación y Ciencia: Project AAILE (HUM2004-05111-C02- 01), Ramon y Cajal, Juan de la Cierva programmes and PTACTE/1370/2003 with Fondo Social Europeo. 2 The Spanish Resource Grammar may be downloaded from: http://www.upf.edu/pdi/iula/montserrat.marimon/. processing HPSG grammars (the PET system; Callmeier, 2000). Further linguistic resources that are available in the DELPH-IN repository include broad-coverage grammars for English, German and Japanese as well as smaller grammars for French, Korean, Modern Greek, Norwegian and Portuguese .3 The SRG has a full coverage of closed word classes and it contains about 50,000 lexical entries for open classes (roughtly: 6,600 verbs, 28,000 nouns, 11,200 adjectives and 4,000 adverbs). These lexical entries are organized into a type hierachy of about 400 leaf types (defined by a type hierarchy of around 5,500 types). The grammar also has 40 lexical rules to perf</context>
</contexts>
<marker>Callmeier, 2000</marker>
<rawString>U. Callmeier. 2000. Pet – a platform for experimentation with efficient HPSG processing. Journal of Natural Language Engineering 6(1): Special Issue on Efficient Processing with HPSG: Methods, System, Evaluation, pages 99-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>Minimal Recursion Semantics: An Introduction.</title>
<date>2006</date>
<journal>Research on Language and Computation</journal>
<pages>3--4</pages>
<contexts>
<context position="949" citStr="Copestake et al, 2006" startWordPosition="130" endWordPosition="133">elopment of an open-source HPSG grammar for Spanish implemented within the LKB system. Following a brief description of the main features of the grammar, we present our approach for pre-processing and ongoing research on automatic lexical acquisition.1 1 Introduction In this paper we describe the development of the Spanish Resource Grammar (SRG), an opensource2 medium-coverage grammar for Spanish. The grammar is grounded in the theoretical framework of HPSG (Head-driven Phrase Structure Grammar; Pollard and Sag, 1994) and uses Minimal Recursion Semantics (MRS) for the semantic representation (Copestake et al, 2006). The SRG is implemented within the Linguistic Knowledge Building (LKB) system (Copestake, 2002), based on the basic components of the grammar Matrix, an open–source starter-kit for the development of HPSG grammars developed as part of the LinGO consortium’s multilingual grammar engineering (Bender et al., 2002). The SRG is part of the DELPH-IN open-source repository of linguistic resources and tools for writing (the LKB system), testing (The [incr tsbd()]; Oepen and Carroll, 2000) and efficiently 1 This research was supported by the Spanish Ministerio de Educación y Ciencia: Project AAILE (HU</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2006</marker>
<rawString>A. Copestake, D. Flickinger, C. Pollard and I.A. Sag. 2006. Minimal Recursion Semantics: An Introduction. Research on Language and Computation 3.4:281-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
</authors>
<title>Implementing Typed Features Structure Grammars.</title>
<date>2002</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="1045" citStr="Copestake, 2002" startWordPosition="146" endWordPosition="147">ef description of the main features of the grammar, we present our approach for pre-processing and ongoing research on automatic lexical acquisition.1 1 Introduction In this paper we describe the development of the Spanish Resource Grammar (SRG), an opensource2 medium-coverage grammar for Spanish. The grammar is grounded in the theoretical framework of HPSG (Head-driven Phrase Structure Grammar; Pollard and Sag, 1994) and uses Minimal Recursion Semantics (MRS) for the semantic representation (Copestake et al, 2006). The SRG is implemented within the Linguistic Knowledge Building (LKB) system (Copestake, 2002), based on the basic components of the grammar Matrix, an open–source starter-kit for the development of HPSG grammars developed as part of the LinGO consortium’s multilingual grammar engineering (Bender et al., 2002). The SRG is part of the DELPH-IN open-source repository of linguistic resources and tools for writing (the LKB system), testing (The [incr tsbd()]; Oepen and Carroll, 2000) and efficiently 1 This research was supported by the Spanish Ministerio de Educación y Ciencia: Project AAILE (HUM2004-05111-C02- 01), Ramon y Cajal, Juan de la Cierva programmes and PTACTE/1370/2003 with Fond</context>
</contexts>
<marker>Copestake, 2002</marker>
<rawString>A. Copestake. 2002. Implementing Typed Features Structure Grammars. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
</authors>
<title>Subcategorization acquisition’. As</title>
<date>2002</date>
<tech>Technical Report UCAM-CL-TR-530,</tech>
<institution>University of Cambridge, UK.</institution>
<contexts>
<context position="12412" citStr="Korhonen (2002)" startWordPosition="1884" endWordPosition="1885">ate_le &amp; [STEM &lt;”w”&gt;, SYNSEM.LKEY.KEYREL.PRED time_n_rel] The integration of these messy details allows us to release the analysis process from certain tasks that may be reliably dealt with by shallow external components. 3 Automatic Lexical Acquisition We have investigated Machine Learning (ML) methods applied to the acquisition of the information contained in the lexicon of the SRG. ML applied to lexical acquisition is a very active area of work linked to deep linguistic analysis due to the central role that lexical information has in lexicalized grammars and the costs of handcrafting them. Korhonen (2002), Carroll and Fang (2004), Baldwin (2005), Blunsom and Baldwin (2006), and Zhang and Kordoni (2006) are just a few examples of reported research work on deep lexical acquisition. The most successful systems of lexical acquisition are based on the linguistic idea that the contexts where words occur are associated to particular lexical types. Although the methods are different, most of the systems work upon the syntactic information on words as collected from a corpus, and they develop different techniques to decide whether this information is relevant for type assignment or it is noise, especia</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>A. Korhonen. 2002. ‘Subcategorization acquisition’. As Technical Report UCAM-CL-TR-530, University of Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marimon</author>
</authors>
<title>Integrating Shallow Linguistic Processing into a Unification-based Spanish Grammar.</title>
<date>2002</date>
<booktitle>9th International Conference on Computational Linguistics.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="5688" citStr="Marimon, 2002" startWordPosition="834" endWordPosition="835">much more varied corpus data of the Corpus Tècnic de l’IULA, which includes specialized corpus of written text in the areas of computer science, environment, law, medicine and economics, collected from several sources, such as legal texts, textbooks, research reports, user manuals, ... In these texts sentence length may range up to 70 words. The rest of the paper describes the preprocessing strategy we have adopted and on our on-going research on lexical acquisition. 2 Pre-processing in the SRG Following previous experiments within the Advanced Linguistic Engineering Platform (ALEP) platform (Marimon, 2002), we have integrated a shallow processing tool, the FreeLing tool, as a pre-processing module of the grammar. The FreeLing tool is an open-source4 language analysis tool suite (Atserias et al., 2006) perfoming the following functionalities (though disambiguation, named entity classification and the last three functionalities have not been integrated): • Text tokenization (including MWU and contraction splitting). • Sentence splitting. • Morpho-syntactic analysis and disambiguation. • Named entity detection and classification. • Date/number/currency/ratios/physical magnitude (speed, weight, tem</context>
</contexts>
<marker>Marimon, 2002</marker>
<rawString>M. Marimon. 2002. Integrating Shallow Linguistic Processing into a Unification-based Spanish Grammar. 9th International Conference on Computational Linguistics. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marimon</author>
<author>N Bel</author>
</authors>
<title>Lexical Entry Templates for Robust Deep Parsing.</title>
<date>2004</date>
<booktitle>4th International Conference on Language Resources and Evaluation.</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="9692" citStr="Marimon and Bel, 2004" startWordPosition="1447" endWordPosition="1450">e basis of the category encoded to the lexical tag delivered by FreeLing, for virtually unlimited lexical coverage. 6 2.2 The integration of multiword expressions All multiword expressions in FreeLing are stored in a file. The format of the file is one multiword per line, having three fields each: form, lemma and PoS.7 (2) shows two examples of multiword fixed 5 Actually, Spanish weak pronouns are considered pronominal affixes rather than pronominal clitics. 6 The use of underspecified default lexical entries in a highly lexicalized grammar, however, may increase ambiguity and overgeneration (Marimon and Bel, 2004). 7 FreeLing only handles continuous multiword expressions. expressions; i.e. the ones that are fully lexicalized and never show morpho-syntactic variation, a través de (through) and a buenas horas (finally). (2) a_través_de a_través_de SPS00 a_buenas_horas a_buenas_horas RG The multiword form field may admit lemmas in angle brackets, meaning that any form with that lemma will be a valid component for the multiword. Tags are specified directly or as a reference to the tag of some of the multiword components. (3) builds a multiword with both singular and plural forms (apartado(s) de correos (P.</context>
</contexts>
<marker>Marimon, Bel, 2004</marker>
<rawString>M. Marimon and N. Bel. 2004. Lexical Entry Templates for Robust Deep Parsing. 4th International Conference on Language Resources and Evaluation. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>J Carroll</author>
</authors>
<title>Performance Profiling for Parser Engineering.</title>
<date>2000</date>
<journal>Journal of Natural Language Engineering</journal>
<booktitle>Special Issue on Efficient Processing with HPSG: Methods, System, Evaluation,</booktitle>
<volume>6</volume>
<issue>1</issue>
<pages>81--97</pages>
<contexts>
<context position="1435" citStr="Oepen and Carroll, 2000" startWordPosition="204" endWordPosition="207">ructure Grammar; Pollard and Sag, 1994) and uses Minimal Recursion Semantics (MRS) for the semantic representation (Copestake et al, 2006). The SRG is implemented within the Linguistic Knowledge Building (LKB) system (Copestake, 2002), based on the basic components of the grammar Matrix, an open–source starter-kit for the development of HPSG grammars developed as part of the LinGO consortium’s multilingual grammar engineering (Bender et al., 2002). The SRG is part of the DELPH-IN open-source repository of linguistic resources and tools for writing (the LKB system), testing (The [incr tsbd()]; Oepen and Carroll, 2000) and efficiently 1 This research was supported by the Spanish Ministerio de Educación y Ciencia: Project AAILE (HUM2004-05111-C02- 01), Ramon y Cajal, Juan de la Cierva programmes and PTACTE/1370/2003 with Fondo Social Europeo. 2 The Spanish Resource Grammar may be downloaded from: http://www.upf.edu/pdi/iula/montserrat.marimon/. processing HPSG grammars (the PET system; Callmeier, 2000). Further linguistic resources that are available in the DELPH-IN repository include broad-coverage grammars for English, German and Japanese as well as smaller grammars for French, Korean, Modern Greek, Norweg</context>
</contexts>
<marker>Oepen, Carroll, 2000</marker>
<rawString>S. Oepen and J. Carroll. 2000. Performance Profiling for Parser Engineering. Journal of Natural Language Engineering 6(1): Special Issue on Efficient Processing with HPSG: Methods, System, Evaluation, pages 81-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Pollard</author>
<author>I A Sag</author>
</authors>
<title>Head-driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>The University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="850" citStr="Pollard and Sag, 1994" startWordPosition="115" endWordPosition="118">mon,nuria.bel,sergio.espeja,natalia.seghezzi}@upf.edu Abstract This paper describes work on the development of an open-source HPSG grammar for Spanish implemented within the LKB system. Following a brief description of the main features of the grammar, we present our approach for pre-processing and ongoing research on automatic lexical acquisition.1 1 Introduction In this paper we describe the development of the Spanish Resource Grammar (SRG), an opensource2 medium-coverage grammar for Spanish. The grammar is grounded in the theoretical framework of HPSG (Head-driven Phrase Structure Grammar; Pollard and Sag, 1994) and uses Minimal Recursion Semantics (MRS) for the semantic representation (Copestake et al, 2006). The SRG is implemented within the Linguistic Knowledge Building (LKB) system (Copestake, 2002), based on the basic components of the grammar Matrix, an open–source starter-kit for the development of HPSG grammars developed as part of the LinGO consortium’s multilingual grammar engineering (Bender et al., 2002). The SRG is part of the DELPH-IN open-source repository of linguistic resources and tools for writing (the LKB system), testing (The [incr tsbd()]; Oepen and Carroll, 2000) and efficientl</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C.J. Pollard and I.A. Sag. 1994. Head-driven Phrase Structure Grammar. The University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<booktitle>Series in Machine Learning.</booktitle>
<publisher>Morgan</publisher>
<location>Kaufman, San Mateo, CA.</location>
<contexts>
<context position="20471" citStr="Quinlan, 1993" startWordPosition="3298" endWordPosition="3299"> case of having the same probability for yes and for no, Z is undefined. P(v |sf j i ,x k P(vj |sf k)P(sf,k (6) P sf |vj ) = ( i ,x ) P ( sf i x ) , ) ) (8) Z &apos; (sf i , x=yes = , x=no &apos; (sf i ⎧⎩Z yes⎫ ⎪⎬ ⎪⎭ → → no 109 particular signature there is no context with such a lc, it returns ‘1’. Thus, app_lc is used to reinforce this conclusive evidence in (5), which is now (9). (9) ⎞ ) Z&apos;(sf x=yes,6) = ⎜ EP(sf x=yes |vp⎠⎟* app _lc(sfi, 6 j z Z&apos; (sfi x=no , 6) = E P(sfi&amp;quot;=no |v j) j 3.2 A Decision tree Linguistic motivated features have also been evaluated using a C4.5 Decision Tree (DT) classifier (Quinlan, 1993) in the Weka implementation (Witten and Frank, 2005). These features correspond to the expected contexts for the different nominal and adjectival lexical types. We have trained the DT with all the vectors of the word occurrences that we had in the different gold-standards, using their encoding for the supervised experiment in a 10-fold cross-validation testing (Bel et al. 2007). 3.3 Evaluation and Results For the evaluation, we have applied both methods to the lexical acquisition of nouns and adjectives. We have worked with a PoS tagged corpus of 1,091,314 words. Datasets of 496 adjectives and</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>R.J. Quinlan 1993. C4.5: Programs for Machine Learning. Series in Machine Learning. Morgan Kaufman, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelson</author>
</authors>
<title>Morphological tagging based entirely on Bayesian inference.</title>
<date>1993</date>
<booktitle>9th Nordic Conference on Computational Linguistics.</booktitle>
<location>Stockholm,</location>
<contexts>
<context position="6753" citStr="Samuelson (1993)" startWordPosition="979" endWordPosition="980">yntactic analysis and disambiguation. • Named entity detection and classification. • Date/number/currency/ratios/physical magnitude (speed, weight, temperature, density, etc.) recognition. • Chart-based shallow parsing. • WordNet-based sense annotation. • Dependency parsing. FreeLing also includes a guesser to deal with words which are not found in the lexicon by computing the probability of each possible PoS tag given the longest observed termination string for that word. Smoothing using probabilities of shorter termination strings is also performed. Details can be found in Brants (2000) and Samuelson (1993). Our system integrates the FreeLing tool by means of the LKB Simple PreProcessor Protocol (SPPP; http://wiki.delph-in.net/moin/LkbSppp), which assumes that a preprocessor runs as an external process to the LKB system, and uses the LKB inflectional rule component to convert the PoS tags delivered by the FreeLing tool into partial descriptions of feature structures. 2.1 The integration of PoS tags The integration of the morpho-syntactic analysis in the LKB system using the SPPP protocol means defining inflectional rules that propagate the morpho-syntactic information associated to full-forms, i</context>
</contexts>
<marker>Samuelson, 1993</marker>
<rawString>C. Samuelson. 1993. Morphological tagging based entirely on Bayesian inference. 9th Nordic Conference on Computational Linguistics. Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="20523" citStr="Witten and Frank, 2005" startWordPosition="3304" endWordPosition="3307">s and for no, Z is undefined. P(v |sf j i ,x k P(vj |sf k)P(sf,k (6) P sf |vj ) = ( i ,x ) P ( sf i x ) , ) ) (8) Z &apos; (sf i , x=yes = , x=no &apos; (sf i ⎧⎩Z yes⎫ ⎪⎬ ⎪⎭ → → no 109 particular signature there is no context with such a lc, it returns ‘1’. Thus, app_lc is used to reinforce this conclusive evidence in (5), which is now (9). (9) ⎞ ) Z&apos;(sf x=yes,6) = ⎜ EP(sf x=yes |vp⎠⎟* app _lc(sfi, 6 j z Z&apos; (sfi x=no , 6) = E P(sfi&amp;quot;=no |v j) j 3.2 A Decision tree Linguistic motivated features have also been evaluated using a C4.5 Decision Tree (DT) classifier (Quinlan, 1993) in the Weka implementation (Witten and Frank, 2005). These features correspond to the expected contexts for the different nominal and adjectival lexical types. We have trained the DT with all the vectors of the word occurrences that we had in the different gold-standards, using their encoding for the supervised experiment in a 10-fold cross-validation testing (Bel et al. 2007). 3.3 Evaluation and Results For the evaluation, we have applied both methods to the lexical acquisition of nouns and adjectives. We have worked with a PoS tagged corpus of 1,091,314 words. Datasets of 496 adjectives and 289 nouns were selected among the ones that had occ</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I.H. Witten and E. Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
</authors>
<title>Error mining for wide-coverage grammar engineering.</title>
<date>2004</date>
<booktitle>42th Annual Meeting of the ACL.</booktitle>
<location>Barcelona,</location>
<marker>van Noord, 2004</marker>
<rawString>G. van Noord. 2004. Error mining for wide-coverage grammar engineering. 42th Annual Meeting of the ACL. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>V Kordoni</author>
</authors>
<title>Automated deep lexical acquisition for robust open text processing.</title>
<date>2006</date>
<booktitle>5th International Conference on Language Resources and Evaluation.</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="12511" citStr="Zhang and Kordoni (2006)" startWordPosition="1897" endWordPosition="1900">etails allows us to release the analysis process from certain tasks that may be reliably dealt with by shallow external components. 3 Automatic Lexical Acquisition We have investigated Machine Learning (ML) methods applied to the acquisition of the information contained in the lexicon of the SRG. ML applied to lexical acquisition is a very active area of work linked to deep linguistic analysis due to the central role that lexical information has in lexicalized grammars and the costs of handcrafting them. Korhonen (2002), Carroll and Fang (2004), Baldwin (2005), Blunsom and Baldwin (2006), and Zhang and Kordoni (2006) are just a few examples of reported research work on deep lexical acquisition. The most successful systems of lexical acquisition are based on the linguistic idea that the contexts where words occur are associated to particular lexical types. Although the methods are different, most of the systems work upon the syntactic information on words as collected from a corpus, and they develop different techniques to decide whether this information is relevant for type assignment or it is noise, especially when there are just a few examples. In the LKB grammatical framework, lexical types are defined</context>
</contexts>
<marker>Zhang, Kordoni, 2006</marker>
<rawString>Y. Zhang and V. Kordoni. 2006. Automated deep lexical acquisition for robust open text processing. 5th International Conference on Language Resources and Evaluation. Genoa, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>