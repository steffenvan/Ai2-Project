<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.103271">
<title confidence="0.99058">
Classifying Amharic News Text Using Self-Organizing Maps
</title>
<author confidence="0.999417">
Samuel Eyassu Bj¨orn Gambick*
</author>
<affiliation confidence="0.84404">
Department of Information Science Swedish Institute of Computer Science
Addis Ababa University, Ethiopia Box 1263, SE–164 29 Kista, Sweden
</affiliation>
<email confidence="0.997836">
samueleya@yahoo.com gamback@sics.se
</email>
<sectionHeader confidence="0.993865" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999809941176471">
The paper addresses using artificial neu-
ral networks for classification of Amharic
news items. Amharic is the language for
countrywide communication in Ethiopia
and has its own writing system contain-
ing extensive systematic redundancy. It is
quite dialectally diversified and probably
representative of the languages of a conti-
nent that so far has received little attention
within the language processing field.
The experiments investigated document
clustering around user queries using Self-
Organizing Maps, an unsupervised learn-
ing neural network strategy. The best
ANN model showed a precision of 60.0%
when trying to cluster unseen data, and a
69.5% precision when trying to classify it.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976384615385">
Even though the last years have seen an increasing
trend in investigating applying language processing
methods to other languages than English, most of
the work is still done on very few and mainly Euro-
pean and East-Asian languages; for the vast number
of languages of the African continent there still re-
mains plenty of work to be done. The main obsta-
cles to progress in language processing for these are
two-fold. Firstly, the peculiarities of the languages
themselves might force new strategies to be devel-
oped. Secondly, the lack of already available re-
sources and tools makes the creation and testing of
new ones more difficult and time-consuming.
</bodyText>
<note confidence="0.415093">
*Author for correspondence.
</note>
<page confidence="0.988629">
71
</page>
<bodyText confidence="0.99997790625">
Many of the languages of Africa have few speak-
ers, and some lack a standardised written form, both
creating problems for building language process-
ing systems and reducing the need for such sys-
tems. However, this is not true for the major African
languages and as example of one of those this pa-
per takes Amharic, the Semitic language used for
countrywide communication in Ethiopia. With more
than 20 million speakers, Amharic is today probably
one of the five largest on the continent (albeit diffi-
cult to determine, given the dramatic population size
changes in many African countries in recent years).
The Ethiopian culture is ancient, and so are the
written languages of the area, with Amharic using
its own script. Several computer fonts for the script
have been developed, but for many years it had no
standardised computer representation1 which was a
deterrent to electronic publication. An exponentially
increasing amount of digital information is now be-
ing produced in Ethiopia, but no deep-rooted cul-
ture of information exchange and dissemination has
been established. Different factors are attributed to
this, including lack of digital library facilities and
central resource sites, inadequate resources for elec-
tronic publication of journals and books, and poor
documentation and archive collections. The diffi-
culties to access information have led to low expec-
tations and under-utilization of existing information
resources, even though the need for accurate and fast
information access is acknowledged as a major fac-
tor affecting the success and quality of research and
development, trade and industry (Furzey, 1996).
</bodyText>
<footnote confidence="0.99982975">
1An international standard for Amharic was agreed on only
in year 1998, following Amendment 10 to ISO–10646–1. The
standard was finally incorporated into Unicode in year 2000:
www.unicode.org/charts/PDF/U1200.pdf
</footnote>
<note confidence="0.9928335">
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 71–78,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999958636363636">
In recent years this has lead to an increasing aware-
ness that Amharic language processing resources
and digital information access and storage facili-
ties must be created. To this end, some work has
now been carried out, mainly by Ethiopian Telecom,
the Ethiopian Science and Technology Commission,
Addis Ababa University, the Ge’ez Frontier Foun-
dation, and Ethiopian students abroad. So have, for
example, Sisay and Haller (2003) looked at Amharic
word formation and lexicon building; Nega and Wil-
lett (2002) at stemming; Atelach et al. (2003a) at
treebank building; Daniel (Yacob, 2005) at the col-
lection of an (untagged) corpus, tentatively to be
hosted by Oxford University’s Open Archives Ini-
tiative; and Cowell and Hussain (2003) at charac-
ter recognition.2 See Atelach et al. (2003b) for an
overview of the efforts that have been made so far to
develop language processing tools for Amharic.
The need for investigating Amharic information
access has been acknowledged by the European
Cross-Language Evaluation Forum, which added an
Amharic–English track in 2004. However, the task
addressed was for accessing an English database
in English, with only the original questions being
posed in Amharic (and then translated into English).
Three groups participated in this track, with Atelach
et al. (2004) reporting the best results.
In the present paper we look at the problem of
mapping questions posed in Amharic onto a col-
lection of Amharic news items. We use the Self-
Organizing Map (SOM) model of artificial neural
networks for the task of retrieving the documents
matching a specific query. The SOMs were imple-
mented using the Matlab Neural Network Toolbox.
The rest of the paper is laid out as follows. Sec-
tion 2 discusses artificial neural networks and in par-
ticular the SOM model and its application to infor-
mation access. In Section 3 we describe the Amharic
language and its writing system in more detail to-
gether with the news items corpora used for training
and testing of the networks, while Sections 4 and 5
detail the actual experiments, on text retrieval and
text classification, respectively. Finally, Section 6
sums up the main contents of the paper.
</bodyText>
<footnote confidence="0.97221575">
2In the text we follow the Ethiopian practice of referring to
Ethiopians by their given names. However, the reference list
follows Western standard and is ordered according to surnames
(i.e., the father’s name for an Ethiopian).
</footnote>
<sectionHeader confidence="0.948197" genericHeader="method">
2 Artificial Neural Networks
</sectionHeader>
<bodyText confidence="0.999744684210526">
Artificial Neural Networks (ANN) is a computa-
tional paradigm inspired by the neurological struc-
ture of the human brain, and ANN terminology bor-
rows from neurology: the brain consists of millions
of neurons connected to each other through long and
thin strands called axons; the connecting points be-
tween neurons are called synapses.
ANNs have proved themselves useful in deriving
meaning from complicated or imprecise data; they
can be used to extract patterns and detect trends that
are too complex to be noticed by either humans or
other computational and statistical techniques. Tra-
ditionally, the most common ANN setup has been
the backpropagation architecture (Rumelhart et al.,
1986), a supervised learning strategy where input
data is fed forward in the network to the output
nodes (normally with an intermediate hidden layer
of nodes) while errors in matches are propagated
backwards in the net during training.
</bodyText>
<subsectionHeader confidence="0.991576">
2.1 Self-Organizing Maps
</subsectionHeader>
<bodyText confidence="0.9998762">
Self-Organizing Maps (SOM) is an unsupervised
learning scheme neural network, which was in-
vented by Kohonen (1999). It was originally devel-
oped to project multi-dimensional vectors on a re-
duced dimensional space. Self-organizing systems
can have many kinds of structures, a common one
consists of an input layer and an output layer, with
feed-forward connections from input to output lay-
ers and full connectivity (connections between all
neurons) in the output layer.
A SOM is provided with a set of rules of a lo-
cal nature (a signal affects neurons in the immedi-
ate vicinity of the current neuron), enabling it to
learn to compute an input-output pairing with spe-
cific desirable properties. The learning process con-
sists of repeatedly modifying the synaptic weights
of the connections in the system in response to input
(activation) patterns and in accordance to prescribed
rules, until a final configuration develops. Com-
monly both the weights of the neuron closest match-
ing the inputs and the weights of its neighbourhood
nodes are increased. At the beginning of the training
the neighbourhood (where input patterns cluster de-
pending on their similarity) can be fairly large and
then be allowed to decrease over time.
</bodyText>
<page confidence="0.989239">
72
</page>
<subsectionHeader confidence="0.992233">
2.2 Neural network-based text classification
</subsectionHeader>
<bodyText confidence="0.999990784313726">
Neural networks have been widely used in text clas-
sification, where they can be given terms and hav-
ing the output nodes represent categories. Ruiz
and Srinivasan (1999) utilize an hierarchical array
of backpropagation neural networks for (nonlinear)
classification of MEDLINE records, while Ng et al.
(1997) use the simplest (and linear) type of ANN
classifier, the perceptron. Nonlinear methods have
not been shown to add any performance to linear
ones for text categorization (Sebastiani, 2002).
SOMs have been used for information access
since the beginning of the 90s (Lin et al., 1991). A
SOM may show how documents with similar fea-
tures cluster together by projecting the N-dimen-
sional vector space onto a two-dimensional grid.
The radius of neighbouring nodes may be varied to
include documents that are weaker related. The most
elaborate experiments of using SOMs for document
classification have been undertaken using the WEB-
SOM architecture developed at Helsinki University
of Technology (Honkela et al., 1997; Kohonen et al.,
2000). WEBSOM is based on a hierarchical two-
level SOM structure, with the first level forming his-
togram clusters of words. The second level is used
to reduce the sensitivity of the histogram to small
variations in document content and performs further
clustering to display the document pattern space.
A Self-Organizing Map is capable of simulating
new data sets without the need of retraining itself
when the database is updated; something which is
not true for Latent Semantic Indexing, LSI (Deer-
wester et al., 1990). Moreover, LSI consumes am-
ple time in calculating similarities of new queries
against all documents, but a SOM only needs to cal-
culate similarities versus some representative subset
of old input data and can then map new input straight
onto the most similar models without having to re-
compute the whole mapping.
The SOM model preparation passes through the
processes undertaken by the LSI model and the clas-
sical vector space model (Salton and McGill, 1983).
Hence those models can be taken as particular cases
of the SOM, when the neighbourhood diameter is
maximized. For instance, one can calculate the
LSI model’s similarity measure of documents versus
queries by varying the SOM’s neighbourhood diam-
eter, if the training set is a singular value decom-
position reduced vector space. Tambouratzis et al.
(2003) use SOMs for categorizing texts according to
register and author style and show that the results are
equivalent to those generated by statistical methods.
</bodyText>
<sectionHeader confidence="0.895606" genericHeader="method">
3 Processing Amharic
</sectionHeader>
<bodyText confidence="0.999993636363636">
Ethiopia with some 70 million inhabitants is the
third most populous African country and harbours
more than 80 different languages.3 Three of these
are dominant: Oromo, a Cushitic language spoken
in the South and Central parts of the country and
written using the Latin alphabet; Tigrinya, spoken in
the North and in neighbouring Eritrea; and Amharic,
spoken in most parts of the country, but predomi-
nantly in the Eastern, Western, and Central regions.
Both Amharic and Tigrinya are Semitic and about as
close as are Spanish and Portuguese (Bloor, 1995),
</bodyText>
<subsectionHeader confidence="0.998042">
3.1 The Amharic language and script
</subsectionHeader>
<bodyText confidence="0.999807869565217">
Already a census from 19944 estimated Amharic to
be mother tongue of more than 17 million people,
with at least an additional 5 million second language
speakers. It is today probably the second largest lan-
guage in Ethiopia (after Oromo). The Constitution
of 1994 divided Ethiopia into nine fairly indepen-
dent regions, each with its own nationality language.
However, Amharic is the language for countrywide
communication and was also for a long period the
principal literal language and medium of instruction
in primary and secondary schools in the country,
while higher education is carried out in English.
Amharic and Tigrinya speakers are mainly Ortho-
dox Christians, with the languages drawing com-
mon roots to the ecclesiastic Ge’ez still used by the
Coptic Church. Both languages are written using
the Ge’ez script, horizontally and left-to-right (in
contrast to many other Semitic languages). Writ-
ten Ge’ez can be traced back to at least the 4th
century A.D. The first versions of the script in-
cluded consonants only, while the characters in later
versions represent consonant-vowel (CV) phoneme
pairs. In modern written Amharic, each syllable pat-
</bodyText>
<footnote confidence="0.9985576">
3How many languages there are in a country is as much a po-
litical as a linguistic issue. The number of languages of Ethiopia
and Eritrea together thus differs from 70 up to 420, depending
on the source; however, 82 (plus 4 extinct) is a common number.
4Published by Ethiopia’s Central Statistal Authority 1998.
</footnote>
<page confidence="0.979993">
73
</page>
<table confidence="0.9992186">
Order 1 2 3 4 5 6 7
V /9/ /u/ /i/ /r/ /e/ /i/ /o/
C ����
/s/ n n- n n (L n n
/m/ oo a- °&apos;Z °9 °?� P �
</table>
<tableCaption confidence="0.999875">
Table 1: The orders for n (/s/) and P (/m/)
</tableCaption>
<bodyText confidence="0.998360183673469">
tern comes in seven different forms (called orders),
reflecting the seven vowel sounds. The first order is
the basic form; the other orders are derived from it
by more or less regular modifications indicating the
different vowels. There are 33 basic forms, giving
7*33 syllable patterns, or fidEls.
Two of the base forms represent vowels in isola-
tion (o and ⑨), but the rest are for consonants (or
semivowels classed as consonants) and thus corre-
spond to CV pairs, with the first order being the base
symbol with no explicit vowel indicator (though a
vowel is pronounced: C+/9/). The sixth order is am-
biguous between being just the consonant or C+/i/.
The writing system also includes 20 symbols for
labialised velars (four five-character orders) and 24
for other labialisation. In total, there are 275 fidEls.
The sequences in Table 1 (for n and P) exemplify
the (partial) symmetry of vowel indicators.
Amharic also has its own numbers (twenty sym-
bols, though not widely used nowadays) and its own
punctuation system with eight symbols, where the
space between words looks like a colon:,while the
full stop, comma and semicolon are , : and 1. The
question and exclamation marks have recently been
included in the writing system. For more thorough
discussions of the Ethiopian writing system, see, for
example, Bender et al. (1976) and Bloor (1995).
Amharic words have consonantal roots with
vowel variation expressing difference in interpreta-
tion, making stemming a not-so-useful technique in
information retrieval (no full morphological anal-
yser for the language is available yet). There is no
agreed upon spelling standard for compounds and
the writing system uses multitudes of ways to denote
compound words. In addition, not all the letters of
the Amharic script are strictly necessary for the pro-
nunciation patterns of the language; some were sim-
ply inherited from Ge’ez without having any seman-
tic or phonetic distinction in modern Amharic: there
are many cases where numerous symbols are used to
denote a single phoneme, as well as words that have
extremely different orthographic form and slightly
distinct phonetics, but the same meaning. As a re-
sult of this, lexical variation and homophony is very
common, and obviously deteriorates the effective-
ness of Information Access systems based on strict
term matching; hence the basic idea of this research:
to use the approximative matching enabled by self-
organizing map-based artificial neural networks.
</bodyText>
<subsectionHeader confidence="0.999984">
3.2 Test data and preprocessing
</subsectionHeader>
<bodyText confidence="0.999957459459459">
In our SOM-based experiments, a corpus of news
items was used for text classification. A main ob-
stacle to developing applications for a language like
Amharic is the scarcity of resources. No large cor-
pora for Amharic exist, but we could use a small
corpus of 206 news articles taken from the electronic
news archive of the website of the Walta Information
Center (an Ethiopian news agency). The training
corpus consisted of 101 articles collected by Saba
(Amsalu, 2001), while the test corpus consisted of
the remaining 105 documents collected by Theodros
(GebreMeskel, 2003). The documents were written
using the Amharic software VG2 Main font.
The corpus was matched against 25 queries. The
selection of documents relevant to a given query,
was made by two domain experts (two journal-
ists), one from the Monitor newspaper and the other
from the Walta Information Center. A linguist from
Gonder College participated in making consensus of
the selection of documents made by the two jour-
nalists. Only 16 of the 25 queries were judged to
have a document relevant to them in the 101 docu-
ment training corpus. These 16 queries were found
to be different enough from each other, in the con-
tent they try to address, to help map from document
collection to query contents (which were taken as
class labels). These mappings (assignment) of doc-
uments to 16 distinct classes helped to see retrieval
and classification effectiveness of the ANN model.
The corpus was preprocessed to normalize
spelling and to filter out stopwords. One prepro-
cessing step tried to solve the problems with non-
standardised spelling of compounds, and that the
same sound may be represented with two or more
distinct but redundant written forms. Due to the sys-
tematic redundancy inherited from the Ge’ez, only
about 233 of the 275 fidEls are actually necessary to
</bodyText>
<page confidence="0.996627">
74
</page>
<table confidence="0.9626178">
Sound pattern Matching Amharic characters
/S9/ ➌, �
/r9/ R, 0
/h9/ ⑨, ❷, ❍, ❑, ♣, �
/i9/ Ti, ❷, ❛, ❆
</table>
<tableCaption confidence="0.997273">
Table 2: Examples of character redundancy
</tableCaption>
<bodyText confidence="0.998462083333333">
represent Amharic. Some examples of character re-
dundancy are shown in Table 2. The different forms
were reduced to common representations.
A negative dictionary of 745 words was created,
containing both stopwords that are news specific and
the Amharic text stopwords collected by Nega (Ale-
mayehu and Willett, 2002). The news specific com-
mon terms were manually identified by looking at
their frequency. In a second preprocessing step, the
stopwords were removed from the word collection
before indexing. After the preprocessing, the num-
ber of remaining terms in the corpus was 10,363.
</bodyText>
<sectionHeader confidence="0.985812" genericHeader="method">
4 Text retrieval
</sectionHeader>
<bodyText confidence="0.99999392">
In a set of experiments we investigated the devel-
opment of a retrieval system using Self-Organizing
Maps. The term-by-document matrix produced
from the entire collection of 206 documents was
used to measure the retrieval performance of the sys-
tem, of which 101 documents were used for train-
ing and the remaining for testing. After the prepro-
cessing described in the previous section, a weighted
matrix was generated from the original matrix using
the log-entropy weighting formula (Dumais, 1991).
This helps to enhance the occurrence of a term in
representing a particular document and to degrade
the occurrence of the term in the document col-
lection. The weighted matrix can then be dimen-
sionally reduced by Singular Value Decomposition,
SVD (Berry et al., 1995). SVD makes it possible to
map individual terms to the concept space.
A query of variable size is useful for compar-
ison (when similarity measures are used) only if
its size is matrix-multiplication-compatible with the
documents. The pseudo-query must result from the
global weight obtained in weighing the original ma-
trix to be of any use in ranking relevant documents.
The experiment was carried out in two versions, with
the original vector space and with a reduced one.
</bodyText>
<subsectionHeader confidence="0.996324">
4.1 Clustering in unreduced vector space
</subsectionHeader>
<bodyText confidence="0.99963935483871">
In the first experiment, the selected documents were
indexed using 10,363 dimensional vectors (i.e., one
dimension per term in the corpus) weighted using
log-entropy weighting techniques. These vectors
were fed into an Artificial Neural Network that was
created using a SOM lattice structure for mapping
on a two-dimensional grid. Thereafter a query and
101 documents were fed into the ANN to see how
documents cluster around the query.
For the original, unnormalised (unreduced,
10,363 dimension) vector space we did not try to
train an ANN model for more than 5,000 epochs
(which takes weeks), given that the network perfor-
mance in any case was very bad, and that the net-
work for the reduced vector space had its apex at
that point (as discussed below).
Those documents on the node on which the sin-
gle query lies and those documents in the imme-
diate vicinity of it were taken as being relevant to
the query (the neighbourhood was defined to be six
nodes). Ranking of documents was performed using
the cosine similarity measure, on the single query
versus automatically retrieved relevant documents.
The eleven-point average precision was calculated
over all queries. For this system the average preci-
sion on the test set turned out to be 10.5%, as can be
seen in the second column of Table 3.
The table compares the results on training on the
original vector space to the very much improved
ones obtained by the ANN model trained on the re-
duced vector space, described in the next section.
</bodyText>
<table confidence="0.995348461538461">
Recall Original vector Reduced vector
0.00 0.2080 0.8311
0.10 0.1986 0.7621
0.20 0.1896 0.7420
0.30 0.1728 0.7010
0.40 0.0991 0.6888
0.50 0.0790 0.6546
0.60 0.0678 0.5939
0.70 0.0543 0.5300
0.80 0.0403 0.4789
0.90 0.0340 0.3440
1.00 0.0141 0.2710
Average 0.1052 0.5998
</table>
<tableCaption confidence="0.999023">
Table 3: Eleven-point precision for 16 queries
</tableCaption>
<page confidence="0.997713">
75
</page>
<subsectionHeader confidence="0.99075">
4.2 Clustering in SVD-reduced vector space
</subsectionHeader>
<bodyText confidence="0.999944515151515">
In a second experiment, vectors of numerically in-
dexed documents were converted to weighted matri-
ces and further reduced using SVD, to infer the need
for representing co-occurrence of words in identify-
ing a document. The reduced vector space of 101
pseudo-documents was fed into the neural net for
training. Then, a query together with 105 documents
was given to the trained neural net for simulation and
inference purpose.
For the reduced vectors a wider range of values
could be tried. Thus 100, 200, ... , 1000 epochs
were tried at the beginning of the experiment. The
network performance kept improving and the train-
ing was then allowed to go on for 2000, 3000,
... , 10,000, 20,000 epochs thereafter. The average
classification accuracy was at an apex after 5,000
epochs, as can been seen in Figure 1.
The neural net with the highest accuracy was se-
lected for further analysis. As in the previous model,
documents in the vicinity of the query were ranked
using the cosine similarity measure and the precision
on the test set is illustrated in the third column of Ta-
ble 3. As can be seen in the table, this system was
effective with 60.0% eleven-point average precision
on the test set (each of the 16 queries was tested).
Thus, the performance of the reduced vector
space system was very much better than that ob-
tained using the test set of the normal term docu-
ment matrix that resulted in only 10.5% average pre-
cision. In both cases, the precision of the training set
was assessed using the classification accuracy which
shows how documents with similar features cluster
together (occur on the same or neighbouring nodes).
</bodyText>
<figure confidence="0.94568325">
�
70
65
60
55
50
0 5 10 15 20
Epochs (*103)
</figure>
<figureCaption confidence="0.999872">
Figure 1: Average network classification accuracy
</figureCaption>
<sectionHeader confidence="0.955555" genericHeader="method">
5 Document Classification
</sectionHeader>
<bodyText confidence="0.992770466666667">
In a third experiment, the SVD-reduced vector space
of pseudo-documents was assigned a class label
(query content) to which the documents of the train-
ing set were identified to be more similar (by ex-
perts) and the neural net was trained using the
pseudo-documents and their target classes. This was
performed for 100 to 20,000 epochs and the neural
net with best accuracy was considered for testing.
The average precision on the training set was
found to be 72.8%, while the performance of the
neural net on the test set was 69.5%. A matrix of
simple queries merged with the 101 documents (that
had been used for training) was taken as input to
a SOM-model neural net and eventually, the 101-
dimensional document and single query pairs were
mapped and plotted onto a two-dimensional space.
Figure 2 gives a flavour of the document clustering.
The results of this experiment are compatible with
those of Theodros (GebreMeskel, 2003) who used
the standard vector space model and latent semantic
indexing for text categorization. He reports that the
vector space model gave a precision of 69.1% on the
training set. LSI improved the precision to 71.6%,
which still is somewhat lower than the 72.8% ob-
tained by the SOM model in our experiments. Go-
ing outside Amharic, the results can be compared to
the ones reported by Cai and Hofmann (2003) on the
Reuters-21578 corpus5 which contains 21,578 clas-
sified documents (100 times the documents available
for Amharic). Used an LSI approach they obtained
document average precision figures of 88–90%.
In order to locate the error sources in our exper-
iments, the documents missed by the SOM-based
classifier (documents that were supposed to be clus-
tered on a given class label, but were not found un-
der that label), were examined. The documents that
were rejected as irrelevant by the ANN using re-
duced dimension vector space were found to contain
only a line or two of interest to the query (for the
training set as well as for the test set). Also within
the test set as well as in the training set some relevant
documents had been missed for unclear reasons.
Those documents that had been retrieved as rel-
evant to a query without actually having any rele-
vance to that query had some words that co-occur
</bodyText>
<footnote confidence="0.997407">
5Available at www.daviddlewis.com/resources
</footnote>
<page confidence="0.97419">
76
</page>
<figureCaption confidence="0.999402">
Figure 2: Document clustering at different neuron positions
</figureCaption>
<bodyText confidence="0.9998772">
with the words of the relevant documents. Very im-
portant in this observation was that documents that
could be of some interest to two classes were found
at nodes that are the intersection of the nodes con-
taining the document sets of the two classes.
</bodyText>
<sectionHeader confidence="0.993815" genericHeader="evaluation">
6 Summary and Conclusions
</sectionHeader>
<bodyText confidence="0.999987869565217">
A set of experiments investigated text retrieval of se-
lected Amharic news items using Self-Organizing
Maps, an unsupervised learning neural network
method. 101 training set items, 25 queries, and 105
test set items were selected. The content of each
news item was taken as the basis for document in-
dexing, and the content of the specific query was
taken for query indexing. A term–document ma-
trix was generated and the occurrence of terms per
document was registered. This original matrix was
changed to a weighted matrix using the log-entropy
scheme. The weighted matrix was further reduced
using SVD. The length of the query vector was also
reduced using the global weight vector obtained in
weighing the original matrix.
The ANN model using unnormalised vector space
had a precision of 10.5%, whereas the best ANN
model using reduced dimensional vector space per-
formed at a 60.0% level for the test set. For this con-
figuration we also tried to classify the data around a
query content, taken that query as class label. The
results obtained then were 72.8% for the training set
and 69.5% for the test set, which is encouraging.
</bodyText>
<sectionHeader confidence="0.999139" genericHeader="conclusions">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99269575">
Thanks to Dr. Gashaw Kebede, Kibur Lisanu, Lars
Asker, Lemma Nigussie, and Mesfin Getachew; and
to Atelach Alemu for spotting some nasty bugs.
The work was partially funded by the Faculty of
Informatics at Addis Ababa University and the ICT
support programme of SAREC, the Department for
Research Cooperation at Sida, the Swedish Inter-
national Development Cooperation Agency.
</bodyText>
<page confidence="0.998112">
77
</page>
<sectionHeader confidence="0.989399" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99880459">
Nega Alemayehu and Peter Willett. 2002. Stemming of
Amharic words for information retrieval. Literary and
Linguistic Computing, 17(1):1–17.
Atelach Alemu, Lars Asker, and Gunnar Eriksson.
2003a. An empirical approach to building an Amharic
treebank. In Proc. 2nd Workshop on Treebanks and
Linguistic Theories, V¨axj¨o University, Sweden.
Atelach Alemu, Lars Asker, and Mesfin Getachew.
2003b. Natural language processing for Amharic:
Overview and suggestions for a way forward. In Proc.
10th Conf. Traitement Automatique des Langues Na-
turelles, Batz-sur-Mer, France, pp. 173–182.
Atelach Alemu, Lars Asker, Rickard C¨oster, and Jussi
Karlgren. 2004. Dictionary-based Amharic–English
information retrieval. In 5th Workshop of the Cross
Language Evaluation Forum, Bath, England.
Saba Amsalu. 2001. The application of information re-
trieval techniques to Amharic. MSc Thesis, School of
Information Studies for Africa, Addis Ababa Univer-
sity, Ethiopia.
Marvin Bender, Sydney Head, and Roger Cowley. 1976.
The Ethiopian writing system. In Bender et al., eds,
Language in Ethiopia. Oxford University Press.
Michael Berry, Susan Dumais, and Gawin O’Brien.
1995. Using linear algebra for intelligent information
retrieval. SIAMReview, 37(4):573–595.
Thomas Bloor. 1995. The Ethiopic writing system: a
profile. Journal of the Simplified Spelling Society,
19:30–36.
Lijuan Cai and Thomas Hofmann. 2003. Text catego-
rization by boosting automatically extracted concepts.
In Proc. 26th Int. Conf. Research and Development in
Information Retrieval, pp. 182–189, Toronto, Canada.
John Cowell and Fiaz Hussain. 2003. Amharic character
recognition using a fast signature based algorithm. In
Proc. 7th Int. Conf. Image Visualization, pp. 384–389,
London, England.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal
of the American Society for Information Science,
41(6):391–407.
Susan Dumais. 1991. Improving the retrieval of informa-
tion from external sources. Behavior Research Meth-
ods, Instruments and Computers, 23(2):229–236.
Sisay Fissaha and Johann Haller. 2003. Application of
corpus-based techniques to Amharic texts. In Proc.
MT Summit IX Workshop on Machine Translation for
Semitic Languages, New Orleans, Louisana.
Jane Furzey. 1996. Enpowering socio-economic devel-
opment in Africa utilizing information technology. A
country study for the United Nations Economic Com-
mission for Africa, University of Pennsylvania.
Theodros GebreMeskel. 2003. Amharic text retrieval:
An experiment using latent semantic indexing (LSI)
with singular value decomposition (SVD). MSc The-
sis, School of Information Studies for Africa, Addis
Ababa University, Ethiopia.
Timo Honkela, Samuel Kaski, Krista Lagus, and Teuvo
Kohonen. 1997. WEBSOM — Self-Organizing Maps
of document collections. In Proc. Workshop on Self-
Organizing Maps, pp. 310–315, Espoo, Finland.
Teuvo Kohonen, Samuel Kaski, Krista Lagus, Jarkko
Saloj¨arvi, Jukka Honkela, Vesa Paatero, and Antti
Saarela. 2000. Self organization of a massive doc-
ument collection. IEEE Transactions on Neural Net-
works, 11(3):574–585.
Teuvo Kohonen. 1999. Self-Organization and Associa-
tive Memory. Springer, 3 edition.
Xia Lin, Dagobert Soergel, and Gary Marchionini. 1991.
A self-organizing semantic map for information re-
trieval. In Proc. 14th Int. Conf. Research and Develop-
ment in Information Retrieval, pp. 262–269, Chicago,
Illinois.
Hwee Tou Ng, Wei Boon Goh, and Kok Leong Low.
1997. Feature selection, perceptron learning, and a us-
ability case study for text categorization. In Proc. 20th
Int. Conf. Research and Development in Information
Retrieval, pp. 67–73, Philadelphia, Pennsylvania.
Miguel Ruiz and Padmini Srinivasan. 1999. Hierarchical
neural networks for text categorization. In Proc. 22nd
Int. Conf. Research and Development in Information
Retrieval, pp. 281–282, Berkeley, California.
David Rumelhart, Geoffrey Hinton, and Ronald
Williams. 1986. Learning internal representations by
error propagation. In Rumelhart and McClelland, eds,
Parallel Distributed Processing, vol 1. MIT Press.
Gerard Salton and Michael McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Surveys,
34(1):1–47.
George Tambouratzis, N. Hairetakis, S. Markantonatou,
and G. Carayannis. 2003. Applying the SOM model
to text classification according to register and stylistic
content. Int. Journal ofNeural Systems, 13(1):1–11.
Daniel Yacob. 2005. Developments towards an elec-
tronic Amharic corpus. In Proc. TALN 12 Workshop
on NLP for Under-Resourced Languages, Dourdan,
France, June (to appear).
</reference>
<page confidence="0.998813">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.759976">
<title confidence="0.99989">Classifying Amharic News Text Using Self-Organizing Maps</title>
<author confidence="0.999425">Eyassu Bj¨orn</author>
<affiliation confidence="0.999293">Department of Information Science Swedish Institute of Computer Science</affiliation>
<address confidence="0.807815">Addis Ababa University, Ethiopia Box 1263, SE–164 29 Kista, Sweden</address>
<email confidence="0.951924">samueleya@yahoo.comgamback@sics.se</email>
<abstract confidence="0.9983435">The paper addresses using artificial neural networks for classification of Amharic news items. Amharic is the language for countrywide communication in Ethiopia and has its own writing system containing extensive systematic redundancy. It is quite dialectally diversified and probably representative of the languages of a continent that so far has received little attention within the language processing field. The experiments investigated document clustering around user queries using Self- Organizing Maps, an unsupervised learning neural network strategy. The best ANN model showed a precision of 60.0% when trying to cluster unseen data, and a 69.5% precision when trying to classify it.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nega Alemayehu</author>
<author>Peter Willett</author>
</authors>
<title>Stemming of Amharic words for information retrieval.</title>
<date>2002</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>17--1</pages>
<contexts>
<context position="17844" citStr="Alemayehu and Willett, 2002" startWordPosition="2867" endWordPosition="2871">two or more distinct but redundant written forms. Due to the systematic redundancy inherited from the Ge’ez, only about 233 of the 275 fidEls are actually necessary to 74 Sound pattern Matching Amharic characters /S9/ , � /r9/ R, 0 /h9/ , , ❍, ❑, ♣, � /i9/ Ti, , ❛, ❆ Table 2: Examples of character redundancy represent Amharic. Some examples of character redundancy are shown in Table 2. The different forms were reduced to common representations. A negative dictionary of 745 words was created, containing both stopwords that are news specific and the Amharic text stopwords collected by Nega (Alemayehu and Willett, 2002). The news specific common terms were manually identified by looking at their frequency. In a second preprocessing step, the stopwords were removed from the word collection before indexing. After the preprocessing, the number of remaining terms in the corpus was 10,363. 4 Text retrieval In a set of experiments we investigated the development of a retrieval system using Self-Organizing Maps. The term-by-document matrix produced from the entire collection of 206 documents was used to measure the retrieval performance of the system, of which 101 documents were used for training and the remaining </context>
</contexts>
<marker>Alemayehu, Willett, 2002</marker>
<rawString>Nega Alemayehu and Peter Willett. 2002. Stemming of Amharic words for information retrieval. Literary and Linguistic Computing, 17(1):1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atelach Alemu</author>
<author>Lars Asker</author>
<author>Gunnar Eriksson</author>
</authors>
<title>An empirical approach to building an Amharic treebank.</title>
<date>2003</date>
<booktitle>In Proc. 2nd Workshop on Treebanks and Linguistic Theories,</booktitle>
<institution>V¨axj¨o University, Sweden.</institution>
<marker>Alemu, Asker, Eriksson, 2003</marker>
<rawString>Atelach Alemu, Lars Asker, and Gunnar Eriksson. 2003a. An empirical approach to building an Amharic treebank. In Proc. 2nd Workshop on Treebanks and Linguistic Theories, V¨axj¨o University, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atelach Alemu</author>
<author>Lars Asker</author>
<author>Mesfin Getachew</author>
</authors>
<title>Natural language processing for Amharic: Overview and suggestions for a way forward.</title>
<date>2003</date>
<booktitle>In Proc. 10th Conf. Traitement Automatique des Langues Naturelles,</booktitle>
<pages>173--182</pages>
<location>Batz-sur-Mer, France,</location>
<marker>Alemu, Asker, Getachew, 2003</marker>
<rawString>Atelach Alemu, Lars Asker, and Mesfin Getachew. 2003b. Natural language processing for Amharic: Overview and suggestions for a way forward. In Proc. 10th Conf. Traitement Automatique des Langues Naturelles, Batz-sur-Mer, France, pp. 173–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atelach Alemu</author>
<author>Lars Asker</author>
<author>Rickard C¨oster</author>
<author>Jussi Karlgren</author>
</authors>
<title>Dictionary-based Amharic–English information retrieval.</title>
<date>2004</date>
<booktitle>In 5th Workshop of the Cross Language Evaluation Forum,</booktitle>
<location>Bath, England.</location>
<marker>Alemu, Asker, C¨oster, Karlgren, 2004</marker>
<rawString>Atelach Alemu, Lars Asker, Rickard C¨oster, and Jussi Karlgren. 2004. Dictionary-based Amharic–English information retrieval. In 5th Workshop of the Cross Language Evaluation Forum, Bath, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saba Amsalu</author>
</authors>
<title>The application of information retrieval techniques to Amharic. MSc Thesis,</title>
<date>2001</date>
<institution>School of Information Studies for Africa, Addis Ababa University,</institution>
<contexts>
<context position="16022" citStr="Amsalu, 2001" startWordPosition="2564" endWordPosition="2565">his research: to use the approximative matching enabled by selforganizing map-based artificial neural networks. 3.2 Test data and preprocessing In our SOM-based experiments, a corpus of news items was used for text classification. A main obstacle to developing applications for a language like Amharic is the scarcity of resources. No large corpora for Amharic exist, but we could use a small corpus of 206 news articles taken from the electronic news archive of the website of the Walta Information Center (an Ethiopian news agency). The training corpus consisted of 101 articles collected by Saba (Amsalu, 2001), while the test corpus consisted of the remaining 105 documents collected by Theodros (GebreMeskel, 2003). The documents were written using the Amharic software VG2 Main font. The corpus was matched against 25 queries. The selection of documents relevant to a given query, was made by two domain experts (two journalists), one from the Monitor newspaper and the other from the Walta Information Center. A linguist from Gonder College participated in making consensus of the selection of documents made by the two journalists. Only 16 of the 25 queries were judged to have a document relevant to them</context>
</contexts>
<marker>Amsalu, 2001</marker>
<rawString>Saba Amsalu. 2001. The application of information retrieval techniques to Amharic. MSc Thesis, School of Information Studies for Africa, Addis Ababa University, Ethiopia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marvin Bender</author>
<author>Sydney Head</author>
<author>Roger Cowley</author>
</authors>
<title>The Ethiopian writing system.</title>
<date>1976</date>
<booktitle>In Bender et al., eds, Language in Ethiopia.</booktitle>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="14391" citStr="Bender et al. (1976)" startWordPosition="2303" endWordPosition="2306">velars (four five-character orders) and 24 for other labialisation. In total, there are 275 fidEls. The sequences in Table 1 (for n and P) exemplify the (partial) symmetry of vowel indicators. Amharic also has its own numbers (twenty symbols, though not widely used nowadays) and its own punctuation system with eight symbols, where the space between words looks like a colon:,while the full stop, comma and semicolon are , : and 1. The question and exclamation marks have recently been included in the writing system. For more thorough discussions of the Ethiopian writing system, see, for example, Bender et al. (1976) and Bloor (1995). Amharic words have consonantal roots with vowel variation expressing difference in interpretation, making stemming a not-so-useful technique in information retrieval (no full morphological analyser for the language is available yet). There is no agreed upon spelling standard for compounds and the writing system uses multitudes of ways to denote compound words. In addition, not all the letters of the Amharic script are strictly necessary for the pronunciation patterns of the language; some were simply inherited from Ge’ez without having any semantic or phonetic distinction in</context>
</contexts>
<marker>Bender, Head, Cowley, 1976</marker>
<rawString>Marvin Bender, Sydney Head, and Roger Cowley. 1976. The Ethiopian writing system. In Bender et al., eds, Language in Ethiopia. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Berry</author>
<author>Susan Dumais</author>
<author>Gawin O’Brien</author>
</authors>
<title>Using linear algebra for intelligent information retrieval.</title>
<date>1995</date>
<journal>SIAMReview,</journal>
<volume>37</volume>
<issue>4</issue>
<marker>Berry, Dumais, O’Brien, 1995</marker>
<rawString>Michael Berry, Susan Dumais, and Gawin O’Brien. 1995. Using linear algebra for intelligent information retrieval. SIAMReview, 37(4):573–595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Bloor</author>
</authors>
<title>The Ethiopic writing system: a profile.</title>
<date>1995</date>
<journal>Journal of the Simplified Spelling Society,</journal>
<pages>19--30</pages>
<contexts>
<context position="11410" citStr="Bloor, 1995" startWordPosition="1795" endWordPosition="1796">ose generated by statistical methods. 3 Processing Amharic Ethiopia with some 70 million inhabitants is the third most populous African country and harbours more than 80 different languages.3 Three of these are dominant: Oromo, a Cushitic language spoken in the South and Central parts of the country and written using the Latin alphabet; Tigrinya, spoken in the North and in neighbouring Eritrea; and Amharic, spoken in most parts of the country, but predominantly in the Eastern, Western, and Central regions. Both Amharic and Tigrinya are Semitic and about as close as are Spanish and Portuguese (Bloor, 1995), 3.1 The Amharic language and script Already a census from 19944 estimated Amharic to be mother tongue of more than 17 million people, with at least an additional 5 million second language speakers. It is today probably the second largest language in Ethiopia (after Oromo). The Constitution of 1994 divided Ethiopia into nine fairly independent regions, each with its own nationality language. However, Amharic is the language for countrywide communication and was also for a long period the principal literal language and medium of instruction in primary and secondary schools in the country, whil</context>
<context position="14408" citStr="Bloor (1995)" startWordPosition="2308" endWordPosition="2309">er orders) and 24 for other labialisation. In total, there are 275 fidEls. The sequences in Table 1 (for n and P) exemplify the (partial) symmetry of vowel indicators. Amharic also has its own numbers (twenty symbols, though not widely used nowadays) and its own punctuation system with eight symbols, where the space between words looks like a colon:,while the full stop, comma and semicolon are , : and 1. The question and exclamation marks have recently been included in the writing system. For more thorough discussions of the Ethiopian writing system, see, for example, Bender et al. (1976) and Bloor (1995). Amharic words have consonantal roots with vowel variation expressing difference in interpretation, making stemming a not-so-useful technique in information retrieval (no full morphological analyser for the language is available yet). There is no agreed upon spelling standard for compounds and the writing system uses multitudes of ways to denote compound words. In addition, not all the letters of the Amharic script are strictly necessary for the pronunciation patterns of the language; some were simply inherited from Ge’ez without having any semantic or phonetic distinction in modern Amharic: </context>
</contexts>
<marker>Bloor, 1995</marker>
<rawString>Thomas Bloor. 1995. The Ethiopic writing system: a profile. Journal of the Simplified Spelling Society, 19:30–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijuan Cai</author>
<author>Thomas Hofmann</author>
</authors>
<title>Text categorization by boosting automatically extracted concepts.</title>
<date>2003</date>
<booktitle>In Proc. 26th Int. Conf. Research and Development in Information Retrieval,</booktitle>
<pages>182--189</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="24351" citStr="Cai and Hofmann (2003)" startWordPosition="3960" endWordPosition="3963"> single query pairs were mapped and plotted onto a two-dimensional space. Figure 2 gives a flavour of the document clustering. The results of this experiment are compatible with those of Theodros (GebreMeskel, 2003) who used the standard vector space model and latent semantic indexing for text categorization. He reports that the vector space model gave a precision of 69.1% on the training set. LSI improved the precision to 71.6%, which still is somewhat lower than the 72.8% obtained by the SOM model in our experiments. Going outside Amharic, the results can be compared to the ones reported by Cai and Hofmann (2003) on the Reuters-21578 corpus5 which contains 21,578 classified documents (100 times the documents available for Amharic). Used an LSI approach they obtained document average precision figures of 88–90%. In order to locate the error sources in our experiments, the documents missed by the SOM-based classifier (documents that were supposed to be clustered on a given class label, but were not found under that label), were examined. The documents that were rejected as irrelevant by the ANN using reduced dimension vector space were found to contain only a line or two of interest to the query (for th</context>
</contexts>
<marker>Cai, Hofmann, 2003</marker>
<rawString>Lijuan Cai and Thomas Hofmann. 2003. Text categorization by boosting automatically extracted concepts. In Proc. 26th Int. Conf. Research and Development in Information Retrieval, pp. 182–189, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Cowell</author>
<author>Fiaz Hussain</author>
</authors>
<title>Amharic character recognition using a fast signature based algorithm.</title>
<date>2003</date>
<booktitle>In Proc. 7th Int. Conf. Image Visualization,</booktitle>
<pages>384--389</pages>
<location>London, England.</location>
<contexts>
<context position="4405" citStr="Cowell and Hussain (2003)" startWordPosition="668" endWordPosition="671">al information access and storage facilities must be created. To this end, some work has now been carried out, mainly by Ethiopian Telecom, the Ethiopian Science and Technology Commission, Addis Ababa University, the Ge’ez Frontier Foundation, and Ethiopian students abroad. So have, for example, Sisay and Haller (2003) looked at Amharic word formation and lexicon building; Nega and Willett (2002) at stemming; Atelach et al. (2003a) at treebank building; Daniel (Yacob, 2005) at the collection of an (untagged) corpus, tentatively to be hosted by Oxford University’s Open Archives Initiative; and Cowell and Hussain (2003) at character recognition.2 See Atelach et al. (2003b) for an overview of the efforts that have been made so far to develop language processing tools for Amharic. The need for investigating Amharic information access has been acknowledged by the European Cross-Language Evaluation Forum, which added an Amharic–English track in 2004. However, the task addressed was for accessing an English database in English, with only the original questions being posed in Amharic (and then translated into English). Three groups participated in this track, with Atelach et al. (2004) reporting the best results. </context>
</contexts>
<marker>Cowell, Hussain, 2003</marker>
<rawString>John Cowell and Fiaz Hussain. 2003. Amharic character recognition using a fast signature based algorithm. In Proc. 7th Int. Conf. Image Visualization, pp. 384–389, London, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>George Furnas</author>
<author>Thomas Landauer</author>
<author>Richard Harshman</author>
</authors>
<date>1990</date>
<contexts>
<context position="9868" citStr="Deerwester et al., 1990" startWordPosition="1543" endWordPosition="1547">WEBSOM architecture developed at Helsinki University of Technology (Honkela et al., 1997; Kohonen et al., 2000). WEBSOM is based on a hierarchical twolevel SOM structure, with the first level forming histogram clusters of words. The second level is used to reduce the sensitivity of the histogram to small variations in document content and performs further clustering to display the document pattern space. A Self-Organizing Map is capable of simulating new data sets without the need of retraining itself when the database is updated; something which is not true for Latent Semantic Indexing, LSI (Deerwester et al., 1990). Moreover, LSI consumes ample time in calculating similarities of new queries against all documents, but a SOM only needs to calculate similarities versus some representative subset of old input data and can then map new input straight onto the most similar models without having to recompute the whole mapping. The SOM model preparation passes through the processes undertaken by the LSI model and the classical vector space model (Salton and McGill, 1983). Hence those models can be taken as particular cases of the SOM, when the neighbourhood diameter is maximized. For instance, one can calculat</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan Dumais, George Furnas, Thomas Landauer, and Richard Harshman. 1990.</rawString>
</citation>
<citation valid="false">
<title>Indexing by latent semantic analysis.</title>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<marker></marker>
<rawString>Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Dumais</author>
</authors>
<title>Improving the retrieval of information from external sources.</title>
<date>1991</date>
<journal>Behavior Research Methods, Instruments and Computers,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="18627" citStr="Dumais, 1991" startWordPosition="2994" endWordPosition="2995">on before indexing. After the preprocessing, the number of remaining terms in the corpus was 10,363. 4 Text retrieval In a set of experiments we investigated the development of a retrieval system using Self-Organizing Maps. The term-by-document matrix produced from the entire collection of 206 documents was used to measure the retrieval performance of the system, of which 101 documents were used for training and the remaining for testing. After the preprocessing described in the previous section, a weighted matrix was generated from the original matrix using the log-entropy weighting formula (Dumais, 1991). This helps to enhance the occurrence of a term in representing a particular document and to degrade the occurrence of the term in the document collection. The weighted matrix can then be dimensionally reduced by Singular Value Decomposition, SVD (Berry et al., 1995). SVD makes it possible to map individual terms to the concept space. A query of variable size is useful for comparison (when similarity measures are used) only if its size is matrix-multiplication-compatible with the documents. The pseudo-query must result from the global weight obtained in weighing the original matrix to be of a</context>
</contexts>
<marker>Dumais, 1991</marker>
<rawString>Susan Dumais. 1991. Improving the retrieval of information from external sources. Behavior Research Methods, Instruments and Computers, 23(2):229–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sisay Fissaha</author>
<author>Johann Haller</author>
</authors>
<title>Application of corpus-based techniques to Amharic texts.</title>
<date>2003</date>
<booktitle>In Proc. MT Summit IX Workshop on Machine Translation for Semitic Languages,</booktitle>
<location>New Orleans, Louisana.</location>
<marker>Fissaha, Haller, 2003</marker>
<rawString>Sisay Fissaha and Johann Haller. 2003. Application of corpus-based techniques to Amharic texts. In Proc. MT Summit IX Workshop on Machine Translation for Semitic Languages, New Orleans, Louisana.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Furzey</author>
</authors>
<title>Enpowering socio-economic development in Africa utilizing information technology. A country study for the United Nations Economic Commission for Africa,</title>
<date>1996</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3290" citStr="Furzey, 1996" startWordPosition="504" endWordPosition="505"> information exchange and dissemination has been established. Different factors are attributed to this, including lack of digital library facilities and central resource sites, inadequate resources for electronic publication of journals and books, and poor documentation and archive collections. The difficulties to access information have led to low expectations and under-utilization of existing information resources, even though the need for accurate and fast information access is acknowledged as a major factor affecting the success and quality of research and development, trade and industry (Furzey, 1996). 1An international standard for Amharic was agreed on only in year 1998, following Amendment 10 to ISO–10646–1. The standard was finally incorporated into Unicode in year 2000: www.unicode.org/charts/PDF/U1200.pdf Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 71–78, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics In recent years this has lead to an increasing awareness that Amharic language processing resources and digital information access and storage facilities must be created. To this end, some work has now been carried out,</context>
</contexts>
<marker>Furzey, 1996</marker>
<rawString>Jane Furzey. 1996. Enpowering socio-economic development in Africa utilizing information technology. A country study for the United Nations Economic Commission for Africa, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theodros GebreMeskel</author>
</authors>
<title>Amharic text retrieval: An experiment using latent semantic indexing (LSI) with singular value decomposition (SVD). MSc Thesis,</title>
<date>2003</date>
<institution>School of Information Studies for Africa, Addis Ababa University,</institution>
<contexts>
<context position="16128" citStr="GebreMeskel, 2003" startWordPosition="2579" endWordPosition="2580"> networks. 3.2 Test data and preprocessing In our SOM-based experiments, a corpus of news items was used for text classification. A main obstacle to developing applications for a language like Amharic is the scarcity of resources. No large corpora for Amharic exist, but we could use a small corpus of 206 news articles taken from the electronic news archive of the website of the Walta Information Center (an Ethiopian news agency). The training corpus consisted of 101 articles collected by Saba (Amsalu, 2001), while the test corpus consisted of the remaining 105 documents collected by Theodros (GebreMeskel, 2003). The documents were written using the Amharic software VG2 Main font. The corpus was matched against 25 queries. The selection of documents relevant to a given query, was made by two domain experts (two journalists), one from the Monitor newspaper and the other from the Walta Information Center. A linguist from Gonder College participated in making consensus of the selection of documents made by the two journalists. Only 16 of the 25 queries were judged to have a document relevant to them in the 101 document training corpus. These 16 queries were found to be different enough from each other, </context>
<context position="23944" citStr="GebreMeskel, 2003" startWordPosition="3891" endWordPosition="3892">00 to 20,000 epochs and the neural net with best accuracy was considered for testing. The average precision on the training set was found to be 72.8%, while the performance of the neural net on the test set was 69.5%. A matrix of simple queries merged with the 101 documents (that had been used for training) was taken as input to a SOM-model neural net and eventually, the 101- dimensional document and single query pairs were mapped and plotted onto a two-dimensional space. Figure 2 gives a flavour of the document clustering. The results of this experiment are compatible with those of Theodros (GebreMeskel, 2003) who used the standard vector space model and latent semantic indexing for text categorization. He reports that the vector space model gave a precision of 69.1% on the training set. LSI improved the precision to 71.6%, which still is somewhat lower than the 72.8% obtained by the SOM model in our experiments. Going outside Amharic, the results can be compared to the ones reported by Cai and Hofmann (2003) on the Reuters-21578 corpus5 which contains 21,578 classified documents (100 times the documents available for Amharic). Used an LSI approach they obtained document average precision figures o</context>
</contexts>
<marker>GebreMeskel, 2003</marker>
<rawString>Theodros GebreMeskel. 2003. Amharic text retrieval: An experiment using latent semantic indexing (LSI) with singular value decomposition (SVD). MSc Thesis, School of Information Studies for Africa, Addis Ababa University, Ethiopia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Honkela</author>
<author>Samuel Kaski</author>
<author>Krista Lagus</author>
<author>Teuvo Kohonen</author>
</authors>
<title>WEBSOM — Self-Organizing Maps of document collections.</title>
<date>1997</date>
<booktitle>In Proc. Workshop on SelfOrganizing Maps,</booktitle>
<pages>310--315</pages>
<location>Espoo, Finland.</location>
<contexts>
<context position="9332" citStr="Honkela et al., 1997" startWordPosition="1456" endWordPosition="1459">e not been shown to add any performance to linear ones for text categorization (Sebastiani, 2002). SOMs have been used for information access since the beginning of the 90s (Lin et al., 1991). A SOM may show how documents with similar features cluster together by projecting the N-dimensional vector space onto a two-dimensional grid. The radius of neighbouring nodes may be varied to include documents that are weaker related. The most elaborate experiments of using SOMs for document classification have been undertaken using the WEBSOM architecture developed at Helsinki University of Technology (Honkela et al., 1997; Kohonen et al., 2000). WEBSOM is based on a hierarchical twolevel SOM structure, with the first level forming histogram clusters of words. The second level is used to reduce the sensitivity of the histogram to small variations in document content and performs further clustering to display the document pattern space. A Self-Organizing Map is capable of simulating new data sets without the need of retraining itself when the database is updated; something which is not true for Latent Semantic Indexing, LSI (Deerwester et al., 1990). Moreover, LSI consumes ample time in calculating similarities </context>
</contexts>
<marker>Honkela, Kaski, Lagus, Kohonen, 1997</marker>
<rawString>Timo Honkela, Samuel Kaski, Krista Lagus, and Teuvo Kohonen. 1997. WEBSOM — Self-Organizing Maps of document collections. In Proc. Workshop on SelfOrganizing Maps, pp. 310–315, Espoo, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teuvo Kohonen</author>
<author>Samuel Kaski</author>
</authors>
<title>Krista Lagus, Jarkko Saloj¨arvi, Jukka Honkela, Vesa Paatero, and Antti Saarela.</title>
<date>2000</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>11</volume>
<issue>3</issue>
<marker>Kohonen, Kaski, 2000</marker>
<rawString>Teuvo Kohonen, Samuel Kaski, Krista Lagus, Jarkko Saloj¨arvi, Jukka Honkela, Vesa Paatero, and Antti Saarela. 2000. Self organization of a massive document collection. IEEE Transactions on Neural Networks, 11(3):574–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teuvo Kohonen</author>
</authors>
<title>Self-Organization and Associative Memory.</title>
<date>1999</date>
<volume>3</volume>
<pages>edition.</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="7158" citStr="Kohonen (1999)" startWordPosition="1112" endWordPosition="1113">xtract patterns and detect trends that are too complex to be noticed by either humans or other computational and statistical techniques. Traditionally, the most common ANN setup has been the backpropagation architecture (Rumelhart et al., 1986), a supervised learning strategy where input data is fed forward in the network to the output nodes (normally with an intermediate hidden layer of nodes) while errors in matches are propagated backwards in the net during training. 2.1 Self-Organizing Maps Self-Organizing Maps (SOM) is an unsupervised learning scheme neural network, which was invented by Kohonen (1999). It was originally developed to project multi-dimensional vectors on a reduced dimensional space. Self-organizing systems can have many kinds of structures, a common one consists of an input layer and an output layer, with feed-forward connections from input to output layers and full connectivity (connections between all neurons) in the output layer. A SOM is provided with a set of rules of a local nature (a signal affects neurons in the immediate vicinity of the current neuron), enabling it to learn to compute an input-output pairing with specific desirable properties. The learning process c</context>
</contexts>
<marker>Kohonen, 1999</marker>
<rawString>Teuvo Kohonen. 1999. Self-Organization and Associative Memory. Springer, 3 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xia Lin</author>
<author>Dagobert Soergel</author>
<author>Gary Marchionini</author>
</authors>
<title>A self-organizing semantic map for information retrieval.</title>
<date>1991</date>
<booktitle>In Proc. 14th Int. Conf. Research and Development in Information Retrieval,</booktitle>
<pages>262--269</pages>
<location>Chicago, Illinois.</location>
<contexts>
<context position="8903" citStr="Lin et al., 1991" startWordPosition="1390" endWordPosition="1393">-based text classification Neural networks have been widely used in text classification, where they can be given terms and having the output nodes represent categories. Ruiz and Srinivasan (1999) utilize an hierarchical array of backpropagation neural networks for (nonlinear) classification of MEDLINE records, while Ng et al. (1997) use the simplest (and linear) type of ANN classifier, the perceptron. Nonlinear methods have not been shown to add any performance to linear ones for text categorization (Sebastiani, 2002). SOMs have been used for information access since the beginning of the 90s (Lin et al., 1991). A SOM may show how documents with similar features cluster together by projecting the N-dimensional vector space onto a two-dimensional grid. The radius of neighbouring nodes may be varied to include documents that are weaker related. The most elaborate experiments of using SOMs for document classification have been undertaken using the WEBSOM architecture developed at Helsinki University of Technology (Honkela et al., 1997; Kohonen et al., 2000). WEBSOM is based on a hierarchical twolevel SOM structure, with the first level forming histogram clusters of words. The second level is used to re</context>
</contexts>
<marker>Lin, Soergel, Marchionini, 1991</marker>
<rawString>Xia Lin, Dagobert Soergel, and Gary Marchionini. 1991. A self-organizing semantic map for information retrieval. In Proc. 14th Int. Conf. Research and Development in Information Retrieval, pp. 262–269, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Wei Boon Goh</author>
<author>Kok Leong Low</author>
</authors>
<title>Feature selection, perceptron learning, and a usability case study for text categorization.</title>
<date>1997</date>
<booktitle>In Proc. 20th Int. Conf. Research and Development in Information Retrieval,</booktitle>
<pages>67--73</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="8620" citStr="Ng et al. (1997)" startWordPosition="1344" endWordPosition="1347">closest matching the inputs and the weights of its neighbourhood nodes are increased. At the beginning of the training the neighbourhood (where input patterns cluster depending on their similarity) can be fairly large and then be allowed to decrease over time. 72 2.2 Neural network-based text classification Neural networks have been widely used in text classification, where they can be given terms and having the output nodes represent categories. Ruiz and Srinivasan (1999) utilize an hierarchical array of backpropagation neural networks for (nonlinear) classification of MEDLINE records, while Ng et al. (1997) use the simplest (and linear) type of ANN classifier, the perceptron. Nonlinear methods have not been shown to add any performance to linear ones for text categorization (Sebastiani, 2002). SOMs have been used for information access since the beginning of the 90s (Lin et al., 1991). A SOM may show how documents with similar features cluster together by projecting the N-dimensional vector space onto a two-dimensional grid. The radius of neighbouring nodes may be varied to include documents that are weaker related. The most elaborate experiments of using SOMs for document classification have be</context>
</contexts>
<marker>Ng, Goh, Low, 1997</marker>
<rawString>Hwee Tou Ng, Wei Boon Goh, and Kok Leong Low. 1997. Feature selection, perceptron learning, and a usability case study for text categorization. In Proc. 20th Int. Conf. Research and Development in Information Retrieval, pp. 67–73, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Ruiz</author>
<author>Padmini Srinivasan</author>
</authors>
<title>Hierarchical neural networks for text categorization.</title>
<date>1999</date>
<booktitle>In Proc. 22nd Int. Conf. Research and Development in Information Retrieval,</booktitle>
<pages>281--282</pages>
<location>Berkeley, California.</location>
<contexts>
<context position="8481" citStr="Ruiz and Srinivasan (1999)" startWordPosition="1325" endWordPosition="1328"> to input (activation) patterns and in accordance to prescribed rules, until a final configuration develops. Commonly both the weights of the neuron closest matching the inputs and the weights of its neighbourhood nodes are increased. At the beginning of the training the neighbourhood (where input patterns cluster depending on their similarity) can be fairly large and then be allowed to decrease over time. 72 2.2 Neural network-based text classification Neural networks have been widely used in text classification, where they can be given terms and having the output nodes represent categories. Ruiz and Srinivasan (1999) utilize an hierarchical array of backpropagation neural networks for (nonlinear) classification of MEDLINE records, while Ng et al. (1997) use the simplest (and linear) type of ANN classifier, the perceptron. Nonlinear methods have not been shown to add any performance to linear ones for text categorization (Sebastiani, 2002). SOMs have been used for information access since the beginning of the 90s (Lin et al., 1991). A SOM may show how documents with similar features cluster together by projecting the N-dimensional vector space onto a two-dimensional grid. The radius of neighbouring nodes m</context>
</contexts>
<marker>Ruiz, Srinivasan, 1999</marker>
<rawString>Miguel Ruiz and Padmini Srinivasan. 1999. Hierarchical neural networks for text categorization. In Proc. 22nd Int. Conf. Research and Development in Information Retrieval, pp. 281–282, Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Rumelhart</author>
<author>Geoffrey Hinton</author>
<author>Ronald Williams</author>
</authors>
<title>Learning internal representations by error propagation.</title>
<date>1986</date>
<booktitle>In Rumelhart and McClelland, eds, Parallel Distributed Processing,</booktitle>
<volume>1</volume>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6788" citStr="Rumelhart et al., 1986" startWordPosition="1053" endWordPosition="1056">red by the neurological structure of the human brain, and ANN terminology borrows from neurology: the brain consists of millions of neurons connected to each other through long and thin strands called axons; the connecting points between neurons are called synapses. ANNs have proved themselves useful in deriving meaning from complicated or imprecise data; they can be used to extract patterns and detect trends that are too complex to be noticed by either humans or other computational and statistical techniques. Traditionally, the most common ANN setup has been the backpropagation architecture (Rumelhart et al., 1986), a supervised learning strategy where input data is fed forward in the network to the output nodes (normally with an intermediate hidden layer of nodes) while errors in matches are propagated backwards in the net during training. 2.1 Self-Organizing Maps Self-Organizing Maps (SOM) is an unsupervised learning scheme neural network, which was invented by Kohonen (1999). It was originally developed to project multi-dimensional vectors on a reduced dimensional space. Self-organizing systems can have many kinds of structures, a common one consists of an input layer and an output layer, with feed-f</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>David Rumelhart, Geoffrey Hinton, and Ronald Williams. 1986. Learning internal representations by error propagation. In Rumelhart and McClelland, eds, Parallel Distributed Processing, vol 1. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="10326" citStr="Salton and McGill, 1983" startWordPosition="1620" endWordPosition="1623"> data sets without the need of retraining itself when the database is updated; something which is not true for Latent Semantic Indexing, LSI (Deerwester et al., 1990). Moreover, LSI consumes ample time in calculating similarities of new queries against all documents, but a SOM only needs to calculate similarities versus some representative subset of old input data and can then map new input straight onto the most similar models without having to recompute the whole mapping. The SOM model preparation passes through the processes undertaken by the LSI model and the classical vector space model (Salton and McGill, 1983). Hence those models can be taken as particular cases of the SOM, when the neighbourhood diameter is maximized. For instance, one can calculate the LSI model’s similarity measure of documents versus queries by varying the SOM’s neighbourhood diameter, if the training set is a singular value decomposition reduced vector space. Tambouratzis et al. (2003) use SOMs for categorizing texts according to register and author style and show that the results are equivalent to those generated by statistical methods. 3 Processing Amharic Ethiopia with some 70 million inhabitants is the third most populous </context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="8809" citStr="Sebastiani, 2002" startWordPosition="1375" endWordPosition="1376">milarity) can be fairly large and then be allowed to decrease over time. 72 2.2 Neural network-based text classification Neural networks have been widely used in text classification, where they can be given terms and having the output nodes represent categories. Ruiz and Srinivasan (1999) utilize an hierarchical array of backpropagation neural networks for (nonlinear) classification of MEDLINE records, while Ng et al. (1997) use the simplest (and linear) type of ANN classifier, the perceptron. Nonlinear methods have not been shown to add any performance to linear ones for text categorization (Sebastiani, 2002). SOMs have been used for information access since the beginning of the 90s (Lin et al., 1991). A SOM may show how documents with similar features cluster together by projecting the N-dimensional vector space onto a two-dimensional grid. The radius of neighbouring nodes may be varied to include documents that are weaker related. The most elaborate experiments of using SOMs for document classification have been undertaken using the WEBSOM architecture developed at Helsinki University of Technology (Honkela et al., 1997; Kohonen et al., 2000). WEBSOM is based on a hierarchical twolevel SOM struc</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Tambouratzis</author>
<author>N Hairetakis</author>
<author>S Markantonatou</author>
<author>G Carayannis</author>
</authors>
<title>Applying the SOM model to text classification according to register and stylistic content.</title>
<date>2003</date>
<journal>Int. Journal ofNeural Systems,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="10680" citStr="Tambouratzis et al. (2003)" startWordPosition="1676" endWordPosition="1679">old input data and can then map new input straight onto the most similar models without having to recompute the whole mapping. The SOM model preparation passes through the processes undertaken by the LSI model and the classical vector space model (Salton and McGill, 1983). Hence those models can be taken as particular cases of the SOM, when the neighbourhood diameter is maximized. For instance, one can calculate the LSI model’s similarity measure of documents versus queries by varying the SOM’s neighbourhood diameter, if the training set is a singular value decomposition reduced vector space. Tambouratzis et al. (2003) use SOMs for categorizing texts according to register and author style and show that the results are equivalent to those generated by statistical methods. 3 Processing Amharic Ethiopia with some 70 million inhabitants is the third most populous African country and harbours more than 80 different languages.3 Three of these are dominant: Oromo, a Cushitic language spoken in the South and Central parts of the country and written using the Latin alphabet; Tigrinya, spoken in the North and in neighbouring Eritrea; and Amharic, spoken in most parts of the country, but predominantly in the Eastern, </context>
</contexts>
<marker>Tambouratzis, Hairetakis, Markantonatou, Carayannis, 2003</marker>
<rawString>George Tambouratzis, N. Hairetakis, S. Markantonatou, and G. Carayannis. 2003. Applying the SOM model to text classification according to register and stylistic content. Int. Journal ofNeural Systems, 13(1):1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Yacob</author>
</authors>
<title>Developments towards an electronic Amharic corpus.</title>
<date>2005</date>
<booktitle>In Proc. TALN 12 Workshop on NLP for Under-Resourced Languages,</booktitle>
<location>Dourdan, France,</location>
<note>(to appear).</note>
<contexts>
<context position="4258" citStr="Yacob, 2005" startWordPosition="646" endWordPosition="647">omputational Linguistics In recent years this has lead to an increasing awareness that Amharic language processing resources and digital information access and storage facilities must be created. To this end, some work has now been carried out, mainly by Ethiopian Telecom, the Ethiopian Science and Technology Commission, Addis Ababa University, the Ge’ez Frontier Foundation, and Ethiopian students abroad. So have, for example, Sisay and Haller (2003) looked at Amharic word formation and lexicon building; Nega and Willett (2002) at stemming; Atelach et al. (2003a) at treebank building; Daniel (Yacob, 2005) at the collection of an (untagged) corpus, tentatively to be hosted by Oxford University’s Open Archives Initiative; and Cowell and Hussain (2003) at character recognition.2 See Atelach et al. (2003b) for an overview of the efforts that have been made so far to develop language processing tools for Amharic. The need for investigating Amharic information access has been acknowledged by the European Cross-Language Evaluation Forum, which added an Amharic–English track in 2004. However, the task addressed was for accessing an English database in English, with only the original questions being po</context>
</contexts>
<marker>Yacob, 2005</marker>
<rawString>Daniel Yacob. 2005. Developments towards an electronic Amharic corpus. In Proc. TALN 12 Workshop on NLP for Under-Resourced Languages, Dourdan, France, June (to appear).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>