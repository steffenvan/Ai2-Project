<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9975535">
Dependency-based Convolutional Neural Networks
for Sentence Embedding∗
</title>
<author confidence="0.995971">
Mingbo Ma† Liang Huang† ‡
</author>
<affiliation confidence="0.982877">
†Graduate Center &amp; Queens College
City University of New York
</affiliation>
<email confidence="0.811472">
{mma2,lhuang}gc.cuny.edu
</email>
<sectionHeader confidence="0.990514" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999939533333333">
In sentence modeling and classification,
convolutional neural network approaches
have recently achieved state-of-the-art re-
sults, but all such efforts process word vec-
tors sequentially and neglect long-distance
dependencies. To combine deep learn-
ing with linguistic structures, we pro-
pose a dependency-based convolution ap-
proach, making use of tree-based n-grams
rather than surface ones, thus utlizing non-
local interactions between words. Our
model improves sequential baselines on all
four sentiment and question classification
tasks, and achieves the highest published
accuracy on TREC.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962454545455">
Convolutional neural networks (CNNs), originally
invented in computer vision (LeCun et al., 1995),
has recently attracted much attention in natural
language processing (NLP) on problems such as
sequence labeling (Collobert et al., 2011), seman-
tic parsing (Yih et al., 2014), and search query
retrieval (Shen et al., 2014). In particular, recent
work on CNN-based sentence modeling (Kalch-
brenner et al., 2014; Kim, 2014) has achieved ex-
cellent, often state-of-the-art, results on various
classification tasks such as sentiment, subjectivity,
and question-type classification. However, despite
their celebrated success, there remains a major
limitation from the linguistics perspective: CNNs,
being invented on pixel matrices in image process-
ing, only consider sequential n-grams that are con-
secutive on the surface string and neglect long-
distance dependencies, while the latter play an im-
portant role in many linguistic phenomena such as
negation, subordination, and wh-extraction, all of
which might dully affect the sentiment, subjectiv-
ity, or other categorization of the sentence.
</bodyText>
<footnote confidence="0.964722666666667">
∗ This work was done at both IBM and CUNY, and was supported in
part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank
Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions.
</footnote>
<note confidence="0.931431666666667">
Bing Xiang‡ Bowen Zhou‡
‡IBM Watson Group
T. J. Watson Research Center, IBM
</note>
<email confidence="0.841001">
{lhuang,bingxia,zhou}@us.ibm.com
</email>
<bodyText confidence="0.999608">
Indeed, in the sentiment analysis literature, re-
searchers have incorporated long-distance infor-
mation from syntactic parse trees, but the results
are somewhat inconsistent: some reported small
improvements (Gamon, 2004; Matsumoto et al.,
2005), while some otherwise (Dave et al., 2003;
Kudo and Matsumoto, 2004). As a result, syn-
tactic features have yet to become popular in the
sentiment analysis community. We suspect one
of the reasons for this is data sparsity (according
to our experiments, tree n-grams are significantly
sparser than surface n-grams), but this problem
has largely been alleviated by the recent advances
in word embedding. Can we combine the advan-
tages of both worlds?
So we propose a very simple dependency-based
convolutional neural networks (DCNNs). Our
model is similar to Kim (2014), but while his se-
quential CNNs put a word in its sequential con-
text, ours considers a word and its parent, grand-
parent, great-grand-parent, and siblings on the de-
pendency tree. This way we incorporate long-
distance information that are otherwise unavail-
able on the surface string.
Experiments on three classification tasks
demonstrate the superior performance of our
DCNNs over the baseline sequential CNNs. In
particular, our accuracy on the TREC dataset
outperforms all previously published results
in the literature, including those with heavy
hand-engineered features.
Independently of this work, Mou et al. (2015,
unpublished) reported related efforts; see Sec. 3.3.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="method">
2 Dependency-based Convolution
</sectionHeader>
<bodyText confidence="0.999822">
The original CNN, first proposed by LeCun et
al. (1995), applies convolution kernels on a se-
ries of continuous areas of given images, and was
adapted to NLP by Collobert et al. (2011). Fol-
lowing Kim (2014), one dimensional convolution
operates the convolution kernel in sequential order
in Equation 1, where xi E Rd represents the d di-
mensional word representation for the i-th word in
</bodyText>
<page confidence="0.974773">
174
</page>
<note confidence="0.8968144">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 174–179,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
ROOT
Despite the film ’s shortcomings the stories are quietly moving .
</note>
<figureCaption confidence="0.996081">
Figure 1: Dependency tree of an example sentence from the Movie Reviews dataset.
</figureCaption>
<bodyText confidence="0.776253571428571">
Fg nng pe Rw datat
the senenc, ad ⊕ is the concatenation opera.
the senence, and ⊕ is the concatenation operator
Therefore �xi,j refers to concatenated word vector
Theree xi,j refes to concatenated word vector
m the h wd th + j) r
from the i-th word to the (i + j)-th word:
</bodyText>
<equation confidence="0.941237">
5Ei,j = xi ⊕ xi+1 ⊕ ··· ⊕ xi+j (1)
d �
</equation>
<bodyText confidence="0.7100446">
Sequential wrd conatnatin �xi,j works as
n-gram models which feeds loca infomation into
n-grammodels which feeds local information into
convolution operations. However, this setting can
convolution operations. However, this setting can
</bodyText>
<subsectionHeader confidence="0.335177">
not capture longdtance relationships unles we
</subsectionHeader>
<bodyText confidence="0.7591728">
ge t wi efiiy w wold
not capture ong-distance relationships unless we
itbl au th dt pit pbl
enlarge the window indefinitely which would in-
In order to cpture the longdistanc dependen
</bodyText>
<subsectionHeader confidence="0.502299">
evitably caus the data sparsity problem.
</subsectionHeader>
<bodyText confidence="0.900459">
ces we propose the dependency tree-b
In order capture the long-dstance depende-
volution model (DTCNN) Figure 1 illustrates an
cies wpropose the dependency-based convolu-
example frm te Movie Reviews (MR) datase
tion model (DCNN). Figure 1 illustrates an exam-
</bodyText>
<note confidence="0.52491">
(Pang and Lee, 2005) h seie o s se
</note>
<bodyText confidence="0.871166227272727">
ce s oiosly poi, but hi i qite d
ple from the Movie Reviews (MR) dataset (Pang
filt f til CNN bu nm
and Lee, 2005). The sentimnt of this sentence
windows wuld include the highl negative word
is obviously positive, but is quite difficult for
“shortcomings”, and the disance between “De-
sequential CNNs because many n-gram windows
spite” and hortcomings is quite long DTCNN,
would include the highly negative word “short-
however, coud capture the trebased bigram
comings”, and the distance between “Despite” and
Dpe – orcig, ippig h s-
mnt d h bsed iam “ROOT i
“shortcomings” is qute long. DCNN, however,
ties” which is highly pitiv
could capture the tree-based bigram “Despite –
shortcomings&amp;quot;, thus flipping the sentiment, and o Aes h
We dn ou atenation n dpe
the tree-based trigram “ROOT – moving – sto-
dytforidifi
ries”, which is highly positive.
</bodyText>
<subsectionHeader confidence="0.998508">
2.1 Convolution on Ancestor Paths
</subsectionHeader>
<bodyText confidence="0.7953175">
h fi k(i) h ih
We defie ou concatenation based on the epen-
ancestor index which is rcursively defined as:
dency tree for a given modifier xi:
</bodyText>
<equation confidence="0.946418">
xi,k = xi ⊕ xp(i) ⊕ ··· ⊕ Xpk_1(i)0(2)
</equation>
<bodyText confidence="0.951105">
where function pk(i) returns the i-th word’s k-th
</bodyText>
<figureCaption confidence="0.642152">
Figure 2 (left) llustrate ancestor paths patens
</figureCaption>
<equation confidence="0.771489555555556">
ancestor index, which is recursively defined as:
it vs odr wy trt
li ith d wh i
pk(i)y = p(pk−1(i))eiiff k &gt; 0 (3) = 0
Figure 2 (left) illustrates ancestor paths patterns
quence xi,k, the convoluton operation applies a
lt ∈ k×d toi, a ba
with various orders. We always start the convo-
ibdiqt4
</equation>
<bodyText confidence="0.961069375">
lution with xi and concatenate with its ancestors.
If the root node is reached, we add “ROOT” as
i f(w i,k + b) )
dummy ancestors (vertical padding).
For a given tree-based concatenated word se-
quence xi,k, the convolution operation applies a
filter w ∈ Rk×d to xi,k with a bias term b de-
scribed in equation 4:
</bodyText>
<equation confidence="0.887871">
ci = f(w · xi,k + b) (4)
ectified linear unit (ReLu) or sigmid on.
</equation>
<bodyText confidence="0.8523195">
where f is a non-linear activation function such as
The filter w is applied o each word in the sen-
rectified linear unit (ReLu) or sigmoid function.
enc geng atue mp ∈ l
The filter w is applied to each word in the sen-
tence, generating the feature map c ∈ Rl:
</bodyText>
<equation confidence="0.740594666666667">
[c1 , ] (5
h l is th lth f th t
ree Poolng
c = [c1, c2, ··· , cl]
rop
nation in Eq 4 can be regarded as pattern
</equation>
<bodyText confidence="0.871191">
where l is the length of the sentence
iati In suentil CNN maovertimool
</bodyText>
<subsectionHeader confidence="0.999869">
2.2 Max-Over-Tree Pooling and Dropout
</subsectionHeader>
<bodyText confidence="0.8919555">
ng (Coobert et al, 2011; Km, 2014) operates
The filters convolve with different word concate-
over the feature map to get the maxmum acti
nation in Eq. 4 can be regarded as pattern detec-
aaxpntgfea
tion: only the most similar pattern between the
DCNN l l th mi
ivation from feaure map to detect the strongest
words and the filter could return the maximum ac-
tivat. In sequential CNNs max-over-time pool-
ctivation over the whole tree (ie., over the whole
ig (Collobert et al., 2011; Kim, 2014) operates
entence). Since he tree no longer defines a se-
quential tim diction, we efr to our pooling
over the feature map to get the maximum acti-
vation cˆ = max c representing the entire feature
</bodyText>
<subsectionHeader confidence="0.403151">
s maovee poog
</subsectionHeader>
<bodyText confidence="0.979596636363636">
de o capture nouh ariatio e
dly itii th t f filte t dtt difft
map. Our DCNNs also pool the maximum activa-
ion from feature map to detect the strongest ac-
tructure paterns Each filtr’s height s the num
ber of words considred and h wdth s alwys
tivation over the whole tree (i.e., over the whole
sentence). Sienceothe tree no longeredefines a se-
quential “time&amp;quot; direction, we refer toyour pooling
as “max-over-tree” pooling.
ur r- po
f li ih dff filt
ght multiple features carry diffeent structura
In order to capture enough variations, we ran-
nfrato become t final represenation of the
domly initialize the set of filters to detect different
nput sentnce Then, his sentence reprsenation
structure patterns. Each filter’s height is the num-
s pased to a fuly connecte soft-max layer and
ber of words considered and the width is always
equal to the dimensionality d of word representa-
ps a isut e ffet b
Nl eok fe ffer f in
ng Following Kim (2014) we employ random
tion. Each filter will be represented by only one
dropot on pnultimate layer (Hintn et al. 2012)
feature after max-over-tree pooling. After a series
n order t preve co-adaptation of hidden unit.
of convolution with different filter with different
n our experiments, w set our drop out rat as 05
heights, multiple features carry different structural
informationbecome the final representation of the
nd learning rate as 0.95 by default. Following
</bodyText>
<equation confidence="0.894386">
K (2),
</equation>
<bodyText confidence="0.862791333333333">
input sentence. Then, this sentence representation
ng one ugh
gdit dt huffld iibth with
he Adadelta udate rul (Zeiler 2012)
is passed to a fully connected soft-max layer and
outputs a distribution over different labels.
</bodyText>
<sectionHeader confidence="0.853989" genericHeader="method">
3 Cli Sibi
</sectionHeader>
<bodyText confidence="0.947772454545454">
cestorpaths alone i not enough o captre
Neural networks often suffer from overtrain-
many inguistic phnoena such as conjunctin
ing. Following Kim (2014), we employ random
dropout on penultimate layer (Hinton et al., 2014).
in order to prevent co-adaptation of hidden units.
In our experiments, we set our drop out rate as 0.5
and learning rate as 0.95 by default. Following
Kim (2014), training is done through stochastic
gradient descent over shuffled mini-batches with
the Adadelta update rule (Zeiler, 2012).
</bodyText>
<figure confidence="0.970051615384615">
(5)
.
175
ancestor paths siblings
n pattern(s) n pattern(s)
3 m h g 2 s
m
4 m h g g2 3 s h t s m
m
5 m h g g2 4 t h
g3
s m s m h g
le rebd cout pe Wod
</figure>
<figureCaption confidence="0.999381">
Figure 2: Convolution patterns on trees. Word concatenation always starts with m, while h, g, and g2
</figureCaption>
<bodyText confidence="0.9483915">
aatayaw,wlg,ad
o pa, gr pa, a gg pt, t
denote parent, grand parent, and great-grand parent, etc., and “ ” denotes words excluded in convolution.
d do wr xlude nvolu
</bodyText>
<subsectionHeader confidence="0.8843965">
2.3 Convolution on Siblings
Cnvolution on Siblings
</subsectionHeader>
<bodyText confidence="0.995732083333333">
Ancestor paths alone is not enough to capture
estor paths alone is not enough o capture
ny linguistic phenomena such as conjunction.
many linguistic phenomena such as conjunction.
pired by higer-oder depndency parsing -
Inspired by higher-order dependency parsing (Mc-
nald nd Pereia, 2006; Ko and Colins, 21,
Donald and Pereira, 2006; Koo and Collins, 2010),
we also incrporate sblings for a given or
also incorporate siblings for a given word in
various ways. See Figure 2 (right) for details.
ous ways See Table 1 (right) for details
</bodyText>
<subsectionHeader confidence="0.7966965">
2.4 Combined Model
Cmbined Model
</subsectionHeader>
<bodyText confidence="0.99214652">
werful as it s tuctural information stil does
Powerful as it is, structural information still does
not fully cover sequential information. Also, pars-
fully cover sequential information Also pars
ing errors (which are common especialy for in-
errors (which are cmmon especially for in
formal text such as online reviews) directly affect
al tet ch a onli ries) dirctly affect
DCNN performance while sequential n-grams are
CNN pefmc hil eqtial ngam
always correctly observed. To best exploit both in-
formation, we want to combine both models. The
alays cly obed To b eloi
easiest way ofcombination is to concatenate these
nrn, w wnt o bh m
representations together, then feed into fully con-
The easiest way of combination is to con
enate these representaions togeher, then feed
nected soft-max neural networks. In these cases,
combine with different feture from different type
fully connected soft-max neural networks. In
se cases, combine wth different featur rom
of sources could stabilize the performance. The
erent typ of souces could stabiliz
final sentence representation is thus:
</bodyText>
<equation confidence="0.955893285714286">
...,
i �c(1),
(N)1
� �� �
(1
sequential
, ..., c)
</equation>
<bodyText confidence="0.979894">
where Na,
and N are the number of ancestor,
</bodyText>
<equation confidence="0.914292666666667">
we use
g
Ns,
</equation>
<bodyText confidence="0.9327954">
ere Na N and N are the number of ancesor
sibling, and sequential filters. In practice,
ing and sequentialfilters In practice we use
100 filters for each template in Figure 2 . The fully
combined representation is 1,100-dimensional by
filt f eah tlt Tbl 1 Th fly
contrast to 300-dimensional for sequential CNN.
bid tti i 1100dimil b
Table 1 summarizes results in the context of other
high-performing efforts in the literature. We use
</bodyText>
<sectionHeader confidence="0.352869" genericHeader="method">
Expimnt
</sectionHeader>
<bodyText confidence="0.822190357142857">
three benchmark datasets in two categories: senti-
implement our DTCNN on op of the open
rc CNN code y Kim (2014).1 Table 2
ment analysis on both Movie Review (MR) (Pangmmarizs our results i the context of other
and Lee, 2005) and Stanford Sentiment Treebankh-performing fforts in the iterture We se
(SST-1) (Socher et al., 2013) datasets, and ques-
tion classification on TREC (Li and Roth, 2002).
e benchmark datasets in two categories: senti
dataset without a development set (MR), we ran-
domly choose 10% of the training data to indicate
early stopping. In order to have a fare compari-
son with baseline CNN, we also use 3 to 5 as our
window size. Most of
GPU
get better results.2 Our implementation,
on top of Kim
code,3 will
Both sentiment analysis datasets (MR and
The differences
between them are mainly in the different
of categories and whether the standard split
is given. There are 10,662 sentences in the MR
dataset. Each instance is labeled positive or
one sentence.
standard split. Our
model achieves an accuracy of 49.5 which is sec-
ond only to Irsoy an
tial could generate be
</bodyText>
<subsectionHeader confidence="0.998509">
3.1 Sentiment Analysismenn
</subsectionHeader>
<bodyText confidence="0.9543115">
SST-
on on tub
</bodyText>
<subsectionHeader confidence="0.969521">
3.1 Sentiment Analysis
</subsectionHeader>
<figure confidence="0.561472571428571">
1) are based on movie reviews.
num-
Bthstintlyisde(MRandS
bers
1) a bsed ovie reiw The differen
betwee them are mainly in the dfferent n
neg-
</figure>
<bodyText confidence="0.951581238095238">
bers f categories and whether the sandard s
ative, and in most cases contains
is given There are 10,662 sentences in the M
Since no standard data split is given, following theliterature we use 10 fold cross vaidation to include
dataset Each instance is labeled positive or native ad in most cases contains one setn
every sentence in training and testing at least once.Cocatenaing with sibling and sequentia infor-
Since no standard data splt is given, following
literature we se 10 fold cross validtion to icu
mation obviously improves DCNNs, and the finalmodel outperforms the baseline sequentia CNNs
every sentence in training and esting at least onby 0.4, ad tes with Zhu et al. (2015).
Concaenating with sibling and seqaton obvously improvs tree-basd CNNs a
Different from MR, the Stanford Sentimentthe final model outperforms the baseline sequ
Treebank (SST-1) annotates finer-grained labels,tial CNNs by 04 and ties with Zhu et al (2015
very positive, positive, neutral, negative and verynegative, on an extension of the MR dataset. There
Diffrt f MR, th Stfd Sti
are 11,855 sentences with
Tebak (SST1) ts finergraine lb
d Cardie (2014).
ry poiive, piive, netral, ni d
and thus can not be used as gold-standard trees.
e 11,855 sentences with standard split. O
</bodyText>
<footnote confidence="0.763362571428572">
1The phrase-structure trees in SST-1 are actually automatically parsed,
odel achieves an accuracy of 49.5 whi
2GPU only supports float32 while CPU supports float64.
d only to Irsoy and Cardie (2014) W
3https://github.comw/yoonkim/CNN_sentence
trt to 300-dimen
3 Experiments
</footnote>
<page confidence="0.984548">
176
</page>
<bodyText confidence="0.80841775">
ST1) (Socher et al., 2013) dataets, and qu
For all datasets, we first obtain the dependencytion classfication on TREC (L and Roth, 200
parse tree from Stanford parser (Manning et al.,2014).1 Different window ize for different choic
For all datasets, we first obtain the dependenparse tree from Sanford parser (Maning et
of convolution are shown in Figure 2. For the
our results are generated by
early stopping In order to have a fare comp
due to its efficiency, however CPU could po-
</bodyText>
<equation confidence="0.823323736842105">
so ith bseli CNN we als e 3 to 5 as
tentially
(2014)’s
be released.4
idw siz Most f or esults r gnea
b GPU d i ffii h CPU
4https://github.com/cosmmb/DCNN
to 100 f thi tk
C = ��c(1)
a , ..., C(N.)
) � �� �
ˆ (N (1) � �� ˆ (
1)
C�,
�c(Ns)
s �
ancestors
., c siblings
i
</equation>
<table confidence="0.99113664">
Category Model MR SST-1 TREC TREC-2
This work DCNNs: ancestor 80.4† 47.7† 95.4† 88.4†
DCNNs: ancestor+sibling 81.7† 48.3† 95.6† 89.0†
DCNNs: ancestor+sibling+sequential 81.9 49.5 95.4† 88.8†
M
CNNs CNNs-non-static (Kim, 2014) – baseline 81.5 48.0 93.6 aor+si
CNNs-multichannel (Kim, 2014) 81.1 DTN 92.2 86.4∗
Deep CNNs (Kalchbrenner et al., 2014) - 47.4 93.0 86.0∗
DTNs: ao -
48.5
DTNs:
Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.7 43.2 CNmne ( -
CNNs - CNn - -
Recursive Neural Tensor (Socher et al., 2013) - 45.7 - ch-
Deep Recursive NNs (Irsoy and Cardie, 2014) De CN
49.8 -
Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.9 NNs - -
Re
48.0
Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - Re - -
48.7
Hand-coded rules SVMS (Silva et al., 2011) - ecurNN( 90.8
95.0 h t
t
er dp ng ParagpVec (Le ad M
</table>
<tableCaption confidence="0.999886">
Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.
</tableCaption>
<equation confidence="0.805154545454546">
Hddd l SVM (Sil t l, 2011)
TREC-2 is TREC with fine grained labels. †Results generated by GPU (all others generated by CPU).
∗Results generated from Kim (2014)’s implementation.
roo
t
What is Hawaii ’s state flower ?
⇒ loc
roo
t
enty⇒ desc
t
</equation>
<bodyText confidence="0.875896578947368">
What does a defibrill
ator do ?
⇒ enty
such as
numeric:distance,
and entity:vehicle. To make our task more real-
istic and challenging, we also test the proposed
model with respect to the 50 subcategories. There
are obvious improvements over sequential CNNs
from the last column of Table 1. Like ours, Silva
et al. (2011) is a tree-based system but it uses
constituency trees compared to ours dependency
trees. They report a higher fine-grained accuracy
of 90.8 but their parser is trained only on the Ques-
tionBank (Judge et al., 2006) while we used the
standard Stanford parser trained on both the Penn
Treebank and QuestionBank. Moreover, as men-
tioned above, their approach is rule-based while
ours is automatically learned.
</bodyText>
<subsectionHeader confidence="0.998208">
3.3 Discussions and Examples
</subsectionHeader>
<bodyText confidence="0.91982825">
Compared with sentiment analysis, the advantage
of our proposed model is obviously more substan-
tial on the TREC dataset. Based on our error an
numeric:temperature,
</bodyText>
<equation confidence="0.61750725">
al-
ysis, we conclude that this is mainly due to the
roo
t
</equation>
<bodyText confidence="0.5648275">
Nothing plot wise is worth emaili
ng home about
</bodyText>
<figure confidence="0.988929">
(d) mild negative
⇒ mild positive
What is the temperature at the center of the eart
h ?
(e) NUM:temp
⇒ NUM:dist
Christopher Columbus ’ three ships ?
⇒ LOC:other
</figure>
<subsectionHeader confidence="0.994124">
3.2 Question Classification
</subsectionHeader>
<bodyText confidence="0.985065642857143">
In the TREC dataset, the entire dataset of 5,952
sentences are classified into the following 6 cate-
gories: abbreviation, entity, description, location
and numeric. In this experiment, DCNNs easily
outperform any other methods even with ancestor
convolution only. DCNNs with sibling achieve the
best performance in the published literature. DC-
NNs combined with sibling and sequential infor-
mation might suffer from overfitting on the train-
ing data based on our observation. One thing
to note here is that our best result even exceeds
SVMS (Silva et al., 2011) with 60 hand-coded
rules.
The TREC dataset also provides subcategories
</bodyText>
<figure confidence="0.990892">
(a) enty
What is natural gas
composed of ?
roo
(c) desc
roo
What were
(f) ENTY:veh
an
</figure>
<figureCaption confidence="0.9032145">
Figure 3: Examples from TREC (a–c), SST-1 (d)
and TREC with finegrained label (f) that ar
</figureCaption>
<bodyText confidence="0.9135455">
d TREC with fine-grained label (e–f) that are
msclassified by the baseline CNN but corectly
</bodyText>
<footnote confidence="0.4760438">
misclassified by the baseline CNN but correctly
labeled by our DTCNN. For exampl (a) should
lald b or DCNN. For example, (a) should be
be enity but is labeled location by CNN.
entity but is labeled location by CNN.
</footnote>
<equation confidence="0.986119666666667">
t
roo
t
</equation>
<page confidence="0.9855">
177
</page>
<figure confidence="0.989322368421053">
root
What is the speed hummingbirds fly ?
Wat is the speed mmb
(noun)
root
What is the meltingepoint of copper ?
(a) num ⇒ enty (a) num ⇒ enty and desc
(a)
root
What body of water are the Canary Islands in ?
h bd f h C
(b) loc ⇒ enty (b
root
What position did Willie Davis play in baseball ?
root
What did Jesse Jackson organize ?
(b) hum ⇒ enty and enty
root
(c) hum ⇒ enty
</figure>
<figureCaption confidence="0.999594">
Figure 4: Examples from TREC datasets that
msclssified by DTCNN but correctly labeled by
Figure 3: Examples from TREC datase
</figureCaption>
<bodyText confidence="0.986573421052631">
misclassifid by DCNN but correctly labeled
baseline CNN For examle, () should e numr-
miclasified by DTCNN but correctly
baseline CNN. For example, (a) shouldabe nu
ical butsiswlabeleddentity by DCNN.
difference of the parsentree quality between
two tasks. In sentiment analysis, the dataset
collected fromcthe RottenmTomatoes website whi
includes many irregular usage of language.nSo
of the secntencesssevenocome from languages oth
than English. The errors inbparsentreesTinevitabl
affect the classificationkaccuracy.aHowever,
parserlworks substantially betternonathe TRE
dataset since allcquestionsTareCinnformal writt
English, and theutraining setefor Stanford pars
already includesFtheeQuestionBank (Judge et
2006)wwhichlincludes 2,000 TREChsentences.
Figurea3yvisualizesaexamplesuwhere CNN err
while DCNN does not. For example, CNN la-
bels (a) asrlocation due to “Hawaii” and “state&amp;quot;
whileptheelong-distanceabackbone &amp;quot;Whate—aflow
is clearlyxasking for an entity.oSimilarly, in
DCNN captures the obviously negative tree-based
trigram “Nothingo—dworthh—temailing”. Note th
our model alsorworksewithInon-projectivendep
dency trees such as theaone ind(b). The last two
amples in Figure 3 visualize cases wherenDCNN
outperforms the baseline CNNs in fine-grained
TREC. In example (e), thehword “temperature” is
at second from the top and is root of a 8 word span
“the ... earth”. When we use a window of size 5
for tree convolution, every words in that span get
convolved with “temperature” and this should be
the reason why DCNN get correct.
Figure 4 showcases examples where baseline
CNNs get better results than DCNNs. Example
(a) is misclassified as entity by DCNN due to pars-
ing/tagging error (the Stanford parser performs its
</bodyText>
<footnote confidence="0.5361755">
5 http://nlp.stanford.edu/software/parser-faq.shtml
What is the electrical output in Madrid , Spain ?
</footnote>
<listItem confidence="0.4185785">
(c) enty ⇒ num and num
d baseline CNN.
</listItem>
<bodyText confidence="0.93474075">
For example,y(a)Dshouldabednumerical butNis la-
beled entity by DCNN and description by CNN.
ging). eThe word “fl
should be a verbNinste
fly&amp;quot; should be a rel
n.
nces that are
DC
Examplea(a) is
methods due to
“point” whic
ddintg. This
. Apparently,
r.ed by word e
,pd
e an annotatio
before submitting to ACL 2015
b-lished)mhavebi
e ase on ei
Shortlywe
, based their unpub-
lishedlanguages (Mou et
our dependency
mod and
all it 014)),
work in programming
6 performs convolution on
erethancword
any,hresemblanc
dependency-based model. Their
el is related, but always includes a node
s children (resembling Iyyer et al. (2
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999446333333333">
We have presented a very simple dependency-
based convolution framework which outperforms
sequential CNN baselines on modeling sentences.
</bodyText>
<footnote confidence="0.7631445">
6Both their 2014 and 2015 reports proposed (independently of each other
and independently of our work) the term “tree-based convolution” (TBCNN).
</footnote>
<figure confidence="0.881664333333333">
are
by
mer-
</figure>
<figureCaption confidence="0.2629785">
Figureg5: Examples from TREC datasets that are
misclassified by both DCNN an
</figureCaption>
<bodyText confidence="0.847288851851852">
d “teer-
en-
seqal CNN baselines on various classification
cursiv node presentations rath
We have presented a very simple d
t me ould nsi
ex-
tas Ensio
roo based convolutio framewrk
beddings, thus baring little, if
lbls ad titcy t A
the own part-of-speech tag
is the end of themsentence
ch noun, and “hummingbirds
me clause modifyingn“speed”
era e Therte arefsomessente
y by both thetbaselinemCNNnand
the shows three such examples.
Csclassified asbnumerical by both
enoambiguous meaningpof the word
er5 difficult toucapturenby word embe
reported concurrent andarelated e
d
l
e , learnedmMou et al. (2015, unpu
er&amp;quot; pendently
(d), Theirncoastituency mo
</bodyText>
<figure confidence="0.98118468">
al., canpmeanolocation,bopinion, etc
numericalgaspect isnnot captu
at al.,l2014),
s ding. Example (c) might
NN. Figure 5
not
the
h is
word
the
mbed-
n error.
nde-
fforts.
pretrained re-
em-
e to
which is a variant of our sibling model and always
flat. By contrast, our ancestor model looks at the
vertical path from any word to its ancestors, being
linguistically motivated (Shen et al., 2008).
misclassified
y” at
ad of
ative
</figure>
<page confidence="0.98251">
178
</page>
<sectionHeader confidence="0.981207" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999917566037736">
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12.
Kushal Dave, Steve Lawrence, and David M Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In
Proceedings of World Wide Web.
Michael Gamon. 2004. Sentiment classification on
customer feedback data: noisy data, large feature
vectors, and the role of linguistic analysis. In Pro-
ceedings of COLING.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2014. Improving neural networks by
preventing co-adaptation of feature detectors.
Journal of Machine Learning Research, 15.
Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems, pages 2096–2104.
Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daum´e III. 2014. A neural
network for factoid question answering over para-
graphs. In Proceedings of EMNLP.
John Judge, Aoife Cahill, and Josef van Genabith.
2006. Questionbank: Creating a corpus of parse-
annotated questions. In Proceedings of COLING.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of EMNLP.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL.
Taku Kudo and Yuji Matsumoto. 2004. A boosting
algorithm for classification of semi-structured text.
In Proceedings of EMNLP.
Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of ICML.
Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,
J. Denker, H. Drucker, I. Guyon, U. Mller,
E. Sckinger, P. Simard, and V. Vapnik. 1995. Com-
parison of learning algorithms for handwritten digit
recognition. In Int’l Conf. on Artificial Neural Nets.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of COLING.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of ACL:
Demonstrations, pages 55–60.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using
word sub-sequences and dependency sub-trees. In
Proceedings of PA-KDD.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL.
Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang.
2014. TBCNN: A tree-based convolutional neu-
ral network for programming language processing.
Unpublished manuscript: http://arxiv.org/
abs/1409.5718.
Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and
Zhi Jin. 2015. Discriminative neural sentence
modeling by tree-based convolution. Unpublished
manuscript: http://arxiv.org/abs/1504.
01106v5. Version 5 dated June 2, 2015; Version 1
(“Tree-based Convolution: A New Architecture for
Sentence Modeling”) dated Apr 5, 2015.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings ofACL,
pages 115–124.
Libin Shen, Lucas Champollion, and Aravind K Joshi.
2008. LTAG-spinal and the treebank. Language Re-
sources and Evaluation, 42(1):1–19.
Yelong Shen, Xiaodong he, Jianfeng Gao, Li Deng, and
Gregoire Mesnil. 2014. Learning semantic repre-
sentations using convolutional neural networks for
web search. In Proceedings of WWW.
J. Silva, L. Coheur, A. C. Mendes, and Andreas
Wichert. 2011. From symbolic to sub-symbolic in-
formation in question classification. Artificial Intel-
ligence Review, 35.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
EMNLP 2011.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP 2013.
Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation question
answering. In Proceedings of ACL.
Mattgew Zeiler. 2012. Adadelta: An adaptive learning
rate method. Unpublished manuscript: http://
arxiv.org/abs/1212.5701.
Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over tree structures.
In Proceedings of ICML.
</reference>
<page confidence="0.99881">
179
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.229004">
<title confidence="0.974062">Dependency-based Convolutional Neural Networks</title>
<author confidence="0.532612">Sentence</author>
<affiliation confidence="0.700725">Center &amp; Queens College City University of New York</affiliation>
<abstract confidence="0.998672125">In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To combine deep learning with linguistic structures, we propose a dependency-based convolution apmaking use of tree-based rather than surface ones, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<contexts>
<context position="1034" citStr="Collobert et al., 2011" startWordPosition="136" endWordPosition="139">ncies. To combine deep learning with linguistic structures, we propose a dependency-based convolution approach, making use of tree-based n-grams rather than surface ones, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC. 1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect lon</context>
<context position="3895" citStr="Collobert et al. (2011)" startWordPosition="572" endWordPosition="575">e surface string. Experiments on three classification tasks demonstrate the superior performance of our DCNNs over the baseline sequential CNNs. In particular, our accuracy on the TREC dataset outperforms all previously published results in the literature, including those with heavy hand-engineered features. Independently of this work, Mou et al. (2015, unpublished) reported related efforts; see Sec. 3.3. 2 Dependency-based Convolution The original CNN, first proposed by LeCun et al. (1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al. (2011). Following Kim (2014), one dimensional convolution operates the convolution kernel in sequential order in Equation 1, where xi E Rd represents the d dimensional word representation for the i-th word in 174 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 174–179, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ROOT Despite the film ’s shortcomings the stories are quietly moving . Figure 1: Dependency tree of an example senten</context>
<context position="8416" citStr="Collobert et al., 2011" startWordPosition="1370" endWordPosition="1373">on in Eq 4 can be regarded as pattern where l is the length of the sentence iati In suentil CNN maovertimool 2.2 Max-Over-Tree Pooling and Dropout ng (Coobert et al, 2011; Km, 2014) operates The filters convolve with different word concateover the feature map to get the maxmum acti nation in Eq. 4 can be regarded as pattern detecaaxpntgfea tion: only the most similar pattern between the DCNN l l th mi ivation from feaure map to detect the strongest words and the filter could return the maximum activat. In sequential CNNs max-over-time poolctivation over the whole tree (ie., over the whole ig (Collobert et al., 2011; Kim, 2014) operates entence). Since he tree no longer defines a sequential tim diction, we efr to our pooling over the feature map to get the maximum activation cˆ = max c representing the entire feature s maovee poog de o capture nouh ariatio e dly itii th t f filte t dtt difft map. Our DCNNs also pool the maximum activaion from feature map to detect the strongest actructure paterns Each filtr’s height s the num ber of words considred and h wdth s alwys tivation over the whole tree (i.e., over the whole sentence). Sienceothe tree no longeredefines a sequential “time&amp;quot; direction, we refer toy</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of World Wide Web.</booktitle>
<contexts>
<context position="2484" citStr="Dave et al., 2003" startWordPosition="351" endWordPosition="354">the sentence. ∗ This work was done at both IBM and CUNY, and was supported in part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions. Bing Xiang‡ Bowen Zhou‡ ‡IBM Watson Group T. J. Watson Research Center, IBM {lhuang,bingxia,zhou}@us.ibm.com Indeed, in the sentiment analysis literature, researchers have incorporated long-distance information from syntactic parse trees, but the results are somewhat inconsistent: some reported small improvements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al., 2003; Kudo and Matsumoto, 2004). As a result, syntactic features have yet to become popular in the sentiment analysis community. We suspect one of the reasons for this is data sparsity (according to our experiments, tree n-grams are significantly sparser than surface n-grams), but this problem has largely been alleviated by the recent advances in word embedding. Can we combine the advantages of both worlds? So we propose a very simple dependency-based convolutional neural networks (DCNNs). Our model is similar to Kim (2014), but while his sequential CNNs put a word in its sequential context, ours </context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="2418" citStr="Gamon, 2004" startWordPosition="342" endWordPosition="343">ect the sentiment, subjectivity, or other categorization of the sentence. ∗ This work was done at both IBM and CUNY, and was supported in part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions. Bing Xiang‡ Bowen Zhou‡ ‡IBM Watson Group T. J. Watson Research Center, IBM {lhuang,bingxia,zhou}@us.ibm.com Indeed, in the sentiment analysis literature, researchers have incorporated long-distance information from syntactic parse trees, but the results are somewhat inconsistent: some reported small improvements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al., 2003; Kudo and Matsumoto, 2004). As a result, syntactic features have yet to become popular in the sentiment analysis community. We suspect one of the reasons for this is data sparsity (according to our experiments, tree n-grams are significantly sparser than surface n-grams), but this problem has largely been alleviated by the recent advances in word embedding. Can we combine the advantages of both worlds? So we propose a very simple dependency-based convolutional neural networks (DCNNs). Our model is similar to Kim (2014), but whi</context>
</contexts>
<marker>Gamon, 2004</marker>
<rawString>Michael Gamon. 2004. Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>15</volume>
<marker>Hinton, Srivastava, 2014</marker>
<rawString>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Improving neural networks by preventing co-adaptation of feature detectors. Journal of Machine Learning Research, 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep recursive neural networks for compositionality in language.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2096--2104</pages>
<contexts>
<context position="16197" citStr="Irsoy and Cardie (2014)" startWordPosition="2703" endWordPosition="2706">ne sequ Treebank (SST-1) annotates finer-grained labels,tial CNNs by 04 and ties with Zhu et al (2015 very positive, positive, neutral, negative and verynegative, on an extension of the MR dataset. There Diffrt f MR, th Stfd Sti are 11,855 sentences with Tebak (SST1) ts finergraine lb d Cardie (2014). ry poiive, piive, netral, ni d and thus can not be used as gold-standard trees. e 11,855 sentences with standard split. O 1The phrase-structure trees in SST-1 are actually automatically parsed, odel achieves an accuracy of 49.5 whi 2GPU only supports float32 while CPU supports float64. d only to Irsoy and Cardie (2014) W 3https://github.comw/yoonkim/CNN_sentence trt to 300-dimen 3 Experiments 176 ST1) (Socher et al., 2013) dataets, and qu For all datasets, we first obtain the dependencytion classfication on TREC (L and Roth, 200 parse tree from Stanford parser (Manning et al.,2014).1 Different window ize for different choic For all datasets, we first obtain the dependenparse tree from Sanford parser (Maning et of convolution are shown in Figure 2. For the our results are generated by early stopping In order to have a fare comp due to its efficiency, however CPU could poso ith bseli CNN we als e 3 to 5 as te</context>
<context position="17606" citStr="Irsoy and Cardie, 2014" startWordPosition="2952" endWordPosition="2955">) C�, �c(Ns) s � ancestors ., c siblings i Category Model MR SST-1 TREC TREC-2 This work DCNNs: ancestor 80.4† 47.7† 95.4† 88.4† DCNNs: ancestor+sibling 81.7† 48.3† 95.6† 89.0† DCNNs: ancestor+sibling+sequential 81.9 49.5 95.4† 88.8† M CNNs CNNs-non-static (Kim, 2014) – baseline 81.5 48.0 93.6 aor+si CNNs-multichannel (Kim, 2014) 81.1 DTN 92.2 86.4∗ Deep CNNs (Kalchbrenner et al., 2014) - 47.4 93.0 86.0∗ DTNs: ao - 48.5 DTNs: Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.7 43.2 CNmne ( - CNNs - CNn - - Recursive Neural Tensor (Socher et al., 2013) - 45.7 - chDeep Recursive NNs (Irsoy and Cardie, 2014) De CN 49.8 - Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.9 NNs - - Re 48.0 Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - Re - - 48.7 Hand-coded rules SVMS (Silva et al., 2011) - ecurNN( 90.8 95.0 h t t er dp ng ParagpVec (Le ad M Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets. Hddd l SVM (Sil t l, 2011) TREC-2 is TREC with fine grained labels. †Results generated by GPU (all others generated by CPU). ∗Results generated from Kim (2014)’s implementation. roo t What is Hawaii ’s state flower ? ⇒ loc roo t enty⇒ desc t What does a de</context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Advances in Neural Information Processing Systems, pages 2096–2104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Iyyer</author>
<author>Jordan Boyd-Graber</author>
<author>Leonardo Claudino</author>
<author>Richard Socher</author>
<author>Hal Daum´e</author>
</authors>
<title>A neural network for factoid question answering over paragraphs.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Iyyer, Boyd-Graber, Claudino, Socher, Daum´e, 2014</marker>
<rawString>Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum´e III. 2014. A neural network for factoid question answering over paragraphs. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Judge</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Questionbank: Creating a corpus of parseannotated questions.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING.</booktitle>
<marker>Judge, Cahill, van Genabith, 2006</marker>
<rawString>John Judge, Aoife Cahill, and Josef van Genabith. 2006. Questionbank: Creating a corpus of parseannotated questions. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1205" citStr="Kalchbrenner et al., 2014" startWordPosition="163" endWordPosition="167">, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC. 1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the latter play an important role in many linguistic phenomena such as negation, subordination, and wh-extraction, all of which might dully a</context>
<context position="17372" citStr="Kalchbrenner et al., 2014" startWordPosition="2907" endWordPosition="2910"> CPU could poso ith bseli CNN we als e 3 to 5 as tentially (2014)’s be released.4 idw siz Most f or esults r gnea b GPU d i ffii h CPU 4https://github.com/cosmmb/DCNN to 100 f thi tk C = ��c(1) a , ..., C(N.) ) � �� � ˆ (N (1) � �� ˆ ( 1) C�, �c(Ns) s � ancestors ., c siblings i Category Model MR SST-1 TREC TREC-2 This work DCNNs: ancestor 80.4† 47.7† 95.4† 88.4† DCNNs: ancestor+sibling 81.7† 48.3† 95.6† 89.0† DCNNs: ancestor+sibling+sequential 81.9 49.5 95.4† 88.8† M CNNs CNNs-non-static (Kim, 2014) – baseline 81.5 48.0 93.6 aor+si CNNs-multichannel (Kim, 2014) 81.1 DTN 92.2 86.4∗ Deep CNNs (Kalchbrenner et al., 2014) - 47.4 93.0 86.0∗ DTNs: ao - 48.5 DTNs: Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.7 43.2 CNmne ( - CNNs - CNn - - Recursive Neural Tensor (Socher et al., 2013) - 45.7 - chDeep Recursive NNs (Irsoy and Cardie, 2014) De CN 49.8 - Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.9 NNs - - Re 48.0 Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - Re - - 48.7 Hand-coded rules SVMS (Silva et al., 2011) - ecurNN( 90.8 95.0 h t t er dp ng ParagpVec (Le ad M Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets. Hddd l SVM (Sil t l, 2</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1217" citStr="Kim, 2014" startWordPosition="168" endWordPosition="169">teractions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC. 1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the latter play an important role in many linguistic phenomena such as negation, subordination, and wh-extraction, all of which might dully affect the se</context>
<context position="3009" citStr="Kim (2014)" startWordPosition="437" endWordPosition="438">ements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al., 2003; Kudo and Matsumoto, 2004). As a result, syntactic features have yet to become popular in the sentiment analysis community. We suspect one of the reasons for this is data sparsity (according to our experiments, tree n-grams are significantly sparser than surface n-grams), but this problem has largely been alleviated by the recent advances in word embedding. Can we combine the advantages of both worlds? So we propose a very simple dependency-based convolutional neural networks (DCNNs). Our model is similar to Kim (2014), but while his sequential CNNs put a word in its sequential context, ours considers a word and its parent, grandparent, great-grand-parent, and siblings on the dependency tree. This way we incorporate longdistance information that are otherwise unavailable on the surface string. Experiments on three classification tasks demonstrate the superior performance of our DCNNs over the baseline sequential CNNs. In particular, our accuracy on the TREC dataset outperforms all previously published results in the literature, including those with heavy hand-engineered features. Independently of this work,</context>
<context position="8428" citStr="Kim, 2014" startWordPosition="1374" endWordPosition="1375">ed as pattern where l is the length of the sentence iati In suentil CNN maovertimool 2.2 Max-Over-Tree Pooling and Dropout ng (Coobert et al, 2011; Km, 2014) operates The filters convolve with different word concateover the feature map to get the maxmum acti nation in Eq. 4 can be regarded as pattern detecaaxpntgfea tion: only the most similar pattern between the DCNN l l th mi ivation from feaure map to detect the strongest words and the filter could return the maximum activat. In sequential CNNs max-over-time poolctivation over the whole tree (ie., over the whole ig (Collobert et al., 2011; Kim, 2014) operates entence). Since he tree no longer defines a sequential tim diction, we efr to our pooling over the feature map to get the maximum activation cˆ = max c representing the entire feature s maovee poog de o capture nouh ariatio e dly itii th t f filte t dtt difft map. Our DCNNs also pool the maximum activaion from feature map to detect the strongest actructure paterns Each filtr’s height s the num ber of words considred and h wdth s alwys tivation over the whole tree (i.e., over the whole sentence). Sienceothe tree no longeredefines a sequential “time&amp;quot; direction, we refer toyour pooling </context>
<context position="10425" citStr="Kim (2014)" startWordPosition="1718" endWordPosition="1719"> filter with different n our experiments, w set our drop out rat as 05 heights, multiple features carry different structural informationbecome the final representation of the nd learning rate as 0.95 by default. Following K (2), input sentence. Then, this sentence representation ng one ugh gdit dt huffld iibth with he Adadelta udate rul (Zeiler 2012) is passed to a fully connected soft-max layer and outputs a distribution over different labels. 3 Cli Sibi cestorpaths alone i not enough o captre Neural networks often suffer from overtrainmany inguistic phnoena such as conjunctin ing. Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al., 2014). in order to prevent co-adaptation of hidden units. In our experiments, we set our drop out rate as 0.5 and learning rate as 0.95 by default. Following Kim (2014), training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012). (5) . 175 ancestor paths siblings n pattern(s) n pattern(s) 3 m h g 2 s m 4 m h g g2 3 s h t s m m 5 m h g g2 4 t h g3 s m s m h g le rebd cout pe Wod Figure 2: Convolution patterns on trees. Word concatenation always starts with m, while h</context>
<context position="13559" citStr="Kim (2014)" startWordPosition="2264" endWordPosition="2265"> where Na, and N are the number of ancestor, we use g Ns, ere Na N and N are the number of ancesor sibling, and sequential filters. In practice, ing and sequentialfilters In practice we use 100 filters for each template in Figure 2 . The fully combined representation is 1,100-dimensional by filt f eah tlt Tbl 1 Th fly contrast to 300-dimensional for sequential CNN. bid tti i 1100dimil b Table 1 summarizes results in the context of other high-performing efforts in the literature. We use Expimnt three benchmark datasets in two categories: sentiimplement our DTCNN on op of the open rc CNN code y Kim (2014).1 Table 2 ment analysis on both Movie Review (MR) (Pangmmarizs our results i the context of other and Lee, 2005) and Stanford Sentiment Treebankh-performing fforts in the iterture We se (SST-1) (Socher et al., 2013) datasets, and question classification on TREC (Li and Roth, 2002). e benchmark datasets in two categories: senti dataset without a development set (MR), we randomly choose 10% of the training data to indicate early stopping. In order to have a fare comparison with baseline CNN, we also use 3 to 5 as our window size. Most of GPU get better results.2 Our implementation, on top of Ki</context>
<context position="17251" citStr="Kim, 2014" startWordPosition="2890" endWordPosition="2891">e our results are generated by early stopping In order to have a fare comp due to its efficiency, however CPU could poso ith bseli CNN we als e 3 to 5 as tentially (2014)’s be released.4 idw siz Most f or esults r gnea b GPU d i ffii h CPU 4https://github.com/cosmmb/DCNN to 100 f thi tk C = ��c(1) a , ..., C(N.) ) � �� � ˆ (N (1) � �� ˆ ( 1) C�, �c(Ns) s � ancestors ., c siblings i Category Model MR SST-1 TREC TREC-2 This work DCNNs: ancestor 80.4† 47.7† 95.4† 88.4† DCNNs: ancestor+sibling 81.7† 48.3† 95.6† 89.0† DCNNs: ancestor+sibling+sequential 81.9 49.5 95.4† 88.8† M CNNs CNNs-non-static (Kim, 2014) – baseline 81.5 48.0 93.6 aor+si CNNs-multichannel (Kim, 2014) 81.1 DTN 92.2 86.4∗ Deep CNNs (Kalchbrenner et al., 2014) - 47.4 93.0 86.0∗ DTNs: ao - 48.5 DTNs: Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.7 43.2 CNmne ( - CNNs - CNn - - Recursive Neural Tensor (Socher et al., 2013) - 45.7 - chDeep Recursive NNs (Irsoy and Cardie, 2014) De CN 49.8 - Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.9 NNs - - Re 48.0 Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - Re - - 48.7 Hand-coded rules SVMS (Silva et al., 2011) - ecurNN( 90.8 95.0 h t t er dp ng ParagpVec (Le a</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="11609" citStr="Koo and Collins, 2010" startWordPosition="1932" endWordPosition="1935">tenation always starts with m, while h, g, and g2 aatayaw,wlg,ad o pa, gr pa, a gg pt, t denote parent, grand parent, and great-grand parent, etc., and “ ” denotes words excluded in convolution. d do wr xlude nvolu 2.3 Convolution on Siblings Cnvolution on Siblings Ancestor paths alone is not enough to capture estor paths alone is not enough o capture ny linguistic phenomena such as conjunction. many linguistic phenomena such as conjunction. pired by higer-oder depndency parsing - Inspired by higher-order dependency parsing (Mcnald nd Pereia, 2006; Ko and Colins, 21, Donald and Pereira, 2006; Koo and Collins, 2010), we also incrporate sblings for a given or also incorporate siblings for a given word in various ways. See Figure 2 (right) for details. ous ways See Table 1 (right) for details 2.4 Combined Model Cmbined Model werful as it s tuctural information stil does Powerful as it is, structural information still does not fully cover sequential information. Also, parsfully cover sequential information Also pars ing errors (which are common especialy for inerrors (which are cmmon especially for in formal text such as online reviews) directly affect al tet ch a onli ries) dirctly affect DCNN performance </context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A boosting algorithm for classification of semi-structured text.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2511" citStr="Kudo and Matsumoto, 2004" startWordPosition="355" endWordPosition="358">s work was done at both IBM and CUNY, and was supported in part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions. Bing Xiang‡ Bowen Zhou‡ ‡IBM Watson Group T. J. Watson Research Center, IBM {lhuang,bingxia,zhou}@us.ibm.com Indeed, in the sentiment analysis literature, researchers have incorporated long-distance information from syntactic parse trees, but the results are somewhat inconsistent: some reported small improvements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al., 2003; Kudo and Matsumoto, 2004). As a result, syntactic features have yet to become popular in the sentiment analysis community. We suspect one of the reasons for this is data sparsity (according to our experiments, tree n-grams are significantly sparser than surface n-grams), but this problem has largely been alleviated by the recent advances in word embedding. Can we combine the advantages of both worlds? So we propose a very simple dependency-based convolutional neural networks (DCNNs). Our model is similar to Kim (2014), but while his sequential CNNs put a word in its sequential context, ours considers a word and its pa</context>
</contexts>
<marker>Kudo, Matsumoto, 2004</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2004. A boosting algorithm for classification of semi-structured text. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="17743" citStr="Mikolov, 2014" startWordPosition="2981" endWordPosition="2982">ibling 81.7† 48.3† 95.6† 89.0† DCNNs: ancestor+sibling+sequential 81.9 49.5 95.4† 88.8† M CNNs CNNs-non-static (Kim, 2014) – baseline 81.5 48.0 93.6 aor+si CNNs-multichannel (Kim, 2014) 81.1 DTN 92.2 86.4∗ Deep CNNs (Kalchbrenner et al., 2014) - 47.4 93.0 86.0∗ DTNs: ao - 48.5 DTNs: Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.7 43.2 CNmne ( - CNNs - CNn - - Recursive Neural Tensor (Socher et al., 2013) - 45.7 - chDeep Recursive NNs (Irsoy and Cardie, 2014) De CN 49.8 - Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.9 NNs - - Re 48.0 Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - Re - - 48.7 Hand-coded rules SVMS (Silva et al., 2011) - ecurNN( 90.8 95.0 h t t er dp ng ParagpVec (Le ad M Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets. Hddd l SVM (Sil t l, 2011) TREC-2 is TREC with fine grained labels. †Results generated by GPU (all others generated by CPU). ∗Results generated from Kim (2014)’s implementation. roo t What is Hawaii ’s state flower ? ⇒ loc roo t enty⇒ desc t What does a defibrill ator do ? ⇒ enty such as numeric:distance, and entity:vehicle. To make our task more realistic and challenging, we also test the </context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y LeCun</author>
<author>L Jackel</author>
<author>L Bottou</author>
<author>A Brunot</author>
<author>C Cortes</author>
<author>J Denker</author>
<author>H Drucker</author>
<author>I Guyon</author>
<author>U Mller</author>
<author>E Sckinger</author>
<author>P Simard</author>
<author>V Vapnik</author>
</authors>
<title>Comparison of learning algorithms for handwritten digit recognition.</title>
<date>1995</date>
<booktitle>In Int’l Conf. on Artificial Neural Nets.</booktitle>
<contexts>
<context position="895" citStr="LeCun et al., 1995" startWordPosition="116" endWordPosition="119">ve recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To combine deep learning with linguistic structures, we propose a dependency-based convolution approach, making use of tree-based n-grams rather than surface ones, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC. 1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being </context>
<context position="3767" citStr="LeCun et al. (1995)" startWordPosition="549" endWordPosition="552">, and siblings on the dependency tree. This way we incorporate longdistance information that are otherwise unavailable on the surface string. Experiments on three classification tasks demonstrate the superior performance of our DCNNs over the baseline sequential CNNs. In particular, our accuracy on the TREC dataset outperforms all previously published results in the literature, including those with heavy hand-engineered features. Independently of this work, Mou et al. (2015, unpublished) reported related efforts; see Sec. 3.3. 2 Dependency-based Convolution The original CNN, first proposed by LeCun et al. (1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al. (2011). Following Kim (2014), one dimensional convolution operates the convolution kernel in sequential order in Equation 1, where xi E Rd represents the d dimensional word representation for the i-th word in 174 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 174–179, Beijing, China, July 26-31, 2015. c�2015 Association for Computational L</context>
</contexts>
<marker>LeCun, Jackel, Bottou, Brunot, Cortes, Denker, Drucker, Guyon, Mller, Sckinger, Simard, Vapnik, 1995</marker>
<rawString>Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes, J. Denker, H. Drucker, I. Guyon, U. Mller, E. Sckinger, P. Simard, and V. Vapnik. 1995. Comparison of learning algorithms for handwritten digit recognition. In Int’l Conf. on Artificial Neural Nets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="13841" citStr="Li and Roth, 2002" startWordPosition="2308" endWordPosition="2311">00-dimensional by filt f eah tlt Tbl 1 Th fly contrast to 300-dimensional for sequential CNN. bid tti i 1100dimil b Table 1 summarizes results in the context of other high-performing efforts in the literature. We use Expimnt three benchmark datasets in two categories: sentiimplement our DTCNN on op of the open rc CNN code y Kim (2014).1 Table 2 ment analysis on both Movie Review (MR) (Pangmmarizs our results i the context of other and Lee, 2005) and Stanford Sentiment Treebankh-performing fforts in the iterture We se (SST-1) (Socher et al., 2013) datasets, and question classification on TREC (Li and Roth, 2002). e benchmark datasets in two categories: senti dataset without a development set (MR), we randomly choose 10% of the training data to indicate early stopping. In order to have a fare comparison with baseline CNN, we also use 3 to 5 as our window size. Most of GPU get better results.2 Our implementation, on top of Kim code,3 will Both sentiment analysis datasets (MR and The differences between them are mainly in the different of categories and whether the standard split is given. There are 10,662 sentences in the MR dataset. Each instance is labeled positive or one sentence. standard split. Ou</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL: Demonstrations,</booktitle>
<pages>55--60</pages>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of ACL: Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shotaro Matsumoto</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Sentiment classification using word sub-sequences and dependency sub-trees.</title>
<date>2005</date>
<booktitle>In Proceedings of PA-KDD.</booktitle>
<contexts>
<context position="2443" citStr="Matsumoto et al., 2005" startWordPosition="344" endWordPosition="347">ment, subjectivity, or other categorization of the sentence. ∗ This work was done at both IBM and CUNY, and was supported in part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions. Bing Xiang‡ Bowen Zhou‡ ‡IBM Watson Group T. J. Watson Research Center, IBM {lhuang,bingxia,zhou}@us.ibm.com Indeed, in the sentiment analysis literature, researchers have incorporated long-distance information from syntactic parse trees, but the results are somewhat inconsistent: some reported small improvements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al., 2003; Kudo and Matsumoto, 2004). As a result, syntactic features have yet to become popular in the sentiment analysis community. We suspect one of the reasons for this is data sparsity (according to our experiments, tree n-grams are significantly sparser than surface n-grams), but this problem has largely been alleviated by the recent advances in word embedding. Can we combine the advantages of both worlds? So we propose a very simple dependency-based convolutional neural networks (DCNNs). Our model is similar to Kim (2014), but while his sequential CNNs pu</context>
</contexts>
<marker>Matsumoto, Takamura, Okumura, 2005</marker>
<rawString>Shotaro Matsumoto, Hiroya Takamura, and Manabu Okumura. 2005. Sentiment classification using word sub-sequences and dependency sub-trees. In Proceedings of PA-KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Mou</author>
<author>Ge Li</author>
<author>Zhi Jin</author>
<author>Lu Zhang</author>
<author>Tao Wang</author>
</authors>
<title>TBCNN: A tree-based convolutional neural network for programming language processing.</title>
<date>2014</date>
<note>Unpublished manuscript: http://arxiv.org/ abs/1409.5718.</note>
<marker>Mou, Li, Jin, Zhang, Wang, 2014</marker>
<rawString>Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang. 2014. TBCNN: A tree-based convolutional neural network for programming language processing. Unpublished manuscript: http://arxiv.org/ abs/1409.5718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Mou</author>
<author>Hao Peng</author>
<author>Ge Li</author>
<author>Yan Xu</author>
<author>Lu Zhang</author>
<author>Zhi Jin</author>
</authors>
<title>Discriminative neural sentence modeling by tree-based convolution.</title>
<date>2015</date>
<booktitle>Unpublished manuscript: http://arxiv.org/abs/1504. 01106v5. Version 5 dated</booktitle>
<contexts>
<context position="3626" citStr="Mou et al. (2015" startWordPosition="529" endWordPosition="532"> but while his sequential CNNs put a word in its sequential context, ours considers a word and its parent, grandparent, great-grand-parent, and siblings on the dependency tree. This way we incorporate longdistance information that are otherwise unavailable on the surface string. Experiments on three classification tasks demonstrate the superior performance of our DCNNs over the baseline sequential CNNs. In particular, our accuracy on the TREC dataset outperforms all previously published results in the literature, including those with heavy hand-engineered features. Independently of this work, Mou et al. (2015, unpublished) reported related efforts; see Sec. 3.3. 2 Dependency-based Convolution The original CNN, first proposed by LeCun et al. (1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al. (2011). Following Kim (2014), one dimensional convolution operates the convolution kernel in sequential order in Equation 1, where xi E Rd represents the d dimensional word representation for the i-th word in 174 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conf</context>
</contexts>
<marker>Mou, Peng, Li, Xu, Zhang, Jin, 2015</marker>
<rawString>Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2015. Discriminative neural sentence modeling by tree-based convolution. Unpublished manuscript: http://arxiv.org/abs/1504. 01106v5. Version 5 dated June 2, 2015; Version 1 (“Tree-based Convolution: A New Architecture for Sentence Modeling”) dated Apr 5, 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="5623" citStr="Pang and Lee, 2005" startWordPosition="849" endWordPosition="853">owever, this setting can convolution operations. However, this setting can not capture longdtance relationships unles we ge t wi efiiy w wold not capture ong-distance relationships unless we itbl au th dt pit pbl enlarge the window indefinitely which would inIn order to cpture the longdistanc dependen evitably caus the data sparsity problem. ces we propose the dependency tree-b In order capture the long-dstance dependevolution model (DTCNN) Figure 1 illustrates an cies wpropose the dependency-based convoluexample frm te Movie Reviews (MR) datase tion model (DCNN). Figure 1 illustrates an exam(Pang and Lee, 2005) h seie o s se ce s oiosly poi, but hi i qite d ple from the Movie Reviews (MR) dataset (Pang filt f til CNN bu nm and Lee, 2005). The sentimnt of this sentence windows wuld include the highl negative word is obviously positive, but is quite difficult for “shortcomings”, and the disance between “Desequential CNNs because many n-gram windows spite” and hortcomings is quite long DTCNN, would include the highly negative word “shorthowever, coud capture the trebased bigram comings”, and the distance between “Despite” and Dpe – orcig, ippig h smnt d h bsed iam “ROOT i “shortcomings” is qute long. D</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings ofACL, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Lucas Champollion</author>
<author>Aravind K Joshi</author>
</authors>
<title>LTAG-spinal and the treebank.</title>
<date>2008</date>
<journal>Language Resources and Evaluation,</journal>
<volume>42</volume>
<issue>1</issue>
<marker>Shen, Champollion, Joshi, 2008</marker>
<rawString>Libin Shen, Lucas Champollion, and Aravind K Joshi. 2008. LTAG-spinal and the treebank. Language Resources and Evaluation, 42(1):1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yelong Shen</author>
<author>Xiaodong he</author>
<author>Jianfeng Gao</author>
<author>Li Deng</author>
<author>Gregoire Mesnil</author>
</authors>
<title>Learning semantic representations using convolutional neural networks for web search.</title>
<date>2014</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="1119" citStr="Shen et al., 2014" startWordPosition="151" endWordPosition="154">convolution approach, making use of tree-based n-grams rather than surface ones, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC. 1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the latter play an important role in many linguistic ph</context>
</contexts>
<marker>Shen, he, Gao, Deng, Mesnil, 2014</marker>
<rawString>Yelong Shen, Xiaodong he, Jianfeng Gao, Li Deng, and Gregoire Mesnil. 2014. Learning semantic representations using convolutional neural networks for web search. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Silva</author>
<author>L Coheur</author>
<author>A C Mendes</author>
<author>Andreas Wichert</author>
</authors>
<title>From symbolic to sub-symbolic information in question classification.</title>
<date>2011</date>
<journal>Artificial Intelligence Review,</journal>
<volume>35</volume>
<contexts>
<context position="17800" citStr="Silva et al., 2011" startWordPosition="2991" endWordPosition="2994">ng+sequential 81.9 49.5 95.4† 88.8† M CNNs CNNs-non-static (Kim, 2014) – baseline 81.5 48.0 93.6 aor+si CNNs-multichannel (Kim, 2014) 81.1 DTN 92.2 86.4∗ Deep CNNs (Kalchbrenner et al., 2014) - 47.4 93.0 86.0∗ DTNs: ao - 48.5 DTNs: Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.7 43.2 CNmne ( - CNNs - CNn - - Recursive Neural Tensor (Socher et al., 2013) - 45.7 - chDeep Recursive NNs (Irsoy and Cardie, 2014) De CN 49.8 - Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.9 NNs - - Re 48.0 Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - Re - - 48.7 Hand-coded rules SVMS (Silva et al., 2011) - ecurNN( 90.8 95.0 h t t er dp ng ParagpVec (Le ad M Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets. Hddd l SVM (Sil t l, 2011) TREC-2 is TREC with fine grained labels. †Results generated by GPU (all others generated by CPU). ∗Results generated from Kim (2014)’s implementation. roo t What is Hawaii ’s state flower ? ⇒ loc roo t enty⇒ desc t What does a defibrill ator do ? ⇒ enty such as numeric:distance, and entity:vehicle. To make our task more realistic and challenging, we also test the proposed model with respect to the 50 subcategories. Ther</context>
<context position="19961" citStr="Silva et al., 2011" startWordPosition="3356" endWordPosition="3359">umbus ’ three ships ? ⇒ LOC:other 3.2 Question Classification In the TREC dataset, the entire dataset of 5,952 sentences are classified into the following 6 categories: abbreviation, entity, description, location and numeric. In this experiment, DCNNs easily outperform any other methods even with ancestor convolution only. DCNNs with sibling achieve the best performance in the published literature. DCNNs combined with sibling and sequential information might suffer from overfitting on the training data based on our observation. One thing to note here is that our best result even exceeds SVMS (Silva et al., 2011) with 60 hand-coded rules. The TREC dataset also provides subcategories (a) enty What is natural gas composed of ? roo (c) desc roo What were (f) ENTY:veh an Figure 3: Examples from TREC (a–c), SST-1 (d) and TREC with finegrained label (f) that ar d TREC with fine-grained label (e–f) that are msclassified by the baseline CNN but corectly misclassified by the baseline CNN but correctly labeled by our DTCNN. For exampl (a) should lald b or DCNN. For example, (a) should be be enity but is labeled location by CNN. entity but is labeled location by CNN. t roo t 177 root What is the speed hummingbir</context>
</contexts>
<marker>Silva, Coheur, Mendes, Wichert, 2011</marker>
<rawString>J. Silva, L. Coheur, A. C. Mendes, and Andreas Wichert. 2011. From symbolic to sub-symbolic information in question classification. Artificial Intelligence Review, 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="17470" citStr="Socher et al., 2011" startWordPosition="2924" endWordPosition="2927">ts r gnea b GPU d i ffii h CPU 4https://github.com/cosmmb/DCNN to 100 f thi tk C = ��c(1) a , ..., C(N.) ) � �� � ˆ (N (1) � �� ˆ ( 1) C�, �c(Ns) s � ancestors ., c siblings i Category Model MR SST-1 TREC TREC-2 This work DCNNs: ancestor 80.4† 47.7† 95.4† 88.4† DCNNs: ancestor+sibling 81.7† 48.3† 95.6† 89.0† DCNNs: ancestor+sibling+sequential 81.9 49.5 95.4† 88.8† M CNNs CNNs-non-static (Kim, 2014) – baseline 81.5 48.0 93.6 aor+si CNNs-multichannel (Kim, 2014) 81.1 DTN 92.2 86.4∗ Deep CNNs (Kalchbrenner et al., 2014) - 47.4 93.0 86.0∗ DTNs: ao - 48.5 DTNs: Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.7 43.2 CNmne ( - CNNs - CNn - - Recursive Neural Tensor (Socher et al., 2013) - 45.7 - chDeep Recursive NNs (Irsoy and Cardie, 2014) De CN 49.8 - Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.9 NNs - - Re 48.0 Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - Re - - 48.7 Hand-coded rules SVMS (Silva et al., 2011) - ecurNN( 90.8 95.0 h t t er dp ng ParagpVec (Le ad M Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets. Hddd l SVM (Sil t l, 2011) TREC-2 is TREC with fine grained labels. †Results generated by GPU (all others generated by C</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. In Proceedings of EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="13775" citStr="Socher et al., 2013" startWordPosition="2297" endWordPosition="2300">each template in Figure 2 . The fully combined representation is 1,100-dimensional by filt f eah tlt Tbl 1 Th fly contrast to 300-dimensional for sequential CNN. bid tti i 1100dimil b Table 1 summarizes results in the context of other high-performing efforts in the literature. We use Expimnt three benchmark datasets in two categories: sentiimplement our DTCNN on op of the open rc CNN code y Kim (2014).1 Table 2 ment analysis on both Movie Review (MR) (Pangmmarizs our results i the context of other and Lee, 2005) and Stanford Sentiment Treebankh-performing fforts in the iterture We se (SST-1) (Socher et al., 2013) datasets, and question classification on TREC (Li and Roth, 2002). e benchmark datasets in two categories: senti dataset without a development set (MR), we randomly choose 10% of the training data to indicate early stopping. In order to have a fare comparison with baseline CNN, we also use 3 to 5 as our window size. Most of GPU get better results.2 Our implementation, on top of Kim code,3 will Both sentiment analysis datasets (MR and The differences between them are mainly in the different of categories and whether the standard split is given. There are 10,662 sentences in the MR dataset. Eac</context>
<context position="16303" citStr="Socher et al., 2013" startWordPosition="2716" endWordPosition="2719">sitive, positive, neutral, negative and verynegative, on an extension of the MR dataset. There Diffrt f MR, th Stfd Sti are 11,855 sentences with Tebak (SST1) ts finergraine lb d Cardie (2014). ry poiive, piive, netral, ni d and thus can not be used as gold-standard trees. e 11,855 sentences with standard split. O 1The phrase-structure trees in SST-1 are actually automatically parsed, odel achieves an accuracy of 49.5 whi 2GPU only supports float32 while CPU supports float64. d only to Irsoy and Cardie (2014) W 3https://github.comw/yoonkim/CNN_sentence trt to 300-dimen 3 Experiments 176 ST1) (Socher et al., 2013) dataets, and qu For all datasets, we first obtain the dependencytion classfication on TREC (L and Roth, 200 parse tree from Stanford parser (Manning et al.,2014).1 Different window ize for different choic For all datasets, we first obtain the dependenparse tree from Sanford parser (Maning et of convolution are shown in Figure 2. For the our results are generated by early stopping In order to have a fare comp due to its efficiency, however CPU could poso ith bseli CNN we als e 3 to 5 as tentially (2014)’s be released.4 idw siz Most f or esults r gnea b GPU d i ffii h CPU 4https://github.com/co</context>
<context position="17551" citStr="Socher et al., 2013" startWordPosition="2941" endWordPosition="2944">= ��c(1) a , ..., C(N.) ) � �� � ˆ (N (1) � �� ˆ ( 1) C�, �c(Ns) s � ancestors ., c siblings i Category Model MR SST-1 TREC TREC-2 This work DCNNs: ancestor 80.4† 47.7† 95.4† 88.4† DCNNs: ancestor+sibling 81.7† 48.3† 95.6† 89.0† DCNNs: ancestor+sibling+sequential 81.9 49.5 95.4† 88.8† M CNNs CNNs-non-static (Kim, 2014) – baseline 81.5 48.0 93.6 aor+si CNNs-multichannel (Kim, 2014) 81.1 DTN 92.2 86.4∗ Deep CNNs (Kalchbrenner et al., 2014) - 47.4 93.0 86.0∗ DTNs: ao - 48.5 DTNs: Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.7 43.2 CNmne ( - CNNs - CNn - - Recursive Neural Tensor (Socher et al., 2013) - 45.7 - chDeep Recursive NNs (Irsoy and Cardie, 2014) De CN 49.8 - Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.9 NNs - - Re 48.0 Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - Re - - 48.7 Hand-coded rules SVMS (Silva et al., 2011) - ecurNN( 90.8 95.0 h t t er dp ng ParagpVec (Le ad M Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets. Hddd l SVM (Sil t l, 2011) TREC-2 is TREC with fine grained labels. †Results generated by GPU (all others generated by CPU). ∗Results generated from Kim (2014)’s implementation. roo t What is Hawaii ’s</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Xiaodong He</author>
<author>Christopher Meek</author>
</authors>
<title>Semantic parsing for single-relation question answering.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1071" citStr="Yih et al., 2014" startWordPosition="143" endWordPosition="146">stic structures, we propose a dependency-based convolution approach, making use of tree-based n-grams rather than surface ones, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC. 1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the lat</context>
</contexts>
<marker>Yih, He, Meek, 2014</marker>
<rawString>Wen-tau Yih, Xiaodong He, and Christopher Meek. 2014. Semantic parsing for single-relation question answering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mattgew Zeiler</author>
</authors>
<title>Adadelta: An adaptive learning rate method.</title>
<date>2012</date>
<note>Unpublished manuscript: http:// arxiv.org/abs/1212.5701.</note>
<contexts>
<context position="10167" citStr="Zeiler 2012" startWordPosition="1677" endWordPosition="1678">llowing Kim (2014) we employ random tion. Each filter will be represented by only one dropot on pnultimate layer (Hintn et al. 2012) feature after max-over-tree pooling. After a series n order t preve co-adaptation of hidden unit. of convolution with different filter with different n our experiments, w set our drop out rat as 05 heights, multiple features carry different structural informationbecome the final representation of the nd learning rate as 0.95 by default. Following K (2), input sentence. Then, this sentence representation ng one ugh gdit dt huffld iibth with he Adadelta udate rul (Zeiler 2012) is passed to a fully connected soft-max layer and outputs a distribution over different labels. 3 Cli Sibi cestorpaths alone i not enough o captre Neural networks often suffer from overtrainmany inguistic phnoena such as conjunctin ing. Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al., 2014). in order to prevent co-adaptation of hidden units. In our experiments, we set our drop out rate as 0.5 and learning rate as 0.95 by default. Following Kim (2014), training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rul</context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>Mattgew Zeiler. 2012. Adadelta: An adaptive learning rate method. Unpublished manuscript: http:// arxiv.org/abs/1212.5701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Parinaz Sobhani</author>
<author>Hongyu Guo</author>
</authors>
<title>Long short-term memory over tree structures.</title>
<date>2015</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="15421" citStr="Zhu et al. (2015)" startWordPosition="2580" endWordPosition="2583">contains is given There are 10,662 sentences in the M Since no standard data split is given, following theliterature we use 10 fold cross vaidation to include dataset Each instance is labeled positive or native ad in most cases contains one setn every sentence in training and testing at least once.Cocatenaing with sibling and sequentia inforSince no standard data splt is given, following literature we se 10 fold cross validtion to icu mation obviously improves DCNNs, and the finalmodel outperforms the baseline sequentia CNNs every sentence in training and esting at least onby 0.4, ad tes with Zhu et al. (2015). Concaenating with sibling and seqaton obvously improvs tree-basd CNNs a Different from MR, the Stanford Sentimentthe final model outperforms the baseline sequ Treebank (SST-1) annotates finer-grained labels,tial CNNs by 04 and ties with Zhu et al (2015 very positive, positive, neutral, negative and verynegative, on an extension of the MR dataset. There Diffrt f MR, th Stfd Sti are 11,855 sentences with Tebak (SST1) ts finergraine lb d Cardie (2014). ry poiive, piive, netral, ni d and thus can not be used as gold-standard trees. e 11,855 sentences with standard split. O 1The phrase-structure </context>
<context position="17665" citStr="Zhu et al., 2015" startWordPosition="2965" endWordPosition="2968">1 TREC TREC-2 This work DCNNs: ancestor 80.4† 47.7† 95.4† 88.4† DCNNs: ancestor+sibling 81.7† 48.3† 95.6† 89.0† DCNNs: ancestor+sibling+sequential 81.9 49.5 95.4† 88.8† M CNNs CNNs-non-static (Kim, 2014) – baseline 81.5 48.0 93.6 aor+si CNNs-multichannel (Kim, 2014) 81.1 DTN 92.2 86.4∗ Deep CNNs (Kalchbrenner et al., 2014) - 47.4 93.0 86.0∗ DTNs: ao - 48.5 DTNs: Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.7 43.2 CNmne ( - CNNs - CNn - - Recursive Neural Tensor (Socher et al., 2013) - 45.7 - chDeep Recursive NNs (Irsoy and Cardie, 2014) De CN 49.8 - Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.9 NNs - - Re 48.0 Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - Re - - 48.7 Hand-coded rules SVMS (Silva et al., 2011) - ecurNN( 90.8 95.0 h t t er dp ng ParagpVec (Le ad M Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets. Hddd l SVM (Sil t l, 2011) TREC-2 is TREC with fine grained labels. †Results generated by GPU (all others generated by CPU). ∗Results generated from Kim (2014)’s implementation. roo t What is Hawaii ’s state flower ? ⇒ loc roo t enty⇒ desc t What does a defibrill ator do ? ⇒ enty such as numeric:distance, and enti</context>
</contexts>
<marker>Zhu, Sobhani, Guo, 2015</marker>
<rawString>Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long short-term memory over tree structures. In Proceedings of ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>