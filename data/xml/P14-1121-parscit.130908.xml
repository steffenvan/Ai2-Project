<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000882">
<title confidence="0.997368">
Looking at Unbalanced Specialized Comparable Corpora
for Bilingual Lexicon Extraction
</title>
<author confidence="0.971481">
Emmanuel Morin and Amir Hazem
</author>
<affiliation confidence="0.885797">
Universit´e de Nantes, LINA UMR CNRS 6241
</affiliation>
<address confidence="0.745297">
2 rue de la houssini`ere, BP 92208, 44322 Nantes Cedex 03, France
</address>
<email confidence="0.872423">
{emmanuel.morin,amir.hazem}@univ-nantes.fr
</email>
<sectionHeader confidence="0.995003" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999918105263158">
The main work in bilingual lexicon ex-
traction from comparable corpora is based
on the implicit hypothesis that corpora are
balanced. However, the historical context-
based projection method dedicated to this
task is relatively insensitive to the sizes
of each part of the comparable corpus.
Within this context, we have carried out
a study on the influence of unbalanced
specialized comparable corpora on the
quality of bilingual terminology extraction
through different experiments. Moreover,
we have introduced a regression model
that boosts the observations of word co-
occurrences used in the context-based pro-
jection method. Our results show that the
use of unbalanced specialized comparable
corpora induces a significant gain in the
quality of extracted lexicons.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948614035088">
The bilingual lexicon extraction task from bilin-
gual corpora was initially addressed by using par-
allel corpora (i.e. a corpus that contains source
texts and their translation). However, despite
good results in the compilation of bilingual lex-
icons, parallel corpora are scarce resources, es-
pecially for technical domains and for language
pairs not involving English. For these reasons,
research in bilingual lexicon extraction has fo-
cused on another kind of bilingual corpora com-
prised of texts sharing common features such as
domain, genre, sampling period, etc. without hav-
ing a source text/target text relationship (McEnery
and Xiao, 2007). These corpora, well known now
as comparable corpora, have also initially been
introduced as non-parallel corpora (Fung, 1995;
Rapp, 1995), and non-aligned corpora (Tanaka
and Iwasaki, 1996). According to Fung and Che-
ung (2004), who range bilingual corpora from par-
allel corpora to quasi-comparable corpora going
through comparable corpora, there is a continuum
from parallel to comparable corpora (i.e. a kind of
filiation).
The bilingual lexicon extraction task from com-
parable corpora inherits this filiation. For instance,
the historical context-based projection method
(Fung, 1995; Rapp, 1995), known as the standard
approach, dedicated to this task seems implicitly
to lead to work with balanced comparable corpora
in the same way as for parallel corpora (i.e. each
part of the corpus is composed of the same amount
of data).
In this paper we want to show that the assump-
tion that comparable corpora should be balanced
for bilingual lexicon extraction task is unfounded.
Moreover, this assumption is prejudicial for spe-
cialized comparable corpora, especially when in-
volving the English language for which many doc-
uments are available due the prevailing position
of this language as a standard for international
scientific publications. Within this context, our
main contribution consists in a re-reading of the
standard approach putting emphasis on the un-
founded assumption of the balance of the spe-
cialized comparable corpora. In specialized do-
mains, the comparable corpora are traditionally of
small size (around 1 million words) in comparison
with comparable corpus-based general language
(up to 100 million words). Consequently, the ob-
servations of word co-occurrences which is the ba-
sis of the standard approach are unreliable. To
make them more reliable, our second contribution
is to contrast different regression models in order
to boost the observations of word co-occurrences.
This strategy allows to improve the quality of ex-
tracted bilingual lexicons from comparable cor-
pora.
</bodyText>
<page confidence="0.933966">
1284
</page>
<note confidence="0.864847">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1284–1293,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.969687" genericHeader="method">
2 Bilingual Lexicon Extraction
</sectionHeader>
<bodyText confidence="0.99994">
In this section, we first describe the standard ap-
proach that deals with the task of bilingual lexi-
con extraction from comparable corpora. We then
present an extension of this approach based on re-
gression models. Finally, we discuss works related
to this study.
</bodyText>
<subsectionHeader confidence="0.994751">
2.1 Standard Approach
</subsectionHeader>
<bodyText confidence="0.999851507936508">
The main work in bilingual lexicon extraction
from comparable corpora is based on lexical con-
text analysis and relies on the simple observation
that a word and its translation tend to appear in
the same lexical contexts. The basis of this obser-
vation consists in the identification of “first-order
affinities” for each source and target language:
“First-order affinities describe what other words
are likely to be found in the immediate vicinity
of a given word” (Grefenstette, 1994, p. 279).
These affinities can be represented by context vec-
tors, and each vector element represents a word
which occurs within the window of the word to
be translated (e.g. a seven-word window approxi-
mates syntactic dependencies). In order to empha-
size significant words in the context vector and to
reduce word-frequency effects, the context vectors
are normalized according to an association mea-
sure. Then, the translation is obtained by compar-
ing the source context vector to each translation
candidate vector after having translated each ele-
ment of the source vector with a general dictio-
nary.
The implementation of the standard approach
can be carried out by applying the following
three steps (Rapp, 1999; Chiao and Zweigenbaum,
2002; D´ejean et al., 2002; Morin et al., 2007;
Laroche and Langlais, 2010, among others):
Computing context vectors We collect all the
words in the context of each word i and count
their occurrence frequency in a window of
n words around i. For each word i of the
source and the target languages, we obtain
a context vector vi which gathers the set of
co-occurrence words j associated with the
number of times that j and i occur together
cooc(i, j). In order to identify specific words
in the lexical context and to reduce word-
frequency effects, we normalize context vec-
tors using an association score such as Mu-
tual Information, Log-likelihood, or the dis-
counted log-odds (LO) (Evert, 2005) (see
equation 1 and Table 1 where N = a + b +
c + d).
Transferring context vectors Using a bilingual
dictionary, we translate the elements of the
source context vector. If the bilingual dictio-
nary provides several translations for an ele-
ment, we consider all of them but weight the
different translations according to their fre-
quency in the target language.
Finding candidate translations For a word to be
translated, we compute the similarity be-
tween the translated context vector and all
target vectors through vector distance mea-
sures such as Jaccard or Cosine (see equa-
tion 2 where associj stands for “association
score”, vk is the transferred context vector of
the word k to translate, and vl is the con-
text vector of the word l in the target lan-
guage). Finally, the candidate translations of
a word are the target words ranked following
the similarity score.
</bodyText>
<equation confidence="0.963396666666667">
j ¬j
a = cooc(i, j) b = cooc(i, ¬j)
c = cooc(¬i, j) d = cooc(¬i, ¬j)
Table 1: Contingency table
(a+ 12) × (d+ 1 2)
LO(i, j) =log (b + 1 2) × (c+ 1 2) (1)
� associ assock
Cosinev� = t t t (2)
�&amp; assoclt2 �&amp; assoc2 t
</equation>
<bodyText confidence="0.99449075">
This approach is sensitive to the choice of pa-
rameters such as the size of the context, the choice
of the association and similarity measures. The
most complete study about the influence of these
parameters on the quality of word alignment has
been carried out by Laroche and Langlais (2010).
The standard approach is used by most re-
searchers so far (Rapp, 1995; Fung, 1998; Pe-
ters and Picchi, 1998; Rapp, 1999; Chiao and
Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier
et al., 2004; Morin et al., 2007; Laroche and
Langlais, 2010; Prochasson and Fung, 2011;
</bodyText>
<equation confidence="0.726339">
i
¬i
</equation>
<page confidence="0.9764">
1285
</page>
<table confidence="0.997981181818182">
References Domain Languages Source/Target Sizes
Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words
Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data
Rapp (1999) Newspaper GE/EN 135/163 million words
Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words
D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words
Morin et al. (2007) Medical FR/JP 693,666/807,287 words
Otero (2007) European Parliament SP/EN 14/17 million words
Ismail and Manandhar (2010) European Parliament EN/SP 500,000/500,000 sentences
Bouamor et al. (2013) Financial FR/EN 402,486/756,840 words
- Medical FR/EN 396,524/524,805 words
</table>
<tableCaption confidence="0.999628">
Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction
</tableCaption>
<bodyText confidence="0.9984989">
Bouamor et al., 2013, among others) with the im-
plicit hypothesis that comparable corpora are bal-
anced. As McEnery and Xiao (2007, p. 21) ob-
serve, a specialized comparable corpus is built
as balanced by analogy with a parallel corpus:
“Therefore, in relation to parallel corpora, it is
more likely for comparable corpora to be designed
as general balanced corpora.”. For instance, Ta-
ble 2 describes the comparable corpora used in the
main work dedicated to bilingual lexicon extrac-
tion for which the ratio between the size of the
source and the target texts is comprised between
1 and 1.8.
In fact, the assumption that words which have
the same meaning in different languages should
have the same lexical context distributions does
not involve working with balanced comparable
corpora. To our knowledge, no attention1 has
been paid to the problem of using unbalanced
comparable corpora for bilingual lexicon extrac-
tion. Since the context vectors are computed from
each part of the comparable corpus rather than
through the parts of the comparable corpora, the
standard approach is relatively insensitive to dif-
ferences in corpus sizes. The only precaution for
using the standard approach with unbalanced cor-
pora is to normalize the association measure (for
instance, this can be done by dividing each entry
of a given context vector by the sum of its associ-
ation scores).
</bodyText>
<subsectionHeader confidence="0.990295">
2.2 Prediction Model
</subsectionHeader>
<bodyText confidence="0.967252">
Since comparable corpora are usually small in spe-
cialized domains (see Table 2), the discrimina-
</bodyText>
<footnote confidence="0.937193">
1We only found mention of this aspect in Diab and Finch
(2000, p. 1501) “In principle, we do not have to have the
same size corpora in order for the approach to work”.
</footnote>
<bodyText confidence="0.999944783783784">
tive power of context vectors (i.e. the observa-
tions of word co-occurrences) is reduced. One
way to deal with this problem is to re-estimate
co-occurrence counts by a prediction function
(Hazem and Morin, 2013). This consists in as-
signing to each observed co-occurrence count of
a small comparable corpora, a new value learned
beforehand from a large training corpus.
In order to make co-occurrence counts more
discriminant and in the same way as Hazem
and Morin (2013), one strategy consists in ad-
dressing this problem through regression: given
training corpora of small and large size (abun-
dant in the general domain), we predict word co-
occurrence counts in order to make them more
reliable. We then apply the resulting regression
function to each word co-occurrence count as a
pre-processing step of the standard approach. Our
work differs from Hazem and Morin (2013) in two
ways. First, while they experienced the linear re-
gression model, we propose to contrast different
regression models. Second, we apply regression
to unbalanced comparable corpora and study the
impact of prediction when applied to the source
texts, the target texts and both source and target
texts of the used comparable corpora.
We use regression analysis to describe the rela-
tionship between word co-occurrence counts in a
large corpus (the response variable) and word co-
occurrence counts in a small corpus (the predictor
variable). As most regression models have already
been described in great detail (Christensen, 1997;
Agresti, 2007), the derivation of most models is
only briefly introduced in this work.
As we can not claim that the prediction of word
co-occurrence counts is a linear problem, we con-
sider in addition to the simple linear regression
</bodyText>
<page confidence="0.94716">
1286
</page>
<bodyText confidence="0.999937375">
model (Lin), a generalized linear model which is
the logistic regression model (Logit) and non lin-
ear regression models such as polynomial regres-
sion model (Polyn) of order n. Given an input
vector x E Rm, where x1,...,xm represent fea-
tures, we find a prediction y� E Rm for the co-
occurrence count of a couple of words y E R us-
ing one of the regression models presented below:
</bodyText>
<equation confidence="0.99997025">
yLin = β0 + β1x (3)
1
yLogit = 1 + exp(�(β0 + β1x)) (4)
�yPolyn = β0 + β1x + β2x2 + ... + βnxn (5)
</equation>
<bodyText confidence="0.998928333333333">
where βi are the parameters to estimate.
Let us denote by f the regression function and
by cooc(wi, wj) the co-occurrence count of the
words wi and wj. The resulting predicted value of
cooc(wi, wj), noted cooc(wi, wj) is given by the
following equation:
</bodyText>
<equation confidence="0.602602">
cooc(wi, wj) = f(cooc(wi, wj)) (6)
</equation>
<subsectionHeader confidence="0.613648">
2.3 Related Work
</subsectionHeader>
<bodyText confidence="0.999954396226415">
In the past few years, several contributions have
been proposed to improve each step of the stan-
dard approach.
Prochasson et al. (2009) enhance the represen-
tativeness of the context vector by strengthening
the context words that happen to be transliterated
words and scientific compound words in the target
language. Ismail and Manandhar (2010) also sug-
gest that context vectors should be based on the
most important contextually relevant words (in-
domain terms), and thus propose a method for fil-
tering the noise of the context vectors. In another
way, Rubino and Linar`es (2011) improve the con-
text words based on the hypothesis that a word and
its candidate translations share thematic similari-
ties. Yu and Tsujii (2009) and Otero (2007) pro-
pose, for their part, to replace the window-based
method by a syntax-based method in order to im-
prove the representation of the lexical context.
To improve the transfer context vectors step,
and increase the number of elements of translated
context vectors, Chiao and Zweigenbaum (2003)
and Morin and Prochasson (2011) combine a stan-
dard general language dictionary with a special-
ized dictionary, whereas D´ejean et al. (2002) use
the hierarchical properties of a specialized the-
saurus. Koehn and Knight (2002) automatically
induce the initial seed bilingual dictionary by us-
ing identical spelling features such as cognates
and similar contexts. As regards the problem of
words ambiguities, Bouamor et al. (2013) carried
out word sense disambiguation process only in
the target language whereas Gaussier et al. (2004)
solve the problem through the source and target
languages by using approaches based on CCA
(Canonical Correlation Analysis) and multilingual
PLSA (Probabilistic Latent Semantic Analysis).
The rank of candidate translations can be im-
proved by integrating different heuristics. For in-
stance, Chiao and Zweigenbaum (2002) introduce
a heuristic based on word distribution symme-
try. From the ranked list of candidate translations,
the standard approach is applied in the reverse
direction to find the source counterparts of the
first target candidate translations. And then only
the target candidate translations that had the ini-
tial source word among the first reverse candidate
translations are kept. Laroche and Langlais (2010)
suggest a heuristic based on the graphic similarity
between source and target terms. Here, candidate
translations which are cognates of the word to be
translated are ranked first among the list of trans-
lation candidates.
</bodyText>
<sectionHeader confidence="0.989792" genericHeader="method">
3 Linguistic Resources
</sectionHeader>
<bodyText confidence="0.9998615">
In this section, we outline the different textual re-
sources used for our experiments: the comparable
corpora, the bilingual dictionary and the terminol-
ogy reference lists.
</bodyText>
<subsectionHeader confidence="0.999225">
3.1 Specialized Comparable Corpora
</subsectionHeader>
<bodyText confidence="0.999852692307692">
For our experiments, we used two specialized
French/English comparable corpora:
Breast cancer corpus This comparable corpus is
composed of documents collected from the
Elsevier website2. The documents were taken
from the medical domain within the sub-
domain of “breast cancer”. We have auto-
matically selected the documents published
between 2001 and 2008 where the title or the
keywords contain the term cancer du sein in
French and breast cancer in English. We col-
lected 130 French documents (about 530,000
words) and 1,640 English documents (about
</bodyText>
<footnote confidence="0.992876">
2http://www.elsevier.com
</footnote>
<page confidence="0.989113">
1287
</page>
<bodyText confidence="0.999632777777778">
7.4 million words). We split the English doc-
uments into 14 parts each containing about
530,000 words.
Diabetes corpus The documents making up the
French part of the comparable corpus have
been craweled from the web using three
keywords: diab`ete (diabetes), alimentation
(food), and ob´esit´e (obesity). After a man-
ual selection, we only kept the documents
which were relative to the medical domain.
As a result, 65 French documents were ex-
tracted (about 257,000 words). The English
part has been extracted from the medical
website PubMed3 using the keywords: dia-
betes, nutrition and feeding. We only kept
the free fulltext available documents. As a re-
sult, 2,339 English documents were extracted
(about 3,5 million words). We also split the
English documents into 14 parts each con-
taining about 250,000 words.
The French and English documents were then
normalised through the following linguistic pre-
processing steps: tokenisation, part-of-speech tag-
ging, and lemmatisation. These steps were car-
ried out using the TTC TermSuite4 that applies
the same method to several languages including
French and English. Finally, the function words
were removed and the words occurring less than
twice in the French part and in each English part
were discarded. Table 3 shows the number of dis-
tinct words (# words) after these steps. It also
indicates the comparability degree in percentage
(comp.) between the French part and each English
part of each comparable corpus. The comparabil-
ity measure (Li and Gaussier, 2010) is based on
the expectation of finding the translation for each
word in the corpus and gives a good idea about
how two corpora are comparable. We can notice
that all the comparable corpora have a high degree
of comparability with a better comparability of the
breast cancer corpora as opposed to the diabetes
corpora. In the remainder of this article, [breast
cancer corpus i] for instance stands for the breast
cancer comparable corpus composed of the unique
French part and the English part i (i E [1, 14]).
</bodyText>
<subsectionHeader confidence="0.998915">
3.2 Bilingual Dictionary
</subsectionHeader>
<bodyText confidence="0.9853055">
The bilingual dictionary used in our experiments
is the French/English dictionary ELRA-M0033
</bodyText>
<footnote confidence="0.9998045">
3http://www.ncbi.nlm.nih.gov/pubmed/
4http://code.google.com/p/ttc-project
</footnote>
<table confidence="0.999950578947368">
Breast cancer Diabetes
# words (comp.) # words (comp.)
French 7,376 4,982
Part 1
English
Part 1 8,214 (79.2) 5,181 (75.2)
Part 2 7,788 (78.8) 5,446 (75.9)
Part 3 8,370 (78.8) 5,610 (76.6)
Part 4 7,992 (79.3) 5,426 (74.8)
Part 5 7,958 (78.7) 5,610 (75.0)
Part 6 8,230 (79.1) 5,719 (73.6)
Part 7 8,035 (78.3) 5,362 (75.6)
Part 8 8,008 (78.8) 5,432 (74.6)
Part 9 8,334 (79.6) 5,398 (74.2)
Part 10 7,978 (79.1) 5,059 (75.6)
Part 11 8,373 (79.4) 5,264 (74.9)
Part 12 8,065 (78.9) 4,644 (73.4)
Part 13 7,847 (80.0) 5,369 (74.8)
Part 14 8,457 (78.9) 5,669 (74.8)
</table>
<tableCaption confidence="0.890911333333333">
Table 3: Number of distinct words (# words) and
degree of comparability (comp.) for each compa-
rable corpora
</tableCaption>
<bodyText confidence="0.99487175">
available from the ELRA catalogue5. This re-
source is a general language dictionary which con-
tains only a few terms related to the medical do-
main.
</bodyText>
<subsectionHeader confidence="0.997006">
3.3 Terminology Reference Lists
</subsectionHeader>
<bodyText confidence="0.999974071428572">
To evaluate the quality of terminology extrac-
tion, we built a bilingual terminology reference
list for each comparable corpus. We selected
all French/English single words from the UMLS6
meta-thesaurus. We kept only i) the French sin-
gle words which occur more than four times in the
French part and ii) the English single words which
occur more than four times in each English part
i7. As a result of filtering, 169 French/English
single words were extracted for the breast can-
cer corpus and 244 French/English single words
were extracted for the diabetes corpus. It should
be noted that the evaluation of terminology ex-
traction using specialized comparable corpora of-
</bodyText>
<footnote confidence="0.996712">
5http://www.elra.info/
6http://www.nlm.nih.gov/research/umls
7The threshold sets to four is required to build a bilin-
gual terminology reference list composed of about a hundred
words. This value is very low to obtain representative context
vectors. For instance, Prochasson and Fung (2011) showed
that the standard approach is not relevant for infrequent words
(since the context vectors are very unrepresentative i.e. poor
in information).
</footnote>
<page confidence="0.81929">
1288
</page>
<table confidence="0.99805325">
Breast cancer corpus
1 2 3 4 5 6 7 8 9 10 11 12 13 14
Balanced 26.1 26.2 21.0 27.0 22.8 27.1 26.3 25.8 29.2 23.3 21.7 29.6 29.1 26.1
Unbalanced 26.1 31.9 34.7 36.0 37.7 36.4 36.6 37.2 39.8 40.5 40.6 42.3 40.9 41.6
Diabetes corpus
1 2 3 4 5 6 7 8 9 10 11 12 13 14
Balanced 13.6 13.5 11.9 14.6 14.6 11.0 16.5 10.5 12.9 13.3 15.2 11.8 13.0 14.3
Unbalanced 13.6 17.5 18.9 21.2 23.4 23.8 24.8 24.7 24.7 24.4 24.8 25.2 26.0 24.9
</table>
<tableCaption confidence="0.95931">
Table 4: Results (MAP %) of the standard approach using the balanced and unbalanced comparable
corpora
</tableCaption>
<bodyText confidence="0.660645">
ten relies on lists of a small size: 95 single
words in Chiao and Zweigenbaum (2002), 100 in
Morin et al. (2007), 125 and 79 in Bouamor et
al. (2013).
</bodyText>
<sectionHeader confidence="0.99884" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.998102222222222">
In this section, we present experiments to evaluate
the influence of comparable corpus size and pre-
diction models on the quality of bilingual termi-
nology extraction.
We present the results obtained for the terms be-
longing to the reference list for English to French
direction measured in terms of the Mean Average
Precision (MAP) (Manning et al., 2008) as fol-
lows:
</bodyText>
<equation confidence="0.995474666666667">
|Ref |1
(7)
Ref  |� rz
</equation>
<bodyText confidence="0.999030666666667">
where |Ref |is the number of terms of the refer-
ence list and rz the rank of the correct candidate
translation i.
</bodyText>
<subsectionHeader confidence="0.998656">
4.1 Standard Approach Evaluation
</subsectionHeader>
<bodyText confidence="0.999958528301887">
In order to evaluate the influence of corpus size on
the bilingual terminology extraction task, two ex-
periments have been carried out using the standard
approach. We first performed an experiment using
each comparable corpus independently of the oth-
ers (we refer to these corpora as balanced corpora).
We then conducted a second experiment where we
varied the size of the English part of the compara-
ble corpus, from 530,000 to 7.4 million words for
the breast cancer corpus in 530,000 words steps,
and from 250,000 to 3.5 million words for the di-
abetes corpus in 250,000 words steps (we refer to
these corpora as unbalanced corpora). In the ex-
periments reported here, the size of the context
window w was set to 3 (i.e. a seven-word window
that approximates syntactic dependencies), the re-
tained association and similarity measures were
the discounted log-odds and the Cosine (see Sec-
tion 2.1). The results shown were those that give
the best performance for the comparable corpora
used individually.
Table 4 shows the results of the standard ap-
proach on the balanced and the unbalanced breast
cancer and diabetes comparable corpora. Each
column corresponds to the English part i (i E
[1,14]) of a given comparable corpus. The first
line presents the results for each individual com-
parable corpus and the second line presents the re-
sults for the cumulative comparable corpus. For
instance, the column 3 indicates the MAP obtained
by using a comparable corpus that is composed i)
only of [breast cancer corpus 3] (MAP of 21.0%),
and ii) of [breast cancer corpus 1, 2 and 3] (MAP
of 34.7%).
As a preliminary remark, we can notice that the
results differ noticeably according to the compa-
rable corpus used individually (MAP variation be-
tween 21.0% and 29.6% for the breast cancer cor-
pora and between 10.5% and 16.5% for the dia-
betes corpora). We can also note that the MAP
of all the unbalanced comparable corpora is al-
ways higher than any individual comparable cor-
pus. Overall, starting with a MAP of 26.1% as
provided by the balanced [breast cancer corpus 1],
we are able to increase it to 42.3% with the un-
balanced [breast cancer corpus 12] (the variation
observed for some unbalanced corpora such as
[diabetes corpus 12, 13 and 14] can be explained
by the fact that adding more data in the source
language increases the error rate of the translation
phase of the standard approach, which leads to the
introduction of additional noise in the translated
context vectors).
</bodyText>
<equation confidence="0.98572">
MAP(Ref) = 1
</equation>
<page confidence="0.984261">
1289
</page>
<table confidence="0.99967625">
Balanced breast cancer corpus
1 2 3 4 5 6 7 8 9 10 11 12 13 14
No prediction 26.1 26.2 21.0 27.0 22.8 27.1 26.3 25.8 29.2 23.3 21.7 29.6 29.1 26.1
Sourcep-d 26.5 26.0 23.0 30.0 25.4 30.1 28.3 29.4 32.1 24.9 24.4 30.5 30.1 29.0
Targetp,.ed 19.5 20.0 17.2 23.4 19.9 23.1 21.4 21.6 24.1 19.3 18.1 26.6 24.3 22.6
Sourcep-d + Targetp-d 23.9 21.9 20.5 25.8 23.5 25.3 24.1 26.1 27.4 22.5 21.0 25.6 28.5 24.6
Balanced diabetes corpus
1 2 3 4 5 6 7 8 9 10 11 12 13 14
No prediction 13.6 13.5 11.9 14.6 14.6 11.0 16.5 10.5 12.9 13.3 15.2 11.8 13.0 14.3
Sourcep-d 13.9 14.3 12.6 15.5 14.9 10.9 17.6 11.1 14.0 14.2 16.4 13.3 13.5 15.7
Targetp-d 09.8 09.0 08.3 11.9 10.1 08.0 15.9 07.3 10.8 10.0 10.1 08.8 10.8 10.2
Sourcep-d + Targetp-d 10.9 11.0 09.0 13.6 11.8 08.6 15.4 07.7 12.8 11.5 11.9 10.5 11.7 11.8
</table>
<tableCaption confidence="0.797378">
Table 5: Results (MAP %) of the standard approach using the Lin regression model on the balanced
breast cancer and diabetes corpora (comparison of predicting the source side, the target side and both
sides of the comparable corpora)
</tableCaption>
<subsectionHeader confidence="0.991017">
4.2 Prediction Evaluation
</subsectionHeader>
<bodyText confidence="0.999843">
The aim of this experiment is two-fold: first, we
want to evaluate the usefulness of predicting word
co-occurrence counts and second, we want to find
out whether it is more appropriate to apply predic-
tion to the source side, the target side or both sides
of the bilingual comparable corpora.
</bodyText>
<table confidence="0.999255833333333">
Breast cancer Diabetes
No prediction 29.6 16.5
Lin 30.5 17.6
Poly2 30.6 17.5
Poly3 30.4 17.6
Logit 22.3 13.6
</table>
<tableCaption confidence="0.983494">
Table 6: Results (MAP %) of the standard ap-
</tableCaption>
<bodyText confidence="0.625359">
proach using different regression models on the
balanced breast cancer and diabetes corpora
</bodyText>
<subsubsectionHeader confidence="0.597677">
4.2.1 Regression Models Comparison
</subsubsectionHeader>
<bodyText confidence="0.999975176470588">
We contrast the prediction models presented in
Section 2.2 to findout which is the most appropri-
ate model to use as a pre-processing step of the
standard approach. We chose the balanced corpora
where the standard approach has shown the best
results in the previous experiment, namely [breast
cancer corpus 12] and [diabetes corpus 7].
Table 6 shows a comparison between the
standard approach without prediction noted No
prediction and the standard approach with pre-
diction models. We contrast the simple linear re-
gression model (Lin) with the second and the third
order polynomial regressions (Poly2 and Poly3)
and the logistic regression model (Logit). We
can notice that except for the Logit model, all the
regression models outperform the baseline (No
prediction). Also, as we can see, the results
obtained with the linear and polynomial regres-
sions are very close. This suggests that both linear
and polynomial regressions are suitable as a pre-
processing step of the standard approach, while
the logistic regression seems to be inappropriate
according to the results shown in Table 6.
That said, the gain of regression models is not
significant. This may be due to the regression pa-
rameters that have been learned from a training
corpus of the general domain. Another reason that
could explain these results is the prediction pro-
cess. We applied the same regression function
to all co-occurrence counts while learning mod-
els for low and high frequencies should have been
more appropriate. In the light of the above results,
we believe that prediction can be beneficial to our
task.
</bodyText>
<subsectionHeader confidence="0.765682">
4.2.2 Source versus Target Prediction
</subsectionHeader>
<bodyText confidence="0.999975846153846">
Table 5 shows a comparison between the standard
approach without prediction noted No prediction
and the standard approach based on the predic-
tion of the source side noted 5ourcepred, the tar-
get side noted Targetpred and both sides noted
5ourcepred+Targetpred. If prediction can not re-
place a large amount of data, it aims at increasing
co-occurrence counts as if large amounts of data
were at our disposal. In this case, applying pre-
diction to the source side may simulate a config-
uration of using unbalanced comparable corpora
where the source side is n times bigger than the
target side. Predicting the target side only, may
</bodyText>
<page confidence="0.920668">
1290
</page>
<figure confidence="0.990048060606061">
MaP (%)
MaP (%)
50
45
40
35
30
25
20
15
10
5
01 2 3 4 5 6 7 8 9 10 11 12 13 14
[English-i]-French breast cancer corpus
(a)
[English-i]-French diabetes corpus
(b)
35
30
25
20
15
10
5
01 2 3 4 5 6 7 8 9 10 11 12 13 14
Balanced
Balanced+Prediction
Unbalanced
Unbalanced+Prediction
Balanced
Balanced+Prediction
Unbalanced
Unbalanced+Prediction
</figure>
<figureCaption confidence="0.820485">
Figure 1: Results (MAP %) of the standard approach using the best configurations of the prediction
models (Lin for Balanced + Prediction and Poly2 for Unbalanced + Prediction) on the breast
cancer and the diabetes corpora
</figureCaption>
<bodyText confidence="0.99984484">
leads us to the opposite configuration where the
target side is n times bigger than the source side.
Finally, predicting both sides may simulate a large
comparable corpora on both sides. In this experi-
ment, we chose to use the linear regression model
(Lin) for the prediction part. That said, the other
regression models have shown the same behavior
as Lin.
We can see that the best results are obtained by
the 5ourcep,,d approach for both comparable cor-
pora. We can also notice that predicting the tar-
get side and both sides of the comparable corpora
degrades the results. It is not surprising that pre-
dicting the target side only leads to lower results,
since it is well known that a better characterization
of a word to translate (given from the source side)
leads to better results. We can deduce from Ta-
ble 5 that source prediction is the most appropriate
configuration to improve the quality of extracted
lexicons. This configuration which simulates the
use of unbalanced corpora leads us to think that
using prediction with unbalanced comparable cor-
pora should also increase the performance of the
standard approach. This assumption is evaluated
in the next Subsection.
</bodyText>
<subsectionHeader confidence="0.999838">
4.3 Predicting Unbalanced Corpora
</subsectionHeader>
<bodyText confidence="0.9999588">
In this last experiment we contrast the standard
approach applied to the balanced and unbalanced
corpora noted Balanced and Unbalanced with
the standard approach combined with the predic-
tion model noted Balanced + Prediction and
</bodyText>
<subsubsectionHeader confidence="0.876193">
Unbalanced + Prediction.
</subsubsectionHeader>
<bodyText confidence="0.999476419354839">
Figure 1(a) illustrates the results of the exper-
iments conducted on the breast cancer corpus.
We can see that the Unbalanced approach sig-
nificantly outperforms the baseline (Balanced).
The big difference between the Balanced and
the Unbalanced approaches would indicate that
the latter is optimal. We can also notice that the
prediction model applied to the balanced corpus
(Balanced + Prediction) slightly outperforms
the baseline while the Unbalanced + Prediction
approach significantly outperforms the three other
approaches (moreover the variation observed with
the Unbalanced approach are lower than the
Unbalanced + Prediction approach). Overall,
the prediction increases the performance of the
standard approach especially for unbalanced cor-
pora.
The results of the experiments conducted on
the diabetes corpus are shown in Figure 1(b). As
for the previous experiment, we can see that the
Unbalanced approach significantly outperforms
the Balanced approach. This confirms the unbal-
anced hypothesis and would motivate the use of
unbalanced corpora when they are available. We
can also notice that the Balanced + Prediction
approach slightly outperforms the baseline while
the Unbalanced+Prediction approach gives the
best results. Here also, the prediction increases the
performance of the standard approach especially
for unbalanced corpora. It is clear that in addi-
tion to the benefit of using unbalanced comparable
</bodyText>
<page confidence="0.965812">
1291
</page>
<bodyText confidence="0.999771">
corpora, prediction shows a positive impact on the
performance of the standard approach.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999980153846154">
In this paper, we have studied how an unbalanced
specialized comparable corpus could influence the
quality of the bilingual lexicon extraction. This as-
pect represents a significant interest when working
with specialized comparable corpora for which the
quantity of the data collected may differ depend-
ing on the languages involved, especially when in-
volving the English language as many scientific
documents are available. More precisely, our dif-
ferent experiments show that using an unbalanced
specialized comparable corpus always improves
the quality of word translations. Thus, the MAP
goes up from 29.6% (best result on the balanced
corpora) to 42.3% (best result on the unbalanced
corpora) in the breast cancer domain, and from
16.5% to 26.0% in the diabetes domain. Addition-
ally, these results can be improved by using a pre-
diction model of the word co-occurrence counts.
Here, the MAP goes up from 42.3% (best result
on the unbalanced corpora) to 46.9% (best result
on the unbalanced corpora with prediction) in the
breast cancer domain, and from 26.0% to 29.8%
in the diabetes domain. We hope that this study
will pave the way for using specialized unbalanced
comparable corpora for bilingual lexicon extrac-
tion.
</bodyText>
<sectionHeader confidence="0.999063" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.623317">
This work is supported by the French National Re-
search Agency under grant ANR-12-CORD-0020.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999314097222222">
Alan Agresti. 2007. An Introduction to Categorical
Data Analysis (2nd ed.). Wiley &amp; Sons, Inc., Hobo-
ken, New Jersey.
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2013. Context vector disambigua-
tion for bilingual lexicon extraction from compa-
rable corpora. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL’13), pages 759–764, Sofia, Bulgaria.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings of
the 19th International Conference on Computational
Linguistics (COLING’02), pages 1208–1212, Tapei,
Taiwan.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003.
The Effect of a General Lexicon in Corpus-Based
Identification of French-English Medical Word
Translations. In The New Navigators: from Profes-
sionals to Patients, Actes Medical Informatics Eu-
rope, pages 397–402.
Ronald Christensen. 1997. Log-Linear Models and
Logistic Regression. Springer-Verlag, Berlin.
Herv´e D´ejean, Fatia Sadat, and ´Eric Gaussier. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In Proceedings of the 19th International Conference
on Computational Linguistics (COLING’02), pages
218–224, Tapei, Taiwan.
Mona T. Diab and Steve Finch. 2000. A Statistical
Word-Level Translation Model for Comparable Cor-
pora. In Proceedings of the 6th International Con-
ference on Computer-Assisted Information Retrieval
(RIAO’00), pages 1500–1501, Paris, France.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
Universit¨at Stuttgart, Germany.
Pascale Fung and Percy Cheung. 2004. Multi-
level bootstrapping for extracting parallel sentences
from a quasi-comparable corpus. In Proceedings
of the 20th International Conference on Computa-
tional Linguistics (COLING’04), pages 1051–1057,
Geneva, Switzerland.
Pascale Fung and Kathleen McKeown. 1997. Finding
Terminology Translations from Non-parallel Cor-
pora. In Proceedings of the 5th Annual Workshop
on Very Large Corpora (VLC’97), pages 192–202,
Hong Kong.
Pascale Fung. 1995. Compiling Bilingual Lexicon
Entries from a non-Parallel English-Chinese Cor-
pus. In Proceedings of the 3rd Annual Workshop
on Very Large Corpora (VLC’95), pages 173–183,
Cambridge, MA, USA.
Pascale Fung. 1998. A Statistical View on Bilin-
gual Lexicon Extraction: From Parallel Corpora to
Non-parallel Corpora. In David Farwell, Laurie
Gerber, and Eduard Hovy, editors, Proceedings of
the 3rd Conference of the Association for Machine
Translation in the Americas (AMTA’98), pages 1–
16, Langhorne, PA, USA.
´Eric Gaussier, Jean-Michel Renders, Irena Matveeva,
Cyril Goutte, and Herv´e D´ejean. 2004. A
Geometric View on Bilingual Lexicon Extraction
from Comparable Corpora. In Proceedings of the
42nd Annual Meeting of the Association for Com-
putational Linguistics (ACL’04), pages 526–533,
Barcelona, Spain.
Gregory Grefenstette. 1994. Corpus-Derived First,
Second and Third-Order Word Affinities. In Pro-
ceedings of the 6th Congress of the European As-
sociation for Lexicography (EURALEX’94), pages
279–290, Amsterdam, The Netherlands.
</reference>
<page confidence="0.826963">
1292
</page>
<reference confidence="0.999801242424242">
Amir Hazem and Emmanuel Morin. 2013. Word
co-occurrence counts prediction for bilingual ter-
minology extraction from comparable corpora. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing (IJCNLP’13),
pages 1392–1400, Nagoya, Japan.
Azniah Ismail and Suresh Manandhar. 2010. Bilingual
lexicon extraction from comparable corpora using
in-domain terms. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING’10), pages 481–489, Beijing, China.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition (ULA’02), pages 9–16,
Philadelphia, PA, USA.
Audrey Laroche and Philippe Langlais. 2010. Revis-
iting Context-based Projection Methods for Term-
Translation Spotting in Comparable Corpora. In
Proceedings of the 23rd International Conference
on Computational Linguistics (COLING’10), pages
617–625, Beijing, China.
Bo Li and ´Eric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics (COLING’10), pages 644–652, Beijing, China.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schtze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Anthony McEnery and Zhonghua Xiao. 2007. Paral-
lel and comparable corpora: What are they up to?
In Gunilla Anderman and Margaret Rogers, editors,
Incorporating Corpora: Translation and the Lin-
guist, Multilingual Matters, chapter 2, pages 18–31.
Clevedon, UK.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora (BUCC’11), pages 27–34, Portland,
OR, USA.
Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual Terminology
Mining – Using Brain, not brawn comparable cor-
pora. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL’07), pages 664–671, Prague, Czech Republic.
Pablo Gamallo Otero. 2007. Learning bilingual lexi-
cons from comparable english and spanish corpora.
In Proceedings of the 11th Conference on Machine
Translation Summit (MT Summit XI), pages 191–
198, Copenhagen, Denmark.
Carol Peters and Eugenio Picchi. 1998. Cross-
language information retrieval: A system for com-
parable corpus querying. In Gregory Grefenstette,
editor, Cross-language information retrieval, chap-
ter 7, pages 81–90. Kluwer Academic Publishers.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
Word Translation Extraction from Aligned Compa-
rable Documents. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics (ACL’11), pages 1327–1335, Portland, OR,
USA.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexicon
extraction from small comparable corpora. In Pro-
ceedings of the 12th Conference on Machine Trans-
lation Summit (MT Summit XII), pages 284–291, Ot-
tawa, Canada.
Reinhard Rapp. 1995. Identify Word Translations in
Non-Parallel Texts. In Proceedings of the 35th An-
nual Meeting of the Association for Computational
Linguistics (ACL’95), pages 320–322, Boston, MA,
USA.
Reinhard Rapp. 1999. Automatic Identification of
Word Translations from Unrelated English and Ger-
man Corpora. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics (ACL’99), pages 519–526, College Park,
MD, USA.
Rapha¨el Rubino and Georges Linar`es. 2011. A multi-
view approach for term translation spotting. In Pro-
ceedings of the 12th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing’11), pages 29–40, Tokyo, Japan.
Kumiko Tanaka and Hideya Iwasaki. 1996. Extraction
of Lexical Translations from Non-Aligned Corpora.
In Proceedings of the 16th International Conference
on Computational Linguistics (COLING’96), pages
580–585, Copenhagen, Denmark.
Kun Yu and Junichi Tsujii. 2009. Extracting bilin-
gual dictionary from comparable corpora with de-
pendency heterogeneity. In Proceedings of the
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT’09),
pages 121–124, Boulder, CO, USA.
</reference>
<page confidence="0.972832">
1293
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.347021">
<title confidence="0.9967365">Looking at Unbalanced Specialized Comparable for Bilingual Lexicon Extraction</title>
<author confidence="0.93188">Morin</author>
<affiliation confidence="0.672204">Universit´e de Nantes, LINA UMR CNRS 2 rue de la houssini`ere, BP 92208, 44322 Nantes Cedex 03,</affiliation>
<abstract confidence="0.99981045">The main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced. However, the historical contextbased projection method dedicated to this task is relatively insensitive to the sizes of each part of the comparable corpus. Within this context, we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments. Moreover, we have introduced a regression model that boosts the observations of word cooccurrences used in the context-based projection method. Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan Agresti</author>
</authors>
<title>An Introduction to Categorical Data Analysis</title>
<date>2007</date>
<editor>(2nd ed.).</editor>
<publisher>Wiley &amp; Sons, Inc.,</publisher>
<location>Hoboken, New Jersey.</location>
<contexts>
<context position="11679" citStr="Agresti, 2007" startWordPosition="1864" endWordPosition="1865">experienced the linear regression model, we propose to contrast different regression models. Second, we apply regression to unbalanced comparable corpora and study the impact of prediction when applied to the source texts, the target texts and both source and target texts of the used comparable corpora. We use regression analysis to describe the relationship between word co-occurrence counts in a large corpus (the response variable) and word cooccurrence counts in a small corpus (the predictor variable). As most regression models have already been described in great detail (Christensen, 1997; Agresti, 2007), the derivation of most models is only briefly introduced in this work. As we can not claim that the prediction of word co-occurrence counts is a linear problem, we consider in addition to the simple linear regression 1286 model (Lin), a generalized linear model which is the logistic regression model (Logit) and non linear regression models such as polynomial regression model (Polyn) of order n. Given an input vector x E Rm, where x1,...,xm represent features, we find a prediction y� E Rm for the cooccurrence count of a couple of words y E R using one of the regression models presented below:</context>
</contexts>
<marker>Agresti, 2007</marker>
<rawString>Alan Agresti. 2007. An Introduction to Categorical Data Analysis (2nd ed.). Wiley &amp; Sons, Inc., Hoboken, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dhouha Bouamor</author>
<author>Nasredine Semmar</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Context vector disambiguation for bilingual lexicon extraction from comparable corpora.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13),</booktitle>
<pages>759--764</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="8342" citStr="Bouamor et al. (2013)" startWordPosition="1323" endWordPosition="1326"> Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 words Otero (2007) European Parliament SP/EN 14/17 million words Ismail and Manandhar (2010) European Parliament EN/SP 500,000/500,000 sentences Bouamor et al. (2013) Financial FR/EN 402,486/756,840 words - Medical FR/EN 396,524/524,805 words Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction Bouamor et al., 2013, among others) with the implicit hypothesis that comparable corpora are balanced. As McEnery and Xiao (2007, p. 21) observe, a specialized comparable corpus is built as balanced by analogy with a parallel corpus: “Therefore, in relation to parallel corpora, it is more likely for comparable corpora to be designed as general balanced corpora.”. For instance, Table 2 describes the comparable corpora used in the m</context>
<context position="14140" citStr="Bouamor et al. (2013)" startWordPosition="2283" endWordPosition="2286">to improve the representation of the lexical context. To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D´ejean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. As regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using approaches based on CCA (Canonical Correlation Analysis) and multilingual PLSA (Probabilistic Latent Semantic Analysis). The rank of candidate translations can be improved by integrating different heuristics. For instance, Chiao and Zweigenbaum (2002) introduce a heuristic based on word distribution symmetry. From the ranked list of candidate translations, the standard approach is applied in the reverse direction to find th</context>
<context position="20846" citStr="Bouamor et al. (2013)" startWordPosition="3364" endWordPosition="3367">Balanced 26.1 26.2 21.0 27.0 22.8 27.1 26.3 25.8 29.2 23.3 21.7 29.6 29.1 26.1 Unbalanced 26.1 31.9 34.7 36.0 37.7 36.4 36.6 37.2 39.8 40.5 40.6 42.3 40.9 41.6 Diabetes corpus 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Balanced 13.6 13.5 11.9 14.6 14.6 11.0 16.5 10.5 12.9 13.3 15.2 11.8 13.0 14.3 Unbalanced 13.6 17.5 18.9 21.2 23.4 23.8 24.8 24.7 24.7 24.4 24.8 25.2 26.0 24.9 Table 4: Results (MAP %) of the standard approach using the balanced and unbalanced comparable corpora ten relies on lists of a small size: 95 single words in Chiao and Zweigenbaum (2002), 100 in Morin et al. (2007), 125 and 79 in Bouamor et al. (2013). 4 Experiments and Results In this section, we present experiments to evaluate the influence of comparable corpus size and prediction models on the quality of bilingual terminology extraction. We present the results obtained for the terms belonging to the reference list for English to French direction measured in terms of the Mean Average Precision (MAP) (Manning et al., 2008) as follows: |Ref |1 (7) Ref |� rz where |Ref |is the number of terms of the reference list and rz the rank of the correct candidate translation i. 4.1 Standard Approach Evaluation In order to evaluate the influence of c</context>
</contexts>
<marker>Bouamor, Semmar, Zweigenbaum, 2013</marker>
<rawString>Dhouha Bouamor, Nasredine Semmar, and Pierre Zweigenbaum. 2013. Context vector disambiguation for bilingual lexicon extraction from comparable corpora. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13), pages 759–764, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Looking for candidate translational equivalents in specialized, comparable corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING’02),</booktitle>
<pages>1208--1212</pages>
<location>Tapei, Taiwan.</location>
<contexts>
<context position="5435" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="823" endWordPosition="826"> occurs within the window of the word to be translated (e.g. a seven-word window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary. The implementation of the standard approach can be carried out by applying the following three steps (Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others): Computing context vectors We collect all the words in the context of each word i and count their occurrence frequency in a window of n words around i. For each word i of the source and the target languages, we obtain a context vector vi which gathers the set of co-occurrence words j associated with the number of times that j and i occur together cooc(i, j). In order to identify specific words in the lexical context and to reduce wordfrequency effects, we normalize context vectors using an association score s</context>
<context position="7643" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="1223" endWordPosition="1226">c(i, ¬j) c = cooc(¬i, j) d = cooc(¬i, ¬j) Table 1: Contingency table (a+ 12) × (d+ 1 2) LO(i, j) =log (b + 1 2) × (c+ 1 2) (1) � associ assock Cosinev� = t t t (2) �&amp; assoclt2 �&amp; assoc2 t This approach is sensitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 words Otero (2007) European Parliament SP/EN 14/17 million words Is</context>
<context position="14564" citStr="Chiao and Zweigenbaum (2002)" startWordPosition="2344" endWordPosition="2347">2) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. As regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using approaches based on CCA (Canonical Correlation Analysis) and multilingual PLSA (Probabilistic Latent Semantic Analysis). The rank of candidate translations can be improved by integrating different heuristics. For instance, Chiao and Zweigenbaum (2002) introduce a heuristic based on word distribution symmetry. From the ranked list of candidate translations, the standard approach is applied in the reverse direction to find the source counterparts of the first target candidate translations. And then only the target candidate translations that had the initial source word among the first reverse candidate translations are kept. Laroche and Langlais (2010) suggest a heuristic based on the graphic similarity between source and target terms. Here, candidate translations which are cognates of the word to be translated are ranked first among the lis</context>
<context position="20781" citStr="Chiao and Zweigenbaum (2002)" startWordPosition="3350" endWordPosition="3353">nformation). 1288 Breast cancer corpus 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Balanced 26.1 26.2 21.0 27.0 22.8 27.1 26.3 25.8 29.2 23.3 21.7 29.6 29.1 26.1 Unbalanced 26.1 31.9 34.7 36.0 37.7 36.4 36.6 37.2 39.8 40.5 40.6 42.3 40.9 41.6 Diabetes corpus 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Balanced 13.6 13.5 11.9 14.6 14.6 11.0 16.5 10.5 12.9 13.3 15.2 11.8 13.0 14.3 Unbalanced 13.6 17.5 18.9 21.2 23.4 23.8 24.8 24.7 24.7 24.4 24.8 25.2 26.0 24.9 Table 4: Results (MAP %) of the standard approach using the balanced and unbalanced comparable corpora ten relies on lists of a small size: 95 single words in Chiao and Zweigenbaum (2002), 100 in Morin et al. (2007), 125 and 79 in Bouamor et al. (2013). 4 Experiments and Results In this section, we present experiments to evaluate the influence of comparable corpus size and prediction models on the quality of bilingual terminology extraction. We present the results obtained for the terms belonging to the reference list for English to French direction measured in terms of the Mean Average Precision (MAP) (Manning et al., 2008) as follows: |Ref |1 (7) Ref |� rz where |Ref |is the number of terms of the reference list and rz the rank of the correct candidate translation i. 4.1 Sta</context>
</contexts>
<marker>Chiao, Zweigenbaum, 2002</marker>
<rawString>Yun-Chuang Chiao and Pierre Zweigenbaum. 2002. Looking for candidate translational equivalents in specialized, comparable corpora. In Proceedings of the 19th International Conference on Computational Linguistics (COLING’02), pages 1208–1212, Tapei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>The Effect of a General Lexicon in Corpus-Based Identification of French-English Medical Word Translations.</title>
<date>2003</date>
<booktitle>In The New Navigators: from Professionals to Patients, Actes Medical Informatics Europe,</booktitle>
<pages>397--402</pages>
<contexts>
<context position="13714" citStr="Chiao and Zweigenbaum (2003)" startWordPosition="2218" endWordPosition="2221"> most important contextually relevant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar`es (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context. To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D´ejean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. As regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using a</context>
</contexts>
<marker>Chiao, Zweigenbaum, 2003</marker>
<rawString>Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The Effect of a General Lexicon in Corpus-Based Identification of French-English Medical Word Translations. In The New Navigators: from Professionals to Patients, Actes Medical Informatics Europe, pages 397–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Christensen</author>
</authors>
<title>Log-Linear Models and Logistic Regression.</title>
<date>1997</date>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="11663" citStr="Christensen, 1997" startWordPosition="1862" endWordPosition="1863"> First, while they experienced the linear regression model, we propose to contrast different regression models. Second, we apply regression to unbalanced comparable corpora and study the impact of prediction when applied to the source texts, the target texts and both source and target texts of the used comparable corpora. We use regression analysis to describe the relationship between word co-occurrence counts in a large corpus (the response variable) and word cooccurrence counts in a small corpus (the predictor variable). As most regression models have already been described in great detail (Christensen, 1997; Agresti, 2007), the derivation of most models is only briefly introduced in this work. As we can not claim that the prediction of word co-occurrence counts is a linear problem, we consider in addition to the simple linear regression 1286 model (Lin), a generalized linear model which is the logistic regression model (Logit) and non linear regression models such as polynomial regression model (Polyn) of order n. Given an input vector x E Rm, where x1,...,xm represent features, we find a prediction y� E Rm for the cooccurrence count of a couple of words y E R using one of the regression models </context>
</contexts>
<marker>Christensen, 1997</marker>
<rawString>Ronald Christensen. 1997. Log-Linear Models and Logistic Regression. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herv´e D´ejean</author>
<author>Fatia Sadat</author>
<author>´Eric Gaussier</author>
</authors>
<title>An approach based on multilingual thesauri and model combination for bilingual lexicon extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING’02),</booktitle>
<pages>218--224</pages>
<location>Tapei, Taiwan.</location>
<marker>D´ejean, Sadat, Gaussier, 2002</marker>
<rawString>Herv´e D´ejean, Fatia Sadat, and ´Eric Gaussier. 2002. An approach based on multilingual thesauri and model combination for bilingual lexicon extraction. In Proceedings of the 19th International Conference on Computational Linguistics (COLING’02), pages 218–224, Tapei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona T Diab</author>
<author>Steve Finch</author>
</authors>
<title>A Statistical Word-Level Translation Model for Comparable Corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th International Conference on Computer-Assisted Information Retrieval (RIAO’00),</booktitle>
<pages>1500--1501</pages>
<location>Paris, France.</location>
<contexts>
<context position="10056" citStr="Diab and Finch (2000" startWordPosition="1599" endWordPosition="1602"> context vectors are computed from each part of the comparable corpus rather than through the parts of the comparable corpora, the standard approach is relatively insensitive to differences in corpus sizes. The only precaution for using the standard approach with unbalanced corpora is to normalize the association measure (for instance, this can be done by dividing each entry of a given context vector by the sum of its association scores). 2.2 Prediction Model Since comparable corpora are usually small in specialized domains (see Table 2), the discrimina1We only found mention of this aspect in Diab and Finch (2000, p. 1501) “In principle, we do not have to have the same size corpora in order for the approach to work”. tive power of context vectors (i.e. the observations of word co-occurrences) is reduced. One way to deal with this problem is to re-estimate co-occurrence counts by a prediction function (Hazem and Morin, 2013). This consists in assigning to each observed co-occurrence count of a small comparable corpora, a new value learned beforehand from a large training corpus. In order to make co-occurrence counts more discriminant and in the same way as Hazem and Morin (2013), one strategy consists </context>
</contexts>
<marker>Diab, Finch, 2000</marker>
<rawString>Mona T. Diab and Steve Finch. 2000. A Statistical Word-Level Translation Model for Comparable Corpora. In Proceedings of the 6th International Conference on Computer-Assisted Information Retrieval (RIAO’00), pages 1500–1501, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The Statistics of Word Cooccurrences: Word Pairs and Collocations.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at Stuttgart,</institution>
<contexts>
<context position="6123" citStr="Evert, 2005" startWordPosition="945" endWordPosition="946">g others): Computing context vectors We collect all the words in the context of each word i and count their occurrence frequency in a window of n words around i. For each word i of the source and the target languages, we obtain a context vector vi which gathers the set of co-occurrence words j associated with the number of times that j and i occur together cooc(i, j). In order to identify specific words in the lexical context and to reduce wordfrequency effects, we normalize context vectors using an association score such as Mutual Information, Log-likelihood, or the discounted log-odds (LO) (Evert, 2005) (see equation 1 and Table 1 where N = a + b + c + d). Transferring context vectors Using a bilingual dictionary, we translate the elements of the source context vector. If the bilingual dictionary provides several translations for an element, we consider all of them but weight the different translations according to their frequency in the target language. Finding candidate translations For a word to be translated, we compute the similarity between the translated context vector and all target vectors through vector distance measures such as Jaccard or Cosine (see equation 2 where associj stand</context>
</contexts>
<marker>Evert, 2005</marker>
<rawString>Stefan Evert. 2005. The Statistics of Word Cooccurrences: Word Pairs and Collocations. Ph.D. thesis, Universit¨at Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Percy Cheung</author>
</authors>
<title>Multilevel bootstrapping for extracting parallel sentences from a quasi-comparable corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING’04),</booktitle>
<pages>1051--1057</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1927" citStr="Fung and Cheung (2004)" startWordPosition="280" endWordPosition="284"> parallel corpora are scarce resources, especially for technical domains and for language pairs not involving English. For these reasons, research in bilingual lexicon extraction has focused on another kind of bilingual corpora comprised of texts sharing common features such as domain, genre, sampling period, etc. without having a source text/target text relationship (McEnery and Xiao, 2007). These corpora, well known now as comparable corpora, have also initially been introduced as non-parallel corpora (Fung, 1995; Rapp, 1995), and non-aligned corpora (Tanaka and Iwasaki, 1996). According to Fung and Cheung (2004), who range bilingual corpora from parallel corpora to quasi-comparable corpora going through comparable corpora, there is a continuum from parallel to comparable corpora (i.e. a kind of filiation). The bilingual lexicon extraction task from comparable corpora inherits this filiation. For instance, the historical context-based projection method (Fung, 1995; Rapp, 1995), known as the standard approach, dedicated to this task seems implicitly to lead to work with balanced comparable corpora in the same way as for parallel corpora (i.e. each part of the corpus is composed of the same amount of da</context>
</contexts>
<marker>Fung, Cheung, 2004</marker>
<rawString>Pascale Fung and Percy Cheung. 2004. Multilevel bootstrapping for extracting parallel sentences from a quasi-comparable corpus. In Proceedings of the 20th International Conference on Computational Linguistics (COLING’04), pages 1051–1057, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Kathleen McKeown</author>
</authors>
<title>Finding Terminology Translations from Non-parallel Corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Annual Workshop on Very Large Corpora (VLC’97),</booktitle>
<pages>192--202</pages>
<location>Hong Kong.</location>
<contexts>
<context position="7908" citStr="Fung and McKeown (1997)" startWordPosition="1264" endWordPosition="1267">ntext, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 words Otero (2007) European Parliament SP/EN 14/17 million words Ismail and Manandhar (2010) European Parliament EN/SP 500,000/500,000 sentences Bouamor et al. (2013) Financial FR/EN 402,486/756,840 words - Medical FR/EN 396,524/524,805 words Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction </context>
</contexts>
<marker>Fung, McKeown, 1997</marker>
<rawString>Pascale Fung and Kathleen McKeown. 1997. Finding Terminology Translations from Non-parallel Corpora. In Proceedings of the 5th Annual Workshop on Very Large Corpora (VLC’97), pages 192–202, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>Compiling Bilingual Lexicon Entries from a non-Parallel English-Chinese Corpus.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd Annual Workshop on Very Large Corpora (VLC’95),</booktitle>
<pages>173--183</pages>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1825" citStr="Fung, 1995" startWordPosition="267" endWordPosition="268">heir translation). However, despite good results in the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pairs not involving English. For these reasons, research in bilingual lexicon extraction has focused on another kind of bilingual corpora comprised of texts sharing common features such as domain, genre, sampling period, etc. without having a source text/target text relationship (McEnery and Xiao, 2007). These corpora, well known now as comparable corpora, have also initially been introduced as non-parallel corpora (Fung, 1995; Rapp, 1995), and non-aligned corpora (Tanaka and Iwasaki, 1996). According to Fung and Cheung (2004), who range bilingual corpora from parallel corpora to quasi-comparable corpora going through comparable corpora, there is a continuum from parallel to comparable corpora (i.e. a kind of filiation). The bilingual lexicon extraction task from comparable corpora inherits this filiation. For instance, the historical context-based projection method (Fung, 1995; Rapp, 1995), known as the standard approach, dedicated to this task seems implicitly to lead to work with balanced comparable corpora in t</context>
</contexts>
<marker>Fung, 1995</marker>
<rawString>Pascale Fung. 1995. Compiling Bilingual Lexicon Entries from a non-Parallel English-Chinese Corpus. In Proceedings of the 3rd Annual Workshop on Very Large Corpora (VLC’95), pages 173–183, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>A Statistical View on Bilingual Lexicon Extraction: From Parallel Corpora to Non-parallel Corpora. In</title>
<date>1998</date>
<booktitle>Proceedings of the 3rd Conference of the Association for Machine Translation in the Americas (AMTA’98),</booktitle>
<pages>1--16</pages>
<editor>David Farwell, Laurie Gerber, and Eduard Hovy, editors,</editor>
<location>Langhorne, PA, USA.</location>
<contexts>
<context position="7577" citStr="Fung, 1998" startWordPosition="1214" endWordPosition="1215">the similarity score. j ¬j a = cooc(i, j) b = cooc(i, ¬j) c = cooc(¬i, j) d = cooc(¬i, ¬j) Table 1: Contingency table (a+ 12) × (d+ 1 2) LO(i, j) =log (b + 1 2) × (c+ 1 2) (1) � associ assock Cosinev� = t t t (2) �&amp; assoclt2 �&amp; assoc2 t This approach is sensitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 w</context>
</contexts>
<marker>Fung, 1998</marker>
<rawString>Pascale Fung. 1998. A Statistical View on Bilingual Lexicon Extraction: From Parallel Corpora to Non-parallel Corpora. In David Farwell, Laurie Gerber, and Eduard Hovy, editors, Proceedings of the 3rd Conference of the Association for Machine Translation in the Americas (AMTA’98), pages 1– 16, Langhorne, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>´Eric Gaussier</author>
<author>Jean-Michel Renders</author>
<author>Irena Matveeva</author>
<author>Cyril Goutte</author>
<author>Herv´e D´ejean</author>
</authors>
<title>A Geometric View on Bilingual Lexicon Extraction from Comparable Corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL’04),</booktitle>
<pages>526--533</pages>
<location>Barcelona,</location>
<marker>Gaussier, Renders, Matveeva, Goutte, D´ejean, 2004</marker>
<rawString>´Eric Gaussier, Jean-Michel Renders, Irena Matveeva, Cyril Goutte, and Herv´e D´ejean. 2004. A Geometric View on Bilingual Lexicon Extraction from Comparable Corpora. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL’04), pages 526–533, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Corpus-Derived First, Second and Third-Order Word Affinities.</title>
<date>1994</date>
<booktitle>In Proceedings of the 6th Congress of the European Association for Lexicography (EURALEX’94),</booktitle>
<pages>279--290</pages>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="4694" citStr="Grefenstette, 1994" startWordPosition="707" endWordPosition="708">resent an extension of this approach based on regression models. Finally, we discuss works related to this study. 2.1 Standard Approach The main work in bilingual lexicon extraction from comparable corpora is based on lexical context analysis and relies on the simple observation that a word and its translation tend to appear in the same lexical contexts. The basis of this observation consists in the identification of “first-order affinities” for each source and target language: “First-order affinities describe what other words are likely to be found in the immediate vicinity of a given word” (Grefenstette, 1994, p. 279). These affinities can be represented by context vectors, and each vector element represents a word which occurs within the window of the word to be translated (e.g. a seven-word window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary. </context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Corpus-Derived First, Second and Third-Order Word Affinities. In Proceedings of the 6th Congress of the European Association for Lexicography (EURALEX’94), pages 279–290, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Hazem</author>
<author>Emmanuel Morin</author>
</authors>
<title>Word co-occurrence counts prediction for bilingual terminology extraction from comparable corpora.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing (IJCNLP’13),</booktitle>
<pages>1392--1400</pages>
<location>Nagoya, Japan.</location>
<contexts>
<context position="10373" citStr="Hazem and Morin, 2013" startWordPosition="1654" endWordPosition="1657">measure (for instance, this can be done by dividing each entry of a given context vector by the sum of its association scores). 2.2 Prediction Model Since comparable corpora are usually small in specialized domains (see Table 2), the discrimina1We only found mention of this aspect in Diab and Finch (2000, p. 1501) “In principle, we do not have to have the same size corpora in order for the approach to work”. tive power of context vectors (i.e. the observations of word co-occurrences) is reduced. One way to deal with this problem is to re-estimate co-occurrence counts by a prediction function (Hazem and Morin, 2013). This consists in assigning to each observed co-occurrence count of a small comparable corpora, a new value learned beforehand from a large training corpus. In order to make co-occurrence counts more discriminant and in the same way as Hazem and Morin (2013), one strategy consists in addressing this problem through regression: given training corpora of small and large size (abundant in the general domain), we predict word cooccurrence counts in order to make them more reliable. We then apply the resulting regression function to each word co-occurrence count as a pre-processing step of the sta</context>
</contexts>
<marker>Hazem, Morin, 2013</marker>
<rawString>Amir Hazem and Emmanuel Morin. 2013. Word co-occurrence counts prediction for bilingual terminology extraction from comparable corpora. In Proceedings of the Sixth International Joint Conference on Natural Language Processing (IJCNLP’13), pages 1392–1400, Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Azniah Ismail</author>
<author>Suresh Manandhar</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora using in-domain terms.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10),</booktitle>
<pages>481--489</pages>
<location>Beijing, China.</location>
<contexts>
<context position="8268" citStr="Ismail and Manandhar (2010)" startWordPosition="1314" endWordPosition="1317">02; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 words Otero (2007) European Parliament SP/EN 14/17 million words Ismail and Manandhar (2010) European Parliament EN/SP 500,000/500,000 sentences Bouamor et al. (2013) Financial FR/EN 402,486/756,840 words - Medical FR/EN 396,524/524,805 words Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction Bouamor et al., 2013, among others) with the implicit hypothesis that comparable corpora are balanced. As McEnery and Xiao (2007, p. 21) observe, a specialized comparable corpus is built as balanced by analogy with a parallel corpus: “Therefore, in relation to parallel corpora, it is more likely for comparable corpora to be designed as general balanced corpo</context>
<context position="13029" citStr="Ismail and Manandhar (2010)" startWordPosition="2105" endWordPosition="2108">ters to estimate. Let us denote by f the regression function and by cooc(wi, wj) the co-occurrence count of the words wi and wj. The resulting predicted value of cooc(wi, wj), noted cooc(wi, wj) is given by the following equation: cooc(wi, wj) = f(cooc(wi, wj)) (6) 2.3 Related Work In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vector by strengthening the context words that happen to be transliterated words and scientific compound words in the target language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar`es (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context. To improve the transfer context vectors step, and increa</context>
</contexts>
<marker>Ismail, Manandhar, 2010</marker>
<rawString>Azniah Ismail and Suresh Manandhar. 2010. Bilingual lexicon extraction from comparable corpora using in-domain terms. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10), pages 481–489, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition (ULA’02),</booktitle>
<pages>9--16</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="13938" citStr="Koehn and Knight (2002)" startWordPosition="2253" endWordPosition="2256">that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context. To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D´ejean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. As regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using approaches based on CCA (Canonical Correlation Analysis) and multilingual PLSA (Probabilistic Latent Semantic Analysis). The rank of candidate translations can be improved by integrating different heuristics. For instance, Ch</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition (ULA’02), pages 9–16, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Laroche</author>
<author>Philippe Langlais</author>
</authors>
<title>Revisiting Context-based Projection Methods for TermTranslation Spotting in Comparable Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10),</booktitle>
<pages>617--625</pages>
<location>Beijing, China.</location>
<contexts>
<context position="5505" citStr="Laroche and Langlais, 2010" startWordPosition="835" endWordPosition="838">rd window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary. The implementation of the standard approach can be carried out by applying the following three steps (Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others): Computing context vectors We collect all the words in the context of each word i and count their occurrence frequency in a window of n words around i. For each word i of the source and the target languages, we obtain a context vector vi which gathers the set of co-occurrence words j associated with the number of times that j and i occur together cooc(i, j). In order to identify specific words in the lexical context and to reduce wordfrequency effects, we normalize context vectors using an association score such as Mutual Information, Log-likelihood, or the discounted log-odds </context>
<context position="7495" citStr="Laroche and Langlais (2010)" startWordPosition="1197" endWordPosition="1200">get language). Finally, the candidate translations of a word are the target words ranked following the similarity score. j ¬j a = cooc(i, j) b = cooc(i, ¬j) c = cooc(¬i, j) d = cooc(¬i, ¬j) Table 1: Contingency table (a+ 12) × (d+ 1 2) LO(i, j) =log (b + 1 2) × (c+ 1 2) (1) � associ assock Cosinev� = t t t (2) �&amp; assoclt2 �&amp; assoc2 t This approach is sensitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medic</context>
<context position="14971" citStr="Laroche and Langlais (2010)" startWordPosition="2406" endWordPosition="2409">cal Correlation Analysis) and multilingual PLSA (Probabilistic Latent Semantic Analysis). The rank of candidate translations can be improved by integrating different heuristics. For instance, Chiao and Zweigenbaum (2002) introduce a heuristic based on word distribution symmetry. From the ranked list of candidate translations, the standard approach is applied in the reverse direction to find the source counterparts of the first target candidate translations. And then only the target candidate translations that had the initial source word among the first reverse candidate translations are kept. Laroche and Langlais (2010) suggest a heuristic based on the graphic similarity between source and target terms. Here, candidate translations which are cognates of the word to be translated are ranked first among the list of translation candidates. 3 Linguistic Resources In this section, we outline the different textual resources used for our experiments: the comparable corpora, the bilingual dictionary and the terminology reference lists. 3.1 Specialized Comparable Corpora For our experiments, we used two specialized French/English comparable corpora: Breast cancer corpus This comparable corpus is composed of documents</context>
</contexts>
<marker>Laroche, Langlais, 2010</marker>
<rawString>Audrey Laroche and Philippe Langlais. 2010. Revisiting Context-based Projection Methods for TermTranslation Spotting in Comparable Corpora. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10), pages 617–625, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Li</author>
<author>´Eric Gaussier</author>
</authors>
<title>Improving corpus comparability for bilingual lexicon extraction from comparable corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10),</booktitle>
<pages>644--652</pages>
<location>Beijing, China.</location>
<contexts>
<context position="17512" citStr="Li and Gaussier, 2010" startWordPosition="2801" endWordPosition="2804">inguistic preprocessing steps: tokenisation, part-of-speech tagging, and lemmatisation. These steps were carried out using the TTC TermSuite4 that applies the same method to several languages including French and English. Finally, the function words were removed and the words occurring less than twice in the French part and in each English part were discarded. Table 3 shows the number of distinct words (# words) after these steps. It also indicates the comparability degree in percentage (comp.) between the French part and each English part of each comparable corpus. The comparability measure (Li and Gaussier, 2010) is based on the expectation of finding the translation for each word in the corpus and gives a good idea about how two corpora are comparable. We can notice that all the comparable corpora have a high degree of comparability with a better comparability of the breast cancer corpora as opposed to the diabetes corpora. In the remainder of this article, [breast cancer corpus i] for instance stands for the breast cancer comparable corpus composed of the unique French part and the English part i (i E [1, 14]). 3.2 Bilingual Dictionary The bilingual dictionary used in our experiments is the French/E</context>
</contexts>
<marker>Li, Gaussier, 2010</marker>
<rawString>Bo Li and ´Eric Gaussier. 2010. Improving corpus comparability for bilingual lexicon extraction from comparable corpora. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10), pages 644–652, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schtze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="21226" citStr="Manning et al., 2008" startWordPosition="3426" endWordPosition="3429">sults (MAP %) of the standard approach using the balanced and unbalanced comparable corpora ten relies on lists of a small size: 95 single words in Chiao and Zweigenbaum (2002), 100 in Morin et al. (2007), 125 and 79 in Bouamor et al. (2013). 4 Experiments and Results In this section, we present experiments to evaluate the influence of comparable corpus size and prediction models on the quality of bilingual terminology extraction. We present the results obtained for the terms belonging to the reference list for English to French direction measured in terms of the Mean Average Precision (MAP) (Manning et al., 2008) as follows: |Ref |1 (7) Ref |� rz where |Ref |is the number of terms of the reference list and rz the rank of the correct candidate translation i. 4.1 Standard Approach Evaluation In order to evaluate the influence of corpus size on the bilingual terminology extraction task, two experiments have been carried out using the standard approach. We first performed an experiment using each comparable corpus independently of the others (we refer to these corpora as balanced corpora). We then conducted a second experiment where we varied the size of the English part of the comparable corpus, from 530</context>
</contexts>
<marker>Manning, Raghavan, Schtze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony McEnery</author>
<author>Zhonghua Xiao</author>
</authors>
<title>Parallel and comparable corpora: What are they up to?</title>
<date>2007</date>
<booktitle>In Gunilla Anderman</booktitle>
<pages>18--31</pages>
<editor>and Margaret Rogers, editors,</editor>
<location>Clevedon, UK.</location>
<contexts>
<context position="1699" citStr="McEnery and Xiao, 2007" startWordPosition="247" endWordPosition="250">on extraction task from bilingual corpora was initially addressed by using parallel corpora (i.e. a corpus that contains source texts and their translation). However, despite good results in the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pairs not involving English. For these reasons, research in bilingual lexicon extraction has focused on another kind of bilingual corpora comprised of texts sharing common features such as domain, genre, sampling period, etc. without having a source text/target text relationship (McEnery and Xiao, 2007). These corpora, well known now as comparable corpora, have also initially been introduced as non-parallel corpora (Fung, 1995; Rapp, 1995), and non-aligned corpora (Tanaka and Iwasaki, 1996). According to Fung and Cheung (2004), who range bilingual corpora from parallel corpora to quasi-comparable corpora going through comparable corpora, there is a continuum from parallel to comparable corpora (i.e. a kind of filiation). The bilingual lexicon extraction task from comparable corpora inherits this filiation. For instance, the historical context-based projection method (Fung, 1995; Rapp, 1995),</context>
<context position="8636" citStr="McEnery and Xiao (2007" startWordPosition="1366" endWordPosition="1369">weigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 words Otero (2007) European Parliament SP/EN 14/17 million words Ismail and Manandhar (2010) European Parliament EN/SP 500,000/500,000 sentences Bouamor et al. (2013) Financial FR/EN 402,486/756,840 words - Medical FR/EN 396,524/524,805 words Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction Bouamor et al., 2013, among others) with the implicit hypothesis that comparable corpora are balanced. As McEnery and Xiao (2007, p. 21) observe, a specialized comparable corpus is built as balanced by analogy with a parallel corpus: “Therefore, in relation to parallel corpora, it is more likely for comparable corpora to be designed as general balanced corpora.”. For instance, Table 2 describes the comparable corpora used in the main work dedicated to bilingual lexicon extraction for which the ratio between the size of the source and the target texts is comprised between 1 and 1.8. In fact, the assumption that words which have the same meaning in different languages should have the same lexical context distributions do</context>
</contexts>
<marker>McEnery, Xiao, 2007</marker>
<rawString>Anthony McEnery and Zhonghua Xiao. 2007. Parallel and comparable corpora: What are they up to? In Gunilla Anderman and Margaret Rogers, editors, Incorporating Corpora: Translation and the Linguist, Multilingual Matters, chapter 2, pages 18–31. Clevedon, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>Emmanuel Prochasson</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora enhanced with parallel corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th Workshop on Building and Using Comparable Corpora (BUCC’11),</booktitle>
<pages>27--34</pages>
<location>Portland, OR, USA.</location>
<contexts>
<context position="13746" citStr="Morin and Prochasson (2011)" startWordPosition="2223" endWordPosition="2226">vant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar`es (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context. To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D´ejean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. As regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using approaches based on CCA (Canonica</context>
</contexts>
<marker>Morin, Prochasson, 2011</marker>
<rawString>Emmanuel Morin and Emmanuel Prochasson. 2011. Bilingual lexicon extraction from comparable corpora enhanced with parallel corpora. In Proceedings of the 4th Workshop on Building and Using Comparable Corpora (BUCC’11), pages 27–34, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>B´eatrice Daille</author>
<author>Koichi Takeuchi</author>
<author>Kyo Kageura</author>
</authors>
<title>Bilingual Terminology Mining – Using Brain, not brawn comparable corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL’07),</booktitle>
<pages>664--671</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5477" citStr="Morin et al., 2007" startWordPosition="831" endWordPosition="834">ted (e.g. a seven-word window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary. The implementation of the standard approach can be carried out by applying the following three steps (Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others): Computing context vectors We collect all the words in the context of each word i and count their occurrence frequency in a window of n words around i. For each word i of the source and the target languages, we obtain a context vector vi which gathers the set of co-occurrence words j associated with the number of times that j and i occur together cooc(i, j). In order to identify specific words in the lexical context and to reduce wordfrequency effects, we normalize context vectors using an association score such as Mutual Information, Log-likelihood,</context>
<context position="7708" citStr="Morin et al., 2007" startWordPosition="1235" endWordPosition="1238">2) × (d+ 1 2) LO(i, j) =log (b + 1 2) × (c+ 1 2) (1) � associ assock Cosinev� = t t t (2) �&amp; assoclt2 �&amp; assoc2 t This approach is sensitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 words Otero (2007) European Parliament SP/EN 14/17 million words Ismail and Manandhar (2010) European Parliament EN/SP 500,000/500,0</context>
<context position="20809" citStr="Morin et al. (2007)" startWordPosition="3356" endWordPosition="3359">s 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Balanced 26.1 26.2 21.0 27.0 22.8 27.1 26.3 25.8 29.2 23.3 21.7 29.6 29.1 26.1 Unbalanced 26.1 31.9 34.7 36.0 37.7 36.4 36.6 37.2 39.8 40.5 40.6 42.3 40.9 41.6 Diabetes corpus 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Balanced 13.6 13.5 11.9 14.6 14.6 11.0 16.5 10.5 12.9 13.3 15.2 11.8 13.0 14.3 Unbalanced 13.6 17.5 18.9 21.2 23.4 23.8 24.8 24.7 24.7 24.4 24.8 25.2 26.0 24.9 Table 4: Results (MAP %) of the standard approach using the balanced and unbalanced comparable corpora ten relies on lists of a small size: 95 single words in Chiao and Zweigenbaum (2002), 100 in Morin et al. (2007), 125 and 79 in Bouamor et al. (2013). 4 Experiments and Results In this section, we present experiments to evaluate the influence of comparable corpus size and prediction models on the quality of bilingual terminology extraction. We present the results obtained for the terms belonging to the reference list for English to French direction measured in terms of the Mean Average Precision (MAP) (Manning et al., 2008) as follows: |Ref |1 (7) Ref |� rz where |Ref |is the number of terms of the reference list and rz the rank of the correct candidate translation i. 4.1 Standard Approach Evaluation In</context>
</contexts>
<marker>Morin, Daille, Takeuchi, Kageura, 2007</marker>
<rawString>Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi, and Kyo Kageura. 2007. Bilingual Terminology Mining – Using Brain, not brawn comparable corpora. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL’07), pages 664–671, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Gamallo Otero</author>
</authors>
<title>Learning bilingual lexicons from comparable english and spanish corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th Conference on Machine Translation Summit (MT Summit XI),</booktitle>
<pages>191--198</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="8194" citStr="Otero (2007)" startWordPosition="1306" endWordPosition="1307">ers and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 words Otero (2007) European Parliament SP/EN 14/17 million words Ismail and Manandhar (2010) European Parliament EN/SP 500,000/500,000 sentences Bouamor et al. (2013) Financial FR/EN 402,486/756,840 words - Medical FR/EN 396,524/524,805 words Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction Bouamor et al., 2013, among others) with the implicit hypothesis that comparable corpora are balanced. As McEnery and Xiao (2007, p. 21) observe, a specialized comparable corpus is built as balanced by analogy with a parallel corpus: “Therefore, in relation to parallel corpora, it is m</context>
<context position="13424" citStr="Otero (2007)" startWordPosition="2174" endWordPosition="2175">. (2009) enhance the representativeness of the context vector by strengthening the context words that happen to be transliterated words and scientific compound words in the target language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar`es (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context. To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D´ejean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spellin</context>
</contexts>
<marker>Otero, 2007</marker>
<rawString>Pablo Gamallo Otero. 2007. Learning bilingual lexicons from comparable english and spanish corpora. In Proceedings of the 11th Conference on Machine Translation Summit (MT Summit XI), pages 191– 198, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Peters</author>
<author>Eugenio Picchi</author>
</authors>
<title>Crosslanguage information retrieval: A system for comparable corpus querying.</title>
<date>1998</date>
<booktitle>Cross-language information retrieval, chapter 7,</booktitle>
<pages>81--90</pages>
<editor>In Gregory Grefenstette, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="7602" citStr="Peters and Picchi, 1998" startWordPosition="1216" endWordPosition="1220">ty score. j ¬j a = cooc(i, j) b = cooc(i, ¬j) c = cooc(¬i, j) d = cooc(¬i, ¬j) Table 1: Contingency table (a+ 12) × (d+ 1 2) LO(i, j) =log (b + 1 2) × (c+ 1 2) (1) � associ assock Cosinev� = t t t (2) �&amp; assoclt2 �&amp; assoc2 t This approach is sensitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 words Otero (2007) Europea</context>
</contexts>
<marker>Peters, Picchi, 1998</marker>
<rawString>Carol Peters and Eugenio Picchi. 1998. Crosslanguage information retrieval: A system for comparable corpus querying. In Gregory Grefenstette, editor, Cross-language information retrieval, chapter 7, pages 81–90. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Prochasson</author>
<author>Pascale Fung</author>
</authors>
<title>Rare Word Translation Extraction from Aligned Comparable Documents.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL’11),</booktitle>
<pages>1327--1335</pages>
<location>Portland, OR, USA.</location>
<contexts>
<context position="7763" citStr="Prochasson and Fung, 2011" startWordPosition="1243" endWordPosition="1246"> (1) � associ assock Cosinev� = t t t (2) �&amp; assoclt2 �&amp; assoc2 t This approach is sensitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 words Otero (2007) European Parliament SP/EN 14/17 million words Ismail and Manandhar (2010) European Parliament EN/SP 500,000/500,000 sentences Bouamor et al. (2013) Financial FR/EN 402,</context>
<context position="20014" citStr="Prochasson and Fung (2011)" startWordPosition="3202" endWordPosition="3205">ch occur more than four times in each English part i7. As a result of filtering, 169 French/English single words were extracted for the breast cancer corpus and 244 French/English single words were extracted for the diabetes corpus. It should be noted that the evaluation of terminology extraction using specialized comparable corpora of5http://www.elra.info/ 6http://www.nlm.nih.gov/research/umls 7The threshold sets to four is required to build a bilingual terminology reference list composed of about a hundred words. This value is very low to obtain representative context vectors. For instance, Prochasson and Fung (2011) showed that the standard approach is not relevant for infrequent words (since the context vectors are very unrepresentative i.e. poor in information). 1288 Breast cancer corpus 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Balanced 26.1 26.2 21.0 27.0 22.8 27.1 26.3 25.8 29.2 23.3 21.7 29.6 29.1 26.1 Unbalanced 26.1 31.9 34.7 36.0 37.7 36.4 36.6 37.2 39.8 40.5 40.6 42.3 40.9 41.6 Diabetes corpus 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Balanced 13.6 13.5 11.9 14.6 14.6 11.0 16.5 10.5 12.9 13.3 15.2 11.8 13.0 14.3 Unbalanced 13.6 17.5 18.9 21.2 23.4 23.8 24.8 24.7 24.7 24.4 24.8 25.2 26.0 24.9 Table 4: Results (MA</context>
</contexts>
<marker>Prochasson, Fung, 2011</marker>
<rawString>Emmanuel Prochasson and Pascale Fung. 2011. Rare Word Translation Extraction from Aligned Comparable Documents. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL’11), pages 1327–1335, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Prochasson</author>
<author>Emmanuel Morin</author>
<author>Kyo Kageura</author>
</authors>
<title>Anchor points for bilingual lexicon extraction from small comparable corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference on Machine Translation Summit (MT Summit XII),</booktitle>
<pages>284--291</pages>
<location>Ottawa, Canada.</location>
<contexts>
<context position="12820" citStr="Prochasson et al. (2009)" startWordPosition="2074" endWordPosition="2077">ount of a couple of words y E R using one of the regression models presented below: yLin = β0 + β1x (3) 1 yLogit = 1 + exp(�(β0 + β1x)) (4) �yPolyn = β0 + β1x + β2x2 + ... + βnxn (5) where βi are the parameters to estimate. Let us denote by f the regression function and by cooc(wi, wj) the co-occurrence count of the words wi and wj. The resulting predicted value of cooc(wi, wj), noted cooc(wi, wj) is given by the following equation: cooc(wi, wj) = f(cooc(wi, wj)) (6) 2.3 Related Work In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vector by strengthening the context words that happen to be transliterated words and scientific compound words in the target language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar`es (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2</context>
</contexts>
<marker>Prochasson, Morin, Kageura, 2009</marker>
<rawString>Emmanuel Prochasson, Emmanuel Morin, and Kyo Kageura. 2009. Anchor points for bilingual lexicon extraction from small comparable corpora. In Proceedings of the 12th Conference on Machine Translation Summit (MT Summit XII), pages 284–291, Ottawa, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identify Word Translations in Non-Parallel Texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL’95),</booktitle>
<pages>320--322</pages>
<location>Boston, MA, USA.</location>
<contexts>
<context position="1838" citStr="Rapp, 1995" startWordPosition="269" endWordPosition="270">tion). However, despite good results in the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pairs not involving English. For these reasons, research in bilingual lexicon extraction has focused on another kind of bilingual corpora comprised of texts sharing common features such as domain, genre, sampling period, etc. without having a source text/target text relationship (McEnery and Xiao, 2007). These corpora, well known now as comparable corpora, have also initially been introduced as non-parallel corpora (Fung, 1995; Rapp, 1995), and non-aligned corpora (Tanaka and Iwasaki, 1996). According to Fung and Cheung (2004), who range bilingual corpora from parallel corpora to quasi-comparable corpora going through comparable corpora, there is a continuum from parallel to comparable corpora (i.e. a kind of filiation). The bilingual lexicon extraction task from comparable corpora inherits this filiation. For instance, the historical context-based projection method (Fung, 1995; Rapp, 1995), known as the standard approach, dedicated to this task seems implicitly to lead to work with balanced comparable corpora in the same way a</context>
<context position="7565" citStr="Rapp, 1995" startWordPosition="1212" endWordPosition="1213">d following the similarity score. j ¬j a = cooc(i, j) b = cooc(i, ¬j) c = cooc(¬i, j) d = cooc(¬i, ¬j) Table 1: Contingency table (a+ 12) × (d+ 1 2) LO(i, j) =log (b + 1 2) × (c+ 1 2) (1) � associ assock Cosinev� = t t t (2) �&amp; assoclt2 �&amp; assoc2 t This approach is sensitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,6</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identify Word Translations in Non-Parallel Texts. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL’95), pages 320–322, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic Identification of Word Translations from Unrelated English and German Corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99),</booktitle>
<pages>519--526</pages>
<location>College Park, MD, USA.</location>
<contexts>
<context position="5406" citStr="Rapp, 1999" startWordPosition="821" endWordPosition="822">a word which occurs within the window of the word to be translated (e.g. a seven-word window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary. The implementation of the standard approach can be carried out by applying the following three steps (Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others): Computing context vectors We collect all the words in the context of each word i and count their occurrence frequency in a window of n words around i. For each word i of the source and the target languages, we obtain a context vector vi which gathers the set of co-occurrence words j associated with the number of times that j and i occur together cooc(i, j). In order to identify specific words in the lexical context and to reduce wordfrequency effects, we normalize context vectors</context>
<context position="7614" citStr="Rapp, 1999" startWordPosition="1221" endWordPosition="1222">, j) b = cooc(i, ¬j) c = cooc(¬i, j) d = cooc(¬i, ¬j) Table 1: Contingency table (a+ 12) × (d+ 1 2) LO(i, j) =log (b + 1 2) × (c+ 1 2) (1) � associ assock Cosinev� = t t t (2) �&amp; assoclt2 �&amp; assoc2 t This approach is sensitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 words Otero (2007) European Parliament</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic Identification of Word Translations from Unrelated English and German Corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99), pages 519–526, College Park, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rapha¨el Rubino</author>
<author>Georges Linar`es</author>
</authors>
<title>A multiview approach for term translation spotting.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing’11),</booktitle>
<pages>29--40</pages>
<location>Tokyo, Japan.</location>
<marker>Rapha¨el Rubino, Linar`es, 2011</marker>
<rawString>Rapha¨el Rubino and Georges Linar`es. 2011. A multiview approach for term translation spotting. In Proceedings of the 12th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing’11), pages 29–40, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kumiko Tanaka</author>
<author>Hideya Iwasaki</author>
</authors>
<title>Extraction of Lexical Translations from Non-Aligned Corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING’96),</booktitle>
<pages>580--585</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1890" citStr="Tanaka and Iwasaki, 1996" startWordPosition="274" endWordPosition="277">n the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pairs not involving English. For these reasons, research in bilingual lexicon extraction has focused on another kind of bilingual corpora comprised of texts sharing common features such as domain, genre, sampling period, etc. without having a source text/target text relationship (McEnery and Xiao, 2007). These corpora, well known now as comparable corpora, have also initially been introduced as non-parallel corpora (Fung, 1995; Rapp, 1995), and non-aligned corpora (Tanaka and Iwasaki, 1996). According to Fung and Cheung (2004), who range bilingual corpora from parallel corpora to quasi-comparable corpora going through comparable corpora, there is a continuum from parallel to comparable corpora (i.e. a kind of filiation). The bilingual lexicon extraction task from comparable corpora inherits this filiation. For instance, the historical context-based projection method (Fung, 1995; Rapp, 1995), known as the standard approach, dedicated to this task seems implicitly to lead to work with balanced comparable corpora in the same way as for parallel corpora (i.e. each part of the corpus</context>
<context position="7848" citStr="Tanaka and Iwasaki (1996)" startWordPosition="1255" endWordPosition="1258">nsitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; i ¬i 1285 References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data Rapp (1999) Newspaper GE/EN 135/163 million words Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words D´ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 words Otero (2007) European Parliament SP/EN 14/17 million words Ismail and Manandhar (2010) European Parliament EN/SP 500,000/500,000 sentences Bouamor et al. (2013) Financial FR/EN 402,486/756,840 words - Medical FR/EN 396,524/524,805 words Table 2: Characteristics of t</context>
</contexts>
<marker>Tanaka, Iwasaki, 1996</marker>
<rawString>Kumiko Tanaka and Hideya Iwasaki. 1996. Extraction of Lexical Translations from Non-Aligned Corpora. In Proceedings of the 16th International Conference on Computational Linguistics (COLING’96), pages 580–585, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Yu</author>
<author>Junichi Tsujii</author>
</authors>
<title>Extracting bilingual dictionary from comparable corpora with dependency heterogeneity.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’09),</booktitle>
<pages>121--124</pages>
<location>Boulder, CO, USA.</location>
<contexts>
<context position="13407" citStr="Yu and Tsujii (2009)" startWordPosition="2169" endWordPosition="2172">pproach. Prochasson et al. (2009) enhance the representativeness of the context vector by strengthening the context words that happen to be transliterated words and scientific compound words in the target language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar`es (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context. To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D´ejean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using </context>
</contexts>
<marker>Yu, Tsujii, 2009</marker>
<rawString>Kun Yu and Junichi Tsujii. 2009. Extracting bilingual dictionary from comparable corpora with dependency heterogeneity. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’09), pages 121–124, Boulder, CO, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>