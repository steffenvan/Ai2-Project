<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001505">
<title confidence="0.9971615">
Parsing Syntactic and Semantic Dependencies for Multiple Languages
with A Pipeline Approach
</title>
<author confidence="0.999061">
Han Ren, Donghong Ji Jing Wan, Mingyao Zhang
</author>
<affiliation confidence="0.956964">
School of Computer Science Center for Study of Language &amp; Information
Wuhan University Wuhan University
Wuhan 430079, China Wuhan 430079, China
</affiliation>
<email confidence="0.9708095">
cslotus@mail.whu.edu.cn {jennifer.wanj, my.zhang}@gmail.com
donghong_ji@yahoo.com
</email>
<sectionHeader confidence="0.995355" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999403928571429">
This paper describes a pipelined approach for
CoNLL-09 shared task on joint learning of
syntactic and semantic dependencies. In the
system, we handle syntactic dependency pars-
ing with a transition-based approach and util-
ize MaltParser as the base model. For SRL,
we utilize a Maximum Entropy model to iden-
tify predicate senses and classify arguments.
Experimental results show that the average
performance of our system for all languages
achieves 67.81% of macro F1 Score, 78.01%
of syntactic accuracy, 56.69% of semantic la-
beled F1, 71.66% of macro precision and
64.66% of micro recall.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998933352941177">
Given a sentence with corresponding part-of-
speech for each word, the task of syntactic and se-
mantic dependency parsing contains two folds: (1)
identifying the syntactic head of each word and
assigning the dependency relationship between the
word and its head; (2) identifying predicates with
proper senses and labeling semantic dependencies
for them.
For data-driven syntactic dependency parsing,
many approaches are based on supervised learning
using treebank or annotated datasets. Currently,
graph-based and transition-based algorithms are
two dominating approaches that are employed by
many researchers, especially in previous CoNLL
shared tasks. Graph-based algorithms (Eisner,
1996; McDonald et al., 2005) assume a series of
dependency tree candidates for a sentence and the
</bodyText>
<page confidence="0.993631">
97
</page>
<bodyText confidence="0.999910916666667">
goal is to find the dependency tree with highest
score. Transition-based algorithms (Yamada and
Matsumoto, 2003; Nivre et al., 2004) utilize transi-
tion histories learned from dependencies within
sentences to predict next state transition and build
the optimal transition sequence. Although different
strategies were considered, two approaches yielded
comparable results at previous tasks.
Semantic role labeling contains two problems:
identification and labeling. Identification is a bi-
nary classification problem, and the goal is to iden-
tify annotated units in a sentence; while labeling is
a multi-class classification problem, which is to
assign arguments with appropriate semantic roles.
Hacioglu (2004) utilized predicate-argument struc-
ture and map dependency relations to semantic
roles. Liu et al. (2005) combined two problems
into a classification one, avoiding some annotated
units being excluded due to some incorrect identi-
fication results. In addition, various features are
also selected to improve accuracy of SRL.
In this paper, we propose a pipelined approach
for CoNLL-09 shared task on joint learning of syn-
tactic and semantic dependencies, and describe our
system that can handle multiple languages. In the
system, we handle syntactic dependency parsing
with a transition-based approach. For SRL, we util-
ize Maximum Entropy model to identify predicate
senses and classify arguments.
The remain of the paper is organized as follows.
In Section 2, we discuss the processing mechanism
containing syntactic and semantic dependency
parsing of our system in detail. In Section 3, we
give the evaluation results and analysis. Finally,
the conclusion and future work are given in Sec-
tion 4.
</bodyText>
<note confidence="0.61385">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 97–102,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<listItem confidence="0.413251">
2 System Description pendency relation t→n exists, it will be appended
into A and t will be removed from S.
</listItem>
<bodyText confidence="0.999025166666667">
The system, which is a two-stage pipeline, proc-
esses syntactic and semantic dependencies respec-
tively. To reduce the difficulties in SRL, predicates
of each sentence in all training and evaluation data
are labeled, thus predicate identification can be
ignored.
</bodyText>
<figureCaption confidence="0.993829">
Figure 1. System Architectures
</figureCaption>
<bodyText confidence="0.999872285714286">
For syntactic dependencies, we employ a state-
of-the-art dependency parser and basic plus ex-
tended features for parsing. For semantic depend-
encies, a Maximum Entropy Model is used both in
predicate sense identification and semantic role
labeling. Following subsections will show compo-
nents of our system in detail.
</bodyText>
<subsectionHeader confidence="0.99534">
2.1 Syntactic Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.99982525">
In the system, MaltParser1 is employed for syntac-
tic dependency parsing. MaltParser is a data-driven
deterministic dependency parser, based on a Sup-
port Vector Machine classifier. An extensive re-
search (Nivre, 2007) parsing with 9 different
languages shows that the parser is language-
independent and yields good results.
MaltParser supports two kinds of parsing algo-
rithms: Nivre’s algorithms and Covington’s incre-
mental algorithms. Nivre’s algorithms, which are
deterministic algorithms consisting of a series of
shift-reduce procedures, defines four operations:
</bodyText>
<listItem confidence="0.885350666666667">
· Right. For a given triple &lt;t|S, n|I, A&gt;, S
represents STACK and I represents INPUT. If
dependency relation t → n exists, it will be
</listItem>
<footnote confidence="0.744678">
1 http://w3.msi.vxu.se/~jha/maltparser/
</footnote>
<bodyText confidence="0.997048185185185">
·Left. For a given triple &lt;t|S, n|I, A&gt;, if de-
pendency relation n→t exists, it will be appended
into A and n will be pushed into S.
·Reduce. If dependency relation n→t does not
exist, and the parent node of t exists left to it, t will
be removed from S.
·Shift. If none of the above satisfies, n will be
pushed into S.
The deterministic algorithm simplifies determi-
nation for Reduce operation. As a matter of fact,
some languages, such as Chinese, have more flexi-
ble word order, and some words have a long dis-
tance with their children. In this case, t should not
be removed from S, but be handled with Shift op-
eration. Otherwise, dependency relations between t
and its children will never be identified, thus se-
quential errors of dependency relations may occur
after the Reduce operation.
For syntactic dependencies with long distance,
an improved Reduce strategy is: if the dependency
relation between n and t does not exist, and the
parent node of t exists left to it and the dependency
relation between the parent node and n, t will be
removed from S. The Reduce operation is projec-
tive, since it doesn’t influence the following pars-
ing procedures. The Improved algorithm is
described as follows:
</bodyText>
<listItem confidence="0.998129555555556">
(1) one of the four operations is performed ac-
cording to the dependency relation between t and n
until EOS; if only one token remains in S, go to (3).
(2) continue to select operations for remaining
tokens in S; when Shift procedure is performed,
push t to S; if only one token remains in S and I
contains more tokens than only EOS, goto (1).
(3) label all odd tokens in S as ROOT, pointing
to EOS.
</listItem>
<bodyText confidence="0.999807222222222">
We also utilize history-based feature models
implemented in the parser to predict the next action
in the deterministic derivation of a dependency
structure. The parser provides some default fea-
tures that is general for most languages: (1) part-
of-speech features of TOP and NEXT and follow-
ing 3 tokens; (2) dependency features of TOP con-
taining leftmost and rightmost dependents, and of
NEXT containing leftmost dependents; (3) Lexical
</bodyText>
<page confidence="0.995023">
98
</page>
<bodyText confidence="0.9997458">
features of TOP, head of TOP, NEXT and follow-
ing one token. We also extend features for multiple
languages: (1) count of part-of-speech features of
following tokens extend to 5; (2) part-of-speech
and dependent features of head of TOP.
</bodyText>
<subsectionHeader confidence="0.999453">
2.2 Semantic Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999909923076923">
Each defacto predicate in training and evaluation
data of CoNLL09 is labeled with a sign ‘Y’, which
simplifies the work of semantic dependency pars-
ing. In our system, semantic dependency parsing is
a pipeline that contains two parts: predicate sense
identification and semantic role labeling. For
predicate sense identification, each predicate is
assigned a certain sense number. For semantic role
labeling, local and global features are selected.
Features of each part are trained by a classification
algorithm. Both parts employ a Maximum Entropy
Tool MaxEnt in a free package OpenNLP2 as a
classifier.
</bodyText>
<subsectionHeader confidence="0.623069">
2.2.1 Predicate Sense Identification
</subsectionHeader>
<bodyText confidence="0.999952615384616">
The goal of predicate sense identification is to de-
cide the correct frame for a predicate. According to
PropBank (Palmer, et al., 2005), predicates contain
one or more rolesets corresponding to different
senses. In our system, a classifier is employed to
identify each predicate’s sense.
Suppose C = {01, 02, ... , NL } is the sense set
(NL is the count of categories corresponding to the
language L, eg., in Chinese training set NL = 10
since predicates have at most 10 senses in the set),
and ti is the ith sense of word w in sentence s. The
model is implemented to assign each predicate to
the most probatilistic sense.
</bodyText>
<equation confidence="0.993388">
t = argmaxi∈CP(w  |s , ti) (1)
</equation>
<bodyText confidence="0.973623333333333">
Features for predicate sense identification are
listed as follows:
· WORD, LEMMA, DEPREL: The lexical
form and lemma of the predicate; the dependency
relation between the predicate and its head; for
Chinese and Japanese, WORD is ignored.
· HEAD_WORD, HEAD_POS: The lexical
form and part-of-speech of the head of the predi-
cate.
</bodyText>
<footnote confidence="0.573417">
2 http://maxent.sourceforge.net/
</footnote>
<listItem confidence="0.731038454545455">
· CHILD_WORD_SET, CHILD_POS_SET,
CHILD_DEP_SET: The lexical form, part-of-
speech and dependency relation of dependents of
the predicate.
·LSIB_WORD, LSIB_POS, LSIB_DEPREL,
RSIB_WORD, RSIB_POS, RSIB_DEPREL: The
lexical form, part-of-speech and dependency rela-
tion of the left and right sibling token of the predi-
cate. Features of sibling tokens are adopted,
because senses of some predicates can be inferred
from its left or right sibling.
</listItem>
<bodyText confidence="0.999309333333333">
For English data set, we handle verbal and
nominal predicates respectively; for other lan-
guages, we handle all predicates with one classifier.
If a predicate in the evaluation data does not exist
in the training data, it is assigned the most frequent
sense label in the training data.
</bodyText>
<subsubsectionHeader confidence="0.734842">
2.2.2 Semantic Role Labeling
</subsubsectionHeader>
<bodyText confidence="0.908805965517241">
Semantic role labeling task contains two parts: ar-
gument identification and argument classification.
In our system the two parts are combined as one
classification task. Our reason is that those argu-
ment candidates that potentially become semantic
roles of corresponding predicates should not be
pruned by incorrect argument identification. In our
system, a predicate-argument pair consists of any
token (except predicates) and any predicate in a
sentence. However, we find that argument classifi-
cation is a time-consuming procedure in the ex-
periment because the classifier spends much time
on a great many of invalid predicate-argument
pairs. To reduce useless computing, we add a sim-
ple pruning method based on heuristic rules to re-
move invalid pairs, such as punctuations and some
functional words.
Features used in our system are based on (Ha-
cioglu, 2004) and (Pradhan et al, 2005), and de-
scribed as follows:
·WORD, LEMMA, DEPREL: The same with
those mentioned in section 2.2.1.
·VOICE: For verbs, the feature is Active or
Passive; for nouns, it is null.
·POSITION: The word’s position correspond-
ing to its predicate: Left, Right or Self.
·PRED: The lemma plus sense of the word.
·PRED_POS: The part-of-speech of the predi-
cate.
</bodyText>
<page confidence="0.959634">
99
</page>
<listItem confidence="0.985913857142857">
·LEFTM_WORD, LEFTM_POS, RIGHTM_
WORD, RIGHTM_POS: Leftmost and rightmost
word and their part-of-speech of the word.
· POS_PATH: All part-of-speech from the
word to its predicate, including Up, Down, Left
and Right, eg. “NNTVV1CC1VV”.
·DEPREL_PATH: Dependency relations from
the word to its predicate, eg. “COMPTRELCT
COMP1”.
· ANC_POS_PATH, ANC_DEPREL_PATH:
Similar to POS_PATH and DEPREL_PATH, part-
of-speech and dependency relations from the word
to the common ancestor with its predicate.
·PATH_LEN: Count of passing words from
the word to its predicate.
· FAMILY: Relationship between the word
and its predicate, including Child, Parent, Descen-
dant, Ancestor, Sibling, Self and Null.
· PRED_CHD_POS, PRED_CHD_DEPREL:
Part-of-speech and dependency relations of all
children of the word’s predicate.
</listItem>
<bodyText confidence="0.991962615384615">
For different languages, some features men-
tioned above are invalid and should be removed,
and some extended features could improve the per-
formance of the classifier. In our system we mainly
focus on Chinese, therefore, WORD and VOICE
should be removed when processing Chinese data
set. We also adopt some features proposed by (Xue,
2008):
· POS_PATH_BA, POS_PATH_SB, POS_
PATH_LB: BA and BEI are functional words that
impact the order of arguments. In PropBank, BA
words have the POS tag BA, and BEI words have
two POS tags: SB (short BEI) and LB (long BEI).
</bodyText>
<sectionHeader confidence="0.996747" genericHeader="introduction">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.99987">
Our experiments are based on a PC with a Intel
Core 2 Duo 2.1G CPU and 2G memory. Training
and evaluation data (Taulé et al., 2008; Xue et al.,
2008; Hajič et al., 2006; Palmer et al., 2002; Bur-
chardt et al., 2006; Kawahara et al., 2002) have
been converted to a uniform CoNLL Shared Task
format. In all experiments, SVM and ME model
are trained using training data, and tested with
development data of all languages.
The system for closed challenge is designed as
two parts. For syntactic dependency training and
parsing, we utilize the projective model in Malt-
Parser for data sets. We also follow default settings
in MaltParser, such as assigned parameters for
LIBSVM and combined prediction strategy, and
utilize improved approaches mentioned in section
2. For semantic dependency training and parsing,
we choose the count of iteration as 100 and cutoff
value as 10 for the ME model. Table 1 shows the
training time for syntactic and semantic depend-
ency of all languages. Parsing time for syntactic is
not more than 30 minutes, and for semantic is not
more than 5 minutes of each language.
</bodyText>
<table confidence="0.994381375">
syn prd sem
English 7h 12min 47min
Chinese 8h 18min 61min
Japanese 7h 14min 46min
Czech 13h 46min 77min
German 6h 16min 54min
Spanish 6h 15min 55min
Catalan 6h 15min 50min
</table>
<tableCaption confidence="0.994751666666667">
Table 1. Training cost for all languages. syn, prd and
sem mean training time for syntactic dependency, predi-
cate identification and semantic dependency.
</tableCaption>
<subsectionHeader confidence="0.998521">
3.1 Syntactic Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999948">
We utilize MaltParser with improved algorithms
mentioned in section 2.1 for syntactic dependency
parsing, and the results are shown in Table 2.
</bodyText>
<table confidence="0.999166">
LAS UAS label-acc.
English 87.57 89.98 92.19
Chinese 79.17 81.22 85.94
Japanese 91.47 92.57 97.28
Czech 57.30 75.66 65.39
German 76.63 80.31 85.97
Spanish 76.11 84.40 84.69
Catalan 77.84 86.41 85.78
</table>
<tableCaption confidence="0.999929">
Table 2. Performance of syntactic dependency parsing
</tableCaption>
<bodyText confidence="0.960350071428571">
Table 2 indicates that parsing for Japanese and
English data sets has a better performance than
other languages, partly because determinative algo-
rithm and history-based grammar are more suited
for these two languages. To compare the perform-
ance of our approach of improved deterministic
algorithm and extended features, we make another
experiment that utilize original arc-standard algo-
rithm and base features for syntactic experiments.
Due to time limitation, the experiments are only
based on Chinese training and evaluation data. The
results show that LAS and UAS drops about 2.7%
and 2.2% for arc-standard algorithm, 1.6% and
1.2% for base features. They indicate that our de-
</bodyText>
<page confidence="0.982124">
100
</page>
<bodyText confidence="0.99996784">
terministic algorithm and the extend features can
help to improve syntactic dependency parsing. We
also notice that the results of Czech achieve a
lower performance than other languages. It mainly
because the language has more rich morphology,
usually accompanied by more flexible word order.
Although using a large training set, linguistic prop-
erties greatly influence the parsing result. In addi-
tion, extended features are not suited for this
language and the feature model should be opti-
mized individually.
For all of the experiments we mainly focus on
the language of Chinese. When parsing Chinese
data sets we find that the focus words where most
of the errors occur are almost punctuations, such as
commas and full stops. Apart from errors of punc-
tuations, most errors occur on prepositions such as
the Chinese word ‘at’. Most of these problems
come from assigning the incorrect dependencies,
and the reason is that the parsing algorithm con-
cerns the form rather than the function of these
words. In addition, the prediction of dependency
relation ROOT achieves lower precision and recall
than others, indicating that MaltParser overpredicts
dependencies to the root.
</bodyText>
<subsectionHeader confidence="0.999868">
3.2 Semantic Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.99848675">
MaxEnt is employed as our classifier to train and
parse semantic dependencies, and the results are
shown in Table 3, in which all criterions are la-
beled.
</bodyText>
<table confidence="0.999331">
P R F1
English 76.57 60.45 67.56
Chinese 75.45 69.92 72.58
Japanese 91.93 43.15 58.73
Czech 68.83 57.78 62.82
German 62.96 47.75 54.31
Spanish 40.11 39.50 39.80
Catalan 41.34 40.66 41.00
</table>
<tableCaption confidence="0.999826">
Table 3. Performance of semantic dependency parsing
</tableCaption>
<bodyText confidence="0.999969933333334">
As shown in Table 3, the scores of the latter
five languages are quite lower than those of the
former two languages, and the main reason could
be inferred from the scores of Table 2 that the drop
of the performance of semantic dependency pars-
ing comes from the low performance of syntactic
dependency parsing. Another reason is that, mor-
phological features are not be utilized in the classi-
fier. Our post experiments after submission show
that average performance could improve the per-
formance after adding morphological and some
combined features. In addition, difference between
precision and recall indicates that the classification
procedure works better than the identification
procedure in semantic role labeling.
For Chinese, semantic role of some words with
part-of-speech VE have been mislabeled. It’s
mainly because that these words in Chinese have
multiple part-of-speech. The errors of POS and
PRED greatly influence the system to perform
these words. Another main problem occurs on the
pairs NN + A0/A1. Identification of the two pairs
are much lower than VA/VC/VE/VV + A0/A1
pairs. The reason is that the identification of nomi-
nal predicates have more errors than that of verbal
predicates due to the combination of SRL for these
two kinds of predicates. For further study, verbal
predicates and nominal predicates should be han-
dled respectively so that the overall performance
can be improved.
</bodyText>
<subsectionHeader confidence="0.997896">
3.3 Overall Performance
</subsectionHeader>
<bodyText confidence="0.9999746">
The average performance of our system for all lan-
guages achieves 67.81% of macro F1 Score,
78.01% of syntactic accuracy, 56.69% of semantic
labeled F1, 71.66% of macro precision and 64.66%
of micro recall.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999921166666667">
In this paper, we propose a pipelined approach for
CoNLL-09 shared task on joint learning of syntac-
tic and semantic dependencies, and describe our
system that can handle multiple languages. Our
system focuses on improving the performance of
syntactic and semantic dependency respectively.
Experimental results show that the overall per-
formance can be improved for multiple languages
by long distance dependency algorithm and ex-
tended history-based features. Besides, the system
fits for verbal predicates than nominal predicates
and the classification procedure works better than
identification procedure in semantic role labeling.
For further study, respective process should be
handled between these two kinds of predicates, and
argument identification should be improved by
using more discriminative features for a better
overall performance.
</bodyText>
<page confidence="0.998561">
101
</page>
<sectionHeader confidence="0.998328" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99866975">
This work is supported by the Natural Science
Foundation of China under Grant Nos.60773011,
90820005, and Independent Research Foundation
of Wuhan University.
</bodyText>
<sectionHeader confidence="0.989984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999602131868132">
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Padó and Manfred Pinkal. 2006.
The SALSA Corpus: a German Corpus Resource for
Lexical Semantics. Proceedings of the Sth Interna-
tional Conference on Language Resources and
Evaluation (LREC-2006). Genoa, Italy.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING), pp.340–345.
Kadri Hacioglu. 2004. Semantic Role Labeling Using
Dependency Trees. In Proceedings of the Interna-
tional Conference on Computational Linguistics
(COLING).
Jan Haji6, Jarmila Panevová, Eva Haji6ová, Petr Sgall,
Petr Pajas, Jan Št6pánek, Jiri Havelka, Marie Mikulo-
vá and Zden&amp; Žabokrtský. 2006. The Prague De-
pendency Treebank 2.0. CD-ROM. Linguistic Data
Consortium, Philadelphia, Pennsylvania, USA. ISBN
1-58563-370-4. LDC Cat. No. LDC2006T01. URL:
http://ldc.upenn.edu.
Jan Haji6, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria Antonia Martí, Lluís
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Št6pánek, Pavel Straiiák, Mihai Surdeanu,
Nianwen Xue and Yi Zhang. 2009. The CoNLL 2009
Shared Task: Syntactic and Semantic Dependencies
in Multiple Languages. Proceedings of the 13th
Conference on Computational Natural Language
Learning (CoNLL-2009). Boulder, Colorado, USA.
June 4-5. pp.3-22.
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida.
2002. Construction of a Japanese Relevance-tagged
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain. pp.2008-2013.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pp.91–98.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of the 8th Conference on Computational Natural Lan-
guage Learning (CoNLL), pp.49–56.
Joakim Nivre. 2004. Incrementality in Deterministic
Dependency Parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together. Workshop
at ACL-2004, Barcelona, Spain, pp.50-57.
Joakim Nivre and Johan Hall. 2005. MaltParser: A lan-
guage-independent system for data-driven depend-
ency parsing. In Proceedings of the Fourth Workshop
on Treebanks and Linguistic Theories (TLT).
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural language Engineering, Volume 13, Is-
sue 02, pp.95-135.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin and Daniel Jurafsky.
2005. Support Vector Learning for Semantic Argu-
ment classification. Machine Learning Journal, 2005,
60(3): 11−39.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluís Màrquez, and Joakim Nivre. 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
Mariona Taulé, Maria Antònia Martí and Marta Reca-
sens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation (LREC-2008). Marrakech, Morocco.
Liu Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system us-
ing maximum entropy classifier. In Proceedings of
the 8th Conference on Computational Natural Lan-
guage Learning (CoNLL).
Nianwen Xue. 2008. Labeling Chinese Predicates with
Semantic roles. Computational Linguistics, 34(2):
225-255.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143-172.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pp.195–
206.
</reference>
<page confidence="0.998604">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.772678">
<title confidence="0.9998745">Parsing Syntactic and Semantic Dependencies for Multiple with A Pipeline Approach</title>
<author confidence="0.998903">Han Ren</author>
<author confidence="0.998903">Donghong Ji Jing Wan</author>
<author confidence="0.998903">Mingyao Zhang</author>
<affiliation confidence="0.999976">School of Computer Science Center for Study of Language &amp; Information Wuhan University Wuhan University</affiliation>
<address confidence="0.999125">Wuhan 430079, China Wuhan 430079, China</address>
<email confidence="0.90661">cslotus@mail.whu.edu.cn{jennifer.wanj,donghong_ji@yahoo.com</email>
<abstract confidence="0.996415466666667">This paper describes a pipelined approach for CoNLL-09 shared task on joint learning of syntactic and semantic dependencies. In the system, we handle syntactic dependency parsing with a transition-based approach and utilize MaltParser as the base model. For SRL, we utilize a Maximum Entropy model to identify predicate senses and classify arguments. Experimental results show that the average performance of our system for all languages achieves 67.81% of macro F1 Score, 78.01% of syntactic accuracy, 56.69% of semantic labeled F1, 71.66% of macro precision and 64.66% of micro recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Padó</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA Corpus: a German Corpus Resource for Lexical Semantics.</title>
<date>2006</date>
<booktitle>Proceedings of the Sth International Conference on Language Resources and Evaluation (LREC-2006).</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="12687" citStr="Burchardt et al., 2006" startWordPosition="1984" endWordPosition="1988">system we mainly focus on Chinese, therefore, WORD and VOICE should be removed when processing Chinese data set. We also adopt some features proposed by (Xue, 2008): · POS_PATH_BA, POS_PATH_SB, POS_ PATH_LB: BA and BEI are functional words that impact the order of arguments. In PropBank, BA words have the POS tag BA, and BEI words have two POS tags: SB (short BEI) and LB (long BEI). 3 Experimental Results Our experiments are based on a PC with a Intel Core 2 Duo 2.1G CPU and 2G memory. Training and evaluation data (Taulé et al., 2008; Xue et al., 2008; Hajič et al., 2006; Palmer et al., 2002; Burchardt et al., 2006; Kawahara et al., 2002) have been converted to a uniform CoNLL Shared Task format. In all experiments, SVM and ME model are trained using training data, and tested with development data of all languages. The system for closed challenge is designed as two parts. For syntactic dependency training and parsing, we utilize the projective model in MaltParser for data sets. We also follow default settings in MaltParser, such as assigned parameters for LIBSVM and combined prediction strategy, and utilize improved approaches mentioned in section 2. For semantic dependency training and parsing, we choo</context>
</contexts>
<marker>Burchardt, Erk, Frank, Kowalski, Padó, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Padó and Manfred Pinkal. 2006. The SALSA Corpus: a German Corpus Resource for Lexical Semantics. Proceedings of the Sth International Conference on Language Resources and Evaluation (LREC-2006). Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>340--345</pages>
<contexts>
<context position="1662" citStr="Eisner, 1996" startWordPosition="234" endWordPosition="235">f syntactic and semantic dependency parsing contains two folds: (1) identifying the syntactic head of each word and assigning the dependency relationship between the word and its head; (2) identifying predicates with proper senses and labeling semantic dependencies for them. For data-driven syntactic dependency parsing, many approaches are based on supervised learning using treebank or annotated datasets. Currently, graph-based and transition-based algorithms are two dominating approaches that are employed by many researchers, especially in previous CoNLL shared tasks. Graph-based algorithms (Eisner, 1996; McDonald et al., 2005) assume a series of dependency tree candidates for a sentence and the 97 goal is to find the dependency tree with highest score. Transition-based algorithms (Yamada and Matsumoto, 2003; Nivre et al., 2004) utilize transition histories learned from dependencies within sentences to predict next state transition and build the optimal transition sequence. Although different strategies were considered, two approaches yielded comparable results at previous tasks. Semantic role labeling contains two problems: identification and labeling. Identification is a binary classificati</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th International Conference on Computational Linguistics (COLING), pp.340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
</authors>
<title>Semantic Role Labeling Using Dependency Trees.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="2466" citStr="Hacioglu (2004)" startWordPosition="350" endWordPosition="351">and Matsumoto, 2003; Nivre et al., 2004) utilize transition histories learned from dependencies within sentences to predict next state transition and build the optimal transition sequence. Although different strategies were considered, two approaches yielded comparable results at previous tasks. Semantic role labeling contains two problems: identification and labeling. Identification is a binary classification problem, and the goal is to identify annotated units in a sentence; while labeling is a multi-class classification problem, which is to assign arguments with appropriate semantic roles. Hacioglu (2004) utilized predicate-argument structure and map dependency relations to semantic roles. Liu et al. (2005) combined two problems into a classification one, avoiding some annotated units being excluded due to some incorrect identification results. In addition, various features are also selected to improve accuracy of SRL. In this paper, we propose a pipelined approach for CoNLL-09 shared task on joint learning of syntactic and semantic dependencies, and describe our system that can handle multiple languages. In the system, we handle syntactic dependency parsing with a transition-based approach. F</context>
<context position="10715" citStr="Hacioglu, 2004" startWordPosition="1669" endWordPosition="1671">tic roles of corresponding predicates should not be pruned by incorrect argument identification. In our system, a predicate-argument pair consists of any token (except predicates) and any predicate in a sentence. However, we find that argument classification is a time-consuming procedure in the experiment because the classifier spends much time on a great many of invalid predicate-argument pairs. To reduce useless computing, we add a simple pruning method based on heuristic rules to remove invalid pairs, such as punctuations and some functional words. Features used in our system are based on (Hacioglu, 2004) and (Pradhan et al, 2005), and described as follows: ·WORD, LEMMA, DEPREL: The same with those mentioned in section 2.2.1. ·VOICE: For verbs, the feature is Active or Passive; for nouns, it is null. ·POSITION: The word’s position corresponding to its predicate: Left, Right or Self. ·PRED: The lemma plus sense of the word. ·PRED_POS: The part-of-speech of the predicate. 99 ·LEFTM_WORD, LEFTM_POS, RIGHTM_ WORD, RIGHTM_POS: Leftmost and rightmost word and their part-of-speech of the word. · POS_PATH: All part-of-speech from the word to its predicate, including Up, Down, Left and Right, eg. “NNTV</context>
</contexts>
<marker>Hacioglu, 2004</marker>
<rawString>Kadri Hacioglu. 2004. Semantic Role Labeling Using Dependency Trees. In Proceedings of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Haji6</author>
<author>Jarmila Panevová</author>
</authors>
<title>Eva Haji6ová, Petr Sgall, Petr Pajas, Jan Št6pánek, Jiri Havelka, Marie Mikulová and Zden&amp; Žabokrtský.</title>
<date>2006</date>
<booktitle>The Prague Dependency Treebank 2.0. CD-ROM. Linguistic Data Consortium,</booktitle>
<tech>ISBN 1-58563-370-4. LDC Cat. No. LDC2006T01. URL: http://ldc.upenn.edu.</tech>
<location>Philadelphia, Pennsylvania, USA.</location>
<marker>Haji6, Panevová, 2006</marker>
<rawString>Jan Haji6, Jarmila Panevová, Eva Haji6ová, Petr Sgall, Petr Pajas, Jan Št6pánek, Jiri Havelka, Marie Mikulová and Zden&amp; Žabokrtský. 2006. The Prague Dependency Treebank 2.0. CD-ROM. Linguistic Data Consortium, Philadelphia, Pennsylvania, USA. ISBN 1-58563-370-4. LDC Cat. No. LDC2006T01. URL: http://ldc.upenn.edu.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Haji6</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
</authors>
<title>Maria Antonia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Št6pánek, Pavel Straiiák, Mihai Surdeanu, Nianwen Xue and Yi Zhang.</title>
<date>2009</date>
<booktitle>The CoNLL 2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009).</booktitle>
<pages>4--5</pages>
<location>Boulder, Colorado, USA.</location>
<marker>Haji6, Ciaramita, Johansson, Kawahara, 2009</marker>
<rawString>Jan Haji6, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Antonia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Št6pánek, Pavel Straiiák, Mihai Surdeanu, Nianwen Xue and Yi Zhang. 2009. The CoNLL 2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009). Boulder, Colorado, USA. June 4-5. pp.3-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Koiti Hasida</author>
</authors>
<title>Construction of a Japanese Relevance-tagged Corpus.</title>
<date>2002</date>
<booktitle>Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC2002). Las</booktitle>
<pages>2008--2013</pages>
<location>Palmas,</location>
<contexts>
<context position="12711" citStr="Kawahara et al., 2002" startWordPosition="1989" endWordPosition="1992">n Chinese, therefore, WORD and VOICE should be removed when processing Chinese data set. We also adopt some features proposed by (Xue, 2008): · POS_PATH_BA, POS_PATH_SB, POS_ PATH_LB: BA and BEI are functional words that impact the order of arguments. In PropBank, BA words have the POS tag BA, and BEI words have two POS tags: SB (short BEI) and LB (long BEI). 3 Experimental Results Our experiments are based on a PC with a Intel Core 2 Duo 2.1G CPU and 2G memory. Training and evaluation data (Taulé et al., 2008; Xue et al., 2008; Hajič et al., 2006; Palmer et al., 2002; Burchardt et al., 2006; Kawahara et al., 2002) have been converted to a uniform CoNLL Shared Task format. In all experiments, SVM and ME model are trained using training data, and tested with development data of all languages. The system for closed challenge is designed as two parts. For syntactic dependency training and parsing, we utilize the projective model in MaltParser for data sets. We also follow default settings in MaltParser, such as assigned parameters for LIBSVM and combined prediction strategy, and utilize improved approaches mentioned in section 2. For semantic dependency training and parsing, we choose the count of iteratio</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 2002. Construction of a Japanese Relevance-tagged Corpus. Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC2002). Las Palmas, Spain. pp.2008-2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1686" citStr="McDonald et al., 2005" startWordPosition="236" endWordPosition="239">d semantic dependency parsing contains two folds: (1) identifying the syntactic head of each word and assigning the dependency relationship between the word and its head; (2) identifying predicates with proper senses and labeling semantic dependencies for them. For data-driven syntactic dependency parsing, many approaches are based on supervised learning using treebank or annotated datasets. Currently, graph-based and transition-based algorithms are two dominating approaches that are employed by many researchers, especially in previous CoNLL shared tasks. Graph-based algorithms (Eisner, 1996; McDonald et al., 2005) assume a series of dependency tree candidates for a sentence and the 97 goal is to find the dependency tree with highest score. Transition-based algorithms (Yamada and Matsumoto, 2003; Nivre et al., 2004) utilize transition histories learned from dependencies within sentences to predict next state transition and build the optimal transition sequence. Although different strategies were considered, two approaches yielded comparable results at previous tasks. Semantic role labeling contains two problems: identification and labeling. Identification is a binary classification problem, and the goal</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pp.91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>49--56</pages>
<contexts>
<context position="1891" citStr="Nivre et al., 2004" startWordPosition="269" endWordPosition="272">r senses and labeling semantic dependencies for them. For data-driven syntactic dependency parsing, many approaches are based on supervised learning using treebank or annotated datasets. Currently, graph-based and transition-based algorithms are two dominating approaches that are employed by many researchers, especially in previous CoNLL shared tasks. Graph-based algorithms (Eisner, 1996; McDonald et al., 2005) assume a series of dependency tree candidates for a sentence and the 97 goal is to find the dependency tree with highest score. Transition-based algorithms (Yamada and Matsumoto, 2003; Nivre et al., 2004) utilize transition histories learned from dependencies within sentences to predict next state transition and build the optimal transition sequence. Although different strategies were considered, two approaches yielded comparable results at previous tasks. Semantic role labeling contains two problems: identification and labeling. Identification is a binary classification problem, and the goal is to identify annotated units in a sentence; while labeling is a multi-class classification problem, which is to assign arguments with appropriate semantic roles. Hacioglu (2004) utilized predicate-argum</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL), pp.49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in Deterministic Dependency Parsing.</title>
<date>2004</date>
<booktitle>In Incremental Parsing: Bringing Engineering and Cognition Together. Workshop at ACL-2004,</booktitle>
<pages>50--57</pages>
<location>Barcelona,</location>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in Deterministic Dependency Parsing. In Incremental Parsing: Bringing Engineering and Cognition Together. Workshop at ACL-2004, Barcelona, Spain, pp.50-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
</authors>
<title>MaltParser: A language-independent system for data-driven dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth Workshop on Treebanks and Linguistic Theories (TLT).</booktitle>
<marker>Nivre, Hall, 2005</marker>
<rawString>Joakim Nivre and Johan Hall. 2005. MaltParser: A language-independent system for data-driven dependency parsing. In Proceedings of the Fourth Workshop on Treebanks and Linguistic Theories (TLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural language Engineering, Volume 13, Issue</journal>
<volume>02</volume>
<pages>95--135</pages>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov and Erwin Marsi. 2007. MaltParser: A languageindependent system for data-driven dependency parsing. Natural language Engineering, Volume 13, Issue 02, pp.95-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support Vector Learning for Semantic Argument classification.</title>
<date>2005</date>
<journal>Machine Learning Journal,</journal>
<volume>60</volume>
<issue>3</issue>
<pages>11--39</pages>
<contexts>
<context position="10741" citStr="Pradhan et al, 2005" startWordPosition="1673" endWordPosition="1676">nding predicates should not be pruned by incorrect argument identification. In our system, a predicate-argument pair consists of any token (except predicates) and any predicate in a sentence. However, we find that argument classification is a time-consuming procedure in the experiment because the classifier spends much time on a great many of invalid predicate-argument pairs. To reduce useless computing, we add a simple pruning method based on heuristic rules to remove invalid pairs, such as punctuations and some functional words. Features used in our system are based on (Hacioglu, 2004) and (Pradhan et al, 2005), and described as follows: ·WORD, LEMMA, DEPREL: The same with those mentioned in section 2.2.1. ·VOICE: For verbs, the feature is Active or Passive; for nouns, it is null. ·POSITION: The word’s position corresponding to its predicate: Left, Right or Self. ·PRED: The lemma plus sense of the word. ·PRED_POS: The part-of-speech of the predicate. 99 ·LEFTM_WORD, LEFTM_POS, RIGHTM_ WORD, RIGHTM_POS: Leftmost and rightmost word and their part-of-speech of the word. · POS_PATH: All part-of-speech from the word to its predicate, including Up, Down, Left and Right, eg. “NNTVV1CC1VV”. ·DEPREL_PATH: De</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin and Daniel Jurafsky. 2005. Support Vector Learning for Semantic Argument classification. Machine Learning Journal, 2005, 60(3): 11−39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluís Màrquez</author>
<author>Joakim Nivre</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</booktitle>
<marker>Surdeanu, Johansson, Meyers, Màrquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluís Màrquez, and Joakim Nivre. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taulé</author>
</authors>
<title>Maria Antònia Martí and Marta Recasens.</title>
<date>2008</date>
<booktitle>Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008).</booktitle>
<location>Marrakech, Morocco.</location>
<marker>Taulé, 2008</marker>
<rawString>Mariona Taulé, Maria Antònia Martí and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008). Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liu Ting</author>
<author>Wanxiang Che</author>
<author>Sheng Li</author>
<author>Yuxuan Hu</author>
<author>Huaijun Liu</author>
</authors>
<title>Semantic role labeling system using maximum entropy classifier.</title>
<date>2005</date>
<booktitle>In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<marker>Ting, Che, Li, Hu, Liu, 2005</marker>
<rawString>Liu Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and Huaijun Liu. 2005. Semantic role labeling system using maximum entropy classifier. In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Labeling Chinese Predicates with Semantic roles.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<pages>225--255</pages>
<contexts>
<context position="12229" citStr="Xue, 2008" startWordPosition="1901" endWordPosition="1902">m the word to its predicate. · FAMILY: Relationship between the word and its predicate, including Child, Parent, Descendant, Ancestor, Sibling, Self and Null. · PRED_CHD_POS, PRED_CHD_DEPREL: Part-of-speech and dependency relations of all children of the word’s predicate. For different languages, some features mentioned above are invalid and should be removed, and some extended features could improve the performance of the classifier. In our system we mainly focus on Chinese, therefore, WORD and VOICE should be removed when processing Chinese data set. We also adopt some features proposed by (Xue, 2008): · POS_PATH_BA, POS_PATH_SB, POS_ PATH_LB: BA and BEI are functional words that impact the order of arguments. In PropBank, BA words have the POS tag BA, and BEI words have two POS tags: SB (short BEI) and LB (long BEI). 3 Experimental Results Our experiments are based on a PC with a Intel Core 2 Duo 2.1G CPU and 2G memory. Training and evaluation data (Taulé et al., 2008; Xue et al., 2008; Hajič et al., 2006; Palmer et al., 2002; Burchardt et al., 2006; Kawahara et al., 2002) have been converted to a uniform CoNLL Shared Task format. In all experiments, SVM and ME model are trained using tra</context>
</contexts>
<marker>Xue, 2008</marker>
<rawString>Nianwen Xue. 2008. Labeling Chinese Predicates with Semantic roles. Computational Linguistics, 34(2): 225-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<pages>15--1</pages>
<marker>Xue, Palmer, 2009</marker>
<rawString>Nianwen Xue and Martha Palmer. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>195--206</pages>
<contexts>
<context position="1870" citStr="Yamada and Matsumoto, 2003" startWordPosition="265" endWordPosition="268">ifying predicates with proper senses and labeling semantic dependencies for them. For data-driven syntactic dependency parsing, many approaches are based on supervised learning using treebank or annotated datasets. Currently, graph-based and transition-based algorithms are two dominating approaches that are employed by many researchers, especially in previous CoNLL shared tasks. Graph-based algorithms (Eisner, 1996; McDonald et al., 2005) assume a series of dependency tree candidates for a sentence and the 97 goal is to find the dependency tree with highest score. Transition-based algorithms (Yamada and Matsumoto, 2003; Nivre et al., 2004) utilize transition histories learned from dependencies within sentences to predict next state transition and build the optimal transition sequence. Although different strategies were considered, two approaches yielded comparable results at previous tasks. Semantic role labeling contains two problems: identification and labeling. Identification is a binary classification problem, and the goal is to identify annotated units in a sentence; while labeling is a multi-class classification problem, which is to assign arguments with appropriate semantic roles. Hacioglu (2004) uti</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pp.195– 206.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>