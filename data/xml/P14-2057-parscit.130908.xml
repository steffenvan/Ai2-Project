<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000379">
<title confidence="0.908848">
Tri-Training for Authorship Attribution with Limited Training Data
</title>
<author confidence="0.810691">
Tieyun Qian Bing Liu Li Chen Zhiyong Peng
</author>
<affiliation confidence="0.5868065">
State Key Laboratory Dept. of Computer Sci- State Key Laboratory of Computer School,
of Software Eng., ence, Univ. of Illinois at Software Eng., Wuhan University
Wuhan University Chicago Wuhan University 430072, Hubei, China
430072, Hubei, China IL, USA, 60607 430072, Hubei, China peng@whu.edu.cn
</affiliation>
<email confidence="0.945017">
qty@whu.edu.cn liub@cs.uic.edu ccnuchenli@163.com
</email>
<sectionHeader confidence="0.992862" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999173521739131">
Authorship attribution (AA) aims to identify
the authors of a set of documents. Traditional
studies in this area often assume that there are
a large set of labeled documents available for
training. However, in the real life, it is often
difficult or expensive to collect a large set of
labeled data. For example, in the online review
domain, most reviewers (authors) only write a
few reviews, which are not enough to serve as
the training data for accurate classification. In
this paper, we present a novel three-view tri-
training method to iteratively identify authors
of unlabeled data to augment the training set.
The key idea is to first represent each docu-
ment in three distinct views, and then perform
tri-training to exploit the large amount of un-
labeled documents. Starting from 10 training
documents per author, we systematically eval-
uate the effectiveness of the proposed tri-
training method for AA. Experimental results
show that the proposed approach outperforms
the state-of-the-art semi-supervised method
CNG+SVM and other baselines.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931706896552">
Existing approaches to authorship attribution
(AA) are mainly based on supervised classifica-
tion (Stamatatos, 2009, Kim et al., 2011, Serous-
si et al., 2012). Although this is an effective ap-
proach, it has a major weakness, i.e., for each
author a large number of his/her articles are
needed as the training data. This is possible if the
author has written a large number of articles, but
will be difficult if he/she has not. For example, in
the online review domain, most authors (review-
ers) only write a few reviews (documents). It was
shown that on average each reviewer only has
2.72 reviews in amazon.com, and only 8% of the
reviewers have at least 5 reviews (Jindal and Liu,
2008). The small number of labeled documents
makes it extremely challenging for supervised
learning to train an accurate classifier.
In this paper, we consider AA with only a few
labeled examples. By exploiting the redundancy
in human languages, we tackle the problem using
a new three-view tri-training algorithm (TTA).
Specifically, we first represent each document in
three distinct views, and then tri-train three clas-
sifiers in these views. The predictions of two
classifiers on unlabeled examples are used to
augment the training set for the third classifier.
This process repeats until a termination condition
is met. The enlarged labeled sets are finally used
to train classifiers to classify the test data.
To our knowledge, no existing work has ad-
dressed AA in a tri-training framework. The AA
problem with limited training data was attempted
in (Stamatatos, 2007; Luyckx and Daelemans,
2008). However, neither of them used a semi-
supervised approach to augment the training set
with additional documents. Kourtis and Stama-
tatos (2011) introduced a variant of the self-
training method in (Nigam and Ghani, 2000).
Note that the original self-training uses one clas-
sifier on one view. However, the self-training
method in (Kourtis and Stamatatos, 2011) uses
two classifiers (CNG and SVM) on one view.
Both the self-training and tri-training are semi-
supervised learning methods. However, the pro-
posed approach is not a simple extension of the
self-training method CNG+SVM of (Kourtis and
Stamatatos, 2011). There are key differences.
First, in their experimental setting, about 115
and 129 documents per author on average are
used for two experimental corpora. This number
of labeled documents is still very large. We con-
sider a much more realistic problem, where the
size of the training set is very small. Only 10
samples per author are used in training.
Second, CNG+SVM uses two learning methods
on a single character n-gram view. In contrast,
besides the character n-gram view, we also make
use of the lexical and syntactic views. That is,
</bodyText>
<page confidence="0.987226">
345
</page>
<bodyText confidence="0.984272260869565">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 345–351,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
three distinct views are used for building classi-
fiers. The redundant information in human lan-
guage is combined in the tri-training procedure.
Third, in each round of self-training in
CNG+SVM, each classifier is refined by the same
newly labeled examples. However, in the pro-
posed tri-training method (TTA), the examples
labeled by the classifiers of every two views are
added to the third view. By doing so, each classi-
fier can borrow information from the other two
views. And the predictions made by two classifi-
ers are more reliable than those by one classifier.
The main contribution of this paper is thus the
proposed three-view tri-training scheme which
has a much better generalization ability by ex-
ploiting three different views of the same docu-
ment. Experimental results on the IMDb review
dataset show that the proposed method dramati-
cally improves the CNG+SVM method. It also
outperforms the co-training method (Blum and
Mitchell, 1998) based on our proposed views.
</bodyText>
<sectionHeader confidence="0.99978" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.972474666666667">
Existing AA methods either focused on finding
suitable features or on developing effective
techniques. Example features include function
words (Argamon et al., 2007), richness features
(Gamon 2004), punctuation frequencies (Graham
et al., 2005), character (Grieve, 2007), word
(Burrows, 1992) and POS n-grams (Gamon,
2004; Hirst and Feiguina, 2007), rewrite rules
(Halteren et al., 1996), and similarities (Qian and
Liu, 2013). On developing effective learning
techniques, supervised classification has been the
dominant approach, e.g., neural networks
(Graham et al., 2005; Zheng et al., 2006),
decision tree (Uzuner and Katz, 2005; Zhao and
Zobel, 2005), logistic regression (Madigan et al.,
2005), SVM (Diederich et al., 2000; Gamon
2004; Li et al., 2006; Kim et al., 2011), etc.
The main problem in the traditional research is
the unrealistic size of the training set. A size of
about 10,000 words per author is regarded as a
reasonable training set size (Argamon et al.,
2007, Burrows, 2003). When no long documents
are available, tens or hundreds of short texts are
used (Halteren, 2007; Hirst and Feiguina, 2007;
Schwartz et al., 2013).
Apart from the existing works dealing with
limited data discussed in the introduction, our
preliminary study in (Qian et al., 2014) used one
learning method on two views, but it is inferior
to the proposed method in this paper.
Input: A small set of labeled documents L = {l1,..., lr}, a large
set of unlabeled documents U = {u1,..., us}, and a set of test
documents T = {t1,..., tt},
</bodyText>
<figure confidence="0.974949947368421">
Parameters: the number of iterations k, the size of selected un-
labeled documents u
Output: tk’s class assignment
1 Extract views Lc, Ll, Ls, Uc, Ul, Us, Tc, Tl, Ts from L, U, T
2 Loop for k iterations:
3 Randomly select u unlabeled documents U&apos; from U;
4 Learn the first view classifier C1 from L1 (L1=Lc, Ll, or Ls);
5 Use C1 to label docs in U&apos; based on U1(U1=Uc, Ul, or Us)
6 Learn the second view classifier C2 from L2 (L23,L1)
7 Use C2 to label documents in U&apos; based on U2 (U23,U1);
8 Learn the third view classifier C3 from L3 (L23,L1, L2)
9 Use C3 to label documents in U&apos; based on U3 (U23,U1, U2);
10 Up1 = {u  |ue U&apos;, u.label by C2 = u.label by C3};
11 Up2 = {u  |ue U&apos;, u.label by C1 = u.label by C3};
12 Up3 = {u  |ue U&apos;, u.label by C1 = u.label by C2};
13 U = U - U&apos;, Li = Li v Upi (i=1..3);
14 Learn three classifiers C1, C2, C3 from L1, L2, L3;
15 Use Ci to label tk in Ti (i=1..3);
16 Aggregate results from three views
</figure>
<figureCaption confidence="0.999978">
Figure 1: The tri-training algorithm (TTA)
</figureCaption>
<sectionHeader confidence="0.784981" genericHeader="method">
3 Proposed Tri-Training Algorithm
</sectionHeader>
<subsectionHeader confidence="0.992411">
3.1 Overall Framework
</subsectionHeader>
<bodyText confidence="0.9999742">
We represent each document in three feature
views: the character view, the lexical view and
the syntactic view. Each view consists of a set of
features in the respective type. A classifier can
be learned from any of these views. We propose
a three-view training algorithm to deal with the
problem of limited training data. Logistic
regression (LR) is used as the learner. The
overall framework is shown in Figure 1.
Given the labeled, unlabeled, and test sets L,
U, and T, step 1 extracts the character, lexical,
and syntactic views from L, U, and T,
respectively. Steps 2-13 iteratively tri-train three
classifiers by adding the data which are assigned
the same label by two classifiers into the training
set of the third classifier. The algorithm first
randomly selects u unlabeled documents from U
to create a pool U’ of examples. Note that we can
directly select from the large unlabeled set U.
However, it is shown in (Blum and Mitchell
2008) that a smaller pool can force the classifiers
to select instances that are more representative of
the underlying distribution that generates U.
Hence we set the parameter u to a size of about
1% of the whole unlabeled set, which allows us
to observe the effects of different number of
iterations. It then iterates over the following
steps. First, use character, lexical and syntactic
views on the current labeled set to train three
classifiers C1, C2, and C3. See Steps 4-9. Second,
</bodyText>
<page confidence="0.994982">
346
</page>
<bodyText confidence="0.999982466666667">
allow two of these three classifiers to classify the
unlabeled set U’ and choose p documents with
agreed labels. See Steps 10-12. The selected
documents are then added to the third labeled set
for the label assigned (a label is an author here),
and the u documents are removed from the
unlabeled pool U’ (line 13). We call this way of
augmenting the training sets InterAdding. The
one used in (Kourtis and Stamatatos, 2011) is
called SelfAdding as it uses only a single view
and adds to the same training set. Steps 14-15
assign the test document to a category (author)
using the classifier learned from the three views
in the augmented labeled data, respectively. Step
16 aggregates the results from three classifiers.
</bodyText>
<subsectionHeader confidence="0.997957">
3.2 Character View
</subsectionHeader>
<bodyText confidence="0.999984">
The features in the character view are the
character n-grams of a document. Character n-
grams are simple and easily available for any
natural language. For a fair comparison with the
previous work in (Kourtis and Stamatatos, 2011),
we extract frequencies of 3-grams at the
character-level. The vocabulary size for character
3-grams in our experiment is 28584.
</bodyText>
<subsectionHeader confidence="0.999309">
3.3 Lexical View
</subsectionHeader>
<bodyText confidence="0.9999915">
The lexical view consists of word unigrams of a
document. We represent each article by a vector
of word frequencies. The vocabulary size for
unigrams in our experiment is 195274.
</bodyText>
<subsectionHeader confidence="0.952503">
3.4 Syntactic View
</subsectionHeader>
<bodyText confidence="0.999996090909091">
The syntactic view consists of the syntactic
features of a document. We use four content-
independent structures including n-grams of POS
tags (n = 1..3) and rewrite rules (Kim et al.,
2011). The vocabulary sizes for POS 1-grams,
POS 2-grams, POS 3-grams, and rewrite rules in
our experiment are 63, 1917, 21950, and 19240,
respectively. These four types of syntactic
structures are merged into a single vector. Hence
the syntactic view of a document is represented
as a vector of 43140 components.
</bodyText>
<subsectionHeader confidence="0.997769">
3.5 Aggregating Results from Three Views
</subsectionHeader>
<bodyText confidence="0.999970133333333">
In testing, once we obtain the prediction values
from three classifiers for a test document tk, an
additional algorithm is used to decide the final
author attribution. One simple method is voting.
However, this method is weaker than the three
methods below. It is also hard to compare with
the self-training method CNG+SVM in (Kourtis
and Stamatatos, 2011) as it only has two classifi-
ers. Hence we present three other strategies to
further aggregate the results from the three
views. These methods require the classifier to
produce a numeric score to reflect the positive or
negative certainty. Many classification algo-
rithms give such scores, e.g., SVM and logistic
regression. The three methods are as follows:
</bodyText>
<listItem confidence="0.998232666666667">
1) ScoreSum: The learned model first classifies
all test cases in T. Then for each test case tk,
this method sums up all scores of positive
classifications from the three views. It then
assigns tk to the author with the highest score.
2) ScoreSqSum: This method works similarly to
ScoreSum above except that it sums up the
squared scores of positive classifications.
3) ScoreMax: This method works similarly to the
ScoreSum method as well except that it finds
the maximum classification score for each test
document.
</listItem>
<sectionHeader confidence="0.996283" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9999542">
We now evaluate the proposed method. We use
logistic regression (LR) with L2 regularization
(Fan et al., 2008) and the SVMmulticlass (SVM)
system (Joachims, 2007) with its default settings
as the classifiers.
</bodyText>
<subsectionHeader confidence="0.984113">
4.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999994">
We conduct experiments on the IM )b dataset
(Seroussi et al., 2010). This data set contains the
IM )b reviews in May 2009. It has 62,000 re-
views by 62 users (1,000 reviews per user). For
each author/reviewer, we further split his/her
documents into the labeled, unlabeled, and test
sets. 1% of one author’s documents, i.e., 10 doc-
uments per author, are used as the labeled data
for training, 79% are used as unlabeled data, and
the rest 20% are used for testing. We extract and
compute the character and lexical features direct-
ly from the raw data, and use the Stanford PCFG
parser (Klein and Manning, 2003) to generate the
grammar structures of sentences in each review
for extracting syntactic features. We normalize
each feature’s value to the [0, 1] interval by di-
viding the maximum value of this feature in the
training set. We use the micro-averaged classifi-
cation accuracy as the evaluation metric.
</bodyText>
<sectionHeader confidence="0.805936" genericHeader="method">
4.2 Baseline methods
</sectionHeader>
<bodyText confidence="0.9999708">
We use six self-training baselines and three co-
training baselines. Self-training in (Kourtis and
Stamatatos, 2011) uses two different classifiers
on one view, and co-training uses one classifier
on two views. All baselines except CNG+SVM
</bodyText>
<page confidence="0.996834">
347
</page>
<bodyText confidence="0.998126090909091">
on the character view are our extensions.
Self-training using CNG+SVM on character,
lexical and syntactic views respectively: This
gives three baselines. It self-trains two classifi-
ers from the character 3-gram, lexical, and syn-
tactic views using CNG and SVM classifiers
(Kourtis and Stamatatos, 2011). CNG is a pro-
file-based method which represents the author
as the N most frequent character n-grams of all
his/her training texts. The original method ap-
plied only CNG and SVM on the character n-
gram view. Since our results show that its per-
formance is extremely poor, we are curious
what the reason is. Can this be due to the clas-
sifier or to the view? In order to differentiate
the effects of views and classifiers, we present
two additional types of baselines. The first type
is to extend CNG+SVM method to lexical and
syntactic views as well. The second type is to
extend CNG+SVM method by replacing CNG
with LR to show a fair comparison with our
framework.
Self-training using LR+SVM on character, lexi-
cal, and syntactic views: This is the second
type extension. It also gives us three baselines.
It again uses the character, lexical and syntac-
tic view and SVM as one of the two classifiers.
The other classifier uses LR rather than CNG.
Co-training using LR on Char+Lex, Char+Syn,
and Lex+Syn views: This also gives us three
baselines. Each baseline co-trains two classifi-
ers from every two views of the character 3-
gram, lexical, and syntactic views.
</bodyText>
<subsectionHeader confidence="0.50022">
4.3 Results and analysis
(1) Effects of learning algorithms
</subsectionHeader>
<bodyText confidence="0.996836333333333">
We first evaluate the effects of learning algo-
rithms on tri-training. We use SVM and LR as
the learners as they are among the best methods.
</bodyText>
<figureCaption confidence="0.998182">
Figure 2. Effects of SVM and LR on tri-training
</figureCaption>
<bodyText confidence="0.9997575">
The effects of SVM and LR on tri-training are
shown in Fig. 2. For the aggregation results, we
draw the curves for ScoreSum. The results for
other two stratigies are similar. It is clear that LR
outperforms SVM by a large margin for tri-
training when the number of iterations (k) is
small. One possible reason is that LR is more
tolerant to over-fitting caused by the small
number of training samples. Hence, we use LR
for tri-training in all experiments.
</bodyText>
<listItem confidence="0.828539">
(2) Effects of aggregation strategies
</listItem>
<bodyText confidence="0.995799">
We show the effects of the three proposed
aggregation strategies. Table 1 indicates that
ScoreSum (SS) is the best.
</bodyText>
<table confidence="0.997832222222222">
k Single View Results Aggregated Results
Lex Char Syn SM SS SQ
0 45.75 32.88 33.96 41.11 46.85 44.61
10 74.63 66.05 56.99 73.41 78.82 76.41
20 82.30 74.92 65.05 81.63 86.19 84.05
30 86.86 79.12 68.85 85.29 89.69 87.74
40 89.16 81.81 70.85 87.83 91.52 89.99
50 90.56 83.14 72.06 89.11 92.58 91.17
60 91.69 84.13 73.23 90.05 93.15 91.82
</table>
<tableCaption confidence="0.999866">
Table 1. Effects of three aggregation strategies:
</tableCaption>
<author confidence="0.189484">
ScoreMax(SM), ScoreSum(SS), and ScoreSq-Sum(SQ)
</author>
<bodyText confidence="0.995327166666667">
We also observe that both ScoreSum and
ScoreSqSum (SQ) perform better than ScoreMax
(SM) and all single view cases. This suggests
that the decision made from a number of scores
is much more reliable than that made from only
one score. ScoreSum is our default strategy.
</bodyText>
<listItem confidence="0.794719">
(3) Effects of data augmenting strategies
</listItem>
<bodyText confidence="0.962073">
We now see the effects of data adding methods
to augment the labeled set in Fig. 3.
Figure 3. Effects of data augmenting methods on
tri-training
We use two strategies. One is our InterAdding
approach and the other is the SelfAdding
approach in (Kourtis and Stamatatos, 2011), as
introduced in Section 3.1. We can see that by
adding newly classified samples by two
classifiers to the third view, tri-training gets
better and better results rapidly. For example, the
accuracy for k = 10 iterations grows from 61.24
for SelfAdding to 78.82 for InterAdding, an
absolute increase of 17.58%. This implies that by
integrating more information from other views,
learning can improve greatly.
</bodyText>
<listItem confidence="0.710762">
(4) Comparison with self-training baselines
</listItem>
<bodyText confidence="0.900043">
We show the results of CNG+SVM in Table 2. It
is clear that CNG is almost unable to correctly
</bodyText>
<page confidence="0.996759">
348
</page>
<bodyText confidence="0.9921915">
classify any test case. Its accuracy is only 1.26%
at the start. This directly leads to the failure of
the self-training. The reason is that the other
classifier SVM can augment nearly 0 documents
from the unlabeled set. We also tuned the param-
eter I for CIG, but it makes little difference.
</bodyText>
<table confidence="0.998681444444444">
k Self-Training on Char Aggregated Results
CNG SVM SM SS SQ
0 1.26 33.22 32.35 32.47 27.00
10 1.26 32.35 32.35 32.47 27.00
20 1.26 32.35 32.35 32.47 27.00
30 1.26 32.35 32.35 32.47 27.00
40 1.26 33.60 33.60 33.69 29.07
50 1.26 33.60 33.60 33.69 29.07
60 1.27 33.54 33.60 33.69 29.07
</table>
<tableCaption confidence="0.999477">
Table 2. Results for the CIG+SVM baseline
</tableCaption>
<bodyText confidence="0.999796076923077">
To distinguish the effects of views from classi-
fiers, we conduct two more types of experiments.
First, we apply CIG+SVM to the lexical and
syntactic views. The results are even worse. Its
accuracy drops to 0.58% and 1.21%, respectively.
Next, we replace CIG with LR and apply
LR+SVM to all three views. We only show their
best results in Table 3, either on a single view or
aggregation. The details are omitted due to space
limitations. We can see significant improvements
over their corresponding results of CIG+SVM.
This demonstrates that the learning methods are
critical to self-training as well.
</bodyText>
<table confidence="0.999705333333333">
k Tri SelfTrain:CNG+SVM SelfTrain:LR+SVM
Train Char lex Syn Char Lex Syn
0 46.85 33.22 45.44 34.50 33.22 45.75 34.48
10 78.82 32.47 45.44 34.50 62.56 73.78 51.94
20 86.19 32.47 45.44 34.09 71.21 81.44 59.88
30 89.69 32.47 45.44 34.09 75.21 84.68 63.70
40 91.52 33.69 45.44 34.09 77.46 88.25 65.74
50 92.58 33.69 45.44 34.09 78.64 88.25 67.45
60 93.15 33.69 45.44 34.09 79.54 89.31 68.37
</table>
<tableCaption confidence="0.999761">
Table 3. Self-training variations
</tableCaption>
<bodyText confidence="0.987476482758621">
From Table 3, we can also see that our tri-
training approach outperforms all self-training
baselines by a large margin. For example, the
accuracy for LR+SVM on the lexical view is
89.31%.Although this is the best for self-training,
it is worse than 93.15% of tri-training.
The reason that self-training does not work
well in general is the following: When the train-
ing set is small, the available data may not reflect
the true distribution of the whole data. Then clas-
sifiers will be biased and their classifications will
be biased too. In testing, the biased classifiers
will not have good accuracy. However, in tri-
training, and co-training, each individual view
may be biased but the views are independent.
Then each view is more likely to produce ran-
dom samples for the other views and thus reduce
the bias of each view as the iterations progress.
(5) Comparison with co-training baselines
We now compare tri-training with co-training
(Blum and Mitchell, 1998) in Table 4. Again, tri-
training beats co-training consistently. The best
performance of co-training is 92.81% achieved
on the character and lexical views after 60 itera-
tions. However, the accuracy is worse than that
of tri-training. The key reason is that tri-training
considers three views, while co-training uses on-
ly two. Also, the predictions by two classifiers
are more reliable than those by one classifier.
</bodyText>
<table confidence="0.999501777777778">
k Tri Co-Train
Train Char+Lex Char+Syn Lex+Syn
0 46.85 45.75 42.02 45.75
10 78.82 78.84 75.89 78.85
20 86.19 86.02 82.59 85.63
30 89.69 89.32 85.77 88.98
40 91.52 91.14 87.52 91.16
50 92.58 92.19 88.46 92.02
60 93.15 92.81 89.21 92.50
</table>
<tableCaption confidence="0.999871">
Table 4. Co-training vs. tri-training
</tableCaption>
<bodyText confidence="0.999826384615385">
In (Qian, et al., 2014), we systematically inves-
tigated the effects of learning methods and views
using a special co-training approach with two
views. Learning was applied on two views but
the data augmentation method was like that in
self-training. The best result there was 91.23%,
worse than 92.81% of co-training here in Table 4,
which is worse than 93.15% of Tri-Training.
Overall, Tri-training performs the best and co-
training is better than self-training and co-self-
training. This indicates that learning on different
views can better exploit the redundancy in texts
to achieve superior classification results.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99997">
In this paper, we investigated the problem of au-
thorship attribution with very few labeled exam-
ples. A novel three-view tri-training method was
proposed to utilize natural views of human lan-
guages, i.e., the character, lexical and syntactic
views, for classification. We evaluated the pro-
posed method and compared it with state-of-the-
art baselines. Results showed that the proposed
method outperformed all baseline methods.
Our future work will extend the work by in-
cluding more views such as the stylistic and vo-
cabulary richness views. Additional experiments
will also be conducted to determine the general
behavior of the tri-training approach.
</bodyText>
<sectionHeader confidence="0.996515" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996862333333333">
This work was supported in part by the NSFC
projects (61272275, 61232002, 61379044), and
the 111 project (B07037).
</bodyText>
<page confidence="0.998591">
349
</page>
<sectionHeader confidence="0.990619" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992989413793103">
S. Argamon, C. Whitelaw, P. Chase, S. R. Hota, N.
Garg, and S. Levitan. 2007. Stylistic text
classification using functional lexical features.
JASIST 58, 802–822
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In: COLT. pp.
92–100
J. Burrows. 1992. Not unless you ask nicely: The
interpretative nexus between analysis and
information. Literary and Linguistic Computing
7:91-109.
J. Burrows. 2007. All the way through: Testing for
authorship in different frequency data. LLC 22,
27–47
R-E. Fan, K-W. Chang, C-J. Hsieh, X-R. Wang, and
C-J. Lin. 2008. Liblinear: A library for large linear
classification. JMLR 9, 1871–1874
J. Diederich, J. Kindermann, E. Leopold, G. Paass, G.
F. Informationstechnik, and D-S. Augustin. 2000.
Authorship attribution with support vector
machines. Applied Intelligence 19:109-123.
M. Gamon. 2004. Linguistic correlates of style:
authorship classification with deep linguistic
analysis features. In COLING.
N. Graham, G. Hirst, and B. Marthi. 2005.
Segmenting documents by stylistic character.
Natural Language Engineering, 11:397-415.
J. Grieve. 2007. Quantitative authorship attribution:
An evaluation of techniques. LLC 22:251-270.
H. van Halteren, F. Tweedie, and H. Baayen. 1996.
Outside the cave of shadows: using syntactic
annotation to enhance authorship attribution.
Literary and Linguistic Computing 11:121-132.
H. van Halteren. 2007. Author verification by
linguistic profiling: An exploration of the
parameter space. TSLP 4, 1–17
G. Hirst, and O. Feiguina. 2007. Bigrams of syntactic
labels for authorship discrimination of short texts.
LLC 22, 405–417
N. Jindal and B. Liu. 2008. Opinion spam and
analysis. In: WSDM. pp. 29–230
T. Joachims. 2007. www.cs.cornell.edu/people
/tj/svmlight/old/svmmulticlassv2.12.html
S. Kim, H. Kim, T. Weninger, J. Han, and H. D.
Kim. 2011. Authorship classification: a
discriminative syntactic tree mining approach. In:
SIGIR. pp. 455–464
D. Klein and C. D. Manning. 2003 Accurate
unlexicalized parsing. In: ACL. pp. 423–430
I. Kourtis and E. Stamatatos, 2011. Author
identification using semi-supervised learning. In:
Notebook for PAN at CLEF 2011
J. Li, R. Zheng, and H. Chen. 2006. From fingerprint
to writeprint. Communications of the ACM 49:76-
82.
K. Luyckx and W. Daelemans, 2008. Authorship
attribution and verification with many authors and
limited data. In: COLING. pp. 513–520
D. Madigan, A. Genkin, D. Lewis, A. Argamon, D.
Fradkin, and L. Ye, 2005. Author Identification on
the Large Scale. In CSNA.
K. Nigam and R. Ghani. 2000. Analyzing the
effectiveness and applicability of co-training. In
Proc. of CIKM, pp.86–93
T. Qian, B. Liu. 2013 Identifying Multiple Userids of
the Same Author. EMNLP, pp. 1124-1135
T. Qian, B. Liu, M. Zhong, G. He. 2014. Co-Training
on Authorship Attribution with Very Few Labeled
Examples: Methods. vs. Views. In SIGIR, to
appear.
R. Schwartz, O. Tsur, A. Rappoport, M. Koppel. 2013.
Authorship Attribution of Micro-Messages.
EMNLP. pp. 1880-1891
Y. Seroussi, F. Bohnert and Zukerman,.2012.
Authorship attribution with author-aware topic
models. In: ACL. pp. 264–269
Y. Seroussi, I. Zukerman, and F. Bohnert. 2010.
Collaborative inference of sentiments from texts.
In: UMAP. pp. 195–206
E. Stamatatos. 2007. Author identification using
imbalanced and limited training texts. In: TIR. pp.
237–241
E. Stamatatos. 2009. A survey of modern authorship
attribution methods. JASIST 60:538–556
Ö. Uzuner and B. Katz. 2005. A comparative study of
language models for book and author recognition.
Proc. of the 2nd IJCNLP, 969-980.
</reference>
<page confidence="0.965273">
350
</page>
<reference confidence="0.997652625">
Y. Zhao and J. Zobel. 2005. Effective and scalable
authorship attribution using function words. In
Proc. of Information Retrival Technology, 174-
189.
R. Zheng, J. Li, H. Chen, and Z. Huang. 2006. A
framework for authorship identification of online
messages: Writing style features and classification
techniques. JASIST 57:378-393.
</reference>
<page confidence="0.998671">
351
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.586470">
<title confidence="0.999879">Tri-Training for Authorship Attribution with Limited Training Data</title>
<author confidence="0.84724">Tieyun Qian Bing Liu Li Chen Zhiyong Peng</author>
<affiliation confidence="0.8493575">State Key Laboratory of Software Eng., of Computer State Key Laboratory Computer ence, Univ. of Illinois at Software Eng., Wuhan University</affiliation>
<address confidence="0.9677275">Wuhan University Chicago Wuhan University 430072, Hubei, China 430072, Hubei, China IL, USA, 60607 430072, Hubei, China peng@whu.edu.cn</address>
<email confidence="0.976695">qty@whu.edu.cnliub@cs.uic.educcnuchenli@163.com</email>
<abstract confidence="0.997431208333333">Authorship attribution (AA) aims to identify the authors of a set of documents. Traditional studies in this area often assume that there are a large set of labeled documents available for training. However, in the real life, it is often difficult or expensive to collect a large set of labeled data. For example, in the online review domain, most reviewers (authors) only write a few reviews, which are not enough to serve as the training data for accurate classification. In this paper, we present a novel three-view tritraining method to iteratively identify authors of unlabeled data to augment the training set. The key idea is to first represent each document in three distinct views, and then perform tri-training to exploit the large amount of unlabeled documents. Starting from 10 training documents per author, we systematically evaluate the effectiveness of the proposed tritraining method for AA. Experimental results show that the proposed approach outperforms the state-of-the-art semi-supervised method CNG+SVM and other baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>C Whitelaw</author>
<author>P Chase</author>
<author>S R Hota</author>
<author>N Garg</author>
<author>S Levitan</author>
</authors>
<title>Stylistic text classification using functional lexical features.</title>
<date>2007</date>
<journal>JASIST</journal>
<volume>58</volume>
<pages>802--822</pages>
<contexts>
<context position="5628" citStr="Argamon et al., 2007" startWordPosition="884" endWordPosition="887">than those by one classifier. The main contribution of this paper is thus the proposed three-view tri-training scheme which has a much better generalization ability by exploiting three different views of the same document. Experimental results on the IMDb review dataset show that the proposed method dramatically improves the CNG+SVM method. It also outperforms the co-training method (Blum and Mitchell, 1998) based on our proposed views. 2 Related Work Existing AA methods either focused on finding suitable features or on developing effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et</context>
</contexts>
<marker>Argamon, Whitelaw, Chase, Hota, Garg, Levitan, 2007</marker>
<rawString>S. Argamon, C. Whitelaw, P. Chase, S. R. Hota, N. Garg, and S. Levitan. 2007. Stylistic text classification using functional lexical features. JASIST 58, 802–822</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<pages>92--100</pages>
<location>In: COLT.</location>
<contexts>
<context position="5418" citStr="Blum and Mitchell, 1998" startWordPosition="853" endWordPosition="856">beled by the classifiers of every two views are added to the third view. By doing so, each classifier can borrow information from the other two views. And the predictions made by two classifiers are more reliable than those by one classifier. The main contribution of this paper is thus the proposed three-view tri-training scheme which has a much better generalization ability by exploiting three different views of the same document. Experimental results on the IMDb review dataset show that the proposed method dramatically improves the CNG+SVM method. It also outperforms the co-training method (Blum and Mitchell, 1998) based on our proposed views. 2 Related Work Existing AA methods either focused on finding suitable features or on developing effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (G</context>
<context position="20452" citStr="Blum and Mitchell, 1998" startWordPosition="3402" endWordPosition="3405"> following: When the training set is small, the available data may not reflect the true distribution of the whole data. Then classifiers will be biased and their classifications will be biased too. In testing, the biased classifiers will not have good accuracy. However, in tritraining, and co-training, each individual view may be biased but the views are independent. Then each view is more likely to produce random samples for the other views and thus reduce the bias of each view as the iterations progress. (5) Comparison with co-training baselines We now compare tri-training with co-training (Blum and Mitchell, 1998) in Table 4. Again, tritraining beats co-training consistently. The best performance of co-training is 92.81% achieved on the character and lexical views after 60 iterations. However, the accuracy is worse than that of tri-training. The key reason is that tri-training considers three views, while co-training uses only two. Also, the predictions by two classifiers are more reliable than those by one classifier. k Tri Co-Train Train Char+Lex Char+Syn Lex+Syn 0 46.85 45.75 42.02 45.75 10 78.82 78.84 75.89 78.85 20 86.19 86.02 82.59 85.63 30 89.69 89.32 85.77 88.98 40 91.52 91.14 87.52 91.16 50 92</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In: COLT. pp. 92–100</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Burrows</author>
</authors>
<title>Not unless you ask nicely: The interpretative nexus between analysis and information.</title>
<date>1992</date>
<booktitle>Literary and Linguistic Computing</booktitle>
<pages>7--91</pages>
<contexts>
<context position="5755" citStr="Burrows, 1992" startWordPosition="902" endWordPosition="903"> better generalization ability by exploiting three different views of the same document. Experimental results on the IMDb review dataset show that the proposed method dramatically improves the CNG+SVM method. It also outperforms the co-training method (Blum and Mitchell, 1998) based on our proposed views. 2 Related Work Existing AA methods either focused on finding suitable features or on developing effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,</context>
</contexts>
<marker>Burrows, 1992</marker>
<rawString>J. Burrows. 1992. Not unless you ask nicely: The interpretative nexus between analysis and information. Literary and Linguistic Computing 7:91-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Burrows</author>
</authors>
<title>All the way through: Testing for authorship in different frequency data.</title>
<date>2007</date>
<journal>LLC</journal>
<volume>22</volume>
<pages>27--47</pages>
<marker>Burrows, 2007</marker>
<rawString>J. Burrows. 2007. All the way through: Testing for authorship in different frequency data. LLC 22, 27–47</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>JMLR</journal>
<volume>9</volume>
<pages>1871--1874</pages>
<contexts>
<context position="12675" citStr="Fan et al., 2008" startWordPosition="2092" endWordPosition="2095"> classifies all test cases in T. Then for each test case tk, this method sums up all scores of positive classifications from the three views. It then assigns tk to the author with the highest score. 2) ScoreSqSum: This method works similarly to ScoreSum above except that it sums up the squared scores of positive classifications. 3) ScoreMax: This method works similarly to the ScoreSum method as well except that it finds the maximum classification score for each test document. 4 Experimental Evaluation We now evaluate the proposed method. We use logistic regression (LR) with L2 regularization (Fan et al., 2008) and the SVMmulticlass (SVM) system (Joachims, 2007) with its default settings as the classifiers. 4.1 Experiment Setup We conduct experiments on the IM )b dataset (Seroussi et al., 2010). This data set contains the IM )b reviews in May 2009. It has 62,000 reviews by 62 users (1,000 reviews per user). For each author/reviewer, we further split his/her documents into the labeled, unlabeled, and test sets. 1% of one author’s documents, i.e., 10 documents per author, are used as the labeled data for training, 79% are used as unlabeled data, and the rest 20% are used for testing. We extract and co</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R-E. Fan, K-W. Chang, C-J. Hsieh, X-R. Wang, and C-J. Lin. 2008. Liblinear: A library for large linear classification. JMLR 9, 1871–1874</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Diederich</author>
<author>J Kindermann</author>
<author>E Leopold</author>
<author>G Paass</author>
<author>G F Informationstechnik</author>
<author>D-S Augustin</author>
</authors>
<title>Authorship attribution with support vector machines.</title>
<date>2000</date>
<journal>Applied Intelligence</journal>
<pages>19--109</pages>
<contexts>
<context position="6191" citStr="Diederich et al., 2000" startWordPosition="965" endWordPosition="968">mple features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size (Argamon et al., 2007, Burrows, 2003). When no long documents are available, tens or hundreds of short texts are used (Halteren, 2007; Hirst and Feiguina, 2007; Schwartz et al., 2013). Apart from the existing works dealing with limited data discussed in the introduction, our preliminary study in (Qian et al., 2014) used one learning method on two views, but it is inf</context>
</contexts>
<marker>Diederich, Kindermann, Leopold, Paass, Informationstechnik, Augustin, 2000</marker>
<rawString>J. Diederich, J. Kindermann, E. Leopold, G. Paass, G. F. Informationstechnik, and D-S. Augustin. 2000. Authorship attribution with support vector machines. Applied Intelligence 19:109-123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
</authors>
<title>Linguistic correlates of style: authorship classification with deep linguistic analysis features.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="5660" citStr="Gamon 2004" startWordPosition="890" endWordPosition="891">tribution of this paper is thus the proposed three-view tri-training scheme which has a much better generalization ability by exploiting three different views of the same document. Experimental results on the IMDb review dataset show that the proposed method dramatically improves the CNG+SVM method. It also outperforms the co-training method (Blum and Mitchell, 1998) based on our proposed views. 2 Related Work Existing AA methods either focused on finding suitable features or on developing effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main probl</context>
</contexts>
<marker>Gamon, 2004</marker>
<rawString>M. Gamon. 2004. Linguistic correlates of style: authorship classification with deep linguistic analysis features. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Graham</author>
<author>G Hirst</author>
<author>B Marthi</author>
</authors>
<title>Segmenting documents by stylistic character.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<pages>11--397</pages>
<contexts>
<context position="5707" citStr="Graham et al., 2005" startWordPosition="894" endWordPosition="897">oposed three-view tri-training scheme which has a much better generalization ability by exploiting three different views of the same document. Experimental results on the IMDb review dataset show that the proposed method dramatically improves the CNG+SVM method. It also outperforms the co-training method (Blum and Mitchell, 1998) based on our proposed views. 2 Related Work Existing AA methods either focused on finding suitable features or on developing effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealist</context>
</contexts>
<marker>Graham, Hirst, Marthi, 2005</marker>
<rawString>N. Graham, G. Hirst, and B. Marthi. 2005. Segmenting documents by stylistic character. Natural Language Engineering, 11:397-415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Grieve</author>
</authors>
<title>Quantitative authorship attribution: An evaluation of techniques.</title>
<date>2007</date>
<journal>LLC</journal>
<pages>22--251</pages>
<contexts>
<context position="5733" citStr="Grieve, 2007" startWordPosition="899" endWordPosition="900">heme which has a much better generalization ability by exploiting three different views of the same document. Experimental results on the IMDb review dataset show that the proposed method dramatically improves the CNG+SVM method. It also outperforms the co-training method (Blum and Mitchell, 1998) based on our proposed views. 2 Related Work Existing AA methods either focused on finding suitable features or on developing effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training se</context>
</contexts>
<marker>Grieve, 2007</marker>
<rawString>J. Grieve. 2007. Quantitative authorship attribution: An evaluation of techniques. LLC 22:251-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>F Tweedie</author>
<author>H Baayen</author>
</authors>
<title>Outside the cave of shadows: using syntactic annotation to enhance authorship attribution. Literary and Linguistic Computing</title>
<date>1996</date>
<pages>11--121</pages>
<marker>van Halteren, Tweedie, Baayen, 1996</marker>
<rawString>H. van Halteren, F. Tweedie, and H. Baayen. 1996. Outside the cave of shadows: using syntactic annotation to enhance authorship attribution. Literary and Linguistic Computing 11:121-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
</authors>
<title>Author verification by linguistic profiling: An exploration of the parameter space.</title>
<date>2007</date>
<journal>TSLP</journal>
<volume>4</volume>
<pages>1--17</pages>
<marker>van Halteren, 2007</marker>
<rawString>H. van Halteren. 2007. Author verification by linguistic profiling: An exploration of the parameter space. TSLP 4, 1–17</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>O Feiguina</author>
</authors>
<title>Bigrams of syntactic labels for authorship discrimination of short texts.</title>
<date>2007</date>
<journal>LLC</journal>
<volume>22</volume>
<pages>405--417</pages>
<contexts>
<context position="5811" citStr="Hirst and Feiguina, 2007" startWordPosition="909" endWordPosition="912">three different views of the same document. Experimental results on the IMDb review dataset show that the proposed method dramatically improves the CNG+SVM method. It also outperforms the co-training method (Blum and Mitchell, 1998) based on our proposed views. 2 Related Work Existing AA methods either focused on finding suitable features or on developing effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable trainin</context>
</contexts>
<marker>Hirst, Feiguina, 2007</marker>
<rawString>G. Hirst, and O. Feiguina. 2007. Bigrams of syntactic labels for authorship discrimination of short texts. LLC 22, 405–417</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Jindal</author>
<author>B Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In: WSDM.</booktitle>
<pages>29--230</pages>
<contexts>
<context position="2212" citStr="Jindal and Liu, 2008" startWordPosition="346" endWordPosition="349">sed on supervised classification (Stamatatos, 2009, Kim et al., 2011, Seroussi et al., 2012). Although this is an effective approach, it has a major weakness, i.e., for each author a large number of his/her articles are needed as the training data. This is possible if the author has written a large number of articles, but will be difficult if he/she has not. For example, in the online review domain, most authors (reviewers) only write a few reviews (documents). It was shown that on average each reviewer only has 2.72 reviews in amazon.com, and only 8% of the reviewers have at least 5 reviews (Jindal and Liu, 2008). The small number of labeled documents makes it extremely challenging for supervised learning to train an accurate classifier. In this paper, we consider AA with only a few labeled examples. By exploiting the redundancy in human languages, we tackle the problem using a new three-view tri-training algorithm (TTA). Specifically, we first represent each document in three distinct views, and then tri-train three classifiers in these views. The predictions of two classifiers on unlabeled examples are used to augment the training set for the third classifier. This process repeats until a terminatio</context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>N. Jindal and B. Liu. 2008. Opinion spam and analysis. In: WSDM. pp. 29–230</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<date>2007</date>
<note>www.cs.cornell.edu/people /tj/svmlight/old/svmmulticlassv2.12.html</note>
<contexts>
<context position="12727" citStr="Joachims, 2007" startWordPosition="2101" endWordPosition="2102">se tk, this method sums up all scores of positive classifications from the three views. It then assigns tk to the author with the highest score. 2) ScoreSqSum: This method works similarly to ScoreSum above except that it sums up the squared scores of positive classifications. 3) ScoreMax: This method works similarly to the ScoreSum method as well except that it finds the maximum classification score for each test document. 4 Experimental Evaluation We now evaluate the proposed method. We use logistic regression (LR) with L2 regularization (Fan et al., 2008) and the SVMmulticlass (SVM) system (Joachims, 2007) with its default settings as the classifiers. 4.1 Experiment Setup We conduct experiments on the IM )b dataset (Seroussi et al., 2010). This data set contains the IM )b reviews in May 2009. It has 62,000 reviews by 62 users (1,000 reviews per user). For each author/reviewer, we further split his/her documents into the labeled, unlabeled, and test sets. 1% of one author’s documents, i.e., 10 documents per author, are used as the labeled data for training, 79% are used as unlabeled data, and the rest 20% are used for testing. We extract and compute the character and lexical features directly fr</context>
</contexts>
<marker>Joachims, 2007</marker>
<rawString>T. Joachims. 2007. www.cs.cornell.edu/people /tj/svmlight/old/svmmulticlassv2.12.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>H Kim</author>
<author>T Weninger</author>
<author>J Han</author>
<author>H D Kim</author>
</authors>
<title>Authorship classification: a discriminative syntactic tree mining approach. In: SIGIR.</title>
<date>2011</date>
<pages>455--464</pages>
<contexts>
<context position="1659" citStr="Kim et al., 2011" startWordPosition="246" endWordPosition="249">eled data to augment the training set. The key idea is to first represent each document in three distinct views, and then perform tri-training to exploit the large amount of unlabeled documents. Starting from 10 training documents per author, we systematically evaluate the effectiveness of the proposed tritraining method for AA. Experimental results show that the proposed approach outperforms the state-of-the-art semi-supervised method CNG+SVM and other baselines. 1 Introduction Existing approaches to authorship attribution (AA) are mainly based on supervised classification (Stamatatos, 2009, Kim et al., 2011, Seroussi et al., 2012). Although this is an effective approach, it has a major weakness, i.e., for each author a large number of his/her articles are needed as the training data. This is possible if the author has written a large number of articles, but will be difficult if he/she has not. For example, in the online review domain, most authors (reviewers) only write a few reviews (documents). It was shown that on average each reviewer only has 2.72 reviews in amazon.com, and only 8% of the reviewers have at least 5 reviews (Jindal and Liu, 2008). The small number of labeled documents makes i</context>
<context position="6239" citStr="Kim et al., 2011" startWordPosition="975" endWordPosition="978"> 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size (Argamon et al., 2007, Burrows, 2003). When no long documents are available, tens or hundreds of short texts are used (Halteren, 2007; Hirst and Feiguina, 2007; Schwartz et al., 2013). Apart from the existing works dealing with limited data discussed in the introduction, our preliminary study in (Qian et al., 2014) used one learning method on two views, but it is inferior to the proposed method in this paper. Inpu</context>
<context position="10959" citStr="Kim et al., 2011" startWordPosition="1814" endWordPosition="1817">uage. For a fair comparison with the previous work in (Kourtis and Stamatatos, 2011), we extract frequencies of 3-grams at the character-level. The vocabulary size for character 3-grams in our experiment is 28584. 3.3 Lexical View The lexical view consists of word unigrams of a document. We represent each article by a vector of word frequencies. The vocabulary size for unigrams in our experiment is 195274. 3.4 Syntactic View The syntactic view consists of the syntactic features of a document. We use four contentindependent structures including n-grams of POS tags (n = 1..3) and rewrite rules (Kim et al., 2011). The vocabulary sizes for POS 1-grams, POS 2-grams, POS 3-grams, and rewrite rules in our experiment are 63, 1917, 21950, and 19240, respectively. These four types of syntactic structures are merged into a single vector. Hence the syntactic view of a document is represented as a vector of 43140 components. 3.5 Aggregating Results from Three Views In testing, once we obtain the prediction values from three classifiers for a test document tk, an additional algorithm is used to decide the final author attribution. One simple method is voting. However, this method is weaker than the three methods</context>
</contexts>
<marker>Kim, Kim, Weninger, Han, Kim, 2011</marker>
<rawString>S. Kim, H. Kim, T. Weninger, J. Han, and H. D. Kim. 2011. Authorship classification: a discriminative syntactic tree mining approach. In: SIGIR. pp. 455–464</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<date>2003</date>
<booktitle>Accurate unlexicalized parsing. In: ACL.</booktitle>
<pages>423--430</pages>
<contexts>
<context position="13402" citStr="Klein and Manning, 2003" startWordPosition="2217" endWordPosition="2220">Experiment Setup We conduct experiments on the IM )b dataset (Seroussi et al., 2010). This data set contains the IM )b reviews in May 2009. It has 62,000 reviews by 62 users (1,000 reviews per user). For each author/reviewer, we further split his/her documents into the labeled, unlabeled, and test sets. 1% of one author’s documents, i.e., 10 documents per author, are used as the labeled data for training, 79% are used as unlabeled data, and the rest 20% are used for testing. We extract and compute the character and lexical features directly from the raw data, and use the Stanford PCFG parser (Klein and Manning, 2003) to generate the grammar structures of sentences in each review for extracting syntactic features. We normalize each feature’s value to the [0, 1] interval by dividing the maximum value of this feature in the training set. We use the micro-averaged classification accuracy as the evaluation metric. 4.2 Baseline methods We use six self-training baselines and three cotraining baselines. Self-training in (Kourtis and Stamatatos, 2011) uses two different classifiers on one view, and co-training uses one classifier on two views. All baselines except CNG+SVM 347 on the character view are our extensio</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003 Accurate unlexicalized parsing. In: ACL. pp. 423–430</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kourtis</author>
<author>E Stamatatos</author>
</authors>
<title>Author identification using semi-supervised learning. In: Notebook for PAN at CLEF</title>
<date>2011</date>
<contexts>
<context position="3251" citStr="Kourtis and Stamatatos (2011)" startWordPosition="507" endWordPosition="511">ee classifiers in these views. The predictions of two classifiers on unlabeled examples are used to augment the training set for the third classifier. This process repeats until a termination condition is met. The enlarged labeled sets are finally used to train classifiers to classify the test data. To our knowledge, no existing work has addressed AA in a tri-training framework. The AA problem with limited training data was attempted in (Stamatatos, 2007; Luyckx and Daelemans, 2008). However, neither of them used a semisupervised approach to augment the training set with additional documents. Kourtis and Stamatatos (2011) introduced a variant of the selftraining method in (Nigam and Ghani, 2000). Note that the original self-training uses one classifier on one view. However, the self-training method in (Kourtis and Stamatatos, 2011) uses two classifiers (CNG and SVM) on one view. Both the self-training and tri-training are semisupervised learning methods. However, the proposed approach is not a simple extension of the self-training method CNG+SVM of (Kourtis and Stamatatos, 2011). There are key differences. First, in their experimental setting, about 115 and 129 documents per author on average are used for two </context>
<context position="9881" citStr="Kourtis and Stamatatos, 2011" startWordPosition="1637" endWordPosition="1640"> of iterations. It then iterates over the following steps. First, use character, lexical and syntactic views on the current labeled set to train three classifiers C1, C2, and C3. See Steps 4-9. Second, 346 allow two of these three classifiers to classify the unlabeled set U’ and choose p documents with agreed labels. See Steps 10-12. The selected documents are then added to the third labeled set for the label assigned (a label is an author here), and the u documents are removed from the unlabeled pool U’ (line 13). We call this way of augmenting the training sets InterAdding. The one used in (Kourtis and Stamatatos, 2011) is called SelfAdding as it uses only a single view and adds to the same training set. Steps 14-15 assign the test document to a category (author) using the classifier learned from the three views in the augmented labeled data, respectively. Step 16 aggregates the results from three classifiers. 3.2 Character View The features in the character view are the character n-grams of a document. Character ngrams are simple and easily available for any natural language. For a fair comparison with the previous work in (Kourtis and Stamatatos, 2011), we extract frequencies of 3-grams at the character-le</context>
<context position="11665" citStr="Kourtis and Stamatatos, 2011" startWordPosition="1928" endWordPosition="1931"> rules in our experiment are 63, 1917, 21950, and 19240, respectively. These four types of syntactic structures are merged into a single vector. Hence the syntactic view of a document is represented as a vector of 43140 components. 3.5 Aggregating Results from Three Views In testing, once we obtain the prediction values from three classifiers for a test document tk, an additional algorithm is used to decide the final author attribution. One simple method is voting. However, this method is weaker than the three methods below. It is also hard to compare with the self-training method CNG+SVM in (Kourtis and Stamatatos, 2011) as it only has two classifiers. Hence we present three other strategies to further aggregate the results from the three views. These methods require the classifier to produce a numeric score to reflect the positive or negative certainty. Many classification algorithms give such scores, e.g., SVM and logistic regression. The three methods are as follows: 1) ScoreSum: The learned model first classifies all test cases in T. Then for each test case tk, this method sums up all scores of positive classifications from the three views. It then assigns tk to the author with the highest score. 2) Score</context>
<context position="13836" citStr="Kourtis and Stamatatos, 2011" startWordPosition="2284" endWordPosition="2287">beled data, and the rest 20% are used for testing. We extract and compute the character and lexical features directly from the raw data, and use the Stanford PCFG parser (Klein and Manning, 2003) to generate the grammar structures of sentences in each review for extracting syntactic features. We normalize each feature’s value to the [0, 1] interval by dividing the maximum value of this feature in the training set. We use the micro-averaged classification accuracy as the evaluation metric. 4.2 Baseline methods We use six self-training baselines and three cotraining baselines. Self-training in (Kourtis and Stamatatos, 2011) uses two different classifiers on one view, and co-training uses one classifier on two views. All baselines except CNG+SVM 347 on the character view are our extensions. Self-training using CNG+SVM on character, lexical and syntactic views respectively: This gives three baselines. It self-trains two classifiers from the character 3-gram, lexical, and syntactic views using CNG and SVM classifiers (Kourtis and Stamatatos, 2011). CNG is a profile-based method which represents the author as the N most frequent character n-grams of all his/her training texts. The original method applied only CNG an</context>
<context position="17299" citStr="Kourtis and Stamatatos, 2011" startWordPosition="2870" endWordPosition="2873">eMax(SM), ScoreSum(SS), and ScoreSq-Sum(SQ) We also observe that both ScoreSum and ScoreSqSum (SQ) perform better than ScoreMax (SM) and all single view cases. This suggests that the decision made from a number of scores is much more reliable than that made from only one score. ScoreSum is our default strategy. (3) Effects of data augmenting strategies We now see the effects of data adding methods to augment the labeled set in Fig. 3. Figure 3. Effects of data augmenting methods on tri-training We use two strategies. One is our InterAdding approach and the other is the SelfAdding approach in (Kourtis and Stamatatos, 2011), as introduced in Section 3.1. We can see that by adding newly classified samples by two classifiers to the third view, tri-training gets better and better results rapidly. For example, the accuracy for k = 10 iterations grows from 61.24 for SelfAdding to 78.82 for InterAdding, an absolute increase of 17.58%. This implies that by integrating more information from other views, learning can improve greatly. (4) Comparison with self-training baselines We show the results of CNG+SVM in Table 2. It is clear that CNG is almost unable to correctly 348 classify any test case. Its accuracy is only 1.2</context>
</contexts>
<marker>Kourtis, Stamatatos, 2011</marker>
<rawString>I. Kourtis and E. Stamatatos, 2011. Author identification using semi-supervised learning. In: Notebook for PAN at CLEF 2011</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>R Zheng</author>
<author>H Chen</author>
</authors>
<title>From fingerprint to writeprint.</title>
<date>2006</date>
<journal>Communications of the ACM</journal>
<pages>49--76</pages>
<contexts>
<context position="6220" citStr="Li et al., 2006" startWordPosition="971" endWordPosition="974"> (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size (Argamon et al., 2007, Burrows, 2003). When no long documents are available, tens or hundreds of short texts are used (Halteren, 2007; Hirst and Feiguina, 2007; Schwartz et al., 2013). Apart from the existing works dealing with limited data discussed in the introduction, our preliminary study in (Qian et al., 2014) used one learning method on two views, but it is inferior to the proposed method </context>
</contexts>
<marker>Li, Zheng, Chen, 2006</marker>
<rawString>J. Li, R. Zheng, and H. Chen. 2006. From fingerprint to writeprint. Communications of the ACM 49:76-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Luyckx</author>
<author>W Daelemans</author>
</authors>
<title>Authorship attribution and verification with many authors and limited data. In: COLING.</title>
<date>2008</date>
<pages>513--520</pages>
<contexts>
<context position="3109" citStr="Luyckx and Daelemans, 2008" startWordPosition="486" endWordPosition="489"> new three-view tri-training algorithm (TTA). Specifically, we first represent each document in three distinct views, and then tri-train three classifiers in these views. The predictions of two classifiers on unlabeled examples are used to augment the training set for the third classifier. This process repeats until a termination condition is met. The enlarged labeled sets are finally used to train classifiers to classify the test data. To our knowledge, no existing work has addressed AA in a tri-training framework. The AA problem with limited training data was attempted in (Stamatatos, 2007; Luyckx and Daelemans, 2008). However, neither of them used a semisupervised approach to augment the training set with additional documents. Kourtis and Stamatatos (2011) introduced a variant of the selftraining method in (Nigam and Ghani, 2000). Note that the original self-training uses one classifier on one view. However, the self-training method in (Kourtis and Stamatatos, 2011) uses two classifiers (CNG and SVM) on one view. Both the self-training and tri-training are semisupervised learning methods. However, the proposed approach is not a simple extension of the self-training method CNG+SVM of (Kourtis and Stamatato</context>
</contexts>
<marker>Luyckx, Daelemans, 2008</marker>
<rawString>K. Luyckx and W. Daelemans, 2008. Authorship attribution and verification with many authors and limited data. In: COLING. pp. 513–520</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Madigan</author>
<author>A Genkin</author>
<author>D Lewis</author>
<author>A Argamon</author>
<author>D Fradkin</author>
<author>L Ye</author>
</authors>
<title>Author Identification on the Large Scale. In CSNA.</title>
<date>2005</date>
<contexts>
<context position="6162" citStr="Madigan et al., 2005" startWordPosition="960" endWordPosition="963">ng effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size (Argamon et al., 2007, Burrows, 2003). When no long documents are available, tens or hundreds of short texts are used (Halteren, 2007; Hirst and Feiguina, 2007; Schwartz et al., 2013). Apart from the existing works dealing with limited data discussed in the introduction, our preliminary study in (Qian et al., 2014) used one learning metho</context>
</contexts>
<marker>Madigan, Genkin, Lewis, Argamon, Fradkin, Ye, 2005</marker>
<rawString>D. Madigan, A. Genkin, D. Lewis, A. Argamon, D. Fradkin, and L. Ye, 2005. Author Identification on the Large Scale. In CSNA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>R Ghani</author>
</authors>
<title>Analyzing the effectiveness and applicability of co-training.</title>
<date>2000</date>
<booktitle>In Proc. of CIKM,</booktitle>
<pages>86--93</pages>
<contexts>
<context position="3326" citStr="Nigam and Ghani, 2000" startWordPosition="521" endWordPosition="524">mples are used to augment the training set for the third classifier. This process repeats until a termination condition is met. The enlarged labeled sets are finally used to train classifiers to classify the test data. To our knowledge, no existing work has addressed AA in a tri-training framework. The AA problem with limited training data was attempted in (Stamatatos, 2007; Luyckx and Daelemans, 2008). However, neither of them used a semisupervised approach to augment the training set with additional documents. Kourtis and Stamatatos (2011) introduced a variant of the selftraining method in (Nigam and Ghani, 2000). Note that the original self-training uses one classifier on one view. However, the self-training method in (Kourtis and Stamatatos, 2011) uses two classifiers (CNG and SVM) on one view. Both the self-training and tri-training are semisupervised learning methods. However, the proposed approach is not a simple extension of the self-training method CNG+SVM of (Kourtis and Stamatatos, 2011). There are key differences. First, in their experimental setting, about 115 and 129 documents per author on average are used for two experimental corpora. This number of labeled documents is still very large.</context>
</contexts>
<marker>Nigam, Ghani, 2000</marker>
<rawString>K. Nigam and R. Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In Proc. of CIKM, pp.86–93</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Qian</author>
<author>B Liu</author>
</authors>
<date>2013</date>
<booktitle>Identifying Multiple Userids of the Same Author. EMNLP,</booktitle>
<pages>1124--1135</pages>
<contexts>
<context position="5889" citStr="Qian and Liu, 2013" startWordPosition="921" endWordPosition="924">dataset show that the proposed method dramatically improves the CNG+SVM method. It also outperforms the co-training method (Blum and Mitchell, 1998) based on our proposed views. 2 Related Work Existing AA methods either focused on finding suitable features or on developing effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size (Argamon et al., 2007, Burrows, 2003). When no long documents are a</context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>T. Qian, B. Liu. 2013 Identifying Multiple Userids of the Same Author. EMNLP, pp. 1124-1135</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Qian</author>
<author>B Liu</author>
<author>M Zhong</author>
<author>G He</author>
</authors>
<title>Co-Training on Authorship Attribution with Very Few Labeled Examples: Methods. vs. Views. In SIGIR,</title>
<date>2014</date>
<note>to appear.</note>
<contexts>
<context position="6738" citStr="Qian et al., 2014" startWordPosition="1058" endWordPosition="1061">logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size (Argamon et al., 2007, Burrows, 2003). When no long documents are available, tens or hundreds of short texts are used (Halteren, 2007; Hirst and Feiguina, 2007; Schwartz et al., 2013). Apart from the existing works dealing with limited data discussed in the introduction, our preliminary study in (Qian et al., 2014) used one learning method on two views, but it is inferior to the proposed method in this paper. Input: A small set of labeled documents L = {l1,..., lr}, a large set of unlabeled documents U = {u1,..., us}, and a set of test documents T = {t1,..., tt}, Parameters: the number of iterations k, the size of selected unlabeled documents u Output: tk’s class assignment 1 Extract views Lc, Ll, Ls, Uc, Ul, Us, Tc, Tl, Ts from L, U, T 2 Loop for k iterations: 3 Randomly select u unlabeled documents U&apos; from U; 4 Learn the first view classifier C1 from L1 (L1=Lc, Ll, or Ls); 5 Use C1 to label docs in U&apos;</context>
<context position="21162" citStr="Qian, et al., 2014" startWordPosition="3519" endWordPosition="3522">training is 92.81% achieved on the character and lexical views after 60 iterations. However, the accuracy is worse than that of tri-training. The key reason is that tri-training considers three views, while co-training uses only two. Also, the predictions by two classifiers are more reliable than those by one classifier. k Tri Co-Train Train Char+Lex Char+Syn Lex+Syn 0 46.85 45.75 42.02 45.75 10 78.82 78.84 75.89 78.85 20 86.19 86.02 82.59 85.63 30 89.69 89.32 85.77 88.98 40 91.52 91.14 87.52 91.16 50 92.58 92.19 88.46 92.02 60 93.15 92.81 89.21 92.50 Table 4. Co-training vs. tri-training In (Qian, et al., 2014), we systematically investigated the effects of learning methods and views using a special co-training approach with two views. Learning was applied on two views but the data augmentation method was like that in self-training. The best result there was 91.23%, worse than 92.81% of co-training here in Table 4, which is worse than 93.15% of Tri-Training. Overall, Tri-training performs the best and cotraining is better than self-training and co-selftraining. This indicates that learning on different views can better exploit the redundancy in texts to achieve superior classification results. 5 Con</context>
</contexts>
<marker>Qian, Liu, Zhong, He, 2014</marker>
<rawString>T. Qian, B. Liu, M. Zhong, G. He. 2014. Co-Training on Authorship Attribution with Very Few Labeled Examples: Methods. vs. Views. In SIGIR, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schwartz</author>
<author>O Tsur</author>
<author>A Rappoport</author>
<author>M Koppel</author>
</authors>
<date>2013</date>
<journal>Authorship Attribution of Micro-Messages. EMNLP.</journal>
<pages>1880--1891</pages>
<contexts>
<context position="6605" citStr="Schwartz et al., 2013" startWordPosition="1037" endWordPosition="1040"> approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size (Argamon et al., 2007, Burrows, 2003). When no long documents are available, tens or hundreds of short texts are used (Halteren, 2007; Hirst and Feiguina, 2007; Schwartz et al., 2013). Apart from the existing works dealing with limited data discussed in the introduction, our preliminary study in (Qian et al., 2014) used one learning method on two views, but it is inferior to the proposed method in this paper. Input: A small set of labeled documents L = {l1,..., lr}, a large set of unlabeled documents U = {u1,..., us}, and a set of test documents T = {t1,..., tt}, Parameters: the number of iterations k, the size of selected unlabeled documents u Output: tk’s class assignment 1 Extract views Lc, Ll, Ls, Uc, Ul, Us, Tc, Tl, Ts from L, U, T 2 Loop for k iterations: 3 Randomly </context>
</contexts>
<marker>Schwartz, Tsur, Rappoport, Koppel, 2013</marker>
<rawString>R. Schwartz, O. Tsur, A. Rappoport, M. Koppel. 2013. Authorship Attribution of Micro-Messages. EMNLP. pp. 1880-1891</rawString>
</citation>
<citation valid="false">
<authors>
<author>Y Seroussi</author>
<author>F</author>
</authors>
<title>Bohnert and Zukerman,.2012. Authorship attribution with author-aware topic models. In: ACL.</title>
<pages>264--269</pages>
<marker>Seroussi, F, </marker>
<rawString>Y. Seroussi, F. Bohnert and Zukerman,.2012. Authorship attribution with author-aware topic models. In: ACL. pp. 264–269</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Seroussi</author>
<author>I Zukerman</author>
<author>F Bohnert</author>
</authors>
<title>Collaborative inference of sentiments from texts. In: UMAP.</title>
<date>2010</date>
<pages>195--206</pages>
<contexts>
<context position="12862" citStr="Seroussi et al., 2010" startWordPosition="2121" endWordPosition="2124">e highest score. 2) ScoreSqSum: This method works similarly to ScoreSum above except that it sums up the squared scores of positive classifications. 3) ScoreMax: This method works similarly to the ScoreSum method as well except that it finds the maximum classification score for each test document. 4 Experimental Evaluation We now evaluate the proposed method. We use logistic regression (LR) with L2 regularization (Fan et al., 2008) and the SVMmulticlass (SVM) system (Joachims, 2007) with its default settings as the classifiers. 4.1 Experiment Setup We conduct experiments on the IM )b dataset (Seroussi et al., 2010). This data set contains the IM )b reviews in May 2009. It has 62,000 reviews by 62 users (1,000 reviews per user). For each author/reviewer, we further split his/her documents into the labeled, unlabeled, and test sets. 1% of one author’s documents, i.e., 10 documents per author, are used as the labeled data for training, 79% are used as unlabeled data, and the rest 20% are used for testing. We extract and compute the character and lexical features directly from the raw data, and use the Stanford PCFG parser (Klein and Manning, 2003) to generate the grammar structures of sentences in each rev</context>
</contexts>
<marker>Seroussi, Zukerman, Bohnert, 2010</marker>
<rawString>Y. Seroussi, I. Zukerman, and F. Bohnert. 2010. Collaborative inference of sentiments from texts. In: UMAP. pp. 195–206</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stamatatos</author>
</authors>
<title>Author identification using imbalanced and limited training texts. In: TIR.</title>
<date>2007</date>
<pages>237--241</pages>
<contexts>
<context position="3080" citStr="Stamatatos, 2007" startWordPosition="484" endWordPosition="485">he problem using a new three-view tri-training algorithm (TTA). Specifically, we first represent each document in three distinct views, and then tri-train three classifiers in these views. The predictions of two classifiers on unlabeled examples are used to augment the training set for the third classifier. This process repeats until a termination condition is met. The enlarged labeled sets are finally used to train classifiers to classify the test data. To our knowledge, no existing work has addressed AA in a tri-training framework. The AA problem with limited training data was attempted in (Stamatatos, 2007; Luyckx and Daelemans, 2008). However, neither of them used a semisupervised approach to augment the training set with additional documents. Kourtis and Stamatatos (2011) introduced a variant of the selftraining method in (Nigam and Ghani, 2000). Note that the original self-training uses one classifier on one view. However, the self-training method in (Kourtis and Stamatatos, 2011) uses two classifiers (CNG and SVM) on one view. Both the self-training and tri-training are semisupervised learning methods. However, the proposed approach is not a simple extension of the self-training method CNG+</context>
</contexts>
<marker>Stamatatos, 2007</marker>
<rawString>E. Stamatatos. 2007. Author identification using imbalanced and limited training texts. In: TIR. pp. 237–241</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stamatatos</author>
</authors>
<title>A survey of modern authorship attribution methods.</title>
<date>2009</date>
<journal>JASIST</journal>
<pages>60--538</pages>
<contexts>
<context position="1641" citStr="Stamatatos, 2009" startWordPosition="244" endWordPosition="245">y authors of unlabeled data to augment the training set. The key idea is to first represent each document in three distinct views, and then perform tri-training to exploit the large amount of unlabeled documents. Starting from 10 training documents per author, we systematically evaluate the effectiveness of the proposed tritraining method for AA. Experimental results show that the proposed approach outperforms the state-of-the-art semi-supervised method CNG+SVM and other baselines. 1 Introduction Existing approaches to authorship attribution (AA) are mainly based on supervised classification (Stamatatos, 2009, Kim et al., 2011, Seroussi et al., 2012). Although this is an effective approach, it has a major weakness, i.e., for each author a large number of his/her articles are needed as the training data. This is possible if the author has written a large number of articles, but will be difficult if he/she has not. For example, in the online review domain, most authors (reviewers) only write a few reviews (documents). It was shown that on average each reviewer only has 2.72 reviews in amazon.com, and only 8% of the reviewers have at least 5 reviews (Jindal and Liu, 2008). The small number of labeled</context>
</contexts>
<marker>Stamatatos, 2009</marker>
<rawString>E. Stamatatos. 2009. A survey of modern authorship attribution methods. JASIST 60:538–556</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ö Uzuner</author>
<author>B Katz</author>
</authors>
<title>A comparative study of language models for book and author recognition.</title>
<date>2005</date>
<booktitle>Proc. of the 2nd IJCNLP,</booktitle>
<pages>969--980</pages>
<contexts>
<context position="6095" citStr="Uzuner and Katz, 2005" startWordPosition="950" endWordPosition="953"> methods either focused on finding suitable features or on developing effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size (Argamon et al., 2007, Burrows, 2003). When no long documents are available, tens or hundreds of short texts are used (Halteren, 2007; Hirst and Feiguina, 2007; Schwartz et al., 2013). Apart from the existing works dealing with limited data discussed in the introduction, o</context>
</contexts>
<marker>Uzuner, Katz, 2005</marker>
<rawString>Ö. Uzuner and B. Katz. 2005. A comparative study of language models for book and author recognition. Proc. of the 2nd IJCNLP, 969-980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhao</author>
<author>J Zobel</author>
</authors>
<title>Effective and scalable authorship attribution using function words.</title>
<date>2005</date>
<booktitle>In Proc. of Information Retrival Technology,</booktitle>
<pages>174--189</pages>
<contexts>
<context position="6118" citStr="Zhao and Zobel, 2005" startWordPosition="954" endWordPosition="957"> on finding suitable features or on developing effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size (Argamon et al., 2007, Burrows, 2003). When no long documents are available, tens or hundreds of short texts are used (Halteren, 2007; Hirst and Feiguina, 2007; Schwartz et al., 2013). Apart from the existing works dealing with limited data discussed in the introduction, our preliminary study in</context>
</contexts>
<marker>Zhao, Zobel, 2005</marker>
<rawString>Y. Zhao and J. Zobel. 2005. Effective and scalable authorship attribution using function words. In Proc. of Information Retrival Technology, 174-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zheng</author>
<author>J Li</author>
<author>H Chen</author>
<author>Z Huang</author>
</authors>
<title>A framework for authorship identification of online messages: Writing style features and classification techniques.</title>
<date>2006</date>
<journal>JASIST</journal>
<pages>57--378</pages>
<contexts>
<context position="6057" citStr="Zheng et al., 2006" startWordPosition="944" endWordPosition="947">ed views. 2 Related Work Existing AA methods either focused on finding suitable features or on developing effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size (Argamon et al., 2007, Burrows, 2003). When no long documents are available, tens or hundreds of short texts are used (Halteren, 2007; Hirst and Feiguina, 2007; Schwartz et al., 2013). Apart from the existing works dealing with limited</context>
</contexts>
<marker>Zheng, Li, Chen, Huang, 2006</marker>
<rawString>R. Zheng, J. Li, H. Chen, and Z. Huang. 2006. A framework for authorship identification of online messages: Writing style features and classification techniques. JASIST 57:378-393.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>