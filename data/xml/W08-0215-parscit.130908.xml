<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000674">
<title confidence="0.9932865">
Psychocomputational Linguistics:
A Gateway to the Computational Linguistics Curriculum
</title>
<author confidence="0.98841">
William Gregory Sakas
</author>
<affiliation confidence="0.984673333333333">
Department of Computer Science, Hunter College
Ph.D. Programs in Linguistics and Computer Science, The Graduate Center
City University of New York (CUNY)
</affiliation>
<address confidence="0.9834485">
695 Park Avenue, North 1008
New York, NY, USA, 10065
</address>
<email confidence="0.99843">
sakas@hunter.cuny.edu
</email>
<sectionHeader confidence="0.995604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9964144">
Computational modeling of human language
processes is a small but growing subfield of
computational linguistics. This paper
describes a course that makes use of recent
research in psychocomputational modeling as
a framework to introduce a number of
mainstream computational linguistics
concepts to an audience of linguistics,
cognitive science and computer science
doctoral students. The emphasis on what I
take to be the largely interdisciplinary nature
of computational linguistics is particularly
germane for the computer science students.
Since 2002 the course has been taught three
times under the auspices of the MA/PhD
program in Linguistics at The City University
of New York’s Graduate Center. A brief
description of some of the students’
experiences after having taken the course is
also provided.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.96611522">
A relatively small (but growing) subfield of
computational linguistics, psychocomputational
modeling affords a strong foundation from which
to introduce graduate students in linguistics to
various computational techniques, and students in
computer science1 (CS) to a variety of topics in
1 For rhetorical reasons I will often crudely partition the
student makeup of the course into linguistics students and CS
students. This preempts lengthy illocutions such as “ ... the
students with a strong computational background as compared
to students with a strong linguistics background.” In fact there
have been students from other academic disciplines in
attendance bringing with them a range of technical facility in
both CS and linguistics; linguistics students with an
psycholinguistics, though it has rarely been
incorporated into the computational linguistics
curriculum.
Psychocomputational modeling involves the
construction of computer models that embody one
or more psycholinguistic theories of natural
(human) language processing and use. Over the
past decade or so, there&apos;s been renewed interest
within the computational linguistics community
related to the possibility of incorporating human
language strategies into computational language
technologies. This is evidenced by the occassional
special session at computational linguistics
meetings (e.g., ACL-1999 Thematic Session on
Computational Psycholinguistics), several
workshops (e.g., COLING-2004, ACL-2005
Psychocomputational Models of Human Language
Acquisition, ACL-2004 Incremental Parsing:
Bringing Engineering and Cognition Together),
recent conference themes (e.g., CoNLL-2008 &amp;quot;...
models that explain natural phenomena relating to
human language&amp;quot;) and regular invitations to
psycholinguists to deliver plenary addresses at
recent ACL and COLING meetings.
Unfortunately, it is too often the case that
computational linguistics programs (primarily
those housed in computer science departments)
delay the introduction of cross-disciplinary
psycholinguistic / computational linguistics
approaches until either late in a student&apos;s course of
study (usually as an elective) or not at all. At the
City University of New York (CUNY)’s Graduate
Center (the primary Ph.D.-granting school of the
university) I have created a course that presents
undergraduate degree in CS; and CS students with a strong
undergraduate background in theoretical linguistics.
</bodyText>
<page confidence="0.912319">
120
</page>
<note confidence="0.739656">
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 120–128,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999814">
research in this cross-disciplinary area relatively
early in the graduate curriculum. I have also run an
undergraduate version of the course at Hunter
College, CUNY in the interdisciplinary Thomas
Hunter Honors Program.
I contend that a course developed within the
mélange of psycholinguistics and computational
linguistics is not only a valuable asset in a student&apos;s
repertoire of graduate experience, but can
effectively be used as a springboard to introduce a
variety of techniques and topics central to the
broader field of CL/NLP.
</bodyText>
<sectionHeader confidence="0.985156" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.992133833333333">
The CUNY Graduate Center (hereafter, GC) has
doctoral programs in both computer science and
linguistics. The Linguistics Program also contains
two master&apos;s tracks. Closely linked to both
programs, but administratively independent of
either, there exists a Cognitive Science
Concentration.2 In spring of 2000, I was asked by
the Cognitive Science Coordinator to create an
interdisciplinary course in &amp;quot;computing and
language&amp;quot; that would be attractive to linguistics,
speech and hearing, computer science, philosophy,
mathematics, psychology and anthropology
students. One might imagine how a newly-minted
Ph.D. might react to this request. Well this one, not
yet realizing the potential for abuse of junior
faculty (a slightly more sage me later wondered if
there was a last minute sabbatical-related
cancellation of some course that needed to be
replaced ASAP) dove in and developed
Computational Mechanisms of Syntax Acquisition.
The course was designed to cover generative
and non-generative linguistics and debates
surrounding (child) first language acquisition
principally focused on the question of Chomsky’s
Universal Grammar (UG), or not? Four
computational models drawn from diverse
paradigms – connectionist learning, statistical
formal language induction, principles-and-
parameters acquisition and acquisition in an
optimality theory framework3 – were presented and
</bodyText>
<footnote confidence="0.997686">
2 This is not a state-registered program, but rather an &amp;quot;in-
house&amp;quot; means of allowing students to receive recognition of
interdisciplinary studiy in cognitive science.
3 Although the semester ended as we were only just getting to
cover optimality theoretic acquisition.
</footnote>
<bodyText confidence="0.999805846153846">
discussion of the UG-or-not debate was framed in
the context of these models.
Over the past eight years, I&apos;ve taught three
variations of this course gradually molding the
course away from a seminar format into a
seminar/lecture format, dropping a large chunk of
the UG-or-not debate, and targeting the course
primarily for students who are in search of a &amp;quot;taste&amp;quot;
of computational linguistics who might very well
go on to take other CL-related course work.4 What
follows is a description of the design
considerations, student makeup, and course content
focusing on its last instantiation in Spring 2007.
</bodyText>
<sectionHeader confidence="0.998246" genericHeader="method">
3 The Course: Computational Natural
</sectionHeader>
<subsectionHeader confidence="0.978307">
Language Learning
</subsectionHeader>
<bodyText confidence="0.999653862068966">
Most readers will recognize the most recent title of
the course which was shamelessly borrowed from
the ACL’s Special Interest Group on Natural
Language Learning’s annual meeting. Although
largely an NLP-oriented meeting, the title and
indeed many of the themes of the meeting’s CFPs
over the years accurately portray the material
covered in the course.
The course is currently housed in the GC’s
Linguistics Program and is primarily designed to
serve linguistics doctoral and masters students who
want some exposure to computational linguistics
but with a decidedly linguistics emphasis.
Importantly though, the course often needs to serve
a high percentage of students from other graduate
programs.
The GC Linguistics and Computer Science
Programs also offer other computational linguistics
(CL) courses: a Natural Language Processing
(NLP) applications survey course, a corpus
analysis course, a statistical NLP course and a CL
methods sequence (in addition to a small variety of
electives). Although (at least until recently, see
Section 7) these courses are not taught within a
structured CL curriculum, they effectively serve as
the “meat-and-potatoes” CL courses which require
projects and assignments involving programming,
a considerable math component and extensive
experimentation with existing NLP/CL
</bodyText>
<footnote confidence="0.9970446">
4 Though the details of the undergraduate version of the course
are beyond the scope of this paper, it is worth noting that it did
not undergo this gradual revision; it was structured much as
the original Cognitive Science version of the course, actually
with an increased UG-or-not discussion.
</footnote>
<page confidence="0.997536">
121
</page>
<bodyText confidence="0.999752461538462">
applications. The students taking these classes
have already reached the point where they intend
to include a substantial amount of computational
linguistics in their dissertation or master’s thesis.
Computational Natural Language Learning is
somewhat removed from these other courses and
the design considerations were purposefully
directed at providing an “appetizer” that would
both entice interested students into taking other
courses, and prepare them with some experience in
computational linguistics techniques. Over time the
course has evolved to incorporate the following set
of prerequisites and goals.
</bodyText>
<listItem confidence="0.9997164">
• No programming prerequisite, no introduction to
programming Many of the students who take the
course are first or second year linguistics students
who have had little or no programming
background. Students are aware that
&amp;quot;Programming for Linguists&amp;quot; is part of the CL
methods sequence. They come to this course
looking for either an overview of CL, or for how
CL concepts might be relevant to psycholinguistic
or theoretical linguistics research.
</listItem>
<bodyText confidence="0.9724685">
Often there are students who have had a
substantial programming background – including
graduate students in computer science. This hasn’t
proved to be problematic since the assignments
and projects are designed not to involve
programming.
</bodyText>
<listItem confidence="0.7494478">
• Slight math prerequisite, exposure to
probabilities, and information theory Students are
expected to be comfortable with basic algebra. I
dissuade students from taking the course who are
intimidated by a one-line formula with a few Greek
letters in it. Students are not expected to know
what a conditional probability is, but will leave the
course with a decent grasp of basic
(undergraduate-level) concepts in probability and
information theory.
</listItem>
<bodyText confidence="0.999942857142857">
This lightweight math prerequisite actually does
split the class for a brief time during the semester
as the probability/information theory lecture and
assignment is a cinch for the CS students, and
typically quite difficult for the linguistics students.
But this is balanced by the implementation of the
design consideration expressed in the next bullet.
</bodyText>
<listItem confidence="0.986442">
• No linguistics prerequisite, exposure to syntactic
theory Students need to know what a syntax tree is
</listItem>
<bodyText confidence="0.992602571428571">
(at least in the formal language sense) but do not
need to know a particular theory of human
language syntax (e.g., X-bar theory or even S →
NP VP). By the end of the semester students will
be comfortable with elementary syntax beyond the
level covered by most undergraduate “Ling 101”
courses.
</bodyText>
<listItem confidence="0.911606428571429">
• Preparation for follow-up CL courses Students
leaving this course should be comfortably prepared
to enter the other GC computational linguistics
offerings.5
• Appreciation of the interdisciplinary nature of
CL Not all students move on to other
computational linguistics courses. Perhaps the
most important goal of the course is to expose CS
and linguistics students (and others) to the role that
computational linguistics can play in areas of
theoretical linguistics and cognitive science
research, and conversely to the role that cognitive
science and linguistics can play in the field of
computational linguistics.
</listItem>
<subsectionHeader confidence="0.998919">
3.1 Topical units
</subsectionHeader>
<bodyText confidence="0.991360541666667">
In this section I present the syllabus of the course
framed in topical units. They have varied over the
years; what follows is the content of the course
mostly as it was taught in Spring 2007.
Janet Dean Fodor and I lead an active
psychocomputational modeling research group at
the City University of New York: CUNY CoLAG –
CUNY Computational Language Acquisition
Group which is primarily dedicated to the design
and evaluation of computational models of first
language acquisition. Most, though not all, of the
topical units purposefully contain material that
intersects with CUNY CoLAG’s ongoing research
efforts.
The depth of coverage of the units is designed to
give the students some fluency in computational
issues (e.g., use and ramifications of Markov
assumptions), and a basic understanding beyond
exposure to the computational mechanisms of CL
(e.g., derivation of the standard MLE bigram
5 The one exception in the Linguistics Program is Corpus
Linguistics which has a programming prerequisite, and the
occasional CL elective course in Computer Science targeted
primarily for their more advanced students.
</bodyText>
<page confidence="0.989063">
122
</page>
<bodyText confidence="0.999694041666667">
probability formula), but not designed to allow
students to bypass a more computationally rigorous
NLP survey course. The same is true of the breadth
of coverage; a comprehensive survey is not the
goal. For example, in the ngram unit, typically no
more than two or at most three smoothing
techniques are covered.
Note that the citations in this section are mostly
required reading, but some articles are optional. It
has been my experience however, that the students
by and large read most of the material since the
readings were highly directed (i.e., which sections
and pages are most relevant to the course.)
Supplemental materials that present introductory
mathematics and tutorial presentations are not
exhaustively listed, but included Jurafsky and
Martin (2000), Goldsmith (2007, previously
online) and a host of (other) online resources.
History of linguistics and computation [1
lecture] The history is framed around the question
“Is computational linguistics, well uh, linguistics?”
We conclude with “It was, then it wasn’t, now
maybe it is, or at least in part, should be.” The
material is tightly tied to Lee (2004); with
additional discussion along the lines of Sakas
(2004).
Syntax [1 lecture] This is a crash course in syntax
using a context-free grammar with
transformational movement. The more difficult
topics include topicalization (including null-topic),
Wh-movement and verb-second phenomena. We
make effective use of an in-house database of
abstract though linguistically viable cross-
linguistic sentence patterns and tree structures –
the CUNY CoLAG Domain (Sakas, 2003). The
point of this lecture is to introduce non-linguistics
students to the intricacies of a linguistically viable
grammatical theory.
Language Acquisition [1 lecture] We discuss
some of the current debates in L1 (a child’s first)
language acquisition surrounding “no negative
evidence” (Marcus, 1993), Poverty of the Stimulus
(Pullum, 1996), and Chomsky’s conceptualization
of Universal Grammar. This is the least
computational lecture of the semester, although it
often generates some of the most energized
discussion. The language acquisition unit is the
central arena in which we stage most of the rest of
the topics in the course.
Gold and the Subset Principle [2 lectures]
During the presentation of Gold’s (1967) and
Angluin&apos;s (1980) proofs and discussion of how
they might be used to argue (often incorrectly) for
a Universal Grammar (Johnson, 2004) some core
CL topics are introduced including formal
language classes (the Chomsky Hierarchy) and the
notions of hypothesis space and search space. The
first (toy) probabilistic analyses are also presented
(e.g., given a finite enumeration and a probability p
that an arbitrary non-target grammar licenses a
sentence in the input sample, what is the “worst
case” number of sentences required to converge on
the target grammar?)
Next, the Subset Principle and linguistic
overgeneralization (Fodor and Sakas, 2005) is
introduced. An important focus is on how keeping
(statistical) track of what’s not encountered might
supply a ‘retreat’ mechanism to pull back from an
over-general hypothesis. Although the
mathematical details of how the statistics might
work are omitted, this topic leads neatly into a unit
on Bayesian learning later in the semester.
This is an important two lectures. It&apos;s the first
unit where students are exposed to the use of
computational techniques applied to theoretical
issues in psycholinguistics. By this point, students
often are intellectually engaged in the debates
surrounding L1 acquisition. To understand the
arguments presented in this unit students need to
flex their computational muscles for the first time.
Connectionism [3 lectures] This unit covers the
basics of Simple Recurrent Network (SRN)
learning (Elman, 1990, 1993). More or less, Elman
argues that language acquisition is not necessarily
the acquisition of rules operating over atomic
linguistic units (e.g., phrase markers) but rather the
process of capturing the “dynamics” of word
patterns in the input stream. He demonstrates how
this can be simulated in an SRN paradigm.
The mechanics of how an SRN operates and can
be used to model language acquisition phenomena
is covered but more importantly core concepts
common to most all supervised machine learning
paradigms are emphasized. Topics include how
training and testing corpora are developed and
used, cross validation, hill-climbing, learning bias,
</bodyText>
<page confidence="0.995279">
123
</page>
<bodyText confidence="0.999647391752578">
linear and non-linear separation of the hypothesis
space, etc. A critique of SRN learning is also
covered (Marcus, 1998) which presents the
important distinction between generalization
performance and learning within the training
space in a way that is approachable by non-CS
students, but also interesting to CS-students.
Information retrieval [1 lecture] Elman (1990)
uses hierarchal clustering to analyze some of his
results. I use Elman&apos;s application of clustering to
take a brief digression from the psycholinguistics
theme of the course and present an introduction to
vector space models and document clustering.
This is the most challenging technical lecture of
the semester and is included only when there are a
relatively high proportion of CS students in
attendance. Most of the linguistics students get a
decent feel for the material, but most require a
second exposure it in another course to fully
understand the math. That said, the linguistics
students do understand how weight heuristics are
used to effectively represent documents in vectors
(though most linguistics students have a hard time
swallowing the bag-of-words paradigm at face
value), and how vectors can be nearer or farther
from each other in a hypothesis space.
Ngram language models [3 lecture] In this unit
we return to psycholinguistics. Reali and
Christiansen, (2005) present a simple ngram
language model of child-directed speech to argue
against the need for innate UG-provided
knowledge of hierarchal syntactic structure. Basic
probability and information theory is introduced –
conditional probabilities and Markov assumptions,
the chain rule, Bayes Rule, maximum likelihood
estimates, entropy, etc. Although relatively easy
for the CS students (they had their hands full with
the syntax unit), introduction of this material is
invaluable to the linguistics students who need to
be somewhat fluent in it before entering our other
CL offerings.
We continue with a presentation of the sparse data
problem, Zipf’s law, corpus cross-entropy and a
handful of smoothing techniques (Reali &amp;
Christiansen use a particularly impoverished
version of deleted interpolation). We continue with
a discussion of the pros and cons of employing
Markov assumptions in computational linguistics
generally, the relationship of Markov assumptions
to incremental learning and psycholinguistic
modeling, and the use of cross-entropy as an
evaluation metric, and end with a brief discussion
of the descriptional necessity (or not) of traditional
generative grammars (Pereira, 2000).
.
&amp;quot;Ideal&amp;quot; learning, Bayesian learning and
computational resources [1 lecture] Regier and
Gahl (2004) in response to Lidz et al. (2003)
present a Bayesian learning model that learns the
correct structural referent for anaphoric &amp;quot;one&amp;quot; in
English from a corpus of child-directed speech.
Similarly to Reali &amp; Christiansen (op. cit.), they
argue against the need for innate knowledge of
hierarchal structure since their Bayesian model
starts tabula rasa and learns from linear word
strings with no readily observable structure.
The fundamental mechanics of Bayesian
inference is presented. Since most Bayesian
models are able to retreat from overgeneral
hypotheses in the absence of positive evidence, the
course returns to overgeneralization errors, the
Subset Principle and the alternative of using
statistics as a possible retreat mechanism.
Computationally heavy (&amp;quot;ideal&amp;quot;) batch processing,
and incremental (psychologically plausible)
processing are contrasted here as is the use of
heuristics (psycholinguistically-based or not) to
mitigate the potentially huge computational cost of
searching a large domain.
Principle and parameters [2 lectures] As the
academic scheduling has worked out, the course is
usually offered during years when the Linguistics
Program does not offer a linguistics-based
learnability course. As a result, there is a unit on
acquisition within a principles-and-parameters
(P&amp;P) framework (Chomsky, 1981). Roughly, in
the P&amp;P framework cross-linguistic commonalities
are considered principles, and language variation is
standardly specified by the settings of a bank of
binary parameters (i.e., UG = principles +
parameters; a specific language = principles +
parameters + parameter settings).
Although this unit is the furthest away from
mainstream CL, it has served as a useful means to
introduce deterministic learning (Sakas and Fodor,
2001), versus non-deterministic learning (Yang,
2002), the effectiveness of hill-climbing in
</bodyText>
<page confidence="0.9969">
124
</page>
<bodyText confidence="0.999879468085107">
linguistically smooth and non-smooth domains,6 as
well as the notion of computational complexity and
combinatorial explosion (n binary parameters
yields a search space of 2n possible grammars).
Finally, and perhaps most importantly there is
extensive discussion of the difficulty of building
computational systems that can efficiently and
correctly learn to navigate through domains with
an enormous amount of ambiguity.
In the P&amp;P framework ambiguity stems from
competition of cross-linguistic structural analyses
of surface word order patterns. For example, given
a (tensed) S V O sentence pattern, is the V situated
under the phrase maker I (English), or under the
phrase marker C (German)? Although this is a
somewhat different form of ambiguity than the
within-language structural ambiguity that is all too
familiar to those of use working in CL, it serves as
useful background material for the next unit.
Part of speech tagging and statistical parsing [3
lectures] In this unit we begin by putting aside the
psycholinguistics umbrella of the course and cover
introductory CL in a more traditional manner.
Using Charniak (1997) as the primary reading, we
cover rudimentary HMM&apos;s, and probabilistic
CFG&apos;s. We use supplemental materials to introduce
lexicalized statistical parsing (e.g., Jurafsky and
Martin, 2000 and online materials). We then turn
back to psycholinguistics and after a (somewhat
condensed overview) of human sentence
processing, discuss the viability of probabilistic
parsing as a model of human sentence processing
(Keller, 2005). This unit, more than some others, is
lightweight on detailed computational mechanics;
the material is presented throughout at a level
similar to that of Charniak’s article. For example
the specifics of EM algorithms are not covered
although what they do, and why they are necessary
are.
The Linguistics Program at CUNY is very active
in human sentence processing research and this
unit is of interest to many of the linguistics
students. In particular we contrast computational
approaches that employ nondeterminism and
parallelism to mainstream psycholinguistics
models which are primarily deterministic, serial
and employ a reanalysis strategy when evaluating a
</bodyText>
<page confidence="0.504783">
6 By &amp;quot;smooth&amp;quot;, I mean a correlation between the similarity of
</page>
<bodyText confidence="0.979301071428571">
grammars, and the similarity of languages they generate.
parse “online” (though of course there is a
significant amount of research that falls outside of
this mainstream). We then focus on issues of
computational resources that each paradigm
requires.
In some ways the last lectures of this unit best
embody the goal of exposing the students to the
potential of interdisciplinary research in
computational linguistics. The CS students leave
with an appreciation of psycholinguistic
approaches to human sentence processing, and the
linguistics students with a firm grasp of the
effectiveness of computational approaches.
</bodyText>
<sectionHeader confidence="0.965124" genericHeader="method">
4 Assignments and Projects
</sectionHeader>
<bodyText confidence="0.99968825">
Across the three most recent incarnations of the
course the number and difficulty of the
assignments and projects has varied quite a bit. In
the last version, there were three assignments (five
to ten hours of student effort each) and one project
(twenty to thirty hours effort).
Due to the typically small size of the course,
assignments and projects (beyond weekly
readings) were often individually tailored and
assessed. The goal of the assignments was to
concretize the CL aspects of the primarily
psycholinguistic readings with either hands-on use
of the computer, mathematically-oriented problem
sets, or a critical evaluation of the CL
methodologies employed. A handful of examples
follow.
</bodyText>
<listItem confidence="0.9452716">
• Gold and the Subset Principle (Assignments)
All students are asked to formulate a Markov chain
(though at this point in the course, not by that
name) of a Gold-style enumeration learner
operating over a small finite domain (e.g., 4
</listItem>
<bodyText confidence="0.790109777777778">
grammars, 12 sentences and a sentence to grammar
mapping). The more mathematically inclined are
additionally asked to calculate the expected value
of the number of input sentences consumed by a
learner operating over an enumeration of n
grammars and given a generalized mapping of
sentences to grammars, or to formally prove the
learnability of any finite domain of languages
given text (positive) presentation of input.
</bodyText>
<listItem confidence="0.91146">
• Connectionism (Assignments) All students
were asked to pick a language from the CUNY
CoLAG domain, develop a training and test set
</listItem>
<page confidence="0.997852">
125
</page>
<bodyText confidence="0.999888428571429">
from that language using existing software and run
a predict-the-next-word SRN simulation on either a
MatLab or TLearn neural network platform.
Linguistics and CS students were paired on this
assignment. When the assignment is given, a
relatively detailed after-class lab tutorial on how to
run the software is presented.
</bodyText>
<listItem confidence="0.685515">
• Ngram language models (Projects) One CS
</listItem>
<bodyText confidence="0.8076556875">
student implemented a version of Reali and
Christiansen’s experiment and was asked to
evaluate the effectiveness of different smoothing
techniques on child-directed speech and to design a
study of how to evaluate differences between
child-directed speech and adult-to-adult speech in
terms of language modeling. A linguistics student
was asked to write a paper explaining how one
could develop a computational evaluation of how a
bigram learner might be evaluated longitudinally.
(I.e., to answer the question, how can one measure
the effectiveness of a language model after each
input sentence?). Another linguistics student (with
strong CS skills) created an annotation tool that
semi-automatically mapped child-directed speech
in French onto the CoLAG Domain tag set.
</bodyText>
<sectionHeader confidence="0.768665" genericHeader="method">
5 Students: Past and Current
</sectionHeader>
<bodyText confidence="0.982109291666667">
As mentioned earlier, the Linguistics Doctoral
Program at CUNY has just recently begun to
structure their computational linguistics offerings
into a cohesive course of study (described briefly
in Section 7). During the past several years
Computational Natural Learning has been offered
on an ad hoc basis primarily in response to student
demand and demographics of students’
computational skills. Since the course was not
originally intended to serve any specific function
as part of a larger curriculum, and was not
integrated into a reoccurring schedule there has
been little need to carry out a systematic evaluation
of the impact of the course on students’ academic
careers. Still a few anecdotal accounts will help
give a picture of the course’s effectiveness.
After the first Cognitive Science offering of the
course in 2000, approximately 30 graduate
students have taken one of the three subsequent
incarnations. Two of the earliest linguistics
students went on to take undergraduate CS courses
in programming and statistics, and subsequently
came back to take graduate level CL courses.7
They have obtained their doctorates and are
currently working in industry as computational
linguists. One is a lead software engineer for an
information retrieval startup company in New
York that does email data mining. And though I’ve
lost track of the other student, she was at one point
working for a large software company on the west
coast.
I am currently the advisor of one CS student,
and two linguistics students who have taken the
course. One linguistics student is in the throws of
writing a dissertation on the plausibility of
exploiting statistical regularities of various
syntactic structures (contra regularities of word
strings) in child-directed speech during L1
acquisition. The other is relatively early in her
academic career, but is interested in working on
computational semantics and discourse analysis
within a categorial grammar framework. Her
thoughts currently revolve around establishing
(and formalizing) relationships between traditional
linguistics-oriented semantics and a computational
semantics paradigm. She hopes to make
contributions to both linguistics and CL. The CS
student, also early in his career, is interested in
semi-productive multi-word expressions and how
young children can come to acquire them. His idea
is to employ a learning component in a machine
translation system that can be trained to translate
productive metaphors between a variety of
languages.
These five students have chosen to pursue
specific areas of study and research directly as a
result of having taken Computational Natural
Language Learning early in their careers.
I am also sitting on two CS students’ second
qualifying exam committees. One is working on
machine translation of Hebrew and the other
working on (relatively) mainstream word-sense
disambiguation. Both of their qualifying exam
papers show a sensitivity to psycholinguistics that
I’m frankly quite happy to see, and am sure
wouldn’t have been present without their having
taken the course.
The parsing unit was just added this past spring
semester and I’ve had two recent discussions with
7 The CL methods sequence was established only 3 years ago,
previously students were encouraged to develop their basic
computational skills at one of CUNY’s undergraduate schools.
</bodyText>
<page confidence="0.996114">
126
</page>
<bodyText confidence="0.9998896">
a second year linguistics student about
incorporating a statistical component into a current
psycholinguistic model of human sentence
processing. Another second year student has
expressed interested in doing a comprehensive
study of neural network models of human sentence
parsing for his first qualifying paper. It’s not clear
that they will ultimately pursue these directions,
but I’m certain they wouldn’t have thought of the
possibilities if they hadn’t taken the Computational
Natural Language Learning.
Finally, most all of the students who have taken
the course have also taken the NLP-survey course
(no programming required), slightly less than a
third have moved on to the CL methods sequence
(includes an introduction to programming), or if
they have some CS background move directly to
Corpus Analysis (programming experience
required as a prerequisite). We hope that
eventually, especially in light of the GC’s new
computational linguistics program, the course will
serve as the gateway for many more students to
begin to pursue studies that will lead to research
areas in both psychocomputational modeling and
more mainstream CL.
</bodyText>
<sectionHeader confidence="0.998656" genericHeader="method">
6 Brief Discussion
</sectionHeader>
<bodyText confidence="0.9999814">
It is my view that computational linguistics is by
nature a cross-disciplinary endeavor. Indeed, one
could argue that only after the incorporation of
techniques and strategies gleaned from theoretical
advances in psychocomputational modeling of
language, can we achieve truly transparent (to the
user) human-computer language applications.
That argument notwithstanding, a course such as
the one described in this paper can effectively
serve as an introduction to an assortment of
concepts in computational linguistics that can
broaden the intellectual horizons of both CS and
linguistics students, as well providing a foundation
that students can build on in the pursuit of more
advanced studies in the field.
</bodyText>
<sectionHeader confidence="0.988496" genericHeader="method">
7 Postscript: The Future of the Course
</sectionHeader>
<bodyText confidence="0.999964055555556">
The CUNY Graduate Center has recently created a
structured computational linguistics program
housed in the GC’s Linguistics Program. The
program consists of a Computational Linguistics
Concentration in the Linguistics Master’s
subprogram, and particularly relevant to the
discussion in this article, a Computational
Linguistics Certificate8 (both fall under the
acronym CLC). Any City University of New York
doctoral student can enroll in CLC concurrently
with enrollment in their primary doctoral program
(as one might imagine, we expect a substantial
number of Linguistics and CS doctoral candidates
to enroll in the CLC program).
Due to my newly-acquired duties as director of
the CLC program and to scheduling constraints on
CLC faculty teaching assignments, the course
cannot be offered again until Fall 2009 or Spring
2010. At that time Computational Natural
Language Learning will need to morph into a more
technically advanced elective course in applied
machine learning techniques in computational
linguistics (or some such) since the CLC course of
study currently posits the NLP survey course and
the CL Methods sequence as the first year
introductory requirements.
However, I expect that a course similar to the
one described here will supplement the NLP
survey course as a first year requirement in Fall
2010. The course will be billed as having broad
appeal and made available to both CLC students
and linguistics, CS and other students who might
not want or require the “meat-and-potatoes” that
CLC offers, but who only desire a CL “appetizer”.
Though if the appetizer is tasty enough, students
may well hunger for the main course.
</bodyText>
<sectionHeader confidence="0.998227" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999616">
I would like to thank the three anonymous
reviewers for helpful comments, and the many
intellectually diverse and engaging students I’ve
had the pleasure to introduce to the field of
computational linguistics.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9429385">
Angluin, D. (1980). Inductive inference of formal
languages from positive data. Information and
Control 45:117-135.
Charniak, E. (1997). Statistical Techniques for Natural
language Parsing. AI Magazine 18:33-44.
Chomsky, N. (1981). Lectures on government and
binding: Studies in generative grammar. Dordrecht:
Foris.
8 Pending state Department of Education approval, hopefully
to be received in Spring 2009.
</reference>
<page confidence="0.977237">
127
</page>
<reference confidence="0.999764272727273">
Elman, J. L. (1990). Finding structure in time. Cognitive
Science 14:179-211.
Elman, J. L. (1993). Learning and development in
neural networks: The importance of starting small.
Cognition 48:71-99.
Fodor, J. D., and Sakas, W. G. (2005). The Subset
Principle in syntax: Costs of compliance. Journal of
Linguistics 41:513-569.
Gold, E. M. (1967). Language identification in the limit.
Information and Control 10:447-474.
Goldsmith, J. (2007). Probability for Linguists.
Mathematics and Social Sciences 180:5-40.
Johnson, K. (2004). Gold’s Theorem and Cognitive
Science. Philosophy of Science 71:571-592.
Jurafsky, D., and Martin, J. H. (2000). Speech and
Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics,
and Speech Recognition
Keller, F. (2005). Probabilistic Models of Human
Sentence Processing. Presented at Probabilistic
Models of Cognition: The Mathematics of Mind,
IPAM workshop, Los Angeles.
Lee, L. (2004). &amp;quot;I&apos;m sorry Dave, I&apos;m afraid I can&apos;t do
that&amp;quot; : Linguistics, statistics, and natural language
processing circa 2001. In Computer Science:
Reflections on the Field, 111-118.
Washington:National Academies Press.
Lidz, J., Waxman, S., and Freedman, J. (2003). What
infants know about syntax but couldn’t have learned:
Experimental evidence for syntactic structure at 18
months. Cognition 89:65-73.
Marcus, G. F. (1993). Negative evidence in language
acquisition. Cognition 46:53-85.
Marcus, G. F. (1998). Can connectionism save
constructivism? Cognition 66:153-182.
Pereira, F. (2000) Formal grammar and information
theory: Together again. Philosophical Transactions
of the Royal Society A358:1239-1253.
Pullum, G. K. (1996). Learnability, hyperlearning, and
the poverty of the stimulus. Proceedings of the 22nd
Annual Meeting of the Berkley Linguistics Society:
General Session and Parasession on the Role of
Learnability in Grammatical Theory, Berkeley: 498-
513.
Reali, F., and Christiansen, M. H. (2005). Uncovering
the richness of the stimulus: Structural dependence
and indirect statistical evidence. Cognitive Science
29:1007-10018.
Regier, T., and Gahl, S. (2004). Learning the
unlearnable: The role of missing evidence. Cognition
93:147-155.
Sakas, W. G., and Fodor, J. D. (2001). The Structural
Triggers Learner. In Language Acquisition and
Learnability, ed. S. Bertolo, 172-233. Cambridge:
Cambridge University Press.
Sakas, W. G. (2003) A Word-Order Database for
Testing Computational Models of Language
Acquisition, Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics,
ACL-2003: 415-422.
Sakas, W. G. (2004) Introduction. Proceedings of the
First Workshop on Psycho-computational Models of
Human Language Acquisition, COLING-2004.
Geneva: iv-vi.
Yang, C. D. (2002). Knowledge and learning in natural
language. New York: Oxford University Press.
</reference>
<page confidence="0.996751">
128
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.365957">
<title confidence="0.9983675">Psychocomputational Linguistics: A Gateway to the Computational Linguistics Curriculum</title>
<author confidence="0.99975">William Gregory</author>
<affiliation confidence="0.755637">Department of Computer Science, Hunter Ph.D. Programs in Linguistics and Computer Science, The Graduate City University of New York</affiliation>
<address confidence="0.9852145">695 Park Avenue, North New York, NY, USA,</address>
<email confidence="0.999926">sakas@hunter.cuny.edu</email>
<abstract confidence="0.999053">Computational modeling of human language processes is a small but growing subfield of computational linguistics. This paper describes a course that makes use of recent research in psychocomputational modeling as a framework to introduce a number of mainstream computational concepts to an audience of linguistics, cognitive science and computer science doctoral students. The emphasis on what I take to be the largely interdisciplinary nature of computational linguistics is particularly germane for the computer science students. Since 2002 the course has been taught three times under the auspices of the MA/PhD program in Linguistics at The City University of New York’s Graduate Center. A brief description of some of the students’ experiences after having taken the course is also provided.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Angluin</author>
</authors>
<title>Inductive inference of formal languages from positive data.</title>
<date>1980</date>
<journal>Information and Control</journal>
<pages>45--117</pages>
<marker>Angluin, 1980</marker>
<rawString>Angluin, D. (1980). Inductive inference of formal languages from positive data. Information and Control 45:117-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Techniques for Natural language Parsing.</title>
<date>1997</date>
<journal>AI Magazine</journal>
<pages>18--33</pages>
<contexts>
<context position="22606" citStr="Charniak (1997)" startWordPosition="3349" endWordPosition="3350"> surface word order patterns. For example, given a (tensed) S V O sentence pattern, is the V situated under the phrase maker I (English), or under the phrase marker C (German)? Although this is a somewhat different form of ambiguity than the within-language structural ambiguity that is all too familiar to those of use working in CL, it serves as useful background material for the next unit. Part of speech tagging and statistical parsing [3 lectures] In this unit we begin by putting aside the psycholinguistics umbrella of the course and cover introductory CL in a more traditional manner. Using Charniak (1997) as the primary reading, we cover rudimentary HMM&apos;s, and probabilistic CFG&apos;s. We use supplemental materials to introduce lexicalized statistical parsing (e.g., Jurafsky and Martin, 2000 and online materials). We then turn back to psycholinguistics and after a (somewhat condensed overview) of human sentence processing, discuss the viability of probabilistic parsing as a model of human sentence processing (Keller, 2005). This unit, more than some others, is lightweight on detailed computational mechanics; the material is presented throughout at a level similar to that of Charniak’s article. For </context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Charniak, E. (1997). Statistical Techniques for Natural language Parsing. AI Magazine 18:33-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Lectures on government and binding: Studies in generative grammar.</title>
<date>1981</date>
<location>Dordrecht: Foris.</location>
<contexts>
<context position="20941" citStr="Chomsky, 1981" startWordPosition="3098" endWordPosition="3099">sible retreat mechanism. Computationally heavy (&amp;quot;ideal&amp;quot;) batch processing, and incremental (psychologically plausible) processing are contrasted here as is the use of heuristics (psycholinguistically-based or not) to mitigate the potentially huge computational cost of searching a large domain. Principle and parameters [2 lectures] As the academic scheduling has worked out, the course is usually offered during years when the Linguistics Program does not offer a linguistics-based learnability course. As a result, there is a unit on acquisition within a principles-and-parameters (P&amp;P) framework (Chomsky, 1981). Roughly, in the P&amp;P framework cross-linguistic commonalities are considered principles, and language variation is standardly specified by the settings of a bank of binary parameters (i.e., UG = principles + parameters; a specific language = principles + parameters + parameter settings). Although this unit is the furthest away from mainstream CL, it has served as a useful means to introduce deterministic learning (Sakas and Fodor, 2001), versus non-deterministic learning (Yang, 2002), the effectiveness of hill-climbing in 124 linguistically smooth and non-smooth domains,6 as well as the notio</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, N. (1981). Lectures on government and binding: Studies in generative grammar. Dordrecht: Foris.</rawString>
</citation>
<citation valid="true">
<title>8 Pending state Department of Education approval, hopefully to be received in Spring</title>
<date>2009</date>
<marker>2009</marker>
<rawString>8 Pending state Department of Education approval, hopefully to be received in Spring 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive Science</journal>
<pages>14--179</pages>
<contexts>
<context position="16309" citStr="Elman, 1990" startWordPosition="2414" endWordPosition="2415">cs might work are omitted, this topic leads neatly into a unit on Bayesian learning later in the semester. This is an important two lectures. It&apos;s the first unit where students are exposed to the use of computational techniques applied to theoretical issues in psycholinguistics. By this point, students often are intellectually engaged in the debates surrounding L1 acquisition. To understand the arguments presented in this unit students need to flex their computational muscles for the first time. Connectionism [3 lectures] This unit covers the basics of Simple Recurrent Network (SRN) learning (Elman, 1990, 1993). More or less, Elman argues that language acquisition is not necessarily the acquisition of rules operating over atomic linguistic units (e.g., phrase markers) but rather the process of capturing the “dynamics” of word patterns in the input stream. He demonstrates how this can be simulated in an SRN paradigm. The mechanics of how an SRN operates and can be used to model language acquisition phenomena is covered but more importantly core concepts common to most all supervised machine learning paradigms are emphasized. Topics include how training and testing corpora are developed and use</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Elman, J. L. (1990). Finding structure in time. Cognitive Science 14:179-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Learning and development in neural networks: The importance of starting small.</title>
<date>1993</date>
<journal>Cognition</journal>
<pages>48--71</pages>
<marker>Elman, 1993</marker>
<rawString>Elman, J. L. (1993). Learning and development in neural networks: The importance of starting small. Cognition 48:71-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Fodor</author>
<author>W G Sakas</author>
</authors>
<title>The Subset Principle in syntax: Costs of compliance.</title>
<date>2005</date>
<journal>Journal of Linguistics</journal>
<pages>41--513</pages>
<contexts>
<context position="15465" citStr="Fodor and Sakas, 2005" startWordPosition="2284" endWordPosition="2287"> and discussion of how they might be used to argue (often incorrectly) for a Universal Grammar (Johnson, 2004) some core CL topics are introduced including formal language classes (the Chomsky Hierarchy) and the notions of hypothesis space and search space. The first (toy) probabilistic analyses are also presented (e.g., given a finite enumeration and a probability p that an arbitrary non-target grammar licenses a sentence in the input sample, what is the “worst case” number of sentences required to converge on the target grammar?) Next, the Subset Principle and linguistic overgeneralization (Fodor and Sakas, 2005) is introduced. An important focus is on how keeping (statistical) track of what’s not encountered might supply a ‘retreat’ mechanism to pull back from an over-general hypothesis. Although the mathematical details of how the statistics might work are omitted, this topic leads neatly into a unit on Bayesian learning later in the semester. This is an important two lectures. It&apos;s the first unit where students are exposed to the use of computational techniques applied to theoretical issues in psycholinguistics. By this point, students often are intellectually engaged in the debates surrounding L1 </context>
</contexts>
<marker>Fodor, Sakas, 2005</marker>
<rawString>Fodor, J. D., and Sakas, W. G. (2005). The Subset Principle in syntax: Costs of compliance. Journal of Linguistics 41:513-569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<title>Language identification in the limit.</title>
<date>1967</date>
<journal>Information and Control</journal>
<pages>10--447</pages>
<marker>Gold, 1967</marker>
<rawString>Gold, E. M. (1967). Language identification in the limit. Information and Control 10:447-474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Probability for Linguists.</title>
<date>2007</date>
<journal>Mathematics and Social Sciences</journal>
<pages>180--5</pages>
<contexts>
<context position="13289" citStr="Goldsmith (2007" startWordPosition="1957" endWordPosition="1958">ehensive survey is not the goal. For example, in the ngram unit, typically no more than two or at most three smoothing techniques are covered. Note that the citations in this section are mostly required reading, but some articles are optional. It has been my experience however, that the students by and large read most of the material since the readings were highly directed (i.e., which sections and pages are most relevant to the course.) Supplemental materials that present introductory mathematics and tutorial presentations are not exhaustively listed, but included Jurafsky and Martin (2000), Goldsmith (2007, previously online) and a host of (other) online resources. History of linguistics and computation [1 lecture] The history is framed around the question “Is computational linguistics, well uh, linguistics?” We conclude with “It was, then it wasn’t, now maybe it is, or at least in part, should be.” The material is tightly tied to Lee (2004); with additional discussion along the lines of Sakas (2004). Syntax [1 lecture] This is a crash course in syntax using a context-free grammar with transformational movement. The more difficult topics include topicalization (including null-topic), Wh-movemen</context>
</contexts>
<marker>Goldsmith, 2007</marker>
<rawString>Goldsmith, J. (2007). Probability for Linguists. Mathematics and Social Sciences 180:5-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Johnson</author>
</authors>
<title>Gold’s Theorem and Cognitive Science.</title>
<date>2004</date>
<journal>Philosophy of Science</journal>
<pages>71--571</pages>
<contexts>
<context position="14953" citStr="Johnson, 2004" startWordPosition="2209" endWordPosition="2210">uisition surrounding “no negative evidence” (Marcus, 1993), Poverty of the Stimulus (Pullum, 1996), and Chomsky’s conceptualization of Universal Grammar. This is the least computational lecture of the semester, although it often generates some of the most energized discussion. The language acquisition unit is the central arena in which we stage most of the rest of the topics in the course. Gold and the Subset Principle [2 lectures] During the presentation of Gold’s (1967) and Angluin&apos;s (1980) proofs and discussion of how they might be used to argue (often incorrectly) for a Universal Grammar (Johnson, 2004) some core CL topics are introduced including formal language classes (the Chomsky Hierarchy) and the notions of hypothesis space and search space. The first (toy) probabilistic analyses are also presented (e.g., given a finite enumeration and a probability p that an arbitrary non-target grammar licenses a sentence in the input sample, what is the “worst case” number of sentences required to converge on the target grammar?) Next, the Subset Principle and linguistic overgeneralization (Fodor and Sakas, 2005) is introduced. An important focus is on how keeping (statistical) track of what’s not e</context>
</contexts>
<marker>Johnson, 2004</marker>
<rawString>Johnson, K. (2004). Gold’s Theorem and Cognitive Science. Philosophy of Science 71:571-592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition</title>
<date>2000</date>
<contexts>
<context position="13272" citStr="Jurafsky and Martin (2000)" startWordPosition="1953" endWordPosition="1956">breadth of coverage; a comprehensive survey is not the goal. For example, in the ngram unit, typically no more than two or at most three smoothing techniques are covered. Note that the citations in this section are mostly required reading, but some articles are optional. It has been my experience however, that the students by and large read most of the material since the readings were highly directed (i.e., which sections and pages are most relevant to the course.) Supplemental materials that present introductory mathematics and tutorial presentations are not exhaustively listed, but included Jurafsky and Martin (2000), Goldsmith (2007, previously online) and a host of (other) online resources. History of linguistics and computation [1 lecture] The history is framed around the question “Is computational linguistics, well uh, linguistics?” We conclude with “It was, then it wasn’t, now maybe it is, or at least in part, should be.” The material is tightly tied to Lee (2004); with additional discussion along the lines of Sakas (2004). Syntax [1 lecture] This is a crash course in syntax using a context-free grammar with transformational movement. The more difficult topics include topicalization (including null-t</context>
<context position="22791" citStr="Jurafsky and Martin, 2000" startWordPosition="3372" endWordPosition="3375">? Although this is a somewhat different form of ambiguity than the within-language structural ambiguity that is all too familiar to those of use working in CL, it serves as useful background material for the next unit. Part of speech tagging and statistical parsing [3 lectures] In this unit we begin by putting aside the psycholinguistics umbrella of the course and cover introductory CL in a more traditional manner. Using Charniak (1997) as the primary reading, we cover rudimentary HMM&apos;s, and probabilistic CFG&apos;s. We use supplemental materials to introduce lexicalized statistical parsing (e.g., Jurafsky and Martin, 2000 and online materials). We then turn back to psycholinguistics and after a (somewhat condensed overview) of human sentence processing, discuss the viability of probabilistic parsing as a model of human sentence processing (Keller, 2005). This unit, more than some others, is lightweight on detailed computational mechanics; the material is presented throughout at a level similar to that of Charniak’s article. For example the specifics of EM algorithms are not covered although what they do, and why they are necessary are. The Linguistics Program at CUNY is very active in human sentence processing</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Jurafsky, D., and Martin, J. H. (2000). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keller</author>
</authors>
<title>Probabilistic Models of Human Sentence Processing. Presented at Probabilistic Models of Cognition: The Mathematics of Mind, IPAM workshop,</title>
<date>2005</date>
<location>Los Angeles.</location>
<contexts>
<context position="23027" citStr="Keller, 2005" startWordPosition="3408" endWordPosition="3409">atistical parsing [3 lectures] In this unit we begin by putting aside the psycholinguistics umbrella of the course and cover introductory CL in a more traditional manner. Using Charniak (1997) as the primary reading, we cover rudimentary HMM&apos;s, and probabilistic CFG&apos;s. We use supplemental materials to introduce lexicalized statistical parsing (e.g., Jurafsky and Martin, 2000 and online materials). We then turn back to psycholinguistics and after a (somewhat condensed overview) of human sentence processing, discuss the viability of probabilistic parsing as a model of human sentence processing (Keller, 2005). This unit, more than some others, is lightweight on detailed computational mechanics; the material is presented throughout at a level similar to that of Charniak’s article. For example the specifics of EM algorithms are not covered although what they do, and why they are necessary are. The Linguistics Program at CUNY is very active in human sentence processing research and this unit is of interest to many of the linguistics students. In particular we contrast computational approaches that employ nondeterminism and parallelism to mainstream psycholinguistics models which are primarily determi</context>
</contexts>
<marker>Keller, 2005</marker>
<rawString>Keller, F. (2005). Probabilistic Models of Human Sentence Processing. Presented at Probabilistic Models of Cognition: The Mathematics of Mind, IPAM workshop, Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>I&apos;m sorry Dave, I&apos;m afraid I can&apos;t do that&amp;quot; : Linguistics, statistics, and natural language processing circa</title>
<date>2004</date>
<booktitle>In Computer Science: Reflections on the Field,</booktitle>
<pages>111--118</pages>
<publisher>Washington:National Academies Press.</publisher>
<contexts>
<context position="13631" citStr="Lee (2004)" startWordPosition="2013" endWordPosition="2014">eadings were highly directed (i.e., which sections and pages are most relevant to the course.) Supplemental materials that present introductory mathematics and tutorial presentations are not exhaustively listed, but included Jurafsky and Martin (2000), Goldsmith (2007, previously online) and a host of (other) online resources. History of linguistics and computation [1 lecture] The history is framed around the question “Is computational linguistics, well uh, linguistics?” We conclude with “It was, then it wasn’t, now maybe it is, or at least in part, should be.” The material is tightly tied to Lee (2004); with additional discussion along the lines of Sakas (2004). Syntax [1 lecture] This is a crash course in syntax using a context-free grammar with transformational movement. The more difficult topics include topicalization (including null-topic), Wh-movement and verb-second phenomena. We make effective use of an in-house database of abstract though linguistically viable crosslinguistic sentence patterns and tree structures – the CUNY CoLAG Domain (Sakas, 2003). The point of this lecture is to introduce non-linguistics students to the intricacies of a linguistically viable grammatical theory. </context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Lee, L. (2004). &amp;quot;I&apos;m sorry Dave, I&apos;m afraid I can&apos;t do that&amp;quot; : Linguistics, statistics, and natural language processing circa 2001. In Computer Science: Reflections on the Field, 111-118. Washington:National Academies Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lidz</author>
<author>S Waxman</author>
<author>J Freedman</author>
</authors>
<title>What infants know about syntax but couldn’t have learned: Experimental evidence for syntactic structure at 18 months.</title>
<date>2003</date>
<journal>Cognition</journal>
<pages>89--65</pages>
<contexts>
<context position="19646" citStr="Lidz et al. (2003)" startWordPosition="2913" endWordPosition="2916">(Reali &amp; Christiansen use a particularly impoverished version of deleted interpolation). We continue with a discussion of the pros and cons of employing Markov assumptions in computational linguistics generally, the relationship of Markov assumptions to incremental learning and psycholinguistic modeling, and the use of cross-entropy as an evaluation metric, and end with a brief discussion of the descriptional necessity (or not) of traditional generative grammars (Pereira, 2000). . &amp;quot;Ideal&amp;quot; learning, Bayesian learning and computational resources [1 lecture] Regier and Gahl (2004) in response to Lidz et al. (2003) present a Bayesian learning model that learns the correct structural referent for anaphoric &amp;quot;one&amp;quot; in English from a corpus of child-directed speech. Similarly to Reali &amp; Christiansen (op. cit.), they argue against the need for innate knowledge of hierarchal structure since their Bayesian model starts tabula rasa and learns from linear word strings with no readily observable structure. The fundamental mechanics of Bayesian inference is presented. Since most Bayesian models are able to retreat from overgeneral hypotheses in the absence of positive evidence, the course returns to overgeneralizat</context>
</contexts>
<marker>Lidz, Waxman, Freedman, 2003</marker>
<rawString>Lidz, J., Waxman, S., and Freedman, J. (2003). What infants know about syntax but couldn’t have learned: Experimental evidence for syntactic structure at 18 months. Cognition 89:65-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G F Marcus</author>
</authors>
<title>Negative evidence in language acquisition.</title>
<date>1993</date>
<journal>Cognition</journal>
<pages>46--53</pages>
<contexts>
<context position="14397" citStr="Marcus, 1993" startWordPosition="2121" endWordPosition="2122">formational movement. The more difficult topics include topicalization (including null-topic), Wh-movement and verb-second phenomena. We make effective use of an in-house database of abstract though linguistically viable crosslinguistic sentence patterns and tree structures – the CUNY CoLAG Domain (Sakas, 2003). The point of this lecture is to introduce non-linguistics students to the intricacies of a linguistically viable grammatical theory. Language Acquisition [1 lecture] We discuss some of the current debates in L1 (a child’s first) language acquisition surrounding “no negative evidence” (Marcus, 1993), Poverty of the Stimulus (Pullum, 1996), and Chomsky’s conceptualization of Universal Grammar. This is the least computational lecture of the semester, although it often generates some of the most energized discussion. The language acquisition unit is the central arena in which we stage most of the rest of the topics in the course. Gold and the Subset Principle [2 lectures] During the presentation of Gold’s (1967) and Angluin&apos;s (1980) proofs and discussion of how they might be used to argue (often incorrectly) for a Universal Grammar (Johnson, 2004) some core CL topics are introduced includin</context>
</contexts>
<marker>Marcus, 1993</marker>
<rawString>Marcus, G. F. (1993). Negative evidence in language acquisition. Cognition 46:53-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G F Marcus</author>
</authors>
<title>Can connectionism save constructivism?</title>
<date>1998</date>
<journal>Cognition</journal>
<pages>66--153</pages>
<contexts>
<context position="17084" citStr="Marcus, 1998" startWordPosition="2533" endWordPosition="2534">kers) but rather the process of capturing the “dynamics” of word patterns in the input stream. He demonstrates how this can be simulated in an SRN paradigm. The mechanics of how an SRN operates and can be used to model language acquisition phenomena is covered but more importantly core concepts common to most all supervised machine learning paradigms are emphasized. Topics include how training and testing corpora are developed and used, cross validation, hill-climbing, learning bias, 123 linear and non-linear separation of the hypothesis space, etc. A critique of SRN learning is also covered (Marcus, 1998) which presents the important distinction between generalization performance and learning within the training space in a way that is approachable by non-CS students, but also interesting to CS-students. Information retrieval [1 lecture] Elman (1990) uses hierarchal clustering to analyze some of his results. I use Elman&apos;s application of clustering to take a brief digression from the psycholinguistics theme of the course and present an introduction to vector space models and document clustering. This is the most challenging technical lecture of the semester and is included only when there are a </context>
</contexts>
<marker>Marcus, 1998</marker>
<rawString>Marcus, G. F. (1998). Can connectionism save constructivism? Cognition 66:153-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Formal grammar and information theory: Together again.</title>
<date>2000</date>
<journal>Philosophical Transactions of the Royal Society</journal>
<pages>358--1239</pages>
<contexts>
<context position="19510" citStr="Pereira, 2000" startWordPosition="2894" endWordPosition="2895"> We continue with a presentation of the sparse data problem, Zipf’s law, corpus cross-entropy and a handful of smoothing techniques (Reali &amp; Christiansen use a particularly impoverished version of deleted interpolation). We continue with a discussion of the pros and cons of employing Markov assumptions in computational linguistics generally, the relationship of Markov assumptions to incremental learning and psycholinguistic modeling, and the use of cross-entropy as an evaluation metric, and end with a brief discussion of the descriptional necessity (or not) of traditional generative grammars (Pereira, 2000). . &amp;quot;Ideal&amp;quot; learning, Bayesian learning and computational resources [1 lecture] Regier and Gahl (2004) in response to Lidz et al. (2003) present a Bayesian learning model that learns the correct structural referent for anaphoric &amp;quot;one&amp;quot; in English from a corpus of child-directed speech. Similarly to Reali &amp; Christiansen (op. cit.), they argue against the need for innate knowledge of hierarchal structure since their Bayesian model starts tabula rasa and learns from linear word strings with no readily observable structure. The fundamental mechanics of Bayesian inference is presented. Since most Ba</context>
</contexts>
<marker>Pereira, 2000</marker>
<rawString>Pereira, F. (2000) Formal grammar and information theory: Together again. Philosophical Transactions of the Royal Society A358:1239-1253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Pullum</author>
</authors>
<title>Learnability, hyperlearning, and the poverty of the stimulus.</title>
<date>1996</date>
<booktitle>Proceedings of the 22nd Annual Meeting of the Berkley Linguistics Society: General Session and Parasession on the Role of Learnability in Grammatical Theory,</booktitle>
<pages>498--513</pages>
<location>Berkeley:</location>
<contexts>
<context position="14437" citStr="Pullum, 1996" startWordPosition="2127" endWordPosition="2128"> topics include topicalization (including null-topic), Wh-movement and verb-second phenomena. We make effective use of an in-house database of abstract though linguistically viable crosslinguistic sentence patterns and tree structures – the CUNY CoLAG Domain (Sakas, 2003). The point of this lecture is to introduce non-linguistics students to the intricacies of a linguistically viable grammatical theory. Language Acquisition [1 lecture] We discuss some of the current debates in L1 (a child’s first) language acquisition surrounding “no negative evidence” (Marcus, 1993), Poverty of the Stimulus (Pullum, 1996), and Chomsky’s conceptualization of Universal Grammar. This is the least computational lecture of the semester, although it often generates some of the most energized discussion. The language acquisition unit is the central arena in which we stage most of the rest of the topics in the course. Gold and the Subset Principle [2 lectures] During the presentation of Gold’s (1967) and Angluin&apos;s (1980) proofs and discussion of how they might be used to argue (often incorrectly) for a Universal Grammar (Johnson, 2004) some core CL topics are introduced including formal language classes (the Chomsky H</context>
</contexts>
<marker>Pullum, 1996</marker>
<rawString>Pullum, G. K. (1996). Learnability, hyperlearning, and the poverty of the stimulus. Proceedings of the 22nd Annual Meeting of the Berkley Linguistics Society: General Session and Parasession on the Role of Learnability in Grammatical Theory, Berkeley: 498-513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Reali</author>
<author>M H Christiansen</author>
</authors>
<title>Uncovering the richness of the stimulus: Structural dependence and indirect statistical evidence.</title>
<date>2005</date>
<journal>Cognitive Science</journal>
<pages>29--1007</pages>
<contexts>
<context position="18316" citStr="Reali and Christiansen, (2005)" startWordPosition="2720" endWordPosition="2723">latively high proportion of CS students in attendance. Most of the linguistics students get a decent feel for the material, but most require a second exposure it in another course to fully understand the math. That said, the linguistics students do understand how weight heuristics are used to effectively represent documents in vectors (though most linguistics students have a hard time swallowing the bag-of-words paradigm at face value), and how vectors can be nearer or farther from each other in a hypothesis space. Ngram language models [3 lecture] In this unit we return to psycholinguistics. Reali and Christiansen, (2005) present a simple ngram language model of child-directed speech to argue against the need for innate UG-provided knowledge of hierarchal syntactic structure. Basic probability and information theory is introduced – conditional probabilities and Markov assumptions, the chain rule, Bayes Rule, maximum likelihood estimates, entropy, etc. Although relatively easy for the CS students (they had their hands full with the syntax unit), introduction of this material is invaluable to the linguistics students who need to be somewhat fluent in it before entering our other CL offerings. We continue with a </context>
</contexts>
<marker>Reali, Christiansen, 2005</marker>
<rawString>Reali, F., and Christiansen, M. H. (2005). Uncovering the richness of the stimulus: Structural dependence and indirect statistical evidence. Cognitive Science 29:1007-10018.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Regier</author>
<author>S Gahl</author>
</authors>
<title>Learning the unlearnable: The role of missing evidence.</title>
<date>2004</date>
<journal>Cognition</journal>
<pages>93--147</pages>
<contexts>
<context position="19612" citStr="Regier and Gahl (2004)" startWordPosition="2906" endWordPosition="2909">and a handful of smoothing techniques (Reali &amp; Christiansen use a particularly impoverished version of deleted interpolation). We continue with a discussion of the pros and cons of employing Markov assumptions in computational linguistics generally, the relationship of Markov assumptions to incremental learning and psycholinguistic modeling, and the use of cross-entropy as an evaluation metric, and end with a brief discussion of the descriptional necessity (or not) of traditional generative grammars (Pereira, 2000). . &amp;quot;Ideal&amp;quot; learning, Bayesian learning and computational resources [1 lecture] Regier and Gahl (2004) in response to Lidz et al. (2003) present a Bayesian learning model that learns the correct structural referent for anaphoric &amp;quot;one&amp;quot; in English from a corpus of child-directed speech. Similarly to Reali &amp; Christiansen (op. cit.), they argue against the need for innate knowledge of hierarchal structure since their Bayesian model starts tabula rasa and learns from linear word strings with no readily observable structure. The fundamental mechanics of Bayesian inference is presented. Since most Bayesian models are able to retreat from overgeneral hypotheses in the absence of positive evidence, the</context>
</contexts>
<marker>Regier, Gahl, 2004</marker>
<rawString>Regier, T., and Gahl, S. (2004). Learning the unlearnable: The role of missing evidence. Cognition 93:147-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W G Sakas</author>
<author>J D Fodor</author>
</authors>
<title>The Structural Triggers Learner.</title>
<date>2001</date>
<booktitle>In Language Acquisition and Learnability, ed. S. Bertolo,</booktitle>
<pages>172--233</pages>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge:</location>
<contexts>
<context position="21382" citStr="Sakas and Fodor, 2001" startWordPosition="3162" endWordPosition="3165">istics Program does not offer a linguistics-based learnability course. As a result, there is a unit on acquisition within a principles-and-parameters (P&amp;P) framework (Chomsky, 1981). Roughly, in the P&amp;P framework cross-linguistic commonalities are considered principles, and language variation is standardly specified by the settings of a bank of binary parameters (i.e., UG = principles + parameters; a specific language = principles + parameters + parameter settings). Although this unit is the furthest away from mainstream CL, it has served as a useful means to introduce deterministic learning (Sakas and Fodor, 2001), versus non-deterministic learning (Yang, 2002), the effectiveness of hill-climbing in 124 linguistically smooth and non-smooth domains,6 as well as the notion of computational complexity and combinatorial explosion (n binary parameters yields a search space of 2n possible grammars). Finally, and perhaps most importantly there is extensive discussion of the difficulty of building computational systems that can efficiently and correctly learn to navigate through domains with an enormous amount of ambiguity. In the P&amp;P framework ambiguity stems from competition of cross-linguistic structural an</context>
</contexts>
<marker>Sakas, Fodor, 2001</marker>
<rawString>Sakas, W. G., and Fodor, J. D. (2001). The Structural Triggers Learner. In Language Acquisition and Learnability, ed. S. Bertolo, 172-233. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W G Sakas</author>
</authors>
<title>A Word-Order Database for Testing Computational Models of Language Acquisition,</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, ACL-2003:</booktitle>
<pages>415--422</pages>
<contexts>
<context position="14096" citStr="Sakas, 2003" startWordPosition="2078" endWordPosition="2079">inguistics?” We conclude with “It was, then it wasn’t, now maybe it is, or at least in part, should be.” The material is tightly tied to Lee (2004); with additional discussion along the lines of Sakas (2004). Syntax [1 lecture] This is a crash course in syntax using a context-free grammar with transformational movement. The more difficult topics include topicalization (including null-topic), Wh-movement and verb-second phenomena. We make effective use of an in-house database of abstract though linguistically viable crosslinguistic sentence patterns and tree structures – the CUNY CoLAG Domain (Sakas, 2003). The point of this lecture is to introduce non-linguistics students to the intricacies of a linguistically viable grammatical theory. Language Acquisition [1 lecture] We discuss some of the current debates in L1 (a child’s first) language acquisition surrounding “no negative evidence” (Marcus, 1993), Poverty of the Stimulus (Pullum, 1996), and Chomsky’s conceptualization of Universal Grammar. This is the least computational lecture of the semester, although it often generates some of the most energized discussion. The language acquisition unit is the central arena in which we stage most of th</context>
</contexts>
<marker>Sakas, 2003</marker>
<rawString>Sakas, W. G. (2003) A Word-Order Database for Testing Computational Models of Language Acquisition, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, ACL-2003: 415-422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W G Sakas</author>
</authors>
<title>Introduction.</title>
<date>2004</date>
<booktitle>Proceedings of the First Workshop on Psycho-computational Models of Human Language Acquisition, COLING-2004. Geneva: iv-vi.</booktitle>
<contexts>
<context position="13691" citStr="Sakas (2004)" startWordPosition="2022" endWordPosition="2023">es are most relevant to the course.) Supplemental materials that present introductory mathematics and tutorial presentations are not exhaustively listed, but included Jurafsky and Martin (2000), Goldsmith (2007, previously online) and a host of (other) online resources. History of linguistics and computation [1 lecture] The history is framed around the question “Is computational linguistics, well uh, linguistics?” We conclude with “It was, then it wasn’t, now maybe it is, or at least in part, should be.” The material is tightly tied to Lee (2004); with additional discussion along the lines of Sakas (2004). Syntax [1 lecture] This is a crash course in syntax using a context-free grammar with transformational movement. The more difficult topics include topicalization (including null-topic), Wh-movement and verb-second phenomena. We make effective use of an in-house database of abstract though linguistically viable crosslinguistic sentence patterns and tree structures – the CUNY CoLAG Domain (Sakas, 2003). The point of this lecture is to introduce non-linguistics students to the intricacies of a linguistically viable grammatical theory. Language Acquisition [1 lecture] We discuss some of the curr</context>
</contexts>
<marker>Sakas, 2004</marker>
<rawString>Sakas, W. G. (2004) Introduction. Proceedings of the First Workshop on Psycho-computational Models of Human Language Acquisition, COLING-2004. Geneva: iv-vi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Yang</author>
</authors>
<title>Knowledge and learning in natural language.</title>
<date>2002</date>
<publisher>University Press.</publisher>
<location>New York: Oxford</location>
<contexts>
<context position="21430" citStr="Yang, 2002" startWordPosition="3169" endWordPosition="3170">ity course. As a result, there is a unit on acquisition within a principles-and-parameters (P&amp;P) framework (Chomsky, 1981). Roughly, in the P&amp;P framework cross-linguistic commonalities are considered principles, and language variation is standardly specified by the settings of a bank of binary parameters (i.e., UG = principles + parameters; a specific language = principles + parameters + parameter settings). Although this unit is the furthest away from mainstream CL, it has served as a useful means to introduce deterministic learning (Sakas and Fodor, 2001), versus non-deterministic learning (Yang, 2002), the effectiveness of hill-climbing in 124 linguistically smooth and non-smooth domains,6 as well as the notion of computational complexity and combinatorial explosion (n binary parameters yields a search space of 2n possible grammars). Finally, and perhaps most importantly there is extensive discussion of the difficulty of building computational systems that can efficiently and correctly learn to navigate through domains with an enormous amount of ambiguity. In the P&amp;P framework ambiguity stems from competition of cross-linguistic structural analyses of surface word order patterns. For examp</context>
</contexts>
<marker>Yang, 2002</marker>
<rawString>Yang, C. D. (2002). Knowledge and learning in natural language. New York: Oxford University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>